<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1698310857959" as="style"/><link rel="stylesheet" href="styles.css?v=1698310857959"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/">Jina AI launches open-source 8k text embedding</a> <span class="domain">(<a href="https://jina.ai">jina.ai</a>)</span></div><div class="subtext"><span>artex_xh</span> | <span>135 comments</span></div><br/><div><div id="38020327" class="c"><input type="checkbox" id="c-38020327" checked=""/><div class="controls bullet"><span class="by">burcs</span><span>|</span><a href="#38020753">next</a><span>|</span><label class="collapse" for="c-38020327">[-]</label><label class="expand" for="c-38020327">[21 more]</label></div><br/><div class="children"><div class="content">This is great news!<p>It feels like open-source is closing the gap with &quot;Open&quot;AI which is really exciting, and the acceleration towards parity is faster than more advancements made on the closed source models. Maybe it&#x27;s wishful thinking though?</div><br/><div id="38021387" class="c"><input type="checkbox" id="c-38021387" checked=""/><div class="controls bullet"><span class="by">udev4096</span><span>|</span><a href="#38020327">parent</a><span>|</span><a href="#38020753">next</a><span>|</span><label class="collapse" for="c-38021387">[-]</label><label class="expand" for="c-38021387">[20 more]</label></div><br/><div class="children"><div class="content">Is it tho? It&#x27;s not really open source if they don&#x27;t give us the information regarding training datasets</div><br/><div id="38021477" class="c"><input type="checkbox" id="c-38021477" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021387">parent</a><span>|</span><a href="#38020753">next</a><span>|</span><label class="collapse" for="c-38021477">[-]</label><label class="expand" for="c-38021477">[19 more]</label></div><br/><div class="children"><div class="content">It definitely is open source even if they don’t disclose all details behind the training</div><br/><div id="38021512" class="c"><input type="checkbox" id="c-38021512" checked=""/><div class="controls bullet"><span class="by">SOLAR_FIELDS</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021477">parent</a><span>|</span><a href="#38021571">next</a><span>|</span><label class="collapse" for="c-38021512">[-]</label><label class="expand" for="c-38021512">[14 more]</label></div><br/><div class="children"><div class="content">The very definition of what constitutes open source is being called into question in these kinds of discussions about AI. Without the training details and the weights being made fully open it’s hard to really call something truly open, even if it happens to meet some arbitrary definition of “open source”.<p>A good definition of “truly open” is whether the exact same results can be reproduced by someone with no extra information from only what has been made available. If that is not possible, because the reproduction methodology is closed (a common reason, like in this case) then what has been made available is not truly open.<p>We can sit here and technically argue whether or not the subject matter violated some arbitrary “open source” definition but it still doesn’t change the fact that it’s not truly open in spirit</div><br/><div id="38023330" class="c"><input type="checkbox" id="c-38023330" checked=""/><div class="controls bullet"><span class="by">pjc50</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021512">parent</a><span>|</span><a href="#38023112">next</a><span>|</span><label class="collapse" for="c-38023330">[-]</label><label class="expand" for="c-38023330">[1 more]</label></div><br/><div class="children"><div class="content">The old Stallman definition used the phrase &quot;preferred form for modification&quot; rather than the more specific &quot;source code&quot;. What do you need to effectively modify an AI model?</div><br/></div></div><div id="38023112" class="c"><input type="checkbox" id="c-38023112" checked=""/><div class="controls bullet"><span class="by">m3at</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021512">parent</a><span>|</span><a href="#38023330">prev</a><span>|</span><a href="#38022773">next</a><span>|</span><label class="collapse" for="c-38023112">[-]</label><label class="expand" for="c-38023112">[1 more]</label></div><br/><div class="children"><div class="content">To take an other example, would you call a game that has its code and all assets (ex. character sprites) freely available open source? Or would the process that was used to create the assets in the first place also be required to be considered open?<p>The parallel can be made with model weights being static assets delivered in their completed state.<p>(I favor the full process being released especially for scientific reproducibility, but this is an other point)</div><br/></div></div><div id="38022773" class="c"><input type="checkbox" id="c-38022773" checked=""/><div class="controls bullet"><span class="by">abriosi</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021512">parent</a><span>|</span><a href="#38023112">prev</a><span>|</span><a href="#38021863">next</a><span>|</span><label class="collapse" for="c-38022773">[-]</label><label class="expand" for="c-38022773">[3 more]</label></div><br/><div class="children"><div class="content">Imagine someone giving you a executable binary without the source code and calling it &quot;open source&quot;</div><br/><div id="38022985" class="c"><input type="checkbox" id="c-38022985" checked=""/><div class="controls bullet"><span class="by">jyrkesh</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38022773">parent</a><span>|</span><a href="#38022986">next</a><span>|</span><label class="collapse" for="c-38022985">[-]</label><label class="expand" for="c-38022985">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m actually mostly in your camp here. But it&#x27;s complicated with AI.<p>What if someone gave you a binary and the source code, but not a compiler? Maybe not even a language spec?<p>Or what if they gave you a binary and the source code and a fully documented language spec, and both of &#x27;em all the way down to the compiler? BUT it only runs on special proprietary silicon? Or maybe even the silicon is fully documented, but producing that silicon is effectively out of reach to all but F100 companies?<p>It&#x27;s turtles all the way down...</div><br/></div></div><div id="38022986" class="c"><input type="checkbox" id="c-38022986" checked=""/><div class="controls bullet"><span class="by">DougBTX</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38022773">parent</a><span>|</span><a href="#38022985">prev</a><span>|</span><a href="#38021863">next</a><span>|</span><label class="collapse" for="c-38022986">[-]</label><label class="expand" for="c-38022986">[1 more]</label></div><br/><div class="children"><div class="content">You can pass in any command line arguments you like, so it must be open source</div><br/></div></div></div></div><div id="38021863" class="c"><input type="checkbox" id="c-38021863" checked=""/><div class="controls bullet"><span class="by">okaram</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021512">parent</a><span>|</span><a href="#38022773">prev</a><span>|</span><a href="#38022943">next</a><span>|</span><label class="collapse" for="c-38021863">[-]</label><label class="expand" for="c-38021863">[4 more]</label></div><br/><div class="children"><div class="content">Notice you are creating your own arbitrary definition of &#x27;truly open&#x27;, which IMHO corresponds more with &#x27;reproducible&#x27;.<p>We already have a definition of open source. I don&#x27;t see any reason to change it.</div><br/><div id="38022026" class="c"><input type="checkbox" id="c-38022026" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021863">parent</a><span>|</span><a href="#38021936">next</a><span>|</span><label class="collapse" for="c-38022026">[-]</label><label class="expand" for="c-38022026">[1 more]</label></div><br/><div class="children"><div class="content">Problem is, the literal&#x2F;default definition of &quot;open source&quot; is meaningless&#x2F;worthless in this context. It&#x27;s the weights, training data and methodology that matter for those models - NOT the inference shell.<p>It&#x27;s basically like giving people a binary program and calling it open source because the compiler and runtime used are open source.</div><br/></div></div><div id="38021936" class="c"><input type="checkbox" id="c-38021936" checked=""/><div class="controls bullet"><span class="by">losteric</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021863">parent</a><span>|</span><a href="#38022026">prev</a><span>|</span><a href="#38022943">next</a><span>|</span><label class="collapse" for="c-38021936">[-]</label><label class="expand" for="c-38021936">[2 more]</label></div><br/><div class="children"><div class="content">The inference runtime software is open, the weights are an opaque binary. Publishing the training data, hyperparameters, process, etc - that would make the whole thing &quot;open source&quot;.</div><br/><div id="38023138" class="c"><input type="checkbox" id="c-38023138" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021936">parent</a><span>|</span><a href="#38022943">next</a><span>|</span><label class="collapse" for="c-38023138">[-]</label><label class="expand" for="c-38023138">[1 more]</label></div><br/><div class="children"><div class="content">The quake engine is still open source even though it doesn&#x27;t come with the quake game assets, no?<p>It seems unreasonable to require the training data just to be called open source, given it has similar copyright challenges as game assets.<p>Of course, this wouldn&#x27;t make the model reproducible. But that&#x27;s different from open source.</div><br/></div></div></div></div></div></div><div id="38022943" class="c"><input type="checkbox" id="c-38022943" checked=""/><div class="controls bullet"><span class="by">otikik</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021512">parent</a><span>|</span><a href="#38021863">prev</a><span>|</span><a href="#38022335">next</a><span>|</span><label class="collapse" for="c-38022943">[-]</label><label class="expand" for="c-38022943">[1 more]</label></div><br/><div class="children"><div class="content">Well the other day on this very website there were some very opinionated voices stating that Open Source is “exclusively what OSI defines”. I am not on that camp, more like in yours. To me there’s open source and OSI-approved open source. But you will encounter people very set on that other opinion, which I found interesting.<p>Make no mistake, I am super grateful to OSI for their efforts and most of my code out there uses one of their licenses. I just think they are limited by the circumstances. Some things I consider open are not conforming to their licenses and, like here, some things that conform might not be <i>really open</i>.</div><br/></div></div><div id="38022335" class="c"><input type="checkbox" id="c-38022335" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021512">parent</a><span>|</span><a href="#38022943">prev</a><span>|</span><a href="#38021946">next</a><span>|</span><label class="collapse" for="c-38022335">[-]</label><label class="expand" for="c-38022335">[2 more]</label></div><br/><div class="children"><div class="content">So if someone includes images in their project they need to tell you every brush stroke that led to the final image?<p>All sorts of intangibles end up in open source projects. This isn’t a science experiment that needs replication. They’re not trying to prove how they came up with the image&#x2F;code&#x2F;model.</div><br/><div id="38022851" class="c"><input type="checkbox" id="c-38022851" checked=""/><div class="controls bullet"><span class="by">xnorswap</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38022335">parent</a><span>|</span><a href="#38021946">next</a><span>|</span><label class="collapse" for="c-38022851">[-]</label><label class="expand" for="c-38022851">[1 more]</label></div><br/><div class="children"><div class="content">Those &quot;Brush Strokes&quot; are effectively the source code. To be considered open source, yes source code needs to be provided along side the binaries (the &quot;image&quot;).</div><br/></div></div></div></div><div id="38021946" class="c"><input type="checkbox" id="c-38021946" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021512">parent</a><span>|</span><a href="#38022335">prev</a><span>|</span><a href="#38021571">next</a><span>|</span><label class="collapse" for="c-38021946">[-]</label><label class="expand" for="c-38021946">[1 more]</label></div><br/><div class="children"><div class="content">Then a lot of stuff is not open source. Have you tried reproducing random GitHub repos, especially in machine learning?</div><br/></div></div></div></div><div id="38021571" class="c"><input type="checkbox" id="c-38021571" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021477">parent</a><span>|</span><a href="#38021512">prev</a><span>|</span><a href="#38020753">next</a><span>|</span><label class="collapse" for="c-38021571">[-]</label><label class="expand" for="c-38021571">[4 more]</label></div><br/><div class="children"><div class="content">How do you define &quot;source&quot;, then?<p>By this logic any freely downloadable executable software (a.k.a. freeware) is also open source, even though they don&#x27;t disclose all details on how to build it.</div><br/><div id="38021601" class="c"><input type="checkbox" id="c-38021601" checked=""/><div class="controls bullet"><span class="by">mogwire</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021571">parent</a><span>|</span><a href="#38020753">next</a><span>|</span><label class="collapse" for="c-38021601">[-]</label><label class="expand" for="c-38021601">[3 more]</label></div><br/><div class="children"><div class="content">Source would be the way the data is produced so that you can replicate it yourself and make changes.<p>If I hand you a beer for free that’s freeware. If I hand you the recipe and instructions to brew the beer that is open source.<p>We muddy the waters too much lately and call “free” to use things “open source”.</div><br/><div id="38022438" class="c"><input type="checkbox" id="c-38022438" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38021601">parent</a><span>|</span><a href="#38020753">next</a><span>|</span><label class="collapse" for="c-38022438">[-]</label><label class="expand" for="c-38022438">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>If I hand you a beer for free that’s freeware. If I hand you the recipe and instructions to brew the beer that is open source.</i><p>Yeah, but what those &quot;open source&quot; models are is like you handing me a bottle of beer, plus the instructions to <i>make the glass bottle</i>. You&#x27;re open-sourcing something, just not the part that matters. It&#x27;s not &quot;open source beer&quot;, it&#x27;s &quot;beer in an open-source bottle&quot;. In the same fashion, those models aren&#x27;t open source - they&#x27;re closed models inside a tiny open-source inference script.</div><br/><div id="38022971" class="c"><input type="checkbox" id="c-38022971" checked=""/><div class="controls bullet"><span class="by">imranhou</span><span>|</span><a href="#38020327">root</a><span>|</span><a href="#38022438">parent</a><span>|</span><a href="#38020753">next</a><span>|</span><label class="collapse" for="c-38022971">[-]</label><label class="expand" for="c-38022971">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps one more thing that is missing in context is that I&#x27;m also getting the right to alter that beer by adding anything I like to it and redistributing it, without knowing its true recipe.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38020753" class="c"><input type="checkbox" id="c-38020753" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#38020327">prev</a><span>|</span><a href="#38020552">next</a><span>|</span><label class="collapse" for="c-38020753">[-]</label><label class="expand" for="c-38020753">[31 more]</label></div><br/><div class="children"><div class="content">What is the use case for an 8k token embedding? My (somewhat limited) experience with long context models is they aren&#x27;t great for RAG. I get the impression they are optimized for something else, like writing 8k+ tokens rather than synthesizing responses.<p>Isn&#x27;t the normal way of using embedding to find relevant text snippets for a RAG prompt? Where is it better to have coarser retrieval?</div><br/><div id="38021364" class="c"><input type="checkbox" id="c-38021364" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38020753">parent</a><span>|</span><a href="#38020994">next</a><span>|</span><label class="collapse" for="c-38021364">[-]</label><label class="expand" for="c-38021364">[2 more]</label></div><br/><div class="children"><div class="content">&gt; What is the use case for an 8k token embedding?<p>Calculating embeddings on larger documents than smaller-window embedding models.<p>&gt; My (somewhat limited) experience with long context models is they aren&#x27;t great for RAG.<p>The only reason they wouldn&#x27;t be great for RAG is that they aren&#x27;t great at using information in their context window, which is possible (ISTR that some models have a strong recency bias within the window, for instance) but I don&#x27;t think is a general problem of long context models.<p>&gt; Isn&#x27;t the normal way of using embedding to find relevant text snippets for a RAG prompt?<p>I would say the usual use is for search and semantic similarity comparisons generally. RAG is itself an application of search, but its not the only one.</div><br/><div id="38022947" class="c"><input type="checkbox" id="c-38022947" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021364">parent</a><span>|</span><a href="#38020994">next</a><span>|</span><label class="collapse" for="c-38022947">[-]</label><label class="expand" for="c-38022947">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how the perfomance fair when context size is increased. Intuitively this should be higher, but some quantized models I&#x27;ve tested showed noticeably worst performance.</div><br/></div></div></div></div><div id="38020994" class="c"><input type="checkbox" id="c-38020994" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#38020753">parent</a><span>|</span><a href="#38021364">prev</a><span>|</span><a href="#38021029">next</a><span>|</span><label class="collapse" for="c-38020994">[-]</label><label class="expand" for="c-38020994">[13 more]</label></div><br/><div class="children"><div class="content">Is this what you mean by RAG? <a href="https:&#x2F;&#x2F;www.promptingguide.ai&#x2F;techniques&#x2F;rag" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.promptingguide.ai&#x2F;techniques&#x2F;rag</a>?</div><br/><div id="38021128" class="c"><input type="checkbox" id="c-38021128" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38020994">parent</a><span>|</span><a href="#38021031">next</a><span>|</span><label class="collapse" for="c-38021128">[-]</label><label class="expand" for="c-38021128">[11 more]</label></div><br/><div class="children"><div class="content">I have an explanation of RAG in the context of embeddings here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;#answering-questions-with-retrieval-augmented-generation" rel="nofollow noreferrer">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;#answering-...</a></div><br/><div id="38021353" class="c"><input type="checkbox" id="c-38021353" checked=""/><div class="controls bullet"><span class="by">Grimburger</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021128">parent</a><span>|</span><a href="#38021031">next</a><span>|</span><label class="collapse" for="c-38021353">[-]</label><label class="expand" for="c-38021353">[10 more]</label></div><br/><div class="children"><div class="content">You could just sum it up for us all rather than do a divert to your blog?<p>It&#x27;s Retrieval Augmented Generation btw.<p>To quote:<p>&gt; The key idea is this: a user asks a question. You search your private documents for content that appears relevant to the question, then paste excerpts of that content into the LLM (respecting its size limit, usually between 3,000 and 6,000 words) along with the original question.<p>&gt; The LLM can then answer the question based on the additional content you provided.</div><br/><div id="38021479" class="c"><input type="checkbox" id="c-38021479" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021353">parent</a><span>|</span><a href="#38022826">next</a><span>|</span><label class="collapse" for="c-38021479">[-]</label><label class="expand" for="c-38021479">[8 more]</label></div><br/><div class="children"><div class="content">&gt;  You could just sum it up for us all rather than do a divert to your blog?<p>Why? Have links gone out of fashion?<p>I even linked directly to the relevant section rather than linking to the top of the page.<p>The paper that coined the term used the hyphen, though I think I prefer it without: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.11401" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.11401</a></div><br/><div id="38021504" class="c"><input type="checkbox" id="c-38021504" checked=""/><div class="controls bullet"><span class="by">Grimburger</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021479">parent</a><span>|</span><a href="#38022826">next</a><span>|</span><label class="collapse" for="c-38021504">[-]</label><label class="expand" for="c-38021504">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Have links gone out of fashion?<p>Yes.<p>You wrote far more words than needed to answer the comment, I did it for you instead.</div><br/><div id="38021533" class="c"><input type="checkbox" id="c-38021533" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021504">parent</a><span>|</span><a href="#38021967">next</a><span>|</span><label class="collapse" for="c-38021533">[-]</label><label class="expand" for="c-38021533">[4 more]</label></div><br/><div class="children"><div class="content">One of the reasons I write so much stuff is so I can provide links to things I&#x27;ve written to answer relevant questions.</div><br/><div id="38021883" class="c"><input type="checkbox" id="c-38021883" checked=""/><div class="controls bullet"><span class="by">scubbo</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021533">parent</a><span>|</span><a href="#38022519">next</a><span>|</span><label class="collapse" for="c-38021883">[-]</label><label class="expand" for="c-38021883">[1 more]</label></div><br/><div class="children"><div class="content">And those of us with the sense to value your insight, and the attention-span to read more than tweet-sized content, thank you for it.</div><br/></div></div><div id="38022519" class="c"><input type="checkbox" id="c-38022519" checked=""/><div class="controls bullet"><span class="by">discordance</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021533">parent</a><span>|</span><a href="#38021883">prev</a><span>|</span><a href="#38021972">next</a><span>|</span><label class="collapse" for="c-38022519">[-]</label><label class="expand" for="c-38022519">[1 more]</label></div><br/><div class="children"><div class="content">Thanks so much for your writings and for posting the link (and also for Datasette!). I&#x27;ve learned in the past few months from your blog.</div><br/></div></div><div id="38021972" class="c"><input type="checkbox" id="c-38021972" checked=""/><div class="controls bullet"><span class="by">mhog_hn</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021533">parent</a><span>|</span><a href="#38022519">prev</a><span>|</span><a href="#38021967">next</a><span>|</span><label class="collapse" for="c-38021972">[-]</label><label class="expand" for="c-38021972">[1 more]</label></div><br/><div class="children"><div class="content">Thank you, nice blog.</div><br/></div></div></div></div><div id="38021967" class="c"><input type="checkbox" id="c-38021967" checked=""/><div class="controls bullet"><span class="by">gkbrk</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021504">parent</a><span>|</span><a href="#38021533">prev</a><span>|</span><a href="#38022834">next</a><span>|</span><label class="collapse" for="c-38021967">[-]</label><label class="expand" for="c-38021967">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Links have gone out of fashion&quot; is an odd thing to write on a Link Aggregator website.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38021031" class="c"><input type="checkbox" id="c-38021031" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38020994">parent</a><span>|</span><a href="#38021128">prev</a><span>|</span><a href="#38021029">next</a><span>|</span><label class="collapse" for="c-38021031">[-]</label><label class="expand" for="c-38021031">[1 more]</label></div><br/><div class="children"><div class="content">Yes</div><br/></div></div></div></div><div id="38021029" class="c"><input type="checkbox" id="c-38021029" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#38020753">parent</a><span>|</span><a href="#38020994">prev</a><span>|</span><a href="#38020552">next</a><span>|</span><label class="collapse" for="c-38021029">[-]</label><label class="expand" for="c-38021029">[15 more]</label></div><br/><div class="children"><div class="content">You could get a facsimile to a summary for a full article or short story. Reducing an 8k token article to a summary using a completions model would cost far more. So if you need to search through collections of contracts, scientific papers, movie scripts, etc. for recommendations&#x2F;clustering then bigger input sizes can do that in one shot.<p>Think of it like skipping the square root step in Euclidean distance. Perfectly valid as long as you don’t want a distance so much as a way to compare distances. And doing so skips the most computationally expensive operation.</div><br/><div id="38021080" class="c"><input type="checkbox" id="c-38021080" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021029">parent</a><span>|</span><a href="#38020552">next</a><span>|</span><label class="collapse" for="c-38021080">[-]</label><label class="expand" for="c-38021080">[14 more]</label></div><br/><div class="children"><div class="content">I think I&#x27;m missing something: like, yeah, it&#x27;s vector search for bigger text chunks. But arguably vector search with bigger text chunks is _definitively_ worse -- this isn&#x27;t doing summarization, just turning about 25 pages of text to 1024 floats, which you then can use cosine similarity to measure the semantic similarity to other text<p>I&#x27;d much rather know what paragraph to look in than what 25 pages to look in</div><br/><div id="38021111" class="c"><input type="checkbox" id="c-38021111" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021080">parent</a><span>|</span><a href="#38022577">next</a><span>|</span><label class="collapse" for="c-38021111">[-]</label><label class="expand" for="c-38021111">[12 more]</label></div><br/><div class="children"><div class="content">I imagine it&#x27;s more useful for finding related articles and clustering things than for semantic search, which will work much better against smaller chunks - especially if you&#x27;re implementing Retrieval Augmented Generation.</div><br/><div id="38021260" class="c"><input type="checkbox" id="c-38021260" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021111">parent</a><span>|</span><a href="#38022577">next</a><span>|</span><label class="collapse" for="c-38021260">[-]</label><label class="expand" for="c-38021260">[11 more]</label></div><br/><div class="children"><div class="content">I think the point is: if you compress 25 pages of text into 1024 floats, you will lose a ton of information, regardless of what the use case is, so you&#x27;re probably still better of with chunking.</div><br/><div id="38022504" class="c"><input type="checkbox" id="c-38022504" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021260">parent</a><span>|</span><a href="#38023041">next</a><span>|</span><label class="collapse" for="c-38022504">[-]</label><label class="expand" for="c-38022504">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>if you compress 25 pages of text into 1024 floats, you will lose a ton of information</i><p>Sure, but then if you do it one page at a time, or one paragraph at a time, you lose ton of <i>meaning</i> - after all, individual paragraphs aren&#x27;t independent of each other. And meaning is kind of the whole point of the exercise.<p>Or put another way, squashing a ton of text loses you some high-frequency information, while chunking cuts off the low-frequency parts. Ideally you&#x27;d want to retain both.</div><br/></div></div><div id="38023041" class="c"><input type="checkbox" id="c-38023041" checked=""/><div class="controls bullet"><span class="by">imranhou</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021260">parent</a><span>|</span><a href="#38022504">prev</a><span>|</span><a href="#38021267">next</a><span>|</span><label class="collapse" for="c-38023041">[-]</label><label class="expand" for="c-38023041">[1 more]</label></div><br/><div class="children"><div class="content">Good point, I wonder how different it is to use a large context here vs having some other model summarize an 8k article into a small paragraph and using embedding from the paragraph instead where such a large context wouldn&#x27;t be necessary.</div><br/></div></div><div id="38021267" class="c"><input type="checkbox" id="c-38021267" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021260">parent</a><span>|</span><a href="#38023041">prev</a><span>|</span><a href="#38021944">next</a><span>|</span><label class="collapse" for="c-38021267">[-]</label><label class="expand" for="c-38021267">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been getting great results for related documents by embedding entire blog posts, e.g. here: <a href="https:&#x2F;&#x2F;til.simonwillison.net&#x2F;gis&#x2F;pmtiles#related" rel="nofollow noreferrer">https:&#x2F;&#x2F;til.simonwillison.net&#x2F;gis&#x2F;pmtiles#related</a><p>I&#x27;m not sure how I would do that after chunking.</div><br/><div id="38021838" class="c"><input type="checkbox" id="c-38021838" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021267">parent</a><span>|</span><a href="#38021944">next</a><span>|</span><label class="collapse" for="c-38021838">[-]</label><label class="expand" for="c-38021838">[5 more]</label></div><br/><div class="children"><div class="content">Did you compare with simple baselines like bag-of-words and word vectors?</div><br/><div id="38021868" class="c"><input type="checkbox" id="c-38021868" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021838">parent</a><span>|</span><a href="#38021944">next</a><span>|</span><label class="collapse" for="c-38021868">[-]</label><label class="expand" for="c-38021868">[4 more]</label></div><br/><div class="children"><div class="content">My previous implementation used TF-IDF - I basically took all the words in the post and turned them into a giant &quot;word OR word OR word OR word&quot; search query and piped that through SQLite full-text search. <a href="https:&#x2F;&#x2F;til.simonwillison.net&#x2F;sqlite&#x2F;related-content" rel="nofollow noreferrer">https:&#x2F;&#x2F;til.simonwillison.net&#x2F;sqlite&#x2F;related-content</a><p>I jumped straight from that to OpenAI embeddings. The results were good enough that I didn&#x27;t spend time investigating other approaches.</div><br/><div id="38022864" class="c"><input type="checkbox" id="c-38022864" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021868">parent</a><span>|</span><a href="#38021995">next</a><span>|</span><label class="collapse" for="c-38022864">[-]</label><label class="expand" for="c-38022864">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Into a giant &quot;word OR word OR word OR word&quot;<p>Does that mean you&#x27;d return other docs if they share just one word?<p>The idea of tfidf is that it gives you a vector (maybe combined with pca or a random dimensionality reduction) that you can use just like an Ada embedding. But you still need vector search.</div><br/></div></div><div id="38021995" class="c"><input type="checkbox" id="c-38021995" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021868">parent</a><span>|</span><a href="#38022864">prev</a><span>|</span><a href="#38021944">next</a><span>|</span><label class="collapse" for="c-38021995">[-]</label><label class="expand" for="c-38021995">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not quite tfidf though. I agree you can get better results than that with Ada embeddings, but I would argue you can get even better results with embeddings from smaller chunks.</div><br/><div id="38022093" class="c"><input type="checkbox" id="c-38022093" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021995">parent</a><span>|</span><a href="#38021944">next</a><span>|</span><label class="collapse" for="c-38022093">[-]</label><label class="expand" for="c-38022093">[1 more]</label></div><br/><div class="children"><div class="content">I guess technically it&#x27;s bm25, since it&#x27;s using the rank mechanism in SQLite FTS5: <a href="https:&#x2F;&#x2F;www.sqlite.org&#x2F;fts5.html#sorting_by_auxiliary_function_results" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sqlite.org&#x2F;fts5.html#sorting_by_auxiliary_functi...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="38021944" class="c"><input type="checkbox" id="c-38021944" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021260">parent</a><span>|</span><a href="#38021267">prev</a><span>|</span><a href="#38022577">next</a><span>|</span><label class="collapse" for="c-38021944">[-]</label><label class="expand" for="c-38021944">[2 more]</label></div><br/><div class="children"><div class="content">Ever read the back of a book?</div><br/><div id="38022629" class="c"><input type="checkbox" id="c-38022629" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021944">parent</a><span>|</span><a href="#38022577">next</a><span>|</span><label class="collapse" for="c-38022629">[-]</label><label class="expand" for="c-38022629">[1 more]</label></div><br/><div class="children"><div class="content">You mean the marketing blurb? Those tend to carry low information value, sometimes even <i>negative</i> - as in, if you didn&#x27;t know anything else about the book, reading the blurb will make you <i>even more wrong</i> about it than you were. This is a common feature of marketing copy.</div><br/></div></div></div></div></div></div></div></div><div id="38022577" class="c"><input type="checkbox" id="c-38022577" checked=""/><div class="controls bullet"><span class="by">antman</span><span>|</span><a href="#38020753">root</a><span>|</span><a href="#38021080">parent</a><span>|</span><a href="#38021111">prev</a><span>|</span><a href="#38020552">next</a><span>|</span><label class="collapse" for="c-38022577">[-]</label><label class="expand" for="c-38022577">[1 more]</label></div><br/><div class="children"><div class="content">you could do both</div><br/></div></div></div></div></div></div></div></div><div id="38020552" class="c"><input type="checkbox" id="c-38020552" checked=""/><div class="controls bullet"><span class="by">jncraton</span><span>|</span><a href="#38020753">prev</a><span>|</span><a href="#38023187">next</a><span>|</span><label class="collapse" for="c-38020552">[-]</label><label class="expand" for="c-38020552">[5 more]</label></div><br/><div class="children"><div class="content">This is great to see. It looks like the size of the embedding vector is half the size of text-embedding-ada-002 (768 vs 1536) while providing competitive performance. This will save space in databases and make lookups somewhat faster.<p>For those unaware, if 512 tokens of context is sufficient for your use case, there are already many options that outperform text-embedding-ada-002 on common benchmarks:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard</a></div><br/><div id="38020878" class="c"><input type="checkbox" id="c-38020878" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38020552">parent</a><span>|</span><a href="#38023187">next</a><span>|</span><label class="collapse" for="c-38020878">[-]</label><label class="expand" for="c-38020878">[4 more]</label></div><br/><div class="children"><div class="content">The 768D-sized embeddings compared to OpenAI&#x27;s 1536D embeddings are actually a feature outside of index size.<p>In my experience, OpenAI&#x27;s embeddings are overspecified and do very poorly with cosine similarity out of the box as they match syntax more than semantic meaning (which is important as that&#x27;s the metric for RAG). Ideally you&#x27;d want cosine similarity in the range of [-1, 1] on a variety of data but in my experience the results are [0.6, 0.8].</div><br/><div id="38022739" class="c"><input type="checkbox" id="c-38022739" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38020552">root</a><span>|</span><a href="#38020878">parent</a><span>|</span><a href="#38022106">next</a><span>|</span><label class="collapse" for="c-38022739">[-]</label><label class="expand" for="c-38022739">[1 more]</label></div><br/><div class="children"><div class="content">Unless I&#x27;m missing something, it should be possible to map out in advance which dimensions represent syntactic aspects, and then downweigh or remove them for similarity comparisons. And that map should be a function of the model alone, i.e. fully reusable. Are there any efforts to map out the latent space of ada models like that?</div><br/></div></div><div id="38022106" class="c"><input type="checkbox" id="c-38022106" checked=""/><div class="controls bullet"><span class="by">karxxm</span><span>|</span><a href="#38020552">root</a><span>|</span><a href="#38020878">parent</a><span>|</span><a href="#38022739">prev</a><span>|</span><a href="#38023187">next</a><span>|</span><label class="collapse" for="c-38022106">[-]</label><label class="expand" for="c-38022106">[2 more]</label></div><br/><div class="children"><div class="content">You wrote „out of the box“, did you find a way to improve this?</div><br/><div id="38022207" class="c"><input type="checkbox" id="c-38022207" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#38020552">root</a><span>|</span><a href="#38022106">parent</a><span>|</span><a href="#38023187">next</a><span>|</span><label class="collapse" for="c-38022207">[-]</label><label class="expand" for="c-38022207">[1 more]</label></div><br/><div class="children"><div class="content">You can do PCA or some other dimensionality reduction technique. That’ll reduce computation and improve signal&#x2F;noise ratio when comparing vectors.</div><br/></div></div></div></div></div></div></div></div><div id="38023187" class="c"><input type="checkbox" id="c-38023187" checked=""/><div class="controls bullet"><span class="by">Kutsuya</span><span>|</span><a href="#38020552">prev</a><span>|</span><a href="#38020655">next</a><span>|</span><label class="collapse" for="c-38023187">[-]</label><label class="expand" for="c-38023187">[1 more]</label></div><br/><div class="children"><div class="content">this is super cool! I wish there was an easy to understand and follow guide on how to make your own embedding, for llama2 for example. All I can find are various guides that already assume you know everything there is to training an embedding.<p>I just want to make an embedding between a conversation of me and my friend and simulate talking to them. Is this a hard thing to train to begin with?<p>If anyone knows or could help me with this, I would be very grateful!</div><br/></div></div><div id="38020655" class="c"><input type="checkbox" id="c-38020655" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38023187">prev</a><span>|</span><a href="#38020376">next</a><span>|</span><label class="collapse" for="c-38020655">[-]</label><label class="expand" for="c-38020655">[7 more]</label></div><br/><div class="children"><div class="content">I just shipped a new llm-embed-jina plugin for my LLM tool which provides access to these new Jina models: <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-embed-jina">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-embed-jina</a><p>Here&#x27;s how to try it out.<p>First, install LLM. Use pip or pipx or brew:<p><pre><code>    brew install llm
</code></pre>
Next install the new plugin:<p><pre><code>    llm install llm-embed-jina
</code></pre>
You can confirm the new models are now available to LLM by running:<p><pre><code>    llm embed-models
</code></pre>
You should see a list that includes &quot;jina-embeddings-v2-small-en&quot; and &quot;jina-embeddings-v2-base-en&quot;<p>To embed a string using the small model, run this:<p><pre><code>    llm embed -m jina-embeddings-v2-small-en -c &#x27;Hello world&#x27;
</code></pre>
That will output a JSON array of 512 floating point numbers (see my explainer here for what those are: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;#what-are-embeddings" rel="nofollow noreferrer">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;#what-are-e...</a>)<p>Embeddings are only really interesting if you store them and use them for comparisons.<p>Here&#x27;s how to use the &quot;llm embed-multi&quot; command to create embeddings for the 30 most recent issues in my LLM GitHub repository:<p><pre><code>    curl &#x27;https:&#x2F;&#x2F;api.github.com&#x2F;repos&#x2F;simonw&#x2F;llm&#x2F;issues?state=all&amp;filter=all&#x27; \
    | jq &#x27;[.[] | {id: .id, title: .title}]&#x27; \
    | llm embed-multi -m jina-embeddings-v2-small-en jina-llm-issues - \
    --store
</code></pre>
This creates a collection called &quot;jina-llm-issues&quot; in a default SQLite database on your machine (the path to that can be found using &quot;llm collections path&quot;).<p>To search for issues in that collection with titles most similar to the term &quot;bug&quot;:<p><pre><code>    llm similar jina-llm-issues -c &#x27;bug&#x27;
</code></pre>
Or for issues most similar to another existing issue by ID:<p><pre><code>    llm similar jina-llm-issues 1922688957
</code></pre>
Full documentation on what you can do with LLM and embeddings here: <a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;embeddings&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;embeddings&#x2F;index.html</a><p>Alternative recipe - this creates embeddings for every single README.md in the current directory and its subdirectories. Run this somewhere with a node_modules folder and you should get a whole lot of interesting stuff:<p><pre><code>    llm embed-multi jina-readmes \
      -m jina-embeddings-v2-small-en \
      --files . &#x27;**&#x2F;README.md&#x27; --store
</code></pre>
Then search them like this:<p><pre><code>    llm similar jina-readmes -c &#x27;backup tools&#x27;</code></pre></div><br/><div id="38021434" class="c"><input type="checkbox" id="c-38021434" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020655">parent</a><span>|</span><a href="#38021264">next</a><span>|</span><label class="collapse" for="c-38021434">[-]</label><label class="expand" for="c-38021434">[1 more]</label></div><br/><div class="children"><div class="content">Wrote this up on my blog: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;26&#x2F;llm-embed-jina&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;26&#x2F;llm-embed-jina&#x2F;</a></div><br/></div></div><div id="38021264" class="c"><input type="checkbox" id="c-38021264" checked=""/><div class="controls bullet"><span class="by">bosky101</span><span>|</span><a href="#38020655">parent</a><span>|</span><a href="#38021434">prev</a><span>|</span><a href="#38022125">next</a><span>|</span><label class="collapse" for="c-38021264">[-]</label><label class="expand" for="c-38021264">[1 more]</label></div><br/><div class="children"><div class="content">The only feedback I had from your embedding post was<p><pre><code>    wish we could create the array of floating points without openai

</code></pre>
Great timely turnaround time, good sir. Ht</div><br/></div></div><div id="38022125" class="c"><input type="checkbox" id="c-38022125" checked=""/><div class="controls bullet"><span class="by">mike_ivanov</span><span>|</span><a href="#38020655">parent</a><span>|</span><a href="#38021264">prev</a><span>|</span><a href="#38020674">next</a><span>|</span><label class="collapse" for="c-38022125">[-]</label><label class="expand" for="c-38022125">[2 more]</label></div><br/><div class="children"><div class="content">JFYI, this is what happens on my M1 Macbook:<p>$ brew install llm
$ llm
ModuleNotFoundError: No module named &#x27;typing_extensions&#x27;<p>Not sure where to report it.</div><br/><div id="38022287" class="c"><input type="checkbox" id="c-38022287" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020655">root</a><span>|</span><a href="#38022125">parent</a><span>|</span><a href="#38020674">next</a><span>|</span><label class="collapse" for="c-38022287">[-]</label><label class="expand" for="c-38022287">[1 more]</label></div><br/><div class="children"><div class="content">Whoa, that is a weird one. Do you know what version of Python you have from Homebrew?<p>It looks like that package is correctly listed in the formula: <a href="https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;homebrew-core&#x2F;blob&#x2F;a0048881ba9a283cdc523bb09f608668004ad6f0&#x2F;Formula&#x2F;l&#x2F;llm.rb#L27">https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;homebrew-core&#x2F;blob&#x2F;a0048881ba9a2...</a></div><br/></div></div></div></div><div id="38020674" class="c"><input type="checkbox" id="c-38020674" checked=""/><div class="controls bullet"><span class="by">X6S1x6Okd1st</span><span>|</span><a href="#38020655">parent</a><span>|</span><a href="#38022125">prev</a><span>|</span><a href="#38020939">next</a><span>|</span><label class="collapse" for="c-38020674">[-]</label><label class="expand" for="c-38020674">[1 more]</label></div><br/><div class="children"><div class="content">Thank you so much for all the work you&#x27;ve put into llm!</div><br/></div></div><div id="38020939" class="c"><input type="checkbox" id="c-38020939" checked=""/><div class="controls bullet"><span class="by">dazzaji</span><span>|</span><a href="#38020655">parent</a><span>|</span><a href="#38020674">prev</a><span>|</span><a href="#38020376">next</a><span>|</span><label class="collapse" for="c-38020939">[-]</label><label class="expand" for="c-38020939">[1 more]</label></div><br/><div class="children"><div class="content">Excellent! And you were just saying how risky it is to rely long-term on OpenAI text embeddings in your post on the topic. The timing for this open source option worked out nicely.</div><br/></div></div></div></div><div id="38020376" class="c"><input type="checkbox" id="c-38020376" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#38020655">prev</a><span>|</span><a href="#38021291">next</a><span>|</span><label class="collapse" for="c-38020376">[-]</label><label class="expand" for="c-38020376">[4 more]</label></div><br/><div class="children"><div class="content">Impressive work.<p>I wonder what would be the best way to use 8k embeddings. It’s a lot of information to keep in a vector, so things like “precision” of the embedding space and its ability to distinguish very similar large documents will be key.<p>Maybe it can be useful for coarse similarity matching, for example to detect plagiarism?</div><br/><div id="38020416" class="c"><input type="checkbox" id="c-38020416" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38020376">parent</a><span>|</span><a href="#38021291">next</a><span>|</span><label class="collapse" for="c-38020416">[-]</label><label class="expand" for="c-38020416">[3 more]</label></div><br/><div class="children"><div class="content">8K is the context length. Their vector dimension size is actual much smaller, which is great for a number of use cases, though maybe not the ones you are thinking about.</div><br/><div id="38020631" class="c"><input type="checkbox" id="c-38020631" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#38020376">root</a><span>|</span><a href="#38020416">parent</a><span>|</span><a href="#38020480">next</a><span>|</span><label class="collapse" for="c-38020631">[-]</label><label class="expand" for="c-38020631">[1 more]</label></div><br/><div class="children"><div class="content">Yes that’s also how I understood it. Maybe it was ambiguously expressed, but I mean “8k tokens as input is a lot of information to encode”</div><br/></div></div></div></div></div></div><div id="38021291" class="c"><input type="checkbox" id="c-38021291" checked=""/><div class="controls bullet"><span class="by">marinhero</span><span>|</span><a href="#38020376">prev</a><span>|</span><a href="#38020756">next</a><span>|</span><label class="collapse" for="c-38021291">[-]</label><label class="expand" for="c-38021291">[5 more]</label></div><br/><div class="children"><div class="content">How well do LLMS like this work with a non-English language? Or are these open source models limited to English?</div><br/><div id="38021545" class="c"><input type="checkbox" id="c-38021545" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38021291">parent</a><span>|</span><a href="#38021991">next</a><span>|</span><label class="collapse" for="c-38021545">[-]</label><label class="expand" for="c-38021545">[1 more]</label></div><br/><div class="children"><div class="content">Quite a few of the top ranked models on this leaderboard are multilingual: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;BAAI&#x2F;bge-large-en-v1.5" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;BAAI&#x2F;bge-large-en-v1.5</a> FlagEmbedding for example describes itself as covering Chinese and English.</div><br/></div></div><div id="38021991" class="c"><input type="checkbox" id="c-38021991" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#38021291">parent</a><span>|</span><a href="#38021545">prev</a><span>|</span><a href="#38021831">next</a><span>|</span><label class="collapse" for="c-38021991">[-]</label><label class="expand" for="c-38021991">[2 more]</label></div><br/><div class="children"><div class="content">Stability has a Japanese port which is getting lots of work
<a href="https:&#x2F;&#x2F;twitter.com&#x2F;StabilityAI_JP&#x2F;status&#x2F;1716998578244407597" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;StabilityAI_JP&#x2F;status&#x2F;171699857824440759...</a></div><br/><div id="38023150" class="c"><input type="checkbox" id="c-38023150" checked=""/><div class="controls bullet"><span class="by">m3at</span><span>|</span><a href="#38021291">root</a><span>|</span><a href="#38021991">parent</a><span>|</span><a href="#38021831">next</a><span>|</span><label class="collapse" for="c-38023150">[-]</label><label class="expand" for="c-38023150">[1 more]</label></div><br/><div class="children"><div class="content">This is not an embedding model though. Yes you can always extract some embeddings from somewhere, but for most LLMs those won&#x27;t perform well for retrieval (which makes sense as it&#x27;s not what the models are optimizing for)</div><br/></div></div></div></div><div id="38021831" class="c"><input type="checkbox" id="c-38021831" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#38021291">parent</a><span>|</span><a href="#38021991">prev</a><span>|</span><a href="#38020756">next</a><span>|</span><label class="collapse" for="c-38021831">[-]</label><label class="expand" for="c-38021831">[1 more]</label></div><br/><div class="children"><div class="content">That depends on whether the training data contained languages other than English.</div><br/></div></div></div></div><div id="38020756" class="c"><input type="checkbox" id="c-38020756" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#38021291">prev</a><span>|</span><a href="#38022553">next</a><span>|</span><label class="collapse" for="c-38020756">[-]</label><label class="expand" for="c-38020756">[4 more]</label></div><br/><div class="children"><div class="content">Ada is one of the (if not the) worst model offered by OpenAI, though ...</div><br/><div id="38020804" class="c"><input type="checkbox" id="c-38020804" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020756">parent</a><span>|</span><a href="#38022553">next</a><span>|</span><label class="collapse" for="c-38020804">[-]</label><label class="expand" for="c-38020804">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re thinking of the old &quot;ada&quot; GPT-3 model - the one that was a companion to &quot;davinci&quot; and &quot;babbage&quot;.<p>I believe &quot;text-embedding-ada-002&quot; is entirely unrelated to those old GPT-3 models. It&#x27;s a recent embedding model (released in December 2022 - <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;new-and-improved-embedding-model" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;new-and-improved-embedding-model</a> ) which OpenAI claim is their best current best available embedding model.<p>I understand your confusion: OpenAI are notoriously bad at naming things!</div><br/><div id="38020821" class="c"><input type="checkbox" id="c-38020821" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#38020756">root</a><span>|</span><a href="#38020804">parent</a><span>|</span><a href="#38022553">next</a><span>|</span><label class="collapse" for="c-38020821">[-]</label><label class="expand" for="c-38020821">[2 more]</label></div><br/><div class="children"><div class="content">Oh, thanks for clarifying!<p>Edit: looking at the press release, the improvement over old Ada is ... marginal? And Ada-01 is&#x2F;was a poor performing model, tbh. I guess I&#x27;ll have to run some tests, but at first sight it doesn&#x27;t seem that wow-ey.</div><br/><div id="38022948" class="c"><input type="checkbox" id="c-38022948" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#38020756">root</a><span>|</span><a href="#38020821">parent</a><span>|</span><a href="#38022553">next</a><span>|</span><label class="collapse" for="c-38022948">[-]</label><label class="expand" for="c-38022948">[1 more]</label></div><br/><div class="children"><div class="content">So just to be super clear, this is an embedding model. It generates no text. It’s not outputting words.<p>Maybe I am assuming incorrectly, but I think the poor performance you are referring to is the old Ada completion model, where the output is text. That was poor indeed.</div><br/></div></div></div></div></div></div></div></div><div id="38022553" class="c"><input type="checkbox" id="c-38022553" checked=""/><div class="controls bullet"><span class="by">dylanjcastillo</span><span>|</span><a href="#38020756">prev</a><span>|</span><a href="#38021154">next</a><span>|</span><label class="collapse" for="c-38022553">[-]</label><label class="expand" for="c-38022553">[2 more]</label></div><br/><div class="children"><div class="content">I wonder how much better is this, compared to taking the average ( or some other aggregation) of embeddings with a smaller context length. Has anyone done a similar comparison?</div><br/><div id="38022630" class="c"><input type="checkbox" id="c-38022630" checked=""/><div class="controls bullet"><span class="by">pietro72ohboy</span><span>|</span><a href="#38022553">parent</a><span>|</span><a href="#38021154">next</a><span>|</span><label class="collapse" for="c-38022630">[-]</label><label class="expand" for="c-38022630">[1 more]</label></div><br/><div class="children"><div class="content">The issue with averaging is that over large inputs, it drowns out small signal. For example, there is a chance that it completely loses a reference to something made only in a single sentence somewhere in a large document.</div><br/></div></div></div></div><div id="38021154" class="c"><input type="checkbox" id="c-38021154" checked=""/><div class="controls bullet"><span class="by">nicognaw</span><span>|</span><a href="#38022553">prev</a><span>|</span><a href="#38022939">next</a><span>|</span><label class="collapse" for="c-38021154">[-]</label><label class="expand" for="c-38021154">[1 more]</label></div><br/><div class="children"><div class="content">Jina AI itself is also a great framework to expose APIs from deep neural net models and deploy them to Kubernetes clusters, which I think is very promising, but they didn&#x27;t get as much hype as I predicted that they deserved.</div><br/></div></div><div id="38022939" class="c"><input type="checkbox" id="c-38022939" checked=""/><div class="controls bullet"><span class="by">extasia</span><span>|</span><a href="#38021154">prev</a><span>|</span><a href="#38021759">next</a><span>|</span><label class="collapse" for="c-38022939">[-]</label><label class="expand" for="c-38022939">[1 more]</label></div><br/><div class="children"><div class="content">Is this a text encoder model, BERT style?</div><br/></div></div><div id="38021252" class="c"><input type="checkbox" id="c-38021252" checked=""/><div class="controls bullet"><span class="by">pknerd</span><span>|</span><a href="#38020865">prev</a><span>|</span><a href="#38020413">next</a><span>|</span><label class="collapse" for="c-38021252">[-]</label><label class="expand" for="c-38021252">[7 more]</label></div><br/><div class="children"><div class="content">Pardon my ignorance in advance but could it be used to &quot;chat&quot; with PDFs and websites? I am looking for OpenAI alternatives as I am in learning phase</div><br/><div id="38021696" class="c"><input type="checkbox" id="c-38021696" checked=""/><div class="controls bullet"><span class="by">lofties</span><span>|</span><a href="#38021252">parent</a><span>|</span><a href="#38023190">next</a><span>|</span><label class="collapse" for="c-38021696">[-]</label><label class="expand" for="c-38021696">[2 more]</label></div><br/><div class="children"><div class="content">No. “Chatting with PDFs” is (mostly) taking a users chat message, retrieve relevant content via e.g embedding search, then feed that into an LLM with a prompt that’s something along the lines of “given this information, can you answer this question”.<p>This tool helps with embedding part.<p>I’ve built a bunch of ”chat with your PDFs” bots, do reach out if you have any questions me at brian.jp.</div><br/><div id="38021775" class="c"><input type="checkbox" id="c-38021775" checked=""/><div class="controls bullet"><span class="by">pknerd</span><span>|</span><a href="#38021252">root</a><span>|</span><a href="#38021696">parent</a><span>|</span><a href="#38023190">next</a><span>|</span><label class="collapse" for="c-38021775">[-]</label><label class="expand" for="c-38021775">[1 more]</label></div><br/><div class="children"><div class="content">Actually I wanna use langchain. OpwnAI is not free. I wanted to test two use cases:<p>- chat with documents(pdf, doc etc)<p>- chat with website. Like, if I integrate with an ecommerce site, I can ask questions from the website. What options do I have in free for both cloud and locally?</div><br/></div></div></div></div><div id="38023190" class="c"><input type="checkbox" id="c-38023190" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#38021252">parent</a><span>|</span><a href="#38021696">prev</a><span>|</span><a href="#38021349">next</a><span>|</span><label class="collapse" for="c-38023190">[-]</label><label class="expand" for="c-38023190">[1 more]</label></div><br/><div class="children"><div class="content">using the bing tab of microsoft edge browser, you can chat with PDFs and i think they use GTP4 or equivalent</div><br/></div></div><div id="38021349" class="c"><input type="checkbox" id="c-38021349" checked=""/><div class="controls bullet"><span class="by">clarkmcc</span><span>|</span><a href="#38021252">parent</a><span>|</span><a href="#38023190">prev</a><span>|</span><a href="#38021670">next</a><span>|</span><label class="collapse" for="c-38021349">[-]</label><label class="expand" for="c-38021349">[2 more]</label></div><br/><div class="children"><div class="content">Check out my little side project for chatting with PDFs. You should be able to load most models including this one. <a href="https:&#x2F;&#x2F;github.com&#x2F;clarkmcc&#x2F;chitchat">https:&#x2F;&#x2F;github.com&#x2F;clarkmcc&#x2F;chitchat</a></div><br/><div id="38021487" class="c"><input type="checkbox" id="c-38021487" checked=""/><div class="controls bullet"><span class="by">pknerd</span><span>|</span><a href="#38021252">root</a><span>|</span><a href="#38021349">parent</a><span>|</span><a href="#38021670">next</a><span>|</span><label class="collapse" for="c-38021487">[-]</label><label class="expand" for="c-38021487">[1 more]</label></div><br/><div class="children"><div class="content">This looks cool so can it be used to feed Website&#x2F;Products data in CSV&#x2F;JSON format and &quot;chat&quot; with it?</div><br/></div></div></div></div><div id="38021670" class="c"><input type="checkbox" id="c-38021670" checked=""/><div class="controls bullet"><span class="by">canadaduane</span><span>|</span><a href="#38021252">parent</a><span>|</span><a href="#38021349">prev</a><span>|</span><a href="#38020413">next</a><span>|</span><label class="collapse" for="c-38021670">[-]</label><label class="expand" for="c-38021670">[1 more]</label></div><br/><div class="children"><div class="content">No, this is an embedding model, not a text completion model.</div><br/></div></div></div></div><div id="38020413" class="c"><input type="checkbox" id="c-38020413" checked=""/><div class="controls bullet"><span class="by">Nitrolo</span><span>|</span><a href="#38021252">prev</a><span>|</span><a href="#38020425">next</a><span>|</span><label class="collapse" for="c-38020413">[-]</label><label class="expand" for="c-38020413">[4 more]</label></div><br/><div class="children"><div class="content">Is there something like oobabooga to easily run this in a click-and-run way? Where I can load up a model, a text, and ask it questions?</div><br/><div id="38020706" class="c"><input type="checkbox" id="c-38020706" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020413">parent</a><span>|</span><a href="#38020975">next</a><span>|</span><label class="collapse" for="c-38020706">[-]</label><label class="expand" for="c-38020706">[1 more]</label></div><br/><div class="children"><div class="content">See my comment here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38020655">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38020655</a> for a CLI tool that lets you do this.<p>Note that embedding models are a different kind of thing from a Large Language Model, so it&#x27;s not the kind of model you can ask questions.<p>It&#x27;s a model which can take text and turn it into an array of floating point numbers, which you can then use to implement things like semantic search and related documents.<p>More on that here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;</a></div><br/></div></div><div id="38020975" class="c"><input type="checkbox" id="c-38020975" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38020413">parent</a><span>|</span><a href="#38020706">prev</a><span>|</span><a href="#38020581">next</a><span>|</span><label class="collapse" for="c-38020975">[-]</label><label class="expand" for="c-38020975">[1 more]</label></div><br/><div class="children"><div class="content">The Hugging Face page for the model has a two line load-and-encode Python code demo: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-base-en" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-base-en</a></div><br/></div></div><div id="38020581" class="c"><input type="checkbox" id="c-38020581" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38020413">parent</a><span>|</span><a href="#38020975">prev</a><span>|</span><a href="#38020425">next</a><span>|</span><label class="collapse" for="c-38020581">[-]</label><label class="expand" for="c-38020581">[1 more]</label></div><br/><div class="children"><div class="content">iirc ooba has its own integrated vectordb called superbooga.<p>I bet you could hack this in.</div><br/></div></div></div></div><div id="38020425" class="c"><input type="checkbox" id="c-38020425" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38020413">prev</a><span>|</span><a href="#38020314">next</a><span>|</span><label class="collapse" for="c-38020425">[-]</label><label class="expand" for="c-38020425">[3 more]</label></div><br/><div class="children"><div class="content">Does anyone know what they are using for this comparison and ranking? And where does instruct-xl stand in the mix?</div><br/><div id="38020445" class="c"><input type="checkbox" id="c-38020445" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38020425">parent</a><span>|</span><a href="#38020314">next</a><span>|</span><label class="collapse" for="c-38020445">[-]</label><label class="expand" for="c-38020445">[2 more]</label></div><br/><div class="children"><div class="content">Oh duh, it’s right in the post and instructor-xl is number 9. And so many new participants now!</div><br/><div id="38020520" class="c"><input type="checkbox" id="c-38020520" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38020425">root</a><span>|</span><a href="#38020445">parent</a><span>|</span><a href="#38020314">next</a><span>|</span><label class="collapse" for="c-38020520">[-]</label><label class="expand" for="c-38020520">[1 more]</label></div><br/><div class="children"><div class="content">The ranking are here:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard</a><p>It’s amazing how many new and better ones there are since I last looked a few months ago. Instructor-xl was number 1, now it is number 9, and its size is more than 10x the number 2 ranked!<p>Things move fast!</div><br/></div></div></div></div></div></div><div id="38020675" class="c"><input type="checkbox" id="c-38020675" checked=""/><div class="controls bullet"><span class="by">neximo64</span><span>|</span><a href="#38020314">prev</a><span>|</span><a href="#38021640">next</a><span>|</span><label class="collapse" for="c-38020675">[-]</label><label class="expand" for="c-38020675">[2 more]</label></div><br/><div class="children"><div class="content">Does it match OpenAI on number of params?</div><br/><div id="38020918" class="c"><input type="checkbox" id="c-38020918" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38020675">parent</a><span>|</span><a href="#38021640">next</a><span>|</span><label class="collapse" for="c-38020918">[-]</label><label class="expand" for="c-38020918">[1 more]</label></div><br/><div class="children"><div class="content">No one knows since OpenAI has not disclosed the number of paramerers their embeddings model uses.</div><br/></div></div></div></div><div id="38021640" class="c"><input type="checkbox" id="c-38021640" checked=""/><div class="controls bullet"><span class="by">backendEngineer</span><span>|</span><a href="#38020675">prev</a><span>|</span><a href="#38020557">next</a><span>|</span><label class="collapse" for="c-38021640">[-]</label><label class="expand" for="c-38021640">[1 more]</label></div><br/><div class="children"><div class="content">oh thank god I first read Jira...</div><br/></div></div><div id="38020557" class="c"><input type="checkbox" id="c-38020557" checked=""/><div class="controls bullet"><span class="by">e1g</span><span>|</span><a href="#38021640">prev</a><span>|</span><a href="#38020409">next</a><span>|</span><label class="collapse" for="c-38020557">[-]</label><label class="expand" for="c-38020557">[11 more]</label></div><br/><div class="children"><div class="content">Their OpenAI benchmark is GPT3 (text-embedding-ada-002), not GPT4.</div><br/><div id="38020701" class="c"><input type="checkbox" id="c-38020701" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020557">parent</a><span>|</span><a href="#38020409">next</a><span>|</span><label class="collapse" for="c-38020701">[-]</label><label class="expand" for="c-38020701">[10 more]</label></div><br/><div class="children"><div class="content">&quot;text-embedding-ada-002&quot; isn&#x27;t GPT3, it&#x27;s a different kind of model. Embedding models and Large Language Models aren&#x27;t the same thing.</div><br/><div id="38020974" class="c"><input type="checkbox" id="c-38020974" checked=""/><div class="controls bullet"><span class="by">e1g</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38020701">parent</a><span>|</span><a href="#38020409">next</a><span>|</span><label class="collapse" for="c-38020974">[-]</label><label class="expand" for="c-38020974">[9 more]</label></div><br/><div class="children"><div class="content">LLMs and embedding models are certainly different, but it&#x27;s a useful benchmark to calibrate expectations. OpenAI released text-embedding-ada-002 a year ago, and they describe the ada model as[1] &quot;the original GPT-3 base model [...] capable of very simple tasks, usually the fastest model in the GPT-3 series&quot;.<p>It&#x27;s fair to expect GPT3-level results - not GPT 3.5 and certainly not open-source tiny GPT4 as some might think when they read &quot;rivaling OpenAI&quot;.<p>[1] <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;whisper" rel="nofollow noreferrer">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;whisper</a></div><br/><div id="38021065" class="c"><input type="checkbox" id="c-38021065" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38020974">parent</a><span>|</span><a href="#38021055">next</a><span>|</span><label class="collapse" for="c-38021065">[-]</label><label class="expand" for="c-38021065">[7 more]</label></div><br/><div class="children"><div class="content">No, you&#x27;re confusing two things here.<p>&quot;text-ada-001&quot; is LLM in the GPT3 family, described as &quot;Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost&quot;<p>&quot;text-embedding-ada-002&quot; is entirely different - that page describes it as &quot;Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost.&quot;</div><br/><div id="38021230" class="c"><input type="checkbox" id="c-38021230" checked=""/><div class="controls bullet"><span class="by">e1g</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38021065">parent</a><span>|</span><a href="#38021155">next</a><span>|</span><label class="collapse" for="c-38021230">[-]</label><label class="expand" for="c-38021230">[5 more]</label></div><br/><div class="children"><div class="content">OpenAI doesn&#x27;t say directly what text-embedding-ada-002 is, but in the release blog post they show that performance is comparable to davinci&#x2F;curie, which places it firmly in the universe of GPT3. I understand it&#x27;s not a straight line comparison, but to me it&#x27;s still a useful mental heuristic about what to expect.<p>[1] <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;new-and-improved-embedding-model" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;new-and-improved-embedding-model</a> (see &quot;Model improvements&quot;)</div><br/><div id="38021473" class="c"><input type="checkbox" id="c-38021473" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38021230">parent</a><span>|</span><a href="#38021439">next</a><span>|</span><label class="collapse" for="c-38021473">[-]</label><label class="expand" for="c-38021473">[3 more]</label></div><br/><div class="children"><div class="content">You mean this table here?<p><pre><code>    text-embedding-ada-002     53.3
    text-search-davinci-*-001 52.8
    text-search-curie-*-001     50.9
    text-search-babbage-*-001 50.4
    text-search-ada-*-001     49.0
</code></pre>
That&#x27;s not comparing it to the davinci&#x2F;curie&#x2F;babbage GPT3 models, it&#x27;s comparing to the &quot;search-text-*&quot; family.<p>Those were introduced in <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;introducing-text-and-code-embeddings" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;introducing-text-and-code-embeddings</a> as the first public release of embeddings models from OpenAI.<p>&gt; We’re releasing three families of embedding models, each tuned to perform well on different functionalities: text similarity, text search, and code search. The models take either text or code as input and return an embedding vector.<p>It&#x27;s not at all clear to me if there&#x27;s any relationship between those and the GPT3 davinci&#x2F;curie&#x2F;babbage&#x2F;ada models.<p>My guess is that OpenAI&#x27;s naming convention back then was &quot;davinci is the best one, then curie, then babbage, then ada&quot;.</div><br/><div id="38021678" class="c"><input type="checkbox" id="c-38021678" checked=""/><div class="controls bullet"><span class="by">e1g</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38021473">parent</a><span>|</span><a href="#38021439">next</a><span>|</span><label class="collapse" for="c-38021678">[-]</label><label class="expand" for="c-38021678">[2 more]</label></div><br/><div class="children"><div class="content">How interesting. I assumed that a consistent codename such as Ada&#x2F;Davinci refers to the lineage&#x2F;DNA of the OpenAI model from which a distinct product was created. But I can see how these codenames could be &quot;just&quot; a revision label of A&#x2F;B&#x2F;C&#x2F;D (Ada&#x2F;Babbage&#x2F;Curie&#x2F;Davinci), similar to &quot;Pro&#x2F;Max&#x2F;Ultra&quot;. If true, a product named &quot;M2 Ultra&quot; could have nothing to do with another product called &quot;Watch Ultra&quot;.</div><br/><div id="38021719" class="c"><input type="checkbox" id="c-38021719" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38021678">parent</a><span>|</span><a href="#38021439">next</a><span>|</span><label class="collapse" for="c-38021719">[-]</label><label class="expand" for="c-38021719">[1 more]</label></div><br/><div class="children"><div class="content">Wow I genuinely hadn&#x27;t noticed the A&#x2F;B&#x2F;C&#x2F;D thing!</div><br/></div></div></div></div></div></div><div id="38021439" class="c"><input type="checkbox" id="c-38021439" checked=""/><div class="controls bullet"><span class="by">helloplanets</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38021230">parent</a><span>|</span><a href="#38021473">prev</a><span>|</span><a href="#38021155">next</a><span>|</span><label class="collapse" for="c-38021439">[-]</label><label class="expand" for="c-38021439">[1 more]</label></div><br/><div class="children"><div class="content">Reading through that article, the specific Davinci&#x2F;Curie models they seem to be referring to are called the following: &#x27;text-search-davinci-001&#x27;, &#x27;text-search-curie-001&#x27;, &#x27;text-similarity-davinci-001&#x27; and &#x27;text-similarity-curie-001&#x27;.<p>Are you sure these have anything to do with &#x27;text-davinci-003&#x27; or &#x27;text-curie-001&#x27;?<p>Will have to agree with everyone here that OpenAI is good at being extremely confusing. It seems like the logic might be something along the lines of the &#x27;text-search&#x27; portion being the actual type of the model, while the &#x27;curie-001&#x27; &#x2F; &#x27;&lt;name&gt;-&lt;number&gt;&#x27; format is just a personalized way of expressing the version of that type of model. And the whole &#x27;GPT&lt;number&gt;&#x27; category used to be a sort family of models, but now they&#x27;ve just switched it to the actual name of the newer gargantuan LLMs. Then, because the &#x27;GPT&lt;number&gt;&#x27; models are now that different thing altogether these days, the newest &#x27;text-embedding&#x27; model is just named &#x27;ada-&lt;number&gt;&#x27; because it&#x27;s on that iteration of the &#x27;text-embedding&#x27; type of model, adhering to the older principle of naming their models? Not sure, ha. Definitely feels like doing some detective work.</div><br/></div></div></div></div><div id="38021155" class="c"><input type="checkbox" id="c-38021155" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38021065">parent</a><span>|</span><a href="#38021230">prev</a><span>|</span><a href="#38021055">next</a><span>|</span><label class="collapse" for="c-38021155">[-]</label><label class="expand" for="c-38021155">[1 more]</label></div><br/><div class="children"><div class="content">tl;dr OpenAI is bad at product naming.</div><br/></div></div></div></div><div id="38021055" class="c"><input type="checkbox" id="c-38021055" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38020557">root</a><span>|</span><a href="#38020974">parent</a><span>|</span><a href="#38021065">prev</a><span>|</span><a href="#38020409">next</a><span>|</span><label class="collapse" for="c-38021055">[-]</label><label class="expand" for="c-38021055">[1 more]</label></div><br/><div class="children"><div class="content">When people talked about GPT-3 they always referred to davinci which is the largest model, not ada.</div><br/></div></div></div></div></div></div></div></div><div id="38020409" class="c"><input type="checkbox" id="c-38020409" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#38020557">prev</a><span>|</span><a href="#38020628">next</a><span>|</span><label class="collapse" for="c-38020409">[-]</label><label class="expand" for="c-38020409">[6 more]</label></div><br/><div class="children"><div class="content">Anyone got links to examples of text embedding?</div><br/><div id="38020630" class="c"><input type="checkbox" id="c-38020630" checked=""/><div class="controls bullet"><span class="by">RossBencina</span><span>|</span><a href="#38020409">parent</a><span>|</span><a href="#38020554">next</a><span>|</span><label class="collapse" for="c-38020630">[-]</label><label class="expand" for="c-38020630">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI have a brief explainer with a bunch of example use cases here:<p><a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings&#x2F;what-are-embeddings" rel="nofollow noreferrer">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings&#x2F;what-are-...</a></div><br/></div></div><div id="38020554" class="c"><input type="checkbox" id="c-38020554" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#38020409">parent</a><span>|</span><a href="#38020630">prev</a><span>|</span><a href="#38020628">next</a><span>|</span><label class="collapse" for="c-38020554">[-]</label><label class="expand" for="c-38020554">[4 more]</label></div><br/><div class="children"><div class="content">Easiest example is taking three words: Universe, University, College.<p>- University and Universe are similar alphabetically.<p>- University and College are similar in meaning.<p>Take embeddings for those three words and `University` will be near `College`, while `Universe` will be further away, because embeddings capture meaning:<p>University&lt;--&gt;College&lt;--------------&gt;Universe<p>_<p>With old school search you&#x27;d need to handle the special case of treating University and College as similar, but embeddings already handle it.<p>With embeddings you can do math to find how similar two results are, based on how close their vectors are. The closer the embeddings, the closer the meaning.</div><br/><div id="38020972" class="c"><input type="checkbox" id="c-38020972" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#38020409">root</a><span>|</span><a href="#38020554">parent</a><span>|</span><a href="#38020628">next</a><span>|</span><label class="collapse" for="c-38020972">[-]</label><label class="expand" for="c-38020972">[3 more]</label></div><br/><div class="children"><div class="content">Another interesting point is that math can be performed on embedding vectors: emb(&quot;king&quot;) - emb(&quot;man&quot;) + emb(&quot;woman&quot;) = emb(&quot;queen&quot;).</div><br/><div id="38021450" class="c"><input type="checkbox" id="c-38021450" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38020409">root</a><span>|</span><a href="#38020972">parent</a><span>|</span><a href="#38020628">next</a><span>|</span><label class="collapse" for="c-38021450">[-]</label><label class="expand" for="c-38021450">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a property of Word2Vec <i>specifically</i> due to how it&#x27;s trained (a shallow network where most of the &quot;logic&quot; would be contained within the embeddings themselves). Using it for embeddings generated from LLMs or Embedding layers will not give as fun results; in practice the only thing you can do is average or cluster them.</div><br/><div id="38023028" class="c"><input type="checkbox" id="c-38023028" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38020409">root</a><span>|</span><a href="#38021450">parent</a><span>|</span><a href="#38020628">next</a><span>|</span><label class="collapse" for="c-38023028">[-]</label><label class="expand" for="c-38023028">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>That&#x27;s a property of Word2Vec specifically due to how it&#x27;s trained (a shallow network where most of the &quot;logic&quot; would be contained within the embeddings themselves).</i><p>Is it though? I thought the LLM-based embeddings are <i>even more fun</i> for this, as you have many more interesting directions to move in. I.e. not just:<p>emb(&quot;king&quot;) - emb(&quot;man&quot;) + emb(&quot;woman&quot;) = emb(&quot;queen&quot;)<p>But also e.g.:<p>emb(&lt;insert a couple paragraph long positive book review&gt;) + a<i>v(sad) + b</i>v(short) - c*v(positive) = emb(&lt;a single paragraph, negative and depressing review&gt;)<p>Where a, b, c are some constants to tweak, and v(X) is a vector for quality X, which you can get by embedding a bunch of texts expressing the quality X and averaging them out (or doing some other dimensional reduction trickery).<p>I&#x27;ve suggested this on HN some time ago, but only been told that I&#x27;m confused and the idea is not even wrong. But then, there was this talk on some AI conference recently[0], where the speaker demonstrated exactly this kind of latent space translations of text in a language model.<p>--<p>[0] - <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=veShHxQYPzo&amp;t=13980s">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=veShHxQYPzo&amp;t=13980s</a> - &quot;The Hidden Life of Embeddings&quot;, by Linus Lee from Notion.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38020628" class="c"><input type="checkbox" id="c-38020628" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#38020409">prev</a><span>|</span><a href="#38020778">next</a><span>|</span><label class="collapse" for="c-38020628">[-]</label><label class="expand" for="c-38020628">[2 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t fine tune without using their library tied to their cloud? Did I misunderstand? Do you need fine tune?</div><br/></div></div><div id="38020778" class="c"><input type="checkbox" id="c-38020778" checked=""/><div class="controls bullet"><span class="by">Zuiii</span><span>|</span><a href="#38020628">prev</a><span>|</span><a href="#38020432">next</a><span>|</span><label class="collapse" for="c-38020778">[-]</label><label class="expand" for="c-38020778">[2 more]</label></div><br/><div class="children"><div class="content">Color me surprised! it looks like its actually open source (Apache 2.0) and not the usual false advertising by some two-faced company or institution. Links  here:<p>* <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-base-en" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-base-en</a>
* <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-small-en" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-small-en</a></div><br/></div></div><div id="38020432" class="c"><input type="checkbox" id="c-38020432" checked=""/><div class="controls bullet"><span class="by">RossBencina</span><span>|</span><a href="#38020778">prev</a><span>|</span><a href="#38023327">next</a><span>|</span><label class="collapse" for="c-38020432">[-]</label><label class="expand" for="c-38020432">[8 more]</label></div><br/><div class="children"><div class="content">Some relevant stats from the link:<p>8192 token input sequence length<p>768 embedding dimensions<p>0.27GB model (with 0.07GB model also available)<p>Tokeniser: BertTokenizer [1], 30528 token vocab [2]<p>Is an 8K sequence length directly comparable to text-embedding-ada-002 if the vocabulary is much smaller? I seem to remember its tokeniser has a larger vocabulary.<p>[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-base-en&#x2F;blob&#x2F;main&#x2F;tokenizer_config.json" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-base-en&#x2F;blo...</a><p>[2] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-base-en&#x2F;blob&#x2F;main&#x2F;vocab.txt" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;jinaai&#x2F;jina-embeddings-v2-base-en&#x2F;blo...</a></div><br/><div id="38020767" class="c"><input type="checkbox" id="c-38020767" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#38020432">parent</a><span>|</span><a href="#38021130">next</a><span>|</span><label class="collapse" for="c-38020767">[-]</label><label class="expand" for="c-38020767">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Is an 8K sequence length directly comparable to text-embedding-ada-002 if the vocabulary is much smaller? I seem to remember its tokeniser has a larger vocabulary.<p>Words that aren&#x27;t in the vocabulary can still be represented by multiple tokens. Some models can input and output valid UTF-8 at the byte level (rather than needing a unique token for each codepoint). For example RWKV-World.</div><br/><div id="38020795" class="c"><input type="checkbox" id="c-38020795" checked=""/><div class="controls bullet"><span class="by">space_fountain</span><span>|</span><a href="#38020432">root</a><span>|</span><a href="#38020767">parent</a><span>|</span><a href="#38021130">next</a><span>|</span><label class="collapse" for="c-38020795">[-]</label><label class="expand" for="c-38020795">[3 more]</label></div><br/><div class="children"><div class="content">A large vocabulary means less tokens are needed to represent the same information</div><br/><div id="38021717" class="c"><input type="checkbox" id="c-38021717" checked=""/><div class="controls bullet"><span class="by">HPMOR</span><span>|</span><a href="#38020432">root</a><span>|</span><a href="#38020795">parent</a><span>|</span><a href="#38021130">next</a><span>|</span><label class="collapse" for="c-38021717">[-]</label><label class="expand" for="c-38021717">[2 more]</label></div><br/><div class="children"><div class="content">*fewer<p>Less is used for qualitative data like “I love him less”. Whereas fewer is used for countable things like “I need fewer tokens.”</div><br/><div id="38021871" class="c"><input type="checkbox" id="c-38021871" checked=""/><div class="controls bullet"><span class="by">scubbo</span><span>|</span><a href="#38020432">root</a><span>|</span><a href="#38021717">parent</a><span>|</span><a href="#38021130">next</a><span>|</span><label class="collapse" for="c-38021871">[-]</label><label class="expand" for="c-38021871">[1 more]</label></div><br/><div class="children"><div class="content">Username checks out.</div><br/></div></div></div></div></div></div></div></div><div id="38021130" class="c"><input type="checkbox" id="c-38021130" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#38020432">parent</a><span>|</span><a href="#38020767">prev</a><span>|</span><a href="#38020930">next</a><span>|</span><label class="collapse" for="c-38021130">[-]</label><label class="expand" for="c-38021130">[1 more]</label></div><br/><div class="children"><div class="content">A uniform distribution over 30528 tokens is just under 15 bits of information per token, whereas a vocabulary size of ~60000 would be just under 16 bits per token. In practice it&#x27;s not uniform, but this shows that they&#x27;re in the same ballpark.</div><br/></div></div></div></div></div></div></div></div></div></body></html>