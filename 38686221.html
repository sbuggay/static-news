<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702976459616" as="style"/><link rel="stylesheet" href="styles.css?v=1702976459616"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://simonwillison.net/2023/Dec/18/mistral/">Many options for running Mistral models in your terminal using LLM</a> <span class="domain">(<a href="https://simonwillison.net">simonwillison.net</a>)</span></div><div class="subtext"><span>simonw</span> | <span>73 comments</span></div><br/><div><div id="38688806" class="c"><input type="checkbox" id="c-38688806" checked=""/><div class="controls bullet"><span class="by">kristianpaul</span><span>|</span><a href="#38687647">next</a><span>|</span><label class="collapse" for="c-38688806">[-]</label><label class="expand" for="c-38688806">[10 more]</label></div><br/><div class="children"><div class="content">I run in Linux system it via cli using Ollama, very easy to setup<p><a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mistral">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mistral</a><p>curl <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;install.sh">https:&#x2F;&#x2F;ollama.ai&#x2F;install.sh</a> | sh<p>ollama run mistral:text</div><br/><div id="38692929" class="c"><input type="checkbox" id="c-38692929" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#38688806">parent</a><span>|</span><a href="#38693042">next</a><span>|</span><label class="collapse" for="c-38692929">[-]</label><label class="expand" for="c-38692929">[1 more]</label></div><br/><div class="children"><div class="content">I want to like Ollama, but I wish it didn&#x27;t obfuscate the actual directives (full prompt) that it sends to the underlying model. Ollama uses a custom templatizing script in its Modelfiles to translate user input into the format that a specific model expects ([INST], etc), but it can be difficult to tell if it&#x27;s working as expected because it won&#x27;t show up in the logs at all.<p>Other than that it&#x27;s a great project - very easy to get started and has a solid API implementation. I&#x27;ve got it running on both a Win 10 + WSL2 docker and on a Mac M1.</div><br/></div></div><div id="38693042" class="c"><input type="checkbox" id="c-38693042" checked=""/><div class="controls bullet"><span class="by">giblaz</span><span>|</span><a href="#38688806">parent</a><span>|</span><a href="#38692929">prev</a><span>|</span><a href="#38691323">next</a><span>|</span><label class="collapse" for="c-38693042">[-]</label><label class="expand" for="c-38693042">[1 more]</label></div><br/><div class="children"><div class="content">Ollama is great. I discovered it today while looking for a way to serve LLMs locally for my terminal command generator tool (cmdh: <a href="https:&#x2F;&#x2F;github.com&#x2F;pgibler&#x2F;cmdh">https:&#x2F;&#x2F;github.com&#x2F;pgibler&#x2F;cmdh</a>) and was able to get it up and running and implement support for it very easily.</div><br/></div></div><div id="38691323" class="c"><input type="checkbox" id="c-38691323" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38688806">parent</a><span>|</span><a href="#38693042">prev</a><span>|</span><a href="#38690899">next</a><span>|</span><label class="collapse" for="c-38691323">[-]</label><label class="expand" for="c-38691323">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, I was surprised Ollama was not mentioned as it’s by far the easiest to get started with. If it only had real grammar support, I’d never have to use another library again (it has a JSON mode that generally works, however).</div><br/><div id="38693068" class="c"><input type="checkbox" id="c-38693068" checked=""/><div class="controls bullet"><span class="by">Tomte</span><span>|</span><a href="#38688806">root</a><span>|</span><a href="#38691323">parent</a><span>|</span><a href="#38690899">next</a><span>|</span><label class="collapse" for="c-38693068">[-]</label><label class="expand" for="c-38693068">[2 more]</label></div><br/><div class="children"><div class="content">What is grammar support? I&#x27;ve seen that mentioned several times now. Does it allow to restrict the output to a given template, or am I totally wrong there?</div><br/><div id="38693227" class="c"><input type="checkbox" id="c-38693227" checked=""/><div class="controls bullet"><span class="by">biwills</span><span>|</span><a href="#38688806">root</a><span>|</span><a href="#38693068">parent</a><span>|</span><a href="#38690899">next</a><span>|</span><label class="collapse" for="c-38693227">[-]</label><label class="expand" for="c-38693227">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Here&#x27;s a quick rundown of grammar in llama.cpp (link on the docs of faraday.dev which runs llama.cpp under the hood)<p><a href="https:&#x2F;&#x2F;docs.faraday.dev&#x2F;character-creation&#x2F;grammars" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.faraday.dev&#x2F;character-creation&#x2F;grammars</a></div><br/></div></div></div></div></div></div><div id="38690899" class="c"><input type="checkbox" id="c-38690899" checked=""/><div class="controls bullet"><span class="by">traverseda</span><span>|</span><a href="#38688806">parent</a><span>|</span><a href="#38691323">prev</a><span>|</span><a href="#38687647">next</a><span>|</span><label class="collapse" for="c-38690899">[-]</label><label class="expand" for="c-38690899">[4 more]</label></div><br/><div class="children"><div class="content">Huh, I&#x27;ve fought with a few of these things on my laptop, no nvidia GPU, limited ram, etc.<p>This actually worked as advertised.</div><br/><div id="38691152" class="c"><input type="checkbox" id="c-38691152" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#38688806">root</a><span>|</span><a href="#38690899">parent</a><span>|</span><a href="#38687647">next</a><span>|</span><label class="collapse" for="c-38691152">[-]</label><label class="expand" for="c-38691152">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the performance like (quality and speed wise)?</div><br/><div id="38692766" class="c"><input type="checkbox" id="c-38692766" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38688806">root</a><span>|</span><a href="#38691152">parent</a><span>|</span><a href="#38691675">next</a><span>|</span><label class="collapse" for="c-38692766">[-]</label><label class="expand" for="c-38692766">[1 more]</label></div><br/><div class="children"><div class="content">Yesterday I tried mixtral 7bx8 running on the CPU. With an Intel 11th gen chip and 64gb DDR4 at 3200mhz, I got around 2-4 tokens&#x2F;second in a small context, this gets progressively slower as the context grows.<p>You would get a much better experience with apple silicon and lots of RAM</div><br/></div></div><div id="38691675" class="c"><input type="checkbox" id="c-38691675" checked=""/><div class="controls bullet"><span class="by">all2</span><span>|</span><a href="#38688806">root</a><span>|</span><a href="#38691152">parent</a><span>|</span><a href="#38692766">prev</a><span>|</span><a href="#38687647">next</a><span>|</span><label class="collapse" for="c-38691675">[-]</label><label class="expand" for="c-38691675">[1 more]</label></div><br/><div class="children"><div class="content">Mistral 7b is serviceable for short contexts. If you have a longer conversation, token generation can start to lag a lot.</div><br/></div></div></div></div></div></div></div></div><div id="38687647" class="c"><input type="checkbox" id="c-38687647" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#38688806">prev</a><span>|</span><a href="#38692636">next</a><span>|</span><label class="collapse" for="c-38687647">[-]</label><label class="expand" for="c-38687647">[4 more]</label></div><br/><div class="children"><div class="content">Mixtral-8x7B-Instruct is also available in a llamafile now: <a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile#other-example-llamafiles">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile#other-example-llam...</a></div><br/><div id="38688317" class="c"><input type="checkbox" id="c-38688317" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38687647">parent</a><span>|</span><a href="#38692735">next</a><span>|</span><label class="collapse" for="c-38688317">[-]</label><label class="expand" for="c-38688317">[1 more]</label></div><br/><div class="children"><div class="content">Just added that option - you can talk to it from my LLM CLI tool using its OpenAI-compatible API endpoint: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Dec&#x2F;18&#x2F;mistral&#x2F;#llamafile-openai" rel="nofollow noreferrer">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Dec&#x2F;18&#x2F;mistral&#x2F;#llamafile-ope...</a></div><br/></div></div><div id="38692735" class="c"><input type="checkbox" id="c-38692735" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#38687647">parent</a><span>|</span><a href="#38688317">prev</a><span>|</span><a href="#38692636">next</a><span>|</span><label class="collapse" for="c-38692735">[-]</label><label class="expand" for="c-38692735">[2 more]</label></div><br/><div class="children"><div class="content">If I have the baseline Mixtral-8x7B .pth, how do I convert that to a llamafile compatible .gguf?</div><br/><div id="38692831" class="c"><input type="checkbox" id="c-38692831" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38687647">root</a><span>|</span><a href="#38692735">parent</a><span>|</span><a href="#38692636">next</a><span>|</span><label class="collapse" for="c-38692831">[-]</label><label class="expand" for="c-38692831">[1 more]</label></div><br/><div class="children"><div class="content">Just download the GGUF from huggingface. @TheBloke always uploads GGUF versions of the popular models</div><br/></div></div></div></div></div></div><div id="38692636" class="c"><input type="checkbox" id="c-38692636" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#38687647">prev</a><span>|</span><a href="#38691164">next</a><span>|</span><label class="collapse" for="c-38692636">[-]</label><label class="expand" for="c-38692636">[2 more]</label></div><br/><div class="children"><div class="content">I’ve been wondering, without stuff like llama.cpp and ollama etc how exactly were you intended to run Mistral when they released it?</div><br/><div id="38692742" class="c"><input type="checkbox" id="c-38692742" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38692636">parent</a><span>|</span><a href="#38691164">next</a><span>|</span><label class="collapse" for="c-38692742">[-]</label><label class="expand" for="c-38692742">[1 more]</label></div><br/><div class="children"><div class="content">The main framework for running LLMs was pytorch, which AFAIK has a slow CPU implementation, so only GPU&#x2F;CUDA users would be covered.<p>Llama.cpp was the project that popularized running LLMs on the CPU due to its very efficient implementation. Ollama is a frontend to it.</div><br/></div></div></div></div><div id="38691164" class="c"><input type="checkbox" id="c-38691164" checked=""/><div class="controls bullet"><span class="by">cloudking</span><span>|</span><a href="#38692636">prev</a><span>|</span><a href="#38688556">next</a><span>|</span><label class="collapse" for="c-38691164">[-]</label><label class="expand" for="c-38691164">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s great that there are many comments about being able to run models locally, but I rarely see comments about what folks are using them for. Would love to learn more about use cases and problems being solved.</div><br/><div id="38692874" class="c"><input type="checkbox" id="c-38692874" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38691164">parent</a><span>|</span><a href="#38691564">next</a><span>|</span><label class="collapse" for="c-38692874">[-]</label><label class="expand" for="c-38692874">[1 more]</label></div><br/><div class="children"><div class="content">I use them for coding, you&#x27;d be surprised at how much you can get done with deepseek coder 6.7b<p>Most GPT plugins for code editors will also work for local models since you can have OpenAI API stubs running locally.<p>Clearly it is nowhere near GPT-4 capacity, but if you ask simple boilerplate things (&quot;write a class with the following methods&quot;, then &quot;write unit tests for it&quot;) it will mostly work. Even if it doesn&#x27;t, you can manually fix it, and it still can save you some time.<p>Always review code generated by LLM, regardless if it comes from GPT-4!</div><br/></div></div><div id="38691564" class="c"><input type="checkbox" id="c-38691564" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#38691164">parent</a><span>|</span><a href="#38692874">prev</a><span>|</span><a href="#38691256">next</a><span>|</span><label class="collapse" for="c-38691564">[-]</label><label class="expand" for="c-38691564">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a goldrush and no one wants to share their secrets.<p>Here&#x27;s a good example from previous HN comments: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38482347">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38482347</a></div><br/></div></div><div id="38691256" class="c"><input type="checkbox" id="c-38691256" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#38691164">parent</a><span>|</span><a href="#38691564">prev</a><span>|</span><a href="#38688556">next</a><span>|</span><label class="collapse" for="c-38691256">[-]</label><label class="expand" for="c-38691256">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s be honest, they&#x27;re using them mostly for <i>ahem</i> personal time.</div><br/></div></div></div></div><div id="38688556" class="c"><input type="checkbox" id="c-38688556" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#38691164">prev</a><span>|</span><a href="#38687654">next</a><span>|</span><label class="collapse" for="c-38688556">[-]</label><label class="expand" for="c-38688556">[11 more]</label></div><br/><div class="children"><div class="content">This may be a dumb question but how do you update an LLM with new information after its knowledge cutoff?</div><br/><div id="38689045" class="c"><input type="checkbox" id="c-38689045" checked=""/><div class="controls bullet"><span class="by">pkaler</span><span>|</span><a href="#38688556">parent</a><span>|</span><a href="#38688712">next</a><span>|</span><label class="collapse" for="c-38689045">[-]</label><label class="expand" for="c-38689045">[2 more]</label></div><br/><div class="children"><div class="content">The term that you&#x27;re looking for is Retrieval Augmented Generation (RAG).<p><a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;sagemaker&#x2F;latest&#x2F;dg&#x2F;jumpstart-foundation-models-customize-rag.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;sagemaker&#x2F;latest&#x2F;dg&#x2F;jumpstart-fo...</a></div><br/></div></div><div id="38688712" class="c"><input type="checkbox" id="c-38688712" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38688556">parent</a><span>|</span><a href="#38689045">prev</a><span>|</span><a href="#38688891">next</a><span>|</span><label class="collapse" for="c-38688712">[-]</label><label class="expand" for="c-38688712">[2 more]</label></div><br/><div class="children"><div class="content">Generally, you train it again entirely from scratch.<p>It&#x27;s possible to introduce new information by fine-tuning a new model on top of the existing model, but it&#x27;s debatable how effective that is for introducing new information - most fine-tuning success stories I&#x27;ve seen focus on teaching a model how to perform new kinds of task as opposed to teaching it new &quot;facts&quot;.<p>If you want a model to have access to updated information, the best way to do that is still via Retrieval Augmented Generation. That&#x27;s the fancy name for the trick where you give the model the ability to run searches for information relevant to the user&#x27;s questions and then invisibly paste that content into the prompt - effectively what Bing, Bard and ChatGPT do when they run searches against their attached search engines.</div><br/><div id="38692177" class="c"><input type="checkbox" id="c-38692177" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38688556">root</a><span>|</span><a href="#38688712">parent</a><span>|</span><a href="#38688891">next</a><span>|</span><label class="collapse" for="c-38692177">[-]</label><label class="expand" for="c-38692177">[1 more]</label></div><br/><div class="children"><div class="content">Most LoRAs are less effective for facts since changes largely shift attention (Q and K, not V layers) and only touch a tiny percentage of weights at that, however full fine-tunes on models <i>are</i> pretty effective for introducing new facts (you could probably use ReLoRA as well).<p>There are also newer techniques like or ROME that could edit individual facts, and you might also be able to get there when you are updating by doing a DPO tune of the old vs the new answers as well.<p>While I agree that RAG&#x2F;tool use (with consistency checking) might be overall best approach for facts, being able to update&#x2F;tune for model drift is probably going to still be important.<p>I&#x27;d also disagree about the training entirely from scratch - unless you&#x27;re changing architecture&#x2F;building a brand new foundational model or have unlimited time&#x2F;compute budget, that seems like the worst option (and pretty unrealistic) for most people.</div><br/></div></div></div></div><div id="38688891" class="c"><input type="checkbox" id="c-38688891" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#38688556">parent</a><span>|</span><a href="#38688712">prev</a><span>|</span><a href="#38688697">next</a><span>|</span><label class="collapse" for="c-38688891">[-]</label><label class="expand" for="c-38688891">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s a dumb question at all.  It depends on your objectives and how much resources you&#x27;re willing to spend.<p>These open weights models can be retrained.  Start with a foundational model like Llama2 or something and expose it to more recent training data that includes whatever updated information you want it to have access to.  This is relatively expensive, but allows for big changes to the model.<p>If you have some relatively small subset of new information you want to bring in, you could build a Lora.  Then either run your model with the Lora, or fold the Lora into your base model.  This is relatively cheap, but fairly narrow in terms of your updates.<p>In the long run, it might be that Retrieval Augmented Generation (RAG) is the way to go.  Here, your embeddings go into a vector database, and the model reads from there.  Then you just need to update the database for the model to have access to new information.<p>This LLM stuff is new enough that anything like best practices are still being worked out.  The optimal way to bring in new information could be a variant of one of the methods I mentioned above, or some combination of all three, or something else altogether.</div><br/></div></div><div id="38688697" class="c"><input type="checkbox" id="c-38688697" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38688556">parent</a><span>|</span><a href="#38688891">prev</a><span>|</span><a href="#38688734">next</a><span>|</span><label class="collapse" for="c-38688697">[-]</label><label class="expand" for="c-38688697">[3 more]</label></div><br/><div class="children"><div class="content">- in context learning (simply put it into context)<p>- finetuning, as in restarting the models checkpoint and relearning it on the previous + new data<p>- adding extra neurons (e.g. LoRA adapters) at certain places and restarting learning<p>Oh, in classic machine learning there&#x27;s also the &quot;bagging&#x2F;boosting classifiers&quot; option, but I have no knowledge if that can be applied to a ANN.</div><br/><div id="38688733" class="c"><input type="checkbox" id="c-38688733" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38688556">root</a><span>|</span><a href="#38688697">parent</a><span>|</span><a href="#38688734">next</a><span>|</span><label class="collapse" for="c-38688733">[-]</label><label class="expand" for="c-38688733">[2 more]</label></div><br/><div class="children"><div class="content">Have you seen LoRA adapters actually work for this kind of thing?<p>The leaked Google &quot;We have no moat&quot; memo was very excited about LoRA style techniques, but it&#x27;s not clear to me that it&#x27;s been proven as a technique yet.</div><br/><div id="38688779" class="c"><input type="checkbox" id="c-38688779" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38688556">root</a><span>|</span><a href="#38688733">parent</a><span>|</span><a href="#38688734">next</a><span>|</span><label class="collapse" for="c-38688779">[-]</label><label class="expand" for="c-38688779">[1 more]</label></div><br/><div class="children"><div class="content">No, I have tried about 3 times and all failed. Altough it might be that I&#x27;m simply doing it wrong.<p>There are people (can point at a discord server) claiming it works for them and that they even sell finetuned models to business clients.<p>EDIT: I found one of the articles I tried to follow: <a href="https:&#x2F;&#x2F;www.mlexpert.io&#x2F;prompt-engineering&#x2F;chatbot-with-local-llm-using-langchain" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.mlexpert.io&#x2F;prompt-engineering&#x2F;chatbot-with-loca...</a><p>EDIT2: Ignore above. This seemed much more promising: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pnwVz64jNvw" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pnwVz64jNvw</a> . Author provides consulting services and seemed very nice and approachable</div><br/></div></div></div></div></div></div><div id="38688734" class="c"><input type="checkbox" id="c-38688734" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#38688556">parent</a><span>|</span><a href="#38688697">prev</a><span>|</span><a href="#38688720">next</a><span>|</span><label class="collapse" for="c-38688734">[-]</label><label class="expand" for="c-38688734">[1 more]</label></div><br/><div class="children"><div class="content">The easiest way is to let it do web searches with a tool former&#x2F;plugin framework.<p>For specifically knowledge you want it to be able to recall (like knowledge base articles or blog posts) vector database embeddings are best.<p>For knowledge you want it to operationalize, like being able to program in a new language the last resort is finetuning but this is not easy, requires massive amounts of high quality data, and is not generally effective for things which do not have a large amount of data to fine tune on (tens of thousands of pages worth of content).</div><br/></div></div></div></div><div id="38687654" class="c"><input type="checkbox" id="c-38687654" checked=""/><div class="controls bullet"><span class="by">MilaM</span><span>|</span><a href="#38688556">prev</a><span>|</span><a href="#38688755">next</a><span>|</span><label class="collapse" for="c-38687654">[-]</label><label class="expand" for="c-38687654">[26 more]</label></div><br/><div class="children"><div class="content">This is a bit off-topic and probably a noob question. But maybe someone with experience could help me. I&#x27;m about to buy a M2 or M3 Mac to replace my Intel Mac, and would like to play around with locally run LLMs in the future. How much RAM do I have to buy in order not to run into bottlenecks? I&#x27;m aware that you would probably want a beefier GPU for better performance. But as I said, I&#x27;m not worried about speed so much.</div><br/><div id="38687889" class="c"><input type="checkbox" id="c-38687889" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#38687654">parent</a><span>|</span><a href="#38687731">next</a><span>|</span><label class="collapse" for="c-38687889">[-]</label><label class="expand" for="c-38687889">[5 more]</label></div><br/><div class="children"><div class="content">TheBloke&#x27;s GGUF model card tells you how much RAM you&#x27;ll want to run the various versions of the model:
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Mixtral-8x7B-v0.1-GGUF#provided-files" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Mixtral-8x7B-v0.1-GGUF#provi...</a><p>In the article, Simon mentions the Q6_K.gguf model, which is about 40GB.  A Mac Studio can handle this, but any of these models are going to be a tight fit or impossible on a Mac laptop without swapping to disk.  Maybe NVME is fast enough that swapping isn&#x27;t too terrible.<p>In my experience, the Mixtral models work pretty well on llama.cpp on my Linux workstation with a 10GB GPU, and offloading the rest to CPU.<p>It is impressive how fast the smaller models are improving.  Still, a safe rule of thumb is the more RAM the better.<p>Also, really question how much you need to run these models locally.  If you just want to play around with these models, it&#x27;s probably far more cost effective to rent something in the cloud.</div><br/><div id="38689338" class="c"><input type="checkbox" id="c-38689338" checked=""/><div class="controls bullet"><span class="by">williamstein</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687889">parent</a><span>|</span><a href="#38687731">next</a><span>|</span><label class="collapse" for="c-38689338">[-]</label><label class="expand" for="c-38689338">[4 more]</label></div><br/><div class="children"><div class="content">I tried Mixtral via ollama on my Apple M1 Max with 32GB of RAM, and was a total nonstarter. I ended up having to powercycle my machine.    I then just used two L4 GPU&#x27;s on Google Cloud (so 48GB of GPU RAM, see [1]) and it was very smooth and fast there.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;sagemathinc&#x2F;cocalc-howto&#x2F;blob&#x2F;main&#x2F;ollama.md">https:&#x2F;&#x2F;github.com&#x2F;sagemathinc&#x2F;cocalc-howto&#x2F;blob&#x2F;main&#x2F;ollama...</a></div><br/><div id="38690070" class="c"><input type="checkbox" id="c-38690070" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38689338">parent</a><span>|</span><a href="#38687731">next</a><span>|</span><label class="collapse" for="c-38690070">[-]</label><label class="expand" for="c-38690070">[3 more]</label></div><br/><div class="children"><div class="content">Wow, as an author of the project I&#x27;m so sorry about you having to restart your computer. The memory management in Ollama needs a lot of improvement – will be working on this a bunch going forward. I also have a M1 32GB Mac and it&#x27;s unfortunately just below the amount of memory Mixtral needs to run well (for now!)</div><br/><div id="38691749" class="c"><input type="checkbox" id="c-38691749" checked=""/><div class="controls bullet"><span class="by">ls612</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38690070">parent</a><span>|</span><a href="#38687731">next</a><span>|</span><label class="collapse" for="c-38691749">[-]</label><label class="expand" for="c-38691749">[2 more]</label></div><br/><div class="children"><div class="content">Will the Mixtral 8x7 run well on a 64GB M2 Max then?</div><br/><div id="38692116" class="c"><input type="checkbox" id="c-38692116" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38691749">parent</a><span>|</span><a href="#38687731">next</a><span>|</span><label class="collapse" for="c-38692116">[-]</label><label class="expand" for="c-38692116">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using an M2 64GB and Mixtral works pretty well.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38687731" class="c"><input type="checkbox" id="c-38687731" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#38687654">parent</a><span>|</span><a href="#38687889">prev</a><span>|</span><a href="#38689372">next</a><span>|</span><label class="collapse" for="c-38687731">[-]</label><label class="expand" for="c-38687731">[9 more]</label></div><br/><div class="children"><div class="content">If you want to run decently heavy models, I&#x27;d recommend getting at a minimum getting 48GB. This allows you to run 34b llama models with ease, 70b models quantized, mixtral without problems.<p>If you want to run most models, get 64GB. This just gives you some more room to work with.<p>If you want to run anything, get 128GB or more. Unquantized 70b? Check. Goliath 120b? Check.<p>Note that high end consumer gpus end at 24GB VRAM. I have one 7900xtx for running llms, and the best it can reliably run is 4-bit quantized 34b models, anything larger is partially in regular ram.</div><br/><div id="38687866" class="c"><input type="checkbox" id="c-38687866" checked=""/><div class="controls bullet"><span class="by">MilaM</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687731">parent</a><span>|</span><a href="#38689372">next</a><span>|</span><label class="collapse" for="c-38687866">[-]</label><label class="expand" for="c-38687866">[8 more]</label></div><br/><div class="children"><div class="content">Thank you for this detailed response. I&#x27;m not sure if it was clear, but I was going to use just the Apple Silicon CPU&#x2F;GPU, not an external one from Nvidia.<p>Is there anything useful you can do with 24 or 32GB of RAM with llms? Regular M2 Mac minis can only be ordered with up to 24GB of RAM. The Pro Mac mini M2 is upgradable to 32GB RAM.</div><br/><div id="38687887" class="c"><input type="checkbox" id="c-38687887" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687866">parent</a><span>|</span><a href="#38688580">next</a><span>|</span><label class="collapse" for="c-38687887">[-]</label><label class="expand" for="c-38687887">[2 more]</label></div><br/><div class="children"><div class="content">32GB will run a good quantized Mixtral, though I can&#x27;t confidently explain how much of a quality difference there is from unquantized.</div><br/><div id="38688575" class="c"><input type="checkbox" id="c-38688575" checked=""/><div class="controls bullet"><span class="by">cschneid</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687887">parent</a><span>|</span><a href="#38688580">next</a><span>|</span><label class="collapse" for="c-38688575">[-]</label><label class="expand" for="c-38688575">[1 more]</label></div><br/><div class="children"><div class="content">Data Point: I am currently having issues getting Mixtral Q4_K_M running in LMStudio on my 32gb M1 Max. I&#x27;m trying Q3 to see if it fits.<p>I can have it run in on &#x27;cpu&#x27; which is very slow, but offloading to the GPU runs out of memory.</div><br/></div></div></div></div><div id="38688580" class="c"><input type="checkbox" id="c-38688580" checked=""/><div class="controls bullet"><span class="by">robterrell</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687866">parent</a><span>|</span><a href="#38687887">prev</a><span>|</span><a href="#38688283">next</a><span>|</span><label class="collapse" for="c-38688580">[-]</label><label class="expand" for="c-38688580">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been unable to get Mixtral (mixtral-8x7b-instruct-v0.1.Q6_K.gguf) to run well on my M2 MacBook Air (24 GB). It&#x27;s super slow and eventually freezes after about 12-15 tokens of a response. You should look at M3 options with more RAM -- 64 GB or even the weird-sounding 96 GB might be a good choice.</div><br/><div id="38689326" class="c"><input type="checkbox" id="c-38689326" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38688580">parent</a><span>|</span><a href="#38688283">next</a><span>|</span><label class="collapse" for="c-38689326">[-]</label><label class="expand" for="c-38689326">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;17kcgjv&#x2F;how_does_apples_new_m3_128gb_ram_macbook_pro&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;17kcgjv&#x2F;how_doe...</a> reddit thread talks about some of the pros and cons of a m3 max with 128 Gb costing ~5-6K</div><br/><div id="38693088" class="c"><input type="checkbox" id="c-38693088" checked=""/><div class="controls bullet"><span class="by">mkesper</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38689326">parent</a><span>|</span><a href="#38688283">next</a><span>|</span><label class="collapse" for="c-38693088">[-]</label><label class="expand" for="c-38693088">[1 more]</label></div><br/><div class="children"><div class="content">You can buy multiple 4090s for that money and will get real GPUs including tensor cores. Still relevant it seems: <a href="https:&#x2F;&#x2F;timdettmers.com&#x2F;2023&#x2F;01&#x2F;30&#x2F;which-gpu-for-deep-learning&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;timdettmers.com&#x2F;2023&#x2F;01&#x2F;30&#x2F;which-gpu-for-deep-learni...</a></div><br/></div></div></div></div></div></div><div id="38688283" class="c"><input type="checkbox" id="c-38688283" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687866">parent</a><span>|</span><a href="#38688580">prev</a><span>|</span><a href="#38689372">next</a><span>|</span><label class="collapse" for="c-38688283">[-]</label><label class="expand" for="c-38688283">[2 more]</label></div><br/><div class="children"><div class="content">I was talking about m2 Macs. Just comparing that the best you can do with a gpu is 24GB, Macs go far beyond because of their integrated memory.</div><br/><div id="38688341" class="c"><input type="checkbox" id="c-38688341" checked=""/><div class="controls bullet"><span class="by">MilaM</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38688283">parent</a><span>|</span><a href="#38689372">next</a><span>|</span><label class="collapse" for="c-38688341">[-]</label><label class="expand" for="c-38688341">[1 more]</label></div><br/><div class="children"><div class="content">Ok, sorry. I did not understand that you just mentioned that to give more context. Totally makes sense.</div><br/></div></div></div></div></div></div></div></div><div id="38689372" class="c"><input type="checkbox" id="c-38689372" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#38687654">parent</a><span>|</span><a href="#38687731">prev</a><span>|</span><a href="#38687865">next</a><span>|</span><label class="collapse" for="c-38689372">[-]</label><label class="expand" for="c-38689372">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;17kcgjv&#x2F;how_does_apples_new_m3_128gb_ram_macbook_pro&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;17kcgjv&#x2F;how_doe...</a> reddit thread has some ideas.<p>What I can draw from reading of that thread is that you can buy a Desktop Rig with 200GB memory bandwidth (comparable to m3 pro and max) and a lot of expansion capability (256GB RAM). You should find out if that&#x27;s still good enough for your local use case for token per second or training.<p>Then just use SSH&#x2F;XTerm(and possibly ngrok) to login with good speed from anywhere into your rig with a light M2 ?</div><br/></div></div><div id="38687865" class="c"><input type="checkbox" id="c-38687865" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38687654">parent</a><span>|</span><a href="#38689372">prev</a><span>|</span><a href="#38688569">next</a><span>|</span><label class="collapse" for="c-38687865">[-]</label><label class="expand" for="c-38687865">[8 more]</label></div><br/><div class="children"><div class="content">Get as much RAM as possible.<p>16GB is not enough.<p>32GB is enough to run quantized Mixtral, which is the current best openly licensed model.<p>... but who knows what will emerge in the next 12 months?<p>I have 64GB and I&#x27;m regretting not shelling out for more.<p>Frustratingly you still have WAY more options for running interesting models on a Linux or Windows NVIDIA device, but I like Mac for a bunch of other reasons.</div><br/><div id="38688065" class="c"><input type="checkbox" id="c-38688065" checked=""/><div class="controls bullet"><span class="by">Casteil</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687865">parent</a><span>|</span><a href="#38688286">next</a><span>|</span><label class="collapse" for="c-38688065">[-]</label><label class="expand" for="c-38688065">[1 more]</label></div><br/><div class="children"><div class="content">I run 7b&#x2F;13b models pretty gracefully on a 16GB M1 Pro, but it does leave me wanting a little more headroom for other things like Docker and browser eating multiple gigabytes of ram themselves.<p>Maybe keep an eye out for M1 &#x2F; M2 deals with high ram config? I&#x27;ve seen 64GB MBPs lately for &lt;$2300 (slickdeals.net)</div><br/></div></div><div id="38688286" class="c"><input type="checkbox" id="c-38688286" checked=""/><div class="controls bullet"><span class="by">MilaM</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687865">parent</a><span>|</span><a href="#38688065">prev</a><span>|</span><a href="#38687985">next</a><span>|</span><label class="collapse" for="c-38688286">[-]</label><label class="expand" for="c-38688286">[2 more]</label></div><br/><div class="children"><div class="content">Thank you (and all the adjacent comments). I don&#x27;t really need so much RAM for my regular work. Running the llms locally would only be for fun and experiments.<p>I think 32GB might be the best middle ground for my needs and budget constraints.<p>It&#x27;s really a pity that you can&#x27;t extend RAM in most Apple Silicon Macs and have to decide carefully upfront.</div><br/><div id="38688401" class="c"><input type="checkbox" id="c-38688401" checked=""/><div class="controls bullet"><span class="by">amrrs</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38688286">parent</a><span>|</span><a href="#38687985">next</a><span>|</span><label class="collapse" for="c-38688401">[-]</label><label class="expand" for="c-38688401">[1 more]</label></div><br/><div class="children"><div class="content">I ended up ordering a 36GB M3 for similar reason.<p>I currently run Mistral and a few mistral derivatives using Ollama with decent inference speed on a 2019 Intel Mac 32GB. So I assumed the new one with 32ish should do a better job.<p>I&#x27;ve tried vision model Llava as well, a bit more latency but works fine.<p>With Apple&#x27;s own Mlx things might improve .</div><br/></div></div></div></div><div id="38687985" class="c"><input type="checkbox" id="c-38687985" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687865">parent</a><span>|</span><a href="#38688286">prev</a><span>|</span><a href="#38688022">next</a><span>|</span><label class="collapse" for="c-38687985">[-]</label><label class="expand" for="c-38687985">[3 more]</label></div><br/><div class="children"><div class="content">Why not both? :)<p>Bait aside, I&#x27;d love to read about how are you using those models. I&#x27;m mostly interested in code comprehension and meeting summarisation.</div><br/><div id="38688035" class="c"><input type="checkbox" id="c-38688035" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687985">parent</a><span>|</span><a href="#38688022">next</a><span>|</span><label class="collapse" for="c-38688035">[-]</label><label class="expand" for="c-38688035">[2 more]</label></div><br/><div class="children"><div class="content">I still mostly use GPT-4 to get actual work done, because until a Mixtral came along it felt like the local models were no competition.<p>I&#x27;m going to bump up my usage of Mixtral a bit now to see how it feels for that kind of stuff.</div><br/><div id="38688247" class="c"><input type="checkbox" id="c-38688247" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38688035">parent</a><span>|</span><a href="#38688022">next</a><span>|</span><label class="collapse" for="c-38688247">[-]</label><label class="expand" for="c-38688247">[1 more]</label></div><br/><div class="children"><div class="content">Ah, same here. Have been itching to make some use of those local llms for some time, always ending up in chatgpt as well.<p>Although for those napkin like ideas gpt4 (including the turbo variant) get costly quickly.</div><br/></div></div></div></div></div></div><div id="38688022" class="c"><input type="checkbox" id="c-38688022" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#38687654">root</a><span>|</span><a href="#38687865">parent</a><span>|</span><a href="#38687985">prev</a><span>|</span><a href="#38688569">next</a><span>|</span><label class="collapse" for="c-38688022">[-]</label><label class="expand" for="c-38688022">[1 more]</label></div><br/><div class="children"><div class="content">Just got 64gb as well but about to refund it for a bigger one after reading your comment :)</div><br/></div></div></div></div><div id="38688569" class="c"><input type="checkbox" id="c-38688569" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#38687654">parent</a><span>|</span><a href="#38687865">prev</a><span>|</span><a href="#38688614">next</a><span>|</span><label class="collapse" for="c-38688569">[-]</label><label class="expand" for="c-38688569">[1 more]</label></div><br/><div class="children"><div class="content">long story short, for a machine that has about 5+ years of shelf life it isn&#x27;t a bad value prop to go and buy the absolute top end, especially if you expect some roi out of it, just be aware that if you don&#x27;t want the top end for llm the m2 currently provides more value because m2 has memory bandwith than the m3 at the middle range.<p>memory bandwidth is the key to model speed, and memory size is what enable you to use larger model (quantization let you push thing further, to a point) so one thing to note is that on the M3 pro&#x2F;max only the top end model gets the full bandwidh, while the m1&#x2F;m2 pro enjoy full bandwidth from a smaller memory size. this may be important if you value speed above model size or vice versa.  M2 Pro, M2 Max get approximately 200 GB&#x2F;s and 400 GB&#x2F;s, but things are more complicated for m3: M3 Pro gets 150mb&#x2F;s, and M3 max gets 300mb&#x2F;s at 36gb and 400mb&#x2F;s at 48gb<p>few more things to note:<p>it&#x27;s absolutely fine to go and play around with llm but even with a llm monster machine there&#x27;s nothing wrong in starting with smaller models and learning how to squeeze the maximum amount of work out of them. the learning do transfer to larger model. this may or may not be important if at some point you&#x27;ll want to monetize or deploy to production what you learned. while the mac itself is a good investment for personal use, once you move to servers, cost skyrockets with model size, because of supply constraints on 40gb+ memory gpus. if you are dependent to a 70b parameter model, you&#x27;ll have a hard time to make a cost effective solution. if it&#x27;s stricly to playing around, you can disregard this concern<p>even if you&#x27;re playing around, a 70b is going to run at 7 tokens &#x2F; second, which is fine for a local chat, but if you are writing a program and need inference in bulk, it&#x27;s fairly slow.<p>another thing of note is that while the field is still undecided on which size and architecture is good enough, the moltitude of small fish experimenting with tuning and mix of instructions are largely experimenting on smaller models. currently my favorite is openhermes-2.5-mistral-7b-16k, but it&#x27;s not an indication that mistrals are strictly better than llama2, more an indication that experimenting with 7b is more cost effective for third parties without access to gpu than experimenting with 13b, and so you&#x27;ll find 13b model kinda stagnating, with many of them trained in a period where people didn&#x27;t really know the best parameters for finetuning and are so to say a bit behind the curve. a few tuners are working at 70b models, but these seems to be pivoting to mixtrals and the likes, which will cause a similar stagnation on the top end, that is, until llama3 or the next mixtral size drops, then, who knows</div><br/></div></div><div id="38688614" class="c"><input type="checkbox" id="c-38688614" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#38687654">parent</a><span>|</span><a href="#38688569">prev</a><span>|</span><a href="#38688755">next</a><span>|</span><label class="collapse" for="c-38688614">[-]</label><label class="expand" for="c-38688614">[1 more]</label></div><br/><div class="children"><div class="content">Rule of thumb: when buying a new Mac, always max RAM.</div><br/></div></div></div></div><div id="38688755" class="c"><input type="checkbox" id="c-38688755" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#38687654">prev</a><span>|</span><a href="#38688537">next</a><span>|</span><label class="collapse" for="c-38688755">[-]</label><label class="expand" for="c-38688755">[3 more]</label></div><br/><div class="children"><div class="content">Anybody have a Mistral invitation they can spare? I’m super curious how the medium performs compared to GPT3.5&#x2F;4</div><br/><div id="38688801" class="c"><input type="checkbox" id="c-38688801" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38688755">parent</a><span>|</span><a href="#38688537">next</a><span>|</span><label class="collapse" for="c-38688801">[-]</label><label class="expand" for="c-38688801">[2 more]</label></div><br/><div class="children"><div class="content">Are you on their waitlist? I got through that in less than 48 hours, so it might be worth a shot.</div><br/><div id="38689148" class="c"><input type="checkbox" id="c-38689148" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#38688755">root</a><span>|</span><a href="#38688801">parent</a><span>|</span><a href="#38688537">next</a><span>|</span><label class="collapse" for="c-38689148">[-]</label><label class="expand" for="c-38689148">[1 more]</label></div><br/><div class="children"><div class="content">I am, I signed up late though and got an email about it last night “ Access to our API is currently invitation-only, but we&#x27;ll let you know when you can subscribe to get access to our best models.”</div><br/></div></div></div></div></div></div><div id="38688537" class="c"><input type="checkbox" id="c-38688537" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#38688755">prev</a><span>|</span><a href="#38688319">next</a><span>|</span><label class="collapse" for="c-38688537">[-]</label><label class="expand" for="c-38688537">[2 more]</label></div><br/><div class="children"><div class="content">All this needless difficulty when you can just do the right thing and use oogabooga.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a></div><br/><div id="38688770" class="c"><input type="checkbox" id="c-38688770" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38688537">parent</a><span>|</span><a href="#38688319">next</a><span>|</span><label class="collapse" for="c-38688770">[-]</label><label class="expand" for="c-38688770">[1 more]</label></div><br/><div class="children"><div class="content">Looks like that provides an OpenAI-compatible API endpoint, which means you can use my LLM command-line utility against with the same pattern as for Llamafile: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Dec&#x2F;18&#x2F;mistral&#x2F;#llamafile-openai" rel="nofollow noreferrer">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Dec&#x2F;18&#x2F;mistral&#x2F;#llamafile-ope...</a></div><br/></div></div></div></div><div id="38688319" class="c"><input type="checkbox" id="c-38688319" checked=""/><div class="controls bullet"><span class="by">mrbonner</span><span>|</span><a href="#38688537">prev</a><span>|</span><a href="#38687612">next</a><span>|</span><label class="collapse" for="c-38688319">[-]</label><label class="expand" for="c-38688319">[2 more]</label></div><br/><div class="children"><div class="content">Any option to use a SaaS UI to use those OS models? I’m ok with paying the API access for Anya ale for example and use the ApI for the UI.</div><br/><div id="38688431" class="c"><input type="checkbox" id="c-38688431" checked=""/><div class="controls bullet"><span class="by">ntonozzi</span><span>|</span><a href="#38688319">parent</a><span>|</span><a href="#38687612">next</a><span>|</span><label class="collapse" for="c-38688431">[-]</label><label class="expand" for="c-38688431">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been using baseten (<a href="https:&#x2F;&#x2F;baseten.co" rel="nofollow noreferrer">https:&#x2F;&#x2F;baseten.co</a>) and it’s been fun and has reasonable prices. Sometimes you can run some of these models from the hugging face model page, but it’s hit or miss.</div><br/></div></div></div></div><div id="38687612" class="c"><input type="checkbox" id="c-38687612" checked=""/><div class="controls bullet"><span class="by">bitdeep</span><span>|</span><a href="#38688319">prev</a><span>|</span><label class="collapse" for="c-38687612">[-]</label><label class="expand" for="c-38687612">[8 more]</label></div><br/><div class="children"><div class="content">You missed ollama option</div><br/><div id="38688187" class="c"><input type="checkbox" id="c-38688187" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38687612">parent</a><span>|</span><a href="#38687749">next</a><span>|</span><label class="collapse" for="c-38688187">[-]</label><label class="expand" for="c-38688187">[1 more]</label></div><br/><div class="children"><div class="content">I was hoping I could run my LLM CLI tool against Ollama via their localhost API, but it looks like they don&#x27;t offer an OpenAI-compatible endpoint yet.<p>If they add that it will work out of the box: <a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;other-models.html#openai-compatible-models" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;other-models.html#openai-...</a><p>Otherwise someone would need to write a plugin for it, which would probably be pretty simple - I imagine it would look a bit like the llm-mistral plugin but adapted for the Ollama API design: <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-mistral&#x2F;blob&#x2F;main&#x2F;llm_mistral.py">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-mistral&#x2F;blob&#x2F;main&#x2F;llm_mistral....</a></div><br/></div></div><div id="38687749" class="c"><input type="checkbox" id="c-38687749" checked=""/><div class="controls bullet"><span class="by">M4v3R</span><span>|</span><a href="#38687612">parent</a><span>|</span><a href="#38688187">prev</a><span>|</span><label class="collapse" for="c-38687749">[-]</label><label class="expand" for="c-38687749">[6 more]</label></div><br/><div class="children"><div class="content">Which honestly is the easiest option of them all if you own an Apple Silicon based Mac. You just download the ollama and then run `ollama run mixtral` (or choose a quantization from their models page if you don&#x27;t have enough ram to run the defalt q4 model) and that&#x27;s it.</div><br/><div id="38688003" class="c"><input type="checkbox" id="c-38688003" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38687612">root</a><span>|</span><a href="#38687749">parent</a><span>|</span><a href="#38687859">next</a><span>|</span><label class="collapse" for="c-38688003">[-]</label><label class="expand" for="c-38688003">[4 more]</label></div><br/><div class="children"><div class="content">I tried an hour ago and had a can&#x27;t load model error. Everything up to date. Is there any special step?</div><br/><div id="38688098" class="c"><input type="checkbox" id="c-38688098" checked=""/><div class="controls bullet"><span class="by">mmcclure</span><span>|</span><a href="#38687612">root</a><span>|</span><a href="#38688003">parent</a><span>|</span><a href="#38688467">next</a><span>|</span><label class="collapse" for="c-38688098">[-]</label><label class="expand" for="c-38688098">[1 more]</label></div><br/><div class="children"><div class="content">Tried `ollama pull mixtral` just now and it seems to be working, albeit pretty slowly.</div><br/></div></div><div id="38688467" class="c"><input type="checkbox" id="c-38688467" checked=""/><div class="controls bullet"><span class="by">Casteil</span><span>|</span><a href="#38687612">root</a><span>|</span><a href="#38688003">parent</a><span>|</span><a href="#38688098">prev</a><span>|</span><a href="#38687859">next</a><span>|</span><label class="collapse" for="c-38688467">[-]</label><label class="expand" for="c-38688467">[2 more]</label></div><br/><div class="children"><div class="content">How much RAM do you have? Mixtral is a beast and the non quantized model wants 40GB+ of memory.</div><br/><div id="38689652" class="c"><input type="checkbox" id="c-38689652" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38687612">root</a><span>|</span><a href="#38688467">parent</a><span>|</span><a href="#38687859">next</a><span>|</span><label class="collapse" for="c-38689652">[-]</label><label class="expand" for="c-38689652">[1 more]</label></div><br/><div class="children"><div class="content">Ah, that might be it! I have only 32</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>