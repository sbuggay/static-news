<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700557264901" as="style"/><link rel="stylesheet" href="styles.css?v=1700557264901"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.secondstate.io/articles/mistral-7b-instruct-v0.1/">Self-Hosting Open Source LLMs: Cross Devices and Local Deployment of Mistral 7B</a>Â <span class="domain">(<a href="https://www.secondstate.io">www.secondstate.io</a>)</span></div><div class="subtext"><span>3Sophons</span> | <span>17 comments</span></div><br/><div><div id="38360770" class="c"><input type="checkbox" id="c-38360770" checked=""/><div class="controls bullet"><span class="by">tyayers</span><span>|</span><a href="#38360586">next</a><span>|</span><label class="collapse" for="c-38360770">[-]</label><label class="expand" for="c-38360770">[1 more]</label></div><br/><div class="children"><div class="content">Impressive use of wasmedge, great to see ML projects getting away from python for efficiency&#x27;s sake, thanks for sharing!</div><br/></div></div><div id="38360586" class="c"><input type="checkbox" id="c-38360586" checked=""/><div class="controls bullet"><span class="by">xeckr</span><span>|</span><a href="#38360770">prev</a><span>|</span><a href="#38360330">next</a><span>|</span><label class="collapse" for="c-38360586">[-]</label><label class="expand" for="c-38360586">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing this!<p>I read somewhere that Mistral 7B had a similar performance to GPT-3, but it seems to be miles behind it unfortunately.</div><br/><div id="38360635" class="c"><input type="checkbox" id="c-38360635" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#38360586">parent</a><span>|</span><a href="#38360330">next</a><span>|</span><label class="collapse" for="c-38360635">[-]</label><label class="expand" for="c-38360635">[2 more]</label></div><br/><div class="children"><div class="content">Remember GPT-3 != ChatGPT. I could perhaps believe it is similar to GPT-3, but it&#x27;s certainly miles behind ChatGPT even with 3.5</div><br/><div id="38360735" class="c"><input type="checkbox" id="c-38360735" checked=""/><div class="controls bullet"><span class="by">xeckr</span><span>|</span><a href="#38360586">root</a><span>|</span><a href="#38360635">parent</a><span>|</span><a href="#38360330">next</a><span>|</span><label class="collapse" for="c-38360735">[-]</label><label class="expand" for="c-38360735">[1 more]</label></div><br/><div class="children"><div class="content">I was talking about GPT-3, which I have interacted with via the developer API.<p>To be fair to the developers of Mistral, it&#x27;s still amazing that I can run this on my computer, and it&#x27;s certainly better than the first LLaMA.</div><br/></div></div></div></div></div></div><div id="38360330" class="c"><input type="checkbox" id="c-38360330" checked=""/><div class="controls bullet"><span class="by">jesterson</span><span>|</span><a href="#38360586">prev</a><span>|</span><a href="#38360266">next</a><span>|</span><label class="collapse" for="c-38360330">[-]</label><label class="expand" for="c-38360330">[4 more]</label></div><br/><div class="children"><div class="content">Does anyone have any feedback on using those open-source models with any language except English? Particularly non-western group of languages like korean&#x2F;japanese&#x2F;chinese?<p>Will assess myself but wonder if anyone tried.</div><br/><div id="38361130" class="c"><input type="checkbox" id="c-38361130" checked=""/><div class="controls bullet"><span class="by">clarionbell</span><span>|</span><a href="#38360330">parent</a><span>|</span><a href="#38360419">next</a><span>|</span><label class="collapse" for="c-38361130">[-]</label><label class="expand" for="c-38361130">[1 more]</label></div><br/><div class="children"><div class="content">They work, but they need way more tokens to express themselves. Still, it beats paying OpenAI for the privilege.</div><br/></div></div><div id="38360419" class="c"><input type="checkbox" id="c-38360419" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38360330">parent</a><span>|</span><a href="#38361130">prev</a><span>|</span><a href="#38360528">next</a><span>|</span><label class="collapse" for="c-38360419">[-]</label><label class="expand" for="c-38360419">[1 more]</label></div><br/><div class="children"><div class="content">For Chinese, there are this Yi Model by 0.1 AI 
Yi 34B <a href="https:&#x2F;&#x2F;www.secondstate.io&#x2F;articles&#x2F;yi-34b&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.secondstate.io&#x2F;articles&#x2F;yi-34b&#x2F;</a>
Uncencored Yi: <a href="https:&#x2F;&#x2F;www.secondstate.io&#x2F;articles&#x2F;dolphin-2.2-yi-34b&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.secondstate.io&#x2F;articles&#x2F;dolphin-2.2-yi-34b&#x2F;</a><p>and also Baichuan <a href="https:&#x2F;&#x2F;www.secondstate.io&#x2F;articles&#x2F;baichuan2-13b-chat&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.secondstate.io&#x2F;articles&#x2F;baichuan2-13b-chat&#x2F;</a><p>There is an Arabic one called Jais mentioned by Satya a few days ago on the Microsoft dev day would like to try out</div><br/></div></div><div id="38360528" class="c"><input type="checkbox" id="c-38360528" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#38360330">parent</a><span>|</span><a href="#38360419">prev</a><span>|</span><a href="#38360266">next</a><span>|</span><label class="collapse" for="c-38360528">[-]</label><label class="expand" for="c-38360528">[1 more]</label></div><br/><div class="children"><div class="content">Alibaba&#x27;s Qwen is excellent for Chinese.</div><br/></div></div></div></div><div id="38360266" class="c"><input type="checkbox" id="c-38360266" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#38360330">prev</a><span>|</span><a href="#38360044">next</a><span>|</span><label class="collapse" for="c-38360266">[-]</label><label class="expand" for="c-38360266">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp can run the Q4 variant of the same at 30tok&#x2F;s on an M1 Pro unlike the 20tok&#x2F;s being quoted</div><br/></div></div><div id="38360044" class="c"><input type="checkbox" id="c-38360044" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#38360266">prev</a><span>|</span><label class="collapse" for="c-38360044">[-]</label><label class="expand" for="c-38360044">[7 more]</label></div><br/><div class="children"><div class="content">Mistral have created a docker image which hosts their model in vllm. Vllm creates an openai like http API interface.<p><a href="https:&#x2F;&#x2F;docs.mistral.ai&#x2F;quickstart&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.mistral.ai&#x2F;quickstart&#x2F;</a></div><br/><div id="38360381" class="c"><input type="checkbox" id="c-38360381" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38360044">parent</a><span>|</span><a href="#38360257">next</a><span>|</span><label class="collapse" for="c-38360381">[-]</label><label class="expand" for="c-38360381">[5 more]</label></div><br/><div class="children"><div class="content">While Mistral&#x27;s Docker image provides a practical solution, it&#x27;s important to note that Docker isn&#x27;t truly cross-platform. A Docker image compiled for x86 architecture won&#x27;t run on ARM, which can be a limitation for diverse hardware environments. On the other hand, Rust + WebAssembly (Wasm) offers genuine cross-platform capabilities. Wasm is not only faster to start and more resource-efficient but also more secure. This makes it exceptionally suitable for handling large language models, ensuring high performance while being cost-effective. In scenarios where diverse hardware support and efficiency are crucial, Rust + Wasm emerges as a superior choice over Docker.</div><br/><div id="38360426" class="c"><input type="checkbox" id="c-38360426" checked=""/><div class="controls bullet"><span class="by">hobofan</span><span>|</span><a href="#38360044">root</a><span>|</span><a href="#38360381">parent</a><span>|</span><a href="#38360257">next</a><span>|</span><label class="collapse" for="c-38360426">[-]</label><label class="expand" for="c-38360426">[4 more]</label></div><br/><div class="children"><div class="content">Is there any data to back up the &quot;faster to start and more resource-efficient&quot;, especially if you compare it to the native non-Python solutions that most people are using to run LLMs on local machines?<p>I&#x27;m as big of a fan of Rust and WASM as the next person, but throwing around claims like that without benchmarks is one of the quickest ways to get your product dismissed.</div><br/><div id="38360680" class="c"><input type="checkbox" id="c-38360680" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38360044">root</a><span>|</span><a href="#38360426">parent</a><span>|</span><a href="#38360695">next</a><span>|</span><label class="collapse" for="c-38360680">[-]</label><label class="expand" for="c-38360680">[2 more]</label></div><br/><div class="children"><div class="content">Faster is compared to Python. Portable, more secure and lightweight are compared with Python and other native solutions. In terms of benchmarks, Rust &#x2F; C++ is 50,000x faster than Python; WasmEdge runtime + portable app is 30M compared with 4G Python and 300MB llama.cpp Docker image that is NOT portable across CPU or GPU; Wasm sandbox is more secure than native binary.</div><br/><div id="38361124" class="c"><input type="checkbox" id="c-38361124" checked=""/><div class="controls bullet"><span class="by">hobofan</span><span>|</span><a href="#38360044">root</a><span>|</span><a href="#38360680">parent</a><span>|</span><a href="#38360695">next</a><span>|</span><label class="collapse" for="c-38361124">[-]</label><label class="expand" for="c-38361124">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for clarifying!</div><br/></div></div></div></div><div id="38360695" class="c"><input type="checkbox" id="c-38360695" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38360044">root</a><span>|</span><a href="#38360426">parent</a><span>|</span><a href="#38360680">prev</a><span>|</span><a href="#38360257">next</a><span>|</span><label class="collapse" for="c-38360695">[-]</label><label class="expand" for="c-38360695">[1 more]</label></div><br/><div class="children"><div class="content">Details <a href="https:&#x2F;&#x2F;www.cncf.io&#x2F;blog&#x2F;2023&#x2F;10&#x2F;30&#x2F;rust-webassembly-building-infrastructure-for-large-language-model-ecosystems&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.cncf.io&#x2F;blog&#x2F;2023&#x2F;10&#x2F;30&#x2F;rust-webassembly-buildin...</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>