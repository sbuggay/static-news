<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703149258625" as="style"/><link rel="stylesheet" href="styles.css?v=1703149258625"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/SJTU-IPADS/PowerInfer">High-Speed Large Language Model Serving on PCs with Consumer-Grade GPUs</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>dataminer</span> | <span>81 comments</span></div><br/><div><div id="38710319" class="c"><input type="checkbox" id="c-38710319" checked=""/><div class="controls bullet"><span class="by">phh</span><span>|</span><a href="#38709708">next</a><span>|</span><label class="collapse" for="c-38710319">[-]</label><label class="expand" for="c-38710319">[30 more]</label></div><br/><div class="children"><div class="content">Took me a while to understand what their &quot;hot&quot; and &quot;cold&quot;  neurons meant, since in most ML I do, there is no such notion. And their paper doesn&#x27;t directly define it (or I missed it)<p>After some thoughts, in ReLU it does make sense, because half of the function is constant, so you can say that you&#x27;re &quot;cold&quot; if that neuron&#x27;s ReLU-ed output is often 0 . So I checked whether ReLU was common in LLMs, original llama doesn&#x27;t use ReLU. But after (re-)reading the github, it actually only works on ReLU models. Turns out that there is a group of people &quot;fine-tuning&quot; (I would rather call that re-training, since you start by breaking the model?) models to use ReLU to allow for that sparsity: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;SparseLLM" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;SparseLLM</a><p>So this is sadly not applicable to any model you can find on the internet, but that sounds like a great progress anyway. Possibly this might shift the compromises back to bigger models but with &quot;less ideal&quot; activations. Also I&#x27;m curious what would be the legal impacts on it (since USA and EU refers to a model&#x27;s FLOPs&#x2F;number of parameters... How do you compute it with sparsity? Do you average?)<p>I think that a possible avenue for future research in that area is keeping original activation (like llama keeping SwiGLU), but using quantification to define &quot;hot&quot; and &quot;cold&quot; neurons to be saturation areas. (For example, saying that this activation function, below -1. at 8 bit, is equivalent to -infinity, and thus this is a cold neuron)</div><br/><div id="38711021" class="c"><input type="checkbox" id="c-38711021" checked=""/><div class="controls bullet"><span class="by">boredumb</span><span>|</span><a href="#38710319">parent</a><span>|</span><a href="#38710398">next</a><span>|</span><label class="collapse" for="c-38711021">[-]</label><label class="expand" for="c-38711021">[25 more]</label></div><br/><div class="children"><div class="content">&gt; Also I&#x27;m curious what would be the legal impacts on it (since USA and EU refers to a model&#x27;s FLOPs&#x2F;number of parameters... How do you compute it with sparsity? Do you average?).<p>How&#x2F;when did these types of regulations come about? This feels like an insane thing to have to keep in mind while developing.</div><br/><div id="38711751" class="c"><input type="checkbox" id="c-38711751" checked=""/><div class="controls bullet"><span class="by">phh</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38711021">parent</a><span>|</span><a href="#38711544">next</a><span>|</span><label class="collapse" for="c-38711751">[-]</label><label class="expand" for="c-38711751">[10 more]</label></div><br/><div class="children"><div class="content">&gt; How&#x2F;when did these types of regulations come about?<p>I can&#x27;t say much about US. As I see it, EU pretty much copied US about that part. There was nothing related to computation in the EU&#x27;s AI Act projects until few months ago, it was purely a &quot;what kind of data processing are you allowed to do?&quot;</div><br/><div id="38712943" class="c"><input type="checkbox" id="c-38712943" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38711751">parent</a><span>|</span><a href="#38711544">next</a><span>|</span><label class="collapse" for="c-38712943">[-]</label><label class="expand" for="c-38712943">[9 more]</label></div><br/><div class="children"><div class="content">Politely, what the hell are you talking about? Who is telling anyone what they can or cannot compute?</div><br/><div id="38713028" class="c"><input type="checkbox" id="c-38713028" checked=""/><div class="controls bullet"><span class="by">iamjackg</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38712943">parent</a><span>|</span><a href="#38713455">next</a><span>|</span><label class="collapse" for="c-38713028">[-]</label><label class="expand" for="c-38713028">[6 more]</label></div><br/><div class="children"><div class="content">US:<p><a href="https:&#x2F;&#x2F;www.whitehouse.gov&#x2F;briefing-room&#x2F;presidential-actions&#x2F;2023&#x2F;10&#x2F;30&#x2F;executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.whitehouse.gov&#x2F;briefing-room&#x2F;presidential-action...</a><p>&quot;Until such technical conditions are defined, the Secretary shall require compliance with these reporting requirements for:<p><pre><code>          (i)   any model that was trained using a quantity of computing power greater than 1026 integer or floating-point operations, or using primarily biological sequence data and using a quantity of computing power greater than 1023 integer or floating-point operations[...]&quot;
</code></pre>
EU:<p><a href="https:&#x2F;&#x2F;thefuturesociety.org&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;12&#x2F;EU-AI-Act-Compliance-Analysis.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;thefuturesociety.org&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;12&#x2F;EU-A...</a></div><br/><div id="38717954" class="c"><input type="checkbox" id="c-38717954" checked=""/><div class="controls bullet"><span class="by">hayley-patton</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38713028">parent</a><span>|</span><a href="#38713356">next</a><span>|</span><label class="collapse" for="c-38717954">[-]</label><label class="expand" for="c-38717954">[1 more]</label></div><br/><div class="children"><div class="content">Are they trying to bring back SIMD-within-a-register? Though that only gives you ~one order of magnitude doing packed 4-bit stuff with 64-bit GPRs. And perhaps fixed-point, sign-exponent and posits are unregulated.</div><br/></div></div><div id="38713356" class="c"><input type="checkbox" id="c-38713356" checked=""/><div class="controls bullet"><span class="by">geon</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38713028">parent</a><span>|</span><a href="#38717954">prev</a><span>|</span><a href="#38713451">next</a><span>|</span><label class="collapse" for="c-38713356">[-]</label><label class="expand" for="c-38713356">[3 more]</label></div><br/><div class="children"><div class="content">&gt; 1026<p>&gt; 1023<p>Should be 10^26 and 10^23.</div><br/><div id="38713874" class="c"><input type="checkbox" id="c-38713874" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38713356">parent</a><span>|</span><a href="#38713451">next</a><span>|</span><label class="collapse" for="c-38713874">[-]</label><label class="expand" for="c-38713874">[2 more]</label></div><br/><div class="children"><div class="content">Probably I did this wrong but I’m getting an approximation of 300K H100s completes that in a month. At least they choose something fairly large it seems. Not sure how LoRA or other incremental training is handled.</div><br/><div id="38715825" class="c"><input type="checkbox" id="c-38715825" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38713874">parent</a><span>|</span><a href="#38713451">next</a><span>|</span><label class="collapse" for="c-38715825">[-]</label><label class="expand" for="c-38715825">[1 more]</label></div><br/><div class="children"><div class="content">Depends on which spec you used, since the law doesn&#x27;t specify the floating point width. If you used FP8 ops on the H100 SXM then a single GPU would hit the limit in 25265285497.72612 seconds. 300,000 GPUs would pass 10^26 FP8 ops in 23 hours.</div><br/></div></div></div></div></div></div></div></div><div id="38713455" class="c"><input type="checkbox" id="c-38713455" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38712943">parent</a><span>|</span><a href="#38713028">prev</a><span>|</span><a href="#38711544">next</a><span>|</span><label class="collapse" for="c-38713455">[-]</label><label class="expand" for="c-38713455">[2 more]</label></div><br/><div class="children"><div class="content">anyone with a functional government.</div><br/></div></div></div></div></div></div><div id="38711544" class="c"><input type="checkbox" id="c-38711544" checked=""/><div class="controls bullet"><span class="by">radicalbyte</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38711021">parent</a><span>|</span><a href="#38711751">prev</a><span>|</span><a href="#38712932">next</a><span>|</span><label class="collapse" for="c-38711544">[-]</label><label class="expand" for="c-38711544">[13 more]</label></div><br/><div class="children"><div class="content">The EU messed up with the GDPR - they should have implemented it at least a decade earlier and ignored the lobby which lead to the cookie banner instead of either an outright ban on tracking for all but a tiny number of purposes. Such a ban would have had a negligible impact on the tech industry financially but would have had huge privacy rewards.<p>They&#x27;re trying to get in early on AI so as not to make the same mistake again. Which might result in them making the opposite mistake.</div><br/><div id="38714147" class="c"><input type="checkbox" id="c-38714147" checked=""/><div class="controls bullet"><span class="by">quocanh</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38711544">parent</a><span>|</span><a href="#38714479">prev</a><span>|</span><a href="#38712932">next</a><span>|</span><label class="collapse" for="c-38714147">[-]</label><label class="expand" for="c-38714147">[11 more]</label></div><br/><div class="children"><div class="content">Tiny negligible impact on the industry (Except cut advertising revenue in half, but who cares. What do ads pay for anyways?)</div><br/><div id="38715680" class="c"><input type="checkbox" id="c-38715680" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38714147">parent</a><span>|</span><a href="#38712932">next</a><span>|</span><label class="collapse" for="c-38715680">[-]</label><label class="expand" for="c-38715680">[10 more]</label></div><br/><div class="children"><div class="content">&gt; What do ads pay for anyways?<p>Making the world a worse place? If you look carefully you’ll realize most of the harms and negative effects of technology are due to it being primarily funded by advertising and trying to maximize ad revenue.</div><br/><div id="38718144" class="c"><input type="checkbox" id="c-38718144" checked=""/><div class="controls bullet"><span class="by">quocanh</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38715680">parent</a><span>|</span><a href="#38716228">next</a><span>|</span><label class="collapse" for="c-38718144">[-]</label><label class="expand" for="c-38718144">[1 more]</label></div><br/><div class="children"><div class="content">And the largest benefit of modern technology comes from the fact that so much of it is &quot;free&quot; (ad-supported). Without ads, there would simply be no effect at all.</div><br/></div></div><div id="38716228" class="c"><input type="checkbox" id="c-38716228" checked=""/><div class="controls bullet"><span class="by">jayd16</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38715680">parent</a><span>|</span><a href="#38718144">prev</a><span>|</span><a href="#38712932">next</a><span>|</span><label class="collapse" for="c-38716228">[-]</label><label class="expand" for="c-38716228">[8 more]</label></div><br/><div class="children"><div class="content">Ads seem less harmful than, say, mobile game rewards (gambling).  Plenty of dark patterns in the paid space too.  Banning ads would not be a panacea.</div><br/><div id="38717410" class="c"><input type="checkbox" id="c-38717410" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38716228">parent</a><span>|</span><a href="#38716355">next</a><span>|</span><label class="collapse" for="c-38717410">[-]</label><label class="expand" for="c-38717410">[1 more]</label></div><br/><div class="children"><div class="content">All those mobile games frequently require advertising in the first place to race their customers&#x2F;victims. We should definitely ban a lot of the dark patterns which would coincidentally improve AAA games which use similar patterns (eg increasing duration of gameplay because of grinding mechanics).</div><br/></div></div><div id="38716355" class="c"><input type="checkbox" id="c-38716355" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38716228">parent</a><span>|</span><a href="#38717410">prev</a><span>|</span><a href="#38716575">next</a><span>|</span><label class="collapse" for="c-38716355">[-]</label><label class="expand" for="c-38716355">[4 more]</label></div><br/><div class="children"><div class="content">Mobile games are only harmful to a relatively tiny group of addicted gamers, while internet ads have very serious consequences acting on society as a whole.<p>I don’t think mobile gaming companies have a potential to destroy free press, or negatively affect mental health of wide population of teenagers, or invade privacy of billions of people. They simply don’t have the scale for any of that.</div><br/><div id="38717320" class="c"><input type="checkbox" id="c-38717320" checked=""/><div class="controls bullet"><span class="by">slimsag</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38716355">parent</a><span>|</span><a href="#38716575">next</a><span>|</span><label class="collapse" for="c-38717320">[-]</label><label class="expand" for="c-38717320">[3 more]</label></div><br/><div class="children"><div class="content">Ads are harmful, no doubt, but I do not think they are more harmful than the normalization of gambling in our society.<p>&#x27;I watched an ad, and then [my entire life was destroyed]&#x27; is quite hard to imagine, unless it&#x27;s an ad for an MLM, crypto, entrepreneurship scam, or gambling.<p>On the other hand, I absolutely know people who started out in soft gambling who then proceeded to throw their life (and sometimes families) away trying to catch the next high with higher and higher stakes gambling until they lost everything, and then some.<p>We also don&#x27;t really know the impact gambling is going to have in the near future. Loot boxes, online gambling, internet celebrity gambling, etc. really only became popular around ~2010 or later, and the kids who have been growing up with low-risk gambling as a daily accessible thing on their iPads have not come into adulthood yet.</div><br/><div id="38717417" class="c"><input type="checkbox" id="c-38717417" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38717320">parent</a><span>|</span><a href="#38716575">next</a><span>|</span><label class="collapse" for="c-38717417">[-]</label><label class="expand" for="c-38717417">[2 more]</label></div><br/><div class="children"><div class="content">Not an either or situation. We should do both.</div><br/><div id="38717549" class="c"><input type="checkbox" id="c-38717549" checked=""/><div class="controls bullet"><span class="by">slimsag</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38717417">parent</a><span>|</span><a href="#38716575">next</a><span>|</span><label class="collapse" for="c-38717549">[-]</label><label class="expand" for="c-38717549">[1 more]</label></div><br/><div class="children"><div class="content">The parent comment downplayed the importance of mobile gaming&#x2F;gambling. I simply rebutted.</div><br/></div></div></div></div></div></div></div></div><div id="38716575" class="c"><input type="checkbox" id="c-38716575" checked=""/><div class="controls bullet"><span class="by">genman</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38716228">parent</a><span>|</span><a href="#38716355">prev</a><span>|</span><a href="#38712932">next</a><span>|</span><label class="collapse" for="c-38716575">[-]</label><label class="expand" for="c-38716575">[2 more]</label></div><br/><div class="children"><div class="content">I see again and again this non-argument on HN. Yes, if you get robbed but not killed then it is a better outcome than getting killed but this doesn&#x27;t make robbing good by any measure.</div><br/><div id="38716961" class="c"><input type="checkbox" id="c-38716961" checked=""/><div class="controls bullet"><span class="by">8n4vidtmkvmk</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38716575">parent</a><span>|</span><a href="#38712932">next</a><span>|</span><label class="collapse" for="c-38716961">[-]</label><label class="expand" for="c-38716961">[1 more]</label></div><br/><div class="children"><div class="content">But what if you make the punishment for robbing harsher than murder? Maybe people start killing you after robbing you to get a lesser sentence. It happens in some parts of the world, if they accidentally hit you with their car they&#x27;ll run over you again to finish the job because if you sue or go after them it&#x27;ll be real bad. 
Point is we have to be careful about how we regulate things or we can shift things in an even worse direction.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38710398" class="c"><input type="checkbox" id="c-38710398" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38710319">parent</a><span>|</span><a href="#38711021">prev</a><span>|</span><a href="#38710617">next</a><span>|</span><label class="collapse" for="c-38710398">[-]</label><label class="expand" for="c-38710398">[3 more]</label></div><br/><div class="children"><div class="content">That is a huge caveat to leave out of a readme, especially one that claims llama compatibility.</div><br/><div id="38716281" class="c"><input type="checkbox" id="c-38716281" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38710398">parent</a><span>|</span><a href="#38710617">next</a><span>|</span><label class="collapse" for="c-38716281">[-]</label><label class="expand" for="c-38716281">[2 more]</label></div><br/><div class="children"><div class="content">They don’t make that claim as far as I can tell. Just that they support llama2 models.</div><br/><div id="38717895" class="c"><input type="checkbox" id="c-38717895" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38710319">root</a><span>|</span><a href="#38716281">parent</a><span>|</span><a href="#38710617">next</a><span>|</span><label class="collapse" for="c-38717895">[-]</label><label class="expand" for="c-38717895">[1 more]</label></div><br/><div class="children"><div class="content">Well it&#x27;s not <i>really</i> support of llama 2 if it has to be extensively finetuned to &quot;convert&quot; the model.</div><br/></div></div></div></div></div></div><div id="38710617" class="c"><input type="checkbox" id="c-38710617" checked=""/><div class="controls bullet"><span class="by">acqq</span><span>|</span><a href="#38710319">parent</a><span>|</span><a href="#38710398">prev</a><span>|</span><a href="#38709708">next</a><span>|</span><label class="collapse" for="c-38710617">[-]</label><label class="expand" for="c-38710617">[1 more]</label></div><br/><div class="children"><div class="content">Indeed<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;SparseLLM&#x2F;ReluFalcon-40B" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;SparseLLM&#x2F;ReluFalcon-40B</a><p>&quot;We utilize PowerInfer for inference&quot;</div><br/></div></div></div></div><div id="38709708" class="c"><input type="checkbox" id="c-38709708" checked=""/><div class="controls bullet"><span class="by">127</span><span>|</span><a href="#38710319">prev</a><span>|</span><a href="#38709591">next</a><span>|</span><label class="collapse" for="c-38709708">[-]</label><label class="expand" for="c-38709708">[10 more]</label></div><br/><div class="children"><div class="content">Running uncensored Mixtral on this would be really nice. More than 3 bits quantized for 4090.</div><br/><div id="38709870" class="c"><input type="checkbox" id="c-38709870" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38709708">parent</a><span>|</span><a href="#38711451">next</a><span>|</span><label class="collapse" for="c-38709870">[-]</label><label class="expand" for="c-38709870">[4 more]</label></div><br/><div class="children"><div class="content">Downvoters care to comment? Uncensored llm versions typically perform better (at least on benchmarks) to their &quot;lobotomized&quot; or aligned counterparts</div><br/><div id="38710901" class="c"><input type="checkbox" id="c-38710901" checked=""/><div class="controls bullet"><span class="by">infotainment</span><span>|</span><a href="#38709708">root</a><span>|</span><a href="#38709870">parent</a><span>|</span><a href="#38711451">next</a><span>|</span><label class="collapse" for="c-38710901">[-]</label><label class="expand" for="c-38710901">[3 more]</label></div><br/><div class="children"><div class="content">Probably because the parent comment didn&#x27;t contain much of substance. &quot;Oh, I&#x27;d love to see this with [insert my favorite model here]&quot; doesn&#x27;t really add a lot to the discussion.<p>For example, the parent commenter could have talked about the specific attributes of that model that make it superior. I personally am aware that Mixtral is one of the best performing models right now, but is everyone else? Also, does Mixtral need to be uncensored? I&#x27;ve used vanilla Mistral for some...interesting...prompts and had no issues with it moralizing at me.</div><br/><div id="38717624" class="c"><input type="checkbox" id="c-38717624" checked=""/><div class="controls bullet"><span class="by">lannisterstark</span><span>|</span><a href="#38709708">root</a><span>|</span><a href="#38710901">parent</a><span>|</span><a href="#38713684">next</a><span>|</span><label class="collapse" for="c-38717624">[-]</label><label class="expand" for="c-38717624">[1 more]</label></div><br/><div class="children"><div class="content">I mean, does it need to? Not every comment has to be plethora of hidden information. Sometimes people are just excited.</div><br/></div></div></div></div></div></div><div id="38711451" class="c"><input type="checkbox" id="c-38711451" checked=""/><div class="controls bullet"><span class="by">legel</span><span>|</span><a href="#38709708">parent</a><span>|</span><a href="#38709870">prev</a><span>|</span><a href="#38710044">next</a><span>|</span><label class="collapse" for="c-38711451">[-]</label><label class="expand" for="c-38711451">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, so they demo a bigger model on an RTX 4090 with 24 GB VRAM. Granted an implementation of sparse activations with the Mixture of Experts could be non-trivial, I think it’s a brilliant move, that could potentially allow for even, e.g., CPU only processing and&#x2F;or much cheaper GPU processing… Mixtral technically already has neural network controlled sparse activations, but like the Inception meme says: we must go deeper…</div><br/></div></div><div id="38710044" class="c"><input type="checkbox" id="c-38710044" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#38709708">parent</a><span>|</span><a href="#38711451">prev</a><span>|</span><a href="#38716277">next</a><span>|</span><label class="collapse" for="c-38710044">[-]</label><label class="expand" for="c-38710044">[3 more]</label></div><br/><div class="children"><div class="content">Dual GPUs should be considered normal&#x2F;consumer grade setup, hopefully they&#x27;ll add it soon, on 4bits it&#x27;s enough with plenty of space for context.<p>This whole thing is a fork of llamacpp, also hoping it&#x27;ll all go upstream sooner or later.</div><br/><div id="38716993" class="c"><input type="checkbox" id="c-38716993" checked=""/><div class="controls bullet"><span class="by">8n4vidtmkvmk</span><span>|</span><a href="#38709708">root</a><span>|</span><a href="#38710044">parent</a><span>|</span><a href="#38716277">next</a><span>|</span><label class="collapse" for="c-38716993">[-]</label><label class="expand" for="c-38716993">[2 more]</label></div><br/><div class="children"><div class="content">4090s aren&#x27;t really normal either. How many people have dual GPUs? I don&#x27;t think it helps much with games last I checked so you&#x27;d only buy 2 for AI.</div><br/><div id="38717990" class="c"><input type="checkbox" id="c-38717990" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#38709708">root</a><span>|</span><a href="#38716993">parent</a><span>|</span><a href="#38716277">next</a><span>|</span><label class="collapse" for="c-38717990">[-]</label><label class="expand" for="c-38717990">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more about what&#x27;s possible to build. Dual 4090 or 3090 is possible to setup without hassle. Beyond that not really because it&#x27;d be above home power socket rating, not possible to fit on the board and case etc.<p>It&#x27;s true you can also build dual A6000 with 48+48 = 96GB VRAM also, but that&#x27;s $10k+ setup just for GPUs on legacy generation.</div><br/></div></div></div></div></div></div><div id="38716277" class="c"><input type="checkbox" id="c-38716277" checked=""/><div class="controls bullet"><span class="by">llamaInSouth</span><span>|</span><a href="#38709708">parent</a><span>|</span><a href="#38710044">prev</a><span>|</span><a href="#38709591">next</a><span>|</span><label class="collapse" for="c-38716277">[-]</label><label class="expand" for="c-38716277">[1 more]</label></div><br/><div class="children"><div class="content">looking good <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=q2KpPUOsBCs" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=q2KpPUOsBCs</a></div><br/></div></div></div></div><div id="38709591" class="c"><input type="checkbox" id="c-38709591" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38709708">prev</a><span>|</span><a href="#38715025">next</a><span>|</span><label class="collapse" for="c-38709591">[-]</label><label class="expand" for="c-38709591">[3 more]</label></div><br/><div class="children"><div class="content">This is super cool.<p>For all the love llama.cpp gets, its method of dGPU offloading (prompt processing on GPU and then just splitting the model down the middle) is relatively simple. But its interesting that there even <i>is</i> so much &quot;activation sparsity&quot; to take advantage of. The traditional thinking in ML is that memory access is very random.<p>Hopefully the &quot;cold&quot; neurons eventually get offloaded to the IGP instead?<p>Also, its curious that they are considering a Metal kernel. I thought the performance advantage came from the hybrid memory pool... seems like that would only help old AMD Macs, unless I am missing something?</div><br/><div id="38711185" class="c"><input type="checkbox" id="c-38711185" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38709591">parent</a><span>|</span><a href="#38715025">next</a><span>|</span><label class="collapse" for="c-38711185">[-]</label><label class="expand" for="c-38711185">[2 more]</label></div><br/><div class="children"><div class="content">The only thing I could think of on the question of Apple Silicon and Metal is that they think they could still split out the cold neurons to the CPU&#x2F;Accelerate and the hot ones on the GPU and utilize both. The speedup is likely less if there is already no copying of data between GPU&#x2F;CPU and using the unified memory. Still, it would be great if you could use even more of the capabilities of the chip simultaneously. In order to avoid thermal throttling they should use the efficiency cores only (I think this is what game mode does).</div><br/><div id="38718350" class="c"><input type="checkbox" id="c-38718350" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38709591">root</a><span>|</span><a href="#38711185">parent</a><span>|</span><a href="#38715025">next</a><span>|</span><label class="collapse" for="c-38718350">[-]</label><label class="expand" for="c-38718350">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t make much sense to me. The GPU&#x27;s task energy is so much lower than even the e cores, and AFIAK the GPU&#x27;s compute isn&#x27;t even fully utilized for local inference.</div><br/></div></div></div></div></div></div><div id="38715025" class="c"><input type="checkbox" id="c-38715025" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38709591">prev</a><span>|</span><a href="#38710208">next</a><span>|</span><label class="collapse" for="c-38715025">[-]</label><label class="expand" for="c-38715025">[4 more]</label></div><br/><div class="children"><div class="content">Since they mentioned they’re working on Mistral-7B, I’d like to note that my GPU-only implementation of Mistral uses slightly over 5GB of VRAM: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml</a><p>Runs pretty good on most consumer-grade GPUs, but so far it only supports Windows OS.</div><br/><div id="38717332" class="c"><input type="checkbox" id="c-38717332" checked=""/><div class="controls bullet"><span class="by">m1sta_</span><span>|</span><a href="#38715025">parent</a><span>|</span><a href="#38717822">next</a><span>|</span><label class="collapse" for="c-38717332">[-]</label><label class="expand" for="c-38717332">[2 more]</label></div><br/><div class="children"><div class="content">This looks really really interesting. Any idea whether it would run on a laptop with an Intel Core i7?</div><br/><div id="38718070" class="c"><input type="checkbox" id="c-38718070" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#38715025">root</a><span>|</span><a href="#38717332">parent</a><span>|</span><a href="#38717822">next</a><span>|</span><label class="collapse" for="c-38718070">[-]</label><label class="expand" for="c-38718070">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I run it on CPU using LLMStudio. It&#x27;s very fast.</div><br/></div></div></div></div><div id="38717822" class="c"><input type="checkbox" id="c-38717822" checked=""/><div class="controls bullet"><span class="by">v3ss0n</span><span>|</span><a href="#38715025">parent</a><span>|</span><a href="#38717332">prev</a><span>|</span><a href="#38710208">next</a><span>|</span><label class="collapse" for="c-38717822">[-]</label><label class="expand" for="c-38717822">[1 more]</label></div><br/><div class="children"><div class="content">try ollama , only needs about 4GB it uses llmcpp</div><br/></div></div></div></div><div id="38710208" class="c"><input type="checkbox" id="c-38710208" checked=""/><div class="controls bullet"><span class="by">jupp0r</span><span>|</span><a href="#38715025">prev</a><span>|</span><a href="#38709612">next</a><span>|</span><label class="collapse" for="c-38710208">[-]</label><label class="expand" for="c-38710208">[4 more]</label></div><br/><div class="children"><div class="content">From my understanding in this implementation there is some amount of knowledge about the model itself needed to determine what parts to place in system memory vs what parts to place in GPU memory. Can this ideally be computed automatically or will future models have some sort of interface for placement algorithms like this to help automate this? If the algorithm needs to be adopted for each model architecture, it&#x27;s going to be a lot of work to maintain this project.</div><br/><div id="38710492" class="c"><input type="checkbox" id="c-38710492" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#38710208">parent</a><span>|</span><a href="#38709612">next</a><span>|</span><label class="collapse" for="c-38710492">[-]</label><label class="expand" for="c-38710492">[3 more]</label></div><br/><div class="children"><div class="content">That sounds about right.  They provide a script to combine their &quot;Predictor&quot; weights to the original models, but I don&#x27;t see anything obvious in the front page of the Github repo about how to create those weights.<p>A 10x speed improvement is really impressive.  If this kind of improvement is reproducible across other models, then presumably identifying hot and cold neurons for inference optimization should go on to become a normal part of model development process.</div><br/><div id="38712938" class="c"><input type="checkbox" id="c-38712938" checked=""/><div class="controls bullet"><span class="by">thelastparadise</span><span>|</span><a href="#38710208">root</a><span>|</span><a href="#38710492">parent</a><span>|</span><a href="#38709612">next</a><span>|</span><label class="collapse" for="c-38712938">[-]</label><label class="expand" for="c-38712938">[2 more]</label></div><br/><div class="children"><div class="content">Like JVM &quot;hot spots,&quot; or JIT optimization.</div><br/><div id="38713645" class="c"><input type="checkbox" id="c-38713645" checked=""/><div class="controls bullet"><span class="by">jupp0r</span><span>|</span><a href="#38710208">root</a><span>|</span><a href="#38712938">parent</a><span>|</span><a href="#38709612">next</a><span>|</span><label class="collapse" for="c-38713645">[-]</label><label class="expand" for="c-38713645">[1 more]</label></div><br/><div class="children"><div class="content">Or profile guided optimization.</div><br/></div></div></div></div></div></div></div></div><div id="38709612" class="c"><input type="checkbox" id="c-38709612" checked=""/><div class="controls bullet"><span class="by">EwanG</span><span>|</span><a href="#38710208">prev</a><span>|</span><a href="#38710561">next</a><span>|</span><label class="collapse" for="c-38709612">[-]</label><label class="expand" for="c-38709612">[2 more]</label></div><br/><div class="children"><div class="content">The important stuff from the readme (if you&#x27;re not looking to tinker with it directly):<p>We have tested PowerInfer on the following platforms:<p>x86-64 CPU (with AVX2 instructions) on Linux<p>x86-64 CPU and NVIDIA GPU on Linux<p>Apple M Chips on macOS (As we do not optimize for Mac, the performance improvement is not significant now.)<p>And new features coming soon:<p>Mistral-7B model<p>Metal backend for sparse inference on macOS</div><br/><div id="38714641" class="c"><input type="checkbox" id="c-38714641" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#38709612">parent</a><span>|</span><a href="#38710561">next</a><span>|</span><label class="collapse" for="c-38714641">[-]</label><label class="expand" for="c-38714641">[1 more]</label></div><br/><div class="children"><div class="content">Also worth mentioning the downloadable llama2 models, and the convert.py file.</div><br/></div></div></div></div><div id="38710561" class="c"><input type="checkbox" id="c-38710561" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38709612">prev</a><span>|</span><a href="#38716860">next</a><span>|</span><label class="collapse" for="c-38710561">[-]</label><label class="expand" for="c-38710561">[10 more]</label></div><br/><div class="children"><div class="content">Everyone compares against llama.cpp because it&#x27;s easy mode. Llama.cpp is slow! Everyone should know this. They should compare against exllamav2 or other optimized implementations.</div><br/><div id="38712847" class="c"><input type="checkbox" id="c-38712847" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#38710561">parent</a><span>|</span><a href="#38710924">next</a><span>|</span><label class="collapse" for="c-38712847">[-]</label><label class="expand" for="c-38712847">[1 more]</label></div><br/><div class="children"><div class="content">Yeah but exllama doesn&#x27;t do grammars so I&#x27;m stuck with llama.cpp<p>Also apparently exllama has a few side effects in coherence <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;17w57eu&#x2F;llm_format_comparisonbenchmark_70b_gguf_vs_exl2&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;17w57eu&#x2F;llm_for...</a></div><br/></div></div><div id="38710924" class="c"><input type="checkbox" id="c-38710924" checked=""/><div class="controls bullet"><span class="by">nulld3v</span><span>|</span><a href="#38710561">parent</a><span>|</span><a href="#38712847">prev</a><span>|</span><a href="#38713932">next</a><span>|</span><label class="collapse" for="c-38710924">[-]</label><label class="expand" for="c-38710924">[2 more]</label></div><br/><div class="children"><div class="content">ExLlama is GPU only right? This speedup is for GPU + CPU split use cases.</div><br/><div id="38711069" class="c"><input type="checkbox" id="c-38711069" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38710561">root</a><span>|</span><a href="#38710924">parent</a><span>|</span><a href="#38713932">next</a><span>|</span><label class="collapse" for="c-38711069">[-]</label><label class="expand" for="c-38711069">[1 more]</label></div><br/><div class="children"><div class="content">Oh I see, they are running a 40B model unquantized, whereas exllamav2 would have to use 4-bit quantization to fit. Given the quality of 4-bit quantization these days and the speed boost it provides I question the utility of running unquantized for serving purposes.<p>I see they have a 4-bit benchmark lower down in the page. That&#x27;s where they ought to compare against exllamav2.</div><br/></div></div></div></div><div id="38713932" class="c"><input type="checkbox" id="c-38713932" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#38710561">parent</a><span>|</span><a href="#38710924">prev</a><span>|</span><a href="#38711286">next</a><span>|</span><label class="collapse" for="c-38713932">[-]</label><label class="expand" for="c-38713932">[2 more]</label></div><br/><div class="children"><div class="content">In this case they&#x27;re comparing against llama.cpp because the code is literally a modification of llama.cpp. I&#x27;m not talking about using the ggml lib for matrix calculations, it&#x27;s literally using the llama.cpp main.cpp and other normal llama.cpp code. It&#x27;s a fork. It is <i>directly</i> comparable.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;4543">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;4543</a>  [Review] Merge PowerInfer with llama.cpp mainline #4543<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;discussions&#x2F;4534#discussioncomment-7900305">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;discussions&#x2F;4534#disc...</a> &quot;The x11 speedup is kind of cherrypicked because the llama.cpp GPU code for Falcon 40b is just not well-optimized.&quot;</div><br/><div id="38714905" class="c"><input type="checkbox" id="c-38714905" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38710561">root</a><span>|</span><a href="#38713932">parent</a><span>|</span><a href="#38711286">next</a><span>|</span><label class="collapse" for="c-38714905">[-]</label><label class="expand" for="c-38714905">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for pointing that out, I didn&#x27;t notice that. That makes sense.<p>I still think a comparison with exllamav2 or other optimized inference library would make sense too.</div><br/></div></div></div></div><div id="38711286" class="c"><input type="checkbox" id="c-38711286" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38710561">parent</a><span>|</span><a href="#38713932">prev</a><span>|</span><a href="#38716860">next</a><span>|</span><label class="collapse" for="c-38711286">[-]</label><label class="expand" for="c-38711286">[4 more]</label></div><br/><div class="children"><div class="content">What do you recommend that is faster that I can package into an app for distribution?</div><br/><div id="38711403" class="c"><input type="checkbox" id="c-38711403" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38710561">root</a><span>|</span><a href="#38711286">parent</a><span>|</span><a href="#38716860">next</a><span>|</span><label class="collapse" for="c-38711403">[-]</label><label class="expand" for="c-38711403">[3 more]</label></div><br/><div class="children"><div class="content">I have packaged exllamav2 (plus a lot of other stuff) into an app for distribution here: <a href="https:&#x2F;&#x2F;apps.microsoft.com&#x2F;detail&#x2F;9NC624PBFGB7" rel="nofollow noreferrer">https:&#x2F;&#x2F;apps.microsoft.com&#x2F;detail&#x2F;9NC624PBFGB7</a><p>I used pyinstaller. It was difficult because Python makes these things difficult. But it works. It does require an Nvidia GPU. MLC-LLM is another option that might be easier to package and potentially able to run on AMD.</div><br/><div id="38711594" class="c"><input type="checkbox" id="c-38711594" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38710561">root</a><span>|</span><a href="#38711403">parent</a><span>|</span><a href="#38716860">next</a><span>|</span><label class="collapse" for="c-38711594">[-]</label><label class="expand" for="c-38711594">[2 more]</label></div><br/><div class="children"><div class="content">Oh yeah, I want to work on AMD&#x2F;Intel&#x2F;NVIDIA and MacOS, even iOS&#x2F;Android.<p>I&#x27;ve been following MLC-LLM as well. Right now I am just using JS&#x2F;WASM from Huggingface, but later I will want something more performant.</div><br/><div id="38712130" class="c"><input type="checkbox" id="c-38712130" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38710561">root</a><span>|</span><a href="#38711594">parent</a><span>|</span><a href="#38716860">next</a><span>|</span><label class="collapse" for="c-38712130">[-]</label><label class="expand" for="c-38712130">[1 more]</label></div><br/><div class="children"><div class="content">Yeah if you want maximum performance on multiple platforms you&#x27;ll probably have to package multiple frameworks. Llama.cpp might be a decently fast option on Apple Silicon, I&#x27;m not sure of the state of the art there.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38716860" class="c"><input type="checkbox" id="c-38716860" checked=""/><div class="controls bullet"><span class="by">robwwilliams</span><span>|</span><a href="#38710561">prev</a><span>|</span><a href="#38713101">next</a><span>|</span><label class="collapse" for="c-38716860">[-]</label><label class="expand" for="c-38716860">[1 more]</label></div><br/><div class="children"><div class="content">Scale-free network topology enables a crude but effective split of neurons into hot and cold classes—hot neurons at home on the GPU and larger numbers of cold neurons that benefit from more memory on the CPU. Clever!</div><br/></div></div><div id="38713101" class="c"><input type="checkbox" id="c-38713101" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#38716860">prev</a><span>|</span><a href="#38710326">next</a><span>|</span><label class="collapse" for="c-38713101">[-]</label><label class="expand" for="c-38713101">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Hybrid CPU&#x2F;GPU Utilization: Seamlessly integrates memory&#x2F;computation capabilities of CPU and GPU for a balanced workload and faster processing.<p>Does this means that it runs at same time at both CPU and GPU, being faster than a CPU-only or a GPU-only implementation on the same device?<p>edit: when running on integrated GPUs, can this benefit from the improved communication between CPU and GPU?</div><br/><div id="38714633" class="c"><input type="checkbox" id="c-38714633" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#38713101">parent</a><span>|</span><a href="#38710326">next</a><span>|</span><label class="collapse" for="c-38714633">[-]</label><label class="expand" for="c-38714633">[1 more]</label></div><br/><div class="children"><div class="content">GPU-only will be faster if you have enough VRAM.<p>But if you want to run a model that requires more VRAM than you have, the current approach is to use llama.cpp and specify n_gpu_layers. That works, but is slower than GPU-only.<p>OP claims to be 10x as fast as llama.cpp in the case when you can&#x27;t fit the whole model in VRAM.</div><br/></div></div></div></div><div id="38710326" class="c"><input type="checkbox" id="c-38710326" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#38713101">prev</a><span>|</span><a href="#38711155">next</a><span>|</span><label class="collapse" for="c-38710326">[-]</label><label class="expand" for="c-38710326">[1 more]</label></div><br/><div class="children"><div class="content">It’s not too much faster than exllama2 with flash attention, no?</div><br/></div></div><div id="38709747" class="c"><input type="checkbox" id="c-38709747" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38711155">prev</a><span>|</span><a href="#38713675">next</a><span>|</span><label class="collapse" for="c-38709747">[-]</label><label class="expand" for="c-38709747">[4 more]</label></div><br/><div class="children"><div class="content">how much speed increase do we get on CPU only configurations? has anyone tested it in such cases?</div><br/><div id="38711042" class="c"><input type="checkbox" id="c-38711042" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#38709747">parent</a><span>|</span><a href="#38710333">next</a><span>|</span><label class="collapse" for="c-38711042">[-]</label><label class="expand" for="c-38711042">[2 more]</label></div><br/><div class="children"><div class="content">CPU-only is impractical for most use cases and this will only become more true over time as models become larger. The mediocre perf&#x2F;$ and perf&#x2F;watt makes it not worth the effort</div><br/><div id="38716402" class="c"><input type="checkbox" id="c-38716402" checked=""/><div class="controls bullet"><span class="by">hobobaggins</span><span>|</span><a href="#38709747">root</a><span>|</span><a href="#38711042">parent</a><span>|</span><a href="#38710333">next</a><span>|</span><label class="collapse" for="c-38716402">[-]</label><label class="expand" for="c-38716402">[1 more]</label></div><br/><div class="children"><div class="content">Might be worth it in a datacenter, especially if it&#x27;s operating other servers alongside (perhaps I&#x2F;O bound web serving or something); perf&#x2F;$ does matter, definitely, but the state of the art is moving quickly (getting faster&#x2F;more efficient) and optimizing some models for CPU is still relevant IMO.</div><br/></div></div></div></div><div id="38710333" class="c"><input type="checkbox" id="c-38710333" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#38709747">parent</a><span>|</span><a href="#38711042">prev</a><span>|</span><a href="#38713675">next</a><span>|</span><label class="collapse" for="c-38710333">[-]</label><label class="expand" for="c-38710333">[1 more]</label></div><br/><div class="children"><div class="content">This architecture is specifically aimed at optimizing GPU use.</div><br/></div></div></div></div><div id="38713675" class="c"><input type="checkbox" id="c-38713675" checked=""/><div class="controls bullet"><span class="by">causality0</span><span>|</span><a href="#38709747">prev</a><span>|</span><a href="#38709812">next</a><span>|</span><label class="collapse" for="c-38713675">[-]</label><label class="expand" for="c-38713675">[1 more]</label></div><br/><div class="children"><div class="content">All the &quot;consumer grade GPUs&quot; terminology makes it seem like you could run it on a variety of models, but like <i>so many</i> of these posts, is this a 4090 exclusive?</div><br/></div></div><div id="38713832" class="c"><input type="checkbox" id="c-38713832" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#38709812">prev</a><span>|</span><a href="#38711296">next</a><span>|</span><label class="collapse" for="c-38713832">[-]</label><label class="expand" for="c-38713832">[1 more]</label></div><br/><div class="children"><div class="content">This will be really cool once there&#x27;s the ability to generate the sparse predictor files for arbitrary models rather than just the 4 they&#x27;ve done it with. Looking through the page and code it doesn&#x27;t seem like the tools to do that step are included. Guess I&#x27;ll wait on this one a bit. Hopefully these features will be merged back into llama.cpp as options eventually since this is based on the normal llama.cpp code (ie, not just using the ggml matrix lib).</div><br/></div></div><div id="38709596" class="c"><input type="checkbox" id="c-38709596" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38711296">prev</a><span>|</span><label class="collapse" for="c-38709596">[-]</label><label class="expand" for="c-38709596">[4 more]</label></div><br/><div class="children"><div class="content">&quot;Power*&quot; made me think of Microsoft, so I was almost expecting this to be Windows-specific. (PowerShell, PowerPoint, Power BI, Power Apps, Power Automate... I&#x27;m probably forgetting some.)</div><br/><div id="38709815" class="c"><input type="checkbox" id="c-38709815" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#38709596">parent</a><span>|</span><a href="#38710912">next</a><span>|</span><label class="collapse" for="c-38709815">[-]</label><label class="expand" for="c-38709815">[2 more]</label></div><br/><div class="children"><div class="content">PowerToys are probably the original (going back to PowerToys for Windows 95)<p>Edit: <a href="https:&#x2F;&#x2F;socket3.wordpress.com&#x2F;2016&#x2F;10&#x2F;22&#x2F;using-windows-95-powertoys&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;socket3.wordpress.com&#x2F;2016&#x2F;10&#x2F;22&#x2F;using-windows-95-po...</a></div><br/><div id="38709882" class="c"><input type="checkbox" id="c-38709882" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38709596">root</a><span>|</span><a href="#38709815">parent</a><span>|</span><a href="#38710912">next</a><span>|</span><label class="collapse" for="c-38709882">[-]</label><label class="expand" for="c-38709882">[1 more]</label></div><br/><div class="children"><div class="content">PowerPoint existed in the late 80s, I think, although Microsoft acquired it from what I understand.</div><br/></div></div></div></div><div id="38710912" class="c"><input type="checkbox" id="c-38710912" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38709596">parent</a><span>|</span><a href="#38709815">prev</a><span>|</span><label class="collapse" for="c-38710912">[-]</label><label class="expand" for="c-38710912">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;PowerPC" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;PowerPC</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>