<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1732525268515" as="style"/><link rel="stylesheet" href="styles.css?v=1732525268515"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/huggingface/smollm">Full LLM training and evaluation toolkit</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>testerui</span> | <span>6 comments</span></div><br/><div><div id="42229071" class="c"><input type="checkbox" id="c-42229071" checked=""/><div class="controls bullet"><span class="by">timhigins</span><span>|</span><a href="#42229132">next</a><span>|</span><label class="collapse" for="c-42229071">[-]</label><label class="expand" for="c-42229071">[3 more]</label></div><br/><div class="children"><div class="content">Might be worth updating the title to &quot;SmolLM: state-of-the-art small language model trained on open datasets&quot; (See the first table of <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;smollm" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;smollm</a> for benchmarks)<p>It was fascinating digging into this to find their dataset weights defined in a declarative YAML file [2]. 70% is from FineWeb&#x2F;Commoncrawl but filtered using a classifier trained on Llama-70b&#x27;s rating from 0-5 of the educational content of the text [3]. This is something we know small models like Phi-3 have been doing for a while, but it&#x27;s great to see a fully open reproduction of it that beats their benchmarks. Definitely supports the idea you can get even better reasoning at smaller model sizes by carefully filtering and curating your training data (and generating good synthetic data from&#x2F;distilling bigger models).<p>You can see the 450k Llama educational value scores here:
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;HuggingFaceFW&#x2F;fineweb-edu-llama3-annotations&#x2F;viewer&#x2F;default&#x2F;train?f[score][min]=3&amp;f[score][max]=4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;HuggingFaceFW&#x2F;fineweb-edu-ll...</a>
It&#x27;s interesting, I think the text with 3 scores is really good, but the 5 scores pick content that is not very reasoning or information-heavy but just mentions education or a worksheet. For SmolLM they just took the documents with scores &gt;= 3 so it doesn&#x27;t matter a ton.<p>2. <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;smollm&#x2F;blob&#x2F;9efce803bc7e3772704b3599922f7098abc2d53d&#x2F;pre-training&#x2F;smollm1&#x2F;config_smollm1_1B.yaml#L13-L17">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;smollm&#x2F;blob&#x2F;9efce803bc7e37727...</a>
3. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;HuggingFaceFW&#x2F;fineweb-edu-classifier" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;HuggingFaceFW&#x2F;fineweb-edu-classifier</a></div><br/><div id="42229130" class="c"><input type="checkbox" id="c-42229130" checked=""/><div class="controls bullet"><span class="by">timhigins</span><span>|</span><a href="#42229071">parent</a><span>|</span><a href="#42230200">next</a><span>|</span><label class="collapse" for="c-42229130">[-]</label><label class="expand" for="c-42229130">[1 more]</label></div><br/><div class="children"><div class="content">Update: While SmolLM was SOTA at the time of release in July, SmolLM 2 1.7B (which is the newest release) is not currently the best model under 2B params on <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_...</a></div><br/></div></div></div></div><div id="42229132" class="c"><input type="checkbox" id="c-42229132" checked=""/><div class="controls bullet"><span class="by">abeppu</span><span>|</span><a href="#42229071">prev</a><span>|</span><a href="#42230143">next</a><span>|</span><label class="collapse" for="c-42229132">[-]</label><label class="expand" for="c-42229132">[1 more]</label></div><br/><div class="children"><div class="content">While it&#x27;s great that this is open source, and I understand the pressure for smaller models that can be run in a wider range of contexts, I continue to be annoyed that authors keep posting comparisons to models which are slightly smaller.<p>In this page, SmolLM2-1.7B does a bit better than Qwen2.5-1.5B which is ahead of Llama3.2-1B. At the next size level up, in other comparisons I&#x27;ve seen that e.g. Phi-3.5 (which is ~3.8B params) does a bit better than Llama 3.2 3B. Gemma 2 has a 9B size, llama 3.1 has an 8B size and I think when that came out Mistral had a 7B model -- so whenever a new &quot;small&quot; thing does &quot;better&quot; than its peers, we can&#x27;t easily see whether it&#x27;s because of any of the many small choices that the authors made were actually better.</div><br/></div></div><div id="42230143" class="c"><input type="checkbox" id="c-42230143" checked=""/><div class="controls bullet"><span class="by">bashfulpup</span><span>|</span><a href="#42229132">prev</a><span>|</span><label class="collapse" for="c-42230143">[-]</label><label class="expand" for="c-42230143">[1 more]</label></div><br/><div class="children"><div class="content">Pythia is stupidly easy to use.<p>Then hookup a simple test harness.
- this is like a grand total of 3 commands
- git pull, install, point and run a model</div><br/></div></div></div></div></div></div></div></body></html>