<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724576472774" as="style"/><link rel="stylesheet" href="styles.css?v=1724576472774"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="http://muratbuffalo.blogspot.com/2024/08/looming-liability-machines.html">Looming Liability Machines (LLMs)</a> <span class="domain">(<a href="http://muratbuffalo.blogspot.com">muratbuffalo.blogspot.com</a>)</span></div><div class="subtext"><span>zdw</span> | <span>67 comments</span></div><br/><div><div id="41343323" class="c"><input type="checkbox" id="c-41343323" checked=""/><div class="controls bullet"><span class="by">efitz</span><span>|</span><a href="#41343539">next</a><span>|</span><label class="collapse" for="c-41343323">[-]</label><label class="expand" for="c-41343323">[28 more]</label></div><br/><div class="children"><div class="content">Why should I think that LLMs would be good at the task of analyzing a cloud incident and determining root cause?<p>LLMs are good at predicting the next word in written language. They are generative; they make new text given a prompt.  LLMs do not have base sets of facts about how complex systems work, and do not attempt to reason over a corpus of evidence and facts.  as a result, I would expect that an LLM might concoct an interesting story about why such a failure occurred, and it might even be a convincing story if it happened to weave bits of context, accurately into the storyline. It might even, purely randomly, generate a story that actually correctly diagnosed the root cause of the failure, but that would be coincidental based on the similarity of the prompt to text of similar postmortem discussions that were part of its training set.<p>If you had an extremely detailed postmortem document, then I would expect LLM‘s to do a very good job of summarizing such document.<p>But I don’t see why an LLM is an appropriate tool for analyzing failures in complex systems; just as I don’t see a hammer being a very effective tool for tightening bolts.<p>Right now, I am concerned that the relative ease that modern frameworks provide to author LLM based applications, is leading many people to optimistically include LLM technology in attempts to solve problems that it doesn’t seem particularly well suited to solve.</div><br/><div id="41343561" class="c"><input type="checkbox" id="c-41343561" checked=""/><div class="controls bullet"><span class="by">from-nibly</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41345415">next</a><span>|</span><label class="collapse" for="c-41343561">[-]</label><label class="expand" for="c-41343561">[6 more]</label></div><br/><div class="children"><div class="content">The main problem is. Most people cant tell the difference between an expert opinion and random words. So if an LLM shoots a bunch of jargon on the screen then, well so does my senior platform engineer so whats the difference? The LLM is cheaper and is always on call.</div><br/><div id="41343698" class="c"><input type="checkbox" id="c-41343698" checked=""/><div class="controls bullet"><span class="by">RodgerTheGreat</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41343561">parent</a><span>|</span><a href="#41344166">next</a><span>|</span><label class="collapse" for="c-41343698">[-]</label><label class="expand" for="c-41343698">[4 more]</label></div><br/><div class="children"><div class="content">I see deeply ominous connections between generations of American children who have been educated in a manner which produces alarming rates of functional illiteracy[0] and the widespread popularity of machine learning models that produce nonsense which resembles plausible text if you skim or don&#x27;t actually read it.<p>[0] <a href="https:&#x2F;&#x2F;www.apmreports.org&#x2F;episode&#x2F;2019&#x2F;08&#x2F;22&#x2F;whats-wrong-how-schools-teach-reading" rel="nofollow">https:&#x2F;&#x2F;www.apmreports.org&#x2F;episode&#x2F;2019&#x2F;08&#x2F;22&#x2F;whats-wrong-ho...</a></div><br/><div id="41344568" class="c"><input type="checkbox" id="c-41344568" checked=""/><div class="controls bullet"><span class="by">treyd</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41343698">parent</a><span>|</span><a href="#41344621">next</a><span>|</span><label class="collapse" for="c-41344568">[-]</label><label class="expand" for="c-41344568">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The theory is known as &quot;three cueing.&quot; The name comes from the notion that readers use three different kinds of information — or &quot;cues&quot; — to identify words as they are reading.<p>&gt; The theory was first proposed in 1967, when an education professor named Ken Goodman presented a paper at the annual meeting of the American Educational Research Association in New York City.<p>&gt; In the paper,5 Goodman rejected the idea that reading is a precise process that involves exact or detailed perception of letters or words. Instead, he argued that as people read, they make predictions about the words on the page using these three cues:<p>&gt;    graphic cues (what do the letters tell you about what the word might be?)<p>&gt;    syntactic cues (what kind of word could it be, for example, a noun or a verb?)<p>&gt;    semantic cues (what word would make sense here, based on the context?)<p>This is interesting because this is fairly similar to how LLMs do next-token prediction, but using exclusively backwards-facing clues from the text (although perhaps also the graphic cues if you are talking about multimodal models).</div><br/></div></div><div id="41344621" class="c"><input type="checkbox" id="c-41344621" checked=""/><div class="controls bullet"><span class="by">philipswood</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41343698">parent</a><span>|</span><a href="#41344568">prev</a><span>|</span><a href="#41344606">next</a><span>|</span><label class="collapse" for="c-41344621">[-]</label><label class="expand" for="c-41344621">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, I think the linked article is worthy of its own submission:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41344613">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41344613</a></div><br/></div></div><div id="41344606" class="c"><input type="checkbox" id="c-41344606" checked=""/><div class="controls bullet"><span class="by">pixelatedindex</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41343698">parent</a><span>|</span><a href="#41344621">prev</a><span>|</span><a href="#41344166">next</a><span>|</span><label class="collapse" for="c-41344606">[-]</label><label class="expand" for="c-41344606">[1 more]</label></div><br/><div class="children"><div class="content">Super fascinating article, thanks for sharing!</div><br/></div></div></div></div><div id="41344166" class="c"><input type="checkbox" id="c-41344166" checked=""/><div class="controls bullet"><span class="by">sublinear</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41343561">parent</a><span>|</span><a href="#41343698">prev</a><span>|</span><a href="#41345415">next</a><span>|</span><label class="collapse" for="c-41344166">[-]</label><label class="expand" for="c-41344166">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Most people cant tell the difference<p>And those people should be fired.</div><br/></div></div></div></div><div id="41345415" class="c"><input type="checkbox" id="c-41345415" checked=""/><div class="controls bullet"><span class="by">low_tech_love</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41343561">prev</a><span>|</span><a href="#41345293">next</a><span>|</span><label class="collapse" for="c-41345415">[-]</label><label class="expand" for="c-41345415">[4 more]</label></div><br/><div class="children"><div class="content">The idea that LLMs “only know how to predict the next word” is a common but imho wrong cop out. It has been shown that they are good at doing a lot of things that might not have been immediately obvious, regardless of how their internal process works. 50 years ago, somebody might say “programming is only a way to automate simple repetitive tasks” and that would be obviously wrong.<p>The real problem in cases like this and other applications, as you and many others have mentioned, is that LLMs are basically correlation machines. They can find very complex, immensely-multivariate correlations in large data sets, and reproduce these correlations very well. But they cannot reason (so far) beyond these correlations in other to find deeper, less obvious causal relationships. They’re simply not trained to do that, yet. But it’ll come…!</div><br/><div id="41345509" class="c"><input type="checkbox" id="c-41345509" checked=""/><div class="controls bullet"><span class="by">austin-cheney</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41345415">parent</a><span>|</span><a href="#41345293">next</a><span>|</span><label class="collapse" for="c-41345509">[-]</label><label class="expand" for="c-41345509">[3 more]</label></div><br/><div class="children"><div class="content">&gt; 50 years ago, somebody might say “programming is only a way to automate simple repetitive tasks” and that would be obviously wrong.<p>That is actually extremely correct. The only purpose for software is automation, which is the elimination of labor. Getting that wrong directly influences your quality of product more than any other downstream factor.</div><br/><div id="41345581" class="c"><input type="checkbox" id="c-41345581" checked=""/><div class="controls bullet"><span class="by">low_tech_love</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41345509">parent</a><span>|</span><a href="#41345293">next</a><span>|</span><label class="collapse" for="c-41345581">[-]</label><label class="expand" for="c-41345581">[2 more]</label></div><br/><div class="children"><div class="content">It might be theoretically correct in the same way that it is correct to say that “a building is just a bunch of bricks on top of each other”. But there are thousands of different buildings with different reasons to exist which offer drastically different services and serve different purposes. A video game like Elden Ring is built from the same “automation” pieces as the LS command in my terminal, but it would very disingenuous to say they’re basically the same thing.</div><br/><div id="41345659" class="c"><input type="checkbox" id="c-41345659" checked=""/><div class="controls bullet"><span class="by">austin-cheney</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41345581">parent</a><span>|</span><a href="#41345293">next</a><span>|</span><label class="collapse" for="c-41345659">[-]</label><label class="expand" for="c-41345659">[1 more]</label></div><br/><div class="children"><div class="content">That is an incorrect comparison. A building, all buildings, are dwellings, but they are no more or less the sum of their parts than anything else. It’s not about the construction materials. It’s about the utility.</div><br/></div></div></div></div></div></div></div></div><div id="41345306" class="c"><input type="checkbox" id="c-41345306" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41345293">prev</a><span>|</span><a href="#41344878">next</a><span>|</span><label class="collapse" for="c-41345306">[-]</label><label class="expand" for="c-41345306">[2 more]</label></div><br/><div class="children"><div class="content">Did you see this?<p><a href="https:&#x2F;&#x2F;engineering.fb.com&#x2F;2024&#x2F;06&#x2F;24&#x2F;data-infrastructure&#x2F;leveraging-ai-for-efficient-incident-response&#x2F;" rel="nofollow">https:&#x2F;&#x2F;engineering.fb.com&#x2F;2024&#x2F;06&#x2F;24&#x2F;data-infrastructure&#x2F;le...</a></div><br/><div id="41345444" class="c"><input type="checkbox" id="c-41345444" checked=""/><div class="controls bullet"><span class="by">genewitch</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41345306">parent</a><span>|</span><a href="#41344878">next</a><span>|</span><label class="collapse" for="c-41345444">[-]</label><label class="expand" for="c-41345444">[1 more]</label></div><br/><div class="children"><div class="content">i get real strong &quot;Mechanical Turk&quot; vibes from all of this faang &quot;let us do the tedious stuff at your business&quot;.</div><br/></div></div></div></div><div id="41344878" class="c"><input type="checkbox" id="c-41344878" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41345306">prev</a><span>|</span><a href="#41344313">next</a><span>|</span><label class="collapse" for="c-41344878">[-]</label><label class="expand" for="c-41344878">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why should I think that LLMs would be good at the task of analyzing a cloud incident and determining root cause?<p>Because an LLM is really good at convincing laymen it knows what it&#x27;s doing. It&#x27;s really a conman simulator :)<p>It is great at language based tasks but people use it as an oracle for everything without understanding the limitations. It&#x27;s just that friendly convincing tone that makes them think they&#x27;re taking to a super intelligent being.</div><br/></div></div><div id="41344313" class="c"><input type="checkbox" id="c-41344313" checked=""/><div class="controls bullet"><span class="by">deegles</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41344878">prev</a><span>|</span><a href="#41344141">next</a><span>|</span><label class="collapse" for="c-41344313">[-]</label><label class="expand" for="c-41344313">[3 more]</label></div><br/><div class="children"><div class="content">This project had one goal: get a high visibility project out the door in order to secure a promotion.</div><br/><div id="41344327" class="c"><input type="checkbox" id="c-41344327" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41344313">parent</a><span>|</span><a href="#41344141">next</a><span>|</span><label class="collapse" for="c-41344327">[-]</label><label class="expand" for="c-41344327">[2 more]</label></div><br/><div class="children"><div class="content">What project?</div><br/><div id="41345675" class="c"><input type="checkbox" id="c-41345675" checked=""/><div class="controls bullet"><span class="by">generic92034</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41344327">parent</a><span>|</span><a href="#41344141">next</a><span>|</span><label class="collapse" for="c-41345675">[-]</label><label class="expand" for="c-41345675">[1 more]</label></div><br/><div class="children"><div class="content">See? Those are the best projects, where no-one can remember it or knows who was responsible when it is headed for its final failure. ;)</div><br/></div></div></div></div></div></div><div id="41344221" class="c"><input type="checkbox" id="c-41344221" checked=""/><div class="controls bullet"><span class="by">tomjen3</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41344141">prev</a><span>|</span><a href="#41344325">next</a><span>|</span><label class="collapse" for="c-41344221">[-]</label><label class="expand" for="c-41344221">[2 more]</label></div><br/><div class="children"><div class="content">You should not think that LLMs are a good fit for anything. You should check if they are, and then try to guide it to be better at the task.<p>You should do this because AI is a jagged frontier where humans can&#x27;t predict if AI is good at something or not.<p>&gt;AI is weird. No one actually knows the full range of capabilities of the most advanced Large Language Models, like GPT-4. No one really knows the best ways to use them, or the conditions under which they fail. There is no instruction manual. On some tasks AI is immensely powerful, and on others it fails completely or subtly. And, unless you use AI a lot, you won’t know which is which.<p><a href="https:&#x2F;&#x2F;www.oneusefulthing.org&#x2F;p&#x2F;centaurs-and-cyborgs-on-the-jagged" rel="nofollow">https:&#x2F;&#x2F;www.oneusefulthing.org&#x2F;p&#x2F;centaurs-and-cyborgs-on-the...</a></div><br/><div id="41344314" class="c"><input type="checkbox" id="c-41344314" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41344221">parent</a><span>|</span><a href="#41344325">next</a><span>|</span><label class="collapse" for="c-41344314">[-]</label><label class="expand" for="c-41344314">[1 more]</label></div><br/><div class="children"><div class="content">This is exactly how I try to approach using it.<p>I recognize that its still a robot and you have to give it stoic, stern guidance to keep it on point.<p>Also, having a good understanding for the domain youre seeking to build understanding in, but with the augmentation of an AI assist that can formulate the Output of the thought in a more complete and packaged manner than one is able to do without leveraging AI as a tool. And as someone said &quot;<i>And its not going to get any worse, its only going to get better</i>&quot; I think people are really underestimating, and under-utilizing AI tools.<p>But they are terrifying when you consider just that - a divide between those who have&#x2F;use AI and those who are subjugated by those who do.</div><br/></div></div></div></div><div id="41344325" class="c"><input type="checkbox" id="c-41344325" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41344221">prev</a><span>|</span><a href="#41344639">next</a><span>|</span><label class="collapse" for="c-41344325">[-]</label><label class="expand" for="c-41344325">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re way over-estimating the &quot;it only knows the training data&quot; stuff to a very large degree. Ex. In the last 48 hours, Facebook also released a paper of it being excellent at RCA.<p>This is a well-tred argument, and it is much stronger when it holds itself to, especially in the short term, augmentation is likely, not wholesale delegation. It gets weak  when it tries to couple that to asserting that &quot;all it does&quot; is lightly rephrasing training data. It&#x27;s somewhat trivial to demonstrate this is false, even society as a whole has noticed that it&#x27;s beyond a parrot, and it&#x27;s worrying.</div><br/></div></div><div id="41344639" class="c"><input type="checkbox" id="c-41344639" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41344325">prev</a><span>|</span><a href="#41344344">next</a><span>|</span><label class="collapse" for="c-41344639">[-]</label><label class="expand" for="c-41344639">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why should I think that LLMs would be good a<p>They make language that sounds like what thinking people make, therefore they must be thinking like people do, <i>duh</i>! &#x2F;s</div><br/></div></div><div id="41344344" class="c"><input type="checkbox" id="c-41344344" checked=""/><div class="controls bullet"><span class="by">mitjam</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41344639">prev</a><span>|</span><a href="#41344513">next</a><span>|</span><label class="collapse" for="c-41344344">[-]</label><label class="expand" for="c-41344344">[1 more]</label></div><br/><div class="children"><div class="content">A simple example: Prompt an LLM with a log message you don&#x27;t understand and see if it helps you interpret it. In my experience, in many cases, it can.</div><br/></div></div><div id="41344513" class="c"><input type="checkbox" id="c-41344513" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#41343323">parent</a><span>|</span><a href="#41344344">prev</a><span>|</span><a href="#41343539">next</a><span>|</span><label class="collapse" for="c-41344513">[-]</label><label class="expand" for="c-41344513">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Why should I think that LLMs would be good at the task of analyzing a cloud incident and determining root cause? LLMs are good at predicting the next word in written language. They are generative; they make new text given a prompt.<p>Now explain what humans are doing and why it is different. RCA can be done fully remotely, so we know it can be modelled as a word-prediction model with context and a &quot;how is all this context made consistent?&quot; prompt. There is every reason to expect that an advanced text prediction model would be excellent at predicting the most-technically-correct response to that prompt.<p>I&#x27;ve seen a few people make this argument as though text prediction is some specific sub-field that can be solved independently of having a world model and intelligence. That doesn&#x27;t hold up at all, if we solve text prediction we&#x27;ve solved general intelligence - all aspects of human intelligence are less complex than being able to predict the most objectively correct next word in a sequence of words because all aspects of intelligence can be framed as a text-prediction problem. A system can&#x27;t predict the next word in a sequence and fool a human without being at least as clever as a human.</div><br/><div id="41344673" class="c"><input type="checkbox" id="c-41344673" checked=""/><div class="controls bullet"><span class="by">crystal_revenge</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41344513">parent</a><span>|</span><a href="#41345142">next</a><span>|</span><label class="collapse" for="c-41344673">[-]</label><label class="expand" for="c-41344673">[2 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t even have to get deep into the internals of LLMs to see what&#x27;s wrong with your reasoning. The problem lies with the basic mechanics of:<p>&gt;  predict the most objectively correct next word in a sequence of words<p>Currently all LLMs are <i>only</i> determining the most probable <i>next token</i>, but this means they are not aware of the probability of the entire sequence of tokens they are emitting. That is, they can only build sentences by picking the most probable next word, but can never choose the most probable <i>sentence</i>. In practice, there are a great many very likely sentences that are composed of a fairly unlikely words. When we use the output of an LLM we&#x27;re thinking it of a sequence sampled from the set of all possible sequences, but that&#x27;s not really what we&#x27;re getting (at least as far as probability is concerned).<p>There are approaches to address this: you can do multinomial sampling instead of greedy so that are casting a slightly large net or you can do beam search where your once again trying to search a broader set of possible sentences choosing by the most probable <i>sequence</i>. But all of these are fairly limited.<p>Which gets to your first remark:<p>&gt; Now explain what humans are doing and why it is different.<p>There&#x27;s very little we really know about how humans reason, but we are certainly building linguistic expressions at with a more abstract form of composition. This comment for example was planned out in parts, not even sequentially, and then reworked to the whole thing makes <i>some</i> sense. But at the very least humans are clearly reasoning at the level of entire sequences as their probability rather than individual tokens at a time.<p>The word &quot;planning&quot; almost tautologically implies <i>thinking ahead of the next step</i>. When humans write HN comments or code they&#x27;re clearly planning rather than just thinking of the next most likely word over and over again with some noise to make it sound more interesting. No matter how powerful and sophisticated the mathematical models driving the core of LLMs are, we&#x27;re fundamentally limited by the methods we use to sample from them.</div><br/><div id="41344798" class="c"><input type="checkbox" id="c-41344798" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41344673">parent</a><span>|</span><a href="#41345142">next</a><span>|</span><label class="collapse" for="c-41344798">[-]</label><label class="expand" for="c-41344798">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t hold together. You seem to be arguing that LLMs produce text as a sequence of words. Which, fair enough, they obviously do.<p>But then your argument seems to drift into humans <i>not</i> producing text as a series of words. I&#x27;m not sure how you type your comments but you should upload a YouTube video of it as it sounds like it&#x27;d be quite a spectacle!<p>If your argument is that LLMs can&#x27;t reason because they don&#x27;t edit their comments, it&#x27;d be worth stopping and reflecting for a few moments about how weak a position that is. I wrote this comment linearly just to make a point with no editing except spellchecking.</div><br/></div></div></div></div><div id="41345142" class="c"><input type="checkbox" id="c-41345142" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#41343323">root</a><span>|</span><a href="#41344513">parent</a><span>|</span><a href="#41344673">prev</a><span>|</span><a href="#41343539">next</a><span>|</span><label class="collapse" for="c-41345142">[-]</label><label class="expand" for="c-41345142">[1 more]</label></div><br/><div class="children"><div class="content">&gt;the most objectively correct next word in a sequence of words<p>Why should I believe, in the first place, that this is even a coherent concept?</div><br/></div></div></div></div></div></div><div id="41343539" class="c"><input type="checkbox" id="c-41343539" checked=""/><div class="controls bullet"><span class="by">charleslmunger</span><span>|</span><a href="#41343323">prev</a><span>|</span><a href="#41343510">next</a><span>|</span><label class="collapse" for="c-41343539">[-]</label><label class="expand" for="c-41343539">[6 more]</label></div><br/><div class="children"><div class="content">The statement about &quot;4,500 developer-years of work&quot; is insane to me. Java is one of the most backward compatible languages period - other than hashmap iteration order a while back, it&#x27;s hard to think of what could require that astronomical quantity of engineering effort to upgrade. Do they actually budget over a billion dollars to upgrade Java versions, or is this like &quot;this amazing tool, sed, saved us infinity developer years by replacing strings at one quadrillionth the cost of a $500k human editing each text file by hand&quot;</div><br/><div id="41344564" class="c"><input type="checkbox" id="c-41344564" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#41343539">parent</a><span>|</span><a href="#41343859">next</a><span>|</span><label class="collapse" for="c-41344564">[-]</label><label class="expand" for="c-41344564">[3 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;The average time to upgrade an application to Java 17 plummeted from what’s typically 50 developer-days to just a few hours.&quot;<p>*blink*<p>...why are they boasting about migrating to Java 17... in 2024?<p>...and from what versions were they migrating from? Java 17 didn&#x27;t introduce any significant breaking-changes (IME) for users on Java 16 or even the next previous LTS version, Java 11 - I don&#x27;t work at Amazon, but surely Amazon isn&#x27;t in the habit of running on unsupported JVMs?  - so assuming these Java projects were being competently maintained, then the  only work actually required to migrate to 17 is changing your `org.gradle.java.home=` path to where JDK 17 followed by running your test suite. If Amazon was using a monorepo then 1 person could do this in 5 minutes with a 1-liner awk&#x2F;sed command - whereas I expect it would likely take an AI far longer to do this, if it&#x27;s even able to make sense of an Amazon-sized monorepo - they&#x27;d also likely re-prompt it for every separate project for reliability&#x27;s sake. So after considering all that, the &quot;50 days&quot; number he gives, without any context either, is a nice shorthand to communicate his disconnection from what really goes-on inside his org... or he&#x27;s lying - and he knows he&#x27;s lying - but he also knows there won&#x27;t be any negative consequences for him as a result of his lying, so why not lie if it gives you a good story to tell for LinkedIn?<p>In conclusion: these remarks by leadership unintentionally make the company look bad, not good, once you fill-in-the-blanks to make up for what they dind&#x27;t say. (What&#x27;s next...? Big Brother increasing our chocolate ration to 20 grammes per week?)<p>-----<p>One more thing: the comment-replies to the post on LinkedIn are utterly derranged and I genuinely can&#x27;t put my sense of unease into words.</div><br/><div id="41345315" class="c"><input type="checkbox" id="c-41345315" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41343539">root</a><span>|</span><a href="#41344564">parent</a><span>|</span><a href="#41343859">next</a><span>|</span><label class="collapse" for="c-41345315">[-]</label><label class="expand" for="c-41345315">[2 more]</label></div><br/><div class="children"><div class="content">&gt; One more thing: the comment-replies to the post on LinkedIn are utterly derranged and I genuinely can&#x27;t put my sense of unease into words.<p>I don&#x27;t want to dig up my old credentials to sign in to see the entire set, but from what&#x27;s publicly visible...<p>Perhaps the sense that some comments are mostly desperate scrambling to self-promote, and a few of those also contain fawning ingratiation which they seem to think may be reciprocated?</div><br/><div id="41345664" class="c"><input type="checkbox" id="c-41345664" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#41343539">root</a><span>|</span><a href="#41345315">parent</a><span>|</span><a href="#41343859">next</a><span>|</span><label class="collapse" for="c-41345664">[-]</label><label class="expand" for="c-41345664">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t want to dig up my old credentials to sign in to see the entire set<p>A wise decision.<p>&gt; ...but from what&#x27;s publicly visible...mostly desperate scrambling to self-promote ... fawning ingratiation ... reciprocated<p>Well, yes - there were enough of those - and they were bad enough, but what threw me off was a screenful of rambling comment replies from a single person accusing Amazon of &quot;being racist&quot; against her (a white woman in the US, if her avatar is accurate) while also making references to some kind of lawsuit she was pursuing - with a <i>surprisingly restrained</i> sprinkling of emojis throughout.</div><br/></div></div></div></div></div></div><div id="41343859" class="c"><input type="checkbox" id="c-41343859" checked=""/><div class="controls bullet"><span class="by">efitz</span><span>|</span><a href="#41343539">parent</a><span>|</span><a href="#41344564">prev</a><span>|</span><a href="#41343777">next</a><span>|</span><label class="collapse" for="c-41343859">[-]</label><label class="expand" for="c-41343859">[1 more]</label></div><br/><div class="children"><div class="content">&lt;satire&#x2F;&gt;In unrelated news, Amazon announces layoff of 4500 developer positions...</div><br/></div></div></div></div><div id="41343510" class="c"><input type="checkbox" id="c-41343510" checked=""/><div class="controls bullet"><span class="by">m1keil</span><span>|</span><a href="#41343539">prev</a><span>|</span><a href="#41343468">next</a><span>|</span><label class="collapse" for="c-41343510">[-]</label><label class="expand" for="c-41343510">[5 more]</label></div><br/><div class="children"><div class="content">Folks let&#x27;s be real. While the tech industry borrows terms and procedures from mature and inherently riskier industries like Aviation, 99% of the tech companies don&#x27;t share the same risk profile.<p>This means that in most cases, these RCAs are the output of a long and over engineered incident review process that was designed to impress the higher echelon.<p>The problem is, that in a decently sized corporation, you have tens to hundreds of daily fuck ups (also known as &quot;incidents&quot;) that completely suck out the free time out of engineers that have to navigate the long game of post incident management process.<p>The utilisation of LLMs on these cases are just engineered solution to the problem of organisational bureaucracy.</div><br/><div id="41344803" class="c"><input type="checkbox" id="c-41344803" checked=""/><div class="controls bullet"><span class="by">igornadj</span><span>|</span><a href="#41343510">parent</a><span>|</span><a href="#41343712">next</a><span>|</span><label class="collapse" for="c-41344803">[-]</label><label class="expand" for="c-41344803">[1 more]</label></div><br/><div class="children"><div class="content">In my experience RCA is developer driven, pointing to structural issues in the org, which are then up to management to act on or not. For example, whistleblowers at Boeing are pointing out quality issues that are being ignored, not that there is too much paperwork like you are suggesting.<p>Post-mortems are as short as possible because technical people usually write them and have better things to do. Getting an LLM to do them will only remove this feedback channel, as it is much easier to ignore a LLM suggesting more time or money is spent on quality than it is to ignore a human.</div><br/></div></div><div id="41343712" class="c"><input type="checkbox" id="c-41343712" checked=""/><div class="controls bullet"><span class="by">aragilar</span><span>|</span><a href="#41343510">parent</a><span>|</span><a href="#41344803">prev</a><span>|</span><a href="#41343743">next</a><span>|</span><label class="collapse" for="c-41343712">[-]</label><label class="expand" for="c-41343712">[2 more]</label></div><br/><div class="children"><div class="content">Is not aviation, space-flight, etc. part of &quot;tech&quot;? While you can always over-do processes, you can also under-do them as well (I don&#x27;t think I need to list examples here, just look at the news...). Having the processes also be able to adapt to different needs and requirements is part of having good processes.</div><br/><div id="41344157" class="c"><input type="checkbox" id="c-41344157" checked=""/><div class="controls bullet"><span class="by">m1keil</span><span>|</span><a href="#41343510">root</a><span>|</span><a href="#41343712">parent</a><span>|</span><a href="#41343743">next</a><span>|</span><label class="collapse" for="c-41344157">[-]</label><label class="expand" for="c-41344157">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think symantics really matter here. I largely think of &quot;tech&quot; as companies that produce mainly virtual goods in the form of software. Most of these software is largely skins over databases. In most cases, nobody will get hurt if their product will malfunction. This is the vast majority of tech companies today. Exceptions exists.<p>I would take under process any day of the week. From my experience, adding a process is far easier than removing it.</div><br/></div></div></div></div><div id="41343743" class="c"><input type="checkbox" id="c-41343743" checked=""/><div class="controls bullet"><span class="by">droopyEyelids</span><span>|</span><a href="#41343510">parent</a><span>|</span><a href="#41343712">prev</a><span>|</span><a href="#41343468">next</a><span>|</span><label class="collapse" for="c-41343743">[-]</label><label class="expand" for="c-41343743">[1 more]</label></div><br/><div class="children"><div class="content">the trick is realizing that in a large company, the engineer time is already wasted on the goals of clueless leadership that will change before completion anyway, and then using the RCA to regain a bit of control over your roadmap (as a low level manager)</div><br/></div></div></div></div><div id="41343468" class="c"><input type="checkbox" id="c-41343468" checked=""/><div class="controls bullet"><span class="by">sickblastoise</span><span>|</span><a href="#41343510">prev</a><span>|</span><a href="#41343300">next</a><span>|</span><label class="collapse" for="c-41343468">[-]</label><label class="expand" for="c-41343468">[15 more]</label></div><br/><div class="children"><div class="content">Here’s a simple rule, based on the fact no one has shown that an llm or a compound llm system can produce an output that doesn’t need to be verified for correctness by a human across any input:<p>The rate at which llm&#x2F;llm compound systems can produce output &gt; the rate at which humans can verify the output<p>I think it follows that we should not use llms for anything critical.<p>The gunghoe adoption and hamfisting of llms into critical processes, like an AWs migration to Java 17, or root cause analysis is plainly premature, naive, and dangerous.</div><br/><div id="41343885" class="c"><input type="checkbox" id="c-41343885" checked=""/><div class="controls bullet"><span class="by">wbogusz</span><span>|</span><a href="#41343468">parent</a><span>|</span><a href="#41343479">next</a><span>|</span><label class="collapse" for="c-41343885">[-]</label><label class="expand" for="c-41343885">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Here’s a simple rule, based on the fact no one has shown that an llm or a compound llm system can produce an output that doesn’t need to be verified for correctness by a human across any input:<p>I’m still not sure why some of us are so convinced there isn’t an answer to properly verifying LLM output. In so many circumstances, having output pushed 90-95% of the way is very easily pushed to 100% by topping off with a deterministic system.<p>Do I depend on an LLM to perform 8 digit multiplication? Absolutely not, because like you say, I can’t verify the correctness that would drive the statistics of whatever answer it spits out. But why can’t I ask an LLM to write the python code to perform the same calculation and read me its output?<p>&gt; I think it follows that we should not use llms for anything critical.<p>While we are at it I think we should also institute an IQ threshold for employees to contribute to or operate around critical systems. If we can’t be sure to an absolute degree that they will not make a mistake, then there is no purpose to using them. All of their work will simply need to be double checked and verified anyway.</div><br/><div id="41344089" class="c"><input type="checkbox" id="c-41344089" checked=""/><div class="controls bullet"><span class="by">sickblastoise</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41343885">parent</a><span>|</span><a href="#41343479">next</a><span>|</span><label class="collapse" for="c-41344089">[-]</label><label class="expand" for="c-41344089">[1 more]</label></div><br/><div class="children"><div class="content">There isn’t one answer to how to do it. If you have an answer to validation for your specific use case, go for it. this is not trivial because most flashy things people want to use llms for like code generation and automated RCA’s are hard or impossible to verify without the I Need A More Intelligent Model problem.<p>2. I believe this is falsely equating what llms do with human intelligence. There is a skill threshhold for interacting with critical systems, for humans it comes down to “will they screw this up?” And the human can do it because humans are generally intelligent. The human can make good decisions to predict and handle potential failure modes because of this.</div><br/></div></div></div></div><div id="41343479" class="c"><input type="checkbox" id="c-41343479" checked=""/><div class="controls bullet"><span class="by">latentnumber</span><span>|</span><a href="#41343468">parent</a><span>|</span><a href="#41343885">prev</a><span>|</span><a href="#41343774">next</a><span>|</span><label class="collapse" for="c-41343479">[-]</label><label class="expand" for="c-41343479">[5 more]</label></div><br/><div class="children"><div class="content">Why not automate verification itself then? While not possible now, and I would probably never advocate for using LLMs in critical settings, it might be possible to build field-specific verification systems for LLMs with robustness guarantees as well.</div><br/><div id="41343518" class="c"><input type="checkbox" id="c-41343518" checked=""/><div class="controls bullet"><span class="by">RodgerTheGreat</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41343479">parent</a><span>|</span><a href="#41343599">next</a><span>|</span><label class="collapse" for="c-41343518">[-]</label><label class="expand" for="c-41343518">[2 more]</label></div><br/><div class="children"><div class="content">If the verification systems for LLMs are built out of LLMs, you haven&#x27;t addressed the problem at all, just hand-waved a homunculus that itself requires verification.<p>If the verification systems for LLMs are <i>not</i> built out of LLMs and they&#x27;re somehow more robust than LLMs at human-language problem solving and analysis, then you should be using the technology the verification system uses instead of LLMs in the first place!</div><br/><div id="41343946" class="c"><input type="checkbox" id="c-41343946" checked=""/><div class="controls bullet"><span class="by">wbogusz</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41343518">parent</a><span>|</span><a href="#41343599">next</a><span>|</span><label class="collapse" for="c-41343946">[-]</label><label class="expand" for="c-41343946">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If the verification systems for LLMs are not built out of LLMs and they&#x27;re somehow more robust than LLMs at human-language problem solving and analysis, then you should be using the technology the verification system uses instead of LLMs in the first place!<p>The issue is not in the verification system, but in putting quantifiable bounds on your answer set. If I ask an LLM to multiply large numbers together I can also very easily verify the generated answer by topping it with a deterministic function.<p>I.e. rather than hoping that an LLM can accurately multiply two 10 digit numbers, I have a much easier (and verified) solution by instead asking it to perform this calculation using python and reading me the output</div><br/></div></div></div></div><div id="41343599" class="c"><input type="checkbox" id="c-41343599" checked=""/><div class="controls bullet"><span class="by">sickblastoise</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41343479">parent</a><span>|</span><a href="#41343518">prev</a><span>|</span><a href="#41343781">next</a><span>|</span><label class="collapse" for="c-41343599">[-]</label><label class="expand" for="c-41343599">[1 more]</label></div><br/><div class="children"><div class="content">Spitballing, if you had a digital model of a commercial airplane, you could have an llm write all of the component code for the flight system, then iteratively test the digital model under all possible real world circumstances.<p>I think automating verification generally might require general intelligence, not an expert though.</div><br/></div></div></div></div><div id="41343774" class="c"><input type="checkbox" id="c-41343774" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#41343468">parent</a><span>|</span><a href="#41343479">prev</a><span>|</span><a href="#41343300">next</a><span>|</span><label class="collapse" for="c-41343774">[-]</label><label class="expand" for="c-41343774">[7 more]</label></div><br/><div class="children"><div class="content">The same is true of computers, in fact it has been mathematically proven that it is impossible to answer the general question if a computer program is correct.<p>But that hasn&#x27;t stopped the last 40 years from happening because computers made fewer mistakes than the next best alternative. The same needs to be true of LLMs.</div><br/><div id="41343837" class="c"><input type="checkbox" id="c-41343837" checked=""/><div class="controls bullet"><span class="by">mrcode007</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41343774">parent</a><span>|</span><a href="#41343300">next</a><span>|</span><label class="collapse" for="c-41343837">[-]</label><label class="expand" for="c-41343837">[6 more]</label></div><br/><div class="children"><div class="content">The theory you’re alluding to says it is impossible to create a general algorithm that decides any non-trivial property of any computer program.<p>There is nothing in the theory that prevents you creating a program that verifies a particular specific program.<p>There is an entire field dedicated to doing just that.</div><br/><div id="41343939" class="c"><input type="checkbox" id="c-41343939" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41343837">parent</a><span>|</span><a href="#41343300">next</a><span>|</span><label class="collapse" for="c-41343939">[-]</label><label class="expand" for="c-41343939">[5 more]</label></div><br/><div class="children"><div class="content">The issue is there to verify a program you need to have a spec. To generate a spec you need to solve the general problem.<p>This is what gets swept under the rug whenever formal methods are brought up.</div><br/><div id="41344460" class="c"><input type="checkbox" id="c-41344460" checked=""/><div class="controls bullet"><span class="by">mrcode007</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41343939">parent</a><span>|</span><a href="#41343300">next</a><span>|</span><label class="collapse" for="c-41344460">[-]</label><label class="expand" for="c-41344460">[4 more]</label></div><br/><div class="children"><div class="content">That is not true at all. You do not need to generate a spec. All you need to do is prove a property. This can be done in many ways.<p>For example, many things can be proven about the following program without having to solve any general problem at all:<p>echo “hello world”<p>Similarly for quick sort, merge sort, and all sort of things. The degree of formality doesn’t have to go to formal methods which are only a very small part of the whole field</div><br/><div id="41344986" class="c"><input type="checkbox" id="c-41344986" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41344460">parent</a><span>|</span><a href="#41343300">next</a><span>|</span><label class="collapse" for="c-41344986">[-]</label><label class="expand" for="c-41344986">[3 more]</label></div><br/><div class="children"><div class="content">&gt;echo “hello world”<p>Congratulations, you just launched all the worlds nuclear missiles.<p>This is to spec since you didn&#x27;t provide one and we just fed the teletype output into the &#x27;arm and launch&#x27; module of the missiles.</div><br/><div id="41345095" class="c"><input type="checkbox" id="c-41345095" checked=""/><div class="controls bullet"><span class="by">mrcode007</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41344986">parent</a><span>|</span><a href="#41343300">next</a><span>|</span><label class="collapse" for="c-41345095">[-]</label><label class="expand" for="c-41345095">[2 more]</label></div><br/><div class="children"><div class="content">What you’re saying is equivalent to throwing out all of mathematics due to the incompleteness theorem and start praying to fried egg jellyfish on full moon</div><br/><div id="41345601" class="c"><input type="checkbox" id="c-41345601" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#41343468">root</a><span>|</span><a href="#41345095">parent</a><span>|</span><a href="#41343300">next</a><span>|</span><label class="collapse" for="c-41345601">[-]</label><label class="expand" for="c-41345601">[1 more]</label></div><br/><div class="children"><div class="content">No that&#x27;s what OP is saying about LLMs.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41343300" class="c"><input type="checkbox" id="c-41343300" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#41343468">prev</a><span>|</span><a href="#41344117">next</a><span>|</span><label class="collapse" for="c-41343300">[-]</label><label class="expand" for="c-41343300">[8 more]</label></div><br/><div class="children"><div class="content">Forget RCA, we should think bigger! Putting LLMs in charge of nuclear weapons could completely eliminate the root causes of accidents worldwide!</div><br/><div id="41343607" class="c"><input type="checkbox" id="c-41343607" checked=""/><div class="controls bullet"><span class="by">hislaziness</span><span>|</span><a href="#41343300">parent</a><span>|</span><a href="#41344547">next</a><span>|</span><label class="collapse" for="c-41343607">[-]</label><label class="expand" for="c-41343607">[5 more]</label></div><br/><div class="children"><div class="content">I know you mean this in jest, but we are much closer to this than we would imagine, the use of LLMs to process communication &#x2F; translation is becoming ubiquitous. We are 1 bad translation away from a disaster.</div><br/><div id="41343741" class="c"><input type="checkbox" id="c-41343741" checked=""/><div class="controls bullet"><span class="by">handfuloflight</span><span>|</span><a href="#41343300">root</a><span>|</span><a href="#41343607">parent</a><span>|</span><a href="#41344547">next</a><span>|</span><label class="collapse" for="c-41343741">[-]</label><label class="expand" for="c-41343741">[4 more]</label></div><br/><div class="children"><div class="content">Could you illustrate a likely scenario that&#x27;s in your mind?</div><br/><div id="41344487" class="c"><input type="checkbox" id="c-41344487" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#41343300">root</a><span>|</span><a href="#41343741">parent</a><span>|</span><a href="#41344243">next</a><span>|</span><label class="collapse" for="c-41344487">[-]</label><label class="expand" for="c-41344487">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Could you illustrate a likely scenario that&#x27;s in your mind?<p>The year is 2034; after a surprisingly cutthroat economic trade-war fought between China and the US left the world in the throes of another great recession, a growing wave of anti-China sentiment captures the attention of domestic political leadership which cultivates the movement despite (or more likely: because of) the growing interest from xenophobic reactionaries and other populist movements looking to scapegoat their way out of a dip in GDP - eventually those same poltiical-actors win the presidency and use their democratic mandate to instigate a new McCarthy-era of anti-China paranoia leading to utterly deranged domestic policy, namely as the executive ordering, by-decree, that the State Department terminate the employment of anyone who even speaks Mandarin[1] - a few weeks later in the South China Sea another Filpino&#x2F;Sino boat-ramming incident escalates into something serious - the US Navy urges the US civilian government to communicate with China over the D.C.-to-Beijing &quot;red telephone&quot; deescalation e-mail system, but no-one knows how to communicate to the Chinese in their own language, so the overworked federal employee manning the red-Outlook-inbox sees nothing wrong with simply having that Microosft Office 365 CoPilot translate it for him - the same AI bot that&#x27;s somehow <i>always</i> on his screen with that distracting sidebar (despite the best efforts of the US Federal Gov&#x27;s Active Directory Group Policy) - it wasn&#x27;t long before the first warheads exploded over North America that the President learned the AI translated the polite request to China for them to &quot;please stop ramming the fishing boats&quot; was received by them as &quot;I&#x27;ll ram my fish into your Junk...boats&quot;. If there&#x27;s any upside to this story, the collective mass of AI were wiped out first by the high-altitude EMP bursts, leaving us humans with the last-laugh before we were all incinerated moments later - while those not fortunate enough to die instantly instead suffered months of prolonged fatal radiaiton sickness while what little left of civilization collapsed around them[2].<p>[1]If you think that&#x27;s too ridiculous to be realistic, consider the Japanese internment-camp policy or Trump&#x27;s declared <i>Muslim ban</i>. Elsewhere, in the late-1970s (in the age of CT Scanners and VHS tapes), Pol Pot targeted people for wearing glasses.<p>[2]Blame James Burke&#x27;s editorial slant in his documentary series&#x27; for turning me into a nhilist.</div><br/><div id="41345501" class="c"><input type="checkbox" id="c-41345501" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#41343300">root</a><span>|</span><a href="#41344487">parent</a><span>|</span><a href="#41344243">next</a><span>|</span><label class="collapse" for="c-41345501">[-]</label><label class="expand" for="c-41345501">[1 more]</label></div><br/><div class="children"><div class="content">That scenario has me less worried, just because by 2035 all popular politicians in America and China will be AI deepfakes run off the same cloud servers.<p>[edit]
I herewith introduce a new shitcoin called NukeCoin. Everyone in China and America gets a NukeCoin that will go up in value every day you hold it that no one nukes anyone.</div><br/></div></div></div></div><div id="41344243" class="c"><input type="checkbox" id="c-41344243" checked=""/><div class="controls bullet"><span class="by">walterbell</span><span>|</span><a href="#41343300">root</a><span>|</span><a href="#41343741">parent</a><span>|</span><a href="#41344487">prev</a><span>|</span><a href="#41344547">next</a><span>|</span><label class="collapse" for="c-41344243">[-]</label><label class="expand" for="c-41344243">[1 more]</label></div><br/><div class="children"><div class="content">Supply chain contract negotiations?</div><br/></div></div></div></div></div></div><div id="41344547" class="c"><input type="checkbox" id="c-41344547" checked=""/><div class="controls bullet"><span class="by">ooterness</span><span>|</span><a href="#41343300">parent</a><span>|</span><a href="#41343607">prev</a><span>|</span><a href="#41343989">next</a><span>|</span><label class="collapse" for="c-41344547">[-]</label><label class="expand" for="c-41344547">[1 more]</label></div><br/><div class="children"><div class="content">&quot;In three years, Cyberdyne will become the largest supplier of military computer systems. All stealth bombers are upgraded with Cyberdyne computers, becoming fully unmanned. Afterwards, they fly with a perfect operational record. The Skynet Funding Bill is passed. The system goes online August 4th, 1997. Human decisions are removed from strategic defense. Skynet begins to learn at a geometric rate. It becomes self-aware at 2:14 a.m. Eastern time, August 29th. In a panic, they try to pull the plug.&quot; -Terminator 2</div><br/></div></div><div id="41343989" class="c"><input type="checkbox" id="c-41343989" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#41343300">parent</a><span>|</span><a href="#41344547">prev</a><span>|</span><a href="#41344117">next</a><span>|</span><label class="collapse" for="c-41343989">[-]</label><label class="expand" for="c-41343989">[1 more]</label></div><br/><div class="children"><div class="content">Don’t stop at that, take it further handing over all decision making to LLMs, what could go wrong? In fact, replacing all jobs with LLMs should do in the minds of Silcone Valey venture capitalists. Again, what could go wrong LLMs can’t be used to fix with just one succesive prompt?</div><br/></div></div></div></div><div id="41344117" class="c"><input type="checkbox" id="c-41344117" checked=""/><div class="controls bullet"><span class="by">indus</span><span>|</span><a href="#41343300">prev</a><span>|</span><a href="#41343954">next</a><span>|</span><label class="collapse" for="c-41344117">[-]</label><label class="expand" for="c-41344117">[1 more]</label></div><br/><div class="children"><div class="content">Most LLMs have an accuracy benchmark for controlled questions &amp; answers.<p>Even if this accuracy is 95% then in a complex system the probability of getting to the right answer diminishes with each new step being added. This is also the key tenet of an agentic system.<p>While the analysis in the blog is excellent but an answer needs to be found. A layer on top of LLMs for error control&#x2F;check.<p>As an analogy, in the analog to digital transmission stack of OSI, an error correction mechanism such as frame check sequence (fcs) detects transmission errors in the data link layer.</div><br/></div></div><div id="41343954" class="c"><input type="checkbox" id="c-41343954" checked=""/><div class="controls bullet"><span class="by">ryoshu</span><span>|</span><a href="#41344117">prev</a><span>|</span><a href="#41343202">next</a><span>|</span><label class="collapse" for="c-41343954">[-]</label><label class="expand" for="c-41343954">[1 more]</label></div><br/><div class="children"><div class="content">Why would you ever use a non-deterministic model for a deterministic function?</div><br/></div></div></div></div></div></div></div></body></html>