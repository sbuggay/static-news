<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683104453332" as="style"/><link rel="stylesheet" href="styles.css?v=1683104453332"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/openlm-research/open_llama">OpenLLaMA: An Open Reproduction of LLaMA</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>sadiq</span> | <span>9 comments</span></div><br/><div><div id="35799703" class="c"><input type="checkbox" id="c-35799703" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#35799472">next</a><span>|</span><label class="collapse" for="c-35799703">[-]</label><label class="expand" for="c-35799703">[1 more]</label></div><br/><div class="children"><div class="content">Really exciting how fast fully pre-trained new models are appearing.<p>Here&#x27;s another repo (with the same &quot;open-llama&quot; name) that has been available on hugging face as well for a few weeks. (different training dataset)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;s-JoL&#x2F;Open-Llama">https:&#x2F;&#x2F;github.com&#x2F;s-JoL&#x2F;Open-Llama</a>
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;s-JoL&#x2F;Open-Llama-V1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;s-JoL&#x2F;Open-Llama-V1</a></div><br/></div></div><div id="35799472" class="c"><input type="checkbox" id="c-35799472" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#35799703">prev</a><span>|</span><label class="collapse" for="c-35799472">[-]</label><label class="expand" for="c-35799472">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not clear from the GitHub; are there any plans to eventually train the 30 or 65 billion weight LLaMA models? The 65B model seems comparable to GPT3.5 for many things, and can run fine on a beefy desktop just on CPU (CPU ram is much cheaper than GPU ram). It&#x27;d be amazing to have an open source version.</div><br/><div id="35799542" class="c"><input type="checkbox" id="c-35799542" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#35799472">parent</a><span>|</span><label class="collapse" for="c-35799542">[-]</label><label class="expand" for="c-35799542">[6 more]</label></div><br/><div class="children"><div class="content">I ran the 30b and 65b Q4 on a laptop with 64 gb of RAM (8&#x2F;16 CPU). It worked but token&#x2F;s was very low for it to be practically useful.</div><br/><div id="35799578" class="c"><input type="checkbox" id="c-35799578" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#35799472">root</a><span>|</span><a href="#35799542">parent</a><span>|</span><a href="#35799714">next</a><span>|</span><label class="collapse" for="c-35799578">[-]</label><label class="expand" for="c-35799578">[4 more]</label></div><br/><div class="children"><div class="content">How low? I think everybody has different requirements there.</div><br/><div id="35799616" class="c"><input type="checkbox" id="c-35799616" checked=""/><div class="controls bullet"><span class="by">extasia</span><span>|</span><a href="#35799472">root</a><span>|</span><a href="#35799578">parent</a><span>|</span><a href="#35799714">next</a><span>|</span><label class="collapse" for="c-35799616">[-]</label><label class="expand" for="c-35799616">[3 more]</label></div><br/><div class="children"><div class="content">I ran it on a modern desktop and was getting sub 1 token&#x2F;s</div><br/><div id="35799626" class="c"><input type="checkbox" id="c-35799626" checked=""/><div class="controls bullet"><span class="by">asah</span><span>|</span><a href="#35799472">root</a><span>|</span><a href="#35799616">parent</a><span>|</span><a href="#35799714">next</a><span>|</span><label class="collapse" for="c-35799626">[-]</label><label class="expand" for="c-35799626">[2 more]</label></div><br/><div class="children"><div class="content">could it parallelize across multiple PCs ?</div><br/><div id="35799660" class="c"><input type="checkbox" id="c-35799660" checked=""/><div class="controls bullet"><span class="by">serialx</span><span>|</span><a href="#35799472">root</a><span>|</span><a href="#35799626">parent</a><span>|</span><a href="#35799714">next</a><span>|</span><label class="collapse" for="c-35799660">[-]</label><label class="expand" for="c-35799660">[1 more]</label></div><br/><div class="children"><div class="content">No since it’s stateful in the sense that inferencing is dependent on the past generated tokens.</div><br/></div></div></div></div></div></div></div></div><div id="35799714" class="c"><input type="checkbox" id="c-35799714" checked=""/><div class="controls bullet"><span class="by">simion314</span><span>|</span><a href="#35799472">root</a><span>|</span><a href="#35799542">parent</a><span>|</span><a href="#35799578">prev</a><span>|</span><label class="collapse" for="c-35799714">[-]</label><label class="expand" for="c-35799714">[1 more]</label></div><br/><div class="children"><div class="content">slow could be useful if you do not want to chat with it, and instead you could  code it to do a long running job, like code review your entire project like a code analysis tool. Or summarize a lot of content.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>