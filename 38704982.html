<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703062865710" as="style"/><link rel="stylesheet" href="styles.css?v=1703062865710"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://huggingface.co/papers/2312.11514">LLM in a Flash: Efficient LLM Inference with Limited Memory</a> <span class="domain">(<a href="https://huggingface.co">huggingface.co</a>)</span></div><div class="subtext"><span>ghshephard</span> | <span>15 comments</span></div><br/><div><div id="38705012" class="c"><input type="checkbox" id="c-38705012" checked=""/><div class="controls bullet"><span class="by">MBCook</span><span>|</span><a href="#38706136">next</a><span>|</span><label class="collapse" for="c-38705012">[-]</label><label class="expand" for="c-38705012">[5 more]</label></div><br/><div class="children"><div class="content">I wonder how much of the model you can avoid loading before you start to see a real performance difference.<p>Let’s say you want to maintain 90% of everything-in-RAM performance.<p>Can you get away with only using half the memory? Do you need 90% of the memory? Maybe 95%?<p>Basically how fast do you lose performance compared to the maximum by cutting RAM. The charts are comparing their algorithm vs a basic one for the less RAM case, which is a different (but good!) question.<p>If you can get good performance by NOT loading an entire eight gig model into memory of a cell phone that’s obviously a very useful thing.</div><br/><div id="38705797" class="c"><input type="checkbox" id="c-38705797" checked=""/><div class="controls bullet"><span class="by">dwd</span><span>|</span><a href="#38705012">parent</a><span>|</span><a href="#38706027">next</a><span>|</span><label class="collapse" for="c-38705797">[-]</label><label class="expand" for="c-38705797">[1 more]</label></div><br/><div class="children"><div class="content">Apple was running a model double the size of the available memory. Not sure if that was a sweet spot they found or if you could sacrifice response time to run even bigger models.<p>The paper is worth a read in full as what they are doing is pretty cool:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2312.11514" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2312.11514</a><p>Highlight from the paper...<p>&quot;Then, we introduce two complementary techniques to minimize data transfer and maximize flash memory throughput:<p>• Windowing: We load parameters for only the past few tokens, reusing activations from recently computed tokens. This sliding window approach reduces the number of IO requests to load weights.<p>• Row-column bundling: We store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. This increases throughput by reading larger chunks.&quot;</div><br/></div></div><div id="38706027" class="c"><input type="checkbox" id="c-38706027" checked=""/><div class="controls bullet"><span class="by">RecycledEle</span><span>|</span><a href="#38705012">parent</a><span>|</span><a href="#38705797">prev</a><span>|</span><a href="#38706136">next</a><span>|</span><label class="collapse" for="c-38706027">[-]</label><label class="expand" for="c-38706027">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m just thinking out loud. Nothing in this post is authoritative.<p>Theoretically, the time consumed to inference a single token with part of the model stored in flash should be equal to the time to inference that token if the whole model was in RAM, plus the time required to load the part of the model stored in flash memory.<p>I assume we do not need to write back to flash, but I&#x27;m not an LLM expert so I could be wrong.<p>I assume we have many (more than 10) layers so we can leave a fairly small amount of our RAM available to load one layer after another. Most nontrivial LLMs have many dozens of layers, so this seems plausible.<p>If we are not bottlenecking on our RAM during inferencing then we might be able to load the next layer from flash into RAM through a DMA transfer while inferencing our current layer. I don&#x27;t think that would work on single-processor systems due to us always bottlenecking on RAM.<p>Maybe a dual-processor system could load one layer into RAM on one processor while inferencing on the previous layer on the other processor, and thus run really big LLMs in a small amount of RAM?<p>I&#x27;m sitting next to a pile of parts to build a new LLM AI machine. (z840, dual processor) and I look forward to playing with this stuff.</div><br/><div id="38706137" class="c"><input type="checkbox" id="c-38706137" checked=""/><div class="controls bullet"><span class="by">dwd</span><span>|</span><a href="#38705012">root</a><span>|</span><a href="#38706027">parent</a><span>|</span><a href="#38706136">next</a><span>|</span><label class="collapse" for="c-38706137">[-]</label><label class="expand" for="c-38706137">[2 more]</label></div><br/><div class="children"><div class="content">There was also a throwaway comment about experts:<p>&quot;Mixture of Experts (Yiet al., 2023) have a sparse structure in their feed forward layer. This property can be used to combine with our method for enabling larger MoEs on device.&quot;<p>Assuming this implementation would allow for running Mixtral 8x7b on a 16Gb M1, I&#x27;m happy.</div><br/><div id="38706574" class="c"><input type="checkbox" id="c-38706574" checked=""/><div class="controls bullet"><span class="by">treffer</span><span>|</span><a href="#38705012">root</a><span>|</span><a href="#38706137">parent</a><span>|</span><a href="#38706136">next</a><span>|</span><label class="collapse" for="c-38706574">[-]</label><label class="expand" for="c-38706574">[1 more]</label></div><br/><div class="children"><div class="content">I would think so, I have it quantized at 8 bit (q8) and that ticks in at ~47GB.<p>Q4 should be well below the 32GB (2x 16GB).<p>I am wondering if the same implementation could be done for RAM to CPU, too. Those are said to be data transfer limited, so minimizing the RAM to CPU cache transfer should help there, too?</div><br/></div></div></div></div></div></div></div></div><div id="38706136" class="c"><input type="checkbox" id="c-38706136" checked=""/><div class="controls bullet"><span class="by">pulse7</span><span>|</span><a href="#38705012">prev</a><span>|</span><a href="#38706357">next</a><span>|</span><label class="collapse" for="c-38706136">[-]</label><label class="expand" for="c-38706136">[3 more]</label></div><br/><div class="children"><div class="content">Why bring &quot;model parameters ... on demand to DRAM&quot;? Maybe it is better to move LLM processing right unto flash controller chip... (after adding bfloat16 and matrix multiplication support into controller circuitry)</div><br/><div id="38706170" class="c"><input type="checkbox" id="c-38706170" checked=""/><div class="controls bullet"><span class="by">jng</span><span>|</span><a href="#38706136">parent</a><span>|</span><a href="#38706546">next</a><span>|</span><label class="collapse" for="c-38706170">[-]</label><label class="expand" for="c-38706170">[1 more]</label></div><br/><div class="children"><div class="content">Probably because changing the software to use a different read pattern is doable in a few weeks&#x2F;months on your existing systems, and changing anything in the flash controller is a wicked project probably only available to hardware manufacturers, and which will take months to years given the immensely slower hardware iteration cycles (even if it&#x27;s &quot;just&quot; firmware changes).</div><br/></div></div><div id="38706546" class="c"><input type="checkbox" id="c-38706546" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38706136">parent</a><span>|</span><a href="#38706170">prev</a><span>|</span><a href="#38706357">next</a><span>|</span><label class="collapse" for="c-38706546">[-]</label><label class="expand" for="c-38706546">[1 more]</label></div><br/><div class="children"><div class="content">The bottleneck is anyways going to be flash read speed so it doesn&#x27;t matter there are 10 extra steps or if output is computed in the flash.</div><br/></div></div></div></div><div id="38706357" class="c"><input type="checkbox" id="c-38706357" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#38706136">prev</a><span>|</span><a href="#38706160">next</a><span>|</span><label class="collapse" for="c-38706357">[-]</label><label class="expand" for="c-38706357">[1 more]</label></div><br/><div class="children"><div class="content">Did Apple buy an Iranian company?</div><br/></div></div><div id="38706206" class="c"><input type="checkbox" id="c-38706206" checked=""/><div class="controls bullet"><span class="by">philippta</span><span>|</span><a href="#38705227">prev</a><span>|</span><label class="collapse" for="c-38706206">[-]</label><label class="expand" for="c-38706206">[3 more]</label></div><br/><div class="children"><div class="content">Would it be possible to mmap the model parameters, allowing us to run even larger models? How much of an performance impact would that have?</div><br/><div id="38706268" class="c"><input type="checkbox" id="c-38706268" checked=""/><div class="controls bullet"><span class="by">fancy_pantser</span><span>|</span><a href="#38706206">parent</a><span>|</span><a href="#38706272">next</a><span>|</span><label class="collapse" for="c-38706268">[-]</label><label class="expand" for="c-38706268">[1 more]</label></div><br/><div class="children"><div class="content">Sure, this is common e.g. <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;613">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;613</a></div><br/></div></div><div id="38706272" class="c"><input type="checkbox" id="c-38706272" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#38706206">parent</a><span>|</span><a href="#38706268">prev</a><span>|</span><label class="collapse" for="c-38706272">[-]</label><label class="expand" for="c-38706272">[1 more]</label></div><br/><div class="children"><div class="content">mmap isn&#x27;t magic, it&#x27;s just one of many mechanisms for getting data off disk and into memory</div><br/></div></div></div></div></div></div></div></div></div></body></html>