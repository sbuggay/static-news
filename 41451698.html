<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725526862753" as="style"/><link rel="stylesheet" href="styles.css?v=1725526862753"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/lmnr-ai/lmnr">Show HN: Laminar – Open-Source DataDog + PostHog for LLM Apps, Built in Rust</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>skull8888888</span> | <span>24 comments</span></div><br/><div><div id="41453832" class="c"><input type="checkbox" id="c-41453832" checked=""/><div class="controls bullet"><span class="by">findingMeaning</span><span>|</span><a href="#41453566">next</a><span>|</span><label class="collapse" for="c-41453832">[-]</label><label class="expand" for="c-41453832">[5 more]</label></div><br/><div class="children"><div class="content">Everything is LLMs these days. LLMs this, LLMs that. Am I really missing out something from these muted models? Back when it was released, they were so much capable but now everything is muted to the point they are mostly autocomplete on steroids.<p>How can adding analytics to a system that is designed to act like humans produce any good? What is the goal here? Could you clarify why would some need to analyze LLMs out of all the things?<p>&gt; Rich text data makes LLM traces unique, so we let you track “semantic metrics” (like what your AI agent is actually saying) and connect those metrics to where they happen in the trace<p>But why does it matter? Because at the current state these are muted LLMs overseen by the big company. We have very little to control the behavior and whatever we give it, it will mostly be &#x27;politically&#x27; correct.<p>&gt; One thing missing from all LLM observability platforms right now is an adequate search over traces.<p>Again, why do we need to evaluate LLMs? Unless you are working in a security, I see no purpose because these models aren&#x27;t as capable as they used to be. Everything is muted.<p>For context: I don&#x27;t even need to prompt engineer these days because it just gives similar result by using the default prompt. My prompts these are literally three words because it gets more of the job done that way than giving elaborate prompt with precise example and context.</div><br/><div id="41453965" class="c"><input type="checkbox" id="c-41453965" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#41453832">parent</a><span>|</span><a href="#41454200">next</a><span>|</span><label class="collapse" for="c-41453965">[-]</label><label class="expand" for="c-41453965">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not &quot;muted&quot;. You just got used to them and figured out that they don&#x27;t actually generete knew knowledge or information, they only give a statistically average summary of the top Google query. (I.e., they are super bland, boring and predictable.)</div><br/></div></div><div id="41454200" class="c"><input type="checkbox" id="c-41454200" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41453832">parent</a><span>|</span><a href="#41453965">prev</a><span>|</span><a href="#41453566">next</a><span>|</span><label class="collapse" for="c-41454200">[-]</label><label class="expand" for="c-41454200">[3 more]</label></div><br/><div class="children"><div class="content">Hey there, apologies for the late reply.<p>&gt; Could you clarify why would some need to analyze LLMs out of all the things?<p>When you want to understand trends of the output of your Agent &#x2F; RAG on scale, without looking manually at each trace, you need to another LLM to process the output. For instance, you want to understand what is the most common topic discussed with your agent. You can prompt another LLM to extract this info, Laminar will host everything, and turn this data into metrics.<p>&gt; Why do we need to evaluate LLMs?<p>You right, devs who want to evaluate output of the LLM apps, truly care about the quality or some other metric. For this kind of cases evals are invaluable. Good example would be, AI drive-through agents or AI voice agents for mortgages (use cases we&#x27;ve seen on Laminar)</div><br/><div id="41454457" class="c"><input type="checkbox" id="c-41454457" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#41453832">root</a><span>|</span><a href="#41454200">parent</a><span>|</span><a href="#41453566">next</a><span>|</span><label class="collapse" for="c-41454457">[-]</label><label class="expand" for="c-41454457">[2 more]</label></div><br/><div class="children"><div class="content">Topic modelling and classifications are real problems in LLM observability and evaluation, glad to see a platform doing this.<p>I see that you have chained prompts, does that mean I can define agents and functions inside the platform without having it in the code?</div><br/><div id="41454552" class="c"><input type="checkbox" id="c-41454552" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41453832">root</a><span>|</span><a href="#41454457">parent</a><span>|</span><a href="#41453566">next</a><span>|</span><label class="collapse" for="c-41454552">[-]</label><label class="expand" for="c-41454552">[1 more]</label></div><br/><div class="children"><div class="content">Yes! Our pipeline builder is pretty versatile. You can define conditional routing, parallel branches, and cycles. Right now we support LLM node and util nodes (json extractor). If you can defined your logic purely from those nodes (and in majority of cases you will be), then great, you can host everything on Laminar! You follow this guide (<a href="https:&#x2F;&#x2F;docs.lmnr.ai&#x2F;tutorials&#x2F;control-flow-with-LLM">https:&#x2F;&#x2F;docs.lmnr.ai&#x2F;tutorials&#x2F;control-flow-with-LLM</a>) it&#x27;s bit outdated by gives you a good idea on how to create and run pipelines.</div><br/></div></div></div></div></div></div></div></div><div id="41453566" class="c"><input type="checkbox" id="c-41453566" checked=""/><div class="controls bullet"><span class="by">gitroom</span><span>|</span><a href="#41453832">prev</a><span>|</span><a href="#41452994">next</a><span>|</span><label class="collapse" for="c-41453566">[-]</label><label class="expand" for="c-41453566">[2 more]</label></div><br/><div class="children"><div class="content">How will you distinguish Laminar as &quot;the Supabase for LLMOps&quot; from the many LLM observability platforms already claiming similar aims? Is the integration of text analytics into execution traces your secret sauce? Or, could this perceived advantage just add complexity for developers who like their systems simple and their setups minimal?</div><br/><div id="41453674" class="c"><input type="checkbox" id="c-41453674" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41453566">parent</a><span>|</span><a href="#41452994">next</a><span>|</span><label class="collapse" for="c-41453674">[-]</label><label class="expand" for="c-41453674">[1 more]</label></div><br/><div class="children"><div class="content">Hey there! Good question. Our main distinguishing features are:<p>* Ingestion of Otel traces<p>* Semantic events-based analytics<p>* Semantically searchable traces<p>* High performance, reliability and efficiency out of the box, thanks to our stack<p>* High quality FE which is fully open-source<p>* LLM Pipeline manager, first of it&#x27;s kind, highly customizable and optimized for performance<p>* Ability to track progression of locally run evals, combining full flexibility of running code locally without need to manage data infra<p>* Very generous free tier plan. Our infra is so efficient, that we can accommodate large number of free tier users without scaling it too much.<p>And many more to come in the coming weeks! On of our biggest next priorities is to focus on high quality docs.<p>All of these features can be used as standalone products, similar to Supabase. So, devs who prefer keep things lightweight might just use our tracing solution and be very happy with it.</div><br/></div></div></div></div><div id="41452994" class="c"><input type="checkbox" id="c-41452994" checked=""/><div class="controls bullet"><span class="by">7thpower</span><span>|</span><a href="#41453566">prev</a><span>|</span><a href="#41453001">next</a><span>|</span><label class="collapse" for="c-41452994">[-]</label><label class="expand" for="c-41452994">[2 more]</label></div><br/><div class="children"><div class="content">I’m always game for an LLM observability platform that is potentially affordable, at least during the early phases of development.<p>I was using DD at work and found it to be incredibly helpful but now that I am on my own, I am much more price sensitive.<p>Still, having a low friction way to see how things are running, check inputs&#x2F;outputs is a game changer.<p>One challenge I have run into is a lack of support for Anthropic models. The platforms that do have support are missing key pieces of info like the system prompt. (Prob a skill issue on my end).<p>Also they seem to all be tightly coupled to langchain, etc which is a no-go.<p>Will check this out over the next week or two. Very exciting!</div><br/><div id="41453037" class="c"><input type="checkbox" id="c-41453037" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41452994">parent</a><span>|</span><a href="#41453001">next</a><span>|</span><label class="collapse" for="c-41453037">[-]</label><label class="expand" for="c-41453037">[1 more]</label></div><br/><div class="children"><div class="content">Totally agree, observability is a must for LLM apps. We wanted to build something of extremely high quality but to be affordable for solo devs, that&#x27;s why open-source and why very generous free tier on our managed version.<p>Regarding Anthropic instrumentation, we support it out of the box! You don&#x27;t even need to wrap anything, just do laminar initialize and you should see detailed traces. We also support images! Hit me up at robert@lmnr.ai if you need help onboarding or setting up local version</div><br/></div></div></div></div><div id="41453001" class="c"><input type="checkbox" id="c-41453001" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41452994">prev</a><span>|</span><a href="#41453058">next</a><span>|</span><label class="collapse" for="c-41453001">[-]</label><label class="expand" for="c-41453001">[4 more]</label></div><br/><div class="children"><div class="content">&gt; One thing missing from all LLM observability platforms right now is an adequate search over traces.<p>Why did you decide to build a whole platform and include this feature on top, rather than adding search to (for example) Grafana Tempo?</div><br/><div id="41453093" class="c"><input type="checkbox" id="c-41453093" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41453001">parent</a><span>|</span><a href="#41453058">next</a><span>|</span><label class="collapse" for="c-41453093">[-]</label><label class="expand" for="c-41453093">[3 more]</label></div><br/><div class="children"><div class="content">Valid point. For us searchable and especially semantically searchable, traces &#x2F; spans really make sense only in the context of tracing LLM apps. And then, we view it as a powerful feature, but, primarily in the context of AI&#x2F;LLM-native observability platform. For us, the ultimate goal is to build the comprehensive platform, with features which are extremely useful for observability and development of LLM&#x2F;GenAI apps.</div><br/><div id="41454139" class="c"><input type="checkbox" id="c-41454139" checked=""/><div class="controls bullet"><span class="by">carlmr</span><span>|</span><a href="#41453001">root</a><span>|</span><a href="#41453093">parent</a><span>|</span><a href="#41453058">next</a><span>|</span><label class="collapse" for="c-41454139">[-]</label><label class="expand" for="c-41454139">[2 more]</label></div><br/><div class="children"><div class="content">&gt;For us searchable and especially semantically searchable, traces &#x2F; spans really make sense only in the context of tracing LLM apps.<p>I know LLM is the new shiny thing right now. Why is semantic search of traces only useful for LLMs?<p>I&#x27;ve been working in CI&#x2F;CD and at a large enough scale, searchability of logs was always an issue. Especially as many tools produce a lot of output with warnings and errors that mislead you.<p>Is the search feature only working in an LLM context? If so why?</div><br/><div id="41454220" class="c"><input type="checkbox" id="c-41454220" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41453001">root</a><span>|</span><a href="#41454139">parent</a><span>|</span><a href="#41453058">next</a><span>|</span><label class="collapse" for="c-41454220">[-]</label><label class="expand" for="c-41454220">[1 more]</label></div><br/><div class="children"><div class="content">Now that you mentioned it,
&gt; warnings and errors that mislead you<p>it really makes sense. I guess what I was pointing into, is that when you have really rich text (in your case it would be error descriptions), searching over them semantically is a must have feature.<p>But you are right, being an output of LLM is not a requirement.</div><br/></div></div></div></div></div></div></div></div><div id="41453058" class="c"><input type="checkbox" id="c-41453058" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#41453001">prev</a><span>|</span><a href="#41453087">next</a><span>|</span><label class="collapse" for="c-41453058">[-]</label><label class="expand" for="c-41453058">[2 more]</label></div><br/><div class="children"><div class="content">To my eye this looks quite a bit more serious and useful than the naive buzzword bingo test would suggest.<p>I really like the stack these folks have chosen.</div><br/><div id="41453112" class="c"><input type="checkbox" id="c-41453112" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41453058">parent</a><span>|</span><a href="#41453087">next</a><span>|</span><label class="collapse" for="c-41453112">[-]</label><label class="expand" for="c-41453112">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! We thought a lot about what would make a great title but couldn&#x27;t really find anything else which would convey info as densely as current title. We also love our current stack :). I think Rust is perfect language to handle span ingestion and it marries perfectly with the rest of our stack.</div><br/></div></div></div></div><div id="41453087" class="c"><input type="checkbox" id="c-41453087" checked=""/><div class="controls bullet"><span class="by">bn-l</span><span>|</span><a href="#41453058">prev</a><span>|</span><a href="#41453241">next</a><span>|</span><label class="collapse" for="c-41453087">[-]</label><label class="expand" for="c-41453087">[2 more]</label></div><br/><div class="children"><div class="content">Does it do event sourcing like inngest where I can do the “saga pattern”?</div><br/><div id="41453159" class="c"><input type="checkbox" id="c-41453159" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41453087">parent</a><span>|</span><a href="#41453241">next</a><span>|</span><label class="collapse" for="c-41453159">[-]</label><label class="expand" for="c-41453159">[1 more]</label></div><br/><div class="children"><div class="content">You mean like triggering another processing pipeline from the output of current processing pipeline?</div><br/></div></div></div></div><div id="41453241" class="c"><input type="checkbox" id="c-41453241" checked=""/><div class="controls bullet"><span class="by">mxu_</span><span>|</span><a href="#41453087">prev</a><span>|</span><a href="#41452424">next</a><span>|</span><label class="collapse" for="c-41453241">[-]</label><label class="expand" for="c-41453241">[2 more]</label></div><br/><div class="children"><div class="content">looks cool I wish I had this when I started YC</div><br/><div id="41453244" class="c"><input type="checkbox" id="c-41453244" checked=""/><div class="controls bullet"><span class="by">skull8888888</span><span>|</span><a href="#41453241">parent</a><span>|</span><a href="#41452424">next</a><span>|</span><label class="collapse" for="c-41453244">[-]</label><label class="expand" for="c-41453244">[1 more]</label></div><br/><div class="children"><div class="content">Thank you!</div><br/></div></div></div></div></div></div></div></div></div></body></html>