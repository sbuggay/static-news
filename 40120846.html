<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713862859130" as="style"/><link rel="stylesheet" href="styles.css?v=1713862859130"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2404.10076">FPGA Architecture for Deep Learning: Survey and Future Directions</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>matt_d</span> | <span>59 comments</span></div><br/><div><div id="40127163" class="c"><input type="checkbox" id="c-40127163" checked=""/><div class="controls bullet"><span class="by">zachbee</span><span>|</span><a href="#40126647">next</a><span>|</span><label class="collapse" for="c-40127163">[-]</label><label class="expand" for="c-40127163">[9 more]</label></div><br/><div class="children"><div class="content">The big challenge when it comes to using FPGAs for deep learning is pretty simple: all of that reprogrammability comes at a performance cost. If you&#x27;re doing something highly specific that conventional GPUs are bad at, like genomics research [1] or high-frequency trading [2], the performance tradeoff is worth it. But for deep learning, GPUs and AI ASICs are highly optimized for most of these computations, and an FPGA won&#x27;t offer huge performance increases.<p>The main advantage FPGAs offer is being able to take advantage of new model optimizations much earlier than ASIC implementations could. Those proposed ternary LLMs could potentially run much faster on FPGAs, because the hardware could be optimized for exclusively ternary ops. [3]<p>Not to toot my own horn, but I wrote up a blog post recently about building practical FPGA acceleration and which applications are best suited for it: <a href="https:&#x2F;&#x2F;www.zach.be&#x2F;p&#x2F;how-to-build-a-commercial-open-source" rel="nofollow">https:&#x2F;&#x2F;www.zach.be&#x2F;p&#x2F;how-to-build-a-commercial-open-source</a><p>[1] <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;solutions&#x2F;case-studies&#x2F;munich-leukemia-lab&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;solutions&#x2F;case-studies&#x2F;munich-leukemi...</a><p>[2] <a href="https:&#x2F;&#x2F;careers.imc.com&#x2F;us&#x2F;en&#x2F;blogarticle&#x2F;how-are-fpgas-used-in-trading" rel="nofollow">https:&#x2F;&#x2F;careers.imc.com&#x2F;us&#x2F;en&#x2F;blogarticle&#x2F;how-are-fpgas-used...</a><p>[3] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.17764" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.17764</a></div><br/><div id="40127856" class="c"><input type="checkbox" id="c-40127856" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#40127163">parent</a><span>|</span><a href="#40127306">next</a><span>|</span><label class="collapse" for="c-40127856">[-]</label><label class="expand" for="c-40127856">[3 more]</label></div><br/><div class="children"><div class="content">Are you trying to scare people away from FPGAs? GPUs aren&#x27;t actually that _good_ at deep learning, but they are in the right place at the right time.<p>You can rent high end FPGAs on AWS, <a href="https:&#x2F;&#x2F;github.com&#x2F;aws&#x2F;aws-fpga">https:&#x2F;&#x2F;github.com&#x2F;aws&#x2F;aws-fpga</a> there is no better time to get into FPGAs. On the low end there is the excellent <a href="https:&#x2F;&#x2F;hackaday.com&#x2F;2019&#x2F;01&#x2F;14&#x2F;ulx3s-an-open-source-lattice-ecp5-fpga-pcb&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hackaday.com&#x2F;2019&#x2F;01&#x2F;14&#x2F;ulx3s-an-open-source-lattice...</a><p>Modern FPGA platforms like Xilinx Alveo have 35TB&#x2F;s of SRAM bandwidth and 460GB&#x2F;s of HBM bandwidth. <a href="https:&#x2F;&#x2F;www.xilinx.com&#x2F;products&#x2F;boards-and-kits&#x2F;alveo&#x2F;u55c.html#specifications" rel="nofollow">https:&#x2F;&#x2F;www.xilinx.com&#x2F;products&#x2F;boards-and-kits&#x2F;alveo&#x2F;u55c.h...</a></div><br/><div id="40129105" class="c"><input type="checkbox" id="c-40129105" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#40127163">root</a><span>|</span><a href="#40127856">parent</a><span>|</span><a href="#40129232">next</a><span>|</span><label class="collapse" for="c-40129105">[-]</label><label class="expand" for="c-40129105">[1 more]</label></div><br/><div class="children"><div class="content">If I remember correctly about 80% of a modern FPGA&#x27;s silicon is is used for connections. FPGA have their uses and very often a big part in them is the <i>Field Programmability</i>. If that is not required, there is no good reason another solution (ASIC, GPU, etc.) couldn&#x27;t beat the FPGA in theory. Now, in practice there are some niches, where this is not absolutely true, but I agree with GP that I see challenges for deep learning.</div><br/></div></div><div id="40129232" class="c"><input type="checkbox" id="c-40129232" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#40127163">root</a><span>|</span><a href="#40127856">parent</a><span>|</span><a href="#40129105">prev</a><span>|</span><a href="#40127306">next</a><span>|</span><label class="collapse" for="c-40129232">[-]</label><label class="expand" for="c-40129232">[1 more]</label></div><br/><div class="children"><div class="content">500GB&#x2F;s is going to limit it to at best 1&#x2F;4 the DL performance of an nvidia gpu. I’m not sure what the floating point perf of these FPGAs are but I imagine that also might set a fundamental performance limit at a small fraction of a GPU.</div><br/></div></div></div></div><div id="40127306" class="c"><input type="checkbox" id="c-40127306" checked=""/><div class="controls bullet"><span class="by">kadushka</span><span>|</span><a href="#40127163">parent</a><span>|</span><a href="#40127856">prev</a><span>|</span><a href="#40127353">next</a><span>|</span><label class="collapse" for="c-40127306">[-]</label><label class="expand" for="c-40127306">[1 more]</label></div><br/><div class="children"><div class="content">There are two other problems with FPGAs:<p>1. They are hard to use (program). If you&#x27;re a regular ML engineer, there will be a steep learning curve with Verilog&#x2F;VHDL and the specifics of the chip you choose, especially if you want to squeeze all the performance out of it. For most researchers it&#x27;s just not worth it. And for production deployment it&#x27;s not worth the risk of investing into an unproven platform. Microsoft tried it many years ago to accelerate their search&#x2F;whatever, and I think they abandoned it.<p>2. Cost. High performance FPGA chips are expensive. Like A100 to H100 price range. Very few people would be willing to spend this much to accelerate their DL models unless the speedup is &gt; 2x compared to GPUs.</div><br/></div></div><div id="40127353" class="c"><input type="checkbox" id="c-40127353" checked=""/><div class="controls bullet"><span class="by">IX-103</span><span>|</span><a href="#40127163">parent</a><span>|</span><a href="#40127306">prev</a><span>|</span><a href="#40129223">next</a><span>|</span><label class="collapse" for="c-40127353">[-]</label><label class="expand" for="c-40127353">[1 more]</label></div><br/><div class="children"><div class="content">FPGAs are also reasonably good at breadboarding modules to be added to ASICs. You scale down the timing and you can run the same HDL and perform software integration at the same time as the HDL is optimized.<p>Much cheaper and faster than gate level simulation.</div><br/></div></div><div id="40129223" class="c"><input type="checkbox" id="c-40129223" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#40127163">parent</a><span>|</span><a href="#40127353">prev</a><span>|</span><a href="#40129249">next</a><span>|</span><label class="collapse" for="c-40129223">[-]</label><label class="expand" for="c-40129223">[2 more]</label></div><br/><div class="children"><div class="content">Every couple of years I revisit the FPGA topic, eager to build something exciting. I always end up with a ton of research, where I learn a lot but ultimately shy away from building something.<p>This is because I cannot find a project that is doable and affordable for a hobbyist but at the same time <i>requires</i> an FPGA in some sense. To put it bluntly: I can blink a LED for a fiver with a micro instead of spending hundreds for an FPGA.<p>So, assuming I am reasonably experienced in software development and electronics and I have 1000 USD and a week to spend.<p>What could I build that shows off the capabilities of an FPGA?</div><br/><div id="40129649" class="c"><input type="checkbox" id="c-40129649" checked=""/><div class="controls bullet"><span class="by">scottapotamas</span><span>|</span><a href="#40127163">root</a><span>|</span><a href="#40129223">parent</a><span>|</span><a href="#40129249">next</a><span>|</span><label class="collapse" for="c-40129649">[-]</label><label class="expand" for="c-40129649">[1 more]</label></div><br/><div class="children"><div class="content">Reasonably experienced and &#x27;a week&#x27; can mean vastly different things... It&#x27;s certainly easier to keep the cost down with longer time-frames.<p>For a focus on electronics rather than implementing some kind of toy &#x27;algorithm accelerator&#x27;, I find low-hanging&#x2F;interesting projects where the combination of requirements exceed a micro&#x27;s peripheral capabilities - i.e. multiple input&#x2F;output&#x2F;processing tasks which could be performed on a micro individually, but adding synchronisation or latency requirements makes it rather non-trivial.<p>- Very wide&#x2F;parallel input&#x2F;output tasks: ADC&#x2F;DACs for higher samplerate&#x2F;bitdepth&#x2F;channel count than typically accessible with even high-end micros<p>- Implementing unique&#x2F;specialised protocols which would have required bit-banging, abuse of timer&#x2F;other peripherals on a micro (i.e. interesting things people achieve with PIO blocks on RP2040 etc)<p>- Signal processing: digital filters and control systems are great because you can see&#x2F;hear&#x2F;interact with the output which can help build a sense of achievement.<p>When starting out, it&#x27;s also less overwhelming to start with smaller parts and allocate the budget to the rest of the electronics. They&#x27;re still incredibly capable and won&#x27;t seem as under-utilised. Some random project ideas:<p>- Driving large frame-buffers to display(s) or large sets of LED matrices at high frame rate - <a href="https:&#x2F;&#x2F;gregdavill.com&#x2F;posts&#x2F;d20&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gregdavill.com&#x2F;posts&#x2F;d20&#x2F;</a><p>- Realtime audio filters - the Eurorack community might have some inspiration.<p>- Multi-channel synchonous detection, lock-in amplifiers, distributed timing reference&#x2F;control,<p>- Find a sensing application that&#x27;s interesting and then take it to the logical extreme - arrays of photo&#x2F;hall-effect sensors sampled at high speed and displayed, accelerometers&#x2F;IMU sensor fusion<p>- Laser galvanometers and piezo actuators are getting more accessible<p>- Small but precise&#x2F;fast motion stages for positioning or sensing might present a good combination of input, output, filtering and control systems.<p>- With more time&#x2F;experience you could branch into more interesting (IMO) areas like RF or imaging systems.<p>With more info about your interest areas I can give more specific suggestions.</div><br/></div></div></div></div><div id="40129249" class="c"><input type="checkbox" id="c-40129249" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40127163">parent</a><span>|</span><a href="#40129223">prev</a><span>|</span><a href="#40126647">next</a><span>|</span><label class="collapse" for="c-40129249">[-]</label><label class="expand" for="c-40129249">[1 more]</label></div><br/><div class="children"><div class="content">Um, no? The actual problem is that most FPGAs already have DPUs for machine learning integrated on them. Some Xilinx FPGAs have 400 &quot;AI Engines&quot; which provide significantly more compute than the programmable logic, the almost 2000 DSP slices or the ARM cores. This means that the problem with FPGAs is primarily lack of SRAM and limited memory bandwidth.<p><a href="https:&#x2F;&#x2F;www.xilinx.com&#x2F;products&#x2F;boards-and-kits&#x2F;vck190.html" rel="nofollow">https:&#x2F;&#x2F;www.xilinx.com&#x2F;products&#x2F;boards-and-kits&#x2F;vck190.html</a></div><br/></div></div></div></div><div id="40126647" class="c"><input type="checkbox" id="c-40126647" checked=""/><div class="controls bullet"><span class="by">woopsn</span><span>|</span><a href="#40127163">prev</a><span>|</span><a href="#40127795">next</a><span>|</span><label class="collapse" for="c-40126647">[-]</label><label class="expand" for="c-40126647">[2 more]</label></div><br/><div class="children"><div class="content">Reconfigurable logic may be used to implement fairly small models, in applications where they&#x27;re already employed and adding a coprocessor specifically for ML is either infeasible or doesn&#x27;t make sense. As the paper mentions, you need hard logic blocks for arithmetic (if not floating point), and these are always in short supply. In DSP applications I&#x27;ve worked on we used the fpga for timing and i&#x2F;o, to control jitter and sample from many ADCs in parallel - but apart from some filtering then ran the numbers on an adjacent non-reconfigurable core. You can get huge chips with a lot of hard logic built in, but they&#x27;re relatively expensive compared to fpga + traditional coprocessor with shared memory. The high end application specific chips are shockingly expensive - worse than GPUs because there is comparatively little market for them. We had one evaluation board that was like $100K iirc.</div><br/><div id="40127145" class="c"><input type="checkbox" id="c-40127145" checked=""/><div class="controls bullet"><span class="by">ZoomerCretin</span><span>|</span><a href="#40126647">parent</a><span>|</span><a href="#40127795">next</a><span>|</span><label class="collapse" for="c-40127145">[-]</label><label class="expand" for="c-40127145">[1 more]</label></div><br/><div class="children"><div class="content">Yep. Google has TPUs, and AMD&#x2F;Nvidia&#x2F;Intel&#x2F;Apple&#x2F;Qualcomm all have tensor coprocessors now. From a CPU or GPU to an FPGA, the cost&#x2F;benefit is huge. With every device having tensor cores, not so much. ASIP and ASICs are likely the way to go, at least for common operations like matrix multiplication.</div><br/></div></div></div></div><div id="40127795" class="c"><input type="checkbox" id="c-40127795" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#40126647">prev</a><span>|</span><a href="#40129255">next</a><span>|</span><label class="collapse" for="c-40127795">[-]</label><label class="expand" for="c-40127795">[12 more]</label></div><br/><div class="children"><div class="content">My pet project is to take these ideas and go to the logical end, arriving at a systolic array I call a BitGrid.<p>It&#x27;s a Cartesian grid of 4 bit look up tables, with bits to&#x2F;from each neighbor. This allows each output to be independent, maximizing utilization.<p>To solve timing issues, each cell would be clocked, with 2 phases for the grid, in a checkerboard pattern. This makes all inputs stable, and timing deterministic. Unlike an FPGA, latency is not the primary limit of performance, as everything is thus pipelined.<p>I wrote a simulator, and started learning VHDL in order to program an FPGA board I bought for prototyping the concept. My eventual goal is an ASIC through tiny tapeout.<p>The big software hurdle is compiling expressions into a directed graph of bitwise operations.<p>Because data only travels to the neighbors, all the lines in a chip are short, and it should be possible to use far fewer metallization layers in an actual chip than a CPU for example.</div><br/><div id="40127826" class="c"><input type="checkbox" id="c-40127826" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#40127795">parent</a><span>|</span><a href="#40129253">next</a><span>|</span><label class="collapse" for="c-40127826">[-]</label><label class="expand" for="c-40127826">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;tinytapeout.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tinytapeout.com&#x2F;</a> now lest you purchase additional tiles for $50, each tile supports about 1k digital logic gates.<p>Next one closes June 1.<p><a href="https:&#x2F;&#x2F;tinytapeout.com&#x2F;faq&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tinytapeout.com&#x2F;faq&#x2F;</a><p>You might enjoy this talk from the last Latchup on Wave Pipelining<p><a href="https:&#x2F;&#x2F;fossi-foundation.org&#x2F;latch-up&#x2F;2024#riding-the-wave-building-wave-pipelines-in-fpgas" rel="nofollow">https:&#x2F;&#x2F;fossi-foundation.org&#x2F;latch-up&#x2F;2024#riding-the-wave-b...</a><p><a href="https:&#x2F;&#x2F;www.cs.princeton.edu&#x2F;courses&#x2F;archive&#x2F;fall01&#x2F;cs597a&#x2F;wave.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.princeton.edu&#x2F;courses&#x2F;archive&#x2F;fall01&#x2F;cs597a&#x2F;w...</a></div><br/></div></div><div id="40129253" class="c"><input type="checkbox" id="c-40129253" checked=""/><div class="controls bullet"><span class="by">rlupi</span><span>|</span><a href="#40127795">parent</a><span>|</span><a href="#40127826">prev</a><span>|</span><a href="#40127888">next</a><span>|</span><label class="collapse" for="c-40129253">[-]</label><label class="expand" for="c-40129253">[1 more]</label></div><br/><div class="children"><div class="content">Interesting idea.<p>The main question for me is if it will be efficient, in the sense that you need program&#x2F;models that can be binpacked into the size of your design and need data all at the same time in various stages otherwise a lot of your silicon will be under-utilized (since you don&#x27;t have memory, you can&#x27;t trade between compute and RAM to efficiently use your silicon die size).<p>Rather than our current breed of neural network architectures and models, you&#x27;d probably need to look into alternatives like spiking neural network and see if they can store data as frequency and activation patterns.</div><br/></div></div><div id="40127888" class="c"><input type="checkbox" id="c-40127888" checked=""/><div class="controls bullet"><span class="by">ooterness</span><span>|</span><a href="#40127795">parent</a><span>|</span><a href="#40129253">prev</a><span>|</span><a href="#40128375">next</a><span>|</span><label class="collapse" for="c-40127888">[-]</label><label class="expand" for="c-40127888">[4 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re describing is a cellular automaton, in the same vein as of Conway&#x27;s Game of Life. You can do lots of interesting things with those, but it&#x27;s emphatically not where I&#x27;d start for a flexible computing platform.<p>Why not go the extra mile, and make each tile a small CPU? There&#x27;s a Zachtronics have called TIS-100 with this premise.<p><a href="https:&#x2F;&#x2F;store.steampowered.com&#x2F;app&#x2F;370360&#x2F;TIS100&#x2F;" rel="nofollow">https:&#x2F;&#x2F;store.steampowered.com&#x2F;app&#x2F;370360&#x2F;TIS100&#x2F;</a></div><br/><div id="40128464" class="c"><input type="checkbox" id="c-40128464" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#40127795">root</a><span>|</span><a href="#40127888">parent</a><span>|</span><a href="#40129290">next</a><span>|</span><label class="collapse" for="c-40128464">[-]</label><label class="expand" for="c-40128464">[2 more]</label></div><br/><div class="children"><div class="content">Because each cell has it&#x27;s own state, and 64 bits of &quot;program&quot; (16 bits in each of the 4 LUTs), it&#x27;s unlike the game of life, where the rule is the same for each cell.<p>I looked at a lot of choices for architecture, and wanted to allow data paths to cross without conflict, and the 4 in&#x2F;4 out choice worked best without going <i>too</i> far.<p>Someone did work out how you could run the game of life on a BitGrid, it&#x27;s in the Esoteric Languages wiki<p><a href="https:&#x2F;&#x2F;esolangs.org&#x2F;wiki&#x2F;Bitgrid" rel="nofollow">https:&#x2F;&#x2F;esolangs.org&#x2F;wiki&#x2F;Bitgrid</a><p>I see it as something like a Turing machine, a bit less abstract, and much, <i>much</i> faster at computing real results.   I hope it can democratize access to PetaFLOPS.<p>The question I can&#x27;t seem to find an answer to is simple... how much power does a 4 bit in&#x2F;out set of LUTs with a latch take statically? How many femtojoules does it take to switch?<p>If those numbers are good enough, it&#x27;s entirely possible that really fast compute is on the table of possibilities.  If not, it&#x27;s another Turing machine.</div><br/><div id="40129173" class="c"><input type="checkbox" id="c-40129173" checked=""/><div class="controls bullet"><span class="by">ooterness</span><span>|</span><a href="#40127795">root</a><span>|</span><a href="#40128464">parent</a><span>|</span><a href="#40129290">next</a><span>|</span><label class="collapse" for="c-40129173">[-]</label><label class="expand" for="c-40129173">[1 more]</label></div><br/><div class="children"><div class="content">The power is easy enough to calculate.<p>Let&#x27;s take the a Kintex Ultrascale+ from Xilinx as a fairly typical example of a modern FPGA.  Relevant documentation is the UltraScale Architecture CLB User Guide [1] and the Xilinx Power Estimator spreadsheet [2].<p>Each &quot;slice&quot; contains two flip-flops and a lookup table with 6 input bits and 2 output bits. So two slices is enough to implement each cell with room to spare.<p>Let&#x27;s say you have a 200 x 200 grid = 40k cells.  That&#x27;s 80k LUTs and 160k flip-flops.  That&#x27;s about 29% of the resources on a XCKU9P.  If we assume a 100 MHz clock and 25% toggle rate (somewhat arbitrary), that&#x27;s 4e12 state-changes per second.  The spreadsheet indicates that circuit will consume 850 mW, or about 200 fJ per state-change.<p>That said, this is NOT an efficient way to do arithmetic. You&#x27;d need N cells to do a fixed-point addition with N-bit arguments, and O(N^2) (give or take) to do a fixed-point multiplication. Floating point requires orders of magnitude more. There&#x27;s a reason modern FPGAs have dedicated paths for fast addition and hardwired multiplier macros.<p>[1] <a href="https:&#x2F;&#x2F;www.xilinx.com&#x2F;content&#x2F;dam&#x2F;xilinx&#x2F;support&#x2F;documents&#x2F;user_guides&#x2F;ug574-ultrascale-clb.pdf" rel="nofollow">https:&#x2F;&#x2F;www.xilinx.com&#x2F;content&#x2F;dam&#x2F;xilinx&#x2F;support&#x2F;documents&#x2F;...</a><p>[2] <a href="https:&#x2F;&#x2F;www.xilinx.com&#x2F;products&#x2F;technology&#x2F;power&#x2F;xpe.html" rel="nofollow">https:&#x2F;&#x2F;www.xilinx.com&#x2F;products&#x2F;technology&#x2F;power&#x2F;xpe.html</a></div><br/></div></div></div></div><div id="40129290" class="c"><input type="checkbox" id="c-40129290" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40127795">root</a><span>|</span><a href="#40127888">parent</a><span>|</span><a href="#40128464">prev</a><span>|</span><a href="#40128375">next</a><span>|</span><label class="collapse" for="c-40129290">[-]</label><label class="expand" for="c-40129290">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Why not go the extra mile, and make each tile a small CPU?<p>Xilinx AI Engine and Ryzen AI is exactly that.</div><br/></div></div></div></div><div id="40128375" class="c"><input type="checkbox" id="c-40128375" checked=""/><div class="controls bullet"><span class="by">pyinstallwoes</span><span>|</span><a href="#40127795">parent</a><span>|</span><a href="#40127888">prev</a><span>|</span><a href="#40129133">next</a><span>|</span><label class="collapse" for="c-40128375">[-]</label><label class="expand" for="c-40128375">[1 more]</label></div><br/><div class="children"><div class="content">Sounds similar to Greenarray :<p><a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230331155747&#x2F;https:&#x2F;&#x2F;www.greenarraychips.com&#x2F;home&#x2F;about&#x2F;index.php" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230331155747&#x2F;https:&#x2F;&#x2F;www.green...</a></div><br/></div></div><div id="40129133" class="c"><input type="checkbox" id="c-40129133" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#40127795">parent</a><span>|</span><a href="#40128375">prev</a><span>|</span><a href="#40128404">next</a><span>|</span><label class="collapse" for="c-40129133">[-]</label><label class="expand" for="c-40129133">[1 more]</label></div><br/><div class="children"><div class="content">Systolic arrays are essentially how matmul is implemented in tensor cores in GPUs and TPUs.</div><br/></div></div><div id="40128404" class="c"><input type="checkbox" id="c-40128404" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#40127795">parent</a><span>|</span><a href="#40129133">prev</a><span>|</span><a href="#40127962">next</a><span>|</span><label class="collapse" for="c-40128404">[-]</label><label class="expand" for="c-40128404">[1 more]</label></div><br/><div class="children"><div class="content">DE Shaw has a systolic supercomputer: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Anton_(computer)" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Anton_(computer)</a></div><br/></div></div><div id="40127962" class="c"><input type="checkbox" id="c-40127962" checked=""/><div class="controls bullet"><span class="by">woopsn</span><span>|</span><a href="#40127795">parent</a><span>|</span><a href="#40128404">prev</a><span>|</span><a href="#40129255">next</a><span>|</span><label class="collapse" for="c-40127962">[-]</label><label class="expand" for="c-40127962">[2 more]</label></div><br/><div class="children"><div class="content">Cool idea. What does the expression&#x2F;spec language look like? I would guess it has to be mapped... not straightforwardly. It seems like your goal is maximize throughput, but then the datapath would have to be planar, no?</div><br/><div id="40128602" class="c"><input type="checkbox" id="c-40128602" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#40127795">root</a><span>|</span><a href="#40127962">parent</a><span>|</span><a href="#40129255">next</a><span>|</span><label class="collapse" for="c-40128602">[-]</label><label class="expand" for="c-40128602">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have a language for it... I&#x27;ve been stuck at analysis paralysis for far, far too long on this one. The code I did write, was all figured out by hand.<p>I&#x27;m thinking it&#x27;ll have to end up being a set of equations, much like the tables that get spewed when you compile VHDL for an FPGA.</div><br/></div></div></div></div></div></div><div id="40129255" class="c"><input type="checkbox" id="c-40129255" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#40127795">prev</a><span>|</span><a href="#40127980">next</a><span>|</span><label class="collapse" for="c-40129255">[-]</label><label class="expand" for="c-40129255">[1 more]</label></div><br/><div class="children"><div class="content">Meanwhile others are trying to bring back analog CPUs for similar kinds of workloads, it is going to be interesting which ones end up winning in the long run.<p>For example, <a href="https:&#x2F;&#x2F;mythic.ai&#x2F;products&#x2F;m1076-analog-matrix-processor&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mythic.ai&#x2F;products&#x2F;m1076-analog-matrix-processor&#x2F;</a></div><br/></div></div><div id="40127980" class="c"><input type="checkbox" id="c-40127980" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#40129255">prev</a><span>|</span><a href="#40129652">next</a><span>|</span><label class="collapse" for="c-40127980">[-]</label><label class="expand" for="c-40127980">[1 more]</label></div><br/><div class="children"><div class="content">Is there any small risc-v soft-core with big ass SVE (scalable vector extensions)? I would like to play around with them but the only option seem to be cloud instances like gravitron and small (128bit) like the licheerv nano (c906) which also seems to only support a beta version of the standard.</div><br/></div></div><div id="40129652" class="c"><input type="checkbox" id="c-40129652" checked=""/><div class="controls bullet"><span class="by">hoseja</span><span>|</span><a href="#40127980">prev</a><span>|</span><a href="#40125882">next</a><span>|</span><label class="collapse" for="c-40129652">[-]</label><label class="expand" for="c-40129652">[1 more]</label></div><br/><div class="children"><div class="content">Have them self-modify.</div><br/></div></div><div id="40125882" class="c"><input type="checkbox" id="c-40125882" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40129652">prev</a><span>|</span><a href="#40127061">next</a><span>|</span><label class="collapse" for="c-40125882">[-]</label><label class="expand" for="c-40125882">[7 more]</label></div><br/><div class="children"><div class="content">Dangs got some work cut out for him tonight</div><br/><div id="40125904" class="c"><input type="checkbox" id="c-40125904" checked=""/><div class="controls bullet"><span class="by">ronsor</span><span>|</span><a href="#40125882">parent</a><span>|</span><a href="#40127283">next</a><span>|</span><label class="collapse" for="c-40125904">[-]</label><label class="expand" for="c-40125904">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve never seen so much spam in a thread before.</div><br/><div id="40126913" class="c"><input type="checkbox" id="c-40126913" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#40125882">root</a><span>|</span><a href="#40125904">parent</a><span>|</span><a href="#40127283">next</a><span>|</span><label class="collapse" for="c-40126913">[-]</label><label class="expand" for="c-40126913">[2 more]</label></div><br/><div class="children"><div class="content">Its across everything on the frontpage. Hundreds of spam links.<p>It looks very much like a concerted attack of some kind. The spam seems quite ineffective, so I&#x27;m wondering if the &quot;real&quot; attack is trying to get the moderators to hell-ban a legitimate person to silence them or something.<p>I dunno, its really weird. Its hard for me to take this spam at face value, I can&#x27;t imagine that this website is a good target for such spam. Its clearly just here to annoy the moderators and make them tired vs... something. Reminds me of Starcraft Muta Harass: you&#x27;re attacking the APM and attention-span of the opponent, not actually trying to straight up win with Muta. If that makes any sense.</div><br/><div id="40126965" class="c"><input type="checkbox" id="c-40126965" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#40125882">root</a><span>|</span><a href="#40126913">parent</a><span>|</span><a href="#40127283">next</a><span>|</span><label class="collapse" for="c-40126965">[-]</label><label class="expand" for="c-40126965">[1 more]</label></div><br/><div class="children"><div class="content">Or it&#x27;s some kind of weird-ass SEO campaign (not that it&#x27;ll work, dang has implemented rel=nofollow a few months ago). Man, I&#x27;ve been on HN for &gt;11 years now, literally never saw a spam campaign this intensive.</div><br/></div></div></div></div></div></div><div id="40127283" class="c"><input type="checkbox" id="c-40127283" checked=""/><div class="controls bullet"><span class="by">cabbageears</span><span>|</span><a href="#40125882">parent</a><span>|</span><a href="#40125904">prev</a><span>|</span><a href="#40126923">next</a><span>|</span><label class="collapse" for="c-40127283">[-]</label><label class="expand" for="c-40127283">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    function modifyElements(pSel, cSel, rxStr) {
        const regex = new RegExp(rxStr, &#x27;i&#x27;);
        const pEls = document.querySelectorAll(pSel);
        pEls.forEach(pEl =&gt; {
            const fEl = pEl.querySelector(cSel);
            if (fEl &amp;&amp; regex.test(fEl.textContent)) {
                pEl.style.display = &#x27;none&#x27;;
            }
        });
    }
    
    let rx = &#x2F;(hi are u lonely|want (an )?ai gf?)&#x2F;i;
    
    modifyElements(&quot;.athing.comtr&quot;, &quot;.comment&quot;, rx);</code></pre></div><br/></div></div><div id="40126923" class="c"><input type="checkbox" id="c-40126923" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#40125882">parent</a><span>|</span><a href="#40127283">prev</a><span>|</span><a href="#40127061">next</a><span>|</span><label class="collapse" for="c-40126923">[-]</label><label class="expand" for="c-40126923">[2 more]</label></div><br/><div class="children"><div class="content">Given they all link to Discord, I wonder if @dang should consider contacting their abuse team to investigate and take down those channels.</div><br/><div id="40127860" class="c"><input type="checkbox" id="c-40127860" checked=""/><div class="controls bullet"><span class="by">Rexxar</span><span>|</span><a href="#40125882">root</a><span>|</span><a href="#40126923">parent</a><span>|</span><a href="#40127061">next</a><span>|</span><label class="collapse" for="c-40127860">[-]</label><label class="expand" for="c-40127860">[1 more]</label></div><br/><div class="children"><div class="content">Maybe it&#x27;s exactly what they want (I didn&#x27;t check the links). Maybe they want to make someone they don&#x27;t like banned.</div><br/></div></div></div></div></div></div><div id="40127061" class="c"><input type="checkbox" id="c-40127061" checked=""/><div class="controls bullet"><span class="by">ZoomerCretin</span><span>|</span><a href="#40125882">prev</a><span>|</span><a href="#40125761">next</a><span>|</span><label class="collapse" for="c-40127061">[-]</label><label class="expand" for="c-40127061">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad to see this is being studied. I did a brief half-semester project summarizing the usefulness of accelerators after the death of Dennard&#x27;s law and Moore&#x27;s law (this was when Intel was still pushing out incremental improvements on 14nm). The short summary is that accelerators offered substantial performance per watt improvement, at the cost of longer development timelines, cost of manufacturing in the case of ASICs and ASIPs, and the inability to fit large algorithms on FPGAs. Though Microsoft found a way to daisy-chain FPGAs for Bing&#x27;s page rank, it just didn&#x27;t produce the order-of-magnitude improvement necessary to justify moving to FPGAs. With smaller algorithms or bigger FPGAs or occasional operation offloading (like what we do now with TPUs and GPUs), they could be good candidates for accelerators.<p>Of course the real killer is how easy it is(n&#x27;t) to develop for them. AMD offers much better performance per dollar than Nvidia, but of course their poor drivers make using their hardware a fool&#x27;s errand.<p>RADs sound like a good idea. We should be making reprogrammable hardware available and easy to use for everyone.</div><br/><div id="40128965" class="c"><input type="checkbox" id="c-40128965" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#40127061">parent</a><span>|</span><a href="#40128807">next</a><span>|</span><label class="collapse" for="c-40128965">[-]</label><label class="expand" for="c-40128965">[1 more]</label></div><br/><div class="children"><div class="content">Brainwave is&#x2F;was very successful? Still serving your searches! 
(I work on one of the teams that came out of brainwave, not with them directly)<p><a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;project&#x2F;project-brainwave&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;project&#x2F;project-bra...</a><p><a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;publication&#x2F;a-configurable-cloud-scale-dnn-processor-for-real-time-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;publication&#x2F;a-confi...</a><p><a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;blog&#x2F;a-microsoft-custom-data-type-for-efficient-inference&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;blog&#x2F;a-microsoft-cu...</a> -&gt; This led to the OCP MX work:<p><a href="https:&#x2F;&#x2F;www.opencompute.org&#x2F;blog&#x2F;amd-arm-intel-meta-microsoft-nvidia-and-qualcomm-standardize-next-generation-narrow-precision-data-formats-for-ai" rel="nofollow">https:&#x2F;&#x2F;www.opencompute.org&#x2F;blog&#x2F;amd-arm-intel-meta-microsof...</a></div><br/></div></div><div id="40128807" class="c"><input type="checkbox" id="c-40128807" checked=""/><div class="controls bullet"><span class="by">15155</span><span>|</span><a href="#40127061">parent</a><span>|</span><a href="#40128965">prev</a><span>|</span><a href="#40125761">next</a><span>|</span><label class="collapse" for="c-40128807">[-]</label><label class="expand" for="c-40128807">[1 more]</label></div><br/><div class="children"><div class="content">&gt; AMD offers much better performance per dollar than Nvidia, but of course their poor drivers make using their hardware a fool&#x27;s errand.<p>I would never use their IP or drivers, but Xilinx hardware is top notch. What drivers are you using? XRT?</div><br/></div></div></div></div></div></div></div></div></div></body></html>