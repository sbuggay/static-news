<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720688470030" as="style"/><link rel="stylesheet" href="styles.css?v=1720688470030"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://vlmsareblind.github.io/">Vision language models are blind</a> <span class="domain">(<a href="https://vlmsareblind.github.io">vlmsareblind.github.io</a>)</span></div><div class="subtext"><span>taesiri</span> | <span>143 comments</span></div><br/><div><div id="40929002" class="c"><input type="checkbox" id="c-40929002" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#40930322">next</a><span>|</span><label class="collapse" for="c-40929002">[-]</label><label class="expand" for="c-40929002">[49 more]</label></div><br/><div class="children"><div class="content">Entertaining, but I think the conclusion is way off.<p>&gt; their vision is, at best, like that of a person with myopia seeing fine details as blurry<p>is a crazy thing to write in an abstract. Did they try to probe that hypothesis at all? I could (well actually I can&#x27;t) share some examples from my job of GPT-4v doing some pretty difficult fine-grained visual tasks that invalidate this.<p>Personally, I rate this paper [1], which makes the argument that these huge GenAI models are pretty good at things - <i>assuming that it has seen a LOT of that type of data during training</i> (which is true of a great many things). If you make up tasks like this, then yes can be REALLY bad at them, and initial impressions of AGI get harder to justify. But in practice, we aren&#x27;t just making up tasks to trip up these models. They can be very performant on some tasks and the authors have not presented any real evidence about these two modes.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.04125" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.04125</a></div><br/><div id="40929161" class="c"><input type="checkbox" id="c-40929161" checked=""/><div class="controls bullet"><span class="by">SomaticPirate</span><span>|</span><a href="#40929002">parent</a><span>|</span><a href="#40934753">next</a><span>|</span><label class="collapse" for="c-40929161">[-]</label><label class="expand" for="c-40929161">[26 more]</label></div><br/><div class="children"><div class="content">There are quite a few &quot;ai apologists&quot; in the comments but I think the title is fair when these models are marketed towards low vision people (&quot;Be my eyes&quot; <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Zq710AKC1gg" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Zq710AKC1gg</a>) as the equivalent to human vision. These models are implied to be human level equivalents when they are not.<p>This paper demonstrates that there are still some major gaps where simple problems confound the models in unexpected ways. These is important work to elevate otherwise people may start to believe that these models are suitable for general application when they still need safeguards and copious warnings.</div><br/><div id="40931595" class="c"><input type="checkbox" id="c-40931595" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40929161">parent</a><span>|</span><a href="#40930121">next</a><span>|</span><label class="collapse" for="c-40931595">[-]</label><label class="expand" for="c-40931595">[3 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re throwing &quot;citation needed&quot; tags on stuff, how about the first sentence?<p>&quot;Large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini-1.5 Pro are powering countless image-text processing applications&quot;<p>I don&#x27;t know how many a &quot;countless&quot; is, but I think we&#x27;ve gotten really sloppy in terms of what counts for LLMs as a demonstrated, durable win in a concrete task attached to well-measured outcomes and holding up over even modest periods of time.<p>This stuff is really promising and lots of builders are making lots of nifty things, so if that counts as an application then maybe we&#x27;re at countless, but in the enterprise and in government and in refereed academic literature we seem to be at the proof-of-concept phase. Impressive chat bots as a use case are pretty dialed in, enough people claim that they help with coding that I tend to believe it&#x27;s a real thing (I never seem to come out ahead of going directly to the source, StackOverflow).<p>The amount of breathless press on this seems &quot;countless&quot;, so maybe I missed the totally rigorous case study on how X company became Y percent more profitable by doing Z thing with LLMs (or similar), and if so I&#x27;d be grateful for citations, but neither Google nor any of the big models seem to know about it.</div><br/><div id="40933195" class="c"><input type="checkbox" id="c-40933195" checked=""/><div class="controls bullet"><span class="by">dsr_</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40931595">parent</a><span>|</span><a href="#40934139">next</a><span>|</span><label class="collapse" for="c-40933195">[-]</label><label class="expand" for="c-40933195">[1 more]</label></div><br/><div class="children"><div class="content">&quot;maybe I missed the totally rigorous case study on how X company became Y percent more profitable by doing Z thing with LLMs (or similar), and if so I&#x27;d be grateful for citations, but neither Google nor any of the big models seem to know about it.&quot;<p>Goldman Sachs recently issued a report.<p><a href="https:&#x2F;&#x2F;www.goldmansachs.com&#x2F;intelligence&#x2F;pages&#x2F;gs-research&#x2F;gen-ai-too-much-spend-too-little-benefit&#x2F;report.pdf" rel="nofollow">https:&#x2F;&#x2F;www.goldmansachs.com&#x2F;intelligence&#x2F;pages&#x2F;gs-research&#x2F;...</a><p>&quot;We estimate that the AI infrastructure buildout will cost over
$1tn in the next several years alone, which includes spending
on data centers, utilities, and applications. So, the crucial
question is: What $1tn problem will AI solve? Replacing low-
wage jobs with tremendously costly technology is basically the
polar opposite of the prior technology transitions I’ve witnessed
in my thirty years of closely following the tech industry&quot;</div><br/></div></div></div></div><div id="40930121" class="c"><input type="checkbox" id="c-40930121" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40929161">parent</a><span>|</span><a href="#40931595">prev</a><span>|</span><a href="#40932212">next</a><span>|</span><label class="collapse" for="c-40930121">[-]</label><label class="expand" for="c-40930121">[3 more]</label></div><br/><div class="children"><div class="content">Yea, really if you look at human learning&#x2F;seeing&#x2F;acting there is a feedback loop that LLM for example isn&#x27;t able to complete and train on.<p>You see an object. First you have to learn how to control all your body functions to move toward it and grasp it. This teaches you about the 3 dimensional world and things like gravity. You may not know the terms, but it is baked in your learning model. After you get an object you start building a classification list &quot;hot&quot;, &quot;sharp&quot;, &quot;soft and fuzzy&quot;, &quot;tasty&quot;, &quot;slick&quot;. Your learning model builds up a list of properties of objects and &quot;expected&quot; properties of objects.<p>Once you have this &#x27;database&#x27; you create as a human, you can apply the logic to achieve tasks. &quot;Walk 10 feet forward, but avoid the sharp glass just to the left&quot;. You have to have spatial awareness, object awareness, and prediction ability.<p>Models &#x27;kind of&#x27; have this, but its seemingly haphazard, kind of like a child that doesn&#x27;t know how to put all the pieces together yet. I think a lot of embodied robot testing where the embodied model feeds back training to the LLM&#x2F;vision model will have to occur before this is even somewhat close to reliable.</div><br/><div id="40934102" class="c"><input type="checkbox" id="c-40934102" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40930121">parent</a><span>|</span><a href="#40932212">next</a><span>|</span><label class="collapse" for="c-40934102">[-]</label><label class="expand" for="c-40934102">[2 more]</label></div><br/><div class="children"><div class="content">Embodied is useful, but I think not necessary <i>even if</i> you need learning in a 3D environment. <i>Synthesized embodiment</i> should be enough. While in some cases[0] it may have problems with fidelity, simulating embodied experience <i>in silico</i> scales much better, and more importantly, <i>we have control over time flow</i>. Humans always learn in real-time, while with simulated embodiment, we could cram <i>years</i> of subjective-time experiences into a model in seconds, and then for novel scenarios, spend an hour per each second of subjective time running a high-fidelity physics simulation[1].<p>--<p>[0] - Like if you plugged a 3D game engine into the training loop.<p>[1] - Results of which we could hopefully reuse in training later. And yes, a simulation could itself be a recording of carefully executed experiment in real world.</div><br/><div id="40934700" class="c"><input type="checkbox" id="c-40934700" checked=""/><div class="controls bullet"><span class="by">pegasus</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40934102">parent</a><span>|</span><a href="#40932212">next</a><span>|</span><label class="collapse" for="c-40934700">[-]</label><label class="expand" for="c-40934700">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Like if you plugged a 3D game engine into the training loop<p>Isn&#x27;t this what <i>synthesized embodiment</i> basically always is? As long as the application of the resulting technology is in a restricted, well controlled environment, as is the case for example for an assembly-line robot, this is a great strategy.  But I expect fidelity problems will make this technique ultimately a bad idea for anything that&#x27;s supposed to interact with humans. Like self-driving cars, for example.  Unless, again, those self-driving cars are segregated in dedicated lanes.</div><br/></div></div></div></div></div></div><div id="40932212" class="c"><input type="checkbox" id="c-40932212" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40929161">parent</a><span>|</span><a href="#40930121">prev</a><span>|</span><a href="#40933944">next</a><span>|</span><label class="collapse" for="c-40932212">[-]</label><label class="expand" for="c-40932212">[12 more]</label></div><br/><div class="children"><div class="content">I disagree. I think the title, abstract, and conclusion not only misrepresents the state of the models but it misrepresents Thier own findings.<p>They have identified a class of problems that the models perform poorly at and have given a good description of the failure.  They portray this as a representative example of the behaviour in general.  This has not been shown and is probably not true.<p>I don&#x27;t think that models have been portrayed as equivalent to humans.  Like most AI in it has been shown as vastly superior in some areas and profoundly ignorant in others.   Media can overblow things and enthusiasts can talk about future advances as if they have already arrived, but I don&#x27;t think these are typical portayals by the AI Field in general.</div><br/><div id="40932288" class="c"><input type="checkbox" id="c-40932288" checked=""/><div class="controls bullet"><span class="by">youssefabdelm</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40932212">parent</a><span>|</span><a href="#40932685">next</a><span>|</span><label class="collapse" for="c-40932288">[-]</label><label class="expand" for="c-40932288">[10 more]</label></div><br/><div class="children"><div class="content">Exactly... I&#x27;ve found GPT-4o to be good at OCR for instance... doesn&#x27;t seem &quot;blind&quot; to me.</div><br/><div id="40934489" class="c"><input type="checkbox" id="c-40934489" checked=""/><div class="controls bullet"><span class="by">prmoustache</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40932288">parent</a><span>|</span><a href="#40934176">next</a><span>|</span><label class="collapse" for="c-40934489">[-]</label><label class="expand" for="c-40934489">[1 more]</label></div><br/><div class="children"><div class="content">Well maybe not blind but the analogy with myopia might stand.<p>For exemple in the case of OCR, a person with myopia will usually be able to make up letters and words even without his glasses based on his expectation (similar to vlm training) of seeing letters and words in, say, a sign. He might not see them all clearly and do some errors but might recognize some letters easily and make up the rest based on context, words recognition, etc. Basically experience.<p>I also have a funny anecdote about my partner, which has sever myopia, who once found herself outside her house without her glasses on, and saw something on the grass right in front. She told her then brother in law &quot;look, a squirrel&quot; Only for the &quot;squirrel&quot; to take off while shouting its typical caws. It was a crow. This is typical of VLM&#x27;s hallucinations.</div><br/></div></div><div id="40934176" class="c"><input type="checkbox" id="c-40934176" checked=""/><div class="controls bullet"><span class="by">Foobar8568</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40932288">parent</a><span>|</span><a href="#40934489">prev</a><span>|</span><a href="#40932651">next</a><span>|</span><label class="collapse" for="c-40934176">[-]</label><label class="expand" for="c-40934176">[2 more]</label></div><br/><div class="children"><div class="content">I know that GPT-4o is fairly poor to recognize music sheets and notes. 
Totally off the marks, more often than not, even the first note is not recognize on a first week solfège book.<p>So unless I missed something but as far as I am concerned, they are optimized for benchmarks.<p>So while I enjoy gen AI, image-to-text is highly subpart.</div><br/><div id="40934206" class="c"><input type="checkbox" id="c-40934206" checked=""/><div class="controls bullet"><span class="by">youssefabdelm</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40934176">parent</a><span>|</span><a href="#40932651">next</a><span>|</span><label class="collapse" for="c-40934206">[-]</label><label class="expand" for="c-40934206">[1 more]</label></div><br/><div class="children"><div class="content">Useful to know, thank you!</div><br/></div></div></div></div><div id="40932651" class="c"><input type="checkbox" id="c-40932651" checked=""/><div class="controls bullet"><span class="by">spookie</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40932288">parent</a><span>|</span><a href="#40934176">prev</a><span>|</span><a href="#40932320">next</a><span>|</span><label class="collapse" for="c-40932651">[-]</label><label class="expand" for="c-40932651">[5 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t really need a LLM for OCR. Hell, I suppose they just run a python script in its VM and rephrase the output.<p>At least that&#x27;s what I would do. Perhaps the script would be a &quot;specialist model&quot; in a sense.</div><br/><div id="40932896" class="c"><input type="checkbox" id="c-40932896" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40932651">parent</a><span>|</span><a href="#40932320">next</a><span>|</span><label class="collapse" for="c-40932896">[-]</label><label class="expand" for="c-40932896">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not that you need an LLM for OCR but the fact that an LLM can do OCR (and handwriting recognition which is much harder) despite not being made specifically for that purpose is indicative of something. The jump from knowing &quot;this is a picture of a paper with writing on it&quot; like what you get with CLIP to being able to reproduce what&#x27;s on the paper is, to me, close enough to seeing that the difference isn&#x27;t meaningful anymore.</div><br/><div id="40933040" class="c"><input type="checkbox" id="c-40933040" checked=""/><div class="controls bullet"><span class="by">acheong08</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40932896">parent</a><span>|</span><a href="#40932320">next</a><span>|</span><label class="collapse" for="c-40933040">[-]</label><label class="expand" for="c-40933040">[3 more]</label></div><br/><div class="children"><div class="content">GPT-4v is provided with OCR</div><br/><div id="40933245" class="c"><input type="checkbox" id="c-40933245" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40933040">parent</a><span>|</span><a href="#40933109">next</a><span>|</span><label class="collapse" for="c-40933245">[-]</label><label class="expand" for="c-40933245">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a common misconception.<p>Sometimes if you upload an image to ChatGPT and ask for OCR it will run Python code that executes Tesseract, but that&#x27;s effectively a bug: GPT-4 vision works much better than that, and it will use GPT-4 vision if you tell it &quot;don&#x27;t use Python&quot; or similar.</div><br/></div></div><div id="40933109" class="c"><input type="checkbox" id="c-40933109" checked=""/><div class="controls bullet"><span class="by">letmevoteplease</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40933040">parent</a><span>|</span><a href="#40933245">prev</a><span>|</span><a href="#40932320">next</a><span>|</span><label class="collapse" for="c-40933109">[-]</label><label class="expand" for="c-40933109">[1 more]</label></div><br/><div class="children"><div class="content">No reason to believe that. Open source VLMs can do OCR.[1]<p>[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;opencompass&#x2F;open_vlm_leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;opencompass&#x2F;open_vlm_leaderboa...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="40932685" class="c"><input type="checkbox" id="c-40932685" checked=""/><div class="controls bullet"><span class="by">subroutine</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40932212">parent</a><span>|</span><a href="#40932288">prev</a><span>|</span><a href="#40933944">next</a><span>|</span><label class="collapse" for="c-40932685">[-]</label><label class="expand" for="c-40932685">[1 more]</label></div><br/><div class="children"><div class="content">I think the conclusion of the paper is far more mundane. It&#x27;s curious that VLM can recognize complex novel objects in a trained category, but cannot perform basic visual tasks that human toddlers can perform (e.g. recognizing when two lines intersect or when two circles overlap). Nevertheless I&#x27;m sure these models can explain in great detail what intersecting lines are, and even what they look like. So while LLMs might have image processing capabilities, they clearly do not see the way humans <i>see</i>. That, I think, would be a more apt title for their abstract.</div><br/></div></div></div></div><div id="40933944" class="c"><input type="checkbox" id="c-40933944" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40929161">parent</a><span>|</span><a href="#40932212">prev</a><span>|</span><a href="#40929272">next</a><span>|</span><label class="collapse" for="c-40933944">[-]</label><label class="expand" for="c-40933944">[1 more]</label></div><br/><div class="children"><div class="content">Simple is a relative statement.  There are vision problems where monkeys are far better than humans.  Some may look at human vision and memory and think that we lack basic skills.<p>With AINwe are creating intelligence but with different strengths and weaknesses.  I think we will continue to be surprised at how well they work on some problems and how poor they do at some  “simple” ones.</div><br/></div></div><div id="40929272" class="c"><input type="checkbox" id="c-40929272" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40929161">parent</a><span>|</span><a href="#40933944">prev</a><span>|</span><a href="#40934075">next</a><span>|</span><label class="collapse" for="c-40929272">[-]</label><label class="expand" for="c-40929272">[3 more]</label></div><br/><div class="children"><div class="content">The paper I linked should hopefully mark me out as far from an AI apologist, it&#x27;s actually really bad news for GenAI if correct. All I mean to say is the clickbait conclusion and the evidence do not match up.</div><br/><div id="40931494" class="c"><input type="checkbox" id="c-40931494" checked=""/><div class="controls bullet"><span class="by">Melomololotolo</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40929272">parent</a><span>|</span><a href="#40934075">next</a><span>|</span><label class="collapse" for="c-40931494">[-]</label><label class="expand" for="c-40931494">[2 more]</label></div><br/><div class="children"><div class="content">We have started the ara of ai.<p>It really doesn&#x27;t matter how good current llms are.<p>They have been good enough to start this ara.<p>And no it&#x27;s not and never has been just llms. Look what Nvidia is doing with ml.<p>Whisper huge advantage, segment anything again huge. Alpha fold 2 again huge.<p>All the robot announcements -&gt; huge<p>I doubt we will reach agi just through llms. We will reach agi through multi modal, mix of experts, some kind of feedback loop, etc.<p>But the stone started to roll.<p>And you know I prefer to hear about ai advantages for the next 10-30 years. That&#x27;s a lot better than the crypto shit we had the last 5 years.</div><br/><div id="40933694" class="c"><input type="checkbox" id="c-40933694" checked=""/><div class="controls bullet"><span class="by">zwnow</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40931494">parent</a><span>|</span><a href="#40934075">next</a><span>|</span><label class="collapse" for="c-40933694">[-]</label><label class="expand" for="c-40933694">[1 more]</label></div><br/><div class="children"><div class="content">We won&#x27;t reach agi in our lifetimes.</div><br/></div></div></div></div></div></div><div id="40934075" class="c"><input type="checkbox" id="c-40934075" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40929161">parent</a><span>|</span><a href="#40929272">prev</a><span>|</span><a href="#40931393">next</a><span>|</span><label class="collapse" for="c-40934075">[-]</label><label class="expand" for="c-40934075">[1 more]</label></div><br/><div class="children"><div class="content">Be My Eyes user here.  I disagree with your uninformed opinion.  Be My Eyes is more often than not more useful then a human.  And I am reporting from personal experience.  What experience do you have?</div><br/></div></div><div id="40931393" class="c"><input type="checkbox" id="c-40931393" checked=""/><div class="controls bullet"><span class="by">Melomololotolo</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40929161">parent</a><span>|</span><a href="#40934075">prev</a><span>|</span><a href="#40934753">next</a><span>|</span><label class="collapse" for="c-40931393">[-]</label><label class="expand" for="c-40931393">[2 more]</label></div><br/><div class="children"><div class="content">Ah yes the blind person who constantly needs to know if two lines intersect.<p>Let&#x27;s just ignore what a blind person normally needs to know.<p>You know what blind people ask? Sometimes there daily routine is broken because there is some type of construction and models can tell you this.<p>Sometimes they need to read a basic sign and models can do this.<p>Those models help people already and they will continue to get better.<p>I&#x27;m not sure if I&#x27;m more frustrated how condescending the authors are or your ignorance.<p>Valid criticism doesn&#x27;t need to be shitty</div><br/><div id="40931940" class="c"><input type="checkbox" id="c-40931940" checked=""/><div class="controls bullet"><span class="by">shagie</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40931393">parent</a><span>|</span><a href="#40934753">next</a><span>|</span><label class="collapse" for="c-40931940">[-]</label><label class="expand" for="c-40931940">[1 more]</label></div><br/><div class="children"><div class="content">As an aside... from 2016 this is what was a valid use case for a blind person with an app.<p>Seeing AI 2016 Prototype - A Microsoft research project  - <a href="https:&#x2F;&#x2F;youtu.be&#x2F;R2mC-NUAmMk" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;R2mC-NUAmMk</a><p><a href="https:&#x2F;&#x2F;www.seeingai.com" rel="nofollow">https:&#x2F;&#x2F;www.seeingai.com</a> are the actual working apps.<p>The version from 2016 I recall showing (pun not intended) to a coworker who had some significant vision impairments and he was <i>really</i> excited about what it could do back then.<p>---<p>I still remain quite impressed with its ability to parse the picture and likely reason behind it <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;JZBTk2t" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;JZBTk2t</a></div><br/></div></div></div></div></div></div><div id="40934753" class="c"><input type="checkbox" id="c-40934753" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#40929002">parent</a><span>|</span><a href="#40929161">prev</a><span>|</span><a href="#40934130">next</a><span>|</span><label class="collapse" for="c-40934753">[-]</label><label class="expand" for="c-40934753">[1 more]</label></div><br/><div class="children"><div class="content">&gt; these huge GenAI models are pretty good at things<p>Is this the sales pitch though? Because 15 years ago, I had a scanner with an app that can scan a text document and produce the text on Windows. The machine had something like 256Mb of RAM.<p>Tech can be <i>extremely</i> good at niches in isolation. You can have an OCR system 10 years ago and it&#x27;ll be extremely reliable at the single task it&#x27;s configured to do.<p>AI is supposed to bring a new paradigm, where the tech is not limited to the specific niche the developers have scoped it to. However, if it <i>reliably</i> fails to detect simple things a regular person should not get wrong, then the whole value proposition is kicked out of the window.</div><br/></div></div><div id="40934130" class="c"><input type="checkbox" id="c-40934130" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40929002">parent</a><span>|</span><a href="#40934753">prev</a><span>|</span><a href="#40930013">next</a><span>|</span><label class="collapse" for="c-40934130">[-]</label><label class="expand" for="c-40934130">[1 more]</label></div><br/><div class="children"><div class="content">Entertaining is indeed the right word. Nice job identifying corner cases of models&#x27; visual processing; curiously, they&#x27;re not far conceptually from some optical illusions that reliably trip humans up. But to call the models &quot;blind&quot; or imply their low performance in general? That&#x27;s <i>trivially invalidated</i> by just <i>taking your phone out and feeding a photo to ChatGPT app</i>.<p>Like, seriously. One poster below whines about &quot;AI apologists&quot; and BeMyEyes, but again, it&#x27;s all trivially testable with your phone and $20&#x2F;month subscription. It works spectacularly well on <i>real world tasks</i>. Not perfectly, sure, but good enough to be useful <i>in practice</i> and better than alternatives (which often don&#x27;t exist).</div><br/></div></div><div id="40930013" class="c"><input type="checkbox" id="c-40930013" checked=""/><div class="controls bullet"><span class="by">FrenchDevRemote</span><span>|</span><a href="#40929002">parent</a><span>|</span><a href="#40934130">prev</a><span>|</span><a href="#40930749">next</a><span>|</span><label class="collapse" for="c-40930013">[-]</label><label class="expand" for="c-40930013">[5 more]</label></div><br/><div class="children"><div class="content">&gt; their vision is, at best, like that of a person with myopia seeing fine details as blurry<p>It&#x27;s not that far from reality, most models sees images in very low resolution&#x2F;limited colors,
so not so far from this description</div><br/><div id="40930482" class="c"><input type="checkbox" id="c-40930482" checked=""/><div class="controls bullet"><span class="by">vikramkr</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40930013">parent</a><span>|</span><a href="#40933085">next</a><span>|</span><label class="collapse" for="c-40930482">[-]</label><label class="expand" for="c-40930482">[2 more]</label></div><br/><div class="children"><div class="content">They didn&#x27;t test that claim at all though. Vision isn&#x27;t some sort of 1D sliding scale with every vision condition lying along one axis.<p>First of all myopia isn&#x27;t &#x27;seeing fine details as blurry&#x27; - it&#x27;s nearsightedness - and whatever else this post tested it definitely didn&#x27;t test depth perception.<p>And second - inability to see fine details is a distinct&#x2F;different thing from not being able to count intersections and the other things tested here. That hypothesis, if valid, would imply that improving the resolution of the image that the model can process would improve its performance on these tasks even if reasoning abilities were the same. That - does not make sense. Plenty of the details in these images that these models are tripping up on are perfectly distinguishable at low resolutions. Counting rows and columns of blank grids is not going to improve with more resolution.<p>I mean, I&#x27;d argue that the phrasing of the hypothesis (&quot;At best, like that of a person with myopia&quot;) doesn&#x27;t make sense at all. I don&#x27;t think a person with myopia would have any trouble with these tasks if you zoomed into the relevant area, or held the image close. I have a very strong feeling that these models would continue to suffer on these tasks if you zoomed in. Nearsighted != unable to count squares.</div><br/><div id="40933697" class="c"><input type="checkbox" id="c-40933697" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40930482">parent</a><span>|</span><a href="#40933085">next</a><span>|</span><label class="collapse" for="c-40933697">[-]</label><label class="expand" for="c-40933697">[1 more]</label></div><br/><div class="children"><div class="content">It seems to me they&#x27;ve brought up myopia only to make it more approachable to people how blurry something is, implying they believe models work with a blurry image just like a nearsighted person sees blurry images at a distance.<p>While myopia is common, it&#x27;s not the best choice of analogy and &quot;blurry vision&quot; is probably clear enough.<p>Still, I&#x27;d only see it as a bad choice of analogy — I can&#x27;t imagine anyone mistaking optical focus problems for static image processing problems — so in the usual HN recommendation, I&#x27;d treat their example in the most favourable sense.</div><br/></div></div></div></div><div id="40930294" class="c"><input type="checkbox" id="c-40930294" checked=""/><div class="controls bullet"><span class="by">blackmesaind</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40930013">parent</a><span>|</span><a href="#40933085">prev</a><span>|</span><a href="#40930749">next</a><span>|</span><label class="collapse" for="c-40930294">[-]</label><label class="expand" for="c-40930294">[1 more]</label></div><br/><div class="children"><div class="content">My thoughts as well. I too would have trouble with the overlapping lines tests if all the images underwent convolution.</div><br/></div></div></div></div><div id="40930749" class="c"><input type="checkbox" id="c-40930749" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#40929002">parent</a><span>|</span><a href="#40930013">prev</a><span>|</span><a href="#40929102">next</a><span>|</span><label class="collapse" for="c-40930749">[-]</label><label class="expand" for="c-40930749">[3 more]</label></div><br/><div class="children"><div class="content">&gt;I could (well actually I can&#x27;t)<p>I like the idea that these models are so good at some sort of specific and secret bit of visual processing that things like “counting shapes” and “beating a coin toss for accuracy” shouldn’t be considered when evaluating them.</div><br/><div id="40931272" class="c"><input type="checkbox" id="c-40931272" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40930749">parent</a><span>|</span><a href="#40930810">next</a><span>|</span><label class="collapse" for="c-40931272">[-]</label><label class="expand" for="c-40931272">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are bad at counting things just in general. It’s hard to say whether the failures here are vision based or just an inherent weakness of the language model.</div><br/></div></div><div id="40930810" class="c"><input type="checkbox" id="c-40930810" checked=""/><div class="controls bullet"><span class="by">vikramkr</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40930749">parent</a><span>|</span><a href="#40931272">prev</a><span>|</span><a href="#40929102">next</a><span>|</span><label class="collapse" for="c-40930810">[-]</label><label class="expand" for="c-40930810">[1 more]</label></div><br/><div class="children"><div class="content">Those don&#x27;t really have anything to do with fine detail&#x2F;nearsightedness. What they measured is valid&#x2F;interesting - what they concluded is unrelated.</div><br/></div></div></div></div><div id="40933253" class="c"><input type="checkbox" id="c-40933253" checked=""/><div class="controls bullet"><span class="by">itkovian_</span><span>|</span><a href="#40929002">parent</a><span>|</span><a href="#40929102">prev</a><span>|</span><a href="#40931280">next</a><span>|</span><label class="collapse" for="c-40933253">[-]</label><label class="expand" for="c-40933253">[3 more]</label></div><br/><div class="children"><div class="content">I think gpt4o is probably doing some ocr as preprocessing. It&#x27;s not really controversial to say the vmls today don&#x27;t pick up fine grained details - we all know this. Can just look at the output of a vae to know this is true.</div><br/><div id="40933292" class="c"><input type="checkbox" id="c-40933292" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40933253">parent</a><span>|</span><a href="#40934320">next</a><span>|</span><label class="collapse" for="c-40933292">[-]</label><label class="expand" for="c-40933292">[1 more]</label></div><br/><div class="children"><div class="content">If so, it&#x27;s better than any other ocr on the market.<p>I think they just train it on a bunch of text.<p>Maybe counting squares in a grid was not probably considered important enough to train for.</div><br/></div></div><div id="40934320" class="c"><input type="checkbox" id="c-40934320" checked=""/><div class="controls bullet"><span class="by">_flux</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40933253">parent</a><span>|</span><a href="#40933292">prev</a><span>|</span><a href="#40931280">next</a><span>|</span><label class="collapse" for="c-40934320">[-]</label><label class="expand" for="c-40934320">[1 more]</label></div><br/><div class="children"><div class="content">Why do you think it&#x27;s probable? The much smaller llava that I can run in my consumer GPU can also do &quot;OCR&quot;, yet I don&#x27;t believe anyone has hidden any OCR engine inside llama.cpp.</div><br/></div></div></div></div><div id="40931280" class="c"><input type="checkbox" id="c-40931280" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40929002">parent</a><span>|</span><a href="#40933253">prev</a><span>|</span><a href="#40929050">next</a><span>|</span><label class="collapse" for="c-40931280">[-]</label><label class="expand" for="c-40931280">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Did they try to probe that hypothesis at all?<p>I think this is a communication issue and you&#x27;re being a bit myopic in your interpretation. It is clearly an analogy meant for communication and is not an actual hypothesis. Sure, they could have used a better analogy and they could have done other tests, but the paper still counters quite common claims (from researchers) about VLMs.<p>&gt; I could (well actually I can&#x27;t) share some examples from my job of GPT-4v doing some pretty difficult fine-grained visual tasks that invalidate this.<p>I find it hard to believe that there is no example you can give. It surely doesn&#x27;t have to be exactly your training data. If it is this good, surely you can create an example no problem. If you just don&#x27;t want to, that&#x27;s okay, but then don&#x27;t say it.<p>But I have further questions. Do you have complicated prompting? Or any prompt engineering? It sure does matter how robust these models are to prompting. There&#x27;s a huge difference between a model being able to accomplish a task and a model being able to perform a task in a non-very-specific environment. This is no different than something working in a tech demo and not in the hand of the user.<p>&gt; But in practice, we aren&#x27;t just making up tasks to trip up these models.<p>I see this sentiment quite often and it is baffling to me.<p>First off, these tasks are not clearly designed to trick these models. A model failing at a task is not suddenly &quot;designed to trick a model.&quot; Its common with the river crossing puzzles where they&#x27;re rewritten to be like &quot;all animals can fit in the boat.&quot; If that is &quot;designed to trick a model&quot;, then the model must be a stochastic parrot and not a generalist. It is very important that we test things where we do know the answer to because, unfortunately, we&#x27;re not clairvoyant and can&#x27;t test questions we don&#x27;t know the answer to. Which is the common case in the real world usage.<p>Second, so what if a test was designed to trick up a model? Shouldn&#x27;t we be determining when and where models fail? Is that not a critical question in understanding how to use them properly? This seems doubly important if they are tasks that humans don&#x27;t have challenges with them.<p>&gt; They can be very performant on some tasks and the authors have not presented any real evidence about these two modes.<p>I don&#x27;t think people are claiming that large models can&#x27;t be performant on some tasks. If they are, they&#x27;re rejecting trivially verifiable reality. But not every criticism and has to also contain positive points. There&#x27;s plenty of papers and a lot of hype already doing that. And if we&#x27;re going to be critical of anything, shouldn&#x27;t it be that the companies creating these models -- selling them, and even charging researchers to perform these types of experiments that the can and are used to improve their products -- should be much more clear about the limitations of their models? If we need balance, then I think there&#x27;s bigger fish to fry than Auburn and Alberta Universities.</div><br/><div id="40931890" class="c"><input type="checkbox" id="c-40931890" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40931280">parent</a><span>|</span><a href="#40933567">next</a><span>|</span><label class="collapse" for="c-40931890">[-]</label><label class="expand" for="c-40931890">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; I think this is a communication issue and you&#x27;re being a bit myopic in your interpretation. It is clearly an analogy meant for communication and is not an actual hypothesis.</i><p>I don&#x27;t know, words have meanings. If that&#x27;s a communication issue, it&#x27;s on part of the authors. To me, this wording in a what is supposed to be a research paper abstract clearly suggests the insufficient resolution as the cause. How else should I interpret it?<p><i>&gt; The shockingly poor performance of four state-of-the-art VLMs suggests their vision is, at best, like that of a person with myopia seeing fine details as blurry</i><p>And indeed, increasing the resolution is expensive, and the best VLMs have something like 1000x1000. But the low resolution is clearly not the issue here, and the authors don&#x27;t actually talk about it in the paper.<p><i>&gt;I find it hard to believe that there is no example you can give.</i><p>I&#x27;m not the person you&#x27;re answering to, but I actually lazily tried two of authors&#x27; examples in a less performant VLM (CogVLM), and was surprised it passed those, making me wonder whether I can trust their conclusions until I reproduce their results. LLMs and VLMs have all kinds of weird failure modes, it&#x27;s not a secret they fail at some trivial tasks and their behavior is still not well understood. But working with these models and narrowing it down is notoriously like trying to nail a jelly to the wall. If I was able to do this in a cursory check, what else is there? More than one research paper in this area is wrong from the start.</div><br/><div id="40932982" class="c"><input type="checkbox" id="c-40932982" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40931890">parent</a><span>|</span><a href="#40933567">next</a><span>|</span><label class="collapse" for="c-40932982">[-]</label><label class="expand" for="c-40932982">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t know, words have meanings.<p>That&#x27;s quite true. Words mean exactly what people agree upon them meaning. Which does not require everyone, or else slang wouldn&#x27;t exist. Nor the dictionary, which significantly lags. Regardless, I do not think this is even an unusual use of the word, though I agree the mention of myopia is. The usage makes sense if you consider that both myopic and resolution have more than a singular meaning.<p><pre><code>  Myopic:
  lacking in foresight or __discernment__ : narrow in perspective and without concern for broader implications

  Resolution:
  the process or capability of making distinguishable the individual parts of an object, closely adjacent optical images, or sources of light
</code></pre>
I agree that there are far better ways to communicate. But my main gripe is that they said it was &quot;their hypothesis.&quot; If reading the abstract as a whole, I find it an odd conclusion to come to. It doesn&#x27;t pair with the words that follow with blind guessing (and I am not trying to defend the abstract. It is a bad abstract). But if you read the intro and look at the context of their landing page, I find it quite difficult to come to this conclusion. It is poorly written, but it is still not hard to decode the key concepts the authors are trying to convey.<p>I feel the need to reiterate that language has 3 key aspects to it: the concept attempted to be conveyed, the words that concept is lossy encoded into, and the lossy decoding of the person interpreting it. Communication doesn&#x27;t work by you reading&#x2F;listening to words and looking up those words in a dictionary. Communication is a problem where you use words (context&#x2F;body language&#x2F;symbols&#x2F;etc) to decrease the noise and get the reciever to reasonably decode your intended message. And unfortunately we&#x27;re in a global world and many different factors, such as culture, greatly affect how one encodes and&#x2F;or decodes language. It only becomes more important to recognize the fuzziness around language here. Being more strict and leaning into the database view of language only leads to more errors.<p>&gt; But the low resolution is clearly not the issue here, and the authors don&#x27;t actually talk about it in the paper.<p>Because they didn&#x27;t claim that image size and sharpness was an issue. They claimed the VLM cannot resolve the images &quot;as if&quot; they were blurry. Determining what the VLM actually &quot;sees&quot; is quite challenging. And I&#x27;ll mention that arguably they did test some factors that relate to blurriness. Which is why I&#x27;m willing to overlook the poor analogy.<p>&gt; I actually lazily tried two of authors&#x27; examples in a less performant VLM (CogVLM), and was surprised it passed those<p>I&#x27;m not. Depending on the examples you pulled, 2 random ones passing isn&#x27;t unlikely given the results.<p>Something I generally do not like about these types of papers is that they often do not consider augmentations. Since these models tend to be quite sensitive to both the text (prompt) inputs and image inputs. This is quite common in generators in general. Even the way you load in and scale an image can have significant performance differences. I&#x27;ve seen significant differences in simple things like loading an image from numpy, PIL, tensorflow, or torch have different results. But I have to hand it to these authors, they looked at some of this. In the appendix they go through with confusion matrices and look at the factors that determine misses. They could have gone deeper and tried other things, but it is a more than reasonable amount of work for a paper.</div><br/></div></div></div></div><div id="40933567" class="c"><input type="checkbox" id="c-40933567" checked=""/><div class="controls bullet"><span class="by">ClumsyPilot</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40931280">parent</a><span>|</span><a href="#40931890">prev</a><span>|</span><a href="#40929050">next</a><span>|</span><label class="collapse" for="c-40933567">[-]</label><label class="expand" for="c-40933567">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Second, so what if a test was designed to trick up a model? Shouldn&#x27;t we be determining when and where models fail? Is that not a critical question in understanding how to use them properly?<p>People are rushing to build this AI into all kinds of products, and they actively don’t want to know where the problems are.<p>The real world outside is designed to trip up the model. Strange things happen all the time.<p>Because software developers have no governing body, no oaths of ethics and no spine someone will end up dead in a ditch from malfunctioning AI.</div><br/><div id="40934160" class="c"><input type="checkbox" id="c-40934160" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40933567">parent</a><span>|</span><a href="#40929050">next</a><span>|</span><label class="collapse" for="c-40934160">[-]</label><label class="expand" for="c-40934160">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>The real world outside is designed to trip up the model. Strange things happen all the time.</i><p>Counterpoint: real world is heavily sanitized towards things that don&#x27;t trip human visual perception up too much, or otherwise inconvenience us. ML models are trained on that, and for that. They&#x27;re <i>not</i> trained for dealing with synthetic images, that couldn&#x27;t possibly exist in reality, <i>and</i> designed to trip visual processing algorithms up.<p>Also:<p>&gt; <i>People are rushing to build this AI into all kinds of products, and they actively don’t want to know where the problems are.</i><p>Glass half-full (of gasoline) take: those products will trip over real-world problems, identifying them in the process, and the models will get better walking over the corpses of failed AI-get-rich-quick companies. The people involved may not want to know where the problems are, but by deploying the models, they&#x27;ll reveal those problems to all.<p>&gt; <i>Because software developers have no governing body, no oaths of ethics and no spine someone will end up dead in a ditch from malfunctioning AI.</i><p>That, unfortunately, I 100% agree with. Though AI isn&#x27;t special here - not giving a fuck kills people regardless of the complexity of software involved.</div><br/><div id="40934737" class="c"><input type="checkbox" id="c-40934737" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40929002">root</a><span>|</span><a href="#40934160">parent</a><span>|</span><a href="#40929050">next</a><span>|</span><label class="collapse" for="c-40934737">[-]</label><label class="expand" for="c-40934737">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They&#x27;re not trained for dealing with synthetic images, that couldn&#x27;t possibly exist in reality, and designed to trip visual processing algorithms up<p>Neither of these claims are true. ML is highly trained on synthetic images. In fact, synthetic data generation is the way forward for the scale is all you need people. And there are also loads of synthetic images out in the wild. Everything from line art to abstract nonsense. Just take a walk down town near the bars.<p>&gt; not giving a fuck kills people regardless of the complexity of software involved.<p>What has me the most frustrated is that this &quot;move fast break things and don&#x27;t bother cleaning up&quot; attitude is not only common in industry but also in academia. But these two are incredibly intertwined these days and it&#x27;s hard to publish without support from industry because people only evaluate on benchmarks. And if you&#x27;re going to hack your benchmarks, you just throw a shit ton of compute at it. Who cares where the metrics fail?</div><br/></div></div></div></div></div></div></div></div><div id="40929050" class="c"><input type="checkbox" id="c-40929050" checked=""/><div class="controls bullet"><span class="by">diwank</span><span>|</span><a href="#40929002">parent</a><span>|</span><a href="#40931280">prev</a><span>|</span><a href="#40932524">next</a><span>|</span><label class="collapse" for="c-40929050">[-]</label><label class="expand" for="c-40929050">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I think their findings are def interesting but the title and the strong claims are a tad hyperbolic.</div><br/></div></div></div></div><div id="40930322" class="c"><input type="checkbox" id="c-40930322" checked=""/><div class="controls bullet"><span class="by">jetrink</span><span>|</span><a href="#40929002">prev</a><span>|</span><a href="#40932934">next</a><span>|</span><label class="collapse" for="c-40930322">[-]</label><label class="expand" for="c-40930322">[4 more]</label></div><br/><div class="children"><div class="content">I had a remarkable experience with GPT-4o yesterday. Our garage door started to fall down recently, so I inspected it and found that our landlord had installed the wire rope clips incorrectly, leading to the torsion cables losing tension. I didn&#x27;t know what that piece of hardware was called, so I asked ChatGPT and it identified the part as I expected it to. As a test, I asked if there was anything notable about the photo. ChatGPT correctly identified that the cables were installed backwards, with the side of the cable that was (previously) under tension on top of the slack end, instead of sandwiched securely in the middle. To diagnose that requires tracing the cable through space and inferring which end is under tension from the geometry, though I can&#x27;t rule out an educated guess.<p>What was really remarkable though was that it failed to notice that one of the two nuts was obviously missing, even after I told it there was a second problem with the installation.<p>Screenshot: <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;QqCNzOM" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;QqCNzOM</a></div><br/><div id="40930402" class="c"><input type="checkbox" id="c-40930402" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#40930322">parent</a><span>|</span><a href="#40932344">next</a><span>|</span><label class="collapse" for="c-40930402">[-]</label><label class="expand" for="c-40930402">[2 more]</label></div><br/><div class="children"><div class="content">A <i>human</i> would need to trace the cable. An LLM may just be responding based on (1) the fact that you&#x27;re asking about the clip in the first place, and that commonly happens when there&#x27;s something wrong; and (2) that this is a very common failure mode. This is supported by it bringing up the &quot;never saddle a dead horse&quot; mnemonic, which suggests the issue is common.<p>After you fix it, you should try asking the same questions!</div><br/></div></div><div id="40932344" class="c"><input type="checkbox" id="c-40932344" checked=""/><div class="controls bullet"><span class="by">fn-mote</span><span>|</span><a href="#40930322">parent</a><span>|</span><a href="#40930402">prev</a><span>|</span><a href="#40932934">next</a><span>|</span><label class="collapse" for="c-40932344">[-]</label><label class="expand" for="c-40932344">[1 more]</label></div><br/><div class="children"><div class="content">As a human, I was unable to see enough in that picture to infer which side was supposed to be under tension. I’m not trained, but I know what I expected to see from your description.<p>Like my sister post, I’m skeptical that the LLM didn’t just get lucky.</div><br/></div></div></div></div><div id="40932934" class="c"><input type="checkbox" id="c-40932934" checked=""/><div class="controls bullet"><span class="by">joelburget</span><span>|</span><a href="#40930322">prev</a><span>|</span><a href="#40929107">next</a><span>|</span><label class="collapse" for="c-40932934">[-]</label><label class="expand" for="c-40932934">[6 more]</label></div><br/><div class="children"><div class="content">Vision Transformers do a shocking amount of compression in the tokenizer. In the [Chameleon paper](<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.09818" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.09818</a>) they say the tokenizer &quot;encodes a 512 × 512 image into 1024 discrete tokens from a codebook of size 8192&quot;. That&#x27;s 256 pixels per token (512 * 512 &#x2F; 1024). If we assume that a pixel is 24 bits (3x 8 bit channels), this implies that they&#x27;ve compressed 256 * 24 = 6144 bits into 13 = (log2(8192)). [An Image is Worth 32 Tokens for Reconstruction and Generation](<a href="https:&#x2F;&#x2F;yucornetto.github.io&#x2F;projects&#x2F;titok.html" rel="nofollow">https:&#x2F;&#x2F;yucornetto.github.io&#x2F;projects&#x2F;titok.html</a>) pushes this even further. If these models work similarly, it&#x27;s no wonder they struggle with some vision tasks.</div><br/><div id="40933553" class="c"><input type="checkbox" id="c-40933553" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40932934">parent</a><span>|</span><a href="#40933107">next</a><span>|</span><label class="collapse" for="c-40933553">[-]</label><label class="expand" for="c-40933553">[1 more]</label></div><br/><div class="children"><div class="content">It’s not as simple as that. If you ask GPT-4o to create a copy of these images, it generally creates one faithfully (e.g. an image with 5 squares will be produced), so it’s “seeing” things reasonably enough.<p>It doesn’t seem to have the logic though to answer these questions.<p>The complete data set is here to play around with it yourself: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;XAI&#x2F;vlmsareblind&#x2F;viewer&#x2F;default&#x2F;train" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;XAI&#x2F;vlmsareblind&#x2F;viewer&#x2F;defa...</a></div><br/></div></div><div id="40933107" class="c"><input type="checkbox" id="c-40933107" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#40932934">parent</a><span>|</span><a href="#40933553">prev</a><span>|</span><a href="#40933488">next</a><span>|</span><label class="collapse" for="c-40933107">[-]</label><label class="expand" for="c-40933107">[3 more]</label></div><br/><div class="children"><div class="content">GPT-4o is very good at some visual tasks like optical character recognition. So the selective blindness might just be what you say here -- all of its capacity is dedicated to minimizing loss on a few narrow tasks that had the most training data (like OCR). So it&#x27;s not necessarily an inherent failure of the architecture to generalize, it could just be a capacity issue that will naturally be resolved with more scale.</div><br/><div id="40933939" class="c"><input type="checkbox" id="c-40933939" checked=""/><div class="controls bullet"><span class="by">sushid</span><span>|</span><a href="#40932934">root</a><span>|</span><a href="#40933107">parent</a><span>|</span><a href="#40933488">next</a><span>|</span><label class="collapse" for="c-40933939">[-]</label><label class="expand" for="c-40933939">[2 more]</label></div><br/><div class="children"><div class="content">Is that not just traditional OCR applied on top of LLM?</div><br/><div id="40934010" class="c"><input type="checkbox" id="c-40934010" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#40932934">root</a><span>|</span><a href="#40933939">parent</a><span>|</span><a href="#40933488">next</a><span>|</span><label class="collapse" for="c-40934010">[-]</label><label class="expand" for="c-40934010">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible they have a software layer that does that. But I was assuming they don&#x27;t, because the open source multimodal models don&#x27;t.</div><br/></div></div></div></div></div></div><div id="40933488" class="c"><input type="checkbox" id="c-40933488" checked=""/><div class="controls bullet"><span class="by">buryat</span><span>|</span><a href="#40932934">parent</a><span>|</span><a href="#40933107">prev</a><span>|</span><a href="#40929107">next</a><span>|</span><label class="collapse" for="c-40933488">[-]</label><label class="expand" for="c-40933488">[1 more]</label></div><br/><div class="children"><div class="content">for some reason I started thinking about trying to describe the taste of a fruit to someone who hasn&#x27;t tried it as something that can be similar to this as a non-visual sensory modal in humans</div><br/></div></div></div></div><div id="40929107" class="c"><input type="checkbox" id="c-40929107" checked=""/><div class="controls bullet"><span class="by">JeremyHerrman</span><span>|</span><a href="#40932934">prev</a><span>|</span><a href="#40928918">next</a><span>|</span><label class="collapse" for="c-40929107">[-]</label><label class="expand" for="c-40929107">[7 more]</label></div><br/><div class="children"><div class="content">VLMs so far have never been good at counting objects or spatial relationships (e.g. the coffee is to the right of the microwave).<p>There are ways to help the VLM out - Set of Marks [0] from Microsoft being the most prominent, which uses segmentation to outline and label sections of the image before sending to the VLM.<p>Providing &quot;speakable&quot; labels to regions helps ground the visual abilities of VLMs and is why in this paper the performance is so much better when words are present in the grid for &quot;Task 6: Counting the rows and columns of a grid&quot;<p>0: <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;SoM">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;SoM</a></div><br/><div id="40930302" class="c"><input type="checkbox" id="c-40930302" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#40929107">parent</a><span>|</span><a href="#40931245">next</a><span>|</span><label class="collapse" for="c-40930302">[-]</label><label class="expand" for="c-40930302">[3 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t know counting objects was a problem. That&#x27;s pretty ironic because the very first implementation of a neural net (AFAIK) is the numa-rete artificial retina developed at the Biological Computer Lab [0] circa 1960. It was a parallel analog computer composed of &quot;nuerons&quot; each with a photocell that could be arranged in a grid and count &quot;the number of objects independent of their size, location and form, and independent of strength of illumination&quot; [1] - this paper may be of interest to those in the field, &quot;Perception of Form in Biological and Man Made Systems&quot; Heinz Von Foerster 1962<p>[0] <a href="https:&#x2F;&#x2F;distributedmuseum.illinois.edu&#x2F;exhibit&#x2F;biological_computer_laboratory&#x2F;" rel="nofollow">https:&#x2F;&#x2F;distributedmuseum.illinois.edu&#x2F;exhibit&#x2F;biological_co...</a><p>[1] <a href="https:&#x2F;&#x2F;sites.evergreen.edu&#x2F;arunchandra&#x2F;wp-content&#x2F;uploads&#x2F;sites&#x2F;395&#x2F;2018&#x2F;05&#x2F;bcl082.pdf" rel="nofollow">https:&#x2F;&#x2F;sites.evergreen.edu&#x2F;arunchandra&#x2F;wp-content&#x2F;uploads&#x2F;s...</a></div><br/><div id="40930748" class="c"><input type="checkbox" id="c-40930748" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#40929107">root</a><span>|</span><a href="#40930302">parent</a><span>|</span><a href="#40931245">next</a><span>|</span><label class="collapse" for="c-40930748">[-]</label><label class="expand" for="c-40930748">[2 more]</label></div><br/><div class="children"><div class="content">It really shouldn&#x27;t be surprising that these models fail to do anything that _they weren&#x27;t trained to do_.  It&#x27;s trivially easy to train a model to count stuff.  The wild thing about transformer based models is that their capabilities are _way_ beyond what you&#x27;d expect from token prediction.  Figuring out what their limitations actually are is interesting because nobody fully knows what their limitations are.</div><br/><div id="40931773" class="c"><input type="checkbox" id="c-40931773" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#40929107">root</a><span>|</span><a href="#40930748">parent</a><span>|</span><a href="#40931245">next</a><span>|</span><label class="collapse" for="c-40931773">[-]</label><label class="expand" for="c-40931773">[1 more]</label></div><br/><div class="children"><div class="content">I agree that these open ended transformers are way more interesting and impressive than a purpose built count-the-polygons model, but if the model doesn&#x27;t generalize well enough to figure out how to count the polygons, I can&#x27;t be convinced that they&#x27;ll perform usefully on a more sophisticated task.<p>I agree this research is really interesting, but I didn&#x27;t have an a priori expectation of what token prediction could accomplish, so my reaction to a lot of the claims and counterclaims of this new tech is that it&#x27;s good at fooling people and giving plausible but baseless results. It makes for good research but dangerous in the hands of a market attempting to exploit it.</div><br/></div></div></div></div></div></div><div id="40931245" class="c"><input type="checkbox" id="c-40931245" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#40929107">parent</a><span>|</span><a href="#40930302">prev</a><span>|</span><a href="#40929261">next</a><span>|</span><label class="collapse" for="c-40931245">[-]</label><label class="expand" for="c-40931245">[2 more]</label></div><br/><div class="children"><div class="content">Vision models use CLiP or something similar, which has no conception of anything specific in the image. It sees embeddings which correlate similarly to text embeddings. Take an image then describe it &#x27;there are birds sitting on a power line in front of a blue sky with some clouds&#x27;, get the embeddings from that and the embeddings from that picture and line them up. If you ask if there are birds in it, it would know, but not how many, unless it was common to describe the number of birds sitting on things and it happened often enough that the number counted was the number in the image descriptions it trained on. If you want to count objects you want something like YOLO.</div><br/><div id="40931881" class="c"><input type="checkbox" id="c-40931881" checked=""/><div class="controls bullet"><span class="by">JeremyHerrman</span><span>|</span><a href="#40929107">root</a><span>|</span><a href="#40931245">parent</a><span>|</span><a href="#40929261">next</a><span>|</span><label class="collapse" for="c-40931881">[-]</label><label class="expand" for="c-40931881">[1 more]</label></div><br/><div class="children"><div class="content">VLMs like PaliGemma and Florence-2 support object detection and segmentation, so it&#x27;s becoming more common to have YOLO like capabilities built into VLMs.<p>Another benefit of VLMs which support object detection is that they are open vocabulary, meaning you don&#x27;t have to define the classes ahead of time. Additionally fine tuning tends to keep the previous detection capabilities instead of erasing all previous classes like fine tuning YOLO.</div><br/></div></div></div></div></div></div><div id="40928918" class="c"><input type="checkbox" id="c-40928918" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40929107">prev</a><span>|</span><a href="#40929027">next</a><span>|</span><label class="collapse" for="c-40928918">[-]</label><label class="expand" for="c-40928918">[22 more]</label></div><br/><div class="children"><div class="content">Wow, that is <i>embarrassingly bad performance</i> for current SOTA models (GPT-4o, Gemini-1.5 Pro, Sonnet-3, Sonnet-3.5), which are advertised and sold as being able to understand images, e.g., for guiding the blind or tutoring children in geometry!<p>The tasks at which they fail are ridiculously simple for human beings, including, for example:<p>* counting the number of times two lines intersect;<p>* detecting whether two circles overlap;<p>* selecting which letter is being circled in a word;<p>* counting the number of circles in an Olympic-like logo.<p>This should be at the top of the front page.</div><br/><div id="40929026" class="c"><input type="checkbox" id="c-40929026" checked=""/><div class="controls bullet"><span class="by">tensor</span><span>|</span><a href="#40928918">parent</a><span>|</span><a href="#40933552">next</a><span>|</span><label class="collapse" for="c-40929026">[-]</label><label class="expand" for="c-40929026">[20 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how this is &quot;embarrassing&quot; in the slightest. These models are not human brains, and the fact that people equate them with human brains is an embarrassing failure of the humans more than anything about the models.<p>It&#x27;s entirely unsurprising that there are numerous cases that these models can&#x27;t handle that are &quot;obvious to humans.&quot; Machine learning has had this property since its invention and it&#x27;s a classic mistake humans make dealing with these systems.<p>Humans assume that because a machine learning model has above human accuracy on task X that it implies that it must also have that ability at all the other tasks. While a human with amazing ability at X would indeed have amazing abilities at other tasks, this is not true of machine learning models 
The opposite thinking is also wrong, that because the model can&#x27;t do well on task Y it must be unreliable and it&#x27;s ability on task X is somehow an illusion and not to be trusted.</div><br/><div id="40929067" class="c"><input type="checkbox" id="c-40929067" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929026">parent</a><span>|</span><a href="#40929563">next</a><span>|</span><label class="collapse" for="c-40929067">[-]</label><label class="expand" for="c-40929067">[12 more]</label></div><br/><div class="children"><div class="content">It is embarrassingly, shockingly bad, because these models are <i>advertised</i> and <i>sold</i> as being capable of understanding images.<p>Evidently, all these models still fall short.</div><br/><div id="40929403" class="c"><input type="checkbox" id="c-40929403" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929067">parent</a><span>|</span><a href="#40934305">next</a><span>|</span><label class="collapse" for="c-40929403">[-]</label><label class="expand" for="c-40929403">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s surprising because these models are pretty ok at some vision tasks.  The existence of a clear failure mode is interesting and informative, not embarrassing.</div><br/></div></div><div id="40934305" class="c"><input type="checkbox" id="c-40934305" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929067">parent</a><span>|</span><a href="#40929403">prev</a><span>|</span><a href="#40931903">next</a><span>|</span><label class="collapse" for="c-40934305">[-]</label><label class="expand" for="c-40934305">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re hardly being advertised or sold on that premise. They advertise and sell themselves, because <i>people try them out and find out they work</i>, and tell their friends and&#x2F;or audiences. ChatGPT is probably the single biggest bona-fide organic marketing success story in recorded history.</div><br/></div></div><div id="40930151" class="c"><input type="checkbox" id="c-40930151" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929067">parent</a><span>|</span><a href="#40931903">prev</a><span>|</span><a href="#40930297">next</a><span>|</span><label class="collapse" for="c-40930151">[-]</label><label class="expand" for="c-40930151">[1 more]</label></div><br/><div class="children"><div class="content">Humans are also shockingly bad on these tasks. And guess where the labeling was coming from…</div><br/></div></div><div id="40930297" class="c"><input type="checkbox" id="c-40930297" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929067">parent</a><span>|</span><a href="#40930151">prev</a><span>|</span><a href="#40929623">next</a><span>|</span><label class="collapse" for="c-40930297">[-]</label><label class="expand" for="c-40930297">[4 more]</label></div><br/><div class="children"><div class="content">I see this complaint about LLMs all the time - that they&#x27;re advertised as being infallible but fail the moment you give them a simple logic puzzle or ask for a citation.<p>And yet... every interface to every LLM has a &quot;ChatGPT can make mistakes. Check important info.&quot; style disclaimer.<p>The hype around this stuff may be deafening, but it&#x27;s often not entirely the direct fault of the model vendors themselves, who even put out lengthy papers describing their many flaws.</div><br/><div id="40930358" class="c"><input type="checkbox" id="c-40930358" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40930297">parent</a><span>|</span><a href="#40930976">next</a><span>|</span><label class="collapse" for="c-40930358">[-]</label><label class="expand" for="c-40930358">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s evidently a large gap between what researchers publish, the disclaimers a vendor makes, and what gets broadcast on CNBC, no surprise there.</div><br/></div></div><div id="40930976" class="c"><input type="checkbox" id="c-40930976" checked=""/><div class="controls bullet"><span class="by">jampekka</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40930297">parent</a><span>|</span><a href="#40930358">prev</a><span>|</span><a href="#40929623">next</a><span>|</span><label class="collapse" for="c-40930976">[-]</label><label class="expand" for="c-40930976">[2 more]</label></div><br/><div class="children"><div class="content">A bit like how Tesla Full Self-Driving is not to be used as self-driving. Or any other small print. Or ads in general. Lying by deliberately giving the wrong impression.</div><br/><div id="40932376" class="c"><input type="checkbox" id="c-40932376" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40930976">parent</a><span>|</span><a href="#40929623">next</a><span>|</span><label class="collapse" for="c-40932376">[-]</label><label class="expand" for="c-40932376">[1 more]</label></div><br/><div class="children"><div class="content">It would have to be called ChatAGI to be like TeslaFSD, where the company named it something it is most definitely not</div><br/></div></div></div></div></div></div><div id="40929623" class="c"><input type="checkbox" id="c-40929623" checked=""/><div class="controls bullet"><span class="by">knowaveragejoe</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929067">parent</a><span>|</span><a href="#40930297">prev</a><span>|</span><a href="#40932380">next</a><span>|</span><label class="collapse" for="c-40929623">[-]</label><label class="expand" for="c-40929623">[2 more]</label></div><br/><div class="children"><div class="content">Not only are they capable of understanding images(the kind people might actually feed into such a system - photographs), but they&#x27;re pretty good at it.<p>A modern robot would struggle to fold socks and put them in a drawer, but they&#x27;re great at making cars.</div><br/><div id="40930290" class="c"><input type="checkbox" id="c-40930290" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929623">parent</a><span>|</span><a href="#40932380">next</a><span>|</span><label class="collapse" for="c-40930290">[-]</label><label class="expand" for="c-40930290">[1 more]</label></div><br/><div class="children"><div class="content">I mean, with some of the recent demos, robots have got a lot better at folding stuff and putting it up. Not saying it&#x27;s anywhere close to human level, but it has taken a pretty massive leap from being a joke just a few years ago.</div><br/></div></div></div></div><div id="40932380" class="c"><input type="checkbox" id="c-40932380" checked=""/><div class="controls bullet"><span class="by">fennecbutt</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929067">parent</a><span>|</span><a href="#40929623">prev</a><span>|</span><a href="#40929563">next</a><span>|</span><label class="collapse" for="c-40932380">[-]</label><label class="expand" for="c-40932380">[1 more]</label></div><br/><div class="children"><div class="content">Why do people expect these models, designed to be humanlike in their training, to be 100% perfect?<p>Humans fuck up all the time.</div><br/></div></div></div></div><div id="40929563" class="c"><input type="checkbox" id="c-40929563" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929026">parent</a><span>|</span><a href="#40929067">prev</a><span>|</span><a href="#40930536">next</a><span>|</span><label class="collapse" for="c-40929563">[-]</label><label class="expand" for="c-40929563">[3 more]</label></div><br/><div class="children"><div class="content">These models are marketed as being able to guide the blind or tutoring children using direct camera access.<p>Promoting those use cases and models failing in these ways is irresponsible. So, yeah, maybe the models are not embarrasing but the hype definitely is.</div><br/><div id="40929633" class="c"><input type="checkbox" id="c-40929633" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929563">parent</a><span>|</span><a href="#40931289">next</a><span>|</span><label class="collapse" for="c-40929633">[-]</label><label class="expand" for="c-40929633">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; Promoting those use cases and models failing in these ways is irresponsible.</i><p>Yes, <i>exactly</i>.</div><br/></div></div></div></div><div id="40930536" class="c"><input type="checkbox" id="c-40930536" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929026">parent</a><span>|</span><a href="#40929563">prev</a><span>|</span><a href="#40931963">next</a><span>|</span><label class="collapse" for="c-40930536">[-]</label><label class="expand" for="c-40930536">[1 more]</label></div><br/><div class="children"><div class="content">Well said.<p>It doesn&#x27;t matter how they are marketed or described or held up to some standard generated by wishful thinking. And it especially doesn&#x27;t matter what it would mean if a human were to make the same error.<p>It matters what they are, what they&#x27;re doing, and how they&#x27;re doing it. Feel free to be embarrassed if <i>you</i> are claiming they can do what they can&#x27;t and are maybe even selling them on that basis. But there&#x27;s nothing embarrassing about their current set of capabilities. They are very good at what they are very good at. Expecting those capabilities to generalize as they would if they were human is like getting embarrassed that your screwdriver can&#x27;t pound in a nail, when it is ever so good at driving in screws.</div><br/></div></div><div id="40931963" class="c"><input type="checkbox" id="c-40931963" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929026">parent</a><span>|</span><a href="#40930536">prev</a><span>|</span><a href="#40929863">next</a><span>|</span><label class="collapse" for="c-40931963">[-]</label><label class="expand" for="c-40931963">[2 more]</label></div><br/><div class="children"><div class="content">&gt; is an embarrassing failure of the humans more than anything about the models<p>No, it&#x27;s a failure of the companies who are advertising them as capable of doing something which they are not (assisting people with low vision)</div><br/><div id="40932146" class="c"><input type="checkbox" id="c-40932146" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40931963">parent</a><span>|</span><a href="#40929863">next</a><span>|</span><label class="collapse" for="c-40932146">[-]</label><label class="expand" for="c-40932146">[1 more]</label></div><br/><div class="children"><div class="content">But they CAN assist people with low vision. I&#x27;ve talked to someone who&#x27;s been using a product based on GPT-4o and absolutely loves it.<p>Low vision users understand the limitations of accessibility technology better than anyone else. They will VERY quickly figure out what this tech can be used for effectively and what it can&#x27;t.</div><br/></div></div></div></div><div id="40929863" class="c"><input type="checkbox" id="c-40929863" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#40928918">root</a><span>|</span><a href="#40929026">parent</a><span>|</span><a href="#40931963">prev</a><span>|</span><a href="#40933552">next</a><span>|</span><label class="collapse" for="c-40929863">[-]</label><label class="expand" for="c-40929863">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d expect them to be trained on simple geometry since you can create arbitrarily large synthetic training set for that.</div><br/></div></div></div></div><div id="40933552" class="c"><input type="checkbox" id="c-40933552" checked=""/><div class="controls bullet"><span class="by">drodgers</span><span>|</span><a href="#40928918">parent</a><span>|</span><a href="#40929026">prev</a><span>|</span><a href="#40929027">next</a><span>|</span><label class="collapse" for="c-40933552">[-]</label><label class="expand" for="c-40933552">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t help but read comments like this as:<p>&quot;My talking dog always makes mistakes on calculus problems: how embarrassingly bad!&quot;<p>Has the expectation treadmill really advanced so quickly that sub-human performance on any category of problems is now an embarrassment?</div><br/></div></div></div></div><div id="40929027" class="c"><input type="checkbox" id="c-40929027" checked=""/><div class="controls bullet"><span class="by">diwank</span><span>|</span><a href="#40928918">prev</a><span>|</span><a href="#40933566">next</a><span>|</span><label class="collapse" for="c-40929027">[-]</label><label class="expand" for="c-40929027">[1 more]</label></div><br/><div class="children"><div class="content">I think while the findings are interesting, the title &quot;Vision language models are blind&quot; is hyperbolic and misleading of the findings. It is true that VLMs have a very different way of sensing and working with image inputs and for low resolutions each image is split up into blocks and then mapped into discrete tokens. They actually really dont have access to finegrained details because this mapping is quite lossy. So, in that sense, these results completely make sense and were not surprising to me but &quot;blind&quot; is a strong implication and the work isnt really able to back that.<p>Take the first example of the four 2D line plots for instance. I asked Sonnet 3.5 for an answer and retried it 5 times and 2 out of 5 times, it said something like this:<p><i>Me</i>:<p>here&#x27;s an image with four 2-D line plots. Can you tell me A) how many lines are in each plot? and B) how many times do they intersect each other in each plot?<p>Write your answers as:
Plot 1 (leftmost): ...<p><i>Sonnet</i>:<p>I&#x27;ll analyze each plot from left to right and provide the requested information:<p>Plot 1 (leftmost):
A) 2 lines (one red, one blue)
B) 0 intersections (the lines do not cross)<p>Plot 2:
A) 2 lines (one red, one blue)
B) 1 intersection (the lines cross once near the right edge)<p>Plot 3:
A) 2 lines (one red, one blue)
B) 1 intersection (the lines cross once near the center)<p>Plot 4 (rightmost):
A) 2 lines (one red, one blue)
B) 2 intersections (the lines cross twice, once near the left side and once near the right side)<p>Each plot consistently shows two lines, one red and one blue, with varying intersection patterns across the four plots.<p>---<p>Which isn&#x27;t completely correct (the 3rd plot has 2 intersections) but still pretty good and the other 3 times, it said plot 3 has 1 intersection.</div><br/></div></div><div id="40933566" class="c"><input type="checkbox" id="c-40933566" checked=""/><div class="controls bullet"><span class="by">poikroequ</span><span>|</span><a href="#40929027">prev</a><span>|</span><a href="#40929876">next</a><span>|</span><label class="collapse" for="c-40933566">[-]</label><label class="expand" for="c-40933566">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s ironic, they fail these seemingly simple tests that are trivial even for a child to solve. Yet, I used Gemini to read a postcard containing handwritten Russian cursive text with lots of visual noise (postmarks and whatnot). It was able to read the text and translate it into English. I didn&#x27;t even need to tell it the text is Russian.<p>On the one hand, it&#x27;s incredible what these LLMs are capable of. On the other hand, they often fall flat on their face with seemingly simple problems like this. We are seeing the same from self driving cars, getting into accidents in scenarios that almost any human driver could have easily avoided.</div><br/></div></div><div id="40929876" class="c"><input type="checkbox" id="c-40929876" checked=""/><div class="controls bullet"><span class="by">akavi</span><span>|</span><a href="#40933566">prev</a><span>|</span><a href="#40929540">next</a><span>|</span><label class="collapse" for="c-40929876">[-]</label><label class="expand" for="c-40929876">[6 more]</label></div><br/><div class="children"><div class="content">Speaking as someone with only a tenuous grasp of how VLMs work, this naïvely feels like a place where the &quot;embodiement&quot; folks might have a point: Humans have the ability to &quot;refine&quot; their perception of an image iteratively, focusing in on areas of interest, while VLMs have to process the entire image at the same level of fidelity.<p>I&#x27;m curious if there&#x27;d be a way to emulate this (have the visual tokens be low fidelity at first, but allow the VLM to emit tokens that correspond to &quot;focusing&quot; on a region of the image with greater resolution). I&#x27;m not sure if&#x2F;how it&#x27;s possible to performantly train a model with &quot;interactive&quot; data like that, though</div><br/><div id="40930397" class="c"><input type="checkbox" id="c-40930397" checked=""/><div class="controls bullet"><span class="by">efskap</span><span>|</span><a href="#40929876">parent</a><span>|</span><a href="#40931128">next</a><span>|</span><label class="collapse" for="c-40930397">[-]</label><label class="expand" for="c-40930397">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this the attention mechanism, the reason we&#x27;re using transformers for these things? Maybe not greater resolution per se, but focusing on a region with greater neural connectivity</div><br/><div id="40931078" class="c"><input type="checkbox" id="c-40931078" checked=""/><div class="controls bullet"><span class="by">akavi</span><span>|</span><a href="#40929876">root</a><span>|</span><a href="#40930397">parent</a><span>|</span><a href="#40931128">next</a><span>|</span><label class="collapse" for="c-40931078">[-]</label><label class="expand" for="c-40931078">[1 more]</label></div><br/><div class="children"><div class="content">Ah, good point!<p>But the model is downstream of the &quot;patch&quot; tokenization, so the cut-down in resolution (compression) of the image has already occurred <i>prior</i> to the point where the model can direct greater &quot;attention&quot;.<p>I think the synthesis is that I&#x27;m proposing a per-pixel tokenization with a transformer block whose purpose is to output information at a compression level &quot;equivalent&quot; to that of the patch tokens (is this what an autoencoder is?), but where the attention vector is a function of the full state of the LLM (ie, inclusive of the text surrounding the image)).<p>Naïvely, I&#x27;d think a layer like this that is agnostic to the LLM state needn&#x27;t be any more computationally costly than the patching computation (both are big honks of linear algebra?), but idk how expensive the &quot;full context attention&quot; feedback is...<p>(I apologize to anyone who actually understands transformers for my gratuitous (ab|mis)use of terminology)</div><br/></div></div></div></div><div id="40931128" class="c"><input type="checkbox" id="c-40931128" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40929876">parent</a><span>|</span><a href="#40930397">prev</a><span>|</span><a href="#40931042">next</a><span>|</span><label class="collapse" for="c-40931128">[-]</label><label class="expand" for="c-40931128">[1 more]</label></div><br/><div class="children"><div class="content">Lots and lots of eye tracking data paired with what was being looked at in order to emulate human attention processing might be one of the lower hanging fruits for improving it.</div><br/></div></div><div id="40931042" class="c"><input type="checkbox" id="c-40931042" checked=""/><div class="controls bullet"><span class="by">Brechreiz</span><span>|</span><a href="#40929876">parent</a><span>|</span><a href="#40931128">prev</a><span>|</span><a href="#40929540">next</a><span>|</span><label class="collapse" for="c-40931042">[-]</label><label class="expand" for="c-40931042">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Humans have the ability to &quot;refine&quot; their perception of an image iteratively<p>That&#x27;s not related to embodied cognition.</div><br/><div id="40931125" class="c"><input type="checkbox" id="c-40931125" checked=""/><div class="controls bullet"><span class="by">akavi</span><span>|</span><a href="#40929876">root</a><span>|</span><a href="#40931042">parent</a><span>|</span><a href="#40929540">next</a><span>|</span><label class="collapse" for="c-40931125">[-]</label><label class="expand" for="c-40931125">[1 more]</label></div><br/><div class="children"><div class="content">Is embodied cognition not at least in part about interactivity? I perform action (emit tokens) and receive feedback (non-self-generated tokens)</div><br/></div></div></div></div></div></div><div id="40929540" class="c"><input type="checkbox" id="c-40929540" checked=""/><div class="controls bullet"><span class="by">mglz</span><span>|</span><a href="#40929876">prev</a><span>|</span><a href="#40933506">next</a><span>|</span><label class="collapse" for="c-40929540">[-]</label><label class="expand" for="c-40929540">[3 more]</label></div><br/><div class="children"><div class="content">I tought some Computational Geometry courses and efficiently computing the intersections of N line segments is not as straightforward as you might initially think. Since somewhere some computation must be done to recognize this and LLMs are not specifically trained for this task, it&#x27;s not suprising they struggle.<p>In general, basic geometry seems under-explored by learning.</div><br/><div id="40932517" class="c"><input type="checkbox" id="c-40932517" checked=""/><div class="controls bullet"><span class="by">jordan_bonecut</span><span>|</span><a href="#40929540">parent</a><span>|</span><a href="#40932752">next</a><span>|</span><label class="collapse" for="c-40932517">[-]</label><label class="expand" for="c-40932517">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but so is telling if a photo contains a dog or understanding sentiment in a paragraph of text. Complexity isn&#x27;t quite the issue, I think it is that there is a distinction between the type of reasoning which these models have learnt and that which is necessary for concrete mathematical reasoning.</div><br/></div></div><div id="40932752" class="c"><input type="checkbox" id="c-40932752" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#40929540">parent</a><span>|</span><a href="#40932517">prev</a><span>|</span><a href="#40933506">next</a><span>|</span><label class="collapse" for="c-40932752">[-]</label><label class="expand" for="c-40932752">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Since somewhere some computation must be done to recognize this</i><p>Humans don&#x27;t have a &quot;compute intersections&quot; ability (other than a few who have learned it laboriously through algebra), we have a &quot;see things and count them&quot; mechanism.  We aren&#x27;t visually taking lines in a planar space and determining where they cross. We know what an intersection looks like, we see one, increment a counter, and find the next one. If it&#x27;s less than around five, we do this all at once. Otherwise we literally count, sometimes in small groups, sometimes one at a time.</div><br/></div></div></div></div><div id="40933506" class="c"><input type="checkbox" id="c-40933506" checked=""/><div class="controls bullet"><span class="by">yantrams</span><span>|</span><a href="#40929540">prev</a><span>|</span><a href="#40930028">next</a><span>|</span><label class="collapse" for="c-40933506">[-]</label><label class="expand" for="c-40933506">[3 more]</label></div><br/><div class="children"><div class="content">Tested these problems with llava-v1.6-mistral-7b and the results aren&#x27;t bad. Maybe I just got lucky with these samples<p>Intersecting Lines
<a href="https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;s24aeawxasrgj0cgkzabtj53rc">https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;s24aeawxasrgj0cgkzabtj53rc</a><p>Overlapping Circles
<a href="https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;0w026pgbgxrgg0cgkzcv11k384">https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;0w026pgbgxrgg0cgkzcv11k384</a><p>Touching Circles
<a href="https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;105se4p2mnrgm0cgkzcvm83tdc">https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;105se4p2mnrgm0cgkzcvm83tdc</a><p>Circled Text
<a href="https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;3kdrb26nwdrgj0cgkzerez14wc">https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;3kdrb26nwdrgj0cgkzerez14wc</a><p>Nested Squares
<a href="https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;1ycah63hr1rgg0cgkzf99srpxm">https:&#x2F;&#x2F;replicate.com&#x2F;p&#x2F;1ycah63hr1rgg0cgkzf99srpxm</a></div><br/><div id="40933542" class="c"><input type="checkbox" id="c-40933542" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40933506">parent</a><span>|</span><a href="#40930028">next</a><span>|</span><label class="collapse" for="c-40933542">[-]</label><label class="expand" for="c-40933542">[2 more]</label></div><br/><div class="children"><div class="content">These are really interesting examples, thanks for sharing.</div><br/><div id="40933572" class="c"><input type="checkbox" id="c-40933572" checked=""/><div class="controls bullet"><span class="by">yantrams</span><span>|</span><a href="#40933506">root</a><span>|</span><a href="#40933542">parent</a><span>|</span><a href="#40930028">next</a><span>|</span><label class="collapse" for="c-40933572">[-]</label><label class="expand" for="c-40933572">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re welcome. I recently noticed I get better performance with VLMs when the queries are phrased this way - Descriptive Keys instead of explaining the problem in sentences. Similar to COT reasoning that many people claim gives better results, I personally found querying in this sequence - existenceOfEntity, numberOfEntities followed by propertiesOfEntities etc tends to give better results. I haven&#x27;t verified any of this rigorously so please do take it with a pinch of salt :)</div><br/></div></div></div></div></div></div><div id="40930028" class="c"><input type="checkbox" id="c-40930028" checked=""/><div class="controls bullet"><span class="by">jeromeparadis</span><span>|</span><a href="#40933506">prev</a><span>|</span><a href="#40929289">next</a><span>|</span><label class="collapse" for="c-40930028">[-]</label><label class="expand" for="c-40930028">[1 more]</label></div><br/><div class="children"><div class="content">One use-case I always try is to have an AI try to read a school calendar image where days off are or days of interest are highlighted using a legend. i.e.: days with a square, circle or triangle or different color, etc.<p>When asking days for specific days of interest for the school year, AIs always struggle. They get some days right but forget some or fabulate new days. They fare a bit better if you remove some of the noise and give them only a picture of a month but even then, it&#x27;s unreliable.</div><br/></div></div><div id="40929289" class="c"><input type="checkbox" id="c-40929289" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40930028">prev</a><span>|</span><a href="#40926735">next</a><span>|</span><label class="collapse" for="c-40929289">[-]</label><label class="expand" for="c-40929289">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been generally frustrated at the lack of analysis of vision LLMs generally.<p>They&#x27;re clearly a very exciting category of technology, and a pretty recent one - they only got good last October with GPT-4 Vision, but since then we&#x27;ve had more vision models from Anthropic and Google Gemini.<p>There&#x27;s so much more information about there about text prompting compared to image prompting. I  feel starved for useful information about their capabilities: what are vision models good and bad at, and what are the best ways to put them to work?</div><br/><div id="40929867" class="c"><input type="checkbox" id="c-40929867" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40929289">parent</a><span>|</span><a href="#40929396">next</a><span>|</span><label class="collapse" for="c-40929867">[-]</label><label class="expand" for="c-40929867">[1 more]</label></div><br/><div class="children"><div class="content">Anthropic have some interesting cookbook examples that provide advice on using their multimodal models here: <a href="https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;anthropic-cookbook&#x2F;tree&#x2F;main&#x2F;multimodal">https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;anthropic-cookbook&#x2F;tree&#x2F;main&#x2F;m...</a><p>I&#x27;ve assembled a bunch more notes here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;tags&#x2F;vision-llms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;tags&#x2F;vision-llms&#x2F;</a></div><br/></div></div><div id="40929396" class="c"><input type="checkbox" id="c-40929396" checked=""/><div class="controls bullet"><span class="by">r2_pilot</span><span>|</span><a href="#40929289">parent</a><span>|</span><a href="#40929867">prev</a><span>|</span><a href="#40926735">next</a><span>|</span><label class="collapse" for="c-40929396">[-]</label><label class="expand" for="c-40929396">[2 more]</label></div><br/><div class="children"><div class="content">Why not use them yourself if you have access? I have been using Claude 3.5 Sonnet for gardening recently, and while it&#x27;s not perfect(and can be a little blind unless you tell it to focus on a specific thing), it&#x27;s helped me understand how to keep my plants alive in some challenging conditions(for me; this is my second or third attempt at gardening so it&#x27;s all challenging lol). But just a experiment with it and see where the capabilities lie. I do agree that certain classes of visual data are challenging for it.</div><br/><div id="40929715" class="c"><input type="checkbox" id="c-40929715" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40929289">root</a><span>|</span><a href="#40929396">parent</a><span>|</span><a href="#40926735">next</a><span>|</span><label class="collapse" for="c-40929715">[-]</label><label class="expand" for="c-40929715">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used them a bunch. I want to learn from other people&#x27;s experiences as well.<p>Some of my notes so far:<p>- <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Apr&#x2F;17&#x2F;ai-for-data-journalism&#x2F;#structured-data-extraction" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Apr&#x2F;17&#x2F;ai-for-data-journalism...</a> - my datasette-extract plugin, for structured data from both text and images<p>- <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Apr&#x2F;17&#x2F;ai-for-data-journalism&#x2F;#campaign-finance-failure" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Apr&#x2F;17&#x2F;ai-for-data-journalism...</a> - where they failed to extract data from a handwritten scanned document in various weird ways<p>- <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Feb&#x2F;21&#x2F;gemini-pro-video&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Feb&#x2F;21&#x2F;gemini-pro-video&#x2F;</a> talks about video inputs to Gemini Pro (which are actually image inputs, it splits them up to one frame per second)</div><br/></div></div></div></div></div></div><div id="40926735" class="c"><input type="checkbox" id="c-40926735" checked=""/><div class="controls bullet"><span class="by">taesiri</span><span>|</span><a href="#40929289">prev</a><span>|</span><a href="#40929187">next</a><span>|</span><label class="collapse" for="c-40926735">[-]</label><label class="expand" for="c-40926735">[1 more]</label></div><br/><div class="children"><div class="content">This paper examines the limitations of current vision-based language models, such as GPT-4 and Sonnet 3.5, in performing low-level vision tasks. Despite their high scores on numerous multimodal benchmarks, these models often fail on very basic cases. This raises a crucial question: are we evaluating these models accurately?</div><br/></div></div><div id="40929187" class="c"><input type="checkbox" id="c-40929187" checked=""/><div class="controls bullet"><span class="by">_vaporwave_</span><span>|</span><a href="#40926735">prev</a><span>|</span><a href="#40930831">next</a><span>|</span><label class="collapse" for="c-40929187">[-]</label><label class="expand" for="c-40929187">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s really interesting that there&#x27;s a huge performance discrepancy between these SOTA models. In the Olympic logo example, GPT-4o is below the baseline accuracy of 20% (worse than randomly guessing) while Sonnet-3.5 was correct ~76% of the time.<p>Does anyone have any technical insight or intuition as to why this large variation exists?</div><br/><div id="40933564" class="c"><input type="checkbox" id="c-40933564" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40929187">parent</a><span>|</span><a href="#40930831">next</a><span>|</span><label class="collapse" for="c-40933564">[-]</label><label class="expand" for="c-40933564">[1 more]</label></div><br/><div class="children"><div class="content">The question wasn’t “yes or no” but instead required an exact number: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;XAI&#x2F;vlmsareblind&#x2F;viewer&#x2F;default&#x2F;train" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;XAI&#x2F;vlmsareblind&#x2F;viewer&#x2F;defa...</a><p>Playing around with GPT-4o, it knows enough to make a copy of an image that is reasonable but it still can’t answer the questions.<p>ChatGPT went down a rabbit hole of trying to write python code, but it took lots of prompting for it to notice its mistake when solving one of the intersecting line questions.</div><br/></div></div></div></div><div id="40930831" class="c"><input type="checkbox" id="c-40930831" checked=""/><div class="controls bullet"><span class="by">gnutrino</span><span>|</span><a href="#40929187">prev</a><span>|</span><a href="#40932498">next</a><span>|</span><label class="collapse" for="c-40930831">[-]</label><label class="expand" for="c-40930831">[3 more]</label></div><br/><div class="children"><div class="content">My guess is that the systems are running image recognition models, and maybe OCR on images, and then just piping that data as tokens into an LLM.  So you are only ever going to get results as good as existing images models with the results filtered through an LLM.<p>To me, this is only interesting if compared with results of image recognition models that can already answer these types of questions (if they even exist, I haven&#x27;t looked).<p>Maybe the service is smart enough to look at the question, and then choose one or more models to process the image, but not sure as I can&#x27;t find anything on their sites about how it works.</div><br/><div id="40932175" class="c"><input type="checkbox" id="c-40932175" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40930831">parent</a><span>|</span><a href="#40931314">next</a><span>|</span><label class="collapse" for="c-40932175">[-]</label><label class="expand" for="c-40932175">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not how they work. The original GPT-4 paper has some detail: <a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;papers&#x2F;gpt-4.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;papers&#x2F;gpt-4.pdf</a><p>Or read up on PaliGemma: <a href="https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;big_vision&#x2F;blob&#x2F;main&#x2F;big_vision&#x2F;configs&#x2F;proj&#x2F;paligemma&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;big_vision&#x2F;blob&#x2F;main&#x2F;big_...</a></div><br/></div></div><div id="40931314" class="c"><input type="checkbox" id="c-40931314" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#40930831">parent</a><span>|</span><a href="#40932175">prev</a><span>|</span><a href="#40932498">next</a><span>|</span><label class="collapse" for="c-40931314">[-]</label><label class="expand" for="c-40931314">[1 more]</label></div><br/><div class="children"><div class="content">&gt; My guess is that the systems are running image recognition models<p>Your guess is incorrect. Look up CLIP, BLIP, and SigLip for an idea of how they work.</div><br/></div></div></div></div><div id="40932498" class="c"><input type="checkbox" id="c-40932498" checked=""/><div class="controls bullet"><span class="by">jordan_bonecut</span><span>|</span><a href="#40930831">prev</a><span>|</span><a href="#40932470">next</a><span>|</span><label class="collapse" for="c-40932498">[-]</label><label class="expand" for="c-40932498">[4 more]</label></div><br/><div class="children"><div class="content">This is an interesting article and goes along with how I understand how such models interpret input data. I&#x27;m not sure I would characterize the results as blurry vision, but maybe an inability to process what they see in a concrete manner.<p>All the LLMs and multi-modal models I&#x27;ve seen lack concrete reasoning. For instance, ask ChatGPT to perform 2 tasks, to summarize a chunk of text and to count how many words are in this chunk. ChatGPT will do a very good job summarizing the text and an awful job at counting the words. ChatGPT and all the transformer based models I&#x27;ve seen fail at similar concrete&#x2F;mathematical reasoning tasks. This is the core problem of creating AGI and it generally seems like no one has made any progress towards synthesizing something with both a high and low level of intelligence.<p>My (unproven and probably incorrect) theory is that under the hood these networks lack information processing loops which make recursive tasks, like solving a math problem, very difficult.</div><br/><div id="40934526" class="c"><input type="checkbox" id="c-40934526" checked=""/><div class="controls bullet"><span class="by">empiricus</span><span>|</span><a href="#40932498">parent</a><span>|</span><a href="#40932537">next</a><span>|</span><label class="collapse" for="c-40934526">[-]</label><label class="expand" for="c-40934526">[1 more]</label></div><br/><div class="children"><div class="content">I hope you are aware of the fact that LLMs does not have direct access to the stream of words&#x2F;characters. It is one of the most basic things to know about their implementation.</div><br/></div></div><div id="40932537" class="c"><input type="checkbox" id="c-40932537" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#40932498">parent</a><span>|</span><a href="#40934526">prev</a><span>|</span><a href="#40932470">next</a><span>|</span><label class="collapse" for="c-40932537">[-]</label><label class="expand" for="c-40932537">[2 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, I tried your test with ChatGPT 4o<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;79c5c6e1-e6a9-441b-acb3-54882303a891" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;79c5c6e1-e6a9-441b-acb3-54882303a8...</a><p>Of course as usual, LLMs are horrible with Math.<p>Funny enough, the next time it verified the word count by counting it out until I specifically told it to use Python<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;79e7b922-9b0f-4df9-98d0-2cd72d704176" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;79e7b922-9b0f-4df9-98d0-2cd72d7041...</a></div><br/><div id="40933093" class="c"><input type="checkbox" id="c-40933093" checked=""/><div class="controls bullet"><span class="by">infiar</span><span>|</span><a href="#40932498">root</a><span>|</span><a href="#40932537">parent</a><span>|</span><a href="#40932470">next</a><span>|</span><label class="collapse" for="c-40933093">[-]</label><label class="expand" for="c-40933093">[1 more]</label></div><br/><div class="children"><div class="content">This counting words task reminded me of a youtube video:
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-9XKiOXaHlI" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-9XKiOXaHlI</a>
Maybe LLMs are somehow more like monkeys.</div><br/></div></div></div></div></div></div><div id="40932470" class="c"><input type="checkbox" id="c-40932470" checked=""/><div class="controls bullet"><span class="by">Rebuff5007</span><span>|</span><a href="#40932498">prev</a><span>|</span><a href="#40930092">next</a><span>|</span><label class="collapse" for="c-40932470">[-]</label><label class="expand" for="c-40932470">[1 more]</label></div><br/><div class="children"><div class="content">In fairness, Mira Murati said GPT-4 is only high school level [1]. Maybe it takes PhD level to understand basic shapes?<p>[1] <a href="https:&#x2F;&#x2F;www.ccn.com&#x2F;news&#x2F;technology&#x2F;openais-gpt-5-phd-level-intelligence-2026-cto-mira-murati&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ccn.com&#x2F;news&#x2F;technology&#x2F;openais-gpt-5-phd-level-...</a></div><br/></div></div><div id="40930092" class="c"><input type="checkbox" id="c-40930092" checked=""/><div class="controls bullet"><span class="by">pjs_</span><span>|</span><a href="#40932470">prev</a><span>|</span><a href="#40932785">next</a><span>|</span><label class="collapse" for="c-40930092">[-]</label><label class="expand" for="c-40930092">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t like this paper for the following reasons:<p>- The language is unnecessarily scathing<p>- They repeatedly show data where the models are getting things <i>right</i> 70, 80, 90% of the time, and then show a list of what they call &quot;qualitative samples&quot; (what does &quot;qualitative&quot; mean? &quot;cherry-picked&quot;?) which look very bad. But it got the answer right 70&#x2F;80&#x2F;90% of the time! That&#x27;s hardly &quot;blind&quot;...<p>- Various of the tasks hinge on the distinction between two objects &quot;exactly touching&quot; vs. &quot;very nearly touching&quot; vs. &quot;very slightly overlapping&quot;, a problem which (i) is hard for humans and (ii) is particularly (presumably deliberately) sensitive to resolution&#x2F;precision, where we should not be surprised that models fail<p>- The main fish-shaped example given in task 1 seems genuinely ambiguous to me - do the lines &quot;intersect&quot; once or twice? The tail of the fish clearly has a crossing, but the nose of the fish seems a bit fishy to me... is that really an intersection?<p>- AFAIC deranged skepticism is just as bad as deranged hype, the framing here is at risk of appealing to the former<p>It&#x27;s absolutely fair to make the point that these models are not perfect, fail a bunch of the time, and to point out the edge cases where they suck. That moves the field forwards. But the hyperbole (as pointed out by another commenter) is very annoying.</div><br/><div id="40932827" class="c"><input type="checkbox" id="c-40932827" checked=""/><div class="controls bullet"><span class="by">numeri</span><span>|</span><a href="#40930092">parent</a><span>|</span><a href="#40930699">next</a><span>|</span><label class="collapse" for="c-40932827">[-]</label><label class="expand" for="c-40932827">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also confused about some of the figures&#x27; captions, which don&#x27;t seem to match the results:<p>- &quot;Only Sonnet-3.5 can count the squares in a majority of the images&quot;, but Sonnet-3, Gemini-1.5 and Sonnet-3.5 all have accuracy of &gt;50%<p>- &quot;Sonnet-3.5 tends to conservatively answer &quot;No&quot; regardless of the actual distance between the two circles.&quot;, but it somehow gets 91% accuracy? That doesn&#x27;t sound like it tends to answer &quot;No&quot; regardless of distance.</div><br/></div></div><div id="40930699" class="c"><input type="checkbox" id="c-40930699" checked=""/><div class="controls bullet"><span class="by">neuronet</span><span>|</span><a href="#40930092">parent</a><span>|</span><a href="#40932827">prev</a><span>|</span><a href="#40932881">next</a><span>|</span><label class="collapse" for="c-40930699">[-]</label><label class="expand" for="c-40930699">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, the paper has an emoji in the _title_, so I wouldn&#x27;t read it as a particularly particularly serious academic study as much as the equivalent of the Gawker of AI research. It is a &quot;gotcha&quot; paper that exploits some blind spots (sorry) that will easily be patched up with a few batches of training. I do think it highlights the lack of AGI in these things, which some people lacking situational awareness might need to see.</div><br/></div></div><div id="40932881" class="c"><input type="checkbox" id="c-40932881" checked=""/><div class="controls bullet"><span class="by">schneehertz</span><span>|</span><a href="#40930092">parent</a><span>|</span><a href="#40930699">prev</a><span>|</span><a href="#40932785">next</a><span>|</span><label class="collapse" for="c-40932881">[-]</label><label class="expand" for="c-40932881">[2 more]</label></div><br/><div class="children"><div class="content">I am not sure where their experimental data came from. I tested it on GPT-4o using the prompt and images they provided, and the success rate was quite high, with significant differences from the results they provided.</div><br/><div id="40933588" class="c"><input type="checkbox" id="c-40933588" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40930092">root</a><span>|</span><a href="#40932881">parent</a><span>|</span><a href="#40932785">next</a><span>|</span><label class="collapse" for="c-40933588">[-]</label><label class="expand" for="c-40933588">[1 more]</label></div><br/><div class="children"><div class="content">Their examples are here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;XAI&#x2F;vlmsareblind&#x2F;viewer&#x2F;default&#x2F;train" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;XAI&#x2F;vlmsareblind&#x2F;viewer&#x2F;defa...</a><p>ChatGPT whiffs completely on very obvious images.</div><br/></div></div></div></div></div></div><div id="40932785" class="c"><input type="checkbox" id="c-40932785" checked=""/><div class="controls bullet"><span class="by">randomtree</span><span>|</span><a href="#40930092">prev</a><span>|</span><a href="#40929196">next</a><span>|</span><label class="collapse" for="c-40932785">[-]</label><label class="expand" for="c-40932785">[1 more]</label></div><br/><div class="children"><div class="content">I guess I know what&#x27;s coming to every captcha tomorrow.</div><br/></div></div><div id="40929196" class="c"><input type="checkbox" id="c-40929196" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#40932785">prev</a><span>|</span><a href="#40932792">next</a><span>|</span><label class="collapse" for="c-40929196">[-]</label><label class="expand" for="c-40929196">[1 more]</label></div><br/><div class="children"><div class="content">Could some of the &quot;wrong&quot; answers be the LLM attempting to give an explanation rather than the answer, eg. instead of answering &#x27;X&#x27;, the LLM answers &#x27;The letter is partially hidden by the oval, so cannot be certain, but it appears to be the english letter X&#x27;.<p>The scoring criteria would rank this answer as &#x27;T&#x27;, which is wrong.</div><br/></div></div><div id="40932792" class="c"><input type="checkbox" id="c-40932792" checked=""/><div class="controls bullet"><span class="by">michaelhoney</span><span>|</span><a href="#40929196">prev</a><span>|</span><a href="#40929554">next</a><span>|</span><label class="collapse" for="c-40932792">[-]</label><label class="expand" for="c-40932792">[1 more]</label></div><br/><div class="children"><div class="content">This says to me that there are huge opportunities for improvement in providing  vision modules for LLMs. Human minds aren&#x27;t made of just one kind of thing: we have all sorts of hacky modular capabilities – there&#x27;s no reason to think that a future AGI wouldn&#x27;t also.</div><br/></div></div><div id="40929554" class="c"><input type="checkbox" id="c-40929554" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#40932792">prev</a><span>|</span><a href="#40930907">next</a><span>|</span><label class="collapse" for="c-40929554">[-]</label><label class="expand" for="c-40929554">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not anything like &quot;myopia&quot;, though.<p>FWIW I tried the line intersection and the circled letter test from the article with CogVLM (which is far from reaching the current SotA) and it correctly passed both. I haven&#x27;t tried it with Sonnet&#x2F;4o but I suspect there might be something wrong with how the author did their tests. Don&#x27;t get me wrong, but too many &quot;the model can&#x27;t do that&quot; claims ended up with demonstrations of the model doing exactly that...</div><br/></div></div><div id="40930907" class="c"><input type="checkbox" id="c-40930907" checked=""/><div class="controls bullet"><span class="by">nichohel</span><span>|</span><a href="#40929554">prev</a><span>|</span><a href="#40932985">next</a><span>|</span><label class="collapse" for="c-40930907">[-]</label><label class="expand" for="c-40930907">[2 more]</label></div><br/><div class="children"><div class="content">Vision language models are blind because they lack the Cartesian Theater, which you and I have. Which you and I say we have.</div><br/><div id="40932433" class="c"><input type="checkbox" id="c-40932433" checked=""/><div class="controls bullet"><span class="by">codeulike</span><span>|</span><a href="#40930907">parent</a><span>|</span><a href="#40932985">next</a><span>|</span><label class="collapse" for="c-40932433">[-]</label><label class="expand" for="c-40932433">[1 more]</label></div><br/><div class="children"><div class="content">Does the part of you that &#x27;looks at&#x27; your cartesian theatre also have a cartesian theatre?</div><br/></div></div></div></div><div id="40932985" class="c"><input type="checkbox" id="c-40932985" checked=""/><div class="controls bullet"><span class="by">kristianpaul</span><span>|</span><a href="#40930907">prev</a><span>|</span><a href="#40934021">next</a><span>|</span><label class="collapse" for="c-40932985">[-]</label><label class="expand" for="c-40932985">[1 more]</label></div><br/><div class="children"><div class="content">We see through thoughts and memories. We see when we desire, the vision just adds on a word pf thoughts and consciousness of being conscious.<p>Vision links thoughts with reality</div><br/></div></div><div id="40934021" class="c"><input type="checkbox" id="c-40934021" checked=""/><div class="controls bullet"><span class="by">nothrowaways</span><span>|</span><a href="#40932985">prev</a><span>|</span><a href="#40927567">next</a><span>|</span><label class="collapse" for="c-40934021">[-]</label><label class="expand" for="c-40934021">[1 more]</label></div><br/><div class="children"><div class="content">The next version will solve all of it.</div><br/></div></div><div id="40927567" class="c"><input type="checkbox" id="c-40927567" checked=""/><div class="controls bullet"><span class="by">rezaghanbari1</span><span>|</span><a href="#40934021">prev</a><span>|</span><a href="#40930065">next</a><span>|</span><label class="collapse" for="c-40927567">[-]</label><label class="expand" for="c-40927567">[3 more]</label></div><br/><div class="children"><div class="content">Some of these samples are shocking. How do these models answer chart-based questions, I mean when they can&#x27;t even count the intersections between two lines?</div><br/><div id="40928896" class="c"><input type="checkbox" id="c-40928896" checked=""/><div class="controls bullet"><span class="by">RodgerTheGreat</span><span>|</span><a href="#40927567">parent</a><span>|</span><a href="#40929838">next</a><span>|</span><label class="collapse" for="c-40928896">[-]</label><label class="expand" for="c-40928896">[1 more]</label></div><br/><div class="children"><div class="content">Same way they answer any question: piece together a statistically probable sequence of words to follow the prompt. All they know about an image is a handful of words a classifier might choose to describe it. If those words have nothing to do with the question being asked, they can&#x27;t nudge the model in the general direction of a correct answer, so it&#x27;s a crapshoot- even moreso than usual.</div><br/></div></div><div id="40929838" class="c"><input type="checkbox" id="c-40929838" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40927567">parent</a><span>|</span><a href="#40928896">prev</a><span>|</span><a href="#40930065">next</a><span>|</span><label class="collapse" for="c-40929838">[-]</label><label class="expand" for="c-40929838">[1 more]</label></div><br/><div class="children"><div class="content">The dataset most likely contains chart descriptions that describe the raw data, but not the visual interactions of the individual pixels.</div><br/></div></div></div></div><div id="40930065" class="c"><input type="checkbox" id="c-40930065" checked=""/><div class="controls bullet"><span class="by">axblount</span><span>|</span><a href="#40927567">prev</a><span>|</span><a href="#40931473">next</a><span>|</span><label class="collapse" for="c-40930065">[-]</label><label class="expand" for="c-40930065">[1 more]</label></div><br/><div class="children"><div class="content">Would you say they have <i>Blindsight</i>?</div><br/></div></div><div id="40931473" class="c"><input type="checkbox" id="c-40931473" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#40930065">prev</a><span>|</span><a href="#40930018">next</a><span>|</span><label class="collapse" for="c-40931473">[-]</label><label class="expand" for="c-40931473">[1 more]</label></div><br/><div class="children"><div class="content">I love some of the interpretations there. For example &quot;Fig. 10: Only Sonnet-3.5 can count the squares in a majority of the images.&quot;, when that model simply returns &quot;4&quot; for every question and happens to be right.</div><br/></div></div><div id="40930018" class="c"><input type="checkbox" id="c-40930018" checked=""/><div class="controls bullet"><span class="by">tantalor</span><span>|</span><a href="#40931473">prev</a><span>|</span><a href="#40929664">next</a><span>|</span><label class="collapse" for="c-40930018">[-]</label><label class="expand" for="c-40930018">[2 more]</label></div><br/><div class="children"><div class="content">Are the &quot;random-baseline accuracy&quot; numbers correct?<p>In the &quot;Two circles&quot; test, do they really have 50% chance of overlapping? I think this comes from &quot;Distances between circle perimeters: -0.15 to 0.5 times the diameter&quot; but doesn&#x27;t say the distribution they use.</div><br/><div id="40930183" class="c"><input type="checkbox" id="c-40930183" checked=""/><div class="controls bullet"><span class="by">jdlshore</span><span>|</span><a href="#40930018">parent</a><span>|</span><a href="#40929664">next</a><span>|</span><label class="collapse" for="c-40930183">[-]</label><label class="expand" for="c-40930183">[1 more]</label></div><br/><div class="children"><div class="content">They asked the AI a question with a yes&#x2F;no response. If the AI chose randomly, it would be correct 50% of the time. That’s what “random baseline accuracy” means.</div><br/></div></div></div></div><div id="40929664" class="c"><input type="checkbox" id="c-40929664" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#40930018">prev</a><span>|</span><label class="collapse" for="c-40929664">[-]</label><label class="expand" for="c-40929664">[4 more]</label></div><br/><div class="children"><div class="content">A few comments below talk about how tokenizing images using stuff like CLIP de-facto yields blurry image descriptions, and so these are ‘blind’ by some definitions. Another angle of blurring not much discussed is that the images are rescaled down; different resolutions for different models. I wouldn’t be surprised if Sonnet 3.5 had a higher res base image it feeds in to the model.<p>Either way, I would guess that we’ll need new model architectures for multimodal to get really good at some of this, and even then some of these tasks are adjacent to things that we know LLMs are already bad at (numeric logic, for instance).<p>As context lengths get longer, devoting more tokens to the image tokenization should help a bit here as well. Anyway, I’d anticipate next year we’d see 80s and 90s for most of these scores with next gen models.</div><br/><div id="40933599" class="c"><input type="checkbox" id="c-40933599" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40929664">parent</a><span>|</span><a href="#40929802">next</a><span>|</span><label class="collapse" for="c-40933599">[-]</label><label class="expand" for="c-40933599">[1 more]</label></div><br/><div class="children"><div class="content">At least for gpt 4o, it can create a facsimile of images that it still can’t analyze properly, so I think it’s more than just its “eyes” that are broken.<p>It clearly wasn’t trained on this task and suffers accordingly.<p>However, with chatgpt, it will create python to do the analysis and has better results.</div><br/></div></div><div id="40929802" class="c"><input type="checkbox" id="c-40929802" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40929664">parent</a><span>|</span><a href="#40933599">prev</a><span>|</span><label class="collapse" for="c-40929802">[-]</label><label class="expand" for="c-40929802">[2 more]</label></div><br/><div class="children"><div class="content">The problem with the current crop of projectors such as LLaVA is that as far as I know do not take the previous conversation into account. You only really get zero shot responses. This means that you cannot steer the model towards paying attention to specific instruction related details. The projector simply creates a token representation of the visuals (not necessarily human language tokens) and the LLM just processes that as usual.</div><br/><div id="40931104" class="c"><input type="checkbox" id="c-40931104" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#40929664">root</a><span>|</span><a href="#40929802">parent</a><span>|</span><label class="collapse" for="c-40931104">[-]</label><label class="expand" for="c-40931104">[1 more]</label></div><br/><div class="children"><div class="content">The original gpt4 did this too, it had almost no memory before or after the image provided. I haven’t tested gpt4o on this directly, but my feeling is that it’s better from casual usage.<p>I do think some of these thin line drawings are likely extra hard to tokenize depending on the image scaling sizes for tokenization. I’d wager thicker lines would help, although obviously not all of this is just ‘poor tokenization’.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>