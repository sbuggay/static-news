<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730451665107" as="style"/><link rel="stylesheet" href="styles.css?v=1730451665107"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://technicalwriting.dev/data/embeddings.html">Embeddings Are Underrated</a> <span class="domain">(<a href="https://technicalwriting.dev">technicalwriting.dev</a>)</span></div><div class="subtext"><span>misonic</span> | <span>22 comments</span></div><br/><div><div id="42014936" class="c"><input type="checkbox" id="c-42014936" checked=""/><div class="controls bullet"><span class="by">mrob</span><span>|</span><a href="#42014173">next</a><span>|</span><label class="collapse" for="c-42014936">[-]</label><label class="expand" for="c-42014936">[1 more]</label></div><br/><div class="children"><div class="content">Embeddings are the only aspect of modern AI I&#x27;m excited about because they&#x27;re the only one that gives more power to humans instead of taking it away. They&#x27;re the &quot;bicycle for our minds&quot; of Steve Jobs fame; intelligence amplification not intelligence replacement. IMO, the biggest improvement in computer usability in my lifetime was the introduction of fast and ubiquitous local search. I use Firefox&#x27;s &quot;Find in Page&quot; feature probably 10 or more times per day. I use find and grep probably every day. When I read man pages or logs, I navigate by search. Git would be vastly less useful without git grep. Embeddings have the potential to solve the biggest weakness of search by giving us fuzzy search that&#x27;s actually useful.</div><br/></div></div><div id="42014173" class="c"><input type="checkbox" id="c-42014173" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#42014936">prev</a><span>|</span><a href="#42014831">next</a><span>|</span><label class="collapse" for="c-42014173">[-]</label><label class="expand" for="c-42014173">[7 more]</label></div><br/><div class="children"><div class="content">That was a good post. Vector Embeddings are in some sense a summary of a doc that&#x27;s unique similar to a hashcode of a doc. It makes me think it would be cool if there were some universal standard for generating embeddings, but I guess they&#x27;ll be different for each AI model, so they can&#x27;t have the same kind of &quot;permanence&quot; hash codes have.<p>It definitely also seems like there should be lots of ways to utilize &quot;Cosine Similarity&quot; (or other closeness algos) in databases and other information processing apps that we haven&#x27;t really exploited yet. For example you could almost build a new kind of Job Search Service that matches job descriptions to job candidates based on nothing but a vector similarity between resume and job description. That&#x27;s probably so obvious it&#x27;s being done, already.</div><br/><div id="42014866" class="c"><input type="checkbox" id="c-42014866" checked=""/><div class="controls bullet"><span class="by">genuinelydang</span><span>|</span><a href="#42014173">parent</a><span>|</span><a href="#42014911">next</a><span>|</span><label class="collapse" for="c-42014866">[-]</label><label class="expand" for="c-42014866">[1 more]</label></div><br/><div class="children"><div class="content">”you could almost build a new kind of Job Search Service that matches job descriptions to job candidates”<p>The key word being ”almost”. Yes, you can get similarity matches between job requirements and candidate resumes, but those matches are not useful for the task of finding an optimal candidate for a job.<p>For example, say a job requires A and B.<p>Candidate 1 is a junior who has done some work with A, B and C.<p>Candidate 2 is a senior and knows A, B, C, D, E and F by heart. All are relevant to the job and would make 2 the optimal candidate, even though C–F are not explicitly stated in the job requirements.<p>Candidate 1 would seem a much better candidate than 2, because 1’s embedding vector is closer to the job embedding vector.</div><br/></div></div><div id="42014911" class="c"><input type="checkbox" id="c-42014911" checked=""/><div class="controls bullet"><span class="by">rasulkireev</span><span>|</span><a href="#42014173">parent</a><span>|</span><a href="#42014866">prev</a><span>|</span><a href="#42014449">next</a><span>|</span><label class="collapse" for="c-42014911">[-]</label><label class="expand" for="c-42014911">[1 more]</label></div><br/><div class="children"><div class="content">I tried doing something like that: <a href="https:&#x2F;&#x2F;gettjalerts.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gettjalerts.com&#x2F;</a><p>I added semantic search, but I&#x27;m workin on adding resume upload&#x2F;parsing to do automatic matching.</div><br/></div></div><div id="42014449" class="c"><input type="checkbox" id="c-42014449" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#42014173">parent</a><span>|</span><a href="#42014911">prev</a><span>|</span><a href="#42014776">next</a><span>|</span><label class="collapse" for="c-42014449">[-]</label><label class="expand" for="c-42014449">[1 more]</label></div><br/><div class="children"><div class="content">For one point of inspiration, see <a href="https:&#x2F;&#x2F;entropicthoughts.com&#x2F;determining-tag-quality" rel="nofollow">https:&#x2F;&#x2F;entropicthoughts.com&#x2F;determining-tag-quality</a><p>I really like the picture you are drawing with &quot;semantic hashes&quot;!</div><br/></div></div><div id="42014776" class="c"><input type="checkbox" id="c-42014776" checked=""/><div class="controls bullet"><span class="by">helloplanets</span><span>|</span><a href="#42014173">parent</a><span>|</span><a href="#42014449">prev</a><span>|</span><a href="#42014831">next</a><span>|</span><label class="collapse" for="c-42014776">[-]</label><label class="expand" for="c-42014776">[3 more]</label></div><br/><div class="children"><div class="content">I guess it might be possible to retroactively create an embeddings model which could take several different models&#x27; embeddings, and translate them into the same format.</div><br/><div id="42014878" class="c"><input type="checkbox" id="c-42014878" checked=""/><div class="controls bullet"><span class="by">genuinelydang</span><span>|</span><a href="#42014173">root</a><span>|</span><a href="#42014776">parent</a><span>|</span><a href="#42014831">next</a><span>|</span><label class="collapse" for="c-42014878">[-]</label><label class="expand" for="c-42014878">[2 more]</label></div><br/><div class="children"><div class="content">No. That’s like saying you can transplant a person’s neuronal action potentials into another person’s brain and have it make sense to them.</div><br/><div id="42015032" class="c"><input type="checkbox" id="c-42015032" checked=""/><div class="controls bullet"><span class="by">helloplanets</span><span>|</span><a href="#42014173">root</a><span>|</span><a href="#42014878">parent</a><span>|</span><a href="#42014831">next</a><span>|</span><label class="collapse" for="c-42015032">[-]</label><label class="expand" for="c-42015032">[1 more]</label></div><br/><div class="children"><div class="content">That metaphor is skipping the most important part in between! You wouldn&#x27;t be transplanting anything directly, you&#x27;d have a separate step in between, which would attempt to translate these action potentials.<p>The point of the translating model in between would be that it would re weight each and every one of the values of the embedding, after being trained on a massive dataset of original text -&gt; vector embedding for model A + vector embedding for model B. If you have billions of parameters trained to do this translation between just two specific models to start with, wouldn&#x27;t this be in the realm of possible?</div><br/></div></div></div></div></div></div></div></div><div id="42014831" class="c"><input type="checkbox" id="c-42014831" checked=""/><div class="controls bullet"><span class="by">nerdright</span><span>|</span><a href="#42014173">prev</a><span>|</span><a href="#42015069">next</a><span>|</span><label class="collapse" for="c-42014831">[-]</label><label class="expand" for="c-42014831">[1 more]</label></div><br/><div class="children"><div class="content">Great post indeed! I totally agree that embeddings are underrated. I feel like the &quot;information retrieval&#x2F;discovery&quot; world is stuck using spears (i.e., term&#x2F;keyword-based discovery) instead of embracing the modern tools (i.e., semantic-based discovery).<p>The other day I found myself trying to figure out some common themes across a bunch of comments I was looking at. I felt lazy to go through all of them so I turned my attention to the &quot;Sentence Transformers&quot; lib. I converted each comment into a vector embedding, applied k-means clustering on these embeddings, then gave each cluster to ChatGPT to summarize the corresponding comments. I have to admit, it was fun doing this and saved me lots of time!</div><br/></div></div><div id="42015069" class="c"><input type="checkbox" id="c-42015069" checked=""/><div class="controls bullet"><span class="by">imgabe</span><span>|</span><a href="#42014831">prev</a><span>|</span><a href="#42014683">next</a><span>|</span><label class="collapse" for="c-42015069">[-]</label><label class="expand" for="c-42015069">[1 more]</label></div><br/><div class="children"><div class="content">Is there any benefit to fine-tuning a model on your corpus before using it to generate embeddings? Would that improve the quality of the matches?</div><br/></div></div><div id="42014683" class="c"><input type="checkbox" id="c-42014683" checked=""/><div class="controls bullet"><span class="by">thund</span><span>|</span><a href="#42015069">prev</a><span>|</span><a href="#42014036">next</a><span>|</span><label class="collapse" for="c-42014683">[-]</label><label class="expand" for="c-42014683">[1 more]</label></div><br/><div class="children"><div class="content">Doesn’t OpenAI embedding model support 8191&#x2F;8192 tokens? That aside, declaring a winner by token size is misleading. There are more important factors like cross language support and precision for example</div><br/></div></div><div id="42014036" class="c"><input type="checkbox" id="c-42014036" checked=""/><div class="controls bullet"><span class="by">kaycebasques</span><span>|</span><a href="#42014683">prev</a><span>|</span><a href="#42014634">next</a><span>|</span><label class="collapse" for="c-42014036">[-]</label><label class="expand" for="c-42014036">[3 more]</label></div><br/><div class="children"><div class="content">Cool, first time I&#x27;ve seen one of my posts trend without me submitting it myself. Hopefully it&#x27;s clear from the domain name and intro that I&#x27;m suggesting technical writers are underrating how useful embeddings can be in our work. I know ML practitioners do not underrate them.</div><br/><div id="42014384" class="c"><input type="checkbox" id="c-42014384" checked=""/><div class="controls bullet"><span class="by">donavanm</span><span>|</span><a href="#42014036">parent</a><span>|</span><a href="#42014107">next</a><span>|</span><label class="collapse" for="c-42014384">[-]</label><label class="expand" for="c-42014384">[1 more]</label></div><br/><div class="children"><div class="content">You might want to highlight chunking and how embeddings can&#x2F;should represent subsections of your document as well. It seems relevant to me for cases like similarity or semantics search, getting the reader to the relevant portion of the document or page.<p>Theres probably some interesting ideas around tokenization and metadata as well. For example, if you’re processing the raw file I expect you want to strip out a lot of markup before tokenization of the content. Conversely, some markup like code blocks or examples would be meaningful for tokenization and embedding anyways.<p>I wonder if both of those ideas can be combined for something like automated footnotes and annotations. Linking or mouseover relevant content from elsewhere in the documentation.</div><br/></div></div><div id="42014107" class="c"><input type="checkbox" id="c-42014107" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42014036">parent</a><span>|</span><a href="#42014384">prev</a><span>|</span><a href="#42014634">next</a><span>|</span><label class="collapse" for="c-42014107">[-]</label><label class="expand" for="c-42014107">[1 more]</label></div><br/><div class="children"><div class="content">Yeah embeddings are the unsung killer feature of LLMs</div><br/></div></div></div></div><div id="42014634" class="c"><input type="checkbox" id="c-42014634" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#42014036">prev</a><span>|</span><a href="#42014125">next</a><span>|</span><label class="collapse" for="c-42014634">[-]</label><label class="expand" for="c-42014634">[1 more]</label></div><br/><div class="children"><div class="content">Great post!<p>One quick minor note is that the resulting embeddings for the same text string could be different, depending on what you specify the input type as for retrieval tasks (i.e. query or document) -- check out the `input_type` parameter here: <a href="https:&#x2F;&#x2F;docs.voyageai.com&#x2F;reference&#x2F;embeddings-api" rel="nofollow">https:&#x2F;&#x2F;docs.voyageai.com&#x2F;reference&#x2F;embeddings-api</a>.</div><br/></div></div><div id="42014125" class="c"><input type="checkbox" id="c-42014125" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#42014634">prev</a><span>|</span><a href="#42014850">next</a><span>|</span><label class="collapse" for="c-42014125">[-]</label><label class="expand" for="c-42014125">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure why the voyage-3 models aren&#x27;t on the MTEB leaderboard. The code for the leaderboard suggests they should be there: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard&#x2F;commit&#x2F;b7faae9e2db6d721cacc15cb923f29b8bb9115a4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard&#x2F;commit&#x2F;b7faae...</a><p>But I don&#x27;t see them when I filter the list for &#x27;voyage&#x27;.</div><br/><div id="42014613" class="c"><input type="checkbox" id="c-42014613" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#42014125">parent</a><span>|</span><a href="#42014465">next</a><span>|</span><label class="collapse" for="c-42014613">[-]</label><label class="expand" for="c-42014613">[1 more]</label></div><br/><div class="children"><div class="content">(I work at Voyage)<p>Many of the top-performing models that you see on the MTEB retrieval for English and Chinese tend to overfit to the benchmark nowadays. voyage-3 and voyage-3-lite are also pretty small in size compared to a lot of the 7B models that take the top spots, and we don&#x27;t want to hurt performance on other real-world tasks just to do well on MTEB.</div><br/></div></div><div id="42014465" class="c"><input type="checkbox" id="c-42014465" checked=""/><div class="controls bullet"><span class="by">newrotik</span><span>|</span><a href="#42014125">parent</a><span>|</span><a href="#42014613">prev</a><span>|</span><a href="#42014850">next</a><span>|</span><label class="collapse" for="c-42014465">[-]</label><label class="expand" for="c-42014465">[1 more]</label></div><br/><div class="children"><div class="content">It is unclear this model should be on that leaderboard because we don&#x27;t know whether it has been trained on mteb test data.<p>It is worth noting that their own published material [0] does not entail any score from any dataset from the  mteb benchmark.<p>This may sound nit picky, but considering transformers&#x27; parroting capabilities, having seen test data during training should be expected to completely invalidate those scores.<p>[0] see excel spreadsheet linked here <a href="https:&#x2F;&#x2F;blog.voyageai.com&#x2F;2024&#x2F;09&#x2F;18&#x2F;voyage-3&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.voyageai.com&#x2F;2024&#x2F;09&#x2F;18&#x2F;voyage-3&#x2F;</a></div><br/></div></div></div></div><div id="42014850" class="c"><input type="checkbox" id="c-42014850" checked=""/><div class="controls bullet"><span class="by">empiko</span><span>|</span><a href="#42014125">prev</a><span>|</span><a href="#42014495">next</a><span>|</span><label class="collapse" for="c-42014850">[-]</label><label class="expand" for="c-42014850">[2 more]</label></div><br/><div class="children"><div class="content">My hot take: embeddings are overrated. They are overfitted on word overlap, leading to both many false positives and false negatives. If you identify a specific problem with them (&quot;I really want to match items like these, but it does not work&quot;), it is almost impossible to fix them. I often see them being used inappropriately, by people who read about their magical properties, but didn&#x27;t really care about evaluating their results.</div><br/><div id="42014943" class="c"><input type="checkbox" id="c-42014943" checked=""/><div class="controls bullet"><span class="by">cheevly</span><span>|</span><a href="#42014850">parent</a><span>|</span><a href="#42014495">next</a><span>|</span><label class="collapse" for="c-42014943">[-]</label><label class="expand" for="c-42014943">[1 more]</label></div><br/><div class="children"><div class="content">You can easily fix this using embedding arithmetic to build embedding classifiers.</div><br/></div></div></div></div><div id="42014495" class="c"><input type="checkbox" id="c-42014495" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#42014850">prev</a><span>|</span><label class="collapse" for="c-42014495">[-]</label><label class="expand" for="c-42014495">[1 more]</label></div><br/><div class="children"><div class="content">Is there some way to compare different embeddings for different use cases?</div><br/></div></div></div></div></div></div></div></body></html>