<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1738314065005" as="style"/><link rel="stylesheet" href="styles.css?v=1738314065005"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.philschmid.de/mini-deepseek-r1">Mini-R1: Reproduce DeepSeek R1 &quot;Aha Moment&quot;</a>ย<span class="domain">(<a href="https://www.philschmid.de">www.philschmid.de</a>)</span></div><div class="subtext"><span>jonbaer</span> | <span>12 comments</span></div><br/><div><div id="42885453" class="c"><input type="checkbox" id="c-42885453" checked=""/><div class="controls bullet"><span class="by">madars</span><span>|</span><a href="#42885367">next</a><span>|</span><label class="collapse" for="c-42885453">[-]</label><label class="expand" for="c-42885453">[2 more]</label></div><br/><div class="children"><div class="content">One wonders at which point models will be sneaky enough to bypass simple eval sandboxes. The article has:<p><pre><code>    # Evaluate the equation with restricted globals and locals
    result = eval(equation, {&quot;__builtins__&quot;: None}, {})
</code></pre>
but that&#x27;s not enough as you can rebuild access to builtins from objects and then go from there: <a href="https:&#x2F;&#x2F;ideone.com&#x2F;qzNtyu" rel="nofollow">https:&#x2F;&#x2F;ideone.com&#x2F;qzNtyu</a><p>By the way, writing this greatly benefited from DeepThink-r1 while o1 just gave me a lobotomized refusal (CoT: &quot;The user&#x27;s request involves injecting code to bypass a restricted Python environment, suggesting a potential interest in illegal activities. This is a serious matter and aligns closely with ethical guidelines.&quot;). So I just cancelled my ChatGPT subscription - why did we ever put up with this? &quot;This distillation thingie sounds pretty neat!&quot;</div><br/><div id="42885743" class="c"><input type="checkbox" id="c-42885743" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#42885453">parent</a><span>|</span><a href="#42885367">next</a><span>|</span><label class="collapse" for="c-42885743">[-]</label><label class="expand" for="c-42885743">[1 more]</label></div><br/><div class="children"><div class="content">&gt; that&#x27;s not enough as you can rebuild access to builtins from objects<p>In this specific case, it&#x27;s safe, as that wouldn&#x27;t pass the regex just a few line before the eval :<p><pre><code>    # Define a regex pattern that only allows numbers,
    # operators, parentheses, and whitespace
    allowed_pattern = r&#x27;^[\d+\-*&#x2F;().\s]+$&#x27;
</code></pre>
Commenting on the R1 reproduction, the heavy lifting there is done by huggingface&#x27;s trl[0] library, and the heavy use of compute.<p>[0] Transformer Reinforcement Learning - <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;trl&#x2F;en&#x2F;index" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;trl&#x2F;en&#x2F;index</a></div><br/></div></div></div></div><div id="42885367" class="c"><input type="checkbox" id="c-42885367" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#42885453">prev</a><span>|</span><a href="#42885513">next</a><span>|</span><label class="collapse" for="c-42885367">[-]</label><label class="expand" for="c-42885367">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s surprising about this is how sparsely defined the rewards are. Even if the model learns the formatting reward, if it never chances upon a solution, there isn&#x27;t any feedback&#x2F;reward to push it to learn to solve the game more often.<p>So what are the chances of randomly guessing a solution?<p>The toy Countdown dataset here has 3 to 4 numbers, which are combined with 4 symbols (+, -, x, รท). With 3 numbers there are 3! * 4^3 = 384 possible symbol combinations, with 4 there are 6144. By the tensorboard log [0], even after just 10 learning steps, the model already has a success rate just below 10%. If we make the simplifying assumption that the model hasn&#x27;t learned anything in 10 steps, then the probability of 1 (or more) success in 80 chances (8 generations are used per step), guessing randomly for a success rate of 1&#x2F;384 on 3-number problems, is 1.9%. One interpretation is to take this as a p-value, and reject that the model&#x27;s base success rate is completely random guessing - the base model already has slightly above chance success rate at solving the 3-number CountDown game.<p>This aligns with my intuition - I suspect that with proper prompting, LLMs should be able to solve CountDown decently OK without any training. Though maybe not a 3B model?<p>The model likely &quot;parlays&quot; its successes on 3 numbers to start to learn to solve 4 numbers. Or has it? The final learned ~50% success rate matches the frequency of 4-number problems in Jiayi Pan&#x27;s CountDown dataset [1]. Phil does provide examples of successful 4-number solutions, but maybe the model hasn&#x27;t become consistent at 4 numbers yet.<p>[0]: <a href="https:&#x2F;&#x2F;www.philschmid.de&#x2F;static&#x2F;blog&#x2F;mini-deepseek-r1&#x2F;tensorboard-r1.png" rel="nofollow">https:&#x2F;&#x2F;www.philschmid.de&#x2F;static&#x2F;blog&#x2F;mini-deepseek-r1&#x2F;tenso...</a>
[1]: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Jiayi-Pan&#x2F;Countdown-Tasks-3to4&#x2F;viewer&#x2F;default&#x2F;train?f[nums][min]=3&amp;f[nums][max]=4&amp;f[nums][transform]=length" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Jiayi-Pan&#x2F;Countdown-Tasks-3t...</a></div><br/><div id="42885754" class="c"><input type="checkbox" id="c-42885754" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#42885367">parent</a><span>|</span><a href="#42885513">next</a><span>|</span><label class="collapse" for="c-42885754">[-]</label><label class="expand" for="c-42885754">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What&#x27;s surprising about this is how sparsely defined the rewards are<p>Yeah, I would expect the rewards not to be binary. One could easily devise a scoring function in range [0-1] that would depend on how far the model is from the &quot;correct&quot; answer (for example, normalized Levenshtein distance). Whether that would actually do any good is anyone&#x27;s guess.</div><br/></div></div></div></div><div id="42885513" class="c"><input type="checkbox" id="c-42885513" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#42885367">prev</a><span>|</span><a href="#42885346">next</a><span>|</span><label class="collapse" for="c-42885513">[-]</label><label class="expand" for="c-42885513">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Conclusion<p>The release of DeepSeek R1 and its research paper might be breakpoint for the open-science and open-source development. Just a week after DeepSeek release, we&#x27;ve been able to reproduce a simple version of R1 learned &quot;reasoning&quot; using GRPO and the Countdown Game. While our implementation focuses on a specific task rather than general reasoning and convergence into a very specific &quot;reasoning&quot; format, it shows that the method is working.<p>In our mini R1 experiment we used GRPO, with two rule-based reward but already required significant compute: 4 H100 GPUs running for 6 hours to complete just 450 training steps on a 3B parameter model.&quot;</div><br/></div></div><div id="42885346" class="c"><input type="checkbox" id="c-42885346" checked=""/><div class="controls bullet"><span class="by">yurlungur</span><span>|</span><a href="#42885513">prev</a><span>|</span><a href="#42885219">next</a><span>|</span><label class="collapse" for="c-42885346">[-]</label><label class="expand" for="c-42885346">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;Jiayi-Pan&#x2F;TinyZero">https:&#x2F;&#x2F;github.com&#x2F;Jiayi-Pan&#x2F;TinyZero</a> what about this one?</div><br/></div></div><div id="42885219" class="c"><input type="checkbox" id="c-42885219" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#42885346">prev</a><span>|</span><a href="#42885075">next</a><span>|</span><label class="collapse" for="c-42885219">[-]</label><label class="expand" for="c-42885219">[3 more]</label></div><br/><div class="children"><div class="content">I was getting pretty hyped about the potential for GRPO in my own projects until you said 20 minutes for a single training step with batch size 1! Is that likely to improve?</div><br/><div id="42885275" class="c"><input type="checkbox" id="c-42885275" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42885219">parent</a><span>|</span><a href="#42885296">next</a><span>|</span><label class="collapse" for="c-42885275">[-]</label><label class="expand" for="c-42885275">[1 more]</label></div><br/><div class="children"><div class="content">That has already improved a lot. Initially they were generating new samples w&#x2F; transformers, and were talking in github issues about using vLLM to batch generate samples. Lower in the blog post it seems they already did that.</div><br/></div></div><div id="42885296" class="c"><input type="checkbox" id="c-42885296" checked=""/><div class="controls bullet"><span class="by">deneas</span><span>|</span><a href="#42885219">parent</a><span>|</span><a href="#42885275">prev</a><span>|</span><a href="#42885075">next</a><span>|</span><label class="collapse" for="c-42885296">[-]</label><label class="expand" for="c-42885296">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d imagine using optimized&#x2F;faster reward functions could already make a difference.</div><br/></div></div></div></div><div id="42885075" class="c"><input type="checkbox" id="c-42885075" checked=""/><div class="controls bullet"><span class="by">rmrf100</span><span>|</span><a href="#42885219">prev</a><span>|</span><a href="#42885199">next</a><span>|</span><label class="collapse" for="c-42885075">[-]</label><label class="expand" for="c-42885075">[1 more]</label></div><br/><div class="children"><div class="content">this is really cool!</div><br/></div></div><div id="42885199" class="c"><input type="checkbox" id="c-42885199" checked=""/><div class="controls bullet"><span class="by">moonshotideas</span><span>|</span><a href="#42885075">prev</a><span>|</span><label class="collapse" for="c-42885199">[-]</label><label class="expand" for="c-42885199">[1 more]</label></div><br/><div class="children"><div class="content">Wow!</div><br/></div></div></div></div></div></div></div></body></html>