<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719306050237" as="style"/><link rel="stylesheet" href="styles.css?v=1719306050237"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://simonwillison.net/2024/Jun/17/cli-language-models/">Language models on the command line</a> <span class="domain">(<a href="https://simonwillison.net">simonwillison.net</a>)</span></div><div class="subtext"><span>rednafi</span> | <span>33 comments</span></div><br/><div><div id="40783589" class="c"><input type="checkbox" id="c-40783589" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40785613">next</a><span>|</span><label class="collapse" for="c-40783589">[-]</label><label class="expand" for="c-40783589">[15 more]</label></div><br/><div class="children"><div class="content">This was a workshop I gave in my <a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llm.datasette.io&#x2F;</a> CLI tool.<p>What other CLI tools are people using to work with LLMs in the terminal?<p>There one comment here about <a href="https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider">https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider</a> and Ollama is probably the most widely used CLI tool at the moment: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;README.md#quickstart">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;README.md#quickst...</a></div><br/><div id="40785421" class="c"><input type="checkbox" id="c-40785421" checked=""/><div class="controls bullet"><span class="by">FergusArgyll</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40783653">next</a><span>|</span><label class="collapse" for="c-40785421">[-]</label><label class="expand" for="c-40785421">[1 more]</label></div><br/><div class="children"><div class="content">I made my own python wrapper around the Gemini API, I wanted one feature:
there is a default system prompt but -s overrides that with a temporary system prompt. -p is for the prompt, if it is left out, the cli is interactive (one long chat until I exit) but with -p I can use regular linux utils<p>for example: I wrote a short bash script which uses yt-dlp and ffmpeg to download a song, convert it to my preferred format and then uses gemini to add metadata.<p><pre><code>  artist=$(gemini -s &quot;Please respond with the name of the Artist based on 
  the songs title. do not use any other words, just the artist name.
  example:
  &#x27;Bruce Springsteen - Old Dan Tucker [S-GHbDFrwlU].opus&#x27;
  Bruce Springsteen&quot; -p &quot;$opus_file&quot; | tr -d &#x27;\n&#x27;)</code></pre></div><br/></div></div><div id="40783653" class="c"><input type="checkbox" id="c-40783653" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40785421">prev</a><span>|</span><a href="#40784900">next</a><span>|</span><label class="collapse" for="c-40783653">[-]</label><label class="expand" for="c-40783653">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; What other CLI tools are people using to work with LLMs in the terminal?</i><p>I use aichat: <a href="https:&#x2F;&#x2F;github.com&#x2F;sigoden&#x2F;aichat">https:&#x2F;&#x2F;github.com&#x2F;sigoden&#x2F;aichat</a><p>I especially like the terminal integration where I can type a natural language request at the terminal and press Alt+E to have it converted to a command to run.</div><br/><div id="40784797" class="c"><input type="checkbox" id="c-40784797" checked=""/><div class="controls bullet"><span class="by">kelvie</span><span>|</span><a href="#40783589">root</a><span>|</span><a href="#40783653">parent</a><span>|</span><a href="#40784900">next</a><span>|</span><label class="collapse" for="c-40784797">[-]</label><label class="expand" for="c-40784797">[1 more]</label></div><br/><div class="children"><div class="content">I also use aichat. For some reason (well, I know the reason, it&#x27;s shell quoting) I don&#x27;t like passing prompts in via command line, so having a simple text-based TUI like a readline prompt is nice.</div><br/></div></div></div></div><div id="40784900" class="c"><input type="checkbox" id="c-40784900" checked=""/><div class="controls bullet"><span class="by">nyellin</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40783653">prev</a><span>|</span><a href="#40785725">next</a><span>|</span><label class="collapse" for="c-40784900">[-]</label><label class="expand" for="c-40784900">[1 more]</label></div><br/><div class="children"><div class="content">Most of the cli tools just wrap an LLM, but don&#x27;t give it access to the data it needs to be useful. Aider is an exception of course - it gives great results because it feeds the LLM your source files.<p>We built <a href="http:&#x2F;&#x2F;github.com&#x2F;robusta-dev&#x2F;holmesgpt&#x2F;">http:&#x2F;&#x2F;github.com&#x2F;robusta-dev&#x2F;holmesgpt&#x2F;</a> to investigate Prometheus&#x2F;Jira&#x2F;PagerDuty issues. We&#x27;re able to get pretty good results (we benchmark extensively) because we use function-calling to give the LLM read acess to relevant data. I think we&#x27;re the only open source AIOps tool, and the only AIOps tool period that does something more complex than RAG + summarization.</div><br/></div></div><div id="40785725" class="c"><input type="checkbox" id="c-40785725" checked=""/><div class="controls bullet"><span class="by">Jaydenaus</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40784900">prev</a><span>|</span><a href="#40784992">next</a><span>|</span><label class="collapse" for="c-40785725">[-]</label><label class="expand" for="c-40785725">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using this one: <a href="https:&#x2F;&#x2F;github.com&#x2F;egoist&#x2F;shell-ask">https:&#x2F;&#x2F;github.com&#x2F;egoist&#x2F;shell-ask</a><p>very similar, although the ability to continue a conversation like you can in yours is a killer feature I wish it had.</div><br/></div></div><div id="40784992" class="c"><input type="checkbox" id="c-40784992" checked=""/><div class="controls bullet"><span class="by">snthpy</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40785725">prev</a><span>|</span><a href="#40784101">next</a><span>|</span><label class="collapse" for="c-40784992">[-]</label><label class="expand" for="c-40784992">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t tried it yet but this appeared a few days ago and I&#x27;m a big fan of Textual.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;paulrobello&#x2F;parllama">https:&#x2F;&#x2F;github.com&#x2F;paulrobello&#x2F;parllama</a></div><br/></div></div><div id="40784101" class="c"><input type="checkbox" id="c-40784101" checked=""/><div class="controls bullet"><span class="by">evmar</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40784992">prev</a><span>|</span><a href="#40786028">next</a><span>|</span><label class="collapse" for="c-40784101">[-]</label><label class="expand" for="c-40784101">[4 more]</label></div><br/><div class="children"><div class="content">I am using a hacky one I wrote myself.<p>I looked at llm but it doesn&#x27;t appear to have a mechanism for multi-shot prompting, where you provide both prompts and responses within your query.  (Ref <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;prompt-engineering&#x2F;tactic-provide-examples" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;prompt-engineering&#x2F;t...</a> .)  Maybe take this as a feature request?<p>It feels like the &#x27;template&#x27; system in llm might be able to encompass this but the docs don&#x27;t appear to provide a reference for the yaml format, only examples.  I guess that is another feature request, sorry!<p>(BTW if you haven&#x27;t seen <a href="https:&#x2F;&#x2F;docs.divio.com&#x2F;documentation-system&#x2F;" rel="nofollow">https:&#x2F;&#x2F;docs.divio.com&#x2F;documentation-system&#x2F;</a> it really changed how I think about documentation)</div><br/><div id="40784147" class="c"><input type="checkbox" id="c-40784147" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40783589">root</a><span>|</span><a href="#40784101">parent</a><span>|</span><a href="#40786028">next</a><span>|</span><label class="collapse" for="c-40784147">[-]</label><label class="expand" for="c-40784147">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, I&#x27;ve been thinking a bit about the multi shot thing. I&#x27;ve had great results from Claude by &quot;faking&quot; the previous conversation to include example question&#x2F;answer pairs.<p>With LLM you can do that using the undocumented Python Conversation API, but it&#x27;s undocumented for a reason (I don&#x27;t think it&#x27;s good enough yet). You could also fake a previous conversation through the CLI tool but that is VERY undocumented - you would have to write fake rows into the SQLite database!<p>I also want to support the Claude thing where you can prefill the start of the response - amazing for things like forcing HTML by refilling an HTML doctype.</div><br/><div id="40784249" class="c"><input type="checkbox" id="c-40784249" checked=""/><div class="controls bullet"><span class="by">evmar</span><span>|</span><a href="#40783589">root</a><span>|</span><a href="#40784147">parent</a><span>|</span><a href="#40786028">next</a><span>|</span><label class="collapse" for="c-40784249">[-]</label><label class="expand" for="c-40784249">[2 more]</label></div><br/><div class="children"><div class="content">In case it helps any, here are some more details about what I used it for.  (Summary: providing examples of a specific translation task, to make it use a particular API in the output.)
<a href="https:&#x2F;&#x2F;inuh.net&#x2F;@evmar&#x2F;112001414385042731" rel="nofollow">https:&#x2F;&#x2F;inuh.net&#x2F;@evmar&#x2F;112001414385042731</a><p>And what I expected of llm is for the template file to (optionally) contain an array of prompt&#x2F;response pairs.  You could even imagine the --save flag constructing one from an ongoing conversation of llm -c maybe.</div><br/><div id="40784463" class="c"><input type="checkbox" id="c-40784463" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40783589">root</a><span>|</span><a href="#40784249">parent</a><span>|</span><a href="#40786028">next</a><span>|</span><label class="collapse" for="c-40784463">[-]</label><label class="expand" for="c-40784463">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, doing this with templates is a great idea.</div><br/></div></div></div></div></div></div></div></div><div id="40786028" class="c"><input type="checkbox" id="c-40786028" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40784101">prev</a><span>|</span><a href="#40783916">next</a><span>|</span><label class="collapse" for="c-40786028">[-]</label><label class="expand" for="c-40786028">[1 more]</label></div><br/><div class="children"><div class="content">I am experimenting with my own tool[1] which is built using asnycio and prompt-toolkit.  Made it particularily easy to define function_tools via a decorator which can automagically transform async defs into pydantic models.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;mlang&#x2F;ass">https:&#x2F;&#x2F;github.com&#x2F;mlang&#x2F;ass</a></div><br/></div></div><div id="40783916" class="c"><input type="checkbox" id="c-40783916" checked=""/><div class="controls bullet"><span class="by">pjot</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40786028">prev</a><span>|</span><a href="#40783840">next</a><span>|</span><label class="collapse" for="c-40783916">[-]</label><label class="expand" for="c-40783916">[1 more]</label></div><br/><div class="children"><div class="content">openai&#x27;s python client includes a cli actually - im not sure if its really documented anywhere.<p><pre><code>  $ openai api chat.completions.create -g &#x27;user&#x27; &#x27;say hello&#x27; -m &#x27;gpt-3.5-turbo&#x27;

  $ Hello! How can I assist you today?</code></pre></div><br/></div></div><div id="40783840" class="c"><input type="checkbox" id="c-40783840" checked=""/><div class="controls bullet"><span class="by">bt1a</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40783916">prev</a><span>|</span><a href="#40784217">next</a><span>|</span><label class="collapse" for="c-40783840">[-]</label><label class="expand" for="c-40783840">[1 more]</label></div><br/><div class="children"><div class="content">(aichat in tangential comment looks much better) Besides aider and ollama, I think shell_gpt <a href="https:&#x2F;&#x2F;github.com&#x2F;TheR1D&#x2F;shell_gpt">https:&#x2F;&#x2F;github.com&#x2F;TheR1D&#x2F;shell_gpt</a> is great for quick chats &#x2F; lookups. Being able to quickly cat files to repl sessions saves a lot of time.<p>I need to integrate distil whisper large v3, aider, and shell_gpt to tidy up a lot of my disjointed LLM use. As someone else mentioned, the commits created by aider allow me to &quot;skip&quot; some intermediate steps that would be required when working on coding tasks with other frameworks.</div><br/></div></div><div id="40784217" class="c"><input type="checkbox" id="c-40784217" checked=""/><div class="controls bullet"><span class="by">saulpw</span><span>|</span><a href="#40783589">parent</a><span>|</span><a href="#40783840">prev</a><span>|</span><a href="#40785613">next</a><span>|</span><label class="collapse" for="c-40784217">[-]</label><label class="expand" for="c-40784217">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve appreciated <a href="https:&#x2F;&#x2F;github.com&#x2F;cthulahoops&#x2F;chatcli">https:&#x2F;&#x2F;github.com&#x2F;cthulahoops&#x2F;chatcli</a> which is simple and straightforward.</div><br/></div></div></div></div><div id="40785613" class="c"><input type="checkbox" id="c-40785613" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#40783589">prev</a><span>|</span><a href="#40783988">next</a><span>|</span><label class="collapse" for="c-40785613">[-]</label><label class="expand" for="c-40785613">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We have implemented basic RAG—Retrieval Augmented Generation, where search results are used to answer a question—using a terminal script that scrapes search results from Google and pipes them into an LLM.<p>I love this. Simple and effective. RAG is just search leveled up with LLMs. Such an obvious thing to do. We know how to do search and can use it to unlock vast amounts of knowledge. Instead of letting LLMs dream up facts by compressing all knowledge into them, a better use of them is letting them summarize and reason about the facts it finds. IMHO the art is actually going to be in letting them come up with the right query as well. Or queries. It could be a lot more exhaustive in its searches than we could be.</div><br/><div id="40785943" class="c"><input type="checkbox" id="c-40785943" checked=""/><div class="controls bullet"><span class="by">RansomStark</span><span>|</span><a href="#40785613">parent</a><span>|</span><a href="#40783988">next</a><span>|</span><label class="collapse" for="c-40785943">[-]</label><label class="expand" for="c-40785943">[1 more]</label></div><br/><div class="children"><div class="content">While I agree with the sentiment in general, I&#x27;ve came to the conclusion that what I really want is the flexibility of the natural language interface that LLM&#x27;s provide, and the return of the correct document. No &#x27;reasoning&#x27;, no summarizing, just better search [0].<p>The issue with the current generation of models is that they can&#x27;t reason, they may do very well at pretending to reason, but they can&#x27;t [1]. Reasoning requires the ability to identify and reuse patterns, and while there has been some advancement in this area [2] with getting models to learn the underlying pattern and rule, it doesn&#x27;t generalize. This results in models that will happily tell you that a statement is both true and false, and be unable to identify the logical problem with that.<p>Even creating summaries is difficult, and LLM&#x27;s are more than happy to hallucinate even when summarizing documents, providing incorrect, or entirely made up facts [3]. The general workaround is multiple runs with the same work and averaging the response, but that&#x27;s a lot of work, and energy.<p>[0] <a href="https:&#x2F;&#x2F;win-vector.com&#x2F;2024&#x2F;05&#x2F;21&#x2F;i-want-flexible-queries-not-rag&#x2F;" rel="nofollow">https:&#x2F;&#x2F;win-vector.com&#x2F;2024&#x2F;05&#x2F;21&#x2F;i-want-flexible-queries-no...</a><p>[1] <a href="https:&#x2F;&#x2F;medium.com&#x2F;@konstantine_45825&#x2F;gpt-4-cant-reason-2eab795e2523" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@konstantine_45825&#x2F;gpt-4-cant-reason-2eab...</a><p>[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.15071" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.15071</a><p>[3] <a href="https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;gpt-4o-hallucinating-at-temp-0-unusable-in-production&#x2F;746750" rel="nofollow">https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;gpt-4o-hallucinating-at-temp-...</a></div><br/></div></div></div></div><div id="40783988" class="c"><input type="checkbox" id="c-40783988" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#40785613">prev</a><span>|</span><a href="#40783939">next</a><span>|</span><label class="collapse" for="c-40783988">[-]</label><label class="expand" for="c-40783988">[1 more]</label></div><br/><div class="children"><div class="content">Fantastic work here! I&#x27;m working on a local tool, affectionately called Descartes, which does something similar—but with a spotlight-like UX for the non-hackers out there.<p>I do think that LLMs have the potential to fundamentally change the way we interact with our computers. There&#x27;s a lot of edge cases (especially when combining it with the inaccurate science of screen readers) but it&#x27;s pretty mind-blowing when it works. I&#x27;m working on a blog post, but here&#x27;s my little proof of concept working on both Windows in a web browser[1] and MacOS in the Finder [2].<p>[1] <a href="https:&#x2F;&#x2F;vimeo.com&#x2F;931907811" rel="nofollow">https:&#x2F;&#x2F;vimeo.com&#x2F;931907811</a><p>[2] <a href="https:&#x2F;&#x2F;dvt.name&#x2F;wp-content&#x2F;uploads&#x2F;2024&#x2F;04&#x2F;image-11.png" rel="nofollow">https:&#x2F;&#x2F;dvt.name&#x2F;wp-content&#x2F;uploads&#x2F;2024&#x2F;04&#x2F;image-11.png</a></div><br/></div></div><div id="40783939" class="c"><input type="checkbox" id="c-40783939" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#40783988">prev</a><span>|</span><a href="#40783442">next</a><span>|</span><label class="collapse" for="c-40783939">[-]</label><label class="expand" for="c-40783939">[5 more]</label></div><br/><div class="children"><div class="content">I wrote one to help with creating command line commands. It just hits openai api with a prompt asking for just a code block to run on bash + whatever is passed in, and then it prints the command out. I wrote it because I can never remember all the weird command args for all the tools.<p>$ bashy find large files over 10 gb<p>find &#x2F; -type f -size +10G</div><br/><div id="40784000" class="c"><input type="checkbox" id="c-40784000" checked=""/><div class="controls bullet"><span class="by">QuiDortDine</span><span>|</span><a href="#40783939">parent</a><span>|</span><a href="#40783442">next</a><span>|</span><label class="collapse" for="c-40784000">[-]</label><label class="expand" for="c-40784000">[4 more]</label></div><br/><div class="children"><div class="content">Do you have a followup command to execute the suggested command?</div><br/><div id="40784070" class="c"><input type="checkbox" id="c-40784070" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#40783939">root</a><span>|</span><a href="#40784000">parent</a><span>|</span><a href="#40784022">next</a><span>|</span><label class="collapse" for="c-40784070">[-]</label><label class="expand" for="c-40784070">[2 more]</label></div><br/><div class="children"><div class="content">No, I want to review or tweak them, just in case it&#x27;s trying to do something bad. It&#x27;s usually pretty good though.</div><br/><div id="40785332" class="c"><input type="checkbox" id="c-40785332" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40783939">root</a><span>|</span><a href="#40784070">parent</a><span>|</span><a href="#40784022">next</a><span>|</span><label class="collapse" for="c-40785332">[-]</label><label class="expand" for="c-40785332">[1 more]</label></div><br/><div class="children"><div class="content">If you use zsh and are willing to source your scripts you can do<p><pre><code>  print -z $command
</code></pre>
And the command will appear on your cli as if you had written it.<p>I don&#x27;t think you can do this in bash. Interestingly, this is something that seems quite difficult to both google and ask GPT for help. Both get confused and are thinking different questions are being asked. Probably because there are similar more common questions but the subtleties of possible wordings makes it difficult to differentiate.</div><br/></div></div></div></div><div id="40784022" class="c"><input type="checkbox" id="c-40784022" checked=""/><div class="controls bullet"><span class="by">sdf4j</span><span>|</span><a href="#40783939">root</a><span>|</span><a href="#40784000">parent</a><span>|</span><a href="#40784070">prev</a><span>|</span><a href="#40783442">next</a><span>|</span><label class="collapse" for="c-40784022">[-]</label><label class="expand" for="c-40784022">[1 more]</label></div><br/><div class="children"><div class="content">$ bashy evaluate the previous command output</div><br/></div></div></div></div></div></div><div id="40783442" class="c"><input type="checkbox" id="c-40783442" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#40783939">prev</a><span>|</span><a href="#40785271">next</a><span>|</span><label class="collapse" for="c-40783442">[-]</label><label class="expand" for="c-40783442">[1 more]</label></div><br/><div class="children"><div class="content">Aider continues to be the best way to interact with LLMs while coding, and its a command line tool.<p>Copilot is pretty good, but the forced change &gt; commit &gt; QA process that Aider forces you through is really powerful.</div><br/></div></div><div id="40785271" class="c"><input type="checkbox" id="c-40785271" checked=""/><div class="controls bullet"><span class="by">qiakai</span><span>|</span><a href="#40783442">prev</a><span>|</span><a href="#40783641">next</a><span>|</span><label class="collapse" for="c-40785271">[-]</label><label class="expand" for="c-40785271">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What other CLI tools are people using to work with LLMs in the terminal?<p>I personally love using x-cmd. Small size (1.1MB), open source, interactive operation<p>[1] <a href="https:&#x2F;&#x2F;www.x-cmd.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.x-cmd.com&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;www.x-cmd.com&#x2F;mod&#x2F;openai" rel="nofollow">https:&#x2F;&#x2F;www.x-cmd.com&#x2F;mod&#x2F;openai</a></div><br/></div></div><div id="40783641" class="c"><input type="checkbox" id="c-40783641" checked=""/><div class="controls bullet"><span class="by">liamYC</span><span>|</span><a href="#40785271">prev</a><span>|</span><a href="#40783471">next</a><span>|</span><label class="collapse" for="c-40783641">[-]</label><label class="expand" for="c-40783641">[1 more]</label></div><br/><div class="children"><div class="content">This is awesome, thanks for sharing. I find this kind of tool really useful, Aider in particular. I made my own cli tool for interacting with GPT. It’s really useful with the -c flag for generating code especially bash commands I&#x27;ve forgotten<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ljsimpkin&#x2F;chat-gpt-cli-tool">https:&#x2F;&#x2F;github.com&#x2F;ljsimpkin&#x2F;chat-gpt-cli-tool</a></div><br/></div></div><div id="40783471" class="c"><input type="checkbox" id="c-40783471" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40783641">prev</a><span>|</span><label class="collapse" for="c-40783471">[-]</label><label class="expand" for="c-40783471">[6 more]</label></div><br/><div class="children"><div class="content">I wish llm were more stable, but unfortunately things just kept breaking out of the blue without me touching any settings of the program. I often had to reinstall the package. but finally I gave up and implemented my own.</div><br/><div id="40783516" class="c"><input type="checkbox" id="c-40783516" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40783471">parent</a><span>|</span><label class="collapse" for="c-40783516">[-]</label><label class="expand" for="c-40783516">[5 more]</label></div><br/><div class="children"><div class="content">Which plugins are you using? Did you install via pipx or Homebrew or something else?</div><br/><div id="40784540" class="c"><input type="checkbox" id="c-40784540" checked=""/><div class="controls bullet"><span class="by">jimmcslim</span><span>|</span><a href="#40783471">root</a><span>|</span><a href="#40783516">parent</a><span>|</span><a href="#40783600">next</a><span>|</span><label class="collapse" for="c-40784540">[-]</label><label class="expand" for="c-40784540">[1 more]</label></div><br/><div class="children"><div class="content">I followed along with the blog post, but got unstuck with llm-cmd not working on Mac OS. Looks like this PR would fix it <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-cmd&#x2F;pull&#x2F;12">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-cmd&#x2F;pull&#x2F;12</a></div><br/></div></div><div id="40783600" class="c"><input type="checkbox" id="c-40783600" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40783471">root</a><span>|</span><a href="#40783516">parent</a><span>|</span><a href="#40784540">prev</a><span>|</span><label class="collapse" for="c-40783600">[-]</label><label class="expand" for="c-40783600">[3 more]</label></div><br/><div class="children"><div class="content">I used pipx and used AnthropicAI&#x27;s plugin to use Claude. After 2 weeks of perfectly working, suddenly I got the &quot;this model is invalid&quot; error.<p>PS: I appreciate the work put into llm tho, it&#x27;s a neat program I used with my Automator scripts to bring LLMs to macOS before Apple Intelligence was announced. I just wish the stability was not a concern.</div><br/><div id="40784165" class="c"><input type="checkbox" id="c-40784165" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40783471">root</a><span>|</span><a href="#40783600">parent</a><span>|</span><a href="#40783705">next</a><span>|</span><label class="collapse" for="c-40784165">[-]</label><label class="expand" for="c-40784165">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s weird. Were you using llm-claude or llm-claude-3?</div><br/></div></div><div id="40783705" class="c"><input type="checkbox" id="c-40783705" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#40783471">root</a><span>|</span><a href="#40783600">parent</a><span>|</span><a href="#40784165">prev</a><span>|</span><label class="collapse" for="c-40783705">[-]</label><label class="expand" for="c-40783705">[1 more]</label></div><br/><div class="children"><div class="content">The plugins seem to need to be reinstalled after every upgrade</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>