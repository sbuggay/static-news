<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714899712614" as="style"/><link rel="stylesheet" href="styles.css?v=1714899712614"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://o565.com/llm-text-compression/">Drink Me: (Ab)Using a LLM to Compress Text</a> <span class="domain">(<a href="https://o565.com">o565.com</a>)</span></div><div class="subtext"><span>alexmolas</span> | <span>34 comments</span></div><br/><div><div id="40261540" class="c"><input type="checkbox" id="c-40261540" checked=""/><div class="controls bullet"><span class="by">blackle</span><span>|</span><a href="#40262622">next</a><span>|</span><label class="collapse" for="c-40261540">[-]</label><label class="expand" for="c-40261540">[1 more]</label></div><br/><div class="children"><div class="content">There is a way to do this same compression by utilizing the raw probability distributions that the LLM produces as output. Fabrice Bellard has some experiments with doing this with transformers: <a href="https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;</a><p>The idea is that if you can produce an accurate probably distribution over the next bit&#x2F;byte&#x2F;token, then you can compress things with an entropy compressor (huffman encoding, range encoding, asymmetric numeral systems, etc). This comment is too small of a space to explain fully how they work, but it suffices to say that pretty much every good compression algorithm models probability distributions in some way.</div><br/></div></div><div id="40262622" class="c"><input type="checkbox" id="c-40262622" checked=""/><div class="controls bullet"><span class="by">Intralexical</span><span>|</span><a href="#40261540">prev</a><span>|</span><a href="#40262990">next</a><span>|</span><label class="collapse" for="c-40262622">[-]</label><label class="expand" for="c-40262622">[1 more]</label></div><br/><div class="children"><div class="content">You can already pre-train compression on text without using an LLM:<p><pre><code>    $ curl https:&#x2F;&#x2F;www.gutenberg.org&#x2F;cache&#x2F;epub&#x2F;11&#x2F;pg11.txt &gt; text.txt
    $ split -n 500 text.txt trainpart.
</code></pre>
Using a normal compression algorithm:<p><pre><code>    $ zstd --train trainpart.* -o dictionary
    Save dictionary of size 112640 into file dictionary

    $ zstd -vD dictionary text.txt 
    *** Zstandard CLI (64-bit) v1.5.5, by Yann Collet ***
    text.txt             : 15.41%   (   170 KiB =&gt;   26.2 KiB, text.txt.zst)
</code></pre>
For this example, ZSTD warns that the dictionary training set is 10X-100X too small to be efficient. Realistically, I guess you&#x27;d train it over E.G. the entire Gutenberg library. Then you can distribute specific books to people who already have the dictionary.<p>Or:<p><pre><code>    $ curl -L https:&#x2F;&#x2F;archive.org&#x2F;download&#x2F;completeworksofl1920carr&#x2F;completeworksofl1920carr_hocr_searchtext.txt.gz |
        gzip -d |
        sed -E &#x27;s&#x2F;\s+&#x2F; &#x2F;g&#x27; &gt; FullTextsSample.txt

    $ zstd -v -19 --patch-from FullTextsSample.txt text.txt
    text.txt             : 16.50%   (   170 KiB =&gt;   28.1 KiB, text.txt.zst)
</code></pre>
Not sure how much performance would drop for realistic use. But there are also some knobs you can tune.<p>Refer to:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;zstd&#x2F;#dictionary-compression-how-to">https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;zstd&#x2F;#dictionary-compression-how...</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;zstd&#x2F;wiki&#x2F;Zstandard-as-a-patching-engine">https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;zstd&#x2F;wiki&#x2F;Zstandard-as-a-patchin...</a><p><pre><code>    $ man zstd
</code></pre>
- Dictionary occupies only kilobytes or megabytes of storage, instead of gigabytes or terabytes.<p>- Dictionary can be re-trained for specific data at negligble cost.<p>- Compression and decompression are deterministic by default.<p>- Doesn&#x27;t take large amount of GPU resources to compress&#x2F;decompression.<p>- This is actually designed to do this.</div><br/></div></div><div id="40262990" class="c"><input type="checkbox" id="c-40262990" checked=""/><div class="controls bullet"><span class="by">throwaway81523</span><span>|</span><a href="#40262622">prev</a><span>|</span><a href="#40261833">next</a><span>|</span><label class="collapse" for="c-40262990">[-]</label><label class="expand" for="c-40262990">[1 more]</label></div><br/><div class="children"><div class="content">A compression method something like this figured into Vernor Vinge&#x27;s 1988-era short story &quot;The Blabber&quot;.</div><br/></div></div><div id="40261833" class="c"><input type="checkbox" id="c-40261833" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#40262990">prev</a><span>|</span><a href="#40260370">next</a><span>|</span><label class="collapse" for="c-40261833">[-]</label><label class="expand" for="c-40261833">[2 more]</label></div><br/><div class="children"><div class="content">A problem with this is that the inference has to be 100% deterministic. For example of you change tailing order of matmul you may get slightly different results. It&#x27;s easy to imagine people changing tailing in llama.cpp or other libraries. You are very unlikely to get bit exact results from different libraries.<p>Interestingly (though maybe not relevant here) you can also get different results from multi-user inference systems depending on what other requests are in the batch. It&#x27;s possible to avoid this but I&#x27;m pretty sure most systems don&#x27;t.<p>The &quot;slightly different&quot; bit of course makes it worse - it will work 99% of the time.</div><br/><div id="40263061" class="c"><input type="checkbox" id="c-40263061" checked=""/><div class="controls bullet"><span class="by">david_draco</span><span>|</span><a href="#40261833">parent</a><span>|</span><a href="#40260370">next</a><span>|</span><label class="collapse" for="c-40263061">[-]</label><label class="expand" for="c-40263061">[1 more]</label></div><br/><div class="children"><div class="content">Probably could set the seed and temperature and store these. Run with a few variations and keeping the best would probably work even better, at the expense of more computation.</div><br/></div></div></div></div><div id="40260370" class="c"><input type="checkbox" id="c-40260370" checked=""/><div class="controls bullet"><span class="by">clay_the_ripper</span><span>|</span><a href="#40261833">prev</a><span>|</span><a href="#40261840">next</a><span>|</span><label class="collapse" for="c-40260370">[-]</label><label class="expand" for="c-40260370">[4 more]</label></div><br/><div class="children"><div class="content">It does seem like this is a possible method to test if an LLM has your data in it.<p>People have found other ways  to do that of course, but this is pretty clever.</div><br/><div id="40261831" class="c"><input type="checkbox" id="c-40261831" checked=""/><div class="controls bullet"><span class="by">mvkel</span><span>|</span><a href="#40260370">parent</a><span>|</span><a href="#40261840">next</a><span>|</span><label class="collapse" for="c-40261831">[-]</label><label class="expand" for="c-40261831">[3 more]</label></div><br/><div class="children"><div class="content">Not necessarily. This also uncovers the weakness of the NYT lawsuit.<p>Imagine in your corpus of training data is the following:<p>- bloga.com: &quot;I read in the NYT that &#x27;it rains cats and dogs twice per year&#x27;&quot;<p>- blogb.com: &quot;according to the NYT, &#x27;cats and dogs level rain occurs 2 times per year.&quot;<p>- newssite.com: &quot;cats and dogs rain events happen twice per year, according to the New York Times&quot;<p>Now, you chat with an LLM trained on this data, asking it &quot;how many times per year does it rain cats and dogs?&quot;<p>&quot;According to the New York Times, it rains cats and dogs twice per year.&quot;<p>NYT content was never in the training data, however it -is- mentioned a lot on various sources throughout commoncrawl-approved sources, therefore gets a higher probability association with next token.<p>Zoom that out to full articles quoted throughout the web, and you get false positives.</div><br/><div id="40261934" class="c"><input type="checkbox" id="c-40261934" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40260370">root</a><span>|</span><a href="#40261831">parent</a><span>|</span><a href="#40261929">prev</a><span>|</span><a href="#40261840">next</a><span>|</span><label class="collapse" for="c-40261934">[-]</label><label class="expand" for="c-40261934">[1 more]</label></div><br/><div class="children"><div class="content">They were getting huge chunks, verbatim of NYT articles out. I remember being stunned. Then I remember finding out there was some sort of trick to it that made it seem sillier.</div><br/></div></div></div></div></div></div><div id="40261840" class="c"><input type="checkbox" id="c-40261840" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#40260370">prev</a><span>|</span><a href="#40261821">next</a><span>|</span><label class="collapse" for="c-40261840">[-]</label><label class="expand" for="c-40261840">[1 more]</label></div><br/><div class="children"><div class="content">There was this: Compression Represents Intelligence Linearly, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2404.09937v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2404.09937v1</a></div><br/></div></div><div id="40261821" class="c"><input type="checkbox" id="c-40261821" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40261840">prev</a><span>|</span><a href="#40262879">next</a><span>|</span><label class="collapse" for="c-40261821">[-]</label><label class="expand" for="c-40261821">[3 more]</label></div><br/><div class="children"><div class="content">Did I miss it, or did he completely leave out the name of the LLM model he used?</div><br/><div id="40262593" class="c"><input type="checkbox" id="c-40262593" checked=""/><div class="controls bullet"><span class="by">number6</span><span>|</span><a href="#40261821">parent</a><span>|</span><a href="#40262879">next</a><span>|</span><label class="collapse" for="c-40262593">[-]</label><label class="expand" for="c-40262593">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ll use llama.cpp via its python bindings.<p>llama.cpp also has a link to the model</div><br/><div id="40263171" class="c"><input type="checkbox" id="c-40263171" checked=""/><div class="controls bullet"><span class="by">gliptic</span><span>|</span><a href="#40261821">root</a><span>|</span><a href="#40262593">parent</a><span>|</span><a href="#40262879">next</a><span>|</span><label class="collapse" for="c-40263171">[-]</label><label class="expand" for="c-40263171">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp isn&#x27;t a model though.</div><br/></div></div></div></div></div></div><div id="40262879" class="c"><input type="checkbox" id="c-40262879" checked=""/><div class="controls bullet"><span class="by">tednoob</span><span>|</span><a href="#40261821">prev</a><span>|</span><a href="#40262905">next</a><span>|</span><label class="collapse" for="c-40262879">[-]</label><label class="expand" for="c-40262879">[2 more]</label></div><br/><div class="children"><div class="content">Is this method used during training? Seems to me there could be a point to only backpropagate when the model is wrong?</div><br/><div id="40263077" class="c"><input type="checkbox" id="c-40263077" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#40262879">parent</a><span>|</span><a href="#40262905">next</a><span>|</span><label class="collapse" for="c-40263077">[-]</label><label class="expand" for="c-40263077">[1 more]</label></div><br/><div class="children"><div class="content">I mean this is implicit in back propagation, say, you need to store gradients anyway but if you get to a zero loss than you are just done.</div><br/></div></div></div></div><div id="40262905" class="c"><input type="checkbox" id="c-40262905" checked=""/><div class="controls bullet"><span class="by">Tiberium</span><span>|</span><a href="#40262879">prev</a><span>|</span><a href="#40261900">next</a><span>|</span><label class="collapse" for="c-40262905">[-]</label><label class="expand" for="c-40262905">[1 more]</label></div><br/><div class="children"><div class="content">Am I missing something, or is there no repo to try it out?</div><br/></div></div><div id="40261900" class="c"><input type="checkbox" id="c-40261900" checked=""/><div class="controls bullet"><span class="by">personjerry</span><span>|</span><a href="#40262905">prev</a><span>|</span><a href="#40261860">next</a><span>|</span><label class="collapse" for="c-40261900">[-]</label><label class="expand" for="c-40261900">[5 more]</label></div><br/><div class="children"><div class="content">I mean, sure this is compression in the sense that I can send you a tiny &quot;compressed text&quot; and all you need is this multi-terabyte model to decompress it!</div><br/><div id="40261950" class="c"><input type="checkbox" id="c-40261950" checked=""/><div class="controls bullet"><span class="by">markisus</span><span>|</span><a href="#40261900">parent</a><span>|</span><a href="#40262376">next</a><span>|</span><label class="collapse" for="c-40261950">[-]</label><label class="expand" for="c-40261950">[3 more]</label></div><br/><div class="children"><div class="content">But we don’t usually count the decompression program size when evaluating compression ratio. Eg 7-Zip is about 1 MB, but you don’t think about that when evaluating particular 7z files.</div><br/><div id="40263115" class="c"><input type="checkbox" id="c-40263115" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#40261900">root</a><span>|</span><a href="#40261950">parent</a><span>|</span><a href="#40261978">next</a><span>|</span><label class="collapse" for="c-40263115">[-]</label><label class="expand" for="c-40263115">[1 more]</label></div><br/><div class="children"><div class="content">We do when it&#x27;s the Hutter prize, otherwise it&#x27;s easy to cheat.<p>But sure, it&#x27;s a constant factor, so if you compress enough data you can always ignore it.</div><br/></div></div><div id="40261978" class="c"><input type="checkbox" id="c-40261978" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#40261900">root</a><span>|</span><a href="#40261950">parent</a><span>|</span><a href="#40263115">prev</a><span>|</span><a href="#40262376">next</a><span>|</span><label class="collapse" for="c-40261978">[-]</label><label class="expand" for="c-40261978">[1 more]</label></div><br/><div class="children"><div class="content">We would if it’s a multi-gigabyte program the receiver doesn’t have installed.</div><br/></div></div></div></div><div id="40262376" class="c"><input type="checkbox" id="c-40262376" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#40261900">parent</a><span>|</span><a href="#40261950">prev</a><span>|</span><a href="#40261860">next</a><span>|</span><label class="collapse" for="c-40262376">[-]</label><label class="expand" for="c-40262376">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an existing idea called shared dictionary compression. Everybody pre-agrees on some statistical priors about the data, and you use them to improve the compression ratio.<p>This is just the gigascale version of that.</div><br/></div></div></div></div><div id="40261860" class="c"><input type="checkbox" id="c-40261860" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#40261900">prev</a><span>|</span><a href="#40245530">next</a><span>|</span><label class="collapse" for="c-40261860">[-]</label><label class="expand" for="c-40261860">[1 more]</label></div><br/><div class="children"><div class="content">On the topic of very strong compression: <a href="http:&#x2F;&#x2F;prize.hutter1.net&#x2F;" rel="nofollow">http:&#x2F;&#x2F;prize.hutter1.net&#x2F;</a></div><br/></div></div><div id="40245530" class="c"><input type="checkbox" id="c-40245530" checked=""/><div class="controls bullet"><span class="by">pronoiac</span><span>|</span><a href="#40261860">prev</a><span>|</span><a href="#40262023">next</a><span>|</span><label class="collapse" for="c-40245530">[-]</label><label class="expand" for="c-40245530">[9 more]</label></div><br/><div class="children"><div class="content">I expected general-purpose compressors to do better, but I was wrong. For the whole document:<p><pre><code>  174355 pg11.txt
   60907 pg11.txt.gz-9
   58590 pg11.txt.zstd-9
   54164 pg11.txt.xz-9
   25360 [from blog post]</code></pre></div><br/><div id="40262417" class="c"><input type="checkbox" id="c-40262417" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#40245530">parent</a><span>|</span><a href="#40254542">next</a><span>|</span><label class="collapse" for="c-40262417">[-]</label><label class="expand" for="c-40262417">[2 more]</label></div><br/><div class="children"><div class="content">LLMs blow away traditional compressors because they are very good predictors, and prediction and compression are the same operation.<p>You can convert any predictor into a lossless compressor by feeding the output probabilities into an entropy coding algorithm. LLMs can get compression ratios as high as 95% (0.4 bits per character) on english text.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2404.09937v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2404.09937v1</a></div><br/><div id="40262540" class="c"><input type="checkbox" id="c-40262540" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40245530">root</a><span>|</span><a href="#40262417">parent</a><span>|</span><a href="#40254542">next</a><span>|</span><label class="collapse" for="c-40262540">[-]</label><label class="expand" for="c-40262540">[1 more]</label></div><br/><div class="children"><div class="content">The 1-1 correspondence between prediction and compression is one of the most counterintuitive and fascinating things in all of AI to me.</div><br/></div></div></div></div><div id="40254542" class="c"><input type="checkbox" id="c-40254542" checked=""/><div class="controls bullet"><span class="by">Intralexical</span><span>|</span><a href="#40245530">parent</a><span>|</span><a href="#40262417">prev</a><span>|</span><a href="#40262017">next</a><span>|</span><label class="collapse" for="c-40254542">[-]</label><label class="expand" for="c-40254542">[2 more]</label></div><br/><div class="children"><div class="content">IDK why, but BZIP2 seems to do somewhat better that other compression algorithms for natural language text:<p><pre><code>    $ curl https:&#x2F;&#x2F;www.gutenberg.org&#x2F;cache&#x2F;epub&#x2F;11&#x2F;pg11.txt | bzip2 --best | wc
        246    1183   48925
</code></pre>
Also, ZSTD goes all the way up to `--ultra -22` plus `--long=31` (4GB window— Irrelevant here since the file fits in the default 8MB anyway).</div><br/><div id="40263204" class="c"><input type="checkbox" id="c-40263204" checked=""/><div class="controls bullet"><span class="by">pastage</span><span>|</span><a href="#40245530">root</a><span>|</span><a href="#40254542">parent</a><span>|</span><a href="#40262017">next</a><span>|</span><label class="collapse" for="c-40263204">[-]</label><label class="expand" for="c-40263204">[1 more]</label></div><br/><div class="children"><div class="content">You can use a preshared dictionary with bzip2 and zstd so you can get that down alot by using different dictionaries depending on certain rules. I dont know if it helps with literature but I had great success in sending databases with free text like that. In the end it was easier to just use one dictionary for everything and just skip the rules.</div><br/></div></div></div></div><div id="40262017" class="c"><input type="checkbox" id="c-40262017" checked=""/><div class="controls bullet"><span class="by">anonu</span><span>|</span><a href="#40245530">parent</a><span>|</span><a href="#40254542">prev</a><span>|</span><a href="#40261848">next</a><span>|</span><label class="collapse" for="c-40262017">[-]</label><label class="expand" for="c-40262017">[2 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t you factor in the size of the compression tools needed?</div><br/><div id="40263014" class="c"><input type="checkbox" id="c-40263014" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40245530">root</a><span>|</span><a href="#40262017">parent</a><span>|</span><a href="#40261848">next</a><span>|</span><label class="collapse" for="c-40263014">[-]</label><label class="expand" for="c-40263014">[1 more]</label></div><br/><div class="children"><div class="content">Not if you&#x27;re interested in compressing more than one file - compression tools are a fixed, one-time cost in size.</div><br/></div></div></div></div><div id="40261848" class="c"><input type="checkbox" id="c-40261848" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40245530">parent</a><span>|</span><a href="#40262017">prev</a><span>|</span><a href="#40254465">next</a><span>|</span><label class="collapse" for="c-40261848">[-]</label><label class="expand" for="c-40261848">[1 more]</label></div><br/><div class="children"><div class="content">And here&#x27;s the opposite - using gzip as a (merely adequate) &quot;large&quot; language model: <a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.findings-acl.426.pdf" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.findings-acl.426.pdf</a><p>By the way, if you want to see how well gzip actually models language, take any gzipped file, flip a few bits, and unzip it. If it gives you a checksum error, ignore that. You might have to unzip in a streaming way so that it can&#x27;t tell the checksum is wrong until it&#x27;s already printed the wrong data that you want to see.</div><br/></div></div><div id="40254465" class="c"><input type="checkbox" id="c-40254465" checked=""/><div class="controls bullet"><span class="by">EgoIncarnate</span><span>|</span><a href="#40245530">parent</a><span>|</span><a href="#40261848">prev</a><span>|</span><a href="#40262023">next</a><span>|</span><label class="collapse" for="c-40254465">[-]</label><label class="expand" for="c-40254465">[1 more]</label></div><br/><div class="children"><div class="content">Which model did you use?</div><br/></div></div></div></div><div id="40262023" class="c"><input type="checkbox" id="c-40262023" checked=""/><div class="controls bullet"><span class="by">anonu</span><span>|</span><a href="#40245530">prev</a><span>|</span><a href="#40254565">next</a><span>|</span><label class="collapse" for="c-40262023">[-]</label><label class="expand" for="c-40262023">[1 more]</label></div><br/><div class="children"><div class="content">URL to the text is basically compression.</div><br/></div></div><div id="40254565" class="c"><input type="checkbox" id="c-40254565" checked=""/><div class="controls bullet"><span class="by">Intralexical</span><span>|</span><a href="#40262023">prev</a><span>|</span><label class="collapse" for="c-40254565">[-]</label><label class="expand" for="c-40254565">[1 more]</label></div><br/><div class="children"><div class="content">The practical way to do this is to train a custom dictionary with a normal compression algorithm.</div><br/></div></div></div></div></div></div></div></body></html>