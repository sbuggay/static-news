<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705395658247" as="style"/><link rel="stylesheet" href="styles.css?v=1705395658247"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://1a3orn.com/sub/machine-learning-bans.html">Many AI safety orgs have tried to criminalize currently-existing open-source AI</a>Â <span class="domain">(<a href="https://1a3orn.com">1a3orn.com</a>)</span></div><div class="subtext"><span>sroussey</span> | <span>68 comments</span></div><br/><div><div id="39010218" class="c"><input type="checkbox" id="c-39010218" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#39010089">next</a><span>|</span><label class="collapse" for="c-39010218">[-]</label><label class="expand" for="c-39010218">[37 more]</label></div><br/><div class="children"><div class="content">AI safety people are hypocrites. If they practiced what they preached, they&#x27;d be calling for <i>all</i> AI to be banned, ala Dune. There are AI harms that don&#x27;t care about whether or not the weights are available, and are playing out today.<p>I&#x27;m talking about the ability of any AI system to obfuscate plagiarism[0] and spam the Internet with technically distinct rewords of the same text. This is currently the most lucrative use of AI, and none of the AI safety people are talking about stopping it.<p>[0] No, I don&#x27;t mean the training sets - though AI systems seem to be suspiciously really good at remembering them, too.</div><br/><div id="39011076" class="c"><input type="checkbox" id="c-39011076" checked=""/><div class="controls bullet"><span class="by">hiAndrewQuinn</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39010887">next</a><span>|</span><label class="collapse" for="c-39011076">[-]</label><label class="expand" for="c-39011076">[1 more]</label></div><br/><div class="children"><div class="content">All <i>future</i> AI research should be banned, a la Bostrom&#x27;s vulnerable world hypothesis [1]. Every time you pull the trigger in Russian roulette and survive, the next person&#x27;s chance of getting the bullet is higher.<p>[1]: <a href="https:&#x2F;&#x2F;nickbostrom.com&#x2F;papers&#x2F;vulnerable.pdf" rel="nofollow">https:&#x2F;&#x2F;nickbostrom.com&#x2F;papers&#x2F;vulnerable.pdf</a></div><br/></div></div><div id="39010887" class="c"><input type="checkbox" id="c-39010887" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39011076">prev</a><span>|</span><a href="#39010805">next</a><span>|</span><label class="collapse" for="c-39010887">[-]</label><label class="expand" for="c-39010887">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m talking about the ability of any AI system to obfuscate plagiarism and spam the Internet with technically distinct rewords of the same text. This is currently the most lucrative use of AI, and none of the AI safety people are talking about stopping it.<p>This should be an explicitly allowed practice, it is following the spirit of copyright to the letter - use the ideas, facts, methods or styles while avoiding to copy the protected expression and characters. LLMs can do paraphrasing, summarisation, QA pairs or comparisons with other texts.<p>We should never try to put ideas under copyright or we might find out humans also have to abide by the same restrictive rules, because anyone could be secretly using AI, so all human texts need to be checked from now on for copyright infringement with the same strictness.<p>The good part about this practice is that a model trained on reworded text will never spit out the original word for word, under any circumstances because it never saw it during training. Should be required pre-processing for copyrighted texts. Also removing PII. Much more useful as a model if you can be sure it won&#x27;t infringe word for word.</div><br/></div></div><div id="39010805" class="c"><input type="checkbox" id="c-39010805" checked=""/><div class="controls bullet"><span class="by">war321</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39010887">prev</a><span>|</span><a href="#39010456">next</a><span>|</span><label class="collapse" for="c-39010805">[-]</label><label class="expand" for="c-39010805">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure they do. I follow a few of these safetyist people on twitter and they absolutely argue that companies like OpenAI, Google, Tencent and literally anyone else training a potential AGI should stop training runs and put them under oversight at best and no one should even make an AGI at worst.<p>They just go after open source as well since they&#x27;re at least aware that open models that anyone can share and use aren&#x27;t restricted by an API and, to use a really overused soundbyte, &quot;can&#x27;t be put back in the box&quot;.</div><br/></div></div><div id="39010456" class="c"><input type="checkbox" id="c-39010456" checked=""/><div class="controls bullet"><span class="by">JoshTriplett</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39010805">prev</a><span>|</span><a href="#39010780">next</a><span>|</span><label class="collapse" for="c-39010456">[-]</label><label class="expand" for="c-39010456">[22 more]</label></div><br/><div class="children"><div class="content">&gt; AI safety people are hypocrites. If they practiced what they preached, they&#x27;d be calling for all AI to be banned<p>They <i>are</i> calling for all AI (above a certain capability level) to be banned. Not just open, not just closed, <i>all</i>.<p>There are risks that apply only to open. There are risks that apply only to closed. But nobody should be developing AGI without <i>incredibly robustly proven</i> alignment, open <i>or</i> closed, any more than people should be developing nuclear weapons in their garage.<p>&gt; This is currently the most lucrative use of AI, and none of the AI safety people are talking about stopping it.<p>Because AI safety people are not the strawmen you are hypothesizing. They&#x27;re arguing against taking existential risks. AI being a laundering operation for copyright violations is certainly a problem. It&#x27;s not an existential risk.<p>If you want to argue, concretely and with evidence, why you think it <i>isn&#x27;t</i> an existential risk, that&#x27;s an argument you could reasonably make. But don&#x27;t portray people as ineffectively doing the thing you <i>think</i> they should be doing, when they are in fact <i>not</i> trying to do that, and only trying to do something they deem more important.</div><br/><div id="39010738" class="c"><input type="checkbox" id="c-39010738" checked=""/><div class="controls bullet"><span class="by">kolektiv</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010456">parent</a><span>|</span><a href="#39010907">next</a><span>|</span><label class="collapse" for="c-39010738">[-]</label><label class="expand" for="c-39010738">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not convinced the onus should be on one side to prove why something <i>isn&#x27;t</i> an existential risk. We don&#x27;t start with an assumption that something is world-ending about anything else; we generally need to see a plausibly worked-through example of how the world ends, using technology we can all broadly agree exists&#x2F;will shortly exist.<p>If we&#x27;re talking about nuclear weapons, for example, the tech is clear, the pattern of human behaviour is clear: they could cause immense, species-level damage. There&#x27;s really little to argue about. With AI, there still seems to be a lot of hand-waving between where we are now and &quot;AGI&quot;. What we have now is in many ways impressive, but the onus is still on the claimant to show that it&#x27;s going to turn into something much more dangerous through some known progression. At the moment there is a very big, underpants gnomes-style &quot;?&quot; gap before we get to AGI&#x2F;profit, and if people are basing this on currently secret tech, then they&#x27;re going to have to reveal it if they want people to think they&#x27;re doing something other than creating a legislative moat.</div><br/><div id="39010836" class="c"><input type="checkbox" id="c-39010836" checked=""/><div class="controls bullet"><span class="by">JoshTriplett</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010738">parent</a><span>|</span><a href="#39010892">next</a><span>|</span><label class="collapse" for="c-39010836">[-]</label><label class="expand" for="c-39010836">[2 more]</label></div><br/><div class="children"><div class="content">AI safety &#x2F; x-risk folks have in fact made extensive and detailed arguments. Occasionally, folks arguing against them rise to the same standard. But most of the arguments against AI safety look a lot more like name-calling and derision: &quot;nuh-uh, that&#x27;s sci-fi and unrealistic (mic drop)&quot;. That&#x27;s not a counterargument.<p>&gt; If we&#x27;re talking about nuclear weapons, for example, the tech is clear, the pattern of human behaviour is clear: they could cause immense, species-level damage.<p>That&#x27;s easy to say now, now that the damage is largely done, they&#x27;ve been not only tested but <i>used</i>, many countries have them, the knowledge for how to make them is widespread.<p>How many people arguing against AI safety today would also have argued for widespread nuclear proliferation when the technology was still in development and nothing had been exploded yet? How many would have argued against nuclear regulation as being unnecessary, or derided those arguing for such regulation as unrealistic or sci-fi-based?</div><br/><div id="39011084" class="c"><input type="checkbox" id="c-39011084" checked=""/><div class="controls bullet"><span class="by">kolektiv</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010836">parent</a><span>|</span><a href="#39010892">next</a><span>|</span><label class="collapse" for="c-39011084">[-]</label><label class="expand" for="c-39011084">[1 more]</label></div><br/><div class="children"><div class="content">I understand your point, I think - and certainly I don&#x27;t want to go anywhere near name-calling or derision, that doesn&#x27;t help anyone. But I am reminded of arguments I&#x27;ve had with creationists (I am not comparing you with them, but sometimes the general tone of the debate). It seems like one side is making an extraordinary claim, and then demanding the other side rebut it, and that&#x27;s not something that seems reasonable to me.<p>The thing about nuclear weapons is that the theoretical science was clear before the testing - building and testing them was proof by demonstration, but many people agreed with the theory well before that. How they would be used was certainly debated, but there was a clear and well-explained proposal for every step of their creation, which could be tested and falsified if needed. I don&#x27;t think that&#x27;s the case here - there seems to be more of a claim for a general acceleration with an inevitable endpoint, and that claim of inevitability feels very short on grounding.<p>I am more than prepared to admit that I may not be seeing (for various reasons) the evidence that this is near&#x2F;possible - but I would also claim that nobody is convincingly showing any either.</div><br/></div></div></div></div><div id="39010892" class="c"><input type="checkbox" id="c-39010892" checked=""/><div class="controls bullet"><span class="by">jagrsw</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010738">parent</a><span>|</span><a href="#39010836">prev</a><span>|</span><a href="#39010907">next</a><span>|</span><label class="collapse" for="c-39010892">[-]</label><label class="expand" for="c-39010892">[3 more]</label></div><br/><div class="children"><div class="content">&gt; seems to be a lot of hand-waving between where we are now and &quot;AGI&quot;.<p>Modeling an entity that surpasses our intelligence, especially one that interacts with us, is an extraordinarily challenging, if not impossible, task.<p>Concerning the potential for harm, consider the example of Vladimir Putin, who could theoretically cause widespread destruction using nuclear weapons. Although safeguards exist, these could be circumvented if someone with his authority were determined enough, perhaps by strategically placing loyal individuals in key positions.<p>Putin, with his specific level of intelligence, attained his powerful position through a mix of deliberate actions and chance, the latter being difficult to quantify. An AGI, being more intelligent, could achieve a similar level of power. This could be accomplished through more technical means than traditional political processes (those being slow and subject to chance), though it could also engage in standard political maneuvers like election participation or manipulation, by human proxies if needed.<p>TL;DR It could do (in terms of negative consequences) at least whatever Vladimir P. can do, and he can bring civilization to its knees.</div><br/><div id="39011007" class="c"><input type="checkbox" id="c-39011007" checked=""/><div class="controls bullet"><span class="by">kolektiv</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010892">parent</a><span>|</span><a href="#39010975">next</a><span>|</span><label class="collapse" for="c-39011007">[-]</label><label class="expand" for="c-39011007">[1 more]</label></div><br/><div class="children"><div class="content">Oh, absolutely - such an entity obviously could! Modelling the behaviour of such an entity is very difficult indeed, as you&#x27;d need to make all kinds of assumptions without basis. However, you only need to model this behaviour once you&#x27;ve posited the likely existence of such an entity - and that&#x27;s where (purely subjectively) it feels like there&#x27;s a gap.<p>Nothing has yet convinced me (and I am absolutely honest about the fact that I&#x27;m not a deep expert and also not privy to the inner workings of relevant organisations) that it&#x27;s likely to exist soon. I am very open to being convinced by evidence - but an &quot;argument from trajectory&quot; seems to be what we have at the moment, and so far, those have stalled at local maxima every single time.<p>We&#x27;ve built some incredibly impressive tools, but so far, nothing that looks or feels like a concept of will (note, not consciousness) yet, to the best of my knowledge.</div><br/></div></div><div id="39010975" class="c"><input type="checkbox" id="c-39010975" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010892">parent</a><span>|</span><a href="#39011007">prev</a><span>|</span><a href="#39010907">next</a><span>|</span><label class="collapse" for="c-39010975">[-]</label><label class="expand" for="c-39010975">[1 more]</label></div><br/><div class="children"><div class="content">How would and AGI launch nuclear missiles from their silicon GPUs? Social engineering?</div><br/></div></div></div></div></div></div><div id="39010907" class="c"><input type="checkbox" id="c-39010907" checked=""/><div class="controls bullet"><span class="by">suslik</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010456">parent</a><span>|</span><a href="#39010738">prev</a><span>|</span><a href="#39010793">next</a><span>|</span><label class="collapse" for="c-39010907">[-]</label><label class="expand" for="c-39010907">[3 more]</label></div><br/><div class="children"><div class="content">What is the reason to believe that LLMs are an evolutionary step towards AGI at all? In my mind there is a rather large leap from estimating a conditional probability of a next token over some space to a conscious entity with its own goals and purpose. Should we ban a linear regression while we&#x27;re at it?<p>It would be great to see some evidence that this risk is real. All I&#x27;ve witnessed so far was scaremongering posts from apparatchicks of all shapes and colors, many of whom have either a vested interest in restricting AI research by others (but not by them, because they are safe and responsible and harmless), or established a lucrative paper-pushing, shoulder-rubbing career around &#x27;AI safety&#x27; - and thus are strongly incentivised to double down on that.<p>A security org in a large company will keep tightening the screws until everything halts; a transport security agency, given free reigh, would strip everyone naked and administer a couple of profilactic kicks for a good measure - and so on. That&#x27;s just the nature of it - organisations do what they do to maintain themselves. It is critical to keep these things on a leash. Similarly, an AI Safety org must proseletyse excistential risks of AI - because a lack of evidence of such is an existential risk for themselves.<p>A real risk, which we do have evidence for, is that LLMs might disrupt knowledge-based economy and threaten many key professions - but how is this conceptually different from any technological revolution? Perhaps in a hundred years lawyers, radiologists, and, indeed, software developers, will find themselves in the bin of history - together with flint chippers, chariot benders, drakkar berserkers and so forth. That&#x27;d be great if we planned for that - and I don&#x27;t feel like we do enough. Instead, the focus is on AGIs and that some poor 13-year-old soul might occasionally read the word &#x27;nipple&#x27;.</div><br/><div id="39011069" class="c"><input type="checkbox" id="c-39011069" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010907">parent</a><span>|</span><a href="#39010935">next</a><span>|</span><label class="collapse" for="c-39011069">[-]</label><label class="expand" for="c-39011069">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What is the reason to believe that LLMs are an evolutionary step towards AGI at all?<p>Because this is the marketing pitch of the current wave of venture capital financed AI companies. :-)</div><br/></div></div><div id="39010935" class="c"><input type="checkbox" id="c-39010935" checked=""/><div class="controls bullet"><span class="by">JoshTriplett</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010907">parent</a><span>|</span><a href="#39011069">prev</a><span>|</span><a href="#39010793">next</a><span>|</span><label class="collapse" for="c-39010935">[-]</label><label class="expand" for="c-39010935">[1 more]</label></div><br/><div class="children"><div class="content">&gt; many of whom have either a vested interest in restricting AI research by others (but not by them, because they are safe and responsible and harmless),<p>Anyone who argues that <i>other people</i> shouldn&#x27;t build AGI but <i>they</i> should is indeed selling snake oil.<p>The existence of opportunistic people co-opting a message does not invalidate the original message: <i>don&#x27;t build AGI, don&#x27;t risk building AGI, don&#x27;t assume it will be obvious in advance where the line is and how much capability is safe</i>.</div><br/></div></div></div></div><div id="39010793" class="c"><input type="checkbox" id="c-39010793" checked=""/><div class="controls bullet"><span class="by">rokkitmensch</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010456">parent</a><span>|</span><a href="#39010907">prev</a><span>|</span><a href="#39010663">next</a><span>|</span><label class="collapse" for="c-39010793">[-]</label><label class="expand" for="c-39010793">[5 more]</label></div><br/><div class="children"><div class="content">I oppose regulating what calculations humans may perform in the strongest possible terms.</div><br/><div id="39010915" class="c"><input type="checkbox" id="c-39010915" checked=""/><div class="controls bullet"><span class="by">JoshTriplett</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010793">parent</a><span>|</span><a href="#39010988">next</a><span>|</span><label class="collapse" for="c-39010915">[-]</label><label class="expand" for="c-39010915">[3 more]</label></div><br/><div class="children"><div class="content">Ten years ago, even five years ago, I would have said exactly the same thing. I am <i>extremely</i> pro-FOSS.<p>Forget the particulars for just a moment. Forget arguments about the probability of the existential risk, whatever your personal assessment of that risk is.<p>Can we agree that people should not be able to <i>unilaterally</i> take existential risks with the future of humanity without the consent of humanity, based solely on their <i>unilateral</i> assessment of those risks?<p>Because lately it seems like people can&#x27;t even agree on <i>that</i> much, or worse, won&#x27;t even answer the question without dodging it and playing games of rhetoric.<p>If we <i>can</i> agree on that, then the argument comes down to: how do we fairly evaluate an existential risk, <i>taking it seriously</i>, and determine at what point an existential risk becomes sufficient that people can no longer take unilateral actions that incur that risk?<p>You can absolutely argue that you think the existential risk is unlikely. That&#x27;s an argument that&#x27;s reasonable to have. But for the time when that argument is active and ongoing, even assuming you only agree that it&#x27;s a <i>possibility</i> rather than a <i>probability</i>, are we as a species in fact <i>capable</i> of handling even a <i>potential</i> existential risk like this by some kind of consensus, rather than a free-for-all? Because right now the answer is looking a lot like &quot;no&quot;.</div><br/><div id="39011000" class="c"><input type="checkbox" id="c-39011000" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010915">parent</a><span>|</span><a href="#39010995">next</a><span>|</span><label class="collapse" for="c-39011000">[-]</label><label class="expand" for="c-39011000">[1 more]</label></div><br/><div class="children"><div class="content">No, we can&#x27;t. People have never been able to trust each other  so much that they would allow the risk of being marginalised in the name of safety. We don&#x27;t trust people. Other people are out to get us, or to get ahead. We still think in tribal logic.<p>If they say &quot;safety&quot; we hear &quot;we want to get an edge by hindering you&quot;, or &quot;we want to protect our nice social position by blocking others who would use AI to bootstrap themselves&quot;. We are adversaries that collaborate and compete at the same time. That is why open source AI is the only way ahead, it places the least amount of control on some people by other people.</div><br/></div></div><div id="39010995" class="c"><input type="checkbox" id="c-39010995" checked=""/><div class="controls bullet"><span class="by">trevyn</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010915">parent</a><span>|</span><a href="#39011000">prev</a><span>|</span><a href="#39010988">next</a><span>|</span><label class="collapse" for="c-39010995">[-]</label><label class="expand" for="c-39010995">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Can we agree that people should not be able to unilaterally take existential risks with the future of humanity without the consent of humanity, based solely on their unilateral assessment of those risks?</i><p>This has nothing to do with should. There are at the very least a handful of people who can, today, unilaterally take risks with the future of humanity without the consent of humanity. I do not see any reason to think that will change in the future. If these people can build something that they believe is the equivalent of nuclear weapons, you better believe they will.<p>As they say, the cat is already out of the bag.</div><br/></div></div></div></div><div id="39010988" class="c"><input type="checkbox" id="c-39010988" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010793">parent</a><span>|</span><a href="#39010915">prev</a><span>|</span><a href="#39010663">next</a><span>|</span><label class="collapse" for="c-39010988">[-]</label><label class="expand" for="c-39010988">[1 more]</label></div><br/><div class="children"><div class="content">Given how dangerous humans can be (they can invent GPT4) maybe we should just make sure education is forbidden and educated people jailed. Just to be sure. &#x2F;s</div><br/></div></div></div></div><div id="39010663" class="c"><input type="checkbox" id="c-39010663" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010456">parent</a><span>|</span><a href="#39010793">prev</a><span>|</span><a href="#39010883">next</a><span>|</span><label class="collapse" for="c-39010663">[-]</label><label class="expand" for="c-39010663">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s also ban cryptography because nuclear devices&#x2F;children.</div><br/></div></div><div id="39010883" class="c"><input type="checkbox" id="c-39010883" checked=""/><div class="controls bullet"><span class="by">trevyn</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010456">parent</a><span>|</span><a href="#39010663">prev</a><span>|</span><a href="#39010667">next</a><span>|</span><label class="collapse" for="c-39010883">[-]</label><label class="expand" for="c-39010883">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>nobody should be developing AGI without incredibly robustly proven alignment, open or closed, any more than people should be developing nuclear weapons in their garage.</i><p>I have an alternate proposal: We assume that someone, somewhere will develop AGI without any sort of âalignmentâ, plan our lives accordingly, and help other humans plan their lives accordingly.</div><br/><div id="39010967" class="c"><input type="checkbox" id="c-39010967" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010883">parent</a><span>|</span><a href="#39010667">next</a><span>|</span><label class="collapse" for="c-39010967">[-]</label><label class="expand" for="c-39010967">[2 more]</label></div><br/><div class="children"><div class="content">I think that assumption is why Yudkowsky suggested an international binding agreement to not develop a &quot;too smart&quot; AI (the terms AGI and ASI mean different things to different people) wouldn&#x27;t be worth the paper it was written on unless everyone was prepared to enforce it with air strikes on any sufficiently large computer cluster.</div><br/><div id="39011064" class="c"><input type="checkbox" id="c-39011064" checked=""/><div class="controls bullet"><span class="by">trevyn</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010967">parent</a><span>|</span><a href="#39010667">next</a><span>|</span><label class="collapse" for="c-39011064">[-]</label><label class="expand" for="c-39011064">[1 more]</label></div><br/><div class="children"><div class="content">I think it would help the discussion to understand what the world is like outside of the US and Europe (andâ¦ Japan?). There are no rules out here. There is no law. It is a fucking free-for-all. Might makes right. Do there exist GPUs? Shit will get trained.</div><br/></div></div></div></div></div></div><div id="39010667" class="c"><input type="checkbox" id="c-39010667" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010456">parent</a><span>|</span><a href="#39010883">prev</a><span>|</span><a href="#39010780">next</a><span>|</span><label class="collapse" for="c-39010667">[-]</label><label class="expand" for="c-39010667">[3 more]</label></div><br/><div class="children"><div class="content">&gt;They are calling for all AI (above a certain capability level) to be banned. Not just open, not just closed, all.<p>That&#x27;s not true if you read the article.</div><br/><div id="39010790" class="c"><input type="checkbox" id="c-39010790" checked=""/><div class="controls bullet"><span class="by">JoshTriplett</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010667">parent</a><span>|</span><a href="#39010780">next</a><span>|</span><label class="collapse" for="c-39010790">[-]</label><label class="expand" for="c-39010790">[2 more]</label></div><br/><div class="children"><div class="content">I did read the article. Several of the organizations mentioned simply don&#x27;t talk about openness, and are instead talking about any model with sufficiently powerful capabilities, so it&#x27;s not obvious why the article is making their comments about open models rather than about any model. Some of the others have made side comments about openness making it harder to take back capabilities once released, but as far as I can tell, even those organizations are still primarily concerned with capabilities, and would be comparably concerned by a proprietary model with those capabilities.<p>Some folks may well have co-opted the term &quot;AI safety&quot; to mean something other than safety, but the point of AI safety is to set an upper bound on capabilities and push for alignment, and that&#x27;s true whether a model is open or closed.</div><br/><div id="39010834" class="c"><input type="checkbox" id="c-39010834" checked=""/><div class="controls bullet"><span class="by">war321</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010790">parent</a><span>|</span><a href="#39010780">next</a><span>|</span><label class="collapse" for="c-39010834">[-]</label><label class="expand" for="c-39010834">[1 more]</label></div><br/><div class="children"><div class="content">The safety movement really isn&#x27;t as organized as many here would think.<p>Doesn&#x27;t help that safety and alignment means different things to different people. Some use it to refer to near term issues like copyright infringement, bias, labor devaluation, etc. While others use it for potential long term issues like pdoom,  runaway ASIs and human extinction. The former sees the latter as head in the cloud futurists ignoring real world problems, whiles the latter sees the former as worrying about minor issues in the face of (potential) catastrophe.</div><br/></div></div></div></div></div></div></div></div><div id="39010780" class="c"><input type="checkbox" id="c-39010780" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39010456">prev</a><span>|</span><a href="#39010560">next</a><span>|</span><label class="collapse" for="c-39010780">[-]</label><label class="expand" for="c-39010780">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s way too late to ban any of this. How do you propose to make that work? That would be like banning all &quot;malicious software&quot;, it&#x27;s a preposterous idea when you even begin to think about the practical side of it. And where do you draw the line? Is my XGBoost model &quot;AI&quot;, or are we only banning <i>generative</i> AI? Is a Markov chain &quot;generative AI&quot;?</div><br/></div></div><div id="39010560" class="c"><input type="checkbox" id="c-39010560" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39010780">prev</a><span>|</span><a href="#39010488">next</a><span>|</span><label class="collapse" for="c-39010560">[-]</label><label class="expand" for="c-39010560">[2 more]</label></div><br/><div class="children"><div class="content">I wonder how long would it take this to get fixed, if I fed some current best-seller novels into an LLM and instructed it to reword it, renaming the characters and places, and shared the result publicly for free?<p>Although I  fear the response would be that powerful AI orgs would put copyright filters in place and lobby for legalization for mandatory AI-DRM in open source AI as well.</div><br/></div></div><div id="39010488" class="c"><input type="checkbox" id="c-39010488" checked=""/><div class="controls bullet"><span class="by">Simon_ORourke</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39010560">prev</a><span>|</span><a href="#39010779">next</a><span>|</span><label class="collapse" for="c-39010488">[-]</label><label class="expand" for="c-39010488">[1 more]</label></div><br/><div class="children"><div class="content">You forgot to add &quot;rent-seeking hypocrites&quot;. Not one of them actually advance either social concerns or technical approaches to AI. They seem to exist in a space with solicits funding to pay some mouth-piece top-dollar to produce a report haranguing existing AI model for some nebulous future threat. Same with the clowns in the Distributed AI Research Institute, all &quot;won&#x27;t somebody think of the children&quot; style shrieking to get in the news while keeping their hand out for funding - hypocrites is right!</div><br/></div></div><div id="39010779" class="c"><input type="checkbox" id="c-39010779" checked=""/><div class="controls bullet"><span class="by">mtillman</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39010488">prev</a><span>|</span><a href="#39010412">next</a><span>|</span><label class="collapse" for="c-39010779">[-]</label><label class="expand" for="c-39010779">[1 more]</label></div><br/><div class="children"><div class="content">Important to remember that in Dune, the AI made the right decision which precipitated a whole lot of fun to read nonsense.</div><br/></div></div><div id="39010412" class="c"><input type="checkbox" id="c-39010412" checked=""/><div class="controls bullet"><span class="by">Llamamoe</span><span>|</span><a href="#39010218">parent</a><span>|</span><a href="#39010779">prev</a><span>|</span><a href="#39010089">next</a><span>|</span><label class="collapse" for="c-39010412">[-]</label><label class="expand" for="c-39010412">[5 more]</label></div><br/><div class="children"><div class="content">The by far biggest harm to society of AI is the devalustion of human creative output and replacing real humans with cheap AI solutions funneling wealth to the rich.<p>Compared to that, an open source LLM telling a curious teenager how to make gunpowder is... laughable.<p>This entire debacle is an example of disgusting &quot;think of the children!&quot; doublespeak, officially about safety, but really about locking shit down under corporate control.</div><br/><div id="39010630" class="c"><input type="checkbox" id="c-39010630" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010412">parent</a><span>|</span><a href="#39010896">next</a><span>|</span><label class="collapse" for="c-39010630">[-]</label><label class="expand" for="c-39010630">[1 more]</label></div><br/><div class="children"><div class="content">It sounds similar to the fears media rang about tat you can find a bomb making tutorial on the Internet - people were
genuinely afraid of that.<p>(Otoh an agi can bring unforseen consequences for humanity - and thatâs a genuine fear)</div><br/></div></div><div id="39010896" class="c"><input type="checkbox" id="c-39010896" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010412">parent</a><span>|</span><a href="#39010630">prev</a><span>|</span><a href="#39010665">next</a><span>|</span><label class="collapse" for="c-39010896">[-]</label><label class="expand" for="c-39010896">[1 more]</label></div><br/><div class="children"><div class="content">if the &#x27;devalustion&#x27; of human creative output and replacing real humans with cheap ai solutions funneling wealth to the rich is in fact the &#x27;by far biggest harm&#x27; then basically there&#x27;s nothing to worry about.  no government would ban or even restrict ai on those grounds<p>even the &#x27;terrists can figure out how to build an a-bomb&#x27; problem is relatively inconsequential<p>what ai safety people are worried about, by contrast, is that on april 22 of next year, at 7:07:33 utc, every person in the world will keel over dead at once, because the ai doesn&#x27;t need them and they pose a risk to its objectives.  or worse things than that<p>i don&#x27;t think that&#x27;s going to happen, but that&#x27;s what they&#x27;re concerned about</div><br/></div></div><div id="39010665" class="c"><input type="checkbox" id="c-39010665" checked=""/><div class="controls bullet"><span class="by">badgersnake</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010412">parent</a><span>|</span><a href="#39010896">prev</a><span>|</span><a href="#39010089">next</a><span>|</span><label class="collapse" for="c-39010665">[-]</label><label class="expand" for="c-39010665">[2 more]</label></div><br/><div class="children"><div class="content">The biggest harm is the torrent of AI generated misinformation used to manipulate people. Unfortunately itâs pretty hard to come up with a solution for that.</div><br/><div id="39010766" class="c"><input type="checkbox" id="c-39010766" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#39010218">root</a><span>|</span><a href="#39010665">parent</a><span>|</span><a href="#39010089">next</a><span>|</span><label class="collapse" for="c-39010766">[-]</label><label class="expand" for="c-39010766">[1 more]</label></div><br/><div class="children"><div class="content">Or the torrent of great information.<p>How are we all leaping to the bad? The world is better than it was 20 years ago.<p>I&#x27;m almost certain we&#x27;ll have AI assistants telling us relevant world news and information, keeping us up to tabs with everything we need to know, and completely removing distractions from life.</div><br/></div></div></div></div></div></div></div></div><div id="39010089" class="c"><input type="checkbox" id="c-39010089" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#39010218">prev</a><span>|</span><a href="#39010190">next</a><span>|</span><label class="collapse" for="c-39010089">[-]</label><label class="expand" for="c-39010089">[1 more]</label></div><br/><div class="children"><div class="content">There are many types of safety. For example, protecting your profits! I&#x27;d imagine that if we trace the money back these organisations will look a lot like lobbyists for existing companies in the AI space. I recall Microsoft&#x27;s licence enforcement effort was done with that sort of scheme, I think they used the BSA [0]. It has been a while though so maybe it was a different group.<p>Anyway, point being, if they can lobby for something unpopular under a different brand, that is how to do it. Much less PR risk.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Software_Alliance" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Software_Alliance</a></div><br/></div></div><div id="39010190" class="c"><input type="checkbox" id="c-39010190" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#39010089">prev</a><span>|</span><a href="#39010628">next</a><span>|</span><label class="collapse" for="c-39010190">[-]</label><label class="expand" for="c-39010190">[13 more]</label></div><br/><div class="children"><div class="content">There is a bigger reason why the end of open source AI might be close: as soon as training data becomes licensed, thatâs it for open source AI. Poof.<p>I wish I could be more eloquent on this point, but Iâve mostly just been depressed about this seeming inevitability.<p>Hopefully it wonât be the case. But how could it be otherwise? Hundreds of thousands of people are mad at openai and midjourney for doing exactly what open source AI needs to do in order to survive: fine tune or train from scratch.<p>As soon as some politician makes it a platform issue, it seems like the law will simply be rewritten to prevent companies from using training data at will. Itâs such a compelling story: &quot;big companies are stealing data owned by small businesses and individuals.&quot; So even if the court cases are decided in OpenAIâs favor, itâs not at all clear that the issue will be settled.</div><br/><div id="39010268" class="c"><input type="checkbox" id="c-39010268" checked=""/><div class="controls bullet"><span class="by">idle_zealot</span><span>|</span><a href="#39010190">parent</a><span>|</span><a href="#39010675">next</a><span>|</span><label class="collapse" for="c-39010268">[-]</label><label class="expand" for="c-39010268">[5 more]</label></div><br/><div class="children"><div class="content">The advantage that open AI (not the company) has is that if using copyrighted content as training data without licensing it is found to be illegal, they can just keep doing it. There&#x27;s plenty of FOSS software basically designed to violate copyright law (comic readers, home media center servers&#x2F;clients, torrent clients) that big tech cannot compete with lest they face legal consequences. Basically what I&#x27;m saying is that the open source community will continue to use books3 and scraped images to train while facebook and the like get stuck in legal quagmire.<p>Of course, this ignores the fact that popular &quot;open&quot; models of today were actually trained with facebook or other companies&#x27; computational resources, so unless some cheaper way to train models were developed we would actually be stuck with proprietary models trained with lots of compute but unable to use unlicensed training data, and open models that can use whatever data they like but must operate in the shadows without access to much compute for training.</div><br/><div id="39010319" class="c"><input type="checkbox" id="c-39010319" checked=""/><div class="controls bullet"><span class="by">livueta</span><span>|</span><a href="#39010190">root</a><span>|</span><a href="#39010268">parent</a><span>|</span><a href="#39010323">next</a><span>|</span><label class="collapse" for="c-39010319">[-]</label><label class="expand" for="c-39010319">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right that there are areas like torrenting where big tech can&#x27;t really go for fear of legal consequences, but there&#x27;s also the cat-and-mouse game played by IP holders against, say, torrent trackers, which leads me to think that<p>&gt; open models that can use whatever data they like but must operate in the shadows without access to much compute for training<p>sounds pretty analogous to private trackers, where high-quality stuff is available but not in full public view. If rightsholders and big tech, abetted by states, crack down on open models I think you&#x27;re right that the open source community will continue to train on liberated content, but it&#x27;s not going to be as open and free-flowing as things are now. Going back to the compute problem, I can imagine analogues of private trackers where contributors, not wanting to expose themselves to whatever the law-firm letter&#x2F;ISP strike analogue will be in this space, use invite-only closed networks to pool compute resources for training on encumbered content.</div><br/></div></div><div id="39010323" class="c"><input type="checkbox" id="c-39010323" checked=""/><div class="controls bullet"><span class="by">ayende</span><span>|</span><a href="#39010190">root</a><span>|</span><a href="#39010268">parent</a><span>|</span><a href="#39010319">prev</a><span>|</span><a href="#39010374">next</a><span>|</span><label class="collapse" for="c-39010323">[-]</label><label class="expand" for="c-39010323">[2 more]</label></div><br/><div class="children"><div class="content">As long as the cost for training a model is in the 7+ figures, that means that any such open model is bound to be tracked to someone with deep enough pockets to sue.<p>Consider that you just spend a few millions on training a model on copyrighted data. Release it would reveal that, problem.<p>I _guess_ you can try doing training in the public, like Seti @ Home or something like that, which distributes the risk? But no idea if this is even possible in this context.</div><br/><div id="39010346" class="c"><input type="checkbox" id="c-39010346" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39010190">root</a><span>|</span><a href="#39010323">parent</a><span>|</span><a href="#39010374">next</a><span>|</span><label class="collapse" for="c-39010346">[-]</label><label class="expand" for="c-39010346">[1 more]</label></div><br/><div class="children"><div class="content">Is the training cost equally that high if you do adversarial training?</div><br/></div></div></div></div><div id="39010374" class="c"><input type="checkbox" id="c-39010374" checked=""/><div class="controls bullet"><span class="by">shkkmo</span><span>|</span><a href="#39010190">root</a><span>|</span><a href="#39010268">parent</a><span>|</span><a href="#39010323">prev</a><span>|</span><a href="#39010675">next</a><span>|</span><label class="collapse" for="c-39010374">[-]</label><label class="expand" for="c-39010374">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s plenty of FOSS software basically designed to violate copyright law (comic readers, home media center servers&#x2F;clients, torrent clients)<p>The key difference is that those projects don&#x27;t violate copyright themselves, but facilitate users doing so. If training without a license is infringement then projects that are doing so will struggle to host code&#x2F;models publicly or access other parts of internet infrastructure.</div><br/></div></div></div></div><div id="39010675" class="c"><input type="checkbox" id="c-39010675" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#39010190">parent</a><span>|</span><a href="#39010268">prev</a><span>|</span><a href="#39010303">next</a><span>|</span><label class="collapse" for="c-39010675">[-]</label><label class="expand" for="c-39010675">[1 more]</label></div><br/><div class="children"><div class="content">Depends on what you want to use AI for - for a smarter AI we could probably do with the open source material, but better training techniques and some innovative breakthroughs in the model.<p>Also, an interesting side note is that certain countries may actually want their language corpus involved in training AI - so they may have copyrights that are more AI friendly. I imagine Poland or Czech Republic wanting as much as their data to be involved in training LLMs because it gives their cultures more exposure. Even more so with African countries.</div><br/></div></div><div id="39010303" class="c"><input type="checkbox" id="c-39010303" checked=""/><div class="controls bullet"><span class="by">mrtksn</span><span>|</span><a href="#39010190">parent</a><span>|</span><a href="#39010675">prev</a><span>|</span><a href="#39010700">next</a><span>|</span><label class="collapse" for="c-39010303">[-]</label><label class="expand" for="c-39010303">[1 more]</label></div><br/><div class="children"><div class="content">We have infinite data, a microphone and a camera can generate huge amount of it and the public domain literature is wast. Billions of people learn like that everyday.</div><br/></div></div><div id="39010700" class="c"><input type="checkbox" id="c-39010700" checked=""/><div class="controls bullet"><span class="by">gfodor</span><span>|</span><a href="#39010190">parent</a><span>|</span><a href="#39010303">prev</a><span>|</span><a href="#39010635">next</a><span>|</span><label class="collapse" for="c-39010700">[-]</label><label class="expand" for="c-39010700">[1 more]</label></div><br/><div class="children"><div class="content">The US maintaining its lead is a national security issue. Larry Summers is now on the board of OpenAI. The copyright holders are not going to win, or they are, the training will continue anyway, but the technology will be (for now) kept in the hands of the military and IC.</div><br/></div></div><div id="39010635" class="c"><input type="checkbox" id="c-39010635" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#39010190">parent</a><span>|</span><a href="#39010700">prev</a><span>|</span><a href="#39010921">next</a><span>|</span><label class="collapse" for="c-39010635">[-]</label><label class="expand" for="c-39010635">[2 more]</label></div><br/><div class="children"><div class="content">my question is - if it&#x27;s deemed illegal to train AI using proprietary data sets, what&#x27;s to stop companies from using their already existing LLMs to generate training data?</div><br/><div id="39010695" class="c"><input type="checkbox" id="c-39010695" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#39010190">root</a><span>|</span><a href="#39010635">parent</a><span>|</span><a href="#39010921">next</a><span>|</span><label class="collapse" for="c-39010695">[-]</label><label class="expand" for="c-39010695">[1 more]</label></div><br/><div class="children"><div class="content">The fear is that you wonât get better model from training on synthetic data from a worse model,
and you will miss a lot of modern day knowledge.<p>Imho the first is not necessarily so, but the second will be a real hindrance. You can talk about the pandemic with LLMs because they were trained on peopleâs
comments on the subject, but if we had training limits you wouldnât be able to do so.<p>Otoh with many cases a sufficiently smart AI can just reach out to the open source material - e.g. if you want to know about modern day politics of Europe, you can read reports of commissions, and if you want info on a new framework&#x2F;tech, you can just read source code or scientific papers</div><br/></div></div></div></div><div id="39010921" class="c"><input type="checkbox" id="c-39010921" checked=""/><div class="controls bullet"><span class="by">DiscourseFan</span><span>|</span><a href="#39010190">parent</a><span>|</span><a href="#39010635">prev</a><span>|</span><a href="#39010252">next</a><span>|</span><label class="collapse" for="c-39010921">[-]</label><label class="expand" for="c-39010921">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&quot;big companies are stealing data owned by small businesses and individuals.&quot;<p>I mean, they are.<p>Unfortunately, AI at this point in time (LLMs, Midjourney, etc.) do appear to be not <i>much</i> more than highly technical and complex forms of intellectual property theft, which are given the futuristic name of &quot;AI&quot; to cover what they are actually doing. That isn&#x27;t to say that they are <i>entirely</i> performing intellectual property theft; the models themselves are very fascinating in how they function, especially as sort of &quot;extra-rational&quot; entities that clearly have a logic, though not one we can fully understand.<p>But the sort of philosophical, metaphysical, technological side of LLMs and Midjourney are clearly not being <i>explored</i> by OpenAI in a responsible manner, otherwise they wouldn&#x27;t have just straight up stolen, seemingly, NYT articles. It&#x27;s just another example of Silicon Valley VCs using a wonderful technology solely for exploitative profits, just like how google search became a surveillance tool (and plenty of other examples to go along with that).<p>Its only right that we acknowledge the work of the human beings (all the art, writing, etc.) that went into the creation of the model, instead of pretending that there is some totally non-human machine intellect that is going to take over the world through its vast intelligence and negate all human action and all human work etc. etc. That is just a fantasy generated by wealthy Silicon Valley VCs as justification to themselves and those working under them, and those they are stealing from, to cover precisely that work that they would rather avoid paying for.</div><br/></div></div></div></div><div id="39010628" class="c"><input type="checkbox" id="c-39010628" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#39010190">prev</a><span>|</span><a href="#39011008">next</a><span>|</span><label class="collapse" for="c-39010628">[-]</label><label class="expand" for="c-39010628">[2 more]</label></div><br/><div class="children"><div class="content">Itâs kind of funny to me that these organizations are naming FLOP thresholds and crowning MMLU as the relevant evaluation metric. Seems that several of them have copy-pasted similar thresholds. As compute becomes cheaper, these thresholds will become cheaper and cheaper. Perhaps we will look back on them as quaint and nearsighted.</div><br/><div id="39010760" class="c"><input type="checkbox" id="c-39010760" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#39010628">parent</a><span>|</span><a href="#39011008">next</a><span>|</span><label class="collapse" for="c-39010760">[-]</label><label class="expand" for="c-39010760">[1 more]</label></div><br/><div class="children"><div class="content">Well, FLOP make sense,
kind of - you can make computing as cheap as you want, but without algorithmic improvements the FLOP count will stay the same. And algorithmic improvements will probably be discrete - happening once in a while, not continuous. There is also a mathematical limit to how efficient we can make the training - we donât know the limit
yet, but there is a point below which we wonât get.</div><br/></div></div></div></div><div id="39011008" class="c"><input type="checkbox" id="c-39011008" checked=""/><div class="controls bullet"><span class="by">RamblingCTO</span><span>|</span><a href="#39010628">prev</a><span>|</span><a href="#39010216">next</a><span>|</span><label class="collapse" for="c-39011008">[-]</label><label class="expand" for="c-39011008">[1 more]</label></div><br/><div class="children"><div class="content">Who are these people and where do they get their funding from? Feels like sock-puppets of sama or something? Is there any insight into that?</div><br/></div></div><div id="39010216" class="c"><input type="checkbox" id="c-39010216" checked=""/><div class="controls bullet"><span class="by">RishabhKharyal</span><span>|</span><a href="#39011008">prev</a><span>|</span><a href="#39010126">next</a><span>|</span><label class="collapse" for="c-39010216">[-]</label><label class="expand" for="c-39010216">[1 more]</label></div><br/><div class="children"><div class="content">1. Licensing of data will be huge bottleneck
2. Uncensored results will be used against opensource models questions hovering in dark or grey area
3. Limited compute compared to big corp and model size gap 7B for opensource and closed source would be magnitude bigger</div><br/></div></div><div id="39010126" class="c"><input type="checkbox" id="c-39010126" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#39010216">prev</a><span>|</span><a href="#39010781">next</a><span>|</span><label class="collapse" for="c-39010126">[-]</label><label class="expand" for="c-39010126">[1 more]</label></div><br/><div class="children"><div class="content">Never underestimate the desire of existing systems of control or power to self-propagate.</div><br/></div></div><div id="39010781" class="c"><input type="checkbox" id="c-39010781" checked=""/><div class="controls bullet"><span class="by">war321</span><span>|</span><a href="#39010126">prev</a><span>|</span><a href="#39010312">next</a><span>|</span><label class="collapse" for="c-39010781">[-]</label><label class="expand" for="c-39010781">[1 more]</label></div><br/><div class="children"><div class="content">Wish this wasn&#x27;t as common as it turned out to be sadly. Best thing to hope for is that the ability to train models continue to get cheaper and more accessible over time. Figuring out how to go from tens&#x2F;hundreds of millions of images needed for a foundation model to thousands or hundreds would be a start.</div><br/></div></div><div id="39010312" class="c"><input type="checkbox" id="c-39010312" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#39010781">prev</a><span>|</span><a href="#39010153">next</a><span>|</span><label class="collapse" for="c-39010312">[-]</label><label class="expand" for="c-39010312">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure there are many well meaning ai safety researchers, but I also see a lott of &quot;ai for me but not for thee&#x27; moat digging safety hypocrisy.</div><br/></div></div><div id="39010153" class="c"><input type="checkbox" id="c-39010153" checked=""/><div class="controls bullet"><span class="by">google234123</span><span>|</span><a href="#39010312">prev</a><span>|</span><a href="#39010333">next</a><span>|</span><label class="collapse" for="c-39010153">[-]</label><label class="expand" for="c-39010153">[1 more]</label></div><br/><div class="children"><div class="content">Iâm curious if the board of Anthropic AI will eventually try and kill the company because if Iâm not mistaken many of the members share this mindset</div><br/></div></div><div id="39010333" class="c"><input type="checkbox" id="c-39010333" checked=""/><div class="controls bullet"><span class="by">rootsudo</span><span>|</span><a href="#39010153">prev</a><span>|</span><a href="#39010264">next</a><span>|</span><label class="collapse" for="c-39010333">[-]</label><label class="expand" for="c-39010333">[2 more]</label></div><br/><div class="children"><div class="content">Why would we care about these &quot;AI Safety Orgs?&quot;<p>As a developer, do I need their certification or is it like a MADD situation where we if we don&#x27;t observe and diminish their appeal&#x2F;growth we will get laws with draconian measures that only benefit the big players?  e.g. a PAC? (Don&#x27;t get me wrong, drunk driving is &quot;bad&quot;, but for a group like MADD to exist, eh and meh.)<p>As of now, there is no need to really submit these these safety orgs, so as long as we don&#x27;t care for their approval - who cares, right?<p>Or is the optics far gone now that these orgs control the conversation?<p>Also, fun question: with AI safety orgs now attempting to police - who really polices the police &#x2F; do other orgs&#x2F;countries have the same rules, safety and artificial guard rails?</div><br/><div id="39010457" class="c"><input type="checkbox" id="c-39010457" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#39010333">parent</a><span>|</span><a href="#39010264">next</a><span>|</span><label class="collapse" for="c-39010457">[-]</label><label class="expand" for="c-39010457">[1 more]</label></div><br/><div class="children"><div class="content">MADD on one end that limit work at all, and SOC2&#x2F;ISO&#x2F;NIST style compliance limitations on the other that chill work in practice.<p>I&#x27;m more worried about the latter as already starting to bias project &amp; funding decisions in fintechs &amp; various govs we work with. Both a concrete practical concern today, and economics &amp; law are generally tied in theory anyways.</div><br/></div></div></div></div><div id="39010264" class="c"><input type="checkbox" id="c-39010264" checked=""/><div class="controls bullet"><span class="by">tehjoker</span><span>|</span><a href="#39010333">prev</a><span>|</span><a href="#39010573">next</a><span>|</span><label class="collapse" for="c-39010264">[-]</label><label class="expand" for="c-39010264">[3 more]</label></div><br/><div class="children"><div class="content">standard capitalist play to set up a moat by instilling fear</div><br/><div id="39010329" class="c"><input type="checkbox" id="c-39010329" checked=""/><div class="controls bullet"><span class="by">gdfgsdf</span><span>|</span><a href="#39010264">parent</a><span>|</span><a href="#39010573">next</a><span>|</span><label class="collapse" for="c-39010329">[-]</label><label class="expand" for="c-39010329">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Many AI safety orgs have tried to __criminalize__ currently-existing open-source AI<p>standard socialist play to use the government&#x27;s hand to achieve their own objectives</div><br/><div id="39010385" class="c"><input type="checkbox" id="c-39010385" checked=""/><div class="controls bullet"><span class="by">pmontra</span><span>|</span><a href="#39010264">root</a><span>|</span><a href="#39010329">parent</a><span>|</span><a href="#39010573">next</a><span>|</span><label class="collapse" for="c-39010385">[-]</label><label class="expand" for="c-39010385">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t everybody use the government to do what they want? It&#x27;s kind of the definition of government, in any country and in any political system.</div><br/></div></div></div></div></div></div><div id="39010573" class="c"><input type="checkbox" id="c-39010573" checked=""/><div class="controls bullet"><span class="by">breadbreadbread</span><span>|</span><a href="#39010264">prev</a><span>|</span><label class="collapse" for="c-39010573">[-]</label><label class="expand" for="c-39010573">[3 more]</label></div><br/><div class="children"><div class="content">Ah yes the old &quot;banning things=bad&quot; argument that doesn&#x27;t offer alternatives to fixing the issues with AI. Just ignore the issues with environmental impact, plagiarism, CP and other non-consensual shit in the data sets, scamming capabilities! All the groups asking for regulation here have **funding** and that means they are evil but we are good for using this tool that is massively subsidized by megacorps that have a vested interest in this market.</div><br/><div id="39010712" class="c"><input type="checkbox" id="c-39010712" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#39010573">parent</a><span>|</span><a href="#39010632">next</a><span>|</span><label class="collapse" for="c-39010712">[-]</label><label class="expand" for="c-39010712">[1 more]</label></div><br/><div class="children"><div class="content">Less harmful would be to ban large models from being not open.<p>If you ban open large models in US, you&#x27;ll cripple US and make few megacorps very rich, very quickly. You&#x27;ll drain talent to other (also competing) states. Truly bad actors won&#x27;t be affected, if anything they&#x27;ll get advantage.<p>People draw weird analogies equating llama2 to nuclear device etc. nonsense but the closer analogy would be to ban on semiconductors of certain efficiency for US itself.<p>Similar idiotic argument as for banning cryptography.</div><br/></div></div><div id="39010632" class="c"><input type="checkbox" id="c-39010632" checked=""/><div class="controls bullet"><span class="by">tomalbrc</span><span>|</span><a href="#39010573">parent</a><span>|</span><a href="#39010712">prev</a><span>|</span><label class="collapse" for="c-39010632">[-]</label><label class="expand" for="c-39010632">[1 more]</label></div><br/><div class="children"><div class="content">Seeing the lack of responses to your comment and downvotes, you couldn&#x27;t be more right</div><br/></div></div></div></div></div></div></div></div></div></body></html>