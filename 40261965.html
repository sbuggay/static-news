<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714899712614" as="style"/><link rel="stylesheet" href="styles.css?v=1714899712614"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://infini-ai-lab.github.io/Sequoia-Page/">SEQUOIA: Exact Llama2-70B on an RTX4090 with half-second per-token latency</a> <span class="domain">(<a href="https://infini-ai-lab.github.io">infini-ai-lab.github.io</a>)</span></div><div class="subtext"><span>zinccat</span> | <span>40 comments</span></div><br/><div><div id="40262090" class="c"><input type="checkbox" id="c-40262090" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#40262534">next</a><span>|</span><label class="collapse" for="c-40262090">[-]</label><label class="expand" for="c-40262090">[22 more]</label></div><br/><div class="children"><div class="content">this is quite worrying for OpenAI as the rate token prices have been plummeting thanks to Meta and its going to have to keep cutting its prices while capex remains flat. whatever Sam says in interviews just think the opposite and the whole picture comes together.<p>It&#x27;s almost a mathematical certainty that people who invested in OpenAI will need to reincarnate in multiple universes to ever see that money again but no bother  many are probably NVIDIA stock holders to even out the damage.</div><br/><div id="40262125" class="c"><input type="checkbox" id="c-40262125" checked=""/><div class="controls bullet"><span class="by">j-bos</span><span>|</span><a href="#40262090">parent</a><span>|</span><a href="#40262193">next</a><span>|</span><label class="collapse" for="c-40262125">[-]</label><label class="expand" for="c-40262125">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that why he&#x27;s making rounds to lock down the biggest AI&#x27;s?</div><br/></div></div><div id="40262193" class="c"><input type="checkbox" id="c-40262193" checked=""/><div class="controls bullet"><span class="by">michelsedgh</span><span>|</span><a href="#40262090">parent</a><span>|</span><a href="#40262125">prev</a><span>|</span><a href="#40262395">next</a><span>|</span><label class="collapse" for="c-40262193">[-]</label><label class="expand" for="c-40262193">[9 more]</label></div><br/><div class="children"><div class="content">I agree with you somewhat. You are correct unless they have a much better GPT model that have not released for whatever reason. They are a year ahead than competitors and GPT4 is pretty old now. I find it hard to believe they don’t have much more capable models now. We Will see though</div><br/><div id="40262211" class="c"><input type="checkbox" id="c-40262211" checked=""/><div class="controls bullet"><span class="by">easygenes</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262193">parent</a><span>|</span><a href="#40262382">next</a><span>|</span><label class="collapse" for="c-40262211">[-]</label><label class="expand" for="c-40262211">[6 more]</label></div><br/><div class="children"><div class="content">There&#x27;s wide speculation that what will be branded as either GPT-4.5 or GPT-5 has finished pretraining now and is undergoing internal testing for a fairly near-term release.</div><br/><div id="40262239" class="c"><input type="checkbox" id="c-40262239" checked=""/><div class="controls bullet"><span class="by">michelsedgh</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262211">parent</a><span>|</span><a href="#40262382">next</a><span>|</span><label class="collapse" for="c-40262239">[-]</label><label class="expand" for="c-40262239">[5 more]</label></div><br/><div class="children"><div class="content">My speculation is that internally they have much stronger models like Q* but they won’t be able to release them to public even if they want to for lack of compute and safety and other reasons they see probably…</div><br/><div id="40262842" class="c"><input type="checkbox" id="c-40262842" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262239">parent</a><span>|</span><a href="#40262332">next</a><span>|</span><label class="collapse" for="c-40262842">[-]</label><label class="expand" for="c-40262842">[2 more]</label></div><br/><div class="children"><div class="content">I am curious whether this is true - OAI at least has the reputation in the industry of caring the least about safety of the major labs</div><br/><div id="40262974" class="c"><input type="checkbox" id="c-40262974" checked=""/><div class="controls bullet"><span class="by">hhh</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262842">parent</a><span>|</span><a href="#40262332">next</a><span>|</span><label class="collapse" for="c-40262974">[-]</label><label class="expand" for="c-40262974">[1 more]</label></div><br/><div class="children"><div class="content">If they don’t care about safety (or perceived safety), why do they spend so much time lobotomizing models for safety reasons?</div><br/></div></div></div></div><div id="40262332" class="c"><input type="checkbox" id="c-40262332" checked=""/><div class="controls bullet"><span class="by">kaliqt</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262239">parent</a><span>|</span><a href="#40262842">prev</a><span>|</span><a href="#40263227">next</a><span>|</span><label class="collapse" for="c-40262332">[-]</label><label class="expand" for="c-40262332">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t actually care about safety, that&#x27;s a lie, so compute and business strategy is the only thing stopping them.<p>SoRA is the same. It&#x27;s not ready and it&#x27;s too slow.</div><br/></div></div><div id="40263227" class="c"><input type="checkbox" id="c-40263227" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262239">parent</a><span>|</span><a href="#40262332">prev</a><span>|</span><a href="#40262382">next</a><span>|</span><label class="collapse" for="c-40263227">[-]</label><label class="expand" for="c-40263227">[1 more]</label></div><br/><div class="children"><div class="content">Honestly I&#x27;m pretty puzzled by this mystical fog that hangs over OpenAIs skunkworks projects - don&#x27;t people leave for other jobs&#x2F;go to conferences etc.?<p>I&#x27;m surprised that nobody call tell what they infact do or do not have.</div><br/></div></div></div></div></div></div><div id="40262382" class="c"><input type="checkbox" id="c-40262382" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262193">parent</a><span>|</span><a href="#40262211">prev</a><span>|</span><a href="#40263048">next</a><span>|</span><label class="collapse" for="c-40262382">[-]</label><label class="expand" for="c-40262382">[1 more]</label></div><br/><div class="children"><div class="content">The polish of OpenAI stuff when released has been quite mature since gpt4 or even 3.5.<p>They are no doubt sitting on ultra polished stuff.  When you are the tip of the arrow though and the cutting edge itself it might not be as efficient but does it ever show you things you can’t unsee.<p>When OpenAI can launch a video thing a day after because it’s ready to go.  I am less and less skeptical e dry time they ship because the quality of the first version isn’t sliding back wards even in different areas like video.<p>Maybe releasing it is strategic, or releasing it also requires supporting it infrastructure wise and then some.  That might be a challenge.<p>My feeling is the next model of an k between may have massive efficiency and performance improvements without having to go quantum with brute forcing it.<p>Meanwhile others who are following what OpenAI has done seem to be able to optimize it and make it more efficient whether it’s open source or otherwise.<p>Both are doing important work and I&#x27;m not sure I want to see it as a one winner take all game.<p>The way AI vendors are responding suddenly to another’s launch feels like they are always ready to launch and continue to add functionality to it that could also ship.<p>It reminds me of when Google spent a billion dollars advertising bing had a billion pages indexed. Google stayed quiet. Then when the money was spent by Microsoft, Google simply added a zero or two to their search page, when they used to list how many pages they have indexed. They were just sitting on it already done, announcing it when it’s to their benefit.</div><br/></div></div><div id="40263048" class="c"><input type="checkbox" id="c-40263048" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262193">parent</a><span>|</span><a href="#40262382">prev</a><span>|</span><a href="#40262395">next</a><span>|</span><label class="collapse" for="c-40263048">[-]</label><label class="expand" for="c-40263048">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not saying Claude 3 and Gemini are better than GPT4 in every aspect, but those two models can at least perform addition on arbitrarily long numbers, meanwhile GPT4 struggles.</div><br/></div></div></div></div><div id="40262395" class="c"><input type="checkbox" id="c-40262395" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#40262090">parent</a><span>|</span><a href="#40262193">prev</a><span>|</span><a href="#40262200">next</a><span>|</span><label class="collapse" for="c-40262395">[-]</label><label class="expand" for="c-40262395">[5 more]</label></div><br/><div class="children"><div class="content">I disagree.<p>a) A year after GPT-4 set the bar, it&#x27;s still the best model, despite everyone else not having to do it first. Just copy, and just software. And that&#x27;s not for lack of trying by <i>every other viable prime player on the planet</i> with unprecedented acceleration.<p>Imagine any other piece of software, where the incumbent has a mere 2-3 year head start, in which they had to work out the entire product that everyone else, despite <i>just having to copy</i> and pressing the pedal through the floor is struggling just trying to catch up with.<p>b) The current models including GPT-4 are <i>so</i> bad. The few billions can be made by just by continue playing this game of improvements for a few years and getting better each year. I think people are wildly confused about how big this market is going to be when that happens. They are not squeezing hosting or compute. They are squeezing intelligence. Intelligence is the entire economy. The notion that there would ever not be room for multiple things here, maybe through size or specialisation or cost (as with all other intelligence), and that a few billion dollar are a big deal, is so strange to me.<p>c) The game will at some point, be mostly about infra and optimization. People come to the conclusion that&#x27;s a problem for the incumbents, when our entire industry is mostly about infra and optimization. AWS is infra and optimization. I think even the average hn tinkerer understands that therein lies a proposition that&#x27;s not exactly equivalent to &quot;just rent a few servers and do it yourself&quot;.</div><br/><div id="40262505" class="c"><input type="checkbox" id="c-40262505" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262395">parent</a><span>|</span><a href="#40262200">next</a><span>|</span><label class="collapse" for="c-40262505">[-]</label><label class="expand" for="c-40262505">[4 more]</label></div><br/><div class="children"><div class="content">&gt; A year after GPT-4 set the bar, it&#x27;s still the best model<p>Debatable. Many people find Claude Opus superior, and I know I&#x27;ve found it consistently better for challenging coding questions. More importantly, the delta between GPT-4 and everything else is getting smaller and smaller. Llama 3 is basically interchangeable with GPT-4 for a huge number of tasks, despite its smaller size.</div><br/><div id="40262635" class="c"><input type="checkbox" id="c-40262635" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262505">parent</a><span>|</span><a href="#40262200">next</a><span>|</span><label class="collapse" for="c-40262635">[-]</label><label class="expand" for="c-40262635">[3 more]</label></div><br/><div class="children"><div class="content">GPT-4 was released in March 2023.<p>Which means the research that went into it would&#x27;ve been finalised quite some time prior.<p>Meaning that you&#x27;re getting close to a 2 year head start.</div><br/><div id="40262717" class="c"><input type="checkbox" id="c-40262717" checked=""/><div class="controls bullet"><span class="by">acheong08</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262635">parent</a><span>|</span><a href="#40262697">next</a><span>|</span><label class="collapse" for="c-40262717">[-]</label><label class="expand" for="c-40262717">[1 more]</label></div><br/><div class="children"><div class="content">While they still call it GPT-4, the one topping the rankings are newer iterations of it despite still retaining the same name. The latest one is from 2024-04-09. Sure that one probably finished training a few months ago but it is by no means a 2 year head start.</div><br/></div></div></div></div></div></div></div></div><div id="40262200" class="c"><input type="checkbox" id="c-40262200" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40262090">parent</a><span>|</span><a href="#40262395">prev</a><span>|</span><a href="#40262213">next</a><span>|</span><label class="collapse" for="c-40262200">[-]</label><label class="expand" for="c-40262200">[1 more]</label></div><br/><div class="children"><div class="content">There’s a Pareto frontier where Meta is pushing out the boundaries along the “private” and “cheap” axes.<p>Open AI can release GPT 4.5 or 5 and push out the boundary in the direction of “correctness” and “multimodality”.<p>Either way, we win as customers while the the level of competition remains this hot.<p>I personally want a smart AI much more than a cheap or fast one. Your mileage may vary.</div><br/></div></div><div id="40262213" class="c"><input type="checkbox" id="c-40262213" checked=""/><div class="controls bullet"><span class="by">14u2c</span><span>|</span><a href="#40262090">parent</a><span>|</span><a href="#40262200">prev</a><span>|</span><a href="#40262107">next</a><span>|</span><label class="collapse" for="c-40262213">[-]</label><label class="expand" for="c-40262213">[1 more]</label></div><br/><div class="children"><div class="content">Most of the big &quot;investments&quot; in OpenAI are in the form of compute credits. I fail to see the downside of that.</div><br/></div></div><div id="40262107" class="c"><input type="checkbox" id="c-40262107" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#40262090">parent</a><span>|</span><a href="#40262213">prev</a><span>|</span><a href="#40262365">next</a><span>|</span><label class="collapse" for="c-40262107">[-]</label><label class="expand" for="c-40262107">[1 more]</label></div><br/><div class="children"><div class="content">I suspect that when it costs 0.5c per 100 million generated token, and you can generate 1000 tokens per second, they&#x27;ll be very happy.</div><br/></div></div><div id="40262365" class="c"><input type="checkbox" id="c-40262365" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#40262090">parent</a><span>|</span><a href="#40262107">prev</a><span>|</span><a href="#40262534">next</a><span>|</span><label class="collapse" for="c-40262365">[-]</label><label class="expand" for="c-40262365">[3 more]</label></div><br/><div class="children"><div class="content">Disclaimer: not a fan of &quot;Open&quot;AI<p>Everyone could say anything about open source models, but they&#x27;re comparing themselves to what OpenAI released a year ago. They haven&#x27;t shown all of their cards yet and they have a decent moat already in place; some say they have no moat, I disagree, they have one of the best moats possible which is brand awareness.<p>Sora on its own could bring in billions in revenue; an open-source Sora will take at least another year, if not two, to come out. Then more time until it can run on commodity hardware. An open source model that only runs in a dedicated H100 is actually less useful than a closed model behind an API call; not to detract from open source, I think it&#x27;s the way to go but I&#x27;m just being pragmatic and realistic. There&#x27;s a reason why MS Office is still the top productivity app in the world, even though dozens of open source alternatives exist.</div><br/><div id="40263223" class="c"><input type="checkbox" id="c-40263223" checked=""/><div class="controls bullet"><span class="by">Hendrikto</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262365">parent</a><span>|</span><a href="#40262462">next</a><span>|</span><label class="collapse" for="c-40263223">[-]</label><label class="expand" for="c-40263223">[1 more]</label></div><br/><div class="children"><div class="content">&gt; they have one of the best moats possible which is brand awareness.<p>Do they though?<p>If you talk to &quot;regular people&quot;, everybody knows ChatGPT, but nobody knows or cares about OpenAI. And most of them don‘t even really know that name. They call it ChatUuuuhm, ChatThingy, Chad Gippity, or similar.<p>I think they will just switch, when something better comes along.</div><br/></div></div><div id="40262462" class="c"><input type="checkbox" id="c-40262462" checked=""/><div class="controls bullet"><span class="by">poslathian</span><span>|</span><a href="#40262090">root</a><span>|</span><a href="#40262365">parent</a><span>|</span><a href="#40263223">prev</a><span>|</span><a href="#40262534">next</a><span>|</span><label class="collapse" for="c-40262462">[-]</label><label class="expand" for="c-40262462">[1 more]</label></div><br/><div class="children"><div class="content">MS had yet to fully stabilize that lead a full decade after they had won the os platform standard for ibm compatible pcs. A platform standard moat goes way way beyond a brand advantage.<p>Azure, while significant, has no similar monopoly to support OpenAI. Do you really see a structural advantage to openAI beyond the Microsoft products integrating it?</div><br/></div></div></div></div></div></div><div id="40262534" class="c"><input type="checkbox" id="c-40262534" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40262090">prev</a><span>|</span><a href="#40262041">next</a><span>|</span><label class="collapse" for="c-40262534">[-]</label><label class="expand" for="c-40262534">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t need exact results. FP8 quantization is almost lossless and even 6-bit quantization is usually acceptable. Can this be combined with quantization?</div><br/><div id="40263049" class="c"><input type="checkbox" id="c-40263049" checked=""/><div class="controls bullet"><span class="by">dimask</span><span>|</span><a href="#40262534">parent</a><span>|</span><a href="#40262633">next</a><span>|</span><label class="collapse" for="c-40263049">[-]</label><label class="expand" for="c-40263049">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Can this be combined with quantization?<p>It is in their TODO part in <a href="https:&#x2F;&#x2F;github.com&#x2F;Infini-AI-Lab&#x2F;Sequoia&#x2F;tree&#x2F;main">https:&#x2F;&#x2F;github.com&#x2F;Infini-AI-Lab&#x2F;Sequoia&#x2F;tree&#x2F;main</a></div><br/></div></div><div id="40262633" class="c"><input type="checkbox" id="c-40262633" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#40262534">parent</a><span>|</span><a href="#40263049">prev</a><span>|</span><a href="#40262041">next</a><span>|</span><label class="collapse" for="c-40262633">[-]</label><label class="expand" for="c-40262633">[1 more]</label></div><br/><div class="children"><div class="content">Yes.
 It&#x27;s speculative decoding but instead of generating just a few sequential tokens with the draft model they generate a whole tree of some sort of optimal shape with hundreds of possible sequences.<p>It ends up being somewhat faster than regular speculative decoding in normal setting (GPU only). If you are doing CPU offloading it&#x27;s massively faster.<p>Edit typo</div><br/></div></div></div></div><div id="40262041" class="c"><input type="checkbox" id="c-40262041" checked=""/><div class="controls bullet"><span class="by">freeqaz</span><span>|</span><a href="#40262534">prev</a><span>|</span><a href="#40263055">next</a><span>|</span><label class="collapse" for="c-40262041">[-]</label><label class="expand" for="c-40262041">[2 more]</label></div><br/><div class="children"><div class="content">So this is 8x faster for serving these models than before? Or is this about it being more deterministic? I can&#x27;t quite tell from reading it.</div><br/><div id="40262104" class="c"><input type="checkbox" id="c-40262104" checked=""/><div class="controls bullet"><span class="by">maccam912</span><span>|</span><a href="#40262041">parent</a><span>|</span><a href="#40263055">next</a><span>|</span><label class="collapse" for="c-40262104">[-]</label><label class="expand" for="c-40262104">[1 more]</label></div><br/><div class="children"><div class="content">The idea is to serve models that would normally be considered too large for GPU memory (70 billion parameters at 16 bytes each for 140 GB of memory required). Some people figured out you can offload the model and only have parts of it loaded so a 24 GB GPU like the 4090 can still serve the model, but it goes a lot slower. They have a new way to serve the same model on the same GPU but 8x better throughput. Something about decoding tokens on a smaller model maybe, then just checking multiple tokens on the larger model in a single batch. Magic, but ultimately its the same model, same GPU, same output as before, but much better throughout.</div><br/></div></div></div></div><div id="40263055" class="c"><input type="checkbox" id="c-40263055" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#40262041">prev</a><span>|</span><a href="#40262144">next</a><span>|</span><label class="collapse" for="c-40263055">[-]</label><label class="expand" for="c-40263055">[1 more]</label></div><br/><div class="children"><div class="content">Is it me or is this paper basically missing all technical information?<p>I get that Therese proprietary technology, but if so, can we please not put this on arxiv and pretend it’s a scientific contribution?</div><br/></div></div><div id="40262144" class="c"><input type="checkbox" id="c-40262144" checked=""/><div class="controls bullet"><span class="by">aussieguy1234</span><span>|</span><a href="#40263055">prev</a><span>|</span><a href="#40262156">next</a><span>|</span><label class="collapse" for="c-40262144">[-]</label><label class="expand" for="c-40262144">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m looking at buying 2 X RTX 3060s to run LLama 70b for my new PC I just purchased.<p>Will this work, or do I need a Tesla P40 or two?</div><br/><div id="40262170" class="c"><input type="checkbox" id="c-40262170" checked=""/><div class="controls bullet"><span class="by">dannyw</span><span>|</span><a href="#40262144">parent</a><span>|</span><a href="#40262156">next</a><span>|</span><label class="collapse" for="c-40262170">[-]</label><label class="expand" for="c-40262170">[1 more]</label></div><br/><div class="children"><div class="content">Theoretically there&#x27;s no reason why this shouldn&#x27;t work, but you likely will find the software isn&#x27;t designed for multi-GPU and have to reimplement&#x2F;fix things yourself.<p>You will also be getting about 720GB&#x2F;s of memory bandwidth with 2x3060; instead of 1TB&#x2F;s with the 4090; so expect lower performance.</div><br/></div></div></div></div><div id="40262058" class="c"><input type="checkbox" id="c-40262058" checked=""/><div class="controls bullet"><span class="by">halyconWays</span><span>|</span><a href="#40262156">prev</a><span>|</span><a href="#40262192">next</a><span>|</span><label class="collapse" for="c-40262058">[-]</label><label class="expand" for="c-40262058">[1 more]</label></div><br/><div class="children"><div class="content">Someone get this into koboldcpp</div><br/></div></div><div id="40262192" class="c"><input type="checkbox" id="c-40262192" checked=""/><div class="controls bullet"><span class="by">thelittleone</span><span>|</span><a href="#40262058">prev</a><span>|</span><label class="collapse" for="c-40262192">[-]</label><label class="expand" for="c-40262192">[6 more]</label></div><br/><div class="children"><div class="content">Other than portability and privacy, are there any benefits to running a local model with a 4090, versus running the same model on-demand on a cloud service with the same or more powerful card?</div><br/><div id="40262248" class="c"><input type="checkbox" id="c-40262248" checked=""/><div class="controls bullet"><span class="by">razodactyl</span><span>|</span><a href="#40262192">parent</a><span>|</span><a href="#40262338">next</a><span>|</span><label class="collapse" for="c-40262248">[-]</label><label class="expand" for="c-40262248">[2 more]</label></div><br/><div class="children"><div class="content">There are always going to be pros and cons. That&#x27;s why solutions like managed databases are reality. From an expert perspective it seems like there&#x27;s more to lose but from the perspective of a company with employee turn over, possible data loss, security etc. the benefits start to far outweigh the costs.<p>This reasoning can mostly be applied here. If you want to learn about and pull the LLM apart. Perhaps fine-tune and tinker then 100% go ahead running locally. You however won&#x27;t be able to scale this up easily for a consumer base and the electricity use and heat output starts to become a problem.<p>At some point it&#x27;s more beneficial to pay the provider for inference, this includes upkeep, latest models, faster generation, stability, hosting etc.<p>Pros and cons! Choice is important and Meta is doing the right thing by the AI community and tech community in general by being realistic with these programs. The ecosystem is giving back by being able to access these high quality models.</div><br/><div id="40262398" class="c"><input type="checkbox" id="c-40262398" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#40262192">root</a><span>|</span><a href="#40262248">parent</a><span>|</span><a href="#40262338">next</a><span>|</span><label class="collapse" for="c-40262398">[-]</label><label class="expand" for="c-40262398">[1 more]</label></div><br/><div class="children"><div class="content">What Meta is doing is very nice and differentiates them.<p>I also hope that it ought not change if it became more palatable to not be open.</div><br/></div></div></div></div><div id="40262338" class="c"><input type="checkbox" id="c-40262338" checked=""/><div class="controls bullet"><span class="by">kaliqt</span><span>|</span><a href="#40262192">parent</a><span>|</span><a href="#40262248">prev</a><span>|</span><a href="#40263084">next</a><span>|</span><label class="collapse" for="c-40262338">[-]</label><label class="expand" for="c-40262338">[1 more]</label></div><br/><div class="children"><div class="content">Guaranteed uptime<i>.<p></i> you are the guarantor but that&#x27;s good enough.</div><br/></div></div><div id="40263084" class="c"><input type="checkbox" id="c-40263084" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40262192">parent</a><span>|</span><a href="#40262338">prev</a><span>|</span><a href="#40262412">next</a><span>|</span><label class="collapse" for="c-40263084">[-]</label><label class="expand" for="c-40263084">[1 more]</label></div><br/><div class="children"><div class="content">If you have a robot or self driving car, you&#x27;re going to want on device inference for your vision language models.<p>For video games, being locked to a cloud service means the feature will disappear when the servers are shut down.</div><br/></div></div><div id="40262412" class="c"><input type="checkbox" id="c-40262412" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#40262192">parent</a><span>|</span><a href="#40263084">prev</a><span>|</span><label class="collapse" for="c-40262412">[-]</label><label class="expand" for="c-40262412">[1 more]</label></div><br/><div class="children"><div class="content">Eventually these models will need to run on mobile devices, so commodity desktop GPUs are a good stepping stone.  Alexnet &#x2F; Caffe got traction because they could be run on commodity desktop machines.  Then a few years later phones could run object detection etc.</div><br/></div></div></div></div></div></div></div></div></div></body></html>