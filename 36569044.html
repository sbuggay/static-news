<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688374862775" as="style"/><link rel="stylesheet" href="styles.css?v=1688374862775"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/">Nvidia’s H100: Funny L2, and Tons of Bandwidth</a> <span class="domain">(<a href="https://chipsandcheese.com">chipsandcheese.com</a>)</span></div><div class="subtext"><span>picture</span> | <span>11 comments</span></div><br/><div><div id="36569675" class="c"><input type="checkbox" id="c-36569675" checked=""/><div class="controls bullet"><span class="by">nromiun</span><span>|</span><a href="#36569858">next</a><span>|</span><label class="collapse" for="c-36569675">[-]</label><label class="expand" for="c-36569675">[3 more]</label></div><br/><div class="children"><div class="content">&gt; And sadly, Nvidia does not support OpenCL’s FP16 extension, so FP16 throughput couldn’t be tested<p>What a bummer. But is this really true? I know Nvidia does not report the cl_khr_fp_16 extension, but I saw somewhere that you can still use fp16 types in your code. Has anyone tested this?</div><br/><div id="36570352" class="c"><input type="checkbox" id="c-36570352" checked=""/><div class="controls bullet"><span class="by">ZiiS</span><span>|</span><a href="#36569675">parent</a><span>|</span><a href="#36569858">next</a><span>|</span><label class="collapse" for="c-36570352">[-]</label><label class="expand" for="c-36570352">[2 more]</label></div><br/><div class="children"><div class="content">Yes FP16 is fully optimised on NVidia hardware; but if you don&#x27;t support standards...</div><br/><div id="36570429" class="c"><input type="checkbox" id="c-36570429" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#36569675">root</a><span>|</span><a href="#36570352">parent</a><span>|</span><a href="#36569858">next</a><span>|</span><label class="collapse" for="c-36570429">[-]</label><label class="expand" for="c-36570429">[1 more]</label></div><br/><div class="children"><div class="content">...You gain a near monopoly by getting people to use your proprietary stack?</div><br/></div></div></div></div></div></div><div id="36569858" class="c"><input type="checkbox" id="c-36569858" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36569675">prev</a><span>|</span><a href="#36569537">next</a><span>|</span><label class="collapse" for="c-36569858">[-]</label><label class="expand" for="c-36569858">[1 more]</label></div><br/><div class="children"><div class="content">No mention of the RAM (or was I skimming too hard)?! Used to be pretty crucial.<p>According to Tom&#x27;s Hardware, one could find 80GB boards for around 3500$.</div><br/></div></div><div id="36569537" class="c"><input type="checkbox" id="c-36569537" checked=""/><div class="controls bullet"><span class="by">evertedsphere</span><span>|</span><a href="#36569858">prev</a><span>|</span><label class="collapse" for="c-36569537">[-]</label><label class="expand" for="c-36569537">[6 more]</label></div><br/><div class="children"><div class="content">&gt; This giant die implements 144 Streaming Multiprocessors (SMs), 60 MB of L2 cache, and 12 512-bit HBM memory controllers. We’re testing H100’s PCIe version on Lambda Cloud, which enables 114 of those SMs [...]<p>What does &quot;enabled&quot; mean in this context?</div><br/><div id="36569606" class="c"><input type="checkbox" id="c-36569606" checked=""/><div class="controls bullet"><span class="by">bpye</span><span>|</span><a href="#36569537">parent</a><span>|</span><label class="collapse" for="c-36569606">[-]</label><label class="expand" for="c-36569606">[5 more]</label></div><br/><div class="children"><div class="content">The physical die has 144 SMs but is fused, for binning and SKU differentiation, so that only 114 are usable.</div><br/><div id="36569656" class="c"><input type="checkbox" id="c-36569656" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#36569537">root</a><span>|</span><a href="#36569606">parent</a><span>|</span><label class="collapse" for="c-36569656">[-]</label><label class="expand" for="c-36569656">[4 more]</label></div><br/><div class="children"><div class="content">Seems like an odd detail. Is it safe to assume that this language is meant for investors? I’m the companies I’ve worked for, we never advertised the amount of efused-to-death silicon.</div><br/><div id="36570425" class="c"><input type="checkbox" id="c-36570425" checked=""/><div class="controls bullet"><span class="by">kklimonda</span><span>|</span><a href="#36569537">root</a><span>|</span><a href="#36569656">parent</a><span>|</span><a href="#36570200">next</a><span>|</span><label class="collapse" for="c-36570425">[-]</label><label class="expand" for="c-36570425">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always been like this in GPU space - all reviews have always mentioned number of compute units (be it SMS or &quot;cuda cores&quot;), and the total available for the given architecture is also known. A lot can be told about relative performance of two cards based on that, so this information is useful not only to the investors.</div><br/></div></div><div id="36570200" class="c"><input type="checkbox" id="c-36570200" checked=""/><div class="controls bullet"><span class="by">Culonavirus</span><span>|</span><a href="#36569537">root</a><span>|</span><a href="#36569656">parent</a><span>|</span><a href="#36570425">prev</a><span>|</span><a href="#36570354">next</a><span>|</span><label class="collapse" for="c-36570200">[-]</label><label class="expand" for="c-36570200">[1 more]</label></div><br/><div class="children"><div class="content">Probably a case of &quot;no one complained hard enough yet&quot;, also probably a case of &quot;beggars can&#x27;t be choosers&quot;... when literally billions of VC money is poured into both AI startups and estabilished cloud computing companies, all of that flows directly into Nvidia&#x27;s pockets and it&#x27;s not like this area is free from vendor lock in. When you have a stack that requires Nvidia&#x27;s hardware you are going to pay for Nvidia&#x27;s hardware. We live in a time when any hardware with a  &quot;matrix calculation accelerator&quot; label sells like hot cakes. It&#x27;s a massive bubble (HN doesn&#x27;t like when you use that word in combination with AI, but that&#x27;s what it is), but as with any bubble, people don&#x27;t care, they want to ride that wave while it&#x27;s there. But to get back to the issue, anything Nvidia will sell right now, it&#x27;s just a matter of who is going to be able to buy it first. So no one really complains about some of Nvidia&#x27;s marketing being a little dishonest. Also, even if people cared, being a trillion dollar company on your way to being one of the most valued companies on the planet, you have a lot of options and money for litigation.</div><br/></div></div><div id="36570354" class="c"><input type="checkbox" id="c-36570354" checked=""/><div class="controls bullet"><span class="by">twic</span><span>|</span><a href="#36569537">root</a><span>|</span><a href="#36569656">parent</a><span>|</span><a href="#36570200">prev</a><span>|</span><label class="collapse" for="c-36570354">[-]</label><label class="expand" for="c-36570354">[1 more]</label></div><br/><div class="children"><div class="content">The article says:<p>&gt; We’re testing H100’s PCIe version on Lambda Cloud, which enables 114 of those SMs, 50 MB of L2 cache, and 10 HBM2 memory controllers. The card can draw up to 350 W.<p>&gt; Nvidia also offers a SXM form factor H100, which can draw up to 700W and has 132 SMs enabled.<p>So i wonder if the number of enabled elements is due to a power supply or cooling constraint.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>