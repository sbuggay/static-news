<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716022854490" as="style"/><link rel="stylesheet" href="styles.css?v=1716022854490"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2405.09673">LoRA Learns Less and Forgets Less</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>wolecki</span> | <span>54 comments</span></div><br/><div><div id="40390728" class="c"><input type="checkbox" id="c-40390728" checked=""/><div class="controls bullet"><span class="by">thepasswordis</span><span>|</span><a href="#40390514">next</a><span>|</span><label class="collapse" for="c-40390728">[-]</label><label class="expand" for="c-40390728">[22 more]</label></div><br/><div class="children"><div class="content">I really wish people would be more careful about choosing names for these things.<p>LoRa has been a popular wireless protocol for like 10 years.</div><br/><div id="40390829" class="c"><input type="checkbox" id="c-40390829" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#40390728">parent</a><span>|</span><a href="#40397049">next</a><span>|</span><label class="collapse" for="c-40390829">[-]</label><label class="expand" for="c-40390829">[9 more]</label></div><br/><div class="children"><div class="content">Yes, but this is LoRA, clearly not LoRa.</div><br/><div id="40391278" class="c"><input type="checkbox" id="c-40391278" checked=""/><div class="controls bullet"><span class="by">squarefoot</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40390829">parent</a><span>|</span><a href="#40392648">next</a><span>|</span><label class="collapse" for="c-40391278">[-]</label><label class="expand" for="c-40391278">[4 more]</label></div><br/><div class="children"><div class="content">PP has a point though. I entered &quot;LoRA&quot; on Google, DuckDuckGo, Startpage and Bing, and all returned results in all first pages were about the communication protocol (1). They could have inferred my interests from previous searches, but I never used Bing in the last year or so, so it seems to me someone didn&#x27;t care about name clashes.<p>(1) well, except Google which -surprise- returned about mid page an ad of a local quite expensive chandeliers brand called &quot;LORA&quot;.</div><br/><div id="40393808" class="c"><input type="checkbox" id="c-40393808" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40391278">parent</a><span>|</span><a href="#40395508">prev</a><span>|</span><a href="#40396368">next</a><span>|</span><label class="collapse" for="c-40393808">[-]</label><label class="expand" for="c-40393808">[1 more]</label></div><br/><div class="children"><div class="content">I usually just add a term like &#x27;ml&#x27; or &#x27;nn&#x27; after my search to give the machine context and it is sufficient in most cases.</div><br/></div></div><div id="40396368" class="c"><input type="checkbox" id="c-40396368" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40391278">parent</a><span>|</span><a href="#40393808">prev</a><span>|</span><a href="#40392648">next</a><span>|</span><label class="collapse" for="c-40396368">[-]</label><label class="expand" for="c-40396368">[1 more]</label></div><br/><div class="children"><div class="content">Wait until you find out its a name too</div><br/></div></div></div></div><div id="40392648" class="c"><input type="checkbox" id="c-40392648" checked=""/><div class="controls bullet"><span class="by">atherton33</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40390829">parent</a><span>|</span><a href="#40391278">prev</a><span>|</span><a href="#40397049">next</a><span>|</span><label class="collapse" for="c-40392648">[-]</label><label class="expand" for="c-40392648">[4 more]</label></div><br/><div class="children"><div class="content">I think you mean LoRa®, a registered trademark of Semtech® Corporation, for use only with permission and within specific guidelines. <a href="https:&#x2F;&#x2F;www.semtech.com&#x2F;uploads&#x2F;company&#x2F;FAQ-for-Use-of-LoRa-2017.pdf" rel="nofollow">https:&#x2F;&#x2F;www.semtech.com&#x2F;uploads&#x2F;company&#x2F;FAQ-for-Use-of-LoRa-...</a></div><br/><div id="40395996" class="c"><input type="checkbox" id="c-40395996" checked=""/><div class="controls bullet"><span class="by">mbirth</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40392648">parent</a><span>|</span><a href="#40397049">next</a><span>|</span><label class="collapse" for="c-40395996">[-]</label><label class="expand" for="c-40395996">[3 more]</label></div><br/><div class="children"><div class="content">Not to be confused with Semtex who sell a completely different kind of problem solver.</div><br/><div id="40396525" class="c"><input type="checkbox" id="c-40396525" checked=""/><div class="controls bullet"><span class="by">noisy_boy</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40395996">parent</a><span>|</span><a href="#40397049">next</a><span>|</span><label class="collapse" for="c-40396525">[-]</label><label class="expand" for="c-40396525">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t Semtex an explosive aka RDX?</div><br/><div id="40396662" class="c"><input type="checkbox" id="c-40396662" checked=""/><div class="controls bullet"><span class="by">yau8edq12i</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40396525">parent</a><span>|</span><a href="#40397049">next</a><span>|</span><label class="collapse" for="c-40396662">[-]</label><label class="expand" for="c-40396662">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you got the joke.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40397049" class="c"><input type="checkbox" id="c-40397049" checked=""/><div class="controls bullet"><span class="by">marcinzm</span><span>|</span><a href="#40390728">parent</a><span>|</span><a href="#40390829">prev</a><span>|</span><a href="#40392667">next</a><span>|</span><label class="collapse" for="c-40397049">[-]</label><label class="expand" for="c-40397049">[1 more]</label></div><br/><div class="children"><div class="content">And LORA stood for Level of Repair Analysis since the 70s.</div><br/></div></div><div id="40392667" class="c"><input type="checkbox" id="c-40392667" checked=""/><div class="controls bullet"><span class="by">mobilemidget</span><span>|</span><a href="#40390728">parent</a><span>|</span><a href="#40397049">prev</a><span>|</span><a href="#40393315">next</a><span>|</span><label class="collapse" for="c-40392667">[-]</label><label class="expand" for="c-40392667">[1 more]</label></div><br/><div class="children"><div class="content">I addressed the same in a previous &#x27;lora&#x27; post on HN. For me the name is already reserved for the radio telecommunication meaning. Nothing going to change that.</div><br/></div></div><div id="40393315" class="c"><input type="checkbox" id="c-40393315" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40390728">parent</a><span>|</span><a href="#40392667">prev</a><span>|</span><a href="#40392879">next</a><span>|</span><label class="collapse" for="c-40393315">[-]</label><label class="expand" for="c-40393315">[9 more]</label></div><br/><div class="children"><div class="content">Not sure about &quot;popular&quot;<p>99% of ML engineers wouldn&#x27;t know what it is.</div><br/><div id="40393478" class="c"><input type="checkbox" id="c-40393478" checked=""/><div class="controls bullet"><span class="by">enlyth</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40393315">parent</a><span>|</span><a href="#40396695">next</a><span>|</span><label class="collapse" for="c-40393478">[-]</label><label class="expand" for="c-40393478">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not an ML engineer and the only reason I know that the wireless protocol exists is because in every HN article, there&#x27;s a comment repeating the same complaint</div><br/></div></div><div id="40396695" class="c"><input type="checkbox" id="c-40396695" checked=""/><div class="controls bullet"><span class="by">bmitc</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40393315">parent</a><span>|</span><a href="#40393478">prev</a><span>|</span><a href="#40393618">next</a><span>|</span><label class="collapse" for="c-40396695">[-]</label><label class="expand" for="c-40396695">[1 more]</label></div><br/><div class="children"><div class="content">Not much of a surprise there. The ML culture is to reinvent names for everything.</div><br/></div></div><div id="40393618" class="c"><input type="checkbox" id="c-40393618" checked=""/><div class="controls bullet"><span class="by">Findecanor</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40393315">parent</a><span>|</span><a href="#40396695">prev</a><span>|</span><a href="#40394269">next</a><span>|</span><label class="collapse" for="c-40393618">[-]</label><label class="expand" for="c-40393618">[2 more]</label></div><br/><div class="children"><div class="content">99% of the engineers who are still working in ML (Machine Language) would.<p>A much smaller percent among those who write in ML (the functional programming language) probably, though.</div><br/><div id="40393775" class="c"><input type="checkbox" id="c-40393775" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40393618">parent</a><span>|</span><a href="#40394269">next</a><span>|</span><label class="collapse" for="c-40393775">[-]</label><label class="expand" for="c-40393775">[1 more]</label></div><br/><div class="children"><div class="content">Even if the ML <i>engineers</i> know about the wireless protocol (and I doubt that many do), the scientists&#x2F;researchers who develop these models probably don&#x27;t. They are completely different domain. The lead author on this paper is basically a neuroscientist; some of the other are technically computer scientists, but probably have little hands-on experience with networking beyond whatever they did in undergrad.</div><br/></div></div></div></div><div id="40394269" class="c"><input type="checkbox" id="c-40394269" checked=""/><div class="controls bullet"><span class="by">goodpoint</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40393315">parent</a><span>|</span><a href="#40393618">prev</a><span>|</span><a href="#40392879">next</a><span>|</span><label class="collapse" for="c-40394269">[-]</label><label class="expand" for="c-40394269">[4 more]</label></div><br/><div class="children"><div class="content">...but they should know how to use search engines...</div><br/><div id="40397055" class="c"><input type="checkbox" id="c-40397055" checked=""/><div class="controls bullet"><span class="by">marcinzm</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40394269">parent</a><span>|</span><a href="#40396680">next</a><span>|</span><label class="collapse" for="c-40397055">[-]</label><label class="expand" for="c-40397055">[1 more]</label></div><br/><div class="children"><div class="content">As should have the IoT people to not conflict with the decades old LORA name used for Level of Repair Analysis.</div><br/></div></div><div id="40396680" class="c"><input type="checkbox" id="c-40396680" checked=""/><div class="controls bullet"><span class="by">bryanrasmussen</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40394269">parent</a><span>|</span><a href="#40397055">prev</a><span>|</span><a href="#40392879">next</a><span>|</span><label class="collapse" for="c-40396680">[-]</label><label class="expand" for="c-40396680">[2 more]</label></div><br/><div class="children"><div class="content">at some point we are going to run out of easily pronounceable abbreviations that are unique. Perhaps that point is actually in the past and we should just acknowledge it and move on. Although I guess it could have been Lorall - oops, that&#x27;s a character in World of Warcraft.</div><br/><div id="40397053" class="c"><input type="checkbox" id="c-40397053" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40390728">root</a><span>|</span><a href="#40396680">parent</a><span>|</span><a href="#40392879">next</a><span>|</span><label class="collapse" for="c-40397053">[-]</label><label class="expand" for="c-40397053">[1 more]</label></div><br/><div class="children"><div class="content">Old concepts become obsolete anyway. People can start reusing VCR, etc.</div><br/></div></div></div></div></div></div></div></div><div id="40392879" class="c"><input type="checkbox" id="c-40392879" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#40390728">parent</a><span>|</span><a href="#40393315">prev</a><span>|</span><a href="#40390514">next</a><span>|</span><label class="collapse" for="c-40392879">[-]</label><label class="expand" for="c-40392879">[1 more]</label></div><br/><div class="children"><div class="content">Seriously, that was a terrible name for the wireless system since it&#x27;s been used by the Loyola Online Records Access system for half a decade or more before the radio company shamelessly copied the name.</div><br/></div></div></div></div><div id="40390514" class="c"><input type="checkbox" id="c-40390514" checked=""/><div class="controls bullet"><span class="by">chaos_emergent</span><span>|</span><a href="#40390728">prev</a><span>|</span><a href="#40394120">next</a><span>|</span><label class="collapse" for="c-40390514">[-]</label><label class="expand" for="c-40390514">[13 more]</label></div><br/><div class="children"><div class="content">The findings are that the best fine-tune performance comes from fine-tuning all weights, followed my MLPs, followed by attention heads, using LoRA. Authors assert that the performance difference is based on the target module of the NN.<p>Isn’t an equally valid argument that MLPs tend to constitute a greater number of weights in transformer networks than attention heads, and the performance difference can be traced to a greater number of weights having freedom to change? I’d be curious to know if randomly choosing a subset of matrices to train, regardless of where they are in the network, would provide analogous performance to LoRA on a specific module with comparable learnable weights.</div><br/><div id="40392300" class="c"><input type="checkbox" id="c-40392300" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#40390514">parent</a><span>|</span><a href="#40390541">next</a><span>|</span><label class="collapse" for="c-40392300">[-]</label><label class="expand" for="c-40392300">[3 more]</label></div><br/><div class="children"><div class="content">I think the QLoRA paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.14314" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.14314</a> paper also showed LoRA on all MLP + Attention layers &gt; all MLP layers &gt; just Attention layers.<p>Other papers show finetuning a select few layers can also work well.</div><br/><div id="40395346" class="c"><input type="checkbox" id="c-40395346" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40392300">parent</a><span>|</span><a href="#40390541">next</a><span>|</span><label class="collapse" for="c-40395346">[-]</label><label class="expand" for="c-40395346">[2 more]</label></div><br/><div class="children"><div class="content">Any real world performance comparison between QLoRa and LoRa?</div><br/><div id="40397034" class="c"><input type="checkbox" id="c-40397034" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40395346">parent</a><span>|</span><a href="#40390541">next</a><span>|</span><label class="collapse" for="c-40397034">[-]</label><label class="expand" for="c-40397034">[1 more]</label></div><br/><div class="children"><div class="content">The QLoRA paper itself provided some cool benchmarks across many many experiments - QLoRA is near equivalent to LoRA, with it sometimes exceeding or losing 1-2% accuracy (it depends on the use case)</div><br/></div></div></div></div></div></div><div id="40390541" class="c"><input type="checkbox" id="c-40390541" checked=""/><div class="controls bullet"><span class="by">chaos_emergent</span><span>|</span><a href="#40390514">parent</a><span>|</span><a href="#40392300">prev</a><span>|</span><a href="#40394120">next</a><span>|</span><label class="collapse" for="c-40390541">[-]</label><label class="expand" for="c-40390541">[9 more]</label></div><br/><div class="children"><div class="content">as a follow up curiosity, has anyone tried using LoRA on the entire model for pretraining to compare regular training model performance to LoRA?</div><br/><div id="40391571" class="c"><input type="checkbox" id="c-40391571" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40390541">parent</a><span>|</span><a href="#40390640">next</a><span>|</span><label class="collapse" for="c-40391571">[-]</label><label class="expand" for="c-40391571">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I’ve tested this out. It does train, but the scaling doesn’t seem to pan out. It’ll perform slightly better than the number of trainable parameters, but never improves as you scale, so for now there’s no benefit.</div><br/></div></div><div id="40390640" class="c"><input type="checkbox" id="c-40390640" checked=""/><div class="controls bullet"><span class="by">cabidaher</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40390541">parent</a><span>|</span><a href="#40391571">prev</a><span>|</span><a href="#40391394">next</a><span>|</span><label class="collapse" for="c-40390640">[-]</label><label class="expand" for="c-40390640">[2 more]</label></div><br/><div class="children"><div class="content">This paper [1] does atempt that and reports similar performance compared to conventional pre-training. However, they do start off by doing a normal full-rank training and claim that it is needed to &#x27;warm start&#x27; the training process.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.05695" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.05695</a></div><br/><div id="40391974" class="c"><input type="checkbox" id="c-40391974" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40390640">parent</a><span>|</span><a href="#40391394">next</a><span>|</span><label class="collapse" for="c-40391974">[-]</label><label class="expand" for="c-40391974">[1 more]</label></div><br/><div class="children"><div class="content">Oh yes this paper! The main issue is the scaling of the A and B LoRA matrices. Some papers show scaling the B matrix with larger learning rates (LoRA+) could be beneficial. DoRA for eg learns an auto scaling vector of numbers which tries to alleviate these issues.<p>Galore might be more equivalent to full pretraining with the gradients being low rank.</div><br/></div></div></div></div><div id="40391394" class="c"><input type="checkbox" id="c-40391394" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40390541">parent</a><span>|</span><a href="#40390640">prev</a><span>|</span><a href="#40392790">next</a><span>|</span><label class="collapse" for="c-40391394">[-]</label><label class="expand" for="c-40391394">[4 more]</label></div><br/><div class="children"><div class="content">Do you mean leaving most of the model in its initial, randomised state and only training a LoRA?</div><br/><div id="40391600" class="c"><input type="checkbox" id="c-40391600" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40391394">parent</a><span>|</span><a href="#40392790">next</a><span>|</span><label class="collapse" for="c-40391600">[-]</label><label class="expand" for="c-40391600">[3 more]</label></div><br/><div class="children"><div class="content">I’ve tested specifically this (on my personal time) :) It will train but I found the loss is proportional to the number of trainable parameters. So roughly to hit the performance of a standard 70m param model, you need to train ~70m lora params anyway.</div><br/><div id="40392242" class="c"><input type="checkbox" id="c-40392242" checked=""/><div class="controls bullet"><span class="by">cheald</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40391600">parent</a><span>|</span><a href="#40392790">next</a><span>|</span><label class="collapse" for="c-40392242">[-]</label><label class="expand" for="c-40392242">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worse than that, because lora requires two matrices per layer. At full rank, you have an additional NxN parameters to learn versus full finetuning, where N is min(input_features, output_features).<p>For example, tuning a layer of 128 in x 256 out is 32k params. Learning a full-rank lora for that layer would be two matrices of 128x128 and 128x256 = 48k params.</div><br/><div id="40393316" class="c"><input type="checkbox" id="c-40393316" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40392242">parent</a><span>|</span><a href="#40392790">next</a><span>|</span><label class="collapse" for="c-40393316">[-]</label><label class="expand" for="c-40393316">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, exactly. Though the 48k param lora might be as good as a 48k param layer of higher rank, I haven&#x27;t looked into that case really.</div><br/></div></div></div></div></div></div></div></div><div id="40392790" class="c"><input type="checkbox" id="c-40392790" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40390514">root</a><span>|</span><a href="#40390541">parent</a><span>|</span><a href="#40391394">prev</a><span>|</span><a href="#40394120">next</a><span>|</span><label class="collapse" for="c-40392790">[-]</label><label class="expand" for="c-40392790">[1 more]</label></div><br/><div class="children"><div class="content">i would be shocked if this worked well</div><br/></div></div></div></div></div></div><div id="40394120" class="c"><input type="checkbox" id="c-40394120" checked=""/><div class="controls bullet"><span class="by">iudexgundyr</span><span>|</span><a href="#40390514">prev</a><span>|</span><a href="#40394058">next</a><span>|</span><label class="collapse" for="c-40394120">[-]</label><label class="expand" for="c-40394120">[1 more]</label></div><br/><div class="children"><div class="content">I feel like this is a trivial conclusion. Keeping the rank low in the optimization is a common regularization technique.</div><br/></div></div><div id="40394058" class="c"><input type="checkbox" id="c-40394058" checked=""/><div class="controls bullet"><span class="by">rzzzt</span><span>|</span><a href="#40394120">prev</a><span>|</span><a href="#40392509">next</a><span>|</span><label class="collapse" for="c-40394058">[-]</label><label class="expand" for="c-40394058">[6 more]</label></div><br/><div class="children"><div class="content">This paper has 12 authors, which fascinates me to no end for some unexplainable reason. How does it work? Is it a common occurrence to have this many people working on a submission? Did each of them get at least a paragraph in edgewise?</div><br/><div id="40395299" class="c"><input type="checkbox" id="c-40395299" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#40394058">parent</a><span>|</span><a href="#40394152">next</a><span>|</span><label class="collapse" for="c-40395299">[-]</label><label class="expand" for="c-40395299">[1 more]</label></div><br/><div class="children"><div class="content">The general criteria for authorship require including the people who worked on the experiments and data for the paper, which can be more important contribution than most of the text in that paper. In other experimental fields, there are papers with dozens or even hundreds of authors, because it can take many people to get to a measurement of a single number in the paper.</div><br/></div></div><div id="40394152" class="c"><input type="checkbox" id="c-40394152" checked=""/><div class="controls bullet"><span class="by">repsak</span><span>|</span><a href="#40394058">parent</a><span>|</span><a href="#40395299">prev</a><span>|</span><a href="#40394712">next</a><span>|</span><label class="collapse" for="c-40394152">[-]</label><label class="expand" for="c-40394152">[2 more]</label></div><br/><div class="children"><div class="content">I raise you the Gemini paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.11805" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.11805</a></div><br/><div id="40394297" class="c"><input type="checkbox" id="c-40394297" checked=""/><div class="controls bullet"><span class="by">guyomes</span><span>|</span><a href="#40394058">root</a><span>|</span><a href="#40394152">parent</a><span>|</span><a href="#40394712">next</a><span>|</span><label class="collapse" for="c-40394297">[-]</label><label class="expand" for="c-40394297">[1 more]</label></div><br/><div class="children"><div class="content">All-in with the Foldit paper [1,2].<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Foldit" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Foldit</a><p>[2]: <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nature09304" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nature09304</a></div><br/></div></div></div></div><div id="40394712" class="c"><input type="checkbox" id="c-40394712" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#40394058">parent</a><span>|</span><a href="#40394152">prev</a><span>|</span><a href="#40396673">next</a><span>|</span><label class="collapse" for="c-40394712">[-]</label><label class="expand" for="c-40394712">[1 more]</label></div><br/><div class="children"><div class="content">For a serious answer, this is how it works in my field 
A researcher gets a grant with 3-7 co-investigators. This generates a bunch of data and other resources that will support 10 or more papers. Coinvestigators and PIs will ask their postdocs and grad students to write up a paper. PIs and co-Is go on every paper...because it&#x27;s a paper from their grant. Then the 1 to 4 grad students and post-docs go on the paper, depending on their specific material contributions to the work, be it analysis, conception, or execution, or writing. The numbers can stack up.</div><br/></div></div><div id="40396673" class="c"><input type="checkbox" id="c-40396673" checked=""/><div class="controls bullet"><span class="by">yau8edq12i</span><span>|</span><a href="#40394058">parent</a><span>|</span><a href="#40394712">prev</a><span>|</span><a href="#40392509">next</a><span>|</span><label class="collapse" for="c-40396673">[-]</label><label class="expand" for="c-40396673">[1 more]</label></div><br/><div class="children"><div class="content">Wait until you learn that the paper on the LHC has more than 5000 authors: <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nature.2015.17567" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nature.2015.17567</a></div><br/></div></div></div></div><div id="40392509" class="c"><input type="checkbox" id="c-40392509" checked=""/><div class="controls bullet"><span class="by">chriskanan</span><span>|</span><a href="#40394058">prev</a><span>|</span><a href="#40395827">next</a><span>|</span><label class="collapse" for="c-40392509">[-]</label><label class="expand" for="c-40392509">[1 more]</label></div><br/><div class="children"><div class="content">This study is great and addresses a question I&#x27;ve had about LoRA for a while.<p>In a continual learning paper from last year, I found LoRA was extremely effective for faster fine-tuning and not forgetting the original dataset:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.01904" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.01904</a></div><br/></div></div><div id="40395827" class="c"><input type="checkbox" id="c-40395827" checked=""/><div class="controls bullet"><span class="by">Saris</span><span>|</span><a href="#40392509">prev</a><span>|</span><a href="#40390259">next</a><span>|</span><label class="collapse" for="c-40395827">[-]</label><label class="expand" for="c-40395827">[1 more]</label></div><br/><div class="children"><div class="content">What does LoRa have to do with LLMs? Whoever named this thing screwed up big time.</div><br/></div></div><div id="40390259" class="c"><input type="checkbox" id="c-40390259" checked=""/><div class="controls bullet"><span class="by">hybridtupel</span><span>|</span><a href="#40395827">prev</a><span>|</span><a href="#40390274">next</a><span>|</span><label class="collapse" for="c-40390259">[-]</label><label class="expand" for="c-40390259">[3 more]</label></div><br/><div class="children"><div class="content">This is about Low-rank adaptation. Not to be confused with LoRa the long range proprietary radio communication technique, which hopefully doesn&#x27;t learn at all.</div><br/><div id="40390604" class="c"><input type="checkbox" id="c-40390604" checked=""/><div class="controls bullet"><span class="by">martinky24</span><span>|</span><a href="#40390259">parent</a><span>|</span><a href="#40390274">next</a><span>|</span><label class="collapse" for="c-40390604">[-]</label><label class="expand" for="c-40390604">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Why the hell is LoRa learning&quot; was indeed my first thought...</div><br/><div id="40396791" class="c"><input type="checkbox" id="c-40396791" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#40390259">root</a><span>|</span><a href="#40390604">parent</a><span>|</span><a href="#40390274">next</a><span>|</span><label class="collapse" for="c-40396791">[-]</label><label class="expand" for="c-40396791">[1 more]</label></div><br/><div class="children"><div class="content">This is how the subs were knocked offline in Terminator III</div><br/></div></div></div></div></div></div><div id="40390274" class="c"><input type="checkbox" id="c-40390274" checked=""/><div class="controls bullet"><span class="by">gregmac</span><span>|</span><a href="#40390259">prev</a><span>|</span><a href="#40390436">next</a><span>|</span><label class="collapse" for="c-40390274">[-]</label><label class="expand" for="c-40390274">[2 more]</label></div><br/><div class="children"><div class="content">This is &quot;Low-Rank Adaptation&quot;, &quot;a widely-used parameter-efficient finetuning method for large language models.&quot;<p>Not to be confused with LoRa (&quot;long range&quot;) [1], an Internet of Things radio technology.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;LoRa" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;LoRa</a></div><br/><div id="40390329" class="c"><input type="checkbox" id="c-40390329" checked=""/><div class="controls bullet"><span class="by">chaos_emergent</span><span>|</span><a href="#40390274">parent</a><span>|</span><a href="#40390436">next</a><span>|</span><label class="collapse" for="c-40390329">[-]</label><label class="expand" for="c-40390329">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t this fairly obvious after a two second glance at the abstract</div><br/></div></div></div></div><div id="40390436" class="c"><input type="checkbox" id="c-40390436" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#40390274">prev</a><span>|</span><a href="#40392645">next</a><span>|</span><label class="collapse" for="c-40390436">[-]</label><label class="expand" for="c-40390436">[2 more]</label></div><br/><div class="children"><div class="content">This is Low-rank adaptation. Not to be confused with Lake of the Ozarks Recreation Area.</div><br/><div id="40392354" class="c"><input type="checkbox" id="c-40392354" checked=""/><div class="controls bullet"><span class="by">0cf8612b2e1e</span><span>|</span><a href="#40390436">parent</a><span>|</span><a href="#40392645">next</a><span>|</span><label class="collapse" for="c-40392354">[-]</label><label class="expand" for="c-40392354">[1 more]</label></div><br/><div class="children"><div class="content">Apparently constructed in 1929. You think those wireless people would have been more careful when they reappropriated the name.</div><br/></div></div></div></div><div id="40391500" class="c"><input type="checkbox" id="c-40391500" checked=""/><div class="controls bullet"><span class="by">ssl-3</span><span>|</span><a href="#40392645">prev</a><span>|</span><label class="collapse" for="c-40391500">[-]</label><label class="expand" for="c-40391500">[1 more]</label></div><br/><div class="children"><div class="content">What can we learn about Low Rank Acronyms today?</div><br/></div></div></div></div></div></div></div></body></html>