<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693904466104" as="style"/><link rel="stylesheet" href="styles.css?v=1693904466104"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://espadrine.github.io/blog/posts/chinchilla-s-death.html">Chinchilla’s death</a> <span class="domain">(<a href="https://espadrine.github.io">espadrine.github.io</a>)</span></div><div class="subtext"><span>KolmogorovComp</span> | <span>38 comments</span></div><br/><div><div id="37387171" class="c"><input type="checkbox" id="c-37387171" checked=""/><div class="controls bullet"><span class="by">Straw</span><span>|</span><a href="#37385804">next</a><span>|</span><label class="collapse" for="c-37387171">[-]</label><label class="expand" for="c-37387171">[4 more]</label></div><br/><div class="children"><div class="content">Chinchilla&#x27;s death has been greatly exaggerated. This article makes the same mistake as in the original GPT-3 scaling law of extrapolating from mid-training loss curves- but most of the loss improvement in the middle of training comes from simply dropping the learning rate to reduce the effective noise level from stochastic gradients.<p>If we want to judge the effectiveness of long-training small models, we need to look at _final_ loss as a function of compute, adjusting our LR schedule to token budget as we spend more compute, and then extrapolate on that curve- _not_ the training curve for a fixed budget. Another way to put it: you can&#x27;t drop LR below 0, and LR schedule drives the shape of the training curve, so it doesn&#x27;t make sense to extrapolate a curve beyond the end of training.<p>Of course, the overall point that long training produces gains holds true and Chinchilla says nothing about this- it only aims to minimize training compute.</div><br/><div id="37388457" class="c"><input type="checkbox" id="c-37388457" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#37387171">parent</a><span>|</span><a href="#37388462">next</a><span>|</span><label class="collapse" for="c-37388457">[-]</label><label class="expand" for="c-37388457">[2 more]</label></div><br/><div class="children"><div class="content">I find in the article the assumption almost insulting, that labs haven’t tried training smaller models for longer. Everybody tries that first, of course. It was the common wisdom for decades. No, it doesn’t work better or even as well. The loss curve flattens out.</div><br/><div id="37388776" class="c"><input type="checkbox" id="c-37388776" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#37387171">root</a><span>|</span><a href="#37388457">parent</a><span>|</span><a href="#37388462">next</a><span>|</span><label class="collapse" for="c-37388776">[-]</label><label class="expand" for="c-37388776">[1 more]</label></div><br/><div class="children"><div class="content">On the other hand models are trained once and later used a lot so you can argue that training cost can be traded for future gains.</div><br/></div></div></div></div><div id="37388462" class="c"><input type="checkbox" id="c-37388462" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#37387171">parent</a><span>|</span><a href="#37388457">prev</a><span>|</span><a href="#37385804">next</a><span>|</span><label class="collapse" for="c-37388462">[-]</label><label class="expand" for="c-37388462">[1 more]</label></div><br/><div class="children"><div class="content">The discussion at the end of this article starts to get to the problem with extrapolating.<p>Llama1-65b (roughly Chinchilla optimal) and Llama2-34b used similar compute and although Llama2 not directly comparable is the closest comparator without extrapolating and illustrates the point.</div><br/></div></div></div></div><div id="37385804" class="c"><input type="checkbox" id="c-37385804" checked=""/><div class="controls bullet"><span class="by">newfocogi</span><span>|</span><a href="#37387171">prev</a><span>|</span><a href="#37386126">next</a><span>|</span><label class="collapse" for="c-37385804">[-]</label><label class="expand" for="c-37385804">[10 more]</label></div><br/><div class="children"><div class="content">While the article makes good observations, this would appear to be a major oversight by leading research labs if they could have just kept the gas pedal down on simpler models for longer and gotten better performance. This is HackerNews – can we get someone from OpenAI, DeepMind, or MetaAI to respond and justify why cutting off the smaller models at a lower total compute budget is justified?</div><br/><div id="37385901" class="c"><input type="checkbox" id="c-37385901" checked=""/><div class="controls bullet"><span class="by">v64</span><span>|</span><a href="#37385804">parent</a><span>|</span><a href="#37386631">next</a><span>|</span><label class="collapse" for="c-37385901">[-]</label><label class="expand" for="c-37385901">[2 more]</label></div><br/><div class="children"><div class="content">The Llama 1 paper [1] was one of the earlier models to question the assumption that more params = better model. Since then they&#x27;ve released Llama 2 and this post is offering more evidence that reinforces their hypothesis.<p>I wouldn&#x27;t say it was an oversight by other labs that they missed this. It&#x27;s easier to just increase params on a model over the same training set instead of gathering a larger training set necessary for a smaller model. And at first, increasing model size did seem to be the way forward, but we&#x27;ve since hit diminishing returns. Now that we&#x27;ve hit that point, we&#x27;ve begun exploring other options and the Llamas are early evidence of another way forward.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.13971" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.13971</a></div><br/></div></div><div id="37386631" class="c"><input type="checkbox" id="c-37386631" checked=""/><div class="controls bullet"><span class="by">casualscience</span><span>|</span><a href="#37385804">parent</a><span>|</span><a href="#37385901">prev</a><span>|</span><a href="#37386663">next</a><span>|</span><label class="collapse" for="c-37386631">[-]</label><label class="expand" for="c-37386631">[1 more]</label></div><br/><div class="children"><div class="content">I work with LLMs, won&#x27;t say where, but smaller models stop performing better on benchmarks after a certain point, e.g. they seem to hit their learning capacity (at least with current techniques). Small models struggle to keep context the way larger models do, their outputs are impressive, but lack a certain amount of logical consistency and flow.<p>Whether this is a fundamental issue with the model size or some combination of training technique and model size is yet to be known. But for now, we know what works and are exploring that until we squeeze all the &#x27;easy&#x27; perf we can.</div><br/></div></div><div id="37386663" class="c"><input type="checkbox" id="c-37386663" checked=""/><div class="controls bullet"><span class="by">stephenroller</span><span>|</span><a href="#37385804">parent</a><span>|</span><a href="#37386631">prev</a><span>|</span><a href="#37387339">next</a><span>|</span><label class="collapse" for="c-37386663">[-]</label><label class="expand" for="c-37386663">[5 more]</label></div><br/><div class="children"><div class="content">One noteworthy thing is that no one is posting validation curves, only training curves. All these models will happily bring training loss eventually to near zero with infinite compute, as the model overfits to the dataset -- there are no regularizers in any modern LLMs. The validation curves would be considerably more convincing.<p>The counter argument to above is that none of these models were really trained for multiple-epochs: it&#x27;s hard to overfit data you&#x27;ve only seen once. But to go to 70T tokens, you&#x27;d inevitably have to start using many epochs.</div><br/><div id="37387689" class="c"><input type="checkbox" id="c-37387689" checked=""/><div class="controls bullet"><span class="by">Straw</span><span>|</span><a href="#37385804">root</a><span>|</span><a href="#37386663">parent</a><span>|</span><a href="#37386834">next</a><span>|</span><label class="collapse" for="c-37387689">[-]</label><label class="expand" for="c-37387689">[1 more]</label></div><br/><div class="children"><div class="content">The validation curves will look identical. These models are far too small to overfit to the training set.<p>With a large enough model and many epochs, you can certainly get overfitting, but for one epoch val&#x2F;train curves look exactly the same and I&#x27;d expect that a 7B model will never overfit on 2T tokens no matter how many epochs you do.</div><br/></div></div><div id="37386834" class="c"><input type="checkbox" id="c-37386834" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#37385804">root</a><span>|</span><a href="#37386663">parent</a><span>|</span><a href="#37387689">prev</a><span>|</span><a href="#37388213">next</a><span>|</span><label class="collapse" for="c-37386834">[-]</label><label class="expand" for="c-37386834">[2 more]</label></div><br/><div class="children"><div class="content">&gt; data you&#x27;ve only seen once<p>Is this still true given that they&#x27;re upsampling in the pretraining dataset? I don&#x27;t recall any details on how and to what extent they did this in the Llama2 paper but presumably some fraction of those 2T training tokens is repeated data.<p>MetaAI hasn&#x27;t been as averse to repeated tokens as other groups, they trained the now forgotten about Galactica for multiple epochs with good results.<p>&gt; The validation curves would be considerably more convincing.<p>What are they validating on? I was under the impression they weren&#x27;t splitting the pretraining corpus.</div><br/><div id="37387094" class="c"><input type="checkbox" id="c-37387094" checked=""/><div class="controls bullet"><span class="by">stephenroller</span><span>|</span><a href="#37385804">root</a><span>|</span><a href="#37386834">parent</a><span>|</span><a href="#37388213">next</a><span>|</span><label class="collapse" for="c-37387094">[-]</label><label class="expand" for="c-37387094">[1 more]</label></div><br/><div class="children"><div class="content">The llama1 team did not have a validation set. I don’t know what the Llama2 team did - I left before seeing any of the details.<p>My guess is Llama2 upsamples Wikipedia a good bit, but given they didn’t report <i>any</i> information about training data, it’s hard to say.</div><br/></div></div></div></div><div id="37388213" class="c"><input type="checkbox" id="c-37388213" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37385804">root</a><span>|</span><a href="#37386663">parent</a><span>|</span><a href="#37386834">prev</a><span>|</span><a href="#37387339">next</a><span>|</span><label class="collapse" for="c-37388213">[-]</label><label class="expand" for="c-37388213">[1 more]</label></div><br/><div class="children"><div class="content">&gt; there are no regularizers in any modern LLMs.<p>Using a large &amp; diverse training set is the best regulariser, but I think there is also weight decay and dropout in transformers</div><br/></div></div></div></div><div id="37387339" class="c"><input type="checkbox" id="c-37387339" checked=""/><div class="controls bullet"><span class="by">cornel_io</span><span>|</span><a href="#37385804">parent</a><span>|</span><a href="#37386663">prev</a><span>|</span><a href="#37386126">next</a><span>|</span><label class="collapse" for="c-37387339">[-]</label><label class="expand" for="c-37387339">[1 more]</label></div><br/><div class="children"><div class="content">But...they <i>did</i> that, with Llama 2, and apparently <i>did</i> get better results, at least up to a point.<p>My big WTF is, if you&#x27;re feeding the same amount of data through all of them, then to use the same amount of compute for the smaller models you need to run multiple epochs (with the same data). One thing that&#x27;s always bothered me a bit about &quot;foundation model&quot; LLM training is that it sounds like traditionally they essentially just run a single epoch, and with stochastic gradient descent that&#x27;s certainly leaving something on the table, probably a lot (and also introduces a lot of path-dependence on the order in which data is presented, what with the cosine learning rate rules).<p>I really want to know what would happen if, like in smaller models where we can actually get there (convnets for ImageNet classification, e.g.), we ran enough epochs on each of these models to hit the point where validation loss started increasing even as test loss decreased. It seems like we&#x27;re always squarely in the realm where they&#x27;re still both decreasing, so everything is <i>severely</i> undertrained, even given the available datasets. It&#x27;s easy to come up with &quot;laws&quot; for that regime, but they mean nothing other than that we don&#x27;t have enough compute to properly handle the data.<p>Big takeaway: if the results from this article are legit, it would suggest that we should really be looking at <i>even smaller</i> models, wouldn&#x27;t it? And actually be training them to the &quot;risk overtraining&quot; point?</div><br/></div></div></div></div><div id="37386126" class="c"><input type="checkbox" id="c-37386126" checked=""/><div class="controls bullet"><span class="by">ruuda</span><span>|</span><a href="#37385804">prev</a><span>|</span><a href="#37386004">next</a><span>|</span><label class="collapse" for="c-37386126">[-]</label><label class="expand" for="c-37386126">[4 more]</label></div><br/><div class="children"><div class="content">A related learning rate observation that is obvious in hindsight but not if you are just &quot;tweaking&quot; the learning rate: if you decay the learning rate exponentially, then you can only travel a bounded distance in parameter space, which may not be enough to reach a minimum. In practice that doesn&#x27;t seem to be a problem, but then again, a cosine learning rate schedule doesn&#x27;t look like a problem either.</div><br/><div id="37386146" class="c"><input type="checkbox" id="c-37386146" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#37386126">parent</a><span>|</span><a href="#37386464">next</a><span>|</span><label class="collapse" for="c-37386146">[-]</label><label class="expand" for="c-37386146">[1 more]</label></div><br/><div class="children"><div class="content">Adam&#x2F;AdamW doesn&#x27;t follow any intuitive logic. Pretty much everyone has different experiments with setting learning rates and schedules.</div><br/></div></div><div id="37386464" class="c"><input type="checkbox" id="c-37386464" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37386126">parent</a><span>|</span><a href="#37386146">prev</a><span>|</span><a href="#37386572">next</a><span>|</span><label class="collapse" for="c-37386464">[-]</label><label class="expand" for="c-37386464">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s fine - you can just use SGDR (SGD with restarts). <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1608.03983" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1608.03983</a></div><br/></div></div><div id="37386572" class="c"><input type="checkbox" id="c-37386572" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37386126">parent</a><span>|</span><a href="#37386464">prev</a><span>|</span><a href="#37386004">next</a><span>|</span><label class="collapse" for="c-37386572">[-]</label><label class="expand" for="c-37386572">[1 more]</label></div><br/><div class="children"><div class="content">These LLMs with cosine learning rate are usually decayed just to 1&#x2F;10th the peak LR. Even if you used exponential decay, your LR on your final step could still be arbitrarily large depending on how fast you configure it to decay.</div><br/></div></div></div></div><div id="37386004" class="c"><input type="checkbox" id="c-37386004" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#37386126">prev</a><span>|</span><a href="#37388236">next</a><span>|</span><label class="collapse" for="c-37386004">[-]</label><label class="expand" for="c-37386004">[8 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t we running out of text to train AI? You need much more data to train a smaller LLM to match a larger LLM performance. Considering how everybody is starting to clamp down on their user&#x27;s data, we might have issues getting enough to train something on the scale of PaLM to reach optimal performance.<p>On that note, everyone is probably salivating over Discord&#x27;s data. So so much chat based and detailed conversations, all neatly organized into groups...</div><br/><div id="37386057" class="c"><input type="checkbox" id="c-37386057" checked=""/><div class="controls bullet"><span class="by">leodriesch</span><span>|</span><a href="#37386004">parent</a><span>|</span><a href="#37386463">next</a><span>|</span><label class="collapse" for="c-37386057">[-]</label><label class="expand" for="c-37386057">[4 more]</label></div><br/><div class="children"><div class="content">I am wondering how much value social network data like Twitter or Discord really has when compared to something like Wikipedia or textbooks.<p>It is mostly unformatted, contains slang and is often far from the truth. Is this data really all that useful for training an LLM?</div><br/><div id="37386374" class="c"><input type="checkbox" id="c-37386374" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#37386004">root</a><span>|</span><a href="#37386057">parent</a><span>|</span><a href="#37386068">next</a><span>|</span><label class="collapse" for="c-37386374">[-]</label><label class="expand" for="c-37386374">[1 more]</label></div><br/><div class="children"><div class="content">Unformatted is irrelevant (or a benefit).<p>That is contains slang is a huge benefit - how else will it learn slang?<p>That is often far from the truth is of mixed value. Firstly it is very unclear that a LLM is the best method for holding facts. Secondly, the &quot;untrue&quot; data at least gives the LLM an idea of what could be true given a context, which lets it learn a better model of the world.<p>For example, if it gets a lot of text wrongly claiming that &quot;Winston Churchill is the 40th President of the United States&quot; it will add to the evidence that &quot;The President of the United States&quot; and &quot;Winston Churchill&quot; are both in the class of &quot;people&quot;.<p>This is opposed to nonsense text like &quot;A carpet States President United&quot; (which is just noise).</div><br/></div></div><div id="37386068" class="c"><input type="checkbox" id="c-37386068" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#37386004">root</a><span>|</span><a href="#37386057">parent</a><span>|</span><a href="#37386374">prev</a><span>|</span><a href="#37386463">next</a><span>|</span><label class="collapse" for="c-37386068">[-]</label><label class="expand" for="c-37386068">[2 more]</label></div><br/><div class="children"><div class="content">Social media has more idiosyncratic and human-esque text.<p>People already complain that ChatGPT is too formal and verbose, as an AI language model.</div><br/><div id="37386316" class="c"><input type="checkbox" id="c-37386316" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#37386004">root</a><span>|</span><a href="#37386068">parent</a><span>|</span><a href="#37386463">next</a><span>|</span><label class="collapse" for="c-37386316">[-]</label><label class="expand" for="c-37386316">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s probably more because of RLHF though, they&#x27;ve optimised for certain kind of responses rather than simple model loss on internet text.</div><br/></div></div></div></div></div></div><div id="37386463" class="c"><input type="checkbox" id="c-37386463" checked=""/><div class="controls bullet"><span class="by">anabis</span><span>|</span><a href="#37386004">parent</a><span>|</span><a href="#37386057">prev</a><span>|</span><a href="#37387242">next</a><span>|</span><label class="collapse" for="c-37386463">[-]</label><label class="expand" for="c-37386463">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s probably more training-compute intensive, but they can do drop-out, right?
The strategy they used for ImageNet recognition, when they were using supervised learning and training data was scarse.</div><br/><div id="37386514" class="c"><input type="checkbox" id="c-37386514" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#37386004">root</a><span>|</span><a href="#37386463">parent</a><span>|</span><a href="#37387242">next</a><span>|</span><label class="collapse" for="c-37386514">[-]</label><label class="expand" for="c-37386514">[1 more]</label></div><br/><div class="children"><div class="content">Dropout is one strategy for regularization but doesn&#x27;t guarantee avoiding overfitting, especially now that modern AI models generalize much better than they did during the ImageNet days. Many of the big LLMs use a dropout of 0.1 though.</div><br/></div></div></div></div><div id="37387242" class="c"><input type="checkbox" id="c-37387242" checked=""/><div class="controls bullet"><span class="by">Geee</span><span>|</span><a href="#37386004">parent</a><span>|</span><a href="#37386463">prev</a><span>|</span><a href="#37388236">next</a><span>|</span><label class="collapse" for="c-37387242">[-]</label><label class="expand" for="c-37387242">[1 more]</label></div><br/><div class="children"><div class="content">Using lots of low quality data is counterproductive. Unless you&#x27;re specifically trying to imitate flawed humans.</div><br/></div></div></div></div><div id="37388236" class="c"><input type="checkbox" id="c-37388236" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37386004">prev</a><span>|</span><a href="#37387175">next</a><span>|</span><label class="collapse" for="c-37388236">[-]</label><label class="expand" for="c-37388236">[2 more]</label></div><br/><div class="children"><div class="content">Chinchilla scaling was only good for academics - it prioritises model training cost at the expense of serving cost. If you plan to, you know, actually use the model, not just publish a paper on it, you need the LLaMA approach of training smaller models past the Chinchilla point.<p>But the smaller the model, the more compute is needed to cover the gap. To reach the loss of a 67B model<p>- A 33B model needs 2.3x compute<p>- A 13B model needs 25x compute<p>- A 7B model needs 7837x compute<p>- A 3B model can&#x27;t match the 67B. Ever.</div><br/><div id="37388498" class="c"><input type="checkbox" id="c-37388498" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#37388236">parent</a><span>|</span><a href="#37387175">next</a><span>|</span><label class="collapse" for="c-37388498">[-]</label><label class="expand" for="c-37388498">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Chinchilla scaling was only good for academics<p>I don&#x27;t know if it&#x27;s only good of academics, the point as the paper (as it says) is a scaling law for optimal loss given a fixed compute budget. By design it doesn&#x27;t address inference costs and isn&#x27;t a recipe for &quot;how you should train a LLM for your use case&quot;.<p>If you&#x27;re serving LLMs in a low throughput high cost scenario optimizing loss at the expense of inference cost may very well be your goal, or if you cant pay up front for 25x compute.</div><br/></div></div></div></div><div id="37387175" class="c"><input type="checkbox" id="c-37387175" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#37388236">prev</a><span>|</span><a href="#37386590">next</a><span>|</span><label class="collapse" for="c-37387175">[-]</label><label class="expand" for="c-37387175">[2 more]</label></div><br/><div class="children"><div class="content">Coincidentally, or not? A research team just started a 90 day effort to train a 1.1 billion parameter llama model on 3 trillion tokens.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;jzhang38&#x2F;TinyLlama">https:&#x2F;&#x2F;github.com&#x2F;jzhang38&#x2F;TinyLlama</a></div><br/><div id="37388198" class="c"><input type="checkbox" id="c-37388198" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#37387175">parent</a><span>|</span><a href="#37386590">next</a><span>|</span><label class="collapse" for="c-37388198">[-]</label><label class="expand" for="c-37388198">[1 more]</label></div><br/><div class="children"><div class="content">Not a coincidence, as your link was submitted 16 hours ago: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37379984">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37379984</a> and this submission is a top-level repost of a link posted in the comments there.</div><br/></div></div></div></div><div id="37386590" class="c"><input type="checkbox" id="c-37386590" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#37387175">prev</a><span>|</span><a href="#37385929">next</a><span>|</span><label class="collapse" for="c-37386590">[-]</label><label class="expand" for="c-37386590">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a strong opponent of just how <i>large</i> Large Language Models have become. Yeah, they can do some cool stuff, but I really, <i>really</i> think &gt;100B parameters is a rather silly number. In my opinion, the way you <i>organize and train</i> the parameters is far more important than the parameter <i>count</i>, which I think TFA supports pretty solidly. While I&#x27;ve never personally overseen the training of a model bigger than 2B params, I have trained a lot of smaller models and they consistently surprise me in how capable they are when trained in clever ways on adequately diverse datasets.<p>My wild, unfounded speculation? AGI in 24GB VRAM is feasible :P</div><br/></div></div><div id="37385929" class="c"><input type="checkbox" id="c-37385929" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#37386590">prev</a><span>|</span><a href="#37388089">next</a><span>|</span><label class="collapse" for="c-37385929">[-]</label><label class="expand" for="c-37385929">[3 more]</label></div><br/><div class="children"><div class="content">What is the likelihood of overfitting the smaller models?  It’s not obvious what the criteria and hyperparams are that prevent that.<p>If there’s no overfitting and the results get reproduced then this is a very promising find.</div><br/><div id="37386019" class="c"><input type="checkbox" id="c-37386019" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#37385929">parent</a><span>|</span><a href="#37386403">next</a><span>|</span><label class="collapse" for="c-37386019">[-]</label><label class="expand" for="c-37386019">[1 more]</label></div><br/><div class="children"><div class="content">To elaborate on my comment on the other thread, a workaround to overfitting is to train on so much distinct data that the model can&#x27;t overfit.
Newer large datasets optimize for diversity, as the AI industry is slowing coming to the realization that better data is much more important than large amount of bad data for training LLMs.<p>The writeup of SlimPajama, a heavily-deduped dataset, is a good starting point: <a href="https:&#x2F;&#x2F;www.cerebras.net&#x2F;blog&#x2F;slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.cerebras.net&#x2F;blog&#x2F;slimpajama-a-627b-token-cleane...</a></div><br/></div></div><div id="37386403" class="c"><input type="checkbox" id="c-37386403" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#37385929">parent</a><span>|</span><a href="#37386019">prev</a><span>|</span><a href="#37388089">next</a><span>|</span><label class="collapse" for="c-37386403">[-]</label><label class="expand" for="c-37386403">[1 more]</label></div><br/><div class="children"><div class="content">this is sort of the bigger picture observation I have about LLMs in general: that they seem to be massively over-parameterised, which in a classical statistical world would suggest they will readily over fit and it will be hard to get them to generalise beyond regurgitating their training material. Yet this is not the behaviour we see. So in some sense the &quot;magic&quot; we have seen flourish is that LLMs have managed to devise training mechanisms that do let them learn without over fitting in the context of a large excess of parameters. Part of it may be the sheer volume of training material but I suspect even with that you would get straight up regurgitation because it is so hard to stop the input data being degenerate. So other factors about how the training works must be contributing to this.<p>Of course, this is all my semi-layman speculation about this and I&#x27;m curious what actually knowledgeable people think.</div><br/></div></div></div></div><div id="37386241" class="c"><input type="checkbox" id="c-37386241" checked=""/><div class="controls bullet"><span class="by">bionhoward</span><span>|</span><a href="#37388089">prev</a><span>|</span><label class="collapse" for="c-37386241">[-]</label><label class="expand" for="c-37386241">[2 more]</label></div><br/><div class="children"><div class="content">Maybe it’s a tautochrone curve?</div><br/><div id="37388129" class="c"><input type="checkbox" id="c-37388129" checked=""/><div class="controls bullet"><span class="by">jksk61</span><span>|</span><a href="#37386241">parent</a><span>|</span><label class="collapse" for="c-37388129">[-]</label><label class="expand" for="c-37388129">[1 more]</label></div><br/><div class="children"><div class="content">maybe it is just a sierpinski triangle</div><br/></div></div></div></div></div></div></div></div></div></body></html>