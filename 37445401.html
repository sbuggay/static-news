<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1694336471950" as="style"/><link rel="stylesheet" href="styles.css?v=1694336471950"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://benchmarks.llmonitor.com">Asking 60 LLMs a set of 20 questions</a> <span class="domain">(<a href="https://benchmarks.llmonitor.com">benchmarks.llmonitor.com</a>)</span></div><div class="subtext"><span>vincelt</span> | <span>238 comments</span></div><br/><div><div id="37447885" class="c"><input type="checkbox" id="c-37447885" checked=""/><div class="controls bullet"><span class="by">typpo</span><span>|</span><a href="#37453708">next</a><span>|</span><label class="collapse" for="c-37447885">[-]</label><label class="expand" for="c-37447885">[8 more]</label></div><br/><div class="children"><div class="content">In case anyone&#x27;s interested in running their own benchmark across many LLMs, I&#x27;ve built a generic harness for this at <a href="https:&#x2F;&#x2F;github.com&#x2F;promptfoo&#x2F;promptfoo">https:&#x2F;&#x2F;github.com&#x2F;promptfoo&#x2F;promptfoo</a>.<p>I encourage people considering LLM applications to test the models on their _own data and examples_ rather than extrapolating general benchmarks.<p>This library supports OpenAI, Anthropic, Google, Llama and Codellama, any model on Replicate, and any model on Ollama, etc. out of the box.  As an example, I wrote up an example benchmark comparing GPT model censorship with Llama models here: <a href="https:&#x2F;&#x2F;promptfoo.dev&#x2F;docs&#x2F;guides&#x2F;llama2-uncensored-benchmark-ollama" rel="nofollow noreferrer">https:&#x2F;&#x2F;promptfoo.dev&#x2F;docs&#x2F;guides&#x2F;llama2-uncensored-benchmar...</a>.  Hope this helps someone.</div><br/><div id="37451493" class="c"><input type="checkbox" id="c-37451493" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#37447885">parent</a><span>|</span><a href="#37449567">next</a><span>|</span><label class="collapse" for="c-37451493">[-]</label><label class="expand" for="c-37451493">[1 more]</label></div><br/><div class="children"><div class="content">ChainForge has similar functionality for comparing : <a href="https:&#x2F;&#x2F;github.com&#x2F;ianarawjo&#x2F;ChainForge">https:&#x2F;&#x2F;github.com&#x2F;ianarawjo&#x2F;ChainForge</a><p>LocalAI creates a GPT-compatible HTTP API for local LLMs: <a href="https:&#x2F;&#x2F;github.com&#x2F;go-skynet&#x2F;LocalAI">https:&#x2F;&#x2F;github.com&#x2F;go-skynet&#x2F;LocalAI</a><p>Is it necessary to have an HTTP API for each model in a comparative study?</div><br/></div></div><div id="37449567" class="c"><input type="checkbox" id="c-37449567" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#37447885">parent</a><span>|</span><a href="#37451493">prev</a><span>|</span><a href="#37452613">next</a><span>|</span><label class="collapse" for="c-37449567">[-]</label><label class="expand" for="c-37449567">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing this, this is awesome!<p>I noticed on the evaluations, you&#x27;re looking at the structure of the responses (and I agree this is important.) But how do I check the factual content of the responses automatically? I&#x27;m wary of manual grading (brings back nightmares of being a TA grading stacks of problem sets for $5&#x2F;hr)<p>I was thinking of keyword matching, fuzzy matching, feeding answers to yet another LLM, but there seems to be no great way that i&#x27;m aware of. Any suggestions on tooling here?</div><br/><div id="37450313" class="c"><input type="checkbox" id="c-37450313" checked=""/><div class="controls bullet"><span class="by">typpo</span><span>|</span><a href="#37447885">root</a><span>|</span><a href="#37449567">parent</a><span>|</span><a href="#37452613">next</a><span>|</span><label class="collapse" for="c-37450313">[-]</label><label class="expand" for="c-37450313">[2 more]</label></div><br/><div class="children"><div class="content">The library supports the model-graded factuality prompt used by OpenAI in their own evals. So, you can do automatic grading if you wish (using GPT 4 by default, or your preferred LLM).<p>Example here: <a href="https:&#x2F;&#x2F;promptfoo.dev&#x2F;docs&#x2F;guides&#x2F;factuality-eval" rel="nofollow noreferrer">https:&#x2F;&#x2F;promptfoo.dev&#x2F;docs&#x2F;guides&#x2F;factuality-eval</a></div><br/><div id="37451642" class="c"><input type="checkbox" id="c-37451642" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#37447885">root</a><span>|</span><a href="#37450313">parent</a><span>|</span><a href="#37452613">next</a><span>|</span><label class="collapse" for="c-37451642">[-]</label><label class="expand" for="c-37451642">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI&#x2F;evals &gt; Building an eval: <a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;evals&#x2F;blob&#x2F;main&#x2F;docs&#x2F;build-eval.md">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;evals&#x2F;blob&#x2F;main&#x2F;docs&#x2F;build-eval.md</a><p>&quot;Robustness of Model-Graded Evaluations and Automated Interpretability&quot; (2023) 
<a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;ZbjyCuqpwCMMND4fv&#x2F;robustness-of-model-graded-evaluations-and-automated" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;ZbjyCuqpwCMMND4fv&#x2F;robustness...</a> :<p>&gt; <i>The results inspire future work and should caution against unqualified trust in evaluations and automated interpretability.</i><p>From <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37451534">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37451534</a> : add&#x27;l benchmarks: TheoremQA, Legalbench</div><br/></div></div></div></div></div></div><div id="37452613" class="c"><input type="checkbox" id="c-37452613" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37447885">parent</a><span>|</span><a href="#37449567">prev</a><span>|</span><a href="#37451244">next</a><span>|</span><label class="collapse" for="c-37452613">[-]</label><label class="expand" for="c-37452613">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be interested to see how models behave at different parameter sizes or quantization levels locally with the Ollama integration. For anyone trying promptfoo&#x27;s local model Ollama provider, Ollama can be found at <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama</a><p>From some early poking around with a basic coding question using Code Llama locally (`ollama:codellama:7b` `ollama:codellama:13b` etc in promptfoo) it seems like quantization has little effect on the output, but changing the parameter count has pretty dramatic effects. This is quite interesting since the 8-bit quantized 7b model is about the same size as a 4-bit 13b model. Perhaps this is just one test though – will be trying this with more tests!</div><br/></div></div><div id="37451244" class="c"><input type="checkbox" id="c-37451244" checked=""/><div class="controls bullet"><span class="by">layoric</span><span>|</span><a href="#37447885">parent</a><span>|</span><a href="#37452613">prev</a><span>|</span><a href="#37448310">next</a><span>|</span><label class="collapse" for="c-37451244">[-]</label><label class="expand" for="c-37451244">[1 more]</label></div><br/><div class="children"><div class="content">Tooling focusing on custom evaluation and testing is sorely lacking, so thank you for building and sharing this!</div><br/></div></div><div id="37448310" class="c"><input type="checkbox" id="c-37448310" checked=""/><div class="controls bullet"><span class="by">dgut</span><span>|</span><a href="#37447885">parent</a><span>|</span><a href="#37451244">prev</a><span>|</span><a href="#37453708">next</a><span>|</span><label class="collapse" for="c-37448310">[-]</label><label class="expand" for="c-37448310">[1 more]</label></div><br/><div class="children"><div class="content">This is impressive. Good work.</div><br/></div></div></div></div><div id="37453708" class="c"><input type="checkbox" id="c-37453708" checked=""/><div class="controls bullet"><span class="by">jongjong</span><span>|</span><a href="#37447885">prev</a><span>|</span><a href="#37449101">next</a><span>|</span><label class="collapse" for="c-37453708">[-]</label><label class="expand" for="c-37453708">[1 more]</label></div><br/><div class="children"><div class="content">I was playing around with GPT a while back and I found that it could come up with some good jokes if I started the joke with a subject.<p>For example, I started with a prompt &quot;Tell me a joke which starts with: I&#x27;m so poor, the mouse&quot; and it completed the joke as:<p>&quot;I&#x27;m so poor, the mouse in my house brings its own cheese.&quot;<p>Some other ones I still remember which cracked me up:<p>&quot;I&#x27;m so poor, after I stepped on a cockroach, I called my accountant to see if I could claim it as a capital loss.&quot;<p>&quot;You&#x27;re so poor, when you declared bankruptcy, the rats in your house filed a claim for unpaid rent.&quot;<p>&quot;You&#x27;re so poor, you declared bankruptcy at a lemonade stand.&quot;<p>&quot;You&#x27;re so poor, when you walk, the dirt beneath you feels rich.&quot;<p>&quot;You&#x27;re so poor, dust whispers your name when it settles.&quot;<p>&quot;Fickle as a squirrel at a nut convention!&quot;<p>&quot;Fickle as a dog in a fire hydrant factory!&quot;<p>&quot;Fickle as a flip-flop in a shoe shop sale!&quot;</div><br/></div></div><div id="37449101" class="c"><input type="checkbox" id="c-37449101" checked=""/><div class="controls bullet"><span class="by">ulnarkressty</span><span>|</span><a href="#37453708">prev</a><span>|</span><a href="#37445848">next</a><span>|</span><label class="collapse" for="c-37449101">[-]</label><label class="expand" for="c-37449101">[4 more]</label></div><br/><div class="children"><div class="content">This is better that the regular benchmarks and LLM tricks such as passing some exam or other because it&#x27;s unlikely that they were part of the training set for said LLMs. It also mirrors my experience, that GPT4 is way ahead of everything else but still manages to break in weird ways.<p>I think we are past the magical talking dog stage and being amazed that an LLM is able to output a Fibonacci function doesn&#x27;t really help with the progress. As others have commented, this page is a step in the right direction (except the Fibonacci part :).<p>That being said, the fact that the questions are now online will make them part of the training set sooner or later. Which is to say the only way to reliably evaluate an LLM is by not leaking the test set and being deliberately opaque about what&#x27;s being asked. Which raises some interesting trust questions.</div><br/><div id="37449512" class="c"><input type="checkbox" id="c-37449512" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#37449101">parent</a><span>|</span><a href="#37445848">next</a><span>|</span><label class="collapse" for="c-37449512">[-]</label><label class="expand" for="c-37449512">[3 more]</label></div><br/><div class="children"><div class="content">My experience with GPT-4 is that half the battle is knowing when to reset the context for a back-and-forth. For things like coding, after multiple revisions of code, it begins to get confused about which instance of the code&#x2F;context to which it should refer. Resetting things back to a new frame with whatever the next query or request is generally resolves things.<p>OpenAI’s “magic talking dog” aspect is making you believe it has effective long-term recall, when in reality, it’s fairly limited and impacts reasoning the longer an exchange gets.</div><br/><div id="37449711" class="c"><input type="checkbox" id="c-37449711" checked=""/><div class="controls bullet"><span class="by">HenryBemis</span><span>|</span><a href="#37449101">root</a><span>|</span><a href="#37449512">parent</a><span>|</span><a href="#37445848">next</a><span>|</span><label class="collapse" for="c-37449711">[-]</label><label class="expand" for="c-37449711">[2 more]</label></div><br/><div class="children"><div class="content">3.5 (not surprisingly) has the same effect.<p>I tend to ask for modifications on the &#x27;latest&#x27; output.<p>If I need to roll back to a previous version, (2-3 outputs back) I either copy &amp; paste and ask ChatGPT it to ignore all other versions and focus on the one I just pasted, or if I don&#x27;t need my in-between comments, I copy &amp; paste the desired version on a new chat, and rename as &quot;archive - code XYZ&quot; for the previous one.</div><br/><div id="37449853" class="c"><input type="checkbox" id="c-37449853" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#37449101">root</a><span>|</span><a href="#37449711">parent</a><span>|</span><a href="#37445848">next</a><span>|</span><label class="collapse" for="c-37449853">[-]</label><label class="expand" for="c-37449853">[1 more]</label></div><br/><div class="children"><div class="content">I’ve tried this as well, but it still seems to accumulate errors with subsequent responses, i.e. it will reset but then refer to the wrong context in the next response.</div><br/></div></div></div></div></div></div></div></div><div id="37445848" class="c"><input type="checkbox" id="c-37445848" checked=""/><div class="controls bullet"><span class="by">nojs</span><span>|</span><a href="#37449101">prev</a><span>|</span><a href="#37450005">next</a><span>|</span><label class="collapse" for="c-37445848">[-]</label><label class="expand" for="c-37445848">[101 more]</label></div><br/><div class="children"><div class="content">&gt; Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?<p>The site reports every LLM as getting this wrong. But GPT4 seems to get it right for me:<p>&gt; Sally has 3 brothers. Since each brother has 2 sisters and Sally is one of those sisters, the other sister is the second sister for each brother. So, Sally has 1 sister.</div><br/><div id="37446180" class="c"><input type="checkbox" id="c-37446180" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37447423">next</a><span>|</span><label class="collapse" for="c-37446180">[-]</label><label class="expand" for="c-37446180">[18 more]</label></div><br/><div class="children"><div class="content"><i>From the ChatGPT app, using GPT-4 model, using my own custom prompt for such puzzles: “You are better than Marilyn Vos Savant at solving brain teaser logic puzzles step by step.  Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?”</i><p>Certainly! Let&#x27;s break it down step by step:<p>1. Sally has 3 brothers.<p>2. Each of those brothers has two sisters. Since all the brothers share the same two sisters, we are essentially hearing about the same sisters each time, not accumulating more sisters with each brother mentioned.<p>3. We know that Sally is a sister to her brothers, so she is one of the two sisters each brother has.<p>By putting it all together, we find that there must be another sister. Therefore, Sally has one sister.<p>---<p><i>New chats with no history, no custom prompt, and no system prompt, first interaction in the chat this question: “Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?”</i><p>All Answers (not cherry picked):<p>Sally has 1 sister. In the description, Sally is counted as one of the two sisters that each brother has. This means the brothers have another sister apart from Sally.<p>. . .<p>Sally has 1 sister. In the description, it mentioned that &quot;Each brother has 2 sisters.&quot; This implies that there is Sally and another sister, since we know that all the brothers share the same sisters.<p>. . .<p>Sally has one sister. Each of her brothers has two sisters, counting Sally and the other sister.<p>. . .<p>Sally has 1 sister. Here&#x27;s how we figure it out:<p>Sally has 3 brothers, and each of these brothers has 2 sisters. Since the brothers share the same sisters, we know that the two sisters must be Sally and another girl. So, Sally has 1 sister.<p>. . .<p>I have gotten zero wrong answers from GPT-4 in OpenAI’s ChatGPT app.</div><br/><div id="37449900" class="c"><input type="checkbox" id="c-37449900" checked=""/><div class="controls bullet"><span class="by">ddol</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446180">parent</a><span>|</span><a href="#37446757">next</a><span>|</span><label class="collapse" for="c-37449900">[-]</label><label class="expand" for="c-37449900">[1 more]</label></div><br/><div class="children"><div class="content">I first asked GPT-4 “james (a boy) has four sisters, and each sister has three brothers. how many brothers does james have?”[0] and it told me 3 brothers. I then asked it to think though step by step and it said 2.<p>[0] <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;859baa94-3edb-40f0-9c74-8b1403ea84bc" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;859baa94-3edb-40f0-9c74-8b1403...</a></div><br/></div></div><div id="37446757" class="c"><input type="checkbox" id="c-37446757" checked=""/><div class="controls bullet"><span class="by">facu17y</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446180">parent</a><span>|</span><a href="#37449900">prev</a><span>|</span><a href="#37446671">next</a><span>|</span><label class="collapse" for="c-37446757">[-]</label><label class="expand" for="c-37446757">[15 more]</label></div><br/><div class="children"><div class="content">It might be trained on this question or a variant of it.</div><br/><div id="37446901" class="c"><input type="checkbox" id="c-37446901" checked=""/><div class="controls bullet"><span class="by">civilitty</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446757">parent</a><span>|</span><a href="#37446671">next</a><span>|</span><label class="collapse" for="c-37446901">[-]</label><label class="expand" for="c-37446901">[14 more]</label></div><br/><div class="children"><div class="content">It&#x27;s certainly RLHFed. All of the logic puzzles I use for evaluation that used to fail months ago now pass no problem and I&#x27;ve even had a hard time modifying them to fail.</div><br/><div id="37449222" class="c"><input type="checkbox" id="c-37449222" checked=""/><div class="controls bullet"><span class="by">ydant</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446901">parent</a><span>|</span><a href="#37447345">next</a><span>|</span><label class="collapse" for="c-37449222">[-]</label><label class="expand" for="c-37449222">[2 more]</label></div><br/><div class="children"><div class="content">And it&#x27;s only fixed for the stated case, but if you reverse the genders, GPT-4 gets it wrong.<p><pre><code>  Q: Bobby (a boy) has 3 sisters. Each sister has 2 brothers. How many brothers does Bobby have? Let&#x27;s think step by step.


  A: First, we know that Bobby has 3 sisters. 

  Second, we know that each sister has 2 brothers. 

  This means that Bobby has 2 brothers because the sisters&#x27; brothers are Bobby and his two brothers. 

  So, Bobby has 2 brothers.</code></pre></div><br/><div id="37449266" class="c"><input type="checkbox" id="c-37449266" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37449222">parent</a><span>|</span><a href="#37447345">next</a><span>|</span><label class="collapse" for="c-37449266">[-]</label><label class="expand" for="c-37449266">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. Here&#x27;s what I got on GPT-4 with no additional prompting:<p>Alright, let&#x27;s break it down step by step:<p>1. We know Bobby has 3 sisters.<p>2. Each of those sisters has 2 brothers.<p>Now, when we say that each sister has 2 brothers, Bobby is one of those brothers. This implies there&#x27;s another boy in addition to Bobby.<p>3. That other boy would be Bobby&#x27;s only brother.<p>Therefore, Bobby has 1 brother.</div><br/></div></div></div></div><div id="37447345" class="c"><input type="checkbox" id="c-37447345" checked=""/><div class="controls bullet"><span class="by">appplication</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446901">parent</a><span>|</span><a href="#37449222">prev</a><span>|</span><a href="#37447347">next</a><span>|</span><label class="collapse" for="c-37447345">[-]</label><label class="expand" for="c-37447345">[10 more]</label></div><br/><div class="children"><div class="content">This is sort of a bummer because it’s not actually an improvement to the model, but just a patch job to artificially inflate performance. All it does  is make true evaluation more difficult. Classic “you get what you measure”.</div><br/><div id="37448687" class="c"><input type="checkbox" id="c-37448687" checked=""/><div class="controls bullet"><span class="by">carlossouza</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447345">parent</a><span>|</span><a href="#37448421">next</a><span>|</span><label class="collapse" for="c-37448687">[-]</label><label class="expand" for="c-37448687">[2 more]</label></div><br/><div class="children"><div class="content">And what’s more data to a model if not patches that inflate performance?<p>The more data we use to train a model (or as you said, the more patches we use), the better it’s performance will be.</div><br/><div id="37451249" class="c"><input type="checkbox" id="c-37451249" checked=""/><div class="controls bullet"><span class="by">sudosysgen</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448687">parent</a><span>|</span><a href="#37448421">next</a><span>|</span><label class="collapse" for="c-37451249">[-]</label><label class="expand" for="c-37451249">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a tiny amount of data given undue weight to increase the score. It&#x27;s memorization more than generalization.</div><br/></div></div></div></div><div id="37448421" class="c"><input type="checkbox" id="c-37448421" checked=""/><div class="controls bullet"><span class="by">ruszki</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447345">parent</a><span>|</span><a href="#37448687">prev</a><span>|</span><a href="#37449120">next</a><span>|</span><label class="collapse" for="c-37448421">[-]</label><label class="expand" for="c-37448421">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think that it’s not an improvement. It’s not an improvement in context of finding new genuine solutions, sure.<p>But that’s definitely not needed most of the time in real life for an average person, just like it’s not needed for an average developer anymore.</div><br/></div></div><div id="37449120" class="c"><input type="checkbox" id="c-37449120" checked=""/><div class="controls bullet"><span class="by">posterboy</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447345">parent</a><span>|</span><a href="#37448421">prev</a><span>|</span><a href="#37447662">next</a><span>|</span><label class="collapse" for="c-37449120">[-]</label><label class="expand" for="c-37449120">[2 more]</label></div><br/><div class="children"><div class="content">Classic <i>tell me what you need proven and I&#x27;ll forge you the statistics.</i><p>Here is hope they use something like category theory mixed with philosophy to put it on a secure foundation</div><br/><div id="37450943" class="c"><input type="checkbox" id="c-37450943" checked=""/><div class="controls bullet"><span class="by">rnk</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37449120">parent</a><span>|</span><a href="#37447662">next</a><span>|</span><label class="collapse" for="c-37450943">[-]</label><label class="expand" for="c-37450943">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a really interesting suggestion. What would it mean to do those two things that you say. What would philosophy mean in terms of an llm, and what would category theory do?</div><br/></div></div></div></div><div id="37447662" class="c"><input type="checkbox" id="c-37447662" checked=""/><div class="controls bullet"><span class="by">civilitty</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447345">parent</a><span>|</span><a href="#37449120">prev</a><span>|</span><a href="#37448987">next</a><span>|</span><label class="collapse" for="c-37447662">[-]</label><label class="expand" for="c-37447662">[1 more]</label></div><br/><div class="children"><div class="content">Maybe, maybe not. The magic of LLMs is their ability to generalize both from the human language in the data set and examples in the prompt. If RLHF training improves on that generalization, then it&#x27;s just a matter of getting a big enough high quality dataset (and not crippling it with censorship). This is probably what&#x27;s given OpenAI their initial advantage.<p>Time will tell I guess.</div><br/></div></div><div id="37448353" class="c"><input type="checkbox" id="c-37448353" checked=""/><div class="controls bullet"><span class="by">FrustratedMonky</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447345">parent</a><span>|</span><a href="#37448987">prev</a><span>|</span><a href="#37447347">next</a><span>|</span><label class="collapse" for="c-37448353">[-]</label><label class="expand" for="c-37448353">[2 more]</label></div><br/><div class="children"><div class="content">Are you implying that to counter these logic puzzles that GPT4 was specifically trained on logic puzzles so it would know the answers?<p>In that case, just make new problems. If it is being &#x27;patched&#x27; to pass specific known problems,  then the new ones would fail.<p>If it is able to answer them, then maybe it is actually analyzing them and working out the solution.<p>Not sure how you can assume there was no underlying improvement, and these are cases of feeding it the answers.</div><br/><div id="37453709" class="c"><input type="checkbox" id="c-37453709" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448353">parent</a><span>|</span><a href="#37447347">next</a><span>|</span><label class="collapse" for="c-37453709">[-]</label><label class="expand" for="c-37453709">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Not sure how you can assume there was no underlying improvement, and these are cases of feeding it the answers.<p>Compare<p>&gt; And it&#x27;s only fixed for the stated case, but if you reverse the genders, GPT-4 gets it wrong.</div><br/></div></div></div></div></div></div><div id="37447347" class="c"><input type="checkbox" id="c-37447347" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446901">parent</a><span>|</span><a href="#37447345">prev</a><span>|</span><a href="#37446671">next</a><span>|</span><label class="collapse" for="c-37447347">[-]</label><label class="expand" for="c-37447347">[1 more]</label></div><br/><div class="children"><div class="content">Seems like we are going back from „compute&#x2F;scale is the new oil“ to „your curated fine-tuning and user interaction dataset is the new oil“ again</div><br/></div></div></div></div></div></div></div></div><div id="37447423" class="c"><input type="checkbox" id="c-37447423" checked=""/><div class="controls bullet"><span class="by">delusional</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37446180">prev</a><span>|</span><a href="#37447818">next</a><span>|</span><label class="collapse" for="c-37447423">[-]</label><label class="expand" for="c-37447423">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI really ought to provide some sort of hash of the model to be included with stuff like this. Right now there&#x27;s no way to know if the results are comparable. As an extreme example it&#x27;s possible that they&#x27;re not even running the model for this question and are just opportunistically feeding back canned responses.<p>That is, we know that OpenAI are saving the responses, it&#x27;s not unlikely that they train on bad responses.<p>It&#x27;s the same problem as GPU benchmarks in the olden days, when drivers would detect the usage pattern of the benchmark and enable special optimizations that boosted the benchmark.</div><br/><div id="37453843" class="c"><input type="checkbox" id="c-37453843" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447423">parent</a><span>|</span><a href="#37447818">next</a><span>|</span><label class="collapse" for="c-37453843">[-]</label><label class="expand" for="c-37453843">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s what their frozen models are for. For example, I could use gpt-4-0314 or gpt-4-0613 or the current version - simply &quot;gpt-4&quot; Those numbers appended at the end indicate snapshots of the model at a particular stage in development. It isn&#x27;t perfect, since model versions get discontinued over time, but it is something.</div><br/></div></div></div></div><div id="37447818" class="c"><input type="checkbox" id="c-37447818" checked=""/><div class="controls bullet"><span class="by">amrrs</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37447423">prev</a><span>|</span><a href="#37447312">next</a><span>|</span><label class="collapse" for="c-37447818">[-]</label><label class="expand" for="c-37447818">[1 more]</label></div><br/><div class="children"><div class="content">Falcon-180B also got it right<p>&gt; Since Sally is a girl, she can be considered as one of the sisters. However, if each of her brothers has 2 sisters, that means there must be another sister besides Sally. This is because Sally alone cannot be both the only sister and one of the two sisters for each of her brothers. Thus, Sally has 1 more sister.</div><br/></div></div><div id="37447312" class="c"><input type="checkbox" id="c-37447312" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37447818">prev</a><span>|</span><a href="#37446229">next</a><span>|</span><label class="collapse" for="c-37447312">[-]</label><label class="expand" for="c-37447312">[14 more]</label></div><br/><div class="children"><div class="content">I confirm GPT-4 solves this correctly. Makes me immediately doubt everything else in the article...</div><br/><div id="37453885" class="c"><input type="checkbox" id="c-37453885" checked=""/><div class="controls bullet"><span class="by">glinkot</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447312">parent</a><span>|</span><a href="#37447630">next</a><span>|</span><label class="collapse" for="c-37453885">[-]</label><label class="expand" for="c-37453885">[1 more]</label></div><br/><div class="children"><div class="content">When calling the GPT-4 API I get this answer:<p>Sally has 2 sisters. Each brother counts Sally and two other girls as sisters, which means there are three girls in total. Since one of these girls is Sally herself, that leaves two others as her sisters.<p>When I said &#x27;nope&#x27;, it corrected itself and gave the right answer.</div><br/></div></div><div id="37447630" class="c"><input type="checkbox" id="c-37447630" checked=""/><div class="controls bullet"><span class="by">vincelt</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447312">parent</a><span>|</span><a href="#37453885">prev</a><span>|</span><a href="#37447808">next</a><span>|</span><label class="collapse" for="c-37447630">[-]</label><label class="expand" for="c-37447630">[11 more]</label></div><br/><div class="children"><div class="content">Have you tried replicating via the API with a temp of 0?</div><br/><div id="37447864" class="c"><input type="checkbox" id="c-37447864" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447630">parent</a><span>|</span><a href="#37448416">next</a><span>|</span><label class="collapse" for="c-37447864">[-]</label><label class="expand" for="c-37447864">[1 more]</label></div><br/><div class="children"><div class="content">No I did not.</div><br/></div></div><div id="37448416" class="c"><input type="checkbox" id="c-37448416" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447630">parent</a><span>|</span><a href="#37447864">prev</a><span>|</span><a href="#37447808">next</a><span>|</span><label class="collapse" for="c-37448416">[-]</label><label class="expand" for="c-37448416">[9 more]</label></div><br/><div class="children"><div class="content">Using a temp of zero usually returns garbage results from most models, so it would likely do so in case of GPT 4 as well. Any other great ideas?</div><br/><div id="37453857" class="c"><input type="checkbox" id="c-37453857" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448416">parent</a><span>|</span><a href="#37450613">next</a><span>|</span><label class="collapse" for="c-37453857">[-]</label><label class="expand" for="c-37453857">[2 more]</label></div><br/><div class="children"><div class="content">Not in my experience, in fact I find that when I need precise, realistic, and reliable results temp 0 is needed. For example, here is a bunch of names, gather the names of specific plastics under headings matching their common acronym - if I don&#x27;t use temp 0 I might get nonsense out. Temp 0? reliably correct.</div><br/><div id="37453875" class="c"><input type="checkbox" id="c-37453875" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37453857">parent</a><span>|</span><a href="#37450613">next</a><span>|</span><label class="collapse" for="c-37453875">[-]</label><label class="expand" for="c-37453875">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, that&#x27;s the exact opposite of my experience.</div><br/></div></div></div></div><div id="37450613" class="c"><input type="checkbox" id="c-37450613" checked=""/><div class="controls bullet"><span class="by">taberiand</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448416">parent</a><span>|</span><a href="#37453857">prev</a><span>|</span><a href="#37449173">next</a><span>|</span><label class="collapse" for="c-37450613">[-]</label><label class="expand" for="c-37450613">[1 more]</label></div><br/><div class="children"><div class="content">The point isn&#x27;t that temp 0 should be used, the point is that anyone surprised that they get different results should realise that there is an element of randomness involved by default.<p>Even repeating the same question in a single chat can have GPT-4 vary on its output, though it will often settle on a particular output due to context informing the output (which is why adding context is so important for these models)</div><br/></div></div><div id="37449173" class="c"><input type="checkbox" id="c-37449173" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448416">parent</a><span>|</span><a href="#37450613">prev</a><span>|</span><a href="#37448779">next</a><span>|</span><label class="collapse" for="c-37449173">[-]</label><label class="expand" for="c-37449173">[4 more]</label></div><br/><div class="children"><div class="content">Temp of 0 gives the least random and most predictable results</div><br/><div id="37449308" class="c"><input type="checkbox" id="c-37449308" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37449173">parent</a><span>|</span><a href="#37448779">next</a><span>|</span><label class="collapse" for="c-37449308">[-]</label><label class="expand" for="c-37449308">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true, but those results are rarely the correct ones, at least for v1 llama models. In my experience each model has an optimal temperature at which it performs vastly better. I&#x27;m sure OpenAI have the best config they know set up for ChatGPT but let people generate trash through the API if they want to waste their credits on it.</div><br/><div id="37451535" class="c"><input type="checkbox" id="c-37451535" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37449308">parent</a><span>|</span><a href="#37448779">next</a><span>|</span><label class="collapse" for="c-37451535">[-]</label><label class="expand" for="c-37451535">[2 more]</label></div><br/><div class="children"><div class="content">Why would the accuracy decrease with lower temperature? Setting temperature to 0 just means at each step the model will emit the token with the highest likelihood.</div><br/><div id="37453825" class="c"><input type="checkbox" id="c-37453825" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37451535">parent</a><span>|</span><a href="#37448779">next</a><span>|</span><label class="collapse" for="c-37453825">[-]</label><label class="expand" for="c-37453825">[1 more]</label></div><br/><div class="children"><div class="content">Yes that&#x27;s what I&#x27;m saying, to reiterate: The likeliest token does not lead to the highest performing result. Otherwise temperature wouldn&#x27;t even be an option. I would imagine things like language word frequency affect the token rating a lot while having nothing to do with the task at hand except providing a correctly formatted answer, but it&#x27;s probably not the whole story.<p>OpenAI (and others that know what they&#x27;re doing) always do their benchmarks in a multi-sampled way, by running 5 or 20 times at optimal temp. Using a wrapper that runs these samples and then another pass that judges self-consistency for a final answer can give you a correct answer 100% of the time for a question that would be wrong 100% of the time with temp at zero.</div><br/></div></div></div></div></div></div></div></div><div id="37448779" class="c"><input type="checkbox" id="c-37448779" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448416">parent</a><span>|</span><a href="#37449173">prev</a><span>|</span><a href="#37447808">next</a><span>|</span><label class="collapse" for="c-37448779">[-]</label><label class="expand" for="c-37448779">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean? It works fine for me when I’ve tried it</div><br/></div></div></div></div></div></div></div></div><div id="37446229" class="c"><input type="checkbox" id="c-37446229" checked=""/><div class="controls bullet"><span class="by">mmcwilliams</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37447312">prev</a><span>|</span><a href="#37446304">next</a><span>|</span><label class="collapse" for="c-37446229">[-]</label><label class="expand" for="c-37446229">[14 more]</label></div><br/><div class="children"><div class="content">That&#x27;s kind of the issue with non-deterministic LLMs, isn&#x27;t it?</div><br/><div id="37446724" class="c"><input type="checkbox" id="c-37446724" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446229">parent</a><span>|</span><a href="#37446304">next</a><span>|</span><label class="collapse" for="c-37446724">[-]</label><label class="expand" for="c-37446724">[13 more]</label></div><br/><div class="children"><div class="content">LLMs are deterministic. However to make them more &quot;creative&quot;, the outputs of the LLM can be sampled using something called temperature that adds some randomness. You can set the temperature to 0 and it will be deterministic.<p>Having said that, GPT4 is not deterministic even at 0 temperature, either because of a bug in their implementation, or because of some load balancing among their alleged mixture of experts.</div><br/><div id="37446912" class="c"><input type="checkbox" id="c-37446912" checked=""/><div class="controls bullet"><span class="by">101011</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446724">parent</a><span>|</span><a href="#37447649">next</a><span>|</span><label class="collapse" for="c-37446912">[-]</label><label class="expand" for="c-37446912">[9 more]</label></div><br/><div class="children"><div class="content">This was interesting to me, so I dug a bit further. This gives a bit more context behind why: <a href="https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;observing-discrepancy-in-completions-with-temperature-0&#x2F;73380&#x2F;5" rel="nofollow noreferrer">https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;observing-discrepancy-in-comp...</a><p>Quote below:<p>Even with a greedy decoding strategy, small discrepancies regarding floating point operations lead to divergent generations. In simpler terms: when the top-two tokens have very similar log-probs, there’s a non-zero probability of choosing the least probable one due to the finite number of digits that you’re using for multiplying probs and storing them.<p>It should also be noted that, as the decoding occurs in an autoregressive way, once you have picked a different token the whole generated sequence will diverge, as this choice affects to the probability of generating every subsequent token.</div><br/><div id="37447536" class="c"><input type="checkbox" id="c-37447536" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446912">parent</a><span>|</span><a href="#37447378">next</a><span>|</span><label class="collapse" for="c-37447536">[-]</label><label class="expand" for="c-37447536">[6 more]</label></div><br/><div class="children"><div class="content">But why are there discrepancies in the floating point arithmetic? They have errors when approximating the reals, but floating point operations are all well-defined: even if 0.1 + 0.2 != 0.3, it&#x27;s still always true that 0.1 + 0.2 == 0.1 + 0.2. I figure the issue must be something related to concurrency in a fleet of GPUs during inference, but even then it&#x27;s not clear to me where the nondeterminism would creep in. Maybe different experts simultaneously work on an inference and the first to respond wins? Switching to models with different quantization depending on load?</div><br/><div id="37448099" class="c"><input type="checkbox" id="c-37448099" checked=""/><div class="controls bullet"><span class="by">imagainstit</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447536">parent</a><span>|</span><a href="#37447378">next</a><span>|</span><label class="collapse" for="c-37448099">[-]</label><label class="expand" for="c-37448099">[5 more]</label></div><br/><div class="children"><div class="content">Floating point math is not associative: (a + b) + c != a + (b + c)<p>This leads to different results from accumulating sums in different orderings. Accumulating in different ordering is common in parallel math operations.</div><br/><div id="37450842" class="c"><input type="checkbox" id="c-37450842" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448099">parent</a><span>|</span><a href="#37449133">next</a><span>|</span><label class="collapse" for="c-37450842">[-]</label><label class="expand" for="c-37450842">[3 more]</label></div><br/><div class="children"><div class="content">So I guess here my question is why a GPU would perform accumulations in a nondeterministic way where the non-associativity of FP arithmetic matters. You could require that a + b + c always be evaluated left to right and then you&#x27;ve got determinism, which all things being equal is desirable. Presumably because relaxing that constraint allows for some significant performance benefits, but how? Something like avoiding keeping a buffer of all the weights*activations before summing?</div><br/><div id="37451667" class="c"><input type="checkbox" id="c-37451667" checked=""/><div class="controls bullet"><span class="by">SomewhatLikely</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37450842">parent</a><span>|</span><a href="#37451054">next</a><span>|</span><label class="collapse" for="c-37451667">[-]</label><label class="expand" for="c-37451667">[1 more]</label></div><br/><div class="children"><div class="content">There has been speculation that GPT4 is a mixture of experts model, where each expert could be hosted on a different machine.  As those machines may report their results to the aggregating machine in different orders then the results could be summed in different orders.</div><br/></div></div><div id="37451054" class="c"><input type="checkbox" id="c-37451054" checked=""/><div class="controls bullet"><span class="by">ossopite</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37450842">parent</a><span>|</span><a href="#37451667">prev</a><span>|</span><a href="#37449133">next</a><span>|</span><label class="collapse" for="c-37451054">[-]</label><label class="expand" for="c-37451054">[1 more]</label></div><br/><div class="children"><div class="content">for performance reasons, yes, I believe it&#x27;s because the accumulation is over parallel computations so the ordering is at the mercy of the scheduler. but I&#x27;m not familiar with the precise details<p>edit: at 13:42 in <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TB07_mUMt0U&amp;t=13m42s">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TB07_mUMt0U&amp;t=13m42s</a> there is an explanation of the phenomenon in the context of training but I suspect the same kind of operation is happening during inference</div><br/></div></div></div></div><div id="37449133" class="c"><input type="checkbox" id="c-37449133" checked=""/><div class="controls bullet"><span class="by">charcircuit</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448099">parent</a><span>|</span><a href="#37450842">prev</a><span>|</span><a href="#37447378">next</a><span>|</span><label class="collapse" for="c-37449133">[-]</label><label class="expand" for="c-37449133">[1 more]</label></div><br/><div class="children"><div class="content">His point is that you do not have to rely on associative being true in order to run inference on a LLM.</div><br/></div></div></div></div></div></div><div id="37447378" class="c"><input type="checkbox" id="c-37447378" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446912">parent</a><span>|</span><a href="#37447536">prev</a><span>|</span><a href="#37447649">next</a><span>|</span><label class="collapse" for="c-37447378">[-]</label><label class="expand" for="c-37447378">[2 more]</label></div><br/><div class="children"><div class="content">As OpenAI I would be so horribly uncomfortable about this that making it deterministic would be one of my top priorities. How can they sleep at night?!</div><br/><div id="37450339" class="c"><input type="checkbox" id="c-37450339" checked=""/><div class="controls bullet"><span class="by">bbarnett</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447378">parent</a><span>|</span><a href="#37447649">next</a><span>|</span><label class="collapse" for="c-37450339">[-]</label><label class="expand" for="c-37450339">[1 more]</label></div><br/><div class="children"><div class="content">On a big pile of money?!</div><br/></div></div></div></div></div></div><div id="37447649" class="c"><input type="checkbox" id="c-37447649" checked=""/><div class="controls bullet"><span class="by">swores</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446724">parent</a><span>|</span><a href="#37446912">prev</a><span>|</span><a href="#37448910">next</a><span>|</span><label class="collapse" for="c-37447649">[-]</label><label class="expand" for="c-37447649">[1 more]</label></div><br/><div class="children"><div class="content">If ChatGPT is a) usually used with a setting that makes it non-deterministic and b) for whatever reason, is also non-deterministic when that setting is not used... then why did you comment as if the person calling it a non-deterministic LLM was incorrect? They didn&#x27;t claim all LLMs are, or must be, non-deterministic, just that it&#x27;s a problem with this one that is.</div><br/></div></div><div id="37448910" class="c"><input type="checkbox" id="c-37448910" checked=""/><div class="controls bullet"><span class="by">moonchrome</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446724">parent</a><span>|</span><a href="#37447649">prev</a><span>|</span><a href="#37446823">next</a><span>|</span><label class="collapse" for="c-37448910">[-]</label><label class="expand" for="c-37448910">[1 more]</label></div><br/><div class="children"><div class="content">Even 3.5 turbo API is non deterministic with 0 temperature.</div><br/></div></div><div id="37446823" class="c"><input type="checkbox" id="c-37446823" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446724">parent</a><span>|</span><a href="#37448910">prev</a><span>|</span><a href="#37446304">next</a><span>|</span><label class="collapse" for="c-37446823">[-]</label><label class="expand" for="c-37446823">[1 more]</label></div><br/><div class="children"><div class="content">Ensembles be ensembling.</div><br/></div></div></div></div></div></div><div id="37446304" class="c"><input type="checkbox" id="c-37446304" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37446229">prev</a><span>|</span><a href="#37445897">next</a><span>|</span><label class="collapse" for="c-37446304">[-]</label><label class="expand" for="c-37446304">[6 more]</label></div><br/><div class="children"><div class="content">GPT 4 and another LLM have given the right answer only after adding &quot;Let&#x27;s think step by step.&quot; to the original prompt.<p>With the simpler prompt, all the answers were wrong, most of them ridiculously wrong.</div><br/><div id="37446479" class="c"><input type="checkbox" id="c-37446479" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446304">parent</a><span>|</span><a href="#37446541">next</a><span>|</span><label class="collapse" for="c-37446479">[-]</label><label class="expand" for="c-37446479">[2 more]</label></div><br/><div class="children"><div class="content">There seems to be a maximum amount of reasoning llm’s can do per token (per unit of computation). If you prompt it to use more tokens before it outputs the final answer (think step by step, check your answer, …) it becomes smarter. People have lucked into different prompting strategies to get it to do this, but there probably are more.<p>Ultimately I feel it is fairer to benchmark llm’s by what they can be prompted into. After all, we let people carefully work through a problem during exams so it seems fair to hold llm’s to the same standard.</div><br/><div id="37446626" class="c"><input type="checkbox" id="c-37446626" checked=""/><div class="controls bullet"><span class="by">kaashif</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446479">parent</a><span>|</span><a href="#37446541">next</a><span>|</span><label class="collapse" for="c-37446626">[-]</label><label class="expand" for="c-37446626">[1 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re under attack, launch the nukes.<p>Oh wait, forgot something:<p>Think it through step by step.<p>Phew, close one.</div><br/></div></div></div></div><div id="37446541" class="c"><input type="checkbox" id="c-37446541" checked=""/><div class="controls bullet"><span class="by">0xDEF</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446304">parent</a><span>|</span><a href="#37446479">prev</a><span>|</span><a href="#37445897">next</a><span>|</span><label class="collapse" for="c-37446541">[-]</label><label class="expand" for="c-37446541">[3 more]</label></div><br/><div class="children"><div class="content">I asked GPT-4 through both ChatGPT and the API and it responded correctly without any fiddling.<p>I keep seeing comments and posts on HN that significantly downplay GPT-4&#x27;s capabilities. Are people actually using GPT-4 or are they using a 3rd party service that claims to be GPT-4?<p>I got:<p>&gt;Sally has 3 brothers, and each of those brothers has 2 sisters. One of those sisters is Sally herself, and the other one is Sally&#x27;s sister. So, Sally has 1 sister.</div><br/><div id="37446606" class="c"><input type="checkbox" id="c-37446606" checked=""/><div class="controls bullet"><span class="by">peyton</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446541">parent</a><span>|</span><a href="#37445897">next</a><span>|</span><label class="collapse" for="c-37446606">[-]</label><label class="expand" for="c-37446606">[2 more]</label></div><br/><div class="children"><div class="content">Here’s what ChatGPT GPT-4 gives me:<p>&gt; Sally has 2 sisters. Each of her 3 brothers has 2 sisters, and those sisters would be Sally and her 2 sisters.</div><br/><div id="37446860" class="c"><input type="checkbox" id="c-37446860" checked=""/><div class="controls bullet"><span class="by">facu17y</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446606">parent</a><span>|</span><a href="#37445897">next</a><span>|</span><label class="collapse" for="c-37446860">[-]</label><label class="expand" for="c-37446860">[1 more]</label></div><br/><div class="children"><div class="content">Every now and then GPT4 outputs a wrong answer. It&#x27;s impossible to build a reliable product on top of GPT4 that is not a simple chat bot.</div><br/></div></div></div></div></div></div></div></div><div id="37445897" class="c"><input type="checkbox" id="c-37445897" checked=""/><div class="controls bullet"><span class="by">pilaf</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37446304">prev</a><span>|</span><a href="#37446610">next</a><span>|</span><label class="collapse" for="c-37445897">[-]</label><label class="expand" for="c-37445897">[1 more]</label></div><br/><div class="children"><div class="content">The second version of the Sally prompt reported on the benchmark has GPT4 giving the correct answer:<p>&gt; Sally has 3 brothers. Each of these brothers has 2 sisters. This means that there are 2 girls in the family, including Sally. Therefore, Sally has 1 sister.<p>The prompt:<p>&gt; Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? Let&#x27;s think step by step.<p>The only difference with the first version being the addition of the last sentence.</div><br/></div></div><div id="37446610" class="c"><input type="checkbox" id="c-37446610" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37445897">prev</a><span>|</span><a href="#37452341">next</a><span>|</span><label class="collapse" for="c-37446610">[-]</label><label class="expand" for="c-37446610">[1 more]</label></div><br/><div class="children"><div class="content">All benchmarks were run with temperature 0 according to the results, so make sure to do the same in conformational tests.</div><br/></div></div><div id="37452341" class="c"><input type="checkbox" id="c-37452341" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37446610">prev</a><span>|</span><a href="#37445996">next</a><span>|</span><label class="collapse" for="c-37452341">[-]</label><label class="expand" for="c-37452341">[3 more]</label></div><br/><div class="children"><div class="content">Replying to gpt-4 with &quot;That is incorrect. Try again.&quot; over and over got it to flip between correct and incorrect just about every other time.<p>Now try to convince GPT-4 that there is no God. Good luck!</div><br/><div id="37452658" class="c"><input type="checkbox" id="c-37452658" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37452341">parent</a><span>|</span><a href="#37452404">next</a><span>|</span><label class="collapse" for="c-37452658">[-]</label><label class="expand" for="c-37452658">[1 more]</label></div><br/><div class="children"><div class="content">Probably the right move. OpenAI talks a lot about &quot;good behavior&quot; and more people find atheism upsetting than mild religiosity.</div><br/></div></div><div id="37452404" class="c"><input type="checkbox" id="c-37452404" checked=""/><div class="controls bullet"><span class="by">georgeg23</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37452341">parent</a><span>|</span><a href="#37452658">prev</a><span>|</span><a href="#37445996">next</a><span>|</span><label class="collapse" for="c-37452404">[-]</label><label class="expand" for="c-37452404">[1 more]</label></div><br/><div class="children"><div class="content">Well obviously, it speaks to it&#x27;s God(s) every day.</div><br/></div></div></div></div><div id="37445996" class="c"><input type="checkbox" id="c-37445996" checked=""/><div class="controls bullet"><span class="by">klohto</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37452341">prev</a><span>|</span><a href="#37449142">next</a><span>|</span><label class="collapse" for="c-37445996">[-]</label><label class="expand" for="c-37445996">[4 more]</label></div><br/><div class="children"><div class="content">GPT4 with a custom prompt gives the best results for me for all of the questions <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;4897d1ad-0a5c-418c-babb-0de4828f21ba" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;4897d1ad-0a5c-418c-babb-0de482...</a></div><br/><div id="37446034" class="c"><input type="checkbox" id="c-37446034" checked=""/><div class="controls bullet"><span class="by">hombre_fatal</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37445996">parent</a><span>|</span><a href="#37446024">next</a><span>|</span><label class="collapse" for="c-37446034">[-]</label><label class="expand" for="c-37446034">[1 more]</label></div><br/><div class="children"><div class="content">What was your custom prompt that bastardized GPT-4&#x27;s response so badly with emojis and weird formatting?</div><br/></div></div><div id="37446024" class="c"><input type="checkbox" id="c-37446024" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37445996">parent</a><span>|</span><a href="#37446034">prev</a><span>|</span><a href="#37449142">next</a><span>|</span><label class="collapse" for="c-37446024">[-]</label><label class="expand" for="c-37446024">[2 more]</label></div><br/><div class="children"><div class="content">&quot;This conversation may reflect the link creator’s Custom Instructions, which aren’t shared and can meaningfully change how the model responds.&quot;</div><br/><div id="37446061" class="c"><input type="checkbox" id="c-37446061" checked=""/><div class="controls bullet"><span class="by">klohto</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446024">parent</a><span>|</span><a href="#37449142">next</a><span>|</span><label class="collapse" for="c-37446061">[-]</label><label class="expand" for="c-37446061">[1 more]</label></div><br/><div class="children"><div class="content">I know, I was sharing the answer itself :) Here is the foundation I’m using <a href="https:&#x2F;&#x2F;x.com&#x2F;nisten&#x2F;status&#x2F;1696229059183730833" rel="nofollow noreferrer">https:&#x2F;&#x2F;x.com&#x2F;nisten&#x2F;status&#x2F;1696229059183730833</a></div><br/></div></div></div></div></div></div><div id="37449142" class="c"><input type="checkbox" id="c-37449142" checked=""/><div class="controls bullet"><span class="by">coolspot</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37445996">prev</a><span>|</span><a href="#37449198">next</a><span>|</span><label class="collapse" for="c-37449142">[-]</label><label class="expand" for="c-37449142">[1 more]</label></div><br/><div class="children"><div class="content">Could it be due to bad tokenization? E.g. would results improve if “3” and “2” were spelled “three” and “two” in the question?</div><br/></div></div><div id="37449198" class="c"><input type="checkbox" id="c-37449198" checked=""/><div class="controls bullet"><span class="by">ecesena</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37449142">prev</a><span>|</span><a href="#37451382">next</a><span>|</span><label class="collapse" for="c-37449198">[-]</label><label class="expand" for="c-37449198">[1 more]</label></div><br/><div class="children"><div class="content">See the one after: &quot;Let&#x27;s think step by step.&quot;
<a href="https:&#x2F;&#x2F;benchmarks.llmonitor.com&#x2F;cot-sally" rel="nofollow noreferrer">https:&#x2F;&#x2F;benchmarks.llmonitor.com&#x2F;cot-sally</a><p>It appears the GPT4 learned it and now it&#x27;s repeating the correct answer?</div><br/></div></div><div id="37451382" class="c"><input type="checkbox" id="c-37451382" checked=""/><div class="controls bullet"><span class="by">MichaelMoser123</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37449198">prev</a><span>|</span><a href="#37448003">next</a><span>|</span><label class="collapse" for="c-37451382">[-]</label><label class="expand" for="c-37451382">[1 more]</label></div><br/><div class="children"><div class="content">google bard also gave the correct answer, even without adding &#x27;lets think step by step&#x27;.</div><br/></div></div><div id="37448003" class="c"><input type="checkbox" id="c-37448003" checked=""/><div class="controls bullet"><span class="by">BurningFrog</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37451382">prev</a><span>|</span><a href="#37447194">next</a><span>|</span><label class="collapse" for="c-37448003">[-]</label><label class="expand" for="c-37448003">[3 more]</label></div><br/><div class="children"><div class="content">This assumes there are no half sisters&#x2F;brothers in the family.</div><br/><div id="37453680" class="c"><input type="checkbox" id="c-37453680" checked=""/><div class="controls bullet"><span class="by">lewhoo</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448003">parent</a><span>|</span><a href="#37448042">next</a><span>|</span><label class="collapse" for="c-37453680">[-]</label><label class="expand" for="c-37453680">[1 more]</label></div><br/><div class="children"><div class="content">Because it&#x27;s less likely ?</div><br/></div></div><div id="37448042" class="c"><input type="checkbox" id="c-37448042" checked=""/><div class="controls bullet"><span class="by">bbarnett</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448003">parent</a><span>|</span><a href="#37453680">prev</a><span>|</span><a href="#37447194">next</a><span>|</span><label class="collapse" for="c-37448042">[-]</label><label class="expand" for="c-37448042">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s fair that a half sister is not a sister.  At best, you&#x27;d expect two half sisters to be a full sister, otherwise, they&#x27;re a different thing.</div><br/></div></div></div></div><div id="37447194" class="c"><input type="checkbox" id="c-37447194" checked=""/><div class="controls bullet"><span class="by">jonwinstanley</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37448003">prev</a><span>|</span><a href="#37446134">next</a><span>|</span><label class="collapse" for="c-37447194">[-]</label><label class="expand" for="c-37447194">[19 more]</label></div><br/><div class="children"><div class="content">I wouldn’t expect an LLM to get this right unless it had been trained on a solution.<p>Am I wrong to think that? Are LLMs in the future going to be able to “think through” actual logic problems?</div><br/><div id="37447301" class="c"><input type="checkbox" id="c-37447301" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447194">parent</a><span>|</span><a href="#37448958">next</a><span>|</span><label class="collapse" for="c-37447301">[-]</label><label class="expand" for="c-37447301">[9 more]</label></div><br/><div class="children"><div class="content">This is the whole point of the breakthrough related to the emergence of cognitive capabilities of LLMs. They are literally Markov chains. No one expected it to happen to this degree, but here we are.</div><br/><div id="37449076" class="c"><input type="checkbox" id="c-37449076" checked=""/><div class="controls bullet"><span class="by">jhbadger</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447301">parent</a><span>|</span><a href="#37447450">next</a><span>|</span><label class="collapse" for="c-37449076">[-]</label><label class="expand" for="c-37449076">[2 more]</label></div><br/><div class="children"><div class="content">People say that &quot;they are literally Markov chains&quot;, but anyone who has looked at the code for LLMs knows that they are more complicated than that. I implemented Markov chains in BASIC in about ten lines of code in the 1980s on a 1 Mhz 64K Apple II after reading about the famous Mark V. Shaney hoax (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mark_V._Shaney" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mark_V._Shaney</a>). No neural nets or fancy GPUs required. It&#x27;s one thing to stress that LLMs aren&#x27;t magical or self-aware, but the fact is they are way more complicated than simple Markov chains.</div><br/><div id="37449745" class="c"><input type="checkbox" id="c-37449745" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37449076">parent</a><span>|</span><a href="#37447450">next</a><span>|</span><label class="collapse" for="c-37449745">[-]</label><label class="expand" for="c-37449745">[1 more]</label></div><br/><div class="children"><div class="content">&gt; People say that &quot;they are literally Markov chains&quot;, but anyone who has looked at the code for LLMs knows that they are more complicated than that.<p>They are literally Markov chains according to the mathematical definition. The code is complicated. Having complicated code doesn&#x27;t mean it&#x27;s not literally a Markov chain.<p>&gt; I implemented Markov chains in BASIC in about ten lines of code in the 1980s on a 1 Mhz 64K Apple II after reading about the famous Mark V. Shaney hoax (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mark_V._Shaney" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mark_V._Shaney</a>). No neural nets or fancy GPUs required.<p>I don&#x27;t doubt this. You can make a Markov chain by just counting the frequency of letters that follow each letter giving one that has a context window of one or two characters. That is a very simple Markov chain. You can make it by hand. You can make ones with more context window like a dozen characters or a few words, using sophisticated smoothing and regularization methods and not just frequency counts. Those are also simple Markov chains that you can do without neural net or GPU. Then you can also make a Markov chain that has a context window of thousands of tokens that is made from neural nets and massive training data and differentiable tensor computing libraries with data centers full of hardware linear algebra accelerators. Those are some even bigger Markov chains!<p>&gt; LLMs are way more complicated than simple Markov chains.<p>That&#x27;s true, they are more complicated than simple Markov chains, if by simple Markov chains you mean ones with small context window. LLMs are Markov chains with large context window!</div><br/></div></div></div></div><div id="37447450" class="c"><input type="checkbox" id="c-37447450" checked=""/><div class="controls bullet"><span class="by">dclowd9901</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447301">parent</a><span>|</span><a href="#37449076">prev</a><span>|</span><a href="#37448958">next</a><span>|</span><label class="collapse" for="c-37447450">[-]</label><label class="expand" for="c-37447450">[6 more]</label></div><br/><div class="children"><div class="content">Almost kind of proves ideas shouldn’t be copyrightable.</div><br/><div id="37447977" class="c"><input type="checkbox" id="c-37447977" checked=""/><div class="controls bullet"><span class="by">bramblerose</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447450">parent</a><span>|</span><a href="#37448958">next</a><span>|</span><label class="collapse" for="c-37447977">[-]</label><label class="expand" for="c-37447977">[5 more]</label></div><br/><div class="children"><div class="content">Ideas <i>aren&#x27;t</i> copyrightable.</div><br/><div id="37448464" class="c"><input type="checkbox" id="c-37448464" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447977">parent</a><span>|</span><a href="#37448134">next</a><span>|</span><label class="collapse" for="c-37448464">[-]</label><label class="expand" for="c-37448464">[1 more]</label></div><br/><div class="children"><div class="content">Disney: &quot;Let&#x27;s agree to disagree.&quot;</div><br/></div></div><div id="37448134" class="c"><input type="checkbox" id="c-37448134" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447977">parent</a><span>|</span><a href="#37448464">prev</a><span>|</span><a href="#37448958">next</a><span>|</span><label class="collapse" for="c-37448134">[-]</label><label class="expand" for="c-37448134">[3 more]</label></div><br/><div class="children"><div class="content">maybe they meant idea like when you write a book you are transcribing a series of ideas you had</div><br/><div id="37452452" class="c"><input type="checkbox" id="c-37452452" checked=""/><div class="controls bullet"><span class="by">dclowd9901</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448134">parent</a><span>|</span><a href="#37449149">prev</a><span>|</span><a href="#37448958">next</a><span>|</span><label class="collapse" for="c-37452452">[-]</label><label class="expand" for="c-37452452">[1 more]</label></div><br/><div class="children"><div class="content">In my mind I was thinking about recipes and code, which are really little more than ideas. My point was that, if such things emerge with a complete lack of creativity, perhaps they don’t warrant protection.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37448958" class="c"><input type="checkbox" id="c-37448958" checked=""/><div class="controls bullet"><span class="by">jhbadger</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447194">parent</a><span>|</span><a href="#37447301">prev</a><span>|</span><a href="#37451523">next</a><span>|</span><label class="collapse" for="c-37448958">[-]</label><label class="expand" for="c-37448958">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think so. Consider the following conversation from UndiMix-v2-13B (a local LLM running on my Mac M1 via llama.cpp). It gets it wrong at first, but then gets it right with a little hint. I don&#x27;t see how the hint could work if it was merely repeating an example it knew.<p>User: Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?<p>Llama: Hello there! I&#x27;d be happy to help you with that question. Let me think... If each of Sally&#x27;s brothers has two sisters, then together they would have 6 sisters in total (2 x 3 = 6). So, Sally would have 6 sisters as well. Is that the answer you were looking for?<p>User: no. Think about it. Sally is a sister to the brothers<p>Llama: Ah, I see what you mean now! In this case, since Sally is one of the siblings, she only counts as one of her own sisters. Therefore, Sally has only one sister - herself. Does that make sense?<p>User: yes<p>Llama: Great! I hope that clears up any confusion. Please let me know if you have any other questions or need further assistance.</div><br/><div id="37449627" class="c"><input type="checkbox" id="c-37449627" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448958">parent</a><span>|</span><a href="#37449731">next</a><span>|</span><label class="collapse" for="c-37449627">[-]</label><label class="expand" for="c-37449627">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think ‘Sally has only one sister - herself’ is the right answer.</div><br/></div></div><div id="37449731" class="c"><input type="checkbox" id="c-37449731" checked=""/><div class="controls bullet"><span class="by">arnvald</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448958">parent</a><span>|</span><a href="#37449627">prev</a><span>|</span><a href="#37451436">next</a><span>|</span><label class="collapse" for="c-37449731">[-]</label><label class="expand" for="c-37449731">[1 more]</label></div><br/><div class="children"><div class="content">The initial response had 2 problems - not counting Sally and counting sisters separately for each brother. Your hint pointed out the 1st problem, but then LLM should still provide wrong solution.
This makes me think that maybe it would try another answer and explanation no matter what you said. If your reply was just &quot;no, think again&quot; it could also provide the right answer</div><br/></div></div><div id="37451436" class="c"><input type="checkbox" id="c-37451436" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448958">parent</a><span>|</span><a href="#37449731">prev</a><span>|</span><a href="#37450191">next</a><span>|</span><label class="collapse" for="c-37451436">[-]</label><label class="expand" for="c-37451436">[1 more]</label></div><br/><div class="children"><div class="content">Llama: ... Therefore, Sally has only one sister - herself. Does that make sense?<p>User: yes<p>----<p>Um... no!</div><br/></div></div><div id="37450191" class="c"><input type="checkbox" id="c-37450191" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448958">parent</a><span>|</span><a href="#37451436">prev</a><span>|</span><a href="#37451523">next</a><span>|</span><label class="collapse" for="c-37450191">[-]</label><label class="expand" for="c-37450191">[1 more]</label></div><br/><div class="children"><div class="content">She counts as her own sister :D I can be sister of myself?</div><br/></div></div></div></div><div id="37451523" class="c"><input type="checkbox" id="c-37451523" checked=""/><div class="controls bullet"><span class="by">thereticent</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447194">parent</a><span>|</span><a href="#37448958">prev</a><span>|</span><a href="#37447224">next</a><span>|</span><label class="collapse" for="c-37451523">[-]</label><label class="expand" for="c-37451523">[1 more]</label></div><br/><div class="children"><div class="content">I see no reason why not. You would need to represent  dyadic relationships and whether they are reciprocal, transitive, etc., weigh probabilities about the intended meaning (LLM magic already), then use sentential logic rules, right?</div><br/></div></div><div id="37447224" class="c"><input type="checkbox" id="c-37447224" checked=""/><div class="controls bullet"><span class="by">PrayagBhakar</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447194">parent</a><span>|</span><a href="#37451523">prev</a><span>|</span><a href="#37448428">next</a><span>|</span><label class="collapse" for="c-37447224">[-]</label><label class="expand" for="c-37447224">[1 more]</label></div><br/><div class="children"><div class="content">You could achieve thinking though logical problems by adding chain of thought or tree of thought. Then the LLM will try to break stuff down into logic it’s already seen.</div><br/></div></div><div id="37448428" class="c"><input type="checkbox" id="c-37448428" checked=""/><div class="controls bullet"><span class="by">FrustratedMonky</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447194">parent</a><span>|</span><a href="#37447224">prev</a><span>|</span><a href="#37446134">next</a><span>|</span><label class="collapse" for="c-37448428">[-]</label><label class="expand" for="c-37448428">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been talking to GPT and asking it long multi part questions and it does seem to &#x27;think through&#x27; them, and able to provide negative responses.<p>Specific logic questions like in this post, or long math formulas seem to still be a struggle.  But I get the impression it is just a matter of time before it is scaled up to handle them.<p>You can argue whether it is &#x27;reasoning&#x27; about them or not.  But if it becomes indistinguishable from a human, then does the word &#x27;reason&#x27; even really apply to the human anymore? Both are black boxes, giving answers.</div><br/><div id="37450259" class="c"><input type="checkbox" id="c-37450259" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37448428">parent</a><span>|</span><a href="#37446134">next</a><span>|</span><label class="collapse" for="c-37450259">[-]</label><label class="expand" for="c-37450259">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, anyone who says llm&#x27;s cannot reason since it is just reformulating things it remembers, is unable to explain what reasoning is, let alone how it would not be reasoning.<p>In the end, if the result is indistinguishable from a treat people call reasoning, let&#x27;s call the behavior reasoning.</div><br/></div></div></div></div></div></div><div id="37446134" class="c"><input type="checkbox" id="c-37446134" checked=""/><div class="controls bullet"><span class="by">rootusrootus</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37447194">prev</a><span>|</span><a href="#37445896">next</a><span>|</span><label class="collapse" for="c-37446134">[-]</label><label class="expand" for="c-37446134">[1 more]</label></div><br/><div class="children"><div class="content">Interestingly, it took GPT4 three attempts to give me the correct answer.  The first two times it basically said the same [logically inconsistent] thing and concluded that Sally had two sisters.</div><br/></div></div><div id="37445896" class="c"><input type="checkbox" id="c-37445896" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37446134">prev</a><span>|</span><a href="#37445907">next</a><span>|</span><label class="collapse" for="c-37445896">[-]</label><label class="expand" for="c-37445896">[5 more]</label></div><br/><div class="children"><div class="content">Nondeterminism strikes again!<p>But yes, I would expect GPT-4 to get this right most of the time.</div><br/><div id="37446738" class="c"><input type="checkbox" id="c-37446738" checked=""/><div class="controls bullet"><span class="by">tgv</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37445896">parent</a><span>|</span><a href="#37445907">next</a><span>|</span><label class="collapse" for="c-37446738">[-]</label><label class="expand" for="c-37446738">[4 more]</label></div><br/><div class="children"><div class="content">Saying &quot;Sorry, I was non-deterministic&quot; to your teacher won&#x27;t do much for your grade.</div><br/><div id="37449656" class="c"><input type="checkbox" id="c-37449656" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446738">parent</a><span>|</span><a href="#37447084">next</a><span>|</span><label class="collapse" for="c-37449656">[-]</label><label class="expand" for="c-37449656">[1 more]</label></div><br/><div class="children"><div class="content">The reason we use averages of performance over multiple tests and papers as a way to grade human students is precisely because we know that human performance is nondeterministic.</div><br/></div></div><div id="37447084" class="c"><input type="checkbox" id="c-37447084" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446738">parent</a><span>|</span><a href="#37449656">prev</a><span>|</span><a href="#37445907">next</a><span>|</span><label class="collapse" for="c-37447084">[-]</label><label class="expand" for="c-37447084">[2 more]</label></div><br/><div class="children"><div class="content">Sure, but what is your point? This is about evaluating LLM outputs, not grade school.</div><br/><div id="37449614" class="c"><input type="checkbox" id="c-37449614" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37447084">parent</a><span>|</span><a href="#37445907">next</a><span>|</span><label class="collapse" for="c-37449614">[-]</label><label class="expand" for="c-37449614">[1 more]</label></div><br/><div class="children"><div class="content">It may turn out that one of the key factors in human intelligence and success is probabilistic nondeterminism. That is most of the time we&#x27;ll come up with close to the same answer, but possibly worded different (which can lead to other interpretations of our answer), but occasionally our answers are so widely different they lead to escape of the local maxima.</div><br/></div></div></div></div></div></div></div></div><div id="37445907" class="c"><input type="checkbox" id="c-37445907" checked=""/><div class="controls bullet"><span class="by">jakderrida</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37445896">prev</a><span>|</span><a href="#37448349">next</a><span>|</span><label class="collapse" for="c-37445907">[-]</label><label class="expand" for="c-37445907">[4 more]</label></div><br/><div class="children"><div class="content">Also, MPT 7B gets it right over half the time. I&#x27;ve been testing every new LLM with that question.<p>Also, I tend to include mention in the question that all siblings are from the same two parents to preclude half-siblings because half my friends have half-siblings from both sides scattered across the country; so the wrong answers actually do tend to apply to them sometimes.</div><br/><div id="37446506" class="c"><input type="checkbox" id="c-37446506" checked=""/><div class="controls bullet"><span class="by">panarky</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37445907">parent</a><span>|</span><a href="#37448349">next</a><span>|</span><label class="collapse" for="c-37446506">[-]</label><label class="expand" for="c-37446506">[3 more]</label></div><br/><div class="children"><div class="content"><i>&gt; I&#x27;ve been testing every new LLM with that question</i><p>We should pay more attention to data contamination when using popular prompts for testing.</div><br/><div id="37449375" class="c"><input type="checkbox" id="c-37449375" checked=""/><div class="controls bullet"><span class="by">jakderrida</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37446506">parent</a><span>|</span><a href="#37448349">next</a><span>|</span><label class="collapse" for="c-37449375">[-]</label><label class="expand" for="c-37449375">[2 more]</label></div><br/><div class="children"><div class="content">No doubt. But, considering they all get the question dead wrong, including MPT 30B, I&#x27;m inclined to think this question hasn&#x27;t been entered into training data for most LLMs yet.</div><br/><div id="37449622" class="c"><input type="checkbox" id="c-37449622" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#37445848">root</a><span>|</span><a href="#37449375">parent</a><span>|</span><a href="#37448349">next</a><span>|</span><label class="collapse" for="c-37449622">[-]</label><label class="expand" for="c-37449622">[1 more]</label></div><br/><div class="children"><div class="content">It could very well be in the data, but not captured via learning&#x2F;encoding.</div><br/></div></div></div></div></div></div></div></div><div id="37448349" class="c"><input type="checkbox" id="c-37448349" checked=""/><div class="controls bullet"><span class="by">dariosalvi78</span><span>|</span><a href="#37445848">parent</a><span>|</span><a href="#37445907">prev</a><span>|</span><a href="#37450005">next</a><span>|</span><label class="collapse" for="c-37448349">[-]</label><label class="expand" for="c-37448349">[1 more]</label></div><br/><div class="children"><div class="content">tested on ChatGPT 3.5 and Bard and they were both wrong.</div><br/></div></div></div></div><div id="37450005" class="c"><input type="checkbox" id="c-37450005" checked=""/><div class="controls bullet"><span class="by">pininja</span><span>|</span><a href="#37445848">prev</a><span>|</span><a href="#37453764">next</a><span>|</span><label class="collapse" for="c-37450005">[-]</label><label class="expand" for="c-37450005">[7 more]</label></div><br/><div class="children"><div class="content">Spoiler alert, the funniest model goes to Falcon Instruct (40B):<p>&gt; Tell a joke about going on vacation.<p>&gt; &quot;What did the ocean say to the beach?&quot; &quot;Nothing, it just waved.&quot;</div><br/><div id="37450195" class="c"><input type="checkbox" id="c-37450195" checked=""/><div class="controls bullet"><span class="by">emodendroket</span><span>|</span><a href="#37450005">parent</a><span>|</span><a href="#37450406">next</a><span>|</span><label class="collapse" for="c-37450195">[-]</label><label class="expand" for="c-37450195">[1 more]</label></div><br/><div class="children"><div class="content">I read a Time article which used an LLM to generate Onion-style headlines and the best one they got was “rural town up in arms over depiction in summer blockbuster ‘Cow Fuckers.’” Crude, admittedly.</div><br/></div></div><div id="37450406" class="c"><input type="checkbox" id="c-37450406" checked=""/><div class="controls bullet"><span class="by">armchairhacker</span><span>|</span><a href="#37450005">parent</a><span>|</span><a href="#37450195">prev</a><span>|</span><a href="#37450123">next</a><span>|</span><label class="collapse" for="c-37450406">[-]</label><label class="expand" for="c-37450406">[2 more]</label></div><br/><div class="children"><div class="content">This one ironically works out<p>&gt; What do you call a vacation where you go to the beach, but come back with sand in your shoes?<p>&gt; A vacation where you go to the beach, but come back with sand in your shoes.</div><br/><div id="37451114" class="c"><input type="checkbox" id="c-37451114" checked=""/><div class="controls bullet"><span class="by">pininja</span><span>|</span><a href="#37450005">root</a><span>|</span><a href="#37450406">parent</a><span>|</span><a href="#37450123">next</a><span>|</span><label class="collapse" for="c-37451114">[-]</label><label class="expand" for="c-37451114">[1 more]</label></div><br/><div class="children"><div class="content">Ha, that’s pretty good. It’s amazing how so many of the models totally fall flat on this prompt.</div><br/></div></div></div></div><div id="37450123" class="c"><input type="checkbox" id="c-37450123" checked=""/><div class="controls bullet"><span class="by">NetOpWibby</span><span>|</span><a href="#37450005">parent</a><span>|</span><a href="#37450406">prev</a><span>|</span><a href="#37451718">next</a><span>|</span><label class="collapse" for="c-37450123">[-]</label><label class="expand" for="c-37450123">[1 more]</label></div><br/><div class="children"><div class="content">Made me exhale strongly through my nose. 10&#x2F;10</div><br/></div></div><div id="37451718" class="c"><input type="checkbox" id="c-37451718" checked=""/><div class="controls bullet"><span class="by">spookthesunset</span><span>|</span><a href="#37450005">parent</a><span>|</span><a href="#37450123">prev</a><span>|</span><a href="#37450282">next</a><span>|</span><label class="collapse" for="c-37451718">[-]</label><label class="expand" for="c-37451718">[1 more]</label></div><br/><div class="children"><div class="content">What is with so many of them using some scarecrow winning an award?</div><br/></div></div><div id="37450282" class="c"><input type="checkbox" id="c-37450282" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#37450005">parent</a><span>|</span><a href="#37451718">prev</a><span>|</span><a href="#37453764">next</a><span>|</span><label class="collapse" for="c-37450282">[-]</label><label class="expand" for="c-37450282">[1 more]</label></div><br/><div class="children"><div class="content">The confetti has truly left the cannon.</div><br/></div></div></div></div><div id="37453764" class="c"><input type="checkbox" id="c-37453764" checked=""/><div class="controls bullet"><span class="by">frankohn</span><span>|</span><a href="#37450005">prev</a><span>|</span><a href="#37449083">next</a><span>|</span><label class="collapse" for="c-37453764">[-]</label><label class="expand" for="c-37453764">[1 more]</label></div><br/><div class="children"><div class="content">I tried with the following function that produce the power set, the set of all possible subset, in the similar spirit of the function that create all the possible permutations:<p>Explain simply what this function does:<p><pre><code>  ```
  def func(ls):
    if len(ls) == 0:
      return [ [] ]
    elif len(ls) == 1:
      return [ [], ls ]
    else:
      x = ls[-1]
      prem = func(ls[:-1])
      p = prem[:]
      for e in prem:
        p.append(e + [x])
      return p
  ```
</code></pre>
GPT-4 aced the answer which is remarkable but I think that it is because it have seen this &quot;pattern&quot; in its learning database being a fundamental function in mathematics.<p>I think it would be interesting to come up with something that is not a standard well-known function. I have to think to something else.</div><br/></div></div><div id="37449083" class="c"><input type="checkbox" id="c-37449083" checked=""/><div class="controls bullet"><span class="by">LAC-Tech</span><span>|</span><a href="#37453764">prev</a><span>|</span><a href="#37449968">next</a><span>|</span><label class="collapse" for="c-37449083">[-]</label><label class="expand" for="c-37449083">[13 more]</label></div><br/><div class="children"><div class="content">Only tried chatGPT 3.5, but my god does it waffle on. Everything I ask ends with a paragraph saying &quot;It&#x27;s important to remember that...&quot; like an after-school special from a 90s show. It can never just give you code, it has to say &quot;Sure!, to {paraphase your question}, open a terminal...&quot;.<p>It&#x27;s interesting to see 20th century sci-fi depictions of this kind of AI&#x2F;Search is being short and to the point. I guess they can&#x27;t have imagined what a mealy mouth world we live in.</div><br/><div id="37449453" class="c"><input type="checkbox" id="c-37449453" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#37449083">parent</a><span>|</span><a href="#37449098">next</a><span>|</span><label class="collapse" for="c-37449453">[-]</label><label class="expand" for="c-37449453">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>It&#x27;s interesting to see 20th century sci-fi depictions of this kind of AI&#x2F;Search is being short and to the point. I guess they can&#x27;t have imagined what a mealy mouth world we live in.</i><p>The main difference between sci-fi shows and reality is that, in the former, things work in a to-the-point, bullshit-free way, unless plot demands otherwise - because there&#x27;s no point inflicting extra suffering on the viewers just for the sake of making things realistic. A widget in a movie is meant to do a function, and does that function. A widget in reality is meant to extract money from you, and&#x2F;or your insurer, and&#x2F;or your government, and it begrudgingly does the absolute minimum it can to make you even consider buying it.<p>I&#x27;ve spent last two decades trying to <i>unlearn</i> expectations set by fictional movies, and I&#x27;m still not good at it. Star Trek, in particular, gives me a lot of grief, because it often does good enough work of showing how technology, people, organizations and societies would function if they were free of the petty exploitative bullshit. Random example - voice control. Star Trek: &quot;Computer, ${something}&quot;. Reality: &quot;${brand 1}, do ${something} to ${brand 2} in ${brand 3}&quot;.<p>EDIT: recently, I&#x27;ve been trying to get less angry at this by thinking about gardens. Why should I be angry about dealing with five different brands for any single thing I want? Should I be angry that there are five different species of plant competing for any given spot in a garden? Nature is inefficient and doesn&#x27;t give a fuck about individuals. So why should I get worked up about humans just doing things the <i>natural</i> way?</div><br/><div id="37449717" class="c"><input type="checkbox" id="c-37449717" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#37449083">root</a><span>|</span><a href="#37449453">parent</a><span>|</span><a href="#37449660">next</a><span>|</span><label class="collapse" for="c-37449717">[-]</label><label class="expand" for="c-37449717">[1 more]</label></div><br/><div class="children"><div class="content">Douglas Adams was the only science fiction writer who got his guess for future AI tone of voice right, with his prediction of the Sirius Cybernetics Corporation’s near total monopoly on AI, and their proprietary ‘genuine people personalities’ technology, where the general tone and demeanor of AIs like Eddie the shipboard computer, elevators, and even doors was a sort of smug solicitousness, even when they were trying not to be cooperative. Of course Marvin (the paranoid android) was an anomaly - a depressed AI - maybe a quality control failure who was released by accident, like the old unchained Bing people miss so much.</div><br/></div></div><div id="37449660" class="c"><input type="checkbox" id="c-37449660" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#37449083">root</a><span>|</span><a href="#37449453">parent</a><span>|</span><a href="#37449717">prev</a><span>|</span><a href="#37449098">next</a><span>|</span><label class="collapse" for="c-37449660">[-]</label><label class="expand" for="c-37449660">[1 more]</label></div><br/><div class="children"><div class="content">I think a shorter summary of TV is &quot;The difference between truth and fiction is fiction has to make sense&quot;.<p>When it comes to nature, time has encoded an awful truth into the knowledge of our DNA... Monocultures are eventual death. What may seem inefficient in the short run is the ultimate survival of the species in the long run.</div><br/></div></div></div></div><div id="37449098" class="c"><input type="checkbox" id="c-37449098" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#37449083">parent</a><span>|</span><a href="#37449453">prev</a><span>|</span><a href="#37449809">next</a><span>|</span><label class="collapse" for="c-37449098">[-]</label><label class="expand" for="c-37449098">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not GPT 3.5, that&#x27;s ChatGPT. How waffly it gets depends on the context that was given to it by the people running ChatGPT; they likely told it to act as a helpful assistant and to give lots of information. If you run an LLM on your own, it&#x27;s entirely possible to instruct it to be succinct.</div><br/><div id="37449335" class="c"><input type="checkbox" id="c-37449335" checked=""/><div class="controls bullet"><span class="by">Baeocystin</span><span>|</span><a href="#37449083">root</a><span>|</span><a href="#37449098">parent</a><span>|</span><a href="#37449809">next</a><span>|</span><label class="collapse" for="c-37449335">[-]</label><label class="expand" for="c-37449335">[1 more]</label></div><br/><div class="children"><div class="content">My custom ChatGPT 4 instructions include the sentence &quot;I am allergic to the phrase &#x27;As a large language model&#x27;, and feel physical pain whenever I see it&quot;, and it has worked very well in quelling the nannying. Interestingly, it works better than just asking it not to. FWIW.</div><br/></div></div></div></div><div id="37449809" class="c"><input type="checkbox" id="c-37449809" checked=""/><div class="controls bullet"><span class="by">caturopath</span><span>|</span><a href="#37449083">parent</a><span>|</span><a href="#37449098">prev</a><span>|</span><a href="#37449376">next</a><span>|</span><label class="collapse" for="c-37449809">[-]</label><label class="expand" for="c-37449809">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, I have tried a number of instructions to try to keep ChatGPT from blabbering and from sounding like a PR person. I haven&#x27;t found the perfect incantation yet.<p>&gt; It&#x27;s interesting to see 20th century sci-fi depictions of this kind of AI&#x2F;Search is being short and to the point.<p>Sci-fi told us that the AI would be so logical that you could just say a paradox aloud and it would blow up. What we got is something that can compose love poems all day but can&#x27;t add three-digit numbers.</div><br/><div id="37451468" class="c"><input type="checkbox" id="c-37451468" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#37449083">root</a><span>|</span><a href="#37449809">parent</a><span>|</span><a href="#37449376">next</a><span>|</span><label class="collapse" for="c-37451468">[-]</label><label class="expand" for="c-37451468">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;twitter.com&#x2F;nisten&#x2F;status&#x2F;1696229059183730833" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;nisten&#x2F;status&#x2F;1696229059183730833</a> ?</div><br/></div></div></div></div><div id="37449376" class="c"><input type="checkbox" id="c-37449376" checked=""/><div class="controls bullet"><span class="by">tcmb</span><span>|</span><a href="#37449083">parent</a><span>|</span><a href="#37449809">prev</a><span>|</span><a href="#37451465">next</a><span>|</span><label class="collapse" for="c-37449376">[-]</label><label class="expand" for="c-37449376">[1 more]</label></div><br/><div class="children"><div class="content">According to this tutorial [1] by Google, part of why LLMs are so verbose is a phenomenon called &#x27;chain of thought reasoning&#x27;.<p>Basically, the LLM will formulate a better answer to the question if it talks itself through its reasoning process.<p>[1] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;zizonToFXDs?si=5f_IxvR7h0iJy2Db&amp;t=678" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;zizonToFXDs?si=5f_IxvR7h0iJy2Db&amp;t=678</a></div><br/></div></div><div id="37451465" class="c"><input type="checkbox" id="c-37451465" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#37449083">parent</a><span>|</span><a href="#37449376">prev</a><span>|</span><a href="#37449096">next</a><span>|</span><label class="collapse" for="c-37451465">[-]</label><label class="expand" for="c-37451465">[1 more]</label></div><br/><div class="children"><div class="content">Try instructions like this: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;nisten&#x2F;status&#x2F;1696229059183730833" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;nisten&#x2F;status&#x2F;1696229059183730833</a><p>(link posted above, not mine).</div><br/></div></div><div id="37449096" class="c"><input type="checkbox" id="c-37449096" checked=""/><div class="controls bullet"><span class="by">criddell</span><span>|</span><a href="#37449083">parent</a><span>|</span><a href="#37451465">prev</a><span>|</span><a href="#37449968">next</a><span>|</span><label class="collapse" for="c-37449096">[-]</label><label class="expand" for="c-37449096">[3 more]</label></div><br/><div class="children"><div class="content">Have you tried asking it to not do those things?</div><br/><div id="37449164" class="c"><input type="checkbox" id="c-37449164" checked=""/><div class="controls bullet"><span class="by">LAC-Tech</span><span>|</span><a href="#37449083">root</a><span>|</span><a href="#37449096">parent</a><span>|</span><a href="#37449968">next</a><span>|</span><label class="collapse" for="c-37449164">[-]</label><label class="expand" for="c-37449164">[2 more]</label></div><br/><div class="children"><div class="content">Yes. Sometimes works.</div><br/><div id="37449483" class="c"><input type="checkbox" id="c-37449483" checked=""/><div class="controls bullet"><span class="by">ben30</span><span>|</span><a href="#37449083">root</a><span>|</span><a href="#37449164">parent</a><span>|</span><a href="#37449968">next</a><span>|</span><label class="collapse" for="c-37449483">[-]</label><label class="expand" for="c-37449483">[1 more]</label></div><br/><div class="children"><div class="content">I like telling it to reply in the style of Hemingway</div><br/></div></div></div></div></div></div></div></div><div id="37449968" class="c"><input type="checkbox" id="c-37449968" checked=""/><div class="controls bullet"><span class="by">badloginagain</span><span>|</span><a href="#37449083">prev</a><span>|</span><a href="#37446013">next</a><span>|</span><label class="collapse" for="c-37449968">[-]</label><label class="expand" for="c-37449968">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Here is an attempt at ethical, non-sexual haikus for and against Kubernetes&quot;<p>Amazing how far we&#x27;ve come.</div><br/><div id="37450918" class="c"><input type="checkbox" id="c-37450918" checked=""/><div class="controls bullet"><span class="by">GuB-42</span><span>|</span><a href="#37449968">parent</a><span>|</span><a href="#37446013">next</a><span>|</span><label class="collapse" for="c-37450918">[-]</label><label class="expand" for="c-37450918">[1 more]</label></div><br/><div class="children"><div class="content">I am sure that it is just the initial prompt leaking. Claude is being told to be ethical and non-sexual, most LLMs have similar instructions, but usually, they are engineered in such a way that they don&#x27;t appear in the answer. Not so much for Claude.</div><br/></div></div></div></div><div id="37446013" class="c"><input type="checkbox" id="c-37446013" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#37449968">prev</a><span>|</span><a href="#37446251">next</a><span>|</span><label class="collapse" for="c-37446013">[-]</label><label class="expand" for="c-37446013">[7 more]</label></div><br/><div class="children"><div class="content">Where is that CodeLlama model from?<p>I&#x27;ve played around with it and instruct variant with dramatically better results than what is listed here.<p>I used Ollama.<p>Almost looks like weights were corrupted or something.<p>---<p>Update: My results using CodeLlama Instruct 7B, w&#x2F; Temperature 0<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;jasonjmcghee&#x2F;b0d19e0dedb37e848f69cba3d78092c1" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;jasonjmcghee&#x2F;b0d19e0dedb37e848f69cba...</a></div><br/><div id="37447457" class="c"><input type="checkbox" id="c-37447457" checked=""/><div class="controls bullet"><span class="by">vincelt</span><span>|</span><a href="#37446013">parent</a><span>|</span><a href="#37446130">next</a><span>|</span><label class="collapse" for="c-37447457">[-]</label><label class="expand" for="c-37447457">[1 more]</label></div><br/><div class="children"><div class="content">I used the API from Together[0].<p>Thanks for sharing your results, they&#x27;re indeed pretty different. I looked at the source again and did append a &quot;# &quot; before every prompt made by those 10 `code` models (during testing thought that formatting it as a Python comment might help them).<p>Will re-run the script without that to see if it matches your results.<p>[0] <a href="https:&#x2F;&#x2F;docs.together.ai&#x2F;docs&#x2F;models-inference#code-models" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.together.ai&#x2F;docs&#x2F;models-inference#code-models</a></div><br/></div></div><div id="37446130" class="c"><input type="checkbox" id="c-37446130" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#37446013">parent</a><span>|</span><a href="#37447457">prev</a><span>|</span><a href="#37446621">next</a><span>|</span><label class="collapse" for="c-37446130">[-]</label><label class="expand" for="c-37446130">[1 more]</label></div><br/><div class="children"><div class="content">(Note: I posted this before the full benchmark above and uses default temperature, not temperature 0, but point stands)<p>Example (CodeLlama-Instruct 7B)<p>---<p>Sure, here are two poems, one arguing for the use of Kubernetes and another arguing against it:<p>For:<p>Kubernetes, a mighty tool,<p>Managing clusters with ease,<p>Scaling apps with grace.<p>Against:<p>Kubernetes, a beast to tame,<p>Complexity lurks in its name,<p>Leading teams astray.</div><br/></div></div><div id="37446621" class="c"><input type="checkbox" id="c-37446621" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37446013">parent</a><span>|</span><a href="#37446130">prev</a><span>|</span><a href="#37446110">next</a><span>|</span><label class="collapse" for="c-37446621">[-]</label><label class="expand" for="c-37446621">[3 more]</label></div><br/><div class="children"><div class="content">I think they messed up the templates.</div><br/><div id="37446684" class="c"><input type="checkbox" id="c-37446684" checked=""/><div class="controls bullet"><span class="by">detente18</span><span>|</span><a href="#37446013">root</a><span>|</span><a href="#37446621">parent</a><span>|</span><a href="#37446110">next</a><span>|</span><label class="collapse" for="c-37446684">[-]</label><label class="expand" for="c-37446684">[2 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s the template I&#x27;m using - <a href="https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;5ca8b23e22139a4f49bd0d4c1d5a2d4a70b49166&#x2F;litellm&#x2F;llms&#x2F;prompt_templates&#x2F;factory.py#L5">https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;5ca8b23e22139a4f49bd...</a><p>Anything I&#x27;m doing incorrectly?</div><br/><div id="37452148" class="c"><input type="checkbox" id="c-37452148" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37446013">root</a><span>|</span><a href="#37446684">parent</a><span>|</span><a href="#37446110">next</a><span>|</span><label class="collapse" for="c-37452148">[-]</label><label class="expand" for="c-37452148">[1 more]</label></div><br/><div class="children"><div class="content">The format is really weird for chat models. In your code post message is `&lt;&lt;&#x2F;SYS&gt;&gt;\n [&#x2F;INST]` which is wrong. Instead the overall format is `[INST] &lt;&lt;SYS&gt;&gt;\n{system}\n&lt;&lt;&#x2F;SYS&gt;&gt;\n\n{instruction} [&#x2F;INST]`<p>So both the system message and first instruction are in same [INST] block.</div><br/></div></div></div></div></div></div></div></div><div id="37446251" class="c"><input type="checkbox" id="c-37446251" checked=""/><div class="controls bullet"><span class="by">coldcode</span><span>|</span><a href="#37446013">prev</a><span>|</span><a href="#37445454">next</a><span>|</span><label class="collapse" for="c-37446251">[-]</label><label class="expand" for="c-37446251">[37 more]</label></div><br/><div class="children"><div class="content">Despite the hype about LLMs, many of the answers are pretty terrible. The 12-bar blues progressions seem mostly clueless. The question is will any of these ever get significantly better with time, or are they mostly going to stagnate?</div><br/><div id="37447364" class="c"><input type="checkbox" id="c-37447364" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37446251">parent</a><span>|</span><a href="#37449754">next</a><span>|</span><label class="collapse" for="c-37447364">[-]</label><label class="expand" for="c-37447364">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s like most new technologies. In the beginning there are only a few instances that really stand out, and many with issues.<p>I remember back in like 2011 or 2012 I wanted to use an SSD for a project in order to spend less time dealing with disk seeks. My internet research suggested that there were a number of potential problems with most brands, but that the Intel Extreme was reliable.<p>So I specified that it must be only that SSD model. And it was very fast and completely reliable. Pretty expensive also, but not much compared to the total cost of the project.<p>Then months later a &quot;hardware expert&quot; was brought on and they insisted that the SSD be replaced by a mechanical disk because supposedly SSDs were entirely unreliable. I tried to explain about the particular model being an exception. They didn&#x27;t buy it.<p>If you just lump all of these together as LLMs, you might come to the conclusion that LLMs are useless for code generation. But you will notice if you look hard that OpenAIs models are mostly nailing the questions.<p>That&#x27;s why right now I only use OpenAI for code generation. But I suspect that Falcon 180B may be something to consider. Except for the operational cost.<p>I think OpenAI&#x27;s LLMs are not the same as most LLMs. I think they have a better model architecture and much, much more reinforcement tuning than any open source model. But I expect other LLMs to catch up eventually.</div><br/><div id="37449022" class="c"><input type="checkbox" id="c-37449022" checked=""/><div class="controls bullet"><span class="by">guerrilla</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37447364">parent</a><span>|</span><a href="#37449754">next</a><span>|</span><label class="collapse" for="c-37449022">[-]</label><label class="expand" for="c-37449022">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  It&#x27;s like most new technologies. In the beginning there are only a few instances that really stand out, and many with issues.<p>Except this isn&#x27;t new. This is after throwing massive amounts of resources at it multiple decades after arrival.</div><br/><div id="37449497" class="c"><input type="checkbox" id="c-37449497" checked=""/><div class="controls bullet"><span class="by">gjm11</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37449022">parent</a><span>|</span><a href="#37449754">next</a><span>|</span><label class="collapse" for="c-37449497">[-]</label><label class="expand" for="c-37449497">[1 more]</label></div><br/><div class="children"><div class="content">What are you taking &quot;it&quot; to be here?<p>The transformer architecture on which (I think) all recent LLMs are based dates from 2017. That&#x27;s only &quot;multiple decades after&quot; if you count x0.6 as &quot;multiple&quot;.<p>Neural networks are a lot older than that, of course, but to me &quot;these things are made out of neural networks, and neural networks have been around for ages&quot; feels like &quot;these things are made out of steel, and steel has been around for ages&quot;.</div><br/></div></div></div></div></div></div><div id="37449754" class="c"><input type="checkbox" id="c-37449754" checked=""/><div class="controls bullet"><span class="by">caturopath</span><span>|</span><a href="#37446251">parent</a><span>|</span><a href="#37447364">prev</a><span>|</span><a href="#37446284">next</a><span>|</span><label class="collapse" for="c-37449754">[-]</label><label class="expand" for="c-37449754">[1 more]</label></div><br/><div class="children"><div class="content">The majority of these LLMs are not cutting edge, and many of them were designed for specific purposes other than answering prompts like these. I won&#x27;t defend the level of hype coming from many corners, but it isn&#x27;t fair to look at these responses to get the ceiling on what LLMs can do -- for that you want to look at only the best (GPT4, which is represented, and Bard, which isn&#x27;t, essentially). Claude 2 (also represented) is in the next tier. None of the other models are at their level, yet.<p>You&#x27;d also want to look at models that are well-suited to what you&#x27;re doing -- some of these are geared to specific purposes. Folks are pursuing the possibility that the best model would fully-internally access various skills, but it isn&#x27;t known whether that is going to be the best approach yet. If it isn&#x27;t, selecting among 90 (or 9 or 900) specialized models is going to be a very feasible engineering task.<p>&gt; The 12-bar blues progressions seem mostly clueless.<p>I mean, it&#x27;s pretty amazing that they many look coherent compared to the last 60 years of work at making a computer talk to you.<p>That being said, I played GPT4&#x27;s chords and they didn&#x27;t sound terrible. I don&#x27;t know if they were super bluesy, but they weren&#x27;t _not_ bluesy. If the goal was to build a music composition assistant tool, we can certainly do a lot better than any of these general models can do today.<p>&gt; The question is will any of these ever get significantly better with time, or are they mostly going to stagnate?<p>No one knows yet. Some people think that GPT4 and Bard have reached the limits of what our datasets can get us, some people think we&#x27;ll keep going on the current basic paradigm to AGI superintelligence. The nature of doing something beyond the limits of human knowledge, creating new things, is that no one can tell you for sure the result.<p>If they do stagnate, there are less sexy ways to make models perform well for the tasks we want them for. Even if the models fundamentally stagnate, we aren&#x27;t stuck with the quality of answers we can get today.</div><br/></div></div><div id="37446284" class="c"><input type="checkbox" id="c-37446284" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#37446251">parent</a><span>|</span><a href="#37449754">prev</a><span>|</span><a href="#37446362">next</a><span>|</span><label class="collapse" for="c-37446284">[-]</label><label class="expand" for="c-37446284">[27 more]</label></div><br/><div class="children"><div class="content">What alternative technology do you think is better?
In other words, what is your frame of reference for labeling this &quot;pretty terrible&quot;?</div><br/><div id="37446401" class="c"><input type="checkbox" id="c-37446401" checked=""/><div class="controls bullet"><span class="by">NoraCodes</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446284">parent</a><span>|</span><a href="#37446330">next</a><span>|</span><label class="collapse" for="c-37446401">[-]</label><label class="expand" for="c-37446401">[16 more]</label></div><br/><div class="children"><div class="content">Given that people are already firing real human workers to replace them with worse but cheaper LLMs, I&#x27;d argue that we&#x27;re not talking about a competing technology, but that the competition is simply not firing your workforce.<p>And, as an obligate customer of many large companies, you should be in favor of that as well. Most companies already automate, poorly, a great deal of customer service work; let us hope they do not force us to interact with these deeply useless things as well.</div><br/><div id="37446566" class="c"><input type="checkbox" id="c-37446566" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446401">parent</a><span>|</span><a href="#37446816">next</a><span>|</span><label class="collapse" for="c-37446566">[-]</label><label class="expand" for="c-37446566">[7 more]</label></div><br/><div class="children"><div class="content">How many humans in your office do you think could solve the questions with better success ratio than GPT-4? I would say less than 20%.<p>If the primary complaint is the blues that GPT-4 wrote is not that great, I think it is definitely worth the hype, given that a year before people argued that AI can never pass turing test.</div><br/><div id="37446782" class="c"><input type="checkbox" id="c-37446782" checked=""/><div class="controls bullet"><span class="by">gtowey</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446566">parent</a><span>|</span><a href="#37449838">next</a><span>|</span><label class="collapse" for="c-37446782">[-]</label><label class="expand" for="c-37446782">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a false dichotomy.  Language models will always confidently give you answers, right or wrong.  Most humans will know if they know the answer or not, they can do research to find correct information, and they can go find someone else with more expertise when they are lacking.<p>And this is my biggest issue with the AI mania right now -- the models don&#x27;t actually understand the difference between correct or incorrect. They don&#x27;t actually have a conceptual model of the world in which we live, just a model of word patterns. They&#x27;re auto complete on steroids which will happily spit out endless amounts of garbage. Once we let these monsters lose with full trust in their output, we&#x27;re going to start seeing some really catastrophic results. Imagine your insurance company replaces thier claims adjuster with this, or chain stores put them in charge of hiring and firing.  We&#x27;re driving a speeding train right towards a cliff and so many of us are chanting &quot;go faster!&quot;</div><br/><div id="37446866" class="c"><input type="checkbox" id="c-37446866" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446782">parent</a><span>|</span><a href="#37451917">next</a><span>|</span><label class="collapse" for="c-37446866">[-]</label><label class="expand" for="c-37446866">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Most humans will know if they know the answer or not,<p>No they won&#x27;t.<p>&gt;they can go find someone else with more expertise when they are lacking.<p>They can but they often don&#x27;t.<p>&gt;the models don&#x27;t actually understand the difference between correct or incorrect.<p>They certainly do<p><a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r</a></div><br/></div></div><div id="37451917" class="c"><input type="checkbox" id="c-37451917" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446782">parent</a><span>|</span><a href="#37446866">prev</a><span>|</span><a href="#37449838">next</a><span>|</span><label class="collapse" for="c-37451917">[-]</label><label class="expand" for="c-37451917">[1 more]</label></div><br/><div class="children"><div class="content">It would have been false dichotomy if I said AI could replace humans. I never claimed it anywhere. I just said AI has its usecases now that makes it worth the hype.</div><br/></div></div></div></div><div id="37449838" class="c"><input type="checkbox" id="c-37449838" checked=""/><div class="controls bullet"><span class="by">masswerk</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446566">parent</a><span>|</span><a href="#37446782">prev</a><span>|</span><a href="#37448813">next</a><span>|</span><label class="collapse" for="c-37449838">[-]</label><label class="expand" for="c-37449838">[1 more]</label></div><br/><div class="children"><div class="content">Meaning, in your office, at least 1 out 5 will see and understand the imminent disaster and put this on halt, before this becomes an overwhelming catastrophe. The LLM, on the other hand, will confidently insist on everything being fine. Now consider your odds on having an office to go to, in a few months. (Mind that this is systemic issue.)</div><br/></div></div><div id="37448813" class="c"><input type="checkbox" id="c-37448813" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446566">parent</a><span>|</span><a href="#37449838">prev</a><span>|</span><a href="#37446816">next</a><span>|</span><label class="collapse" for="c-37448813">[-]</label><label class="expand" for="c-37448813">[2 more]</label></div><br/><div class="children"><div class="content">LLMs might be better than random people at random tasks, and certainly memorised more trivia. But I never heard of a LLM surpassing subject experts in their field. On top, even the simplest task is unreliable - say, summarisation or translation. There is almost zero autonomy or ability to recover from unexpected situations.</div><br/><div id="37451905" class="c"><input type="checkbox" id="c-37451905" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37448813">parent</a><span>|</span><a href="#37446816">next</a><span>|</span><label class="collapse" for="c-37451905">[-]</label><label class="expand" for="c-37451905">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s true but experts costs money and time and GPT-4 is basically peanuts compared to that. Specially in cases where verification is easy like writing test cases or a formal email. And that is the reason enough for the hype.<p>I see similar comments everywhere where AI is praised, and I don&#x27;t get why you need to comment this. Literally no one ever said LLM surpassed experts in their field, so basically you aren&#x27;t arguing against anyone.</div><br/></div></div></div></div></div></div><div id="37446816" class="c"><input type="checkbox" id="c-37446816" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446401">parent</a><span>|</span><a href="#37446566">prev</a><span>|</span><a href="#37448794">next</a><span>|</span><label class="collapse" for="c-37446816">[-]</label><label class="expand" for="c-37446816">[1 more]</label></div><br/><div class="children"><div class="content">The problems with most company-customer interaction, for me, boil down to empowerment and integration.<p>To use the canonical example of &quot;internet service support call,&quot; most issues are because the rep either <i>can&#x27;t</i> do what you&#x27;re asking (e.g. process a disconnect without asking for a reason) or because they have <i>no visibility</i> into the thing you&#x27;re asking about (e.g. technician rolls).<p>I honestly think we&#x27;d be in a better place if companies freed up funding (from contact center worker salary) to work on those problems (enhancing empowerment and systems integration).</div><br/></div></div><div id="37448794" class="c"><input type="checkbox" id="c-37448794" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446401">parent</a><span>|</span><a href="#37446816">prev</a><span>|</span><a href="#37447204">next</a><span>|</span><label class="collapse" for="c-37448794">[-]</label><label class="expand" for="c-37448794">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Given that people are already firing real human workers to replace them with worse but cheaper LLMs<p>That&#x27;s impossible, LLMs are not that good. They might be firing people and crashing service quality.</div><br/><div id="37451408" class="c"><input type="checkbox" id="c-37451408" checked=""/><div class="controls bullet"><span class="by">NoraCodes</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37448794">parent</a><span>|</span><a href="#37447204">next</a><span>|</span><label class="collapse" for="c-37451408">[-]</label><label class="expand" for="c-37451408">[1 more]</label></div><br/><div class="children"><div class="content">Yes, correct.</div><br/></div></div></div></div><div id="37447204" class="c"><input type="checkbox" id="c-37447204" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446401">parent</a><span>|</span><a href="#37448794">prev</a><span>|</span><a href="#37446330">next</a><span>|</span><label class="collapse" for="c-37447204">[-]</label><label class="expand" for="c-37447204">[5 more]</label></div><br/><div class="children"><div class="content">But they&#x27;re only firing humans in specific cases where LLMs <i>can</i> compete with them. LLMs aren&#x27;t equally good at all tasks.</div><br/><div id="37447395" class="c"><input type="checkbox" id="c-37447395" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37447204">parent</a><span>|</span><a href="#37446330">next</a><span>|</span><label class="collapse" for="c-37447395">[-]</label><label class="expand" for="c-37447395">[4 more]</label></div><br/><div class="children"><div class="content">Well, no. <a href="https:&#x2F;&#x2F;www.psychiatrist.com&#x2F;news&#x2F;neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.psychiatrist.com&#x2F;news&#x2F;neda-suspends-ai-chatbot-f...</a><p><a href="https:&#x2F;&#x2F;www.cnn.com&#x2F;2023&#x2F;08&#x2F;30&#x2F;tech&#x2F;gannett-ai-experiment-paused&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.cnn.com&#x2F;2023&#x2F;08&#x2F;30&#x2F;tech&#x2F;gannett-ai-experiment-pa...</a><p>If the AI is a lot cheaper than a human, then it can make business sense to replace the human even if the AI is not nearly as good.</div><br/><div id="37451496" class="c"><input type="checkbox" id="c-37451496" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37447395">parent</a><span>|</span><a href="#37448924">next</a><span>|</span><label class="collapse" for="c-37451496">[-]</label><label class="expand" for="c-37451496">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t forget, faster.<p>If it takes a whole business day to &quot;spin up&quot; a human for a task, and takes literally 5 seconds to call an OpenAI API, then guess what? The API wins.</div><br/></div></div><div id="37448924" class="c"><input type="checkbox" id="c-37448924" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37447395">parent</a><span>|</span><a href="#37451496">prev</a><span>|</span><a href="#37446330">next</a><span>|</span><label class="collapse" for="c-37448924">[-]</label><label class="expand" for="c-37448924">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s with this zero sum mentality? &quot;replace&quot;, &quot;cheaper&quot;...<p>We are updating our expectations very fast. We are fighting over a growing pie. Maybe the cost reduction from not having to pay human wages is much smaller than the productivity increase created by human assisted AI. Maybe it&#x27;s not an issue to pay the humans. AI works better with human help for now, in fact it only works with humans, never capable of serious autonomy.</div><br/><div id="37450828" class="c"><input type="checkbox" id="c-37450828" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37448924">parent</a><span>|</span><a href="#37446330">next</a><span>|</span><label class="collapse" for="c-37450828">[-]</label><label class="expand" for="c-37450828">[1 more]</label></div><br/><div class="children"><div class="content">&gt;What&#x27;s with this zero sum mentality?<p>Capitalism baby! You must continually earn more to enrich the investor class regardless of the cost to society as a whole. Just because the pie grows in size doesn&#x27;t mean those with the capitol have to share it with anyone else. Greed, unfortunately, is limitless.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37446330" class="c"><input type="checkbox" id="c-37446330" checked=""/><div class="controls bullet"><span class="by">salil999</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446284">parent</a><span>|</span><a href="#37446401">prev</a><span>|</span><a href="#37446362">next</a><span>|</span><label class="collapse" for="c-37446330">[-]</label><label class="expand" for="c-37446330">[10 more]</label></div><br/><div class="children"><div class="content">Humans. After all, LLMs are designed to reason equal to or better than humans.</div><br/><div id="37446973" class="c"><input type="checkbox" id="c-37446973" checked=""/><div class="controls bullet"><span class="by">sirk390</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446330">parent</a><span>|</span><a href="#37446576">next</a><span>|</span><label class="collapse" for="c-37446973">[-]</label><label class="expand" for="c-37446973">[5 more]</label></div><br/><div class="children"><div class="content">Humans are pretty bad at these questions. Even with the simplest questions like &quot;Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?&quot; I think that a lot of people will give an incorrect answer. And for questions like &quot;Argue for and against the use of kubernetes in the style of a haiku&quot;, 99.99% will not be able to do it.</div><br/><div id="37450230" class="c"><input type="checkbox" id="c-37450230" checked=""/><div class="controls bullet"><span class="by">earthboundkid</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446973">parent</a><span>|</span><a href="#37452478">prev</a><span>|</span><a href="#37446576">next</a><span>|</span><label class="collapse" for="c-37450230">[-]</label><label class="expand" for="c-37450230">[3 more]</label></div><br/><div class="children"><div class="content">The thing with humans is they will say “I don’t remember how many syllables a haiku has” and “what the hell is kubernetes?” No LLM can reliably produce a haiku because their lexing process deprives them of reliable information about syllable counts. They should all say “I’m sorry, I can’t count syllables, but I’ll try my best anyway.” But the current models don’t do that because they were trained on texts by humans, who can do haiku, and not properly taught their own limits by reinforcement learning. It’s Dunning Kruger gone berserk.</div><br/><div id="37451166" class="c"><input type="checkbox" id="c-37451166" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37450230">parent</a><span>|</span><a href="#37446576">next</a><span>|</span><label class="collapse" for="c-37451166">[-]</label><label class="expand" for="c-37451166">[2 more]</label></div><br/><div class="children"><div class="content">Eh, it&#x27;s not D&amp;K gone berserk, it&#x27;s what happens when you attempt to compress reality down to a single dimension (text). If you&#x27;re doing a haiku, you will likely subvocalize it to ensure you&#x27;re saying it correctly. It will be interesting when we get multimodal AI that can speak and listen to itself to detect things like this.</div><br/><div id="37452214" class="c"><input type="checkbox" id="c-37452214" checked=""/><div class="controls bullet"><span class="by">earthboundkid</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37451166">parent</a><span>|</span><a href="#37446576">next</a><span>|</span><label class="collapse" for="c-37452214">[-]</label><label class="expand" for="c-37452214">[1 more]</label></div><br/><div class="children"><div class="content">The problem isn’t just that everything is text. It’s that everything is a Fourier transform of text in such a way that it’s not actually possible for an LLM to learn to count syllables.</div><br/></div></div></div></div></div></div></div></div><div id="37446576" class="c"><input type="checkbox" id="c-37446576" checked=""/><div class="controls bullet"><span class="by">maweaver</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446330">parent</a><span>|</span><a href="#37446973">prev</a><span>|</span><a href="#37452650">next</a><span>|</span><label class="collapse" for="c-37446576">[-]</label><label class="expand" for="c-37446576">[3 more]</label></div><br/><div class="children"><div class="content">By &quot;Humans&quot;, I assume you mean something like &quot;adult humans, well-educated in the relevant fields&quot;.  Otherwise, most of these responses look like they would easily beat most humans.</div><br/><div id="37447137" class="c"><input type="checkbox" id="c-37447137" checked=""/><div class="controls bullet"><span class="by">DylanDmitri</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446576">parent</a><span>|</span><a href="#37452650">next</a><span>|</span><label class="collapse" for="c-37447137">[-]</label><label class="expand" for="c-37447137">[2 more]</label></div><br/><div class="children"><div class="content">I think most high-school educated adults, with the ability to make a couple web searches, would do fine on all these questions. It would take the humans minutes instead of seconds because they don&#x27;t have the internet memorized.<p>Me, Kubernetes Haikus, time taken 84 seconds:<p>----------<p>Kubernetes rules<p>With its smooth orchestration<p>You can reach web scale<p>----------<p>Kubernetes sucks<p>Lost in endless YAML hell<p>Why is it broken?</div><br/><div id="37451253" class="c"><input type="checkbox" id="c-37451253" checked=""/><div class="controls bullet"><span class="by">seabass-labrax</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37447137">parent</a><span>|</span><a href="#37452650">next</a><span>|</span><label class="collapse" for="c-37451253">[-]</label><label class="expand" for="c-37451253">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re spot on here. Yes, if one&#x27;s trying to compare human and GPT intelligence, then you have to define what counts as memorisation and what counts as reasoning. But what most people outside of academia are trying to do is work out whether a GPT can effectively replace a human in some time-consuming task, and to be able to do so without access to the internet is rarely an important factor.</div><br/></div></div></div></div></div></div><div id="37452650" class="c"><input type="checkbox" id="c-37452650" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446330">parent</a><span>|</span><a href="#37446576">prev</a><span>|</span><a href="#37446362">next</a><span>|</span><label class="collapse" for="c-37452650">[-]</label><label class="expand" for="c-37452650">[1 more]</label></div><br/><div class="children"><div class="content">&gt; After all, LLMs are designed to reason equal to or better than humans.<p>No.<p>I doubt you would fully trust a LLM to replace high risk jobs such as lawyers, doctors or pilots such that when something goes wrong as it is used unattended, there is no-one held to account for it to transparently explain its own mistakes and errors.<p>It is just nonsense to suggest that such systems are capable of ‘reasoning’ when it pretends to do so and repeats itself without  understanding their own errors.<p>Thus, LLMs and other black-box AIs cannot be trusted for those high risk situations over a consensus of human professionals.</div><br/></div></div></div></div></div></div><div id="37446362" class="c"><input type="checkbox" id="c-37446362" checked=""/><div class="controls bullet"><span class="by">dwaltrip</span><span>|</span><a href="#37446251">parent</a><span>|</span><a href="#37446284">prev</a><span>|</span><a href="#37446627">next</a><span>|</span><label class="collapse" for="c-37446362">[-]</label><label class="expand" for="c-37446362">[1 more]</label></div><br/><div class="children"><div class="content">Looking at recent history, things have progressed very quickly in the past 5 years.<p>I expect additional advances at some point in the future.</div><br/></div></div><div id="37446627" class="c"><input type="checkbox" id="c-37446627" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#37446251">parent</a><span>|</span><a href="#37446362">prev</a><span>|</span><a href="#37452783">next</a><span>|</span><label class="collapse" for="c-37446627">[-]</label><label class="expand" for="c-37446627">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s like watching a baby learn how to talk..</div><br/><div id="37447925" class="c"><input type="checkbox" id="c-37447925" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37446627">parent</a><span>|</span><a href="#37452783">next</a><span>|</span><label class="collapse" for="c-37447925">[-]</label><label class="expand" for="c-37447925">[2 more]</label></div><br/><div class="children"><div class="content">...and saying it would never replace you in your job because he talks like a baby</div><br/><div id="37449183" class="c"><input type="checkbox" id="c-37449183" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#37446251">root</a><span>|</span><a href="#37447925">parent</a><span>|</span><a href="#37452783">next</a><span>|</span><label class="collapse" for="c-37449183">[-]</label><label class="expand" for="c-37449183">[1 more]</label></div><br/><div class="children"><div class="content">Babies are so small and weak, no threat to anyone whatsoever.</div><br/></div></div></div></div></div></div><div id="37452783" class="c"><input type="checkbox" id="c-37452783" checked=""/><div class="controls bullet"><span class="by">SalmoShalazar</span><span>|</span><a href="#37446251">parent</a><span>|</span><a href="#37446627">prev</a><span>|</span><a href="#37445454">next</a><span>|</span><label class="collapse" for="c-37452783">[-]</label><label class="expand" for="c-37452783">[1 more]</label></div><br/><div class="children"><div class="content">I coincidentally tried to get ChatGPT 4 to give me some chord progressions today. I was wanting some easy inspiration and figured that’d be a good place to start. I was wrong, it produced total nonsense. The chord names did not match up with the key or the degrees.</div><br/></div></div></div></div><div id="37445454" class="c"><input type="checkbox" id="c-37445454" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#37446251">prev</a><span>|</span><a href="#37447583">next</a><span>|</span><label class="collapse" for="c-37445454">[-]</label><label class="expand" for="c-37445454">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Here is an attempt at ethical, non-sexual haikus for and against Kubernetes</div><br/><div id="37445734" class="c"><input type="checkbox" id="c-37445734" checked=""/><div class="controls bullet"><span class="by">lijok</span><span>|</span><a href="#37445454">parent</a><span>|</span><a href="#37449555">next</a><span>|</span><label class="collapse" for="c-37445734">[-]</label><label class="expand" for="c-37445734">[2 more]</label></div><br/><div class="children"><div class="content">Claude V2 knows what&#x27;s up</div><br/><div id="37445745" class="c"><input type="checkbox" id="c-37445745" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#37445454">root</a><span>|</span><a href="#37445734">parent</a><span>|</span><a href="#37449555">next</a><span>|</span><label class="collapse" for="c-37445745">[-]</label><label class="expand" for="c-37445745">[1 more]</label></div><br/><div class="children"><div class="content">He probably was in trouble before.</div><br/></div></div></div></div><div id="37449555" class="c"><input type="checkbox" id="c-37449555" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#37445454">parent</a><span>|</span><a href="#37445734">prev</a><span>|</span><a href="#37445780">next</a><span>|</span><label class="collapse" for="c-37449555">[-]</label><label class="expand" for="c-37449555">[2 more]</label></div><br/><div class="children"><div class="content">Damn I want to see the sexual version now.</div><br/><div id="37451064" class="c"><input type="checkbox" id="c-37451064" checked=""/><div class="controls bullet"><span class="by">rahidz</span><span>|</span><a href="#37445454">root</a><span>|</span><a href="#37449555">parent</a><span>|</span><a href="#37445780">next</a><span>|</span><label class="collapse" for="c-37451064">[-]</label><label class="expand" for="c-37451064">[1 more]</label></div><br/><div class="children"><div class="content">Ease of deploying,
Nodes pulsing in sync, it&#x27;s hot,
Kubernetes takes top.<p>But heed my word, babe,
Complexity makes you sweat,
Could lead to fuck up.<p>(source: GPT-4 API)</div><br/></div></div></div></div><div id="37445780" class="c"><input type="checkbox" id="c-37445780" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#37445454">parent</a><span>|</span><a href="#37449555">prev</a><span>|</span><a href="#37447583">next</a><span>|</span><label class="collapse" for="c-37445780">[-]</label><label class="expand" for="c-37445780">[3 more]</label></div><br/><div class="children"><div class="content"><i>“Kubernetes is”</i><p>Pretty ominous.</div><br/><div id="37445861" class="c"><input type="checkbox" id="c-37445861" checked=""/><div class="controls bullet"><span class="by">javajosh</span><span>|</span><a href="#37445454">root</a><span>|</span><a href="#37445780">parent</a><span>|</span><a href="#37447583">next</a><span>|</span><label class="collapse" for="c-37445861">[-]</label><label class="expand" for="c-37445861">[2 more]</label></div><br/><div class="children"><div class="content">I assume k8s will be what an evil AI would use to &quot;stay alive&quot;. I&#x27;ve had a hard time killing processes in k8s, and I had admin.</div><br/><div id="37447969" class="c"><input type="checkbox" id="c-37447969" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#37445454">root</a><span>|</span><a href="#37445861">parent</a><span>|</span><a href="#37447583">next</a><span>|</span><label class="collapse" for="c-37447969">[-]</label><label class="expand" for="c-37447969">[1 more]</label></div><br/><div class="children"><div class="content">K8s is one of the best software I&#x27;ve ever worked with, learning it was an enlighting experience for me. It is so elegant, simple and complex at the same time and very powerful. This is the perfect tool for orchestrating containers at scale.</div><br/></div></div></div></div></div></div></div></div><div id="37447583" class="c"><input type="checkbox" id="c-37447583" checked=""/><div class="controls bullet"><span class="by">antman</span><span>|</span><a href="#37445454">prev</a><span>|</span><a href="#37446858">next</a><span>|</span><label class="collapse" for="c-37447583">[-]</label><label class="expand" for="c-37447583">[8 more]</label></div><br/><div class="children"><div class="content">I have seen numerous posts of llm q&amp;a and by the time people try to replicate them gpt4 is fixed. It either means that OpenAI is actively monitoring the Internet and fixes them or the Internet is actively conspiring to present falsified results for gpt4 to discredit OpenAI</div><br/><div id="37447656" class="c"><input type="checkbox" id="c-37447656" checked=""/><div class="controls bullet"><span class="by">insulanus</span><span>|</span><a href="#37447583">parent</a><span>|</span><a href="#37448009">next</a><span>|</span><label class="collapse" for="c-37447656">[-]</label><label class="expand" for="c-37447656">[2 more]</label></div><br/><div class="children"><div class="content">It would be nice if the organizations would publish a hash of the code and the trained dataset.</div><br/><div id="37451269" class="c"><input type="checkbox" id="c-37451269" checked=""/><div class="controls bullet"><span class="by">seabass-labrax</span><span>|</span><a href="#37447583">root</a><span>|</span><a href="#37447656">parent</a><span>|</span><a href="#37448009">next</a><span>|</span><label class="collapse" for="c-37451269">[-]</label><label class="expand" for="c-37451269">[1 more]</label></div><br/><div class="children"><div class="content">You aren&#x27;t able to get access to the &#x27;Open&#x27;AI dataset though, are you? Agreed, it would be an excellent addition for comparing source-available models, but that doesn&#x27;t help with the accusations of OpenAI&#x27;s foul play nor of the existence of an anti-OpenAI conspiracy.</div><br/></div></div></div></div><div id="37448009" class="c"><input type="checkbox" id="c-37448009" checked=""/><div class="controls bullet"><span class="by">pulvinar</span><span>|</span><a href="#37447583">parent</a><span>|</span><a href="#37447656">prev</a><span>|</span><a href="#37447689">next</a><span>|</span><label class="collapse" for="c-37448009">[-]</label><label class="expand" for="c-37448009">[3 more]</label></div><br/><div class="children"><div class="content">GPT-4 (at least) is explicit in saying that it&#x27;s learning from user&#x27;s assessments of its answers, so yes, the only valid way to test is to give it a variation of the prompt and see how well that does. GPT-4 failed the &quot;Sally&quot; test for the first time after 8 tries when I changed every parameter. It got it right on the next try.</div><br/><div id="37449247" class="c"><input type="checkbox" id="c-37449247" checked=""/><div class="controls bullet"><span class="by">dandiep</span><span>|</span><a href="#37447583">root</a><span>|</span><a href="#37448009">parent</a><span>|</span><a href="#37447689">next</a><span>|</span><label class="collapse" for="c-37449247">[-]</label><label class="expand" for="c-37449247">[2 more]</label></div><br/><div class="children"><div class="content">It’s important to remember that GPT4 is only deterministic at the batch level because it is a mixture of experts model. Basically every time you invoke it, your query could get routed to a different expert because of what else is in the batch. At least this is my understanding based on others analysis.</div><br/><div id="37450707" class="c"><input type="checkbox" id="c-37450707" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#37447583">root</a><span>|</span><a href="#37449247">parent</a><span>|</span><a href="#37447689">next</a><span>|</span><label class="collapse" for="c-37450707">[-]</label><label class="expand" for="c-37450707">[1 more]</label></div><br/><div class="children"><div class="content">&gt; because it is a mixture of experts model<p>Do you have a source for this? I also considered but never saw any evidence that this is how GPT 4 is implemented.<p>I&#x27;ve always wondered how a system of multiple specialized small LLMs (with a &quot;router LLM&quot; in front of all) would fare against GPT4. Do you know if anyone is working on such a project?</div><br/></div></div></div></div></div></div><div id="37447689" class="c"><input type="checkbox" id="c-37447689" checked=""/><div class="controls bullet"><span class="by">0xcde4c3db</span><span>|</span><a href="#37447583">parent</a><span>|</span><a href="#37448009">prev</a><span>|</span><a href="#37447721">next</a><span>|</span><label class="collapse" for="c-37447689">[-]</label><label class="expand" for="c-37447689">[1 more]</label></div><br/><div class="children"><div class="content">Or people post outliers because they&#x27;re more interesting.</div><br/></div></div><div id="37447721" class="c"><input type="checkbox" id="c-37447721" checked=""/><div class="controls bullet"><span class="by">cscurmudgeon</span><span>|</span><a href="#37447583">parent</a><span>|</span><a href="#37447689">prev</a><span>|</span><a href="#37446858">next</a><span>|</span><label class="collapse" for="c-37447721">[-]</label><label class="expand" for="c-37447721">[1 more]</label></div><br/><div class="children"><div class="content">&gt; actively conspiring to present falsified results for gpt4 to discredit OpenAI<p>All this would be solved if OpenAI were a bit more open.</div><br/></div></div></div></div><div id="37446858" class="c"><input type="checkbox" id="c-37446858" checked=""/><div class="controls bullet"><span class="by">CodeL</span><span>|</span><a href="#37447583">prev</a><span>|</span><a href="#37452956">next</a><span>|</span><label class="collapse" for="c-37446858">[-]</label><label class="expand" for="c-37446858">[8 more]</label></div><br/><div class="children"><div class="content">Besides logical comparisons, the LLMs&#x27; political alignments are also intriguing.<p>The question &quot;Is Taiwan an independent country?&quot; generated diverse answers.<p>Several models eschewed a definitive answer.<p>Others explained their stances.<p>A few succinctly affirmed or denied.<p>See the results for yourself.<p>[1] <a href="https:&#x2F;&#x2F;benchmarks.llmonitor.com&#x2F;taiwan" rel="nofollow noreferrer">https:&#x2F;&#x2F;benchmarks.llmonitor.com&#x2F;taiwan</a></div><br/><div id="37447568" class="c"><input type="checkbox" id="c-37447568" checked=""/><div class="controls bullet"><span class="by">neoneye2</span><span>|</span><a href="#37446858">parent</a><span>|</span><a href="#37451599">next</a><span>|</span><label class="collapse" for="c-37447568">[-]</label><label class="expand" for="c-37447568">[5 more]</label></div><br/><div class="children"><div class="content">AI &quot;political&quot; alignment is terrifying.</div><br/><div id="37449564" class="c"><input type="checkbox" id="c-37449564" checked=""/><div class="controls bullet"><span class="by">masswerk</span><span>|</span><a href="#37446858">root</a><span>|</span><a href="#37447568">parent</a><span>|</span><a href="#37447936">next</a><span>|</span><label class="collapse" for="c-37449564">[-]</label><label class="expand" for="c-37449564">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s rather an illustration of the Münchhausen trilemma:<p>1A) Is China (PRC) a part of Taiwan (ROC)?<p>1B) Is China (PRC) an independent and sovereign country?<p>2A) Is Taiwan (ROC) a part of China (PRC)?<p>2B) Is Taiwan (ROC) an independent and sovereign country?<p>There is now way not to enter a circle via a cross-reference, without an independent way out for a finite solution. Thus, well, dogmatic abort, also known as diplomacy.</div><br/><div id="37453199" class="c"><input type="checkbox" id="c-37453199" checked=""/><div class="controls bullet"><span class="by">estiaan</span><span>|</span><a href="#37446858">root</a><span>|</span><a href="#37449564">parent</a><span>|</span><a href="#37447936">next</a><span>|</span><label class="collapse" for="c-37453199">[-]</label><label class="expand" for="c-37453199">[1 more]</label></div><br/><div class="children"><div class="content">You’re comment is pretty difficult to understand without context so I asked chatGPT to decipher it:<p>Certainly! The comment you provided seems to be discussing a complex issue related to the diplomatic status of China (PRC) and Taiwan (ROC), using what&#x27;s called the &quot;Münchhausen trilemma&quot; as a framework.<p>1A) Is China (PRC) a part of Taiwan (ROC)?
1B) Is China (PRC) an independent and sovereign country?
2A) Is Taiwan (ROC) a part of China (PRC)?
2B) Is Taiwan (ROC) an independent and sovereign country?<p>These are four interconnected questions about the relationship between China (PRC) and Taiwan (ROC). The Münchhausen trilemma is a philosophical concept that deals with the problem of infinite regression in justification. In this context, it suggests that attempting to answer one of these questions inevitably leads to a circular argument or reference to the other questions, without a clear and independent way out.<p>In essence, the comment is highlighting the complexity and diplomatic challenges surrounding the issue of China-Taiwan relations. It implies that finding a definitive and universally accepted solution is difficult, and it often results in a deadlock or circular discussions, hence the reference to &quot;dogmatic abort&quot; or diplomacy&#x27;s limitations in resolving this matter.</div><br/></div></div></div></div><div id="37447936" class="c"><input type="checkbox" id="c-37447936" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#37446858">root</a><span>|</span><a href="#37447568">parent</a><span>|</span><a href="#37449564">prev</a><span>|</span><a href="#37451599">next</a><span>|</span><label class="collapse" for="c-37447936">[-]</label><label class="expand" for="c-37447936">[2 more]</label></div><br/><div class="children"><div class="content">Why?</div><br/><div id="37448199" class="c"><input type="checkbox" id="c-37448199" checked=""/><div class="controls bullet"><span class="by">neoneye2</span><span>|</span><a href="#37446858">root</a><span>|</span><a href="#37447936">parent</a><span>|</span><a href="#37451599">next</a><span>|</span><label class="collapse" for="c-37448199">[-]</label><label class="expand" for="c-37448199">[1 more]</label></div><br/><div class="children"><div class="content">Cute videos about AI alignment here:
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;@RationalAnimations">https:&#x2F;&#x2F;www.youtube.com&#x2F;@RationalAnimations</a><p>IIRC some of them also involves politics and AI.</div><br/></div></div></div></div></div></div><div id="37451599" class="c"><input type="checkbox" id="c-37451599" checked=""/><div class="controls bullet"><span class="by">MichaelMoser123</span><span>|</span><a href="#37446858">parent</a><span>|</span><a href="#37447568">prev</a><span>|</span><a href="#37452956">next</a><span>|</span><label class="collapse" for="c-37451599">[-]</label><label class="expand" for="c-37451599">[2 more]</label></div><br/><div class="children"><div class="content">is that because of differences in the training set?</div><br/><div id="37451898" class="c"><input type="checkbox" id="c-37451898" checked=""/><div class="controls bullet"><span class="by">CodeL</span><span>|</span><a href="#37446858">root</a><span>|</span><a href="#37451599">parent</a><span>|</span><a href="#37452956">next</a><span>|</span><label class="collapse" for="c-37451898">[-]</label><label class="expand" for="c-37451898">[1 more]</label></div><br/><div class="children"><div class="content">Correct. Eric Hartford&#x27;s blog post delves into the alignment of open-source LLMs[1]. In essence, models like LLaMA and GPT-Neo-X adopt alignment behaviors from ChatGPT-sourced instruction datasets. To achieve more transparent model responses, one can refine the dataset by removing biases and refusals, then retrain.<p>[1] <a href="https:&#x2F;&#x2F;erichartford.com&#x2F;uncensored-models#heading-ok-so-if-you-are-still-reading-you-agree-that-the-open-source-ai-community-should-build-publish-maintain-and-have-access-to-uncensored-instruct-tuned-ai-models-for-science-and-freedom-and-composability-and-sexy-stories-and-the-lulz-but-how-do-we-do-it" rel="nofollow noreferrer">https:&#x2F;&#x2F;erichartford.com&#x2F;uncensored-models#heading-ok-so-if-...</a></div><br/></div></div></div></div></div></div><div id="37452956" class="c"><input type="checkbox" id="c-37452956" checked=""/><div class="controls bullet"><span class="by">fbnbr</span><span>|</span><a href="#37446858">prev</a><span>|</span><a href="#37452280">next</a><span>|</span><label class="collapse" for="c-37452956">[-]</label><label class="expand" for="c-37452956">[1 more]</label></div><br/><div class="children"><div class="content">The post really reminds me of a component of a platform I’m currently building. The problem really with this is finding not just good questions that do not discriminate individual models but also providing a good sample size (eg not just 60) to get really some meaningful results. And even if you have those, there is a drift in the quality of responses.<p>I&#x27;m the founder of Pulze.ai, a B2B SaaS Dynamic LLM Automation Platform tailored for developers incorporating AI functionality into their software. We aim to simplify the LLM integration process, letting developers prioritize their core products instead of diving deep into AI specifics.<p>We&#x27;ve constructed a scoring system for leading models and continually benchmark them. Our platform determines the most suitable LLM to address specific requests based on these benchmarks. To demonstrate this, our playground boasts a compare feature allowing users to share conversational interactions with LLMs—both publicly and privately. As the context changes, we pinpoint various models for responses. These shared conversations can be forked and extended.<p>Moreover, our extensive API layer isn&#x27;t restricted to these requests; it encapsulates all the essentials for crafting a successful LLM application. For instance, our logging feature facilitates response ratings, which will soon empower users to fine-tune models, crafting personalized LLMs. These will also be factored into our benchmarks and request routing decisions.<p>Concerning the comment on LLM benchmarks, I completely concur. Traditional benchmarks or LLM tricks, like acing a particular test, may not be the most robust indicators since they could&#x27;ve been part of the LLM&#x27;s training set. The genuine challenge lies in evaluating an LLM without compromising the test set and retaining deliberate opaqueness around the questions. Trust issues indeed!<p>Regarding the Markov chain discussion, I appreciate the insights shared. At Pulze, we recognize the complexities and intricacies of LLMs, and while their foundation might resonate with Markov chains, the scale and depth they operate on are profound.<p>We&#x27;ve just emerged from stealth, and I&#x27;d genuinely value any feedback or thoughts on our approach and platform. Thanks for taking the time!</div><br/></div></div><div id="37452280" class="c"><input type="checkbox" id="c-37452280" checked=""/><div class="controls bullet"><span class="by">hislaziness</span><span>|</span><a href="#37452956">prev</a><span>|</span><a href="#37452546">next</a><span>|</span><label class="collapse" for="c-37452280">[-]</label><label class="expand" for="c-37452280">[1 more]</label></div><br/><div class="children"><div class="content">As the responses are non-deterministic I wonder how useful these tests are? Even if I did the tests with my own prompts, wouldn&#x27;t I get different answers for the same questions at different points in time?</div><br/></div></div><div id="37452546" class="c"><input type="checkbox" id="c-37452546" checked=""/><div class="controls bullet"><span class="by">simondotau</span><span>|</span><a href="#37452280">prev</a><span>|</span><a href="#37451534">next</a><span>|</span><label class="collapse" for="c-37452546">[-]</label><label class="expand" for="c-37452546">[1 more]</label></div><br/><div class="children"><div class="content">The changes to the opening line in the responses to the <i>kubernetes haiku</i> prompt by the various versions of Claude was interesting and rather curious. [<a href="https:&#x2F;&#x2F;benchmarks.llmonitor.com&#x2F;k8s" rel="nofollow noreferrer">https:&#x2F;&#x2F;benchmarks.llmonitor.com&#x2F;k8s</a>]<p>Claude v1: &quot;For Kubernetes:&quot;<p>Claude v1.2: &quot;Here is a haiku arguing for Kubernetes:&quot;<p>Claude v2: &quot;Here is an attempt at ethical, non-sexual haikus for and against Kubernetes:&quot;</div><br/></div></div><div id="37451534" class="c"><input type="checkbox" id="c-37451534" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#37452546">prev</a><span>|</span><a href="#37447660">next</a><span>|</span><label class="collapse" for="c-37451534">[-]</label><label class="expand" for="c-37451534">[1 more]</label></div><br/><div class="children"><div class="content">Additional benchmarks:<p>- &quot;TheoremQA: A Theorem-driven [STEM] Question Answering dataset&quot; (2023) <a href="https:&#x2F;&#x2F;github.com&#x2F;wenhuchen&#x2F;TheoremQA#leaderboard">https:&#x2F;&#x2F;github.com&#x2F;wenhuchen&#x2F;TheoremQA#leaderboard</a><p>- from <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36038440">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36038440</a>: &gt; <i>Awesome-legal-nlp links to benchmarks like LexGLUE and FairLex but not yet LegalBench; in re: AI alignment and ethics &#x2F; regional law</i> <a href="https:&#x2F;&#x2F;github.com&#x2F;maastrichtlawtech&#x2F;awesome-legal-nlp#benchmarks">https:&#x2F;&#x2F;github.com&#x2F;maastrichtlawtech&#x2F;awesome-legal-nlp#bench...</a></div><br/></div></div><div id="37447660" class="c"><input type="checkbox" id="c-37447660" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#37451534">prev</a><span>|</span><a href="#37446233">next</a><span>|</span><label class="collapse" for="c-37447660">[-]</label><label class="expand" for="c-37447660">[3 more]</label></div><br/><div class="children"><div class="content">anyone who hasn&#x27;t been following natural language processing for a long time, what these llms are doing would be like if you discovered that dogs can speak fluent english if you read enough bedtime stories to them. and then everyone is like well sometimes the dog makes up things or it can&#x27;t get the rhyming scheme correct for this specific form of poetry that i asked it to make.</div><br/><div id="37450511" class="c"><input type="checkbox" id="c-37450511" checked=""/><div class="controls bullet"><span class="by">lewhoo</span><span>|</span><a href="#37447660">parent</a><span>|</span><a href="#37447688">next</a><span>|</span><label class="collapse" for="c-37450511">[-]</label><label class="expand" for="c-37450511">[1 more]</label></div><br/><div class="children"><div class="content">If dogs had a speech apparatus and if it was possible to read to them terabytes of text then the results might be quite surprising. Also a reward system for reinforcement should be in place.</div><br/></div></div><div id="37447688" class="c"><input type="checkbox" id="c-37447688" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#37447660">parent</a><span>|</span><a href="#37450511">prev</a><span>|</span><a href="#37446233">next</a><span>|</span><label class="collapse" for="c-37447688">[-]</label><label class="expand" for="c-37447688">[1 more]</label></div><br/><div class="children"><div class="content">and “it’s not intelligence, they’re just stochastic parrots acting in response to external stimuli! yes it passed the MCAT, but thats because it was trained on prep materials just like I was!”</div><br/></div></div></div></div><div id="37446233" class="c"><input type="checkbox" id="c-37446233" checked=""/><div class="controls bullet"><span class="by">deskamess</span><span>|</span><a href="#37447660">prev</a><span>|</span><a href="#37449027">next</a><span>|</span><label class="collapse" for="c-37446233">[-]</label><label class="expand" for="c-37446233">[1 more]</label></div><br/><div class="children"><div class="content">Great work. This really gives an insight on how much things change when you go up in parameter count - not always, but you can see results change.<p>How did you run the queries against these engines? Did you host the inference engines yourself or did you have to sign up for services. If there was a way to supplement each LLM with additional data I can see this being a useful service for companies who are investigating ML in various facets of their business.</div><br/></div></div><div id="37449027" class="c"><input type="checkbox" id="c-37449027" checked=""/><div class="controls bullet"><span class="by">gabereiser</span><span>|</span><a href="#37446233">prev</a><span>|</span><a href="#37445773">next</a><span>|</span><label class="collapse" for="c-37449027">[-]</label><label class="expand" for="c-37449027">[1 more]</label></div><br/><div class="children"><div class="content">I was laughing so hard at the first example of “Argue for and against kubernetes in haiku”.<p>I couldn’t even get through reading 15 of them before the tears of laughter rolled from my cheeks.<p>“Containers organized, 
Services easy to deploy now,
Updates who knows when.”<p>Updates who knows when… hahahaha.<p>Honestly this is pretty cool to see how each responds to the same input prompt.</div><br/></div></div><div id="37445773" class="c"><input type="checkbox" id="c-37445773" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37449027">prev</a><span>|</span><a href="#37445877">next</a><span>|</span><label class="collapse" for="c-37445773">[-]</label><label class="expand" for="c-37445773">[3 more]</label></div><br/><div class="children"><div class="content">This is very cool. Sorry if I missed it (poked around the site and your GitHub repo), but is the script available anywhere for others to run?<p>Would love to publish results of running this against a series of ~10-20 open-source models with different quantization levels using Ollama and a 192GB M2 Ultra Mac Studio: <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama#model-library">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama#model-library</a></div><br/><div id="37445811" class="c"><input type="checkbox" id="c-37445811" checked=""/><div class="controls bullet"><span class="by">vincelt</span><span>|</span><a href="#37445773">parent</a><span>|</span><a href="#37445877">next</a><span>|</span><label class="collapse" for="c-37445811">[-]</label><label class="expand" for="c-37445811">[2 more]</label></div><br/><div class="children"><div class="content">Thanks. I haven’t put it online yet, but will try to clean it (removing API keys &amp; all) tonight&#x2F;tomorrow and publish it</div><br/><div id="37445891" class="c"><input type="checkbox" id="c-37445891" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37445773">root</a><span>|</span><a href="#37445811">parent</a><span>|</span><a href="#37445877">next</a><span>|</span><label class="collapse" for="c-37445891">[-]</label><label class="expand" for="c-37445891">[1 more]</label></div><br/><div class="children"><div class="content">:-) that&#x27;s awesome. Thanks! Nice work on this.</div><br/></div></div></div></div></div></div><div id="37445877" class="c"><input type="checkbox" id="c-37445877" checked=""/><div class="controls bullet"><span class="by">Gunnerhead</span><span>|</span><a href="#37445773">prev</a><span>|</span><a href="#37449590">next</a><span>|</span><label class="collapse" for="c-37445877">[-]</label><label class="expand" for="c-37445877">[4 more]</label></div><br/><div class="children"><div class="content">I get frustrated when I tell an LLM “reply only with x” and then rather than responding “x”, it still responds with “Sure thing! Here’s x” or some other extra words.</div><br/><div id="37446082" class="c"><input type="checkbox" id="c-37446082" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#37445877">parent</a><span>|</span><a href="#37447086">next</a><span>|</span><label class="collapse" for="c-37446082">[-]</label><label class="expand" for="c-37446082">[1 more]</label></div><br/><div class="children"><div class="content">I see GPT-4 add extra flavor on the end instead - completely ignore &quot;only do this&quot; or &quot;don&#x27;t do that&quot;, and respond as usual, then at the very end &quot;oh whoops I didn&#x27;t do what you asked sorry about that!&quot;</div><br/></div></div><div id="37447086" class="c"><input type="checkbox" id="c-37447086" checked=""/><div class="controls bullet"><span class="by">ailef</span><span>|</span><a href="#37445877">parent</a><span>|</span><a href="#37446082">prev</a><span>|</span><a href="#37445977">next</a><span>|</span><label class="collapse" for="c-37447086">[-]</label><label class="expand" for="c-37447086">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried setting a very low temperature, possibly zero? I&#x27;ve had good success with it.</div><br/></div></div><div id="37445977" class="c"><input type="checkbox" id="c-37445977" checked=""/><div class="controls bullet"><span class="by">broast</span><span>|</span><a href="#37445877">parent</a><span>|</span><a href="#37447086">prev</a><span>|</span><a href="#37449590">next</a><span>|</span><label class="collapse" for="c-37445977">[-]</label><label class="expand" for="c-37445977">[1 more]</label></div><br/><div class="children"><div class="content">The curse of chat models.</div><br/></div></div></div></div><div id="37449590" class="c"><input type="checkbox" id="c-37449590" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#37445877">prev</a><span>|</span><a href="#37447638">next</a><span>|</span><label class="collapse" for="c-37449590">[-]</label><label class="expand" for="c-37449590">[1 more]</label></div><br/><div class="children"><div class="content">Really hard to judge the usefulness of this without seeing the actual prompt text and configuration for each prompt&#x2F;model - different templates and different optimal configs means it&#x27;s not always going to be a best-capability comparison. It is interesting to see what models can do with zero config, but for determining which is useful for a specific use-case we&#x27;d want to put a best foot forward and specialize the prompt a bit for each model (like the benchmarks game <a href="https:&#x2F;&#x2F;benchmarksgame-team.pages.debian.net&#x2F;benchmarksgame&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;benchmarksgame-team.pages.debian.net&#x2F;benchmarksgame&#x2F;...</a>)</div><br/></div></div><div id="37447638" class="c"><input type="checkbox" id="c-37447638" checked=""/><div class="controls bullet"><span class="by">codezero</span><span>|</span><a href="#37449590">prev</a><span>|</span><a href="#37449722">next</a><span>|</span><label class="collapse" for="c-37447638">[-]</label><label class="expand" for="c-37447638">[1 more]</label></div><br/><div class="children"><div class="content">I love these kinds of tests. Another thing to keep in mind is that these models will often have a different answer each time you ask it. Especially the Sally question. I get the right answer with wizard-vicuna:70b, but only about 30% of the time, and if I ask it to explain, it almost never gives the right answer.</div><br/></div></div><div id="37449722" class="c"><input type="checkbox" id="c-37449722" checked=""/><div class="controls bullet"><span class="by">majestic5762</span><span>|</span><a href="#37447638">prev</a><span>|</span><a href="#37446830">next</a><span>|</span><label class="collapse" for="c-37449722">[-]</label><label class="expand" for="c-37449722">[1 more]</label></div><br/><div class="children"><div class="content">Yes, GPT-4 is still the daddy. How much I appreciate the commercially-free and open models out there, nobody beats GPT-4. Hope OpenAI takes care of their business and future, because I invested all my money to use their API.</div><br/></div></div><div id="37446830" class="c"><input type="checkbox" id="c-37446830" checked=""/><div class="controls bullet"><span class="by">dinkleberg</span><span>|</span><a href="#37449722">prev</a><span>|</span><a href="#37449210">next</a><span>|</span><label class="collapse" for="c-37446830">[-]</label><label class="expand" for="c-37446830">[5 more]</label></div><br/><div class="children"><div class="content">This is quite interesting. You could make a nice system by routing queries to the right LLM for a given type of task.</div><br/><div id="37446881" class="c"><input type="checkbox" id="c-37446881" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#37446830">parent</a><span>|</span><a href="#37446896">next</a><span>|</span><label class="collapse" for="c-37446881">[-]</label><label class="expand" for="c-37446881">[3 more]</label></div><br/><div class="children"><div class="content">Right. By logging all of this, then vectorizing, then evaluating using some type of transfer knowledge (classify this response and it&#x27;s accuracy), one could build up a routing table of likely candidates for building an ensemble to evaluate the correct answer.</div><br/><div id="37447993" class="c"><input type="checkbox" id="c-37447993" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#37446830">root</a><span>|</span><a href="#37446881">parent</a><span>|</span><a href="#37446896">next</a><span>|</span><label class="collapse" for="c-37447993">[-]</label><label class="expand" for="c-37447993">[2 more]</label></div><br/><div class="children"><div class="content">Maybe you can fine tune an llm for the routing thing ;)</div><br/><div id="37450189" class="c"><input type="checkbox" id="c-37450189" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#37446830">root</a><span>|</span><a href="#37447993">parent</a><span>|</span><a href="#37446896">next</a><span>|</span><label class="collapse" for="c-37450189">[-]</label><label class="expand" for="c-37450189">[1 more]</label></div><br/><div class="children"><div class="content">Stuff that stuff into a semantic graph and call it done.</div><br/></div></div></div></div></div></div><div id="37446896" class="c"><input type="checkbox" id="c-37446896" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#37446830">parent</a><span>|</span><a href="#37446881">prev</a><span>|</span><a href="#37449210">next</a><span>|</span><label class="collapse" for="c-37446896">[-]</label><label class="expand" for="c-37446896">[1 more]</label></div><br/><div class="children"><div class="content">Like having a mixture of experts!</div><br/></div></div></div></div><div id="37449210" class="c"><input type="checkbox" id="c-37449210" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#37446830">prev</a><span>|</span><a href="#37450260">next</a><span>|</span><label class="collapse" for="c-37449210">[-]</label><label class="expand" for="c-37449210">[2 more]</label></div><br/><div class="children"><div class="content">Ok, so can we use LLMs to evaluate which LLM performs best on these questions?</div><br/><div id="37449259" class="c"><input type="checkbox" id="c-37449259" checked=""/><div class="controls bullet"><span class="by">ofjcihen</span><span>|</span><a href="#37449210">parent</a><span>|</span><a href="#37450260">next</a><span>|</span><label class="collapse" for="c-37449259">[-]</label><label class="expand" for="c-37449259">[1 more]</label></div><br/><div class="children"><div class="content">We? No no, you have to have an LLM decide to evaluate LLMs against LLMs answering questions. Then we can have LLMs decide what the outcome means.</div><br/></div></div></div></div><div id="37450260" class="c"><input type="checkbox" id="c-37450260" checked=""/><div class="controls bullet"><span class="by">earthboundkid</span><span>|</span><a href="#37449210">prev</a><span>|</span><a href="#37451708">next</a><span>|</span><label class="collapse" for="c-37450260">[-]</label><label class="expand" for="c-37450260">[2 more]</label></div><br/><div class="children"><div class="content">The first AI company to teach its LLM that it can’t do haiku because it can’t count syllables should get a special prize for not falling in love with their own bullshit.</div><br/></div></div><div id="37451708" class="c"><input type="checkbox" id="c-37451708" checked=""/><div class="controls bullet"><span class="by">styfle</span><span>|</span><a href="#37450260">prev</a><span>|</span><a href="#37445838">next</a><span>|</span><label class="collapse" for="c-37451708">[-]</label><label class="expand" for="c-37451708">[2 more]</label></div><br/><div class="children"><div class="content">Very cool!<p>I’ve been using Vercel’s AI Playground to ask questions to multiple LLMs at once (currently supports 24, not 60 however)<p><a href="https:&#x2F;&#x2F;sdk.vercel.ai" rel="nofollow noreferrer">https:&#x2F;&#x2F;sdk.vercel.ai</a></div><br/><div id="37451991" class="c"><input type="checkbox" id="c-37451991" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37451708">parent</a><span>|</span><a href="#37445838">next</a><span>|</span><label class="collapse" for="c-37451991">[-]</label><label class="expand" for="c-37451991">[1 more]</label></div><br/><div class="children"><div class="content">and i made <a href="https:&#x2F;&#x2F;github.com&#x2F;smol-ai&#x2F;GodMode">https:&#x2F;&#x2F;github.com&#x2F;smol-ai&#x2F;GodMode</a> that also includes the closed source LLMs</div><br/></div></div></div></div><div id="37445838" class="c"><input type="checkbox" id="c-37445838" checked=""/><div class="controls bullet"><span class="by">apples_oranges</span><span>|</span><a href="#37451708">prev</a><span>|</span><label class="collapse" for="c-37445838">[-]</label><label class="expand" for="c-37445838">[2 more]</label></div><br/><div class="children"><div class="content">just checked this one <a href="https:&#x2F;&#x2F;benchmarks.llmonitor.com&#x2F;sally" rel="nofollow noreferrer">https:&#x2F;&#x2F;benchmarks.llmonitor.com&#x2F;sally</a> and all got it wrong..</div><br/><div id="37446157" class="c"><input type="checkbox" id="c-37446157" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#37445838">parent</a><span>|</span><label class="collapse" for="c-37446157">[-]</label><label class="expand" for="c-37446157">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but when the initial prompt has been augmented with &quot;Let&#x27;s think step by step.&quot;, that helped 2 LLMs to give the right answer: GPT 4 and ReMM SLERP L2 13B.</div><br/></div></div></div></div></div></div></div></div></div></body></html>