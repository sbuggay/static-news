<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1698483652248" as="style"/><link rel="stylesheet" href="styles.css?v=1698483652248"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://gonzoml.substack.com/p/the-convolution-empire-strikes-back">The convolution empire strikes back</a>Â <span class="domain">(<a href="https://gonzoml.substack.com">gonzoml.substack.com</a>)</span></div><div class="subtext"><span>che_shr_cat</span> | <span>40 comments</span></div><br/><div><div id="38044289" class="c"><input type="checkbox" id="c-38044289" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38044391">next</a><span>|</span><label class="collapse" for="c-38044289">[-]</label><label class="expand" for="c-38044289">[24 more]</label></div><br/><div class="children"><div class="content">My theory is that architecture doesn&#x27;t matter - convolutional, transformer or recurrent, as long as you can efficiently train models of the same size, what counts is the dataset.<p>Similarly, humans achieve about the same results when they have the same training. Small variations. What matters is not the brain but the education they get.<p>Of course I am exaggerating a bit, just saying there are a multitude of architectures of brain and neural nets with similar abilities, and the differentiating factor is the data not the model.<p>For years we have seen hundreds of papers trying to propose sub-quadratic attention. They all failed to get traction, big labs still use almost vanilla transformer. At some point a paper declared &quot;mixing is all you need&quot; (MLP-Mixers) to replace &quot;attention is all you need&quot;. Just mixing, the optimiser adapts to what it gets.<p>If you think about it, maybe language creates a virtual layer where language operations are performed. And this works similarly in humans and AIs. That&#x27;s why the architecture doesn&#x27;t matter, because it is running the language-OS on top. Similarly for vision.<p>I place 90% the merits of AI on language and 10% on the model architecture. Finding intelligence was inevitable, it was hiding in language, that&#x27;s how we get to be intelligent as well. A human raised without language is even worse than a primitive. Intelligence is encoded in software, not hardware. Our language software has more breadth and depth than any one of us can create or contain.</div><br/><div id="38045124" class="c"><input type="checkbox" id="c-38045124" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#38044289">parent</a><span>|</span><a href="#38047413">next</a><span>|</span><label class="collapse" for="c-38045124">[-]</label><label class="expand" for="c-38045124">[12 more]</label></div><br/><div class="children"><div class="content">&gt; Similarly, humans achieve about the same results when they have the same training. Small variations. What matters is not the brain but the education they get.<p>That&#x27;s misleading. Small average variation overall, but some outliers are dramatically better and dramatically worse.  Some brains produce fields medals, some can barely count change. Unfortunate but true.</div><br/><div id="38048282" class="c"><input type="checkbox" id="c-38048282" checked=""/><div class="controls bullet"><span class="by">mrshadowgoose</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38045124">parent</a><span>|</span><a href="#38045249">next</a><span>|</span><label class="collapse" for="c-38048282">[-]</label><label class="expand" for="c-38048282">[1 more]</label></div><br/><div class="children"><div class="content">Over the years, I&#x27;ve found that the &quot;nurture is the only thing that matters&quot; camp is typically intensely self-delusional.<p>Despite hard evidence to the contrary, some people really really want to pretend that all human brains are equally capable.<p>For example, instances of cognitive disability due to macroscopically visible brain structure deviations or brain damage are usually brushed away as &quot;Well that&#x27;s just a disability and therefore it doesn&#x27;t count! All the other brains are the same!&quot;</div><br/></div></div><div id="38045249" class="c"><input type="checkbox" id="c-38045249" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38045124">parent</a><span>|</span><a href="#38048282">prev</a><span>|</span><a href="#38046459">next</a><span>|</span><label class="collapse" for="c-38045249">[-]</label><label class="expand" for="c-38045249">[3 more]</label></div><br/><div class="children"><div class="content">Yes but they all stood on the shoulders of giants. Alone they couldn&#x27;t have done it. Intelligence is a collective affair.<p>Ideas are self replicators - usually by being useful to people, but their lifecycle is different from that of people, they evolve much faster. It&#x27;s an evolutionary process. Modern man and that of 10,000 years ago are biologically not much different, the difference comes from cultural evolution.</div><br/><div id="38047258" class="c"><input type="checkbox" id="c-38047258" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38045249">parent</a><span>|</span><a href="#38047009">next</a><span>|</span><label class="collapse" for="c-38047258">[-]</label><label class="expand" for="c-38047258">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Alone they couldn&#x27;t have done it. Intelligence is a collective affair.<p>Yes, but the Gausses and Ramanujans of the world disprove the claim that architecture doesn&#x27;t matter, and that only training data matters. The long tails that describe human variance I think show that architecture can matter a lot.<p>I think you might be right that architecture for some machine learning algorithms doesn&#x27;t matter so much given enough data, but the fact that they need so much data to perform well I think implies that there&#x27;s a better architecture awaiting discovery.</div><br/></div></div><div id="38047009" class="c"><input type="checkbox" id="c-38047009" checked=""/><div class="controls bullet"><span class="by">smolder</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38045249">parent</a><span>|</span><a href="#38047258">prev</a><span>|</span><a href="#38046459">next</a><span>|</span><label class="collapse" for="c-38047009">[-]</label><label class="expand" for="c-38047009">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Ideas are self replicators<p>Maybe it&#x27;s worth pointing out: That&#x27;s a core idea of memetics, i.e. the study of memes, in the original sense of that word.</div><br/></div></div></div></div><div id="38046459" class="c"><input type="checkbox" id="c-38046459" checked=""/><div class="controls bullet"><span class="by">klipt</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38045124">parent</a><span>|</span><a href="#38045249">prev</a><span>|</span><a href="#38046787">next</a><span>|</span><label class="collapse" for="c-38046459">[-]</label><label class="expand" for="c-38046459">[6 more]</label></div><br/><div class="children"><div class="content">If you look at fields medalists I think most of them got interested in math super young and got exposed earlier to much more material than average mathematicians.<p>So they may have an IQ advantage, but they definitely also have a data&#x2F;training advantage.</div><br/><div id="38047242" class="c"><input type="checkbox" id="c-38047242" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38046459">parent</a><span>|</span><a href="#38047122">next</a><span>|</span><label class="collapse" for="c-38047242">[-]</label><label class="expand" for="c-38047242">[4 more]</label></div><br/><div class="children"><div class="content">&gt; If you look at fields medalists I think most of them got interested in math super young and got exposed earlier to much more material than average mathematicians<p>Even if that were true, you&#x27;d need only one Ramanujan to disprove the claim that architecture doesn&#x27;t matter. We&#x27;ve seen many more than one.</div><br/><div id="38047777" class="c"><input type="checkbox" id="c-38047777" checked=""/><div class="controls bullet"><span class="by">klipt</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38047242">parent</a><span>|</span><a href="#38047122">next</a><span>|</span><label class="collapse" for="c-38047777">[-]</label><label class="expand" for="c-38047777">[3 more]</label></div><br/><div class="children"><div class="content">It takes both, nature and nurture.<p>If Ramanujan were raised as a goatherd in an illiterate society I don&#x27;t think he would&#x27;ve accomplished much mathematically.</div><br/><div id="38047973" class="c"><input type="checkbox" id="c-38047973" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38047777">parent</a><span>|</span><a href="#38047122">next</a><span>|</span><label class="collapse" for="c-38047973">[-]</label><label class="expand" for="c-38047973">[2 more]</label></div><br/><div class="children"><div class="content">Not quite a goatherd, but Ramanujan&#x27;s father was a clerk in a sari shop. Ramanujan himself had little formal training in mathematics and was entirely self taught, and his research was in isolation until he sent an unsolicited  letter to G.H. Hardy.<p>It&#x27;s as if you trained an LLM on children&#x27;s books and it converged to Terence Tao.</div><br/><div id="38048191" class="c"><input type="checkbox" id="c-38048191" checked=""/><div class="controls bullet"><span class="by">syntheweave</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38047973">parent</a><span>|</span><a href="#38047122">next</a><span>|</span><label class="collapse" for="c-38048191">[-]</label><label class="expand" for="c-38048191">[1 more]</label></div><br/><div class="children"><div class="content">Describing Ramanujan as self-taught downplays that, within colonial India, he had access to some decent resources, and probably more than the median of the era: primary and secondary schooling, some interaction with older students, and  a few of the books on higher mathematics. That was enough to open up all the possibilities he needed to &quot;think mathematically&quot;. By his late teens it had become a total obsession, he ceased studying other subjects and failed out of academia. That is, the things he had <i>already</i> encountered were sufficient to get him started, and then he made things harder on himself because he couldn&#x27;t play by the rules. But he replaced that disadvantage with persistence and trial and error,  submitting what he had to whomever he could contact until, by the time it reached Hardy, it started to resemble formal language that other mathematicians understood.</div><br/></div></div></div></div></div></div></div></div><div id="38047122" class="c"><input type="checkbox" id="c-38047122" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38046459">parent</a><span>|</span><a href="#38047242">prev</a><span>|</span><a href="#38046787">next</a><span>|</span><label class="collapse" for="c-38047122">[-]</label><label class="expand" for="c-38047122">[1 more]</label></div><br/><div class="children"><div class="content">You need to be so smart that you jump the chasm between what school can teach you to what you can teach yourself from undergrad and beyond textbooks. Given the pressure to pass exams, in addition you need to have enough in the tank that learning more maths doesnât risk your hoop jumping to get into university. Ironically you need to do well in many things that isnât maths to be allowed to study maths at an elite university.<p>I am sure it helps alot if your parents are professors!</div><br/></div></div></div></div><div id="38046787" class="c"><input type="checkbox" id="c-38046787" checked=""/><div class="controls bullet"><span class="by">hiAndrewQuinn</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38045124">parent</a><span>|</span><a href="#38046459">prev</a><span>|</span><a href="#38047413">next</a><span>|</span><label class="collapse" for="c-38046787">[-]</label><label class="expand" for="c-38046787">[1 more]</label></div><br/><div class="children"><div class="content">See also: <a href="https:&#x2F;&#x2F;gwern.net&#x2F;iq" rel="nofollow noreferrer">https:&#x2F;&#x2F;gwern.net&#x2F;iq</a></div><br/></div></div></div></div><div id="38047413" class="c"><input type="checkbox" id="c-38047413" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38044289">parent</a><span>|</span><a href="#38045124">prev</a><span>|</span><a href="#38044898">next</a><span>|</span><label class="collapse" for="c-38047413">[-]</label><label class="expand" for="c-38047413">[1 more]</label></div><br/><div class="children"><div class="content">Part of what made the transformer so powerful when compared to RNNs and CNNs in NLP was the fact that they are highly parallelizable. That means you can optimize more parameters in the same amount of training time, which means you can model a more complex function.</div><br/></div></div><div id="38044898" class="c"><input type="checkbox" id="c-38044898" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#38044289">parent</a><span>|</span><a href="#38047413">prev</a><span>|</span><a href="#38046076">next</a><span>|</span><label class="collapse" for="c-38044898">[-]</label><label class="expand" for="c-38044898">[2 more]</label></div><br/><div class="children"><div class="content">I wish someone performed a large scale experiment to evaluate all these alternate architectures. I kind of feel that they get drowned out by new sota results from openai and others. What I wish is something that tries to see if emergent behaviors pop up with enough data and parameters.<p>Maybe vision is special enough that convnets and approache transformer level performance or it could be generalized to any modality. I haven&#x27;t read enough papers to know if someone has already done something like this but everywhere I look on the application side of things, vanilla transformers seems to be dominating</div><br/><div id="38045423" class="c"><input type="checkbox" id="c-38045423" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38044898">parent</a><span>|</span><a href="#38046076">next</a><span>|</span><label class="collapse" for="c-38045423">[-]</label><label class="expand" for="c-38045423">[1 more]</label></div><br/><div class="children"><div class="content">S4 &amp; the H3 paper are probably what you are looking for</div><br/></div></div></div></div><div id="38046076" class="c"><input type="checkbox" id="c-38046076" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#38044289">parent</a><span>|</span><a href="#38044898">prev</a><span>|</span><a href="#38047361">next</a><span>|</span><label class="collapse" for="c-38046076">[-]</label><label class="expand" for="c-38046076">[1 more]</label></div><br/><div class="children"><div class="content">We showed this in our original scaling law paper:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1712.00409.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1712.00409.pdf</a><p>The slope of the power law is determined by the problem and dataset.  Compute, parameter count, and data move you along the curve.  Change in architecture&#x2F;bias is a constant offset.<p>So architecture can give an advantage, but that advantage can be overcome by scale.</div><br/></div></div><div id="38047361" class="c"><input type="checkbox" id="c-38047361" checked=""/><div class="controls bullet"><span class="by">d0mine</span><span>|</span><a href="#38044289">parent</a><span>|</span><a href="#38046076">prev</a><span>|</span><a href="#38044509">next</a><span>|</span><label class="collapse" for="c-38047361">[-]</label><label class="expand" for="c-38047361">[1 more]</label></div><br/><div class="children"><div class="content">&gt; same results when they have the same training<p>Genetics may play a huge role.</div><br/></div></div><div id="38044509" class="c"><input type="checkbox" id="c-38044509" checked=""/><div class="controls bullet"><span class="by">kookamamie</span><span>|</span><a href="#38044289">parent</a><span>|</span><a href="#38047361">prev</a><span>|</span><a href="#38044640">next</a><span>|</span><label class="collapse" for="c-38044509">[-]</label><label class="expand" for="c-38044509">[4 more]</label></div><br/><div class="children"><div class="content">Dataset counts, but also the number of total parameters in the network, i.e. capacity.</div><br/><div id="38044614" class="c"><input type="checkbox" id="c-38044614" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38044509">parent</a><span>|</span><a href="#38045436">next</a><span>|</span><label class="collapse" for="c-38044614">[-]</label><label class="expand" for="c-38044614">[1 more]</label></div><br/><div class="children"><div class="content">Agreed, it&#x27;s in my first phrase &quot;as long as you can efficiently train models of the same size, what counts is the dataset&quot;. But useful sizes are just a few. 7, 13, 35, 70, 120B - because they are targeted to various families of GPUs. A 2T model I can&#x27;t run or too expensive to use on APIs is of no use. Not just dataset size, but data quality matters just as much, and diversity.<p>I believe LLMs will train mostly on synthetic data engineered to have extreme diversity and very high quality. This kind of data confers 5x gains in efficiency as demonstrated by Microsoft in the Phi-1.5 paper.</div><br/></div></div><div id="38045436" class="c"><input type="checkbox" id="c-38045436" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38044509">parent</a><span>|</span><a href="#38044614">prev</a><span>|</span><a href="#38044640">next</a><span>|</span><label class="collapse" for="c-38045436">[-]</label><label class="expand" for="c-38045436">[2 more]</label></div><br/><div class="children"><div class="content">There are really 3 axes - the number of tokens, the number of parameters, and the number of flops per pass.</div><br/><div id="38046703" class="c"><input type="checkbox" id="c-38046703" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#38044289">root</a><span>|</span><a href="#38045436">parent</a><span>|</span><a href="#38044640">next</a><span>|</span><label class="collapse" for="c-38046703">[-]</label><label class="expand" for="c-38046703">[1 more]</label></div><br/><div class="children"><div class="content">This says nothing about data quality.</div><br/></div></div></div></div></div></div><div id="38046912" class="c"><input type="checkbox" id="c-38046912" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#38044289">parent</a><span>|</span><a href="#38044640">prev</a><span>|</span><a href="#38044391">next</a><span>|</span><label class="collapse" for="c-38046912">[-]</label><label class="expand" for="c-38046912">[1 more]</label></div><br/><div class="children"><div class="content">I kind of agree, architecture does seem to matter in broad strokes but neuron-local learning mechanics will take it from there and the rest is up to the dataset.</div><br/></div></div></div></div><div id="38044391" class="c"><input type="checkbox" id="c-38044391" checked=""/><div class="controls bullet"><span class="by">gradascent</span><span>|</span><a href="#38044289">prev</a><span>|</span><a href="#38045608">next</a><span>|</span><label class="collapse" for="c-38044391">[-]</label><label class="expand" for="c-38044391">[3 more]</label></div><br/><div class="children"><div class="content">This is great, but what is a possible use-case of these massive classifier models? I&#x27;m guessing they won&#x27;t be running at the edge, which precludes them from real-time applications like self-driving cars, smartphones, or military. So then what? Facial recognition for police&#x2F;governments or targeted advertisement based on your Instagram&#x2F;Google photos? I&#x27;m genuinely curious.</div><br/><div id="38044855" class="c"><input type="checkbox" id="c-38044855" checked=""/><div class="controls bullet"><span class="by">currymj</span><span>|</span><a href="#38044391">parent</a><span>|</span><a href="#38044823">next</a><span>|</span><label class="collapse" for="c-38044855">[-]</label><label class="expand" for="c-38044855">[1 more]</label></div><br/><div class="children"><div class="content">1) it&#x27;s basic research, 2) you can always chop off the last layer and use the embeddings, which I guess might be useful for something</div><br/></div></div></div></div><div id="38045608" class="c"><input type="checkbox" id="c-38045608" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#38044391">prev</a><span>|</span><a href="#38046544">next</a><span>|</span><label class="collapse" for="c-38045608">[-]</label><label class="expand" for="c-38045608">[2 more]</label></div><br/><div class="children"><div class="content">This is nice because convolutional models seem better for some vision tasks like segmentation which are less obvious how to do with ViTs. Convolution seems like something you fundamentally want to do in order to model translation invariance in vision.</div><br/><div id="38046740" class="c"><input type="checkbox" id="c-38046740" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38045608">parent</a><span>|</span><a href="#38046544">next</a><span>|</span><label class="collapse" for="c-38046740">[-]</label><label class="expand" for="c-38046740">[1 more]</label></div><br/><div class="children"><div class="content">Segment Anything is a transformer<p><a href="https:&#x2F;&#x2F;segment-anything.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;segment-anything.com&#x2F;</a></div><br/></div></div></div></div><div id="38046544" class="c"><input type="checkbox" id="c-38046544" checked=""/><div class="controls bullet"><span class="by">matrix2596</span><span>|</span><a href="#38045608">prev</a><span>|</span><a href="#38044573">next</a><span>|</span><label class="collapse" for="c-38046544">[-]</label><label class="expand" for="c-38046544">[2 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t fully read the paper yet. Isn&#x27;t the strength of Vision Transformers in unsupervised learning, meaning that the data doesn&#x27;t need labels? And don&#x27;t ResNets require labeled data?</div><br/><div id="38047380" class="c"><input type="checkbox" id="c-38047380" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38046544">parent</a><span>|</span><a href="#38044573">next</a><span>|</span><label class="collapse" for="c-38047380">[-]</label><label class="expand" for="c-38047380">[1 more]</label></div><br/><div class="children"><div class="content">In the vision transformer paper they trained on cfar 10 and Imagenet which are supervised learning problems.</div><br/></div></div></div></div><div id="38044573" class="c"><input type="checkbox" id="c-38044573" checked=""/><div class="controls bullet"><span class="by">pjs_</span><span>|</span><a href="#38046544">prev</a><span>|</span><a href="#38043493">next</a><span>|</span><label class="collapse" for="c-38044573">[-]</label><label class="expand" for="c-38044573">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;external-preview.redd.it&#x2F;du7KQXLvBmVqc5G0T3tIEbWsYn8-qvtKTaMaZi7WaQ0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc212d5b696c4d8ad3e7bdeae271b92255c29ee4" rel="nofollow noreferrer">https:&#x2F;&#x2F;external-preview.redd.it&#x2F;du7KQXLvBmVqc5G0T3tIEbWsYn8...</a></div><br/></div></div><div id="38043493" class="c"><input type="checkbox" id="c-38043493" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#38044573">prev</a><span>|</span><label class="collapse" for="c-38043493">[-]</label><label class="expand" for="c-38043493">[7 more]</label></div><br/><div class="children"><div class="content">All machine learning is just convolution in the context of Hopf algebra convolution.</div><br/><div id="38043656" class="c"><input type="checkbox" id="c-38043656" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#38043493">parent</a><span>|</span><a href="#38043858">next</a><span>|</span><label class="collapse" for="c-38043656">[-]</label><label class="expand" for="c-38043656">[2 more]</label></div><br/><div class="children"><div class="content">Is this an intellectual leap aiming to make the field more cohesive, like the quest for unifying theories in physics?</div><br/><div id="38043926" class="c"><input type="checkbox" id="c-38043926" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#38043493">root</a><span>|</span><a href="#38043656">parent</a><span>|</span><a href="#38043858">next</a><span>|</span><label class="collapse" for="c-38043926">[-]</label><label class="expand" for="c-38043926">[1 more]</label></div><br/><div class="children"><div class="content">That is one of the goals, yes. In addition, it seems like you get neural architecture search (architecture is optimized), faster training, inference and interpretability. I&#x27;m working it out as we speak.<p>Ironically, convolution provides some unification in physics too, e.g. renormalization is a convolution.</div><br/></div></div></div></div><div id="38043858" class="c"><input type="checkbox" id="c-38043858" checked=""/><div class="controls bullet"><span class="by">dpflan</span><span>|</span><a href="#38043493">parent</a><span>|</span><a href="#38043656">prev</a><span>|</span><label class="collapse" for="c-38043858">[-]</label><label class="expand" for="c-38043858">[4 more]</label></div><br/><div class="children"><div class="content">Interesting, please do elaborate...</div><br/><div id="38044228" class="c"><input type="checkbox" id="c-38044228" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#38043493">root</a><span>|</span><a href="#38043858">parent</a><span>|</span><label class="collapse" for="c-38044228">[-]</label><label class="expand" for="c-38044228">[3 more]</label></div><br/><div class="children"><div class="content">I read it as a riff on âmonads are just monoids in the category of endofunctorsâ, but maybe that wasn&#x27;t intended.</div><br/><div id="38044280" class="c"><input type="checkbox" id="c-38044280" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#38043493">root</a><span>|</span><a href="#38044228">parent</a><span>|</span><label class="collapse" for="c-38044280">[-]</label><label class="expand" for="c-38044280">[2 more]</label></div><br/><div class="children"><div class="content">It kind of is. The commenter has been working on this formalism for a year or more. I&#x27;m sure he will come by with his link for the Discord channel where he discusses with and finds collaborators soon.</div><br/><div id="38044310" class="c"><input type="checkbox" id="c-38044310" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#38043493">root</a><span>|</span><a href="#38044280">parent</a><span>|</span><label class="collapse" for="c-38044310">[-]</label><label class="expand" for="c-38044310">[1 more]</label></div><br/><div class="children"><div class="content">I has been less than 9 months. But yeah there is a discord if you want to follow progress <a href="https:&#x2F;&#x2F;discord.cofunctional.ai" rel="nofollow noreferrer">https:&#x2F;&#x2F;discord.cofunctional.ai</a>.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>