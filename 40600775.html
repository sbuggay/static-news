<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1717750850254" as="style"/><link rel="stylesheet" href="styles.css?v=1717750850254"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.together.ai/blog/dragonfly-v1">Dragonfly: A large vision-language model with multi-resolution zoom</a> <span class="domain">(<a href="https://www.together.ai">www.together.ai</a>)</span></div><div class="subtext"><span>jasondavies</span> | <span>25 comments</span></div><br/><div><div id="40604380" class="c"><input type="checkbox" id="c-40604380" checked=""/><div class="controls bullet"><span class="by">davidhyde</span><span>|</span><a href="#40601520">next</a><span>|</span><label class="collapse" for="c-40604380">[-]</label><label class="expand" for="c-40604380">[3 more]</label></div><br/><div class="children"><div class="content">&gt; “ Question: Write a detailed radiology note based on the chest X-ray.
 Gold Answer: AP upright and lateral views of the chest were provided. Left chest wall pacer pack is again seen with leads extending into the right heart. ”<p>The bit about a “wall pacer pack is again seen…” leads me to believe this was based on another doctors note about a similar looking X-ray which was probably paired with other information like another scan at the time. That would be problematic imo.</div><br/><div id="40605320" class="c"><input type="checkbox" id="c-40605320" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#40604380">parent</a><span>|</span><a href="#40601520">next</a><span>|</span><label class="collapse" for="c-40605320">[-]</label><label class="expand" for="c-40605320">[2 more]</label></div><br/><div class="children"><div class="content">Problematic for the functionality? If it works well enough, I’m pretty fine with them stealing data to create a useful medical tool.</div><br/><div id="40606661" class="c"><input type="checkbox" id="c-40606661" checked=""/><div class="controls bullet"><span class="by">davidhyde</span><span>|</span><a href="#40604380">root</a><span>|</span><a href="#40605320">parent</a><span>|</span><a href="#40601520">next</a><span>|</span><label class="collapse" for="c-40606661">[-]</label><label class="expand" for="c-40606661">[1 more]</label></div><br/><div class="children"><div class="content">It’s problematic because the LLM is describing another person’s scan and not the one presented to it. It should at least present the other scan as workings and the percentage difference between the two. Finding a similar looking scan is very useful no doubt but if the result is hallucinated that it is less so. Dangerous even. There is no confidence percentage and there should be.</div><br/></div></div></div></div></div></div><div id="40601520" class="c"><input type="checkbox" id="c-40601520" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#40604380">prev</a><span>|</span><a href="#40601338">next</a><span>|</span><label class="collapse" for="c-40601520">[-]</label><label class="expand" for="c-40601520">[2 more]</label></div><br/><div class="children"><div class="content">Ive been sorta following together.ai for a while. Cool company. Is this available to be used by anyone atm? Could I potentially use the model to look at my own chest xrays (I&#x27;ve had a lot)?</div><br/><div id="40602097" class="c"><input type="checkbox" id="c-40602097" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40601520">parent</a><span>|</span><a href="#40601338">next</a><span>|</span><label class="collapse" for="c-40602097">[-]</label><label class="expand" for="c-40602097">[1 more]</label></div><br/><div class="children"><div class="content">They have released the models <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;togethercomputer&#x2F;Llama-3-8B-Dragonfly-Med-v1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;togethercomputer&#x2F;Llama-3-8B-Dragonfly...</a></div><br/></div></div></div></div><div id="40601338" class="c"><input type="checkbox" id="c-40601338" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40601520">prev</a><span>|</span><a href="#40601408">next</a><span>|</span><label class="collapse" for="c-40601338">[-]</label><label class="expand" for="c-40601338">[1 more]</label></div><br/><div class="children"><div class="content">I have been testing out LLMs with the together.ai API, but I can&#x27;t figure out how to use the multimodal models with the API. I don&#x27;t see any in their model list.</div><br/></div></div><div id="40601408" class="c"><input type="checkbox" id="c-40601408" checked=""/><div class="controls bullet"><span class="by">achristmascarl</span><span>|</span><a href="#40601338">prev</a><span>|</span><a href="#40604734">next</a><span>|</span><label class="collapse" for="c-40601408">[-]</label><label class="expand" for="c-40601408">[3 more]</label></div><br/><div class="children"><div class="content">For the model fine-tuned on biomedical image data, does anyone with domain knowledge know how the model&#x27;s answers compare to the &quot;Gold&quot; answers?</div><br/><div id="40602566" class="c"><input type="checkbox" id="c-40602566" checked=""/><div class="controls bullet"><span class="by">rrsp</span><span>|</span><a href="#40601408">parent</a><span>|</span><a href="#40601504">next</a><span>|</span><label class="collapse" for="c-40602566">[-]</label><label class="expand" for="c-40602566">[1 more]</label></div><br/><div class="children"><div class="content">Both the ‘gold’ answer and the model reference a PA and AP view respectively as well as a lateral chest radiograph. The picture only contains a lateral radiograph though.</div><br/></div></div><div id="40601504" class="c"><input type="checkbox" id="c-40601504" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#40601408">parent</a><span>|</span><a href="#40602566">prev</a><span>|</span><a href="#40604734">next</a><span>|</span><label class="collapse" for="c-40601504">[-]</label><label class="expand" for="c-40601504">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have exact domain knowledge but I&#x27;m fairly certain this type of tech has already been employed to do some of the heavy lifting for radiologists reviewing imaging results.</div><br/></div></div></div></div><div id="40604734" class="c"><input type="checkbox" id="c-40604734" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40601408">prev</a><span>|</span><a href="#40601400">next</a><span>|</span><label class="collapse" for="c-40604734">[-]</label><label class="expand" for="c-40604734">[1 more]</label></div><br/><div class="children"><div class="content">Is there a comparable service for audio analysis?</div><br/></div></div><div id="40601400" class="c"><input type="checkbox" id="c-40601400" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#40604734">prev</a><span>|</span><a href="#40603545">next</a><span>|</span><label class="collapse" for="c-40601400">[-]</label><label class="expand" for="c-40601400">[5 more]</label></div><br/><div class="children"><div class="content">Is there a demo or API to test the model? There are so many vision language models these days, it&#x27;s hard to say which one is better, they also use in many cases different benchmarks.</div><br/><div id="40601748" class="c"><input type="checkbox" id="c-40601748" checked=""/><div class="controls bullet"><span class="by">alexey-salmin</span><span>|</span><a href="#40601400">parent</a><span>|</span><a href="#40604615">next</a><span>|</span><label class="collapse" for="c-40601748">[-]</label><label class="expand" for="c-40601748">[2 more]</label></div><br/><div class="children"><div class="content">Huggingface links are in the article 
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;togethercomputer&#x2F;Llama-3-8B-Dragonfly-v1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;togethercomputer&#x2F;Llama-3-8B-Dragonfly...</a></div><br/><div id="40601849" class="c"><input type="checkbox" id="c-40601849" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#40601400">root</a><span>|</span><a href="#40601748">parent</a><span>|</span><a href="#40604615">next</a><span>|</span><label class="collapse" for="c-40601849">[-]</label><label class="expand" for="c-40601849">[1 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t asking about running the model locally, also for that I have to wait for someone to quantize the model.</div><br/></div></div></div></div><div id="40604615" class="c"><input type="checkbox" id="c-40604615" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#40601400">parent</a><span>|</span><a href="#40601748">prev</a><span>|</span><a href="#40603545">next</a><span>|</span><label class="collapse" for="c-40604615">[-]</label><label class="expand" for="c-40604615">[2 more]</label></div><br/><div class="children"><div class="content">together.ai started as an API-based inference provider, so I bet you can target these models directly with your API credits there.</div><br/><div id="40605622" class="c"><input type="checkbox" id="c-40605622" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40601400">root</a><span>|</span><a href="#40604615">parent</a><span>|</span><a href="#40603545">next</a><span>|</span><label class="collapse" for="c-40605622">[-]</label><label class="expand" for="c-40605622">[1 more]</label></div><br/><div class="children"><div class="content">How? I don&#x27;t see any vision-language model in their model list for the API.</div><br/></div></div></div></div></div></div><div id="40603545" class="c"><input type="checkbox" id="c-40603545" checked=""/><div class="controls bullet"><span class="by">darby_nine</span><span>|</span><a href="#40601400">prev</a><span>|</span><label class="collapse" for="c-40603545">[-]</label><label class="expand" for="c-40603545">[9 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t speak for others obviously, but this sort of caption is nauseous:<p>&gt;  In the heart of a vibrant skatepark, a skateboarder is caught in a moment of pure exhilaration. The skateboarder, dressed in a black t-shirt adorned with a yellow graphic and black pants, is suspended in mid-air, performing an impressive trick on a concrete ramp. The skateboarder&#x27;s arms are outstretched, adding balance to the daring stunt. The skatepark itself is a concrete playground, with the skateboarder&#x27;s ramp being the main focus. In the background, palm trees sway gently, adding a touch of nature to the urban setting. A few spectators can be seen in the distance, their attention riveted on the airborne skateboarder. The image captures not just a moment, but a story of skill, courage, and the joy of skateboarding.<p>This seems a lot more like a puff piece from a local publisher trying to fill space, or description of a stock photo to an advertiser, than a description I&#x27;d describe as accurate from a human to another human.</div><br/><div id="40605111" class="c"><input type="checkbox" id="c-40605111" checked=""/><div class="controls bullet"><span class="by">throw310822</span><span>|</span><a href="#40603545">parent</a><span>|</span><a href="#40604149">next</a><span>|</span><label class="collapse" for="c-40605111">[-]</label><label class="expand" for="c-40605111">[1 more]</label></div><br/><div class="children"><div class="content">Came to say the same. Might be the task &quot;describe the picture&quot; that puts it into that mode; however I still hope that no human being would really write such tosh.</div><br/></div></div><div id="40604149" class="c"><input type="checkbox" id="c-40604149" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#40603545">parent</a><span>|</span><a href="#40605111">prev</a><span>|</span><a href="#40603747">next</a><span>|</span><label class="collapse" for="c-40604149">[-]</label><label class="expand" for="c-40604149">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, this is my number one complaint with all recent open source vision models, and it seems like it is only getting worse. It&#x27;s verbose to the point of parody, making it extremely difficult to evaluate what it can actually _see_, and what it&#x27;s just dumbly markov-chaining based on previous text tokens.<p>In GPT4V, you can prompt around this if you know about it, but none of the people collecting datasets for open models appear know or care to apply that, and so we just get this default GPT4V contamination everywhere.<p>The only vision model I enjoy is Google Gemini, simply because it will give you a no-nonsense caption. Of course it still hallucinates things that are not there, but getting a color or object wrong is orders of magnitude less bad than having 3 sentences that have nothing to do with the image.</div><br/><div id="40606160" class="c"><input type="checkbox" id="c-40606160" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#40603545">root</a><span>|</span><a href="#40604149">parent</a><span>|</span><a href="#40603747">next</a><span>|</span><label class="collapse" for="c-40606160">[-]</label><label class="expand" for="c-40606160">[1 more]</label></div><br/><div class="children"><div class="content">I got chatGPT to say<p>&gt; A skateboarder attempts an awkward mid-trick in an average skate park, with distracted onlookers.<p>about the image, but where&#x27;s the fun in that,</div><br/></div></div></div></div><div id="40604062" class="c"><input type="checkbox" id="c-40604062" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40603545">parent</a><span>|</span><a href="#40603747">prev</a><span>|</span><label class="collapse" for="c-40604062">[-]</label><label class="expand" for="c-40604062">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the price of getting an image that accurately represents what you had in mind. Otherwise you could just prompt it with &quot;skateboarder in a skatepark&quot;.</div><br/><div id="40604229" class="c"><input type="checkbox" id="c-40604229" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#40603545">root</a><span>|</span><a href="#40604062">parent</a><span>|</span><a href="#40604490">next</a><span>|</span><label class="collapse" for="c-40604229">[-]</label><label class="expand" for="c-40604229">[2 more]</label></div><br/><div class="children"><div class="content">&gt;In the heart of a vibrant skatepark,<p>Vibrant? It&#x27;s made of white concrete.<p>&gt;a skateboarder is caught in a moment of pure exhilaration.<p>Oh, great we&#x27;re mind-reading from pictures now? His face isn&#x27;t even visible.<p>&gt; adding balance to the daring stunt.<p>What incredible commentary.<p>&gt;palm trees sway gently, adding a touch of nature to the urban setting.<p>The trees aren&#x27;t swaying. But because sometimes trees sway, I guess it&#x27;ll just say it.<p>&gt; A few spectators can be seen in the distance, their attention riveted on the airborne skateboarder<p>The spectators look more like they are just glancing over, nowhere near &quot;riveted&quot;.<p>&gt; The image captures not just a moment, but a story of skill, courage, and the joy of skateboarding.<p>This adds literally nothing.</div><br/><div id="40606038" class="c"><input type="checkbox" id="c-40606038" checked=""/><div class="controls bullet"><span class="by">zimpenfish</span><span>|</span><a href="#40603545">root</a><span>|</span><a href="#40604229">parent</a><span>|</span><a href="#40604490">next</a><span>|</span><label class="collapse" for="c-40606038">[-]</label><label class="expand" for="c-40606038">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Vibrant? It&#x27;s made of white concrete.<p>&quot;vibrant&quot; could apply to the activity, not the physical structure.  There&#x27;s 4 other people in the back of the photo - if you assumed it was a tiny slice of the park, you could say it was vibrant.<p>(to me it doesn&#x27;t look particularly vibrant re: activity but this is just one small corner and I will allow the ML some leeway in its floridity.)</div><br/></div></div></div></div><div id="40604490" class="c"><input type="checkbox" id="c-40604490" checked=""/><div class="controls bullet"><span class="by">darby_nine</span><span>|</span><a href="#40603545">root</a><span>|</span><a href="#40604062">parent</a><span>|</span><a href="#40604229">prev</a><span>|</span><label class="collapse" for="c-40604490">[-]</label><label class="expand" for="c-40604490">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t accurately represent the photo, hence the issue at hand. In many ways &quot;skateboarder in a park&quot; is more accurate than the large number of small inaccuracies this description manages to accrue. (Like many humans, but still the verbosity is very odd for the little detail it actually conveys!)<p>I&#x27;m not trying to argue against the idea of ai-generated titling, just that the product is very inferior to what even mild diletantes of the field might expect from advertised capabilities.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>