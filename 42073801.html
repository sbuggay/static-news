<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730970052096" as="style"/><link rel="stylesheet" href="styles.css?v=1730970052096"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2406.03689">Evaluating the world model implicit in a generative model</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>dsubburam</span> | <span>11 comments</span></div><br/><div><div id="42074486" class="c"><input type="checkbox" id="c-42074486" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42074518">next</a><span>|</span><label class="collapse" for="c-42074486">[-]</label><label class="expand" for="c-42074486">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen some very impressive results just embedding a pre-trained KGE model into a transformer model, and letting it &quot;learn&quot; to query it (I&#x27;ve just used heterogenous loss functions during training with &quot;classifier dimensions&quot; that determine whether to greedily sample from the KGE sidecar, I&#x27;m sure there are much better ways of doing this.). This is just subjective viewpoint obviously, but I&#x27;ve played around quite a lot with this idea, and it&#x27;s very easy to get a an &quot;interactive&quot; small LLM with stable results doing such a thing, the only problem I&#x27;ve found is _updating_ the knowledge cheaply without partially retraining the LLM itself. For small, domain-specific models this isn&#x27;t really an issue though - for personal projects I just use a couple 3090s.<p>I think this stuff will become a <i>lot</i> more fascinating after transformers have bottomed out on their hype curve and become a <i>tool</i> when building specific types of models.</div><br/><div id="42074579" class="c"><input type="checkbox" id="c-42074579" checked=""/><div class="controls bullet"><span class="by">aix1</span><span>|</span><a href="#42074486">parent</a><span>|</span><a href="#42074518">next</a><span>|</span><label class="collapse" for="c-42074579">[-]</label><label class="expand" for="c-42074579">[5 more]</label></div><br/><div class="children"><div class="content">&gt; embedding a pre-trained KGE model into a transformer model<p>Do you have any good pointers (literature, code etc) on the mechanics of this?</div><br/><div id="42074657" class="c"><input type="checkbox" id="c-42074657" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42074486">root</a><span>|</span><a href="#42074579">parent</a><span>|</span><a href="#42074790">next</a><span>|</span><label class="collapse" for="c-42074657">[-]</label><label class="expand" for="c-42074657">[2 more]</label></div><br/><div class="children"><div class="content">Check out PyKEEN [0] and go wild. I like to train a bunch of random models and &quot;overfit&quot; them to the extreme (in my mind overfitting them is the <i>point</i> for this task, you want dense, compressed knowledge). Resize the input and output embeddings of an existing pretrained (but small) LLM (input only necessary if you&#x27;re adding extra metadata on input, but make sure you untie input&#x2F;output weights). You can add a linear layer extension to the transformer blocks, pass it up as some sort of residual, etc. - honestly just find a way to shove it in, detach the KGE from the computation graph and add something learnable between it and wherever you&#x27;re connecting it - like just a couple linear layers and a ReLU. The output side is more important, you can have some indicator logit(s) to determine whether to &quot;read&quot; from the detached graph or sample the outputs of the LLM. Or just always do both and interpret it.<p>(like tinyllama or smaller, or just use whatever karpathy repo is most fun at the moment and train some gpt2 equivalent)<p>[0] <a href="https:&#x2F;&#x2F;pykeen.readthedocs.io&#x2F;en&#x2F;stable&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;pykeen.readthedocs.io&#x2F;en&#x2F;stable&#x2F;index.html</a></div><br/><div id="42074723" class="c"><input type="checkbox" id="c-42074723" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42074486">root</a><span>|</span><a href="#42074657">parent</a><span>|</span><a href="#42074790">next</a><span>|</span><label class="collapse" for="c-42074723">[-]</label><label class="expand" for="c-42074723">[1 more]</label></div><br/><div class="children"><div class="content">Sorry if that was ridiculously vague. I don&#x27;t know a ton about the state of the art, and I&#x27;m really not sure there <i>is</i> one - the papers just seem to get more terminology-dense and the research mostly just seems to end up developing new terminology. My grug-brained philosophy is just to make models small enough you can just shove things in and iterate quick enough in colab or a locally hosted notebook with access to a couple 3090s, or even just modern Ryzen&#x2F;EPYC cores. I like to &quot;evaluate&quot; the raw model using pyro-ppl to do MCMC or SVI on the raw logits on a known holdout dataset.<p>Really always happy to chat about this stuff, with anybody. Would love to explore ideas here, it&#x27;s a fun hobby, and we&#x27;re living in a golden age of open-source structured datasets. I haven&#x27;t actually found a community interested specifically in static knowledge injection. Email in profile, in (ebg_13 encoded).</div><br/></div></div></div></div><div id="42074790" class="c"><input type="checkbox" id="c-42074790" checked=""/><div class="controls bullet"><span class="by">napsternxg</span><span>|</span><a href="#42074486">root</a><span>|</span><a href="#42074579">parent</a><span>|</span><a href="#42074657">prev</a><span>|</span><a href="#42074518">next</a><span>|</span><label class="collapse" for="c-42074790">[-]</label><label class="expand" for="c-42074790">[2 more]</label></div><br/><div class="children"><div class="content">We also did something similar in our NTULM paper at Twitter <a href="https:&#x2F;&#x2F;youtu.be&#x2F;BjAmQjs0sZk?si=PBQyEGBx1MSkeUpX" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;BjAmQjs0sZk?si=PBQyEGBx1MSkeUpX</a><p>Used in non generative language models like BERT but should help with generative models as well.</div><br/><div id="42074831" class="c"><input type="checkbox" id="c-42074831" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42074486">root</a><span>|</span><a href="#42074790">parent</a><span>|</span><a href="#42074518">next</a><span>|</span><label class="collapse" for="c-42074831">[-]</label><label class="expand" for="c-42074831">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing! I&#x27;ll give it a read tomorrow - I do not appear to have read this. I really do wish there were good places for randos like me to discuss this stuff casually. I&#x27;m in so many slack, discord, etc. channels but none of them have the same intensity and hyperfocus as certain IRC channels of yore.</div><br/></div></div></div></div></div></div></div></div><div id="42074518" class="c"><input type="checkbox" id="c-42074518" checked=""/><div class="controls bullet"><span class="by">isaacfrond</span><span>|</span><a href="#42074486">prev</a><span>|</span><a href="#42074317">next</a><span>|</span><label class="collapse" for="c-42074518">[-]</label><label class="expand" for="c-42074518">[2 more]</label></div><br/><div class="children"><div class="content">I think there is a philosophical angle to this. I mean, <i>my</i> world map was constructed by chance interactions with the real world. Does this mean that the my world map is a close to the real world map, as their NN&#x27;s map is to Manhattan? Is my world map full of non-existent streets, exits that are at the wrong place, etc. The NN map of Manhattan works almost 100% correctly when used for normal navigation but breaks apart badly when it has to plan a detour. How brittle is my world map?</div><br/><div id="42074860" class="c"><input type="checkbox" id="c-42074860" checked=""/><div class="controls bullet"><span class="by">cen4</span><span>|</span><a href="#42074518">parent</a><span>|</span><a href="#42074317">next</a><span>|</span><label class="collapse" for="c-42074860">[-]</label><label class="expand" for="c-42074860">[1 more]</label></div><br/><div class="children"><div class="content">Also things are not static in the real world.</div><br/></div></div></div></div><div id="42074317" class="c"><input type="checkbox" id="c-42074317" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42074518">prev</a><span>|</span><label class="collapse" for="c-42074317">[-]</label><label class="expand" for="c-42074317">[2 more]</label></div><br/><div class="children"><div class="content">Wrong as it is, I&#x27;m impressed they were able to get any maps out of their LLM that look vaguely cohesive. The shortest path map has bits of streets downtown and around Central Park that aren&#x27;t totally red, and Central Park itself is clear on all 3 maps.<p>They used eight A100s, but don&#x27;t say how long it took to train their LLM. It would be interesting to know the wall clock time they spent. Their dataset is, relatively speaking, tiny which means it should take fewer resources to replicate from scratch.<p>What&#x27;s interesting though is that the Smalley model performed better, though they don&#x27;t speculate why that is.</div><br/><div id="42074577" class="c"><input type="checkbox" id="c-42074577" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42074317">parent</a><span>|</span><label class="collapse" for="c-42074577">[-]</label><label class="expand" for="c-42074577">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t imagine training took more than a day with 8 A100 even with that vocab size [0] (does lightning do implicit vocab extension maybe?) and a batch size of 1 [1] or 64 [2] or 4096 [3] (I have not trawled through the repo and other wordk enough to see what they are actually using in the paper, and let&#x27;s be real - we&#x27;ve all copied random min&#x2F;nano&#x2F;whatever GPT forks and not bothered renaming stuff). They mentioned their dataset is 120 million tokens, which is miniscule by transformer standards. Even with a more graph-based model making it 10X+ longer to train, 1.20 billion tokens per epoch equivalent shouldn&#x27;t take more than a couple hours with no optimization.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;keyonvafa&#x2F;world-model-evaluation&#x2F;blob&#x2F;949a9e7a436385c33fa58410f8e5e8ec3ca1c766&#x2F;model.py#L59">https:&#x2F;&#x2F;github.com&#x2F;keyonvafa&#x2F;world-model-evaluation&#x2F;blob&#x2F;949...</a>
[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;keyonvafa&#x2F;world-model-evaluation&#x2F;blob&#x2F;949a9e7a436385c33fa58410f8e5e8ec3ca1c766&#x2F;othello_world&#x2F;train_probe_othello.py#L89">https:&#x2F;&#x2F;github.com&#x2F;keyonvafa&#x2F;world-model-evaluation&#x2F;blob&#x2F;949...</a>
[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;keyonvafa&#x2F;world-model-evaluation&#x2F;blob&#x2F;949a9e7a436385c33fa58410f8e5e8ec3ca1c766&#x2F;othello_world&#x2F;mingpt&#x2F;trainer.py#L22">https:&#x2F;&#x2F;github.com&#x2F;keyonvafa&#x2F;world-model-evaluation&#x2F;blob&#x2F;949...</a>
[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;keyonvafa&#x2F;world-model-evaluation&#x2F;blob&#x2F;main&#x2F;othello_world&#x2F;train_gpt_othello.ipynb">https:&#x2F;&#x2F;github.com&#x2F;keyonvafa&#x2F;world-model-evaluation&#x2F;blob&#x2F;mai...</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>