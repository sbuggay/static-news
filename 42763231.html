<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737363652207" as="style"/><link rel="stylesheet" href="styles.css?v=1737363652207"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.lesswrong.com/posts/cu2E8wgmbdZbqeWqb/meemi-s-shortform">FrontierMath was funded by OpenAI</a> <span class="domain">(<a href="https://www.lesswrong.com">www.lesswrong.com</a>)</span></div><div class="subtext"><span>wujerry2000</span> | <span>113 comments</span></div><br/><div><div id="42763463" class="c"><input type="checkbox" id="c-42763463" checked=""/><div class="controls bullet"><span class="by">agnosticmantis</span><span>|</span><a href="#42763450">next</a><span>|</span><label class="collapse" for="c-42763463">[-]</label><label class="expand" for="c-42763463">[39 more]</label></div><br/><div class="children"><div class="content">“… we have a verbal agreement that these materials will not be used in model training”<p>Ha ha ha. Even written agreements are routinely violated as long as the potential upside &gt; downside, and all you have is verbal agreement? And you didn’t disclose this?<p>At the time o3 was released I wrote “this is so impressive that it brings out the pessimist in me”[0], thinking perhaps they were routing API calls to human workers.<p>Now we see in reality I should’ve been more cynical, as they had access to the benchmark data but verbally agreed (wink wink) not to train on it.<p>[0: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;threads?id=agnosticmantis#42476268">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;threads?id=agnosticmantis#42476...</a> ]</div><br/><div id="42763741" class="c"><input type="checkbox" id="c-42763741" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#42763463">parent</a><span>|</span><a href="#42763526">next</a><span>|</span><label class="collapse" for="c-42763741">[-]</label><label class="expand" for="c-42763741">[1 more]</label></div><br/><div class="children"><div class="content">You can still game a test set without training on it, that’s why you usually have a validation set and a test set that you ideally seldom use. Routinely running an evaluation on the test set can get the humans in the loop to overfit the data</div><br/></div></div><div id="42763526" class="c"><input type="checkbox" id="c-42763526" checked=""/><div class="controls bullet"><span class="by">asadotzler</span><span>|</span><a href="#42763463">parent</a><span>|</span><a href="#42763741">prev</a><span>|</span><a href="#42765059">next</a><span>|</span><label class="collapse" for="c-42763526">[-]</label><label class="expand" for="c-42763526">[27 more]</label></div><br/><div class="children"><div class="content">OpenAI doesn&#x27;t respect copyright so why would they let a verbal agreement get in the way of billion$</div><br/><div id="42763748" class="c"><input type="checkbox" id="c-42763748" checked=""/><div class="controls bullet"><span class="by">Rebuff5007</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763526">parent</a><span>|</span><a href="#42765059">next</a><span>|</span><label class="collapse" for="c-42763748">[-]</label><label class="expand" for="c-42763748">[26 more]</label></div><br/><div class="children"><div class="content">Can somehow explain to me how they can simply not respect copyright and get away with it? Also is this a uniquely open-ai problem, or also true of the other llm makers?</div><br/><div id="42764115" class="c"><input type="checkbox" id="c-42764115" checked=""/><div class="controls bullet"><span class="by">pseudo0</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42763982">next</a><span>|</span><label class="collapse" for="c-42764115">[-]</label><label class="expand" for="c-42764115">[1 more]</label></div><br/><div class="children"><div class="content">Their argument is that using copyrighted data for training is transformative, and therefore a form of fair use. There are a number of ongoing lawsuits related to this issue, but so far the AI companies seem to be mostly winning. Eg. <a href="https:&#x2F;&#x2F;www.reuters.com&#x2F;legal&#x2F;litigation&#x2F;openai-gets-partial-win-authors-us-copyright-lawsuit-2024-02-13&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reuters.com&#x2F;legal&#x2F;litigation&#x2F;openai-gets-partial...</a><p>Some artists also tried to sue Stable Diffusion in Andersen v. Stability AI, and so far it looks like it&#x27;s not going anywhere.<p>In the long run I bet we will see licensing deals between the big AI players and the large copyright holders to throw a bit of money their way, in order to make it difficult for new entrants to get training data. Eg. Reddit locking down API access and selling their data to Google.</div><br/></div></div><div id="42763982" class="c"><input type="checkbox" id="c-42763982" checked=""/><div class="controls bullet"><span class="by">ThrowawayR2</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42764115">prev</a><span>|</span><a href="#42763845">next</a><span>|</span><label class="collapse" for="c-42763982">[-]</label><label class="expand" for="c-42763982">[3 more]</label></div><br/><div class="children"><div class="content">The FSF funded some white papers a while ago on CoPilot: <a href="https:&#x2F;&#x2F;www.fsf.org&#x2F;news&#x2F;publication-of-the-fsf-funded-white-papers-on-questions-around-copilot" rel="nofollow">https:&#x2F;&#x2F;www.fsf.org&#x2F;news&#x2F;publication-of-the-fsf-funded-white...</a>.  Take a look at the analysis by two academics versed in law at <a href="https:&#x2F;&#x2F;www.fsf.org&#x2F;licensing&#x2F;copilot&#x2F;copyright-implications-of-the-use-of-code-repositories-to-train-a-machine-learning-model" rel="nofollow">https:&#x2F;&#x2F;www.fsf.org&#x2F;licensing&#x2F;copilot&#x2F;copyright-implications...</a> starting with §II.B that explains why it might be legal.<p>Bradley Kuhn also has a differing opinion in another whitepaper there (<a href="https:&#x2F;&#x2F;www.fsf.org&#x2F;licensing&#x2F;copilot&#x2F;if-software-is-my-copilot-who-programmed-my-software" rel="nofollow">https:&#x2F;&#x2F;www.fsf.org&#x2F;licensing&#x2F;copilot&#x2F;if-software-is-my-copi...</a>) but then again he studied CS, not law.  Nor has the FSF attempted AFAIK to file any suits even though they likely would have if it were an open and shut case.</div><br/><div id="42764453" class="c"><input type="checkbox" id="c-42764453" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763982">parent</a><span>|</span><a href="#42763845">next</a><span>|</span><label class="collapse" for="c-42764453">[-]</label><label class="expand" for="c-42764453">[2 more]</label></div><br/><div class="children"><div class="content">All of the most capable models I use have been clearly trained on the entirety of libgen&#x2F;z-lib. You know it is the first thing they did, it is like 100TB.<p>Some of the models are even coy about it.</div><br/><div id="42766286" class="c"><input type="checkbox" id="c-42766286" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764453">parent</a><span>|</span><a href="#42763845">next</a><span>|</span><label class="collapse" for="c-42766286">[-]</label><label class="expand" for="c-42766286">[1 more]</label></div><br/><div class="children"><div class="content">The models are not self aware of their training data. They are only aware of what the internet has said about previous models’ training data.</div><br/></div></div></div></div></div></div><div id="42763845" class="c"><input type="checkbox" id="c-42763845" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42763982">prev</a><span>|</span><a href="#42764059">next</a><span>|</span><label class="collapse" for="c-42763845">[-]</label><label class="expand" for="c-42763845">[8 more]</label></div><br/><div class="children"><div class="content">A lot of people <i>want</i> AI training to be in breach of copyright somehow, to the point of ignoring the likely outcomes if that were made law. Copyright law is their big cudgel for removing the thing they hate.<p>However, while it isn&#x27;t fully settled yet, at the moment it does not appear to be the case.</div><br/><div id="42764096" class="c"><input type="checkbox" id="c-42764096" checked=""/><div class="controls bullet"><span class="by">elashri</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763845">parent</a><span>|</span><a href="#42765090">next</a><span>|</span><label class="collapse" for="c-42764096">[-]</label><label class="expand" for="c-42764096">[6 more]</label></div><br/><div class="children"><div class="content">A lot of people have problem with selective enforcement of copyright law. Yes, changing them because it is captured by greedy cooperations would be something many would welcome. But currently the problem is that for normal folks doing what openai is doing they would be crushed (metaphorically) under the current copyright law.<p>So it is not like all people who problems with openAI is big cudgel. Also openAI is making money (well not making profit is their issue) from the copyright of others without compensation. Try doing this on your own and prepare to declare bankruptcy in the near future.</div><br/><div id="42764166" class="c"><input type="checkbox" id="c-42764166" checked=""/><div class="controls bullet"><span class="by">cmeacham98</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764096">parent</a><span>|</span><a href="#42765090">next</a><span>|</span><label class="collapse" for="c-42764166">[-]</label><label class="expand" for="c-42764166">[5 more]</label></div><br/><div class="children"><div class="content">Can you give an example of a copyright lawsuit lost by a &#x27;normal person&#x27; that&#x27;s doing the same thing OpenAI is?</div><br/><div id="42764411" class="c"><input type="checkbox" id="c-42764411" checked=""/><div class="controls bullet"><span class="by">elashri</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764166">parent</a><span>|</span><a href="#42765090">next</a><span>|</span><label class="collapse" for="c-42764411">[-]</label><label class="expand" for="c-42764411">[4 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;journa.host&#x2F;@jeremiak&#x2F;113811327999722586" rel="nofollow">https:&#x2F;&#x2F;journa.host&#x2F;@jeremiak&#x2F;113811327999722586</a></div><br/><div id="42765064" class="c"><input type="checkbox" id="c-42765064" checked=""/><div class="controls bullet"><span class="by">adwn</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764411">parent</a><span>|</span><a href="#42765080">next</a><span>|</span><label class="collapse" for="c-42765064">[-]</label><label class="expand" for="c-42765064">[2 more]</label></div><br/><div class="children"><div class="content">No, that is not an example for <i>&quot;&#x27;normal person&#x27; that&#x27;s doing the same thing OpenAI is&quot;</i>. OpenAI aren&#x27;t distributing the copyrighted works, so those aren&#x27;t the same situations.<p>Note that this doesn&#x27;t necessarily mean that one is in the right and one is in the wrong, just that they&#x27;re different from a legal point of view.</div><br/><div id="42766230" class="c"><input type="checkbox" id="c-42766230" checked=""/><div class="controls bullet"><span class="by">BeefWellington</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42765064">parent</a><span>|</span><a href="#42765080">next</a><span>|</span><label class="collapse" for="c-42766230">[-]</label><label class="expand" for="c-42766230">[1 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI aren&#x27;t distributing the copyrighted works, so those aren&#x27;t the same situations.<p>What do you call it when you run a service on the Internet that outputs copyrighted works? To me, putting something up on a website is distribution.</div><br/></div></div></div></div><div id="42765080" class="c"><input type="checkbox" id="c-42765080" checked=""/><div class="controls bullet"><span class="by">chaos_emergent</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764411">parent</a><span>|</span><a href="#42765064">prev</a><span>|</span><a href="#42765090">next</a><span>|</span><label class="collapse" for="c-42765080">[-]</label><label class="expand" for="c-42765080">[1 more]</label></div><br/><div class="children"><div class="content">Aaron Swartz, while an infuriating tragedy, is antithetical to OpenAI&#x27;s claim to transformation; he literally published documents that were behind a licensed paywall.</div><br/></div></div></div></div></div></div></div></div><div id="42765090" class="c"><input type="checkbox" id="c-42765090" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763845">parent</a><span>|</span><a href="#42764096">prev</a><span>|</span><a href="#42764059">next</a><span>|</span><label class="collapse" for="c-42765090">[-]</label><label class="expand" for="c-42765090">[1 more]</label></div><br/><div class="children"><div class="content">A more fundamental argument would be that OpenAI doesn&#x27;t have a legal copy&#x2F;license of all the works they are using. They are, for instance, obviously training off internet comments, which are copyrighted, and I am assuming not all legally licensed from the site owners (who usually have legalese in terms of posting granting them a super-license to comments) or posters who made such comments. I&#x27;m also curious if they&#x27;ve bothered to get legal copies&#x2F;licenses to all the books they are using rather than just grabbing LibGen or whatever. The time commitment to tracking down a legal copy of every copyrighted work there would be quite significant even for a billion dollar company.<p>In any case, if the music industry was able to successfully sue people for thousands of dollars per song for songs downloaded for personal use, what would be a reasonable fine for &quot;stealing&quot;, tweaking, and making billions from something?</div><br/></div></div></div></div><div id="42764059" class="c"><input type="checkbox" id="c-42764059" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42763845">prev</a><span>|</span><a href="#42764964">next</a><span>|</span><label class="collapse" for="c-42764059">[-]</label><label class="expand" for="c-42764059">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Can somehow explain to me how they can simply not respect copyright and get away with it? Also is this a uniquely open-ai problem, or also true of the other llm makers?<p>&quot;Move fast and break things.&quot;[0]<p>Another way to phrase this is:<p><pre><code>  Move fast enough while breaking things and regulations
  can never catch up.
</code></pre>
0 - <a href="https:&#x2F;&#x2F;quotes.guide&#x2F;mark-zuckerberg&#x2F;quote&#x2F;move-fast-and-break-things-unless-you-are-breaking-stuff-you-are-not-moving-fast-enough&#x2F;" rel="nofollow">https:&#x2F;&#x2F;quotes.guide&#x2F;mark-zuckerberg&#x2F;quote&#x2F;move-fast-and-bre...</a></div><br/></div></div><div id="42764964" class="c"><input type="checkbox" id="c-42764964" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42764059">prev</a><span>|</span><a href="#42763768">next</a><span>|</span><label class="collapse" for="c-42764964">[-]</label><label class="expand" for="c-42764964">[1 more]</label></div><br/><div class="children"><div class="content">The short answer is that there is actually a number of active lawsuits alleging copyright violation, but they take time (years) to resolve. And since it&#x27;s only been about two years since we&#x27;ve had the big generative AI blow up, fueled by entities with deep pockets (i.e., you can actually <i>profit</i> off of the lawsuit), there quite literally hasn&#x27;t been enough time for a lawsuit to find them in violation of copyright.<p>And quite frankly, between the announcement of several licensing deals in the past year for new copyrighted content for training, and the recent decision in Warhol &quot;clarifying&quot; the definition of &quot;transformative&quot; for the purposes of fair use, the likelihood of training for AI being found fair is actually quite slim.</div><br/></div></div><div id="42764406" class="c"><input type="checkbox" id="c-42764406" checked=""/><div class="controls bullet"><span class="by">alphan0n</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42763768">prev</a><span>|</span><a href="#42763956">next</a><span>|</span><label class="collapse" for="c-42764406">[-]</label><label class="expand" for="c-42764406">[6 more]</label></div><br/><div class="children"><div class="content">Simply put, if the model isn’t producing an actual copy, they aren’t violating copyright (in the US) under any current definition.<p>As much as people bandy the term around, copyright has never applied to input, and the output of a tool is the responsibility of the end user.<p>If I use a copy machine to reproduce your copyrighted work, I am responsible for that infringement not Xerox.<p>If I coax your copyrighted work out of my phones keyboard suggestion engine letter by letter, and publish it, it’s still me infringing on your copyright, not Apple.<p>If I make a copy of your clip art in Illustratator, is Adobe responsible? Etc.<p>Even if (as I’ve seen argued ad nauseaum) a model was trained on copyrighted works on a piracy website, the copyright holder’s tort would be with the source of the infringing distribution, not the people who read the material.<p>Not to mention, I can walk into any public library and learn something from any book there, would I then owe the authors of the books I learned from a fee to apply that knowledge?</div><br/><div id="42764462" class="c"><input type="checkbox" id="c-42764462" checked=""/><div class="controls bullet"><span class="by">yokem55</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764406">parent</a><span>|</span><a href="#42764836">next</a><span>|</span><label class="collapse" for="c-42764462">[-]</label><label class="expand" for="c-42764462">[2 more]</label></div><br/><div class="children"><div class="content">&gt; As much as people bandy the term around, copyright has never applied to input, and the output of a tool is the responsibility of the end user.<p>Where this breaks down though is that contributory infringement is a still a thing if you offer a service aids in copyright infringement and you don&#x27;t do &quot;enough&quot; to stop it.<p>Ie, it would all be on the end user for folks that self host or rent hardware and run an LLM or Gen Art AI model themselves. But folks that offer a consumer level end to end service like ChatGPT or MidJourney could be on the hook.</div><br/><div id="42765107" class="c"><input type="checkbox" id="c-42765107" checked=""/><div class="controls bullet"><span class="by">alphan0n</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764462">parent</a><span>|</span><a href="#42764836">next</a><span>|</span><label class="collapse" for="c-42765107">[-]</label><label class="expand" for="c-42765107">[1 more]</label></div><br/><div class="children"><div class="content">Right, strictly speaking, the vast majority of copyright infringement falls under liability tort.<p>There are cases where infringement by negligence that could be argued, but as long as there is clear effort to prevent copying in the output of the tool, then there is no tort.<p>If the models are creating copies inadvertently and separately from the efforts of the end users deliberate efforts then yes, the creators of the tool would likely be the responsible party for infringement.<p>If I ask an LLM for a story about vampires and the model spits out The Twilight Saga, that would be problematic. Nor should the model reproduce the story word for word on demand by the end user. But it seems like neither of these examples are likely outcomes with current models.</div><br/></div></div></div></div><div id="42764836" class="c"><input type="checkbox" id="c-42764836" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764406">parent</a><span>|</span><a href="#42764462">prev</a><span>|</span><a href="#42763956">next</a><span>|</span><label class="collapse" for="c-42764836">[-]</label><label class="expand" for="c-42764836">[3 more]</label></div><br/><div class="children"><div class="content">&gt; the copyright holder’s tort would be with the source of the infringing distribution, not the people who read the material.<p>Someone who just reads the material doesn&#x27;t infringe. But someone who copies it, or prepares works that are derivative of it (which can happen even if they don&#x27;t copy a single word or phrase literally), does.<p>&gt; would I then owe the authors of the books I learned from a fee to apply that knowledge?<p>Facts can&#x27;t be copyrighted, so applying the facts you learned is free, but creative works are generally copyrighted. If you write your own book inspired by a book you read, that can be copyright infringement (see The Wind Done Gone). If you use even a tiny fragment of someone else&#x27;s work in your own, even if not consciously, that can be copyright infringement (see My Sweet Lord).</div><br/><div id="42765162" class="c"><input type="checkbox" id="c-42765162" checked=""/><div class="controls bullet"><span class="by">alphan0n</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42764836">parent</a><span>|</span><a href="#42763956">next</a><span>|</span><label class="collapse" for="c-42765162">[-]</label><label class="expand" for="c-42765162">[2 more]</label></div><br/><div class="children"><div class="content">Right, but the onus of responsibility being on the end user publishing the song or creative work in violation of copyright, not the text editor, word processor, musical notation software, etc, correct?<p>A text prediction tool isn’t a person, the data it is trained on is irrelevant to the copyright infringement perpetrated by the end user. They should perform due diligence to prevent liability.</div><br/><div id="42766058" class="c"><input type="checkbox" id="c-42766058" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42765162">parent</a><span>|</span><a href="#42763956">next</a><span>|</span><label class="collapse" for="c-42766058">[-]</label><label class="expand" for="c-42766058">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A text prediction tool isn’t a person, the data it is trained on is irrelevant to the copyright infringement perpetrated by the end user. They should perform due diligence to prevent liability.<p>Huh what? If a program &quot;predicts&quot; some data that is a derivative work of some copyrighted work (that the end user did not input), then ipso facto the tool itself is a derivative work of that copyrighted work, and illegal to distribute without permission. (Does that mean it&#x27;s also illegal to publish and redistribute the brain of a human who&#x27;s memorised a copyrighted work? Probably. I don&#x27;t have a problem with that). How can it possibly be the user&#x27;s responsibility when the user has never seen the copyrighted work being infringed on, only the software maker has?<p>And if you say that OpenAI isn&#x27;t distributing their program but just offering it as a service, then we&#x27;re back to the original situation: in that case OpenAI is illegally distributing derivative works of copyrighted works without permission. It&#x27;s not even a YouTube like situation where some user uploaded the copyrighted work and they&#x27;re just distributing it; OpenAI added the pirated books themselves.</div><br/></div></div></div></div></div></div></div></div><div id="42763956" class="c"><input type="checkbox" id="c-42763956" checked=""/><div class="controls bullet"><span class="by">marxisttemp</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42764406">prev</a><span>|</span><a href="#42764007">next</a><span>|</span><label class="collapse" for="c-42763956">[-]</label><label class="expand" for="c-42763956">[1 more]</label></div><br/><div class="children"><div class="content">“There must be in-groups whom the law protects but does not bind, alongside out-groups whom the law binds but does not protect.”</div><br/></div></div><div id="42764007" class="c"><input type="checkbox" id="c-42764007" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42763956">prev</a><span>|</span><a href="#42763959">next</a><span>|</span><label class="collapse" for="c-42764007">[-]</label><label class="expand" for="c-42764007">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because the copyright is fake and the only thing supporting it were million dollar business. It naturally crumbles while facing billion dollar business.</div><br/></div></div><div id="42763959" class="c"><input type="checkbox" id="c-42763959" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42764007">prev</a><span>|</span><a href="#42764380">next</a><span>|</span><label class="collapse" for="c-42763959">[-]</label><label class="expand" for="c-42763959">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ll find people on this forum especially using the false analogy with a human. Like these things are <i>like</i> or <i>analogous</i> to human minds, and human minds have <i>fair use</i> access, so why shouldn&#x27;t a these?<p>Magical thinking that just so happens to make lots of $$. And after all why would you want to get in the way of profit^H^H^Hgress?</div><br/></div></div><div id="42764380" class="c"><input type="checkbox" id="c-42764380" checked=""/><div class="controls bullet"><span class="by">davidcbc</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763748">parent</a><span>|</span><a href="#42763959">prev</a><span>|</span><a href="#42765059">next</a><span>|</span><label class="collapse" for="c-42764380">[-]</label><label class="expand" for="c-42764380">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re a rich company, they are immune from consequences</div><br/></div></div></div></div></div></div><div id="42765059" class="c"><input type="checkbox" id="c-42765059" checked=""/><div class="controls bullet"><span class="by">charlieyu1</span><span>|</span><a href="#42763463">parent</a><span>|</span><a href="#42763526">prev</a><span>|</span><a href="#42764810">next</a><span>|</span><label class="collapse" for="c-42765059">[-]</label><label class="expand" for="c-42765059">[2 more]</label></div><br/><div class="children"><div class="content">Why would they use the materials in model training? It would defeat the purpose of having a benchmarking set</div><br/><div id="42765903" class="c"><input type="checkbox" id="c-42765903" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42765059">parent</a><span>|</span><a href="#42764810">next</a><span>|</span><label class="collapse" for="c-42765903">[-]</label><label class="expand" for="c-42765903">[1 more]</label></div><br/><div class="children"><div class="content">If you’re a research lab then yes.<p>If you’re a for profit company trying to raise funding and fend off skepticism that your models really aren’t that much better than any one else’s, then…<p>It would be <i>dishonest</i>, but as long as no one found out until after you closed your funding round, there’s plenty of reason you might do this.<p>It comes down to caring about benchmarks and integrity or caring about piles of money.<p>Judge for yourself which one they chose.<p>Perhaps they didn’t train on it.<p>Who knows?<p>It’s fair to be skeptical though, under the circumstances.</div><br/></div></div></div></div><div id="42764810" class="c"><input type="checkbox" id="c-42764810" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42763463">parent</a><span>|</span><a href="#42765059">prev</a><span>|</span><a href="#42765678">next</a><span>|</span><label class="collapse" for="c-42764810">[-]</label><label class="expand" for="c-42764810">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI&#x27;s benchmark results looking like Musk&#x27;s Path of Exile character..</div><br/></div></div><div id="42765678" class="c"><input type="checkbox" id="c-42765678" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#42763463">parent</a><span>|</span><a href="#42764810">prev</a><span>|</span><a href="#42764179">next</a><span>|</span><label class="collapse" for="c-42765678">[-]</label><label class="expand" for="c-42765678">[1 more]</label></div><br/><div class="children"><div class="content">&gt;perhaps they were routing API calls to human workers<p>Honest question, did they?</div><br/></div></div><div id="42763839" class="c"><input type="checkbox" id="c-42763839" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42763463">parent</a><span>|</span><a href="#42764179">prev</a><span>|</span><a href="#42763450">next</a><span>|</span><label class="collapse" for="c-42763839">[-]</label><label class="expand" for="c-42763839">[5 more]</label></div><br/><div class="children"><div class="content">This has me curious about ARC-AGI.<p>Would it have been possible for OpenAI to have gamed ARC-AGI by seeing the first few examples and then quickly mechanical turking a training set, fine tuning their model, then proceeding with the rest of the evaluation?<p>Are there other tricks they could have pulled?<p>It feels like unless a model is being deployed to an impartial evaluator&#x27;s completely air gapped machine, there&#x27;s a ton of room for shenanigans, dishonesty, and outright cheating.</div><br/><div id="42764072" class="c"><input type="checkbox" id="c-42764072" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763839">parent</a><span>|</span><a href="#42763971">next</a><span>|</span><label class="collapse" for="c-42764072">[-]</label><label class="expand" for="c-42764072">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This has me curious about ARC-AGI<p>In the o3 announcement video, the president of ARC Prize said they&#x27;d be partnering with OpenAI to develop the next benchmark.<p>&gt; mechanical turking a training set, fine tuning their model<p>You don&#x27;t need mechanical turking here. You can use an LLM to generate a lot more data that&#x27;s similar to the official training data, and then you can train on that. It sounds like &quot;pulling yourself up by your bootstraps&quot;, but isn&#x27;t. An approach to do this has been published, and it seems to be scaling very well with the amount of such generated training data (They won the 1st paper award)</div><br/></div></div><div id="42763971" class="c"><input type="checkbox" id="c-42763971" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763839">parent</a><span>|</span><a href="#42764072">prev</a><span>|</span><a href="#42765355">next</a><span>|</span><label class="collapse" for="c-42763971">[-]</label><label class="expand" for="c-42763971">[1 more]</label></div><br/><div class="children"><div class="content">In their benchmark, they have a tag &quot;tuned&quot; attached to their o3 result. I guess we need they to inform us of the exact meaning of it to gauge.</div><br/></div></div><div id="42764235" class="c"><input type="checkbox" id="c-42764235" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42763463">root</a><span>|</span><a href="#42763839">parent</a><span>|</span><a href="#42765355">prev</a><span>|</span><a href="#42763450">next</a><span>|</span><label class="collapse" for="c-42764235">[-]</label><label class="expand" for="c-42764235">[1 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI to have gamed ARC-AGI by seeing the first few examples<p>not just few examples. o3 was evaluated on &quot;semi-private&quot; test, which was previously already used for evaluating OAI models, so OAI had access to it already for a long time.</div><br/></div></div></div></div></div></div><div id="42763450" class="c"><input type="checkbox" id="c-42763450" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#42763463">prev</a><span>|</span><a href="#42765088">next</a><span>|</span><label class="collapse" for="c-42763450">[-]</label><label class="expand" for="c-42763450">[9 more]</label></div><br/><div class="children"><div class="content">A co-founder of Epoch left a note in the comments:<p>&gt; We acknowledge that OpenAI does have access to a large fraction of FrontierMath problems and solutions, with the exception of a unseen-by-OpenAI hold-out set that enables us to independently verify model capabilities. However, we have a verbal agreement that these materials will not be used in model training.<p>Ouch. A verbal agreement. As the saying goes, those aren&#x27;t worth the paper they&#x27;re written on, and that&#x27;s doubly true when you&#x27;re dealing with someone with a reputation like Altman&#x27;s.<p>And aside from the obvious flaw in it being a verbal agreement, there are many ways in which OpenAI could <i>technically</i> comply with this agreement while still gaining a massive unfair advantage on the benchmarks to the point of rendering them meaningless. For just one example, knowing the benchmark questions can help you select training data that is tailored to excelling at the benchmarks without technically including the actual question in the training data.</div><br/><div id="42763489" class="c"><input type="checkbox" id="c-42763489" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42763450">parent</a><span>|</span><a href="#42763920">next</a><span>|</span><label class="collapse" for="c-42763489">[-]</label><label class="expand" for="c-42763489">[5 more]</label></div><br/><div class="children"><div class="content">What&#x27;s even more suspicious is that these tweets from Elliot Glazer indicate that they are still &quot;developing&quot; the hold-out set, even though elsewhere Epoch AI strongly implied this already existed: <a href="https:&#x2F;&#x2F;xcancel.com&#x2F;ElliotGlazer&#x2F;status&#x2F;1880809468616950187" rel="nofollow">https:&#x2F;&#x2F;xcancel.com&#x2F;ElliotGlazer&#x2F;status&#x2F;1880809468616950187</a><p>It seems to me that o3&#x27;s 25% benchmark score is 100% data contamination.</div><br/><div id="42765450" class="c"><input type="checkbox" id="c-42765450" checked=""/><div class="controls bullet"><span class="by">EagnaIonat</span><span>|</span><a href="#42763450">root</a><span>|</span><a href="#42763489">parent</a><span>|</span><a href="#42764829">next</a><span>|</span><label class="collapse" for="c-42765450">[-]</label><label class="expand" for="c-42765450">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What&#x27;s even more suspicious is that these tweets from Elliot Glazer indicate that they are still &quot;developing&quot; the hold-out set,<p>There is nothing suspicious about this and the wording seems to be incorrect.<p>A hold-out set is a percentage of the overall data that is used to test a model. It is just not trained on it. Model developers normally have full access to it.<p>There is nothing inherently wrong with training on a full&#x2F;partial hold out set. It just means you have done a different split to train again.<p>The confusion I see here is that people are equating a hold out set to a blind set. That&#x27;s a set of data to test against that the model developers (and model) cannot see.<p>Even so blind sets can also go stale after a few runs and nothing is wrong with ingesting that blind set, as long as you have a new blind set to run against.<p>Trying to game blind set tests is nothing new and it gets very quickly found out.<p>What I took from the original article is that the blind set is likely unbalanced and it answered more easier questions than hard ones.</div><br/></div></div><div id="42764829" class="c"><input type="checkbox" id="c-42764829" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42763450">root</a><span>|</span><a href="#42763489">parent</a><span>|</span><a href="#42765450">prev</a><span>|</span><a href="#42763586">next</a><span>|</span><label class="collapse" for="c-42764829">[-]</label><label class="expand" for="c-42764829">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I just saw Sam Altman speak at YCNYC and I was impressed. I have never actually met him or heard him speak before Monday, but one of his stories really stuck out and went something like this:<p>&gt; &quot;We were trying to get a big client for weeks, and they said no and went with a competitor. The competitor already had a terms sheet from the company were we trying to sign up. It was real serious.<p>&gt; We were devastated, but we decided to fly down and sit in their lobby until they would meet with us. So they finally let us talk to them after most of the day.<p>&gt; We then had a few more meetings, and the company wanted to come visit our offices so they could make sure we were a &#x27;real&#x27; company. At that time, we were only 5 guys. So we hired a bunch of our college friends to &#x27;work&#x27; for us for the day so we could look larger than we actually were. It worked, and we got the contract.&quot;<p>&gt; I think the reason why PG respects Sam so much is he is charismatic, resourceful, and just overall seems like a genuine person.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=3048944">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=3048944</a></div><br/><div id="42765489" class="c"><input type="checkbox" id="c-42765489" checked=""/><div class="controls bullet"><span class="by">AyyEye</span><span>|</span><a href="#42763450">root</a><span>|</span><a href="#42764829">parent</a><span>|</span><a href="#42763586">next</a><span>|</span><label class="collapse" for="c-42765489">[-]</label><label class="expand" for="c-42765489">[1 more]</label></div><br/><div class="children"><div class="content">Nothing says genuine like lying to get a contract.</div><br/></div></div></div></div><div id="42763586" class="c"><input type="checkbox" id="c-42763586" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#42763450">root</a><span>|</span><a href="#42763489">parent</a><span>|</span><a href="#42764829">prev</a><span>|</span><a href="#42763920">next</a><span>|</span><label class="collapse" for="c-42763586">[-]</label><label class="expand" for="c-42763586">[1 more]</label></div><br/><div class="children"><div class="content">This was my assumption all along.</div><br/></div></div></div></div><div id="42763920" class="c"><input type="checkbox" id="c-42763920" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#42763450">parent</a><span>|</span><a href="#42763489">prev</a><span>|</span><a href="#42764194">next</a><span>|</span><label class="collapse" for="c-42763920">[-]</label><label class="expand" for="c-42763920">[2 more]</label></div><br/><div class="children"><div class="content">The questions are designed so that such training data is extremely limited. Tao said it was around half a dozen papers at most, sometimes. That’s not really enough to overfit on without causing other problems.</div><br/><div id="42764207" class="c"><input type="checkbox" id="c-42764207" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#42763450">root</a><span>|</span><a href="#42763920">parent</a><span>|</span><a href="#42764194">next</a><span>|</span><label class="collapse" for="c-42764207">[-]</label><label class="expand" for="c-42764207">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That’s not really enough to overfit on without causing other problems.<p>&quot;Causing other problems&quot; is exactly what I&#x27;m worried about. I would not put it past OpenAI to deliberately overfit on a set of benchmarks in order to keep up the illusion that they&#x27;re still progressing at the rate that the hype has come to expect, then keep the very-dangerous model under wraps for a while to avoid having to explain why it doesn&#x27;t act as smart as they claimed. We still don&#x27;t have access to this model (because, as with everything since GPT-2, it&#x27;s &quot;too dangerous&quot;), so we have no way of independently verifying its utility, which means they have a window where they can claim anything they want. If they release a weaker model than claimed it can always be attributed to guardrails put in place after safety testing confirmed it was dangerous.<p>We&#x27;ll see when the model actually becomes available, but in the meantime it&#x27;s reasonable to guess that it&#x27;s overfitted.</div><br/></div></div></div></div></div></div><div id="42765088" class="c"><input type="checkbox" id="c-42765088" checked=""/><div class="controls bullet"><span class="by">ripped_britches</span><span>|</span><a href="#42763450">prev</a><span>|</span><a href="#42763341">next</a><span>|</span><label class="collapse" for="c-42765088">[-]</label><label class="expand" for="c-42765088">[8 more]</label></div><br/><div class="children"><div class="content">Do people actually think OpenAI is gaming benchmarks?<p>I know they have lost trust and credibility, especially on HN. But this is a company with a giant revenue opportunity to sell products that work.<p>What works for enterprise is very different from “does it beat this benchmark”.<p>No matter how nefarious you think sama is, everything points to “build intelligence as rapidly as possible” rather than “spin our wheels messing with benchmarks”.<p>In fact, even if they did fully lie and game the benchmark - do you even care? As an OpenAI customer, all I care about is that the product works.<p>I code with o1 for hours every day, so I am very excited for o3 to be released via API. And if they trained on private datasets, I honestly don’t care. I just want to get a better coding partner until I’m irrelevant.<p>Final thought - why are these contractors owed a right to know where funding came from? I would definitely be proud to know I contributed to the advancement of the field of AI if I was included in this group.</div><br/><div id="42765502" class="c"><input type="checkbox" id="c-42765502" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#42765088">parent</a><span>|</span><a href="#42765214">next</a><span>|</span><label class="collapse" for="c-42765502">[-]</label><label class="expand" for="c-42765502">[5 more]</label></div><br/><div class="children"><div class="content">Gaming benchmarks has a lot of utility for openAI whether their product works or not.<p>Many people compare models based on benchmarks. So if openAI can appear better to Anthropic, Google, or Meta, by gaming benchmarks, it&#x27;s absolutely in their interest to do so, especially if their product is only slightly behind, because evaluating model quality is very very tricky business these days.<p>In particular, if there is a new benchmark, it&#x27;s doubly in their interest to game it, because they know that other providers will start using and optimizing performance towards that benchmark, in order to &quot;beat&quot; OpenAI and win market share.<p>On a personal level, their model is getting beat handily by Claude Sonnet 3.5 right now. It doesn&#x27;t seem to show in the benchmarks. I wonder why?<p>This is a company which is shedding their coats of ethics and scientific rigor -- so as to be as unencumbered as possible in its footrace to the dollar.</div><br/><div id="42765622" class="c"><input type="checkbox" id="c-42765622" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42765088">root</a><span>|</span><a href="#42765502">parent</a><span>|</span><a href="#42765514">next</a><span>|</span><label class="collapse" for="c-42765622">[-]</label><label class="expand" for="c-42765622">[1 more]</label></div><br/><div class="children"><div class="content">&gt; On a personal level, their model is getting beat handily by Claude Sonnet 3.5 right now. It doesn&#x27;t seem to show in the benchmarks. I wonder why?<p>I do use Sonnet 3.5 personally, but this &quot;beat handily&quot; doesn&#x27;t show on LLM arena. Do OpenAI game that too?</div><br/></div></div><div id="42765514" class="c"><input type="checkbox" id="c-42765514" checked=""/><div class="controls bullet"><span class="by">ripped_britches</span><span>|</span><a href="#42765088">root</a><span>|</span><a href="#42765502">parent</a><span>|</span><a href="#42765622">prev</a><span>|</span><a href="#42765790">next</a><span>|</span><label class="collapse" for="c-42765514">[-]</label><label class="expand" for="c-42765514">[2 more]</label></div><br/><div class="children"><div class="content">I think “getting beat handily” is a HN bubble concept. Depends on what you’re using it for, but I personally prefer 4o for coding. In enterprise usage, i think 4o is smoking 3.5 sonnet, but that’s just my perception from folks I talk to.</div><br/><div id="42765777" class="c"><input type="checkbox" id="c-42765777" checked=""/><div class="controls bullet"><span class="by">hatefulmoron</span><span>|</span><a href="#42765088">root</a><span>|</span><a href="#42765514">parent</a><span>|</span><a href="#42765790">next</a><span>|</span><label class="collapse" for="c-42765777">[-]</label><label class="expand" for="c-42765777">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s true, you&#x27;ll get the same sentiment (&quot;Sonnet 3.5 is much better than GPT4&#x2F;GPT4o [for coding]&quot;) pretty uniformly across Reddit&#x2F;HN&#x2F;Lobsters. I would strongly agree with it in my own testing, although o1 might be much better (I&#x27;m too poor to give it a fair shake.)<p>&gt; In enterprise usage, i think 4o is smoking 3.5 sonnet<p>True. I&#x27;m not sure how many enterprise solutions have given their users an opportunity to test Claude vs. GPT. Most people just use whatever LLM API their software integrates.</div><br/></div></div></div></div><div id="42765790" class="c"><input type="checkbox" id="c-42765790" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#42765088">root</a><span>|</span><a href="#42765502">parent</a><span>|</span><a href="#42765514">prev</a><span>|</span><a href="#42765214">next</a><span>|</span><label class="collapse" for="c-42765790">[-]</label><label class="expand" for="c-42765790">[1 more]</label></div><br/><div class="children"><div class="content">I used to think this, but using o1 quite a bit lately has convinced me otherwise. It’s been 1-shotting the fairly non-trivial coding problems I throw at it and is good about outputting large, complete code blocks. By contrast, Claude immediately starts nagging you about hitting usage limits after a few back and forth and has some kind of hack in place to start abbreviating code when conversations get too long,
even when explicitly instructed to do otherwise. I would imagine that Anthropic can produce a good test time compute model as well, but until they have something publicly available, OpenAI has stolen back the lead.</div><br/></div></div></div></div><div id="42765214" class="c"><input type="checkbox" id="c-42765214" checked=""/><div class="controls bullet"><span class="by">jatins</span><span>|</span><a href="#42765088">parent</a><span>|</span><a href="#42765502">prev</a><span>|</span><a href="#42765808">next</a><span>|</span><label class="collapse" for="c-42765214">[-]</label><label class="expand" for="c-42765214">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Do people actually think OpenAI is gaming benchmarks?<p>I was blown away by chatgpt release and generally have admired OpenAI however I wouldn&#x27;t put it past them<p>At this point their entire marketing strategy seems to be to do vague posting on X&#x2F;Twitter and keep hyping the models so that investors always feel there is something around the corner<p>And I don&#x27;t think they need to do that. Most investors will be throwing money at them either way but maybe when you are looking to raise _billions_ that&#x27;s not enough</div><br/></div></div><div id="42765808" class="c"><input type="checkbox" id="c-42765808" checked=""/><div class="controls bullet"><span class="by">331c8c71</span><span>|</span><a href="#42765088">parent</a><span>|</span><a href="#42765214">prev</a><span>|</span><a href="#42763341">next</a><span>|</span><label class="collapse" for="c-42765808">[-]</label><label class="expand" for="c-42765808">[1 more]</label></div><br/><div class="children"><div class="content">Well I certainly won&#x27;t object if oai marketing was based on testimonials from their fanboy customers instead of rigged benchmark scores %)<p>Your fragrant disregard for ethics and focus on utilitarian aspects is certainly quite extreme to the extent that only a view people would agree with you in my view.</div><br/></div></div></div></div><div id="42763341" class="c"><input type="checkbox" id="c-42763341" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42765088">prev</a><span>|</span><a href="#42763423">next</a><span>|</span><label class="collapse" for="c-42763341">[-]</label><label class="expand" for="c-42763341">[8 more]</label></div><br/><div class="children"><div class="content">Why do people keep taking OpenAIs marketing spin at face value? This keeps happening, like when they neglected to mention that their most impressive Sora demo involved extensive manual editing&#x2F;cleanup work because the studio couldn&#x27;t get Sora to generate what they wanted.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40359425">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40359425</a></div><br/><div id="42763443" class="c"><input type="checkbox" id="c-42763443" checked=""/><div class="controls bullet"><span class="by">th1243127</span><span>|</span><a href="#42763341">parent</a><span>|</span><a href="#42766001">next</a><span>|</span><label class="collapse" for="c-42763443">[-]</label><label class="expand" for="c-42763443">[5 more]</label></div><br/><div class="children"><div class="content">It might be because (very few!) mathematicians like Terence Tao make positive remarks. I think these mathematicians should be very careful to use reproducible and controlled setups that by their nature cannot take place on GPUs in the Azure cloud.<p>I have nothing against scientists promoting the Coq Proof Assistant. But that&#x27;s open source, can be run at home and is fully reproducible.</div><br/><div id="42763553" class="c"><input type="checkbox" id="c-42763553" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42763341">root</a><span>|</span><a href="#42763443">parent</a><span>|</span><a href="#42766001">next</a><span>|</span><label class="collapse" for="c-42763553">[-]</label><label class="expand" for="c-42763553">[4 more]</label></div><br/><div class="children"><div class="content">Keep in mind those mathematicians were kept in the dark about the funding: it is incredibly unethical to invite a coauthor to your paper and not tell where the money came from.<p>It&#x27;s just incredibly scummy behavior: I imagine some of those mathematicians would have declined the collaboration if the funding were transparent.  More so than data contamination, this makes me deeply mistrustful of Epoch AI.</div><br/><div id="42763858" class="c"><input type="checkbox" id="c-42763858" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#42763341">root</a><span>|</span><a href="#42763553">parent</a><span>|</span><a href="#42763559">next</a><span>|</span><label class="collapse" for="c-42763858">[-]</label><label class="expand" for="c-42763858">[1 more]</label></div><br/><div class="children"><div class="content">Wait, I think I somehow knew Epoch AI was getting money from OpenAI. I&#x27;m not sure how, and I didn&#x27;t connect any of the facts together to think of this problem in advance.</div><br/></div></div><div id="42763559" class="c"><input type="checkbox" id="c-42763559" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42763341">root</a><span>|</span><a href="#42763553">parent</a><span>|</span><a href="#42763858">prev</a><span>|</span><a href="#42766001">next</a><span>|</span><label class="collapse" for="c-42763559">[-]</label><label class="expand" for="c-42763559">[2 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t parse any of this, can you explain to a noob? I get lost immediately: funding, coauthor, etc. Only interpretation I&#x27;ve come to is I&#x27;ve missed a scandal involving payola, Terence Tao, and keeping coauthors off papers</div><br/><div id="42763698" class="c"><input type="checkbox" id="c-42763698" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#42763341">root</a><span>|</span><a href="#42763559">parent</a><span>|</span><a href="#42766001">next</a><span>|</span><label class="collapse" for="c-42763698">[-]</label><label class="expand" for="c-42763698">[1 more]</label></div><br/><div class="children"><div class="content">Very few people were told the nature of the funding.</div><br/></div></div></div></div></div></div></div></div><div id="42766001" class="c"><input type="checkbox" id="c-42766001" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#42763341">parent</a><span>|</span><a href="#42763443">prev</a><span>|</span><a href="#42763588">next</a><span>|</span><label class="collapse" for="c-42766001">[-]</label><label class="expand" for="c-42766001">[1 more]</label></div><br/><div class="children"><div class="content">Because they are completely gullible and believe almost everything that OpenAI does without questioning the results.<p>On each product they release, their top researchers are gradually leaving.<p>Everyone now knows what happens when you go against or question OpenAI after working for them, which is why you don&#x27;t see any criticism and more of a cult-like worship.<p>Once again, &quot;AGI&quot; is a complete scam.</div><br/></div></div><div id="42763588" class="c"><input type="checkbox" id="c-42763588" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42763341">parent</a><span>|</span><a href="#42766001">prev</a><span>|</span><a href="#42763423">next</a><span>|</span><label class="collapse" for="c-42763588">[-]</label><label class="expand" for="c-42763588">[1 more]</label></div><br/><div class="children"><div class="content">Because the models have continually matched the quality they claim.<p>Ex. look how much work &quot;very few&quot; has to do in the sibling comment. It&#x27;s like saying &quot;very few physicists [Einstein&#x2F;Feynman&#x2F;Witten]&quot;<p>Its conveniently impossible to falsify the implication that the inverse of &quot;very few&quot; say not positive things. i.e. that the vast majority say negative things<p>You have to go through an incredible level of mental gymnastics, involving many months of gated decisions, where the route chosen involved &quot;gee, I know this is suspectable to confirmation bias, but...&quot;, to end up wondering why people think the models are real if OpenAI has access to data that includes some set of questions.</div><br/></div></div></div></div><div id="42763423" class="c"><input type="checkbox" id="c-42763423" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#42763341">prev</a><span>|</span><a href="#42764061">next</a><span>|</span><label class="collapse" for="c-42763423">[-]</label><label class="expand" for="c-42763423">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Tamay from Epoch AI here. We made a mistake in not being more transparent about OpenAI&#x27;s involvement. We were restricted from disclosing the partnership until around the time o3 launched, and in hindsight we should have negotiated harder for the ability to be transparent to the benchmark contributors as soon as possible. Our contract specifically prevented us from disclosing information about the funding source and the fact that OpenAI has data access to much but not all of the dataset.<p>Not sure if &quot;integrity of the benchmarks&quot; should even be something that you negotiate over, what&#x27;s the value of the benchmark if the results cannot be trusted because of undisclosed relationships and sharing of data? Why would they be restricted from disclosing stuff you normally disclose, and how doesn&#x27;t that raise all sorts of warning flags when proposed even?</div><br/><div id="42763891" class="c"><input type="checkbox" id="c-42763891" checked=""/><div class="controls bullet"><span class="by">aunty_helen</span><span>|</span><a href="#42763423">parent</a><span>|</span><a href="#42763462">next</a><span>|</span><label class="collapse" for="c-42763891">[-]</label><label class="expand" for="c-42763891">[1 more]</label></div><br/><div class="children"><div class="content">This feels like a done deal. This benchmark should be discarded.</div><br/></div></div><div id="42763462" class="c"><input type="checkbox" id="c-42763462" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#42763423">parent</a><span>|</span><a href="#42763891">prev</a><span>|</span><a href="#42764061">next</a><span>|</span><label class="collapse" for="c-42763462">[-]</label><label class="expand" for="c-42763462">[2 more]</label></div><br/><div class="children"><div class="content">&gt;OpenAI has data access to much but not all of the dataset<p>Their head mathematician says they have the full dataset, except a holdout set which they&#x27;re currently developing (i.e. doesn&#x27;t exist yet):<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;singularity&#x2F;comments&#x2F;1i4n0r5&#x2F;comment&#x2F;m7x1vnx&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;singularity&#x2F;comments&#x2F;1i4n0r5&#x2F;commen...</a></div><br/><div id="42766455" class="c"><input type="checkbox" id="c-42766455" checked=""/><div class="controls bullet"><span class="by">menaerus</span><span>|</span><a href="#42763423">root</a><span>|</span><a href="#42763462">parent</a><span>|</span><a href="#42764061">next</a><span>|</span><label class="collapse" for="c-42766455">[-]</label><label class="expand" for="c-42766455">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the link. A holdout set which is yet to be used to verify the 25% claim. He also says that he doesn&#x27;t believe that OpenAI would self-sabotage themselves by tricking the internal benchmarking performance since this will get easily exposed, either by the results from a holdout set or by the public repeating the benchmarks themselves. Seems reasonable to me.</div><br/></div></div></div></div></div></div><div id="42764061" class="c"><input type="checkbox" id="c-42764061" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#42763423">prev</a><span>|</span><a href="#42764027">next</a><span>|</span><label class="collapse" for="c-42764061">[-]</label><label class="expand" for="c-42764061">[4 more]</label></div><br/><div class="children"><div class="content">OpenAI played themselves here. Now nobody is going to take any of their results on this benchmark seriously, ever again. That o3 result has just disappeared in a poof of smoke. If they had blinded themselves properly then that wouldn&#x27;t be the case.<p>Whereas other AI companies now have the opportunity to be first to get a significant result on FrontierMath.</div><br/><div id="42766064" class="c"><input type="checkbox" id="c-42766064" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#42764061">parent</a><span>|</span><a href="#42765103">next</a><span>|</span><label class="collapse" for="c-42766064">[-]</label><label class="expand" for="c-42766064">[1 more]</label></div><br/><div class="children"><div class="content">Conversely, if they didn&#x27;t cheat and they funded creation of the test suite to get &quot;clean&quot; problems (while hiding their participation to prevent getting problems that are somehow tailored to be hard for LLMs specifically), then they have no reasons to fear that all this looks fishy as the test results will soon be vindicated when they&#x27;ll give wider access to the model.<p>I refrain from forming a strong opinion in such situations. My intuition tells me that it&#x27;s not cheating. But, well, it&#x27;s intuition (probably based on my belief that the brain is nothing special physics-wise and it doesn&#x27;t manage to realize unknown quantum algorithms in its warm and messy environment, so that classical computers can reproduce all of its feats when having appropriate algorithms and enough computing power. And math reasoning is just another step on a ladder of capabilities, not something that requires completely different approach). So, we&#x27;ll see.</div><br/></div></div><div id="42765103" class="c"><input type="checkbox" id="c-42765103" checked=""/><div class="controls bullet"><span class="by">colonial</span><span>|</span><a href="#42764061">parent</a><span>|</span><a href="#42766064">prev</a><span>|</span><a href="#42765084">next</a><span>|</span><label class="collapse" for="c-42765103">[-]</label><label class="expand" for="c-42765103">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be surprised if <i>any</i> of their in-house benchmark results are taken seriously after this. As an extremely rough estimate, FrontierMath cost five to six figures to assemble [1] - so from an outside view, they clearly have no qualms with turning cash into quasi-guaranteed benchmark results.<p>[1]: <a href="https:&#x2F;&#x2F;epoch.ai&#x2F;math-problems&#x2F;submit-problem" rel="nofollow">https:&#x2F;&#x2F;epoch.ai&#x2F;math-problems&#x2F;submit-problem</a> - the benchmark is comprised of &quot;hundreds&quot; of questions, so at the absolute lowest it cost 300 * 200 = 60,000 dollars.</div><br/></div></div><div id="42765084" class="c"><input type="checkbox" id="c-42765084" checked=""/><div class="controls bullet"><span class="by">eksu</span><span>|</span><a href="#42764061">parent</a><span>|</span><a href="#42765103">prev</a><span>|</span><a href="#42764027">next</a><span>|</span><label class="collapse" for="c-42765084">[-]</label><label class="expand" for="c-42765084">[1 more]</label></div><br/><div class="children"><div class="content">This risk could be mitigated by publishing the test.</div><br/></div></div></div></div><div id="42764027" class="c"><input type="checkbox" id="c-42764027" checked=""/><div class="controls bullet"><span class="by">bogtog</span><span>|</span><a href="#42764061">prev</a><span>|</span><a href="#42763985">next</a><span>|</span><label class="collapse" for="c-42764027">[-]</label><label class="expand" for="c-42764027">[2 more]</label></div><br/><div class="children"><div class="content">A lot of the comments express some type of deliberate cheating the benchmark. However, even without intentionally trying to game it, if anybody can repeatedly take the same test, then they&#x27;ll be nudged to overfit&#x2F;p-hack.<p>For instance, suppose they conduct an experiment and find that changing some hyper-parameter yields a 2% boost. That could just be noise, it could be a genuine small improvement, or it may be a mix of a genuine boost along with some fortunate noise. An effect may be small enough that researchers would need to rely on their gut to interpret it. Researchers may jump on noise while believing they have discovered true optimizations. Enough of these types of nudges, and some serious benchmark gains can materialize.<p>(Hopefully my comment isn&#x27;t entirely misguided, I don&#x27;t know how they actually do testing or how often they probe their test set)</div><br/><div id="42764062" class="c"><input type="checkbox" id="c-42764062" checked=""/><div class="controls bullet"><span class="by">madars</span><span>|</span><a href="#42764027">parent</a><span>|</span><a href="#42763985">next</a><span>|</span><label class="collapse" for="c-42764062">[-]</label><label class="expand" for="c-42764062">[1 more]</label></div><br/><div class="children"><div class="content">I cringe every time I see &quot;my IQ increased by X points after doing Y&quot; posts on Twitter - yes, you had a practice run on Raven&#x27;s progressive matrices a month ago, that helped, these have a limited question bank and the effect of Y is marginal. That said, obviously, test taking is a skill (separate from background knowledge and both general&#x2F;domain-specific ability) and should be trained if you expect to have life-altering events based on tests (i.e., do an LSAT course if you want to go to law school). Conversely, shouldn&#x27;t be done if you think it will limit you through superstition (&quot;I had a score of X, thus I can only perform around level of X+fudge factor&quot;). For an LLM company a good test score is a valuation-altering event!</div><br/></div></div></div></div><div id="42763985" class="c"><input type="checkbox" id="c-42763985" checked=""/><div class="controls bullet"><span class="by">lionkor</span><span>|</span><a href="#42764027">prev</a><span>|</span><a href="#42763970">next</a><span>|</span><label class="collapse" for="c-42763985">[-]</label><label class="expand" for="c-42763985">[4 more]</label></div><br/><div class="children"><div class="content">People on here were mocking me openly when I pointed out that you can&#x27;t be sure LLMs (or any AIs) are actually smart unless you CAN PROVE that the question you&#x27;re asking isn&#x27;t in the training set (or adjacent like in this case).<p>So with this in mind now, let me repeat: Unless you know that the question AND&#x2F;OR answer are not in the training set or adjacent, do not claim that the AI or similar black box is smart.</div><br/><div id="42764442" class="c"><input type="checkbox" id="c-42764442" checked=""/><div class="controls bullet"><span class="by">pcmoore</span><span>|</span><a href="#42763985">parent</a><span>|</span><a href="#42764479">next</a><span>|</span><label class="collapse" for="c-42764442">[-]</label><label class="expand" for="c-42764442">[2 more]</label></div><br/><div class="children"><div class="content">I ran a test yesterday on ChatGPT and co-pilot asking first if it knew of a specific paper which it confirmed and then to derive simple results from which it was completely incapable of. I know this paper is not widely referenced (ie few known results in the public domain) but has been available for over 15 years with publicly accessible code written by humans. The training set was so sparse it had no ability to &quot;understand&quot; or even regurgitate past the summary text which it listed almost verbatim.</div><br/><div id="42764471" class="c"><input type="checkbox" id="c-42764471" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#42763985">root</a><span>|</span><a href="#42764442">parent</a><span>|</span><a href="#42764479">next</a><span>|</span><label class="collapse" for="c-42764471">[-]</label><label class="expand" for="c-42764471">[1 more]</label></div><br/><div class="children"><div class="content">It is known that current models have terrible sample efficiency. I&#x27;ve been told that it&#x27;s better than I thought it was, but it still isn&#x27;t <i>good</i>.</div><br/></div></div></div></div><div id="42764479" class="c"><input type="checkbox" id="c-42764479" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42763985">parent</a><span>|</span><a href="#42764442">prev</a><span>|</span><a href="#42763970">next</a><span>|</span><label class="collapse" for="c-42764479">[-]</label><label class="expand" for="c-42764479">[1 more]</label></div><br/><div class="children"><div class="content">This all smells like the OpenAI CEO&#x27;s MO. Stupid drama for stupid reasons.</div><br/></div></div></div></div><div id="42763970" class="c"><input type="checkbox" id="c-42763970" checked=""/><div class="controls bullet"><span class="by">MattDaEskimo</span><span>|</span><a href="#42763985">prev</a><span>|</span><a href="#42764769">next</a><span>|</span><label class="collapse" for="c-42763970">[-]</label><label class="expand" for="c-42763970">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s something gross about OpenAI constantly misleading the public.<p>This maneuver by their CEO will destroy FrontierMath and Epoch AI&#x27;s reputation</div><br/><div id="42764040" class="c"><input type="checkbox" id="c-42764040" checked=""/><div class="controls bullet"><span class="by">cbracketdash</span><span>|</span><a href="#42763970">parent</a><span>|</span><a href="#42764769">next</a><span>|</span><label class="collapse" for="c-42764040">[-]</label><label class="expand" for="c-42764040">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of the following proverb:<p>&quot;The integrity of the upright guides them,
    but the unfaithful are destroyed by their duplicity.&quot;<p>(Proverbs 11:3)</div><br/></div></div></div></div><div id="42764769" class="c"><input type="checkbox" id="c-42764769" checked=""/><div class="controls bullet"><span class="by">padolsey</span><span>|</span><a href="#42763970">prev</a><span>|</span><a href="#42765602">next</a><span>|</span><label class="collapse" for="c-42764769">[-]</label><label class="expand" for="c-42764769">[1 more]</label></div><br/><div class="children"><div class="content">Many of these evals are quite easy to game. Often the actual evaluation part of benchmarking is left up to a good-faith actor, which was usually reasonable in academic settings less polluted by capital. AI labs, however, have disincentives to do a thorough or impartial job, so IMO we should never take their word for it. To verify, we need to be able to run these evals ourselves – this is only sometimes possible, as even if the datasets are public, the exact mechanisms of evaluation are not. In the long run, to be completely resilient to gaming via training, we probably need to follow suit of other fields and have third-party non-profit accredited (!!) evaluators who&#x27;s entire premise is to evaluate, red-team, and generally keep AI safe and competent.</div><br/></div></div><div id="42765602" class="c"><input type="checkbox" id="c-42765602" checked=""/><div class="controls bullet"><span class="by">mrg3_2013</span><span>|</span><a href="#42764769">prev</a><span>|</span><a href="#42763790">next</a><span>|</span><label class="collapse" for="c-42765602">[-]</label><label class="expand" for="c-42765602">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI continues to muddy the benchmarks, while Claude continues to improve their intelligence. Claude will win long term. It&#x27;d be wise to not rely on OpenAI at all. They are the first comers who will just burn cash and crash out I suspect.</div><br/></div></div><div id="42763790" class="c"><input type="checkbox" id="c-42763790" checked=""/><div class="controls bullet"><span class="by">wujerry2000</span><span>|</span><a href="#42765602">prev</a><span>|</span><a href="#42763721">next</a><span>|</span><label class="collapse" for="c-42763790">[-]</label><label class="expand" for="c-42763790">[1 more]</label></div><br/><div class="children"><div class="content">My takeaways<p>(1) Companies will probably increasingly invest in building their own evals for their use cases because its becoming clear public&#x2F;allegedly private benchmarks have misaligned incentives with labs sponsoring&#x2F;cheating
(2) Those evals will prob be proprietary &quot;IP&quot; - guarded as closely as the code or research itself
(3) Conversely, public benchmarks are exhausted and SOMEONE has to invest in funding more frontier benchmarks. So this is prob going to continue.</div><br/></div></div><div id="42763721" class="c"><input type="checkbox" id="c-42763721" checked=""/><div class="controls bullet"><span class="by">matt_daemon</span><span>|</span><a href="#42763790">prev</a><span>|</span><a href="#42765098">next</a><span>|</span><label class="collapse" for="c-42763721">[-]</label><label class="expand" for="c-42763721">[1 more]</label></div><br/><div class="children"><div class="content">At this point eval results presented by AI companies are a joke and should not be trusted</div><br/></div></div><div id="42765098" class="c"><input type="checkbox" id="c-42765098" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#42763721">prev</a><span>|</span><a href="#42763649">next</a><span>|</span><label class="collapse" for="c-42765098">[-]</label><label class="expand" for="c-42765098">[1 more]</label></div><br/><div class="children"><div class="content">The problem is, any benchmark on a closed model couldn’t be private even in theory, as the model has to be called to run the benchmark, exposing the contents to whoever owns the model thereafter.<p>HN loves to speculate that OpenAI is some big scam whose seeming ascendance is based on deceptive marketing hype, but o1, to anyone who has tried it seriously is undoubtedly very much within the ballpark of what OpenAI claims it is able to do. If everything they are doing really is just overfitting and gaming the tests, that discrepancy will eventually catch up to them, and people will stop using the APIs and chatgpt</div><br/></div></div><div id="42763649" class="c"><input type="checkbox" id="c-42763649" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#42765098">prev</a><span>|</span><a href="#42765109">next</a><span>|</span><label class="collapse" for="c-42763649">[-]</label><label class="expand" for="c-42763649">[3 more]</label></div><br/><div class="children"><div class="content">My guess is that OpenAI didn&#x27;t cheat as blatantly as just training on the test set. If they had, surely they could have gotten themselves an even higher mark than 25%. But I do buy the comment that they soft-cheated by using elements of the dataset for validation (which is absolutely still a form of data leakage). Even so, I suspect their reported number is roughly legit, because they report numbers on many benchmarks, and they have a good track record of those numbers holding up to private test sets.<p>What&#x27;s much more concerning to me than the integrity of the benchmark number is the general pattern of behavior here from OpenAI and Epoch. We shouldn&#x27;t accept secretly (even secret to the people doing the creation!) funding the creation of a benchmark. I also don&#x27;t see how we can trust in the integrity of EpochAI going forward. This is basically their only meaningful output, and this is how they handled it?</div><br/><div id="42764638" class="c"><input type="checkbox" id="c-42764638" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42763649">parent</a><span>|</span><a href="#42765109">next</a><span>|</span><label class="collapse" for="c-42764638">[-]</label><label class="expand" for="c-42764638">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  If they had, surely they could have gotten themselves an even higher mark than 25%.<p>there is potentially some limitation of LLMs memorizing such complex proofs</div><br/><div id="42765835" class="c"><input type="checkbox" id="c-42765835" checked=""/><div class="controls bullet"><span class="by">woopwoop</span><span>|</span><a href="#42763649">root</a><span>|</span><a href="#42764638">parent</a><span>|</span><a href="#42765109">next</a><span>|</span><label class="collapse" for="c-42765835">[-]</label><label class="expand" for="c-42765835">[1 more]</label></div><br/><div class="children"><div class="content">They aren&#x27;t proofs, they&#x27;re just numbers. All the questions have numerical answers. That&#x27;s how they&#x27;re evaluated.</div><br/></div></div></div></div></div></div><div id="42765109" class="c"><input type="checkbox" id="c-42765109" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#42763649">prev</a><span>|</span><a href="#42763495">next</a><span>|</span><label class="collapse" for="c-42765109">[-]</label><label class="expand" for="c-42765109">[1 more]</label></div><br/><div class="children"><div class="content">They should at least clarify it. The reason they don’t I feel is simply for the hype and mystique.<p>There are ways that you could game the benchmark without adding it to the training set. By repetitively evaluating on the dataset itself it will regress into a validation set, not a test set, even in black box setting, as you can simply  evaluating 100 checkpoints and pick the one that performs the best, rinse and repeat<p>I still believe o3 is the real deal, BUT this gimmick kind sour my appetite a bit, for that those who run the company</div><br/></div></div><div id="42763495" class="c"><input type="checkbox" id="c-42763495" checked=""/><div class="controls bullet"><span class="by">WasimBhai</span><span>|</span><a href="#42765109">prev</a><span>|</span><a href="#42765803">next</a><span>|</span><label class="collapse" for="c-42763495">[-]</label><label class="expand" for="c-42763495">[4 more]</label></div><br/><div class="children"><div class="content">I have been taking a course in AI policy and the O1 and the FrontierMath dataset has been an important mark for me to emphasize the world we are moving toward. It is incredibly sad to know about the conflict of interest here. However, those more knowledgeable, can you explain in plain words, does this revelation compromise OAI&#x27;s claims regarding o3&#x27;s performance on FrontierMath problems?</div><br/><div id="42763547" class="c"><input type="checkbox" id="c-42763547" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42763495">parent</a><span>|</span><a href="#42763517">next</a><span>|</span><label class="collapse" for="c-42763547">[-]</label><label class="expand" for="c-42763547">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worse than just an undeclared conflict of interest. They gave OpenAI all questions and solutions behind the scenes. It&#x27;s hard to chalk this up to only naivete. This is a &quot;sorry you caught me&quot; moment.</div><br/></div></div><div id="42763517" class="c"><input type="checkbox" id="c-42763517" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#42763495">parent</a><span>|</span><a href="#42763547">prev</a><span>|</span><a href="#42765803">next</a><span>|</span><label class="collapse" for="c-42763517">[-]</label><label class="expand" for="c-42763517">[2 more]</label></div><br/><div class="children"><div class="content">They have an oral agreement that OpenAI won&#x27;t use the benchmark in training. Which means first and foremost you have to consider the possibility that they broke that oral agreement and actually included the problems in the training set. Even if they didn&#x27;t, the fact that they had the problems means they could have selectively chosen the training set data to specialize in solving that class of problem, while still technically keeping the verbal agreement.<p>So, yeah, the benchmark needs to be treated as essentially worthless at this point.</div><br/><div id="42763642" class="c"><input type="checkbox" id="c-42763642" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42763495">root</a><span>|</span><a href="#42763517">parent</a><span>|</span><a href="#42765803">next</a><span>|</span><label class="collapse" for="c-42763642">[-]</label><label class="expand" for="c-42763642">[1 more]</label></div><br/><div class="children"><div class="content">If OpenAI wanted the questions&#x2F;solutions, there is going to be a reason for that. This data is not sitting in an unopened folder on Sam&#x27;s computer.<p>There are a lot of ways you can use data to improve a model without directly training on it. A train&#x2F;test validation loop, for example. Or as a wellspring for synthetic data generation. But all of these ways involve some level of data contamination, it&#x27;s unavoidable.</div><br/></div></div></div></div></div></div><div id="42765803" class="c"><input type="checkbox" id="c-42765803" checked=""/><div class="controls bullet"><span class="by">moi2388</span><span>|</span><a href="#42763495">prev</a><span>|</span><a href="#42763440">next</a><span>|</span><label class="collapse" for="c-42765803">[-]</label><label class="expand" for="c-42765803">[1 more]</label></div><br/><div class="children"><div class="content">“… we have a verbal agreement that these materials will not be used in model training”<p>What about model testing before releasing it?</div><br/></div></div><div id="42763440" class="c"><input type="checkbox" id="c-42763440" checked=""/><div class="controls bullet"><span class="by">nioj</span><span>|</span><a href="#42765803">prev</a><span>|</span><a href="#42764250">next</a><span>|</span><label class="collapse" for="c-42763440">[-]</label><label class="expand" for="c-42763440">[2 more]</label></div><br/><div class="children"><div class="content">Related <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42761648">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42761648</a></div><br/></div></div><div id="42764250" class="c"><input type="checkbox" id="c-42764250" checked=""/><div class="controls bullet"><span class="by">suchintan</span><span>|</span><a href="#42763440">prev</a><span>|</span><a href="#42763572">next</a><span>|</span><label class="collapse" for="c-42764250">[-]</label><label class="expand" for="c-42764250">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if more companies should open source their eval model outputs alongside the eval results<p>We tried doing that here at Skyvern (eval.skyvern.com)</div><br/></div></div><div id="42763534" class="c"><input type="checkbox" id="c-42763534" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42763572">prev</a><span>|</span><a href="#42763232">next</a><span>|</span><label class="collapse" for="c-42763534">[-]</label><label class="expand" for="c-42763534">[5 more]</label></div><br/><div class="children"><div class="content">Its increasingly odd to see HN activity that assumes the premise:  if the latest benchmark results involved a benchmark that can be shown to have any data that OpenAI could have accessed, then, the benchmark results were intentionally faked.<p>Last time this confused a bunch of people who didn&#x27;t understand what test vs. train data meant and it resulted in a particular luminary complaining on Twitter, to much guffaws, how troubling the situation was.<p>Literally every comment currently, modulo [1] assumes this and then goes several steps more, and a majority are wildly misusing terms with precise meanings, explaining at least part of their confusion.<p>[1] modulo the one saying this is irrelevant because we&#x27;ll know if it&#x27;s bad when it comes out, which to be fair, if evaluated rationally, we know that doesn&#x27;t help us narrowly with our suspicion FrontierMath benchmarks are all invalid because it trained on (most of) the solutions</div><br/><div id="42765493" class="c"><input type="checkbox" id="c-42765493" checked=""/><div class="controls bullet"><span class="by">EvgeniyZh</span><span>|</span><a href="#42763534">parent</a><span>|</span><a href="#42763232">next</a><span>|</span><label class="collapse" for="c-42765493">[-]</label><label class="expand" for="c-42765493">[4 more]</label></div><br/><div class="children"><div class="content">Why wouldn&#x27;t OpenAI cheat? It&#x27;s an open secret in industry that benchmarks are trained on. Everybody does it, so you need to do that or else your similarly performing model will look worse on paper.<p>And even they respect the agreement, even using test set as a validation set can be a huge advantage. That&#x27;s why validation set and test set are two different terms with precise meaning.<p>As for &quot;knowing it&#x27;s bad&quot;, most people won&#x27;t be able to tell a model scoring 25% and 10% apart. People who are using these models to solve math problems are tiny share of users and even tinier share of revenues. What OpenAI needs is to convince investors that there is still progress in capabilities going at high pace, and gaming the benchmarks makes perfect sense in this context. 25% was surprising and appeared to surpass expectations, which is exactly what OpenAI needs.</div><br/><div id="42765526" class="c"><input type="checkbox" id="c-42765526" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42763534">root</a><span>|</span><a href="#42765493">parent</a><span>|</span><a href="#42763232">next</a><span>|</span><label class="collapse" for="c-42765526">[-]</label><label class="expand" for="c-42765526">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Why wouldn&#x27;t OpenAI cheat? It&#x27;s an open secret in industry that benchmarks are trained on. Everybody does it, so you need to do that or else your similarly performing model will look worse on paper.<p>This starts with a fallacious appeal to cynicism combined with an unsubstantiated claim about widespread misconduct. The &quot;everybody does it&quot; argument is a classic rationalization that doesn&#x27;t actually justify anything. It also misunderstands the reputational and technical stakes - major labs face intense scrutiny of their methods and results, and there&#x27;s plenty of incestuous movement between labs and plenty of leaks.<p>&gt; And even they respect the agreement, even using test set as a validation set can be a huge advantage. That&#x27;s why validation set and test set are two different terms with precise meaning.<p>This part accidentally stumbles into a valid point about ML methodology while completely missing why it matters. Yes, validation and test sets serve different purposes - that&#x27;s precisely why reputable labs maintain strict separations between them. The implication that this basic principle somehow proves misconduct is backwards logic.<p>&gt; People who are using these models to solve math problems are tiny share of users and even tinier share of revenues.<p>This reveals a fundamental misunderstanding of why math capabilities matter. They&#x27;re not primarily about serving math users - they&#x27;re a key metric for abstract reasoning and systematic problem-solving abilities. This is basic ML evaluation theory.<p>&gt; What OpenAI needs is to convince investors that there is still progress in capabilities going at high pace, and gaming the benchmarks makes perfect sense in this context. 25% was surprising and appeared to surpass expectations, which is exactly what OpenAI needs.<p>This concludes with pure speculation presented as fact, combined with a conspiracy theory that lacks any actual evidence. It also displays a shallow understanding of how technical due diligence works in major AI investments - investors at this level typically have deep technical expertise, access to extensive testing and validation, and most damningly, given the reductive appeal to incentive structure:<p><i>They closed the big round weeks before.</i><p>The whole comment reads like someone who has picked up some ML terminology but lacks fundamental understanding of how research evaluation, technical accountability, and institutional incentives actually work in the field. The dismissive tone and casual accusations of misconduct don&#x27;t help their credibility either.</div><br/><div id="42766396" class="c"><input type="checkbox" id="c-42766396" checked=""/><div class="controls bullet"><span class="by">BeefWellington</span><span>|</span><a href="#42763534">root</a><span>|</span><a href="#42765526">parent</a><span>|</span><a href="#42763232">next</a><span>|</span><label class="collapse" for="c-42766396">[-]</label><label class="expand" for="c-42766396">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The &quot;everybody does it&quot; argument is a classic rationalization that doesn&#x27;t actually justify anything.<p>I&#x27;d argue here the more relevant point is &quot;these specific people have been shown to have done it before.&quot;<p>&gt; The whole comment reads like someone who has picked up some ML terminology but lacks fundamental understanding of how research evaluation, technical accountability, and institutional incentives actually work in the field. The dismissive tone and casual accusations of misconduct don&#x27;t help their credibility either.<p>I think what you&#x27;re missing is the observation that so very little of that is actually applied in this case. &quot;AI&quot; here is not being treated as an actual science would be. The majority of the papers pumped out of these places are not real concrete research, not submitted to journals, and not peer reviewed works.</div><br/><div id="42766471" class="c"><input type="checkbox" id="c-42766471" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42763534">root</a><span>|</span><a href="#42766396">parent</a><span>|</span><a href="#42763232">next</a><span>|</span><label class="collapse" for="c-42766471">[-]</label><label class="expand" for="c-42766471">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;d argue here the more relevant point is &quot;these specific people have been shown to have done it before.&quot;<p>This is itself a slippery move. A vague gesture at past misconduct without actually specifying any incidents. If there&#x27;s a clear pattern of documented benchmark manipulation, name it. Which benchmarks? When? What was the evidence? Without specifics, this is just trading one form of handwaving (&quot;everyone does it&quot;) for another (&quot;they did it before&quot;).<p>&gt; &quot;AI&quot; here is not being treated as an actual science would be.<p>There&#x27;s some truth here but also some sleight of hand. Yes, AI development often moves outside traditional academic channels. But, you imply this automatically means less rigor, which doesn&#x27;t follow. Many industry labs have internal review processes, replication requirements, and validation procedures that can be as or more stringent than academic peer review. The fact that something isn&#x27;t in Nature doesn&#x27;t automatically make it less rigorous.<p>&gt; The majority of the papers pumped out of these places are not real concrete research, not submitted to journals, and not peer reviewed works.<p>This combines three questionable implications:<p>- That non-journal publications are automatically &quot;not real concrete research&quot; (tell that to physics&#x2F;math arXiv)<p>- That peer review is binary - either traditional journal review or nothing (ignoring internal review processes, community peer review, public replications)<p>- That volume (&quot;pumped out&quot;) correlates with quality<p>You&#x27;re making a valid critique of AI&#x27;s departure from traditional academic structures, but then making an unjustified leap to assuming this means no rigor at all. It&#x27;s like saying because a restaurant isn&#x27;t Michelin-starred, it must have no food safety standards.<p>This also ignores the massive reputational and financial stakes that create strong incentives for internal rigor. Major labs have to maintain credibility with:<p>- Their own employees.<p>- Other researchers who will try to replicate results.<p>- Partners integrating their technology.<p>- Investors doing technical due diligence.<p>- Regulators scrutinizing their claims.<p>The idea that they would casually risk all that just to bump up one benchmark number (but not <i>too</i> much! just from 10% to 35%) doesn&#x27;t align with the actual incentive structure these organizations face.<p>Both the original comment and this fall into the same trap - mistaking cynicism for sophistication while actually displaying a somewhat superficial understanding of how modern AI research and development actually operates.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42763347" class="c"><input type="checkbox" id="c-42763347" checked=""/><div class="controls bullet"><span class="by">treksis</span><span>|</span><a href="#42763232">prev</a><span>|</span><a href="#42763428">next</a><span>|</span><label class="collapse" for="c-42763347">[-]</label><label class="expand" for="c-42763347">[1 more]</label></div><br/><div class="children"><div class="content">so it was overfit</div><br/></div></div><div id="42763428" class="c"><input type="checkbox" id="c-42763428" checked=""/><div class="controls bullet"><span class="by">numba888</span><span>|</span><a href="#42763347">prev</a><span>|</span><a href="#42763305">next</a><span>|</span><label class="collapse" for="c-42763428">[-]</label><label class="expand" for="c-42763428">[3 more]</label></div><br/><div class="children"><div class="content">if they used it in training it should be 100% hit. most likely they used it to verify and tune parameters.</div><br/><div id="42763681" class="c"><input type="checkbox" id="c-42763681" checked=""/><div class="controls bullet"><span class="by">rrr_oh_man</span><span>|</span><a href="#42763428">parent</a><span>|</span><a href="#42763671">next</a><span>|</span><label class="collapse" for="c-42763681">[-]</label><label class="expand" for="c-42763681">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if they used it in training it should be 100% hit.<p>Not necessarily, no.<p>A statistical model will attempt to minimise overall loss, generally speaking.<p>If it gets 100% accuracy on the training data it&#x27;s usually an overfit. (Hugging the data points too tightly, thereby failing to predict real life cases)</div><br/></div></div><div id="42763671" class="c"><input type="checkbox" id="c-42763671" checked=""/><div class="controls bullet"><span class="by">g-b-r</span><span>|</span><a href="#42763428">parent</a><span>|</span><a href="#42763681">prev</a><span>|</span><a href="#42763305">next</a><span>|</span><label class="collapse" for="c-42763671">[-]</label><label class="expand" for="c-42763671">[1 more]</label></div><br/><div class="children"><div class="content">Had they let it hit 100% it would have been obvious they had the data.<p>They&#x27;ve sure been careful to avoid that, by only using a portion of it or some other technique</div><br/></div></div></div></div><div id="42763481" class="c"><input type="checkbox" id="c-42763481" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42763305">prev</a><span>|</span><a href="#42764569">next</a><span>|</span><label class="collapse" for="c-42763481">[-]</label><label class="expand" for="c-42763481">[1 more]</label></div><br/><div class="children"><div class="content">This don’t really matter much because if the models suck when it comes out evals mean nothing next time</div><br/></div></div></div></div></div></div></div></body></html>