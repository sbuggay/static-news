<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712480451597" as="style"/><link rel="stylesheet" href="styles.css?v=1712480451597"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.05120">More Agents Is All You Need: LLMs performance scales with the number of agents</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>TaurenHunter</span> | <span>129 comments</span></div><br/><div><div id="39957530" class="c"><input type="checkbox" id="c-39957530" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#39957184">next</a><span>|</span><label class="collapse" for="c-39957530">[-]</label><label class="expand" for="c-39957530">[45 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure people in these comments are reading this paper correctly.<p>This seems to essentially disprove the whole idea of multi-agent setups like Chain-of-thought and LLM-Debate.<p>Because this paper introduces their alternative method which simply runs the same query multiple times on the same LLM, without any context shared across queries. And then they run a similarity algorithm on the answers and pick the most common answer. <i>(Which makes sense to me. If an LLM is giving you a mixture of &quot;hallucinations&quot; and correct answers, the correct answers will similar and the hallucinations will hopefully be chaotic)</i><p>And this simple algorithm preform just as well (and sometimes better) than all the other multi-agent algorithms.<p>This suggests that the other multi-agent schemes with their clever prompts aren&#x27;t really doing anything special; Their improve results are coming mostly from the fact that the LLM is run multiple times, that the prompt asks the LLM to pick the best answer.</div><br/><div id="39957635" class="c"><input type="checkbox" id="c-39957635" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39959192">next</a><span>|</span><label class="collapse" for="c-39957635">[-]</label><label class="expand" for="c-39957635">[22 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Because this paper introduces their alternative method which simply runs the same query multiple times on the same LLM, without any context shared across queries. And then they run a similarity algorithm on the answers and pick the most common answer.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lorenz_system" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lorenz_system</a><p>Years ago weather simulations started tweaking input params and running their models over and over. Discarding outliers, taking averages. It works pretty well.<p>Because LLM&#x27;s mostly have random seeds (aka temperature) feeding them the same input and averaging the output is going to get you a better guess.<p>Lorenz also gives some clues (if not an outright explanation) as to why the &quot;hallucination&quot; problem is likely unsolvable.<p>If you buy into this line of thinking then it quickly becomes apparent that LLM&#x27;s are more or less a dead end when it comes to AGI. Simulating isnt emulating... an LLM is as likely to become intelligent as a forecast is to control the weather.</div><br/><div id="39959210" class="c"><input type="checkbox" id="c-39959210" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957635">parent</a><span>|</span><a href="#39957784">next</a><span>|</span><label class="collapse" for="c-39959210">[-]</label><label class="expand" for="c-39959210">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If you buy into this line of thinking then it quickly becomes apparent that LLM&#x27;s are more or less a dead end when it comes to AGI. Simulating isnt emulating... an LLM is as likely to become intelligent as a forecast is to control the weather<p>Up until this point, I agree.<p>This puts humans on too high a pedestal: LLMs aren&#x27;t magic, and we&#x27;re not magic either.<p>(There&#x27;s other reasons for me to think Transformers aren&#x27;t the answer, but not this kind of reasoning).</div><br/><div id="39959346" class="c"><input type="checkbox" id="c-39959346" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39959210">parent</a><span>|</span><a href="#39957784">next</a><span>|</span><label class="collapse" for="c-39959346">[-]</label><label class="expand" for="c-39959346">[1 more]</label></div><br/><div class="children"><div class="content">&gt; we&#x27;re not magic either<p>We pretty much are compared to present-day neural architectures. How many simulated neurons and synapses are in the largest architectures, and how do those numbers compare to humans?</div><br/></div></div></div></div><div id="39957784" class="c"><input type="checkbox" id="c-39957784" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957635">parent</a><span>|</span><a href="#39959210">prev</a><span>|</span><a href="#39957775">next</a><span>|</span><label class="collapse" for="c-39957784">[-]</label><label class="expand" for="c-39957784">[10 more]</label></div><br/><div class="children"><div class="content">&gt; <i>it quickly becomes apparent that LLM&#x27;s are more or less a dead end when it comes to AGI.</i><p>On the contrary, sit and listen in a college cafeteria, and it quickly becomes apparent most conversation participants are LLMs.*<p>&gt; <i>Simulating isnt emulating...</i><p>These are not synonyms, true.<p>&gt; <i>an LLM is as likely to become intelligent as a forecast is to control the weather.</i><p>I don&#x27;t see uncertainty of intelligence as a property of an LLM as being equivalent to certainty of weather control as a effect of a forecast.<p>Among other things, whether weather was controlled would tend to be agreed by all observers, while it&#x27;s often unclear if intelligence is being observed in these threads.  :-)<p>---<p>* While my last line was a joke, humans in LLM mode was not.  We can drive on autopilot, and get where we need to go while not being able to remember how we got there.  We definitely converse on autopilot, indistinguishably from LLMs talking to each other, after an opening line every word of every sentence in the entire exchange perfectly predictable to a stranger.  Are the speakers intelligent?  What about the stranger who knows what they will say next?  To say LLMs are not intelligent is easier if we agree humans spend a good deal of time being unintelligent.</div><br/><div id="39959242" class="c"><input type="checkbox" id="c-39959242" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957784">parent</a><span>|</span><a href="#39957903">next</a><span>|</span><label class="collapse" for="c-39959242">[-]</label><label class="expand" for="c-39959242">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We definitely converse on autopilot, indistinguishably from LLMs talking to each other, after an opening line every word of every sentence in the entire exchange perfectly predictable to a stranger<p>Some people report speaking like this: opening their mouths and not knowing how the sentence will end.<p>I don&#x27;t experience that, I think.<p>Possibly used to? I have in the past had some autonomous verbal responses, for a bit this included echoing greetings — great when it&#x27;s &quot;hello&quot;, embarrassing when it&#x27;s &quot;happy birthday&quot;.<p>&gt; To say LLMs are not intelligent is easier if we agree humans spend a good deal of time being unintelligent<p>Kinda; System 1, system 2 — the best LLMs do better than most people&#x27;s system 1, worse than most people&#x27;s system 2. Bat and ball, $1.10.</div><br/></div></div><div id="39957903" class="c"><input type="checkbox" id="c-39957903" checked=""/><div class="controls bullet"><span class="by">hooande</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957784">parent</a><span>|</span><a href="#39959242">prev</a><span>|</span><a href="#39957959">next</a><span>|</span><label class="collapse" for="c-39957903">[-]</label><label class="expand" for="c-39957903">[1 more]</label></div><br/><div class="children"><div class="content">LLMs were specifically trained to emulate human interaction patterns. Of course we sound like them at times. It&#x27;s the things we can do that they can&#x27;t that are relevant.<p>If I study Einstein and learn to do a really good impression, the statement &quot;Einstein often sounds like karmacondon&quot; will be true. That does not make me Einstein.</div><br/></div></div><div id="39957959" class="c"><input type="checkbox" id="c-39957959" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957784">parent</a><span>|</span><a href="#39957903">prev</a><span>|</span><a href="#39958299">next</a><span>|</span><label class="collapse" for="c-39957959">[-]</label><label class="expand" for="c-39957959">[5 more]</label></div><br/><div class="children"><div class="content">&gt;&gt;&gt; I don&#x27;t see uncertainty of intelligence as a property of an LLM as being equivalent to certainty of weather control as a effect of a forecast.<p>GTA 5 is a simulation. Do you expect to be arrested out side your front door for the car you stole in game?<p>Weather forecasting is a simulation, it tells you what the weather will look like in the next few days. It gets better as we get more sensors, collect more data and build more accurate models based on those two factors. It will never leap to weather.<p>Language forecasting (because this is what an LLM is) is a simulation. It tells you what the next token (word) will be based on what came before it. It gets better as we collect more data and hone and refine these models. It will never make the leap to intelligence.<p>&gt;&gt; To say LLMs are not intelligent is easier if we agree humans spend a good deal of time being unintelligent.<p>To say that LLMs are intelligent means that language is a requirement for intelligence. Thats some fairly magical thinking... buy any sufficently advanced technology...</div><br/><div id="39959273" class="c"><input type="checkbox" id="c-39959273" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957959">parent</a><span>|</span><a href="#39958164">next</a><span>|</span><label class="collapse" for="c-39959273">[-]</label><label class="expand" for="c-39959273">[1 more]</label></div><br/><div class="children"><div class="content">&gt; To say that LLMs are intelligent means that language is a requirement for intelligence. Thats some fairly magical thinking... buy any sufficently advanced technology..<p>When I was a kid, it was <i>the</i> definition of intelligence that separated humans from animals.<p>And there&#x27;s a reason &quot;dumb&quot; means &quot;mute&quot; and independently &quot;stupid&quot;.<p>It may well be an incorrect requirement. It may be a single form of intelligence out of many which happen to correlate in humans, but not in minds created by artifice.<p>But it does have a history.</div><br/></div></div><div id="39958164" class="c"><input type="checkbox" id="c-39958164" checked=""/><div class="controls bullet"><span class="by">theendisney</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957959">parent</a><span>|</span><a href="#39959273">prev</a><span>|</span><a href="#39958426">next</a><span>|</span><label class="collapse" for="c-39958164">[-]</label><label class="expand" for="c-39958164">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if it counts but there is a police chase video online some place with a guy on drugs who claims he thought he was playing gta. The way he throws people out of their vehicle and crashes their car suggests he wasnt lying.</div><br/></div></div><div id="39958426" class="c"><input type="checkbox" id="c-39958426" checked=""/><div class="controls bullet"><span class="by">ToValueFunfetti</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957959">parent</a><span>|</span><a href="#39958164">prev</a><span>|</span><a href="#39958299">next</a><span>|</span><label class="collapse" for="c-39958426">[-]</label><label class="expand" for="c-39958426">[2 more]</label></div><br/><div class="children"><div class="content">Intelligence breaks the pattern here. A simulated intelligence is intelligent, just as simulated math is math and simulated computers are computers. The point of contention shouldn&#x27;t be whether LLMs are intelligences or simulated intelligences, but whether they&#x27;re simulating something else.</div><br/><div id="39958563" class="c"><input type="checkbox" id="c-39958563" checked=""/><div class="controls bullet"><span class="by">qarl</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958426">parent</a><span>|</span><a href="#39958299">next</a><span>|</span><label class="collapse" for="c-39958563">[-]</label><label class="expand" for="c-39958563">[1 more]</label></div><br/><div class="children"><div class="content">Right.  This is Searle&#x27;s &quot;a simulated plane won&#x27;t get you to Japan&quot; argument.<p>That&#x27;s true.  But a simulated calculator is perfectly effective for doing your taxes.</div><br/></div></div></div></div></div></div><div id="39958299" class="c"><input type="checkbox" id="c-39958299" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957784">parent</a><span>|</span><a href="#39957959">prev</a><span>|</span><a href="#39957775">next</a><span>|</span><label class="collapse" for="c-39958299">[-]</label><label class="expand" for="c-39958299">[2 more]</label></div><br/><div class="children"><div class="content">&gt; On the contrary, sit and listen in a college cafeteria, and it quickly becomes apparent most conversation participants are LLMs.*<p>Excuse the bluntness, but you&#x27;re the CTO of a fintech company. Your analysis of people&#x27;s social life is probably the as valuable as a janitors.</div><br/><div id="39958323" class="c"><input type="checkbox" id="c-39958323" checked=""/><div class="controls bullet"><span class="by">taberiand</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958299">parent</a><span>|</span><a href="#39957775">next</a><span>|</span><label class="collapse" for="c-39958323">[-]</label><label class="expand" for="c-39958323">[1 more]</label></div><br/><div class="children"><div class="content">I expect an observant janitor would have quite useful insights into people&#x27;s social lives</div><br/></div></div></div></div></div></div><div id="39957775" class="c"><input type="checkbox" id="c-39957775" checked=""/><div class="controls bullet"><span class="by">RyEgswuCsn</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957635">parent</a><span>|</span><a href="#39957784">prev</a><span>|</span><a href="#39957710">next</a><span>|</span><label class="collapse" for="c-39957775">[-]</label><label class="expand" for="c-39957775">[1 more]</label></div><br/><div class="children"><div class="content">Except that a weather forecasting model can&#x27;t experiment on weather, but a LLM system may be designed to be able to perform experiments and take feedbacks?</div><br/></div></div><div id="39957710" class="c"><input type="checkbox" id="c-39957710" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957635">parent</a><span>|</span><a href="#39957775">prev</a><span>|</span><a href="#39957725">next</a><span>|</span><label class="collapse" for="c-39957710">[-]</label><label class="expand" for="c-39957710">[4 more]</label></div><br/><div class="children"><div class="content">&gt;  LLM&#x27;s are more or less a dead end when it comes to AGI.<p>I don&#x27;t think many people believe that LLMs are a way to AGI (whatever that actually means). But LLMs can still have many valid uses even if their prospects are limited in scope.</div><br/><div id="39959165" class="c"><input type="checkbox" id="c-39959165" checked=""/><div class="controls bullet"><span class="by">aerhardt</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957710">parent</a><span>|</span><a href="#39958388">next</a><span>|</span><label class="collapse" for="c-39959165">[-]</label><label class="expand" for="c-39959165">[1 more]</label></div><br/><div class="children"><div class="content">There are plenty of people - technical and non-technical - who seem to be acting like AGI is right around the corner thanks to LLMs, and who are, more broadly, vastly overstating the current capabilities of LLMs. I’m observing this in real life as much as on the internet. There are two very distinct groups of people that stand out to me: (1) High level execs with vested interests around AI and (2) Managers who haven’t even bothered to create an OpenAI account and are asking their subordinates to use ChatGPT for them, in what is an unforeseen usage of LLMs: by human proxy.</div><br/></div></div><div id="39958388" class="c"><input type="checkbox" id="c-39958388" checked=""/><div class="controls bullet"><span class="by">buu700</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957710">parent</a><span>|</span><a href="#39959165">prev</a><span>|</span><a href="#39957965">next</a><span>|</span><label class="collapse" for="c-39958388">[-]</label><label class="expand" for="c-39958388">[1 more]</label></div><br/><div class="children"><div class="content">I recently read an interesting thread that laid out the case for LLMs being a path to AGI: <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;singularity&#x2F;comments&#x2F;13ox85j&#x2F;how_does_an_llm_even_work&#x2F;jl6unp4" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;singularity&#x2F;comments&#x2F;13ox85j&#x2F;how_do...</a><p>The argument boils down to the idea that language isn&#x27;t simply strings of words or bits of factual information, but an actual encoding of logic. By training statistical models on vast amounts of logic, we&#x27;ve given them a generalizable ability to perform logic. A sufficiently advanced LLM could thus potentially fulfill some definition of AGI.<p>To be clear, this doesn&#x27;t in any way imply that LLMs could ever fit the definition of artificial consciousness, which would be a completely different form of strong AI. They&#x27;re effectively just mathematical functions (albeit extremely complicated ones), which simply take inputs and return outputs without any intervening subjective experience. Even if they can perform a complicated task, retrieve and effectively summarize complicated information, or say all the right things as a conversational partner, they have no concept of the meaning of their output.<p>Maybe that limitation in itself puts a ceiling on their potential. Maybe the best possible LLM can only ever be 99.99% effective, and that 0.01% of the time it will go completely off the rails and disregard its instructions or hallucinate something ridiculous. Maybe the only way to overcome that is by keeping a human or a true artificial consciousness in the loop, in which case LLMs would still be extremely useful, but a flawed AGI if &quot;AGI&quot; at all. Or maybe a sufficiently advanced LLM and&#x2F;or a sufficiently advanced error correction architecture will actually be enough to mitigate those issues.<p>I don&#x27;t have a strong opinion on where LLMs are ultimately headed, but I&#x27;m looking forward to seeing how it all unfolds. It&#x27;s amazing how capabilities that were strictly in the realm of sci-fi so quickly became mundane.</div><br/></div></div><div id="39957965" class="c"><input type="checkbox" id="c-39957965" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957710">parent</a><span>|</span><a href="#39958388">prev</a><span>|</span><a href="#39957725">next</a><span>|</span><label class="collapse" for="c-39957965">[-]</label><label class="expand" for="c-39957965">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; I don&#x27;t think many people believe that LLMs are a way to AGI<p>Please tell Sam Altman ASAP<p>Thanks!</div><br/></div></div></div></div><div id="39957725" class="c"><input type="checkbox" id="c-39957725" checked=""/><div class="controls bullet"><span class="by">DiogenesKynikos</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957635">parent</a><span>|</span><a href="#39957710">prev</a><span>|</span><a href="#39959192">next</a><span>|</span><label class="collapse" for="c-39957725">[-]</label><label class="expand" for="c-39957725">[4 more]</label></div><br/><div class="children"><div class="content">LLMs already are intelligent. They&#x27;re not the same as humans, but they are able to give intelligent answers to highly nontrivial questions.</div><br/><div id="39957842" class="c"><input type="checkbox" id="c-39957842" checked=""/><div class="controls bullet"><span class="by">hackable_sand</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957725">parent</a><span>|</span><a href="#39959192">next</a><span>|</span><label class="collapse" for="c-39957842">[-]</label><label class="expand" for="c-39957842">[3 more]</label></div><br/><div class="children"><div class="content">I have yet to see an LLM that is cooperative. The magic of collaborating with someone is that we can both understand the problem and reason about it.<p>The current degree of LLM intelligence is not compelling for a social creature like me.</div><br/><div id="39959314" class="c"><input type="checkbox" id="c-39959314" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957842">parent</a><span>|</span><a href="#39958172">next</a><span>|</span><label class="collapse" for="c-39959314">[-]</label><label class="expand" for="c-39959314">[1 more]</label></div><br/><div class="children"><div class="content">Surprised to read that.<p>I use them as a cooperative partner by default.<p>Also: quite a few people have had instances work with other instances, sometimes of the same model and sometimes of other models.</div><br/></div></div><div id="39958172" class="c"><input type="checkbox" id="c-39958172" checked=""/><div class="controls bullet"><span class="by">theendisney</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957842">parent</a><span>|</span><a href="#39959314">prev</a><span>|</span><a href="#39959192">next</a><span>|</span><label class="collapse" for="c-39958172">[-]</label><label class="expand" for="c-39958172">[1 more]</label></div><br/><div class="children"><div class="content">Is it even allowed to ask questions??<p>Edit: my sience fiction joke in the 90s was AI though bots chatting in irc channels. They could seemlesly integrate human intelligence that way.</div><br/></div></div></div></div></div></div></div></div><div id="39959192" class="c"><input type="checkbox" id="c-39959192" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39957635">prev</a><span>|</span><a href="#39958757">next</a><span>|</span><label class="collapse" for="c-39959192">[-]</label><label class="expand" for="c-39959192">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Which makes sense to me. If an LLM is giving you a mixture of &quot;hallucinations&quot; and correct answers, the correct answers will similar and the hallucinations will hopefully be chaotic<p>I expect that to give you something close to the confidence of the underlying model to some specific claim, which is good, but I still expect legends (urban and cultural) to be high-ranked.<p>They&#x27;d be very human mistakes, but still mistakes.<p>I think the only way past that is to build a world model, look for contradictions, and then look for new evidence to resolve those contradictions.</div><br/><div id="39959275" class="c"><input type="checkbox" id="c-39959275" checked=""/><div class="controls bullet"><span class="by">dmarchand90</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39959192">parent</a><span>|</span><a href="#39958757">next</a><span>|</span><label class="collapse" for="c-39959275">[-]</label><label class="expand" for="c-39959275">[1 more]</label></div><br/><div class="children"><div class="content">Be interesting to plug this into a bayesian optimization like framework: find out regions of language space where the models maximally disagree and then target those areas for extra training</div><br/></div></div></div></div><div id="39958757" class="c"><input type="checkbox" id="c-39958757" checked=""/><div class="controls bullet"><span class="by">0x008</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39959192">prev</a><span>|</span><a href="#39958703">next</a><span>|</span><label class="collapse" for="c-39958757">[-]</label><label class="expand" for="c-39958757">[2 more]</label></div><br/><div class="children"><div class="content">This is a very similar idea to ensemble models, which have been used for a long time in ML and proven to be very good. You average out the results of several predictors (or you let them vote and pick the most common prediction value), thereby reducing the noise in the prediction by choosing the common denominator of multiple predictions.</div><br/><div id="39958835" class="c"><input type="checkbox" id="c-39958835" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958757">parent</a><span>|</span><a href="#39958703">next</a><span>|</span><label class="collapse" for="c-39958835">[-]</label><label class="expand" for="c-39958835">[1 more]</label></div><br/><div class="children"><div class="content">This is done in aerospace as well… however, even different teams clean room writing to the same spec have the tendency to make the same errors in their code, which ends up breaking the statistical model when this model was selected.</div><br/></div></div></div></div><div id="39958703" class="c"><input type="checkbox" id="c-39958703" checked=""/><div class="controls bullet"><span class="by">m_kos</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39958757">prev</a><span>|</span><a href="#39958097">next</a><span>|</span><label class="collapse" for="c-39958703">[-]</label><label class="expand" for="c-39958703">[1 more]</label></div><br/><div class="children"><div class="content">I had a very similar idea a few months ago. I wanted to use this approach to have the LLM provide the probability that the generated answer is correct. The probability would simply be what fraction of all generated answers was the one selected. (Each generated answer would be generated with a different seed and the question would be of single choice kind.) The two issues I found were 1) the cost, 2) on some problems, LLMs can be wrong more often than they are not.<p>Hopefully, as inference gets cheaper and of higher quality, someone will come up with a more feasible solution.</div><br/></div></div><div id="39958097" class="c"><input type="checkbox" id="c-39958097" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39958703">prev</a><span>|</span><a href="#39957600">next</a><span>|</span><label class="collapse" for="c-39958097">[-]</label><label class="expand" for="c-39958097">[3 more]</label></div><br/><div class="children"><div class="content">Could multiple agents be used such that tokens emitted from LLM A is passed to B and output of B is passed to A meaning 2 agents will be being used to generate an output in a simple round Robin way? Both will share context in this case. My computer isn&#x27;t big enough run two large models but this can be tried on tiny models perhaps.<p>I realize that for more than two and very specialised agents this will require some intelligent way to pass the output to specialist agents only. And also this means that their must be some overlap between the agents.</div><br/><div id="39958301" class="c"><input type="checkbox" id="c-39958301" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958097">parent</a><span>|</span><a href="#39957600">next</a><span>|</span><label class="collapse" for="c-39958301">[-]</label><label class="expand" for="c-39958301">[2 more]</label></div><br/><div class="children"><div class="content">That is what’s already been done under the term &quot;multi-agent&quot;. This paper argues that there’s no need for any such message-passing or context sharing, you just literally run the same query several times on the same model, fully independently, and then pick a &quot;typical&quot; reply according to some similarity metric.</div><br/><div id="39958736" class="c"><input type="checkbox" id="c-39958736" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958301">parent</a><span>|</span><a href="#39957600">next</a><span>|</span><label class="collapse" for="c-39958736">[-]</label><label class="expand" for="c-39958736">[1 more]</label></div><br/><div class="children"><div class="content">The paper says that it enhances multi-agent methods. It is not a replacement for that. It&#x27;s an enhancement for existing methods.</div><br/></div></div></div></div></div></div><div id="39957600" class="c"><input type="checkbox" id="c-39957600" checked=""/><div class="controls bullet"><span class="by">sinuhe69</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39958097">prev</a><span>|</span><a href="#39958785">next</a><span>|</span><label class="collapse" for="c-39957600">[-]</label><label class="expand" for="c-39957600">[8 more]</label></div><br/><div class="children"><div class="content">But if I set the temperature to 0, the model will pick the highest probable token and the output will be always the same. But we already know that by no mean it can guarantee a correct answer. So how can multiple runs be better?</div><br/><div id="39957644" class="c"><input type="checkbox" id="c-39957644" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957600">parent</a><span>|</span><a href="#39958785">next</a><span>|</span><label class="collapse" for="c-39957644">[-]</label><label class="expand" for="c-39957644">[7 more]</label></div><br/><div class="children"><div class="content">Yes, but picking the most similar output from a bunch of queries with a higher temperature is not the same thing as the output from a single low temperature query.</div><br/><div id="39957936" class="c"><input type="checkbox" id="c-39957936" checked=""/><div class="controls bullet"><span class="by">sinuhe69</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957644">parent</a><span>|</span><a href="#39958785">next</a><span>|</span><label class="collapse" for="c-39957936">[-]</label><label class="expand" for="c-39957936">[6 more]</label></div><br/><div class="children"><div class="content">Possibly, but it stills doesn’t explain why multiple runs will result in better answer. In the work, the authors also hasn’t compared the multiple runs results with the single run using zero temperature. So, maybe all the overhead is just to achieve the same result already encoded in the networks? I don’t know.<p>Also the result is somewhat counterintuitive. We know that by low level of understanding, if we ask a student a hard question and he tried many times, the most accurate answer is often not the most popular one but a single answer. And that by retaining memory, reasoning capacity and continuous learning , which is not the case with  LLM.<p>Btw: HN is for discussion. If some just want to vote for the beauty contest, please leave.</div><br/><div id="39958125" class="c"><input type="checkbox" id="c-39958125" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957936">parent</a><span>|</span><a href="#39958372">next</a><span>|</span><label class="collapse" for="c-39958125">[-]</label><label class="expand" for="c-39958125">[3 more]</label></div><br/><div class="children"><div class="content">I found this other paper that tests Temperature: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.05201" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.05201</a><p>It appears that temperature has no impact on problem solving performance. So this paper isn&#x27;t getting improved performance because the token for the correct answer is more probable.<p>My theory is that the multiple queries are allowing the whole probability space of possible answers to be sampled. Not just the probabilities of the most likely output token, but the probabilities of all possible internal model states.<p>And sampling that probability space of the whole model state and finding the average is a very different mathematical operation to just picking a single model state at random and then picking the most probable output tokens.</div><br/><div id="39958403" class="c"><input type="checkbox" id="c-39958403" checked=""/><div class="controls bullet"><span class="by">bt1a</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958125">parent</a><span>|</span><a href="#39958351">next</a><span>|</span><label class="collapse" for="c-39958403">[-]</label><label class="expand" for="c-39958403">[1 more]</label></div><br/><div class="children"><div class="content">If I&#x27;m reading this correctly, they had to discard Llama 2 answers and only use GPT-3.5 given answers to test the hypothesis.<p>GPT-3.5 answering questions through the OAI API alone is not an acceptable method of testing problem solving ability across a range of temperatures. OpenAI does some blackbox wizardry on their end.<p>There are many complex and clever sampling techniques for which temperature is just one (possibly dynamic) component<p>One example from the llama.cpp codebase is dynamic temperature sampling<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;4972&#x2F;files">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;4972&#x2F;files</a><p>Not sure what you mean by whole model state given that there are tens of thousands of possible tokens and the models have billions of parameters in XX,XXX-dimensional space. How many queries across how many sampling methods might you need? Err..how much time? :)</div><br/></div></div><div id="39958351" class="c"><input type="checkbox" id="c-39958351" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958125">parent</a><span>|</span><a href="#39958403">prev</a><span>|</span><a href="#39958372">next</a><span>|</span><label class="collapse" for="c-39958351">[-]</label><label class="expand" for="c-39958351">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if there is a clever&#x2F;more efficient shortcut that could come from before the sample is taken on each token. We have the logits after all.</div><br/></div></div></div></div><div id="39958372" class="c"><input type="checkbox" id="c-39958372" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957936">parent</a><span>|</span><a href="#39958125">prev</a><span>|</span><a href="#39958471">next</a><span>|</span><label class="collapse" for="c-39958372">[-]</label><label class="expand" for="c-39958372">[1 more]</label></div><br/><div class="children"><div class="content">Just from reading comments around, it feels intuitive to me that looking at a heatmap of cascading pendulum would be more “accurate” than looking at just one snapshot, and also that joints on the pendulums don’t necessarily need to be interlinked between iterations of simulations</div><br/></div></div><div id="39958471" class="c"><input type="checkbox" id="c-39958471" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39957936">parent</a><span>|</span><a href="#39958372">prev</a><span>|</span><a href="#39958785">next</a><span>|</span><label class="collapse" for="c-39958471">[-]</label><label class="expand" for="c-39958471">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Also the result is somewhat counterintuitive. We know that by low level of understanding, if we ask a student a hard question and he tried many times, the most accurate answer is often not the most popular one but a single answer.<p>This is a bad analogy.<p>Here’s what is <i>actually happening</i> with no “common sense but wrong” understanding of it:<p>- You have a set of probabilities per token.<p>- You randomize them.<p>This is not a “bad student being asked multiple times” it is a system with randomized probabilities, creating a probability <i>distribution</i>.<p>If you want to see what a probability distribution looks like (eg. An electron cloud) then <i>sampling only once</i> is the wrong way to do it.<p>You basically have two distributions; the first one is the LLM, the second one is the shape generated by adding the random factor in the temperature.<p>This allows you to escape the “local maxima” encoded in the LLM distribution to find <i>highly probable solutions</i> that are outside the sample space of the “zero temperature”.<p>If you want a better analogy, look up at the night sky full of stars. Draw circle in the sky; that’s the LLM distribution.<p>The result from a zero temperature will be the brightest point in that circle.<p>When you push the temperature up, you blur the sky randomly. Some points become brighter, some dimmer, but the <i>radius of the circle increases</i>.<p>If there is a very bright point <i>outside the sample circle</i> 10x brighter than the brightest point <i>inside it</i> then repeated random samples will repeatedly find it.<p>It makes perfect sense that an expanded probability distribution sampled repeatedly could find a “good average solution” if that solution is significantly better than the best “zero temp” solution.<p>This is the same reason we have &#x27;temp&#x27; at all; by widening the solution space probability distribution, you can find better maxima. Turns out, sampling multiple times lets you have more chances to find better maxima.<p>This is more like &quot;well that seems obviously like a good idea&quot; than &quot;somewhat counterintuitive&quot;; it&#x27;s just slow and expensive to do it.<p>You can <i>also</i> adjust the probability distribution by other existing methods, obviously, what&#x27;s surprising here is not that it works, but that it seem to work <i>so well</i>; probably (and I note they did not try this in their paper), a multi-sample + voting on the output from other methods would also be highly effective.</div><br/></div></div></div></div></div></div></div></div><div id="39958785" class="c"><input type="checkbox" id="c-39958785" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39957600">prev</a><span>|</span><a href="#39958039">next</a><span>|</span><label class="collapse" for="c-39958785">[-]</label><label class="expand" for="c-39958785">[1 more]</label></div><br/><div class="children"><div class="content">Good news is that you can use this setup for self supervised RL (artificial dreaming? increasing contrast?).</div><br/></div></div><div id="39958039" class="c"><input type="checkbox" id="c-39958039" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39958785">prev</a><span>|</span><a href="#39958708">next</a><span>|</span><label class="collapse" for="c-39958039">[-]</label><label class="expand" for="c-39958039">[4 more]</label></div><br/><div class="children"><div class="content">how is chain-of-thought multi-agent?</div><br/><div id="39958288" class="c"><input type="checkbox" id="c-39958288" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958039">parent</a><span>|</span><a href="#39958106">next</a><span>|</span><label class="collapse" for="c-39958288">[-]</label><label class="expand" for="c-39958288">[2 more]</label></div><br/><div class="children"><div class="content">How is what’s described here chain-of-thought?</div><br/><div id="39958338" class="c"><input type="checkbox" id="c-39958338" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39957530">root</a><span>|</span><a href="#39958288">parent</a><span>|</span><a href="#39958106">next</a><span>|</span><label class="collapse" for="c-39958338">[-]</label><label class="expand" for="c-39958338">[1 more]</label></div><br/><div class="children"><div class="content">They were replying to this:<p>&gt; This seems to essentially disprove the whole idea of multi-agent setups like Chain-of-thought and LLM-Debate.</div><br/></div></div></div></div></div></div><div id="39958708" class="c"><input type="checkbox" id="c-39958708" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39957530">parent</a><span>|</span><a href="#39958039">prev</a><span>|</span><a href="#39957184">next</a><span>|</span><label class="collapse" for="c-39958708">[-]</label><label class="expand" for="c-39958708">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m not sure people in these comments are reading this paper correctly.
&gt; This seems to essentially disprove the whole idea of multi-agent setups like Chain-of-thought and LLM-Debate.<p>I&#x27;m not sure you have read the paper at all. Chain of thought prompting is not a multi-agent algorithm. The paper says that it enhances existing methods such as prompt engineering (chain of thought) and multi-agent debate. The sampling method presented in the paper is orthogonal to those methods.</div><br/></div></div></div></div><div id="39957184" class="c"><input type="checkbox" id="c-39957184" checked=""/><div class="controls bullet"><span class="by">infogulch</span><span>|</span><a href="#39957530">prev</a><span>|</span><a href="#39957054">next</a><span>|</span><label class="collapse" for="c-39957184">[-]</label><label class="expand" for="c-39957184">[7 more]</label></div><br/><div class="children"><div class="content">This seems related to an interesting recent ACM ByteCast podcast episode with Edward Chang, an Adjunct Professor in the Department of Computer Science at Stanford University. [1] (Note there is a transcript if you don&#x27;t want to listen.)<p>The approach he uses is to arrange for multiple LLMs to dialogue between each other about a discussion topic where the human acts as a moderator instead of the question&#x2F;answer format that LLMs commonly take today. They find that the final answer that multiple LLMs come to in dialogue results in a huge improvement in both precision and accuracy for the same resources.<p>[1]: <a href="https:&#x2F;&#x2F;learning.acm.org&#x2F;bytecast&#x2F;ep50-edward-y-chang" rel="nofollow">https:&#x2F;&#x2F;learning.acm.org&#x2F;bytecast&#x2F;ep50-edward-y-chang</a></div><br/><div id="39957761" class="c"><input type="checkbox" id="c-39957761" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#39957184">parent</a><span>|</span><a href="#39957500">next</a><span>|</span><label class="collapse" for="c-39957761">[-]</label><label class="expand" for="c-39957761">[5 more]</label></div><br/><div class="children"><div class="content">This paper suggests that you don&#x27;t need the debating part: just get LLM to work on the problem independently, and choose the most popular answer.</div><br/><div id="39958688" class="c"><input type="checkbox" id="c-39958688" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39957184">root</a><span>|</span><a href="#39957761">parent</a><span>|</span><a href="#39957963">next</a><span>|</span><label class="collapse" for="c-39958688">[-]</label><label class="expand" for="c-39958688">[1 more]</label></div><br/><div class="children"><div class="content">The paper says that it enhances existing methods such as prompt engineering (chain of thought) and LLM debate. This agent method is orthogonal to LLM debate.</div><br/></div></div><div id="39957963" class="c"><input type="checkbox" id="c-39957963" checked=""/><div class="controls bullet"><span class="by">infogulch</span><span>|</span><a href="#39957184">root</a><span>|</span><a href="#39957761">parent</a><span>|</span><a href="#39958688">prev</a><span>|</span><a href="#39957500">next</a><span>|</span><label class="collapse" for="c-39957963">[-]</label><label class="expand" for="c-39957963">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. Somehow it seems odd to add randomness (temperature) and then wash it away by averaging it out.</div><br/><div id="39958254" class="c"><input type="checkbox" id="c-39958254" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#39957184">root</a><span>|</span><a href="#39957963">parent</a><span>|</span><a href="#39958088">next</a><span>|</span><label class="collapse" for="c-39958254">[-]</label><label class="expand" for="c-39958254">[1 more]</label></div><br/><div class="children"><div class="content">In optimization problems, randomness can often get you out of local minima&#x2F;maxima, and so averaging out a bunch of random search paths might get you better results in the worst case. Something similar might be happening here. The training set will be biased in various ways that might create weird local min&#x2F;max points and so this process could avoid those weird kinks.</div><br/></div></div><div id="39958088" class="c"><input type="checkbox" id="c-39958088" checked=""/><div class="controls bullet"><span class="by">ewild</span><span>|</span><a href="#39957184">root</a><span>|</span><a href="#39957963">parent</a><span>|</span><a href="#39958254">prev</a><span>|</span><a href="#39957500">next</a><span>|</span><label class="collapse" for="c-39958088">[-]</label><label class="expand" for="c-39958088">[1 more]</label></div><br/><div class="children"><div class="content">temp applies to each token so the range of temperature is significantly larger than the average being pulled</div><br/></div></div></div></div></div></div><div id="39957500" class="c"><input type="checkbox" id="c-39957500" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#39957184">parent</a><span>|</span><a href="#39957761">prev</a><span>|</span><a href="#39957054">next</a><span>|</span><label class="collapse" for="c-39957500">[-]</label><label class="expand" for="c-39957500">[1 more]</label></div><br/><div class="children"><div class="content">Is this effectively describing something like crewai?</div><br/></div></div></div></div><div id="39957054" class="c"><input type="checkbox" id="c-39957054" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39957184">prev</a><span>|</span><a href="#39959270">next</a><span>|</span><label class="collapse" for="c-39957054">[-]</label><label class="expand" for="c-39957054">[23 more]</label></div><br/><div class="children"><div class="content">Finally. I&#x27;ve been saying that we need to stop focusing on a single agent getting everything right and instead layer agents for about 16 months now, but it&#x27;s great to have a paper to point to.<p>It&#x27;s interesting that the diminishing returns for tasks flatten out rapidly around the same size as the ideal human meeting sizes: <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;figure&#x2F;18-Optimal-Meeting-Sizes-1_tbl6_3889414" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;figure&#x2F;18-Optimal-Meeting-Sizes...</a><p>If this was done at more granular steps of agent quantity I&#x27;m curious just how closely it would match those numbers.<p>I&#x27;d also really love to see the eventual follow-up where we see how much more performance can be obtained when the agents are each fine tuned towards slightly different aims. I&#x27;d expect there&#x27;d even be a performance lift from just having the agents each set at different temperature levels.<p>Very happy to see the research community starting to step in this direction!</div><br/><div id="39957251" class="c"><input type="checkbox" id="c-39957251" checked=""/><div class="controls bullet"><span class="by">blazingbanana</span><span>|</span><a href="#39957054">parent</a><span>|</span><a href="#39957140">next</a><span>|</span><label class="collapse" for="c-39957251">[-]</label><label class="expand" for="c-39957251">[2 more]</label></div><br/><div class="children"><div class="content">I couldn&#x27;t agree more. You should check out LLMWare&#x27;s SLIM agents (<a href="https:&#x2F;&#x2F;github.com&#x2F;llmware-ai&#x2F;llmware&#x2F;tree&#x2F;main&#x2F;examples&#x2F;SLIM-Agents">https:&#x2F;&#x2F;github.com&#x2F;llmware-ai&#x2F;llmware&#x2F;tree&#x2F;main&#x2F;examples&#x2F;SLI...</a>). It&#x27;s focusing on pretty much exactly this and chaining multiple local LLMs together.<p>A really good topic that ties in with this is the need for deterministic sampling (I may have the terminology a bit incorrect) depending on what the model is indended for. The LLMWare team did a good 2 part video on this here as well (<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=7oMTGhSKuNY" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=7oMTGhSKuNY</a>)<p>I think dedicated miniture LLMs are the way forward.<p>Disclaimer - Not affiliated with them in any way, just think it&#x27;s a really cool project.</div><br/><div id="39958752" class="c"><input type="checkbox" id="c-39958752" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957251">parent</a><span>|</span><a href="#39957140">next</a><span>|</span><label class="collapse" for="c-39958752">[-]</label><label class="expand" for="c-39958752">[1 more]</label></div><br/><div class="children"><div class="content">Great videos.<p>I have one personal niggle: I get annoyed when we end up lying to ourselves. Regarding the 101 section in video 1 - People forgot this the day LLMs came out. I felt this was too generous with the benefit of doubt.<p>This basic point was and remains constantly argued - with “Emergence” and anthropomorphization being the heart of the opposing argument.</div><br/></div></div></div></div><div id="39957140" class="c"><input type="checkbox" id="c-39957140" checked=""/><div class="controls bullet"><span class="by">piloto_ciego</span><span>|</span><a href="#39957054">parent</a><span>|</span><a href="#39957251">prev</a><span>|</span><a href="#39957352">next</a><span>|</span><label class="collapse" for="c-39957140">[-]</label><label class="expand" for="c-39957140">[16 more]</label></div><br/><div class="children"><div class="content">This is how I think humans work.  We have 5 or 8 versions of us running around in our skulls or whatever and one of them is somewhat of a supervisor.</div><br/><div id="39957691" class="c"><input type="checkbox" id="c-39957691" checked=""/><div class="controls bullet"><span class="by">Trasmatta</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957140">parent</a><span>|</span><a href="#39957296">next</a><span>|</span><label class="collapse" for="c-39957691">[-]</label><label class="expand" for="c-39957691">[3 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s way more than 8 even. And it&#x27;s common to have many working as supervisors, often at conflict with each other. And some act out the automatic trauma responses, as they&#x27;re stuck in the past when the trauma occurred.</div><br/><div id="39958626" class="c"><input type="checkbox" id="c-39958626" checked=""/><div class="controls bullet"><span class="by">champdebloom</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957691">parent</a><span>|</span><a href="#39958084">next</a><span>|</span><label class="collapse" for="c-39958626">[-]</label><label class="expand" for="c-39958626">[1 more]</label></div><br/><div class="children"><div class="content">This sounds very much like the internal family systems model: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Internal_Family_Systems_Model" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Internal_Family_Systems_Mode...</a></div><br/></div></div><div id="39958084" class="c"><input type="checkbox" id="c-39958084" checked=""/><div class="controls bullet"><span class="by">piloto_ciego</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957691">parent</a><span>|</span><a href="#39958626">prev</a><span>|</span><a href="#39957296">next</a><span>|</span><label class="collapse" for="c-39958084">[-]</label><label class="expand" for="c-39958084">[1 more]</label></div><br/><div class="children"><div class="content">Right it’s like a random forest or something maybe.</div><br/></div></div></div></div><div id="39957296" class="c"><input type="checkbox" id="c-39957296" checked=""/><div class="controls bullet"><span class="by">xnx</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957140">parent</a><span>|</span><a href="#39957691">prev</a><span>|</span><a href="#39957875">next</a><span>|</span><label class="collapse" for="c-39957296">[-]</label><label class="expand" for="c-39957296">[3 more]</label></div><br/><div class="children"><div class="content">Multi Homunculus Model</div><br/><div id="39958071" class="c"><input type="checkbox" id="c-39958071" checked=""/><div class="controls bullet"><span class="by">piloto_ciego</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957296">parent</a><span>|</span><a href="#39959052">next</a><span>|</span><label class="collapse" for="c-39958071">[-]</label><label class="expand" for="c-39958071">[1 more]</label></div><br/><div class="children"><div class="content">I was thinking something like Julian Jayne’s Bicameral mind, but you’re always arguing with yourself lol</div><br/></div></div><div id="39959052" class="c"><input type="checkbox" id="c-39959052" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957296">parent</a><span>|</span><a href="#39958071">prev</a><span>|</span><a href="#39957875">next</a><span>|</span><label class="collapse" for="c-39959052">[-]</label><label class="expand" for="c-39959052">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like dennett&#x27;s multiple drafts hypothesis.</div><br/></div></div></div></div><div id="39957875" class="c"><input type="checkbox" id="c-39957875" checked=""/><div class="controls bullet"><span class="by">hackable_sand</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957140">parent</a><span>|</span><a href="#39957296">prev</a><span>|</span><a href="#39957720">next</a><span>|</span><label class="collapse" for="c-39957875">[-]</label><label class="expand" for="c-39957875">[1 more]</label></div><br/><div class="children"><div class="content">Maybe. I&#x27;m sure one&#x27;s consciousness corresponds with one&#x27;s guiding philosophy.<p>I don&#x27;t think this supervisor model is generally applicable to people with EFD or some forms of Autism, for example.</div><br/></div></div><div id="39957720" class="c"><input type="checkbox" id="c-39957720" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957140">parent</a><span>|</span><a href="#39957875">prev</a><span>|</span><a href="#39957178">next</a><span>|</span><label class="collapse" for="c-39957720">[-]</label><label class="expand" for="c-39957720">[1 more]</label></div><br/><div class="children"><div class="content">We have tons of specialized components that work together cooperatively and competitively. There’s multiple ways they connect. There also seems to be global processes that happen, like during sleep. There’s over 3,000 cell types per the BRAIN initiative. Every brain forms on it’s own taking shape like something out of a Transformers movie.<p>God’s design is mostly nothing like man’s neural networks. It’s far superior. Brains are also what’s creating all the artificial, neural nets on top of all math, tech, and economic systems that they run on. AI’s got a lot of catching up to do.</div><br/></div></div><div id="39957178" class="c"><input type="checkbox" id="c-39957178" checked=""/><div class="controls bullet"><span class="by">huffmsa</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957140">parent</a><span>|</span><a href="#39957720">prev</a><span>|</span><a href="#39957238">next</a><span>|</span><label class="collapse" for="c-39957178">[-]</label><label class="expand" for="c-39957178">[6 more]</label></div><br/><div class="children"><div class="content">I personally visualize them at a table from time to time.</div><br/><div id="39957294" class="c"><input type="checkbox" id="c-39957294" checked=""/><div class="controls bullet"><span class="by">infogulch</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957178">parent</a><span>|</span><a href="#39957237">next</a><span>|</span><label class="collapse" for="c-39957294">[-]</label><label class="expand" for="c-39957294">[4 more]</label></div><br/><div class="children"><div class="content">Do they have to collaborate to decide who gets to use the forks?</div><br/><div id="39958265" class="c"><input type="checkbox" id="c-39958265" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957294">parent</a><span>|</span><a href="#39957395">next</a><span>|</span><label class="collapse" for="c-39958265">[-]</label><label class="expand" for="c-39958265">[1 more]</label></div><br/><div class="children"><div class="content">Of course not, you don&#x27;t need forks while playing poker.</div><br/></div></div><div id="39957395" class="c"><input type="checkbox" id="c-39957395" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957294">parent</a><span>|</span><a href="#39958265">prev</a><span>|</span><a href="#39957237">next</a><span>|</span><label class="collapse" for="c-39957395">[-]</label><label class="expand" for="c-39957395">[2 more]</label></div><br/><div class="children"><div class="content">That sounds like a culturally bound phenomenon, shouldn&#x27;t they be using chopsticks?</div><br/><div id="39957572" class="c"><input type="checkbox" id="c-39957572" checked=""/><div class="controls bullet"><span class="by">infogulch</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957395">parent</a><span>|</span><a href="#39957237">next</a><span>|</span><label class="collapse" for="c-39957572">[-]</label><label class="expand" for="c-39957572">[1 more]</label></div><br/><div class="children"><div class="content">Chopsticks makes sense because there&#x27;s only 5 of them and they have to share the 5 chopsticks.</div><br/></div></div></div></div></div></div><div id="39957237" class="c"><input type="checkbox" id="c-39957237" checked=""/><div class="controls bullet"><span class="by">_ink_</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957178">parent</a><span>|</span><a href="#39957294">prev</a><span>|</span><a href="#39957238">next</a><span>|</span><label class="collapse" for="c-39957237">[-]</label><label class="expand" for="c-39957237">[1 more]</label></div><br/><div class="children"><div class="content">How do they look? Do they look differently?</div><br/></div></div></div></div><div id="39957238" class="c"><input type="checkbox" id="c-39957238" checked=""/><div class="controls bullet"><span class="by">suriya-ganesh</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957140">parent</a><span>|</span><a href="#39957178">prev</a><span>|</span><a href="#39957352">next</a><span>|</span><label class="collapse" for="c-39957238">[-]</label><label class="expand" for="c-39957238">[1 more]</label></div><br/><div class="children"><div class="content">And et voila, you have the script of inside out. \s<p>But honestly I do think this is how we operate. Depending on our state of metabolism and other psychological factors, the dominant version changes but as a whole we remain the sum total of all these versions.</div><br/></div></div></div></div><div id="39957352" class="c"><input type="checkbox" id="c-39957352" checked=""/><div class="controls bullet"><span class="by">jondwillis</span><span>|</span><a href="#39957054">parent</a><span>|</span><a href="#39957140">prev</a><span>|</span><a href="#39957180">next</a><span>|</span><label class="collapse" for="c-39957352">[-]</label><label class="expand" for="c-39957352">[1 more]</label></div><br/><div class="children"><div class="content">I was working on multi-agent systems for problem solving via <a href="https:&#x2F;&#x2F;github.com&#x2F;agi-merge&#x2F;waggle-dance">https:&#x2F;&#x2F;github.com&#x2F;agi-merge&#x2F;waggle-dance</a> for months last year!</div><br/></div></div><div id="39957180" class="c"><input type="checkbox" id="c-39957180" checked=""/><div class="controls bullet"><span class="by">zarathustreal</span><span>|</span><a href="#39957054">parent</a><span>|</span><a href="#39957352">prev</a><span>|</span><a href="#39957550">next</a><span>|</span><label class="collapse" for="c-39957180">[-]</label><label class="expand" for="c-39957180">[2 more]</label></div><br/><div class="children"><div class="content">“Each fine tuned toward slightly different aims”<p>So…a sort of <i>mixture of experts</i> if you will</div><br/><div id="39958316" class="c"><input type="checkbox" id="c-39958316" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39957054">root</a><span>|</span><a href="#39957180">parent</a><span>|</span><a href="#39957550">next</a><span>|</span><label class="collapse" for="c-39958316">[-]</label><label class="expand" for="c-39958316">[1 more]</label></div><br/><div class="children"><div class="content">Kind of. More like a mixture of a mixture of experts.<p>The problem is MoE on its own isn&#x27;t able to use the context as a scratch pad for differentiated CoT trees.<p>So you have a mixture of token suggestions, but a singular chain of thought.<p>A mixture of both is probably going to perform better than just a mixture of the former, especially given everything we know by now regarding in context learning or the degree of transmission synthetic data is carrying.</div><br/></div></div></div></div><div id="39957550" class="c"><input type="checkbox" id="c-39957550" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#39957054">parent</a><span>|</span><a href="#39957180">prev</a><span>|</span><a href="#39959270">next</a><span>|</span><label class="collapse" for="c-39957550">[-]</label><label class="expand" for="c-39957550">[1 more]</label></div><br/><div class="children"><div class="content">It seems funny that the researchers are studying what people are building to experiment. crewAI is one example.</div><br/></div></div></div></div><div id="39959270" class="c"><input type="checkbox" id="c-39959270" checked=""/><div class="controls bullet"><span class="by">yantrams</span><span>|</span><a href="#39957054">prev</a><span>|</span><a href="#39957153">next</a><span>|</span><label class="collapse" for="c-39959270">[-]</label><label class="expand" for="c-39959270">[1 more]</label></div><br/><div class="children"><div class="content">This is my go to method for pretty much every hard problem that I&#x27;m forced to solve where I don&#x27;t have the domain expertise &#x2F; interest &#x2F;  time. The trick lies in coming up with a clever similarity metric that incorporates penalties etc. You can even go a level deeper and use multiple similarity algorithms and then poll on top of them. Here&#x27;s a taxonomy extractor for text that I made using similar principles that is surprisingly as good as anything else that I&#x27;ve seen - <a href="https:&#x2F;&#x2F;dash.scooptent.com&#x2F;text" rel="nofollow">https:&#x2F;&#x2F;dash.scooptent.com&#x2F;text</a></div><br/></div></div><div id="39957153" class="c"><input type="checkbox" id="c-39957153" checked=""/><div class="controls bullet"><span class="by">nicklecompte</span><span>|</span><a href="#39959270">prev</a><span>|</span><a href="#39957391">next</a><span>|</span><label class="collapse" for="c-39957153">[-]</label><label class="expand" for="c-39957153">[2 more]</label></div><br/><div class="children"><div class="content">One frustration I&#x27;ve had with all this mixture-of-experts research:<p>Randomized Algorithms 101 - or basic stochastic reasoning - suggests that if the temperature parameter is &gt; 0, querying an LLM N times and picking the majority result (perhaps with an N+1th query to the LLM) will generally result in better performance than asking it once and choosing that result.<p>It seems plausible to me that the gains can be further improved with a specialized mixture of different LLMs (which could then be run at temp = 0), or by finding better ways to break tasks into subtasks as this paper suggests. But AFAICT nobody has done anything to actually quantify these hypothetical gains versus the dumb randomized algorithm approach! In particular there might be voting strategies or mixtures - even specific models - where MoE&#x2F;etc is strictly worse than naive repetition.<p>I am a concerned citizen w.r.t LLMs rather than a researcher, so I might be missing something. It just seems odd that LLM researchers forgot the first chapter of Motwani&#x2F;Raghavan.</div><br/><div id="39957538" class="c"><input type="checkbox" id="c-39957538" checked=""/><div class="controls bullet"><span class="by">benaubin</span><span>|</span><a href="#39957153">parent</a><span>|</span><a href="#39957391">next</a><span>|</span><label class="collapse" for="c-39957538">[-]</label><label class="expand" for="c-39957538">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d assume that there&#x27;s a difference between picking the best _token_ across an assortment of randomly selected tokens, versus picking the best _string_ of randomly-selected tokens.</div><br/></div></div></div></div><div id="39957391" class="c"><input type="checkbox" id="c-39957391" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#39957153">prev</a><span>|</span><a href="#39957826">next</a><span>|</span><label class="collapse" for="c-39957391">[-]</label><label class="expand" for="c-39957391">[1 more]</label></div><br/><div class="children"><div class="content">Eyeballing the graphs, it seems that most of the gain is with 10 agents, a bit more with 20, and there are diminishing returns after that. Apparently, more agents isn&#x27;t going to do it.</div><br/></div></div><div id="39957826" class="c"><input type="checkbox" id="c-39957826" checked=""/><div class="controls bullet"><span class="by">trash_cat</span><span>|</span><a href="#39957391">prev</a><span>|</span><a href="#39956021">next</a><span>|</span><label class="collapse" for="c-39957826">[-]</label><label class="expand" for="c-39957826">[5 more]</label></div><br/><div class="children"><div class="content">Is this not an incredibly expensive&#x2F;unsustainable method? I agree with the sentiment that MoE is the way to go as the newer models will probably see diminishing returns. But the compute for a single prompt will suddenly increase 7-15 fold?</div><br/><div id="39958242" class="c"><input type="checkbox" id="c-39958242" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#39957826">parent</a><span>|</span><a href="#39958062">next</a><span>|</span><label class="collapse" for="c-39958242">[-]</label><label class="expand" for="c-39958242">[1 more]</label></div><br/><div class="children"><div class="content">If GPT4 is 20x the price of GPT3.5, but it only takes 10x GPT3.5 runs to get similar quality of response (and likely faster), you&#x27;ll still come out ahead.</div><br/></div></div><div id="39958062" class="c"><input type="checkbox" id="c-39958062" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#39957826">parent</a><span>|</span><a href="#39958242">prev</a><span>|</span><a href="#39958893">next</a><span>|</span><label class="collapse" for="c-39958062">[-]</label><label class="expand" for="c-39958062">[1 more]</label></div><br/><div class="children"><div class="content">&quot;all you need is a six figure OpenAI bill&quot;</div><br/></div></div><div id="39958893" class="c"><input type="checkbox" id="c-39958893" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39957826">parent</a><span>|</span><a href="#39958062">prev</a><span>|</span><a href="#39957904">next</a><span>|</span><label class="collapse" for="c-39958893">[-]</label><label class="expand" for="c-39958893">[1 more]</label></div><br/><div class="children"><div class="content">So what? It&#x27;s not like GPUs are compute starved.</div><br/></div></div><div id="39957904" class="c"><input type="checkbox" id="c-39957904" checked=""/><div class="controls bullet"><span class="by">bigEnotation</span><span>|</span><a href="#39957826">parent</a><span>|</span><a href="#39958893">prev</a><span>|</span><a href="#39956021">next</a><span>|</span><label class="collapse" for="c-39957904">[-]</label><label class="expand" for="c-39957904">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, see GPT 3.5 vs GPT 4 pricing.</div><br/></div></div></div></div><div id="39956021" class="c"><input type="checkbox" id="c-39956021" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#39957826">prev</a><span>|</span><a href="#39957157">next</a><span>|</span><label class="collapse" for="c-39956021">[-]</label><label class="expand" for="c-39956021">[1 more]</label></div><br/><div class="children"><div class="content">They have a public repo: <a href="https:&#x2F;&#x2F;anonymous.4open.science&#x2F;r&#x2F;more_agent_is_all_you_need&#x2F;README.md" rel="nofollow">https:&#x2F;&#x2F;anonymous.4open.science&#x2F;r&#x2F;more_agent_is_all_you_need...</a><p>Prompts they used for the benchmarks: <a href="https:&#x2F;&#x2F;anonymous.4open.science&#x2F;r&#x2F;more_agent_is_all_you_need&#x2F;src&#x2F;prompt_lib.py" rel="nofollow">https:&#x2F;&#x2F;anonymous.4open.science&#x2F;r&#x2F;more_agent_is_all_you_need...</a><p>Super interesting. It would be cool to see something like this, but benchmarking LLM-based agents using a set of tools</div><br/></div></div><div id="39957157" class="c"><input type="checkbox" id="c-39957157" checked=""/><div class="controls bullet"><span class="by">hopfenspergerj</span><span>|</span><a href="#39956021">prev</a><span>|</span><a href="#39958227">next</a><span>|</span><label class="collapse" for="c-39957157">[-]</label><label class="expand" for="c-39957157">[4 more]</label></div><br/><div class="children"><div class="content">Ensemble of any number GPT 3.5 agents is less accurate than one call to GPT-4.</div><br/><div id="39957852" class="c"><input type="checkbox" id="c-39957852" checked=""/><div class="controls bullet"><span class="by">trash_cat</span><span>|</span><a href="#39957157">parent</a><span>|</span><a href="#39958227">next</a><span>|</span><label class="collapse" for="c-39957852">[-]</label><label class="expand" for="c-39957852">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny because GPT-4 is actually a pile of 3.5s. You just need to set it up correctly.</div><br/><div id="39958061" class="c"><input type="checkbox" id="c-39958061" checked=""/><div class="controls bullet"><span class="by">jamala1</span><span>|</span><a href="#39957157">root</a><span>|</span><a href="#39957852">parent</a><span>|</span><a href="#39958056">next</a><span>|</span><label class="collapse" for="c-39958061">[-]</label><label class="expand" for="c-39958061">[1 more]</label></div><br/><div class="children"><div class="content">I guess it&#x27;s the difference between an ensemble and a mixture of experts, i.e. aggregating outputs from (a) model(s) trained on the same data vs different data (GPT-4). Though GPT-4 presumably does not aggregate, but it routes.</div><br/></div></div><div id="39958056" class="c"><input type="checkbox" id="c-39958056" checked=""/><div class="controls bullet"><span class="by">BinRoo</span><span>|</span><a href="#39957157">root</a><span>|</span><a href="#39957852">parent</a><span>|</span><a href="#39958061">prev</a><span>|</span><a href="#39958227">next</a><span>|</span><label class="collapse" for="c-39958056">[-]</label><label class="expand" for="c-39958056">[1 more]</label></div><br/><div class="children"><div class="content">&gt; GPT-4 is actually a pile of 3.5s<p>I understand the intension and reference you&#x27;re making. I bet the implementation of GPT-4 is probably something along those lines. However, spreading speculation in definitive language like that when the truth is unknown is dishonest, wouldn&#x27;t you agree?</div><br/></div></div></div></div></div></div><div id="39957743" class="c"><input type="checkbox" id="c-39957743" checked=""/><div class="controls bullet"><span class="by">atum47</span><span>|</span><a href="#39958227">prev</a><span>|</span><a href="#39958716">next</a><span>|</span><label class="collapse" for="c-39957743">[-]</label><label class="expand" for="c-39957743">[1 more]</label></div><br/><div class="children"><div class="content">If we sum all those &quot;x is all you need&quot; we&#x27;re going to realize that we need a lot of things</div><br/></div></div><div id="39958716" class="c"><input type="checkbox" id="c-39958716" checked=""/><div class="controls bullet"><span class="by">whiteandnerdy</span><span>|</span><a href="#39957743">prev</a><span>|</span><a href="#39957109">next</a><span>|</span><label class="collapse" for="c-39958716">[-]</label><label class="expand" for="c-39958716">[1 more]</label></div><br/><div class="children"><div class="content">I remember hearing that Beam Search doesn&#x27;t work well for LLMs, because it leads to repetitive, generic output.<p>The majority vote sampling technique in this paper sounds like it&#x27;d give similar output to Beam Search, because it&#x27;s sampling sequences of tokens from a joint distribution. So why doesn&#x27;t it give repetitive output like Beam Search does? What am I missing?</div><br/></div></div><div id="39957109" class="c"><input type="checkbox" id="c-39957109" checked=""/><div class="controls bullet"><span class="by">TheCaptain4815</span><span>|</span><a href="#39958716">prev</a><span>|</span><a href="#39958117">next</a><span>|</span><label class="collapse" for="c-39957109">[-]</label><label class="expand" for="c-39957109">[2 more]</label></div><br/><div class="children"><div class="content">Recommend anyone interested in a agent framework lookup AutoGen by Microsoft</div><br/><div id="39958249" class="c"><input type="checkbox" id="c-39958249" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#39957109">parent</a><span>|</span><a href="#39958117">next</a><span>|</span><label class="collapse" for="c-39958249">[-]</label><label class="expand" for="c-39958249">[1 more]</label></div><br/><div class="children"><div class="content">This paper is specifically disproving the efficacy of agentic frameworks like AutoGen.<p>Also, the built-in function-calling in GPT4 is simpler to use than AutoGen2&#x27;s abstraction.</div><br/></div></div></div></div><div id="39958117" class="c"><input type="checkbox" id="c-39958117" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#39957109">prev</a><span>|</span><a href="#39957096">next</a><span>|</span><label class="collapse" for="c-39958117">[-]</label><label class="expand" for="c-39958117">[3 more]</label></div><br/><div class="children"><div class="content">Having given this problem a great deal of thought, I have developed a strong intuition around this. I believe not only is AGI feasible, it is already doable.<p>For example, several hundred GPT-4 based agents specializing in different skill sets should be able to collaboratively solve many problems. Their ability to work on so many facets of the same problem will make them very effective against multidisciplinary problems.<p>What’s the catch? Well, the back and forth has to play out in a serial order, so it cannot be parallelized. At today’s abysmal inference speeds, it may take this AGI many times longer than a trained human. Now imagine the effectiveness of this method when we can speed up inference to several hundred times a minute. Now AGI suddenly becomes way more efficient than a human.</div><br/><div id="39958730" class="c"><input type="checkbox" id="c-39958730" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#39958117">parent</a><span>|</span><a href="#39958121">next</a><span>|</span><label class="collapse" for="c-39958730">[-]</label><label class="expand" for="c-39958730">[1 more]</label></div><br/><div class="children"><div class="content">Definition of AGI at play here?</div><br/></div></div><div id="39958121" class="c"><input type="checkbox" id="c-39958121" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#39958117">parent</a><span>|</span><a href="#39958730">prev</a><span>|</span><a href="#39957096">next</a><span>|</span><label class="collapse" for="c-39958121">[-]</label><label class="expand" for="c-39958121">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a paper about that<p><a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;publication&#x2F;sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;publication&#x2F;sparks-...</a></div><br/></div></div></div></div><div id="39957096" class="c"><input type="checkbox" id="c-39957096" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#39958117">prev</a><span>|</span><a href="#39958060">next</a><span>|</span><label class="collapse" for="c-39957096">[-]</label><label class="expand" for="c-39957096">[2 more]</label></div><br/><div class="children"><div class="content">So this is an ensemble of many LLMs?<p>I wonder how well a bunch of LLMs trained on personal computers, so fairly small, could perform together?<p>Train a LLM on your emails, train an LLM on a text book, download a bunch of arbitrary LLMs from the net you find interesting, throw them all together into a big pile, and use a moderator LLM that knows how to format their output into an assistant format.<p>So, the email LLM would try to autocomplete sentences from your emails, and the text book LLM would try to autocomplete sentences from the text book. People could offer LLMs to download, almost as a way of compressing information, download the LLM of your favorite programming language, and TV series, etc. The important part would be having a moderator algorithm that can shape these LLMs from dumb sentence autocompleters (barely more than a fancy Markov chain) into a coherent assistant format. For example, the text book LLM would just endlessly spew semi-random sentences from the text, but a good moderator algorithm could see that it has sufficiently answered the question and cut it off.<p>In short, it&#x27;s interesting that separate LLMs can integrate with each other and strengthen each other and it makes me wonder if we could build modular LLMs.</div><br/><div id="39957388" class="c"><input type="checkbox" id="c-39957388" checked=""/><div class="controls bullet"><span class="by">mofosyne</span><span>|</span><a href="#39957096">parent</a><span>|</span><a href="#39958060">next</a><span>|</span><label class="collapse" for="c-39957388">[-]</label><label class="expand" for="c-39957388">[1 more]</label></div><br/><div class="children"><div class="content">Your idea inspired me to see what such a microstory based on your idea would look like (Of course generated by ChatGPT3.5):<p>&gt; As I delved into my computer, eager to tackle my to-do list, I was met with an unexpected sight: a digital love triangle among the Language Models (LLMs). The Email LLM, with its quick wit, seemed to be engaging in flirtatious banter with the verbose Textbook LLM, while the Programming Language LLM watched on with amusement. I couldn&#x27;t help but laugh at the absurdity of it all, but as the bickering between the LLMs intensified, I realized their antics were hindering my progress. With a mixture of frustration and amusement, I gently redirected the LLMs back to their intended purpose, finally able to accomplish my task amidst the chaotic comedy within my computer.</div><br/></div></div></div></div><div id="39958060" class="c"><input type="checkbox" id="c-39958060" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#39957096">prev</a><span>|</span><a href="#39958592">next</a><span>|</span><label class="collapse" for="c-39958060">[-]</label><label class="expand" for="c-39958060">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure all the AI companies love the idea of running the same prompt 8 times...</div><br/><div id="39958069" class="c"><input type="checkbox" id="c-39958069" checked=""/><div class="controls bullet"><span class="by">datascienced</span><span>|</span><a href="#39958060">parent</a><span>|</span><a href="#39958592">next</a><span>|</span><label class="collapse" for="c-39958069">[-]</label><label class="expand" for="c-39958069">[1 more]</label></div><br/><div class="children"><div class="content">NVDA does for sure</div><br/></div></div></div></div><div id="39958592" class="c"><input type="checkbox" id="c-39958592" checked=""/><div class="controls bullet"><span class="by">etamponi</span><span>|</span><a href="#39958060">prev</a><span>|</span><a href="#39957376">next</a><span>|</span><label class="collapse" for="c-39958592">[-]</label><label class="expand" for="c-39958592">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this the same as lowering the temperature of the LLM?</div><br/></div></div><div id="39957376" class="c"><input type="checkbox" id="c-39957376" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39958592">prev</a><span>|</span><a href="#39958789">next</a><span>|</span><label class="collapse" for="c-39957376">[-]</label><label class="expand" for="c-39957376">[1 more]</label></div><br/><div class="children"><div class="content">Here is a link to the main diagram: <a href="https:&#x2F;&#x2F;anonymous.4open.science&#x2F;r&#x2F;more_agent_is_all_you_need&#x2F;img&#x2F;algorithm_2.pdf" rel="nofollow">https:&#x2F;&#x2F;anonymous.4open.science&#x2F;r&#x2F;more_agent_is_all_you_need...</a><p>Seems like a pretty brute force approach of frankly just throwing more compute at the query (via semi-statistical means).<p>I&#x27;d be more interested in how to scale this via different agents. i.e. do we have say one type of agent that is specialized to produce ideas, while another is trained to evaluate ideas. Those sort of chains seem like they&#x27;d be powerful - if you can find a way to generalize it</div><br/></div></div><div id="39958789" class="c"><input type="checkbox" id="c-39958789" checked=""/><div class="controls bullet"><span class="by">l_l_m_5_0</span><span>|</span><a href="#39957376">prev</a><span>|</span><a href="#39957078">next</a><span>|</span><label class="collapse" for="c-39958789">[-]</label><label class="expand" for="c-39958789">[1 more]</label></div><br/><div class="children"><div class="content">Averaging LLM outputs will ensure the final output will contain a lot of words with no substance. However, it’s essential to recognize that averaging bad data doesn’t always lead to better results. Garbage in, garbage out — averaging cannot magically transform flawed inputs into accurate outputs.</div><br/></div></div><div id="39957078" class="c"><input type="checkbox" id="c-39957078" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#39958789">prev</a><span>|</span><a href="#39957699">next</a><span>|</span><label class="collapse" for="c-39957078">[-]</label><label class="expand" for="c-39957078">[1 more]</label></div><br/><div class="children"><div class="content">This usage of the word &quot;agent&quot; when they simply mean &quot;another LLM&quot; is sort of nonstandard, no? To me an agent implies some degree of RL.</div><br/></div></div><div id="39957699" class="c"><input type="checkbox" id="c-39957699" checked=""/><div class="controls bullet"><span class="by">kshitij_libra</span><span>|</span><a href="#39957078">prev</a><span>|</span><a href="#39957042">next</a><span>|</span><label class="collapse" for="c-39957699">[-]</label><label class="expand" for="c-39957699">[1 more]</label></div><br/><div class="children"><div class="content">This trend really needs to die. If you can’t come up with an original paper name , maybe the contents aren’t that original either</div><br/></div></div><div id="39957042" class="c"><input type="checkbox" id="c-39957042" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#39957699">prev</a><span>|</span><a href="#39957733">next</a><span>|</span><label class="collapse" for="c-39957042">[-]</label><label class="expand" for="c-39957042">[1 more]</label></div><br/><div class="children"><div class="content">Does it take less compute to train N agents vs one large model? Seem like a big win. Can the majority of the training be done independently or in distributed fashion?</div><br/></div></div><div id="39957733" class="c"><input type="checkbox" id="c-39957733" checked=""/><div class="controls bullet"><span class="by">parentheses</span><span>|</span><a href="#39957042">prev</a><span>|</span><a href="#39957412">next</a><span>|</span><label class="collapse" for="c-39957733">[-]</label><label class="expand" for="c-39957733">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if this performs even better when this is done tokenwise in the inner loop of the LLM</div><br/></div></div><div id="39957860" class="c"><input type="checkbox" id="c-39957860" checked=""/><div class="controls bullet"><span class="by">bigEnotation</span><span>|</span><a href="#39957412">prev</a><span>|</span><a href="#39957433">next</a><span>|</span><label class="collapse" for="c-39957860">[-]</label><label class="expand" for="c-39957860">[1 more]</label></div><br/><div class="children"><div class="content">I thought this is what GPT 4 was, it uses a boosting algorithm over GPT 3.5?</div><br/></div></div><div id="39957433" class="c"><input type="checkbox" id="c-39957433" checked=""/><div class="controls bullet"><span class="by">al2o3cr</span><span>|</span><a href="#39957860">prev</a><span>|</span><a href="#39957108">next</a><span>|</span><label class="collapse" for="c-39957433">[-]</label><label class="expand" for="c-39957433">[1 more]</label></div><br/><div class="children"><div class="content">35x more reasoning for a 10% increase in accuracy?<p>Scaling with a scale factor of almost zero is still scaling, I guess.</div><br/></div></div><div id="39957108" class="c"><input type="checkbox" id="c-39957108" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#39957433">prev</a><span>|</span><a href="#39957215">next</a><span>|</span><label class="collapse" for="c-39957108">[-]</label><label class="expand" for="c-39957108">[3 more]</label></div><br/><div class="children"><div class="content">All these AI researchers treating “..is all you need” a meme or something.</div><br/><div id="39957119" class="c"><input type="checkbox" id="c-39957119" checked=""/><div class="controls bullet"><span class="by">Intralexical</span><span>|</span><a href="#39957108">parent</a><span>|</span><a href="#39957182">next</a><span>|</span><label class="collapse" for="c-39957119">[-]</label><label class="expand" for="c-39957119">[1 more]</label></div><br/><div class="children"><div class="content">I think they&#x27;ve been overfitted.</div><br/></div></div><div id="39957182" class="c"><input type="checkbox" id="c-39957182" checked=""/><div class="controls bullet"><span class="by">latentsea</span><span>|</span><a href="#39957108">parent</a><span>|</span><a href="#39957119">prev</a><span>|</span><a href="#39957215">next</a><span>|</span><label class="collapse" for="c-39957182">[-]</label><label class="expand" for="c-39957182">[1 more]</label></div><br/><div class="children"><div class="content">All you need is all you need.</div><br/></div></div></div></div><div id="39957215" class="c"><input type="checkbox" id="c-39957215" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#39957108">prev</a><span>|</span><a href="#39957011">next</a><span>|</span><label class="collapse" for="c-39957215">[-]</label><label class="expand" for="c-39957215">[12 more]</label></div><br/><div class="children"><div class="content">This trend of &quot;All You Need&quot; in paper titles needs to die. The original &quot;Attention is All You Need&quot; used that title because it is literally true in their case. So many papers just use it as a meme now, and it distracts from the true insight of the paper.</div><br/><div id="39957233" class="c"><input type="checkbox" id="c-39957233" checked=""/><div class="controls bullet"><span class="by">donovanr</span><span>|</span><a href="#39957215">parent</a><span>|</span><a href="#39957390">next</a><span>|</span><label class="collapse" for="c-39957233">[-]</label><label class="expand" for="c-39957233">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s uncreative&#x2F;tired but at least to the point. Too many papers are confused &#x2F; opaque agglomerations of a year&#x27;s worth of research shoehorned into a paper. At least with these you can fairly easily assess whether the claim is supported or not.</div><br/></div></div><div id="39957390" class="c"><input type="checkbox" id="c-39957390" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#39957215">parent</a><span>|</span><a href="#39957233">prev</a><span>|</span><a href="#39957682">next</a><span>|</span><label class="collapse" for="c-39957390">[-]</label><label class="expand" for="c-39957390">[4 more]</label></div><br/><div class="children"><div class="content">&quot;All You Need Considered Harmful.&quot;</div><br/><div id="39957824" class="c"><input type="checkbox" id="c-39957824" checked=""/><div class="controls bullet"><span class="by">naruhodo</span><span>|</span><a href="#39957215">root</a><span>|</span><a href="#39957390">parent</a><span>|</span><a href="#39957518">next</a><span>|</span><label class="collapse" for="c-39957824">[-]</label><label class="expand" for="c-39957824">[1 more]</label></div><br/><div class="children"><div class="content">What if &quot;Considered Harmful&quot; is considered harmful?</div><br/></div></div><div id="39957518" class="c"><input type="checkbox" id="c-39957518" checked=""/><div class="controls bullet"><span class="by">benaubin</span><span>|</span><a href="#39957215">root</a><span>|</span><a href="#39957390">parent</a><span>|</span><a href="#39957824">prev</a><span>|</span><a href="#39957678">next</a><span>|</span><label class="collapse" for="c-39957518">[-]</label><label class="expand" for="c-39957518">[1 more]</label></div><br/><div class="children"><div class="content">&quot;&#x27;All You Need Considered Harmful.&#x27; Is All You Need&quot;</div><br/></div></div><div id="39957678" class="c"><input type="checkbox" id="c-39957678" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#39957215">root</a><span>|</span><a href="#39957390">parent</a><span>|</span><a href="#39957518">prev</a><span>|</span><a href="#39957682">next</a><span>|</span><label class="collapse" for="c-39957678">[-]</label><label class="expand" for="c-39957678">[1 more]</label></div><br/><div class="children"><div class="content">“Make All You Need Great Again Considered Harmful.”</div><br/></div></div></div></div><div id="39957777" class="c"><input type="checkbox" id="c-39957777" checked=""/><div class="controls bullet"><span class="by">jordanpg</span><span>|</span><a href="#39957215">parent</a><span>|</span><a href="#39957682">prev</a><span>|</span><a href="#39957883">next</a><span>|</span><label class="collapse" for="c-39957777">[-]</label><label class="expand" for="c-39957777">[1 more]</label></div><br/><div class="children"><div class="content">I think more or less the same thing about the word &quot;meme&quot; as you have used it.</div><br/></div></div><div id="39957274" class="c"><input type="checkbox" id="c-39957274" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#39957215">parent</a><span>|</span><a href="#39957883">prev</a><span>|</span><a href="#39957011">next</a><span>|</span><label class="collapse" for="c-39957274">[-]</label><label class="expand" for="c-39957274">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s equally annoying to see this comment crop up every single time someone uses the phrase.</div><br/><div id="39957513" class="c"><input type="checkbox" id="c-39957513" checked=""/><div class="controls bullet"><span class="by">karagenit</span><span>|</span><a href="#39957215">root</a><span>|</span><a href="#39957274">parent</a><span>|</span><a href="#39957011">next</a><span>|</span><label class="collapse" for="c-39957513">[-]</label><label class="expand" for="c-39957513">[2 more]</label></div><br/><div class="children"><div class="content">And it’s equally annoying to see this comment about that comment about the paper every time. Recursion!</div><br/><div id="39958470" class="c"><input type="checkbox" id="c-39958470" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#39957215">root</a><span>|</span><a href="#39957513">parent</a><span>|</span><a href="#39957011">next</a><span>|</span><label class="collapse" for="c-39958470">[-]</label><label class="expand" for="c-39958470">[1 more]</label></div><br/><div class="children"><div class="content">This is why I come to Hacker News :)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>