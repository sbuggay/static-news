<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1731056467293" as="style"/><link rel="stylesheet" href="styles.css?v=1731056467293"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/BemiHQ/BemiDB">Show HN: BemiDB – Postgres read replica optimized for analytics</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>exAspArk</span> | <span>92 comments</span></div><br/><div><div id="42084379" class="c"><input type="checkbox" id="c-42084379" checked=""/><div class="controls bullet"><span class="by">banditelol</span><span>|</span><a href="#42080385">next</a><span>|</span><label class="collapse" for="c-42084379">[-]</label><label class="expand" for="c-42084379">[1 more]</label></div><br/><div class="children"><div class="content">Looking at the syncer it seems like copying data to csv from the whole table everytime (?)
Code: <a href="https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;BemiDB&#x2F;blob&#x2F;6d6689b392ce6192fe521ad02b4b11cce150fb44&#x2F;src&#x2F;syncer.go#L173">https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;BemiDB&#x2F;blob&#x2F;6d6689b392ce6192fe521a...</a><p>I cant imagine until at what scale can you do this and is there anything better we can do before using debezium to sync the data via cdc?<p>Edit: add code permalink</div><br/></div></div><div id="42080385" class="c"><input type="checkbox" id="c-42080385" checked=""/><div class="controls bullet"><span class="by">woodhull</span><span>|</span><a href="#42084379">prev</a><span>|</span><a href="#42083939">next</a><span>|</span><label class="collapse" for="c-42080385">[-]</label><label class="expand" for="c-42080385">[8 more]</label></div><br/><div class="children"><div class="content">As much as DuckDB is cute I&#x27;ve mostly come to believe that Clickhouse is the perfect thing to pair Postgres with. This is especially true now that they&#x27;ve acquired PeerDB and are integrating it into the Clickpipes cloud product.<p>DuckDB is neat, and I understand why a company like BemiDB would build their product on top of it, but as a prospective customer embedded databases are a weird choice for serious workloads when there are other good open-source solutions like Clickhouse available.</div><br/><div id="42081672" class="c"><input type="checkbox" id="c-42081672" checked=""/><div class="controls bullet"><span class="by">maxmcd</span><span>|</span><a href="#42080385">parent</a><span>|</span><a href="#42080636">next</a><span>|</span><label class="collapse" for="c-42081672">[-]</label><label class="expand" for="c-42081672">[5 more]</label></div><br/><div class="children"><div class="content">Using duckdb and apache iceberg means that you can run read replicas without any operational burden. Clickhouse is amazing, but they do not allow you to mount dumb read replicas to object storage (yet).<p>I can imagine this product is a very elegant solution for many types of companies&#x2F;teams&#x2F;workloads.</div><br/><div id="42082018" class="c"><input type="checkbox" id="c-42082018" checked=""/><div class="controls bullet"><span class="by">zX41ZdbW</span><span>|</span><a href="#42080385">root</a><span>|</span><a href="#42081672">parent</a><span>|</span><a href="#42080636">next</a><span>|</span><label class="collapse" for="c-42082018">[-]</label><label class="expand" for="c-42082018">[4 more]</label></div><br/><div class="children"><div class="content">You can mount read replicas on object storage in ClickHouse.<p>Example:<p><pre><code>    CREATE DATABASE test;
    USE test;

    CREATE TABLE hackernews_history UUID &#x27;66491946-56e3-4790-a112-d2dc3963e68a&#x27;
    (
        `update_time` DateTime DEFAULT now(),
        `id` UInt32,
        `deleted` UInt8,
        `type` Enum8(&#x27;story&#x27; = 1, &#x27;comment&#x27; = 2, &#x27;poll&#x27; = 3, &#x27;pollopt&#x27; = 4, &#x27;job&#x27; = 5),
        `by` LowCardinality(String),
        `time` DateTime,
        `text` String,
        `dead` UInt8,
        `parent` UInt32,
        `poll` UInt32,
        `kids` Array(UInt32),
        `url` String,
        `score` Int32,
        `title` String,
        `parts` Array(UInt32),
        `descendants` Int32
    )
    ENGINE = ReplacingMergeTree(update_time)
    ORDER BY id
    SETTINGS disk = disk(readonly = true, type = &#x27;s3_plain_rewritable&#x27;, endpoint = &#x27;https:&#x2F;&#x2F;clicklake-test-2.s3.eu-central-1.amazonaws.com&#x2F;&#x27;, use_environment_credentials = false);
</code></pre>
And you can try it right now.<p>Install ClickHouse:<p><pre><code>    curl https:&#x2F;&#x2F;clickhouse.com&#x2F; | sh
    .&#x2F;clickhouse local
</code></pre>
Run the query above to attach the table.<p>The table is updated in real time. For example, here is your comment:<p><pre><code>    :) SELECT * FROM hackernews_history WHERE text LIKE &#x27;%Clickhouse is amazing%&#x27; ORDER BY update_time \G

    Row 1:
    ──────
    update_time: 2024-04-06 16:35:28
    id:          39785472
    deleted:     0
    type:        comment
    by:          mightybyte
    time:        2024-03-21 22:59:20
    text:        I&amp;#x27;ll second this.  Clickhouse is amazing.  I was actually using it today to query some CSV files.  I had to refresh my memory on the syntax so if anyone is interested:&lt;p&gt;&lt;pre&gt;&lt;code&gt;  clickhouse local -q &amp;quot;SELECT foo, sum(bar) FROM file(&amp;#x27;foobar.csv&amp;#x27;, CSV) GROUP BY foo FORMAT Pretty&amp;quot;
    &lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
    Way easier than opening in Excel and creating a pivot table which was my previous workflow.&lt;p&gt;Here&amp;#x27;s a list of the different input and output formats that it supports.&lt;p&gt;&lt;a href=&quot;https:&amp;#x2F;&amp;#x2F;clickhouse.com&amp;#x2F;docs&amp;#x2F;en&amp;#x2F;interfaces&amp;#x2F;formats&quot; rel=&quot;nofollow&quot;&gt;https:&amp;#x2F;&amp;#x2F;clickhouse.com&amp;#x2F;docs&amp;#x2F;en&amp;#x2F;interfaces&amp;#x2F;formats&lt;&#x2F;a&gt;
    dead:        0
    parent:      39784942
    poll:        0
    kids:        [39788575]
    url:         
    score:       0
    title:       
    parts:       []
    descendants: 0

    Row 2:
    ──────
    update_time: 2024-04-06 18:07:34
    id:          31334599
    deleted:     0
    type:        comment
    by:          richieartoul
    time:        2022-05-11 00:54:31
    text:        Not really. Clickhouse is amazing, but if you want to run it at massive scale you’ll have to invest a lot into sharding and clustering and all that. Druid is more distributed by default, but doesn’t support as sophisticated of queries as Clickhouse does.&lt;p&gt;Neither Clickhouse nor Druid can hold a candle to what Snowflake can do in terms of query capabilities, as well as the flexibility and richness of their product.&lt;p&gt;That’s just scratching the surface. They’re completely different product categories IMO, although they have a lot of technical &amp;#x2F; architectural overlap depending on how much you squint.&lt;p&gt;Devil is in the details basically.
    dead:        0
    parent:      31334527
    poll:        0
    kids:        [31334736]
    url:         
    score:       0
    title:       
    parts:       []
    descendants: 0

    Row 3:
    ──────
    update_time: 2024-11-07 22:29:09
    id:          42081672
    deleted:     0
    type:        comment
    by:          maxmcd
    time:        2024-11-07 22:13:12
    text:        Using duckdb and apache iceberg means that you can run read replicas without any operational burden. Clickhouse is amazing, but they do not allow you to mount dumb read replicas to object storage (yet).&lt;p&gt;I can imagine this product is a very elegant solution for many types of companies&amp;#x2F;teams&amp;#x2F;workloads.
    dead:        0
    parent:      42080385
    poll:        0
    kids:        []
    url:         
    score:       0
    title:       
    parts:       []
    descendants: 0

    3 rows in set. Elapsed: 3.981 sec. Processed 42.27 million rows, 14.45 GB (10.62 million rows&#x2F;s., 3.63 GB&#x2F;s.)
    Peak memory usage: 579.26 MiB.</code></pre></div><br/><div id="42085329" class="c"><input type="checkbox" id="c-42085329" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#42080385">root</a><span>|</span><a href="#42082018">parent</a><span>|</span><a href="#42085320">next</a><span>|</span><label class="collapse" for="c-42085329">[-]</label><label class="expand" for="c-42085329">[1 more]</label></div><br/><div class="children"><div class="content">When I try your code I get this, any idea?<p>Query id: daa202a3-874c-4a68-9e3c-974560ba4624<p>Elapsed: 0.092 sec.<p>Received exception:
Code: 499. DB::Exception: The AWS Access Key Id you provided does not exist in our records. (Code: 23, S3 exception: &#x27;InvalidAccessKeyId&#x27;): While processing disk(readonly = true, type = &#x27;s3_plain_rewritable&#x27;, endpoint = &#x27;<a href="https:&#x2F;&#x2F;clicklake-test-2.s3.eu-central-1.amazonaws.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;clicklake-test-2.s3.eu-central-1.amazonaws.com&#x2F;</a>&#x27;, use_environment_credentials = false). (S3_ERROR)</div><br/></div></div><div id="42085320" class="c"><input type="checkbox" id="c-42085320" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#42080385">root</a><span>|</span><a href="#42082018">parent</a><span>|</span><a href="#42085329">prev</a><span>|</span><a href="#42083644">next</a><span>|</span><label class="collapse" for="c-42085320">[-]</label><label class="expand" for="c-42085320">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting, can you give more info on how this could be used for instance in my IoT case where I want to keep the last 3 months (say) of data in Postgres, and dump old data in parquet&#x2F;iceberg on S3, and be able to do analytical queries on the past data? Would that be hard to do?<p>And how does the real-time update work? Could I make it so that my latest data is incrementally sync&#x27;d on S3 (eg &quot;the last 3-months block&quot; is incrementally updated efficiently each time there is new data) ?<p>Do you have example code &#x2F; setup for this?</div><br/></div></div><div id="42083644" class="c"><input type="checkbox" id="c-42083644" checked=""/><div class="controls bullet"><span class="by">maxmcd</span><span>|</span><a href="#42080385">root</a><span>|</span><a href="#42082018">parent</a><span>|</span><a href="#42085320">prev</a><span>|</span><a href="#42080636">next</a><span>|</span><label class="collapse" for="c-42083644">[-]</label><label class="expand" for="c-42083644">[1 more]</label></div><br/><div class="children"><div class="content">Whoops, I forgot that tables maintained in the db are not the same as remote archives. :|</div><br/></div></div></div></div></div></div><div id="42080636" class="c"><input type="checkbox" id="c-42080636" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42080385">parent</a><span>|</span><a href="#42081672">prev</a><span>|</span><a href="#42083939">next</a><span>|</span><label class="collapse" for="c-42080636">[-]</label><label class="expand" for="c-42080636">[2 more]</label></div><br/><div class="children"><div class="content">ClickHouse is definitely a popular choice nowadays. I&#x27;m curious whether you self-host ClickHouse or use their Cloud? We wanted to make BemiDB as simple to run as possible with a single binary and object storage (vs large machines, big disks, clustering, running Temporal for CDC, etc.)</div><br/><div id="42080891" class="c"><input type="checkbox" id="c-42080891" checked=""/><div class="controls bullet"><span class="by">Onavo</span><span>|</span><a href="#42080385">root</a><span>|</span><a href="#42080636">parent</a><span>|</span><a href="#42083939">next</a><span>|</span><label class="collapse" for="c-42080891">[-]</label><label class="expand" for="c-42080891">[1 more]</label></div><br/><div class="children"><div class="content">Clickhouse has an embedded version (<a href="https:&#x2F;&#x2F;github.com&#x2F;chdb-io&#x2F;chdb">https:&#x2F;&#x2F;github.com&#x2F;chdb-io&#x2F;chdb</a>), the issue with duck is that it&#x27;s too buggy for production loads. You can see a nice list of the issues here:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41490707">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41490707</a></div><br/></div></div></div></div></div></div><div id="42083939" class="c"><input type="checkbox" id="c-42083939" checked=""/><div class="controls bullet"><span class="by">cocoflunchy</span><span>|</span><a href="#42080385">prev</a><span>|</span><a href="#42082761">next</a><span>|</span><label class="collapse" for="c-42083939">[-]</label><label class="expand" for="c-42083939">[4 more]</label></div><br/><div class="children"><div class="content">What I would really love is a dead simple way to:
1) connect to my transactional Postgres db
2) define my materialized views
3) have these views update in realtime 
4) query these views with a fast engine<p>And ideally have the whole thing open source and be able to run it in CI<p>We tried peerdb + clickhouse but Clickhouse materialized views are not refreshed when joining tables.<p>Right now we’re back to standard materialized views inside Postgres refreshed once a day but the full refreshes are pretty slow… the operational side is great though, a single db to manage.</div><br/><div id="42083987" class="c"><input type="checkbox" id="c-42083987" checked=""/><div class="controls bullet"><span class="by">benpacker</span><span>|</span><a href="#42083939">parent</a><span>|</span><a href="#42084040">next</a><span>|</span><label class="collapse" for="c-42083987">[-]</label><label class="expand" for="c-42083987">[1 more]</label></div><br/><div class="children"><div class="content">You can do this with Materialize or Feldera. The keyword to look for is “incremental view maintenance”</div><br/></div></div><div id="42084040" class="c"><input type="checkbox" id="c-42084040" checked=""/><div class="controls bullet"><span class="by">capkutay</span><span>|</span><a href="#42083939">parent</a><span>|</span><a href="#42083987">prev</a><span>|</span><a href="#42084590">next</a><span>|</span><label class="collapse" for="c-42084040">[-]</label><label class="expand" for="c-42084040">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s been supported in striim since 2016<p><a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3129292.3129294" rel="nofollow">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3129292.3129294</a></div><br/></div></div><div id="42084590" class="c"><input type="checkbox" id="c-42084590" checked=""/><div class="controls bullet"><span class="by">lsuresh</span><span>|</span><a href="#42083939">parent</a><span>|</span><a href="#42084040">prev</a><span>|</span><a href="#42082761">next</a><span>|</span><label class="collapse" for="c-42084590">[-]</label><label class="expand" for="c-42084590">[1 more]</label></div><br/><div class="children"><div class="content">Yeah you definitely need something like Feldera: <a href="https:&#x2F;&#x2F;github.com&#x2F;feldera&#x2F;feldera">https:&#x2F;&#x2F;github.com&#x2F;feldera&#x2F;feldera</a></div><br/></div></div></div></div><div id="42082761" class="c"><input type="checkbox" id="c-42082761" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#42083939">prev</a><span>|</span><a href="#42078520">next</a><span>|</span><label class="collapse" for="c-42082761">[-]</label><label class="expand" for="c-42082761">[4 more]</label></div><br/><div class="children"><div class="content">I don’t totally understand the fascination with storing analytical data on S3. It’s not fast, and if you’re in a write heavy environment it’s definitely not cheap either.<p>What’s with the avoidance of clickhouse or duckdb paired with insanely fast EBS or even physically attached storage? You can still backup to s3, but using s3 for live analytics queries is missing out on so much of the speed.</div><br/><div id="42084141" class="c"><input type="checkbox" id="c-42084141" checked=""/><div class="controls bullet"><span class="by">potamic</span><span>|</span><a href="#42082761">parent</a><span>|</span><a href="#42083758">next</a><span>|</span><label class="collapse" for="c-42084141">[-]</label><label class="expand" for="c-42084141">[1 more]</label></div><br/><div class="children"><div class="content">Would love the authors to pitch in with their use cases, but I think most people simply do not need sub millisecond analytics. This is mostly replacing typical spark pipelines where you&#x27;re okay with sub second latencies.<p>S3 is the cheapest, fully managed storage you can get that can scale infinitely. When you&#x27;re already archiving to S3, doubling it for analytics saves cost and simplifies data management.</div><br/></div></div><div id="42083758" class="c"><input type="checkbox" id="c-42083758" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42082761">parent</a><span>|</span><a href="#42084141">prev</a><span>|</span><a href="#42083717">next</a><span>|</span><label class="collapse" for="c-42083758">[-]</label><label class="expand" for="c-42083758">[1 more]</label></div><br/><div class="children"><div class="content">My few cents:<p>- Compute and storage separation simplifies managing a system making compute &quot;ephemeral&quot;<p>- Compute resources can be scaled separately without worrying about scaling storage<p>- Object storage provides much higher durability (99.999999999% on S3) compared to disks<p>- Open table formats on S3 become a universal interface in the data space allowing to bring many other data tools if necessary<p>- Costs at scale can actually be lower since there is no data transfer cost within the same region. For example, you can check out WarpStream (Kafka on object storage) case studies that claim saving 5-10x</div><br/></div></div><div id="42083717" class="c"><input type="checkbox" id="c-42083717" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42082761">parent</a><span>|</span><a href="#42083758">prev</a><span>|</span><a href="#42078520">next</a><span>|</span><label class="collapse" for="c-42083717">[-]</label><label class="expand" for="c-42083717">[1 more]</label></div><br/><div class="children"><div class="content">S3 is a protocol understood by &quot;everyone&quot;. If you&#x27;re on AWS, as many are, it&#x27;s basically the only natural choice. But a number of cloud providers, and a bunch of self-hostable software offer an S3 interface.<p>Clickhouse on local NVMe is one possible solution, but then you are married to that solution. An S3 interface is more universal and allows you to mix and match your tools, even though this comes at some expense.</div><br/></div></div></div></div><div id="42078520" class="c"><input type="checkbox" id="c-42078520" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#42082761">prev</a><span>|</span><a href="#42084511">next</a><span>|</span><label class="collapse" for="c-42078520">[-]</label><label class="expand" for="c-42078520">[10 more]</label></div><br/><div class="children"><div class="content">Really cool! I have an IoT use-case where I ingest data, I want to keep like the last 3 months or so in Postgresql, and then store the old data as parquet files on S3<p>I planned initially to do chunks on S3 and do the analytical queries using duckdb, I&#x27;m wondering if your tool would be a good replacement?<p>For now I don&#x27;t have that many analytical queries, I&#x27;m mostly doing visualization of the data points by querying a range (eg last 2 weeks of data for a device)<p>Does it then make sense to use columnar storage or am I better off with &quot;regular Postgres&quot;?<p>Or in my case does your approach provide &quot;best of both worlds&quot; in the sense that I could do some occasional analytical queries on past data stored on S3, and regularly access &quot;last 3 months&quot; data for visualization using the data stored in the regular Postgres?</div><br/><div id="42083997" class="c"><input type="checkbox" id="c-42083997" checked=""/><div class="controls bullet"><span class="by">jconline543</span><span>|</span><a href="#42078520">parent</a><span>|</span><a href="#42078783">next</a><span>|</span><label class="collapse" for="c-42083997">[-]</label><label class="expand" for="c-42083997">[2 more]</label></div><br/><div class="children"><div class="content">If you just need the Postgres -&gt; S3 archival pattern you described, I built a simpler focused tool: pg-archiver (<a href="https:&#x2F;&#x2F;github.com&#x2F;johnonline35&#x2F;pg-archiver">https:&#x2F;&#x2F;github.com&#x2F;johnonline35&#x2F;pg-archiver</a>)<p>It:<p>- Auto-archives old Postgres data to Parquet files on S3<p>- Keeps recent data (default 90 days) in Postgres for fast viz queries<p>- Uses year&#x2F;month partitioning in S3 for basic analytical queries<p>- Configures with just PG connection string and S3 bucket<p>Currently batch-only archival (no real-time sync yet). Much lighter than running a full analytical DB if you mainly need timeseries visualization with occasional historical analysis.<p>Let me know if you try it out!</div><br/><div id="42085352" class="c"><input type="checkbox" id="c-42085352" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#42078520">root</a><span>|</span><a href="#42083997">parent</a><span>|</span><a href="#42078783">next</a><span>|</span><label class="collapse" for="c-42085352">[-]</label><label class="expand" for="c-42085352">[1 more]</label></div><br/><div class="children"><div class="content">Really cool! I&#x27;ll take a look!<p>- can you then easily query it with duckdb &#x2F; clickhouse &#x2F; something else? What do you use yourself? do you have some tutorial &#x2F; toy example to check?<p>- would it be complicated to have the real-time data be also stored somehow on S3 so it would be &quot;transparent&quot; to do query on historical data which includes day data?<p>- what typical &quot;batch data&quot; size makes sense, I guess doing &quot;day batches&quot; might be a bit small and will incurr too many &quot;read&quot; operations (if I have moderate amount of day data), rather than &quot;week batches&quot;? but then the &quot;timelag&quot; increases?</div><br/></div></div></div></div><div id="42078783" class="c"><input type="checkbox" id="c-42078783" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42078520">parent</a><span>|</span><a href="#42083997">prev</a><span>|</span><a href="#42079233">next</a><span>|</span><label class="collapse" for="c-42078783">[-]</label><label class="expand" for="c-42078783">[5 more]</label></div><br/><div class="children"><div class="content">Thank you!<p>Yes, absolutely!<p>1) You could use BemiDB to sync your Postgres data (e.g., partition time-series tables) to S3 in Iceberg format. Iceberg is essentially a &quot;table&quot; abstraction on top of columnar Parquet data files with a schema, history, etc.<p>2) If you don&#x27;t need strong consistency and fine with delayed data (the main trade-off), you can use just BemiDB to query and visualize all data directly from S3. From a query perspective, it&#x27;s like DuckDB that talks Postgres (wire protocol).<p>Feel free to give it a try! And although it&#x27;s a new project, we plan to keep building and improving it based on user feedback.</div><br/><div id="42078915" class="c"><input type="checkbox" id="c-42078915" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#42078520">root</a><span>|</span><a href="#42078783">parent</a><span>|</span><a href="#42079233">next</a><span>|</span><label class="collapse" for="c-42078915">[-]</label><label class="expand" for="c-42078915">[4 more]</label></div><br/><div class="children"><div class="content">Thanks!<p>- Can you give me more info about the strong consistency and delayed data, so I can better picture it with a few examples?<p>- Also, is it possible to do the sync with the columnar data in &quot;more-or-less real-time&quot; (eg do a NOTIFY on a new write in my IoT events table, and push in the storage?)<p>- Would your system also be suited for a kind of &quot;audit-log&quot; data? Eg. if I want to have some kind of audit-table of all the changes in my database, but only want to keep a few weeks worth at hand, and then push the rest on S3, or it doesn&#x27;t make much sense with that kind of data?</div><br/><div id="42079025" class="c"><input type="checkbox" id="c-42079025" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42078520">root</a><span>|</span><a href="#42078915">parent</a><span>|</span><a href="#42079233">next</a><span>|</span><label class="collapse" for="c-42079025">[-]</label><label class="expand" for="c-42079025">[3 more]</label></div><br/><div class="children"><div class="content">For now, BemiDB supports only full Postgres table data re-sync. We plan to enable real-time data syncing from Postgres into S3 by using logical replication (CDC), which is much more reliable than PG NOTIFY.<p>We use logical replication and this exact approach with our other project related to auditing and storing Postgres data changes <a href="https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;bemi">https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;bemi</a>. We&#x27;re thinking about combining these approaches to leverage scalable and affordable separated storage layer on S3.<p>Lmk if that makes sense or if you had any more questions!</div><br/><div id="42079131" class="c"><input type="checkbox" id="c-42079131" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#42078520">root</a><span>|</span><a href="#42079025">parent</a><span>|</span><a href="#42079233">next</a><span>|</span><label class="collapse" for="c-42079131">[-]</label><label class="expand" for="c-42079131">[2 more]</label></div><br/><div class="children"><div class="content">Really interesting thanks! I guess my use-case would rather require incremental updates<p>Ideally it would just sync in real-time and buffer new data in the Bemi binary (with some WAL-like storage to make sure data is preserved on binary crash&#x2F;reload), and when it has enough, push them on S3, etc<p>Is this the kind of approach you&#x27;re going to take?</div><br/><div id="42079198" class="c"><input type="checkbox" id="c-42079198" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42078520">root</a><span>|</span><a href="#42079131">parent</a><span>|</span><a href="#42079233">next</a><span>|</span><label class="collapse" for="c-42079198">[-]</label><label class="expand" for="c-42079198">[1 more]</label></div><br/><div class="children"><div class="content">Yes, we want to use the approach like you described! We&#x27;ll likely wait until enough changes are accumulated by using 2 configurable thresholds: time (like 30s) and size (like 100MB)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42079233" class="c"><input type="checkbox" id="c-42079233" checked=""/><div class="controls bullet"><span class="by">Onavo</span><span>|</span><a href="#42078520">parent</a><span>|</span><a href="#42078783">prev</a><span>|</span><a href="#42084511">next</a><span>|</span><label class="collapse" for="c-42079233">[-]</label><label class="expand" for="c-42079233">[2 more]</label></div><br/><div class="children"><div class="content">Use a clickhouse FDW or something similar, clickhouse has excellent integration with postgres. They also have a great embedded Python version. Their marketing isn&#x27;t as good as Duckdb but in terms of stability and performance they are so much better. Duckdb is very very buggy and full of sharp edges but because of their VC funded developer marketing, you don&#x27;t really hear people talking about it.</div><br/><div id="42079335" class="c"><input type="checkbox" id="c-42079335" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42078520">root</a><span>|</span><a href="#42079233">parent</a><span>|</span><a href="#42084511">next</a><span>|</span><label class="collapse" for="c-42079335">[-]</label><label class="expand" for="c-42079335">[1 more]</label></div><br/><div class="children"><div class="content">I agree that DuckDB may sometimes be buggy because it&#x27;s being actively developed with a large surface area (a universal embeddable query engine that works with different storage layers if I were to simplify).<p>However, DuckDB (non-profit foundation) != MotherDuck (VC funded). These are two separate organizations with different goals. I see DuckDB as a tool, not as a SaaS or a VC-funded company. My hope is that it&#x27;ll be adopted by other projects and not associated with just a single for-profit company.</div><br/></div></div></div></div></div></div><div id="42084511" class="c"><input type="checkbox" id="c-42084511" checked=""/><div class="controls bullet"><span class="by">fanyang01</span><span>|</span><a href="#42078520">prev</a><span>|</span><a href="#42079985">next</a><span>|</span><label class="collapse" for="c-42084511">[-]</label><label class="expand" for="c-42084511">[1 more]</label></div><br/><div class="children"><div class="content">Cool work!<p>Shameless plug: We are working on MyDuck Server: <a href="https:&#x2F;&#x2F;github.com&#x2F;apecloud&#x2F;myduckserver">https:&#x2F;&#x2F;github.com&#x2F;apecloud&#x2F;myduckserver</a>, a similar project that focuses on MySQL analytics but also exposes a Postgres port for direct DuckDB querying. Unlike BemiDB, we store data in the native DuckDB storage engine (instead of Iceberg), which is faster and does not require compaction. We are Apache 2 licensed. The official announcement will come soon, but comments are welcome!</div><br/></div></div><div id="42079985" class="c"><input type="checkbox" id="c-42079985" checked=""/><div class="controls bullet"><span class="by">dangoodmanUT</span><span>|</span><a href="#42084511">prev</a><span>|</span><a href="#42083460">next</a><span>|</span><label class="collapse" for="c-42079985">[-]</label><label class="expand" for="c-42079985">[4 more]</label></div><br/><div class="children"><div class="content">This is probably the most streamlined&#x2F;all-inclusive solution out of all that I&#x27;ve seen, but this has definitely been an extremely saturated space in 2024</div><br/><div id="42083387" class="c"><input type="checkbox" id="c-42083387" checked=""/><div class="controls bullet"><span class="by">dirtbag__dad</span><span>|</span><a href="#42079985">parent</a><span>|</span><a href="#42079987">next</a><span>|</span><label class="collapse" for="c-42083387">[-]</label><label class="expand" for="c-42083387">[2 more]</label></div><br/><div class="children"><div class="content">What are the other viable players? As mentioned in another thread to this post duckdb is not “production-ready.” That has been a non-starter for us at work.</div><br/><div id="42083856" class="c"><input type="checkbox" id="c-42083856" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079985">root</a><span>|</span><a href="#42083387">parent</a><span>|</span><a href="#42079987">next</a><span>|</span><label class="collapse" for="c-42083856">[-]</label><label class="expand" for="c-42083856">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why our current approach is to build missing or not fully functional features ourselves to move fast. For example, DuckDB performs reads from Iceberg tables not according to the spec, can&#x27;t perform writes, etc.</div><br/></div></div></div></div><div id="42079987" class="c"><input type="checkbox" id="c-42079987" checked=""/><div class="controls bullet"><span class="by">dangoodmanUT</span><span>|</span><a href="#42079985">parent</a><span>|</span><a href="#42083387">prev</a><span>|</span><a href="#42083460">next</a><span>|</span><label class="collapse" for="c-42079987">[-]</label><label class="expand" for="c-42079987">[1 more]</label></div><br/><div class="children"><div class="content">mostly everyone riding on duckdb&#x27;s tailcoat</div><br/></div></div></div></div><div id="42083460" class="c"><input type="checkbox" id="c-42083460" checked=""/><div class="controls bullet"><span class="by">nitinreddy88</span><span>|</span><a href="#42079985">prev</a><span>|</span><a href="#42082991">next</a><span>|</span><label class="collapse" for="c-42083460">[-]</label><label class="expand" for="c-42083460">[2 more]</label></div><br/><div class="children"><div class="content">How does update or continuous inserts get written&#x2F;updated to parquet files? Architecture doesn&#x27;t show nor anything in docs.<p>1. All the benchmarks&#x2F;most of the companies, show one time data exists and try querying&#x2F;compressing in different formats which is far from reality<p>2. Do you rewrite parquet data every time new data comes? Or partitioned by something? No examples<p>3. How does update&#x2F;delete works. Update might be niche case. But deletion&#x2F;data retention&#x2F;truncation is must and I don&#x27;t see how you support that</div><br/><div id="42083887" class="c"><input type="checkbox" id="c-42083887" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42083460">parent</a><span>|</span><a href="#42082991">next</a><span>|</span><label class="collapse" for="c-42083887">[-]</label><label class="expand" for="c-42083887">[1 more]</label></div><br/><div class="children"><div class="content">Our initial approach is to do full table re-syncs periodically. Our next step is to enable incremental data syncing by supporting insert&#x2F;update&#x2F;delete according to the Iceberg spec. In short, it&#x27;d produce &quot;diff&quot; Parquet files and &quot;stitch&quot; them using metadata (enabling time travel queries, schema evolution, etc.)</div><br/></div></div></div></div><div id="42082991" class="c"><input type="checkbox" id="c-42082991" checked=""/><div class="controls bullet"><span class="by">dantodor</span><span>|</span><a href="#42083460">prev</a><span>|</span><a href="#42080111">next</a><span>|</span><label class="collapse" for="c-42082991">[-]</label><label class="expand" for="c-42082991">[2 more]</label></div><br/><div class="children"><div class="content">Question: is it possible to use BemiDB in ... don&#x27;t know how to spell it, maybe read-only mode? And by that I mean one Bemi instance that is connected to postgres source, and others that use the produced Iceberg tables to answer queries? Poor man&#x27;s scalability of the query engine :) ... I would also imagine having an instance that is write-only (reads from postgres and produces the Iceberg tables) and one or many query-only engines.
Other than that, great work, will definitely start using it!</div><br/><div id="42083816" class="c"><input type="checkbox" id="c-42083816" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42082991">parent</a><span>|</span><a href="#42080111">next</a><span>|</span><label class="collapse" for="c-42083816">[-]</label><label class="expand" for="c-42083816">[1 more]</label></div><br/><div class="children"><div class="content">Great ideas! We&#x27;ll keep this suggestion related to read&#x2F;write separation in mind. We started with a simple unified solution, but we&#x27;ll keep iterating, listening and addressing any feedback :)</div><br/></div></div></div></div><div id="42080111" class="c"><input type="checkbox" id="c-42080111" checked=""/><div class="controls bullet"><span class="by">levkk</span><span>|</span><a href="#42082991">prev</a><span>|</span><a href="#42080709">next</a><span>|</span><label class="collapse" for="c-42080111">[-]</label><label class="expand" for="c-42080111">[7 more]</label></div><br/><div class="children"><div class="content">Moving data between systems is problematic. Where this product is actually needed (multi-TB databases under load) is where logical replication won&#x27;t be able to sync your tables in time. Conversely, small databases where this will work don&#x27;t really need columnar storage optimizations.</div><br/><div id="42080506" class="c"><input type="checkbox" id="c-42080506" checked=""/><div class="controls bullet"><span class="by">woodhull</span><span>|</span><a href="#42080111">parent</a><span>|</span><a href="#42080260">next</a><span>|</span><label class="collapse" for="c-42080506">[-]</label><label class="expand" for="c-42080506">[2 more]</label></div><br/><div class="children"><div class="content">For my use case of something similar on Clickhouse:<p>We load data from postgres tables that are used to build Clickhouse Dictionaries (a hash table for JOIN-ish operations).<p>The big tables do not arrive via real-time-ish sync from postgres but are bulk-appended using a separate infrastructure.</div><br/><div id="42081844" class="c"><input type="checkbox" id="c-42081844" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42080111">root</a><span>|</span><a href="#42080506">parent</a><span>|</span><a href="#42080260">next</a><span>|</span><label class="collapse" for="c-42081844">[-]</label><label class="expand" for="c-42081844">[1 more]</label></div><br/><div class="children"><div class="content">Would you be able to share how you implemented &quot;bulk-appended using a separate infrastructure&quot; at a high level?</div><br/></div></div></div></div><div id="42080260" class="c"><input type="checkbox" id="c-42080260" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42080111">parent</a><span>|</span><a href="#42080506">prev</a><span>|</span><a href="#42080709">next</a><span>|</span><label class="collapse" for="c-42080260">[-]</label><label class="expand" for="c-42080260">[4 more]</label></div><br/><div class="children"><div class="content">Fair point. We think that BemiDB currently can be useful when used with small and medium Postgres databases. Running complex analytics queries on Postgres can work, but it usually requires tuning it and adding indexes tailored to these queries, which may negatively impact the write performance on the OLTP side or may not be possible if these are ad-hoc queries.<p>&gt; (multi-TB databases under load) is where logical replication won&#x27;t be able to sync your tables in time<p>I think the ceiling for logical replication (and optimization techniques around it) is quite high. But I wonder what people do when it doesn&#x27;t work and scale?</div><br/><div id="42083883" class="c"><input type="checkbox" id="c-42083883" checked=""/><div class="controls bullet"><span class="by">delive</span><span>|</span><a href="#42080111">root</a><span>|</span><a href="#42080260">parent</a><span>|</span><a href="#42080709">next</a><span>|</span><label class="collapse" for="c-42083883">[-]</label><label class="expand" for="c-42083883">[3 more]</label></div><br/><div class="children"><div class="content">What would you consider to be small or medium? I have a use case for analytics on ~1 billion rows that are about 1TB in postgres. Have you tried on that volume?</div><br/><div id="42083940" class="c"><input type="checkbox" id="c-42083940" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42080111">root</a><span>|</span><a href="#42083883">parent</a><span>|</span><a href="#42080709">next</a><span>|</span><label class="collapse" for="c-42083940">[-]</label><label class="expand" for="c-42083940">[2 more]</label></div><br/><div class="children"><div class="content">We haven&#x27;t tested this with 1TB Postgres databases yet, assuming that most companies operating at this scale already built analytics data pipelines :) I&#x27;m curious if you currently move the data from this Postgres to somewhere else, or not yet?</div><br/><div id="42084065" class="c"><input type="checkbox" id="c-42084065" checked=""/><div class="controls bullet"><span class="by">delive</span><span>|</span><a href="#42080111">root</a><span>|</span><a href="#42083940">parent</a><span>|</span><a href="#42080709">next</a><span>|</span><label class="collapse" for="c-42084065">[-]</label><label class="expand" for="c-42084065">[1 more]</label></div><br/><div class="children"><div class="content">Not yet, mostly just kicked the can down the road due to costs. Like you said in another post, careful indexes on postgres get you quite far, but not nearly as flexible as a columnar DB.<p>I think your project is great. I suspect incremental updates will be a big feature for most uptake (one we would need to try this out at least).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42080709" class="c"><input type="checkbox" id="c-42080709" checked=""/><div class="controls bullet"><span class="by">leighleighleigh</span><span>|</span><a href="#42080111">prev</a><span>|</span><a href="#42083976">next</a><span>|</span><label class="collapse" for="c-42080709">[-]</label><label class="expand" for="c-42080709">[3 more]</label></div><br/><div class="children"><div class="content">Definitely checking this out today! I use postgres for ~30 GB of machine learning data (object detection) and have a couple workflows which go through the Postgres-&gt;Parquet-&gt;DuckDB processing route.<p>A couple questions, if you have time:<p>1. How do you guys handle multi-dimensional arrays? I&#x27;ve had issues with a few postgres-facing interfaces (libraries or middleware) where they believe everything is a 1D array!<p>2. I saw you are using pg_duckdb&#x2F;duckdb under the hood. I&#x27;ve had issues calling plain-SQL functions defined on the postgres server, when duckdb is involved. Does BemiDB support them?<p>Thanks for sharing, and good luck with it!</div><br/><div id="42080880" class="c"><input type="checkbox" id="c-42080880" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42080709">parent</a><span>|</span><a href="#42083976">next</a><span>|</span><label class="collapse" for="c-42080880">[-]</label><label class="expand" for="c-42080880">[2 more]</label></div><br/><div class="children"><div class="content">Thank you, please give it a try!<p>Great questions:<p>1. We currently don&#x27;t support multi-dimensional arrays, but we plan to add support for such complex data structures.<p>2. Would you be able to share what type of user-defined functions are these, do they do modify the data or read it?</div><br/><div id="42081639" class="c"><input type="checkbox" id="c-42081639" checked=""/><div class="controls bullet"><span class="by">leighleighleigh</span><span>|</span><a href="#42080709">root</a><span>|</span><a href="#42080880">parent</a><span>|</span><a href="#42083976">next</a><span>|</span><label class="collapse" for="c-42081639">[-]</label><label class="expand" for="c-42081639">[1 more]</label></div><br/><div class="children"><div class="content">1. good to hear!
2. The bulk of them are convenience wrappers which resolve UUIDs into other values, so most are read-only with only a single table lookup.</div><br/></div></div></div></div></div></div><div id="42083976" class="c"><input type="checkbox" id="c-42083976" checked=""/><div class="controls bullet"><span class="by">nwhnwh</span><span>|</span><a href="#42080709">prev</a><span>|</span><a href="#42081378">next</a><span>|</span><label class="collapse" for="c-42083976">[-]</label><label class="expand" for="c-42083976">[1 more]</label></div><br/><div class="children"><div class="content">Any benchmarks against ClickHouse?</div><br/></div></div><div id="42081378" class="c"><input type="checkbox" id="c-42081378" checked=""/><div class="controls bullet"><span class="by">gregw2</span><span>|</span><a href="#42083976">prev</a><span>|</span><a href="#42081527">next</a><span>|</span><label class="collapse" for="c-42081378">[-]</label><label class="expand" for="c-42081378">[2 more]</label></div><br/><div class="children"><div class="content">So it looks like you don&#x27;t use postgres extensions so you can run this on an EC2 against an Aurora Postgres instance and dump files to S3 Iceberg right?<p>And can you then have Glue Catalog auto-crawl them and expose them in Athena? Or are they DuckDB-managed Iceberg tables essentially?</div><br/><div id="42081528" class="c"><input type="checkbox" id="c-42081528" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42081378">parent</a><span>|</span><a href="#42081527">next</a><span>|</span><label class="collapse" for="c-42081528">[-]</label><label class="expand" for="c-42081528">[1 more]</label></div><br/><div class="children"><div class="content">Exactly! You can run it on any server connecting to any Postgres, without installing custom extensions (AWS Aurora supports only a limited number of extensions <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AmazonRDS&#x2F;latest&#x2F;AuroraPostgreSQLReleaseNotes&#x2F;AuroraPostgreSQL.Extensions.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AmazonRDS&#x2F;latest&#x2F;AuroraPostgreSQ...</a>).<p>The Iceberg tables are created separately from the DuckDB query engine. So you should be able to read these Iceberg tables by using any other Iceberg-compatible tools and services like AWS Athena.</div><br/></div></div></div></div><div id="42081527" class="c"><input type="checkbox" id="c-42081527" checked=""/><div class="controls bullet"><span class="by">jakozaur</span><span>|</span><a href="#42081378">prev</a><span>|</span><a href="#42082240">next</a><span>|</span><label class="collapse" for="c-42081527">[-]</label><label class="expand" for="c-42081527">[4 more]</label></div><br/><div class="children"><div class="content">Cool. Every database or data source (e.g. CRM) should produce Iceberg format for you.<p>Though a little sceptical of embedding DuckDB. It is easy and better to isolate Read&#x2F;Write paths, and it has a lot of other benefits.</div><br/><div id="42081596" class="c"><input type="checkbox" id="c-42081596" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42081527">parent</a><span>|</span><a href="#42082240">next</a><span>|</span><label class="collapse" for="c-42081596">[-]</label><label class="expand" for="c-42081596">[3 more]</label></div><br/><div class="children"><div class="content">Iceberg for the win!<p>We actually separate Read&#x2F;Write paths. BemiDB reads by levering DuckDB as a query engine. And it writes to Iceberg completely separately from DuckDB. I&#x27;m curious if that&#x27;s what you imagined.</div><br/><div id="42082053" class="c"><input type="checkbox" id="c-42082053" checked=""/><div class="controls bullet"><span class="by">jakozaur</span><span>|</span><a href="#42081527">root</a><span>|</span><a href="#42081596">parent</a><span>|</span><a href="#42082240">next</a><span>|</span><label class="collapse" for="c-42082053">[-]</label><label class="expand" for="c-42082053">[2 more]</label></div><br/><div class="children"><div class="content">Ideally, I would love many places writing to the Iceberg catalogue and then using it as a data lake.<p>In the data lake, I would have a gateway provisioning DuckDB on demand for each user.</div><br/><div id="42082088" class="c"><input type="checkbox" id="c-42082088" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42081527">root</a><span>|</span><a href="#42082053">parent</a><span>|</span><a href="#42082240">next</a><span>|</span><label class="collapse" for="c-42082088">[-]</label><label class="expand" for="c-42082088">[1 more]</label></div><br/><div class="children"><div class="content">Oh, interesting, thanks for sharing it!</div><br/></div></div></div></div></div></div></div></div><div id="42082240" class="c"><input type="checkbox" id="c-42082240" checked=""/><div class="controls bullet"><span class="by">anentropic</span><span>|</span><a href="#42081527">prev</a><span>|</span><a href="#42081292">next</a><span>|</span><label class="collapse" for="c-42082240">[-]</label><label class="expand" for="c-42082240">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m looking for low latency queries over not-very-big data (40-100M rows) in user-facing dashboards<p>How does the latency of Iceberg-on-S3 compare to say an EBS volume?</div><br/><div id="42083661" class="c"><input type="checkbox" id="c-42083661" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42082240">parent</a><span>|</span><a href="#42081292">next</a><span>|</span><label class="collapse" for="c-42083661">[-]</label><label class="expand" for="c-42083661">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say that querying data from S3 is not ideal when low-latency queries are required. Generally, there could be a few roundtrip requests to fetch metadata (JSON, Avro) and data (Parquet) files, which may lead to around 1s or so latency. However, we have caching on our roadmap (it could be just a simple TTL for the fetched data or some more sophisticated caching depending on the synced &amp; queried data)</div><br/></div></div></div></div><div id="42081292" class="c"><input type="checkbox" id="c-42081292" checked=""/><div class="controls bullet"><span class="by">VoxPelli</span><span>|</span><a href="#42082240">prev</a><span>|</span><a href="#42080613">next</a><span>|</span><label class="collapse" for="c-42081292">[-]</label><label class="expand" for="c-42081292">[5 more]</label></div><br/><div class="children"><div class="content">The AGPL license is a no-go for me.<p>While it’s technically true that it’s an OSI license it’s mostly used to scare away competing cloud vendors from hosting the software, which isn’t in spirit of OSS.<p>Have you looked into the more modern choices?<p>Like the Business Source License that MariaDB created and uses or the Functional Source License that Sentry created as an improvement over the Business Source License? <a href="https:&#x2F;&#x2F;fsl.software&#x2F;" rel="nofollow">https:&#x2F;&#x2F;fsl.software&#x2F;</a><p>Both those licenses have a fair source phase that automatically resolves into an open source phase over time.<p>Thus one gets the best of two worlds: An honest descriptive license for protecting one’s business model + a normal permissive OSS license that ensures longevity and prevents lock-in.</div><br/><div id="42081424" class="c"><input type="checkbox" id="c-42081424" checked=""/><div class="controls bullet"><span class="by">senorrib</span><span>|</span><a href="#42081292">parent</a><span>|</span><a href="#42081620">next</a><span>|</span><label class="collapse" for="c-42081424">[-]</label><label class="expand" for="c-42081424">[1 more]</label></div><br/><div class="children"><div class="content">You’re seriously calling out a perfectly valid OSS for not being “in the spirit of OSS”, and pitching for licenses that are explicitly NOT OSS?!<p>AGPL couldn’t be more in the spirit of OSS. The entire free software movement started to defend the _users_ freedom,  not individual companies’.</div><br/></div></div><div id="42081620" class="c"><input type="checkbox" id="c-42081620" checked=""/><div class="controls bullet"><span class="by">riiii</span><span>|</span><a href="#42081292">parent</a><span>|</span><a href="#42081424">prev</a><span>|</span><a href="#42081406">next</a><span>|</span><label class="collapse" for="c-42081620">[-]</label><label class="expand" for="c-42081620">[1 more]</label></div><br/><div class="children"><div class="content">Because you want to take their hard work, modify it and not share it back to the community?<p>I&#x27;m not crying that &quot;it&#x27;s not for you&quot;.</div><br/></div></div><div id="42081406" class="c"><input type="checkbox" id="c-42081406" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42081292">parent</a><span>|</span><a href="#42081620">prev</a><span>|</span><a href="#42080613">next</a><span>|</span><label class="collapse" for="c-42081406">[-]</label><label class="expand" for="c-42081406">[2 more]</label></div><br/><div class="children"><div class="content">Our philosophy in general is to go to a more open license over time (vs the other direction). So we might consider other more permissive OSI-approved licenses.<p>Would you be able to share why AGPL license is a no-go for you? I&#x27;m genuinely curious about your use case. In simple words, it&#x27;d require a company to open source their BemiDB code only if they made modifications and were distributing it to other users (allowing modifications and using it internally without any restrictions)</div><br/><div id="42081433" class="c"><input type="checkbox" id="c-42081433" checked=""/><div class="controls bullet"><span class="by">senorrib</span><span>|</span><a href="#42081292">root</a><span>|</span><a href="#42081406">parent</a><span>|</span><a href="#42080613">next</a><span>|</span><label class="collapse" for="c-42081433">[-]</label><label class="expand" for="c-42081433">[1 more]</label></div><br/><div class="children"><div class="content">Please, don’t. AGPL is great and you’re fine using it.</div><br/></div></div></div></div></div></div><div id="42080613" class="c"><input type="checkbox" id="c-42080613" checked=""/><div class="controls bullet"><span class="by">partdavid</span><span>|</span><a href="#42081292">prev</a><span>|</span><a href="#42079087">next</a><span>|</span><label class="collapse" for="c-42080613">[-]</label><label class="expand" for="c-42080613">[3 more]</label></div><br/><div class="children"><div class="content">How does this replicate Postgres data? I glanced at the code and saw that it exports to a CSV file then writes out an Iceberg table for an initial snapshot--does it use Postgres logical replication?</div><br/><div id="42080667" class="c"><input type="checkbox" id="c-42080667" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42080613">parent</a><span>|</span><a href="#42079087">next</a><span>|</span><label class="collapse" for="c-42080667">[-]</label><label class="expand" for="c-42080667">[2 more]</label></div><br/><div class="children"><div class="content">Full table re-syncing is our initial solution. Using Postgres logical replication is next on our roadmap!</div><br/><div id="42085378" class="c"><input type="checkbox" id="c-42085378" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#42080613">root</a><span>|</span><a href="#42080667">parent</a><span>|</span><a href="#42079087">next</a><span>|</span><label class="collapse" for="c-42085378">[-]</label><label class="expand" for="c-42085378">[1 more]</label></div><br/><div class="children"><div class="content">So if we have a very large table, it means each time I replicate (how often does that happen?) it needs to rewrite the entire table and upload it to S3, right?</div><br/></div></div></div></div></div></div><div id="42079087" class="c"><input type="checkbox" id="c-42079087" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#42080613">prev</a><span>|</span><a href="#42079328">next</a><span>|</span><label class="collapse" for="c-42079087">[-]</label><label class="expand" for="c-42079087">[2 more]</label></div><br/><div class="children"><div class="content">Query Engine: embeds the DuckDB query engine to run analytical queries.
Storage Layer: uses the Iceberg table format to store data in columnar compressed Parquet files.<p>Smart. Imma test this out for sure.</div><br/><div id="42079197" class="c"><input type="checkbox" id="c-42079197" checked=""/><div class="controls bullet"><span class="by">arjunlol</span><span>|</span><a href="#42079087">parent</a><span>|</span><a href="#42079328">next</a><span>|</span><label class="collapse" for="c-42079197">[-]</label><label class="expand" for="c-42079197">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Give it a try and let us know any feedback :)</div><br/></div></div></div></div><div id="42079328" class="c"><input type="checkbox" id="c-42079328" checked=""/><div class="controls bullet"><span class="by">neeleshs</span><span>|</span><a href="#42079087">prev</a><span>|</span><a href="#42081956">next</a><span>|</span><label class="collapse" for="c-42079328">[-]</label><label class="expand" for="c-42079328">[3 more]</label></div><br/><div class="children"><div class="content">Congratulations! I was looking and pg_analytics from ParadeDB hoping this use case would be solved (the dump from pg to parquet part), but it doesnt yet do it.<p>How does it handle updates?</div><br/><div id="42079442" class="c"><input type="checkbox" id="c-42079442" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079328">parent</a><span>|</span><a href="#42081956">next</a><span>|</span><label class="collapse" for="c-42079442">[-]</label><label class="expand" for="c-42079442">[2 more]</label></div><br/><div class="children"><div class="content">Thank you!<p>The pg_analytics Postgres extension partially supports different file formats. We bet big on Iceberg open table format, which uses Parquet data files under the hood.<p>Our initial approach is to do periodic full table resyncing. The next step is to support incremental Iceberg operations like updates. This will involve creating a new &quot;diff&quot; Parquet file and using the Iceberg metadata to point to the new file version that changes some rows. Later this will enable time travel queries, schema evolution, etc.</div><br/><div id="42079525" class="c"><input type="checkbox" id="c-42079525" checked=""/><div class="controls bullet"><span class="by">neeleshs</span><span>|</span><a href="#42079328">root</a><span>|</span><a href="#42079442">parent</a><span>|</span><a href="#42081956">next</a><span>|</span><label class="collapse" for="c-42079525">[-]</label><label class="expand" for="c-42079525">[1 more]</label></div><br/><div class="children"><div class="content">Fantastic!</div><br/></div></div></div></div></div></div><div id="42081956" class="c"><input type="checkbox" id="c-42081956" checked=""/><div class="controls bullet"><span class="by">lucasfcosta</span><span>|</span><a href="#42079328">prev</a><span>|</span><a href="#42081630">next</a><span>|</span><label class="collapse" for="c-42081956">[-]</label><label class="expand" for="c-42081956">[2 more]</label></div><br/><div class="children"><div class="content">Amazing work, guys! Looking forward to seeing where BemiDB is going.</div><br/><div id="42082063" class="c"><input type="checkbox" id="c-42082063" checked=""/><div class="controls bullet"><span class="by">arjunlol</span><span>|</span><a href="#42081956">parent</a><span>|</span><a href="#42081630">next</a><span>|</span><label class="collapse" for="c-42082063">[-]</label><label class="expand" for="c-42082063">[1 more]</label></div><br/><div class="children"><div class="content">Thank you Lucas!</div><br/></div></div></div></div><div id="42081630" class="c"><input type="checkbox" id="c-42081630" checked=""/><div class="controls bullet"><span class="by">mrbluecoat</span><span>|</span><a href="#42081956">prev</a><span>|</span><a href="#42079082">next</a><span>|</span><label class="collapse" for="c-42081630">[-]</label><label class="expand" for="c-42081630">[2 more]</label></div><br/><div class="children"><div class="content">I hadn&#x27;t heard of Devbox before, so thanks for sharing</div><br/><div id="42081867" class="c"><input type="checkbox" id="c-42081867" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42081630">parent</a><span>|</span><a href="#42079082">next</a><span>|</span><label class="collapse" for="c-42081867">[-]</label><label class="expand" for="c-42081867">[1 more]</label></div><br/><div class="children"><div class="content">Haha, it&#x27;s awesome for isolating project environments (languages, databases, etc.) without using docker</div><br/></div></div></div></div><div id="42079082" class="c"><input type="checkbox" id="c-42079082" checked=""/><div class="controls bullet"><span class="by">hoerzu</span><span>|</span><a href="#42081630">prev</a><span>|</span><a href="#42079293">next</a><span>|</span><label class="collapse" for="c-42079082">[-]</label><label class="expand" for="c-42079082">[6 more]</label></div><br/><div class="children"><div class="content">Can you give an example if I have 5gig (2 million rows)<p>How will it be created differently for columnar access?</div><br/><div id="42079130" class="c"><input type="checkbox" id="c-42079130" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079082">parent</a><span>|</span><a href="#42079293">next</a><span>|</span><label class="collapse" for="c-42079130">[-]</label><label class="expand" for="c-42079130">[5 more]</label></div><br/><div class="children"><div class="content">We ran some benchmarks (TPC-H, designed for OLAP) with ~10M records <a href="https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;BemiDB#benchmark">https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;BemiDB#benchmark</a><p>The BemiDB storage layer produced ~300MB columnar Parquet files (with ZSTD compression) vs 1.6GB of data in Postgres.</div><br/><div id="42080904" class="c"><input type="checkbox" id="c-42080904" checked=""/><div class="controls bullet"><span class="by">Sesse__</span><span>|</span><a href="#42079082">root</a><span>|</span><a href="#42079130">parent</a><span>|</span><a href="#42079293">next</a><span>|</span><label class="collapse" for="c-42080904">[-]</label><label class="expand" for="c-42080904">[4 more]</label></div><br/><div class="children"><div class="content">Does TPC-H SF1 really take _one and a half hours_ for you on regular Postgres? Last time I tried (in the form of DBT-3), it was 22 queries and most of them ran in a couple seconds.</div><br/><div id="42080995" class="c"><input type="checkbox" id="c-42080995" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079082">root</a><span>|</span><a href="#42080904">parent</a><span>|</span><a href="#42079293">next</a><span>|</span><label class="collapse" for="c-42080995">[-]</label><label class="expand" for="c-42080995">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. I haven&#x27;t used the DBT-3 kit, does it add any indexes? I manually added these Postgres indexes <a href="https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;BemiDB&#x2F;blob&#x2F;main&#x2F;benchmark&#x2F;data&#x2F;create-indexes.ddl">https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;BemiDB&#x2F;blob&#x2F;main&#x2F;benchmark&#x2F;data&#x2F;cr...</a> to reduce the main bottlenecks on SF0.1 and reduce the total time from 1h23m13s to 1.5s. But SF1 still took more than 1h</div><br/><div id="42081019" class="c"><input type="checkbox" id="c-42081019" checked=""/><div class="controls bullet"><span class="by">Sesse__</span><span>|</span><a href="#42079082">root</a><span>|</span><a href="#42080995">parent</a><span>|</span><a href="#42079293">next</a><span>|</span><label class="collapse" for="c-42081019">[-]</label><label class="expand" for="c-42081019">[2 more]</label></div><br/><div class="children"><div class="content">It adds a bunch of indexes, yes. I don&#x27;t think anyone really runs TPC-H unindexed unless they are using a database that plain doesn&#x27;t support it; it wouldn&#x27;t really give much meaningful information.<p>Edit: I seemingly don&#x27;t have these benchmarks anymore, and I&#x27;m not going to re-run them now, but I found a very (_very_) roughly similar SF10 run clocking in around seven minutes total. So that&#x27;s the order of magnitude I would be expecting, given ten times as much data.</div><br/><div id="42081104" class="c"><input type="checkbox" id="c-42081104" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079082">root</a><span>|</span><a href="#42081019">parent</a><span>|</span><a href="#42079293">next</a><span>|</span><label class="collapse" for="c-42081104">[-]</label><label class="expand" for="c-42081104">[1 more]</label></div><br/><div class="children"><div class="content">Got it, thanks for sharing it! We&#x27;ll try to look into DBT-3 and the indexes it creates to test with SF10</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42079293" class="c"><input type="checkbox" id="c-42079293" checked=""/><div class="controls bullet"><span class="by">paurora</span><span>|</span><a href="#42079082">prev</a><span>|</span><a href="#42079765">next</a><span>|</span><label class="collapse" for="c-42079293">[-]</label><label class="expand" for="c-42079293">[3 more]</label></div><br/><div class="children"><div class="content">very cool!! We have the same vision with pg_moooncake: <a href="https:&#x2F;&#x2F;github.com&#x2F;Mooncake-Labs&#x2F;pg_mooncake&#x2F;tree&#x2F;main">https:&#x2F;&#x2F;github.com&#x2F;Mooncake-Labs&#x2F;pg_mooncake&#x2F;tree&#x2F;main</a><p>From what I understand, the BemiDB experience is akin to PeerDB + Clickhouse. It&#x27;s not really a Postgres extension?<p>Glad open table formats are becoming mainstream, for everyone.</div><br/><div id="42081812" class="c"><input type="checkbox" id="c-42081812" checked=""/><div class="controls bullet"><span class="by">shayonj</span><span>|</span><a href="#42079293">parent</a><span>|</span><a href="#42079489">next</a><span>|</span><label class="collapse" for="c-42081812">[-]</label><label class="expand" for="c-42081812">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely stoked for pg_mooncake. I really want to see some of these things happening inside PG and taking advantage of the PG internals + native storage. Only bummer is adoption by places where users are currently, say Aurora. But thats probably a problem for another day :)<p>P.S The integration with something like Neon is really cool to see.</div><br/></div></div><div id="42079489" class="c"><input type="checkbox" id="c-42079489" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079293">parent</a><span>|</span><a href="#42081812">prev</a><span>|</span><a href="#42079765">next</a><span>|</span><label class="collapse" for="c-42079489">[-]</label><label class="expand" for="c-42079489">[1 more]</label></div><br/><div class="children"><div class="content">Thanks!<p>We love the pg_moooncake extension (and pg_duckdb used under the hood). Although our approaches are slightly different. Long-term, we want to allow anyone to use BemiDB by using native Postgres logical replication without installing any extensions (many Postgres hosting providers impose their restrictions, upgrading versions might be challenging, OLAP queries may affect OLTP performance if within the same database, etc.)</div><br/></div></div></div></div><div id="42079765" class="c"><input type="checkbox" id="c-42079765" checked=""/><div class="controls bullet"><span class="by">canadiantim</span><span>|</span><a href="#42079293">prev</a><span>|</span><a href="#42081840">next</a><span>|</span><label class="collapse" for="c-42079765">[-]</label><label class="expand" for="c-42079765">[2 more]</label></div><br/><div class="children"><div class="content">How does this compare to ParadeDB? Seems to occupy the same space</div><br/><div id="42079934" class="c"><input type="checkbox" id="c-42079934" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079765">parent</a><span>|</span><a href="#42081840">next</a><span>|</span><label class="collapse" for="c-42079934">[-]</label><label class="expand" for="c-42079934">[1 more]</label></div><br/><div class="children"><div class="content">We love ParadeDB and their team. Their primary focus is search (Elasticsearch on Postgres), but they also have the pg_analytics Postgres extension (foreign data wrappers and embedded DuckDB).<p>The biggest difference is in a Postgres extension vs a separate OLAP process. We want to allow anyone with just Postgres to be able to perform analytics queries without affecting resources in the transactional database, building and installing extensions (might not be possible with some hosting providers), dealing with dependencies and their versions when upgrading Postgres, manually syncing data from Postgres to S3, etc.</div><br/></div></div></div></div><div id="42081840" class="c"><input type="checkbox" id="c-42081840" checked=""/><div class="controls bullet"><span class="by">globular-toast</span><span>|</span><a href="#42079765">prev</a><span>|</span><a href="#42079387">next</a><span>|</span><label class="collapse" for="c-42081840">[-]</label><label class="expand" for="c-42081840">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get how this would do away with the need for some kind of ETL. Most apps use highly normalised schemas that are completely unsuitable for analytical users. Not to mention you wouldn&#x27;t want to couple your app schema to your warehouse schema. Am I missing something? How would this replace traditional data warehousing?</div><br/><div id="42081965" class="c"><input type="checkbox" id="c-42081965" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42081840">parent</a><span>|</span><a href="#42079387">next</a><span>|</span><label class="collapse" for="c-42081965">[-]</label><label class="expand" for="c-42081965">[1 more]</label></div><br/><div class="children"><div class="content">Good point. For more complex scenarios, people would still be able to implement, for example, a Medallion Architecture to progressively improve data quality and structure. Because it is Postgres- and Iceberg-compatible (db and data), it&#x27;s possible to bring more other advanced data tools when it&#x27;s needed to perform data transformation and movement. Currently, we see it as a Postgres read replica for analytics. But it&#x27;s easy to imagine that in the future it could be used as a standalone OSS database on top of a data lakehouse with an open format in S3.</div><br/></div></div></div></div><div id="42079387" class="c"><input type="checkbox" id="c-42079387" checked=""/><div class="controls bullet"><span class="by">winddude</span><span>|</span><a href="#42081840">prev</a><span>|</span><a href="#42078068">next</a><span>|</span><label class="collapse" for="c-42079387">[-]</label><label class="expand" for="c-42079387">[5 more]</label></div><br/><div class="children"><div class="content">difference to something like duckdb?</div><br/><div id="42079503" class="c"><input type="checkbox" id="c-42079503" checked=""/><div class="controls bullet"><span class="by">simlevesque</span><span>|</span><a href="#42079387">parent</a><span>|</span><a href="#42079412">next</a><span>|</span><label class="collapse" for="c-42079503">[-]</label><label class="expand" for="c-42079503">[2 more]</label></div><br/><div class="children"><div class="content">&gt; &gt; Alternatives<p>&gt; DuckDB:<p>&gt; - Designed for OLAP use cases. Easy to run with a single binary.<p>&gt; - Limited support in the data ecosystem (notebooks, BI tools, etc.). Requires manual data syncing and schema mapping for best performance.</div><br/><div id="42079606" class="c"><input type="checkbox" id="c-42079606" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079387">root</a><span>|</span><a href="#42079503">parent</a><span>|</span><a href="#42079412">next</a><span>|</span><label class="collapse" for="c-42079606">[-]</label><label class="expand" for="c-42079606">[1 more]</label></div><br/><div class="children"><div class="content">^ This!<p>Here is the link that briefly describes pros and cons of different alternatives for analytics <a href="https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;BemiDB#alternatives">https:&#x2F;&#x2F;github.com&#x2F;BemiHQ&#x2F;BemiDB#alternatives</a></div><br/></div></div></div></div><div id="42079412" class="c"><input type="checkbox" id="c-42079412" checked=""/><div class="controls bullet"><span class="by">polskibus</span><span>|</span><a href="#42079387">parent</a><span>|</span><a href="#42079503">prev</a><span>|</span><a href="#42078068">next</a><span>|</span><label class="collapse" for="c-42079412">[-]</label><label class="expand" for="c-42079412">[2 more]</label></div><br/><div class="children"><div class="content">How can you setup automatic replication from Postgresql to a duckdb instance?</div><br/><div id="42079571" class="c"><input type="checkbox" id="c-42079571" checked=""/><div class="controls bullet"><span class="by">exAspArk</span><span>|</span><a href="#42079387">root</a><span>|</span><a href="#42079412">parent</a><span>|</span><a href="#42078068">next</a><span>|</span><label class="collapse" for="c-42079571">[-]</label><label class="expand" for="c-42079571">[1 more]</label></div><br/><div class="children"><div class="content">The most common approach is to read Postgres data <i>in</i> DuckDB <a href="https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;extensions&#x2F;postgres.html" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;extensions&#x2F;postgres.html</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>