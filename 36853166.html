<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1690275648550" as="style"/><link rel="stylesheet" href="styles.css?v=1690275648550"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html">Advanced Performance Extensions (APX)</a> <span class="domain">(<a href="https://www.intel.com">www.intel.com</a>)</span></div><div class="subtext"><span>gautamcgoel</span> | <span>85 comments</span></div><br/><div><div id="36854091" class="c"><input type="checkbox" id="c-36854091" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36853546">next</a><span>|</span><label class="collapse" for="c-36854091">[-]</label><label class="expand" for="c-36854091">[1 more]</label></div><br/><div class="children"><div class="content">High-level overview of what&#x27;s changing here:<p>* A new REX-like prefix that extends the number of addressable GPRs to 32 from 16. This only supports instructions that have one-byte opcodes or the 0f prefix, so recent GPR instructions like ADCX or BLSR aren&#x27;t supported with this format, except.<p>* The EVEX prefix (used for AVX-512) is also extended to be usable for GPR instructions instead of just vector instructions. This allows three-address instructions to be defined.<p>* The EVEX prefix for GPR also has a dedicated bit for &quot;do you want to set flags as a result of this instruction.&quot;<p>* New instructions that push&#x2F;pop 2 GPRs at once<p>* New instructions that let you conditionally set flags (basically you can do OR&#x2F;AND in the hardware flags, this sounds useful for compilers).<p>* New instructions for predicated loads.<p>* New 64-bit absolute jump instruction<p>* Also, implementation of the predicated stuff in AVX-512, but for 256-bit vectors. With this note:<p>&gt;  A “converged” version of Intel AVX10 with maximum vector lengths of 256 bits and 32-bit opmask registers will be supported across all Intel processors, while 512-bit vector registers and 64-bit opmasks will continue to be supported on some P-core processors.</div><br/></div></div><div id="36853546" class="c"><input type="checkbox" id="c-36853546" checked=""/><div class="controls bullet"><span class="by">dfox</span><span>|</span><a href="#36854091">prev</a><span>|</span><a href="#36853712">next</a><span>|</span><label class="collapse" for="c-36853546">[-]</label><label class="expand" for="c-36853546">[3 more]</label></div><br/><div class="children"><div class="content">Apparently the Intel&#x27;s marketing had forgotten that they already had a product called iAPX (standing for “Advanced Performance arCHitecture”) and it did not go exactly well :)</div><br/><div id="36853715" class="c"><input type="checkbox" id="c-36853715" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#36853546">parent</a><span>|</span><a href="#36856859">next</a><span>|</span><label class="collapse" for="c-36853715">[-]</label><label class="expand" for="c-36853715">[1 more]</label></div><br/><div class="children"><div class="content">I genuinely thought that&#x27;s what the article was about when I read the title.</div><br/></div></div><div id="36856859" class="c"><input type="checkbox" id="c-36856859" checked=""/><div class="controls bullet"><span class="by">kjs3</span><span>|</span><a href="#36853546">parent</a><span>|</span><a href="#36853715">prev</a><span>|</span><a href="#36853712">next</a><span>|</span><label class="collapse" for="c-36856859">[-]</label><label class="expand" for="c-36856859">[1 more]</label></div><br/><div class="children"><div class="content">You mean the 8086, 80186, 80286 and 80386 and associated support chips?[1]  Perhaps you and I have different definitions of &#x27;go well&#x27;.[2]<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IAPX" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IAPX</a>
[2] I know you&#x27;re thinking about the iAPX432 and forgot the rest</div><br/></div></div></div></div><div id="36853712" class="c"><input type="checkbox" id="c-36853712" checked=""/><div class="controls bullet"><span class="by">pavlov</span><span>|</span><a href="#36853546">prev</a><span>|</span><a href="#36854189">next</a><span>|</span><label class="collapse" for="c-36853712">[-]</label><label class="expand" for="c-36853712">[6 more]</label></div><br/><div class="children"><div class="content">Interesting. More registers and separate destination on instructions is about 40 years overdue, but better late than never.<p>I realized I’ve completely lost track of Intel’s architecture extensions reading this:<p>“They do not change the size and layout of the XSAVE area as they take up the space left behind by the deprecated Intel® MPX registers.”<p>Apparently MPX was Memory Protection Extensions and the design was so flawed, it was removed entirely soon after introduction.</div><br/><div id="36853880" class="c"><input type="checkbox" id="c-36853880" checked=""/><div class="controls bullet"><span class="by">theandrewbailey</span><span>|</span><a href="#36853712">parent</a><span>|</span><a href="#36853793">next</a><span>|</span><label class="collapse" for="c-36853880">[-]</label><label class="expand" for="c-36853880">[4 more]</label></div><br/><div class="children"><div class="content">&gt; More registers is about 40 years overdue, but better late than never.<p>SSE, AMD64, AVX, and AVX-512 say hi.</div><br/><div id="36856906" class="c"><input type="checkbox" id="c-36856906" checked=""/><div class="controls bullet"><span class="by">kjs3</span><span>|</span><a href="#36853712">root</a><span>|</span><a href="#36853880">parent</a><span>|</span><a href="#36853793">next</a><span>|</span><label class="collapse" for="c-36856906">[-]</label><label class="expand" for="c-36856906">[3 more]</label></div><br/><div class="children"><div class="content">And MMX from 1997...so almost 35 of those 40 years ago.</div><br/><div id="36857797" class="c"><input type="checkbox" id="c-36857797" checked=""/><div class="controls bullet"><span class="by">jlokier</span><span>|</span><a href="#36853712">root</a><span>|</span><a href="#36856906">parent</a><span>|</span><a href="#36853793">next</a><span>|</span><label class="collapse" for="c-36857797">[-]</label><label class="expand" for="c-36857797">[2 more]</label></div><br/><div class="children"><div class="content">MMX didn&#x27;t add new registers.  It uses the same FP registers as those present all the way back to the Intel 8087 in 1980.  You could say the MMX registers are older than the 32-bit x86 architecture, and therefore older than EAX, EBX, ESP etc.</div><br/><div id="36858159" class="c"><input type="checkbox" id="c-36858159" checked=""/><div class="controls bullet"><span class="by">kjs3</span><span>|</span><a href="#36853712">root</a><span>|</span><a href="#36857797">parent</a><span>|</span><a href="#36853793">next</a><span>|</span><label class="collapse" for="c-36858159">[-]</label><label class="expand" for="c-36858159">[1 more]</label></div><br/><div class="children"><div class="content">Not exactly, especially in the context of this issue. You could use the MMX registers as &#x27;traditional&#x27; registers (or at least a scratchpad), not quite like the x87 stack.<p><i>There are 8 64-bit MMX registers. To avoid having to add new registers, they were made to overlap with the FPU stack register. This means that the MMX instructions and the FPU instructions cannot be used simultaneously. MMX registers are addressed directly, and do not need to be accessed by pushing and popping in the same way as the FPU registers. </i>[1]<p>[1] <a href="https:&#x2F;&#x2F;en.wikibooks.org&#x2F;wiki&#x2F;X86_Assembly&#x2F;MMX" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikibooks.org&#x2F;wiki&#x2F;X86_Assembly&#x2F;MMX</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="36854189" class="c"><input type="checkbox" id="c-36854189" checked=""/><div class="controls bullet"><span class="by">soulbadguy</span><span>|</span><a href="#36853712">prev</a><span>|</span><a href="#36853937">next</a><span>|</span><label class="collapse" for="c-36854189">[-]</label><label class="expand" for="c-36854189">[3 more]</label></div><br/><div class="children"><div class="content">&gt; extends the number of addressable GPRs to 32 from 16<p>I have always been curious as to why the number of GPRs were limited for so long on X86 given that the instruction set is already variable length, and the CPU have typically a very large number of internal arch-register that could be cheaply addressed.<p>Having looked at the pain of developing a good register allocate in LLVM, and how critical memory access can me in hot&#x2F;tight loops i would have loved to have even more register  something closer to 64 or 128, and let the cpu manage the spilling internally.</div><br/><div id="36855089" class="c"><input type="checkbox" id="c-36855089" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36854189">parent</a><span>|</span><a href="#36855455">next</a><span>|</span><label class="collapse" for="c-36855089">[-]</label><label class="expand" for="c-36855089">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I have always been curious as to why the number of GPRs were limited for so long on X86 given that the instruction set is already variable length, and the CPU have typically a very large number of internal arch-register that could be cheaply addressed.<p>Because it&#x27;s hard-ish to wrench in the encoding bits. x86 registers are addressed via the ModR&#x2F;M byte, which gives you 4 addressing modes × 2 register operands (if you have 8 registers). Note that non-register-register addressing modes include a second SIB byte which means you have up to three registers encoded in an instruction. x86-64 extended the number to 16 by adding a prefix that has 4 bits, which encoded a 32-bit&#x2F;64-bit selector and 1 bit for each of the three possible registers (R, X, B). Note that to keep the prefix to only a 1-byte length, they had to reclaim 16 possible opcodes, whose space is pretty limited.<p>Encoding large register numbers gets pretty chonky in instruction space (3 register ids of 5 bits each is 15 bits of your instruction for operands, and that&#x27;s before you start considering possible opcodes). It also increases the size of context switches (especially thread contexts), since you have to spill all of those registers even if they&#x27;re not filled with useful data.<p>8 registers is definitely too few; it&#x27;s not clear to me if the extra working set size afforded by 32 registers is worth it over the fatter instructions.</div><br/></div></div><div id="36855455" class="c"><input type="checkbox" id="c-36855455" checked=""/><div class="controls bullet"><span class="by">duped</span><span>|</span><a href="#36854189">parent</a><span>|</span><a href="#36855089">prev</a><span>|</span><a href="#36853937">next</a><span>|</span><label class="collapse" for="c-36855455">[-]</label><label class="expand" for="c-36855455">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not really limited since the instruction decoder is actually doing a bunch of work to assign to physical registers. The 16 GPRs are an abstraction, iirc.<p>Even though it&#x27;s only one extra bit to index 16 registers vs 8, it takes an entire extra byte in the encoding (you have to add the REX prefix with the R, X, and B bits set appropriately for the MSB of the registers you&#x27;re modifying in the MODR&#x2F;M byte). And you only get 4 bits to use out of that byte because the leading four are a way to disambiguate it (also iirc, this stuff is terribly documented).<p>And paying an extra byte per GRP instruction is actually kind of expensive. If you look at your compiler output you&#x27;ll see a bunch of Exx instructions even in 64 bit long mode when optimizing for instruction decode&#x2F;size.<p>It turns out a variable length encoding is really hard to extend without getting weird with it.</div><br/></div></div></div></div><div id="36853937" class="c"><input type="checkbox" id="c-36853937" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#36854189">prev</a><span>|</span><a href="#36853239">next</a><span>|</span><label class="collapse" for="c-36853937">[-]</label><label class="expand" for="c-36853937">[8 more]</label></div><br/><div class="children"><div class="content">So .... when will it ship? No mention of physical products anywhere.<p>I wonder why this long delays are still necessary. In the old days yes as there were so many parties to coordinate but nowadays, in theory, Intel could release hardware, the new ISA and compiler&#x2F;OS patches and binaries on the same day.</div><br/><div id="36854481" class="c"><input type="checkbox" id="c-36854481" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36853937">parent</a><span>|</span><a href="#36854208">next</a><span>|</span><label class="collapse" for="c-36854481">[-]</label><label class="expand" for="c-36854481">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I wonder why this long delays are still necessary. In the old days yes as there were so many parties to coordinate but nowadays, in theory, Intel could release hardware, the new ISA and compiler&#x2F;OS patches and binaries on the same day.<p>Most new ISA support requires at least minimal support from the OS to use correctly (e.g., something like saving state on context switching, or reporting hardware support bits correctly). Releasing patches on the same day you release hardware means the new features are literally unusable for all of your customers, and it generally takes several months to go from a patch to a usable OS release. You could in theory release the patches on the same day as the new ISA documentation, but in practice, it&#x27;s likely to take some time because the people writing the patches aren&#x27;t the people writing the new ISA whitepapers and approving their publication and it takes time to go from &quot;oh, I can talk about this now&quot; to actually doing so.</div><br/></div></div><div id="36854208" class="c"><input type="checkbox" id="c-36854208" checked=""/><div class="controls bullet"><span class="by">TheCondor</span><span>|</span><a href="#36853937">parent</a><span>|</span><a href="#36854481">prev</a><span>|</span><a href="#36854211">next</a><span>|</span><label class="collapse" for="c-36854208">[-]</label><label class="expand" for="c-36854208">[1 more]</label></div><br/><div class="children"><div class="content">They floated a &quot;white paper&quot; a few months back about removing legacy instructions. <a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;articles&#x2F;technical&#x2F;envisioning-future-simplified-architecture.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;articles&#x2F;t...</a><p>Taking a little time with these sorts of things isn&#x27;t bad.   This will essentially be a new epic where AMD and Intel may end up being incompatible, at least for a while,  there are tooling changes to make, OS support, all sorts of things.   If they&#x27;re serious about dropping 16bit and 32bit from the architecture, doing it during a change like this might make some sense.   Do it wrong and they could Osborn themselves, but an Apple like migration strategy would be nice and I&#x27;d appreciate it.</div><br/></div></div><div id="36854211" class="c"><input type="checkbox" id="c-36854211" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#36853937">parent</a><span>|</span><a href="#36854208">prev</a><span>|</span><a href="#36856204">next</a><span>|</span><label class="collapse" for="c-36854211">[-]</label><label class="expand" for="c-36854211">[2 more]</label></div><br/><div class="children"><div class="content">They announce it in time so that llvm, gcc, MSVC, Java, .NET and the browser JS engine vendors can update their stacks to be ready when the silicon ships.<p>After all why spend a shitload of money on developing a feature you&#x27;ll want to market when there won&#x27;t be any reason for customers to buy your new hardware?</div><br/><div id="36859328" class="c"><input type="checkbox" id="c-36859328" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#36853937">root</a><span>|</span><a href="#36854211">parent</a><span>|</span><a href="#36856204">next</a><span>|</span><label class="collapse" for="c-36859328">[-]</label><label class="expand" for="c-36859328">[1 more]</label></div><br/><div class="children"><div class="content">Intel employee people who work on Java and LLVM at least, so they could release their own versions of those. I guess V8 wouldn&#x27;t be hard to add.<p>The main reason to do a coordinated release would be competitive advantage, I guess, plus to ensure that there are actually compilers that support their new instructions available right from the start. Intel already release their own Linux distro optimized for their chips because other vendors were targeting the lowest common denominator</div><br/></div></div></div></div><div id="36856204" class="c"><input type="checkbox" id="c-36856204" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#36853937">parent</a><span>|</span><a href="#36854211">prev</a><span>|</span><a href="#36854712">next</a><span>|</span><label class="collapse" for="c-36856204">[-]</label><label class="expand" for="c-36856204">[1 more]</label></div><br/><div class="children"><div class="content"><i>I wonder why this long delays are still necessary</i><p>Making sure the thing actually works? At least I hope they&#x27;re still doing that... and it&#x27;s ironic to see this comment at the same time a rather horrible bug was discovered in AMD&#x27;s CPUs:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36848680">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36848680</a><p>If anything, Intel should probably slow down and take more care: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16058920">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16058920</a></div><br/></div></div><div id="36854712" class="c"><input type="checkbox" id="c-36854712" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#36853937">parent</a><span>|</span><a href="#36856204">prev</a><span>|</span><a href="#36854040">next</a><span>|</span><label class="collapse" for="c-36854712">[-]</label><label class="expand" for="c-36854712">[1 more]</label></div><br/><div class="children"><div class="content">No earlier than Q1 2025.<p>None of the products announced for 2024 (e.g. Sierra Forrest, Granite Rapids, Arrow Lake, Arrow Lake S, Lunar Lake) support this.</div><br/></div></div><div id="36854040" class="c"><input type="checkbox" id="c-36854040" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36853937">parent</a><span>|</span><a href="#36854712">prev</a><span>|</span><a href="#36853239">next</a><span>|</span><label class="collapse" for="c-36854040">[-]</label><label class="expand" for="c-36854040">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t want it to leak so they at least need to announce it before engineering samples go out.</div><br/></div></div></div></div><div id="36853239" class="c"><input type="checkbox" id="c-36853239" checked=""/><div class="controls bullet"><span class="by">serhack_</span><span>|</span><a href="#36853937">prev</a><span>|</span><a href="#36858126">next</a><span>|</span><label class="collapse" for="c-36853239">[-]</label><label class="expand" for="c-36853239">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Intel® APX doubles the number of general-purpose registers (GPRs) from 16 to 32.</div><br/><div id="36853545" class="c"><input type="checkbox" id="c-36853545" checked=""/><div class="controls bullet"><span class="by">_chris_</span><span>|</span><a href="#36853239">parent</a><span>|</span><a href="#36858126">next</a><span>|</span><label class="collapse" for="c-36853545">[-]</label><label class="expand" for="c-36853545">[5 more]</label></div><br/><div class="children"><div class="content">And adds non-destructive instructions.<p>&gt; &quot;In addition, legacy integer instructions now can also use EVEX to encode a dedicated destination register operand – turning them into three-operand instructions and reducing the need for extra register move instructions.&quot;<p>Overall, APX is providing 10% fewer instructions, 10% fewer loads and more than 20% fewer stores.<p>Also adding pop2&#x2F;push2 instructions for moving state faster.<p>And adding more powerful conditional instructions (loads&#x2F;stores&#x2F;compares) and flag-suppression.</div><br/><div id="36853827" class="c"><input type="checkbox" id="c-36853827" checked=""/><div class="controls bullet"><span class="by">FullyFunctional</span><span>|</span><a href="#36853239">root</a><span>|</span><a href="#36853545">parent</a><span>|</span><a href="#36853762">next</a><span>|</span><label class="collapse" for="c-36853827">[-]</label><label class="expand" for="c-36853827">[3 more]</label></div><br/><div class="children"><div class="content">Oh missed your comment and posted essentially the same.  These are all interesting changes, predication certainly, but the thing that actually got me the most excited was the press release comment about:<p>&quot;The processor tracks these new instructions internally and fast-forwards register data between matching PUSH2 and POP2 instructions without going through memory.&quot;<p>I wonder if this implies that pushes don&#x27;t have to commit to memory if they are popped soon enough?  It has always bothered me that we have these huge physical register files but force all the spill and restore to go through memory because of silly anachronistic processor semantics.  With a more flexible PUSH&#x2F;POP semantics we could essentially get the register windows for free.</div><br/><div id="36855921" class="c"><input type="checkbox" id="c-36855921" checked=""/><div class="controls bullet"><span class="by">chc4</span><span>|</span><a href="#36853239">root</a><span>|</span><a href="#36853827">parent</a><span>|</span><a href="#36853762">next</a><span>|</span><label class="collapse" for="c-36855921">[-]</label><label class="expand" for="c-36855921">[2 more]</label></div><br/><div class="children"><div class="content">Intel x86 stack engines have done 0-cycle store&#x2F;load forwarding for years now.</div><br/><div id="36856018" class="c"><input type="checkbox" id="c-36856018" checked=""/><div class="controls bullet"><span class="by">FullyFunctional</span><span>|</span><a href="#36853239">root</a><span>|</span><a href="#36855921">parent</a><span>|</span><a href="#36853762">next</a><span>|</span><label class="collapse" for="c-36856018">[-]</label><label class="expand" for="c-36856018">[1 more]</label></div><br/><div class="children"><div class="content">? That was not my question nor my point.  They hit memory and, as I have since learned, they still do after this.  The wording in the press release was ambiguous.  In other words, the news here is just being able to push&#x2F;pop two in one µop.</div><br/></div></div></div></div></div></div><div id="36853762" class="c"><input type="checkbox" id="c-36853762" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#36853239">root</a><span>|</span><a href="#36853545">parent</a><span>|</span><a href="#36853827">prev</a><span>|</span><a href="#36858126">next</a><span>|</span><label class="collapse" for="c-36853762">[-]</label><label class="expand" for="c-36853762">[1 more]</label></div><br/><div class="children"><div class="content">10% fewer instructions but average instruction is longer, so code density is the same, they claim. This still leaves their ISA with the worst code density of any non-obsolete ISA.</div><br/></div></div></div></div></div></div><div id="36858126" class="c"><input type="checkbox" id="c-36858126" checked=""/><div class="controls bullet"><span class="by">johnklos</span><span>|</span><a href="#36853239">prev</a><span>|</span><a href="#36854154">next</a><span>|</span><label class="collapse" for="c-36858126">[-]</label><label class="expand" for="c-36858126">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Intel® APX demonstrates the advantage of the variable-length instruction encodings of x86 – new features enhancing the entire instruction set can be defined with only incremental changes to the instruction-decode hardware.&quot;<p>In other words, their initial sloppiness was a Feature™: our initial mess was so bad that changes like these don&#x27;t make it any worse!</div><br/></div></div><div id="36854154" class="c"><input type="checkbox" id="c-36854154" checked=""/><div class="controls bullet"><span class="by">jamesy0ung</span><span>|</span><a href="#36858126">prev</a><span>|</span><a href="#36853647">next</a><span>|</span><label class="collapse" for="c-36854154">[-]</label><label class="expand" for="c-36854154">[1 more]</label></div><br/><div class="children"><div class="content">I thought it was going to be related to the iAPX (Intel Advanced Performance Architecture)<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IAPX" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IAPX</a></div><br/></div></div><div id="36853647" class="c"><input type="checkbox" id="c-36853647" checked=""/><div class="controls bullet"><span class="by">KerrAvon</span><span>|</span><a href="#36854154">prev</a><span>|</span><a href="#36853541">next</a><span>|</span><label class="collapse" for="c-36853647">[-]</label><label class="expand" for="c-36853647">[2 more]</label></div><br/><div class="children"><div class="content">So it sounds like (among other things) they&#x27;re adding 3-address integer instructions to an instruction encoding only used for vector instructions today.<p>I was not familiar with the AVX vector instructions at this level of detail.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;EVEX_prefix" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;EVEX_prefix</a></div><br/><div id="36854284" class="c"><input type="checkbox" id="c-36854284" checked=""/><div class="controls bullet"><span class="by">peterfirefly</span><span>|</span><a href="#36853647">parent</a><span>|</span><a href="#36853541">next</a><span>|</span><label class="collapse" for="c-36854284">[-]</label><label class="expand" for="c-36854284">[1 more]</label></div><br/><div class="children"><div class="content">Have you noticed that some of the bits in the EVEX prefix (after the 62h) byte are inverted?<p>That&#x27;s because it hides inside the BOUND instruction in a way that allows it to be used outside of 64-bit mode code.  The BOUND instruction never existed in 64-bit mode, so Intel was free to do whatever they wanted with the 62h opcode, but they thought it would be enable EVEX in 16&#x2F;32-bit code too.<p>The BOUND instruction must take a memory operand -- so the MOD bits can never be 11b (which would specify a register operand).  The MOD bits are the upper two bits of the modrm byte (the byte right after the 62h opcode).  So, if we don&#x27;t allow the &quot;upper&quot; registers in 32-bit mode and only the &quot;lower&quot; 8 registers AND if we invert bit 3 of their register numbers and put those extra register specifier bits in bit 7&#x2F;6&#x2F;5 of the byte after the 62h opcode THEN we can fit EVEX into 32-bit mode, because those bits will always be 111b in 32-bit mode!<p>So outside of 64-bit mode, if we have a 62h opcode with a modrm byte with MOD!=11b: it&#x27;s a BOUND instruction.  If MOD=11b: it&#x27;s an EVEX prefix.</div><br/></div></div></div></div><div id="36853541" class="c"><input type="checkbox" id="c-36853541" checked=""/><div class="controls bullet"><span class="by">FullyFunctional</span><span>|</span><a href="#36853647">prev</a><span>|</span><a href="#36853735">next</a><span>|</span><label class="collapse" for="c-36853541">[-]</label><label class="expand" for="c-36853541">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;legacy integer instructions now can also use EVEX to encode a dedicated destination register operand – turning them into three-operand instructions&quot;<p>x86 ISA is growing more RISC-like.  Definitely saving on stack spilling is a Good Thing™</div><br/></div></div><div id="36853735" class="c"><input type="checkbox" id="c-36853735" checked=""/><div class="controls bullet"><span class="by">FullyFunctional</span><span>|</span><a href="#36853541">prev</a><span>|</span><a href="#36854128">next</a><span>|</span><label class="collapse" for="c-36853735">[-]</label><label class="expand" for="c-36853735">[13 more]</label></div><br/><div class="children"><div class="content">The &quot;wall of text&quot; TL;DR:<p>- +16 registers (thus 32) and optionally separate destination (looking very RISC like now)<p>- PUSH2&#x2F;POP2 <i>with full forwarding</i><p>- Much expanded predication, including predicated loads and stores<p>This is pretty interesting.  Especially the latter can make a big difference for highly unpredictable memory intensive code, like compression.</div><br/><div id="36854126" class="c"><input type="checkbox" id="c-36854126" checked=""/><div class="controls bullet"><span class="by">peterfirefly</span><span>|</span><a href="#36853735">parent</a><span>|</span><a href="#36854125">next</a><span>|</span><label class="collapse" for="c-36854126">[-]</label><label class="expand" for="c-36854126">[2 more]</label></div><br/><div class="children"><div class="content">+ Two-byte REX2 prefix that replaces the REX prefix (and 0F prefix).<p>REX2 is D5 + a byte that is a lot like the lower nibble of a REX prefix, only twice as big.<p>It has two extra bits for the up to three registers that can be named in normal x86 instructions: either two registers or a register operand and a memory operand that can use two registers + a displacement for the memory address.<p>It also contains the W bit like REX does (64-bit).<p>It also has a bit called M0 that indicates whether the instruction is in opcode map 0 (primary opcode map) or opcode map 1 (0F opcode map).<p>That means that a REX2 instruction from opcode map 1 takes the same number of bytes as a REX instruction from opcode map 1.  Instructions from opcode map 0 are one byte longer with REX2 than with REX.<p>Some of the normal ALU instructions also get an EVEX encoding (in map 4).  That allows for a different data destination than before (separate from source the operand(s)).  It also allows for ALU instructions that don&#x27;t change the flags, which must be really nice for the out of order&#x2F;data forwarding circuitry.</div><br/><div id="36856581" class="c"><input type="checkbox" id="c-36856581" checked=""/><div class="controls bullet"><span class="by">drudru</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854126">parent</a><span>|</span><a href="#36854125">next</a><span>|</span><label class="collapse" for="c-36856581">[-]</label><label class="expand" for="c-36856581">[1 more]</label></div><br/><div class="children"><div class="content">0xD5 was the old BCD instruction: AAD</div><br/></div></div></div></div><div id="36854125" class="c"><input type="checkbox" id="c-36854125" checked=""/><div class="controls bullet"><span class="by">_old_dude_</span><span>|</span><a href="#36853735">parent</a><span>|</span><a href="#36854126">prev</a><span>|</span><a href="#36854128">next</a><span>|</span><label class="collapse" for="c-36854125">[-]</label><label class="expand" for="c-36854125">[10 more]</label></div><br/><div class="children"><div class="content">Also AVX-10, E-cores and P-cores having 512 bits vector registers (see the last references).</div><br/><div id="36854341" class="c"><input type="checkbox" id="c-36854341" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854125">parent</a><span>|</span><a href="#36854222">next</a><span>|</span><label class="collapse" for="c-36854341">[-]</label><label class="expand" for="c-36854341">[1 more]</label></div><br/><div class="children"><div class="content">No, the E-cores will implement only a 256-bit subset of AVX-512, which halves the size of the vector registers to 256-bit and the size of the mask registers to 32-bit. The same subset will be implemented on the P-cores combined with E-cores.<p>This subset AVX10&#x2F;256, is the reason for this new specification. It is the Intel response to AMD Zen 4.<p>When their competitor supports AVX-512 on all products, Intel had to do something to remain competitive. Because they believe that supporting the full AVX-512 on their E-cores is too expensive, they have created a subset of AVX-512, including only the instructions with an operand size up to 256 bits.</div><br/></div></div><div id="36854222" class="c"><input type="checkbox" id="c-36854222" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854125">parent</a><span>|</span><a href="#36854341">prev</a><span>|</span><a href="#36854128">next</a><span>|</span><label class="collapse" for="c-36854222">[-]</label><label class="expand" for="c-36854222">[8 more]</label></div><br/><div class="children"><div class="content">If I read the note correctly, P-cores won&#x27;t have 512-bit vector registers, but they will have the other fancy stuff added by AVX-512 (namely, vector predication stuff, static rounding mode instructions, new vector instructions like complex multiply or half-precision float, and 32 vector registers), just only for 128-bit and 256-bit vectors. Which, to be fair, is arguably the more useful parts of AVX-512 anyways; the maximum vector length being upped isn&#x27;t all that interesting.</div><br/><div id="36859506" class="c"><input type="checkbox" id="c-36859506" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854222">parent</a><span>|</span><a href="#36854306">next</a><span>|</span><label class="collapse" for="c-36859506">[-]</label><label class="expand" for="c-36859506">[1 more]</label></div><br/><div class="children"><div class="content">What Intel says exactly:<p>&quot;A “converged” version of Intel AVX10 with maximum vector lengths of 256 bits and 32-bit opmask registers will be supported across all Intel processors, while 512-bit vector registers and 64-bit opmasks will continue to be supported on some P-core processors.&quot;<p>So all future Intel CPUs starting in 2025 will support a 256-bit subset of AVX-512, where AVX-512 is rebranded as AVX10.<p>Only <i>some</i> P-core processors will support the full 512-bit AVX-512 a.k.a. AVX10, which is to be understood that only those server CPUs that contain only P-cores, i.e. the successors of Granite Rapids and Granite Rapids D, will support 512-bit registers and instructions (and 64-bit mask registers instead of 32-bit mask registers).</div><br/></div></div><div id="36854306" class="c"><input type="checkbox" id="c-36854306" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854222">parent</a><span>|</span><a href="#36859506">prev</a><span>|</span><a href="#36854668">next</a><span>|</span><label class="collapse" for="c-36854306">[-]</label><label class="expand" for="c-36854306">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If I read the note correctly, P-cores won&#x27;t have 512-bit vector registers<p>I don’t entirely agree. See the second graphic here: <a href="https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;Intel-AVX10" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;Intel-AVX10</a><p>Mentioned above the graphic:<p>&gt; Part of making AVX10 suitable for both P and E cores is that the converged version has a maximum vector length of 256-bits and found with the E cores while P cores will have optional 512-bit vector use.<p>512-bit support will be optional, so maybe every P-core won’t have it… maybe it’ll be restricted to higher end processors? But it sounds like some <i>will</i> have it, or it wouldn’t be an option at all.</div><br/><div id="36856582" class="c"><input type="checkbox" id="c-36856582" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854306">parent</a><span>|</span><a href="#36854668">next</a><span>|</span><label class="collapse" for="c-36856582">[-]</label><label class="expand" for="c-36856582">[1 more]</label></div><br/><div class="children"><div class="content">presumably it will be the server chips that are all p cores that have it. to me it seems like a dumb choice, but it is consistent with what Intel is doing now</div><br/></div></div></div></div><div id="36854345" class="c"><input type="checkbox" id="c-36854345" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854222">parent</a><span>|</span><a href="#36854668">prev</a><span>|</span><a href="#36854666">next</a><span>|</span><label class="collapse" for="c-36854345">[-]</label><label class="expand" for="c-36854345">[2 more]</label></div><br/><div class="children"><div class="content">P-cores will be 512-bit and E-cores will be 256-bit. Seems unnecessarily complex to me.</div><br/><div id="36859541" class="c"><input type="checkbox" id="c-36859541" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854345">parent</a><span>|</span><a href="#36854666">next</a><span>|</span><label class="collapse" for="c-36859541">[-]</label><label class="expand" for="c-36859541">[1 more]</label></div><br/><div class="children"><div class="content">No, see another more complete reply above.<p>P-cores in hybrid CPUs will be 256-bit (like E-cores).<p>P-cores in (server) CPUs that contain only P-cores will be 512-bit.</div><br/></div></div></div></div><div id="36854666" class="c"><input type="checkbox" id="c-36854666" checked=""/><div class="controls bullet"><span class="by">_old_dude_</span><span>|</span><a href="#36853735">root</a><span>|</span><a href="#36854222">parent</a><span>|</span><a href="#36854345">prev</a><span>|</span><a href="#36854128">next</a><span>|</span><label class="collapse" for="c-36854666">[-]</label><label class="expand" for="c-36854666">[1 more]</label></div><br/><div class="children"><div class="content">yes, thanks,</div><br/></div></div></div></div></div></div></div></div><div id="36854128" class="c"><input type="checkbox" id="c-36854128" checked=""/><div class="controls bullet"><span class="by">muricula</span><span>|</span><a href="#36853735">prev</a><span>|</span><a href="#36853676">next</a><span>|</span><label class="collapse" for="c-36854128">[-]</label><label class="expand" for="c-36854128">[4 more]</label></div><br/><div class="children"><div class="content">It seems like most of these new instructions and registers correspond to the original armv8 base isa. I&#x27;m going to go out on a limb here and suppose that&#x27;s not an accident. Does anyone know why Intel thinks x86 needs them?<p>Is the goal here to increase the decode bandwidth of Intel CPUs?<p>Is the goal to reduce demands on load-store units by increasing the number of registers?<p>Are they hoping to make it easier to port or JIT armv8 asm to Intel CPUs?</div><br/><div id="36854218" class="c"><input type="checkbox" id="c-36854218" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#36854128">parent</a><span>|</span><a href="#36856553">next</a><span>|</span><label class="collapse" for="c-36854218">[-]</label><label class="expand" for="c-36854218">[2 more]</label></div><br/><div class="children"><div class="content">Most new instructions are not inspired by Armv8, they just implement the traditional 3-address format for 32 registers, which predates Armv8 by a few decades.<p>Nevertheless, there are a few instructions inspired by Armv8, mainly PUSH2 and POP2, which correspond to the load register pair and store register pair of Aarch64.</div><br/><div id="36857176" class="c"><input type="checkbox" id="c-36857176" checked=""/><div class="controls bullet"><span class="by">comex</span><span>|</span><a href="#36854128">root</a><span>|</span><a href="#36854218">parent</a><span>|</span><a href="#36856553">next</a><span>|</span><label class="collapse" for="c-36857176">[-]</label><label class="expand" for="c-36857176">[1 more]</label></div><br/><div class="children"><div class="content">The new CCMP conditional-compare instruction is also equivalent to ARMv8’s instruction of the same name.    If a condition passes, compare two registers; if not, set the condition bits to an arbitrary value.<p>On ARM that instruction is a pain in the ass when reading disassemblies, because the on-fail condition bits are just specified as a number from 0 to 15; the disassembler doesn’t bother to label which bits are specified, let alone what conditions they correspond to.  Unfortunately it seems like Intel is doing the same thing in their assembly syntax, at least if I’m reading the document correctly.</div><br/></div></div></div></div><div id="36856553" class="c"><input type="checkbox" id="c-36856553" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#36854128">parent</a><span>|</span><a href="#36854218">prev</a><span>|</span><a href="#36853676">next</a><span>|</span><label class="collapse" for="c-36856553">[-]</label><label class="expand" for="c-36856553">[1 more]</label></div><br/><div class="children"><div class="content">well given the difficulty of making wider x86 decoders it does seem pretty reasonable to try reducing the number of load&#x2F;stores needed</div><br/></div></div></div></div><div id="36853676" class="c"><input type="checkbox" id="c-36853676" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36854128">prev</a><span>|</span><a href="#36854295">next</a><span>|</span><label class="collapse" for="c-36853676">[-]</label><label class="expand" for="c-36853676">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Intel® APX demonstrates the advantage of the variable-length instruction encodings of x86 – new features enhancing the entire instruction set can be defined with only incremental changes to the instruction-decode hardware. This flexibility has allowed Intel® architecture to adapt and flourish over four decades of rapid advances in computing – and it enables the innovations that will keep it thriving into the future.</div><br/></div></div><div id="36854295" class="c"><input type="checkbox" id="c-36854295" checked=""/><div class="controls bullet"><span class="by">soulbadguy</span><span>|</span><a href="#36853676">prev</a><span>|</span><a href="#36853617">next</a><span>|</span><label class="collapse" for="c-36854295">[-]</label><label class="expand" for="c-36854295">[1 more]</label></div><br/><div class="children"><div class="content">couldn&#x27;t find the info anywhere, but any ETA on this ? Or least what is the first Arch supporting this? Redwood Cove or someting later ?</div><br/></div></div><div id="36853617" class="c"><input type="checkbox" id="c-36853617" checked=""/><div class="controls bullet"><span class="by">dmitrygr</span><span>|</span><a href="#36854295">prev</a><span>|</span><a href="#36853760">next</a><span>|</span><label class="collapse" for="c-36853617">[-]</label><label class="expand" for="c-36853617">[31 more]</label></div><br/><div class="children"><div class="content">So, in about 30 years when the majority of the CPUs have this, we can use it. Assuming intel does not gate this just to XEON for no reason whatsoever, like they did to AVX512?</div><br/><div id="36854097" class="c"><input type="checkbox" id="c-36854097" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#36853617">parent</a><span>|</span><a href="#36853899">next</a><span>|</span><label class="collapse" for="c-36854097">[-]</label><label class="expand" for="c-36854097">[4 more]</label></div><br/><div class="children"><div class="content">Performance-intensive code (hot loops) could be compiled multiple times for each architecture extension and switched with CPUID. That&#x27;s how all the various SSE and AVX extensions were rolled out in multimedia code.<p>AFAIK there was AVX512 in higher-end desktop SKUs, but not the latest E-core designs in Intel&#x27;s client chips. The main problems are that:<p>- Operations at 512-bit register widths have significant power draw. Intel chips that support AVX512 have to downclock themselves on AVX512 workloads until their voltage regulators have boosted up to a higher voltage.<p>- The AVX512 register file is too big to physically fit in the E-core[0] footprint.<p>Incidentally I do remember Linus Torvalds specifically complaining that AVX512 was being used to implement memcpy in gcc, because it meant running certain programs would lower system performance. So these new architectures tend to be used a lot sooner than the time it takes for it to be safe to make them your minimum compile target.<p>[0] The BIOS on my Framework laptop refers to these as &quot;Atom cores&quot; - no clue if the current E-core design is derived from Atom or if this is a miscommunication or nickname AMI picked.</div><br/><div id="36854498" class="c"><input type="checkbox" id="c-36854498" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854097">parent</a><span>|</span><a href="#36857251">next</a><span>|</span><label class="collapse" for="c-36854498">[-]</label><label class="expand" for="c-36854498">[1 more]</label></div><br/><div class="children"><div class="content">No, operations at 512-bit register widths do not have significant power draw, as demonstrated by AMD Zen 4.<p>What has significant power draw is the use of double 512-bit floating-point multipliers, as implemented in the Intel server CPUs (though one core with such multipliers draws significantly less power than two cores having the same throughput).<p>AMD uses only double 256-bit floating-point multipliers and in general it uses exactly the same execution units for both 256-bit and 512-bit operations, so the AVX-512 operations do not increase the power draw even when they use 512-bit registers.<p>Also the AVX512 register file is not too big to physically fit in the E-core. Even if the AVX512 register file is 4 times greater than the AVX register file, the E-cores have much a much larger register file used to rename the architecturally visible registers.<p>Despite these facts, Intel still believes that implementing the full AVX-512 ISA in the E-cores is too expensive, so they have created this new specification of AVX10&#x2F;256, which is just a subset of AVX-512 including the instructions with an operand size up to 256 bits, and which will be implemented in all future E-cores after some date, perhaps starting in 2025.</div><br/></div></div><div id="36857251" class="c"><input type="checkbox" id="c-36857251" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854097">parent</a><span>|</span><a href="#36854498">prev</a><span>|</span><a href="#36853899">next</a><span>|</span><label class="collapse" for="c-36857251">[-]</label><label class="expand" for="c-36857251">[2 more]</label></div><br/><div class="children"><div class="content">Yes, E-cores are Atom cores. You can tell because they have -mont in the codename.</div><br/><div id="36859580" class="c"><input type="checkbox" id="c-36859580" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36857251">parent</a><span>|</span><a href="#36853899">next</a><span>|</span><label class="collapse" for="c-36859580">[-]</label><label class="expand" for="c-36859580">[1 more]</label></div><br/><div class="children"><div class="content">Moreover, besides their history, the E-cores continue to be sold using the Atom brand, for instance &quot;Intel Atom® x7425E&quot; (the industrial variant of the Intel N100, one of the Alder Lake N models).</div><br/></div></div></div></div></div></div><div id="36853899" class="c"><input type="checkbox" id="c-36853899" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#36853617">parent</a><span>|</span><a href="#36854097">prev</a><span>|</span><a href="#36853714">next</a><span>|</span><label class="collapse" for="c-36853899">[-]</label><label class="expand" for="c-36853899">[1 more]</label></div><br/><div class="children"><div class="content">Modern compilers already allow for conditional code execution depending on CPU sets, and at least in what concerns JVM implementations and the CLR, their JITs are clever enough to already use parts of AVX512, while they are not perfect, it is better than not using them at all.</div><br/></div></div><div id="36853714" class="c"><input type="checkbox" id="c-36853714" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36853617">parent</a><span>|</span><a href="#36853899">prev</a><span>|</span><a href="#36853736">next</a><span>|</span><label class="collapse" for="c-36853714">[-]</label><label class="expand" for="c-36853714">[19 more]</label></div><br/><div class="children"><div class="content">&quot;Tiered&quot; x86 packages and executables seem like the inevitable direction for linux distros.<p>CachyOS and Clear Linux already do this.<p>Base Arch Linux and openSUSE are working on it. Maybe Fedora too, but I can&#x27;t remeber<p>And it wouldn&#x27;t be totally insane for Windows to do this either.</div><br/><div id="36854361" class="c"><input type="checkbox" id="c-36854361" checked=""/><div class="controls bullet"><span class="by">nwallin</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853714">parent</a><span>|</span><a href="#36853781">next</a><span>|</span><label class="collapse" for="c-36854361">[-]</label><label class="expand" for="c-36854361">[4 more]</label></div><br/><div class="children"><div class="content">This is Gentoo&#x27;s whole shtick. It&#x27;s the core feature. It&#x27;s been supported since day 0, over 20 years ago.<p>I&#x27;m honestly a little amazed that more people don&#x27;t either use Gentoo or adopt its model, given the smorgasbord of mutually incompatible instruction set extensions. It seems very strange that people will buy a CPU that has 32 64-byte ZMM registers with 3 operand instructions, and then use that CPU to run code that operates on 8 16-byte XMM registers with 2 operand instructions.</div><br/><div id="36854513" class="c"><input type="checkbox" id="c-36854513" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854361">parent</a><span>|</span><a href="#36854836">next</a><span>|</span><label class="collapse" for="c-36854513">[-]</label><label class="expand" for="c-36854513">[1 more]</label></div><br/><div class="children"><div class="content">Because building from source is extremely time&#x2F;CPU consuming and also unreliable. Time is valuable. In my last Gentoo attempt, I had to manually fix a few build recipes before I threw in the towel.<p>This also means &quot;riskier&quot; methods (like LTO) have to be omitted by default.<p>Gentoo is great for libre software, security, manual patches, embedded computing and such. But for pure desktop performance, the Clear Linux way is best: aggressive compilation flags&#x2F;libraries, tested by the package maintainers, shipped in 3-4 tiers. And as the Clear Linux devs said, most of the native instructions dont even matter, as the compilers can&#x27;t use them.</div><br/></div></div><div id="36854836" class="c"><input type="checkbox" id="c-36854836" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854361">parent</a><span>|</span><a href="#36854513">prev</a><span>|</span><a href="#36855599">next</a><span>|</span><label class="collapse" for="c-36854836">[-]</label><label class="expand" for="c-36854836">[1 more]</label></div><br/><div class="children"><div class="content">The speedup for most code is really small and things like codecs that really benefit from AVX will detect and use it at runtime.</div><br/></div></div><div id="36855599" class="c"><input type="checkbox" id="c-36855599" checked=""/><div class="controls bullet"><span class="by">Tommstein</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854361">parent</a><span>|</span><a href="#36854836">prev</a><span>|</span><a href="#36853781">next</a><span>|</span><label class="collapse" for="c-36855599">[-]</label><label class="expand" for="c-36855599">[1 more]</label></div><br/><div class="children"><div class="content">Because most people have better shit to do than fix their now-broken system every other time they upgrade their packages. I used to run Gentoo, was told on IRC after like the 9,000th such breakage to go use something else if I didn&#x27;t like their perpetually broken free distribution, and I took their advice.</div><br/></div></div></div></div><div id="36853781" class="c"><input type="checkbox" id="c-36853781" checked=""/><div class="controls bullet"><span class="by">fooyc</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853714">parent</a><span>|</span><a href="#36854361">prev</a><span>|</span><a href="#36853736">next</a><span>|</span><label class="collapse" for="c-36853781">[-]</label><label class="expand" for="c-36853781">[14 more]</label></div><br/><div class="children"><div class="content">How does that work? The binary format embeds variants of the same program?</div><br/><div id="36854032" class="c"><input type="checkbox" id="c-36854032" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853781">parent</a><span>|</span><a href="#36853872">next</a><span>|</span><label class="collapse" for="c-36854032">[-]</label><label class="expand" for="c-36854032">[1 more]</label></div><br/><div class="children"><div class="content">Yes, here is an example how it works for GCC.<p><a href="https:&#x2F;&#x2F;gcc.gnu.org&#x2F;onlinedocs&#x2F;gcc-13.1.0&#x2F;gcc&#x2F;Function-Multiversioning.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;gcc.gnu.org&#x2F;onlinedocs&#x2F;gcc-13.1.0&#x2F;gcc&#x2F;Function-Multi...</a></div><br/></div></div><div id="36853872" class="c"><input type="checkbox" id="c-36853872" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853781">parent</a><span>|</span><a href="#36854032">prev</a><span>|</span><a href="#36853882">next</a><span>|</span><label class="collapse" for="c-36853872">[-]</label><label class="expand" for="c-36853872">[11 more]</label></div><br/><div class="children"><div class="content">On linux distros, the package manager downloads different binaries based on your CPU. Skylake would be x86-64-v3, Zen 4 would be x86-64-v4, for example.<p>And there are different schemes for multiple architectures in the same program, like hwcaps.</div><br/><div id="36853961" class="c"><input type="checkbox" id="c-36853961" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853872">parent</a><span>|</span><a href="#36853882">next</a><span>|</span><label class="collapse" for="c-36853961">[-]</label><label class="expand" for="c-36853961">[10 more]</label></div><br/><div class="children"><div class="content">Isn’t this going to get very unmanageable very soon? Intel seems to add extensions every other year or so.</div><br/><div id="36854353" class="c"><input type="checkbox" id="c-36854353" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853961">parent</a><span>|</span><a href="#36854070">next</a><span>|</span><label class="collapse" for="c-36854353">[-]</label><label class="expand" for="c-36854353">[2 more]</label></div><br/><div class="children"><div class="content">The extensions can be kinda broken down into 4 levels. Basically ancient, old (SSE 4.2), reasonably new (AVX2, Haswell&#x2F;Zen 1 and up), and baseline AVX512.<p><a href="https:&#x2F;&#x2F;developers.redhat.com&#x2F;blog&#x2F;2021&#x2F;01&#x2F;05&#x2F;building-red-hat-enterprise-linux-9-for-the-x86-64-v2-microarchitecture-level#background_of_the_x86_64_microarchitecture_levels" rel="nofollow noreferrer">https:&#x2F;&#x2F;developers.redhat.com&#x2F;blog&#x2F;2021&#x2F;01&#x2F;05&#x2F;building-red-h...</a><p>There is discussion of a fifth level. Someone in the Intel Clear Linux IRC said a fifth level wasn&#x27;t &quot;worth it&quot; for Sapphire Rapids because most of the new AVX512 extensions were not autovectorized by compilers, but that a new level would be needed in the future. Perhaps they were thinking of APX, but couldn&#x27;t disclose it.</div><br/><div id="36855160" class="c"><input type="checkbox" id="c-36855160" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854353">parent</a><span>|</span><a href="#36854070">next</a><span>|</span><label class="collapse" for="c-36855160">[-]</label><label class="expand" for="c-36855160">[1 more]</label></div><br/><div class="children"><div class="content">AVX10&#x2F;APX does sound like a good baseline for v5.</div><br/></div></div></div></div><div id="36854070" class="c"><input type="checkbox" id="c-36854070" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853961">parent</a><span>|</span><a href="#36854353">prev</a><span>|</span><a href="#36853882">next</a><span>|</span><label class="collapse" for="c-36854070">[-]</label><label class="expand" for="c-36854070">[7 more]</label></div><br/><div class="children"><div class="content">It’s easy to fully automate and storage is relatively cheap these days.</div><br/><div id="36854321" class="c"><input type="checkbox" id="c-36854321" checked=""/><div class="controls bullet"><span class="by">xxpor</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854070">parent</a><span>|</span><a href="#36853882">next</a><span>|</span><label class="collapse" for="c-36854321">[-]</label><label class="expand" for="c-36854321">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;d think the issue would be more build infra, every new variant means you have to build the world again</div><br/><div id="36855214" class="c"><input type="checkbox" id="c-36855214" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854321">parent</a><span>|</span><a href="#36854863">next</a><span>|</span><label class="collapse" for="c-36855214">[-]</label><label class="expand" for="c-36855214">[2 more]</label></div><br/><div class="children"><div class="content">Again, compute is surprisingly cheap these days.<p>Work out what it would cost to compile - say - a terabyte of C code at typical cloud spot prices.<p>A large VM with 128 cores can compile the 100 MB Linux kernel source tree in about 30 seconds. So… 200 MB&#x2F;minute or 12 GB&#x2F;hour. This would take 80 hours for a terabyte.<p>A 120 core AMD server is about 50c per hour on Azure (Linux spot pricing).<p>So… about $40 to compile an entire distro. Not exactly breaking the bank.</div><br/><div id="36856311" class="c"><input type="checkbox" id="c-36856311" checked=""/><div class="controls bullet"><span class="by">xxpor</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36855214">parent</a><span>|</span><a href="#36854863">next</a><span>|</span><label class="collapse" for="c-36856311">[-]</label><label class="expand" for="c-36856311">[1 more]</label></div><br/><div class="children"><div class="content">you&#x27;d have to separate out compiling and linking at a bare minimum to get even a semi accurate model. plus a lot of userspace is c++, which is much, much slower.</div><br/></div></div></div></div><div id="36854863" class="c"><input type="checkbox" id="c-36854863" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854321">parent</a><span>|</span><a href="#36855214">prev</a><span>|</span><a href="#36853882">next</a><span>|</span><label class="collapse" for="c-36854863">[-]</label><label class="expand" for="c-36854863">[3 more]</label></div><br/><div class="children"><div class="content">Yes. Also, test it.</div><br/><div id="36855233" class="c"><input type="checkbox" id="c-36855233" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854863">parent</a><span>|</span><a href="#36853882">next</a><span>|</span><label class="collapse" for="c-36855233">[-]</label><label class="expand" for="c-36855233">[2 more]</label></div><br/><div class="children"><div class="content">That can also be largely automated.</div><br/><div id="36855676" class="c"><input type="checkbox" id="c-36855676" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36855233">parent</a><span>|</span><a href="#36853882">next</a><span>|</span><label class="collapse" for="c-36855676">[-]</label><label class="expand" for="c-36855676">[1 more]</label></div><br/><div class="children"><div class="content">LTO does rarely break things in hard to detect ways, but I have never heard of a -march x86 compilation bug.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36853882" class="c"><input type="checkbox" id="c-36853882" checked=""/><div class="controls bullet"><span class="by">slt2021</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853781">parent</a><span>|</span><a href="#36853872">prev</a><span>|</span><a href="#36853736">next</a><span>|</span><label class="collapse" for="c-36853882">[-]</label><label class="expand" for="c-36853882">[1 more]</label></div><br/><div class="children"><div class="content">in the end it will be like any other modern hardware appliance:<p>the hardware is the same design for cost saving purposes, but different features are unlocked for $$$ by a software license key.<p>You want AVX-512? pay up and unlock feature in your CPU and you can now use the feature. This could also enable pay-as-you-go license scheme for CPUs, creating recurring revenue for Intel<p>from the hardware perspective - the same silicon, but different features sold separately</div><br/></div></div></div></div></div></div><div id="36853736" class="c"><input type="checkbox" id="c-36853736" checked=""/><div class="controls bullet"><span class="by">fooyc</span><span>|</span><a href="#36853617">parent</a><span>|</span><a href="#36853714">prev</a><span>|</span><a href="#36853760">next</a><span>|</span><label class="collapse" for="c-36853736">[-]</label><label class="expand" for="c-36853736">[6 more]</label></div><br/><div class="children"><div class="content">Maybe JIT compilers can take profit of this immediately, since they target a single machine?</div><br/><div id="36853916" class="c"><input type="checkbox" id="c-36853916" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853736">parent</a><span>|</span><a href="#36853760">next</a><span>|</span><label class="collapse" for="c-36853916">[-]</label><label class="expand" for="c-36853916">[5 more]</label></div><br/><div class="children"><div class="content">Yup. It&#x27;s one of their theoretical advantages that&#x27;s about to become a lot less theoretical. Historically it hasn&#x27;t made much difference because optional instructions were hard for JIT compilers for most languages to use (in particular high level JITd languages tend not to support vector instructions very well). But a doubling of registers is the sort of extension that any kind of code can immediately profit from.<p>Arguably it will be <i>only</i> JITd languages that benefit from this for quite a while. These sorts of fundamental changes are basically a new ISA and the infrastructure isn&#x27;t really geared up to make doing that easy. Everyone would have to provide two versions of every app and shared library to get the most benefit, maybe even you get combinatorial complexity if people want to upgrade the inter-library calling conventions too. For native AOT compiled code it&#x27;s going to just be a mess.</div><br/><div id="36854050" class="c"><input type="checkbox" id="c-36854050" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853916">parent</a><span>|</span><a href="#36854310">next</a><span>|</span><label class="collapse" for="c-36854050">[-]</label><label class="expand" for="c-36854050">[1 more]</label></div><br/><div class="children"><div class="content">In what concerns the JVM and ART, and the CLR, it is quite practical, even if there is room for improvment.</div><br/></div></div><div id="36854310" class="c"><input type="checkbox" id="c-36854310" checked=""/><div class="controls bullet"><span class="by">xxpor</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853916">parent</a><span>|</span><a href="#36854050">prev</a><span>|</span><a href="#36854339">next</a><span>|</span><label class="collapse" for="c-36854310">[-]</label><label class="expand" for="c-36854310">[1 more]</label></div><br/><div class="children"><div class="content">Gentoo users will finally get to be smug again, once GCC&#x2F;clang have support for them.</div><br/></div></div><div id="36854339" class="c"><input type="checkbox" id="c-36854339" checked=""/><div class="controls bullet"><span class="by">titzer</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36853916">parent</a><span>|</span><a href="#36854310">prev</a><span>|</span><a href="#36853760">next</a><span>|</span><label class="collapse" for="c-36854339">[-]</label><label class="expand" for="c-36854339">[2 more]</label></div><br/><div class="children"><div class="content">All the more reason that Wasm should be the bottom of software :)</div><br/><div id="36858717" class="c"><input type="checkbox" id="c-36858717" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#36853617">root</a><span>|</span><a href="#36854339">parent</a><span>|</span><a href="#36853760">next</a><span>|</span><label class="collapse" for="c-36858717">[-]</label><label class="expand" for="c-36858717">[1 more]</label></div><br/><div class="children"><div class="content">IBM and Burroughs&#x2F;Unisys have already been doing that for decades, with bytecode based executables for their mainframe&#x2F;micros.<p>Or Xerox PARC, with their microcoded CPUs loading the desired interpreter on boot.<p>I guess, it is an idea that keeps being revalidated.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36853760" class="c"><input type="checkbox" id="c-36853760" checked=""/><div class="controls bullet"><span class="by">user20230724</span><span>|</span><a href="#36853617">prev</a><span>|</span><a href="#36858344">next</a><span>|</span><label class="collapse" for="c-36853760">[-]</label><label class="expand" for="c-36853760">[1 more]</label></div><br/><div class="children"><div class="content">Wow, perfect timing</div><br/></div></div><div id="36858344" class="c"><input type="checkbox" id="c-36858344" checked=""/><div class="controls bullet"><span class="by">ribit</span><span>|</span><a href="#36853760">prev</a><span>|</span><label class="collapse" for="c-36858344">[-]</label><label class="expand" for="c-36858344">[1 more]</label></div><br/><div class="children"><div class="content">Do this basically copies ARM’s Aarch64, but with a really awkward instruction encoding? Interesting move, Intel.</div><br/></div></div></div></div></div></div></div></body></html>