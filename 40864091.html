<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720342856172" as="style"/><link rel="stylesheet" href="styles.css?v=1720342856172"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://jtarchie.com/posts/2024-07-02-optimizing-large-scale-openstreetmap-data-with-sqlite">Optimizing Large-Scale OpenStreetMap Data with SQLite</a> <span class="domain">(<a href="https://jtarchie.com">jtarchie.com</a>)</span></div><div class="subtext"><span>thunderbong</span> | <span>33 comments</span></div><br/><div><div id="40891644" class="c"><input type="checkbox" id="c-40891644" checked=""/><div class="controls bullet"><span class="by">wcedmisten</span><span>|</span><a href="#40891535">next</a><span>|</span><label class="collapse" for="c-40891644">[-]</label><label class="expand" for="c-40891644">[12 more]</label></div><br/><div class="children"><div class="content">I recently discovered DuckDB&#x27;s Read_OSM() function [1], which lets you query OSM PBF files directly.<p>For example, it&#x27;s simple to count the cafes in North America in under 30s:<p><pre><code>  SELECT COUNT(*) FROM st_readOSM(&#x27;&#x2F;home&#x2F;wcedmisten&#x2F;Downloads&#x2F;north-america-latest.osm.pbf&#x27;) WHERE tags[&#x27;amenity&#x27;] = [&#x27;cafe&#x27;];
  ┌──────────────┐
  │ count_star() │
  │    int64     │
  ├──────────────┤
  │        57150 │
  └──────────────┘
  Run Time (s): real 24.643 user 379.067204 sys 3.696217

</code></pre>
Unfortunately, I discovered there are still some bugs [2] that need to be ironed out, but it seems very promising for doing high performance queries with minimal effort.<p>[1]: <a href="https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;extensions&#x2F;spatial.html#st_readosm--read-compressed-osm-data" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;extensions&#x2F;spatial.html#st_readosm--...</a><p>[2]: <a href="https:&#x2F;&#x2F;github.com&#x2F;duckdb&#x2F;duckdb_spatial&#x2F;issues&#x2F;349">https:&#x2F;&#x2F;github.com&#x2F;duckdb&#x2F;duckdb_spatial&#x2F;issues&#x2F;349</a></div><br/><div id="40892273" class="c"><input type="checkbox" id="c-40892273" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#40891644">parent</a><span>|</span><a href="#40891722">next</a><span>|</span><label class="collapse" for="c-40892273">[-]</label><label class="expand" for="c-40892273">[4 more]</label></div><br/><div class="children"><div class="content">Fascinating use of DuckDB!<p>Can I ask where you get official OSM PBF data from? (I found these two links, but not sure what data these contain)<p><a href="https:&#x2F;&#x2F;planet.openstreetmap.org&#x2F;pbf&#x2F;" rel="nofollow">https:&#x2F;&#x2F;planet.openstreetmap.org&#x2F;pbf&#x2F;</a><p><a href="http:&#x2F;&#x2F;download.geofabrik.de&#x2F;" rel="nofollow">http:&#x2F;&#x2F;download.geofabrik.de&#x2F;</a></div><br/><div id="40892384" class="c"><input type="checkbox" id="c-40892384" checked=""/><div class="controls bullet"><span class="by">wcedmisten</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40892273">parent</a><span>|</span><a href="#40891722">next</a><span>|</span><label class="collapse" for="c-40892384">[-]</label><label class="expand" for="c-40892384">[3 more]</label></div><br/><div class="children"><div class="content">Those are the most popular sources, and I&#x27;ve used both!<p>The first one is the official OpenStreetMap data, which contains the &quot;planet file&quot; - i.e. all the data for the entire world. But because OSM has so much stuff in it, the planet file is a whopping 76 GB, which can take a long time to process for most tasks. I also recommend using the torrent file for faster download speeds.<p>As a result of the planet&#x27;s size, the German company Geofabrik provides unofficial &quot;extracts&quot; of the data, which are filtered down to a specific region. E.g. all the data in a particular continent, country, or U.S. state. If you click on the &quot;Sub Region&quot; link it will show countries, and if you click on those it will show states.</div><br/><div id="40893356" class="c"><input type="checkbox" id="c-40893356" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40892384">parent</a><span>|</span><a href="#40892532">next</a><span>|</span><label class="collapse" for="c-40893356">[-]</label><label class="expand" for="c-40893356">[1 more]</label></div><br/><div class="children"><div class="content">I was able to find all 10 Whole Foods in the City of Chicago in 22.6s with DuckDB. It&#x27;s amazing! (there are tons more Whole Foods in the metro area, but it found the exact 10 in the city)<p><pre><code>        SELECT 
            tags[&#x27;addr:city&#x27;][1] city,
            tags[&#x27;addr:state&#x27;][1] state,
            tags[&#x27;brand&#x27;][1] brand,
            *, 
        FROM st_readosm(&#x27;us-latest.osm.pbf&#x27;)
        WHERE 1=1
        and city = &#x27;Chicago&#x27;
        and state = &#x27;IL&#x27;
        and brand = &#x27;Whole Foods Market&#x27;
</code></pre>
I&#x27;m sure there are ways to make this faster (partitioning, indexing, COPY TO native format, etc.) but querying a 9.8GB compressed raw format file with data (in key-value fields stored as strings) for the entire United States at this speed is pretty impressive to me.</div><br/></div></div><div id="40892532" class="c"><input type="checkbox" id="c-40892532" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40892384">parent</a><span>|</span><a href="#40893356">prev</a><span>|</span><a href="#40891722">next</a><span>|</span><label class="collapse" for="c-40892532">[-]</label><label class="expand" for="c-40892532">[1 more]</label></div><br/><div class="children"><div class="content">So it&#x27;s GeoFabrik is the same data, but regionalized. This sounds like what I need. I already use DuckDB, so this is great.<p>I appreciate your taking the time to share this tidbit! It&#x27;s a game changer in what I do (geospatial).</div><br/></div></div></div></div></div></div><div id="40891722" class="c"><input type="checkbox" id="c-40891722" checked=""/><div class="controls bullet"><span class="by">winrid</span><span>|</span><a href="#40891644">parent</a><span>|</span><a href="#40892273">prev</a><span>|</span><a href="#40891535">next</a><span>|</span><label class="collapse" for="c-40891722">[-]</label><label class="expand" for="c-40891722">[7 more]</label></div><br/><div class="children"><div class="content">That&#x27;s cool, but not what I would call high performance. If you do these often you would want an index, and should only take single digit ms.</div><br/><div id="40892254" class="c"><input type="checkbox" id="c-40892254" checked=""/><div class="controls bullet"><span class="by">wcedmisten</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40891722">parent</a><span>|</span><a href="#40892023">next</a><span>|</span><label class="collapse" for="c-40892254">[-]</label><label class="expand" for="c-40892254">[5 more]</label></div><br/><div class="children"><div class="content">The reason I call it high performance is that it avoids the hours&#x2F;days of processing (for the planet file)[1] that would be required for pulling the data out of PBF and indexing it. And you&#x27;d also need RAM at least the size of the planet to even get that level of speed.<p>You could certainly amortize this cost for repeated queries, but for one-off queries I haven&#x27;t seen anything faster.<p>[1]: <a href="https:&#x2F;&#x2F;wiki.openstreetmap.org&#x2F;wiki&#x2F;Osm2pgsql&#x2F;benchmarks" rel="nofollow">https:&#x2F;&#x2F;wiki.openstreetmap.org&#x2F;wiki&#x2F;Osm2pgsql&#x2F;benchmarks</a></div><br/><div id="40893827" class="c"><input type="checkbox" id="c-40893827" checked=""/><div class="controls bullet"><span class="by">winrid</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40892254">parent</a><span>|</span><a href="#40892023">next</a><span>|</span><label class="collapse" for="c-40893827">[-]</label><label class="expand" for="c-40893827">[4 more]</label></div><br/><div class="children"><div class="content">You wouldn&#x27;t need hundreds of gigs of ram to answer this query in 5ms. You&#x27;d need a few mb or so to answer this query, after initial indexing is done of course.</div><br/><div id="40893905" class="c"><input type="checkbox" id="c-40893905" checked=""/><div class="controls bullet"><span class="by">ericjmorey</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40893827">parent</a><span>|</span><a href="#40892023">next</a><span>|</span><label class="collapse" for="c-40893905">[-]</label><label class="expand" for="c-40893905">[3 more]</label></div><br/><div class="children"><div class="content">How long do you think it would take to index it?</div><br/><div id="40893987" class="c"><input type="checkbox" id="c-40893987" checked=""/><div class="controls bullet"><span class="by">winrid</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40893905">parent</a><span>|</span><a href="#40892023">next</a><span>|</span><label class="collapse" for="c-40893987">[-]</label><label class="expand" for="c-40893987">[2 more]</label></div><br/><div class="children"><div class="content">Depends on the machine :) hours maybe?</div><br/><div id="40895404" class="c"><input type="checkbox" id="c-40895404" checked=""/><div class="controls bullet"><span class="by">arp242</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40893987">parent</a><span>|</span><a href="#40892023">next</a><span>|</span><label class="collapse" for="c-40895404">[-]</label><label class="expand" for="c-40895404">[1 more]</label></div><br/><div class="children"><div class="content">So for a one-off query DuckDB is tons faster, and easier.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40892023" class="c"><input type="checkbox" id="c-40892023" checked=""/><div class="controls bullet"><span class="by">wild_egg</span><span>|</span><a href="#40891644">root</a><span>|</span><a href="#40891722">parent</a><span>|</span><a href="#40892254">prev</a><span>|</span><a href="#40891535">next</a><span>|</span><label class="collapse" for="c-40892023">[-]</label><label class="expand" for="c-40892023">[1 more]</label></div><br/><div class="children"><div class="content">Not near a computer to try this out but I&#x27;d be surprised if you couldn&#x27;t get a huge speed up by selecting the whole file into a real table first and querying against that. DuckDB should be able to better vectorize operations then</div><br/></div></div></div></div></div></div><div id="40891535" class="c"><input type="checkbox" id="c-40891535" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40891644">prev</a><span>|</span><a href="#40891880">next</a><span>|</span><label class="collapse" for="c-40891535">[-]</label><label class="expand" for="c-40891535">[1 more]</label></div><br/><div class="children"><div class="content">I liked the trick used here for speeding up tag key&#x2F;value queries using a FTS index:<p><pre><code>    SELECT id
    FROM entries e
    JOIN search s ON s.rowid = e.id
    WHERE
    -- use FTS index to find subset of possible results
    search MATCH &#x27;amenity cafe&#x27;
    -- use the subset to find exact matches
    AND tags-&gt;&gt;&#x27;amenity&#x27; = &#x27;cafe&#x27;;</code></pre></div><br/></div></div><div id="40891880" class="c"><input type="checkbox" id="c-40891880" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40891535">prev</a><span>|</span><a href="#40893854">next</a><span>|</span><label class="collapse" for="c-40891880">[-]</label><label class="expand" for="c-40891880">[7 more]</label></div><br/><div class="children"><div class="content">So, hype aside, what&#x27;s the over&#x2F;under on DuckDB vs Sqlite these days? I&#x27;m working on a Thing right now, has started with sqlite due to being a) good enough, b) stupendously optimized and nails hardened, and c) runs on your phone, your toaster, and your server.<p>What&#x27;s DuckDB bringing to the table relative to sqlite, which seems like the boring-and-therefore-best choice?</div><br/><div id="40895865" class="c"><input type="checkbox" id="c-40895865" checked=""/><div class="controls bullet"><span class="by">ilumanty</span><span>|</span><a href="#40891880">parent</a><span>|</span><a href="#40891900">next</a><span>|</span><label class="collapse" for="c-40895865">[-]</label><label class="expand" for="c-40895865">[1 more]</label></div><br/><div class="children"><div class="content">DuckDB only supports either one read+write client or arbitrary read clients with no writer. This is the biggest blocker for adopting it in a real-world hosted app.<p>Rel: <a href="https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;connect&#x2F;concurrency.html#handling-concurrency" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;connect&#x2F;concurrency.html#handling-co...</a></div><br/></div></div><div id="40891900" class="c"><input type="checkbox" id="c-40891900" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40891880">parent</a><span>|</span><a href="#40895865">prev</a><span>|</span><a href="#40893854">next</a><span>|</span><label class="collapse" for="c-40891900">[-]</label><label class="expand" for="c-40891900">[5 more]</label></div><br/><div class="children"><div class="content">Ha, looks like the DuckDB devs have a great and balanced answer for this: 
<a href="https:&#x2F;&#x2F;marclamberti.com&#x2F;blog&#x2F;duckdb-getting-started-for-beginners&#x2F;#DuckDB_vs_SQLite" rel="nofollow">https:&#x2F;&#x2F;marclamberti.com&#x2F;blog&#x2F;duckdb-getting-started-for-beg...</a><p>If you want row storage, use sqlite. If you want columnar storage, use DuckDB.</div><br/><div id="40892220" class="c"><input type="checkbox" id="c-40892220" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40891880">root</a><span>|</span><a href="#40891900">parent</a><span>|</span><a href="#40894442">next</a><span>|</span><label class="collapse" for="c-40892220">[-]</label><label class="expand" for="c-40892220">[3 more]</label></div><br/><div class="children"><div class="content">And thinking about my specific project, it&#x27;s clear that a great solution would allow me to pick row-vs-column storage on a per-table basis.<p>I basically have a graph over vectors, which is a very &#x27;row-storage&#x27; kind of things: I need to get a specific vector (a row of data), get all of its neighbors (rows in the edges table), and do some in-context computations to decide where to walk to next on the graph.<p>However, we also have some data attached to vectors (covariates, tags, etc) which we often want to work with in a more aggregated way. These tables seem possibly more reasonable to approach with a columnar format.</div><br/><div id="40892695" class="c"><input type="checkbox" id="c-40892695" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#40891880">root</a><span>|</span><a href="#40892220">parent</a><span>|</span><a href="#40892448">next</a><span>|</span><label class="collapse" for="c-40892695">[-]</label><label class="expand" for="c-40892695">[1 more]</label></div><br/><div class="children"><div class="content">Knowing how little I know about your problem and constraints, I&#x27;d use SQLite for everything. Then if a specific part seems columnar, do some experiments in DuckDB and see what speedups or space savings you get and then use both.<p>Sounds like you are more than one, so have some folks do some 4 hour spikes on DuckDB where you think it might be useful.<p>I&#x27;d use Rust as the top level, embedding SQLite, DuckDB and Lua. In my application I used SQLite with FTS and then many SQLite databases not more than 2GB in size with rows containing binary blobs that didn&#x27;t uncompress to more than 20 megs each.</div><br/></div></div><div id="40892448" class="c"><input type="checkbox" id="c-40892448" checked=""/><div class="controls bullet"><span class="by">1659447091</span><span>|</span><a href="#40891880">root</a><span>|</span><a href="#40892220">parent</a><span>|</span><a href="#40892695">prev</a><span>|</span><a href="#40894442">next</a><span>|</span><label class="collapse" for="c-40892448">[-]</label><label class="expand" for="c-40892448">[1 more]</label></div><br/><div class="children"><div class="content">&gt; a great solution would allow me to pick row-vs-column storage on a per-table basis ... and do some in-context computations to decide where to walk to next on the graph<p>You may be interested in Datomic[0] or Datascript[1]<p>[0]: <a href="https:&#x2F;&#x2F;www.datomic.com&#x2F;benefits.html" rel="nofollow">https:&#x2F;&#x2F;www.datomic.com&#x2F;benefits.html</a><p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;tonsky&#x2F;datascript">https:&#x2F;&#x2F;github.com&#x2F;tonsky&#x2F;datascript</a></div><br/></div></div></div></div><div id="40894442" class="c"><input type="checkbox" id="c-40894442" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#40891880">root</a><span>|</span><a href="#40891900">parent</a><span>|</span><a href="#40892220">prev</a><span>|</span><a href="#40893854">next</a><span>|</span><label class="collapse" for="c-40894442">[-]</label><label class="expand" for="c-40894442">[1 more]</label></div><br/><div class="children"><div class="content">other(maybe more) important points: indexes vs non-indexes and heavy batch updates vs smaller updates and inserts.</div><br/></div></div></div></div></div></div><div id="40893854" class="c"><input type="checkbox" id="c-40893854" checked=""/><div class="controls bullet"><span class="by">zamazan4ik</span><span>|</span><a href="#40891880">prev</a><span>|</span><a href="#40892919">next</a><span>|</span><label class="collapse" for="c-40893854">[-]</label><label class="expand" for="c-40893854">[2 more]</label></div><br/><div class="children"><div class="content">If you are interested in optimizing the project further, I can suggest you rebuilding SQLite with Profile-Guided Optimization (PGO). I collected as many as possible materials (including many related benchmarks) in my repo: <a href="https:&#x2F;&#x2F;github.com&#x2F;zamazan4ik&#x2F;awesome-pgo">https:&#x2F;&#x2F;github.com&#x2F;zamazan4ik&#x2F;awesome-pgo</a> . Regarding SQLite and PGO, I have the following link: <a href="https:&#x2F;&#x2F;sqlite.org&#x2F;forum&#x2F;forumpost&#x2F;19870fae957d8c1a" rel="nofollow">https:&#x2F;&#x2F;sqlite.org&#x2F;forum&#x2F;forumpost&#x2F;19870fae957d8c1a</a></div><br/><div id="40896188" class="c"><input type="checkbox" id="c-40896188" checked=""/><div class="controls bullet"><span class="by">bomewish</span><span>|</span><a href="#40893854">parent</a><span>|</span><a href="#40892919">next</a><span>|</span><label class="collapse" for="c-40896188">[-]</label><label class="expand" for="c-40896188">[1 more]</label></div><br/><div class="children"><div class="content">Have you tested PGO for FTS in SQLite? It wasn’t clear from all those links. Would be very interested in that.</div><br/></div></div></div></div><div id="40892919" class="c"><input type="checkbox" id="c-40892919" checked=""/><div class="controls bullet"><span class="by">Scaevolus</span><span>|</span><a href="#40893854">prev</a><span>|</span><a href="#40891904">next</a><span>|</span><label class="collapse" for="c-40892919">[-]</label><label class="expand" for="c-40892919">[1 more]</label></div><br/><div class="children"><div class="content">&gt; CREATE INDEX entries_name ON entries(tags-&gt;&gt;&#x27;name&#x27;);
&gt; However, this requires an index per tag, which won’t scale, especially for a dynamic list of tags.<p>That&#x27;s not how indexes work at all. This will be fine.</div><br/></div></div><div id="40891904" class="c"><input type="checkbox" id="c-40891904" checked=""/><div class="controls bullet"><span class="by">mnmatin</span><span>|</span><a href="#40892919">prev</a><span>|</span><a href="#40893429">next</a><span>|</span><label class="collapse" for="c-40891904">[-]</label><label class="expand" for="c-40891904">[1 more]</label></div><br/><div class="children"><div class="content">Sometime ago, I had to extract a large amount of data from OSM and found the process harder than it should have been. Similar to how you shrunk the size by 50% after removing unnecessary tags. Ended up creating a python package &#x27;earth-osm&#x27; (<a href="https:&#x2F;&#x2F;github.com&#x2F;pypsa-meets-earth&#x2F;earth-osm">https:&#x2F;&#x2F;github.com&#x2F;pypsa-meets-earth&#x2F;earth-osm</a>) that makes things more intuitive. Always wanted to push the data into a database but never got around to that....</div><br/></div></div><div id="40893429" class="c"><input type="checkbox" id="c-40893429" checked=""/><div class="controls bullet"><span class="by">dzogchen</span><span>|</span><a href="#40891904">prev</a><span>|</span><a href="#40892497">next</a><span>|</span><label class="collapse" for="c-40893429">[-]</label><label class="expand" for="c-40893429">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This highlights the importance of iterative refinement and the power of combining different technologies to solve problems.<p>This uninformative non-sentence sounds an awful lot like ChatGPT.</div><br/></div></div><div id="40892497" class="c"><input type="checkbox" id="c-40892497" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#40893429">prev</a><span>|</span><a href="#40891905">next</a><span>|</span><label class="collapse" for="c-40892497">[-]</label><label class="expand" for="c-40892497">[1 more]</label></div><br/><div class="children"><div class="content">I went through a similar process when I converted wikipedia xml dumps into sqlite.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28013347">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28013347</a></div><br/></div></div><div id="40894982" class="c"><input type="checkbox" id="c-40894982" checked=""/><div class="controls bullet"><span class="by">marklit</span><span>|</span><a href="#40891905">prev</a><span>|</span><a href="#40893217">next</a><span>|</span><label class="collapse" for="c-40894982">[-]</label><label class="expand" for="c-40894982">[1 more]</label></div><br/><div class="children"><div class="content">The GPKG files <a href="https:&#x2F;&#x2F;github.com&#x2F;marklit&#x2F;osm_split">https:&#x2F;&#x2F;github.com&#x2F;marklit&#x2F;osm_split</a> produces are SQLite files under the hood and come with an R-Tree Spatial index so they&#x27;ll load into QGIS and other GIS software quickly.</div><br/></div></div><div id="40893217" class="c"><input type="checkbox" id="c-40893217" checked=""/><div class="controls bullet"><span class="by">DonnyV</span><span>|</span><a href="#40894982">prev</a><span>|</span><a href="#40895222">next</a><span>|</span><label class="collapse" for="c-40893217">[-]</label><label class="expand" for="c-40893217">[2 more]</label></div><br/><div class="children"><div class="content">If you convert those PBFs to Parquet files. You can then use Duckdb to search them with sub-sec response. Plus you get the added bonus of being able to host in an S3 type cloud storage.<p><a href="https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;extensions&#x2F;httpfs&#x2F;s3api" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;extensions&#x2F;httpfs&#x2F;s3api</a></div><br/><div id="40895955" class="c"><input type="checkbox" id="c-40895955" checked=""/><div class="controls bullet"><span class="by">highway900</span><span>|</span><a href="#40893217">parent</a><span>|</span><a href="#40895222">next</a><span>|</span><label class="collapse" for="c-40895955">[-]</label><label class="expand" for="c-40895955">[1 more]</label></div><br/><div class="children"><div class="content">This is the business. I highly recommend this solution. Streaming queries via duckdb or polars will use little to memory and be lightning fast for very large datasets.</div><br/></div></div></div></div><div id="40895222" class="c"><input type="checkbox" id="c-40895222" checked=""/><div class="controls bullet"><span class="by">kopirgan</span><span>|</span><a href="#40893217">prev</a><span>|</span><a href="#40892105">next</a><span>|</span><label class="collapse" for="c-40895222">[-]</label><label class="expand" for="c-40895222">[1 more]</label></div><br/><div class="children"><div class="content">This is brilliant</div><br/></div></div></div></div></div></div></div></body></html>