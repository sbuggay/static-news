<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1717059669946" as="style"/><link rel="stylesheet" href="styles.css?v=1717059669946"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2403.01643">New attention mechanisms that outperform standard multi-head attention</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>snats</span> | <span>39 comments</span></div><br/><div><div id="40519828" class="c"><input type="checkbox" id="c-40519828" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#40516649">next</a><span>|</span><label class="collapse" for="c-40519828">[-]</label><label class="expand" for="c-40519828">[5 more]</label></div><br/><div class="children"><div class="content">Because self-attention can be replaced with FFT for a loss in accuracy and a reduction in kWh [1], I suspect that the Quantum Fourier Transform can also be substituted for attention in LLMs.<p>[1] <a href="https:&#x2F;&#x2F;syncedreview.com&#x2F;2021&#x2F;05&#x2F;14&#x2F;deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19&#x2F;" rel="nofollow">https:&#x2F;&#x2F;syncedreview.com&#x2F;2021&#x2F;05&#x2F;14&#x2F;deepmind-podracer-tpu-ba...</a><p>&quot;You Need to Pay Better Attention&quot; (2024) 
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2403.01643" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2403.01643</a> :<p>&gt; <i>Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3&#x2F;4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1&#x2F;2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications.</i><p>&quot;Leave No Context Behind: Efficient Infinite Context Transformers&quot; (2024) <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.07143" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.07143</a> :<p>&gt; <i>A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on</i> long-context language modeling benchmarks, <i>1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.</i></div><br/><div id="40519970" class="c"><input type="checkbox" id="c-40519970" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#40519828">parent</a><span>|</span><a href="#40516649">next</a><span>|</span><label class="collapse" for="c-40519970">[-]</label><label class="expand" for="c-40519970">[4 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t believe that FNet paper flew under my radar all this time—what a cool idea! It&#x27;s remarkable to me that it works so well considering I haven&#x27;t heard <i>anyone</i> mention it before! Do you know if any follow-up work was done?</div><br/><div id="40520401" class="c"><input type="checkbox" id="c-40520401" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#40519828">root</a><span>|</span><a href="#40519970">parent</a><span>|</span><a href="#40520407">next</a><span>|</span><label class="collapse" for="c-40520401">[-]</label><label class="expand" for="c-40520401">[1 more]</label></div><br/><div class="children"><div class="content">This is the paper, which appears to be cited in hundreds of others, some of which appear to be about efficiency gains. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2105.03824" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2105.03824</a></div><br/></div></div><div id="40520407" class="c"><input type="checkbox" id="c-40520407" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#40519828">root</a><span>|</span><a href="#40519970">parent</a><span>|</span><a href="#40520401">prev</a><span>|</span><a href="#40516649">next</a><span>|</span><label class="collapse" for="c-40520407">[-]</label><label class="expand" for="c-40520407">[2 more]</label></div><br/><div class="children"><div class="content">I know AI is moving fast but they were only published within the last month or three.</div><br/><div id="40520463" class="c"><input type="checkbox" id="c-40520463" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#40519828">root</a><span>|</span><a href="#40520407">parent</a><span>|</span><a href="#40516649">next</a><span>|</span><label class="collapse" for="c-40520463">[-]</label><label class="expand" for="c-40520463">[1 more]</label></div><br/><div class="children"><div class="content">I was referring to link [1] in GP’s comment, which is to a paper published in 2021, not to either of the more recent papers they published.</div><br/></div></div></div></div></div></div></div></div><div id="40516649" class="c"><input type="checkbox" id="c-40516649" checked=""/><div class="controls bullet"><span class="by">lalaland1125</span><span>|</span><a href="#40519828">prev</a><span>|</span><a href="#40520046">next</a><span>|</span><label class="collapse" for="c-40516649">[-]</label><label class="expand" for="c-40516649">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The Transformer models, used in this experiment, all have a single attention layer with model dimension and context length 32.<p>I think we are going to need to see more experiments here, especially because the theoretical motivations here are weak</div><br/></div></div><div id="40520046" class="c"><input type="checkbox" id="c-40520046" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#40516649">prev</a><span>|</span><a href="#40518998">next</a><span>|</span><label class="collapse" for="c-40520046">[-]</label><label class="expand" for="c-40520046">[1 more]</label></div><br/><div class="children"><div class="content">Another piece of solid work in this space is DeepSeek-v2. They proposed MLA which outperform standard attention a little but reduce KV cache by over a magnitude. Not sure  if these improvements could come together.</div><br/></div></div><div id="40518998" class="c"><input type="checkbox" id="c-40518998" checked=""/><div class="controls bullet"><span class="by">smaddox</span><span>|</span><a href="#40520046">prev</a><span>|</span><a href="#40516592">next</a><span>|</span><label class="collapse" for="c-40518998">[-]</label><label class="expand" for="c-40518998">[1 more]</label></div><br/><div class="children"><div class="content">The first two changes appear theoretically sound, but it&#x27;s not clear that they would result in an actual performance improvement at scale. Their analysis ignores that a single matrix multiplication is typically used to calculate the Q, K, and V values from the inputs.<p>The third change looks like it would break causal masking for auto regressive language models. For masked token language models and ViTs, perhaps it&#x27;s an improvement, though.</div><br/></div></div><div id="40516592" class="c"><input type="checkbox" id="c-40516592" checked=""/><div class="controls bullet"><span class="by">marcinzm</span><span>|</span><a href="#40518998">prev</a><span>|</span><a href="#40518901">next</a><span>|</span><label class="collapse" for="c-40516592">[-]</label><label class="expand" for="c-40516592">[1 more]</label></div><br/><div class="children"><div class="content">These seems very tiny models and as I understand it LLMs behave fairly differently at different scales.<p>The speed performance gain seems to only be on an M2 chip and I wonder if there&#x27;s already much better non-GPU optimized attention approaches out there for those use cases.</div><br/></div></div><div id="40518901" class="c"><input type="checkbox" id="c-40518901" checked=""/><div class="controls bullet"><span class="by">jawon</span><span>|</span><a href="#40516592">prev</a><span>|</span><a href="#40517989">next</a><span>|</span><label class="collapse" for="c-40518901">[-]</label><label class="expand" for="c-40518901">[7 more]</label></div><br/><div class="children"><div class="content">Can anyone point me to models that look like they might actually be useful in moving towards AGI? I feel like I have a basic understanding of the transformer architecture, and multiplying X tokens in a sliding window across a set of static matrices to produce 1 new token does not look like a path to AGI.<p>Yes, the complex feature extraction is impressive. But are there any models that, I don&#x27;t know, are more dynamic? Have a working memory? Have a less limited execution path?</div><br/><div id="40520670" class="c"><input type="checkbox" id="c-40520670" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#40518901">parent</a><span>|</span><a href="#40519796">next</a><span>|</span><label class="collapse" for="c-40520670">[-]</label><label class="expand" for="c-40520670">[1 more]</label></div><br/><div class="children"><div class="content">Current models can already argued to have something like working memory by storing information in little-used parts of the tokens. If placeholder tokens are handed to them that they can use as working memory, performance improves.<p><a href="https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=2dnO3LLiJ1" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=2dnO3LLiJ1</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40329675">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40329675</a></div><br/></div></div><div id="40519796" class="c"><input type="checkbox" id="c-40519796" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#40518901">parent</a><span>|</span><a href="#40520670">prev</a><span>|</span><a href="#40519727">next</a><span>|</span><label class="collapse" for="c-40519796">[-]</label><label class="expand" for="c-40519796">[1 more]</label></div><br/><div class="children"><div class="content">Look at JEPA and Modulo-LLM.<p>Also AGI is a poor term to use because we as humans have no notion of what general intelligence is, does GI have morals and ethics, does it make decisions like we do based on executive functioning or does it work more like how ants do?</div><br/></div></div><div id="40519727" class="c"><input type="checkbox" id="c-40519727" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#40518901">parent</a><span>|</span><a href="#40519796">prev</a><span>|</span><a href="#40519731">next</a><span>|</span><label class="collapse" for="c-40519727">[-]</label><label class="expand" for="c-40519727">[1 more]</label></div><br/><div class="children"><div class="content">The answer might just be scale for all we know.</div><br/></div></div><div id="40519731" class="c"><input type="checkbox" id="c-40519731" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#40518901">parent</a><span>|</span><a href="#40519727">prev</a><span>|</span><a href="#40520090">next</a><span>|</span><label class="collapse" for="c-40519731">[-]</label><label class="expand" for="c-40519731">[1 more]</label></div><br/><div class="children"><div class="content">The answer is simple: AI systems aren’t just one technique, agent, or even <i>agency</i> — any somewhat anthropomorphic ones will be ensemblematic on an extensive and fundamentally-recursive level. LLMs are a groundbreaking technique that solve the “Frame Problem” by emulating human subconscious generative networks.<p>To paraphrase an old comment on here: the problem isn’t a chatbot gaining sapience inside a browser window, the problem is when billions of dollars are allocated to a self-administering ensemble of 10,000 GPT agents, each specialized for some task (aka functions). That, plus Wikipedia, Cyc, WolfraAlpha, YouTube, and <i>Google Books</i> at its fingertips.<p>“General” doesn’t even begin to cover what we’re already capable of, IMO.<p>See: Marvin Minsky, 1991;
<a href="https:&#x2F;&#x2F;ojs.aaai.org&#x2F;aimagazine&#x2F;index.php&#x2F;aimagazine&#x2F;article&#x2F;download&#x2F;894&#x2F;812" rel="nofollow">https:&#x2F;&#x2F;ojs.aaai.org&#x2F;aimagazine&#x2F;index.php&#x2F;aimagazine&#x2F;article...</a></div><br/></div></div><div id="40520090" class="c"><input type="checkbox" id="c-40520090" checked=""/><div class="controls bullet"><span class="by">daavidhauser</span><span>|</span><a href="#40518901">parent</a><span>|</span><a href="#40519731">prev</a><span>|</span><a href="#40519005">next</a><span>|</span><label class="collapse" for="c-40520090">[-]</label><label class="expand" for="c-40520090">[1 more]</label></div><br/><div class="children"><div class="content">xLSTM has a working memory and seems to outperform transformer architectures: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.04517" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.04517</a></div><br/></div></div><div id="40519005" class="c"><input type="checkbox" id="c-40519005" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#40518901">parent</a><span>|</span><a href="#40520090">prev</a><span>|</span><a href="#40517989">next</a><span>|</span><label class="collapse" for="c-40519005">[-]</label><label class="expand" for="c-40519005">[1 more]</label></div><br/><div class="children"><div class="content">There are sporadic attempts at making things more dynamic, like the Neural Turing Machine. It doesn’t seem to buy much actual power.</div><br/></div></div></div></div><div id="40517989" class="c"><input type="checkbox" id="c-40517989" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#40518901">prev</a><span>|</span><a href="#40516646">next</a><span>|</span><label class="collapse" for="c-40517989">[-]</label><label class="expand" for="c-40517989">[1 more]</label></div><br/><div class="children"><div class="content">The models tested are extremely small, a few thousand parameters and the performance is of course not great, I don&#x27;t think we can extrapolate much from this. I don&#x27;t understand why they chose such small models when you can train much larger ones for free on Colab or Kaggle if you really need it.</div><br/></div></div><div id="40516646" class="c"><input type="checkbox" id="c-40516646" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#40517989">prev</a><span>|</span><a href="#40520145">next</a><span>|</span><label class="collapse" for="c-40516646">[-]</label><label class="expand" for="c-40516646">[2 more]</label></div><br/><div class="children"><div class="content">I feel like FlashAttention is the relevant baseline here.</div><br/><div id="40516678" class="c"><input type="checkbox" id="c-40516678" checked=""/><div class="controls bullet"><span class="by">lalaland1125</span><span>|</span><a href="#40516646">parent</a><span>|</span><a href="#40520145">next</a><span>|</span><label class="collapse" for="c-40516678">[-]</label><label class="expand" for="c-40516678">[1 more]</label></div><br/><div class="children"><div class="content">FlashAttention is completely orthogonal to this. This work is about speeding up the computation of Q, K and V vectors while FlashAttention is about speeding up the attention algorithm itself.<p>You could combine the two.</div><br/></div></div></div></div><div id="40520145" class="c"><input type="checkbox" id="c-40520145" checked=""/><div class="controls bullet"><span class="by">skyde</span><span>|</span><a href="#40516646">prev</a><span>|</span><a href="#40516475">next</a><span>|</span><label class="collapse" for="c-40520145">[-]</label><label class="expand" for="c-40520145">[1 more]</label></div><br/><div class="children"><div class="content">Where is the code for it ?</div><br/></div></div><div id="40516475" class="c"><input type="checkbox" id="c-40516475" checked=""/><div class="controls bullet"><span class="by">317070</span><span>|</span><a href="#40520145">prev</a><span>|</span><a href="#40516988">next</a><span>|</span><label class="collapse" for="c-40516475">[-]</label><label class="expand" for="c-40516475">[7 more]</label></div><br/><div class="children"><div class="content">&gt; we evaluate the presented attention mechanisms on MNIST, CIFAR100, IMDB Movie Reviews, and Amazon Reviews datasets.<p>It sounds amazing, but I&#x27;m not holding my breath this one will scale.</div><br/><div id="40517417" class="c"><input type="checkbox" id="c-40517417" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#40516475">parent</a><span>|</span><a href="#40518333">next</a><span>|</span><label class="collapse" for="c-40517417">[-]</label><label class="expand" for="c-40517417">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes it doesn’t need to. You might have a problem that isn’t web scale and where transfer learning is hard. We also need techniques for small datasets even if they are slower to train or are outperformed after 5 billion tokens.</div><br/></div></div><div id="40518333" class="c"><input type="checkbox" id="c-40518333" checked=""/><div class="controls bullet"><span class="by">lgessler</span><span>|</span><a href="#40516475">parent</a><span>|</span><a href="#40517417">prev</a><span>|</span><a href="#40516619">next</a><span>|</span><label class="collapse" for="c-40518333">[-]</label><label class="expand" for="c-40518333">[1 more]</label></div><br/><div class="children"><div class="content">Yep, came here to say this. The big thing about the results here that might not be obvious to someone not in AI is that the models being trained in this paper are very many orders of magnitude smaller than the LLMs we&#x27;ve all heard so much about recently, and they&#x27;re also being trained on specific tasks instead of general language modeling.<p>So I&#x27;m not expecting this will find its way into a LLaMA near me any time soon, but maybe this is an interesting result for people working in the specific domains represented in the evaluations.</div><br/></div></div><div id="40516619" class="c"><input type="checkbox" id="c-40516619" checked=""/><div class="controls bullet"><span class="by">r2_pilot</span><span>|</span><a href="#40516475">parent</a><span>|</span><a href="#40518333">prev</a><span>|</span><a href="#40516988">next</a><span>|</span><label class="collapse" for="c-40516619">[-]</label><label class="expand" for="c-40516619">[4 more]</label></div><br/><div class="children"><div class="content">You could provide the quote in full(&quot;In addition to providing rigorous mathematical comparisons,&quot;) so that the author&#x27;s work in proving their point is not hidden by your effortless snark.</div><br/><div id="40517209" class="c"><input type="checkbox" id="c-40517209" checked=""/><div class="controls bullet"><span class="by">317070</span><span>|</span><a href="#40516475">root</a><span>|</span><a href="#40516619">parent</a><span>|</span><a href="#40516988">next</a><span>|</span><label class="collapse" for="c-40517209">[-]</label><label class="expand" for="c-40517209">[3 more]</label></div><br/><div class="children"><div class="content">I am not sure how much experience you have in this area of research, but maybe I can shed some light on the background here. The &quot;Attention is all you need&quot; paper is now almost 7 years old. Those 7 years have seen a flood of proposals on improving transformers, only very few have been retained.<p>There is very little theoretic about transformer-style architectures. Fundamentally, the proof is in the pudding, not in &quot;mathematical comparisons&quot;. A proposed change needs to scale better, it is all that matters. And the datasets mentioned are simply unsuitable for showing any scaling. I think the biggest dataset in this list, is 160MB compressed.<p>I am not sure why this article was posted here on hackernews. I would estimate even just today, there have probably been about 3 papers posted on arXiv with proposed transformer architecture changes, tested on larger datasets than the ones mentioned here.</div><br/><div id="40517292" class="c"><input type="checkbox" id="c-40517292" checked=""/><div class="controls bullet"><span class="by">317070</span><span>|</span><a href="#40516475">root</a><span>|</span><a href="#40517209">parent</a><span>|</span><a href="#40519020">next</a><span>|</span><label class="collapse" for="c-40517292">[-]</label><label class="expand" for="c-40517292">[1 more]</label></div><br/><div class="children"><div class="content">I checked, and on the 28th of May, arXiv has seen 14 submissions with &quot;transformer&quot; in the title, and I found 3 of them with proposals tested on larger datasets (I did not check all of them, there might have been more than these three).<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.18240" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.18240</a>
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.17951" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.17951</a>
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.17821" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.17821</a></div><br/></div></div><div id="40519020" class="c"><input type="checkbox" id="c-40519020" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#40516475">root</a><span>|</span><a href="#40517209">parent</a><span>|</span><a href="#40517292">prev</a><span>|</span><a href="#40516988">next</a><span>|</span><label class="collapse" for="c-40519020">[-]</label><label class="expand" for="c-40519020">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I am not sure why this article was posted here on hackernews.<p>New is where progress comes from, so new is interesting. New is why we come here, and the first three letters of News.<p>&gt; here is very little theoretic about transformer-style architectures.<p>Only way to fix that is with new.<p>&gt; Fundamentally, the proof is in the pudding, not in &quot;mathematical comparisons&quot;<p>&quot;Can it scale&quot; is something only someone with money can answer. It can be tested, but only if it&#x27;s known. Now new is better known.</div><br/></div></div></div></div></div></div></div></div><div id="40516988" class="c"><input type="checkbox" id="c-40516988" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40516475">prev</a><span>|</span><a href="#40518949">next</a><span>|</span><label class="collapse" for="c-40516988">[-]</label><label class="expand" for="c-40516988">[8 more]</label></div><br/><div class="children"><div class="content">Pressing [X] to doubt.<p>There are many alternatives to the good old transformers: RWKV, Mamba, etc.<p>Yet here we are, still using transformers (actually, just the decoder part). Is it because the industry has so much inertia to pick up new methods? I doubt it because there&#x27;s $BILLIONS in this market and everyone wants a piece of the AI cake, so it doesn&#x27;t make sense to ignore promising methods.<p>Why, then, we barely see any non-transformer production-ready LLM these days?</div><br/><div id="40517757" class="c"><input type="checkbox" id="c-40517757" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#40516988">parent</a><span>|</span><a href="#40518455">next</a><span>|</span><label class="collapse" for="c-40517757">[-]</label><label class="expand" for="c-40517757">[1 more]</label></div><br/><div class="children"><div class="content">I believe the attention mechanism we use now was introduced in 2014 by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio in their paper titled &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot;<p>2014. It took almost a decade for the potential of this technique to be realized and come to the attention (heh) of most developers. I don&#x27;t know what researchers are doing with Mamba and RWKV, but we should let them cook.</div><br/></div></div><div id="40518455" class="c"><input type="checkbox" id="c-40518455" checked=""/><div class="controls bullet"><span class="by">marcinzm</span><span>|</span><a href="#40516988">parent</a><span>|</span><a href="#40517757">prev</a><span>|</span><a href="#40517116">next</a><span>|</span><label class="collapse" for="c-40518455">[-]</label><label class="expand" for="c-40518455">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why, then, we barely see any non-transformer production-ready LLM these days?<p>Because having a 5% better non-transformer model doesn&#x27;t help you if as a result you can&#x27;t use the 10% improvements people publish that only apply to transformers. Very quickly you&#x27;ll be 5% worse than those who stuck with transformers, and have wasted a ton of time and money.</div><br/></div></div><div id="40517116" class="c"><input type="checkbox" id="c-40517116" checked=""/><div class="controls bullet"><span class="by">solidasparagus</span><span>|</span><a href="#40516988">parent</a><span>|</span><a href="#40518455">prev</a><span>|</span><a href="#40517811">next</a><span>|</span><label class="collapse" for="c-40517116">[-]</label><label class="expand" for="c-40517116">[3 more]</label></div><br/><div class="children"><div class="content">It’s going to take time. I can’t speak to the actual quality of mamba other than to say the authors are extraordinary and should be taken seriously.<p>But training a large model requires a huge amount of capital so the biggest runs are designed around risk minimization. And remember, many of the decision makers of these runs made are in their positions by doing transformer-centric work. The true value of mamba is still unclear to me with very long context techniques being effective for transformers.</div><br/><div id="40519236" class="c"><input type="checkbox" id="c-40519236" checked=""/><div class="controls bullet"><span class="by">inciampati</span><span>|</span><a href="#40516988">root</a><span>|</span><a href="#40517116">parent</a><span>|</span><a href="#40517811">next</a><span>|</span><label class="collapse" for="c-40519236">[-]</label><label class="expand" for="c-40519236">[2 more]</label></div><br/><div class="children"><div class="content">To be frank, the long-context techniques that you&#x27;re describing are still extremely limited in the length of context to consider, only a million-token order, and extremely expensive to apply.</div><br/><div id="40520560" class="c"><input type="checkbox" id="c-40520560" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40516988">root</a><span>|</span><a href="#40519236">parent</a><span>|</span><a href="#40517811">next</a><span>|</span><label class="collapse" for="c-40520560">[-]</label><label class="expand" for="c-40520560">[1 more]</label></div><br/><div class="children"><div class="content">And yet quadratic attention is still unavoidable. Anything that lets you have sub quadratic attention is going to have an accuracy Vs performance tradeoff.</div><br/></div></div></div></div></div></div><div id="40517811" class="c"><input type="checkbox" id="c-40517811" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#40516988">parent</a><span>|</span><a href="#40517116">prev</a><span>|</span><a href="#40518949">next</a><span>|</span><label class="collapse" for="c-40517811">[-]</label><label class="expand" for="c-40517811">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I doubt it because there&#x27;s $BILLIONS in this market and everyone wants a piece of the AI cake, so it doesn&#x27;t make sense to ignore promising methods.<p>I also doubt this result. The &quot;why have $BILLIONS not already invested&quot; question is interesting in its own right though. Generally, the literature on the theoretical bounds of swarm optimization is pertinent. Those $BILLIONS aren&#x27;t being invested by a single omniscient entity, so they&#x27;re subject to interesting constraints.<p>As one of many explanations, fragmentation is common. If $BILLIONS are split between greedy, mostly non-interacting entities (e.g., competing companies each trying to replace the transformer in a bounded number of hours and dollars while securing their market dominance), you expect, probabilistically, for each of them to converge on the same strateg(y&#x2F;ies), especially if the &quot;best&quot; alternatives are obvious or globally known for some reason (e.g., some solutions intuitively feel &quot;natural&quot; or your researchers publish early results or you have employee movement between companies or whatever). Riskier strategies won&#x27;t be touched, and you&#x27;ll have $BILLIONS spent duplicating the same most likely alternatives when $MILLIONS would have sufficed.<p>The normal counterpoint is that a few big players dominate the spending, and they would have higher internal coordination. Interestingly though, they don&#x27;t usually, except when that coordination would tend to enforce the same strategies smaller competition are pursuing. How often do you hear about stories like the misaligned Google+ integrations resulting in employee bonuses for poor customer experiences vs a forward-thinking executive actively devoting funds to a meaningful number of competing solutions? Approximately never. It&#x27;s career suicide if you fail and depend on other people for your position, you _are_ actually more likely to outdo the competition with your increased resources if you just lean into the &quot;best&quot; alternatives, and for a whole host of reasons very few executives (except for people with real power) will coordinate a more comprehensive strategy, certainly not one orthogonal to the competition&#x27;s just for the sake of allocating the global $BILLIONS more efficiently.<p>Separately (going back to the merits of the preprint), I&#x27;ll probably read the full thing later, but a few points stuck out as suspicious on an initial skim. Notably, they seem to mix linear transformations in different domains. E.g., `xa` is linear in both `x` and `a`, and `vx` is linear in both `v` and `x`, but `xax` is _not_ linear in `x`, even if you try to &quot;prove&quot; that idea with `v = xa`. Linearity in `v` isn&#x27;t enough to make the composition linear in `x`. A lot of their results seem to rely on eliminating those &quot;redundant&quot; computations, even though the things they&#x27;re replacing with linear computations are actually higher order polynomials. On an initial skim, the other &quot;novel&quot; ideas also don&#x27;t seem well grounded.<p>Their experimental results are decent. That could mean a lot of things (normally that the authors made more errors in their competitors&#x27; than in their own work), but it&#x27;s probably worth looking into for a few hours despite my other complaints.</div><br/><div id="40519251" class="c"><input type="checkbox" id="c-40519251" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#40516988">root</a><span>|</span><a href="#40517811">parent</a><span>|</span><a href="#40518949">next</a><span>|</span><label class="collapse" for="c-40519251">[-]</label><label class="expand" for="c-40519251">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s too late to edit. My initial skepticism was unfounded.<p>Separately, the paper largely comprises of &quot;super attention&quot; and everything else.<p>The &quot;everything else&quot; part of the paper might matter, but it&#x27;s basically just operator fusion and the impacts of doing so on training dynamics, except they left out the impact on performance for different model parameters and didn&#x27;t study how training dynamics impact the result. It&#x27;s not a new idea, even on the ML arxiv, I&#x27;m glad they got good results, and it needs more study before being sold so strongly.<p>The &quot;super attention&quot; part of the paper is interesting. It basically ups the matrix polynomial rank of an attention layer by 1 and claims good results from the process. That&#x27;s believable, especially given that the main contribution of attention is good empirical results from upping the previous layer matrix polynomial rank by a bit. You&#x27;d want to dive into the code and check that they didn&#x27;t screw up the masking before taking the results at face value though (information leakage can make even very weak models seem to perform well).</div><br/></div></div></div></div></div></div><div id="40518949" class="c"><input type="checkbox" id="c-40518949" checked=""/><div class="controls bullet"><span class="by">verisimi</span><span>|</span><a href="#40516988">prev</a><span>|</span><a href="#40521335">next</a><span>|</span><label class="collapse" for="c-40518949">[-]</label><label class="expand" for="c-40518949">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However, the behemothic sizes of these models have introduced numerous challenges, such as expensive and slow training and inference, leading to secondary problems such as high carbon emission, contributing to global warming<p>Yes, there really has been an awful lot of hot air about ai.</div><br/></div></div></div></div></div></div></div></body></html>