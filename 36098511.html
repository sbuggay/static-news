<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685264459835" as="style"/><link rel="stylesheet" href="styles.css?v=1685264459835"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://knowsuchagency.notion.site/Hacking-around-ChatGPT-s-Character-Limits-with-the-Code-Interpreter-d31dcba26e5a432a839bf94c69c7c39f">Hacking Around ChatGPT’s Character Limits with the Code Interpreter</a> <span class="domain">(<a href="https://knowsuchagency.notion.site">knowsuchagency.notion.site</a>)</span></div><div class="subtext"><span>knowsuchagency</span> | <span>9 comments</span></div><br/><div><div id="36100954" class="c"><input type="checkbox" id="c-36100954" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#36101229">next</a><span>|</span><label class="collapse" for="c-36100954">[-]</label><label class="expand" for="c-36100954">[5 more]</label></div><br/><div class="children"><div class="content">The character limit in GPT is a fundamental limit of the software architecture, not an artificial limitation used as an upsell or rate limit. Uploading a file give the plugin access to the file and GPT access to the plugin&#x27;s output, so you might be able to get more informative answers using it, but it&#x27;s not really fair to say that it &quot;bypasses&quot; the character limit in any meaningful way.<p>Consider a similar &quot;bypass&quot;: you can upload the file to the web and the web browser plugin can read the page. You can do this with the whole git repo if it&#x27;s public!</div><br/><div id="36101223" class="c"><input type="checkbox" id="c-36101223" checked=""/><div class="controls bullet"><span class="by">alden5</span><span>|</span><a href="#36100954">parent</a><span>|</span><a href="#36101229">next</a><span>|</span><label class="collapse" for="c-36101223">[-]</label><label class="expand" for="c-36101223">[4 more]</label></div><br/><div class="children"><div class="content">The token limit is 100% an artificial limitation. When ChatGPT first released last november I took the opportunity to try pasting 3k-line codebases into it to get it to walk me through them and it worked perfectly fine, putting that same code in the OpenAI tokenizer tells me it&#x27;s ~33k tokens, way above the limits today. The reason they do this is because every token takes up ~1mb of video memory and that adds up real quick. If you had infinite video memory there would be no &quot;fundamental limit&quot; to how long a LLM can output.<p>OpenAI then has two limits on inputs. The first artificial one ensures that people don&#x27;t get overzealous inputing too much, otherwise they&#x27;ll hit the second hard limit of how much vram their cards have. To the LLM itself there is no difference between characters from the chatbot and human, the only hard limiter is the total number of tokens. I tried this out by inputing a 4k-token string into ChatGPT as many times as I could and it failed on the 20th input, meaning that the hard limit is &gt;80k tokens. Converting this to vram gives us &gt;80gb which is the exact amount of ram the Nvidia a100 card has.</div><br/><div id="36101353" class="c"><input type="checkbox" id="c-36101353" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#36100954">root</a><span>|</span><a href="#36101223">parent</a><span>|</span><a href="#36101243">next</a><span>|</span><label class="collapse" for="c-36101353">[-]</label><label class="expand" for="c-36101353">[2 more]</label></div><br/><div class="children"><div class="content">&gt; When ChatGPT first released last november I took the opportunity to try pasting 3k-line codebases into it to get it to walk me through them and it worked perfectly fine.<p>A common technique to work around the limitations in context length is to simply pull the most recent context that fits into the length. It can be difficult to notice that this happens because oftentimes the full context isn&#x27;t actually necessary. However, specific details from the context are actually lost. For example, if you ask the model to list the filenames back in the same order, and the context was truncated, it would start from the first non-truncated file and the others would be dropped.<p>&gt; If you had infinite video memory there would be no &quot;fundamental limit&quot; to how long a LLM can output.<p>Well, you&#x27;ve certainly got me there. One of the big limits with the transformer architecture today is that the memory usage grows quadratically with context length due to the attention mechanism. This is why there&#x27;s so much interest in alternatives like RWKV &lt;<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36038868" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36038868</a>&gt;, and why scaling them is hard &lt;<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35948742" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35948742</a>&gt;.</div><br/><div id="36101511" class="c"><input type="checkbox" id="c-36101511" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#36100954">root</a><span>|</span><a href="#36101353">parent</a><span>|</span><a href="#36101243">next</a><span>|</span><label class="collapse" for="c-36101511">[-]</label><label class="expand" for="c-36101511">[1 more]</label></div><br/><div class="children"><div class="content">FlashAttention has memory linear in sequence length.
<a href="https:&#x2F;&#x2F;github.com&#x2F;HazyResearch&#x2F;flash-attention">https:&#x2F;&#x2F;github.com&#x2F;HazyResearch&#x2F;flash-attention</a></div><br/></div></div></div></div><div id="36101243" class="c"><input type="checkbox" id="c-36101243" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#36100954">root</a><span>|</span><a href="#36101223">parent</a><span>|</span><a href="#36101353">prev</a><span>|</span><a href="#36101229">next</a><span>|</span><label class="collapse" for="c-36101243">[-]</label><label class="expand" for="c-36101243">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  When ChatGPT first released last november I took the opportunity to try pasting 3k-line codebases into it to get it to walk me through them and it worked perfectly fine
</code></pre>
Are these private codebases, or open source ones? If public, would you mind sharing a link to the ChatGPT session(s)?</div><br/></div></div></div></div></div></div><div id="36101397" class="c"><input type="checkbox" id="c-36101397" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#36101229">prev</a><span>|</span><a href="#36100328">next</a><span>|</span><label class="collapse" for="c-36101397">[-]</label><label class="expand" for="c-36101397">[1 more]</label></div><br/><div class="children"><div class="content">I had thought:<p>1. the GPT-4 code interpreter was just regular GPT-4 with the ability to swap out its context by running python code on one or more files, and<p>2. whenever it uses python to process files, it displays the &#x27;Finished working - Show work&#x27; button.<p>But, based on the session I just had, <i>it seems I was wrong</i>: <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;b97a206a-15cd-456a-9691-41946b6193a4" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;b97a206a-15cd-456a-9691-41946b...</a><p>Here, ChatGPT does all of these things:<p>- gives incomplete information about the files referenced in the JSON file<p>- runs some python code to get the correct list<p>- answers detailed questions about the code *without displaying the &#x27;Finished working&#x27; button, suggesting it answered the series of questions solely from information in its context</div><br/></div></div><div id="36100328" class="c"><input type="checkbox" id="c-36100328" checked=""/><div class="controls bullet"><span class="by">gumballindie</span><span>|</span><a href="#36101397">prev</a><span>|</span><label class="collapse" for="c-36100328">[-]</label><label class="expand" for="c-36100328">[1 more]</label></div><br/><div class="children"><div class="content">This new breed of (ethical) hackers are what gives me hope.<p>Back in the day we had lod, mod, thc, mitnick we had the types of folks that didnt just give in. You know, ka-tet one from many.<p>Thanks to those people we have, at least a pretence of, checks and balances.<p>The battle of the intellects is on.<p>Those who know, know.</div><br/></div></div></div></div></div></div></div></body></html>