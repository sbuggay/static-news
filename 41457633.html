<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725613277131" as="style"/><link rel="stylesheet" href="styles.css?v=1725613277131"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/Mintplex-Labs/anything-llm">Show HN: AnythingLLM – Open-Source, All-in-One Desktop AI Assistant</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>tcarambat1010</span> | <span>65 comments</span></div><br/><div><div id="41459226" class="c"><input type="checkbox" id="c-41459226" checked=""/><div class="controls bullet"><span class="by">DebtDeflation</span><span>|</span><a href="#41458463">next</a><span>|</span><label class="collapse" for="c-41459226">[-]</label><label class="expand" for="c-41459226">[1 more]</label></div><br/><div class="children"><div class="content">This is really nice.  My first reaction was &quot;oh great, another Ollama&#x2F;WebGenUI wrapper on Llama.cpp&quot; but it&#x27;s actually much more - supporting not only the LLM but also embedding models, vector databases, and TTS&#x2F;STT.  Everything needed to build a fully functioning voice chatbot.</div><br/></div></div><div id="41458463" class="c"><input type="checkbox" id="c-41458463" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#41459226">prev</a><span>|</span><a href="#41458374">next</a><span>|</span><label class="collapse" for="c-41458463">[-]</label><label class="expand" for="c-41458463">[11 more]</label></div><br/><div class="children"><div class="content">This looks sweet.<p>Totally irrelevant but ... &quot;Language Learning Model&quot; ? Probably just some brainfart or I&#x27;m missing something but it would be hilarious if the authors of this did this whole project without knowing what LLM stands for.</div><br/><div id="41463803" class="c"><input type="checkbox" id="c-41463803" checked=""/><div class="controls bullet"><span class="by">placebo</span><span>|</span><a href="#41458463">parent</a><span>|</span><a href="#41459267">next</a><span>|</span><label class="collapse" for="c-41463803">[-]</label><label class="expand" for="c-41463803">[1 more]</label></div><br/><div class="children"><div class="content">Thinking about this a bit, Language Learning Model might be a better TLA than Large Language Model since &quot;Large&quot; is a subjective, period-dependant adjective whereas language learning is precisely what these models achieve (with all the derivatives that come as features when you map the world with language)</div><br/></div></div><div id="41459267" class="c"><input type="checkbox" id="c-41459267" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41458463">parent</a><span>|</span><a href="#41463803">prev</a><span>|</span><a href="#41458494">next</a><span>|</span><label class="collapse" for="c-41459267">[-]</label><label class="expand" for="c-41459267">[1 more]</label></div><br/><div class="children"><div class="content">It was indeed a brainfart on my end that has then been in there since the dawn of time. Pretty funny honestly</div><br/></div></div><div id="41458494" class="c"><input type="checkbox" id="c-41458494" checked=""/><div class="controls bullet"><span class="by">mattygee</span><span>|</span><a href="#41458463">parent</a><span>|</span><a href="#41459267">prev</a><span>|</span><a href="#41458374">next</a><span>|</span><label class="collapse" for="c-41458494">[-]</label><label class="expand" for="c-41458494">[8 more]</label></div><br/><div class="children"><div class="content">Large Language Model
All good</div><br/><div id="41458662" class="c"><input type="checkbox" id="c-41458662" checked=""/><div class="controls bullet"><span class="by">rafram</span><span>|</span><a href="#41458463">root</a><span>|</span><a href="#41458494">parent</a><span>|</span><a href="#41458374">next</a><span>|</span><label class="collapse" for="c-41458662">[-]</label><label class="expand" for="c-41458662">[7 more]</label></div><br/><div class="children"><div class="content">The linked GitHub page mentions “Language Learning Models”, so GP is saying that it seems like the authors of this project wrote the whole thing while not knowing what LLM actually stands for.</div><br/><div id="41461864" class="c"><input type="checkbox" id="c-41461864" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#41458463">root</a><span>|</span><a href="#41458662">parent</a><span>|</span><a href="#41459208">next</a><span>|</span><label class="collapse" for="c-41461864">[-]</label><label class="expand" for="c-41461864">[1 more]</label></div><br/><div class="children"><div class="content">I literally said it was probably a brainfart.</div><br/></div></div><div id="41459208" class="c"><input type="checkbox" id="c-41459208" checked=""/><div class="controls bullet"><span class="by">cholantesh</span><span>|</span><a href="#41458463">root</a><span>|</span><a href="#41458662">parent</a><span>|</span><a href="#41461864">prev</a><span>|</span><a href="#41458374">next</a><span>|</span><label class="collapse" for="c-41459208">[-]</label><label class="expand" for="c-41459208">[5 more]</label></div><br/><div class="children"><div class="content">Or it was a simple typo.</div><br/><div id="41459257" class="c"><input type="checkbox" id="c-41459257" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41458463">root</a><span>|</span><a href="#41459208">parent</a><span>|</span><a href="#41458374">next</a><span>|</span><label class="collapse" for="c-41459257">[-]</label><label class="expand" for="c-41459257">[4 more]</label></div><br/><div class="children"><div class="content">It was a typo haha, someone just PR&#x27;d the fix for it. God knows how long it has been there - whoops</div><br/><div id="41461331" class="c"><input type="checkbox" id="c-41461331" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#41458463">root</a><span>|</span><a href="#41459257">parent</a><span>|</span><a href="#41459853">next</a><span>|</span><label class="collapse" for="c-41461331">[-]</label><label class="expand" for="c-41461331">[1 more]</label></div><br/><div class="children"><div class="content">These are the (fun) stories legends come from. Well done overall! Looking forward to evaluating.</div><br/></div></div><div id="41459853" class="c"><input type="checkbox" id="c-41459853" checked=""/><div class="controls bullet"><span class="by">qup</span><span>|</span><a href="#41458463">root</a><span>|</span><a href="#41459257">parent</a><span>|</span><a href="#41461331">prev</a><span>|</span><a href="#41458374">next</a><span>|</span><label class="collapse" for="c-41459853">[-]</label><label class="expand" for="c-41459853">[2 more]</label></div><br/><div class="children"><div class="content">Maybe the assistant can look in the repo and compute that, just sayin&#x27;</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41458374" class="c"><input type="checkbox" id="c-41458374" checked=""/><div class="controls bullet"><span class="by">hannofcart</span><span>|</span><a href="#41458463">prev</a><span>|</span><a href="#41459470">next</a><span>|</span><label class="collapse" for="c-41458374">[-]</label><label class="expand" for="c-41458374">[4 more]</label></div><br/><div class="children"><div class="content">Downloaded and checked it out. Looks great so far. Tried using it to read a bunch of regulatory PDFs using GPT-4o.<p>Some quick and early feedback:<p>1. The citations seem to be a bit dicey. The response seems to give largely correct answers but the citations window seems to show content that&#x27;s a bit garbled.<p>2. Please please add a text search to search within existing chat content. Like if I searched for something about giraffes in one of the chats, search chat history and allow switching to it.</div><br/><div id="41459651" class="c"><input type="checkbox" id="c-41459651" checked=""/><div class="controls bullet"><span class="by">fallinditch</span><span>|</span><a href="#41458374">parent</a><span>|</span><a href="#41459470">next</a><span>|</span><label class="collapse" for="c-41459651">[-]</label><label class="expand" for="c-41459651">[3 more]</label></div><br/><div class="children"><div class="content">For your 2nd suggestion, this would be solved if previous chats were (automatically) added to the vector database. Is this possible in this or similar apps?</div><br/><div id="41461739" class="c"><input type="checkbox" id="c-41461739" checked=""/><div class="controls bullet"><span class="by">shanusmagnus</span><span>|</span><a href="#41458374">root</a><span>|</span><a href="#41459651">parent</a><span>|</span><a href="#41459470">next</a><span>|</span><label class="collapse" for="c-41461739">[-]</label><label class="expand" for="c-41461739">[2 more]</label></div><br/><div class="children"><div class="content">Yes.  The fact that literally no one does this makes me continue to wonder wtf people are actually doing w&#x2F; LLMs, since I feel this need so acutely and the solution is so obvious.</div><br/><div id="41462282" class="c"><input type="checkbox" id="c-41462282" checked=""/><div class="controls bullet"><span class="by">fallinditch</span><span>|</span><a href="#41458374">root</a><span>|</span><a href="#41461739">parent</a><span>|</span><a href="#41459470">next</a><span>|</span><label class="collapse" for="c-41462282">[-]</label><label class="expand" for="c-41462282">[1 more]</label></div><br/><div class="children"><div class="content">I assume that there are people who are doing this. For example, you could set this up via the OpenAI API with an embedding model. The API processes the text using a pre-trained model like 
    text-embedding-ada-002
which converts the text into a vector to store in the database.<p>I assume that this sort of functionality will become more commonplace.</div><br/></div></div></div></div></div></div></div></div><div id="41459470" class="c"><input type="checkbox" id="c-41459470" checked=""/><div class="controls bullet"><span class="by">tencentshill</span><span>|</span><a href="#41458374">prev</a><span>|</span><a href="#41458386">next</a><span>|</span><label class="collapse" for="c-41459470">[-]</label><label class="expand" for="c-41459470">[2 more]</label></div><br/><div class="children"><div class="content">As someone who doesn&#x27;t know what an Embed or Vector is, this has been the only offline AI tool I&#x27;ve been able to install and start using on my standard office PC.</div><br/><div id="41459971" class="c"><input type="checkbox" id="c-41459971" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41459470">parent</a><span>|</span><a href="#41458386">next</a><span>|</span><label class="collapse" for="c-41459971">[-]</label><label class="expand" for="c-41459971">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the point! Shouldn&#x27;t have to know and naturally over time you&#x27;ll learn more and the controls are there when you feel comfortable tuning them</div><br/></div></div></div></div><div id="41458386" class="c"><input type="checkbox" id="c-41458386" checked=""/><div class="controls bullet"><span class="by">101008</span><span>|</span><a href="#41459470">prev</a><span>|</span><a href="#41461974">next</a><span>|</span><label class="collapse" for="c-41458386">[-]</label><label class="expand" for="c-41458386">[11 more]</label></div><br/><div class="children"><div class="content">LLM will become like web frameworks in the future, in the sense that they will be free, open source, and everybody will be able to build on them. Sure, there will be paid options, such as there are paid web frameworks, but most of the time free options will be more than good enough for most of the jobs.</div><br/><div id="41458758" class="c"><input type="checkbox" id="c-41458758" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#41458386">parent</a><span>|</span><a href="#41461969">next</a><span>|</span><label class="collapse" for="c-41458758">[-]</label><label class="expand" for="c-41458758">[7 more]</label></div><br/><div class="children"><div class="content">What is your argumentation for that?  It seems that building LLMs is very costly, and that will probably remain so for quite some time.<p>I think it is equally likely that LLMs will perform worse in the future, because of copyright reasons and increased privacy awareness, leading to less diverse data sets to train on.</div><br/><div id="41458966" class="c"><input type="checkbox" id="c-41458966" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#41458386">root</a><span>|</span><a href="#41458758">parent</a><span>|</span><a href="#41462521">next</a><span>|</span><label class="collapse" for="c-41458966">[-]</label><label class="expand" for="c-41458966">[1 more]</label></div><br/><div class="children"><div class="content">Existing free to use (offline) models are already really really good and open a ton of possibilities for new apps.<p>The only barrier right now is that laptop&#x2F;phone hardware can&#x27;t run them (inference and&#x2F;or training) fast enough. That&#x27;s a problem that won&#x27;t really exist 3 to 10 years from now.<p>That&#x27;s ignoring the fact that creation of new much much better models is going full steam ahead.</div><br/></div></div><div id="41462521" class="c"><input type="checkbox" id="c-41462521" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#41458386">root</a><span>|</span><a href="#41458758">parent</a><span>|</span><a href="#41458966">prev</a><span>|</span><a href="#41462268">next</a><span>|</span><label class="collapse" for="c-41462521">[-]</label><label class="expand" for="c-41462521">[1 more]</label></div><br/><div class="children"><div class="content">&gt; building LLMs is very costly, and that will probably remain so for quite some time.<p>Building LLMs is dropping in cost quickly. Back in mid 2023 training an 7B model had already dropped to $30K and it&#x27;s even cheaper now.<p>&gt; I think it is equally likely that LLMs will perform worse in the future, because of copyright reasons and increased privacy awareness, leading to less diverse data sets to train on.<p>I&#x27;ll bet a lot of money this won&#x27;t happen.<p>Firstly copyright isn&#x27;t settled on this. Secondly people understand a lot more now about how to use less, higher quality data and how to use synthetic data (eg MS Phi series, Persona dataset etc and of course the upcoming OpenAI Strawberry and Orion models which use synthetic data heavily). Thirdly the knowledge about how to use multi-modal data in your LLM is much more widely spread, which means that video and code can both be used to improve LLM performance.<p>[1] <a href="https:&#x2F;&#x2F;arize.com&#x2F;resource&#x2F;mosaicml&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arize.com&#x2F;resource&#x2F;mosaicml&#x2F;</a></div><br/></div></div><div id="41462268" class="c"><input type="checkbox" id="c-41462268" checked=""/><div class="controls bullet"><span class="by">jrm4</span><span>|</span><a href="#41458386">root</a><span>|</span><a href="#41458758">parent</a><span>|</span><a href="#41462521">prev</a><span>|</span><a href="#41460658">next</a><span>|</span><label class="collapse" for="c-41462268">[-]</label><label class="expand" for="c-41462268">[2 more]</label></div><br/><div class="children"><div class="content">I am <i>genuinely baffled</i> at how someone could come up with this take?  I suppose it&#x27;s because I went ahead and copped a $500 card about a month ago, and I can already literally do <i>more</i> than the big paid models presently can. I lack some of the bells and whistles, but I think more importantly, <i>they</i> lack uncensored models.<p>&quot;AI&quot; as we&#x27;re calling it is <i>aggressively</i> defacto open-source, and I can see in no way how that competition doesn&#x27;t drive down prices.</div><br/><div id="41464253" class="c"><input type="checkbox" id="c-41464253" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#41458386">root</a><span>|</span><a href="#41462268">parent</a><span>|</span><a href="#41460658">next</a><span>|</span><label class="collapse" for="c-41464253">[-]</label><label class="expand" for="c-41464253">[1 more]</label></div><br/><div class="children"><div class="content">Training and inference are two different things.  The incentive to make some model open is unrelated to hardware cost.<p>Perhaps you are missing something in my arguments that I can clear up?</div><br/></div></div></div></div><div id="41459312" class="c"><input type="checkbox" id="c-41459312" checked=""/><div class="controls bullet"><span class="by">101008</span><span>|</span><a href="#41458386">root</a><span>|</span><a href="#41458758">parent</a><span>|</span><a href="#41460658">prev</a><span>|</span><a href="#41461969">next</a><span>|</span><label class="collapse" for="c-41459312">[-]</label><label class="expand" for="c-41459312">[1 more]</label></div><br/><div class="children"><div class="content">Building software was very costly and hard 30 years ago. Things get cheaper and simpler with time. LLM will become a tool to use in your work, such as frameworks or libraries. Some people will, other people won&#x27;t (mostly those to maintain projects without them).<p>I hope to be right or I will be jobless lol</div><br/></div></div></div></div><div id="41461969" class="c"><input type="checkbox" id="c-41461969" checked=""/><div class="controls bullet"><span class="by">darepublic</span><span>|</span><a href="#41458386">parent</a><span>|</span><a href="#41458758">prev</a><span>|</span><a href="#41458722">next</a><span>|</span><label class="collapse" for="c-41461969">[-]</label><label class="expand" for="c-41461969">[1 more]</label></div><br/><div class="children"><div class="content">There are no paid web frameworks of note that I&#x27;m aware of</div><br/></div></div><div id="41458722" class="c"><input type="checkbox" id="c-41458722" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41458386">parent</a><span>|</span><a href="#41461969">prev</a><span>|</span><a href="#41458407">next</a><span>|</span><label class="collapse" for="c-41458722">[-]</label><label class="expand" for="c-41458722">[1 more]</label></div><br/><div class="children"><div class="content">So… just open source?</div><br/></div></div><div id="41458407" class="c"><input type="checkbox" id="c-41458407" checked=""/><div class="controls bullet"><span class="by">samdung</span><span>|</span><a href="#41458386">parent</a><span>|</span><a href="#41458722">prev</a><span>|</span><a href="#41461974">next</a><span>|</span><label class="collapse" for="c-41458407">[-]</label><label class="expand" for="c-41458407">[1 more]</label></div><br/><div class="children"><div class="content">I really hope this becomes the norm.</div><br/></div></div></div></div><div id="41461974" class="c"><input type="checkbox" id="c-41461974" checked=""/><div class="controls bullet"><span class="by">indigodaddy</span><span>|</span><a href="#41458386">prev</a><span>|</span><a href="#41458212">next</a><span>|</span><label class="collapse" for="c-41461974">[-]</label><label class="expand" for="c-41461974">[1 more]</label></div><br/><div class="children"><div class="content">Question on the anythingllm hosted.  So is the $50&#x2F;mo just basically you are logging into some sort of Remote Desktop environment and run an anythingllm instance from there that you guys manage?  Everything else is still the same like it’s still byok and all that right?  Or do you get some sort of “ai&#x2F;token” usage with the monthly fee as well?<p>Oh and the other question would be in your hosted version does the instance have access to a gpu that you provide?  Which in that case we could just use a local model with (relative) ease right?<p>And if this hosted service is as I have described, I feel like this sort of service coupled with top tier frontier open source models is the (not too distant) future for AI!—- at least in the short to medium term (which is probably a pretty short period relatively, given the rapid speed of AI development).<p>Thanks</div><br/></div></div><div id="41458212" class="c"><input type="checkbox" id="c-41458212" checked=""/><div class="controls bullet"><span class="by">nunobrito</span><span>|</span><a href="#41461974">prev</a><span>|</span><a href="#41457970">next</a><span>|</span><label class="collapse" for="c-41458212">[-]</label><label class="expand" for="c-41458212">[4 more]</label></div><br/><div class="children"><div class="content">There was an error while installing on Linux, was solved with:<p>&#x27;&#x27;&#x27;
sudo chown root:root &#x2F;home&#x2F;hn&#x2F;AnythingLLMDesktop&#x2F;anythingllm-desktop&#x2F;chrome-sandbox
sudo chmod 4755 &#x2F;home&#x2F;hn&#x2F;AnythingLLMDesktop&#x2F;anythingllm-desktop&#x2F;chrome-sandbox
&#x27;&#x27;&#x27;<p>Other than that it worked really well.</div><br/><div id="41461616" class="c"><input type="checkbox" id="c-41461616" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41458212">parent</a><span>|</span><a href="#41458464">next</a><span>|</span><label class="collapse" for="c-41461616">[-]</label><label class="expand" for="c-41461616">[1 more]</label></div><br/><div class="children"><div class="content">Please don&#x27;t follow this. There has to be a better solution. This will make the chrome-sandbox run as root with access to everything and you really shouldn&#x27;t do that.<p>Figure out how to assign proper permissions instead. (Or at least don&#x27;t invite other people to do the same thing...)</div><br/></div></div><div id="41458464" class="c"><input type="checkbox" id="c-41458464" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#41458212">parent</a><span>|</span><a href="#41461616">prev</a><span>|</span><a href="#41457970">next</a><span>|</span><label class="collapse" for="c-41458464">[-]</label><label class="expand" for="c-41458464">[2 more]</label></div><br/><div class="children"><div class="content">HN doesn&#x27;t use normal markdown; to do code blocks you indent with 4 spaces, ex.<p><pre><code>    sudo chown root:root &#x2F;home&#x2F;hn&#x2F;AnythingLLMDesktop&#x2F;anythingllm-desktop&#x2F;chrome-sandbox
    sudo chmod 4755 &#x2F;home&#x2F;hn&#x2F;AnythingLLMDesktop&#x2F;anythingllm-desktop&#x2F;chrome-sandbox</code></pre></div><br/><div id="41458766" class="c"><input type="checkbox" id="c-41458766" checked=""/><div class="controls bullet"><span class="by">nunobrito</span><span>|</span><a href="#41458212">root</a><span>|</span><a href="#41458464">parent</a><span>|</span><a href="#41457970">next</a><span>|</span><label class="collapse" for="c-41458766">[-]</label><label class="expand" for="c-41458766">[1 more]</label></div><br/><div class="children"><div class="content">Thanks!</div><br/></div></div></div></div></div></div><div id="41457970" class="c"><input type="checkbox" id="c-41457970" checked=""/><div class="controls bullet"><span class="by">phren0logy</span><span>|</span><a href="#41458212">prev</a><span>|</span><a href="#41458843">next</a><span>|</span><label class="collapse" for="c-41457970">[-]</label><label class="expand" for="c-41457970">[4 more]</label></div><br/><div class="children"><div class="content">I have been really impressed with AnythingLLM as a no-fuss way to use LLMs locally and via APIs. For those of us who want to tinker, there&#x27;s a solid range of choice for embedders and vector stores.<p>The single install desktop packaging is very slick. I look forward to the upcoming new features.</div><br/><div id="41458151" class="c"><input type="checkbox" id="c-41458151" checked=""/><div class="controls bullet"><span class="by">candiddevmike</span><span>|</span><a href="#41457970">parent</a><span>|</span><a href="#41458843">next</a><span>|</span><label class="collapse" for="c-41458151">[-]</label><label class="expand" for="c-41458151">[3 more]</label></div><br/><div class="children"><div class="content">Have you tried openwebui?  What are your thoughts on it vs this?</div><br/><div id="41459810" class="c"><input type="checkbox" id="c-41459810" checked=""/><div class="controls bullet"><span class="by">phren0logy</span><span>|</span><a href="#41457970">root</a><span>|</span><a href="#41458151">parent</a><span>|</span><a href="#41458843">next</a><span>|</span><label class="collapse" for="c-41459810">[-]</label><label class="expand" for="c-41459810">[2 more]</label></div><br/><div class="children"><div class="content">I had a much smoother experience with the desktop version of AnythingLLM. There&#x27;s more in openwebui, but the things that are in AnythingLLM are more polished (IMO).</div><br/><div id="41459983" class="c"><input type="checkbox" id="c-41459983" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41457970">root</a><span>|</span><a href="#41459810">parent</a><span>|</span><a href="#41458843">next</a><span>|</span><label class="collapse" for="c-41459983">[-]</label><label class="expand" for="c-41459983">[1 more]</label></div><br/><div class="children"><div class="content">We will have plugins for:
- Auth
- Agent skills
- Data connectors<p>soon, agent skills are first and this should help plug the gaps between offerings since we can&#x27;t build everything for everyone</div><br/></div></div></div></div></div></div></div></div><div id="41458843" class="c"><input type="checkbox" id="c-41458843" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#41457970">prev</a><span>|</span><a href="#41461466">next</a><span>|</span><label class="collapse" for="c-41458843">[-]</label><label class="expand" for="c-41458843">[2 more]</label></div><br/><div class="children"><div class="content">I noticed you put LiteLLM in your list of providers.  Was that just marketing, or did you re-implement model support for all the models LiteLLM already supports separately?</div><br/><div id="41459284" class="c"><input type="checkbox" id="c-41459284" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41458843">parent</a><span>|</span><a href="#41461466">next</a><span>|</span><label class="collapse" for="c-41459284">[-]</label><label class="expand" for="c-41459284">[1 more]</label></div><br/><div class="children"><div class="content">You can use LiteLLM _as the LLM provider_ which is just a relay to a ton of other models. For people who already have it set up connecting to x,y,z providers can keep that work and use it for inferencing as well</div><br/></div></div></div></div><div id="41461466" class="c"><input type="checkbox" id="c-41461466" checked=""/><div class="controls bullet"><span class="by">conception</span><span>|</span><a href="#41458843">prev</a><span>|</span><a href="#41463469">next</a><span>|</span><label class="collapse" for="c-41461466">[-]</label><label class="expand" for="c-41461466">[2 more]</label></div><br/><div class="children"><div class="content">Can this roll into home assistant and provide alexa at home in any capacity?</div><br/><div id="41463450" class="c"><input type="checkbox" id="c-41463450" checked=""/><div class="controls bullet"><span class="by">Kichererbsen</span><span>|</span><a href="#41461466">parent</a><span>|</span><a href="#41463469">next</a><span>|</span><label class="collapse" for="c-41463450">[-]</label><label class="expand" for="c-41463450">[1 more]</label></div><br/><div class="children"><div class="content">i need this too. doesn&#x27;t need to be alexa - i just want my sonos systems at home to be more smart than just adding stuff to shopping lists with hit and miss transcriptions (why is &quot;oat milk&quot; always added as &quot;oatmeal&quot;?</div><br/></div></div></div></div><div id="41463469" class="c"><input type="checkbox" id="c-41463469" checked=""/><div class="controls bullet"><span class="by">somesun</span><span>|</span><a href="#41461466">prev</a><span>|</span><a href="#41458791">next</a><span>|</span><label class="collapse" for="c-41463469">[-]</label><label class="expand" for="c-41463469">[1 more]</label></div><br/><div class="children"><div class="content">where is the desktop app download ?<p>or need to install source code from github ?</div><br/></div></div><div id="41458791" class="c"><input type="checkbox" id="c-41458791" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#41463469">prev</a><span>|</span><a href="#41460062">next</a><span>|</span><label class="collapse" for="c-41458791">[-]</label><label class="expand" for="c-41458791">[1 more]</label></div><br/><div class="children"><div class="content">&gt; AnythingLLM packages as an AppImage but you will not be able to boot if you run just the AppImage.<p>The &#x27;boot&#x27; seems to indicate it will affect the computer&#x27;s startup process, I think you meant to say you will not be able to &#x27;start the application&#x27;</div><br/></div></div><div id="41460062" class="c"><input type="checkbox" id="c-41460062" checked=""/><div class="controls bullet"><span class="by">egamirorrim</span><span>|</span><a href="#41458791">prev</a><span>|</span><a href="#41459153">next</a><span>|</span><label class="collapse" for="c-41460062">[-]</label><label class="expand" for="c-41460062">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve got an open ai API key, and I pay for chatgpt. I&#x27;d imagine switching to this and using openai would end up costing quite a lot? How are people running it relatively cheaply?</div><br/><div id="41460690" class="c"><input type="checkbox" id="c-41460690" checked=""/><div class="controls bullet"><span class="by">monkmartinez</span><span>|</span><a href="#41460062">parent</a><span>|</span><a href="#41460973">next</a><span>|</span><label class="collapse" for="c-41460690">[-]</label><label class="expand" for="c-41460690">[1 more]</label></div><br/><div class="children"><div class="content">Openrouter... you get all the models and its not as expensive as you would think. I spent $3 with aider the other day in like the blink of an eye with Anthropic. I am working on a FASTHTML thingy and loaded all the docs, plus a few huge replicate api files into the vector database. Most of my back and forth usage averaged about $0.02 for each turn with Claude 3.5 Sonnet. To give you an idea: My context + prompt were around 18000 tokens with completions around 1500 tokens.</div><br/></div></div><div id="41460973" class="c"><input type="checkbox" id="c-41460973" checked=""/><div class="controls bullet"><span class="by">aznumeric</span><span>|</span><a href="#41460062">parent</a><span>|</span><a href="#41460690">prev</a><span>|</span><a href="#41459153">next</a><span>|</span><label class="collapse" for="c-41460973">[-]</label><label class="expand" for="c-41460973">[1 more]</label></div><br/><div class="children"><div class="content">One way people keep costs down when using OpenAI with an offline RAG system is by limiting the number of text snippets sent to the API. Instead of sending the whole database, they&#x27;ll typically retrieve only the top 10 (or so) most relevant snippets from the vector database and just send those to OpenAI for processing. This significantly reduces the amount of data being processed and billed by OpenAI.</div><br/></div></div></div></div><div id="41459153" class="c"><input type="checkbox" id="c-41459153" checked=""/><div class="controls bullet"><span class="by">md3911027514</span><span>|</span><a href="#41460062">prev</a><span>|</span><a href="#41458506">next</a><span>|</span><label class="collapse" for="c-41459153">[-]</label><label class="expand" for="c-41459153">[1 more]</label></div><br/><div class="children"><div class="content">this is super sweet for developers (or anyone else) who like to have granular control over their LLM set up<p>- ability to edit system prompt<p>- ability to change LLM temperature<p>- can choose which model to use (open-source or closed)</div><br/></div></div><div id="41458506" class="c"><input type="checkbox" id="c-41458506" checked=""/><div class="controls bullet"><span class="by">santamex</span><span>|</span><a href="#41459153">prev</a><span>|</span><a href="#41457902">next</a><span>|</span><label class="collapse" for="c-41458506">[-]</label><label class="expand" for="c-41458506">[2 more]</label></div><br/><div class="children"><div class="content">How does this differ from chatboxai?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Bin-Huang&#x2F;chatbox">https:&#x2F;&#x2F;github.com&#x2F;Bin-Huang&#x2F;chatbox</a></div><br/><div id="41458749" class="c"><input type="checkbox" id="c-41458749" checked=""/><div class="controls bullet"><span class="by">Ringz</span><span>|</span><a href="#41458506">parent</a><span>|</span><a href="#41457902">next</a><span>|</span><label class="collapse" for="c-41458749">[-]</label><label class="expand" for="c-41458749">[1 more]</label></div><br/><div class="children"><div class="content">Chatboxai is a <i>client</i> for multiple AIs like OpenAi, Cloude, etc. It does not work on your local files and documents.</div><br/></div></div></div></div><div id="41457902" class="c"><input type="checkbox" id="c-41457902" checked=""/><div class="controls bullet"><span class="by">vednig</span><span>|</span><a href="#41458506">prev</a><span>|</span><a href="#41458633">next</a><span>|</span><label class="collapse" for="c-41457902">[-]</label><label class="expand" for="c-41457902">[4 more]</label></div><br/><div class="children"><div class="content">How do you ensure &quot;privacy by default&quot; if you are also providing cloud models?</div><br/><div id="41459304" class="c"><input type="checkbox" id="c-41459304" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41457902">parent</a><span>|</span><a href="#41458833">next</a><span>|</span><label class="collapse" for="c-41459304">[-]</label><label class="expand" for="c-41459304">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not my position to &quot;impose&quot; a preference on a user for an LLM.<p>Privacy by default means &quot;if you use the defaults, it&#x27;s private&quot;.<p>Obviously, if your computer is low-end or you really have been loving GPT-4o or Claude you _can_ use just that external component while still using a local vector db, document storage, etc.<p>So its basically like opt-in for external usage of any particular part of the app, whether that be an LLM, embedder model, vector db, or otherwise.</div><br/></div></div><div id="41458833" class="c"><input type="checkbox" id="c-41458833" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#41457902">parent</a><span>|</span><a href="#41459304">prev</a><span>|</span><a href="#41457968">next</a><span>|</span><label class="collapse" for="c-41458833">[-]</label><label class="expand" for="c-41458833">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t seem like they are &quot;providing&quot; cloud models. They have their backend able to interface with whatever endpoint you want, given you have access. It&#x27;s plainly obvious when interacting with 3rd party providers that it depends on their data &#x2F; privacy policy. When has this ever not been the case?<p>I could just start up a vLLM instance with llama 3.1 and connect their application to it just as easily though. Perfectly secure (as I am able to make it).<p>This seems like such a pedantic thing to complain about.</div><br/></div></div><div id="41457968" class="c"><input type="checkbox" id="c-41457968" checked=""/><div class="controls bullet"><span class="by">hluska</span><span>|</span><a href="#41457902">parent</a><span>|</span><a href="#41458833">prev</a><span>|</span><a href="#41458633">next</a><span>|</span><label class="collapse" for="c-41457968">[-]</label><label class="expand" for="c-41457968">[1 more]</label></div><br/><div class="children"><div class="content">Good question - this was an excellent write up and AnythingLLM looks great. But I’m really curious about that too.<p>Regardless of the answer, OP you’ve done a hell of a lot of work and I hope you’re as proud as you should be. Congratulations on getting all the way to a Show HN.</div><br/></div></div></div></div><div id="41458633" class="c"><input type="checkbox" id="c-41458633" checked=""/><div class="controls bullet"><span class="by">philipjoubert</span><span>|</span><a href="#41457902">prev</a><span>|</span><a href="#41459033">next</a><span>|</span><label class="collapse" for="c-41458633">[-]</label><label class="expand" for="c-41458633">[3 more]</label></div><br/><div class="children"><div class="content">This looks really great. Are you planning on adding shortcut keys anytime soon?</div><br/><div id="41459346" class="c"><input type="checkbox" id="c-41459346" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41458633">parent</a><span>|</span><a href="#41459033">next</a><span>|</span><label class="collapse" for="c-41459346">[-]</label><label class="expand" for="c-41459346">[2 more]</label></div><br/><div class="children"><div class="content">We actually are planning to really lean into the &quot;Desktop assistant&quot; style of things, so yes absolutely. Like how on Mac you can cmd+space to launch spotlight, we can offer the same functionality alongside everything else.<p>WIP!</div><br/><div id="41463871" class="c"><input type="checkbox" id="c-41463871" checked=""/><div class="controls bullet"><span class="by">prometheon1</span><span>|</span><a href="#41458633">root</a><span>|</span><a href="#41459346">parent</a><span>|</span><a href="#41459033">next</a><span>|</span><label class="collapse" for="c-41463871">[-]</label><label class="expand" for="c-41463871">[1 more]</label></div><br/><div class="children"><div class="content">One way of doing this that might save you some effort is writing a plugin for Raycast, but I have no idea whether this covers the use cases you have in mind.<p>Seems doable: <a href="https:&#x2F;&#x2F;github.com&#x2F;raycast&#x2F;extensions&#x2F;blob&#x2F;main&#x2F;extensions&#x2F;openai-gpt&#x2F;src&#x2F;ai.tsx">https:&#x2F;&#x2F;github.com&#x2F;raycast&#x2F;extensions&#x2F;blob&#x2F;main&#x2F;extensions&#x2F;o...</a><p>(Not affiliated, I just have Raycast installed and use it sometimes)</div><br/></div></div></div></div></div></div><div id="41459033" class="c"><input type="checkbox" id="c-41459033" checked=""/><div class="controls bullet"><span class="by">A4ET8a8uTh0</span><span>|</span><a href="#41458633">prev</a><span>|</span><a href="#41458731">next</a><span>|</span><label class="collapse" for="c-41459033">[-]</label><label class="expand" for="c-41459033">[1 more]</label></div><br/><div class="children"><div class="content">This definitely makes it super easy for less technical folks to access it ( got it up and running in less than 5 minutes ). Initial reaction is positive with just Ollama. Everything is automatically detected and if you want to manually set it up, you still can. Lets see how it does after adding huggingface ( quick and painless ).</div><br/></div></div><div id="41458731" class="c"><input type="checkbox" id="c-41458731" checked=""/><div class="controls bullet"><span class="by">ranger_danger</span><span>|</span><a href="#41459033">prev</a><span>|</span><a href="#41457824">next</a><span>|</span><label class="collapse" for="c-41458731">[-]</label><label class="expand" for="c-41458731">[3 more]</label></div><br/><div class="children"><div class="content">$ docker pull mintplexlabs&#x2F;anythingllm
Using default tag: latest
Error response from daemon: Get &quot;<a href="https:&#x2F;&#x2F;registry-1.docker.io&#x2F;v2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;registry-1.docker.io&#x2F;v2&#x2F;</a>&quot;: net&#x2F;http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</div><br/><div id="41459321" class="c"><input type="checkbox" id="c-41459321" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41458731">parent</a><span>|</span><a href="#41457824">next</a><span>|</span><label class="collapse" for="c-41459321">[-]</label><label class="expand" for="c-41459321">[2 more]</label></div><br/><div class="children"><div class="content">This is a connection issue for your docker cli - we have seen this issue before and it had to do with client proxy.<p><a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;48066666" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;48066666</a></div><br/><div id="41461029" class="c"><input type="checkbox" id="c-41461029" checked=""/><div class="controls bullet"><span class="by">ranger_danger</span><span>|</span><a href="#41458731">root</a><span>|</span><a href="#41459321">parent</a><span>|</span><a href="#41457824">next</a><span>|</span><label class="collapse" for="c-41461029">[-]</label><label class="expand" for="c-41461029">[1 more]</label></div><br/><div class="children"><div class="content">I am not behind a proxy, do not have any of those related variables set, and all other attempts at net connections (curl, telnet, browser, etc.) work fine...</div><br/></div></div></div></div></div></div><div id="41457824" class="c"><input type="checkbox" id="c-41457824" checked=""/><div class="controls bullet"><span class="by">ranger_danger</span><span>|</span><a href="#41458731">prev</a><span>|</span><a href="#41457974">next</a><span>|</span><label class="collapse" for="c-41457824">[-]</label><label class="expand" for="c-41457824">[1 more]</label></div><br/><div class="children"><div class="content">Finally.</div><br/></div></div><div id="41457974" class="c"><input type="checkbox" id="c-41457974" checked=""/><div class="controls bullet"><span class="by">anonymous344</span><span>|</span><a href="#41457824">prev</a><span>|</span><label class="collapse" for="c-41457974">[-]</label><label class="expand" for="c-41457974">[2 more]</label></div><br/><div class="children"><div class="content">what kind of pc does it need ? ram, etc?</div><br/><div id="41459336" class="c"><input type="checkbox" id="c-41459336" checked=""/><div class="controls bullet"><span class="by">tcarambat1010</span><span>|</span><a href="#41457974">parent</a><span>|</span><label class="collapse" for="c-41459336">[-]</label><label class="expand" for="c-41459336">[1 more]</label></div><br/><div class="children"><div class="content">Depends on the model you want to run! That is the beauty of it. If you can muster that ability to run a Q4 Llama3.1 8B - great, if your specs are really low end you can always outsource to a cloud provider.<p>Maybe you want to run privately, but can only manage a Gemma-2B - then great, you can use that also.</div><br/></div></div></div></div></div></div></div></div></div></body></html>