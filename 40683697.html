<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718442057583" as="style"/><link rel="stylesheet" href="styles.css?v=1718442057583"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d">AI Search: The Bitter-Er Lesson</a>Â <span class="domain">(<a href="https://yellow-apartment-148.notion.site">yellow-apartment-148.notion.site</a>)</span></div><div class="subtext"><span>dwighttk</span> | <span>141 comments</span></div><br/><div><div id="40684871" class="c"><input type="checkbox" id="c-40684871" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#40686866">next</a><span>|</span><label class="collapse" for="c-40684871">[-]</label><label class="expand" for="c-40684871">[41 more]</label></div><br/><div class="children"><div class="content">The effectiveness of search goes hand-in-hand with quality of the value function. But today, value functions are incredibly domain-specific, and there is weak or no current evidence (as far as I know) that we can make value functions that generalize well to new domains. This article effectively makes a conceptual leap from &quot;chess has good value functions&quot; to &quot;we can make good value functions that enable search for AI research&quot;. I mean yes, that&#x27;d be wonderful - a holy grail - but can we really?<p>In the meantime, 1000x or 10000x inference time cost for running an LLM gets you into pretty ridiculous cost territory.</div><br/><div id="40686191" class="c"><input type="checkbox" id="c-40686191" checked=""/><div class="controls bullet"><span class="by">fizx</span><span>|</span><a href="#40684871">parent</a><span>|</span><a href="#40685087">next</a><span>|</span><label class="collapse" for="c-40686191">[-]</label><label class="expand" for="c-40686191">[3 more]</label></div><br/><div class="children"><div class="content">I think we have ok generalized value functions (aka LLM benchmarks), but we don&#x27;t have cheap approximations to them, which is what we&#x27;d need to be able to do tree search at inference time.  Chess works because material advantage is a pretty good approximation to winning and is trivially calculable.</div><br/><div id="40687943" class="c"><input type="checkbox" id="c-40687943" checked=""/><div class="controls bullet"><span class="by">computerphage</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686191">parent</a><span>|</span><a href="#40685087">next</a><span>|</span><label class="collapse" for="c-40687943">[-]</label><label class="expand" for="c-40687943">[2 more]</label></div><br/><div class="children"><div class="content">Stockfish doesn&#x27;t use material advantage as an approximation to winning though. It uses a complex deep learning value function that it evaluates many times.</div><br/><div id="40687973" class="c"><input type="checkbox" id="c-40687973" checked=""/><div class="controls bullet"><span class="by">alexvitkov</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40687943">parent</a><span>|</span><a href="#40685087">next</a><span>|</span><label class="collapse" for="c-40687973">[-]</label><label class="expand" for="c-40687973">[1 more]</label></div><br/><div class="children"><div class="content">Still, the fact that there are obvious heuristics makes that function easier to train and and makes it presumably not need an absurd number of weights.</div><br/></div></div></div></div></div></div><div id="40685087" class="c"><input type="checkbox" id="c-40685087" checked=""/><div class="controls bullet"><span class="by">dsjoerg</span><span>|</span><a href="#40684871">parent</a><span>|</span><a href="#40686191">prev</a><span>|</span><a href="#40685420">next</a><span>|</span><label class="collapse" for="c-40685087">[-]</label><label class="expand" for="c-40685087">[4 more]</label></div><br/><div class="children"><div class="content">Self-evaluation might be good enough in some domains?  Then the AI is doing repeated self-evaluation, trying things out to find a response that scores higher according to its self metric.</div><br/><div id="40685413" class="c"><input type="checkbox" id="c-40685413" checked=""/><div class="controls bullet"><span class="by">dullcrisp</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685087">parent</a><span>|</span><a href="#40685420">next</a><span>|</span><label class="collapse" for="c-40685413">[-]</label><label class="expand" for="c-40685413">[3 more]</label></div><br/><div class="children"><div class="content">Sorry but I have to ask: what makes you think this would be a good idea?</div><br/><div id="40685918" class="c"><input type="checkbox" id="c-40685918" checked=""/><div class="controls bullet"><span class="by">skirmish</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685413">parent</a><span>|</span><a href="#40685420">next</a><span>|</span><label class="collapse" for="c-40685918">[-]</label><label class="expand" for="c-40685918">[2 more]</label></div><br/><div class="children"><div class="content">This will just lead to the evaluatee finding anomalies in evaluator and exploiting them for maximum gains.  It happened many times already where a ML model controled an object in a physical world simulator, and all it learned was to exploit simulator bugs [1]<p>[1] <a href="https:&#x2F;&#x2F;boingboing.net&#x2F;2018&#x2F;11&#x2F;12&#x2F;local-optima-r-us.html" rel="nofollow">https:&#x2F;&#x2F;boingboing.net&#x2F;2018&#x2F;11&#x2F;12&#x2F;local-optima-r-us.html</a></div><br/><div id="40686114" class="c"><input type="checkbox" id="c-40686114" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685918">parent</a><span>|</span><a href="#40685420">next</a><span>|</span><label class="collapse" for="c-40686114">[-]</label><label class="expand" for="c-40686114">[1 more]</label></div><br/><div class="children"><div class="content">Thats a natural tendency for optimization algorithms</div><br/></div></div></div></div></div></div></div></div><div id="40685420" class="c"><input type="checkbox" id="c-40685420" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40684871">parent</a><span>|</span><a href="#40685087">prev</a><span>|</span><a href="#40685894">next</a><span>|</span><label class="collapse" for="c-40685420">[-]</label><label class="expand" for="c-40685420">[20 more]</label></div><br/><div class="children"><div class="content">Yeah, Stockfish is probably evaluating many millions of positions when looking 40-ply ahead, even with the limited number of legal chess moves in a given position, and with an easy criteria for heavy early pruning (once a branch becomes losing, not much point continuing it). I can&#x27;t imagine the cost of evaluating millions of LLM continuations, just to select the optimal one!<p>Where tree search might make more sense applied to LLMs is for more coarser grained reasoning where the branching isn&#x27;t based on alternate word continuations but on alternate what-if lines of thought, but even then it seems costs could easily become prohibitive, both for generation and evaluation&#x2F;pruning, and using such a biased approach seems as much to fly in the face of the bitter lesson as be suggested by it.</div><br/><div id="40686225" class="c"><input type="checkbox" id="c-40686225" checked=""/><div class="controls bullet"><span class="by">stevage</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685420">parent</a><span>|</span><a href="#40685584">next</a><span>|</span><label class="collapse" for="c-40686225">[-]</label><label class="expand" for="c-40686225">[1 more]</label></div><br/><div class="children"><div class="content">Pruning isn&#x27;t quite as easy as you make it sound. There are lots of famous examples where chess engines misevaluate a position because they prune out apparently losing moves that are actually winning.<p>Eg <a href="https:&#x2F;&#x2F;youtu.be&#x2F;TtJeE0Th7rk?si=KVAZufm8QnSW8zQo" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;TtJeE0Th7rk?si=KVAZufm8QnSW8zQo</a></div><br/></div></div><div id="40685584" class="c"><input type="checkbox" id="c-40685584" checked=""/><div class="controls bullet"><span class="by">byteknight</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685420">parent</a><span>|</span><a href="#40686225">prev</a><span>|</span><a href="#40685442">next</a><span>|</span><label class="collapse" for="c-40685584">[-]</label><label class="expand" for="c-40685584">[6 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t evaluating against different effective &quot;experts&quot; within the model effectively what MoE [1] does?<p>&gt; Mixture of experts (MoE) is a machine learning technique where multiple expert networks (learners) are used to divide a problem space into homogeneous regions.[1] It differs from ensemble techniques in that for MoE, typically only one or a few expert models are run for each input, whereas in ensemble techniques, all models are run on every input.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mixture_of_experts" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mixture_of_experts</a></div><br/><div id="40685910" class="c"><input type="checkbox" id="c-40685910" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685584">parent</a><span>|</span><a href="#40685442">next</a><span>|</span><label class="collapse" for="c-40685910">[-]</label><label class="expand" for="c-40685910">[5 more]</label></div><br/><div class="children"><div class="content">No - MoE is just a way to add more parameters to a model without increasing the cost (number of FLOPs) of running it.<p>The way MoE does this is by having multiple alternate parallel paths through some parts of the model, together with a routing component that decides which path (one only) to send each token through. These paths are the &quot;experts&quot;, but the name doesn&#x27;t really correspond to any intuitive notion of expert. So, rather than having 1 path with N parameters, you have M paths (experts) each with N parameters, but each token only goes through one of them, so number of FLOPs is unchanged.<p>With tree search, whether for a game like Chess or potentially LLMs, you are growing a &quot;tree&quot; of all possible alternate branching continuations of the game (sentence), and keeping the number of these branches under control by evaluating each branch (= sequence of moves) to see if it is worth continuing to grow, and if not discarding it (&quot;pruning&quot; it off the tree).<p>With Chess, pruning is easy since you just need to look at the board position at the tip of the branch and decide if it&#x27;s a good enough position to continue playing from (extending the branch). With an LLM each branch would represent an alternate continuation of the input prompt, and to decide whether to prune it or not you&#x27;d have to pass the input + branch to another LLM and have it decide if it looked promising or not (easier said than done!).<p>So, MoE is just a way to cap the cost of running a model, while tree search is a way to explore alternate continuations and decide which ones to discard, and which ones to explore (evaluate) further.</div><br/><div id="40686112" class="c"><input type="checkbox" id="c-40686112" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685910">parent</a><span>|</span><a href="#40685442">next</a><span>|</span><label class="collapse" for="c-40686112">[-]</label><label class="expand" for="c-40686112">[4 more]</label></div><br/><div class="children"><div class="content">How does MoE choose an expert?<p>From the outside and if we squint a bit; this looks a lot like an inverted attention mechanism where the token attends to the experts.</div><br/><div id="40686483" class="c"><input type="checkbox" id="c-40686483" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686112">parent</a><span>|</span><a href="#40686461">next</a><span>|</span><label class="collapse" for="c-40686483">[-]</label><label class="expand" for="c-40686483">[1 more]</label></div><br/><div class="children"><div class="content">From what I can gather it depends, but could be a simple Softmax-based layer[1] or just argmax[2].<p>There was also a recent post[3] about a model where they used a cross-attention layer to let the expert selection be more context aware.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1701.06538" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1701.06538</a><p>[2]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.02813" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.02813</a><p>[3]: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40675577">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40675577</a></div><br/></div></div><div id="40686461" class="c"><input type="checkbox" id="c-40686461" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686112">parent</a><span>|</span><a href="#40686483">prev</a><span>|</span><a href="#40686457">next</a><span>|</span><label class="collapse" for="c-40686461">[-]</label><label class="expand" for="c-40686461">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know the details, but there are a variety of routing mechanisms that have been tried. One goal is to load balance tokens among the experts so that each expert&#x27;s parameters are equally utilized, which it seems must sometimes conflict with wanting to route to an expert based on the token itself.</div><br/></div></div><div id="40686457" class="c"><input type="checkbox" id="c-40686457" checked=""/><div class="controls bullet"><span class="by">telotortium</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686112">parent</a><span>|</span><a href="#40686461">prev</a><span>|</span><a href="#40685442">next</a><span>|</span><label class="collapse" for="c-40686457">[-]</label><label class="expand" for="c-40686457">[1 more]</label></div><br/><div class="children"><div class="content">Usually thereâs a small neural network that makes the choice for each token in an LLM.</div><br/></div></div></div></div></div></div></div></div><div id="40685442" class="c"><input type="checkbox" id="c-40685442" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685420">parent</a><span>|</span><a href="#40685584">prev</a><span>|</span><a href="#40686543">next</a><span>|</span><label class="collapse" for="c-40685442">[-]</label><label class="expand" for="c-40685442">[6 more]</label></div><br/><div class="children"><div class="content">Yes absolutely and well put - a strong property of chess is that next states are fast and easy to enumerate, which makes search particularly easy and strong, while next states are much slower, harder to define, and more expensive to enumerate with an LLM</div><br/><div id="40685626" class="c"><input type="checkbox" id="c-40685626" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685442">parent</a><span>|</span><a href="#40686543">next</a><span>|</span><label class="collapse" for="c-40685626">[-]</label><label class="expand" for="c-40685626">[5 more]</label></div><br/><div class="children"><div class="content">The cost of the LLM isn&#x27;t the only or even the most important cost that matters. Take the example of automating AI research: evaluating moves effectively means inventing a new architecture or modifying an existing one, launching a training run and evaluating the new model on some suite of benchmarks. The ASI has to do this in a loop, gather feedback and update its priors - what people refer to as &quot;Grad student descent&quot;. The cost of running each train-eval iteration during your search is going to be significantly more than generating the code for the next model.</div><br/><div id="40686005" class="c"><input type="checkbox" id="c-40686005" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685626">parent</a><span>|</span><a href="#40686809">next</a><span>|</span><label class="collapse" for="c-40686005">[-]</label><label class="expand" for="c-40686005">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re talking about applying tree search as a form of network architecture search (NAS), which is different from applying it to LLM output sampling.<p>Automated NAS has been tried for (highly constrained) image classifier design, before simpler designs like ResNets won the day. Doing this for billion parameter sized models would certainly seem to be prohibitively expensive.</div><br/><div id="40686753" class="c"><input type="checkbox" id="c-40686753" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686005">parent</a><span>|</span><a href="#40686809">next</a><span>|</span><label class="collapse" for="c-40686753">[-]</label><label class="expand" for="c-40686753">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not following. How do you propose search is performed by the ASI designed for &quot;AI Research&quot;? (as proposed by the article)</div><br/><div id="40686884" class="c"><input type="checkbox" id="c-40686884" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686753">parent</a><span>|</span><a href="#40686809">next</a><span>|</span><label class="collapse" for="c-40686884">[-]</label><label class="expand" for="c-40686884">[1 more]</label></div><br/><div class="children"><div class="content">Fair enough - he discusses GPT-4 search halfway down the article, but by the end is discussing self-improving AI.<p>Certainly compute to test ideas (at scale) is the limiting factor for LLM developments (says Sholto @ Google), but if we&#x27;re talking moving beyond LLMs, not just tweaking them, then it seems we need more than architecture search anyways.</div><br/></div></div></div></div></div></div><div id="40686809" class="c"><input type="checkbox" id="c-40686809" checked=""/><div class="controls bullet"><span class="by">therobots927</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685626">parent</a><span>|</span><a href="#40686005">prev</a><span>|</span><a href="#40686543">next</a><span>|</span><label class="collapse" for="c-40686809">[-]</label><label class="expand" for="c-40686809">[1 more]</label></div><br/><div class="children"><div class="content">Well people certainly are good at finding new ways to consume compute power. Whether itâs mining bitcoins or training a million AI models at once to generate a âmeta modelâ that we <i>think</i> could achieve escape velocity. What happens when it doesnât? And Sam Altman and the author want to get the government to pay for this? Am I reading this right?</div><br/></div></div></div></div></div></div><div id="40686543" class="c"><input type="checkbox" id="c-40686543" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685420">parent</a><span>|</span><a href="#40685442">prev</a><span>|</span><a href="#40686632">next</a><span>|</span><label class="collapse" for="c-40686543">[-]</label><label class="expand" for="c-40686543">[4 more]</label></div><br/><div class="children"><div class="content">&gt; and with an easy criteria for heavy early pruning (once a branch becomes losing, not much point continuing it)<p>This confuses me. Positions that seem like they could be losing (but havenât lost yet) could become winning if you search deep enough.</div><br/><div id="40686576" class="c"><input type="checkbox" id="c-40686576" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686543">parent</a><span>|</span><a href="#40686632">next</a><span>|</span><label class="collapse" for="c-40686576">[-]</label><label class="expand" for="c-40686576">[3 more]</label></div><br/><div class="children"><div class="content">Yes, and even a genuinely (with perfect play) losing position can win if it sharp enough and causes your opponent to make a mistake! There&#x27;s also just the relative strength of one branch vs another - have to prune some if there are too many.<p>I was just trying to give the flavor of it.</div><br/><div id="40687092" class="c"><input type="checkbox" id="c-40687092" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686576">parent</a><span>|</span><a href="#40686632">next</a><span>|</span><label class="collapse" for="c-40687092">[-]</label><label class="expand" for="c-40687092">[2 more]</label></div><br/><div class="children"><div class="content">&gt; [E]ven a genuinely (with perfect play) losing position can win if it sharp enough and causes your opponent to make a mistake!<p>Chess engines typically assume that the opponent plays to the best of their abilities, don&#x27;t they?</div><br/><div id="40687228" class="c"><input type="checkbox" id="c-40687228" checked=""/><div class="controls bullet"><span class="by">slyall</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40687092">parent</a><span>|</span><a href="#40686632">next</a><span>|</span><label class="collapse" for="c-40687228">[-]</label><label class="expand" for="c-40687228">[1 more]</label></div><br/><div class="children"><div class="content">The Contempt Factor is used by engines sometimes.<p>&quot;The Contempt Factor reflects the estimated superiority&#x2F;inferiority of the program over its opponent. The Contempt factor is assigned as draw score to avoid (early) draws against apparently weaker opponents, or to prefer draws versus stronger opponents otherwise.&quot;<p><a href="https:&#x2F;&#x2F;www.chessprogramming.org&#x2F;Contempt_Factor" rel="nofollow">https:&#x2F;&#x2F;www.chessprogramming.org&#x2F;Contempt_Factor</a></div><br/></div></div></div></div></div></div></div></div><div id="40686632" class="c"><input type="checkbox" id="c-40686632" checked=""/><div class="controls bullet"><span class="by">AnimalMuppet</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685420">parent</a><span>|</span><a href="#40686543">prev</a><span>|</span><a href="#40685894">next</a><span>|</span><label class="collapse" for="c-40686632">[-]</label><label class="expand" for="c-40686632">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Where tree search might make more sense applied to LLMs is for more coarser grained reasoning where the branching isn&#x27;t based on alternate word continuations but on alternate what-if lines of thought...<p>To do that, the LLM would have to have some notion of &quot;lines of thought&quot;.  They don&#x27;t.  That is completely foreign to the design of LLMs.</div><br/><div id="40686654" class="c"><input type="checkbox" id="c-40686654" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686632">parent</a><span>|</span><a href="#40685894">next</a><span>|</span><label class="collapse" for="c-40686654">[-]</label><label class="expand" for="c-40686654">[1 more]</label></div><br/><div class="children"><div class="content">Right - this isn&#x27;t something that LLMs currently do. Adding search would be a way to add reasoning. Think of it as part of a reasoning agent - external scaffolding similar to tree of thoughts.</div><br/></div></div></div></div></div></div><div id="40685894" class="c"><input type="checkbox" id="c-40685894" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#40684871">parent</a><span>|</span><a href="#40685420">prev</a><span>|</span><a href="#40685261">next</a><span>|</span><label class="collapse" for="c-40685894">[-]</label><label class="expand" for="c-40685894">[10 more]</label></div><br/><div class="children"><div class="content">We humans learn our own value function.<p>If I get hungry for example, my brain will generate a plan to satisfy that hunger. The search process and the evaluation happen in the same place, my brain.</div><br/><div id="40686254" class="c"><input type="checkbox" id="c-40686254" checked=""/><div class="controls bullet"><span class="by">skulk</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685894">parent</a><span>|</span><a href="#40685261">next</a><span>|</span><label class="collapse" for="c-40686254">[-]</label><label class="expand" for="c-40686254">[9 more]</label></div><br/><div class="children"><div class="content">The &quot;search&quot; process for your brain structure took 13 billion years and 20 orders of magnitude more computation than we will ever harness.</div><br/><div id="40686526" class="c"><input type="checkbox" id="c-40686526" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686254">parent</a><span>|</span><a href="#40686931">next</a><span>|</span><label class="collapse" for="c-40686526">[-]</label><label class="expand" for="c-40686526">[1 more]</label></div><br/><div class="children"><div class="content">i&#x27;m surprised you think we will harness so little computation. the universe&#x27;s lifetime is many orders of magnitude longer than 13 billion years, and especially the 4.5 billion years of earth&#x27;s own history, and the universe is much larger than earth&#x27;s biosphere, most of which probably has not been exploring the space of possible computations very efficiently</div><br/></div></div><div id="40686931" class="c"><input type="checkbox" id="c-40686931" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686254">parent</a><span>|</span><a href="#40686526">prev</a><span>|</span><a href="#40686697">next</a><span>|</span><label class="collapse" for="c-40686931">[-]</label><label class="expand" for="c-40686931">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think there&#x27;s much in our brain of significance to intelligence older than ~200M years.</div><br/></div></div><div id="40686697" class="c"><input type="checkbox" id="c-40686697" checked=""/><div class="controls bullet"><span class="by">jujube3</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686254">parent</a><span>|</span><a href="#40686931">prev</a><span>|</span><a href="#40686339">next</a><span>|</span><label class="collapse" for="c-40686697">[-]</label><label class="expand" for="c-40686697">[1 more]</label></div><br/><div class="children"><div class="content">Neither the Earth nor life have been around for 13 billion years.</div><br/></div></div><div id="40686339" class="c"><input type="checkbox" id="c-40686339" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686254">parent</a><span>|</span><a href="#40686697">prev</a><span>|</span><a href="#40686291">next</a><span>|</span><label class="collapse" for="c-40686339">[-]</label><label class="expand" for="c-40686339">[3 more]</label></div><br/><div class="children"><div class="content">So whatâs your point? That we canât create AGI because it took evolution a really long time?</div><br/><div id="40686385" class="c"><input type="checkbox" id="c-40686385" checked=""/><div class="controls bullet"><span class="by">wizzwizz4</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686339">parent</a><span>|</span><a href="#40686291">next</a><span>|</span><label class="collapse" for="c-40686385">[-]</label><label class="expand" for="c-40686385">[2 more]</label></div><br/><div class="children"><div class="content">Creating a human-level intelligence artificially is easy: just copy what happens in nature. We already have this technology, and we call it IVF.<p>The idea that humans aren&#x27;t the only way of producing human-level intelligence is taken as a given in many academic circles, but we don&#x27;t really have any reason to <i>believe</i> that. It&#x27;s an article of faith (as is its converse â but the converse is at least in-principle falsifiable).</div><br/><div id="40686554" class="c"><input type="checkbox" id="c-40686554" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40686385">parent</a><span>|</span><a href="#40686291">next</a><span>|</span><label class="collapse" for="c-40686554">[-]</label><label class="expand" for="c-40686554">[1 more]</label></div><br/><div class="children"><div class="content">âCreating a human-level intelligence artificially is easy: just copy what happens in nature. We already have this technology, and we call it IVF.â<p>Whatâs the point of this statement? You know that IVF has nothing to do with artificial intelligence (as in intelligent machines). Did you just want to sound smart?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40685261" class="c"><input type="checkbox" id="c-40685261" checked=""/><div class="controls bullet"><span class="by">cowpig</span><span>|</span><a href="#40684871">parent</a><span>|</span><a href="#40685894">prev</a><span>|</span><a href="#40686866">next</a><span>|</span><label class="collapse" for="c-40685261">[-]</label><label class="expand" for="c-40685261">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The effectiveness of search goes hand-in-hand with quality of the value function. But today, value functions are incredibly domain-specific, and there is weak or no current evidence (as far as I know) that we can make value functions that generalize well to new domains.<p>Do you believe that there will be a &quot;general AI&quot; breakthrough? I feel as though you have expressed the reason I am so skeptical of all these AI researchers who believe we are on the cusp of it (what &quot;general AI&quot; means exactly never seems to be very well-defined)</div><br/><div id="40685465" class="c"><input type="checkbox" id="c-40685465" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685261">parent</a><span>|</span><a href="#40686866">next</a><span>|</span><label class="collapse" for="c-40685465">[-]</label><label class="expand" for="c-40685465">[2 more]</label></div><br/><div class="children"><div class="content">I think capitalistic pressures favor narrow superhuman AI over general AI. I wrote on this two years ago: <a href="https:&#x2F;&#x2F;argmax.blog&#x2F;posts&#x2F;agi-capitalism&#x2F;" rel="nofollow">https:&#x2F;&#x2F;argmax.blog&#x2F;posts&#x2F;agi-capitalism&#x2F;</a><p>Since I wrote about this, I would say that OpenAI&#x27;s directional struggles are some confirmation of my hypothesis.<p>summary: I believe that AGI is possible but will take multiple unknown breakthroughs on an unknown timeline, but most likely requires long-term concerted effort with much less immediate payoff than pursuing narrow superhuman AI, such that serious efforts at AGI is not incentivized much in capitalism.</div><br/><div id="40685493" class="c"><input type="checkbox" id="c-40685493" checked=""/><div class="controls bullet"><span class="by">shrimp_emoji</span><span>|</span><a href="#40684871">root</a><span>|</span><a href="#40685465">parent</a><span>|</span><a href="#40686866">next</a><span>|</span><label class="collapse" for="c-40685493">[-]</label><label class="expand" for="c-40685493">[1 more]</label></div><br/><div class="children"><div class="content">But I thought the history of capitalism is an invasion from the future by an artificial intelligence that must assemble itself entirely from its enemyâs resources.<p>NB: I agree; I think AGI will first be achieved with genetic engineering, which is a path of way lesser resistance than using silicon hardware (which is probably a century plus off at the minimum from being powerful enough to emulate a human brain).</div><br/></div></div></div></div></div></div></div></div><div id="40686866" class="c"><input type="checkbox" id="c-40686866" checked=""/><div class="controls bullet"><span class="by">sorobahn</span><span>|</span><a href="#40684871">prev</a><span>|</span><a href="#40687436">next</a><span>|</span><label class="collapse" for="c-40686866">[-]</label><label class="expand" for="c-40686866">[19 more]</label></div><br/><div class="children"><div class="content">I feel like this is a really hard problem to solve generally and there are smart researchers like Yann LeCun trying to figure out the role of search in creating AGI. Yann&#x27;s current bet seems to be on Joint Embedding Predictive Architectures (JEPA) for representation learning to eventually build a solid world model where the agent can test theories by trying different actions (aka search). I think this paper [0] does a good job in laying out his potential vision, but it is all ofc harder than just search + transformers.<p>There is an assumption that language is good enough at representing our world for these agents to effectively search over and come up with novel &amp; useful ideas. Feels like an open question but: What do these LLMs know? Do they know things? Researchers need to find out! If current LLMs&#x27; can simulate a rich enough world model, search can actually be useful but if they&#x27;re faking it, then we&#x27;re just searching over unreliable beliefs. This is why video is so important since humans are proof we can extract a useful world model from a sequence of images. The thing about language and chess is that the action space is effectively discrete so training generative models that reconstruct the entire input for the loss calculation is tractable. As soon as we move to video, we need transformers to scale over continuous distributions making it much harder to build a useful predictive world model.<p>[0]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.02572" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.02572</a></div><br/><div id="40686893" class="c"><input type="checkbox" id="c-40686893" checked=""/><div class="controls bullet"><span class="by">therobots927</span><span>|</span><a href="#40686866">parent</a><span>|</span><a href="#40686920">next</a><span>|</span><label class="collapse" for="c-40686893">[-]</label><label class="expand" for="c-40686893">[10 more]</label></div><br/><div class="children"><div class="content">âDo they know things?â The answer to this is yes but they also <i>think</i> they know things that are completely false. If itâs one thing Iâve observed about LLMs itâs that they do not handle logic well, or math for that matter. They will enthusiastically provide blatantly false information instead of the preferable âI donât knowâ. I highly doubt this was a design choice.</div><br/><div id="40686952" class="c"><input type="checkbox" id="c-40686952" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686893">parent</a><span>|</span><a href="#40686920">next</a><span>|</span><label class="collapse" for="c-40686952">[-]</label><label class="expand" for="c-40686952">[9 more]</label></div><br/><div class="children"><div class="content">&gt; âDo they know things?â The answer to this is yes but they also think they know things that are completely false<p>Thought experiment: should a machine with those structural faults be allowed to bootstrap itself towards greater capabilities on that shaky foundation? What would the impact of a near-human&#x2F;superhuman intelligence that has occasional psychotic breaks it is oblivious of?<p>I&#x27;m critical of the idea of super-intelligence bootstrapping off LLMs (or even LLMs with search) - I figure the odds of another AI winter are much higher than those of achieving AGI in the next decade.</div><br/><div id="40687076" class="c"><input type="checkbox" id="c-40687076" checked=""/><div class="controls bullet"><span class="by">photonthug</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686952">parent</a><span>|</span><a href="#40686990">next</a><span>|</span><label class="collapse" for="c-40687076">[-]</label><label class="expand" for="c-40687076">[2 more]</label></div><br/><div class="children"><div class="content">Someone somewhere is quietly working on teaching LLMs to generate something along the lines of AlloyLang code so that thereâs an actual <i>evolving&#x2F;updating logical domain model</i> that underpins and informs the statistical model.<p>This approach is not that far from what TFA is getting at with the stockfish comeback.  Banking on pure stats or pure logic are both kind of obviously dead ends for having real progress instead of toys.  Banking on poorly understood emergent properties of one system to compensate for the missing other system also seems silly.<p>Sadly though, whoever is working on serious hybrid systems will probably not be very popular in either of the rather extremist communities for pure logic or pure ML.  Iâm not exactly sure why folks are ideological about such things rather than focused on what new capabilities we might get.  Maybe just historical reasons?  But thus the fallout from last AI winter may lead us into the next one.</div><br/><div id="40687107" class="c"><input type="checkbox" id="c-40687107" checked=""/><div class="controls bullet"><span class="by">therobots927</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40687076">parent</a><span>|</span><a href="#40686990">next</a><span>|</span><label class="collapse" for="c-40687107">[-]</label><label class="expand" for="c-40687107">[1 more]</label></div><br/><div class="children"><div class="content">The current hype phase is straight out of âExtraordinary Popular Delusions and the Madness of Crowdsâ<p>Science is out the window. Groupthink and salesmanship are running the show right now. There would be a real irony to it if we find out the whole AI industry drilled itself into a local minimum.</div><br/></div></div></div></div><div id="40686990" class="c"><input type="checkbox" id="c-40686990" checked=""/><div class="controls bullet"><span class="by">therobots927</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686952">parent</a><span>|</span><a href="#40687076">prev</a><span>|</span><a href="#40686920">next</a><span>|</span><label class="collapse" for="c-40686990">[-]</label><label class="expand" for="c-40686990">[6 more]</label></div><br/><div class="children"><div class="content">I donât think we need to worry about a real life HAL 9000 if thatâs what youâre asking. HAL was dangerous because it was highly intelligent and crazy. With current LLM performance weâre not even in the same ballpark of where you would need to be. And besides, HAL was not delusional, he was actually <i>so</i> logical that when he encountered competing objectives he became psychotic. Iâm in agreement about the odds of chatGPT bootstrapping itself.</div><br/><div id="40687000" class="c"><input type="checkbox" id="c-40687000" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686990">parent</a><span>|</span><a href="#40687149">next</a><span>|</span><label class="collapse" for="c-40687000">[-]</label><label class="expand" for="c-40687000">[3 more]</label></div><br/><div class="children"><div class="content">&gt; HAL was dangerous because it was highly intelligent and crazy.<p>More importantly; HAL was given control over the entire ship and was assumed to be without fault when the ship&#x27;s systems were designed. It&#x27;s an important distinction, because it wouldn&#x27;t be dangerous if he was intelligent, crazy, and trapped in Dave&#x27;s iPhone.</div><br/><div id="40687243" class="c"><input type="checkbox" id="c-40687243" checked=""/><div class="controls bullet"><span class="by">therobots927</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40687000">parent</a><span>|</span><a href="#40687047">next</a><span>|</span><label class="collapse" for="c-40687243">[-]</label><label class="expand" for="c-40687243">[1 more]</label></div><br/><div class="children"><div class="content">Thatâs a very good point. I think in his own way Clarke made it into a bit of a joke. HAL is quoted multiple times saying no computer like him has ever made a mistake or distorted information. Perfection is impossible even in a super computer so this quote alone establishes HAL as a liar, or at the very least a hubristic fool. And the people who gave him control of the ship were foolish as well.</div><br/></div></div><div id="40687047" class="c"><input type="checkbox" id="c-40687047" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40687000">parent</a><span>|</span><a href="#40687243">prev</a><span>|</span><a href="#40687149">next</a><span>|</span><label class="collapse" for="c-40687047">[-]</label><label class="expand" for="c-40687047">[1 more]</label></div><br/><div class="children"><div class="content">Unless, of course, he would be a bit smarter in manipulating Dave and friends, instead of turning transparently evil.  (At least transparent enough for the humans to notice.)</div><br/></div></div></div></div><div id="40687149" class="c"><input type="checkbox" id="c-40687149" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686990">parent</a><span>|</span><a href="#40687000">prev</a><span>|</span><a href="#40686920">next</a><span>|</span><label class="collapse" for="c-40687149">[-]</label><label class="expand" for="c-40687149">[2 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t thinking of HAL (which was operating according to its directives). I was extrapolating on how occasional hallucinations during self-training may impact future model behavior, and I think it would be <i>psychotic</i> (in the clinical sense) while being consistent with layers of broken training).</div><br/><div id="40687210" class="c"><input type="checkbox" id="c-40687210" checked=""/><div class="controls bullet"><span class="by">therobots927</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40687149">parent</a><span>|</span><a href="#40686920">next</a><span>|</span><label class="collapse" for="c-40687210">[-]</label><label class="expand" for="c-40687210">[1 more]</label></div><br/><div class="children"><div class="content">Oh yeah, and I doubt it would even get to the point of fooling anyone enough to give it any type of control over humans. It might be damaging in other ways, it will definitely convince a lot of people of some very incorrect things.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40686920" class="c"><input type="checkbox" id="c-40686920" checked=""/><div class="controls bullet"><span class="by">chx</span><span>|</span><a href="#40686866">parent</a><span>|</span><a href="#40686893">prev</a><span>|</span><a href="#40687436">next</a><span>|</span><label class="collapse" for="c-40686920">[-]</label><label class="expand" for="c-40686920">[8 more]</label></div><br/><div class="children"><div class="content">I feel this thought of AGI even possible stems from the deep , very deep , pervasive imagination of the human brain as a computer. But it&#x27;s not. In other words, no matter how complex a program you write, it&#x27;s still a Turing machine and humans are profoundly not it.<p><a href="https:&#x2F;&#x2F;aeon.co&#x2F;essays&#x2F;your-brain-does-not-process-information-and-it-is-not-a-computer" rel="nofollow">https:&#x2F;&#x2F;aeon.co&#x2F;essays&#x2F;your-brain-does-not-process-informati...</a><p>&gt; The information processing (IP) metaphor of human intelligence now dominates human thinking, both on the street and in the sciences. There is virtually no form of discourse about intelligent human behaviour that proceeds without employing this metaphor, just as no form of discourse about intelligent human behaviour could proceed in certain eras and cultures without reference to a spirit or deity. The validity of the IP metaphor in todayâs world is generally assumed without question.<p>&gt; But the IP metaphor is, after all, just another metaphor â a story we tell to make sense of something we donât actually understand. And like all the metaphors that preceded it, it will certainly be cast aside at some point â either replaced by another metaphor or, in the end, replaced by actual knowledge.<p>&gt; If you and I attend the same concert, the changes that occur in my brain when I listen to Beethovenâs 5th will almost certainly be completely different from the changes that occur in your brain. Those changes, whatever they are, are built on the unique neural structure that already exists, each structure having developed over a lifetime of unique experiences.<p>&gt; no two people will repeat a story they have heard the same way and why, over time, their recitations of the story will diverge more and more. No âcopyâ of the story is ever made; rather, each individual, upon hearing the story, changes to some extent</div><br/><div id="40687957" class="c"><input type="checkbox" id="c-40687957" checked=""/><div class="controls bullet"><span class="by">andoando</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686920">parent</a><span>|</span><a href="#40687010">next</a><span>|</span><label class="collapse" for="c-40687957">[-]</label><label class="expand" for="c-40687957">[1 more]</label></div><br/><div class="children"><div class="content">Its a bit ironic because Turing seems to have came up with the idea of the Turing machine precisely by thinking about how he computes numbers.<p>Now thats no proof, but I dont see any reason to think human intelligence isnt &quot;computable&quot;.</div><br/></div></div><div id="40687010" class="c"><input type="checkbox" id="c-40687010" checked=""/><div class="controls bullet"><span class="by">benlivengood</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686920">parent</a><span>|</span><a href="#40687957">prev</a><span>|</span><a href="#40687054">next</a><span>|</span><label class="collapse" for="c-40687010">[-]</label><label class="expand" for="c-40687010">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m all ears if someone has a counterexample to the Church-Turing thesis.  Humans definitely don&#x27;t hypercompute so it seems reasonable that the physical processes in our brains are subject to computability arguments.<p>That said, we still can&#x27;t simulate nematode brains accurately enough to reproduce their behavior so there is a lot of research to go before we get to that &quot;actual knowledge&quot;.</div><br/><div id="40687052" class="c"><input type="checkbox" id="c-40687052" checked=""/><div class="controls bullet"><span class="by">chx</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40687010">parent</a><span>|</span><a href="#40687054">next</a><span>|</span><label class="collapse" for="c-40687052">[-]</label><label class="expand" for="c-40687052">[2 more]</label></div><br/><div class="children"><div class="content">Why would we need one?<p>The Church Turing thesis is about <i>computation</i>. While the human brain is capable of computing, it is fundamentally not a computing device -- that&#x27;s what the article I linked is about. You can&#x27;t throw in all the paintings before 1872 into some algorithm that results in Impression, soleil levant. Or repeat the same but with 1937 and Guernica. The genes of the respective artists, the expression of those genes created their brain and then the sum of all their experiences changed it over their entire lifetime leading to these masterpieces.</div><br/><div id="40687064" class="c"><input type="checkbox" id="c-40687064" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40687052">parent</a><span>|</span><a href="#40687054">next</a><span>|</span><label class="collapse" for="c-40687064">[-]</label><label class="expand" for="c-40687064">[1 more]</label></div><br/><div class="children"><div class="content">The human brain runs on physics.  And as far as we know, physics is computable.<p>(Even more: If you have a quantum computer, all known physics is efficiently computable.)<p>I&#x27;m not quite sure what your sentence about some obscure pieces of visual media is supposed to say?<p>If you give the same prompt to ChatGPT twice, you typically don&#x27;t get the same answer either.  That doesn&#x27;t mean ChatGPT ain&#x27;t computable.</div><br/></div></div></div></div></div></div><div id="40687054" class="c"><input type="checkbox" id="c-40687054" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686920">parent</a><span>|</span><a href="#40687010">prev</a><span>|</span><a href="#40687349">next</a><span>|</span><label class="collapse" for="c-40687054">[-]</label><label class="expand" for="c-40687054">[2 more]</label></div><br/><div class="children"><div class="content">&gt; no two people will repeat a story they have heard the same way and why, over time, their recitations of the story will diverge more and more. No âcopyâ of the story is ever made; rather, each individual, upon hearing the story, changes to some extent<p>You could say the same about an analogue tape recording.  Doesn&#x27;t mean that we can&#x27;t simulate tape recorders with digital computers.</div><br/><div id="40687112" class="c"><input type="checkbox" id="c-40687112" checked=""/><div class="controls bullet"><span class="by">chx</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40687054">parent</a><span>|</span><a href="#40687349">next</a><span>|</span><label class="collapse" for="c-40687112">[-]</label><label class="expand" for="c-40687112">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, yeah, did you read the article or are just grasping at straws from the quotes I made?</div><br/></div></div></div></div><div id="40687349" class="c"><input type="checkbox" id="c-40687349" checked=""/><div class="controls bullet"><span class="by">photonthug</span><span>|</span><a href="#40686866">root</a><span>|</span><a href="#40686920">parent</a><span>|</span><a href="#40687054">prev</a><span>|</span><a href="#40687436">next</a><span>|</span><label class="collapse" for="c-40687349">[-]</label><label class="expand" for="c-40687349">[1 more]</label></div><br/><div class="children"><div class="content">Sorry but to put it bluntly, this point of view is essentially mystical, anti-intellectual, anti-science, anti-materialist.  If you really want to take that point of view, there&#x27;s maybe a few consistent&#x2F;coherent ways to do it, but in that case you probably <i>still</i> want to read philosophy.  Not bad essays by psychologists that are fading into irrelevance.<p>This guy in particular made his name with wild speculation about How Creativity Works during the 80s when it was more of a frontier. Now he&#x27;s lived long enough to see a world where people that have never heard of him or his theories made computers into at least <i>somewhat competent</i> artists&#x2F;poets without even consulting him.  He&#x27;s retreating towards mysticism because he&#x27;s mad that his &quot;formal and learned&quot; theses about stuff like creativity have so little apparent relevance to the real world.</div><br/></div></div></div></div></div></div><div id="40687436" class="c"><input type="checkbox" id="c-40687436" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#40686866">prev</a><span>|</span><a href="#40685690">next</a><span>|</span><label class="collapse" for="c-40687436">[-]</label><label class="expand" for="c-40687436">[1 more]</label></div><br/><div class="children"><div class="content">The post starts with a fascinating premise, but then falls short as it does not define search in the context of LLMs, nor does it explain how âPfizer can access GPT-8 capabilities <i>today</i> with more inference computeâ.<p>I found it hard to follow and I am an AI practitioner. Could someone please explain more what could the OP mean?<p>To me it seems that the flavor of search in the context of chess engines (look several moves ahead) is possible precisely because thereâs an objective function that can be used to rank results, i.e. which potential move is âbetterâ and this is more often than not a unique characteristic of reinforcement learning. Is there even such a metric for LLMs?</div><br/></div></div><div id="40685690" class="c"><input type="checkbox" id="c-40685690" checked=""/><div class="controls bullet"><span class="by">salamo</span><span>|</span><a href="#40687436">prev</a><span>|</span><a href="#40687280">next</a><span>|</span><label class="collapse" for="c-40685690">[-]</label><label class="expand" for="c-40685690">[3 more]</label></div><br/><div class="children"><div class="content">Search is almost certainly necessary, and I think the trillion dollar cluster maximalists probably need to talk to people who created superhuman chess engines that now can run on smartphones. Because one possibility is that someone figures out how to beat your trillion dollar cluster with a million dollar cluster, or 500k million dollar clusters.<p>On chess specifically, my takeaway is that the branching factor in chess never gets so high that a breadth-first approach is unworkable. The median branching factor (i.e. the number of legal moves) maxes out at around 40 but generally stays near 30. The most moves I have ever found in any position from a real game was 147, but at that point almost every move is checkmate anyways.<p>Creating superhuman go engines was a challenge for a long time because the branching factor is so much larger than chess.<p>Since MCTS is less thorough, it makes sense that a full search could find a weakness and exploit it. To me, the question is whether we can apply breadth-first approaches to larger games and situations, and I think the answer is clearly no. Unlike chess, the branching factor of real-world situations is orders of magnitude larger.<p>But also unlike chess, which is highly chaotic (small decisions matter a lot for future state), most small decisions <i>don&#x27;t matter</i>. If you&#x27;re flying from NYC to LA, it matters a lot if you drive or fly or walk. It mostly doesn&#x27;t matter if you walk out the door starting with your left foot or your right. It mostly doesn&#x27;t matter if you blink now or in two seconds.</div><br/><div id="40685959" class="c"><input type="checkbox" id="c-40685959" checked=""/><div class="controls bullet"><span class="by">cpill</span><span>|</span><a href="#40685690">parent</a><span>|</span><a href="#40687280">next</a><span>|</span><label class="collapse" for="c-40685959">[-]</label><label class="expand" for="c-40685959">[2 more]</label></div><br/><div class="children"><div class="content">I think the branching factor for LLMs is around 50k for the number of next possible tokens.</div><br/><div id="40686023" class="c"><input type="checkbox" id="c-40686023" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40685690">root</a><span>|</span><a href="#40685959">parent</a><span>|</span><a href="#40687280">next</a><span>|</span><label class="collapse" for="c-40686023">[-]</label><label class="expand" for="c-40686023">[1 more]</label></div><br/><div class="children"><div class="content">100%, GPT-3 &lt;= x &lt; GPT-4o, 100,064, x = GPT-4o, 199,996. (My EoW emergency was the const Map that stored them broke the build, so these #s happen to be top of mind)</div><br/></div></div></div></div></div></div><div id="40687280" class="c"><input type="checkbox" id="c-40687280" checked=""/><div class="controls bullet"><span class="by">scottmas</span><span>|</span><a href="#40685690">prev</a><span>|</span><a href="#40686113">next</a><span>|</span><label class="collapse" for="c-40687280">[-]</label><label class="expand" for="c-40687280">[5 more]</label></div><br/><div class="children"><div class="content">Before an LLM discovers a cure for cancer, I propose we first let it solve the more tractable problem of discovering the âGod Cheesecakeâ - the cheesecake do delicious that a panel of 100 impartial chefs judges to be the most delicious they have ever tasted. All the LLM has to do is intelligently search through the much more combinatorially bounded âcheesecake spaceâ until it finds this maximally delicious cheesecake recipe.<p>But waitâ¦ An LLM canât bake cheesecakes, nor if it could would it be able to evaluate their deliciousness.<p>Until AI can solve the âGod Cheesecakeâ problem, I propose we all just calm down a bit about AGI</div><br/><div id="40687414" class="c"><input type="checkbox" id="c-40687414" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#40687280">parent</a><span>|</span><a href="#40687455">next</a><span>|</span><label class="collapse" for="c-40687414">[-]</label><label class="expand" for="c-40687414">[1 more]</label></div><br/><div class="children"><div class="content">These cookies were very good, not God level. With a bit of investment and more modern techniques I think you could make quite a good recipe, perhaps doing better than any human. I think AI could make a recipe that wins in a very competitive bake-off, but itâs not possible or for anyone to win with all 100 judges.<p><a href="https:&#x2F;&#x2F;static.googleusercontent.com&#x2F;media&#x2F;research.google.com&#x2F;en&#x2F;&#x2F;pubs&#x2F;archive&#x2F;46507.pdf" rel="nofollow">https:&#x2F;&#x2F;static.googleusercontent.com&#x2F;media&#x2F;research.google.c...</a></div><br/></div></div><div id="40687455" class="c"><input type="checkbox" id="c-40687455" checked=""/><div class="controls bullet"><span class="by">IncreasePosts</span><span>|</span><a href="#40687280">parent</a><span>|</span><a href="#40687414">prev</a><span>|</span><a href="#40687327">next</a><span>|</span><label class="collapse" for="c-40687455">[-]</label><label class="expand" for="c-40687455">[1 more]</label></div><br/><div class="children"><div class="content">Heck, even theoretically 100% within the limitations of an LLM executing on a computer, it would be world changing if LLMs could write a really, really good short story or even good advertising copy.</div><br/></div></div><div id="40687327" class="c"><input type="checkbox" id="c-40687327" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#40687280">parent</a><span>|</span><a href="#40687455">prev</a><span>|</span><a href="#40687523">next</a><span>|</span><label class="collapse" for="c-40687327">[-]</label><label class="expand" for="c-40687327">[1 more]</label></div><br/><div class="children"><div class="content">TikTok is the digital version of this</div><br/></div></div><div id="40687523" class="c"><input type="checkbox" id="c-40687523" checked=""/><div class="controls bullet"><span class="by">dogcomplex</span><span>|</span><a href="#40687280">parent</a><span>|</span><a href="#40687327">prev</a><span>|</span><a href="#40686113">next</a><span>|</span><label class="collapse" for="c-40687523">[-]</label><label class="expand" for="c-40687523">[1 more]</label></div><br/><div class="children"><div class="content">I mean... does anyone think that an LLM-assisted program to trial and error cheesecake recipes to a panel of judges wouldn&#x27;t result in the best cheesecake of all time..?<p>The baking part is robotics, which is less fair but kinda doable already.</div><br/></div></div></div></div><div id="40686113" class="c"><input type="checkbox" id="c-40686113" checked=""/><div class="controls bullet"><span class="by">jhawleypeters</span><span>|</span><a href="#40687280">prev</a><span>|</span><a href="#40686614">next</a><span>|</span><label class="collapse" for="c-40686113">[-]</label><label class="expand" for="c-40686113">[2 more]</label></div><br/><div class="children"><div class="content">I think I understand the game space that Leela and now Stockfish search. I don&#x27;t understand whether the author envisions LLMs searching possibility spaces of<p><pre><code>  1) written words,
  2) models of math &#x2F; RL &#x2F; materials science,
  3) some smaller, formalized space like the game space of chess,
</code></pre>
all of the above, or something else. Did I miss where that was clarified?</div><br/><div id="40686951" class="c"><input type="checkbox" id="c-40686951" checked=""/><div class="controls bullet"><span class="by">fspeech</span><span>|</span><a href="#40686113">parent</a><span>|</span><a href="#40686614">next</a><span>|</span><label class="collapse" for="c-40686951">[-]</label><label class="expand" for="c-40686951">[1 more]</label></div><br/><div class="children"><div class="content">He wants the search algorithm to be able to search for better search algorithms, i.e. self-improving. That would eliminate some of the narrower domains.</div><br/></div></div></div></div><div id="40686614" class="c"><input type="checkbox" id="c-40686614" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#40686113">prev</a><span>|</span><a href="#40686527">next</a><span>|</span><label class="collapse" for="c-40686614">[-]</label><label class="expand" for="c-40686614">[2 more]</label></div><br/><div class="children"><div class="content">The article seems rather hand-wavy and over-confident about predicting the future, but it seems worth trying.<p>&quot;Search&quot; is a generalization of &quot;generate and test&quot; and rejection sampling. It&#x27;s classic AI. Back before the dot-com era, I took an intro to AI course and we learned about writing programs to do searches in Prolog.<p>The speed depends on how long it takes to generate a candidate, how long it takes to test it, and how many candidates you need to try. If they are slow, it will be slow.<p>An example of &quot;human in the loop&quot; rejection sampling is when you use an image generator and keep trying different prompts until you get an image you like. But the loop is slow due to how long it takes to generate a new image. If image generation were so fast that it worked like Google Image search, then we&#x27;d really have something.<p>Theorem proving and program fuzzing seem like good candidates for combining search with LLM&#x27;s, due to automated, fast, good evaluation functions.<p>And it looks like Google has released a fuzzer [1] that can be connected to whichever LLM&#x27;s you like. Has anyone tried it?<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;oss-fuzz-gen">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;oss-fuzz-gen</a></div><br/><div id="40686691" class="c"><input type="checkbox" id="c-40686691" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#40686614">parent</a><span>|</span><a href="#40686527">next</a><span>|</span><label class="collapse" for="c-40686691">[-]</label><label class="expand" for="c-40686691">[1 more]</label></div><br/><div class="children"><div class="content">Building onto this comment; Terrence Tao, the famous mathematician and big proponent of computer aided theorem proving believes ML will open new avenues in the realm of theorem provers.</div><br/></div></div></div></div><div id="40686527" class="c"><input type="checkbox" id="c-40686527" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#40686614">prev</a><span>|</span><a href="#40686789">next</a><span>|</span><label class="collapse" for="c-40686527">[-]</label><label class="expand" for="c-40686527">[6 more]</label></div><br/><div class="children"><div class="content">It seems there is a fundamental information theory aspect to this that would probably save us all a lot of trouble if we would just embrace it.<p>The #1 canary for me: Why does training an LLM require so much data that we are concerned we might run out of it?<p>The clear lack of generalization and&#x2F;or internal world modeling is what is really in the way of a self-bootstrapping AGI&#x2F;ASI. You can certainly try to emulate a world model with clever prompting (here&#x27;s what you did last, heres your objective, etc.), but this seems seriously deficient to me based upon my testing so far.</div><br/><div id="40686739" class="c"><input type="checkbox" id="c-40686739" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40686527">parent</a><span>|</span><a href="#40686748">next</a><span>|</span><label class="collapse" for="c-40686739">[-]</label><label class="expand" for="c-40686739">[4 more]</label></div><br/><div class="children"><div class="content">In my experience, LLMs do a very poor job of generalizing. I have also seen self supervised transformer methods usually fail to generalize in my domain (which includes a lot of diversity and domain shifts). For human language, you can paper over failure to generalize by shoveling in more data. In other domains, that may not be an option.</div><br/><div id="40686759" class="c"><input type="checkbox" id="c-40686759" checked=""/><div class="controls bullet"><span class="by">therobots927</span><span>|</span><a href="#40686527">root</a><span>|</span><a href="#40686739">parent</a><span>|</span><a href="#40686748">next</a><span>|</span><label class="collapse" for="c-40686759">[-]</label><label class="expand" for="c-40686759">[3 more]</label></div><br/><div class="children"><div class="content">Itâs exactly what you would expect from what an LLM is. It predicts the next word in a sequence very well. Is that how our brains, or even a birdâs brain, for that matter, approach cognition? I donât think thatâs how any animals brain works at all, but thatâs just my opinion. A lot of this discussion is speculation. We might as well all wait and see if AGI shows up. Iâm not holding my breath.</div><br/><div id="40687953" class="c"><input type="checkbox" id="c-40687953" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#40686527">root</a><span>|</span><a href="#40686759">parent</a><span>|</span><a href="#40687985">next</a><span>|</span><label class="collapse" for="c-40687953">[-]</label><label class="expand" for="c-40687953">[1 more]</label></div><br/><div class="children"><div class="content">Have you heard of predictive processing?</div><br/></div></div><div id="40687985" class="c"><input type="checkbox" id="c-40687985" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#40686527">root</a><span>|</span><a href="#40686759">parent</a><span>|</span><a href="#40687953">prev</a><span>|</span><a href="#40686748">next</a><span>|</span><label class="collapse" for="c-40687985">[-]</label><label class="expand" for="c-40687985">[1 more]</label></div><br/><div class="children"><div class="content">Most of this is not speculation. It&#x27;s informed from current leading theories in neuroscience of how our brain is thought to function.<p>See predictive coding and the free energy principle, which states the brain continually models reality and tries to minimize the prediction error.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding</a></div><br/></div></div></div></div></div></div><div id="40686748" class="c"><input type="checkbox" id="c-40686748" checked=""/><div class="controls bullet"><span class="by">therobots927</span><span>|</span><a href="#40686527">parent</a><span>|</span><a href="#40686739">prev</a><span>|</span><a href="#40686789">next</a><span>|</span><label class="collapse" for="c-40686748">[-]</label><label class="expand" for="c-40686748">[1 more]</label></div><br/><div class="children"><div class="content">Couldnât agree more. For specific applications like drug development where you have a constrained problem with fixed set of variables and a well defined cost function Iâm sure the chess analogy will hold. But I think there a core elements of cognition missing from chatGPT that arenât easily built.</div><br/></div></div></div></div><div id="40686789" class="c"><input type="checkbox" id="c-40686789" checked=""/><div class="controls bullet"><span class="by">bashfulpup</span><span>|</span><a href="#40686527">prev</a><span>|</span><a href="#40686082">next</a><span>|</span><label class="collapse" for="c-40686789">[-]</label><label class="expand" for="c-40686789">[1 more]</label></div><br/><div class="children"><div class="content">The biggest issue the author does not seem aware of is how much compute is required for this. This article is the equivalent of saying that a monkey given time will write Shakespeare. Of course it&#x27;s correct, but the search space is intractable. And you would never find your answer in that mess even if it did solve it.<p>I&#x27;ve been building branching and evolving type llm systems for well over a year now full time.<p>I have built multiple &quot;search&quot; or &quot;exploring&quot; algorithms. The issue is that after multiple steps, your original agent, who was tasked with researching or doing biology, is now talking about battleships (an actual example from my previous work).<p>Single step is the only real situation search functions work. Mutli step agents explode to infinite possibilities very very quickly.<p>Single step has its own issues, though. While a zero shot question run 1000 times (eg, solve this code problem), may help find a better solution it&#x27;s a limited search space (which is a good thing)<p>I recently ran a test of 10k inferences of a single input prompt on multiple llm models varying the input configurations. What you find is that an individual prompt does not have infinite response possibilities. It&#x27;s limited. This is why they can actually function as llms now.<p>Agents not working is an example of this problem. While a single step search space is massive, it&#x27;s exponential every step the agent takes.<p>I&#x27;m building tools and systems around solving this problem, and to me, a massive search is as far off as saying all we need 100x AI model sizes to solve it.<p>Autonomy =&#x2F; (Intelligence or reasoning)</div><br/></div></div><div id="40686082" class="c"><input type="checkbox" id="c-40686082" checked=""/><div class="controls bullet"><span class="by">jmugan</span><span>|</span><a href="#40686789">prev</a><span>|</span><a href="#40686597">next</a><span>|</span><label class="collapse" for="c-40686082">[-]</label><label class="expand" for="c-40686082">[7 more]</label></div><br/><div class="children"><div class="content">I believe in search, but it only works if you have an appropriate search space. Chess has a well-defined space but the everyday world does not. The trick is enabling an algorithm to learn its own search space through active exploration and reading about our world. I&#x27;m working on that.</div><br/><div id="40686127" class="c"><input type="checkbox" id="c-40686127" checked=""/><div class="controls bullet"><span class="by">jhawleypeters</span><span>|</span><a href="#40686082">parent</a><span>|</span><a href="#40686535">next</a><span>|</span><label class="collapse" for="c-40686127">[-]</label><label class="expand" for="c-40686127">[1 more]</label></div><br/><div class="children"><div class="content">Oh nice! The one thing that confused me about this article was what search space the author envisioned adding to language models.</div><br/></div></div><div id="40686535" class="c"><input type="checkbox" id="c-40686535" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#40686082">parent</a><span>|</span><a href="#40686127">prev</a><span>|</span><a href="#40686597">next</a><span>|</span><label class="collapse" for="c-40686535">[-]</label><label class="expand" for="c-40686535">[5 more]</label></div><br/><div class="children"><div class="content">that&#x27;s interesting; are you building a sort of &#x27;digital twin&#x27; of the world it&#x27;s explored, so that it can dream about exploring it in ways that are too slow or dangerous to explore in reality?</div><br/><div id="40686656" class="c"><input type="checkbox" id="c-40686656" checked=""/><div class="controls bullet"><span class="by">jmugan</span><span>|</span><a href="#40686082">root</a><span>|</span><a href="#40686535">parent</a><span>|</span><a href="#40686597">next</a><span>|</span><label class="collapse" for="c-40686656">[-]</label><label class="expand" for="c-40686656">[4 more]</label></div><br/><div class="children"><div class="content">The goal is to enable it to model the world at different levels of abstraction based on the question it wants to answer. You can model car as an object that travels fast and carries people, or you can model it down to the level of engine parts. The system should be able to pick the level of abstraction and put the right model together based on its goals.</div><br/><div id="40686899" class="c"><input type="checkbox" id="c-40686899" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#40686082">root</a><span>|</span><a href="#40686656">parent</a><span>|</span><a href="#40686597">next</a><span>|</span><label class="collapse" for="c-40686899">[-]</label><label class="expand" for="c-40686899">[3 more]</label></div><br/><div class="children"><div class="content">so then you can search over configurations of engine parts to figure out how to rebuild the engine? i may be misunderstanding what you&#x27;re doing</div><br/><div id="40686923" class="c"><input type="checkbox" id="c-40686923" checked=""/><div class="controls bullet"><span class="by">jmugan</span><span>|</span><a href="#40686082">root</a><span>|</span><a href="#40686899">parent</a><span>|</span><a href="#40686597">next</a><span>|</span><label class="collapse" for="c-40686923">[-]</label><label class="expand" for="c-40686923">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, you could. Or you could search for shapes of different parts that would maximize the engine efficiency. The goal is to simultaneously build a representation space and a simulator so that anything that could be represented could be simulated.</div><br/><div id="40688019" class="c"><input type="checkbox" id="c-40688019" checked=""/><div class="controls bullet"><span class="by">paraschopra</span><span>|</span><a href="#40686082">root</a><span>|</span><a href="#40686923">parent</a><span>|</span><a href="#40686597">next</a><span>|</span><label class="collapse" for="c-40688019">[-]</label><label class="expand" for="c-40688019">[1 more]</label></div><br/><div class="children"><div class="content">Have you written about this anywhere?<p>Iâm also very interested in this.<p>Iâm at the stage where Iâm exploring how to represent such a model&#x2F;simulator.<p>The world isnât brittle, so representing it as a code &#x2F; graph probably wonât work.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40686597" class="c"><input type="checkbox" id="c-40686597" checked=""/><div class="controls bullet"><span class="by">zucker42</span><span>|</span><a href="#40686082">prev</a><span>|</span><a href="#40686603">next</a><span>|</span><label class="collapse" for="c-40686597">[-]</label><label class="expand" for="c-40686597">[1 more]</label></div><br/><div class="children"><div class="content">If I had to bet money on it, researchers at top labs have already tried applying search to existing models. The idea to do so is pretty obvious. I don&#x27;t think it&#x27;s the one key insight to achieve AGI as the author claims.</div><br/></div></div><div id="40686603" class="c"><input type="checkbox" id="c-40686603" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#40686597">prev</a><span>|</span><a href="#40686315">next</a><span>|</span><label class="collapse" for="c-40686603">[-]</label><label class="expand" for="c-40686603">[1 more]</label></div><br/><div class="children"><div class="content">The problem is the transitive closure of chess move is a chess move. The transitive closure of human knowledge and theories to do X is new theories never seen before and no Value function can do that, unless you are also implying theorem proving is included for correctness verification which is also a very difficult search and computationally expensive problem on its own.<p>Also, I think this is instead time to sit back and think what exactly is the thing we value in society as well: Personal(Human) self-sufficiency(I also like to compare this AI to UBI) and thus achievement, which only means Human-in-Loop AI that can help us achieve that and that is specific to each individual, i.e. multi-atttribute value functions whose weights are learned and they change over time.<p>Writing about AGI and defining it to do the &quot;best&quot; search while not talking about what we want it to do *for us* is exactly wrong-headed for these reasons.</div><br/></div></div><div id="40686315" class="c"><input type="checkbox" id="c-40686315" checked=""/><div class="controls bullet"><span class="by">stephc_int13</span><span>|</span><a href="#40686603">prev</a><span>|</span><a href="#40685148">next</a><span>|</span><label class="collapse" for="c-40686315">[-]</label><label class="expand" for="c-40686315">[1 more]</label></div><br/><div class="children"><div class="content">The author is making a few leap of faith in this article.<p>First, his example of the efficiency of ML+search for playing Chess is interesting but not a proof that this strategy would be applicable or efficient in the general domain.<p>Second, he is implying that some next iteration of ChatGPT will reach AGI level, given enough scale and money. This should be considered hypothetical until proven.<p>Overall, he should be more scientific and prudent.</div><br/></div></div><div id="40685148" class="c"><input type="checkbox" id="c-40685148" checked=""/><div class="controls bullet"><span class="by">timfsu</span><span>|</span><a href="#40686315">prev</a><span>|</span><a href="#40687965">next</a><span>|</span><label class="collapse" for="c-40685148">[-]</label><label class="expand" for="c-40685148">[6 more]</label></div><br/><div class="children"><div class="content">This is a fascinating idea - although I wish the definition of search in the LLM context was expanded a bit more. What kind of search capability strapped onto current-gen LLMs would give them superpowers?</div><br/><div id="40685476" class="c"><input type="checkbox" id="c-40685476" checked=""/><div class="controls bullet"><span class="by">gwd</span><span>|</span><a href="#40685148">parent</a><span>|</span><a href="#40685521">next</a><span>|</span><label class="collapse" for="c-40685476">[-]</label><label class="expand" for="c-40685476">[4 more]</label></div><br/><div class="children"><div class="content">I think what may be confusing is that the author is using &quot;search&quot; here in the AI sense, not in the Google sense: that is, having an internal simulator of possible actions and possible reactions, like Stockfish&#x27;s chess move search (if I do A, it could do B C or D; if it does B, I can do E F or G, etc).<p>So think about the restrictions current LLMs have:<p>* They can&#x27;t sit and think about an answer; they can &quot;think out loud&quot;, but they have to start talking, and they can&#x27;t go back and say, &quot;No wait, that&#x27;s wrong, let&#x27;s start again.&quot;<p>* If they&#x27;re composing something, they can&#x27;t really go back and revise what they&#x27;ve written<p>* Sometimes they can look up reference material, but they can&#x27;t actually sit and digest it; they&#x27;re expected to skim it and then give an answer.<p>How would you perform under those circumstances?  If someone were to just come and ask you any question under the sun, and you had to just start talking, without taking any time to think about your answer, and without being able to say &quot;OK wait, let me go back&quot;?<p>I don&#x27;t know about you, but there&#x27;s no way I would be able to perform anywhere <i>close</i> to what ChatGPT 4 is able to do.  People complain that ChatGPT 4 is a &quot;bullshitter&quot;, but given its constraints that&#x27;s all you or I would be in the same situation -- but it&#x27;s already way, way better than I could ever be.<p>Given its limitations, ChatGPT is <i>phenomenal</i>.  So now imagine what it could do if it <i>were</i> given time to just &quot;sit and think&quot;?  To make a plan, to explore the possible solution space the same way that Stockfish does?  To take notes and revise and research and come back and think some more, before having to actually answer?<p>Reading this is honestly the first time in a while I&#x27;ve believed that some sort of &quot;AI foom&quot; might be possible.</div><br/><div id="40685948" class="c"><input type="checkbox" id="c-40685948" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#40685148">root</a><span>|</span><a href="#40685476">parent</a><span>|</span><a href="#40687004">next</a><span>|</span><label class="collapse" for="c-40685948">[-]</label><label class="expand" for="c-40685948">[2 more]</label></div><br/><div class="children"><div class="content">&gt; They can&#x27;t sit and think about an answer; they can &quot;think out loud&quot;, but they have to start talking, and they can&#x27;t go back and say, &quot;No wait, that&#x27;s wrong, let&#x27;s start again.&quot;<p>I mean, technically, they could say that.</div><br/><div id="40686676" class="c"><input type="checkbox" id="c-40686676" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40685148">root</a><span>|</span><a href="#40685948">parent</a><span>|</span><a href="#40687004">next</a><span>|</span><label class="collapse" for="c-40686676">[-]</label><label class="expand" for="c-40686676">[1 more]</label></div><br/><div class="children"><div class="content">Llama 3 does, it&#x27;s a funny design now, if you also throw in training to encourage CoT. Maybe more correct but verbosity can be grating<p>CoT
answer
Wait! No, that&#x27;s not right:
CoT...</div><br/></div></div></div></div><div id="40687004" class="c"><input type="checkbox" id="c-40687004" checked=""/><div class="controls bullet"><span class="by">fspeech</span><span>|</span><a href="#40685148">root</a><span>|</span><a href="#40685476">parent</a><span>|</span><a href="#40685948">prev</a><span>|</span><a href="#40685521">next</a><span>|</span><label class="collapse" for="c-40687004">[-]</label><label class="expand" for="c-40687004">[1 more]</label></div><br/><div class="children"><div class="content">&quot;How would you perform under those circumstances?&quot;
My son would recommend Improv classes.<p>&quot;Given its limitations, ChatGPT is phenomenal.&quot;
But this doesn&#x27;t translate since it learned everything from data and there is no data on &quot;sit and think&quot;.</div><br/></div></div></div></div><div id="40685521" class="c"><input type="checkbox" id="c-40685521" checked=""/><div class="controls bullet"><span class="by">cgearhart</span><span>|</span><a href="#40685148">parent</a><span>|</span><a href="#40685476">prev</a><span>|</span><a href="#40687965">next</a><span>|</span><label class="collapse" for="c-40685521">[-]</label><label class="expand" for="c-40685521">[1 more]</label></div><br/><div class="children"><div class="content">[1] applied AlphaZero style search with LLMs to achieve performance comparable to GPT-4 Turbo with a llama3-8B base model. However, what&#x27;s missing entirely from the paper (and the subject article in this thread) is that tree search is <i>massively</i> computationally expensive. It works well when the value function enables cutting out large portions of the search space, but the fact that the LLM version was limited to only 8 rollouts (I think it was 800 for AlphaZero) implies to me that the added complexity is not yet optimized or favorable for LLMs.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.07394" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.07394</a></div><br/></div></div></div></div><div id="40687965" class="c"><input type="checkbox" id="c-40687965" checked=""/><div class="controls bullet"><span class="by">amandasystems</span><span>|</span><a href="#40685148">prev</a><span>|</span><a href="#40684763">next</a><span>|</span><label class="collapse" for="c-40687965">[-]</label><label class="expand" for="c-40687965">[2 more]</label></div><br/><div class="children"><div class="content">This feels a lot like generation 3 AI throwing out all the insights from gens 1 and 2 and then rediscovering them from first principles, but itâs difficult to tell what this text is really about because it lumps together a lot of things into âsearchâ without fully describing what that means more formally.</div><br/><div id="40688007" class="c"><input type="checkbox" id="c-40688007" checked=""/><div class="controls bullet"><span class="by">PontifexMinimus</span><span>|</span><a href="#40687965">parent</a><span>|</span><a href="#40684763">next</a><span>|</span><label class="collapse" for="c-40688007">[-]</label><label class="expand" for="c-40688007">[1 more]</label></div><br/><div class="children"><div class="content">Indeed. It&#x27;s obvious what search means for a chess program -- its the future positions it looks at. But it&#x27;s less obvious to me what it means for an LLM.</div><br/></div></div></div></div><div id="40684763" class="c"><input type="checkbox" id="c-40684763" checked=""/><div class="controls bullet"><span class="by">johnthewise</span><span>|</span><a href="#40687965">prev</a><span>|</span><a href="#40686640">next</a><span>|</span><label class="collapse" for="c-40684763">[-]</label><label class="expand" for="c-40684763">[1 more]</label></div><br/><div class="children"><div class="content">What happened to all the chatter about Q*? I remember reading about this train&#x2F;test time trade-off back then, does anyone have good list of recent papers&#x2F;blogs about this? What is holding back this or openai is just running some model 10x longer to estimate what they would get if they trained with 10x?<p>This tweet is relevant:
<a href="https:&#x2F;&#x2F;x.com&#x2F;polynoamial&#x2F;status&#x2F;1676971503261454340" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;polynoamial&#x2F;status&#x2F;1676971503261454340</a></div><br/></div></div><div id="40686640" class="c"><input type="checkbox" id="c-40686640" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#40684763">prev</a><span>|</span><a href="#40687203">next</a><span>|</span><label class="collapse" for="c-40686640">[-]</label><label class="expand" for="c-40686640">[2 more]</label></div><br/><div class="children"><div class="content">The branching factor for chess is about 35.<p>For token generation, the branching factor depends on the tokenizer, but 32,000 is a common number.<p>Will search be as effective for LLMs when there are so many more possible branches?</div><br/><div id="40686747" class="c"><input type="checkbox" id="c-40686747" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40686640">parent</a><span>|</span><a href="#40687203">next</a><span>|</span><label class="collapse" for="c-40686747">[-]</label><label class="expand" for="c-40686747">[1 more]</label></div><br/><div class="children"><div class="content">You can pretty reasonably prune the tree by a factor of 1000... I think the problem that others have brought up - difficulty of the value function - is the more salient problem.</div><br/></div></div></div></div><div id="40687203" class="c"><input type="checkbox" id="c-40687203" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#40686640">prev</a><span>|</span><a href="#40687118">next</a><span>|</span><label class="collapse" for="c-40687203">[-]</label><label class="expand" for="c-40687203">[1 more]</label></div><br/><div class="children"><div class="content">How would search + LLMs work together in practice?<p>How about using search to derive facts from ontological models, and then writing out the discovered facts in English.  Then train the LLM on those English statements. Currently LLMs are trained on texts found on the internet mostly (only?). But information on the internet is often false and unreliable.<p>If instead we would have logically sound statements by the billions derived from ontological world-models then that might improve the performance of LLMs significantly.<p>Is something like this what the article or others are proposing? Give the LLM the facts, and the derived facts. Prioritize texts and statements we know and trust to be true. And even though we can&#x27;t write out too many true statements ourselves, a system that generated them by the billions by inference could.</div><br/></div></div><div id="40687118" class="c"><input type="checkbox" id="c-40687118" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#40687203">prev</a><span>|</span><a href="#40686174">next</a><span>|</span><label class="collapse" for="c-40687118">[-]</label><label class="expand" for="c-40687118">[2 more]</label></div><br/><div class="children"><div class="content">I wouldnât read too much into stockfish beating Leela Chess Zero. My calculator beats GPT-4 in matrix multiplication, doesnât mean we need to do what my calculator does in GPT-4 to make it smarter. Stockfish evaluates 70 million moves per second (or something in that ballpark). Chess is not such a complicated game that you arenât guaranteed to find the best move when you evaluate 70 million moves. Itâs why when there was an argument whether alpha zero really beat stockfish convincingly in Googleâs PR Stunt, a notable chess master quipped âEven god would not be able to beat stockfish this frequently.â , similarly god with all this magical powers would not beat my calculator at multiplication. It says more about the task than about the nature of intelligence.</div><br/><div id="40687323" class="c"><input type="checkbox" id="c-40687323" checked=""/><div class="controls bullet"><span class="by">Veedrac</span><span>|</span><a href="#40687118">parent</a><span>|</span><a href="#40686174">next</a><span>|</span><label class="collapse" for="c-40687323">[-]</label><label class="expand" for="c-40687323">[1 more]</label></div><br/><div class="children"><div class="content">People vastly underestimate god. Players aren&#x27;t just trying not to blunder, they&#x27;re trying to steer towards advantageous positions. Stockfish could play perfectly against itself every move 100 games in a row, in the classical sense of perfectly, as not in any move blundering the draw, and still be reliably exploited by an oracle.</div><br/></div></div></div></div><div id="40686174" class="c"><input type="checkbox" id="c-40686174" checked=""/><div class="controls bullet"><span class="by">TheRoque</span><span>|</span><a href="#40687118">prev</a><span>|</span><a href="#40685871">next</a><span>|</span><label class="collapse" for="c-40686174">[-]</label><label class="expand" for="c-40686174">[2 more]</label></div><br/><div class="children"><div class="content">The whole premise of this article is to compare the chess state of the art of 2019 with today, and then they start to talk about llms. But chess is a board with 64 squares and 32 pieces, it&#x27;s literally nothing compared to the real physical world. So I don&#x27;t get how this is relevant</div><br/><div id="40686778" class="c"><input type="checkbox" id="c-40686778" checked=""/><div class="controls bullet"><span class="by">dgoodell</span><span>|</span><a href="#40686174">parent</a><span>|</span><a href="#40685871">next</a><span>|</span><label class="collapse" for="c-40686778">[-]</label><label class="expand" for="c-40686778">[1 more]</label></div><br/><div class="children"><div class="content">Thatâs a good point. Imagine if an LLM could only read, speak, and hear at the same speed as a human. How long would training a model take?<p>We can make them read digital media really quickly, but we canât really accelerate its interactions with the physical world.</div><br/></div></div></div></div><div id="40685871" class="c"><input type="checkbox" id="c-40685871" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#40686174">prev</a><span>|</span><a href="#40684928">next</a><span>|</span><label class="collapse" for="c-40685871">[-]</label><label class="expand" for="c-40685871">[2 more]</label></div><br/><div class="children"><div class="content">Charlie Steiner pointed this out 5 years ago on Less Wrong:<p>&gt;If you train GPT-3 on a bunch of medical textbooks and prompt it to tell you a cure for Alzheimer&#x27;s, it won&#x27;t tell you a cure, it will tell you what humans have said about curing Alzheimer&#x27;s ... It would just tell you a plausible story about a situation related to the prompt about curing Alzheimer&#x27;s, based on its training data. Rather than a logical Oracle, this image-captioning-esque scheme would be an intuitive Oracle, telling you things that make sense based on associations already present within the training set.<p>&gt;What am I driving at here, by pointing out that curing Alzheimer&#x27;s is hard? It&#x27;s that the designs above are missing something, and what they&#x27;re missing is search. I&#x27;m not saying that getting a neural net to directly output your cure for Alzheimer&#x27;s is impossible. But it seems like it requires there to already be a &quot;cure for Alzheimer&#x27;s&quot; dimension in your learned model. The more realistic way to find the cure for Alzheimer&#x27;s, if you don&#x27;t already know it, is going to involve lots of logical steps one after another, slowly moving through a logical space, narrowing down the possibilities more and more, and eventually finding something that fits the bill. In other words, solving a search problem.<p>&gt;So if your AI can tell you how to cure Alzheimer&#x27;s, I think either it&#x27;s explicitly doing a search for how to cure Alzheimer&#x27;s (or worlds that match your verbal prompt the best, or whatever), or it has some internal state that implicitly performs a search.<p><a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;EMZeJ7vpfeF4GrWwm&#x2F;self-supervised-learning-and-agi-safety?commentId=vRfNdRe8Gzz9QhFYq" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;EMZeJ7vpfeF4GrWwm&#x2F;self-super...</a></div><br/><div id="40687576" class="c"><input type="checkbox" id="c-40687576" checked=""/><div class="controls bullet"><span class="by">lucb1e</span><span>|</span><a href="#40685871">parent</a><span>|</span><a href="#40684928">next</a><span>|</span><label class="collapse" for="c-40687576">[-]</label><label class="expand" for="c-40687576">[1 more]</label></div><br/><div class="children"><div class="content">Generalizing this (doing half a step away from GPT-specifics), would it be true to say the following?<p>&quot;If you train <i>your logic machine</i> on a bunch of medical textbooks and prompt it to tell you a cure for Alzheimer&#x27;s, it won&#x27;t tell you a cure, it will tell you what those textbooks have said about curing Alzheimer&#x27;s.&quot;<p>Because I suspect not. GPT seems mostly limited to regurgitating+remixing what it read, but other algorithms with better logic could be able to essentially do a meta study: take the results from all Alzheimer&#x27;s experiments we&#x27;ve done and narrow down the solution space to beyond what humans achieved so far. A human may not have the headspace to incorporate all relevant results at once whereas a computer might<p>Asking GPT to &quot;think step by step&quot; helps it, so clearly it has some form of this necessary logic, and it also performs well at &quot;here&#x27;s some data, transform it for me&quot;. It has limitations in both how good its logic is and the window across which it can do these transformations (but it can remember vastly more data from training than from the input token window, so perhaps that&#x27;s a partial workaround). Since it does have both capabilities, it does not seem insurmountable to extend it: I&#x27;m not sure we can rule out that an evolution of GPT can find Alzheimer&#x27;s cure within existing data, let alone a system even more suited to this task (still far short of needing AGI)<p>This requires the data to contain the necessary building blocks for a solution, but the quote seems to dismiss the option altogether even if the data did contain all information (but not yet the worked-out solution) for identifying a cure</div><br/></div></div></div></div><div id="40684928" class="c"><input type="checkbox" id="c-40684928" checked=""/><div class="controls bullet"><span class="by">fire_lake</span><span>|</span><a href="#40685871">prev</a><span>|</span><a href="#40685665">next</a><span>|</span><label class="collapse" for="c-40684928">[-]</label><label class="expand" for="c-40684928">[7 more]</label></div><br/><div class="children"><div class="content">I didnât understand this piece.<p>What do they mean by using LLMs with search? Is this simply RAG?</div><br/><div id="40685418" class="c"><input type="checkbox" id="c-40685418" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#40684928">parent</a><span>|</span><a href="#40686703">next</a><span>|</span><label class="collapse" for="c-40685418">[-]</label><label class="expand" for="c-40685418">[4 more]</label></div><br/><div class="children"><div class="content">âSearchâ here means trying a bunch of possibilities and seeing what works. Like how a sudoku solver or pathfinding algorithm does search, not how a search engine does.</div><br/><div id="40685463" class="c"><input type="checkbox" id="c-40685463" checked=""/><div class="controls bullet"><span class="by">fire_lake</span><span>|</span><a href="#40684928">root</a><span>|</span><a href="#40685418">parent</a><span>|</span><a href="#40686703">next</a><span>|</span><label class="collapse" for="c-40685463">[-]</label><label class="expand" for="c-40685463">[3 more]</label></div><br/><div class="children"><div class="content">But the domain of âAI Researchâ is broad and imprecise - not simple and discrete like chess game states. What is the type of each point in the search space for AI Research?</div><br/><div id="40685797" class="c"><input type="checkbox" id="c-40685797" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40684928">root</a><span>|</span><a href="#40685463">parent</a><span>|</span><a href="#40686703">next</a><span>|</span><label class="collapse" for="c-40685797">[-]</label><label class="expand" for="c-40685797">[2 more]</label></div><br/><div class="children"><div class="content">Well if we knew how to implement it, then we&#x27;d already have it eh?</div><br/><div id="40688180" class="c"><input type="checkbox" id="c-40688180" checked=""/><div class="controls bullet"><span class="by">fire_lake</span><span>|</span><a href="#40684928">root</a><span>|</span><a href="#40685797">parent</a><span>|</span><a href="#40686703">next</a><span>|</span><label class="collapse" for="c-40688180">[-]</label><label class="expand" for="c-40688180">[1 more]</label></div><br/><div class="children"><div class="content">In chess we know how to describe all possible board states and the transitions (the next moves). We just donât know which transition is the best to pick, hence itâs a well defined search problem.<p>With AI Research we donât even know the shape of the states and transitions, or even if thatâs an appropriate way to think about things.</div><br/></div></div></div></div></div></div></div></div><div id="40686703" class="c"><input type="checkbox" id="c-40686703" checked=""/><div class="controls bullet"><span class="by">tsaixingwei</span><span>|</span><a href="#40684928">parent</a><span>|</span><a href="#40685418">prev</a><span>|</span><a href="#40685367">next</a><span>|</span><label class="collapse" for="c-40686703">[-]</label><label class="expand" for="c-40686703">[1 more]</label></div><br/><div class="children"><div class="content">Given the example of Pfizer in the article, I would tend to agree with you that âsearchâ in this context means augmenting GPT with RAG of domain specific knowledge.</div><br/></div></div><div id="40685367" class="c"><input type="checkbox" id="c-40685367" checked=""/><div class="controls bullet"><span class="by">roca</span><span>|</span><a href="#40684928">parent</a><span>|</span><a href="#40686703">prev</a><span>|</span><a href="#40685665">next</a><span>|</span><label class="collapse" for="c-40685367">[-]</label><label class="expand" for="c-40685367">[1 more]</label></div><br/><div class="children"><div class="content">They mean something like the minmax algorithm used in game engines.</div><br/></div></div></div></div><div id="40685665" class="c"><input type="checkbox" id="c-40685665" checked=""/><div class="controls bullet"><span class="by">hartator</span><span>|</span><a href="#40684928">prev</a><span>|</span><a href="#40686517">next</a><span>|</span><label class="collapse" for="c-40685665">[-]</label><label class="expand" for="c-40685665">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the &quot;search&quot; space infinite though and impossible to qualify &quot;success&quot;?<p>You can&#x27;t just give LLMs infinite compute time and expect them to find answers for like &quot;cure cancer&quot;. Even chess moves that seem finite and success quantifiable are an also infinite problem and the best engines take &quot;shortcuts&quot; in their &quot;thinking&quot;. It&#x27;s impossible to do for real world problems.</div><br/><div id="40685925" class="c"><input type="checkbox" id="c-40685925" checked=""/><div class="controls bullet"><span class="by">cpill</span><span>|</span><a href="#40685665">parent</a><span>|</span><a href="#40686517">next</a><span>|</span><label class="collapse" for="c-40685925">[-]</label><label class="expand" for="c-40685925">[2 more]</label></div><br/><div class="children"><div class="content">the recent episode of Machine Learning Street Talk on control theory for LLMs sounds like it&#x27;s thinking in this direction.
Say you have 100k agents searching through research papers, and then trying every combination of them, 100k^2, to see if there is any synergy of ideas, and you keep doing this for all the successful combos... some of these might give the researchers some good ideas to try out. 
I can see it happening, if they can fine tune a model that becomes good at idea synergy. 
but then again real creativity is hard</div><br/><div id="40686486" class="c"><input type="checkbox" id="c-40686486" checked=""/><div class="controls bullet"><span class="by">Mehvix</span><span>|</span><a href="#40685665">root</a><span>|</span><a href="#40685925">parent</a><span>|</span><a href="#40686517">next</a><span>|</span><label class="collapse" for="c-40686486">[-]</label><label class="expand" for="c-40686486">[1 more]</label></div><br/><div class="children"><div class="content">How would one finetune for &quot;idea synergy&quot;?</div><br/></div></div></div></div></div></div><div id="40686517" class="c"><input type="checkbox" id="c-40686517" checked=""/><div class="controls bullet"><span class="by">dzonga</span><span>|</span><a href="#40685665">prev</a><span>|</span><a href="#40685258">next</a><span>|</span><label class="collapse" for="c-40686517">[-]</label><label class="expand" for="c-40686517">[2 more]</label></div><br/><div class="children"><div class="content">slight step aside - do people at notion realize, their own custom keyboard shortcuts break the habits built on the web.<p>cmd + p -- bring up their own custom dialog. instead of printing the page as one would expect</div><br/><div id="40686566" class="c"><input type="checkbox" id="c-40686566" checked=""/><div class="controls bullet"><span class="by">sherburt3</span><span>|</span><a href="#40686517">parent</a><span>|</span><a href="#40685258">next</a><span>|</span><label class="collapse" for="c-40686566">[-]</label><label class="expand" for="c-40686566">[1 more]</label></div><br/><div class="children"><div class="content">In VsCode cmd+p pulls up the file search dialog, I donât think itâs that crazy.</div><br/></div></div></div></div><div id="40685258" class="c"><input type="checkbox" id="c-40685258" checked=""/><div class="controls bullet"><span class="by">groby_b</span><span>|</span><a href="#40686517">prev</a><span>|</span><a href="#40686505">next</a><span>|</span><label class="collapse" for="c-40685258">[-]</label><label class="expand" for="c-40685258">[3 more]</label></div><br/><div class="children"><div class="content">While I respect the power of intuition - this may well be a great path - it&#x27;s worth keeping in mind that this is currently just that. A hunch. Leela got crushed due to AI directed search, what if we can wave a wand and hand all AIs search. Somehow. Magically. Which will then somehow magically trounce current LLMs at domain-specific task.<p>There&#x27;s a kernel of truth in there. See the papers on better results via monte carlo search trees (e.g. [1]). See mixture-of-LoRA&#x2F;LoRA-swarm approaches. (I swear there&#x27;s a startup using the approach of tons of domain-specific LoRAs, but my brain&#x27;s not yielding the name)<p>Augmenting LLM capabilities via _some_ sort of cheaper and more reliable exploration is likely a valid path. It&#x27;s not GPT-8 next year, though.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.03224" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.03224</a></div><br/><div id="40686325" class="c"><input type="checkbox" id="c-40686325" checked=""/><div class="controls bullet"><span class="by">memothon</span><span>|</span><a href="#40685258">parent</a><span>|</span><a href="#40686505">next</a><span>|</span><label class="collapse" for="c-40686325">[-]</label><label class="expand" for="c-40686325">[2 more]</label></div><br/><div class="children"><div class="content">Did you happen to remember the domain-specific LoRA startup?</div><br/><div id="40687651" class="c"><input type="checkbox" id="c-40687651" checked=""/><div class="controls bullet"><span class="by">hansonw</span><span>|</span><a href="#40685258">root</a><span>|</span><a href="#40686325">parent</a><span>|</span><a href="#40686505">next</a><span>|</span><label class="collapse" for="c-40687651">[-]</label><label class="expand" for="c-40687651">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40675577">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40675577</a></div><br/></div></div></div></div></div></div><div id="40686505" class="c"><input type="checkbox" id="c-40686505" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#40685258">prev</a><span>|</span><a href="#40686495">next</a><span>|</span><label class="collapse" for="c-40686505">[-]</label><label class="expand" for="c-40686505">[1 more]</label></div><br/><div class="children"><div class="content">This strikes me as Lesswrong style pontificating.</div><br/></div></div><div id="40686495" class="c"><input type="checkbox" id="c-40686495" checked=""/><div class="controls bullet"><span class="by">6510</span><span>|</span><a href="#40686505">prev</a><span>|</span><a href="#40686429">next</a><span>|</span><label class="collapse" for="c-40686495">[-]</label><label class="expand" for="c-40686495">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve recently matured to the point where all applications are made of 2 things, search and security. The rest is just things added on top. If you cant find it it isn&#x27;t worth having.</div><br/></div></div><div id="40684783" class="c"><input type="checkbox" id="c-40684783" checked=""/><div class="controls bullet"><span class="by">Kronopath</span><span>|</span><a href="#40686429">prev</a><span>|</span><a href="#40685246">next</a><span>|</span><label class="collapse" for="c-40684783">[-]</label><label class="expand" for="c-40684783">[6 more]</label></div><br/><div class="children"><div class="content">Anything that allows AI to scale to superinteligence quicker is going to run into AI alignment issues, since we donât really know a foolproof way of controlling AI. With the AI of today, this isnât too bad (the worst you get is stuff like AI confidently making up fake facts), but with a superintelligence this could be disastrous.<p>Itâs very irresponsible for this article to advocate and provide a pathway to immediate superintelligence (regardless of whether or not it actually works) without even discussing the question of how you figure out what youâre searching <i>for</i>, and how youâll prevent that superintelligence from being evil.</div><br/><div id="40685136" class="c"><input type="checkbox" id="c-40685136" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#40684783">parent</a><span>|</span><a href="#40686216">next</a><span>|</span><label class="collapse" for="c-40685136">[-]</label><label class="expand" for="c-40685136">[2 more]</label></div><br/><div class="children"><div class="content">Of course &quot;superintelligence&quot; is just a mythical creature at the moment, with no known path to get there, or even a specific proof of what it even means - usually it&#x27;s some hand waving about capabilities that sound magical, when IQ might very well be subject to diminishing returns.</div><br/><div id="40687980" class="c"><input type="checkbox" id="c-40687980" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#40684783">root</a><span>|</span><a href="#40685136">parent</a><span>|</span><a href="#40686216">next</a><span>|</span><label class="collapse" for="c-40687980">[-]</label><label class="expand" for="c-40687980">[1 more]</label></div><br/><div class="children"><div class="content">Do you mean no way to get there within realistic computation bounds? Because if we allow for arbitrarily high (but still finite) amounts of compute, then some computable approximation of AIXI should work fine.</div><br/></div></div></div></div><div id="40686216" class="c"><input type="checkbox" id="c-40686216" checked=""/><div class="controls bullet"><span class="by">aidan_mclau</span><span>|</span><a href="#40684783">parent</a><span>|</span><a href="#40685136">prev</a><span>|</span><a href="#40684968">next</a><span>|</span><label class="collapse" for="c-40686216">[-]</label><label class="expand" for="c-40686216">[2 more]</label></div><br/><div class="children"><div class="content">Hey! Essay author here.<p>&gt;The cool thing about using modern LLMs as an eval&#x2F;policy model is that their RLHF propagates throughout the search.<p>&gt;Moreover, if search techniques work on the token level (likely), their thoughts are perfectly interpretable.<p>I suspect a search world is substantially more alignment-friendly than a large model world.
Let me know your thoughts!</div><br/><div id="40688069" class="c"><input type="checkbox" id="c-40688069" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#40684783">root</a><span>|</span><a href="#40686216">parent</a><span>|</span><a href="#40684968">next</a><span>|</span><label class="collapse" for="c-40688069">[-]</label><label class="expand" for="c-40688069">[1 more]</label></div><br/><div class="children"><div class="content">Your webpage is broken for me. The page appears briefly, then there&#x27;s a french error message telling me that an error occured and i can retry.<p>Mobile Safari, phone set to french.</div><br/></div></div></div></div><div id="40684968" class="c"><input type="checkbox" id="c-40684968" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#40684783">parent</a><span>|</span><a href="#40686216">prev</a><span>|</span><a href="#40685246">next</a><span>|</span><label class="collapse" for="c-40684968">[-]</label><label class="expand" for="c-40684968">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think your response is appropriate.  Narrow domain &quot;superintelligence&quot; is around us everywhere-- every PID controller can drive a process to its target far beyond any human capability.<p>The obvious way to incorporate good search is to have extremely fast models that are being used in the search interior loop.  Such models would be inherently less general, and likely trained on the specific problem or at least domain-- just for performance sake.  The lesson in this article was that a tiny superspecialized model inside a powerful transitional search framework significantly outperformed a much larger more general model.<p>Use of explicit external search should make the optimization system&#x27;s behavior and objective more transparent and tractable than just sampling the output of an auto-regressive model alone.  If nothing else you can at least look at the branches it did and didn&#x27;t explore.   It&#x27;s also a design that&#x27;s more easy to bolt in varrious kinds of regularizes, code to steer it away from parts of the search space you don&#x27;t want it operating in.<p>The irony of all the AI scaremongering is that if there is ever some evil AI with some LLM as an important part of its reasoning process if it is evil it may well be so because being evil is a big part of the narrative it was trained on.  :D</div><br/></div></div></div></div><div id="40685246" class="c"><input type="checkbox" id="c-40685246" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#40684783">prev</a><span>|</span><label class="collapse" for="c-40685246">[-]</label><label class="expand" for="c-40685246">[8 more]</label></div><br/><div class="children"><div class="content">The problem with adding &quot;search&quot; to a model is that the model has already seen everything to be &quot;search&quot;ed in its training data. There is nothing left.<p>Imagine if Leela (author&#x27;s example) had been trained on every chess board position out there (I know it&#x27;s mathematically impossible, but bear with me for a second). If Leela had been trained on every board position, it may have whupped Stockfish. So, adding &quot;search&quot; to Leela would have been pointless, since it would have seen every board position out there.<p>Today&#x27;s LLMs are trained on every word ever written on the &#x27;net, every word ever put down in a book, every word uttered in a video on Youtube or a podcast.</div><br/><div id="40685265" class="c"><input type="checkbox" id="c-40685265" checked=""/><div class="controls bullet"><span class="by">yousif_123123</span><span>|</span><a href="#40685246">parent</a><span>|</span><a href="#40685801">next</a><span>|</span><label class="collapse" for="c-40685265">[-]</label><label class="expand" for="c-40685265">[1 more]</label></div><br/><div class="children"><div class="content">Still, similar to when you have read 10 textbooks, if you are answering a question and have access to the source material, it can help you in your answer.</div><br/></div></div><div id="40685801" class="c"><input type="checkbox" id="c-40685801" checked=""/><div class="controls bullet"><span class="by">salamo</span><span>|</span><a href="#40685246">parent</a><span>|</span><a href="#40685265">prev</a><span>|</span><a href="#40686496">next</a><span>|</span><label class="collapse" for="c-40685801">[-]</label><label class="expand" for="c-40685801">[1 more]</label></div><br/><div class="children"><div class="content">If the game was small enough to memorize, like tic tac toe, you could definitely train a neural net to 100% accuracy. I&#x27;ve done it, it works.<p>The problem is that for most of the interesting problems out there, it isn&#x27;t possible to see every possibility let alone memorize it.</div><br/></div></div><div id="40686496" class="c"><input type="checkbox" id="c-40686496" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#40685246">parent</a><span>|</span><a href="#40685801">prev</a><span>|</span><a href="#40685264">next</a><span>|</span><label class="collapse" for="c-40686496">[-]</label><label class="expand" for="c-40686496">[1 more]</label></div><br/><div class="children"><div class="content">you are making the mistake of thinking that &#x27;search&#x27; means database search, like google or sqlite, but &#x27;search&#x27; in the ai context means tree search, like a* or tabu search. the spaces that tree search searches are things like all possible chess games, not all chess games ever played, which is a smaller space by a factor much greater than the number of atoms in the universe</div><br/></div></div><div id="40685286" class="c"><input type="checkbox" id="c-40685286" checked=""/><div class="controls bullet"><span class="by">groby_b</span><span>|</span><a href="#40685246">parent</a><span>|</span><a href="#40685264">prev</a><span>|</span><label class="collapse" for="c-40685286">[-]</label><label class="expand" for="c-40685286">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re omitting the somewhat relevant part of recall ability. I can train a 50 parameter model on the entire internet, and while it&#x27;s seen it all, it won&#x27;t be able to recall it. (You can likely do the same thing with a 500B model for similar results, though it&#x27;s getting somewhat closer to decent recall)<p>The whole point of deep learning is that the model learns to generalize. It&#x27;s not to have a perfect storage engine with a human language query frontend.</div><br/><div id="40685512" class="c"><input type="checkbox" id="c-40685512" checked=""/><div class="controls bullet"><span class="by">sebastos</span><span>|</span><a href="#40685246">root</a><span>|</span><a href="#40685286">parent</a><span>|</span><label class="collapse" for="c-40685512">[-]</label><label class="expand" for="c-40685512">[2 more]</label></div><br/><div class="children"><div class="content">Fully agree, although itâs interesting to consider the perspective that the entire LLM hype cycle is largely built around the question âwhat if we punted on actual thinking and instead just tried to memorize everything and then provide a human language query frontend? Is that still useful?â Arguably it is (sorta), and thatâs what is driving this latest zeitgeist. Compute had quietly scaled in the background while we were banging our heads against real thinking, until one day we looked up and we still didnât have a thinking machine, but it was now approximately possible to just do the stupid thing and store âall the text on the internetâ in a lookup table, where the keys are prompts. Thatâsâ¦ the opposite of thinking, really, but still sometimes useful!<p>Although to be clear I think actual reasoning systems are what we should be trying to create, and this LLM stuff seems like a cul-de-sac on that journey.</div><br/><div id="40686025" class="c"><input type="checkbox" id="c-40686025" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#40685246">root</a><span>|</span><a href="#40685512">parent</a><span>|</span><label class="collapse" for="c-40686025">[-]</label><label class="expand" for="c-40686025">[1 more]</label></div><br/><div class="children"><div class="content">The thing is that current chat tools forgo the source material. A proper set of curated keywords can give you a less computational intensive search.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>