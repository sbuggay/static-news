<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704877265414" as="style"/><link rel="stylesheet" href="styles.css?v=1704877265414"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.quantamagazine.org/magical-error-correction-scheme-proved-inherently-inefficient-20240109/">&#x27;Magical&#x27; Error Correction Scheme Proved Inherently Inefficient</a> <span class="domain">(<a href="https://www.quantamagazine.org">www.quantamagazine.org</a>)</span></div><div class="subtext"><span>digital55</span> | <span>15 comments</span></div><br/><div><div id="38937855" class="c"><input type="checkbox" id="c-38937855" checked=""/><div class="controls bullet"><span class="by">jwr</span><span>|</span><a href="#38935314">next</a><span>|</span><label class="collapse" for="c-38937855">[-]</label><label class="expand" for="c-38937855">[1 more]</label></div><br/><div class="children"><div class="content">The article didn&#x27;t mention erasure coding (fountain&#x2F;raptor codes) and I&#x27;m wondering if the significance of this result applies to their usability or not.</div><br/></div></div><div id="38935314" class="c"><input type="checkbox" id="c-38935314" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#38937855">prev</a><span>|</span><a href="#38933665">next</a><span>|</span><label class="collapse" for="c-38935314">[-]</label><label class="expand" for="c-38935314">[1 more]</label></div><br/><div class="children"><div class="content">This is a really interesting article for anyone interested in error correcting codes.<p>My TL;DR is that there are some codes that are able to reconstruct data with a fixed number of queries (i.e., limiting the fan out of reads for reconstruction). There are two types of these, locally correctable and locally decodable codes. Locally decodable codes allow reconstructing any part of the original message with a fixed number of reads, while locally correctable codes allow reconstructing parts of the code words in the same way.<p>In particular, the article discusses two query and three queries. The best known algorithms in the past for both of these (two and three queries) for either locally correctable codes or locally decodable codes was the Reed-Muller code, however these codes are exponential in length wrt the length of the original message. In 2003 it was found that it&#x27;s not possible to do better for two queries than Reed-Muller codes, but in ~2009, Yekhanin and Efremenko independently discovered algorithms that allow for locally decodable three query codes shorter than this (though not linear in length, I think).<p>Now, it&#x27;s been shown that there are no locally correctable three query codes that are better than Reed-Muller in length, using a technique based on satisfiability.</div><br/></div></div><div id="38933665" class="c"><input type="checkbox" id="c-38933665" checked=""/><div class="controls bullet"><span class="by">x1f604</span><span>|</span><a href="#38935314">prev</a><span>|</span><a href="#38934129">next</a><span>|</span><label class="collapse" for="c-38933665">[-]</label><label class="expand" for="c-38933665">[10 more]</label></div><br/><div class="children"><div class="content">&gt; Consider a company that stores users’ emails in the cloud — that is, on a vast array of servers. You can think of the whole collection of emails as one long message. Now suppose one server crashes. With a Reed-Solomon code, you’d need to perform a massive computation involving all the encoded data to recover your emails from that one lost server. “You would have to look at everything,” said Zeev Dvir, a computer scientist at Princeton University. “That could be billions and billions of emails — it could take a really long time.”<p>I have to take issue with the above characterization. It seems to imply that a server crash means the user has to wait for the data to be reconstructed, or that it will necessarily take a long time for the data to be reconstructed. But I don&#x27;t think either of these claims are true in the general case.<p>We can look at Backblaze for a real world example of how an actual file storage company uses Reed-Solomon for error correction:<p>&gt; Every file uploaded to a Backblaze Vault is broken into pieces before being stored. Each of those pieces is called a “shard.” Parity shards are added to add redundancy so that a file can be fetched from a Backblaze Vault even if some of the pieces are not available.<p>&gt; Each file is stored as 20 shards: 17 data shards and three parity shards. Because those shards are distributed across 20 storage pods in 20 cabinets, the Backblaze Vault is resilient to the failure of a storage pod, power loss to an entire cabinet, or even a cabinet-level networking outage.<p>&gt; Files can be written to the Backblaze Vault when one pod is down, and still have two parity shards to protect the data. Even in the extreme and unlikely case where three storage pods in a Backblaze Vault are offline, the files in the vault are still available because they can be reconstructed from the 17 pieces that are available.<p>So BackBlaze splits each file into 20 shards, with 3 of those being parity shards so that only 17 out of 20 shards are necessary to reconstruct the original file.<p>Regardless of whether you store each email in a separate file, or if you store all your emails in one giant file, the point is that your emails will be divided into 20 pieces across 20 separate physical machines, so that the loss of any one machine (or even an entire cabinet) will not impact your access to your emails.<p>I would be extremely surprised if any real company that was actually in the business of storing user data (e.g. AWS, Azure, GCP, Backblaze etc) would store user data in such a way that the crash of a single server would require a &quot;really long time&quot; for the user data to be recovered. Rather, I think it&#x27;s most likely that the loss of a single server should not have any noticeable impact on the time that it takes for a user to access the data that was stored on that server.<p>As for the second claim, I don&#x27;t think it should take &quot;a really long time&quot; to recover even billions of emails. I know that (depending on the parameters) the Intel ISA-L Reed-Solomon implementation can achieve a throughput of multiple GB&#x2F;s on a single core. So even if you were storing all your emails in a single, really huge file that was tens of gigabytes in size, it still shouldn&#x27;t take more than a few minutes to recover it from the available shards and to regenerate the shard that was lost.</div><br/><div id="38933839" class="c"><input type="checkbox" id="c-38933839" checked=""/><div class="controls bullet"><span class="by">22c</span><span>|</span><a href="#38933665">parent</a><span>|</span><a href="#38935945">next</a><span>|</span><label class="collapse" for="c-38933839">[-]</label><label class="expand" for="c-38933839">[1 more]</label></div><br/><div class="children"><div class="content">I agree that it&#x27;s a strange example to give. This would also be like saying imagine you had a single Reed-Solomon code for your entire hard drive. That would indeed be very painful to recover data, but we don&#x27;t have a single Reed-Solomon code for hard drives. You&#x27;d pick a block size that is suitable for your application.</div><br/></div></div><div id="38935945" class="c"><input type="checkbox" id="c-38935945" checked=""/><div class="controls bullet"><span class="by">shermantanktop</span><span>|</span><a href="#38933665">parent</a><span>|</span><a href="#38933839">prev</a><span>|</span><a href="#38933759">next</a><span>|</span><label class="collapse" for="c-38935945">[-]</label><label class="expand" for="c-38935945">[1 more]</label></div><br/><div class="children"><div class="content">The article offered that example as an extreme, impractical, but easy-to-imagine case to show the utility of using codes over smaller data segments.  I read this article as a discussion about data entropy, data encoding, and information theory.<p>Nowhere did they suggest that concatenating zillions of emails could be a real world system, or that such a system would be good or practical, or that any actual real system used this approach.<p>What you describe with Backblaze is using redundant storage to sidestep the problem, so it&#x27;s apples and oranges.</div><br/></div></div><div id="38933759" class="c"><input type="checkbox" id="c-38933759" checked=""/><div class="controls bullet"><span class="by">vaidhy</span><span>|</span><a href="#38933665">parent</a><span>|</span><a href="#38935945">prev</a><span>|</span><a href="#38934269">next</a><span>|</span><label class="collapse" for="c-38933759">[-]</label><label class="expand" for="c-38933759">[1 more]</label></div><br/><div class="children"><div class="content">There is theoretical inefficiency and practical inefficiency. Something might be O(n^3).. but if your n is small (as in backblaze case, where you do it on a file by file basis, rather than for your filesystem), it is still useful.<p>In other cases, your optimal algorithm might have a large constant cost (setup cost etc) which for small n might make it practically inefficient. n^2+c1 and n^3 + c2, but c2 &gt;&gt;&gt; c1 happens a lot.</div><br/></div></div><div id="38934269" class="c"><input type="checkbox" id="c-38934269" checked=""/><div class="controls bullet"><span class="by">rcxdude</span><span>|</span><a href="#38933665">parent</a><span>|</span><a href="#38933759">prev</a><span>|</span><a href="#38933800">next</a><span>|</span><label class="collapse" for="c-38934269">[-]</label><label class="expand" for="c-38934269">[1 more]</label></div><br/><div class="children"><div class="content">This would be true if you were to optimize for the very extreme case of running an error correction code over all of your data at once. This would give you the absolute best case tradeoff between redundancy and data storage, but would be completely intractable to actually compute, which is the point they are making. In practice error correction is used over smaller fragments of data, which is tractable but also doesn&#x27;t give you as good a tradeoff (i.e. you need to spend more extra space to get the same level of redundancy). From what I understand one of the appeals of the codes mentioned in the article is that it might be tractable to use them in the manner described, in which case you might only need, say 3 extra servers out of thousands in order to lose any three, as opposed to 3 extra out of 20. But it seems like it is not likely.<p>(In practice, I would say existing error correction codes already get you very close to the theoretical limit of this tradeoff already. The fact that these &#x27;magical&#x27; codes don&#x27;t work is not so much of a loss in comparison. While they would perhaps be better, they would not be drastically better).</div><br/></div></div><div id="38933800" class="c"><input type="checkbox" id="c-38933800" checked=""/><div class="controls bullet"><span class="by">Joker_vD</span><span>|</span><a href="#38933665">parent</a><span>|</span><a href="#38934269">prev</a><span>|</span><a href="#38934129">next</a><span>|</span><label class="collapse" for="c-38933800">[-]</label><label class="expand" for="c-38933800">[5 more]</label></div><br/><div class="children"><div class="content">Does it mean that when Blackblaze needs to retrieve me my file, it has to issue 20 parallel network requests, wait for at least 17 of them to complete, then combine the responses into the requested file and only <i>then</i> it can start streaming it to me? That seems kinda bad for latency.</div><br/><div id="38936795" class="c"><input type="checkbox" id="c-38936795" checked=""/><div class="controls bullet"><span class="by">Twirrim</span><span>|</span><a href="#38933665">root</a><span>|</span><a href="#38933800">parent</a><span>|</span><a href="#38933917">next</a><span>|</span><label class="collapse" for="c-38936795">[-]</label><label class="expand" for="c-38936795">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you pay a cost for latency, but you get a phenomal amount of durability at much lower stretch factor.<p>If they make sure they no two shards occupy the same hard disk, they could lose up to three hard disks with your data shared on it and <i>still</i> be able to recreate it. Even if they lose just one, they can immediately reproduce that now missing shard from what they already have. So really you&#x27;d need to talk losing 4 hard disks, each with a shard on, nearly simultaneously.<p>So that&#x27;s roughly the same durability as you&#x27;d get storing 4 copies of the same file. Except in this case it&#x27;s storing just 1.15x the size of the original file (20:17 ratio).  So for every megabyte you store, you need 1.15 megabytes of space instead of 4 megabytes.<p>The single biggest cost for storage services is not hardware, it&#x27;s the per rack operational costs, by a long, long stretch. Erasure encoding is the current best way to keep that stretch factor low, and costs under control.<p>If you think about the different types of storage needs there are, and access speed desires, it&#x27;s even practical to use much higher ratios. You could, for example, choose 40:34 and get similar resilience to as if you had 8x copies of the file, while still at a 1.15x stretch factor. You just have that draw back of needing to fetch 34 shards at access time. If you want to keep that 4x resilience that could be 40:36 which nets you a nice 1.11x stretch factor. If you had just 1 petabyte of storage, that 0.03 savings would be 30 terabytes, a good chunk of a single server.</div><br/></div></div><div id="38933917" class="c"><input type="checkbox" id="c-38933917" checked=""/><div class="controls bullet"><span class="by">22c</span><span>|</span><a href="#38933665">root</a><span>|</span><a href="#38933800">parent</a><span>|</span><a href="#38936795">prev</a><span>|</span><a href="#38933936">next</a><span>|</span><label class="collapse" for="c-38933917">[-]</label><label class="expand" for="c-38933917">[1 more]</label></div><br/><div class="children"><div class="content">No, you are confusing file retrieval with file recovery. The reconstruction only needs to happen if some form corruption is detected (typically in the case of a bad&#x2F;dead disk).</div><br/></div></div><div id="38933936" class="c"><input type="checkbox" id="c-38933936" checked=""/><div class="controls bullet"><span class="by">x1f604</span><span>|</span><a href="#38933665">root</a><span>|</span><a href="#38933800">parent</a><span>|</span><a href="#38933917">prev</a><span>|</span><a href="#38934856">next</a><span>|</span><label class="collapse" for="c-38933936">[-]</label><label class="expand" for="c-38933936">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know exactly how Backblaze does it, but in the normal case, reconstruction is not computationally expensive because the 17 data shards are just pieces of the original file that can be served directly to users.<p>It&#x27;s only when a data shard is lost that computation is necessary to regenerate it using the parity shards.</div><br/></div></div><div id="38934856" class="c"><input type="checkbox" id="c-38934856" checked=""/><div class="controls bullet"><span class="by">sargun</span><span>|</span><a href="#38933665">root</a><span>|</span><a href="#38933800">parent</a><span>|</span><a href="#38933936">prev</a><span>|</span><a href="#38934129">next</a><span>|</span><label class="collapse" for="c-38934856">[-]</label><label class="expand" for="c-38934856">[1 more]</label></div><br/><div class="children"><div class="content">This is actually better for latency, perhaps counter intuitively. Let’s say that each server experiences some high latency requests. Normally, if one server stored that file, you’d get high latency, this scheme on the other hand cuts down on overall latency</div><br/></div></div></div></div></div></div><div id="38934129" class="c"><input type="checkbox" id="c-38934129" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#38933665">prev</a><span>|</span><a href="#38932519">next</a><span>|</span><label class="collapse" for="c-38934129">[-]</label><label class="expand" for="c-38934129">[1 more]</label></div><br/><div class="children"><div class="content">One of my favorite talks on this subject: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xE4jEKx9fTM" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xE4jEKx9fTM</a></div><br/></div></div></div></div></div></div></div></body></html>