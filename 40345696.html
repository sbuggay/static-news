<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715677264332" as="style"/><link rel="stylesheet" href="styles.css?v=1715677264332"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/pipecat-ai/pipecat">Show HN: An open source framework for voice assistants</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>kwindla</span> | <span>33 comments</span></div><br/><div><div id="40346934" class="c"><input type="checkbox" id="c-40346934" checked=""/><div class="controls bullet"><span class="by">awenix</span><span>|</span><a href="#40346398">next</a><span>|</span><label class="collapse" for="c-40346934">[-]</label><label class="expand" for="c-40346934">[4 more]</label></div><br/><div class="children"><div class="content">Nice to see an open source implementation, i have been seeing many startups get into this space like <a href="https:&#x2F;&#x2F;www.retellai.com&#x2F;">https:&#x2F;&#x2F;www.retellai.com&#x2F;</a>, <a href="https:&#x2F;&#x2F;fixie.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;fixie.ai&#x2F;</a> etc. They always end up needing speech-to-speech models (current approach seems speech-text-text-speech with multiple agents handling 1 listening + 1 speaking), excited to see how this plays with recently announced gpt-4o</div><br/><div id="40347672" class="c"><input type="checkbox" id="c-40347672" checked=""/><div class="controls bullet"><span class="by">kwindla</span><span>|</span><a href="#40346934">parent</a><span>|</span><a href="#40350643">next</a><span>|</span><label class="collapse" for="c-40347672">[-]</label><label class="expand" for="c-40347672">[1 more]</label></div><br/><div class="children"><div class="content">Adding to your list: <a href="https:&#x2F;&#x2F;vapi.ai">https:&#x2F;&#x2F;vapi.ai</a>  -- really nice tools.<p>(I try to keep up with all the different layers&#x2F;players in this space.)</div><br/></div></div><div id="40350643" class="c"><input type="checkbox" id="c-40350643" checked=""/><div class="controls bullet"><span class="by">j-bos</span><span>|</span><a href="#40346934">parent</a><span>|</span><a href="#40347672">prev</a><span>|</span><a href="#40348900">next</a><span>|</span><label class="collapse" for="c-40350643">[-]</label><label class="expand" for="c-40350643">[1 more]</label></div><br/><div class="children"><div class="content">How do speech to speech models work? Do they just that many more tokens to capture nuances of spoken language?</div><br/></div></div><div id="40348900" class="c"><input type="checkbox" id="c-40348900" checked=""/><div class="controls bullet"><span class="by">zkoch</span><span>|</span><a href="#40346934">parent</a><span>|</span><a href="#40350643">prev</a><span>|</span><a href="#40346398">next</a><span>|</span><label class="collapse" for="c-40348900">[-]</label><label class="expand" for="c-40348900">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re (fixie.ai) working on on our SLM (speech language model). We&#x27;ll release something soon to play with :)</div><br/></div></div></div></div><div id="40346398" class="c"><input type="checkbox" id="c-40346398" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40346934">prev</a><span>|</span><a href="#40346465">next</a><span>|</span><label class="collapse" for="c-40346398">[-]</label><label class="expand" for="c-40346398">[7 more]</label></div><br/><div class="children"><div class="content">This is great but we really need an audio-to-audio model like they demoed in the open source world. Does anyone know of anything like that?<p>Edit: someone found one: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40346992">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40346992</a></div><br/><div id="40346494" class="c"><input type="checkbox" id="c-40346494" checked=""/><div class="controls bullet"><span class="by">makeitmore</span><span>|</span><a href="#40346398">parent</a><span>|</span><a href="#40346532">next</a><span>|</span><label class="collapse" for="c-40346494">[-]</label><label class="expand" for="c-40346494">[2 more]</label></div><br/><div class="children"><div class="content">Most of the Pipecat examples we&#x27;ve been working on are focused on speech-to-speech. The examples guide you through how to do that (or you can give the hosted  storytelling example a try: <a href="https:&#x2F;&#x2F;storytelling-chatbot.fly.dev&#x2F;" rel="nofollow">https:&#x2F;&#x2F;storytelling-chatbot.fly.dev&#x2F;</a>)<p>We should probably update the example in the README to better represent that, thank you!</div><br/><div id="40346529" class="c"><input type="checkbox" id="c-40346529" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40346398">root</a><span>|</span><a href="#40346494">parent</a><span>|</span><a href="#40346532">next</a><span>|</span><label class="collapse" for="c-40346529">[-]</label><label class="expand" for="c-40346529">[1 more]</label></div><br/><div class="children"><div class="content">Your project is amazing and I&#x27;m not trying to take away from what you have accomplished.<p>But..I looked at the code but didn&#x27;t see any audio-to-audio service or model. Can you link to an example of that?<p>I don&#x27;t mean speech to text to LLM to text to speech. I mean speech-to-speech directly, as in the ML model takes audio as input and outputs audio. As they have now in OpenAI.<p>I am very familiar with the typical multi-model workflow and have implemented it several times.</div><br/></div></div></div></div><div id="40346532" class="c"><input type="checkbox" id="c-40346532" checked=""/><div class="controls bullet"><span class="by">kwindla</span><span>|</span><a href="#40346398">parent</a><span>|</span><a href="#40346494">prev</a><span>|</span><a href="#40346465">next</a><span>|</span><label class="collapse" for="c-40346532">[-]</label><label class="expand" for="c-40346532">[4 more]</label></div><br/><div class="children"><div class="content">An audio-to-audio model is definitely a step forward. And I do think that&#x27;s where things are going to go, generally speaking.<p>For context relating to real-time voice AI: once you&#x27;re down below ~800ms things are fast enough to feel naturally responsive for most people and use cases.<p>The GPT-4o announcement page says they average ~320ms time to first token from an audio prompt. Which is definitely next level and is really, really exciting. You can&#x27;t get to 800ms with any pipeline that includes GPT-4 Turbo today, so this is a big deal.<p>It&#x27;s possible to do ~500ms time to first token by pipelining today&#x27;s fastest transcription, inference, and tts models. (For example, Deepgram transcription, Groq Llama-3, Deepgram Aura voices.)</div><br/><div id="40349455" class="c"><input type="checkbox" id="c-40349455" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#40346398">root</a><span>|</span><a href="#40346532">parent</a><span>|</span><a href="#40350509">next</a><span>|</span><label class="collapse" for="c-40349455">[-]</label><label class="expand" for="c-40349455">[1 more]</label></div><br/><div class="children"><div class="content">Every opening phrase is a platitude like ‘sure let’s do it’. So the OpenAI latency is probably higher, they are just using clever orchestration to generate some filler tokens to make latency lower. Unlikely the initial response at OpenAI is coming from the main model.</div><br/></div></div><div id="40350509" class="c"><input type="checkbox" id="c-40350509" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#40346398">root</a><span>|</span><a href="#40346532">parent</a><span>|</span><a href="#40349455">prev</a><span>|</span><a href="#40346555">next</a><span>|</span><label class="collapse" for="c-40350509">[-]</label><label class="expand" for="c-40350509">[1 more]</label></div><br/><div class="children"><div class="content">Gazelle is (or will be) significantly faster than that.</div><br/></div></div><div id="40346555" class="c"><input type="checkbox" id="c-40346555" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40346398">root</a><span>|</span><a href="#40346532">parent</a><span>|</span><a href="#40350509">prev</a><span>|</span><a href="#40346465">next</a><span>|</span><label class="collapse" for="c-40346555">[-]</label><label class="expand" for="c-40346555">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m familiar with Deepgram, groq, and Eleven Labs. I have recently built something on those and it&#x27;s really not too bad as far as latency. But OpenAI has shown that audio-to-audio can&#x27;t be beat.</div><br/></div></div></div></div></div></div><div id="40346465" class="c"><input type="checkbox" id="c-40346465" checked=""/><div class="controls bullet"><span class="by">johnmaguire</span><span>|</span><a href="#40346398">prev</a><span>|</span><a href="#40351499">next</a><span>|</span><label class="collapse" for="c-40346465">[-]</label><label class="expand" for="c-40346465">[10 more]</label></div><br/><div class="children"><div class="content">Siri came out in October 2011. Amazon Alexa made its debut in November 2014. Google Assistant&#x27;s voice-activated speakers were released in May 2016.<p>From what I can tell, Siri is still a dumpster fire that nobody is willing to use. And I have no personal experience with Alexa, so I can&#x27;t speak to it. But I do have a few Google Home speakers and an Android phone, and I have seen no major improvements in years. In fact, it has gotten worse - for example, you can no longer add items directly to AnyList[0], only Google Keep.<p>Or, as an incredibly simple example of something I thought we&#x27;d get a long time ago, it&#x27;s still unable to interpret two-part requests, e.g. &quot;please repeat that but louder,&quot; or &quot;please turn off the kitchen and dining room lights.&quot;<p>I find voice assistants very useful - especially when driving, lying in bed, cooking, or when I&#x27;m otherwise preoccupied. Yet they have stagnated almost since their debut. I can only imagine nobody has found a viable way to monetize them.<p>What will it take to get a better voice assistant for consumers? Willow[1] doesn&#x27;t seem to have taken off.<p>[0] <a href="https:&#x2F;&#x2F;help.anylist.com&#x2F;articles&#x2F;google-assistant-overview&#x2F;" rel="nofollow">https:&#x2F;&#x2F;help.anylist.com&#x2F;articles&#x2F;google-assistant-overview&#x2F;</a><p>[1] <a href="https:&#x2F;&#x2F;heywillow.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;heywillow.io&#x2F;</a><p>edit: I realize I hijacked your thread to dump something that&#x27;s been on my mind lately. Pipecat looks really cool, and I hope it takes off! I hope to get some time to experiment this weekend.</div><br/><div id="40346996" class="c"><input type="checkbox" id="c-40346996" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#40346465">parent</a><span>|</span><a href="#40347515">next</a><span>|</span><label class="collapse" for="c-40346996">[-]</label><label class="expand" for="c-40346996">[3 more]</label></div><br/><div class="children"><div class="content">For some activities, Siri is just fine. Thinks like “send a text to x” and “remind me to do x when I get home”.<p>And it does fine with no internet access.<p>Except dictation. Much better with internet access than without.</div><br/><div id="40347059" class="c"><input type="checkbox" id="c-40347059" checked=""/><div class="controls bullet"><span class="by">johnmaguire</span><span>|</span><a href="#40346465">root</a><span>|</span><a href="#40346996">parent</a><span>|</span><a href="#40347515">next</a><span>|</span><label class="collapse" for="c-40347059">[-]</label><label class="expand" for="c-40347059">[2 more]</label></div><br/><div class="children"><div class="content">Those are about as basic of an action as you can get. Every assistant supports them. But as soon as you want to know something like &quot;how many teaspoons in a cup,&quot; can Siri still handle it? What about &quot;where is the aurora borealis visible tonight&quot;?<p>Another issue Siri used to struggle was trying to play specific music on Spotify. Is that better these days?</div><br/><div id="40351001" class="c"><input type="checkbox" id="c-40351001" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#40346465">root</a><span>|</span><a href="#40347059">parent</a><span>|</span><a href="#40347515">next</a><span>|</span><label class="collapse" for="c-40351001">[-]</label><label class="expand" for="c-40351001">[1 more]</label></div><br/><div class="children"><div class="content">I asked, it said 48 teaspoons.<p>I asked to play a song my an artist on Spotify and it did it. Popped up in the Dynamic Island.<p>Honestly, I didn’t know it could do that!<p>(It did ask permission to access Spotify data first, but only that first time).</div><br/></div></div></div></div></div></div><div id="40347515" class="c"><input type="checkbox" id="c-40347515" checked=""/><div class="controls bullet"><span class="by">michaelmior</span><span>|</span><a href="#40346465">parent</a><span>|</span><a href="#40346996">prev</a><span>|</span><a href="#40349345">next</a><span>|</span><label class="collapse" for="c-40347515">[-]</label><label class="expand" for="c-40347515">[1 more]</label></div><br/><div class="children"><div class="content">I primarily use Google Home, but I do also have Echo Frames so I use Alexa semi-regularly. My use case is primarily home automation. In that scenario, I find Alexa to be much more responsive than Google Home. I do agree that it seems like Google Home has gotten worse in a number of ways. (As a happy AnyList user, that specific one was frustrating.)</div><br/></div></div><div id="40349345" class="c"><input type="checkbox" id="c-40349345" checked=""/><div class="controls bullet"><span class="by">keb_</span><span>|</span><a href="#40346465">parent</a><span>|</span><a href="#40347515">prev</a><span>|</span><a href="#40346576">next</a><span>|</span><label class="collapse" for="c-40349345">[-]</label><label class="expand" for="c-40349345">[2 more]</label></div><br/><div class="children"><div class="content">I have Alexa (Amazon Echo Show) and my use-case is asking for a news briefing, the weather, playing music, or setting timers.<p>Alexa is a dumpster fire and constantly getting dumber. She also completely disrespects your settings and will re-enable settings you have disabled. She constantly ignores my questions to ask me if I want to try some other new feature instead. She randomly decides to add news stations I have explicitly <i>removed</i> from my Flash Briefing list.<p>I am constantly baffled by how bad it is.</div><br/><div id="40352312" class="c"><input type="checkbox" id="c-40352312" checked=""/><div class="controls bullet"><span class="by">kybernetikos</span><span>|</span><a href="#40346465">root</a><span>|</span><a href="#40349345">parent</a><span>|</span><a href="#40346576">next</a><span>|</span><label class="collapse" for="c-40352312">[-]</label><label class="expand" for="c-40352312">[1 more]</label></div><br/><div class="children"><div class="content">The hardware is nice though. Wish we could run open source assistants on it.</div><br/></div></div></div></div><div id="40346576" class="c"><input type="checkbox" id="c-40346576" checked=""/><div class="controls bullet"><span class="by">petemir</span><span>|</span><a href="#40346465">parent</a><span>|</span><a href="#40349345">prev</a><span>|</span><a href="#40347576">next</a><span>|</span><label class="collapse" for="c-40346576">[-]</label><label class="expand" for="c-40346576">[1 more]</label></div><br/><div class="children"><div class="content">&gt; From what I can tell, Siri is still a dumpster fire that nobody is willing to use. I have no personal experience with Alexa, so I can&#x27;t speak to it<p>I use both (albeit more Alexa than Siri, both just for a really limited functionality set), and FWIW, I believe Alexa is worse than Siri. It can do two things at the same time though (just as your example: &quot;turn on X and turn off Y&quot;, &quot;turn on X for Y seconds&quot;, and things like that).<p>I also feel that it has gotten worse over the years. I read about the possibility of microphones getting dust and therefore capturing worse audio, so I got a dust blower (for other reasons, too), but it didn&#x27;t solve anything.<p>After listening in the app what Alexa picks up (from an Echo and Echo Dot, both 4th. Gen), I have to say that they use really shitty microphones. Furthermore, I have been testing Whisper extensively last month, with audio coming from low-quality sources, and I think a similar model would interpret a lot better my voice than whatever Amazon is using.</div><br/></div></div><div id="40347576" class="c"><input type="checkbox" id="c-40347576" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40346465">parent</a><span>|</span><a href="#40346576">prev</a><span>|</span><a href="#40351499">next</a><span>|</span><label class="collapse" for="c-40347576">[-]</label><label class="expand" for="c-40347576">[2 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s still unable to interpret two-part requests<p>Our car has Google Assistant, and yeah that&#x27;s annoying. Want to turn off steering wheel heater <i>and</i> seat heater? Gotta do two individual requests.<p>That said, it&#x27;s actually quite nice to have voice control over these things. Especially when it&#x27;s heavy traffic and snowing on top of the icy road, and you really want to have eyes on the traffic and both hands on the steering wheel.</div><br/><div id="40348095" class="c"><input type="checkbox" id="c-40348095" checked=""/><div class="controls bullet"><span class="by">johnmaguire</span><span>|</span><a href="#40346465">root</a><span>|</span><a href="#40347576">parent</a><span>|</span><a href="#40351499">next</a><span>|</span><label class="collapse" for="c-40348095">[-]</label><label class="expand" for="c-40348095">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That said, it&#x27;s actually quite nice to have voice control over these things.<p>Yes! I really think voice assistants are underrated. When I talk to iOS users, they have a much less favorable opinion of Siri (and I&#x27;ve watched my partner give up on using it over the past 10 years) and given that iOS has dominant market share in the US, I suspect this is a part of it.<p>But I also think there is just so much &quot;low hanging fruit&quot; that would drastically improve the experience. But I remember that even during &quot;the race&quot; for voice AI, everyone was wondering... how will they monetize this? And I&#x27;m not sure anyone was ever truly able to figure that out.</div><br/></div></div></div></div></div></div><div id="40351499" class="c"><input type="checkbox" id="c-40351499" checked=""/><div class="controls bullet"><span class="by">userhacker</span><span>|</span><a href="#40346465">prev</a><span>|</span><a href="#40347530">next</a><span>|</span><label class="collapse" for="c-40351499">[-]</label><label class="expand" for="c-40351499">[1 more]</label></div><br/><div class="children"><div class="content">Just made <a href="https:&#x2F;&#x2F;feycher.com" rel="nofollow">https:&#x2F;&#x2F;feycher.com</a> thats similar, but has realtime lip syncing as well. Let me know if you are interested and we can chat</div><br/></div></div><div id="40347530" class="c"><input type="checkbox" id="c-40347530" checked=""/><div class="controls bullet"><span class="by">russ</span><span>|</span><a href="#40351499">prev</a><span>|</span><a href="#40351306">next</a><span>|</span><label class="collapse" for="c-40347530">[-]</label><label class="expand" for="c-40347530">[1 more]</label></div><br/><div class="children"><div class="content">LiveKit Agents, which OpenAI uses in voice mode is also open source:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;livekit&#x2F;agents">https:&#x2F;&#x2F;github.com&#x2F;livekit&#x2F;agents</a></div><br/></div></div><div id="40351306" class="c"><input type="checkbox" id="c-40351306" checked=""/><div class="controls bullet"><span class="by">orliesaurus</span><span>|</span><a href="#40347530">prev</a><span>|</span><a href="#40347247">next</a><span>|</span><label class="collapse" for="c-40351306">[-]</label><label class="expand" for="c-40351306">[1 more]</label></div><br/><div class="children"><div class="content">The whole VAD thing is very interesting, keen to learn more about how it works and especially with multiple speakers!</div><br/></div></div><div id="40347247" class="c"><input type="checkbox" id="c-40347247" checked=""/><div class="controls bullet"><span class="by">xan_ps007</span><span>|</span><a href="#40351306">prev</a><span>|</span><a href="#40346273">next</a><span>|</span><label class="collapse" for="c-40347247">[-]</label><label class="expand" for="c-40347247">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re also building bolna an open source voice orchestration: <a href="https:&#x2F;&#x2F;github.com&#x2F;bolna-ai&#x2F;bolna">https:&#x2F;&#x2F;github.com&#x2F;bolna-ai&#x2F;bolna</a></div><br/></div></div><div id="40346273" class="c"><input type="checkbox" id="c-40346273" checked=""/><div class="controls bullet"><span class="by">canadiantim</span><span>|</span><a href="#40347247">prev</a><span>|</span><a href="#40346373">next</a><span>|</span><label class="collapse" for="c-40346273">[-]</label><label class="expand" for="c-40346273">[1 more]</label></div><br/><div class="children"><div class="content">Very cool, great work! I can def self using this when I start building in that direction.</div><br/></div></div><div id="40346373" class="c"><input type="checkbox" id="c-40346373" checked=""/><div class="controls bullet"><span class="by">bamazizi</span><span>|</span><a href="#40346273">prev</a><span>|</span><label class="collapse" for="c-40346373">[-]</label><label class="expand" for="c-40346373">[6 more]</label></div><br/><div class="children"><div class="content">I wonder how the just announced &quot;GPT-4o&quot; with real-time voice impacts projects like this?<p>The demo on real-time multi language translation conversation blew me away!</div><br/><div id="40346654" class="c"><input type="checkbox" id="c-40346654" checked=""/><div class="controls bullet"><span class="by">kwindla</span><span>|</span><a href="#40346373">parent</a><span>|</span><a href="#40346599">next</a><span>|</span><label class="collapse" for="c-40346654">[-]</label><label class="expand" for="c-40346654">[3 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a translation demo in Pipecat using the now ancient and arthritic GPT-4 Turbo model. :-) <a href="https:&#x2F;&#x2F;github.com&#x2F;pipecat-ai&#x2F;pipecat&#x2F;tree&#x2F;main&#x2F;examples&#x2F;translation-chatbot">https:&#x2F;&#x2F;github.com&#x2F;pipecat-ai&#x2F;pipecat&#x2F;tree&#x2F;main&#x2F;examples&#x2F;tra...</a><p>As soon as GPT-4o audio input is available through the APIs, we&#x27;ll add 4o support to Pipecat. For bidirectional real-time audio, I think they&#x27;ll need to make new WebSocket or WebRTC endpoints available.</div><br/><div id="40346771" class="c"><input type="checkbox" id="c-40346771" checked=""/><div class="controls bullet"><span class="by">jshreder</span><span>|</span><a href="#40346373">root</a><span>|</span><a href="#40346654">parent</a><span>|</span><a href="#40346599">next</a><span>|</span><label class="collapse" for="c-40346771">[-]</label><label class="expand" for="c-40346771">[2 more]</label></div><br/><div class="children"><div class="content">Just letting you know it&#x27;s available right now, just specify `gpt-4o` -- for text streaming anyway. I&#x27;d hazard a guess that the audio endpoints are open now, just not documented (like most of the last launches)...</div><br/><div id="40346821" class="c"><input type="checkbox" id="c-40346821" checked=""/><div class="controls bullet"><span class="by">kwindla</span><span>|</span><a href="#40346373">root</a><span>|</span><a href="#40346771">parent</a><span>|</span><a href="#40346599">next</a><span>|</span><label class="collapse" for="c-40346821">[-]</label><label class="expand" for="c-40346821">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, seems to be a drop-in replacement for the existing inference APIs. But I haven&#x27;t found any docs yet for streaming audio&#x2F;video input.</div><br/></div></div></div></div></div></div><div id="40346599" class="c"><input type="checkbox" id="c-40346599" checked=""/><div class="controls bullet"><span class="by">avarun</span><span>|</span><a href="#40346373">parent</a><span>|</span><a href="#40346654">prev</a><span>|</span><a href="#40346837">next</a><span>|</span><label class="collapse" for="c-40346599">[-]</label><label class="expand" for="c-40346599">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, same question here.<p>Building pipelines for bridging LLMs and TTS and STT models with lower latency is fine and all, but when you compare to a natively multimodal model like GPT-4o it seems strictly inferior. The future is clearly voice-native models that are able to understand nuances in voice and speech patterns, and it&#x27;s not exactly a distant future.</div><br/></div></div></div></div></div></div></div></div></div></body></html>