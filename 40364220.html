<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715936463905" as="style"/><link rel="stylesheet" href="styles.css?v=1715936463905"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ai.google.dev/gemini-api/docs/caching">Context caching guide</a> <span class="domain">(<a href="https://ai.google.dev">ai.google.dev</a>)</span></div><div class="subtext"><span>tosh</span> | <span>41 comments</span></div><br/><div><div id="40382302" class="c"><input type="checkbox" id="c-40382302" checked=""/><div class="controls bullet"><span class="by">_pdp_</span><span>|</span><a href="#40384927">next</a><span>|</span><label class="collapse" for="c-40382302">[-]</label><label class="expand" for="c-40382302">[22 more]</label></div><br/><div class="children"><div class="content">We envisioned a system like this over a year ago, but since we lacked technical capabilities in this area, we couldn&#x27;t solve it. To me, it is clear that a lot of information is being transmitted needlessly. There is no need to send an entire book every single time an interaction occurs. Instead, you can start from a checkpoint, remap from memory, and continue from there. Perhaps none of the current LLM service providers have an architecture that allows this, but it seems like something that should be possible and is likely to emerge in the near future.</div><br/><div id="40382366" class="c"><input type="checkbox" id="c-40382366" checked=""/><div class="controls bullet"><span class="by">gradys</span><span>|</span><a href="#40382302">parent</a><span>|</span><a href="#40383500">next</a><span>|</span><label class="collapse" for="c-40382366">[-]</label><label class="expand" for="c-40382366">[16 more]</label></div><br/><div class="children"><div class="content">The size of the cached internal state of the network processing the book is much larger than the size of the book. The resource that is preserved with caching is the compute required to recreate that state.</div><br/><div id="40382440" class="c"><input type="checkbox" id="c-40382440" checked=""/><div class="controls bullet"><span class="by">dfgtyu65r</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382366">parent</a><span>|</span><a href="#40382599">next</a><span>|</span><label class="collapse" for="c-40382440">[-]</label><label class="expand" for="c-40382440">[7 more]</label></div><br/><div class="children"><div class="content">Sure, but a direct forwards pass of the book would surely require more compute than simply loading and setting the hidden state?<p>The second doesn&#x27;t require any matrix operations, it&#x27;s just setting some values.</div><br/><div id="40382548" class="c"><input type="checkbox" id="c-40382548" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382440">parent</a><span>|</span><a href="#40385524">next</a><span>|</span><label class="collapse" for="c-40382548">[-]</label><label class="expand" for="c-40382548">[3 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s just setting some values<p>But it may very well be slower than just recompute it. At least for ordinary MHA and even GQA.<p>So, either a model arch woodoo significantly reducing kv cache size (while keeping roughly the same compute cost), or some really careful implementation moving kv cache of upcoming requests to devices in background [0].<p>[0] My back of envelop calc shows that even then it still does not make sense for, say, Llama 3 70B on H100s. Time to stare at TPU spec harder trying to make sense of it I guess.</div><br/><div id="40383544" class="c"><input type="checkbox" id="c-40383544" checked=""/><div class="controls bullet"><span class="by">sshumaker</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382548">parent</a><span>|</span><a href="#40385524">next</a><span>|</span><label class="collapse" for="c-40383544">[-]</label><label class="expand" for="c-40383544">[2 more]</label></div><br/><div class="children"><div class="content">It depends on how large the input prompt (previous context) is. Also, if you can keep cache on GPU with a LRU mechanism, for certain workloads it&#x27;s very efficient.<p>You can also design an API optimized for batch workloads (say the same core prompt with different data for instruct-style reasoning) - that can result in large savings in those scenarios.</div><br/><div id="40383778" class="c"><input type="checkbox" id="c-40383778" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40383544">parent</a><span>|</span><a href="#40385524">next</a><span>|</span><label class="collapse" for="c-40383778">[-]</label><label class="expand" for="c-40383778">[1 more]</label></div><br/><div class="children"><div class="content">If you can pipeline upcoming requests and tie state to a specific request, doesn&#x27;t that allow you to change how you design physical memory? (at least for inference)<p>Stupid question, but why wouldn&#x27;t {extremely large slow-write, fast-read memory} + {smaller, very fast-write memory} be a feasible hardware architecture?<p>If you know many, many cycles ahead what you&#x27;ll need to have loaded at a specific time.<p>Or hell, maybe it&#x27;s time to go back to memory bank switching.</div><br/></div></div></div></div></div></div><div id="40385524" class="c"><input type="checkbox" id="c-40385524" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382440">parent</a><span>|</span><a href="#40382548">prev</a><span>|</span><a href="#40382546">next</a><span>|</span><label class="collapse" for="c-40385524">[-]</label><label class="expand" for="c-40385524">[1 more]</label></div><br/><div class="children"><div class="content">The throughput of the PCIe link between the CPU and GPU, is far less than the <i>aggregate</i> throughput of the internal interconnects between neighbouring tensor cores.<p>Matrix operations might flow a lot of data around — but that data flow is akin to a bunch of individual people travelling along the individual residential streets they live on. There&#x27;s a lot of movement there, but also a lot of <i>capacity</i> for movement, because there&#x27;s no bottleneck of everyone needing to go to the same place or come from the same place.<p>Persisting the data out of the GPU and then loading it back in, is more like all those people commuting to work and then going back home. Big fan-in onto the PCIe &quot;highway&quot; over to the CPU and into RAM; then big fan-out back. Traffic jams for miles.<p>In the time it takes to restore a 1GB state snapshot from RAM into VRAM, you can probably chew through the equivalent of 1TB or more of intermediate matrix states.</div><br/></div></div><div id="40382546" class="c"><input type="checkbox" id="c-40382546" checked=""/><div class="controls bullet"><span class="by">gradys</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382440">parent</a><span>|</span><a href="#40385524">prev</a><span>|</span><a href="#40382535">next</a><span>|</span><label class="collapse" for="c-40382546">[-]</label><label class="expand" for="c-40382546">[1 more]</label></div><br/><div class="children"><div class="content">I don’t know of any public details on how they implement Context Caching, but that is presumably exactly what they are doing. Just caching the text would be a minimal savings.</div><br/></div></div><div id="40382535" class="c"><input type="checkbox" id="c-40382535" checked=""/><div class="controls bullet"><span class="by">nancarrow</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382440">parent</a><span>|</span><a href="#40382546">prev</a><span>|</span><a href="#40382599">next</a><span>|</span><label class="collapse" for="c-40382535">[-]</label><label class="expand" for="c-40382535">[1 more]</label></div><br/><div class="children"><div class="content">&quot;some&quot; is doing a lot of lifting. # of tokens * # of layers * head dimension * # of heads * 2 (K+V vectors) * 4-16bits (depending on quantization)</div><br/></div></div></div></div><div id="40382599" class="c"><input type="checkbox" id="c-40382599" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382366">parent</a><span>|</span><a href="#40382440">prev</a><span>|</span><a href="#40382547">next</a><span>|</span><label class="collapse" for="c-40382599">[-]</label><label class="expand" for="c-40382599">[6 more]</label></div><br/><div class="children"><div class="content">&gt;The size of the cached internal state of the network processing the book is much larger than the size of the book<p>It&#x27;s funny that sometimes people consider LLMs as compression engines. While a lot of information gets lost in each direction (through the neural net)</div><br/><div id="40382919" class="c"><input type="checkbox" id="c-40382919" checked=""/><div class="controls bullet"><span class="by">shwaj</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382599">parent</a><span>|</span><a href="#40382547">next</a><span>|</span><label class="collapse" for="c-40382919">[-]</label><label class="expand" for="c-40382919">[5 more]</label></div><br/><div class="children"><div class="content">Why is that funny?  Sometimes compression is lossy, like JPEG and H.265</div><br/><div id="40383086" class="c"><input type="checkbox" id="c-40383086" checked=""/><div class="controls bullet"><span class="by">pornel</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382919">parent</a><span>|</span><a href="#40383106">next</a><span>|</span><label class="collapse" for="c-40383086">[-]</label><label class="expand" for="c-40383086">[1 more]</label></div><br/><div class="children"><div class="content">And the internal state of a JPEG decoder can be an order of magnitude larger than the JPEG file (especially progressive JPEG that can&#x27;t stream its output).</div><br/></div></div><div id="40383106" class="c"><input type="checkbox" id="c-40383106" checked=""/><div class="controls bullet"><span class="by">okdood64</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382919">parent</a><span>|</span><a href="#40383086">prev</a><span>|</span><a href="#40382547">next</a><span>|</span><label class="collapse" for="c-40383106">[-]</label><label class="expand" for="c-40383106">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t lose anything with gzip or rar.</div><br/><div id="40383198" class="c"><input type="checkbox" id="c-40383198" checked=""/><div class="controls bullet"><span class="by">giancarlostoro</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40383106">parent</a><span>|</span><a href="#40383584">next</a><span>|</span><label class="collapse" for="c-40383198">[-]</label><label class="expand" for="c-40383198">[1 more]</label></div><br/><div class="children"><div class="content">And just as fast? The issue here is how do you do these things both accurately and while maintaining reasonable speeds.</div><br/></div></div><div id="40383584" class="c"><input type="checkbox" id="c-40383584" checked=""/><div class="controls bullet"><span class="by">fwip</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40383106">parent</a><span>|</span><a href="#40383198">prev</a><span>|</span><a href="#40382547">next</a><span>|</span><label class="collapse" for="c-40383584">[-]</label><label class="expand" for="c-40383584">[1 more]</label></div><br/><div class="children"><div class="content">You can make any lossy compression scheme into a lossless scheme by appending the diff between the original and the compressed. In many cases, this still results in a size savings over the original.<p>You can think of this as a more detailed form of &quot;I before E, except after C, except for species and science and...&quot; Or, if you prefer, as continued terms of a Taylor-series expansion. The more terms you add, the more closely you approximate the original.</div><br/></div></div></div></div></div></div></div></div><div id="40382547" class="c"><input type="checkbox" id="c-40382547" checked=""/><div class="controls bullet"><span class="by">objektif</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382366">parent</a><span>|</span><a href="#40382599">prev</a><span>|</span><a href="#40383500">next</a><span>|</span><label class="collapse" for="c-40382547">[-]</label><label class="expand" for="c-40382547">[2 more]</label></div><br/><div class="children"><div class="content">But isnt the information somehow cached when you start a new chat and build context with say GPT4? If the caching was so large as you say so many chat sessions in parallel would not be possible.</div><br/><div id="40382640" class="c"><input type="checkbox" id="c-40382640" checked=""/><div class="controls bullet"><span class="by">dosinga</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40382547">parent</a><span>|</span><a href="#40383500">next</a><span>|</span><label class="collapse" for="c-40382640">[-]</label><label class="expand" for="c-40382640">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not my understanding. We can&#x27;t be sure how OpenAI does things themselves, but adding messages to a conversation in the API means just rerunning the history through the prompt every time</div><br/></div></div></div></div></div></div><div id="40383500" class="c"><input type="checkbox" id="c-40383500" checked=""/><div class="controls bullet"><span class="by">sshumaker</span><span>|</span><a href="#40382302">parent</a><span>|</span><a href="#40382366">prev</a><span>|</span><a href="#40384029">next</a><span>|</span><label class="collapse" for="c-40383500">[-]</label><label class="expand" for="c-40383500">[2 more]</label></div><br/><div class="children"><div class="content">They are almost certainly doing this internally for their own chat products.<p>The simple version of this just involves saving off the KV cache in the attention layers, and restore it back instead of recomputing. It only requires small changes to inference and the attention layers.<p>The main challenge is being able to do this under scale, e.g. dump the weights out of GPU memory, persist them, and have a system to rapidly reload them as needed (or just regenerate).</div><br/><div id="40383687" class="c"><input type="checkbox" id="c-40383687" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40383500">parent</a><span>|</span><a href="#40384029">next</a><span>|</span><label class="collapse" for="c-40383687">[-]</label><label class="expand" for="c-40383687">[1 more]</label></div><br/><div class="children"><div class="content">2024 is the year of serverless LLM?</div><br/></div></div></div></div><div id="40384029" class="c"><input type="checkbox" id="c-40384029" checked=""/><div class="controls bullet"><span class="by">conradev</span><span>|</span><a href="#40382302">parent</a><span>|</span><a href="#40383500">prev</a><span>|</span><a href="#40383711">next</a><span>|</span><label class="collapse" for="c-40384029">[-]</label><label class="expand" for="c-40384029">[1 more]</label></div><br/><div class="children"><div class="content">To me, context caching is only a subset of what is possible with full control over the model. I consider this a more complete list: <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;aici?tab=readme-ov-file#flexibility">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;aici?tab=readme-ov-file#flexibi...</a><p>Context caching only gets you “forking generation into multiple branches” (i.e. sharing work between multiple generations)</div><br/></div></div><div id="40383711" class="c"><input type="checkbox" id="c-40383711" checked=""/><div class="controls bullet"><span class="by">twobitshifter</span><span>|</span><a href="#40382302">parent</a><span>|</span><a href="#40384029">prev</a><span>|</span><a href="#40384927">next</a><span>|</span><label class="collapse" for="c-40383711">[-]</label><label class="expand" for="c-40383711">[2 more]</label></div><br/><div class="children"><div class="content">I remember hearing that the reason Claude is expensive is that every interaction makes it reread the entire conversation.</div><br/><div id="40383926" class="c"><input type="checkbox" id="c-40383926" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#40382302">root</a><span>|</span><a href="#40383711">parent</a><span>|</span><a href="#40384927">next</a><span>|</span><label class="collapse" for="c-40383926">[-]</label><label class="expand" for="c-40383926">[1 more]</label></div><br/><div class="children"><div class="content">This is the case with all LLMs as far as I know.<p>With the chatGPT api you just send everything up to that point + the new input to get the new output.<p>I think the benefit for the service is that it’s stateless. They just have requests in and out and don’t have to worry about anything else.</div><br/></div></div></div></div></div></div><div id="40384927" class="c"><input type="checkbox" id="c-40384927" checked=""/><div class="controls bullet"><span class="by">waldrews</span><span>|</span><a href="#40382302">prev</a><span>|</span><a href="#40385537">next</a><span>|</span><label class="collapse" for="c-40384927">[-]</label><label class="expand" for="c-40384927">[1 more]</label></div><br/><div class="children"><div class="content">The pricing is still such that you can&#x27;t routinely use a customized Gemini with your own long fixed pre-filled context + a variable short query.  If the one-time compute cost of the caching could be effectively amortized over many queries, this pattern would replace many fine-tuning and RAG cases with something more predictable and controllable.<p>It&#x27;s not as simple as that because the large cache needs to be loaded into GPU memory every time, but optimizations must be feasible if the usage rate is large enough to keep the cache alive in a dedicated machine.<p>Previous discussion: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40034972#40036309">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40034972#40036309</a></div><br/></div></div><div id="40385537" class="c"><input type="checkbox" id="c-40385537" checked=""/><div class="controls bullet"><span class="by">navaed01</span><span>|</span><a href="#40384927">prev</a><span>|</span><a href="#40384394">next</a><span>|</span><label class="collapse" for="c-40385537">[-]</label><label class="expand" for="c-40385537">[1 more]</label></div><br/><div class="children"><div class="content">I’m really glad they released context caching- the are legitimate use cases where a single or multiple users in an organization are all using a long prompt with examples in it. Which impacts cost and speed</div><br/></div></div><div id="40384394" class="c"><input type="checkbox" id="c-40384394" checked=""/><div class="controls bullet"><span class="by">_cs2017_</span><span>|</span><a href="#40385537">prev</a><span>|</span><a href="#40383954">next</a><span>|</span><label class="collapse" for="c-40384394">[-]</label><label class="expand" for="c-40384394">[2 more]</label></div><br/><div class="children"><div class="content">How is this different from KV caching?<p>(For reference, basic KV caching is explained here: <a href="https:&#x2F;&#x2F;www.omrimallis.com&#x2F;posts&#x2F;techniques-for-kv-cache-optimization&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.omrimallis.com&#x2F;posts&#x2F;techniques-for-kv-cache-opt...</a>.)</div><br/><div id="40384876" class="c"><input type="checkbox" id="c-40384876" checked=""/><div class="controls bullet"><span class="by">luckyt</span><span>|</span><a href="#40384394">parent</a><span>|</span><a href="#40383954">next</a><span>|</span><label class="collapse" for="c-40384876">[-]</label><label class="expand" for="c-40384876">[1 more]</label></div><br/><div class="children"><div class="content">There are several issues that make the KV cache as-is unsuitable for caching across requests. First, it requires the cached tokens to be in the exact same position in the sentence, this means it&#x27;s mainly only useful for autoregressive generation where the prefix is always the same. Second, it is extremely big, so without some sort of compression, the cost to store it between requests and the time required to transfer the data to the GPU will outweigh any compute savings.</div><br/></div></div></div></div><div id="40383954" class="c"><input type="checkbox" id="c-40383954" checked=""/><div class="controls bullet"><span class="by">lolpanda</span><span>|</span><a href="#40384394">prev</a><span>|</span><a href="#40381515">next</a><span>|</span><label class="collapse" for="c-40383954">[-]</label><label class="expand" for="c-40383954">[1 more]</label></div><br/><div class="children"><div class="content">i think llama.cpp has context caching with &quot;--prompt-cache&quot; but it will result in a very large cache file. i guess it&#x27;s also very expensive for any inference api provider to support caching as they have to persist the file and load&#x2F;unload it each time.</div><br/></div></div><div id="40381515" class="c"><input type="checkbox" id="c-40381515" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#40383954">prev</a><span>|</span><a href="#40383452">next</a><span>|</span><label class="collapse" for="c-40381515">[-]</label><label class="expand" for="c-40381515">[3 more]</label></div><br/><div class="children"><div class="content">Makes sense.<p>It isn&#x27;t in the list of suggested usecases, but I wonder if this can be used to speed up tree of thoughts or similar prompt&#x2F;search techniques.<p>It could also speed up restricted generation, e.g. when you force the model to output valid json.</div><br/><div id="40382254" class="c"><input type="checkbox" id="c-40382254" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#40381515">parent</a><span>|</span><a href="#40381922">next</a><span>|</span><label class="collapse" for="c-40382254">[-]</label><label class="expand" for="c-40382254">[1 more]</label></div><br/><div class="children"><div class="content">another advantage may be to let the big library providers (LangChain | LlamaIndex) to implement this before releasing to developers at large</div><br/></div></div><div id="40381922" class="c"><input type="checkbox" id="c-40381922" checked=""/><div class="controls bullet"><span class="by">motoboi</span><span>|</span><a href="#40381515">parent</a><span>|</span><a href="#40382254">prev</a><span>|</span><a href="#40383452">next</a><span>|</span><label class="collapse" for="c-40381922">[-]</label><label class="expand" for="c-40381922">[1 more]</label></div><br/><div class="children"><div class="content">I suppose the caching is at a layer before the LLM.</div><br/></div></div></div></div><div id="40383452" class="c"><input type="checkbox" id="c-40383452" checked=""/><div class="controls bullet"><span class="by">sshumaker</span><span>|</span><a href="#40381515">prev</a><span>|</span><a href="#40381641">next</a><span>|</span><label class="collapse" for="c-40383452">[-]</label><label class="expand" for="c-40383452">[1 more]</label></div><br/><div class="children"><div class="content">This is a pretty standard technique if you&#x27;re running the models yourself. e.g. ChatGPT almost certainly does this.<p>There&#x27;s even work that is more sophisticated in this domain that allows &#x27;template&#x27; style partial caching:
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04934" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04934</a></div><br/></div></div><div id="40381510" class="c"><input type="checkbox" id="c-40381510" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#40381641">prev</a><span>|</span><label class="collapse" for="c-40381510">[-]</label><label class="expand" for="c-40381510">[8 more]</label></div><br/><div class="children"><div class="content">I really want this (in OpenAI API). Does google not care about the reputation it&#x27;s getting with this shit:<p>&gt; We&#x27;ll be launching context caching soon, along with technical documentation and SDK support.<p>it&#x27;s not live and god knows when it will be</div><br/><div id="40381603" class="c"><input type="checkbox" id="c-40381603" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#40381510">parent</a><span>|</span><a href="#40381721">next</a><span>|</span><label class="collapse" for="c-40381603">[-]</label><label class="expand" for="c-40381603">[3 more]</label></div><br/><div class="children"><div class="content">They have adopted the strategy of mainly releasing to &amp; iterating with the large enterprise customers, because they (rightly) realized that GA&#x27;ing to every developer makes no monetary sense, unless they are trying to learn something from that launch &#x2F; need to do so for competitive reasons.</div><br/><div id="40381685" class="c"><input type="checkbox" id="c-40381685" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#40381510">root</a><span>|</span><a href="#40381603">parent</a><span>|</span><a href="#40382542">next</a><span>|</span><label class="collapse" for="c-40381685">[-]</label><label class="expand" for="c-40381685">[1 more]</label></div><br/><div class="children"><div class="content">fair point</div><br/></div></div><div id="40382542" class="c"><input type="checkbox" id="c-40382542" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40381510">root</a><span>|</span><a href="#40381603">parent</a><span>|</span><a href="#40381685">prev</a><span>|</span><a href="#40381721">next</a><span>|</span><label class="collapse" for="c-40382542">[-]</label><label class="expand" for="c-40382542">[1 more]</label></div><br/><div class="children"><div class="content">Presumably, this would be a competitive reason, no? It would further the cost savings they GA&#x27;d with Gemini Flash, and it&#x27;s a differentiator from every other provider.</div><br/></div></div></div></div><div id="40381721" class="c"><input type="checkbox" id="c-40381721" checked=""/><div class="controls bullet"><span class="by">aiauthoritydev</span><span>|</span><a href="#40381510">parent</a><span>|</span><a href="#40381603">prev</a><span>|</span><a href="#40382827">next</a><span>|</span><label class="collapse" for="c-40381721">[-]</label><label class="expand" for="c-40381721">[2 more]</label></div><br/><div class="children"><div class="content">This actually is smarter strategy IMO.<p>Throwing the hat in the ring forces your competitors think about it and make them work too. It also gives you first mover advantage making people think you did it first.<p>A lot of AI tools make more sense for their enterprise customers rather than people like us. People like us are good only for hype and not making money.</div><br/><div id="40381968" class="c"><input type="checkbox" id="c-40381968" checked=""/><div class="controls bullet"><span class="by">flipbrad</span><span>|</span><a href="#40381510">root</a><span>|</span><a href="#40381721">parent</a><span>|</span><a href="#40382827">next</a><span>|</span><label class="collapse" for="c-40381968">[-]</label><label class="expand" for="c-40381968">[1 more]</label></div><br/><div class="children"><div class="content">Might help show prior art to defeat software patents too.</div><br/></div></div></div></div><div id="40382827" class="c"><input type="checkbox" id="c-40382827" checked=""/><div class="controls bullet"><span class="by">hidelooktropic</span><span>|</span><a href="#40381510">parent</a><span>|</span><a href="#40381721">prev</a><span>|</span><label class="collapse" for="c-40382827">[-]</label><label class="expand" for="c-40382827">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this what the Assistants API is meant for? Honestly not sure, I haven&#x27;t used it before but their documentation seems to suggest you can set up the assistant to already have this context, then just send API commands to it without said context.</div><br/><div id="40383172" class="c"><input type="checkbox" id="c-40383172" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40381510">root</a><span>|</span><a href="#40382827">parent</a><span>|</span><label class="collapse" for="c-40383172">[-]</label><label class="expand" for="c-40383172">[1 more]</label></div><br/><div class="children"><div class="content">but they charge u for the full context every time, at least for now. latency would suggest that no caching is happening</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>