<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685610079720" as="style"/><link rel="stylesheet" href="styles.css?v=1685610079720"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://sneller.io/blog/decompressing-at-over-10-gigabytes-per-second/">Iguana: fast SIMD-optimized decompression</a> <span class="domain">(<a href="https://sneller.io">sneller.io</a>)</span></div><div class="subtext"><span>l2dy</span> | <span>27 comments</span></div><br/><div><div id="36147982" class="c"><input type="checkbox" id="c-36147982" checked=""/><div class="controls bullet"><span class="by">powturbo</span><span>|</span><a href="#36147320">next</a><span>|</span><label class="collapse" for="c-36147982">[-]</label><label class="expand" for="c-36147982">[11 more]</label></div><br/><div class="children"><div class="content">As general purpose compressor iguana is decompressing a lot slower than advertised, when tested with a typical data compression corpus.<p>It requires: avx512-vbmi2, available only on ice-lake&#x2F;Tiger-lake&#x2F;AMD zen4<p>- Benchmark from encode.su experts: <a href="https:&#x2F;&#x2F;encode.su&#x2F;threads&#x2F;4041-Iguana-a-fast-vectorized-compressor?p=79634&amp;viewfull=1#post79634" rel="nofollow">https:&#x2F;&#x2F;encode.su&#x2F;threads&#x2F;4041-Iguana-a-fast-vectorized-comp...</a><p>- benchmark from the iguana developers here: <a href="https:&#x2F;&#x2F;github.com&#x2F;SnellerInc&#x2F;sneller&#x2F;tree&#x2F;master&#x2F;cmd&#x2F;iguanabench">https:&#x2F;&#x2F;github.com&#x2F;SnellerInc&#x2F;sneller&#x2F;tree&#x2F;master&#x2F;cmd&#x2F;iguana...</a><p>Silesia corpus &#x2F; cpu Xeon Gold 5320<p>zstd -b3 3.186 943.9 MB&#x2F;s<p>zstd -b9 3.574 1015.8 MB&#x2F;s<p>zstd -b18 3.967 910.6 MB&#x2F;s<p>lz4 -b1 2.101 3493.8 MB&#x2F;s<p>lz4 -b5 2.687 3323.5 MB&#x2F;s<p>lz4 -b9 2.721 3381.5 MB&#x2F;s<p>iguana -t=0 2.58 4450 MB&#x2F;s<p>iguana -t=1 3.11 2260 MB&#x2F;s<p>As you can see, iguana with entropy coding enabled (-t 1) has a similar compression ratio to zstd -3, but it decompresses more than twice as quickly. With entropy coding disabled (-t 0), iguana has a compression ratio roughly equivalent to lz4 -5 and decompresses about 33% faster.</div><br/><div id="36148155" class="c"><input type="checkbox" id="c-36148155" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36147982">parent</a><span>|</span><a href="#36148231">next</a><span>|</span><label class="collapse" for="c-36148155">[-]</label><label class="expand" for="c-36148155">[8 more]</label></div><br/><div class="children"><div class="content">&gt; It requires: avx512-vbmi2, available only on skylake-X&#x2F;Tiger-lake&#x2F;AMD zen4<p>Isn&#x27;t Avx 512 also available on Xeon which is most hosted servers, like aws.</div><br/><div id="36148743" class="c"><input type="checkbox" id="c-36148743" checked=""/><div class="controls bullet"><span class="by">bugboy73</span><span>|</span><a href="#36147982">root</a><span>|</span><a href="#36148155">parent</a><span>|</span><a href="#36148381">next</a><span>|</span><label class="collapse" for="c-36148743">[-]</label><label class="expand" for="c-36148743">[1 more]</label></div><br/><div class="children"><div class="content">All major cloud providers (AWS, Azure and GCP) provide instance-types that support AVX512-VBMI2. Iguana targets large amounts of that are decompressed often and speed is of the essence. Those are typical server scenarios, so the AVX512-VBMI2 requirement isn&#x27;t really an issue in common-case scenarios.<p>DISCLAIMER: I work for Sneller</div><br/></div></div><div id="36148381" class="c"><input type="checkbox" id="c-36148381" checked=""/><div class="controls bullet"><span class="by">Aardwolf</span><span>|</span><a href="#36147982">root</a><span>|</span><a href="#36148155">parent</a><span>|</span><a href="#36148743">prev</a><span>|</span><a href="#36148162">next</a><span>|</span><label class="collapse" for="c-36148381">[-]</label><label class="expand" for="c-36148381">[5 more]</label></div><br/><div class="children"><div class="content">No modern intel consumer CPU has avx512 anymore, they only support at most the 10 year old avx2 vector instruction set, because they want to give every single consumer chip efficiency cores, those can&#x27;t handle it, and even the performance cores have this feature removed due to it.<p>The modern AMD Zen 4 CPU&#x27;s for consumers do support avx512 (including avx512_vbmi2).</div><br/><div id="36148620" class="c"><input type="checkbox" id="c-36148620" checked=""/><div class="controls bullet"><span class="by">powturbo</span><span>|</span><a href="#36147982">root</a><span>|</span><a href="#36148381">parent</a><span>|</span><a href="#36148162">next</a><span>|</span><label class="collapse" for="c-36148620">[-]</label><label class="expand" for="c-36148620">[4 more]</label></div><br/><div class="children"><div class="content">&gt; and even the performance cores have to be downgraded for it.<p>Old rumours, it&#x27;s no longer true.</div><br/><div id="36148667" class="c"><input type="checkbox" id="c-36148667" checked=""/><div class="controls bullet"><span class="by">skavi</span><span>|</span><a href="#36147982">root</a><span>|</span><a href="#36148620">parent</a><span>|</span><a href="#36148162">next</a><span>|</span><label class="collapse" for="c-36148667">[-]</label><label class="expand" for="c-36148667">[3 more]</label></div><br/><div class="children"><div class="content">not really rumors, but also not necessarily true. the performance cores on consumer intel parts no longer have AVX-512 enabled. This is because it’s tough to implement on the smaller efficiency cores, and scheduling is more annoying with heterogeneous ISAs.</div><br/><div id="36148765" class="c"><input type="checkbox" id="c-36148765" checked=""/><div class="controls bullet"><span class="by">powturbo</span><span>|</span><a href="#36147982">root</a><span>|</span><a href="#36148667">parent</a><span>|</span><a href="#36148162">next</a><span>|</span><label class="collapse" for="c-36148765">[-]</label><label class="expand" for="c-36148765">[2 more]</label></div><br/><div class="children"><div class="content">I understand downgrading as thermal throtlling.</div><br/><div id="36148799" class="c"><input type="checkbox" id="c-36148799" checked=""/><div class="controls bullet"><span class="by">Aardwolf</span><span>|</span><a href="#36147982">root</a><span>|</span><a href="#36148765">parent</a><span>|</span><a href="#36148162">next</a><span>|</span><label class="collapse" for="c-36148799">[-]</label><label class="expand" for="c-36148799">[1 more]</label></div><br/><div class="children"><div class="content">I did mean have the feature removed, I edited the comment (replaced &quot;downgraded&quot; with &quot;have this feature removed&quot;. I actually originally typed &quot;crippled&quot; but wanted to phrase it more mildly). What I mean is to avoid the heterogeneous cores they chose to disable it in the performance cores rather than e.g. emulate or double pump it in the efficiency cores.<p>And they chose to give you efficiency cores + performance cores without avx512 on all consumer CPU&#x27;s, rather than, what would be my preference, sell some with only performance cores with avx512 like their previous gen CPUs. I&#x27;d be ok with efficiency cores on a laptop, sure, but not on a desktop machine.<p>But AMD already gives plenty fast cores with avx512 now so I&#x27;m satisfied, I&#x27;m simply not an intel user anymore. Of course intel&#x27;s choice still affects me anyway since the less consumers have avx512, the less software will make use of it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36148162" class="c"><input type="checkbox" id="c-36148162" checked=""/><div class="controls bullet"><span class="by">powturbo</span><span>|</span><a href="#36147982">root</a><span>|</span><a href="#36148155">parent</a><span>|</span><a href="#36148381">prev</a><span>|</span><a href="#36148231">next</a><span>|</span><label class="collapse" for="c-36148162">[-]</label><label class="expand" for="c-36148162">[1 more]</label></div><br/><div class="children"><div class="content">Not only AVX-512, but AVX-512 + VBMI2</div><br/></div></div></div></div><div id="36148231" class="c"><input type="checkbox" id="c-36148231" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#36147982">parent</a><span>|</span><a href="#36148155">prev</a><span>|</span><a href="#36147320">next</a><span>|</span><label class="collapse" for="c-36148231">[-]</label><label class="expand" for="c-36148231">[2 more]</label></div><br/><div class="children"><div class="content">vbmi2 is not available on skylakex</div><br/><div id="36148303" class="c"><input type="checkbox" id="c-36148303" checked=""/><div class="controls bullet"><span class="by">powturbo</span><span>|</span><a href="#36147982">root</a><span>|</span><a href="#36148231">parent</a><span>|</span><a href="#36147320">next</a><span>|</span><label class="collapse" for="c-36148303">[-]</label><label class="expand" for="c-36148303">[1 more]</label></div><br/><div class="children"><div class="content">sorry, Ice-Lake not skylake</div><br/></div></div></div></div></div></div><div id="36147320" class="c"><input type="checkbox" id="c-36147320" checked=""/><div class="controls bullet"><span class="by">shoo</span><span>|</span><a href="#36147982">prev</a><span>|</span><a href="#36147682">next</a><span>|</span><label class="collapse" for="c-36147320">[-]</label><label class="expand" for="c-36147320">[6 more]</label></div><br/><div class="children"><div class="content">technically this looks really impressive. great to see a new compression approach that supports extremely high performance decompression, with a high performance open source implementation.<p>re: winning adoption of new compression approaches, there&#x27;s an interesting podcast interview [1] with Yann Collet (of lz4 &#x2F; zstd):<p>Some factors Yann discussed that helped lz4 &amp; zstd gain traction were:  permissive licensing (BSD)  ; implementation in C -- widest support for including it into other software ecosystems ; open development &amp; paying attention to issues raised by users of the software ; the new compression approach able beat an existing popular approach in some use cases with no downside: e.g. if a hypothetical new compression approach has 200% faster decompression but offers 10% worse compression ratios, then there&#x27;s friction for introducing it into an existing system, as the new approach might first require purchase and deployment of additional storage. Whereas a new approach that is 50% faster and has exactly the same or slightly better compression ratios can be adopted with much less friction.<p>It looks like the Iguana code has recently been relicensed with Apache instead of AGPL (which used for the rest of the sneller repo), which could lower the barrier for other projects to consider adopting Iguana, although there are still dependencies from the Iguana code to code in AGPL licensed files elsewhere in the sneller repo.<p>[1] <a href="https:&#x2F;&#x2F;corecursive.com&#x2F;data-compression-yann-collet&#x2F;" rel="nofollow">https:&#x2F;&#x2F;corecursive.com&#x2F;data-compression-yann-collet&#x2F;</a></div><br/><div id="36147448" class="c"><input type="checkbox" id="c-36147448" checked=""/><div class="controls bullet"><span class="by">pmh91</span><span>|</span><a href="#36147320">parent</a><span>|</span><a href="#36147682">next</a><span>|</span><label class="collapse" for="c-36147448">[-]</label><label class="expand" for="c-36147448">[5 more]</label></div><br/><div class="children"><div class="content">Hi! Sneller CTO here.<p>Thanks for pointing out the dependency on the AGPL bits -- I&#x27;m going to fix that ASAP. (There&#x27;s just a small utility library we&#x27;ve got to re-license as Apache-2 as well.)<p>Once we&#x27;re comfortable committing to the format for the long haul, we&#x27;re planning on publishing a C implementation as well. There are still a few tweaks we&#x27;ve been evaluating to try to make the compression ratio a tiny bit more competitive.</div><br/><div id="36147559" class="c"><input type="checkbox" id="c-36147559" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#36147320">root</a><span>|</span><a href="#36147448">parent</a><span>|</span><a href="#36147682">next</a><span>|</span><label class="collapse" for="c-36147559">[-]</label><label class="expand" for="c-36147559">[4 more]</label></div><br/><div class="children"><div class="content">this is very cool. thank you for releasing it! if you had to guess, how much of the performance depends on avx512 specifically? if this could run reasonably well on avx2, IMO this would be a really great general successor to LZ4</div><br/><div id="36147649" class="c"><input type="checkbox" id="c-36147649" checked=""/><div class="controls bullet"><span class="by">pmh91</span><span>|</span><a href="#36147320">root</a><span>|</span><a href="#36147559">parent</a><span>|</span><a href="#36147682">next</a><span>|</span><label class="collapse" for="c-36147649">[-]</label><label class="expand" for="c-36147649">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a good question.<p>For the pure-LZ77 Iguana variant (no rANS encoding), most of the decoding time is spent moving memory around rather than decoding the match+length tuples from the input stream, which suggests the performance difference wouldn&#x27;t be <i>that</i> great if we were only on AVX2, but AVX-512 has a bunch of instructions that are super helpful for parsing our base254 integers quickly. If I had to take a wild guess I&#x27;d say it would cost an additional 15%.<p>One sacrifice we made in the design is that the minimum match offset distance is always 32 bytes, which means we can always perform a literal or match copy by starting with a ymm register load + store. This hurts the compression ratio a bit but it helps performance immensely, and for that reason alone I suspect we&#x27;d still come out ahead of lz4 even without AVX-512.</div><br/><div id="36147921" class="c"><input type="checkbox" id="c-36147921" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#36147320">root</a><span>|</span><a href="#36147649">parent</a><span>|</span><a href="#36147682">next</a><span>|</span><label class="collapse" for="c-36147921">[-]</label><label class="expand" for="c-36147921">[2 more]</label></div><br/><div class="children"><div class="content">I remember reading that AVX-512 hurt the ability of Intel CPUs to turbo and run other tasks in parallel. This was a few years ago and I would hope it’s not the case anymore, especially since AMD has managed to add AVX-512 support too.<p>Have you done any testing with running multiple decompression tasks in parallel, or just running a single decompression task while at the same time running other tasks like maybe a web server?</div><br/><div id="36148840" class="c"><input type="checkbox" id="c-36148840" checked=""/><div class="controls bullet"><span class="by">Asm2D</span><span>|</span><a href="#36147320">root</a><span>|</span><a href="#36147921">parent</a><span>|</span><a href="#36147682">next</a><span>|</span><label class="collapse" for="c-36148840">[-]</label><label class="expand" for="c-36148840">[1 more]</label></div><br/><div class="children"><div class="content">The initial AVX-512 implementation brought a lot of issues with it. The biggest problem was that Intel used 512-bit ALUs from the beginning and I think it was just too much that time (initial 14nm node) - even AMD&#x27;s Zen4 architecture, which came years after Skylake-X, uses 256-bit ALUs for most of the operations except complex shuffles, which use a dedicated 512-bit unit to make them competitive. And from my experience, AMD&#x27;s Zen4 AVX-512 implementation is a very competitive one. I just wish it had faster gathers.<p>Our typical workload at Sneller uses most of the computational power of the machine: we typically execute heavy AVX-512 workloads on all available cores and we compare our processing performance at GB&#x2F;s per core. This is generally why we needed a faster decompression, because before Iguana almost 50% of the computational power was spent in a zstd decompressor, which is scalar. The rest of the code is written in Go, but it&#x27;s insignificant compared to how much time we spend executing AVX-512 now.<p>(I work for Sneller)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36147682" class="c"><input type="checkbox" id="c-36147682" checked=""/><div class="controls bullet"><span class="by">Alifatisk</span><span>|</span><a href="#36147320">prev</a><span>|</span><a href="#36147015">next</a><span>|</span><label class="collapse" for="c-36147682">[-]</label><label class="expand" for="c-36147682">[4 more]</label></div><br/><div class="children"><div class="content">What do people use these for?</div><br/><div id="36148701" class="c"><input type="checkbox" id="c-36148701" checked=""/><div class="controls bullet"><span class="by">bugboy73</span><span>|</span><a href="#36147682">parent</a><span>|</span><a href="#36147954">next</a><span>|</span><label class="collapse" for="c-36148701">[-]</label><label class="expand" for="c-36148701">[1 more]</label></div><br/><div class="children"><div class="content">DISCLAIMER: I&#x27;m one of the Sneller developers<p>Compression is required to make more efficient use of the available bandwidth and RAM. Although AWS provides up to 100Gb&#x2F;s networking, we improve the net throughput using compression. Also we cache data on the computing nodes, so storing it in compressed form allows us to store more data on the node.<p>Our query engine is often limited by memory bandwidth when using uncompressed data. So it needs a lot of data and it needs it fast. Because we cache data in compressed form, decompression is always needed, so it should be as fast as possible. That&#x27;s the primary reason why we developed Iguana, because a faster decompression results in a faster query engine.</div><br/></div></div><div id="36147954" class="c"><input type="checkbox" id="c-36147954" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#36147682">parent</a><span>|</span><a href="#36148701">prev</a><span>|</span><a href="#36147928">next</a><span>|</span><label class="collapse" for="c-36147954">[-]</label><label class="expand" for="c-36147954">[1 more]</label></div><br/><div class="children"><div class="content">Someone else brought up the recent podcast episode of CoRecursive. One benefit that was brought up was compression of the cache close to the user’s physical location. Facebook has a lot of machines around the world dedicated to this sort of cache. They need a balance between compression ratio and compression&#x2F;decompression throughput. There’s no point using a great compression algorithm to achieve an amazing compression ratio if you end up being CPU-bound when you need to decompress it later on.</div><br/></div></div></div></div><div id="36147015" class="c"><input type="checkbox" id="c-36147015" checked=""/><div class="controls bullet"><span class="by">aydyn</span><span>|</span><a href="#36147682">prev</a><span>|</span><a href="#36147378">next</a><span>|</span><label class="collapse" for="c-36147015">[-]</label><label class="expand" for="c-36147015">[4 more]</label></div><br/><div class="children"><div class="content">Decompression speed looks good, but in my experience once you get past a certain point (~X000 MB&#x2F;s) performance gains become pretty marginal in real world applications. I&#x27;d like to see compression speeds and performance on AVX if AVX-512 is not available.</div><br/><div id="36147465" class="c"><input type="checkbox" id="c-36147465" checked=""/><div class="controls bullet"><span class="by">gopalv</span><span>|</span><a href="#36147015">parent</a><span>|</span><a href="#36147275">next</a><span>|</span><label class="collapse" for="c-36147465">[-]</label><label class="expand" for="c-36147465">[1 more]</label></div><br/><div class="children"><div class="content">&gt; in my experience once you get past a certain point (~X000 MB&#x2F;s) performance gains become pretty marginal in real world applications<p>There might be no latency benefits to that, but there&#x27;s always a throughput gain from using fewer cpu cycles over all - the cpu cycles you would have spent decompressing can be used for other things.<p>For that reason single-core performance would still be nice to have and always great to have open source code which is performance centric so that we all get to find out &quot;How the hell did they do it?&quot; (it is very readable assembly [1])<p>&gt; I&#x27;d like to see compression speeds and performance on AVX if AVX-512 is not available.<p>The ARM is probably more interesting in the elastic compute world right now, the AVX world is unlikely to get cheaper.<p>And unlike before, I don&#x27;t need to wait for a vendor to sell me an SoC, Gravitons are already here.<p>[1] - <a href="https:&#x2F;&#x2F;github.com&#x2F;SnellerInc&#x2F;sneller&#x2F;blob&#x2F;master&#x2F;ion&#x2F;zion&#x2F;iguana&#x2F;decompress_iguana_amd64.s#L34">https:&#x2F;&#x2F;github.com&#x2F;SnellerInc&#x2F;sneller&#x2F;blob&#x2F;master&#x2F;ion&#x2F;zion&#x2F;i...</a></div><br/></div></div><div id="36147275" class="c"><input type="checkbox" id="c-36147275" checked=""/><div class="controls bullet"><span class="by">2h</span><span>|</span><a href="#36147015">parent</a><span>|</span><a href="#36147465">prev</a><span>|</span><a href="#36147378">next</a><span>|</span><label class="collapse" for="c-36147275">[-]</label><label class="expand" for="c-36147275">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t agree with this. The whole point of tools like this, is that we don&#x27;t live in age of dial up internet anymore, and haven&#x27;t for a long time.<p>Back then you wanted maximum compression like XZ, because the slowest part of the process was the download. Now especially with XZ awful decompress rate, the slowest part is extract. In many cases it&#x27;s better to have 9% larger size, for 500% increase in extract, like you get with ZST.</div><br/><div id="36147473" class="c"><input type="checkbox" id="c-36147473" checked=""/><div class="controls bullet"><span class="by">scaramanga</span><span>|</span><a href="#36147015">root</a><span>|</span><a href="#36147275">parent</a><span>|</span><a href="#36147378">next</a><span>|</span><label class="collapse" for="c-36147473">[-]</label><label class="expand" for="c-36147473">[1 more]</label></div><br/><div class="children"><div class="content">Definitely there&#x27;s a lot of workloads that are pure data-munging, where you don&#x27;t want to store the whole bloated JSON (or protobuf) mess on eg. S3 where you&#x27;re paying by the byte, but at the same time, you&#x27;re looking at key parts of your infra which are spending double digit percentages of their CPU time just decompressing stuff.<p>Sure, you can say the problem is JSON, protobuf, whatever. But in a huge organization with hundreds or thousands of types of documents, it&#x27;s a needless friction to require custom optimized formats for each one.<p>So a faster decompressor is often a no-brainer of a win and can have substantial dollar values attached.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>