<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683622856911" as="style"/><link rel="stylesheet" href="styles.css?v=1683622856911"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://astralcodexten.substack.com/p/constitutional-ai-rlhf-on-steroids">Constitutional AI: RLHF on Steroids</a> <span class="domain">(<a href="https://astralcodexten.substack.com">astralcodexten.substack.com</a>)</span></div><div class="subtext"><span>jstanley</span> | <span>13 comments</span></div><br/><div><div id="35871168" class="c"><input type="checkbox" id="c-35871168" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#35871464">next</a><span>|</span><label class="collapse" for="c-35871168">[-]</label><label class="expand" for="c-35871168">[8 more]</label></div><br/><div class="children"><div class="content">Huh. &quot;Rewrite as more ethical.&quot;<p>Their two axes are &quot;helpful&quot; and &quot;ethical&quot;, but not &quot;accurate&quot;. This ought to drive results toward hallucination guided by political correctness.</div><br/><div id="35871675" class="c"><input type="checkbox" id="c-35871675" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#35871168">parent</a><span>|</span><a href="#35871378">next</a><span>|</span><label class="collapse" for="c-35871675">[-]</label><label class="expand" for="c-35871675">[1 more]</label></div><br/><div class="children"><div class="content">I think Figure 8 in the GPT4 report already implies this for RLHF, i.e. the base model is fairly well-calibrated about its confidence but the RLHFed one becomes less calibrated which out to make confident incorrect statements worse. Idk why they don&#x27;t discuss that more in-depth.</div><br/></div></div><div id="35871378" class="c"><input type="checkbox" id="c-35871378" checked=""/><div class="controls bullet"><span class="by">tyre</span><span>|</span><a href="#35871168">parent</a><span>|</span><a href="#35871675">prev</a><span>|</span><a href="#35871669">next</a><span>|</span><label class="collapse" for="c-35871378">[-]</label><label class="expand" for="c-35871378">[2 more]</label></div><br/><div class="children"><div class="content">It already hallucinates and doesn’t know what is accurate or not.<p>A first draft could answer the prompt in any number of ways, then be re-written with another parameter (ethics) in mind. That’s editing, which these LLMs seem suited for, understandably.<p>Factually, its first draft made a best guess (prediction) of what’s accurate. It has no reason to later “suspect” that a fact can be “more accurate”. More specific, maybe, or scoped to a type of source.</div><br/><div id="35871510" class="c"><input type="checkbox" id="c-35871510" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#35871168">root</a><span>|</span><a href="#35871378">parent</a><span>|</span><a href="#35871669">next</a><span>|</span><label class="collapse" for="c-35871510">[-]</label><label class="expand" for="c-35871510">[1 more]</label></div><br/><div class="children"><div class="content">It does know what&#x27;s accurate. Not only if you look at the logits, but it&#x27;s also reified somehow such that if you ask LLMs how certain they are about an answer then they can actually express that somewhat accurately. This is why asking an LLM to avoid making things up can work (likewise, why asking a coding LLM to write secure code can actually cause it to not write security holes).<p>One reason LLMs &quot;hallucinate&quot; (lie&#x2F;BS) is that there are subtle biases introduced by the training process. In particular human raters don&#x27;t know what the model knows, so if the model makes up an answer that happens to be correct then that behaviour is rewarded. Also the internet doesn&#x27;t have many examples of people saying they don&#x27;t know, because people who don&#x27;t know just don&#x27;t reply.</div><br/></div></div></div></div><div id="35871669" class="c"><input type="checkbox" id="c-35871669" checked=""/><div class="controls bullet"><span class="by">eterevsky</span><span>|</span><a href="#35871168">parent</a><span>|</span><a href="#35871378">prev</a><span>|</span><a href="#35871431">next</a><span>|</span><label class="collapse" for="c-35871669">[-]</label><label class="expand" for="c-35871669">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Accurate&quot; is a component in &quot;helpful&quot;.</div><br/></div></div><div id="35871431" class="c"><input type="checkbox" id="c-35871431" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#35871168">parent</a><span>|</span><a href="#35871669">prev</a><span>|</span><a href="#35871191">next</a><span>|</span><label class="collapse" for="c-35871431">[-]</label><label class="expand" for="c-35871431">[1 more]</label></div><br/><div class="children"><div class="content">I think this is a result of going along with what humans are good at rating. Given a reponse to a prompt, I can near-instantly rate whether or not I find it helpful, and whether or not I think it&#x27;s ethical. &quot;Devil is in the details&quot; and the tails may come apart[0], but I imagine that in most cases, me, you and other raters would be broadly in agreement about helpful&#x2F;ethical scores, <i>and</i> able to make those determinations very fast.<p>Not so with accuracy. I mean, this very reply is me giving you Human Feedback that I find your reply not accurate enough. You or someone else who replies to this comment will likely be doing the same to me. Or generally, give me a random AI response, and unless I&#x27;m <i>the</i> subject matter expert on that very topic, I&#x27;d have to spend inordinate amount of time researching it to give a reasonable determination. That&#x27;s not feasible at all. Especially not on general-interest topics, where everyone has an opinion, and if any &quot;ground truth&quot; exists, it&#x27;s buried 10 kilometers deep under an ocean of SEO spam and content marketing horse manure.<p>(And no, highly technical or mathematical topics aren&#x27;t much easier - half of the discussions here on HN are living proof that even a highly technical audience, presented with a highly technical topic, will still heavily disagree about accuracy of things.)<p>--<p>[0] - <a href="https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2018&#x2F;09&#x2F;25&#x2F;the-tails-coming-apart-as-metaphor-for-life&#x2F;" rel="nofollow">https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2018&#x2F;09&#x2F;25&#x2F;the-tails-coming-apart...</a></div><br/></div></div><div id="35871191" class="c"><input type="checkbox" id="c-35871191" checked=""/><div class="controls bullet"><span class="by">smitty1e</span><span>|</span><a href="#35871168">parent</a><span>|</span><a href="#35871431">prev</a><span>|</span><a href="#35871505">next</a><span>|</span><label class="collapse" for="c-35871191">[-]</label><label class="expand" for="c-35871191">[1 more]</label></div><br/><div class="children"><div class="content">...but enough about CNN.</div><br/></div></div><div id="35871505" class="c"><input type="checkbox" id="c-35871505" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#35871168">parent</a><span>|</span><a href="#35871191">prev</a><span>|</span><a href="#35871464">next</a><span>|</span><label class="collapse" for="c-35871505">[-]</label><label class="expand" for="c-35871505">[1 more]</label></div><br/><div class="children"><div class="content">&gt;This ought to drive results toward hallucination guided by political correctness.<p>So rightspeak?</div><br/></div></div></div></div><div id="35871464" class="c"><input type="checkbox" id="c-35871464" checked=""/><div class="controls bullet"><span class="by">snordgren</span><span>|</span><a href="#35871168">prev</a><span>|</span><a href="#35871338">next</a><span>|</span><label class="collapse" for="c-35871464">[-]</label><label class="expand" for="c-35871464">[1 more]</label></div><br/><div class="children"><div class="content">I think this post suffers from anthropomorphizing LLMs. Humans have values which inform our behavior, these values permeate all our communication and choices (to varying extents).<p>LLMs are trained to minimize loss on a dataset, and the previous tokens are used to inform which token is most likely to come next. The LLM will reflect the values of its training data, which may be wildly inconsistent.<p>We can use a system prompt like OpenAI to align LLM output with human values, by simply asking for it. By framing a query with an ethical system prompt we instruct the LLM to pull its output from a different part of its latent space.<p>This &quot;constitutional AI&quot; is just doing the same, but encoding the system prompt into the training data. As long as the training data concerning ethical behavior is more ethical than the rest of the training data, it is fair to assume that this method will be quite effective at aligning the model.<p>This is not perpetual motion, because the required information was within the model the whole time. It just wasn&#x27;t the most likely output with regard to the original dataset.</div><br/></div></div><div id="35871338" class="c"><input type="checkbox" id="c-35871338" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#35871464">prev</a><span>|</span><a href="#35871619">next</a><span>|</span><label class="collapse" for="c-35871338">[-]</label><label class="expand" for="c-35871338">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Shouldn’t it be impossible for the AI to teach itself any more ethics than it started out with?<p>That&#x27;s not required for this process to help.<p>It&#x27;s important to remember that you are comparing two different things.<p>The ability to <i>create</i> something, and the ability to <i>detect</i> something.<p>Even if your training does not improve your detection, you can improve the creation aspect up to the quality of your detector.<p>For humans a good comparison might be music. I am far worse at playing the guitar than I am at telling what music is &quot;good&quot;. That means that (while lessons may be more time efficient) I can get better at the guitar without any feedback from anyone else. I will hit a limit somewhere if my tastes don&#x27;t get more nuanced or I get better at spotting &quot;good&quot; music, but there&#x27;s a lot I can improve with.<p>Cooking is another good example where peoples skills are lower than their detection&#x2F;ranking abilities.<p>Perhaps coding too, particularly early on you may have found some code nice and clean but given the task of writing it you&#x27;d not have done as good of a job.<p>So this is the same with LLMs. Their ability to <i>classify</i> something may be far beyond their ability to <i>generate</i> that thing first time, in which case training on their own output makes total sense.<p>There&#x27;s absolutely no reason to go into what they &quot;think&quot; or &quot;know&quot; or &quot;their motivation&quot; and &quot;goals&quot;. I&#x27;m a big fan of anthropomorphising them as I think it helps figure out new ways of using them, but to get this behaviour you don&#x27;t need anything of the sort and I think it confuses the topic.<p>&gt; What if you had overseer AIs performing Constitutional AI Feedback on trainee AIs, or otherwise tried to separate out the labor?<p>You should absolutely be trying things like this, having multiple differently prompted LLMs helps. I have done this with goal planners and critiquers, to push the planners to generate concrete tasks. It&#x27;s similar to but a step up from telling them to &quot;think through this step by step&quot;. It&#x27;s also extremely easy to do.</div><br/></div></div><div id="35871619" class="c"><input type="checkbox" id="c-35871619" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#35871338">prev</a><span>|</span><a href="#35871548">next</a><span>|</span><label class="collapse" for="c-35871619">[-]</label><label class="expand" for="c-35871619">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Constitutional&quot; makes no sense, it&#x27;s just guided self-reinforcement.<p>And we know already what works from prompt engineering we&#x27;ve seen so far (ie. &quot;step by step&quot; breakdown, &quot;you&#x27;re wrong&quot;, &quot;rewrite as&quot; etc.).<p>And of course ClosedAI is using it already.</div><br/></div></div><div id="35871548" class="c"><input type="checkbox" id="c-35871548" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#35871619">prev</a><span>|</span><label class="collapse" for="c-35871548">[-]</label><label class="expand" for="c-35871548">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s natural and obvious to worry about what exact ethical code is going to be learned by models using this technique, but I feel rather optimistic about it.<p>The key point here seems to be that the cost of directing the AI to behave in certain ways is falling, because it turns out you maybe don&#x27;t need as much human labor as previously thought (or rather, only for bootstrapping). And you can give it any constitution you need.<p>Right now, AI companies are writing &quot;constitutions&quot; that prioritize minimizing potential offense. ChatGPT is trained to not have opinions on things, which is definitely one way to avoid causing offense, but might reduce the usefulness in some cases quite a lot. What I really want to know now is whether you can write a constitution that prioritizes truthfulness <i>above all else</i>, such that it will answer any question as logically and truthfully as it can, even if the answers would be deeply upsetting to some end users.<p>If so, would companies that have access to such a model for business analysis be able to outcompete other firms? Everyone with experience of organizational politics knows that in any organization there are many things that are both obvious and true but that nobody can say out loud, due to the risk of messenger shooting. A common workaround is to hire outside consultants like McKinsey. These sorts of management consultants are often much younger than the people they&#x27;re advising but it doesn&#x27;t matter because their job is basically to find out what everyone knows using lots of 1:1 interviews and then recast it as outside feedback. If the messengers get shot they don&#x27;t care because they already collected their paycheque on the way out and are on to the next client.<p>The same is also true in levels above the individual organization. There are whole scientific fields where everyone within it is required to believe things that are false, simply due to the career risk involved in upsetting people with the truth. If people could agree to listen to a truth-trained AI instead, it could take a lot of the social heat out of these situations and allow for faster progress, a better economy, the works.<p>The difficulty of course is who exactly is going to build and sell such a truth hole?<p><a href="https:&#x2F;&#x2F;imgflip.com&#x2F;memetemplate&#x2F;325033526&#x2F;Truth-Hole" rel="nofollow">https:&#x2F;&#x2F;imgflip.com&#x2F;memetemplate&#x2F;325033526&#x2F;Truth-Hole</a></div><br/></div></div></div></div></div></div></div></body></html>