<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1732957257155" as="style"/><link rel="stylesheet" href="styles.css?v=1732957257155"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anthropic.com/research/statistical-approach-to-model-evals">A statistical approach to model evaluations</a> <span class="domain">(<a href="https://www.anthropic.com">www.anthropic.com</a>)</span></div><div class="subtext"><span>RobinHirst11</span> | <span>20 comments</span></div><br/><div><div id="42276099" class="c"><input type="checkbox" id="c-42276099" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#42278949">next</a><span>|</span><label class="collapse" for="c-42276099">[-]</label><label class="expand" for="c-42276099">[16 more]</label></div><br/><div class="children"><div class="content">This does feel a bit like under grad introduction to statistical analysis and surprising anyone felt the need to explain these things. But I also suspect most AI people out there now a days have limited math skills so maybe it’s helpful?</div><br/><div id="42277246" class="c"><input type="checkbox" id="c-42277246" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#42276099">parent</a><span>|</span><a href="#42277857">next</a><span>|</span><label class="collapse" for="c-42277246">[-]</label><label class="expand" for="c-42277246">[12 more]</label></div><br/><div class="children"><div class="content">As an ML researcher who started in physics (this seems common among physics&#x2F;math turned ML people. Which Evan is included), I cannot tell you how bad is it... One year at CVPR when diffusion models hit the scenes I was asking what people&#x27;s covariance was (I had overestimated the model complexity), and the most common answer I got was &quot;how do I calculate that?&quot; People do not understand things like what &quot;pdf&quot; means. People at top schools! I&#x27;ve been told I&#x27;m &quot;gatekeeping&quot; for saying that you should learn math (I say &quot;you don&#x27;t need math to build good models, but you do to understand why they&#x27;re wrong&quot;). Not that you need to, but should. (I guess this explains why Mission Impossible Language Models won best paper...)<p>I swear, the big reason models are black boxes are because we _want_ them to be. There&#x27;s clear anti-sentiment mentality against people doing theory and the result of this shows. I remember not too long ago Yi Tay (under @agihippo but main is @YiTayML) said &quot;fuck theorists&quot;. I guess it&#x27;s not a surprise Deep Mind recently hired him after that &quot;get good&quot; stuff.<p>Also, I&#x27;d like to point out, the author uses &quot;we&quot; but the paper only has one author on it. So may I suggest adding their cat as a coauthor? [0]<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;F._D._C._Willard" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;F._D._C._Willard</a></div><br/><div id="42279960" class="c"><input type="checkbox" id="c-42279960" checked=""/><div class="controls bullet"><span class="by">abhgh</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42277246">parent</a><span>|</span><a href="#42277951">next</a><span>|</span><label class="collapse" for="c-42279960">[-]</label><label class="expand" for="c-42279960">[5 more]</label></div><br/><div class="children"><div class="content">Personal sad story, but hopefully relevant: during my recent PhD I worked on a problem where I used a Dirichlet Process in my solution. That paper has been bouncing around for the past few years getting rejected from every venue I have submitted it to. My interpretation is that most reviewers (there are exceptions - too few to impact the final voting) don&#x27;t understand any non-DL theory anymore and are not willing to read up for the sake of a fair review. This is based on their comments, where we have been told that our solution is complex (maybe? - but no one suggests an alternative), exposition is not clear (we have rewritten the paper a few times - we rewrite it based on comments from venue i to submit to venue i+1 - its a wild goose chase), and in one case, someone said the paper is derivative because it uses Blackwell-MacQueen sampling; their evidence? - they skimmed through a paper we had cited that also used the sampling algorithm. This is like saying a paper is derivative because it uses SGD.<p>I am on the review panel of some conferences too and it is not uncommon to be assigned a paper outside of my comfort zone. That doesn&#x27;t mean I cut and bail. You set aside time, read up on the area, ask authors questions, and judge accordingly. Unfortunately this doesn&#x27;t happen most of the time - people seem to be in a rush to finish their review no matter the quality. At this point, we just mechanically keep resubmitting the paper every once a while.<p>Sorry, end of rant :)</div><br/><div id="42280379" class="c"><input type="checkbox" id="c-42280379" checked=""/><div class="controls bullet"><span class="by">aspenmayer</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42279960">parent</a><span>|</span><a href="#42277951">next</a><span>|</span><label class="collapse" for="c-42280379">[-]</label><label class="expand" for="c-42280379">[4 more]</label></div><br/><div class="children"><div class="content">Is a preprint of your paper available?<p>I looked at your blog a bit and was able to find this, which may be it?<p>&gt; Learning Interpretable Models Using Uncertainty Oracles<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1906.06852" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1906.06852</a><p><a href="https:&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.1906.06852" rel="nofollow">https:&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.1906.06852</a></div><br/><div id="42280409" class="c"><input type="checkbox" id="c-42280409" checked=""/><div class="controls bullet"><span class="by">abhgh</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42280379">parent</a><span>|</span><a href="#42277951">next</a><span>|</span><label class="collapse" for="c-42280409">[-]</label><label class="expand" for="c-42280409">[3 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s the one: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1906.06852" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1906.06852</a></div><br/><div id="42280426" class="c"><input type="checkbox" id="c-42280426" checked=""/><div class="controls bullet"><span class="by">aspenmayer</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42280409">parent</a><span>|</span><a href="#42277951">next</a><span>|</span><label class="collapse" for="c-42280426">[-]</label><label class="expand" for="c-42280426">[2 more]</label></div><br/><div class="children"><div class="content">I copied the DOI for convenience but they’re the same paper.<p>I have no formal math background really so I can’t speak to your methods but I appreciate that you have shared your work freely.<p>Did you have any issues defending your thesis due to the issues you described above related to publishing?<p>Noticed a typo in your abstract:<p>“Maybe” should be “may be” in sentence below (italics):<p>&gt; We show that this technique addresses the above challenges: (a) it arrests the reduction in accuracy that comes from shrinking a model (in some cases we observe  improvement over baselines), and also, (b) that this <i>maybe</i> applied with no change across model families with different notions of size; results are shown for Decision Trees, Linear Probability models and Gradient Boosted Models.</div><br/><div id="42280480" class="c"><input type="checkbox" id="c-42280480" checked=""/><div class="controls bullet"><span class="by">abhgh</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42280426">parent</a><span>|</span><a href="#42277951">next</a><span>|</span><label class="collapse" for="c-42280480">[-]</label><label class="expand" for="c-42280480">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it did come up during my defense, but it was deemed not to be a concern since I had one prior paper [1] (the original one in this thread of work, the paper I linked above was an improvement over it), and my advisor (co-author on both papers) vouched for the quality of the work.<p>Thank you for pointing out the typo - will fix it!<p>[1] <a href="https:&#x2F;&#x2F;www.frontiersin.org&#x2F;journals&#x2F;artificial-intelligence&#x2F;articles&#x2F;10.3389&#x2F;frai.2020.00003&#x2F;pdf" rel="nofollow">https:&#x2F;&#x2F;www.frontiersin.org&#x2F;journals&#x2F;artificial-intelligence...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="42277951" class="c"><input type="checkbox" id="c-42277951" checked=""/><div class="controls bullet"><span class="by">mturmon</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42277246">parent</a><span>|</span><a href="#42279960">prev</a><span>|</span><a href="#42279156">next</a><span>|</span><label class="collapse" for="c-42277951">[-]</label><label class="expand" for="c-42277951">[2 more]</label></div><br/><div class="children"><div class="content">The front matter in Vladimir Vapnik’s book “Statistical Learning Theory” (first edition published 1995) has this quote:<p>*<p>During the last few years at various computer science conferences, I heard reiteration of the following claim:<p>“Complex theories do not work; simple algorithms do.”<p>One of the goals of this book is to show that, at least in the problems of statistical inference, this is not true. I would like to demonstrate that in the area of science a good old principle is valid:<p>“Nothing is more practical than a good theory.”<p>*<p>It’s seen in page xii of the front matter at: <a href="https:&#x2F;&#x2F;link.springer.com&#x2F;content&#x2F;pdf&#x2F;bfm:978-1-4757-3264-1&#x2F;1" rel="nofollow">https:&#x2F;&#x2F;link.springer.com&#x2F;content&#x2F;pdf&#x2F;bfm:978-1-4757-3264-1&#x2F;...</a><p>Vladimir was a friend during this time, and I think about this quote a lot with regards to ML tinkering.</div><br/><div id="42279893" class="c"><input type="checkbox" id="c-42279893" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42277951">parent</a><span>|</span><a href="#42279156">next</a><span>|</span><label class="collapse" for="c-42279893">[-]</label><label class="expand" for="c-42279893">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t had a chance to read that, but that quote suggests I should (especially considering the author and the editors).<p>I often refer to &quot;Elephant Fitting&quot; w.r.t these systems. I suspect you understand this, but I think most think it is just about overfitting. But the way problem isn&#x27;t about the number of parameters, but that parameters need to be justified. As explained by Dyson here[0]. Vladimir&#x27;s quote really reminds me of this. Fermi likewise was stressing the importance of theory.<p>I think it is a profound quote, and you were (are?) lucky to have that friendship. I do think abstraction is at the heart of intelligence. François Chollet discusses it a lot, and he&#x27;s far from alone. It seems to be well agreed upon in the neuroscience and cognitive science communities. I think this is essential to understand in our path forward to developing intelligent systems, because there are plenty of problems that need to be solved in which there is no algorithmic procedure. Where there is no explicit density function. Intractable, doubly intractable, and more problems. Maybe we&#x27;re just too dumb, but it&#x27;s clear there are plateaus where luck is needed to advance. I do not believe our current machines would be capable of closing a gap.<p>[0] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=hV41QEKiMlM" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=hV41QEKiMlM</a></div><br/></div></div></div></div><div id="42279156" class="c"><input type="checkbox" id="c-42279156" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42277246">parent</a><span>|</span><a href="#42277951">prev</a><span>|</span><a href="#42277647">next</a><span>|</span><label class="collapse" for="c-42279156">[-]</label><label class="expand" for="c-42279156">[2 more]</label></div><br/><div class="children"><div class="content">As someone who had questions about some of what you said and feels legitimately scared to ask what you meant out of fear of being judged:<p>&gt;  I&#x27;ve been told I&#x27;m &quot;gatekeeping&quot;<p>I mean...when the alternative is politely (better yet - excitedly) answering the question asked? You kind of are.<p>&gt; I swear, the big reason models are black boxes are because we _want_ them to be.<p>Talk is cheap.<p>&gt;  I guess it&#x27;s not a surprise Deep Mind recently hired him after that &quot;get good&quot; stuff.<p>I agree &quot;fuck theorists&quot; is in no way constructive. But, Deep Mind has objectively helped move the field forward. And your criticism of &quot;get good&quot; stuff? Did you not just tell people to &quot;learn math&quot; rather than help them to understand it yourself? That&#x27;s the _exact_ meaning of the phrase &quot;get good&quot; on the internet. At best you&#x27;re both being about as toxic (at least from your own description).</div><br/><div id="42279828" class="c"><input type="checkbox" id="c-42279828" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42279156">parent</a><span>|</span><a href="#42277647">next</a><span>|</span><label class="collapse" for="c-42279828">[-]</label><label class="expand" for="c-42279828">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; when the alternative is politely (better yet - excitedly) answering the question asked? You kind of are.
</code></pre>
Gatekeeping is controlling access. Not to be confused with hurdles. I&#x27;m more than happy to have more people in &quot;the party.&quot; No one is being excluded in the way that isn&#x27;t also true for any other field. You unfortunately need some level of expertise to be able to understand discussions between experts. But am I stopping you from getting that expertise? No, in fact I&#x27;m very happy to lend a hand! Those aren&#x27;t gates, they&#x27;re hurdles. You don&#x27;t need a specific PhD or to go to a good school or anything. It&#x27;s about the knowledge. If you need a helping hand to get over, ask, because others may not know or may not know if you&#x27;re struggling fruitlessly or struggling as part of the process of improving.<p>But yes, hurdles exist and they are not bad. I sure as hell don&#x27;t want someone that can&#x27;t do calculus designing rocket engines. And you probably don&#x27;t want a rocket engineer performing surgery on you. Call them what you will, but it&#x27;s not a bouncer at the door telling you you&#x27;re not &quot;pretty enough&quot;, which is what gatekeeping is generally used to refer to.<p><pre><code>  &gt; Talk is cheap.
</code></pre>
Sure, but we actually know a lot more about the inner workings of networks than most people realize. Sure, they aren&#x27;t transparent, but that doesn&#x27;t mean they are completely opaque either.<p>But I have no idea how to respond to this comment. What I said was fairly broad and this response is broader. Are you asking for &quot;proof&quot;? Of what? Interpretability? Is not the article proof of it to some degree? Or Golden Gate Claude?<p><pre><code>  &gt;  Did you not just tell people to &quot;learn math&quot; rather than help them to understand it yourself? 
</code></pre>
No? I think you misunderstand. Mind you, this is hacker news. Would you like some books for reference? A roadmap? If you have suggestions for how I should phrase my venting differently, I&#x27;m all ears. But it feels like that would be out of left field to just drop a bunch of random books and requires a lot of words to explain how all these things connect. I&#x27;ve written many &quot;walls of text&quot; here and frankly, anything longer than a paragraph often gets skipped over. It&#x27;s fine, it&#x27;s HN after all.<p><pre><code>  &gt; you&#x27;re both being about as toxic
</code></pre>
Are you aware of the things I&#x27;m referencing? It seems like you are not. Given that, I think you should reserve your judgement and accusations until you know more about the context. (e.g.<p>So I will add more context to clarify my complaints, for any of those interested.
I specifically called out Mission Impossible Language Models[0], so what&#x27;s that about? I suggest reading the paper. The authors create a continuum of difficulties in impossible languages. The hardest being a random word ordering. The claim is that LLMs can&#x27;t learn impossible languages just as well as natural languages. It&#x27;s fairly easy to understand the error in this work. They use perplexity, which is sometimes called &quot;surprisal.&quot; You take it conditioned on the previous words and you calculate what is likely to come next. But perplexity doesn&#x27;t tell you that the model didn&#x27;t learn the language, or even efficiently. The metric isn&#x27;t going to work for a one-to-one comparison with a structured language. The reason being that there is naturally more entropy in the impossible language. Frankly, because there are more words that are equally likely to come next. It&#x27;s comparing coin flips to dice throws.<p>Let&#x27;s use an example: our target sentence will be &quot;My name is godelski.&quot; In a random shuffle language we have 4! (24) ways to represent that sentence that are all __equivalent__. That&#x27;s the key part. In natural language, all I can think of is 2 (&quot;Godelski, my name is&quot; as a highly unlikely alternative). So in natural language if we have &quot;My name&quot; and are predicting the next word, &quot;is&quot; is pretty likely. But in the random language &quot;is&quot; is just as likely as &quot;&lt;name&gt;&quot;. This isn&#x27;t proof that the language isn&#x27;t learned, it is just that the language isn&#x27;t structured. &quot;My name is godelski&quot; and &quot;My name godelski is&quot; are equivalent sentences in a random ordering language. But actually, this gets even harder because the tokenization was trained on natural word order. If you look at Table 1 you&#x27;ll see how this gets messy (notice that &quot;bookshelf&quot; is the tokens &quot; books&quot; (space intentional) &quot;he&quot; &quot;lf&quot;). The picture gets clearer when you look at how they prepared the data (it isn&#x27;t shuffled each time the model gets the data, it is shuffled once and then the model is trained on that. This is not the same as the random language and unless you&#x27;re really lucky, there&#x27;s going to be certain patterns more common than others and so that&#x27;ll just make it more difficult for the model. The dataloader should shuffle sentences, which will teach the model to ignore the patterns. You should also measure perplexity against all valid predictions, not a single one. This one is a killer for me).<p>Side note:<p><pre><code>  &gt; fear of being judged:
</code></pre>
You&#x27;re always going to be judged. Stand up for yourself and what you believe in. Don&#x27;t be afraid of being wrong either. Truth is, you&#x27;re always wrong. It&#x27;s a matter of degree. The problem isn&#x27;t being wrong, it is not being able to change your mind. Even if things get heated between people, there typically isn&#x27;t ill will between them if they believe the other person is capable of changing their mind.<p>Clearly, you have judged me quite harshly.<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.06416" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.06416</a></div><br/></div></div></div></div><div id="42277647" class="c"><input type="checkbox" id="c-42277647" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42277246">parent</a><span>|</span><a href="#42279156">prev</a><span>|</span><a href="#42277857">next</a><span>|</span><label class="collapse" for="c-42277647">[-]</label><label class="expand" for="c-42277647">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s your objection to Mission Impossible Language Models?</div><br/><div id="42280335" class="c"><input type="checkbox" id="c-42280335" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#42276099">root</a><span>|</span><a href="#42277647">parent</a><span>|</span><a href="#42277857">next</a><span>|</span><label class="collapse" for="c-42280335">[-]</label><label class="expand" for="c-42280335">[1 more]</label></div><br/><div class="children"><div class="content">I see you&#x27;re one of the authors.<p>I disagree with the conclusions of the paper. Maybe I have some misunderstandings, and if so, please do correct me. But my reading of it, is that the experiments and evaluations are insufficient to formulate the conclusion made. I think the results even make sense with Chomsky&#x27;s claim. (I&#x27;ll stick to the random shuffle for clarity)<p>It does not appear that the evaluations are considering all possible valid outputs for the next token. Perplexity is not actually a measure of language performance, though it is wonderful that it has worked out so well so far (I suspect due to the structure in languages). The perplexity being higher is not necessarily indicative of poorer performance. I view this as analogous to sequences of coin flips (our natural language) to sequences of dice rolls (our shuffle). One naturally has more randomness than another. A model that successfully learns the former will have lower perplexity than the model that learns the latter.<p>To properly evaluate we need to consider if the model is able to produce valid sentences, and consistently. With our coin and dice analogy let&#x27;s assume we have a sequence of 3 events. Our model conditions on a single flip of heads and we can estimate likelihoods for the sequences HHH, HHT, HTH, HTT, THH, THT, TTH, TTT. Our successful model will tell us that the last 4 are not possible, but that the others are equally likely. Now if we compare to a dice roll, conditioned on a roll of a 1, then the model is not invalid for suggesting higher entropy. That is exactly what we want our model to do. There are just more _valid_ answers. In the same way if we&#x27;re predicting (conditionally) next token, then we should expect a higher perplexity in the &quot;more impossible&quot; languages, but that does not tell us the success of learning the language (I would also expect these models to take longer to converge due to this, just as with coins and dice. I&#x27;ll leave &quot;learn just as well&quot; to Chomsky, as this is ambiguous).<p>Entropy isn&#x27;t enough. Our metric needs to be based on the mass distribution. To compare against one another, we&#x27;d have to normalize the values to their distributions. A direct comparison to one another will always lead to the random shuffle model having higher perplexity (just as with coins and dice), so it is an unfair comparison. Without the normalization we&#x27;d expect to find exactly what is shown in Figure 2.<p>As I understand the writing and the code, you do not compare against all valid tokens, but rather the fixed ones. I&#x27;m just seeing the perplexities counted in the usual way (I see loop over batch, but not for valid permutations). I see the line in the text<p><pre><code>  &gt; dataset shuffling during training.
</code></pre>
So I assume that this means the dataloader is shuffling the selected sentences? I don&#x27;t see this in the code but I&#x27;m happy to trust you if you say yes. But the code makes me think this was generated beforehand (I&#x27;m having dependency issues so can&#x27;t verify).  But if you are generating the perturbations beforehand, then I think the results are irrelevant because you haven&#x27;t been implicitly teaching the model that ordering doesn&#x27;t matter. The fact that results get worse for the models without positional encoding is suggestive of concern here. If position does not matter, why does the positional information increase the model&#x27;s ability to learn? It should be irrelevant to a non-deterministic shuffled language. I am also suspect since the &quot;no shuffle&quot; model appears to have identical learning capabilities w.r.t Fig 2 and 6. (I&#x27;m also seeing a lot of reference to error bars but it isn&#x27;t clear to me what the variance is. Is the bar smaller than the markers? Scaling could really help here as well as placing horizontal bars at the bounds given the visualization of the markers in the legends).<p>As for limitations, I am also suspect there&#x27;s a bias introduced due to tokenization. Since the tokenization embedding is generated from the expected ordering. I think this adds additional complexity that could be reduced, but not eliminated, by shuffling words instead of tokens. Not eliminated because tokens are only dependent on single words, but the sentences themselves. Word pairs and sequences matter.<p>Fwiw, I don&#x27;t agree with Chomsky. Clearly LLMs are extracting structure in language and I think it is obtuse to claim that a system designed for pattern matching won&#x27;t identify these patterns. One doesn&#x27;t need reasoning or abstraction to converge to this, one simply needs sufficient sampling and for structure to exist. Clearly structure exists in the language, so we should expect a sufficient pattern matcher to be able to extract these patterns.</div><br/></div></div></div></div></div></div><div id="42277857" class="c"><input type="checkbox" id="c-42277857" checked=""/><div class="controls bullet"><span class="by">runeblaze</span><span>|</span><a href="#42276099">parent</a><span>|</span><a href="#42277246">prev</a><span>|</span><a href="#42278727">next</a><span>|</span><label class="collapse" for="c-42277857">[-]</label><label class="expand" for="c-42277857">[1 more]</label></div><br/><div class="children"><div class="content">Maths also mean different things. Your average number theorist or algebraic geometer will most likely not touch statistical techniques day-to-day. Reading this Anthropic article was helpful because I am constantly catching up on my statistical background</div><br/></div></div><div id="42278727" class="c"><input type="checkbox" id="c-42278727" checked=""/><div class="controls bullet"><span class="by">lukev</span><span>|</span><a href="#42276099">parent</a><span>|</span><a href="#42277857">prev</a><span>|</span><a href="#42277130">next</a><span>|</span><label class="collapse" for="c-42278727">[-]</label><label class="expand" for="c-42278727">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what it&#x27;s true to suspect, since clearly a lot of very smart people are working in the field, in places.<p>It is empirically true that none of the industry discourse around leaderboards and benchmarks uses any of the techniques this article discusses.</div><br/></div></div><div id="42277130" class="c"><input type="checkbox" id="c-42277130" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42276099">parent</a><span>|</span><a href="#42278727">prev</a><span>|</span><a href="#42278949">next</a><span>|</span><label class="collapse" for="c-42277130">[-]</label><label class="expand" for="c-42277130">[1 more]</label></div><br/><div class="children"><div class="content">AI engineers just use &quot;vibes&quot; currently haha</div><br/></div></div></div></div><div id="42278949" class="c"><input type="checkbox" id="c-42278949" checked=""/><div class="controls bullet"><span class="by">Unlisted6446</span><span>|</span><a href="#42276099">prev</a><span>|</span><a href="#42279817">next</a><span>|</span><label class="collapse" for="c-42278949">[-]</label><label class="expand" for="c-42278949">[1 more]</label></div><br/><div class="children"><div class="content">All things considered, although I&#x27;m in favor of Anthropic&#x27;s suggestions, I&#x27;m surprised that they&#x27;re not recommending more (nominally) advanced statistical methods. I wonder if this is because more advanced methods don&#x27;t have any benefits or if they don&#x27;t want to overwhelm the ML community.<p>For one, they could consider using equivalence testing for comparing models, instead of significance testing. I&#x27;d be surprised if their significance tests were not significant given 10000 eval questions and I don&#x27;t see why they couldn&#x27;t ask the competing models 10000 eval questions?<p>My intuition is that multilevel modelling could help with the clustered standard errors, but I&#x27;ll assume that they know what they&#x27;re doing.</div><br/></div></div><div id="42279817" class="c"><input type="checkbox" id="c-42279817" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#42278949">prev</a><span>|</span><a href="#42277958">next</a><span>|</span><label class="collapse" for="c-42279817">[-]</label><label class="expand" for="c-42279817">[1 more]</label></div><br/><div class="children"><div class="content">Since when the heck did evals change what they referred to. Evals were what you did to check if the output of a model was correct. What happened ?</div><br/></div></div><div id="42277958" class="c"><input type="checkbox" id="c-42277958" checked=""/><div class="controls bullet"><span class="by">ipunchghosts</span><span>|</span><a href="#42279817">prev</a><span>|</span><label class="collapse" for="c-42277958">[-]</label><label class="expand" for="c-42277958">[1 more]</label></div><br/><div class="children"><div class="content">I have been promoting this and saying it since at least 2018. You can see my publication record as evidence!!!<p>&quot;Random seed xxx is all you need&quot; was another demonstration of this need.<p>You actually want a wilcoxon sum rank test as many metrics are not gaussian especially as they get to thier limits!! I.e. accuracy roughly 99 or 100! Then it becomes highly sub gaussian.</div><br/></div></div></div></div></div></div></div></body></html>