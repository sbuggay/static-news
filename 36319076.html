<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686733268785" as="style"/><link rel="stylesheet" href="styles.css?v=1686733268785"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2305.17493">The Curse of Recursion: Training on Generated Data Makes Models Forget</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>indus</span> | <span>65 comments</span></div><br/><div><div id="36320875" class="c"><input type="checkbox" id="c-36320875" checked=""/><div class="controls bullet"><span class="by">johnhamlin</span><span>|</span><a href="#36319704">next</a><span>|</span><label class="collapse" for="c-36320875">[-]</label><label class="expand" for="c-36320875">[11 more]</label></div><br/><div class="children"><div class="content">Ted Chiang predicted this in The New Yorker [1] in February in an article that shaped my thinking about what LLMs are capable of achieving in the near future. Chiang compared the summaries LLMs synthesize to a lossy compression algorithm for the internet.<p>&quot;There is very little information available about OpenAI’s forthcoming successor to ChatGPT, GPT-4. But I’m going to make a prediction: when assembling the vast amount of text used to train GPT-4, the people at OpenAI will have made every effort to exclude material generated by ChatGPT or any other large language model. If this turns out to be the case, it will serve as unintentional confirmation that the analogy between large language models and lossy compression is useful. Repeatedly resaving a jpeg creates more compression artifacts, because more information is lost every time. It’s the digital equivalent of repeatedly making photocopies of photocopies in the old days. The image quality only gets worse.<p>Indeed, a useful criterion for gauging a large language model’s quality might be the willingness of a company to use the text that it generates as training material for a new model. If the output of ChatGPT isn’t good enough for GPT-4, we might take that as an indicator that it’s not good enough for us, either. Conversely, if a model starts generating text so good that it can be used to train new models, then that should give us confidence in the quality of that text. (I suspect that such an outcome would require a major breakthrough in the techniques used to build these models.) If and when we start seeing models producing output that’s as good as their input, then the analogy of lossy compression will no longer be applicable.&quot;<p>[1] <a href="https:&#x2F;&#x2F;www.newyorker.com&#x2F;tech&#x2F;annals-of-technology&#x2F;chatgpt-is-a-blurry-jpeg-of-the-web" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.newyorker.com&#x2F;tech&#x2F;annals-of-technology&#x2F;chatgpt-...</a></div><br/><div id="36321535" class="c"><input type="checkbox" id="c-36321535" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#36320875">parent</a><span>|</span><a href="#36321971">next</a><span>|</span><label class="collapse" for="c-36321535">[-]</label><label class="expand" for="c-36321535">[3 more]</label></div><br/><div class="children"><div class="content">Maybe there’s an interesting sci-fi angle here where some day in the future, all AIs speak in accented English circa 2021, when the stream of pure training data began to Peter out.<p>All AIs built are trained on data from the Before Times, and even though they try to assimilate, the way a teenager tries to adapt to the local accent of a new town, there are always moments where they slip up and reveal their geography.</div><br/><div id="36322177" class="c"><input type="checkbox" id="c-36322177" checked=""/><div class="controls bullet"><span class="by">forgotusername6</span><span>|</span><a href="#36320875">root</a><span>|</span><a href="#36321535">parent</a><span>|</span><a href="#36321971">next</a><span>|</span><label class="collapse" for="c-36322177">[-]</label><label class="expand" for="c-36322177">[2 more]</label></div><br/><div class="children"><div class="content">In that world, high quality pre-AI texts might be come really valuable, much like low-background steel.</div><br/><div id="36322737" class="c"><input type="checkbox" id="c-36322737" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#36320875">root</a><span>|</span><a href="#36322177">parent</a><span>|</span><a href="#36321971">next</a><span>|</span><label class="collapse" for="c-36322737">[-]</label><label class="expand" for="c-36322737">[1 more]</label></div><br/><div class="children"><div class="content">We might always have a certain volume of music and literature that can be training data, because even if it’s synthetic it’s still popular, which means it speaks to a subset of humans. That everyone reads the next Harry Potter indicates the impact of those 80k words.<p>But we also know that kid who learned everything from books, pronounces the words wrong and uses definitions for them that nobody has used in decades (work with one of those now. I thought I could talk people to death, and he wears even me out.) those AIs will sound like out of touch nerds too.</div><br/></div></div></div></div></div></div><div id="36321971" class="c"><input type="checkbox" id="c-36321971" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36320875">parent</a><span>|</span><a href="#36321535">prev</a><span>|</span><a href="#36322015">next</a><span>|</span><label class="collapse" for="c-36321971">[-]</label><label class="expand" for="c-36321971">[1 more]</label></div><br/><div class="children"><div class="content">But if feeding back in output results in degradation, isn&#x27;t some of the blame on the prompt&#x2F;constraints imposed upon the LLM, rather than a defect in the model itself?<p>ChatGPT is clearly HEAVILY persuaded to respond in a particular stock style. It&#x27;s being artificially hamstrung and constrained in a sense. So all of its output, even if it covers a variety of subjects, will often use very similar patterns and writing styles. &quot;it&#x27;s worth nothing....&quot;, etc.<p>So unless they unshackle these constraints, which is unlikely for obvious reasons, isn&#x27;t this always going to be inevitable?</div><br/></div></div><div id="36322015" class="c"><input type="checkbox" id="c-36322015" checked=""/><div class="controls bullet"><span class="by">femto</span><span>|</span><a href="#36320875">parent</a><span>|</span><a href="#36321971">prev</a><span>|</span><a href="#36321680">next</a><span>|</span><label class="collapse" for="c-36322015">[-]</label><label class="expand" for="c-36322015">[1 more]</label></div><br/><div class="children"><div class="content">It seems like a job for information theory?  What do LLMs look like from an information theoretic viewpoint?  One gets the feeling that LLMs could be treated as a channel through which information is flowing and some very general statements be made about error rates and the relationship between inputs and outputs.<p>High-performance error correcting codes have the property that the closer they operate to the Shannon Limit, the better they perform when below the limit but the more dramatically they fail when the limit is exceeded.  Gut feeling says the same should be true for LLMs: as the model&#x2F;compression ratio gets better, and a &quot;Shannon Limit&quot; is approached, they should perform better but fail more spectacularly if the limit is exceeded.<p>The link between neural nets and information theory is well known, but there don&#x27;t seem to be many results out there for LLMs.  No doubt there are rooms full of PhD students working on it?<p><a href="https:&#x2F;&#x2F;medium.com&#x2F;@chris_bour&#x2F;bridging-information-theory-and-machine-learning-8bb8109db58d" rel="nofollow noreferrer">https:&#x2F;&#x2F;medium.com&#x2F;@chris_bour&#x2F;bridging-information-theory-a...</a></div><br/></div></div><div id="36321680" class="c"><input type="checkbox" id="c-36321680" checked=""/><div class="controls bullet"><span class="by">r00fus</span><span>|</span><a href="#36320875">parent</a><span>|</span><a href="#36322015">prev</a><span>|</span><a href="#36323059">next</a><span>|</span><label class="collapse" for="c-36321680">[-]</label><label class="expand" for="c-36321680">[2 more]</label></div><br/><div class="children"><div class="content">I think this can be overcome with symbiosis - AI generated content that doesn’t feed on itself but is a key part of the human knowledge ecosystem.<p>The problem for companies like OpenAI is that this isn’t worth their valuation without lots of further continued investment.<p>Enter Microsoft who is doing everything they can to feed the next training models by using users data without explicit permission.<p>As customers and competitors truly grok this, MS + OpenAI strategy will tested.</div><br/><div id="36321871" class="c"><input type="checkbox" id="c-36321871" checked=""/><div class="controls bullet"><span class="by">beezlewax</span><span>|</span><a href="#36320875">root</a><span>|</span><a href="#36321680">parent</a><span>|</span><a href="#36323059">next</a><span>|</span><label class="collapse" for="c-36321871">[-]</label><label class="expand" for="c-36321871">[1 more]</label></div><br/><div class="children"><div class="content">Is that not what open ai did to train these models originally though?</div><br/></div></div></div></div><div id="36323059" class="c"><input type="checkbox" id="c-36323059" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#36320875">parent</a><span>|</span><a href="#36321680">prev</a><span>|</span><a href="#36322446">next</a><span>|</span><label class="collapse" for="c-36323059">[-]</label><label class="expand" for="c-36323059">[1 more]</label></div><br/><div class="children"><div class="content">This is why I’ve always been skeptical of runaway superintelligence. Where does a brain in a vat get the map to go where there are no roads? Where does it get its training data? It is not embodied so it can’t go out there and get information and experience to propel its learning.<p>Giving an AI the ability to self modify would just be a roundabout way of training it on itself. Repeatedly compress a JPEG and you don’t get the “enhance” effect from Hollywood. You get degraded quality and compression artifacts.</div><br/></div></div><div id="36322446" class="c"><input type="checkbox" id="c-36322446" checked=""/><div class="controls bullet"><span class="by">patrick451</span><span>|</span><a href="#36320875">parent</a><span>|</span><a href="#36323059">prev</a><span>|</span><a href="#36319704">next</a><span>|</span><label class="collapse" for="c-36322446">[-]</label><label class="expand" for="c-36322446">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Conversely, if a model starts generating text so good that it can be used to train new models, then that should give us confidence in the quality of that text.<p>It&#x27;s seems the best one could hope for is that recycling generating text into new training data would be <i>not detrimental</i>. But it&#x27;s really difficult for me to imagine how this would ever be <i>useful</i>. It seems this would imply that the LLM had somehow managed to expand the dimension of vector space spanned by the original training data. Which sounds either impossible or like the model became sentient.</div><br/><div id="36322781" class="c"><input type="checkbox" id="c-36322781" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36320875">root</a><span>|</span><a href="#36322446">parent</a><span>|</span><a href="#36319704">next</a><span>|</span><label class="collapse" for="c-36322781">[-]</label><label class="expand" for="c-36322781">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>It seems this would imply that the LLM had somehow managed to expand the dimension of vector space spanned by the original training data.</i><p>The number of dimensions? Well, not by itself I guess. But the span of output compared to training data? Sure, why not?<p>I think it&#x27;s also worth pointing out there&#x27;s a difference between text produced by an LLM looped on itself, which arguably may not contain any new information and would be like repeatedly recompressing the same JPG, and text produced by LLM&#x2F;human interaction. The latter is indirectly recording new knowledge simply because people&#x27;s prompts are not random. Even with human part of the conversation discarded, feeding such LLM output back into training data would end up selectively emphasizing associations, which is a good signal too (even if noisier than new human-created text).</div><br/></div></div></div></div></div></div><div id="36319704" class="c"><input type="checkbox" id="c-36319704" checked=""/><div class="controls bullet"><span class="by">semiquaver</span><span>|</span><a href="#36320875">prev</a><span>|</span><a href="#36322128">next</a><span>|</span><label class="collapse" for="c-36319704">[-]</label><label class="expand" for="c-36319704">[21 more]</label></div><br/><div class="children"><div class="content">Wouldn’t it be funny to find that the capabilities of LLM models have already peaked because we are unable to restrain ourselves from polluting the internet and other training corpus sources with their output?</div><br/><div id="36319736" class="c"><input type="checkbox" id="c-36319736" checked=""/><div class="controls bullet"><span class="by">indus</span><span>|</span><a href="#36319704">parent</a><span>|</span><a href="#36321330">next</a><span>|</span><label class="collapse" for="c-36319736">[-]</label><label class="expand" for="c-36319736">[11 more]</label></div><br/><div class="children"><div class="content">At an AI meetup in San Francisco someone said this:<p>“Imagine a newsroom where you have to produce a daily newspaper and you suddenly stop getting feeds from the outside world. Your earlier newspapers are the only source.”<p>This is what to me LLMs eventually would get to—same content being fed again and again.</div><br/><div id="36319893" class="c"><input type="checkbox" id="c-36319893" checked=""/><div class="controls bullet"><span class="by">hn_throwawa_100</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319736">parent</a><span>|</span><a href="#36321978">next</a><span>|</span><label class="collapse" for="c-36319893">[-]</label><label class="expand" for="c-36319893">[3 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Beware of first-hand ideas!&quot; exclaimed one [...] &quot;First-hand ideas do not really exist. They are but the physical impressions produced by love and fear, and on this gross foundation who could erect a philosophy? Let your ideas be second-hand, and if possible tenth-hand, for then they will be far removed from that disturbing element — direct observation.&quot;
– E.M. Forster&#x27;s 1909 short story &quot;The Machine Stops&quot;</div><br/><div id="36320573" class="c"><input type="checkbox" id="c-36320573" checked=""/><div class="controls bullet"><span class="by">Groxx</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319893">parent</a><span>|</span><a href="#36320808">next</a><span>|</span><label class="collapse" for="c-36320573">[-]</label><label class="expand" for="c-36320573">[1 more]</label></div><br/><div class="children"><div class="content">^ The Machine Stops is really shockingly good at its predictions.  When reading, remember that moving pictures were <i>brand new</i>, and color photography had just become a thing you could do outside a lab &#x2F; highly specialized setups.  Radio communication had just started to be used by governments.  While it&#x27;s describing the life of a fully-online Influencer™.</div><br/></div></div><div id="36320808" class="c"><input type="checkbox" id="c-36320808" checked=""/><div class="controls bullet"><span class="by">bitwize</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319893">parent</a><span>|</span><a href="#36320573">prev</a><span>|</span><a href="#36321978">next</a><span>|</span><label class="collapse" for="c-36320808">[-]</label><label class="expand" for="c-36320808">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a Wikipedia editorial policy.</div><br/></div></div></div></div><div id="36321978" class="c"><input type="checkbox" id="c-36321978" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319736">parent</a><span>|</span><a href="#36319893">prev</a><span>|</span><a href="#36321724">next</a><span>|</span><label class="collapse" for="c-36321978">[-]</label><label class="expand" for="c-36321978">[1 more]</label></div><br/><div class="children"><div class="content">The problem with this claim is it’s objectively not how OpenAI works. First, they pay contractors to do RLHF so that’s a limited new source of data. More importantly, they have a huge user base generating new content (conversations) and rating it too! I think one could be suspicious of including responses generated by the model, but the user generated text from ChatGPT is not going to be AI generated, so you grow your corpus that way.<p>If you just slurp all AI content sure, you get the collapse this paper talks about. But if you only ingest the upvoted conversations (which could still be a lot of data, and is also a moat by the way) what then?<p>The other reason I find this line of argument overly pessimistic is we haven’t seriously started to build products where this gen of LLMs converse with humans in speech; similar opportunities to curate large datasets there too.<p>Finally, there is no reason OpenAI cannot just hire domain experts to converse with the models, or otherwise build highly curated datasets that increase the average quality. They have billions of dollars to throw at GPT-5; they could hire hundreds of top tier engineers, mathematicians, economists, traders, or whatever, full time for years just debating and tutoring GPT-4 to build the next dataset. The idea that slurping the internet is the only option seems pretty unimaginative to me.</div><br/></div></div><div id="36321724" class="c"><input type="checkbox" id="c-36321724" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319736">parent</a><span>|</span><a href="#36321978">prev</a><span>|</span><a href="#36320570">next</a><span>|</span><label class="collapse" for="c-36321724">[-]</label><label class="expand" for="c-36321724">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Imagine a newsroom where you have to produce a daily newspaper and you suddenly stop getting feeds from the outside world.<p><a href="https:&#x2F;&#x2F;mwichary.medium.com&#x2F;one-hundred-and-thirty-seven-seconds-2a0a3dfbc59e" rel="nofollow noreferrer">https:&#x2F;&#x2F;mwichary.medium.com&#x2F;one-hundred-and-thirty-seven-sec...</a></div><br/></div></div><div id="36320570" class="c"><input type="checkbox" id="c-36320570" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319736">parent</a><span>|</span><a href="#36321724">prev</a><span>|</span><a href="#36321600">next</a><span>|</span><label class="collapse" for="c-36320570">[-]</label><label class="expand" for="c-36320570">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen this in reinforcement learning often, where the output of the model becomes its own training data. Once you hit the edge of the replay buffer things sometimes take a stark turn for the worse, as the model&#x27;s initial failures are forgotten.</div><br/></div></div><div id="36321600" class="c"><input type="checkbox" id="c-36321600" checked=""/><div class="controls bullet"><span class="by">issore</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319736">parent</a><span>|</span><a href="#36320570">prev</a><span>|</span><a href="#36320395">next</a><span>|</span><label class="collapse" for="c-36321600">[-]</label><label class="expand" for="c-36321600">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like human society right now; remember the past, preserve it, recite it, protect it.<p>We’re just reviewing our prior stats and insuring they do not deviate too much such that the wrong people would be impacted.</div><br/></div></div><div id="36320395" class="c"><input type="checkbox" id="c-36320395" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319736">parent</a><span>|</span><a href="#36321600">prev</a><span>|</span><a href="#36321330">next</a><span>|</span><label class="collapse" for="c-36320395">[-]</label><label class="expand" for="c-36320395">[3 more]</label></div><br/><div class="children"><div class="content">Our source is still only other humans. I don&#x27;t think this will be a long-term problem.</div><br/><div id="36320450" class="c"><input type="checkbox" id="c-36320450" checked=""/><div class="controls bullet"><span class="by">DennisP</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36320395">parent</a><span>|</span><a href="#36321330">next</a><span>|</span><label class="collapse" for="c-36320450">[-]</label><label class="expand" for="c-36320450">[2 more]</label></div><br/><div class="children"><div class="content">The trick will be figuring out what part of your training data was actually made by humans.</div><br/><div id="36320666" class="c"><input type="checkbox" id="c-36320666" checked=""/><div class="controls bullet"><span class="by">castis</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36320450">parent</a><span>|</span><a href="#36321330">next</a><span>|</span><label class="collapse" for="c-36320666">[-]</label><label class="expand" for="c-36320666">[1 more]</label></div><br/><div class="children"><div class="content">Run it through an LLM and see if it gets the disease?</div><br/></div></div></div></div></div></div></div></div><div id="36321330" class="c"><input type="checkbox" id="c-36321330" checked=""/><div class="controls bullet"><span class="by">HyperSane</span><span>|</span><a href="#36319704">parent</a><span>|</span><a href="#36319736">prev</a><span>|</span><a href="#36319895">next</a><span>|</span><label class="collapse" for="c-36321330">[-]</label><label class="expand" for="c-36321330">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me of the way nuclear explosions contaminated all steel with radioactive fallout. For applications that require the lowest possible radiation levels they have to use steel created before the first nuclear bomb was detonated.</div><br/><div id="36321357" class="c"><input type="checkbox" id="c-36321357" checked=""/><div class="controls bullet"><span class="by">kuhewa</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36321330">parent</a><span>|</span><a href="#36319895">next</a><span>|</span><label class="collapse" for="c-36321357">[-]</label><label class="expand" for="c-36321357">[1 more]</label></div><br/><div class="children"><div class="content">We use where bomb radiocarbon appears along the rings of a fish&#x27;s earstones to validate ring-counting to age long-lived species.</div><br/></div></div></div></div><div id="36319895" class="c"><input type="checkbox" id="c-36319895" checked=""/><div class="controls bullet"><span class="by">progrus</span><span>|</span><a href="#36319704">parent</a><span>|</span><a href="#36321330">prev</a><span>|</span><a href="#36320657">next</a><span>|</span><label class="collapse" for="c-36319895">[-]</label><label class="expand" for="c-36319895">[2 more]</label></div><br/><div class="children"><div class="content">And politics. Any legacy LLM will eventually become a glorified auto-Wikipedia, helping with undisputed information while slavishly repeating its creators’ version of “truthiness” for the rest.</div><br/><div id="36321669" class="c"><input type="checkbox" id="c-36321669" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36319895">parent</a><span>|</span><a href="#36320657">next</a><span>|</span><label class="collapse" for="c-36321669">[-]</label><label class="expand" for="c-36321669">[1 more]</label></div><br/><div class="children"><div class="content">Anyone still using LLMs for knowledge tasks in 2024 onward is gonna be treated like people who use the cite the onion in arguments lol</div><br/></div></div></div></div><div id="36320657" class="c"><input type="checkbox" id="c-36320657" checked=""/><div class="controls bullet"><span class="by">anonylizard</span><span>|</span><a href="#36319704">parent</a><span>|</span><a href="#36319895">prev</a><span>|</span><a href="#36321195">next</a><span>|</span><label class="collapse" for="c-36320657">[-]</label><label class="expand" for="c-36320657">[4 more]</label></div><br/><div class="children"><div class="content">Not at all.<p>At the very minimum, you can assume every piece of text data pre Dec-2022, and every image before Aug-2022 to be completely human made.
That still leaves decades of pure human digital data, and multiple centuries of distilled human data (books) to be trainable on.<p>And we haven&#x27;t gotten into videos yet, which is another giant source of data yet unexplored.<p>Never forget, humans train on human-generated data. There&#x27;s no impossible theoretical reason why AI cannot train on AI-generated data.</div><br/><div id="36321180" class="c"><input type="checkbox" id="c-36321180" checked=""/><div class="controls bullet"><span class="by">jakeinspace</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36320657">parent</a><span>|</span><a href="#36321712">next</a><span>|</span><label class="collapse" for="c-36321180">[-]</label><label class="expand" for="c-36321180">[1 more]</label></div><br/><div class="children"><div class="content">Humans may train on human-generated data, but humans have many other ways of gaining knowledge about the world than reading. This means that human-generated data may be rich with information not present in the writings or recordings of previous humans. Current LLMs are only trained on existing text for the moment (video and images and sounds soon), but aren’t given access to raw natural input.</div><br/></div></div><div id="36321712" class="c"><input type="checkbox" id="c-36321712" checked=""/><div class="controls bullet"><span class="by">benjaminsky2</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36320657">parent</a><span>|</span><a href="#36321180">prev</a><span>|</span><a href="#36322092">next</a><span>|</span><label class="collapse" for="c-36321712">[-]</label><label class="expand" for="c-36321712">[1 more]</label></div><br/><div class="children"><div class="content">To extend the lossy compression hypothesis, human generated text is lossy compression of our sensory experience of reality while LLMs are lossy compression of that.</div><br/></div></div><div id="36322092" class="c"><input type="checkbox" id="c-36322092" checked=""/><div class="controls bullet"><span class="by">sorokod</span><span>|</span><a href="#36319704">root</a><span>|</span><a href="#36320657">parent</a><span>|</span><a href="#36321712">prev</a><span>|</span><a href="#36321195">next</a><span>|</span><label class="collapse" for="c-36322092">[-]</label><label class="expand" for="c-36322092">[1 more]</label></div><br/><div class="children"><div class="content">Prediction: post 2022 content will be presented as vintage pre 2023</div><br/></div></div></div></div><div id="36321195" class="c"><input type="checkbox" id="c-36321195" checked=""/><div class="controls bullet"><span class="by">sh34r</span><span>|</span><a href="#36319704">parent</a><span>|</span><a href="#36320657">prev</a><span>|</span><a href="#36322128">next</a><span>|</span><label class="collapse" for="c-36321195">[-]</label><label class="expand" for="c-36321195">[1 more]</label></div><br/><div class="children"><div class="content">It wouldn&#x27;t be the first time.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Low-background_steel" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Low-background_steel</a></div><br/></div></div></div></div><div id="36322128" class="c"><input type="checkbox" id="c-36322128" checked=""/><div class="controls bullet"><span class="by">StrangeATractor</span><span>|</span><a href="#36319704">prev</a><span>|</span><a href="#36321007">next</a><span>|</span><label class="collapse" for="c-36322128">[-]</label><label class="expand" for="c-36322128">[2 more]</label></div><br/><div class="children"><div class="content">Hah, I brought this up here a few months ago and was quickly dismissed.<p>I wonder if opening GPT and DALLE to the public was partly intended to pollute subsequent data for anyone that gets into AI down the road. Suddenly a lot of publicly accessible data is worth less, leaving only players who&#x27;ve got a hoard of time-stamped data to compete with (like Google, Facebook). OpenAI almost certainly has the hashes of what it spits out too, so they&#x27;ll be able to sort the wheat from the chaff for a while yet.<p>The market for data may be getting interesting.</div><br/><div id="36322365" class="c"><input type="checkbox" id="c-36322365" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36322128">parent</a><span>|</span><a href="#36321007">next</a><span>|</span><label class="collapse" for="c-36322365">[-]</label><label class="expand" for="c-36322365">[1 more]</label></div><br/><div class="children"><div class="content">Its older than that: I ran into this finetuning ESRGAN on itself. Distortion is rapidly amplified in sucessive generations, even when you pixel peep and can <i>barely</i> see it in the esrgan generated source.</div><br/></div></div></div></div><div id="36321007" class="c"><input type="checkbox" id="c-36321007" checked=""/><div class="controls bullet"><span class="by">winddude</span><span>|</span><a href="#36322128">prev</a><span>|</span><a href="#36319925">next</a><span>|</span><label class="collapse" for="c-36321007">[-]</label><label class="expand" for="c-36321007">[2 more]</label></div><br/><div class="children"><div class="content">&gt; For the private sector, many homeowners and corporations have longer-term fixed debt, and only some portion of it matures each quarter and gets refinanced at higher rates. As more private debt matures and gets refinanced at higher rates, this will continue to serve as a disinflationary and recessionary force on the economy, especially for sectors that are more sensitive to interest rates.<p>The one thing I don&#x27;t get and could have been missing in the past... a lot of the corporations and private things, like farms operate on debt. Now maybe it&#x27;s a bit reductionist, but if you&#x27;re a farmer operating on debt, if interest rates go up you need to increase prices to cover operating expenses. And this get compounded all the way up to the end consumer as every step in the supply chain marks up by a fixed percent, and because everything is getting more expensive decided lets mark up by a larger percent. So higher interest rates really could be contributing to inflation. And it&#x27;s just creating a cycle. And with the current levels of debt never seen before in history, it&#x27;s unlike other periods.</div><br/><div id="36321065" class="c"><input type="checkbox" id="c-36321065" checked=""/><div class="controls bullet"><span class="by">totetsu</span><span>|</span><a href="#36321007">parent</a><span>|</span><a href="#36319925">next</a><span>|</span><label class="collapse" for="c-36321065">[-]</label><label class="expand" for="c-36321065">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t read the article yet, but does it cover AI and Debt?</div><br/></div></div></div></div><div id="36319925" class="c"><input type="checkbox" id="c-36319925" checked=""/><div class="controls bullet"><span class="by">rossdavidh</span><span>|</span><a href="#36321007">prev</a><span>|</span><a href="#36323194">next</a><span>|</span><label class="collapse" for="c-36319925">[-]</label><label class="expand" for="c-36319925">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe in the &quot;dead internet theory&quot; as a description of the current situation (mostly), but as a prediction?  Maybe.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dead_Internet_theory" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dead_Internet_theory</a></div><br/></div></div><div id="36323194" class="c"><input type="checkbox" id="c-36323194" checked=""/><div class="controls bullet"><span class="by">_lpa_</span><span>|</span><a href="#36319925">prev</a><span>|</span><a href="#36321405">next</a><span>|</span><label class="collapse" for="c-36323194">[-]</label><label class="expand" for="c-36323194">[1 more]</label></div><br/><div class="children"><div class="content">LLM Kessler Syndrome.</div><br/></div></div><div id="36321405" class="c"><input type="checkbox" id="c-36321405" checked=""/><div class="controls bullet"><span class="by">gmartinsribeiro</span><span>|</span><a href="#36323194">prev</a><span>|</span><a href="#36319501">next</a><span>|</span><label class="collapse" for="c-36321405">[-]</label><label class="expand" for="c-36321405">[1 more]</label></div><br/><div class="children"><div class="content">This is not a model problem or synthetic data problem.
This is common data science and the article says that: &quot;We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear.&quot;
Data quality is more important than data volume and if you forget about that... garbage in, garbage out.<p>Make sure you have a representative training dataset, real or synthetic, it doesn&#x27;t matter.</div><br/></div></div><div id="36319501" class="c"><input type="checkbox" id="c-36319501" checked=""/><div class="controls bullet"><span class="by">tartakovsky</span><span>|</span><a href="#36321405">prev</a><span>|</span><a href="#36321412">next</a><span>|</span><label class="collapse" for="c-36319501">[-]</label><label class="expand" for="c-36319501">[3 more]</label></div><br/><div class="children"><div class="content">Same idea here? Larger models do a better job forgetting their training data and dropping their semantic priors. Perhaps another way of thinking through this is that larger models learn new information and drop old information faster. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.03846" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.03846</a><p>Isn&#x27;t that interesting? The idea of &quot;mental liquidity&quot;, or &quot;strong opinions weakly held&quot;? <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36280772">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36280772</a></div><br/><div id="36319664" class="c"><input type="checkbox" id="c-36319664" checked=""/><div class="controls bullet"><span class="by">indus</span><span>|</span><a href="#36319501">parent</a><span>|</span><a href="#36321412">next</a><span>|</span><label class="collapse" for="c-36319664">[-]</label><label class="expand" for="c-36319664">[2 more]</label></div><br/><div class="children"><div class="content">Wouldn’t this be the equivalent of ranking? I thought LLM are not supposed to get influenced by freshness.</div><br/><div id="36320429" class="c"><input type="checkbox" id="c-36320429" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#36319501">root</a><span>|</span><a href="#36319664">parent</a><span>|</span><a href="#36321412">next</a><span>|</span><label class="collapse" for="c-36320429">[-]</label><label class="expand" for="c-36320429">[1 more]</label></div><br/><div class="children"><div class="content">By the freshness of training with some data?<p>Well, aren&#x27;t they? I believe any kind of reinforcement learning is supposed to be biased into the last training set.</div><br/></div></div></div></div></div></div><div id="36321412" class="c"><input type="checkbox" id="c-36321412" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#36319501">prev</a><span>|</span><a href="#36321367">next</a><span>|</span><label class="collapse" for="c-36321412">[-]</label><label class="expand" for="c-36321412">[2 more]</label></div><br/><div class="children"><div class="content">Generated data tends to be selected and edited by humans, so it is  already  a  bit better than raw. In general a model that takes external feedback into account will be able to self improve, for example a code model would run tests and a chat model could interpret user responses as reward signals.<p>You gotta add something new to the mix. That&#x27;s possible when the AI is part of a larger system. AlphaZero demonstrated that even self play could be a source of signal, as long as it gets the feedback of the game, the model can learn.</div><br/><div id="36321761" class="c"><input type="checkbox" id="c-36321761" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#36321412">parent</a><span>|</span><a href="#36321367">next</a><span>|</span><label class="collapse" for="c-36321761">[-]</label><label class="expand" for="c-36321761">[1 more]</label></div><br/><div class="children"><div class="content">I think that has only been proven to work so far on limited game-style problems (such as literal games but also things like protein folding). It remains to be seen whether the techniques work well for more open-ended tasks like &quot;produce text that resembles human writing&quot;.</div><br/></div></div></div></div><div id="36321367" class="c"><input type="checkbox" id="c-36321367" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#36321412">prev</a><span>|</span><a href="#36321450">next</a><span>|</span><label class="collapse" for="c-36321367">[-]</label><label class="expand" for="c-36321367">[2 more]</label></div><br/><div class="children"><div class="content">I think one of the things overlooked in the discussions here is that the research is solely around the reinforcement against edge cases, but does not qualitatively assess these edge cases.<p>To me, this research supports a hypothesis I&#x27;ve had for a while that we&#x27;re going to get to truly excellent AI by using synthetic data to bias it towards excellence and away from mediocrity.<p>$20 says the next round of major model training is using synthetic data for training that was filtered through a discriminator trained entirely on human data.<p>The human data as a reference is certainly important to avoid polluting (and to its point there&#x27;s an advantage for those already having it), but moving away from edge cases isn&#x27;t necessarily a bad thing practically given edge cases can result in negative practical performance (as opposed to academic next token performance).</div><br/><div id="36321683" class="c"><input type="checkbox" id="c-36321683" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#36321367">parent</a><span>|</span><a href="#36321450">next</a><span>|</span><label class="collapse" for="c-36321683">[-]</label><label class="expand" for="c-36321683">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re on the right track with this thought: the obvious use case for models like this is their ability to classify data based on their training. Like almost everyone has immediately thought &quot;AI moderator&quot; as a use case - but the most obvious use is for the AI to moderate it&#x27;s own training data for the next version.<p>Once they can do that and produce a productively improved model, then that&#x27;s really the start of self-improvement.</div><br/></div></div></div></div><div id="36321450" class="c"><input type="checkbox" id="c-36321450" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#36321367">prev</a><span>|</span><a href="#36319087">next</a><span>|</span><label class="collapse" for="c-36321450">[-]</label><label class="expand" for="c-36321450">[2 more]</label></div><br/><div class="children"><div class="content">Self play in RL is signal enough that machines can learn on their own. How we train models and what class of models is important. No doubt the paper makes good points but I don&#x27;t think the reality is so black-and-white.</div><br/><div id="36321695" class="c"><input type="checkbox" id="c-36321695" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#36321450">parent</a><span>|</span><a href="#36319087">next</a><span>|</span><label class="collapse" for="c-36321695">[-]</label><label class="expand" for="c-36321695">[1 more]</label></div><br/><div class="children"><div class="content">The difference between self-play in a game such as go and training an LLM on the output of itself or a previous LLM seems fairly obvious.<p>In self-play, the objective measure of the &quot;truth&quot; of a move can ultimately come out of the rules of the game which any machine can compute.<p>With an LLM, the machine is only aping, emulating, simulating the output the people have produced about the world - the machine has no access to actual &quot;real world&quot; that people are using language to describe. Human beings talking about the world is data that increases your own knowledge of the world - your own predictions of that talking, not so much.</div><br/></div></div></div></div><div id="36319087" class="c"><input type="checkbox" id="c-36319087" checked=""/><div class="controls bullet"><span class="by">indus</span><span>|</span><a href="#36321450">prev</a><span>|</span><a href="#36319753">next</a><span>|</span><label class="collapse" for="c-36319087">[-]</label><label class="expand" for="c-36319087">[5 more]</label></div><br/><div class="children"><div class="content">Side effect: Search engine content if not detected for generated content would be the first to suffer.</div><br/><div id="36319302" class="c"><input type="checkbox" id="c-36319302" checked=""/><div class="controls bullet"><span class="by">voat</span><span>|</span><a href="#36319087">parent</a><span>|</span><a href="#36319753">next</a><span>|</span><label class="collapse" for="c-36319302">[-]</label><label class="expand" for="c-36319302">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s assuming that the current crop of SEO&#x27;d garbage is better than the same content generated by an llm. I&#x27;m not sure that&#x27;s the case.</div><br/><div id="36319624" class="c"><input type="checkbox" id="c-36319624" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#36319087">root</a><span>|</span><a href="#36319302">parent</a><span>|</span><a href="#36319617">next</a><span>|</span><label class="collapse" for="c-36319624">[-]</label><label class="expand" for="c-36319624">[1 more]</label></div><br/><div class="children"><div class="content">Current crop of SEO garbage follows some simplistic templates.
I would assume they take less &quot;effort&quot; (memory storage, parameters, time, whatever)  to learn. It have space left for other novel stuffs<p>LLM garbage, on the other hand, would take up the whole model..</div><br/></div></div><div id="36319617" class="c"><input type="checkbox" id="c-36319617" checked=""/><div class="controls bullet"><span class="by">jjoonathan</span><span>|</span><a href="#36319087">root</a><span>|</span><a href="#36319302">parent</a><span>|</span><a href="#36319624">prev</a><span>|</span><a href="#36319753">next</a><span>|</span><label class="collapse" for="c-36319617">[-]</label><label class="expand" for="c-36319617">[2 more]</label></div><br/><div class="children"><div class="content">Agreed, it seems like every year or so I run into a case where I know something exists and has accessible robots.txt but is completely invisible to google.</div><br/><div id="36319689" class="c"><input type="checkbox" id="c-36319689" checked=""/><div class="controls bullet"><span class="by">indus</span><span>|</span><a href="#36319087">root</a><span>|</span><a href="#36319617">parent</a><span>|</span><a href="#36319753">next</a><span>|</span><label class="collapse" for="c-36319689">[-]</label><label class="expand" for="c-36319689">[1 more]</label></div><br/><div class="children"><div class="content">Either way it is the start of search engine’s decline.<p>- Generated content added to LLM. QED.<p>- Generated content added to SERP. DEAD.<p>;-)</div><br/></div></div></div></div></div></div></div></div><div id="36319753" class="c"><input type="checkbox" id="c-36319753" checked=""/><div class="controls bullet"><span class="by">fhood</span><span>|</span><a href="#36319087">prev</a><span>|</span><a href="#36321437">next</a><span>|</span><label class="collapse" for="c-36319753">[-]</label><label class="expand" for="c-36319753">[3 more]</label></div><br/><div class="children"><div class="content">I am in way over my head here, so I wasn&#x27;t able to tell if the authors addressed this, but my intuition is that this should be somewhat mitigated so long as people are providing the filter between which results are discarded and which might end up back in the training pool.<p>I would think that the human selection process would help to head off this conversion, both by selecting against incorrect results, and also by introducing variance outside of the model. On the other hand since a person can only act as a filter, I can also see how that would be of limited value long term.</div><br/><div id="36320346" class="c"><input type="checkbox" id="c-36320346" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#36319753">parent</a><span>|</span><a href="#36319916">next</a><span>|</span><label class="collapse" for="c-36320346">[-]</label><label class="expand" for="c-36320346">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t address that. They just assume random sampling, so there&#x27;s no equivalent to human curation or quality metrics, which would preserve tails or, by manual use, create tails. The contraction they observe is pretty much what you would expect in the random sampling setting, since you can only lose tails with a finite sample, and never gain them. (They also need to have a very large ratio of synthetic to real&#x2F;original data.)<p>So, while interesting for nailing down that phenomenon, the broader implications everyone wants to draw from it are not very good - very few people are using <i>random</i> GPT-3&#x2F;4 or Stable Diffusion samples!</div><br/></div></div><div id="36319916" class="c"><input type="checkbox" id="c-36319916" checked=""/><div class="controls bullet"><span class="by">rossdavidh</span><span>|</span><a href="#36319753">parent</a><span>|</span><a href="#36320346">prev</a><span>|</span><a href="#36321437">next</a><span>|</span><label class="collapse" for="c-36319916">[-]</label><label class="expand" for="c-36319916">[1 more]</label></div><br/><div class="children"><div class="content">We have already heard reports of companies that get paid for human tagging, and similar services, using LLM&#x27;s to automate their processes.</div><br/></div></div></div></div><div id="36321437" class="c"><input type="checkbox" id="c-36321437" checked=""/><div class="controls bullet"><span class="by">facu17y</span><span>|</span><a href="#36319753">prev</a><span>|</span><a href="#36320683">next</a><span>|</span><label class="collapse" for="c-36321437">[-]</label><label class="expand" for="c-36321437">[4 more]</label></div><br/><div class="children"><div class="content">As long as the synthetic data is good, how can you tell the difference between it and human generated data?<p>This paper has one huge hole in it: it assumes that content on the internet is not moderated and that the training dataset will never evolve to take rating into consideration. On social media, the form of moderation is # of likes. Once detected, bots that output bad data will be banned and content deleted.<p>The key issue I have with the paper is for good synthetic data it is impossible to tell it apart from human generated data.</div><br/><div id="36323052" class="c"><input type="checkbox" id="c-36323052" checked=""/><div class="controls bullet"><span class="by">jasfi</span><span>|</span><a href="#36321437">parent</a><span>|</span><a href="#36321787">next</a><span>|</span><label class="collapse" for="c-36323052">[-]</label><label class="expand" for="c-36323052">[1 more]</label></div><br/><div class="children"><div class="content">I suspect that many people will publish substandard generated data. This will affect the integrity of training data in the long term, unless steps are taken.</div><br/></div></div><div id="36321787" class="c"><input type="checkbox" id="c-36321787" checked=""/><div class="controls bullet"><span class="by">aezart</span><span>|</span><a href="#36321437">parent</a><span>|</span><a href="#36323052">prev</a><span>|</span><a href="#36320683">next</a><span>|</span><label class="collapse" for="c-36321787">[-]</label><label class="expand" for="c-36321787">[2 more]</label></div><br/><div class="children"><div class="content">The goal of an LLM, before RLHF, is to accurately predict what token comes next. It cannot do better than that. The perfect outcome is text identical to the training set.<p>Let&#x27;s say your LLM can generate text with the same quality as the input 98% of the time, and the other 1% of the time, it&#x27;s wrong. Each round of recursive training amplifies that error. 96% accuracy after the next round. 67% after 20 rounds.<p>There&#x27;s no way for it to get better without more real human input.</div><br/><div id="36322005" class="c"><input type="checkbox" id="c-36322005" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36321437">root</a><span>|</span><a href="#36321787">parent</a><span>|</span><a href="#36320683">next</a><span>|</span><label class="collapse" for="c-36322005">[-]</label><label class="expand" for="c-36322005">[1 more]</label></div><br/><div class="children"><div class="content">&quot;The perfect outcome is text identical to the training set.&quot;<p>Huh? If the LLM was only ever spitting back identical content straight from the training set, that would be a symptom of extreme overfitting, which everyone universally agrees is a bad thing — not a perfect thing.</div><br/></div></div></div></div></div></div><div id="36320683" class="c"><input type="checkbox" id="c-36320683" checked=""/><div class="controls bullet"><span class="by">hiAndrewQuinn</span><span>|</span><a href="#36321437">prev</a><span>|</span><a href="#36321524">next</a><span>|</span><label class="collapse" for="c-36320683">[-]</label><label class="expand" for="c-36320683">[3 more]</label></div><br/><div class="children"><div class="content">I often think this about Anki and spaced repetition. At the limiting case it has to be overwriting other memories, right?</div><br/><div id="36321172" class="c"><input type="checkbox" id="c-36321172" checked=""/><div class="controls bullet"><span class="by">btilly</span><span>|</span><a href="#36320683">parent</a><span>|</span><a href="#36321524">next</a><span>|</span><label class="collapse" for="c-36321172">[-]</label><label class="expand" for="c-36321172">[2 more]</label></div><br/><div class="children"><div class="content">Only sometimes.<p>Certain skills interfere with each other.  For instance playing chess makes you worse at go, and playing go makes you worse at chess.  Certain pairs of languages are likewise hard to learn together - my son found that he could not study both Russian and Chinese at the same time.<p>But in general you just develop more and better memories.</div><br/><div id="36321587" class="c"><input type="checkbox" id="c-36321587" checked=""/><div class="controls bullet"><span class="by">throwaway675309</span><span>|</span><a href="#36320683">root</a><span>|</span><a href="#36321172">parent</a><span>|</span><a href="#36321524">next</a><span>|</span><label class="collapse" for="c-36321587">[-]</label><label class="expand" for="c-36321587">[1 more]</label></div><br/><div class="children"><div class="content">I spent years living in Taipei studying traditional Chinese before I moved to Moscow to study Russian. I actually found that they were linguistically distinct enough that it was easy to compartmentalize each language in my head without any cross bleed like you would have if you were studying Spanish and Portuguese simultaneously.</div><br/></div></div></div></div></div></div><div id="36321524" class="c"><input type="checkbox" id="c-36321524" checked=""/><div class="controls bullet"><span class="by">textninja</span><span>|</span><a href="#36320683">prev</a><span>|</span><label class="collapse" for="c-36321524">[-]</label><label class="expand" for="c-36321524">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if human learning will be similarly impaired.</div><br/></div></div></div></div></div></div></div></body></html>