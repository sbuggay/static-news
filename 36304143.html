<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686646858271" as="style"/><link rel="stylesheet" href="styles.css?v=1686646858271"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ggerganov/llama.cpp/pull/1827">Llama.cpp: Full CUDA GPU Acceleration</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>gzer0</span> | <span>155 comments</span></div><br/><div><div id="36306965" class="c"><input type="checkbox" id="c-36306965" checked=""/><div class="controls bullet"><span class="by">underdeserver</span><span>|</span><a href="#36304457">next</a><span>|</span><label class="collapse" for="c-36306965">[-]</label><label class="expand" for="c-36306965">[12 more]</label></div><br/><div class="children"><div class="content">Can someone ELI5 why AMD is not in this game? Is it really so much harder to implement this in a non-platform-specific library?</div><br/><div id="36307483" class="c"><input type="checkbox" id="c-36307483" checked=""/><div class="controls bullet"><span class="by">captainbland</span><span>|</span><a href="#36306965">parent</a><span>|</span><a href="#36307298">next</a><span>|</span><label class="collapse" for="c-36307483">[-]</label><label class="expand" for="c-36307483">[1 more]</label></div><br/><div class="children"><div class="content">CUDA is the best supported solution, tends to get you access to the best performance, has a great profiler (it will literally tell you things like &quot;your memory accesses don&#x27;t seem to be coalesced properly&quot; or &quot;your kernel is ALU limited&quot; as well as a bunch of useful stats), even works in windows, all of that.<p>OpenCL is (was?) the main open alternative to CUDA and was mainly backed by AMD and Apple. Apple got bored of it when they decided Metal was the future. AMD got bored of it when they developed rocm and HIP (basically a partially complete compatibility layer with CUDA).<p>There&#x27;s also stuff like DirectML which only works on windows and e.g. various (Vulkan, directx etc.) compute shaders which are really more oriented at games.<p>There&#x27;s also a bit of a performance aspect to it. Obviously GPGPU stuff is massively performance sensitive and CUDA gets you the best performance on the most widely supported platform.<p>AMD are also the main competitor in the space but all but totally dropped support for GPGPU in desktop cards, trying to instead focus on gaming for Radeon and compute in MI&#x2F;CDNA. They seem to have realised their mistake a bit and are now introducing some support for RDNA2+ cards.</div><br/></div></div><div id="36307298" class="c"><input type="checkbox" id="c-36307298" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#36306965">parent</a><span>|</span><a href="#36307483">prev</a><span>|</span><a href="#36307033">next</a><span>|</span><label class="collapse" for="c-36307298">[-]</label><label class="expand" for="c-36307298">[1 more]</label></div><br/><div class="children"><div class="content">Short version: AMD&#x27;s software incompetence. Very few hardware companies have the competence to properly support their HW. You see this problem again and again, HW companies designing HW that&#x27;s great on paper but can&#x27;t be used properly because it&#x27;s not properly supported with SW. Nvidia understands this and has 10 times as many SW engineers than HW engineers. AMD doesn&#x27;t. Intel might too.</div><br/></div></div><div id="36307033" class="c"><input type="checkbox" id="c-36307033" checked=""/><div class="controls bullet"><span class="by">born-jre</span><span>|</span><a href="#36306965">parent</a><span>|</span><a href="#36307298">prev</a><span>|</span><a href="#36307130">next</a><span>|</span><label class="collapse" for="c-36307033">[-]</label><label class="expand" for="c-36307033">[2 more]</label></div><br/><div class="children"><div class="content">short answer is they have somewhat competent hardware but software sucks  
or you can watch george hotz rant about how amd driver sucks<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Mr0rWJhv9jU">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Mr0rWJhv9jU</a></div><br/><div id="36307086" class="c"><input type="checkbox" id="c-36307086" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#36306965">root</a><span>|</span><a href="#36307033">parent</a><span>|</span><a href="#36307130">next</a><span>|</span><label class="collapse" for="c-36307086">[-]</label><label class="expand" for="c-36307086">[1 more]</label></div><br/><div class="children"><div class="content">He got a tarball fix for the driver after his rant got viral. Still not looking good IMHO.<p><a href="https:&#x2F;&#x2F;geohot.github.io&#x2F;blog&#x2F;jekyll&#x2F;update&#x2F;2023&#x2F;06&#x2F;07&#x2F;a-dive-into-amds-drivers.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;geohot.github.io&#x2F;blog&#x2F;jekyll&#x2F;update&#x2F;2023&#x2F;06&#x2F;07&#x2F;a-div...</a></div><br/></div></div></div></div><div id="36307130" class="c"><input type="checkbox" id="c-36307130" checked=""/><div class="controls bullet"><span class="by">marcyb5st</span><span>|</span><a href="#36306965">parent</a><span>|</span><a href="#36307033">prev</a><span>|</span><a href="#36306984">next</a><span>|</span><label class="collapse" for="c-36307130">[-]</label><label class="expand" for="c-36307130">[3 more]</label></div><br/><div class="children"><div class="content">I think there are several reasons.<p>Firstly, nVidia has been at it much longer. Just because of this tools on nVidia side feel easier to set up &#x2F; are more polished (at least that was my feeling when fiddling with ROCm like a year ago).<p>Second but still related to #1, from the beginning even consumers nVidia cards were able to run CUDA and this made so that hobbyist and prosumers&#x2F;researchers on a budget bought nVidia cards compounding even further the time&#x2F;tooling advantage nVidia had. I.e. a huge user base of not only gamers but people that use their cards to do other things than gaming and know that things work on these cards.<p>These are, IMHO, the main reasons why everyone targets CUDA and explain why frameworks like Tensorflow or Pytorch targeted it as a first class citizen.</div><br/><div id="36307226" class="c"><input type="checkbox" id="c-36307226" checked=""/><div class="controls bullet"><span class="by">rapsey</span><span>|</span><a href="#36306965">root</a><span>|</span><a href="#36307130">parent</a><span>|</span><a href="#36306984">next</a><span>|</span><label class="collapse" for="c-36307226">[-]</label><label class="expand" for="c-36307226">[2 more]</label></div><br/><div class="children"><div class="content">If AMD was software competent the frameworks would support their drivers just as well. No one wants a monopoly.</div><br/><div id="36307302" class="c"><input type="checkbox" id="c-36307302" checked=""/><div class="controls bullet"><span class="by">marcyb5st</span><span>|</span><a href="#36306965">root</a><span>|</span><a href="#36307226">parent</a><span>|</span><a href="#36306984">next</a><span>|</span><label class="collapse" for="c-36307302">[-]</label><label class="expand" for="c-36307302">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. Sorry if I gave the impression of being pro-nVidia. I am not.<p>But the reality is that when Tensorflow and Pytorch came to be there was no alternative. Now you need to jump through hoops to make it work with non CUDA hardware.<p>Additionally, while drivers play a role, I think the main difference is in the computing libraries (CUDA vs ROCm)</div><br/></div></div></div></div></div></div><div id="36306984" class="c"><input type="checkbox" id="c-36306984" checked=""/><div class="controls bullet"><span class="by">bilekas</span><span>|</span><a href="#36306965">parent</a><span>|</span><a href="#36307130">prev</a><span>|</span><a href="#36304457">next</a><span>|</span><label class="collapse" for="c-36306984">[-]</label><label class="expand" for="c-36306984">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m no expert but if I understand correctly the CUDA cores are the main pull and the API to them.<p>They&#x27;re supposed to be more optimized and more stable compared to AMD. That&#x27;s how it was before anyway, not sure today.</div><br/><div id="36307003" class="c"><input type="checkbox" id="c-36307003" checked=""/><div class="controls bullet"><span class="by">Aardwolf</span><span>|</span><a href="#36306965">root</a><span>|</span><a href="#36306984">parent</a><span>|</span><a href="#36304457">next</a><span>|</span><label class="collapse" for="c-36307003">[-]</label><label class="expand" for="c-36307003">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the main component for AI matrix multiplication? What makes it so hard to create a good alternative API for matrix multiplication?</div><br/><div id="36307162" class="c"><input type="checkbox" id="c-36307162" checked=""/><div class="controls bullet"><span class="by">bilekas</span><span>|</span><a href="#36306965">root</a><span>|</span><a href="#36307003">parent</a><span>|</span><a href="#36307221">next</a><span>|</span><label class="collapse" for="c-36307162">[-]</label><label class="expand" for="c-36307162">[1 more]</label></div><br/><div class="children"><div class="content">Well I think there are 2 types right ? Tensor cores (which afaik AMD dont have) which are better for matrix ops, and CUDO which are better for general parallel ops.<p>Maybe someone more clever than me can go into the specifics, I only understand the minimum of the low lvl GPU details.<p>Nice high lvl document<p>[0] <a href="https:&#x2F;&#x2F;www.acecloudhosting.com&#x2F;blog&#x2F;cuda-cores-vs-tensor-cores&#x2F;#Difference_Between_CUDA_Cores_and_Tensor_Cores" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.acecloudhosting.com&#x2F;blog&#x2F;cuda-cores-vs-tensor-co...</a></div><br/></div></div><div id="36307221" class="c"><input type="checkbox" id="c-36307221" checked=""/><div class="controls bullet"><span class="by">marcyb5st</span><span>|</span><a href="#36306965">root</a><span>|</span><a href="#36307003">parent</a><span>|</span><a href="#36307162">prev</a><span>|</span><a href="#36304457">next</a><span>|</span><label class="collapse" for="c-36307221">[-]</label><label class="expand" for="c-36307221">[1 more]</label></div><br/><div class="children"><div class="content">I think API for matrix multiplication is just a part of the issue. CUDA tooling has better ergonomics, it&#x27;s easier to set up and treated as first class citizen in tools like Tensorflow and Pytorch.<p>So, while I can&#x27;t talk about the hardware differences in detail, developer experience is greatly on nVidia side and now AMD has a moat to overcome to catch up.</div><br/></div></div></div></div></div></div></div></div><div id="36304457" class="c"><input type="checkbox" id="c-36304457" checked=""/><div class="controls bullet"><span class="by">adeon</span><span>|</span><a href="#36306965">prev</a><span>|</span><a href="#36305531">next</a><span>|</span><label class="collapse" for="c-36304457">[-]</label><label class="expand" for="c-36304457">[54 more]</label></div><br/><div class="children"><div class="content">llama.cpp is great. It started off as CPU-only solution and now looks like it wants to support any computation device it can.<p>I find it interesting that it&#x27;s an example of an ML software that&#x27;s totally detached from Python ML ecosystem and also popular.<p>Is Python so annoying to use that when a compelling non-Python solution appears,  everyone will love it? Less hassle? Or did it take off for a different reason? Interested in hearing thoughts.</div><br/><div id="36304615" class="c"><input type="checkbox" id="c-36304615" checked=""/><div class="controls bullet"><span class="by">slabity</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36306639">next</a><span>|</span><label class="collapse" for="c-36304615">[-]</label><label class="expand" for="c-36304615">[17 more]</label></div><br/><div class="children"><div class="content">&gt; Is Python so annoying to use that when a compelling non-Python solution appears, everyone will love it? Less hassle? Or did it take off for a different reason? Interested in hearing thoughts.<p>For me it&#x27;s less about the language and more about the dependencies. When I want to run a Python ML program, I need to go through the hassle of figuring out what the minimum version is, which package manager&#x2F;distribution I need to use, and what system libraries those dependencies need to function properly. If I want to build a package for my distribution, these problems are dialed up to 11 and make it difficult to integrate (especially when using Nix). On top of that, those dependencies typically hide the juicy details of the program I actually care about.<p>For something like C or C++? Usually the most complicated part is running `make` or `cmake` with `pkgconfig` somewhere in my path. Maybe install some missing system libraries if necessary.<p>I just don&#x27;t want to install yet another hundred copies of dependencies in a virtualenv and just hope it&#x27;s set up correctly.</div><br/><div id="36306739" class="c"><input type="checkbox" id="c-36306739" checked=""/><div class="controls bullet"><span class="by">bhy</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304615">parent</a><span>|</span><a href="#36307409">next</a><span>|</span><label class="collapse" for="c-36306739">[-]</label><label class="expand" for="c-36306739">[2 more]</label></div><br/><div class="children"><div class="content">Well I just spent an hour to diagnose the build failure of llama.cpp due to it picking up wrong nvcc path.<p>Dependency problem still happens even with C&#x2F;C++.</div><br/><div id="36306791" class="c"><input type="checkbox" id="c-36306791" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306739">parent</a><span>|</span><a href="#36307409">next</a><span>|</span><label class="collapse" for="c-36306791">[-]</label><label class="expand" for="c-36306791">[1 more]</label></div><br/><div class="children"><div class="content">I had the same issue... Turned out it was because I used the flat pack version of intellij idea and it had problems with paths. Running from a plain terminal worked fine.</div><br/></div></div></div></div><div id="36307409" class="c"><input type="checkbox" id="c-36307409" checked=""/><div class="controls bullet"><span class="by">troad</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304615">parent</a><span>|</span><a href="#36306739">prev</a><span>|</span><a href="#36305966">next</a><span>|</span><label class="collapse" for="c-36307409">[-]</label><label class="expand" for="c-36307409">[1 more]</label></div><br/><div class="children"><div class="content">Strong agree. I&#x27;ll willingly install a handful dependencies from my distro package manager, where the dependencies are battle-hardened Unixy tools and I can clearly see what they do and how they do it.<p>I&#x27;m not going to install thousands of dodgy-looking packages from pip, the only documentation for which is a Discord channel full of children exchanging &#x27;dank memes&#x27;.<p>I like Python, but I simply do not trust the pip ecosystem at this point (same for npm, etc.).</div><br/></div></div><div id="36305966" class="c"><input type="checkbox" id="c-36305966" checked=""/><div class="controls bullet"><span class="by">drdaeman</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304615">parent</a><span>|</span><a href="#36307409">prev</a><span>|</span><a href="#36305070">next</a><span>|</span><label class="collapse" for="c-36305966">[-]</label><label class="expand" for="c-36305966">[1 more]</label></div><br/><div class="children"><div class="content">&gt; which package manager&#x2F;distribution I need to use, and what system libraries those dependencies need to function properly<p>I don&#x27;t understand why things are so complicated in Python+ML world.<p>Normally, when I have a Python project, I just pick the latest Python version - unless documentation specifically tells me otherwise (like if it&#x27;s still Python 2 or if 3.11 is not yet supported). If the project maintainer had some sense, it will have a requirements list with exact locked versions, so I run `pip install -r requirements.txt` (if there is a requirements.txt), `pipenv sync` (if there is a Pipfile), or `poetry install` (if there&#x27;s pyproject.toml). That&#x27;s three commands to remember, and that&#x27;s not one just because pip (the one de-facto package manager) has its limitations but community hadn&#x27;t really decided on the successor. Kinda like `make` vs automake vs `cmake` (vs `bazel` and other less common stuff; same with Python).<p>External libraries are typically not needed - because they&#x27;ll be either provided in binary form with wheels (prebuilt for all most common system types), or automatically built during the installation process, assuming that `gcc`, `pkgconfig` and essential headers are available.<p>Although, I guess, maybe binary wheels aren&#x27;t covering all those Nvidia driver&#x2F;CUDA variations? I&#x27;m not a ML guy, so I&#x27;m sure how this is handled - I&#x27;ve heard there are binary wheels for CUDA libraries, but never used that.<p>Ideally, there&#x27;s Nix (and poetry2nix) that could take care of everything, but only a few folks write Flakes for their projects.<p>&gt; Usually the most complicated part is running `make` or `cmake` with `pkgconfig` somewhere in my path<p>Getting the correct version of all the dependencies is the trickiest part as there is no universal package managers - so it&#x27;s all highly OS&#x2F;distro specific. Some projects vendor their dependencies just to avoid this (and risk getting stuck with awfully out-of-date stuff).<p>&gt; Maybe install some missing system libraries if necessary.<p>And hope their ABIs (if they&#x27;re just dynamically loaded)&#x2F;headers (if linked with) are still compatible with what the project expects. At least that is my primary frustration when I try to build something and it says it doesn&#x27;t work anymore with whatever OS provides (mostly, Debian stable fault lol). It is not exactly fun to backport a Debian package (twice so if doing this properly and not handwaving it with checkinstall).</div><br/></div></div><div id="36305070" class="c"><input type="checkbox" id="c-36305070" checked=""/><div class="controls bullet"><span class="by">adeon</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304615">parent</a><span>|</span><a href="#36305966">prev</a><span>|</span><a href="#36305524">next</a><span>|</span><label class="collapse" for="c-36305070">[-]</label><label class="expand" for="c-36305070">[3 more]</label></div><br/><div class="children"><div class="content">This is also the reason I like when I see a project in C or C++. It&#x27;s often a .&#x2F;configure &amp;&amp; make or something. Sometimes running a Python project even if dependencies install, there might be some mystery crash because package dependencies were not set correctly or something similar (I had a lot of trouble with AUTOMATIC1111 StableDiffusion UI when using some extensions that installed their own requirements that might be in conflict with the main project).<p>With a boring C project, if it compiles it probably works without hassle.<p>Feels validating that other people have these thoughts too and I&#x27;m not just some old fart.</div><br/><div id="36305717" class="c"><input type="checkbox" id="c-36305717" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305070">parent</a><span>|</span><a href="#36305524">next</a><span>|</span><label class="collapse" for="c-36305717">[-]</label><label class="expand" for="c-36305717">[2 more]</label></div><br/><div class="children"><div class="content">I recently hit the &quot;classic&quot; case.  Saw a CLI tool for an API I&#x27;d like to use, written in Python.  Tried it and found out it didn&#x27;t work on my machine.  I later found out it was a bug in a dependency of that tool.  100 lines of shell script later, I had the functionality I needed, and a codebase which was actually free of unexpected surprises.  I know, this is an extreme example, but as personal anecotes go, Python has lost a lot of trust from my side.  I also wonder how people can write &gt;10k codebases without static types, but that is just me ....</div><br/><div id="36306050" class="c"><input type="checkbox" id="c-36306050" checked=""/><div class="controls bullet"><span class="by">iopq</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305717">parent</a><span>|</span><a href="#36305524">next</a><span>|</span><label class="collapse" for="c-36306050">[-]</label><label class="expand" for="c-36306050">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not that Python is bad. It&#x27;s the people who want to just hack something quick together go to Python, so any time you pick up some software written in Python it&#x27;s marred with all kinds of compatibility issues and bugs where you can&#x27;t just run it<p>The answer is &quot;yeah use this other software to make it work in an isolated way because the whole ecosystem is actually broken&quot; and that&#x27;s somehow acceptable</div><br/></div></div></div></div></div></div><div id="36305524" class="c"><input type="checkbox" id="c-36305524" checked=""/><div class="controls bullet"><span class="by">potatolicious</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304615">parent</a><span>|</span><a href="#36305070">prev</a><span>|</span><a href="#36304801">next</a><span>|</span><label class="collapse" for="c-36305524">[-]</label><label class="expand" for="c-36305524">[2 more]</label></div><br/><div class="children"><div class="content">So much this. As someone whose bread and butter is systems programming for things that run on end-user devices, every time I dig into a Python project I feel like I&#x27;ve been teleported into the darkest timeline, where everything is environment management hell.<p>Even the more complex and annoying scenarios in native-land for dependency management still feels positively idyllic in comparison to Python venvs.</div><br/><div id="36306315" class="c"><input type="checkbox" id="c-36306315" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305524">parent</a><span>|</span><a href="#36304801">next</a><span>|</span><label class="collapse" for="c-36306315">[-]</label><label class="expand" for="c-36306315">[1 more]</label></div><br/><div class="children"><div class="content">When I initially started to learn Python (1.6), virtualenv was starting to be adopted, and since then thing have hardly changed.<p>It also helps that even minor versions introduce breaking changes.<p>I doubt anyone really knows Python that well, unless they are on the core team.</div><br/></div></div></div></div><div id="36304801" class="c"><input type="checkbox" id="c-36304801" checked=""/><div class="controls bullet"><span class="by">taf2</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304615">parent</a><span>|</span><a href="#36305524">prev</a><span>|</span><a href="#36305944">next</a><span>|</span><label class="collapse" for="c-36304801">[-]</label><label class="expand" for="c-36304801">[5 more]</label></div><br/><div class="children"><div class="content">I totally agree with you the irony being python and languages like were built in part to reduce the complexity not only of the language but also to build and run the codeâ¦ I feel machine learning is a low enough level thing that it should not be tied to a high level language like pythonâ¦ so I can use node, ruby, php or whatever by adding a c binding etc that to me is why this is most interesting</div><br/><div id="36305420" class="c"><input type="checkbox" id="c-36305420" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304801">parent</a><span>|</span><a href="#36305944">next</a><span>|</span><label class="collapse" for="c-36305420">[-]</label><label class="expand" for="c-36305420">[4 more]</label></div><br/><div class="children"><div class="content">The problem is that python is designed assuming people want to use system-wide packages. In hindsight, that has turned out to be a mistake. Conda &#x2F; venv try to bridge that gap but theyâre kludgy, complex hacks compared to something like cargo or even npm.<p>Worse, because Python is a dynamic language, you also have to deal with all of that complexity at deployment time. (Vs C&#x2F;C++&#x2F;Zig&#x2F;Rust where you can just ship the compiled binary).</div><br/><div id="36306069" class="c"><input type="checkbox" id="c-36306069" checked=""/><div class="controls bullet"><span class="by">drdaeman</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305420">parent</a><span>|</span><a href="#36305944">next</a><span>|</span><label class="collapse" for="c-36306069">[-]</label><label class="expand" for="c-36306069">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The problem is that python is designed assuming people want to use system-wide packages.<p>This wasn&#x27;t true for decades, `virtualenv` was de-facto standard isolation solution (now baked in as `python -m venv`, still de-facto standard), and `pip` is <i>the</i> package manager (we don&#x27;t talk about setuptools&#x2F;distutils, ssh!). If someone still used system-wide packages that was either because a) they were building a container or some single-purpose system; or b) they were sloppy or had no idea what they&#x27;re doing (most likely, following some crappy tutorial). Or it was distro people creating packages to satisfy dependencies for Python programs - but that&#x27;s a whole different story (and one&#x27;s virtualenv shouldn&#x27;t inherit system packages unless it is really really necessary and iif it makes sense to do so).<p>The problem started when one needed some external non-Python dependencies. Python had invented binary wheels and they&#x27;re around for a while (completely solving issues with e.g. PostgreSQL drivers, no one needs to worry about libpq), but I suppose depending on specific versions of kernel drivers and CUDA libraries is a more complex and nuanced subject.<p>&gt; Vs C&#x2F;C++&#x2F;Zig&#x2F;Rust where you can just ship the compiled binary<p>Only assuming that you can either statically link, or if all libraries&#x27; ABIs are stable (or if you&#x27;re targeting a very specific ABI, but I&#x27;ve had my share of &quot;version `GLIBC_2.xx&#x27; not found&quot;s and not fond of those).<p>In a similar spirit, any Python project can be distributed as one binary (Python interpreter and a ZIP archive, bundled together) plus a set of zero or more .so files.</div><br/><div id="36307242" class="c"><input type="checkbox" id="c-36307242" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306069">parent</a><span>|</span><a href="#36306365">next</a><span>|</span><label class="collapse" for="c-36307242">[-]</label><label class="expand" for="c-36307242">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This wasn&#x27;t true for decades, `virtualenv` was de-facto standard isolation solution (now baked in as `python -m venv`, still de-facto standard)<p>Right; but python itself doesnât check your local virtual environment unless you âactivateâ it (ugh what). And it canât handle transitive dependency conflicts, like node and cargo can. Both of those problems stem from python assuming that a simple, flat set of dependencies are passed in from its environment variables.</div><br/></div></div><div id="36306365" class="c"><input type="checkbox" id="c-36306365" checked=""/><div class="controls bullet"><span class="by">kkfx</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306069">parent</a><span>|</span><a href="#36307242">prev</a><span>|</span><a href="#36305944">next</a><span>|</span><label class="collapse" for="c-36306365">[-]</label><label class="expand" for="c-36306365">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  If someone still used system-wide packages that was either because a) [...] b) [...]<p>Or simply because they are packagers for some distro and they user want a simple way to pull-in some software by it&#x27;s name, while the upstream devs imaging people cloning their public repo and run the software from the checkout in their own home, with regular pull, regularly rebuilding the needed surroundings...<p>Not to talking about modern systems&#x2F;distro with not-really-posix vision like NixOS or Guix System...<p>&gt; In a similar spirit, any Python project can be distributed as one binary<p>A single 10+Gb binary :-D</div><br/></div></div></div></div></div></div></div></div><div id="36305944" class="c"><input type="checkbox" id="c-36305944" checked=""/><div class="controls bullet"><span class="by">leoh</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304615">parent</a><span>|</span><a href="#36304801">prev</a><span>|</span><a href="#36306309">next</a><span>|</span><label class="collapse" for="c-36305944">[-]</label><label class="expand" for="c-36305944">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if having a shared cache would make this more easeful. fwiw nix does that.</div><br/></div></div><div id="36306309" class="c"><input type="checkbox" id="c-36306309" checked=""/><div class="controls bullet"><span class="by">kkfx</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304615">parent</a><span>|</span><a href="#36305944">prev</a><span>|</span><a href="#36306639">next</a><span>|</span><label class="collapse" for="c-36306309">[-]</label><label class="expand" for="c-36306309">[1 more]</label></div><br/><div class="children"><div class="content">My personal recipe (on NixOS) is pip-ed virtual environment for quick tests, or conda inside a nix-shell, on top of a dedicated zfs pool&#x2F;conda mounted in ~&#x2F;.conda with dedup=on so nothing nixified and nothing that last a nixos-rebuild...<p>Many pythonic projects not only in ML world tend to be just developers experiments, so to be run as an experiment, not worth to be packaged as a stable, released program...<p>Oh, BTW projects like home-assistant fell in the same bucket...</div><br/></div></div></div></div><div id="36306639" class="c"><input type="checkbox" id="c-36306639" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304615">prev</a><span>|</span><a href="#36304887">next</a><span>|</span><label class="collapse" for="c-36306639">[-]</label><label class="expand" for="c-36306639">[6 more]</label></div><br/><div class="children"><div class="content">Wow, so interesting to see the &quot;depth&quot; of anti-python feeling in some quarters. I guess that is the backlash from all the hordes of Python-bros.<p>Having used both C++ and Python for some time, the idea that managing C++ dependencies is easier than venv and pip install is one of the moments you wonder how credible is HN opinion on anything.<p>&gt; a compelling non-Python solution appears<p>Confusing a large ML framework like pytorch that allows you to experiment and develop <i>any</i> type of model with a particular optimized implementation in a low level language suggests people are not even aware of basic workflows in this space.<p>&gt; also popular<p>Ofcourse its popular. As in: People are delirious with LLM FOMO but can&#x27;t fork gazillions to cloud gatekeepers or NVIDIA so anybody who can alleviate that pain is like a deus-ex-machina.<p>Ofcourse llama.cpp and its creator are great. But the exercise primarily points out that there isn&#x27;t a unified platfrom to both develop and deploy ML type models in a flexible and hardware agnostic way.<p>p.s. For julia lovers that usually jump at the &quot;two-language problem&quot; of Python: here is your chance to shine. There is a Llama.jl that wraps Llama.cpp. You want to develop a native one.</div><br/><div id="36306782" class="c"><input type="checkbox" id="c-36306782" checked=""/><div class="controls bullet"><span class="by">mook</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306639">parent</a><span>|</span><a href="#36306985">next</a><span>|</span><label class="collapse" for="c-36306782">[-]</label><label class="expand" for="c-36306782">[2 more]</label></div><br/><div class="children"><div class="content">Managing C++ dependencies _is_ much easier! It&#x27;s either &quot;run this setup exe&quot; or &quot;extract this zip file&#x2F;tarball&#x2F;dmg and run&quot;.<p>This is because most people don&#x27;t care about developing the project, just using it. So they don&#x27;t care what the dependencies are, just that things work. C++ might be more difficult to handle dependencies to build things, but few people will look into hacking on the code before checking to see if it&#x27;s even relevant.</div><br/><div id="36306843" class="c"><input type="checkbox" id="c-36306843" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306782">parent</a><span>|</span><a href="#36306985">next</a><span>|</span><label class="collapse" for="c-36306843">[-]</label><label class="expand" for="c-36306843">[1 more]</label></div><br/><div class="children"><div class="content">Maybe this distinction explains indeed the dissonance! But it might be rather shortsighted given the state of those models and the need to tune them.</div><br/></div></div></div></div><div id="36306985" class="c"><input type="checkbox" id="c-36306985" checked=""/><div class="controls bullet"><span class="by">lexandstuff</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306639">parent</a><span>|</span><a href="#36306782">prev</a><span>|</span><a href="#36306848">next</a><span>|</span><label class="collapse" for="c-36306985">[-]</label><label class="expand" for="c-36306985">[1 more]</label></div><br/><div class="children"><div class="content">Just to balance things out: I still love Python. A lot!</div><br/></div></div><div id="36306848" class="c"><input type="checkbox" id="c-36306848" checked=""/><div class="controls bullet"><span class="by">simion314</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306639">parent</a><span>|</span><a href="#36306985">prev</a><span>|</span><a href="#36304887">next</a><span>|</span><label class="collapse" for="c-36306848">[-]</label><label class="expand" for="c-36306848">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Wow, so interesting to see the &quot;depth&quot; of anti-python feeling in some quarters. I guess that is the backlash from all the hordes of Python-bros.<p>I think you are generalizing.
I do not hate on Python the language but this ML projects are a very , very terrible experience. Maybe you can lame the devs of this ML projects, or the ones of the dependencies but the experience is shit.  You can follow a step by step instruction that worked 11 day ago and today is broken.<p>I had similar issues with Python GTK apps,  if the app is old enough then you are crewed because that old GTK version is no longer packaged, if the app is very new then you are screwed again because it depends on latest version of some GTK wrapper&#x2F;helper.</div><br/><div id="36306959" class="c"><input type="checkbox" id="c-36306959" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306848">parent</a><span>|</span><a href="#36304887">next</a><span>|</span><label class="collapse" for="c-36306959">[-]</label><label class="expand" for="c-36306959">[1 more]</label></div><br/><div class="children"><div class="content">I think what has happened is that because Python is sweet and easy to use for many things, it generated irrational expectations that is perfect for <i>all</i> things. But its just an interpreted language that started as a scripting and gluing oriented language.<p>Its deployment story is where this gap frequently shows. Desktop apps at best passable, whereas e.g. android apps practically non-existing despite the efforts of projects like kivy.</div><br/></div></div></div></div></div></div><div id="36304887" class="c"><input type="checkbox" id="c-36304887" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36306639">prev</a><span>|</span><a href="#36306502">next</a><span>|</span><label class="collapse" for="c-36304887">[-]</label><label class="expand" for="c-36304887">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t even write python, really, but I&#x27;ve been interfacing with llama.cpp and whisper.cpp through it recently because it&#x27;s been most convenient. Before that I was using nodejs libraries that just wrap those two cpp libs.<p>I guess since these models are meant to be run &quot;client side&quot; or &quot;at the edge&quot; or whatever you want to call it, it helps if they can be neutrally used with just about any wrapper. Using them from Javascript instead of Python is sort of huge for moving ML off the server and into the client.<p>I haven&#x27;t really dipped my toes into the space until llama and whisper cpp came along because they dropped the barrier extremely low. The documentation is simple, and excellent. The tools that it&#x27;s enabled on top like text-generation-webui are next level easy.<p>git clone. make. download model. run.<p>That&#x27;s it.</div><br/></div></div><div id="36306502" class="c"><input type="checkbox" id="c-36306502" checked=""/><div class="controls bullet"><span class="by">realusername</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304887">prev</a><span>|</span><a href="#36307068">next</a><span>|</span><label class="collapse" for="c-36306502">[-]</label><label class="expand" for="c-36306502">[1 more]</label></div><br/><div class="children"><div class="content">I personally really don&#x27;t like much Python, I find it as tedious to write as Go but without the added performance, typesafety and autocomplete benefits that comes with it in exchange.<p>If I have use a dynamic language, at least make it battery included like Ruby. Sure it&#x27;s also not performant but I get something back in exchange.<p>Python sits in a very uncomfortable spot which I don&#x27;t find a use for. Too verbose for a dynamic language and not performant enough compared to a more static language.<p>The testing culture is also pretty poor in my opinion, packages rarely have proper tests, (and especially in the ML field)</div><br/></div></div><div id="36307068" class="c"><input type="checkbox" id="c-36307068" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36306502">prev</a><span>|</span><a href="#36306295">next</a><span>|</span><label class="collapse" for="c-36307068">[-]</label><label class="expand" for="c-36307068">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been wondering for awhile now - what was ever the benefit to building these things in Python, other than pytorch and numpy being there to experiment on in hobbyist ways? There&#x27;s no way that a serious AI is really going to be built in a scripting language, is there? Once you know what you actually want it to do, you&#x27;re definitely going to rebuild it as close to the metal as you can, right?<p>Not to mention, to really protect source code and all the sugar around the training systems, it&#x27;s going to be a good investment to get out of hobby land and manage your own memory and just code them in C&#x2F;C++.<p>It strikes me that the hobby AI ethos aligns very well with scripting languages in that they both assume the availability of endless resources to push things a little dirtier and messier and see if anything interesting emerges. Which is great for hobby AI. It&#x27;s probably not the future, though, unless resource availability outpaces the imagination of people to write more and more bloated scripts to accomplish what&#x27;s already been proven.</div><br/></div></div><div id="36306295" class="c"><input type="checkbox" id="c-36306295" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36307068">prev</a><span>|</span><a href="#36304468">next</a><span>|</span><label class="collapse" for="c-36306295">[-]</label><label class="expand" for="c-36306295">[1 more]</label></div><br/><div class="children"><div class="content">For me Python&#x27;s main use cases are being BASIC replacement, and a saner alternative to Perl (I like Perl though) for OS scripting.<p>For everything else, I rather use compiled languages with JIT&#x2F;AOT toolchains, and since most &quot;Python libraries&quot; for machine learning are actually C and C++, any language goes, there is nothing special about Python there.</div><br/></div></div><div id="36304468" class="c"><input type="checkbox" id="c-36304468" checked=""/><div class="controls bullet"><span class="by">csjh</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36306295">prev</a><span>|</span><a href="#36304627">next</a><span>|</span><label class="collapse" for="c-36304468">[-]</label><label class="expand" for="c-36304468">[1 more]</label></div><br/><div class="children"><div class="content">I believe it&#x27;s moreso for the (actively pursued) speed optimizations it provides. When inference is already computationally expensive any bit of performance is a big plus</div><br/></div></div><div id="36304627" class="c"><input type="checkbox" id="c-36304627" checked=""/><div class="controls bullet"><span class="by">bartwr</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304468">prev</a><span>|</span><a href="#36304492">next</a><span>|</span><label class="collapse" for="c-36304627">[-]</label><label class="expand" for="c-36304627">[1 more]</label></div><br/><div class="children"><div class="content">Even &quot;compiled&quot; JAX or PyTorch can leave some performance even if you hit the common path (It&#x27;s also &quot;praying&quot; that the compiler actually works if you do anything non-standard).<p>But memory wise, there is almost no optimization or reuse (and it&#x27;s sacrificed for performance), which leads to insane memory usage.<p>And it&#x27;s not that they are bad - but optimal compilation of a graph is combinatorially explosive problem, impossible without heuristics and guesswork (what to reuse and waste memory vs recompute). A good programmer can do a significantly better job.</div><br/></div></div><div id="36304492" class="c"><input type="checkbox" id="c-36304492" checked=""/><div class="controls bullet"><span class="by">plaguuuuuu</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304627">prev</a><span>|</span><a href="#36304497">next</a><span>|</span><label class="collapse" for="c-36304492">[-]</label><label class="expand" for="c-36304492">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using it because there are bindings in other languages I know like .NET</div><br/></div></div><div id="36304497" class="c"><input type="checkbox" id="c-36304497" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304492">prev</a><span>|</span><a href="#36304815">next</a><span>|</span><label class="collapse" for="c-36304497">[-]</label><label class="expand" for="c-36304497">[12 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a great project and an impressive achievement, but I&#x27;m also struggling to understand what people use it for that PyTorch wasn&#x27;t offering. Easy deployment on iOS I guess? I would have thought that&#x27;s a pretty small use case though.<p>Given the author hand-rolled his own FFT, I&#x27;m also guessing it&#x27;s not as performant?</div><br/><div id="36305575" class="c"><input type="checkbox" id="c-36305575" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304497">parent</a><span>|</span><a href="#36304603">next</a><span>|</span><label class="collapse" for="c-36305575">[-]</label><label class="expand" for="c-36305575">[1 more]</label></div><br/><div class="children"><div class="content">Pytorch (+GPU) dependency and python container type diversity <i>are</i> particularly bad. Programmers may not perceive this since they&#x27;re already managing their python environment keeping all the OS&#x2F;libs&#x2F;containers&#x2F;applications in the alignment required for things to work but it&#x27;s quite complex. I couldn&#x27;t do it.<p>In comparison I could just type git clone <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a> and make . And it worked. And since then I&#x27;ve managed to get llama.cpp clBLAS partial GPU acceleration working with my AMD RX 580 8GB. Plus with the llama.cpp CPU mmap stuff I can run multiple LLM IRC bot processes using the same model all sharing the RAM representation for free. Are there even ways to run 2 or 3 bit models in pytorch implementations like llama.cpp can do? It&#x27;s pretty rad I could run a 65B llama in 27 GB of RAM on my 32GB RAM system (and still get better perplexity than 30B 8 bit).</div><br/></div></div><div id="36304603" class="c"><input type="checkbox" id="c-36304603" checked=""/><div class="controls bullet"><span class="by">civilitty</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304497">parent</a><span>|</span><a href="#36305575">prev</a><span>|</span><a href="#36304610">next</a><span>|</span><label class="collapse" for="c-36304603">[-]</label><label class="expand" for="c-36304603">[8 more]</label></div><br/><div class="children"><div class="content">Easy deployment anywhere, not just iOS. I haven&#x27;t used Python in years so I have no idea what package manager is the best now, completely forgot how to use virtualenv, and it only took a few weeks to completely fuck up my local Python install (&quot;Your version of CUDA doesn&#x27;t match the one used to blah blah&quot;)<p>Python is a mess. llama.cpp was literally a git clone followed by &quot;cd llama.cpp &amp;&amp; make &amp;&amp; .&#x2F;main&quot; - I can recite the commands from memory and I haven&#x27;t done any C&#x2F;C++ development in a long time.</div><br/><div id="36305251" class="c"><input type="checkbox" id="c-36305251" checked=""/><div class="controls bullet"><span class="by">cshimmin</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304603">parent</a><span>|</span><a href="#36304840">next</a><span>|</span><label class="collapse" for="c-36305251">[-]</label><label class="expand" for="c-36305251">[6 more]</label></div><br/><div class="children"><div class="content">For most modern ML projects in python you can just do something like `conda env create -f environment.yaml` then straight to `.&#x2F;main.py`. This handles very complex dependencies in a single command.<p>The example you gave works because llama.cpp specifically strives to have no dependencies. But this is not an intrinsically useful goal; there&#x27;s a reason software libraries were invented. I always have fun when I find out that the thing I&#x27;m trying to compile needs -std=C++26 and glibc 3.0, and I&#x27;m running on a HPC cluster where I can&#x27;t use a system level package manager, and I don&#x27;t want to be arsed to dockerize every small thing I want to run.<p>For scientific and ML uses, conda has basically solved the whole &quot;python packaging is a mess&quot; that people seem to still complain about, at least on the end-user side. Sure, conda is slow as hell but there&#x27;s a drop in replacement (mamba) that solves that issue.</div><br/><div id="36305591" class="c"><input type="checkbox" id="c-36305591" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305251">parent</a><span>|</span><a href="#36306255">next</a><span>|</span><label class="collapse" for="c-36305591">[-]</label><label class="expand" for="c-36305591">[3 more]</label></div><br/><div class="children"><div class="content">Conda? Mamba? Or should I use Venv? What are the commands to âactivateâ an environment? And why do I have to do that anyway, given thats not needed in any other programming language? Which of those systems support nested dependencies properly? Do any of them support the dependency graph containing multiple, mutually incompatible copies of the same package?<p>Coming from rust (and nodejs before that), the package management situation in python feels like a mess. Itâs barely better than C and C++ - both of which are also a disaster. (Make? Autotools? CMake? Use vendored dependencies? System dependencies? Which openblas package should I install from apt? Are <i>any</i> of them recent enough? Kill me.)<p>Node: npm install. npm start.<p>Rust: cargo run. Cargo run â-release.<p>I donât want to pick from 18 flavours of âvirtual environmentsâ that I have to remember how to to âactivateâ. And I donât want to deal with transitive dependency conflicts, and I donât want to be wading through my distroâs packages to figure out how to manually install dependencies.<p>I just want to run the program. Python and C both make that much more difficult than it needs to be.</div><br/><div id="36306272" class="c"><input type="checkbox" id="c-36306272" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305591">parent</a><span>|</span><a href="#36306255">next</a><span>|</span><label class="collapse" for="c-36306272">[-]</label><label class="expand" for="c-36306272">[2 more]</label></div><br/><div class="children"><div class="content">&gt; npm install. npm start.<p>Yeah, noâ¦ my current $DAYJOB has a confusing mix of nx, pnpm, npm commands in multiple projects. Python is bad but node is absolutely not a good example.</div><br/><div id="36307324" class="c"><input type="checkbox" id="c-36307324" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306272">parent</a><span>|</span><a href="#36306255">next</a><span>|</span><label class="collapse" for="c-36307324">[-]</label><label class="expand" for="c-36307324">[1 more]</label></div><br/><div class="children"><div class="content">Eh. Node &#x2F; npm on its own is generally fine, especially if you use a private package repository for internally shared libraries. The problems show up when compiling javascript for the web as well as nodejs. If you stick to server side javascript using node and npm, it all works pretty well. Itâs much nicer than venv &#x2F; conda. And it handles transitive dependency conflicts and all sorts of wacky problems.<p>Itâs just that almost nobody does that.<p>What we want instead is to combine typescript, js bundlers, wasm, es modules and node packages all together to make web apps. And thatâs more than enough to bring seasoned engineers to tears. Let alone adding in svelte &#x2F; solidjs compilation on top of all that. I have sweats just thinking about it.</div><br/></div></div></div></div></div></div><div id="36306255" class="c"><input type="checkbox" id="c-36306255" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305251">parent</a><span>|</span><a href="#36305591">prev</a><span>|</span><a href="#36304840">next</a><span>|</span><label class="collapse" for="c-36306255">[-]</label><label class="expand" for="c-36306255">[2 more]</label></div><br/><div class="children"><div class="content">I have no dog in this fight, but the fact that you end your &quot;conda solves this complexity&quot; explanation with &quot;and mamba is a replacement for conda&quot; doesn&#x27;t really sell me on it. Is it just speed as the main reason for mamba? The fact that the &quot;one ring to rule them all&quot; solve for pythons packaging woes even <i>has</i> a replacement sort of defeats the purpose a little.<p>I understand why people find Python&#x27;s packaging story painful, I guess is what I&#x27;m saying.</div><br/><div id="36306617" class="c"><input type="checkbox" id="c-36306617" checked=""/><div class="controls bullet"><span class="by">gre</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36306255">parent</a><span>|</span><a href="#36304840">next</a><span>|</span><label class="collapse" for="c-36306617">[-]</label><label class="expand" for="c-36306617">[1 more]</label></div><br/><div class="children"><div class="content">conda takes up to several minutes to figure out dependencies just to tell you that one library requires an old version of a dependency and another requires a new one, making the whole setup impossible.<p>mamba can do this much quicker.</div><br/></div></div></div></div></div></div><div id="36304840" class="c"><input type="checkbox" id="c-36304840" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304603">parent</a><span>|</span><a href="#36305251">prev</a><span>|</span><a href="#36304610">next</a><span>|</span><label class="collapse" for="c-36304840">[-]</label><label class="expand" for="c-36304840">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the response. Interesting to see a lot of other people echoing the same comments - that dependency management in Python is an absolute PITA.<p>I know exactly what you mean, but I&#x27;m probably so inured to it by now that I&#x27;ve just come to accept it. Obviously not everyone feels this way!</div><br/></div></div></div></div><div id="36306253" class="c"><input type="checkbox" id="c-36306253" checked=""/><div class="controls bullet"><span class="by">slashtom</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304497">parent</a><span>|</span><a href="#36304610">prev</a><span>|</span><a href="#36304815">next</a><span>|</span><label class="collapse" for="c-36306253">[-]</label><label class="expand" for="c-36306253">[1 more]</label></div><br/><div class="children"><div class="content">llama cpp is just cpp inference on the llama model.  PyTorch is a library to train neural networks.  I&#x27;m not sure why people are conflating these two totally different projects..</div><br/></div></div></div></div><div id="36304815" class="c"><input type="checkbox" id="c-36304815" checked=""/><div class="controls bullet"><span class="by">b33j0r</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304497">prev</a><span>|</span><a href="#36304528">next</a><span>|</span><label class="collapse" for="c-36304815">[-]</label><label class="expand" for="c-36304815">[1 more]</label></div><br/><div class="children"><div class="content">Not quite. Nothing to do directly with python. This was the introduction of 4 and 8 bit quantization to a large number of people.<p>There wasnât a python library like that anyone was used to using. Would have always been a C extension anyway.<p>Starting with the cpu in this situation made sense, strangely. There are python wrappers now. I tried to make one for yaâll in rust in April, but haha I had a compiler issue I never solved.</div><br/></div></div><div id="36304528" class="c"><input type="checkbox" id="c-36304528" checked=""/><div class="controls bullet"><span class="by">forgingahead</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304815">prev</a><span>|</span><a href="#36304602">next</a><span>|</span><label class="collapse" for="c-36304528">[-]</label><label class="expand" for="c-36304528">[6 more]</label></div><br/><div class="children"><div class="content">Yes Python is incredibly annoying to use. Their dependency management is a total mess, and it&#x27;s incredible how brittle packages if there are even minor point changes in versions anywhere in a stack.</div><br/><div id="36304705" class="c"><input type="checkbox" id="c-36304705" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304528">parent</a><span>|</span><a href="#36304602">next</a><span>|</span><label class="collapse" for="c-36304705">[-]</label><label class="expand" for="c-36304705">[5 more]</label></div><br/><div class="children"><div class="content">I have to agree. Installing dependencies for some git repos is a total crapshoot. I ended up wasting so much hard drive space with copies of pytorch. Meanwhile llama.cpp is just &quot;make&quot; and takes less time to build than to download one copy of pytorch.</div><br/><div id="36305334" class="c"><input type="checkbox" id="c-36305334" checked=""/><div class="controls bullet"><span class="by">cshimmin</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36304705">parent</a><span>|</span><a href="#36304602">next</a><span>|</span><label class="collapse" for="c-36305334">[-]</label><label class="expand" for="c-36305334">[4 more]</label></div><br/><div class="children"><div class="content">So, the solution is that everyone should write code as self-contained C++ code and not use any software libraries ever. Dependency hell has been solved for all time!</div><br/><div id="36305480" class="c"><input type="checkbox" id="c-36305480" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305334">parent</a><span>|</span><a href="#36305714">next</a><span>|</span><label class="collapse" for="c-36305480">[-]</label><label class="expand" for="c-36305480">[1 more]</label></div><br/><div class="children"><div class="content">Well, in the cases where the libraries are causing more work than they are solving. Then yes.</div><br/></div></div><div id="36305714" class="c"><input type="checkbox" id="c-36305714" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305334">parent</a><span>|</span><a href="#36305480">prev</a><span>|</span><a href="#36305580">next</a><span>|</span><label class="collapse" for="c-36305714">[-]</label><label class="expand" for="c-36305714">[1 more]</label></div><br/><div class="children"><div class="content">Python was released in 1991. It obviously didnât get package management right, just like C and C++ didnât figure it out in the 70s.<p>Take a look at rustâs Cargo for what a modern package manager should look like. Or deno &#x2F; Go if you swing that way.<p>Which old language gets package management right? None of them. None of them get it right.<p>And sure - conda &#x2F; venv &#x2F; CMake &#x2F; etc help. But last centuryâs bad design decisions still shine through.</div><br/></div></div><div id="36305580" class="c"><input type="checkbox" id="c-36305580" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36304457">root</a><span>|</span><a href="#36305334">parent</a><span>|</span><a href="#36305714">prev</a><span>|</span><a href="#36304602">next</a><span>|</span><label class="collapse" for="c-36305580">[-]</label><label class="expand" for="c-36305580">[1 more]</label></div><br/><div class="children"><div class="content">There is a happy medium... somewhere. After following Postgres development for the better part of a decade, I think it&#x27;s definitely closer to the python side of things... But man they (python) do make it hard to like using that ecosystem.<p>The flip side is like you said... You will just have to reimplement everything yourself and then you can never worry about dependencies again! Just hope you didn&#x27;t introduce some obscure security issue in your hashmap implementation .</div><br/></div></div></div></div></div></div></div></div><div id="36304602" class="c"><input type="checkbox" id="c-36304602" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304528">prev</a><span>|</span><a href="#36305574">next</a><span>|</span><label class="collapse" for="c-36304602">[-]</label><label class="expand" for="c-36304602">[1 more]</label></div><br/><div class="children"><div class="content">yeah it&#x27;s sad I guess but half the reason I am recommending this is that it &quot;just works&quot; so much more easily than installing half the Python ecosystem into a conda environment (ironically, just so that Python can then behave as a thin wrapper for calling the underlying native libraries ...)</div><br/></div></div><div id="36305574" class="c"><input type="checkbox" id="c-36305574" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36304602">prev</a><span>|</span><a href="#36306899">next</a><span>|</span><label class="collapse" for="c-36305574">[-]</label><label class="expand" for="c-36305574">[1 more]</label></div><br/><div class="children"><div class="content">Python isn&#x27;t annoying to use, at least for me and many I know. But it isn&#x27;t known for speed. And or easy multithreading.</div><br/></div></div><div id="36306899" class="c"><input type="checkbox" id="c-36306899" checked=""/><div class="controls bullet"><span class="by">MrYellowP</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36305574">prev</a><span>|</span><a href="#36306359">next</a><span>|</span><label class="collapse" for="c-36306899">[-]</label><label class="expand" for="c-36306899">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is Python so annoying to use that when a compelling non-Python solution appears, everyone will love it? Less hassle? Or did it take off for a different reason? Interested in hearing thoughts.<p>Exploring the landscape ends up with you having 29384232938792834234 different python environments, because that one thing requires specific versions of one set of libraries, while that other thing requires different versions of the same library and <i>there is no middle ground</i>.<p>It&#x27;s <i>horribly</i> annoying and I absolutely <i>love</i> python!</div><br/></div></div><div id="36306359" class="c"><input type="checkbox" id="c-36306359" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#36304457">parent</a><span>|</span><a href="#36306899">prev</a><span>|</span><a href="#36305531">next</a><span>|</span><label class="collapse" for="c-36306359">[-]</label><label class="expand" for="c-36306359">[1 more]</label></div><br/><div class="children"><div class="content">The Python apologists are more annoying than the language.<p>Its always been obvious that MLâs marriage to python has always been credential ladened proponents in tangentially related fields following group think.<p>As soon as we got a reason to ignore those PhDs, their gatekept moat evaporated overnight and the community of [co-]dependencies became irrelevant overnight.</div><br/></div></div></div></div><div id="36305531" class="c"><input type="checkbox" id="c-36305531" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#36304457">prev</a><span>|</span><a href="#36304520">next</a><span>|</span><label class="collapse" for="c-36305531">[-]</label><label class="expand" for="c-36305531">[9 more]</label></div><br/><div class="children"><div class="content">Slightly OT:<p>I have been playing around with whisper.cpp; it&#x27;s nice because I can run the large model (quantized to 8-bits) at roughly real-time with cublas on a Ryzen 2700 with a 1050Ti.  I couldn&#x27;t even run the pytorch whisper medium on this card with X11 also running.<p>It blows me away that I can get real-time speech-to-text of this quality on a machine that is almost 5 years old.</div><br/><div id="36306236" class="c"><input type="checkbox" id="c-36306236" checked=""/><div class="controls bullet"><span class="by">nitinreddy88</span><span>|</span><a href="#36305531">parent</a><span>|</span><a href="#36305622">next</a><span>|</span><label class="collapse" for="c-36306236">[-]</label><label class="expand" for="c-36306236">[2 more]</label></div><br/><div class="children"><div class="content">is there any dummy guide to get started with any of these?</div><br/><div id="36307316" class="c"><input type="checkbox" id="c-36307316" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#36305531">root</a><span>|</span><a href="#36306236">parent</a><span>|</span><a href="#36305622">next</a><span>|</span><label class="collapse" for="c-36307316">[-]</label><label class="expand" for="c-36307316">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried the Quick start in the <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;whisper.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;whisper.cpp</a> README?</div><br/></div></div></div></div><div id="36305622" class="c"><input type="checkbox" id="c-36305622" checked=""/><div class="controls bullet"><span class="by">moneywoes</span><span>|</span><a href="#36305531">parent</a><span>|</span><a href="#36306236">prev</a><span>|</span><a href="#36304520">next</a><span>|</span><label class="collapse" for="c-36305622">[-]</label><label class="expand" for="c-36305622">[6 more]</label></div><br/><div class="children"><div class="content">Is it possible to run on apple m1 devices or mobile phones or not yet?</div><br/><div id="36306099" class="c"><input type="checkbox" id="c-36306099" checked=""/><div class="controls bullet"><span class="by">michelb</span><span>|</span><a href="#36305531">root</a><span>|</span><a href="#36305622">parent</a><span>|</span><a href="#36305679">next</a><span>|</span><label class="collapse" for="c-36306099">[-]</label><label class="expand" for="c-36306099">[4 more]</label></div><br/><div class="children"><div class="content">I can recommend the MacWhisper app if you prefer a gui.</div><br/><div id="36306810" class="c"><input type="checkbox" id="c-36306810" checked=""/><div class="controls bullet"><span class="by">Void_</span><span>|</span><a href="#36305531">root</a><span>|</span><a href="#36306099">parent</a><span>|</span><a href="#36305679">next</a><span>|</span><label class="collapse" for="c-36306810">[-]</label><label class="expand" for="c-36306810">[3 more]</label></div><br/><div class="children"><div class="content">And Whisper Memos for iOS <a href="https:&#x2F;&#x2F;whispermemos.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;whispermemos.com&#x2F;</a></div><br/><div id="36306934" class="c"><input type="checkbox" id="c-36306934" checked=""/><div class="controls bullet"><span class="by">b33f</span><span>|</span><a href="#36305531">root</a><span>|</span><a href="#36306810">parent</a><span>|</span><a href="#36305679">next</a><span>|</span><label class="collapse" for="c-36306934">[-]</label><label class="expand" for="c-36306934">[2 more]</label></div><br/><div class="children"><div class="content">The really nice part of Whisper is being able to use it offline and on-device, it seems whisper memos is uploading your audio and notes to a server of unknown security, confidentiality etc.<p>I like Aiko for on-device transcription both in macOS and iOS <a href="https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;aiko&#x2F;id1672085276" rel="nofollow noreferrer">https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;aiko&#x2F;id1672085276</a></div><br/><div id="36307392" class="c"><input type="checkbox" id="c-36307392" checked=""/><div class="controls bullet"><span class="by">Void_</span><span>|</span><a href="#36305531">root</a><span>|</span><a href="#36306934">parent</a><span>|</span><a href="#36305679">next</a><span>|</span><label class="collapse" for="c-36307392">[-]</label><label class="expand" for="c-36307392">[1 more]</label></div><br/><div class="children"><div class="content">Whisper Memos uses OpenAI API. The upside is that it uses the largest model - that would take 2GB on your iPhone.</div><br/></div></div></div></div></div></div></div></div><div id="36305679" class="c"><input type="checkbox" id="c-36305679" checked=""/><div class="controls bullet"><span class="by">raihansaputra</span><span>|</span><a href="#36305531">root</a><span>|</span><a href="#36305622">parent</a><span>|</span><a href="#36306099">prev</a><span>|</span><a href="#36304520">next</a><span>|</span><label class="collapse" for="c-36305679">[-]</label><label class="expand" for="c-36305679">[1 more]</label></div><br/><div class="children"><div class="content">yeah the whisper.cpp github page has a demo for both. Have used it on my M1 MBA for the past few months.</div><br/></div></div></div></div></div></div><div id="36304520" class="c"><input type="checkbox" id="c-36304520" checked=""/><div class="controls bullet"><span class="by">shon</span><span>|</span><a href="#36305531">prev</a><span>|</span><a href="#36307232">next</a><span>|</span><label class="collapse" for="c-36304520">[-]</label><label class="expand" for="c-36304520">[2 more]</label></div><br/><div class="children"><div class="content">Nice to see Georgi has started a company:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1666120568993730561?s=46&amp;t=pO499fGQKTiGvvZPpc-cFw" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1666120568993730561?s=4...</a><p>Godspeed</div><br/><div id="36307438" class="c"><input type="checkbox" id="c-36307438" checked=""/><div class="controls bullet"><span class="by">fnands</span><span>|</span><a href="#36304520">parent</a><span>|</span><a href="#36307232">next</a><span>|</span><label class="collapse" for="c-36307438">[-]</label><label class="expand" for="c-36307438">[1 more]</label></div><br/><div class="children"><div class="content">Nice. He&#x27;s obviously a talented engineer who&#x27;s struck a nerve with the whisper.cpp&#x2F;llama.cpp projects, so hope he has success with whatever he plans to do.</div><br/></div></div></div></div><div id="36307232" class="c"><input type="checkbox" id="c-36307232" checked=""/><div class="controls bullet"><span class="by">mFixman</span><span>|</span><a href="#36304520">prev</a><span>|</span><a href="#36307071">next</a><span>|</span><label class="collapse" for="c-36307232">[-]</label><label class="expand" for="c-36307232">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have a PC with a powerful GPU. What&#x27;s the easiest way I can play with Llama on AWS, Google Cloud, or somebody else&#x27;s computer?</div><br/><div id="36307311" class="c"><input type="checkbox" id="c-36307311" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#36307232">parent</a><span>|</span><a href="#36307071">next</a><span>|</span><label class="collapse" for="c-36307311">[-]</label><label class="expand" for="c-36307311">[1 more]</label></div><br/><div class="children"><div class="content">You can play with Llama on your CPU. Depending on the model you use and the RAM you have available, the performance may be acceptable.</div><br/></div></div></div></div><div id="36307071" class="c"><input type="checkbox" id="c-36307071" checked=""/><div class="controls bullet"><span class="by">supermatt</span><span>|</span><a href="#36307232">prev</a><span>|</span><a href="#36304576">next</a><span>|</span><label class="collapse" for="c-36307071">[-]</label><label class="expand" for="c-36307071">[2 more]</label></div><br/><div class="children"><div class="content">My understanding from reading this is that a 3090 GPU is 2x speedup over a decent modern CPU. Is that really the case, or am I reading it wrong? My initial thought was that it would be far higher. Is this typical of inference for these kind of models? If so, why do we need such expensive hardware? Please excuse my lack of knowledge :)</div><br/><div id="36307182" class="c"><input type="checkbox" id="c-36307182" checked=""/><div class="controls bullet"><span class="by">malf</span><span>|</span><a href="#36307071">parent</a><span>|</span><a href="#36304576">next</a><span>|</span><label class="collapse" for="c-36307182">[-]</label><label class="expand" for="c-36307182">[1 more]</label></div><br/><div class="children"><div class="content">I think it was 2x total speedup vs previous version, which already used gpu for âmostâ things, so the real speedup is 2&#x2F;(1-most), which could be a lot.</div><br/></div></div></div></div><div id="36304576" class="c"><input type="checkbox" id="c-36304576" checked=""/><div class="controls bullet"><span class="by">getcrunk</span><span>|</span><a href="#36307071">prev</a><span>|</span><a href="#36304415">next</a><span>|</span><label class="collapse" for="c-36304576">[-]</label><label class="expand" for="c-36304576">[4 more]</label></div><br/><div class="children"><div class="content">Anyone get performance numbers for other 30 series cards? 3060 12gb?<p>Iâm curious how it compares to his apple silicon numbers</div><br/><div id="36304879" class="c"><input type="checkbox" id="c-36304879" checked=""/><div class="controls bullet"><span class="by">speed_spread</span><span>|</span><a href="#36304576">parent</a><span>|</span><a href="#36304415">next</a><span>|</span><label class="collapse" for="c-36304879">[-]</label><label class="expand" for="c-36304879">[3 more]</label></div><br/><div class="children"><div class="content">Also, someone please let me use my 3050 4GB for something else than stable diffusion generation of silly thumbnail-sized pics. I&#x27;d be happy with an LLM that&#x27;s specialized in insults and car analogies.</div><br/><div id="36305775" class="c"><input type="checkbox" id="c-36305775" checked=""/><div class="controls bullet"><span class="by">Blahah</span><span>|</span><a href="#36304576">root</a><span>|</span><a href="#36304879">parent</a><span>|</span><a href="#36305343">next</a><span>|</span><label class="collapse" for="c-36305775">[-]</label><label class="expand" for="c-36305775">[1 more]</label></div><br/><div class="children"><div class="content">You can split inference between CPU and GPU using whatever available GPU vRAM with llama.cpp. And you can run many small models with 4GB of vRAM. Anything with 3B parameters quantized to 4bit should be fine.</div><br/></div></div><div id="36305343" class="c"><input type="checkbox" id="c-36305343" checked=""/><div class="controls bullet"><span class="by">wing-_-nuts</span><span>|</span><a href="#36304576">root</a><span>|</span><a href="#36304879">parent</a><span>|</span><a href="#36305775">prev</a><span>|</span><a href="#36304415">next</a><span>|</span><label class="collapse" for="c-36305343">[-]</label><label class="expand" for="c-36305343">[1 more]</label></div><br/><div class="children"><div class="content">Did you buy that card for cuda?  Cause otherwise I have no idea why someone would chose a 3050 over a 6600</div><br/></div></div></div></div></div></div><div id="36304415" class="c"><input type="checkbox" id="c-36304415" checked=""/><div class="controls bullet"><span class="by">jfdi</span><span>|</span><a href="#36304576">prev</a><span>|</span><a href="#36306041">next</a><span>|</span><label class="collapse" for="c-36304415">[-]</label><label class="expand" for="c-36304415">[9 more]</label></div><br/><div class="children"><div class="content">Is there a legitimate way to get the weights to actually use this without filling in forms?</div><br/><div id="36306225" class="c"><input type="checkbox" id="c-36306225" checked=""/><div class="controls bullet"><span class="by">dchest</span><span>|</span><a href="#36304415">parent</a><span>|</span><a href="#36304462">next</a><span>|</span><label class="collapse" for="c-36306225">[-]</label><label class="expand" for="c-36306225">[1 more]</label></div><br/><div class="children"><div class="content">You can use OpenLLaMA models (Apache 2.0 license, unrelated to LLaMA apart from their architecture and general approach to training):<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;open-llama-7b-open-instruct-GGML" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;open-llama-7b-open-instruct-...</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;SlyEcho&#x2F;open_llama_7b_ggml" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;SlyEcho&#x2F;open_llama_7b_ggml</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;SlyEcho&#x2F;open_llama_3b_ggml" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;SlyEcho&#x2F;open_llama_3b_ggml</a></div><br/></div></div><div id="36304462" class="c"><input type="checkbox" id="c-36304462" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#36304415">parent</a><span>|</span><a href="#36306225">prev</a><span>|</span><a href="#36304446">next</a><span>|</span><label class="collapse" for="c-36304462">[-]</label><label class="expand" for="c-36304462">[2 more]</label></div><br/><div class="children"><div class="content">Thereâs a torrent linked in the Llama.cpp docs, itâs in a merge request on the LLaMA repo. Has all the files.</div><br/><div id="36306089" class="c"><input type="checkbox" id="c-36306089" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#36304415">root</a><span>|</span><a href="#36304462">parent</a><span>|</span><a href="#36304446">next</a><span>|</span><label class="collapse" for="c-36306089">[-]</label><label class="expand" for="c-36306089">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s almost (or actually is) a &quot;pirated&quot; torrent. So it might not be &quot;legitimate&quot;.</div><br/></div></div></div></div><div id="36304446" class="c"><input type="checkbox" id="c-36304446" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36304415">parent</a><span>|</span><a href="#36304462">prev</a><span>|</span><a href="#36305502">next</a><span>|</span><label class="collapse" for="c-36304446">[-]</label><label class="expand" for="c-36304446">[1 more]</label></div><br/><div class="children"><div class="content">Not if you want the original LLAMA weights, but now there are other models like RedPajama available.</div><br/></div></div><div id="36305502" class="c"><input type="checkbox" id="c-36305502" checked=""/><div class="controls bullet"><span class="by">adultSwim</span><span>|</span><a href="#36304415">parent</a><span>|</span><a href="#36304446">prev</a><span>|</span><a href="#36304451">next</a><span>|</span><label class="collapse" for="c-36305502">[-]</label><label class="expand" for="c-36305502">[1 more]</label></div><br/><div class="children"><div class="content">No</div><br/></div></div><div id="36304451" class="c"><input type="checkbox" id="c-36304451" checked=""/><div class="controls bullet"><span class="by">csjh</span><span>|</span><a href="#36304415">parent</a><span>|</span><a href="#36305502">prev</a><span>|</span><a href="#36306041">next</a><span>|</span><label class="collapse" for="c-36304451">[-]</label><label class="expand" for="c-36304451">[3 more]</label></div><br/><div class="children"><div class="content">They come a dime a dozen on HuggingFace, check out <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;wiki&#x2F;models" rel="nofollow noreferrer">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;wiki&#x2F;models</a> for a few options</div><br/><div id="36304753" class="c"><input type="checkbox" id="c-36304753" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36304415">root</a><span>|</span><a href="#36304451">parent</a><span>|</span><a href="#36306041">next</a><span>|</span><label class="collapse" for="c-36304753">[-]</label><label class="expand" for="c-36304753">[2 more]</label></div><br/><div class="children"><div class="content">Are these done as LLaMA deltas still? I.e. do I need to apply a patch to LLaMA, and so I still need to source LLaMA?</div><br/><div id="36305281" class="c"><input type="checkbox" id="c-36305281" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36304415">root</a><span>|</span><a href="#36304753">parent</a><span>|</span><a href="#36306041">next</a><span>|</span><label class="collapse" for="c-36305281">[-]</label><label class="expand" for="c-36305281">[1 more]</label></div><br/><div class="children"><div class="content">Most of them are merged models, so you don&#x27;t need the base model.<p>It&#x27;s stupidly simple to get going.</div><br/></div></div></div></div></div></div></div></div><div id="36306041" class="c"><input type="checkbox" id="c-36306041" checked=""/><div class="controls bullet"><span class="by">hendry</span><span>|</span><a href="#36304415">prev</a><span>|</span><a href="#36304899">next</a><span>|</span><label class="collapse" for="c-36306041">[-]</label><label class="expand" for="c-36306041">[2 more]</label></div><br/><div class="children"><div class="content">RTX 3090 isn&#x27;t cheap, more than 1000GBP new, crikey!</div><br/><div id="36307327" class="c"><input type="checkbox" id="c-36307327" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#36306041">parent</a><span>|</span><a href="#36304899">next</a><span>|</span><label class="collapse" for="c-36307327">[-]</label><label class="expand" for="c-36307327">[1 more]</label></div><br/><div class="children"><div class="content">Why not get a used one?</div><br/></div></div></div></div><div id="36304899" class="c"><input type="checkbox" id="c-36304899" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#36306041">prev</a><span>|</span><a href="#36304653">next</a><span>|</span><label class="collapse" for="c-36304899">[-]</label><label class="expand" for="c-36304899">[2 more]</label></div><br/><div class="children"><div class="content">Anyone know if whisper.cpp is GPU accelerated yet?</div><br/><div id="36305545" class="c"><input type="checkbox" id="c-36305545" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#36304899">parent</a><span>|</span><a href="#36304653">next</a><span>|</span><label class="collapse" for="c-36305545">[-]</label><label class="expand" for="c-36305545">[1 more]</label></div><br/><div class="children"><div class="content">AFAIK it&#x27;s not fully, but you can use cuBLAS&#x2F;clBlast for a pretty good speedup.</div><br/></div></div></div></div><div id="36304653" class="c"><input type="checkbox" id="c-36304653" checked=""/><div class="controls bullet"><span class="by">gigel82</span><span>|</span><a href="#36304899">prev</a><span>|</span><a href="#36305888">next</a><span>|</span><label class="collapse" for="c-36304653">[-]</label><label class="expand" for="c-36304653">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a total newb about the implementation details, but I&#x27;m curious if a hybrid is possible (GPU+CPU) to enable inference with even larger models than what fits in consumer GPU VRAM.</div><br/><div id="36304673" class="c"><input type="checkbox" id="c-36304673" checked=""/><div class="controls bullet"><span class="by">skirmish</span><span>|</span><a href="#36304653">parent</a><span>|</span><a href="#36305888">next</a><span>|</span><label class="collapse" for="c-36304673">[-]</label><label class="expand" for="c-36304673">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp does it already.  You tell it how many layers to offload to GPU, and it runs remaining ones on CPU.</div><br/></div></div></div></div><div id="36305888" class="c"><input type="checkbox" id="c-36305888" checked=""/><div class="controls bullet"><span class="by">jokethrowaway</span><span>|</span><a href="#36304653">prev</a><span>|</span><a href="#36307411">next</a><span>|</span><label class="collapse" for="c-36305888">[-]</label><label class="expand" for="c-36305888">[1 more]</label></div><br/><div class="children"><div class="content">Great news but I&#x27;d like to know how does it compare with just using torchlib.<p>If this is faster than torchlib this optimizations should flow to torchlib as well<p>Love the idea of not having to deal with python, though; dependency management is just horrible, I&#x27;d much rather have ML projects written in cpp.</div><br/></div></div><div id="36304803" class="c"><input type="checkbox" id="c-36304803" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#36307411">prev</a><span>|</span><a href="#36304225">next</a><span>|</span><label class="collapse" for="c-36304803">[-]</label><label class="expand" for="c-36304803">[36 more]</label></div><br/><div class="children"><div class="content">Got downvoted out of view for saying this once but no less true. Absolutely a pity &amp; a shame no one else has competed with this market dominance by Nvidia. Just a shit world entirely that we are single vendored up.<p>A lot of pissant shitty defenses of monopolization too. Wrong or right, this is a shit world we&#x27;re in now. <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36304225">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36304225</a></div><br/><div id="36305036" class="c"><input type="checkbox" id="c-36305036" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305154">next</a><span>|</span><label class="collapse" for="c-36305036">[-]</label><label class="expand" for="c-36305036">[6 more]</label></div><br/><div class="children"><div class="content">Apple&#x27;s take on GPUs is quite different and very interesting IMO. Shared memory architecture with absolutely massive RAM (and hence VRAM) support, e.g. the new Mac Studio having 192GB RAM&#x2F;VRAM which can run pretty massive models, much more so than is easily consumer-accessible even at the high end with Nvidia 4090s. It&#x27;s not as fast as Nvidia, but it&#x27;s not horribly far off in the latest chips.<p>As LLM adoption grows, I wonder whether Apple&#x27;s approach will start to make more sense for consumer adoption, so that you can run models on your machine without needing to pay large subscription costs for AI-powered apps (since OpenAI et al have fairly high fees). The high cost of using the APIs in my opinion is a drag on certain types of adoption in consumer apps.<p>Llama.cpp actually first started as a way to run LLMs on Macs! At first CPU-only, but then later the first GPU driver backend added was Metal, not anything from Nvidia.</div><br/><div id="36305599" class="c"><input type="checkbox" id="c-36305599" checked=""/><div class="controls bullet"><span class="by">bottled_poe</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305036">parent</a><span>|</span><a href="#36305792">next</a><span>|</span><label class="collapse" for="c-36305599">[-]</label><label class="expand" for="c-36305599">[1 more]</label></div><br/><div class="children"><div class="content">I agree, though itâs still vendor lock-in. I think a lot of devs arenât aware of how tightly integrated these frameworks are with the hardware. Itâs not trivial to separate the two and the companies driving this tech have no motive to do so, while also being amongst a very small number of hardware designers.</div><br/></div></div><div id="36305792" class="c"><input type="checkbox" id="c-36305792" checked=""/><div class="controls bullet"><span class="by">anikom15</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305036">parent</a><span>|</span><a href="#36305599">prev</a><span>|</span><a href="#36305087">next</a><span>|</span><label class="collapse" for="c-36305792">[-]</label><label class="expand" for="c-36305792">[2 more]</label></div><br/><div class="children"><div class="content">I think criticizing Apple for not supporting Nvidia is a classic missing-the-forest-for-the-trees. Unifying memory is a next logical step in general purpose computing. The flexibility that comes from it has unseen potential.</div><br/><div id="36305837" class="c"><input type="checkbox" id="c-36305837" checked=""/><div class="controls bullet"><span class="by">ttflee</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305792">parent</a><span>|</span><a href="#36305087">next</a><span>|</span><label class="collapse" for="c-36305837">[-]</label><label class="expand" for="c-36305837">[1 more]</label></div><br/><div class="children"><div class="content">Thinking what Apple sells is a GPU bundled with a large amount of VRAM and a free ARM CPU makes things easier to accept.<p>NVIDIA never sold a GPU with expandable memory, either.</div><br/></div></div></div></div><div id="36305087" class="c"><input type="checkbox" id="c-36305087" checked=""/><div class="controls bullet"><span class="by">mliker</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305036">parent</a><span>|</span><a href="#36305792">prev</a><span>|</span><a href="#36305154">next</a><span>|</span><label class="collapse" for="c-36305087">[-]</label><label class="expand" for="c-36305087">[2 more]</label></div><br/><div class="children"><div class="content">In addition to the cost, OpenAI censors their models, and while that seems protective at first, if you think about it, if the model is the knowledge graph, then censor data in the graph is censoring free speech.</div><br/><div id="36305737" class="c"><input type="checkbox" id="c-36305737" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305087">parent</a><span>|</span><a href="#36305154">next</a><span>|</span><label class="collapse" for="c-36305737">[-]</label><label class="expand" for="c-36305737">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t personally like model censorship, but you need to come up with a better argument than...whatever that is. It is entirely unconvincing, and seems like you misunderstand what free speech is.</div><br/></div></div></div></div></div></div><div id="36305154" class="c"><input type="checkbox" id="c-36305154" checked=""/><div class="controls bullet"><span class="by">AeiumNE</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305036">prev</a><span>|</span><a href="#36304862">next</a><span>|</span><label class="collapse" for="c-36305154">[-]</label><label class="expand" for="c-36305154">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve said it before and I&#x27;ll say it again. Cuda dominance is the darkest timeline.<p>OpenCL was the utopia timeline.</div><br/><div id="36305625" class="c"><input type="checkbox" id="c-36305625" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305154">parent</a><span>|</span><a href="#36304862">next</a><span>|</span><label class="collapse" for="c-36305625">[-]</label><label class="expand" for="c-36305625">[1 more]</label></div><br/><div class="children"><div class="content">OpenCL still works however :)</div><br/></div></div></div></div><div id="36304862" class="c"><input type="checkbox" id="c-36304862" checked=""/><div class="controls bullet"><span class="by">asynchronous</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305154">prev</a><span>|</span><a href="#36305289">next</a><span>|</span><label class="collapse" for="c-36304862">[-]</label><label class="expand" for="c-36304862">[4 more]</label></div><br/><div class="children"><div class="content">AMD almost pulled it off but software made Nvidia take the lead again. AI being CUDA dependent really gave them an edge, in both the consumer and business market.</div><br/><div id="36305339" class="c"><input type="checkbox" id="c-36305339" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36304862">parent</a><span>|</span><a href="#36305277">next</a><span>|</span><label class="collapse" for="c-36305339">[-]</label><label class="expand" for="c-36305339">[2 more]</label></div><br/><div class="children"><div class="content">I have hope that AMD can close that gap over the next few years. Their hardware is already great, and the business case for investing in AI software is crazy strong. Their stock price would probably get a bump just from announcing the investment.<p>It seems like a no brainer to hire a bunch of people to work on making PyTorch &#x2F; Tensorflow on AMD become a competitive option. Itâll just take a few years.</div><br/><div id="36305399" class="c"><input type="checkbox" id="c-36305399" checked=""/><div class="controls bullet"><span class="by">raverbashing</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305339">parent</a><span>|</span><a href="#36305277">next</a><span>|</span><label class="collapse" for="c-36305399">[-]</label><label class="expand" for="c-36305399">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s been a couple of years already that we hoped better of AMD, to general disappointment</div><br/></div></div></div></div></div></div><div id="36305289" class="c"><input type="checkbox" id="c-36305289" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36304862">prev</a><span>|</span><a href="#36305940">next</a><span>|</span><label class="collapse" for="c-36305289">[-]</label><label class="expand" for="c-36305289">[1 more]</label></div><br/><div class="children"><div class="content">Overview of some of the efforts to move away from cuda: <a href="https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;nvidiaopenaitritonpytorch" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;nvidiaopenaitritonpytorch</a></div><br/></div></div><div id="36305940" class="c"><input type="checkbox" id="c-36305940" checked=""/><div class="controls bullet"><span class="by">earhart</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305289">prev</a><span>|</span><a href="#36305378">next</a><span>|</span><label class="collapse" for="c-36305940">[-]</label><label class="expand" for="c-36305940">[1 more]</label></div><br/><div class="children"><div class="content">IMHO, it comes down to the software.<p>It turns out you need very different kernels for good performance on different GPUs, so OpenCL is a nice tool, but not sufficient; you need a hardware-specific kernel library.<p>From the framework side, each integration is relatively expensive to support, so you really donât want to invest in many of them. Without some sort of kernel API standard, youâre into a proprietary solution, and NVidia did an amazing job at investing in their software, so thatâs the way things go.<p>I think we had a pretty solid foundation for doing something smarter with PlaidML, but after we were bought by Intel, some architectural decisions and some business decisions consigned that to be a research project; I donât know that itâs going anywhere.<p>These days, Iâd probably look into OctoML &#x2F; TVM, or maybe Modular, for a better solution in this spaceâ¦ or just buy NVidia.<p>(I worked a bit on Intelâs Meteor Lake VPU; itâs a lovely machine, but Iâm not sure what the story will be for general framework integrations. I bet OpenVINO will run really well on it, though :-)</div><br/></div></div><div id="36305378" class="c"><input type="checkbox" id="c-36305378" checked=""/><div class="controls bullet"><span class="by">numlock86</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305940">prev</a><span>|</span><a href="#36306312">next</a><span>|</span><label class="collapse" for="c-36305378">[-]</label><label class="expand" for="c-36305378">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Absolutely a pity &amp; a shame no one else has competed with this market dominance by Nvidia.<p>Well, at least you admit it&#x27;s not Nvidia&#x27;s fault. Apparently Apple, Intel and AMD don&#x27;t think there&#x27;s much money to grab here.</div><br/><div id="36305700" class="c"><input type="checkbox" id="c-36305700" checked=""/><div class="controls bullet"><span class="by">rapsey</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305378">parent</a><span>|</span><a href="#36306312">next</a><span>|</span><label class="collapse" for="c-36305700">[-]</label><label class="expand" for="c-36305700">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure Intel and AMD heavily regret their neglect of OpenCL since the rise of LLMs and stable diffusion.</div><br/></div></div></div></div><div id="36306312" class="c"><input type="checkbox" id="c-36306312" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305378">prev</a><span>|</span><a href="#36305202">next</a><span>|</span><label class="collapse" for="c-36306312">[-]</label><label class="expand" for="c-36306312">[1 more]</label></div><br/><div class="children"><div class="content">For me itâs doubly amazing that Intel does not exist in those discussions about alternatives that are already rare. They should write a book about how to blow up a successfully semiconductor company from the inside.</div><br/></div></div><div id="36305202" class="c"><input type="checkbox" id="c-36305202" checked=""/><div class="controls bullet"><span class="by">ygouzerh</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36306312">prev</a><span>|</span><a href="#36305777">next</a><span>|</span><label class="collapse" for="c-36305202">[-]</label><label class="expand" for="c-36305202">[1 more]</label></div><br/><div class="children"><div class="content">High technology hardware needs a lot of investment, and it&#x27;s hard to gather enough resources (human, capital, market share).<p>But AMD is coming strong, and they are trying to compete with Nvidia now. <a href="https:&#x2F;&#x2F;www.forbes.com&#x2F;sites&#x2F;iainmartin&#x2F;2023&#x2F;05&#x2F;31&#x2F;lisa-su-saved-amd-now-she-wants-nvidias-ai-crown&#x2F;?sh=598527cf1ec9" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.forbes.com&#x2F;sites&#x2F;iainmartin&#x2F;2023&#x2F;05&#x2F;31&#x2F;lisa-su-s...</a></div><br/></div></div><div id="36305777" class="c"><input type="checkbox" id="c-36305777" checked=""/><div class="controls bullet"><span class="by">wyldfire</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305202">prev</a><span>|</span><a href="#36305250">next</a><span>|</span><label class="collapse" for="c-36305777">[-]</label><label class="expand" for="c-36305777">[2 more]</label></div><br/><div class="children"><div class="content">Well, we could do a much much better job of it but in fact Qualcomm does compete with NVIDIA for use cases like this (inference).  Both in mobile devices and the data center.<p>Disclaimer: I work at Qualcomm.</div><br/><div id="36306844" class="c"><input type="checkbox" id="c-36306844" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305777">parent</a><span>|</span><a href="#36305250">next</a><span>|</span><label class="collapse" for="c-36306844">[-]</label><label class="expand" for="c-36306844">[1 more]</label></div><br/><div class="children"><div class="content">What is on offer optimized for running these LLMs?</div><br/></div></div></div></div><div id="36305250" class="c"><input type="checkbox" id="c-36305250" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305777">prev</a><span>|</span><a href="#36305060">next</a><span>|</span><label class="collapse" for="c-36305250">[-]</label><label class="expand" for="c-36305250">[1 more]</label></div><br/><div class="children"><div class="content">The others tried with OpenCL to build a more open environment, this will always lose against a single vendor tailoring it&#x27;s solution for their own lineup.<p>Think of Apples ecosystem vs Android or MS&#x27;s Office-Outlook-Teams vs anything else.</div><br/></div></div><div id="36305060" class="c"><input type="checkbox" id="c-36305060" checked=""/><div class="controls bullet"><span class="by">lachlan_gray</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305250">prev</a><span>|</span><a href="#36305184">next</a><span>|</span><label class="collapse" for="c-36305060">[-]</label><label class="expand" for="c-36305060">[1 more]</label></div><br/><div class="children"><div class="content">With any luck projects like MLC will help close the gap<p><a href="https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm">https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm</a></div><br/></div></div><div id="36305184" class="c"><input type="checkbox" id="c-36305184" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305060">prev</a><span>|</span><a href="#36305024">next</a><span>|</span><label class="collapse" for="c-36305184">[-]</label><label class="expand" for="c-36305184">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if it would be possible to emulate&#x2F;translate CUDA to target non-NVIDIA hardware.<p>I suspect it would be more of a legal challenge than a technical one.</div><br/><div id="36305757" class="c"><input type="checkbox" id="c-36305757" checked=""/><div class="controls bullet"><span class="by">mahkeiro</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305184">parent</a><span>|</span><a href="#36305024">next</a><span>|</span><label class="collapse" for="c-36305757">[-]</label><label class="expand" for="c-36305757">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what ROCm was trying to do</div><br/></div></div></div></div><div id="36305024" class="c"><input type="checkbox" id="c-36305024" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305184">prev</a><span>|</span><a href="#36305083">next</a><span>|</span><label class="collapse" for="c-36305024">[-]</label><label class="expand" for="c-36305024">[1 more]</label></div><br/><div class="children"><div class="content">You can also use TPUs or other training cards. Nvidia is just the best one that&#x27;s accessible.<p>But I think you&#x27;re getting downvoted since it&#x27;s very off-topic.</div><br/></div></div><div id="36305707" class="c"><input type="checkbox" id="c-36305707" checked=""/><div class="controls bullet"><span class="by">sharikous</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305083">prev</a><span>|</span><a href="#36305763">next</a><span>|</span><label class="collapse" for="c-36305707">[-]</label><label class="expand" for="c-36305707">[2 more]</label></div><br/><div class="children"><div class="content">WebGPU strikes me as the answer to that. Perhaps I am missing something?</div><br/><div id="36307132" class="c"><input type="checkbox" id="c-36307132" checked=""/><div class="controls bullet"><span class="by">kayvr</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36305707">parent</a><span>|</span><a href="#36305763">next</a><span>|</span><label class="collapse" for="c-36307132">[-]</label><label class="expand" for="c-36307132">[1 more]</label></div><br/><div class="children"><div class="content">It could be. But there&#x27;s quite a bit of momentum behind CUDA. Plus, CUDA is just wicked fast. I wrote a WebGPU version of LLaMA inference and there&#x27;s still a bit of a gap in performance between WebGPU and CUDA. Admittedly, WebGPU can&#x27;t access tensor cores and I undoubtedly need to optimize further.</div><br/></div></div></div></div><div id="36305763" class="c"><input type="checkbox" id="c-36305763" checked=""/><div class="controls bullet"><span class="by">jeron</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305707">prev</a><span>|</span><a href="#36305324">next</a><span>|</span><label class="collapse" for="c-36305763">[-]</label><label class="expand" for="c-36305763">[1 more]</label></div><br/><div class="children"><div class="content">looking forward to AMD MI300, hopefully will be a game changer</div><br/></div></div><div id="36305324" class="c"><input type="checkbox" id="c-36305324" checked=""/><div class="controls bullet"><span class="by">eur0pa</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305763">prev</a><span>|</span><a href="#36304821">next</a><span>|</span><label class="collapse" for="c-36305324">[-]</label><label class="expand" for="c-36305324">[1 more]</label></div><br/><div class="children"><div class="content">There are far more important things to be this worried about</div><br/></div></div><div id="36304821" class="c"><input type="checkbox" id="c-36304821" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305324">prev</a><span>|</span><a href="#36305677">next</a><span>|</span><label class="collapse" for="c-36304821">[-]</label><label class="expand" for="c-36304821">[3 more]</label></div><br/><div class="children"><div class="content">Can hardware vendors even put their differences aside to build such a thing? We can&#x27;t even build a unified open raster graphics API, and now you&#x27;re asking for <i>machine learning</i> acceleration in that vein?</div><br/><div id="36304858" class="c"><input type="checkbox" id="c-36304858" checked=""/><div class="controls bullet"><span class="by">OkayPhysicist</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36304821">parent</a><span>|</span><a href="#36305677">next</a><span>|</span><label class="collapse" for="c-36304858">[-]</label><label class="expand" for="c-36304858">[2 more]</label></div><br/><div class="children"><div class="content">Machine learning would probably be the simpler API. If you can speak Linear Algebra, you&#x27;re most of the way there.</div><br/><div id="36304912" class="c"><input type="checkbox" id="c-36304912" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36304803">root</a><span>|</span><a href="#36304858">parent</a><span>|</span><a href="#36305677">next</a><span>|</span><label class="collapse" for="c-36304912">[-]</label><label class="expand" for="c-36304912">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, and it&#x27;s why projects like the ONNX runtime exist to unify vendor-specific AI accelerators. Covering the basics isn&#x27;t <i>too</i> hard.<p>What GP seems to be asking for is an open CUDA replacement, which is kinda like asking someone to fund a Free and Open Source cruise ship to compete with Carnival for you. You&#x27;ll get somewhere with some effort, luck and good old human intuition, but Nvidia can outspend you 10:1 unless you have funding leverage from FAANG.</div><br/></div></div></div></div></div></div><div id="36305677" class="c"><input type="checkbox" id="c-36305677" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36304821">prev</a><span>|</span><a href="#36304913">next</a><span>|</span><label class="collapse" for="c-36305677">[-]</label><label class="expand" for="c-36305677">[1 more]</label></div><br/><div class="children"><div class="content">tinygrad is trying to address this problem. We&#x27;ll see if it&#x27;s successful.</div><br/></div></div><div id="36304913" class="c"><input type="checkbox" id="c-36304913" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#36304803">parent</a><span>|</span><a href="#36305677">prev</a><span>|</span><a href="#36304225">next</a><span>|</span><label class="collapse" for="c-36304913">[-]</label><label class="expand" for="c-36304913">[1 more]</label></div><br/><div class="children"><div class="content">itâs uhâ¦ not that serious</div><br/></div></div></div></div><div id="36304225" class="c"><input type="checkbox" id="c-36304225" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#36304803">prev</a><span>|</span><label class="collapse" for="c-36304225">[-]</label><label class="expand" for="c-36304225">[16 more]</label></div><br/><div class="children"><div class="content">Such a pity no one else can compete here presently. Would that others be able to gain a position where their software made them competitive on the free market.</div><br/><div id="36304302" class="c"><input type="checkbox" id="c-36304302" checked=""/><div class="controls bullet"><span class="by">theaiquestion</span><span>|</span><a href="#36304225">parent</a><span>|</span><a href="#36304782">next</a><span>|</span><label class="collapse" for="c-36304302">[-]</label><label class="expand" for="c-36304302">[3 more]</label></div><br/><div class="children"><div class="content">Compete with Llama.cpp? Like transformers llama [0], exllama [1] (really fast), or litllama [2] ?<p>exllama is really memory efficient and really fast<p>[0] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;main&#x2F;model_doc&#x2F;llama" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;main&#x2F;model_doc&#x2F;llam...</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllama">https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllama</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;Lightning-AI&#x2F;lit-llama">https:&#x2F;&#x2F;github.com&#x2F;Lightning-AI&#x2F;lit-llama</a><p>EDIT: Or do you mean cuda? Because yeah, it&#x27;s such a shame AMD&#x27;s Rocm is so bad even geohot gave up. it&#x27;s examples don&#x27;t even run without crashing.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;RadeonOpenCompute&#x2F;ROCm&#x2F;issues&#x2F;2198#issuecomment-1574383483">https:&#x2F;&#x2F;github.com&#x2F;RadeonOpenCompute&#x2F;ROCm&#x2F;issues&#x2F;2198#issuec...</a></div><br/><div id="36307030" class="c"><input type="checkbox" id="c-36307030" checked=""/><div class="controls bullet"><span class="by">kayvr</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304302">parent</a><span>|</span><a href="#36304518">next</a><span>|</span><label class="collapse" for="c-36307030">[-]</label><label class="expand" for="c-36307030">[1 more]</label></div><br/><div class="children"><div class="content">Also <a href="https:&#x2F;&#x2F;github.com&#x2F;kayvr&#x2F;TokenHawk">https:&#x2F;&#x2F;github.com&#x2F;kayvr&#x2F;TokenHawk</a>, a WebGPU implementation of LLaMA.<p>edit: Note that this is my project.</div><br/></div></div><div id="36304518" class="c"><input type="checkbox" id="c-36304518" checked=""/><div class="controls bullet"><span class="by">dTal</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304302">parent</a><span>|</span><a href="#36307030">prev</a><span>|</span><a href="#36304782">next</a><span>|</span><label class="collapse" for="c-36304518">[-]</label><label class="expand" for="c-36304518">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the tip about exllama, I&#x27;ve been on the lookout for a readable python implementation to play with that is also fast and has support for quantized datasets.</div><br/></div></div></div></div><div id="36304782" class="c"><input type="checkbox" id="c-36304782" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#36304225">parent</a><span>|</span><a href="#36304302">prev</a><span>|</span><a href="#36304306">next</a><span>|</span><label class="collapse" for="c-36304782">[-]</label><label class="expand" for="c-36304782">[1 more]</label></div><br/><div class="children"><div class="content">Unclear what this is referring to, but if it means CUDA vs other things it is worth noting that:<p>a) CUDA won in a free market because NVidia showed they cared about it<p>b) Llama has support for OpenCL (via CLBlast) and Apple Metal<p>The OpenCL support already has a custom kernel for token generation.</div><br/></div></div><div id="36304306" class="c"><input type="checkbox" id="c-36304306" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36304225">parent</a><span>|</span><a href="#36304782">prev</a><span>|</span><a href="#36304745">next</a><span>|</span><label class="collapse" for="c-36304306">[-]</label><label class="expand" for="c-36304306">[9 more]</label></div><br/><div class="children"><div class="content">There <i>was</i> free competition here, a while ago. OpenCL was formed by Apple, Khronos et al. to stave off CUDA&#x27;s dominance. The platform languished from a lack of commitment though, and Apple eventually gave up on open GPU APIs entirely. Nvidia continued funding CUDA and scaling it for industry application, and the rest is history. The landscape of stakeholders is just too bitter to unseat CUDA for what it&#x27;s used for - your best shot at democratizing AI inferencing acceleration is through something like Microsoft&#x27;s ONNX[0] runtime.<p>[0] <a href="https:&#x2F;&#x2F;onnxruntime.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;onnxruntime.ai&#x2F;</a></div><br/><div id="36305364" class="c"><input type="checkbox" id="c-36305364" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304306">parent</a><span>|</span><a href="#36304786">next</a><span>|</span><label class="collapse" for="c-36305364">[-]</label><label class="expand" for="c-36305364">[1 more]</label></div><br/><div class="children"><div class="content">CUDA had a lot of inertia and opencl brought half baked docs and half baked support out of the gate. If they had focused on simplifying their api to be more user friendly for the 80% use case it could&#x27;ve been a success. Opencl always looked nice on the surface but a few hours in and you&#x27;ve exhausted the docs trying to figure out what to do and there&#x27;s no good example code around. Of course if they really wanted it to succeed they would&#x27;ve built a Cuda to opencl transpiler for the c api or at least a comprehensive migration guide. I&#x27;m not convinced anyone involved was trying to make it popular.</div><br/></div></div><div id="36304786" class="c"><input type="checkbox" id="c-36304786" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304306">parent</a><span>|</span><a href="#36305364">prev</a><span>|</span><a href="#36304503">next</a><span>|</span><label class="collapse" for="c-36304786">[-]</label><label class="expand" for="c-36304786">[1 more]</label></div><br/><div class="children"><div class="content">Note that Llama supports acceleration on both OpenCL and Apple Metal</div><br/></div></div><div id="36304503" class="c"><input type="checkbox" id="c-36304503" checked=""/><div class="controls bullet"><span class="by">chrischen</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304306">parent</a><span>|</span><a href="#36304786">prev</a><span>|</span><a href="#36305108">next</a><span>|</span><label class="collapse" for="c-36304503">[-]</label><label class="expand" for="c-36304503">[5 more]</label></div><br/><div class="children"><div class="content">Thereâs also geohotâs tiny corp betting on AMD gpus.</div><br/><div id="36304514" class="c"><input type="checkbox" id="c-36304514" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304503">parent</a><span>|</span><a href="#36304630">next</a><span>|</span><label class="collapse" for="c-36304514">[-]</label><label class="expand" for="c-36304514">[3 more]</label></div><br/><div class="children"><div class="content">Not any more.</div><br/><div id="36304609" class="c"><input type="checkbox" id="c-36304609" checked=""/><div class="controls bullet"><span class="by">nromiun</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304514">parent</a><span>|</span><a href="#36304655">next</a><span>|</span><label class="collapse" for="c-36304609">[-]</label><label class="expand" for="c-36304609">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;geohot.github.io&#x2F;blog&#x2F;jekyll&#x2F;update&#x2F;2023&#x2F;06&#x2F;07&#x2F;a-dive-into-amds-drivers.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;geohot.github.io&#x2F;blog&#x2F;jekyll&#x2F;update&#x2F;2023&#x2F;06&#x2F;07&#x2F;a-div...</a><p>AMD gave him a binary blob driver and that fixed his problem. Also, tinygrad is the only Python framework I know that has full OpenCL acceleration.</div><br/></div></div><div id="36304655" class="c"><input type="checkbox" id="c-36304655" checked=""/><div class="controls bullet"><span class="by">vvladymyrov</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304514">parent</a><span>|</span><a href="#36304609">prev</a><span>|</span><a href="#36304630">next</a><span>|</span><label class="collapse" for="c-36304655">[-]</label><label class="expand" for="c-36304655">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean? At least as of June 7 geohot was still working on amd drivers builds and stability.
<a href="https:&#x2F;&#x2F;geohot.github.io&#x2F;blog&#x2F;jekyll&#x2F;update&#x2F;2023&#x2F;06&#x2F;07&#x2F;a-dive-into-amds-drivers.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;geohot.github.io&#x2F;blog&#x2F;jekyll&#x2F;update&#x2F;2023&#x2F;06&#x2F;07&#x2F;a-div...</a><p>So far it doesnât look that AMD is fully on board with Tiny Corp, but they are talkingâ¦</div><br/></div></div></div></div></div></div><div id="36305108" class="c"><input type="checkbox" id="c-36305108" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#36304225">root</a><span>|</span><a href="#36304306">parent</a><span>|</span><a href="#36304503">prev</a><span>|</span><a href="#36304745">next</a><span>|</span><label class="collapse" for="c-36305108">[-]</label><label class="expand" for="c-36305108">[1 more]</label></div><br/><div class="children"><div class="content">Why not ggml?</div><br/></div></div></div></div><div id="36304745" class="c"><input type="checkbox" id="c-36304745" checked=""/><div class="controls bullet"><span class="by">angch</span><span>|</span><a href="#36304225">parent</a><span>|</span><a href="#36304306">prev</a><span>|</span><a href="#36304510">next</a><span>|</span><label class="collapse" for="c-36304745">[-]</label><label class="expand" for="c-36304745">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s Fabrice Bellard&#x27;s textsynth server. <a href="https:&#x2F;&#x2F;bellard.org&#x2F;ts_server&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;bellard.org&#x2F;ts_server&#x2F;</a><p>No open source though.</div><br/></div></div><div id="36304510" class="c"><input type="checkbox" id="c-36304510" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#36304225">parent</a><span>|</span><a href="#36304745">prev</a><span>|</span><label class="collapse" for="c-36304510">[-]</label><label class="expand" for="c-36304510">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t a market.</div><br/></div></div></div></div></div></div></div></div></div></body></html>