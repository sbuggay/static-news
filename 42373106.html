<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733821280183" as="style"/><link rel="stylesheet" href="styles.css?v=1733821280183"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://2ro.co/post/769440256241123328/study-opencyc">Is anyone playing with the combination of generative AI and OpenCyc?</a> <span class="domain">(<a href="https://2ro.co">2ro.co</a>)</span></div><div class="subtext"><span>2ro</span> | <span>39 comments</span></div><br/><div><div id="42374545" class="c"><input type="checkbox" id="c-42374545" checked=""/><div class="controls bullet"><span class="by">mindcrime</span><span>|</span><a href="#42374773">next</a><span>|</span><label class="collapse" for="c-42374545">[-]</label><label class="expand" for="c-42374545">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Is anyone playing with the combination of generative AI and OpenCyc?<p>OpenCyc specifically? No. But related semantic KB&#x27;s using the SemanticWeb &#x2F; RDF stack? Yes. That&#x27;s something I&#x27;m spending a fair bit of time on right now.<p>And given that there is an RDF encoding of a portion fof the OpenCyc data out there[1], and that may make it into some of my experiments eventually, I guess the answer is more like &quot;Not exactly, but sort of, maybe&#x27;ish&quot; or something.<p>[1]: <a href="https:&#x2F;&#x2F;sourceforge.net&#x2F;projects&#x2F;texai&#x2F;files&#x2F;open-cyc-rdf&#x2F;" rel="nofollow">https:&#x2F;&#x2F;sourceforge.net&#x2F;projects&#x2F;texai&#x2F;files&#x2F;open-cyc-rdf&#x2F;</a></div><br/><div id="42375030" class="c"><input type="checkbox" id="c-42375030" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#42374545">parent</a><span>|</span><a href="#42374773">next</a><span>|</span><label class="collapse" for="c-42375030">[-]</label><label class="expand" for="c-42375030">[1 more]</label></div><br/><div class="children"><div class="content">Microsoft published quite a bit on the notion of graph RAG which is about enhancing regular RAG (retrieval augmented search) with graph databases. The idea is that instead of just pulling semantically related information to the query, it also pulls in information about things connected to those things. This gives the LLM more contextual information to work with.<p>Sounds like it would probably have. If you combine that with entity recognition and automated graph construction from unstructured data, you might get something that is vaguely useful.</div><br/></div></div></div></div><div id="42374773" class="c"><input type="checkbox" id="c-42374773" checked=""/><div class="controls bullet"><span class="by">pcblues</span><span>|</span><a href="#42374545">prev</a><span>|</span><a href="#42375046">next</a><span>|</span><label class="collapse" for="c-42374773">[-]</label><label class="expand" for="c-42374773">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll be the antiquated person here. Writing&#x2F;speaking well has brought people to knowledge because the authors are uniquely positioned to use genius, humour, gentleness and generosity to bring an inquisitive but ignorant person into a new area of knowledge. When the value of that can be quantified, then we can compare AI &quot;generation&quot; of &quot;efficiently written, useful knowledge&quot; with what we had&#x2F;have.<p>Same for fiction, visual art, raising children, caring for old people, and so on, and so on.</div><br/><div id="42375055" class="c"><input type="checkbox" id="c-42375055" checked=""/><div class="controls bullet"><span class="by">willvarfar</span><span>|</span><a href="#42374773">parent</a><span>|</span><a href="#42375046">next</a><span>|</span><label class="collapse" for="c-42375055">[-]</label><label class="expand" for="c-42375055">[1 more]</label></div><br/><div class="children"><div class="content">With big enough cohorts we can AB test by quantifying outcomes?  Treatment group A has AI-generated texts and Treatment group B has the originals.  We can have a questionnaire for how the group felt about the material etc, but we can also perhaps measure life outcomes over a bigger period e.g. performance at school or in the market place?<p>As I write this it feels naive and reminds me of a thousand ill-thought-out AB website tests etc.  But still :D</div><br/></div></div></div></div><div id="42375046" class="c"><input type="checkbox" id="c-42375046" checked=""/><div class="controls bullet"><span class="by">dannyobrien</span><span>|</span><a href="#42374773">prev</a><span>|</span><a href="#42374526">next</a><span>|</span><label class="collapse" for="c-42375046">[-]</label><label class="expand" for="c-42375046">[1 more]</label></div><br/><div class="children"><div class="content">Unsure specifically, but there&#x27;s a long-standing movement to combine GOFAI symbolic approaches, and modern neurally-influence systems. <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neuro-symbolic_AI" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neuro-symbolic_AI</a></div><br/></div></div><div id="42374526" class="c"><input type="checkbox" id="c-42374526" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#42375046">prev</a><span>|</span><a href="#42374909">next</a><span>|</span><label class="collapse" for="c-42374526">[-]</label><label class="expand" for="c-42374526">[4 more]</label></div><br/><div class="children"><div class="content">I think it would be interesting to use an LLM to distill Wikipedia into a set of assertions, then iterate through combinations of those assertions using OpenCyc.<p>You could look for contradictions between pages on the same subject in different languages, or different pages on related subjects.<p>You could synthesise new assertions based on what the current assertions imply, then render it to a sentence and fact-check it.<p>You could use verified assertions to benchmark language parsing and comprehension for new models. Basically unit test NLP.<p>You could produce a list of new assertions and implications introduced when a new edit to a page is made.</div><br/><div id="42374568" class="c"><input type="checkbox" id="c-42374568" checked=""/><div class="controls bullet"><span class="by">mindcrime</span><span>|</span><a href="#42374526">parent</a><span>|</span><a href="#42374900">next</a><span>|</span><label class="collapse" for="c-42374568">[-]</label><label class="expand" for="c-42374568">[2 more]</label></div><br/><div class="children"><div class="content">Along with that, a portion of the content of Wikipedia is already available in structured assertion form, thanks to DBPedia[1] and Wikidata[2]. I don&#x27;t know the exact percentage, but it&#x27;s a starting point at the very least.<p>[1]: <a href="https:&#x2F;&#x2F;www.dbpedia.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.dbpedia.org&#x2F;</a><p>[2]: <a href="https:&#x2F;&#x2F;www.wikidata.org&#x2F;wiki&#x2F;Wikidata:Main_Page" rel="nofollow">https:&#x2F;&#x2F;www.wikidata.org&#x2F;wiki&#x2F;Wikidata:Main_Page</a></div><br/><div id="42374719" class="c"><input type="checkbox" id="c-42374719" checked=""/><div class="controls bullet"><span class="by">throw310822</span><span>|</span><a href="#42374526">root</a><span>|</span><a href="#42374568">parent</a><span>|</span><a href="#42374900">next</a><span>|</span><label class="collapse" for="c-42374719">[-]</label><label class="expand" for="c-42374719">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve tried to use ChatGPT to produce wikidata queries- it sounds like a great combination. Unfortunately it&#x27;s pretty hard to make it produce valid queries, and to find the wikidata documentation to teach it.</div><br/></div></div></div></div><div id="42374900" class="c"><input type="checkbox" id="c-42374900" checked=""/><div class="controls bullet"><span class="by">wordpad25</span><span>|</span><a href="#42374526">parent</a><span>|</span><a href="#42374568">prev</a><span>|</span><a href="#42374909">next</a><span>|</span><label class="collapse" for="c-42374900">[-]</label><label class="expand" for="c-42374900">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why you need OpenCyc for this, you could just chain LLM for both.<p>I think it would outperform on any language task as language that isn&#x27;t formal requires interpretation which LLMs excel at.</div><br/></div></div></div></div><div id="42374909" class="c"><input type="checkbox" id="c-42374909" checked=""/><div class="controls bullet"><span class="by">ornornor</span><span>|</span><a href="#42374526">prev</a><span>|</span><a href="#42373881">next</a><span>|</span><label class="collapse" for="c-42374909">[-]</label><label class="expand" for="c-42374909">[3 more]</label></div><br/><div class="children"><div class="content">At the risk of sounding dumb: I don’t understand what are the practical applications of OpenCyc. I get LLMs, you can ask questions and they’ll answer, they can write an article, they can summarize documents… what are the practical applications of OpenCyc?</div><br/><div id="42375022" class="c"><input type="checkbox" id="c-42375022" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#42374909">parent</a><span>|</span><a href="#42373881">next</a><span>|</span><label class="collapse" for="c-42375022">[-]</label><label class="expand" for="c-42375022">[2 more]</label></div><br/><div class="children"><div class="content">OpenCyc is the remaining evolution of Cyc, which was based on the idea that symbolic AI (knowledge graphs&#x2F;Semanic Networks) would lead to AGI through scaling the knowledge base. You formalize the world so you can use logic to reason about it.<p>Later approaches of the same idea coning out of the academic Databases community reinvented this particular wheel with far better PR and branded it the Semantic Web with &#x27;Ontologies&#x27; and RDF, OWL and its ilk.<p>While reasonable (pun intended) for small vertical domains, the approach has never made inroads in more broad general intelligence as IMHO it does not deal well with ambiguities, contradictions, multi level or perspective modeling and circular referential meaning in &#x27;real world&#x27; reasoning  and also tends to ignore agentive, transformative and temporal situatedness.<p>Its ideal seems to be a single thruth never changing model of the universe that is simply accepted by all.</div><br/><div id="42375087" class="c"><input type="checkbox" id="c-42375087" checked=""/><div class="controls bullet"><span class="by">2ro</span><span>|</span><a href="#42374909">root</a><span>|</span><a href="#42375022">parent</a><span>|</span><a href="#42373881">next</a><span>|</span><label class="collapse" for="c-42375087">[-]</label><label class="expand" for="c-42375087">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div></div></div></div></div><div id="42373881" class="c"><input type="checkbox" id="c-42373881" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#42374909">prev</a><span>|</span><a href="#42374988">next</a><span>|</span><label class="collapse" for="c-42373881">[-]</label><label class="expand" for="c-42373881">[20 more]</label></div><br/><div class="children"><div class="content">Haven&#x27;t LLMs simply obsoleted OpenCyc? What could introducing OpenCyc add to LLMs, and why wouldn&#x27;t allowing the LLM to look up Wikipedia articles accomplish the same thing?</div><br/><div id="42374092" class="c"><input type="checkbox" id="c-42374092" checked=""/><div class="controls bullet"><span class="by">cookiengineer</span><span>|</span><a href="#42373881">parent</a><span>|</span><a href="#42373929">next</a><span>|</span><label class="collapse" for="c-42374092">[-]</label><label class="expand" for="c-42374092">[3 more]</label></div><br/><div class="children"><div class="content">LLMs have just ignored the fundamental problem of reasoning: symbolic inference. They haven&#x27;t &quot;solved&quot; it, they just don&#x27;t give a damn about logical correctness.</div><br/><div id="42374926" class="c"><input type="checkbox" id="c-42374926" checked=""/><div class="controls bullet"><span class="by">wordpad25</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374092">parent</a><span>|</span><a href="#42373929">next</a><span>|</span><label class="collapse" for="c-42374926">[-]</label><label class="expand" for="c-42374926">[2 more]</label></div><br/><div class="children"><div class="content">logical correctness as in formal logic is a huge step down<p>LLMs understand context and meaning and genuine intention</div><br/><div id="42375040" class="c"><input type="checkbox" id="c-42375040" checked=""/><div class="controls bullet"><span class="by">2ro</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374926">parent</a><span>|</span><a href="#42373929">next</a><span>|</span><label class="collapse" for="c-42375040">[-]</label><label class="expand" for="c-42375040">[1 more]</label></div><br/><div class="children"><div class="content">Cyc apparently addresses this issue with what are termed &quot;microtheories&quot; - in one theory something can be so, and in a different theory it can be not so:<p><a href="https:&#x2F;&#x2F;cyc.com&#x2F;archives&#x2F;glossary&#x2F;microtheory&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cyc.com&#x2F;archives&#x2F;glossary&#x2F;microtheory&#x2F;</a></div><br/></div></div></div></div></div></div><div id="42373929" class="c"><input type="checkbox" id="c-42373929" checked=""/><div class="controls bullet"><span class="by">drdaeman</span><span>|</span><a href="#42373881">parent</a><span>|</span><a href="#42374092">prev</a><span>|</span><a href="#42374596">next</a><span>|</span><label class="collapse" for="c-42373929">[-]</label><label class="expand" for="c-42373929">[14 more]</label></div><br/><div class="children"><div class="content">I’m not familiar with Cyc&#x2F;OpenCyc, but it seems that it’s not just a knowledge base, but also does inference and reasoning - while LLMs don’t reason and will happily produce completely illogical statements.</div><br/><div id="42374744" class="c"><input type="checkbox" id="c-42374744" checked=""/><div class="controls bullet"><span class="by">rcxdude</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42373929">parent</a><span>|</span><a href="#42374341">next</a><span>|</span><label class="collapse" for="c-42374744">[-]</label><label class="expand" for="c-42374744">[1 more]</label></div><br/><div class="children"><div class="content">Such systems tend to be equally good at producing nonsense: mainly because it&#x27;s really hard to make a consistent set of &#x27;facts&#x27;, and once you have inconsistencies, creative enough logic can produce nonsense in any part of the system.</div><br/></div></div><div id="42374341" class="c"><input type="checkbox" id="c-42374341" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42373929">parent</a><span>|</span><a href="#42374744">prev</a><span>|</span><a href="#42374596">next</a><span>|</span><label class="collapse" for="c-42374341">[-]</label><label class="expand" for="c-42374341">[12 more]</label></div><br/><div class="children"><div class="content">Can you please give an example of a “completely illogical statement” produced by o1 model? I suspect it would be easier to get an average human to produce an illogical statement.</div><br/><div id="42374430" class="c"><input type="checkbox" id="c-42374430" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374341">parent</a><span>|</span><a href="#42374447">next</a><span>|</span><label class="collapse" for="c-42374430">[-]</label><label class="expand" for="c-42374430">[6 more]</label></div><br/><div class="children"><div class="content">Give it anything that sounds like a riddle, but isn&#x27;t. Just one example:<p>&gt; H: The surgeon, who is the boy&#x27;s father, says &quot;I can&#x27;t operate on this boy, he&#x27;s my son!&quot; Who is the surgeon of the boy?<p>&gt; O1: The surgeon is the boy’s mother.<p>Also, just because humans don&#x27;t always think rationally doesn&#x27;t mean ChatGPT does.</div><br/><div id="42374910" class="c"><input type="checkbox" id="c-42374910" checked=""/><div class="controls bullet"><span class="by">seymore_12</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374430">parent</a><span>|</span><a href="#42374686">next</a><span>|</span><label class="collapse" for="c-42374910">[-]</label><label class="expand" for="c-42374910">[1 more]</label></div><br/><div class="children"><div class="content">Grok gives this as an excuse for answering &quot;The surgeon is the boy&#x27;s mother.&quot; :<p>&lt;&lt;Because the surgeon, who is the boy&#x27;s father, says, &quot;I can&#x27;t operate on this boy, he&#x27;s my son!&quot; This indicates that there is another parent involved who is also a surgeon. Given that the statement specifies the boy&#x27;s father cannot operate, the other surgeon must be the boy&#x27;s mother.&gt;&gt;
Sounds plausible and on the first read, almost logical.</div><br/></div></div><div id="42374686" class="c"><input type="checkbox" id="c-42374686" checked=""/><div class="controls bullet"><span class="by">throw310822</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374430">parent</a><span>|</span><a href="#42374910">prev</a><span>|</span><a href="#42374463">next</a><span>|</span><label class="collapse" for="c-42374686">[-]</label><label class="expand" for="c-42374686">[3 more]</label></div><br/><div class="children"><div class="content">Ha, good one! Claude gets it wrong too, except for apologizing and correcting itself when questioned:<p>&quot;I was trying to find a clever twist that isn&#x27;t actually there. The riddle appears to just be a straightforward statement - a father who is a surgeon saying he can&#x27;t operate on his son&quot;<p>More than being illogical, it seems that LLMs can be too hasty and too easily attracted by known patterns. People do the same.</div><br/><div id="42374833" class="c"><input type="checkbox" id="c-42374833" checked=""/><div class="controls bullet"><span class="by">varjag</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374686">parent</a><span>|</span><a href="#42374463">next</a><span>|</span><label class="collapse" for="c-42374833">[-]</label><label class="expand" for="c-42374833">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s amazing how great these canned apologies work at anthropomorphising LLMs. It wasn&#x27;t really in haste, it simply failed because the nuance fell below noise in its training set data but you rectified it with your follow-up correction.</div><br/><div id="42374916" class="c"><input type="checkbox" id="c-42374916" checked=""/><div class="controls bullet"><span class="by">throw310822</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374833">parent</a><span>|</span><a href="#42374463">next</a><span>|</span><label class="collapse" for="c-42374916">[-]</label><label class="expand" for="c-42374916">[1 more]</label></div><br/><div class="children"><div class="content">Well, first of all it failed twice: first it spat out the canned riddle answer, then once I asked it to &quot;double check&quot; it said &quot;sorry, I was wrong: the surgeon IS the boy&#x27;s father, so there must be a second surgeon...&quot;<p>Then  the follow up correction did have the effect of making it look harder at the question. It actually wrote:<p>&quot;Let me look at EXACTLY what&#x27;s given&quot; (with the all caps).<p>It&#x27;s not very different from a person that decides to focus harder on a problem once it was fooled by it a couple of times already because it is trickier than it seems. So yes, surprisingly human, with all its flaws.</div><br/></div></div></div></div></div></div><div id="42374463" class="c"><input type="checkbox" id="c-42374463" checked=""/><div class="controls bullet"><span class="by">pezezin</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374430">parent</a><span>|</span><a href="#42374686">prev</a><span>|</span><a href="#42374447">next</a><span>|</span><label class="collapse" for="c-42374463">[-]</label><label class="expand" for="c-42374463">[1 more]</label></div><br/><div class="children"><div class="content">Haha, you are right, I just asked Copilot, and it replied this:<p>&gt; This is a classic riddle! The surgeon is actually the boy&#x27;s mother. The riddle plays on the assumption that a surgeon is typically male, but in this case, the surgeon is the boy&#x27;s mother.<p>&gt; Did you enjoy this riddle? Do you have any more you&#x27;d like to share or solve?</div><br/></div></div></div></div><div id="42374447" class="c"><input type="checkbox" id="c-42374447" checked=""/><div class="controls bullet"><span class="by">wanderer2323</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374341">parent</a><span>|</span><a href="#42374430">prev</a><span>|</span><a href="#42374596">next</a><span>|</span><label class="collapse" for="c-42374447">[-]</label><label class="expand" for="c-42374447">[5 more]</label></div><br/><div class="children"><div class="content">Easy, from my recent chat with o1:
(Asked about left null space)<p>‘’’
 these are the vectors that when viewed as linear functionals, annihilate every column of  A . &lt;…&gt; Another way to view it: these are the vectors orthogonal to the row space.
‘’’<p>It’s quite obvious that vectors that “annihilate the columns” would be orthogonal to the column space not the row space.<p>I don’t know if you think o1 is magic. It still hallucinates, just less often and less obvious.</div><br/><div id="42374454" class="c"><input type="checkbox" id="c-42374454" checked=""/><div class="controls bullet"><span class="by">brokensegue</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374447">parent</a><span>|</span><a href="#42374596">next</a><span>|</span><label class="collapse" for="c-42374454">[-]</label><label class="expand" for="c-42374454">[4 more]</label></div><br/><div class="children"><div class="content">average humans don&#x27;t know what &quot;column spaces&quot; are or what &quot;orthogonal&quot; means</div><br/><div id="42374529" class="c"><input type="checkbox" id="c-42374529" checked=""/><div class="controls bullet"><span class="by">Sabinus</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374454">parent</a><span>|</span><a href="#42374625">next</a><span>|</span><label class="collapse" for="c-42374529">[-]</label><label class="expand" for="c-42374529">[2 more]</label></div><br/><div class="children"><div class="content">Average humans don&#x27;t (usually) confidently give you answers to questions they do now know the meaning of. Nor would you ask them.</div><br/><div id="42374695" class="c"><input type="checkbox" id="c-42374695" checked=""/><div class="controls bullet"><span class="by">throw310822</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374529">parent</a><span>|</span><a href="#42374625">next</a><span>|</span><label class="collapse" for="c-42374695">[-]</label><label class="expand" for="c-42374695">[1 more]</label></div><br/><div class="children"><div class="content">Ah hum. The discriminant is whether they know that they don&#x27;t know. If they don&#x27;t, they will happily spit out whatever comes to their mind.</div><br/></div></div></div></div><div id="42374625" class="c"><input type="checkbox" id="c-42374625" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42373881">root</a><span>|</span><a href="#42374454">parent</a><span>|</span><a href="#42374529">prev</a><span>|</span><a href="#42374596">next</a><span>|</span><label class="collapse" for="c-42374625">[-]</label><label class="expand" for="c-42374625">[1 more]</label></div><br/><div class="children"><div class="content">And why would the &quot;average human&quot; count?!<p>&quot;Support, the calculator gave a bad result for 345987*14569&quot; &#x2F;&#x2F; &quot;Yes, well, also your average human would&quot;<p>...That why we do not ask &quot;average humans&quot;!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42374596" class="c"><input type="checkbox" id="c-42374596" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#42373881">parent</a><span>|</span><a href="#42373929">prev</a><span>|</span><a href="#42374694">next</a><span>|</span><label class="collapse" for="c-42374596">[-]</label><label class="expand" for="c-42374596">[1 more]</label></div><br/><div class="children"><div class="content">No, they are completely orthogonal.<p>LLM are likelyhood completers, and classifiers. OpenCYC brings some logic and rationale into the classifiers. Without rationale LLM will continue hallucinating, spitting out nonsense.</div><br/></div></div><div id="42374694" class="c"><input type="checkbox" id="c-42374694" checked=""/><div class="controls bullet"><span class="by">tpm</span><span>|</span><a href="#42373881">parent</a><span>|</span><a href="#42374596">prev</a><span>|</span><a href="#42374988">next</a><span>|</span><label class="collapse" for="c-42374694">[-]</label><label class="expand" for="c-42374694">[1 more]</label></div><br/><div class="children"><div class="content">LLMs don&#x27;t know what is true (they have no way of knowing that), but they can babble about any topic. OpenCyc contains &#x27;truth&#x27;. If they can be meaningfully combined, it could be good.<p>It&#x27;s the same as using LLM for programming, when you have a way to evaluate the output, then it&#x27;s fine, if not, you can&#x27;t trust the output as it could be completely hallucinated.</div><br/></div></div></div></div><div id="42374988" class="c"><input type="checkbox" id="c-42374988" checked=""/><div class="controls bullet"><span class="by">thom</span><span>|</span><a href="#42373881">prev</a><span>|</span><a href="#42374552">next</a><span>|</span><label class="collapse" for="c-42374988">[-]</label><label class="expand" for="c-42374988">[1 more]</label></div><br/><div class="children"><div class="content">Probably, but the bitter lesson still applies.</div><br/></div></div><div id="42374552" class="c"><input type="checkbox" id="c-42374552" checked=""/><div class="controls bullet"><span class="by">2ro</span><span>|</span><a href="#42374988">prev</a><span>|</span><a href="#42374400">next</a><span>|</span><label class="collapse" for="c-42374552">[-]</label><label class="expand" for="c-42374552">[1 more]</label></div><br/><div class="children"><div class="content">if anyone is interested:<p><a href="https:&#x2F;&#x2F;2ro.co&#x2F;post&#x2F;768337188815536128" rel="nofollow">https:&#x2F;&#x2F;2ro.co&#x2F;post&#x2F;768337188815536128</a><p>(EZ - a language for constraint logic programming)</div><br/></div></div><div id="42374400" class="c"><input type="checkbox" id="c-42374400" checked=""/><div class="controls bullet"><span class="by">brokensegue</span><span>|</span><a href="#42374552">prev</a><span>|</span><a href="#42374453">next</a><span>|</span><label class="collapse" for="c-42374400">[-]</label><label class="expand" for="c-42374400">[1 more]</label></div><br/><div class="children"><div class="content">sometimes i think projects like cyc are like 3n+1 problem for AI. it&#x27;s so alluring.</div><br/></div></div><div id="42374453" class="c"><input type="checkbox" id="c-42374453" checked=""/><div class="controls bullet"><span class="by">K0balt</span><span>|</span><a href="#42374400">prev</a><span>|</span><a href="#42373107">next</a><span>|</span><label class="collapse" for="c-42374453">[-]</label><label class="expand" for="c-42374453">[1 more]</label></div><br/><div class="children"><div class="content">Hmm… maybe we could train &#x2F;tune a model on symbolic logic similar to or even using CycL instead of python, and then when we have it “write code” it would be solving the problem we want it to think about, using symbolic logic?<p>You might be on to something here. The problem being there isn’t billions of tokens worth of CycL out there to train on, or is there?</div><br/></div></div></div></div></div></div></div></body></html>