<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736845251939" as="style"/><link rel="stylesheet" href="styles.css?v=1736845251939"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://stevehanov.ca/blog/index.php?id=132">Lines of code that will beat A/B testing every time (2012)</a> <span class="domain">(<a href="https://stevehanov.ca">stevehanov.ca</a>)</span></div><div class="subtext"><span>Kerrick</span> | <span>127 comments</span></div><br/><div><div id="42688900" class="c"><input type="checkbox" id="c-42688900" checked=""/><div class="controls bullet"><span class="by">awkward</span><span>|</span><a href="#42691248">next</a><span>|</span><label class="collapse" for="c-42688900">[-]</label><label class="expand" for="c-42688900">[23 more]</label></div><br/><div class="children"><div class="content">Pure, disinterested A&#x2F;B testing where the goal is just to find the good way to do it, and there&#x27;s enough leverage and traffic that funding that A&#x2F;B testing is worthwhile is rare.<p>More frequently, A&#x2F;B testing is a political technology that allows teams to move forward with changes to core, vital services of a site or app. By putting a new change behind an A&#x2F;B test, the team technically derisks the change, by allowing it to be undone rapidly, and politically derisks the change, by tying it&#x27;s deployment to rigorous testing that proves it at least does no harm to the existing process before applying it to all users. The change was judged to be valuable when development effort went into it, whether for technical, branding or other reasons.<p>In short, not many people want to funnel users through N code paths with slightly different behaviors, because not many people have a ton of users, a ton of engineering capacity, and a ton of potential upside from marginal improvements. Two path tests solve the more common problem of wanting to make major changes to critical workflows without killing the platform.</div><br/><div id="42691812" class="c"><input type="checkbox" id="c-42691812" checked=""/><div class="controls bullet"><span class="by">xp84</span><span>|</span><a href="#42688900">parent</a><span>|</span><a href="#42690877">next</a><span>|</span><label class="collapse" for="c-42691812">[-]</label><label class="expand" for="c-42691812">[3 more]</label></div><br/><div class="children"><div class="content">&gt; politically derisks the change, by tying it&#x27;s deployment to <i>rigorous testing</i> that proves it at least does no harm to the existing process before applying it to all users.<p>I just want to drop here the anecdata that I&#x27;ve worked for a total of about 10 years in startups that proudly call themselves &quot;data-driven&quot; and which worshipped &quot;A&#x2F;B testing.&quot; One of them hired a data science team which actually did some decently rigorous analysis on our tests and advised things like when we had achieved statistical significance, how many impressions we needed to have, etc. The other did not and just had someone looking at very simple comparisons in Optimizely.<p>In both cases, the influential management people who ultimately owned the decisions would simply rig every &quot;test&quot; to fit the story they already believed, by doing things like running the test until the results looked &quot;positive&quot; but not until it was statistically significant. Or, by measuring several metrics and deciding later on to make the decision based on whichever one was positive [at the time]. Or, by skipping testing entirely and saying we&#x27;d just &quot;used a pre&#x2F;post comparison&quot; to prove it out. Or even by just dismissing a &#x27;failure,&#x27; saying we would do it anyway because it&#x27;s foundational to X, Y, and Z which really <i>will</i> improve (insert metric) The funny part is that none of these people thought they were playing dirty, they believed that they were making their decisions scientifically!<p>Basically, I suspect a lot of small and medium companies say they do &quot;A&#x2F;B testing&quot; and are &quot;data-driven&quot; when really they&#x27;re just using slightly fancy feature flags and relying on some director&#x27;s gut feelings.</div><br/><div id="42692091" class="c"><input type="checkbox" id="c-42692091" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42691812">parent</a><span>|</span><a href="#42694453">next</a><span>|</span><label class="collapse" for="c-42692091">[-]</label><label class="expand" for="c-42692091">[1 more]</label></div><br/><div class="children"><div class="content">At a small enough scale, gut feelings can be totally reasonable; taste is important and I&#x27;d rather follow an opinionated leader with good taste than someone who sits on their hands waiting for &quot;the data&quot;. Anyway, your investors want you to move quickly because they&#x27;re A&#x2F;B testing you for surviveability against everything else in their portfolio.<p>The worst is surely when management make the investments in rigor but then still ignores the guidance and goes with their gut feelings that were available all along.</div><br/></div></div><div id="42694453" class="c"><input type="checkbox" id="c-42694453" checked=""/><div class="controls bullet"><span class="by">petesergeant</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42691812">parent</a><span>|</span><a href="#42692091">prev</a><span>|</span><a href="#42690877">next</a><span>|</span><label class="collapse" for="c-42694453">[-]</label><label class="expand" for="c-42694453">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Basically, I suspect a lot of small and medium companies say they do &quot;A&#x2F;B testing&quot; and are &quot;data-driven&quot; when really they&#x27;re just using slightly fancy feature flags and relying on some director&#x27;s gut feelings.<p>see also Scrum and Agile. Or continuous deployment. Or anything else that&#x27;s hard to do well, and easier to just cargo-cult some results on and call it done.</div><br/></div></div></div></div><div id="42690877" class="c"><input type="checkbox" id="c-42690877" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42688900">parent</a><span>|</span><a href="#42691812">prev</a><span>|</span><a href="#42689080">next</a><span>|</span><label class="collapse" for="c-42690877">[-]</label><label class="expand" for="c-42690877">[8 more]</label></div><br/><div class="children"><div class="content"><i>&gt; In short, not many people want to funnel users through N code paths with slightly different behaviors, because not many people have a ton of users, a ton of engineering capacity, and a ton of potential upside from marginal improvements.</i><p>I’ve been in companies that have tried dozens if not hundreds of A&#x2F;B tests with zero statistically significant results. I figure by the law of probabilities they would have gotten at least a single significant experiment but most products have such small user bases and make such large changes at a time that it’s completely pointless.<p>All my complaints fell on deaf ears until the PM in charge would get on someone’s bad side and then that metric would be used to push them out. I think they’re largely a political tool like all those management consultants that only come in to justify an executive’s predetermined goals.</div><br/><div id="42691772" class="c"><input type="checkbox" id="c-42691772" checked=""/><div class="controls bullet"><span class="by">dkarl</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42690877">parent</a><span>|</span><a href="#42691107">next</a><span>|</span><label class="collapse" for="c-42691772">[-]</label><label class="expand" for="c-42691772">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I’ve been in companies that have tried dozens if not hundreds of A&#x2F;B tests with zero statistically significant results.<p>What I&#x27;ve seen in practice is that some places trust their designers&#x27; decisions and only deploy A&#x2F;B tests when competent people disagree, or there&#x27;s no clear, sound reason to choose one design over another. Surprise surprise, those alternatives almost always test very close to each other!<p>Other places remove virtually all friction from A&#x2F;B testing and then use it religiously for every pixel in their product, and they get results, but often it&#x27;s things like &quot;we discovered that pink doesn&#x27;t work as well as red for a warning button,&quot; stuff they never would have tried if they didn&#x27;t have to feed the A&#x2F;B machine.<p>From all the evidence I&#x27;ve seen in places I&#x27;ve worked, the motivating stories of &quot;we increased revenue 10% by a random change nobody thought would help&quot; may only exist in blog posts.</div><br/><div id="42693318" class="c"><input type="checkbox" id="c-42693318" checked=""/><div class="controls bullet"><span class="by">zeroCalories</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42691772">parent</a><span>|</span><a href="#42691107">next</a><span>|</span><label class="collapse" for="c-42693318">[-]</label><label class="expand" for="c-42693318">[2 more]</label></div><br/><div class="children"><div class="content">I think trusting your designers is probably the way to go for most teams. Good designers have solid intuitions and design principles for what will increase conversion rates. Many designers will still want a&#x2F;b tests because they want to be able to justify their impact, but they should probably be denied. For really important projects designers should do small sample size research to validate their designs like we would do in the past.<p>I think a&#x2F;b tests are still good for measuring stuff like system performance, which can be really hard to predict. Flipping a switch to completely change how you do caching can be scary.</div><br/><div id="42695027" class="c"><input type="checkbox" id="c-42695027" checked=""/><div class="controls bullet"><span class="by">Moru</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42693318">parent</a><span>|</span><a href="#42691107">next</a><span>|</span><label class="collapse" for="c-42695027">[-]</label><label class="expand" for="c-42695027">[1 more]</label></div><br/><div class="children"><div class="content">A&#x2F;B tests for user interface is very annoying when you are on the phone trying to guide someone how to use a website. &quot;Click the green button on the left&quot; - &quot;What do you mean? There is nothing green on the screen.&quot; - &quot;Are you on xyz.com? Can you read out the adress to me please?&quot; ... Oh so many hour wasted in tech support.</div><br/></div></div></div></div></div></div><div id="42691107" class="c"><input type="checkbox" id="c-42691107" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42690877">parent</a><span>|</span><a href="#42691772">prev</a><span>|</span><a href="#42690930">next</a><span>|</span><label class="collapse" for="c-42691107">[-]</label><label class="expand" for="c-42691107">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I’ve been in companies that have tried dozens if not hundreds of A&#x2F;B tests with zero statistically significant results.<p>Well, at least it looks like they avoided p-hacking to show more significance than they had!  That&#x27;s ahead of much of science, alas.</div><br/></div></div><div id="42690930" class="c"><input type="checkbox" id="c-42690930" checked=""/><div class="controls bullet"><span class="by">rco8786</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42690877">parent</a><span>|</span><a href="#42691107">prev</a><span>|</span><a href="#42689080">next</a><span>|</span><label class="collapse" for="c-42690930">[-]</label><label class="expand" for="c-42690930">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I’ve been in companies that have tried dozens if not hundreds of A&#x2F;B tests with zero statistically significant results.<p>Yea, I&#x27;ve been here too. And in every analytics meeting everyone went &quot;well, we know it&#x27;s not statistically significant but we&#x27;ll call it the winner anyway&quot;. Every. Single. Time.<p>Such a waste of resources.</div><br/><div id="42693025" class="c"><input type="checkbox" id="c-42693025" checked=""/><div class="controls bullet"><span class="by">hamandcheese</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42690930">parent</a><span>|</span><a href="#42689080">next</a><span>|</span><label class="collapse" for="c-42693025">[-]</label><label class="expand" for="c-42693025">[2 more]</label></div><br/><div class="children"><div class="content">Is it a waste? You proved the change wasn&#x27;t harmful.</div><br/><div id="42693367" class="c"><input type="checkbox" id="c-42693367" checked=""/><div class="controls bullet"><span class="by">zeroCalories</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42693025">parent</a><span>|</span><a href="#42689080">next</a><span>|</span><label class="collapse" for="c-42693367">[-]</label><label class="expand" for="c-42693367">[1 more]</label></div><br/><div class="children"><div class="content">Statistically insignificant means you didn&#x27;t prove anything by usual standards. I do agree that it&#x27;s not a waste, as knowing that you have a 70% chance that you&#x27;re going in the right direction is better than nothing. The 2 sigma crowd can be both too pessimistic and not pessimistic enough.</div><br/></div></div></div></div></div></div></div></div><div id="42689080" class="c"><input type="checkbox" id="c-42689080" checked=""/><div class="controls bullet"><span class="by">ljm</span><span>|</span><a href="#42688900">parent</a><span>|</span><a href="#42690877">prev</a><span>|</span><a href="#42693013">next</a><span>|</span><label class="collapse" for="c-42689080">[-]</label><label class="expand" for="c-42689080">[6 more]</label></div><br/><div class="children"><div class="content">Tracks that I’ve primarily seen A&#x2F;B tests used as a mechanism for gradual rollout rather than pure data-driven experimentation. Basically expose functionality to internal users by default then slowly expand it outwards to early adopters and then increment it to 100% for GA.<p>It’s helpful in continuous delivery setups since you can test and deploy the functionality and move the bottleneck for releasing beyond that.</div><br/><div id="42690347" class="c"><input type="checkbox" id="c-42690347" checked=""/><div class="controls bullet"><span class="by">baxtr</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42689080">parent</a><span>|</span><a href="#42693013">next</a><span>|</span><label class="collapse" for="c-42690347">[-]</label><label class="expand" for="c-42690347">[5 more]</label></div><br/><div class="children"><div class="content">I wouldn’t call that A&#x2F;B testing but rather a gradual roll-out.</div><br/><div id="42690879" class="c"><input type="checkbox" id="c-42690879" checked=""/><div class="controls bullet"><span class="by">alex7o</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42690347">parent</a><span>|</span><a href="#42694694">next</a><span>|</span><label class="collapse" for="c-42690879">[-]</label><label class="expand" for="c-42690879">[3 more]</label></div><br/><div class="children"><div class="content">I think parent is confusing A&#x2F;B testing with feature flags, which can be used for A&#x2F;B tests but also for roll-outs.</div><br/><div id="42692448" class="c"><input type="checkbox" id="c-42692448" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42690879">parent</a><span>|</span><a href="#42692244">next</a><span>|</span><label class="collapse" for="c-42692448">[-]</label><label class="expand" for="c-42692448">[1 more]</label></div><br/><div class="children"><div class="content">Not the parent but some actual practitioners. A change is based on the gut feeling, and it&#x27;s usually correct, but the internal politics require to demonstrate impartiality, so an &quot;A&#x2F;B test&quot; is run, to show that the change is &quot;objectively better&quot;, whether statistics show that or not.</div><br/></div></div><div id="42692244" class="c"><input type="checkbox" id="c-42692244" checked=""/><div class="controls bullet"><span class="by">awkward</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42690879">parent</a><span>|</span><a href="#42692448">prev</a><span>|</span><a href="#42694694">next</a><span>|</span><label class="collapse" for="c-42692244">[-]</label><label class="expand" for="c-42692244">[1 more]</label></div><br/><div class="children"><div class="content">I’m aware of the distinction. A&#x2F;B testing is the killer app for feature flags from the perspective of business decision makers.</div><br/></div></div></div></div><div id="42694694" class="c"><input type="checkbox" id="c-42694694" checked=""/><div class="controls bullet"><span class="by">cornel_io</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42690347">parent</a><span>|</span><a href="#42690879">prev</a><span>|</span><a href="#42693013">next</a><span>|</span><label class="collapse" for="c-42694694">[-]</label><label class="expand" for="c-42694694">[1 more]</label></div><br/><div class="children"><div class="content">If you roll it back upon seeing problems, then you&#x27;re doing something meaningful, at least. IMO 90+% of the value of A&#x2F;B testing comes from two things, a) forcing engineers to build everything behind flags, and b) making sure features don&#x27;t crater your metrics before freezing them in and making them <i>much</i> more difficult to remove (both politically and technically).<p>Re: b), if you&#x27;ve ever gotten into a screaming match with a game designer angry over the removal of their pet feature, you will really appreciate the political cover that having numbers provides...</div><br/></div></div></div></div></div></div><div id="42693013" class="c"><input type="checkbox" id="c-42693013" checked=""/><div class="controls bullet"><span class="by">hamandcheese</span><span>|</span><a href="#42688900">parent</a><span>|</span><a href="#42689080">prev</a><span>|</span><a href="#42691706">next</a><span>|</span><label class="collapse" for="c-42693013">[-]</label><label class="expand" for="c-42693013">[2 more]</label></div><br/><div class="children"><div class="content">I feel like you are trying to say &quot;sometimes people just need a feature flag&quot;. Which is of course true.</div><br/><div id="42694509" class="c"><input type="checkbox" id="c-42694509" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42693013">parent</a><span>|</span><a href="#42691706">next</a><span>|</span><label class="collapse" for="c-42694509">[-]</label><label class="expand" for="c-42694509">[1 more]</label></div><br/><div class="children"><div class="content">A feature flag can still be fully on or fully off.<p>Why they might conflate A&#x2F;B testing with gradual rollout is control over who gets the feature flag on and who doesn&#x27;t.<p>In a sense, A&#x2F;B testing <i>is</i> a variant of gradual rollout, where you&#x27;ve done it so you can see differences in feature &quot;performance&quot; (eg. funnel dashboards) vs just regular observability (app is not crashing yet).<p>Basically, a gradual rollout for the purposes of an A&#x2F;B test.</div><br/></div></div></div></div><div id="42691706" class="c"><input type="checkbox" id="c-42691706" checked=""/><div class="controls bullet"><span class="by">kavenkanum</span><span>|</span><a href="#42688900">parent</a><span>|</span><a href="#42693013">prev</a><span>|</span><a href="#42690795">next</a><span>|</span><label class="collapse" for="c-42691706">[-]</label><label class="expand" for="c-42691706">[2 more]</label></div><br/><div class="children"><div class="content">Derisking changes may not work sometimes. For example I don&#x27;t use Spotify anymore, because of their ridiculous Ab tests. In one month I saw 3 totally different designs of the home and my fav playlists page on my Android phone. That&#x27;s it. When you open Spotify only when you start your car then it&#x27;s ridiculous that you can&#x27;t find anything and you are in a hurry. That was it. I am no longer subscriber and a user of this shit service.
Sometimes these tests are actually harmful. Maybe others are just driving and trying to manage Spotify at the same time and then we have actual killed people because of this. Harmless Indeed.</div><br/><div id="42695094" class="c"><input type="checkbox" id="c-42695094" checked=""/><div class="controls bullet"><span class="by">Moru</span><span>|</span><a href="#42688900">root</a><span>|</span><a href="#42691706">parent</a><span>|</span><a href="#42690795">next</a><span>|</span><label class="collapse" for="c-42695094">[-]</label><label class="expand" for="c-42695094">[1 more]</label></div><br/><div class="children"><div class="content">Ever tried a support call helping someone using a website that has A&#x2F;B testing on? It&#x27;s a very frustrating experience where you start to think the user on the other side must have mistyped the url. A lot of time wasted on such calls. And yes, the worst is when these things only happen in things you use only when in a hurry.</div><br/></div></div></div></div><div id="42690795" class="c"><input type="checkbox" id="c-42690795" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#42688900">parent</a><span>|</span><a href="#42691706">prev</a><span>|</span><a href="#42691248">next</a><span>|</span><label class="collapse" for="c-42690795">[-]</label><label class="expand" for="c-42690795">[1 more]</label></div><br/><div class="children"><div class="content">Why do you consider it political. Isn&#x27;t it just a wise thing to do?</div><br/></div></div></div></div><div id="42691248" class="c"><input type="checkbox" id="c-42691248" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#42688900">prev</a><span>|</span><a href="#42689221">next</a><span>|</span><label class="collapse" for="c-42691248">[-]</label><label class="expand" for="c-42691248">[11 more]</label></div><br/><div class="children"><div class="content">One of the assumptions of vanilla multi-armed bandits is that the underlying reward rates are fixed. It&#x27;s not valid to assume that in a lot of cases, including e-commerce. The author is dismissive and hard wavy about this and having worked in in e-commerce SaaS I&#x27;d be a bit more cautious.<p>Imagine that you are running MAB on an website with a control&#x2F;treatment variant. After a bit you end up sampling the treatment a little more, say 60&#x2F;40. You now start running a sale - and the conversion rate for both sides goes up equally. But since you are now sampling more from the treatment variant, its aggregate conversion rate goes up faster than the control - you start weighting even more towards that variant.<p>Fluctuating reward rates are everywhere in e-commerce, and tend to destabilise MAB proportions, even on two identical variants, they can even cause it to lean towards the wrong one. There are more sophisticated MAB approaches that try to remove the identical reward-rate assumption - they have to model a lot more uncertainty, and so optimise more conservatively.</div><br/><div id="42694548" class="c"><input type="checkbox" id="c-42694548" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#42691248">parent</a><span>|</span><a href="#42694586">next</a><span>|</span><label class="collapse" for="c-42694548">[-]</label><label class="expand" for="c-42694548">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ...the conversion rate for both sides goes up equally.<p>If the conversion rate &quot;goes up equally&quot;, why did you not measure this and use that as a basis for your decisions?<p>&gt; its aggregate conversion rate goes up faster than the control - you start weighting even more towards that variant.<p>This sounds simply like using bad math. Wouldn&#x27;t this kill most experiments that start with 10% for the variant that do not provide 10x the improvement?</div><br/></div></div><div id="42694586" class="c"><input type="checkbox" id="c-42694586" checked=""/><div class="controls bullet"><span class="by">alex5207</span><span>|</span><a href="#42691248">parent</a><span>|</span><a href="#42694548">prev</a><span>|</span><a href="#42692085">next</a><span>|</span><label class="collapse" for="c-42694586">[-]</label><label class="expand" for="c-42694586">[1 more]</label></div><br/><div class="children"><div class="content">Good point about fluctuating rates for e.g the sales period. But couldn&#x27;t you then pick a metric that doesn&#x27;t fluctuate?<p>Out of curiosity, where did you work? In the same space as you.</div><br/></div></div><div id="42692085" class="c"><input type="checkbox" id="c-42692085" checked=""/><div class="controls bullet"><span class="by">ertdfgcvb</span><span>|</span><a href="#42691248">parent</a><span>|</span><a href="#42694586">prev</a><span>|</span><a href="#42692321">next</a><span>|</span><label class="collapse" for="c-42692085">[-]</label><label class="expand" for="c-42692085">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t follow. In this case would sampling 50&#x2F;50 always give better&#x2F;unbiased results on the experiment?</div><br/><div id="42693179" class="c"><input type="checkbox" id="c-42693179" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#42691248">root</a><span>|</span><a href="#42692085">parent</a><span>|</span><a href="#42693133">next</a><span>|</span><label class="collapse" for="c-42693179">[-]</label><label class="expand" for="c-42693179">[2 more]</label></div><br/><div class="children"><div class="content">Sampling 50&#x2F;50 will always give you the best chance of picking the best ultimate &#x27;winner&#x27; in a fixed time horizon, at the cost of only sampling the winning variant 50% of the time. That&#x27;s true if the reward rates are fixed or not. But some changes in reward rates will also cause MAB aggregate statistics to skew in a way that they shouldn&#x27;t for a 50&#x2F;50 split yeah.</div><br/><div id="42693439" class="c"><input type="checkbox" id="c-42693439" checked=""/><div class="controls bullet"><span class="by">zeroCalories</span><span>|</span><a href="#42691248">root</a><span>|</span><a href="#42693179">parent</a><span>|</span><a href="#42693133">next</a><span>|</span><label class="collapse" for="c-42693439">[-]</label><label class="expand" for="c-42693439">[1 more]</label></div><br/><div class="children"><div class="content">What do you think of using the epsilon-first approach then? We could explore for that fixed time horizon, then start choosing greedy after that. I feel like the only downside is that adding new arms becomes more complicated.</div><br/></div></div></div></div><div id="42693133" class="c"><input type="checkbox" id="c-42693133" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#42691248">root</a><span>|</span><a href="#42692085">parent</a><span>|</span><a href="#42693179">prev</a><span>|</span><a href="#42692321">next</a><span>|</span><label class="collapse" for="c-42693133">[-]</label><label class="expand" for="c-42693133">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div></div></div><div id="42692321" class="c"><input type="checkbox" id="c-42692321" checked=""/><div class="controls bullet"><span class="by">tjbai</span><span>|</span><a href="#42691248">parent</a><span>|</span><a href="#42692085">prev</a><span>|</span><a href="#42689221">next</a><span>|</span><label class="collapse" for="c-42692321">[-]</label><label class="expand" for="c-42692321">[4 more]</label></div><br/><div class="children"><div class="content">I agree that there&#x27;s an exploration-exploitation tradeoff, but for what you specifically suggest wouldn&#x27;t you presumably just normalize by sample size? You wouldn&#x27;t allocate based off total conversions, but rather a percentage.</div><br/><div id="42693317" class="c"><input type="checkbox" id="c-42693317" checked=""/><div class="controls bullet"><span class="by">jvans</span><span>|</span><a href="#42691248">root</a><span>|</span><a href="#42692321">parent</a><span>|</span><a href="#42693341">next</a><span>|</span><label class="collapse" for="c-42693317">[-]</label><label class="expand" for="c-42693317">[1 more]</label></div><br/><div class="children"><div class="content">Imagine a scenario where option B does 10x better than option A during the morning hours but -2x worse the rest of the day. If you start the multi armed bandit in the morning it could converge to option B quickly and dominate the rest of the day even though it performs worse then.<p>Or in the above scenario option B performs a lot better than option A but only with the sale going, otherwise option B performs worse.</div><br/></div></div><div id="42693341" class="c"><input type="checkbox" id="c-42693341" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#42691248">root</a><span>|</span><a href="#42692321">parent</a><span>|</span><a href="#42693317">prev</a><span>|</span><a href="#42689221">next</a><span>|</span><label class="collapse" for="c-42693341">[-]</label><label class="expand" for="c-42693341">[2 more]</label></div><br/><div class="children"><div class="content">Yes but here&#x27;s a exaggerated version - say were to sample for a week at 50&#x2F;50 when the base conversion rate was at 4%, then we sample at 25&#x2F;75 for a week with the base conversion rate bumped up to 8% due to a sale.<p>The average base rate for the first variant is 5.3%, the second is 6.4%. Generally the favoured variant&#x27;s average will shift faster because we are sampling it more.</div><br/></div></div></div></div></div></div><div id="42689221" class="c"><input type="checkbox" id="c-42689221" checked=""/><div class="controls bullet"><span class="by">taion</span><span>|</span><a href="#42691248">prev</a><span>|</span><a href="#42686404">next</a><span>|</span><label class="collapse" for="c-42689221">[-]</label><label class="expand" for="c-42689221">[9 more]</label></div><br/><div class="children"><div class="content">The problem with this approach is that it requires the system doing randomization to be aware of the rewards. That doesn&#x27;t make a lot of sense architecturally – the rewards you care about often relate to how the user engages with your product, and you would generally expect those to be collected via some offline analytics system that is disjoint from your online serving system.<p>Additionally, doing randomization on a per-request basis heavily limits the kinds of user behaviors you can observe. Often you want to consistently assign the same user to the same condition to observe long-term changes in user behavior.<p>This approach is pretty clever on paper but it&#x27;s a poor fit for how experimentation works in practice and from a system design POV.</div><br/><div id="42693146" class="c"><input type="checkbox" id="c-42693146" checked=""/><div class="controls bullet"><span class="by">hruk</span><span>|</span><a href="#42689221">parent</a><span>|</span><a href="#42689902">next</a><span>|</span><label class="collapse" for="c-42693146">[-]</label><label class="expand" for="c-42693146">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know, all of these are pretty surmountable. We&#x27;ve done dynamic pricing with contextual multi-armed bandits, in which each context gets a single decision per time block and gross profit is summed up at the end of each block and used to reward the agent.<p>That being said, I agree that MABs are poor for experimentation (they produce biased estimates that depend on somewhat hard-to-quantify properties of your policy). But they&#x27;re not for experimentation! They&#x27;re for optimizing a target metric.</div><br/></div></div><div id="42689902" class="c"><input type="checkbox" id="c-42689902" checked=""/><div class="controls bullet"><span class="by">ivalm</span><span>|</span><a href="#42689221">parent</a><span>|</span><a href="#42693146">prev</a><span>|</span><a href="#42686404">next</a><span>|</span><label class="collapse" for="c-42689902">[-]</label><label class="expand" for="c-42689902">[7 more]</label></div><br/><div class="children"><div class="content">You can assign multiarm bandit trials on a lazy per user basis.<p>So first time user touches feature A they are assigned to some trial arm T_A and then all subsequent interactions keep them in that trial arm until the trial finishes.</div><br/><div id="42690424" class="c"><input type="checkbox" id="c-42690424" checked=""/><div class="controls bullet"><span class="by">kridsdale1</span><span>|</span><a href="#42689221">root</a><span>|</span><a href="#42689902">parent</a><span>|</span><a href="#42686404">next</a><span>|</span><label class="collapse" for="c-42690424">[-]</label><label class="expand" for="c-42690424">[6 more]</label></div><br/><div class="children"><div class="content">The systems I’ve use pre-allocate users effectively randomly an arm by hashing their user id or equivalent.</div><br/><div id="42693061" class="c"><input type="checkbox" id="c-42693061" checked=""/><div class="controls bullet"><span class="by">ivalm</span><span>|</span><a href="#42689221">root</a><span>|</span><a href="#42690424">parent</a><span>|</span><a href="#42691069">next</a><span>|</span><label class="collapse" for="c-42693061">[-]</label><label class="expand" for="c-42693061">[1 more]</label></div><br/><div class="children"><div class="content">To make sure user id U doesn’t always end up in eg control group it’s useful to concatenate the id with experiment uuid.</div><br/></div></div><div id="42691069" class="c"><input type="checkbox" id="c-42691069" checked=""/><div class="controls bullet"><span class="by">ryan-duve</span><span>|</span><a href="#42689221">root</a><span>|</span><a href="#42690424">parent</a><span>|</span><a href="#42693061">prev</a><span>|</span><a href="#42691012">next</a><span>|</span><label class="collapse" for="c-42691069">[-]</label><label class="expand" for="c-42691069">[1 more]</label></div><br/><div class="children"><div class="content">How do you handle different users having different numbers of trials when calculating the &quot;click through rate&quot; described in the article?</div><br/></div></div><div id="42691012" class="c"><input type="checkbox" id="c-42691012" checked=""/><div class="controls bullet"><span class="by">s1mplicissimus</span><span>|</span><a href="#42689221">root</a><span>|</span><a href="#42690424">parent</a><span>|</span><a href="#42691069">prev</a><span>|</span><a href="#42686404">next</a><span>|</span><label class="collapse" for="c-42691012">[-]</label><label class="expand" for="c-42691012">[3 more]</label></div><br/><div class="children"><div class="content">careful when doing that though!
i&#x27;ve seen some big eyes when people assumed IDs to be uniform randomly distributed and suddenly their &quot;test group&quot; was 15% instead of the intended 1%.
better generate a truely random value using your languages favorite crypto functions and be able to work with it without fear of busting production</div><br/><div id="42692931" class="c"><input type="checkbox" id="c-42692931" checked=""/><div class="controls bullet"><span class="by">np_tedious</span><span>|</span><a href="#42689221">root</a><span>|</span><a href="#42691012">parent</a><span>|</span><a href="#42686404">next</a><span>|</span><label class="collapse" for="c-42692931">[-]</label><label class="expand" for="c-42692931">[2 more]</label></div><br/><div class="children"><div class="content">The user ID is non uniform after hash and mod? How?</div><br/><div id="42693161" class="c"><input type="checkbox" id="c-42693161" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#42689221">root</a><span>|</span><a href="#42692931">parent</a><span>|</span><a href="#42686404">next</a><span>|</span><label class="collapse" for="c-42693161">[-]</label><label class="expand" for="c-42693161">[1 more]</label></div><br/><div class="children"><div class="content">If you mod by anything other than a power of two, it won&#x27;t be. <a href="https:&#x2F;&#x2F;lemire.me&#x2F;blog&#x2F;2019&#x2F;06&#x2F;06&#x2F;nearly-divisionless-random-integer-generation-on-various-systems&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lemire.me&#x2F;blog&#x2F;2019&#x2F;06&#x2F;06&#x2F;nearly-divisionless-random...</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42686404" class="c"><input type="checkbox" id="c-42686404" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#42689221">prev</a><span>|</span><a href="#42686554">next</a><span>|</span><label class="collapse" for="c-42686404">[-]</label><label class="expand" for="c-42686404">[7 more]</label></div><br/><div class="children"><div class="content">As one of the comments below the article states, the probabilistic alternative to epsilon-greedy is worth exploring ad well. Take the &quot;bayesian bandit&quot;, which is not much more complex but a lot more powerful.<p>If you crave more bandits: <a href="https:&#x2F;&#x2F;jamesrledoux.com&#x2F;algorithms&#x2F;bandit-algorithms-epsilon-ucb-exp-python&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jamesrledoux.com&#x2F;algorithms&#x2F;bandit-algorithms-epsilo...</a></div><br/><div id="42686807" class="c"><input type="checkbox" id="c-42686807" checked=""/><div class="controls bullet"><span class="by">timr</span><span>|</span><a href="#42686404">parent</a><span>|</span><a href="#42690427">next</a><span>|</span><label class="collapse" for="c-42686807">[-]</label><label class="expand" for="c-42686807">[4 more]</label></div><br/><div class="children"><div class="content">Just a warning to those people who are potentially implementing it: it doesn&#x27;t really matter. The blog author addresses this, obliquely (says that the simplest thing is best most of the time), but doesn&#x27;t make it explicit.<p>In my experience, obsessing on the best decision strategy is the biggest honeypot for engineers implementing MAB. Epsilon-greedy is <i>very easy to implement</i> and you probably don&#x27;t need anything more. Thompson sampling is a pain in the butt, for not much gain.</div><br/><div id="42690679" class="c"><input type="checkbox" id="c-42690679" checked=""/><div class="controls bullet"><span class="by">blagie</span><span>|</span><a href="#42686404">root</a><span>|</span><a href="#42686807">parent</a><span>|</span><a href="#42690427">next</a><span>|</span><label class="collapse" for="c-42690679">[-]</label><label class="expand" for="c-42690679">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Easy to implement&quot; is a good reason to use bubble sort too.<p>In a normal universe, you just import a different library, so both are the same amount of work to implement.<p>Multiarmed bandit seems theoretically pretty, but it&#x27;s rarely worth it. The complexity isn&#x27;t the numerical algorithm but state management.<p>* Most AB tests can be as simple as a client-side random() and a log file.<p>* Multiarmed bandit means you need an immediate feedback loop, which involves things like adding database columns, worrying about performance (since each render requires another database read), etc. Keep in mind the database needs to now store AB test outcomes and use those for decision-making, and computing those is sometimes nontrivial (if it&#x27;s anything beyond a click-through).<p>* Long-term outcomes matter more than short-term. &quot;Did we retain a customer&quot; is more important than &quot;did we close one sale.&quot;<p>In most systems, the benefits aren&#x27;t worth the complexity. Multiple AB tests also add testing complexity. You want to test three layouts? And three user flows? Now, you have nine cases which need to be tested. Add two color schemes? 18 cases. Add 3 font options? 54 cases. The exponential growth in testing is not fun. Fire-and-forget seems great, but in practice, it&#x27;s fire-and-maintain-exponential complexity.<p>And those conversion differences are usually small enough that being on the wrong side of a single AB test isn&#x27;t expensive.<p>Run the test. Analyze the data. Pick the outcome. Kill the other code path. Perhaps re-analyze the data a year later with different, longer-term metrics. Repeat. That&#x27;s the right level of complexity most of the time.<p>If you step up to multiarm, importing a different library ain&#x27;t bad.</div><br/><div id="42691167" class="c"><input type="checkbox" id="c-42691167" checked=""/><div class="controls bullet"><span class="by">bartread</span><span>|</span><a href="#42686404">root</a><span>|</span><a href="#42690679">parent</a><span>|</span><a href="#42690427">next</a><span>|</span><label class="collapse" for="c-42691167">[-]</label><label class="expand" for="c-42691167">[2 more]</label></div><br/><div class="children"><div class="content">Sorry but bubble sort is a terrible example here. You implement a more difficult sorting algorithm, like quicksort, because the benefits of doing so, versus using bubble sort, are in many cases huge. I.e., the juice is worth the squeeze.<p>Whereas the comment you’re responding to is rightly pointing out that for most orgs, the marginal gains of using an approach more complex than Epsilon greedy probably aren’t worth it. I.e., the juice isn’t worth the squeeze.</div><br/><div id="42693462" class="c"><input type="checkbox" id="c-42693462" checked=""/><div class="controls bullet"><span class="by">zeroCalories</span><span>|</span><a href="#42686404">root</a><span>|</span><a href="#42691167">parent</a><span>|</span><a href="#42690427">next</a><span>|</span><label class="collapse" for="c-42693462">[-]</label><label class="expand" for="c-42693462">[1 more]</label></div><br/><div class="children"><div class="content">Bubble sort is a great example because it can out perform quicksort if the input is small.</div><br/></div></div></div></div></div></div></div></div><div id="42690427" class="c"><input type="checkbox" id="c-42690427" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#42686404">parent</a><span>|</span><a href="#42686807">prev</a><span>|</span><a href="#42686565">next</a><span>|</span><label class="collapse" for="c-42690427">[-]</label><label class="expand" for="c-42690427">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a good derivation of EXP3 algorithm from standard multiplicative weights which is fairly intuitive. The transformation between the two is explained a bit in <a href="https:&#x2F;&#x2F;nerva.cs.uni-bonn.de&#x2F;lib&#x2F;exe&#x2F;fetch.php&#x2F;teaching&#x2F;ws1819&#x2F;vl-aau&#x2F;lecturenotes17.pdf" rel="nofollow">https:&#x2F;&#x2F;nerva.cs.uni-bonn.de&#x2F;lib&#x2F;exe&#x2F;fetch.php&#x2F;teaching&#x2F;ws18...</a>. Once you have the intuition, then the actual choice of parameters is just cranking out the math</div><br/></div></div><div id="42686565" class="c"><input type="checkbox" id="c-42686565" checked=""/><div class="controls bullet"><span class="by">hruk</span><span>|</span><a href="#42686404">parent</a><span>|</span><a href="#42690427">prev</a><span>|</span><a href="#42686554">next</a><span>|</span><label class="collapse" for="c-42686565">[-]</label><label class="expand" for="c-42686565">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve been happy using Thompson sampling in production with this library <a href="https:&#x2F;&#x2F;github.com&#x2F;bayesianbandits&#x2F;bayesianbandits">https:&#x2F;&#x2F;github.com&#x2F;bayesianbandits&#x2F;bayesianbandits</a></div><br/></div></div></div></div><div id="42686554" class="c"><input type="checkbox" id="c-42686554" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#42686404">prev</a><span>|</span><a href="#42688317">next</a><span>|</span><label class="collapse" for="c-42686554">[-]</label><label class="expand" for="c-42686554">[8 more]</label></div><br/><div class="children"><div class="content">A lot of sites don&#x27;t have enough traffic to get statistical significance with this in a reasonable amount of time and it&#x27;s almost always testing a feature more complicated than button color where you aren&#x27;t going to have more than the control and variant.</div><br/><div id="42690417" class="c"><input type="checkbox" id="c-42690417" checked=""/><div class="controls bullet"><span class="by">kridsdale1</span><span>|</span><a href="#42686554">parent</a><span>|</span><a href="#42691130">next</a><span>|</span><label class="collapse" for="c-42690417">[-]</label><label class="expand" for="c-42690417">[1 more]</label></div><br/><div class="children"><div class="content">I’ve only implemented A&#x2F;B&#x2F;C tests at Facebook and Google, with hundreds of millions of DAU on the surfaces in question, and three groups is still often enough to dilute the measurement in question below stat-sig.</div><br/></div></div><div id="42691130" class="c"><input type="checkbox" id="c-42691130" checked=""/><div class="controls bullet"><span class="by">ryan-duve</span><span>|</span><a href="#42686554">parent</a><span>|</span><a href="#42690417">prev</a><span>|</span><a href="#42688576">next</a><span>|</span><label class="collapse" for="c-42691130">[-]</label><label class="expand" for="c-42691130">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A lot of sites don&#x27;t have enough traffic to get statistical significance with this in a reasonable amount of time<p>What&#x27;s nice about AB testing is the decision can be made on point estimates, provided the two choices don&#x27;t have different operational &quot;costs&quot;.  You don&#x27;t need to <i>know</i> that A is better than B, you just need to pick one and the point estimate gives the best answer with the available data.<p>I don&#x27;t know of a way to determine whether A is better than B with statistical significance without letting the experiment run, in practice, for way too long.</div><br/></div></div><div id="42688576" class="c"><input type="checkbox" id="c-42688576" checked=""/><div class="controls bullet"><span class="by">wiml</span><span>|</span><a href="#42686554">parent</a><span>|</span><a href="#42691130">prev</a><span>|</span><a href="#42686903">next</a><span>|</span><label class="collapse" for="c-42688576">[-]</label><label class="expand" for="c-42688576">[3 more]</label></div><br/><div class="children"><div class="content">If the effect size x site traffic is so small it&#x27;s statistically insignificant, why are you doing all this work in the first place? Just choose the option that makes the PHB happy and move on.<p>(But, it&#x27;s more likely that you <i>don&#x27;t know</i> if there&#x27;s a significant effect size)</div><br/><div id="42688656" class="c"><input type="checkbox" id="c-42688656" checked=""/><div class="controls bullet"><span class="by">koliber</span><span>|</span><a href="#42686554">root</a><span>|</span><a href="#42688576">parent</a><span>|</span><a href="#42686903">next</a><span>|</span><label class="collapse" for="c-42688656">[-]</label><label class="expand" for="c-42688656">[2 more]</label></div><br/><div class="children"><div class="content">The PHB wanted A&#x2F;B testing! True story. I&#x27;ve spent two months convincing them that it made no sense with the volume of conversion events we had.</div><br/><div id="42690891" class="c"><input type="checkbox" id="c-42690891" checked=""/><div class="controls bullet"><span class="by">wussboy</span><span>|</span><a href="#42686554">root</a><span>|</span><a href="#42688656">parent</a><span>|</span><a href="#42686903">next</a><span>|</span><label class="collapse" for="c-42690891">[-]</label><label class="expand" for="c-42690891">[1 more]</label></div><br/><div class="children"><div class="content">Another option, &quot;I&#x27;m already doing A&#x2F;B testing, trust me.&quot;</div><br/></div></div></div></div></div></div><div id="42686903" class="c"><input type="checkbox" id="c-42686903" checked=""/><div class="controls bullet"><span class="by">douglee650</span><span>|</span><a href="#42686554">parent</a><span>|</span><a href="#42688576">prev</a><span>|</span><a href="#42688740">next</a><span>|</span><label class="collapse" for="c-42686903">[-]</label><label class="expand" for="c-42686903">[1 more]</label></div><br/><div class="children"><div class="content">Yes wondering what the confidence intervals are.</div><br/></div></div></div></div><div id="42688317" class="c"><input type="checkbox" id="c-42688317" checked=""/><div class="controls bullet"><span class="by">munro</span><span>|</span><a href="#42686554">prev</a><span>|</span><a href="#42693391">next</a><span>|</span><label class="collapse" for="c-42688317">[-]</label><label class="expand" for="c-42688317">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s an interesting write up on various algorithms &amp; different epsilon greedy % values.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;raffg&#x2F;multi_armed_bandit">https:&#x2F;&#x2F;github.com&#x2F;raffg&#x2F;multi_armed_bandit</a><p>It shows 10% exploration performs the best, very great simple algorithm.<p>Also it shows the Thompson Sampling algorithm converges a bit faster-- the best arm chosen by sampling from the beta distribution, and eliminates the explore phase.  And you can use the builtin random.betavariate !<p><a href="https:&#x2F;&#x2F;github.com&#x2F;raffg&#x2F;multi_armed_bandit&#x2F;blob&#x2F;42b7377541cb5c31356e5386af0cf66b5d3da579&#x2F;algorithms&#x2F;thompson_sampling.py#L22">https:&#x2F;&#x2F;github.com&#x2F;raffg&#x2F;multi_armed_bandit&#x2F;blob&#x2F;42b7377541c...</a></div><br/></div></div><div id="42693391" class="c"><input type="checkbox" id="c-42693391" checked=""/><div class="controls bullet"><span class="by">MichaelDickens</span><span>|</span><a href="#42688317">prev</a><span>|</span><a href="#42688402">next</a><span>|</span><label class="collapse" for="c-42693391">[-]</label><label class="expand" for="c-42693391">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>        # for each lever, 
            # calculate the expectation of reward. 
            # This is the number of trials of the lever divided by the total reward 
            # given by that lever.
        # choose the lever with the greatest expectation of reward.
</code></pre>
If I&#x27;m not mistaken, this pseudocode has a bug that will result in choosing the expected <i>worst</i> option rather than the expected <i>best</i> option. I believe it should read &quot;total reward given by the lever divided by the number of trials of that lever&quot;.</div><br/><div id="42693737" class="c"><input type="checkbox" id="c-42693737" checked=""/><div class="controls bullet"><span class="by">reader9274</span><span>|</span><a href="#42693391">parent</a><span>|</span><a href="#42688402">next</a><span>|</span><label class="collapse" for="c-42693737">[-]</label><label class="expand" for="c-42693737">[1 more]</label></div><br/><div class="children"><div class="content">Correct, that&#x27;s why I don&#x27;t trust reading code comments</div><br/></div></div></div></div><div id="42688402" class="c"><input type="checkbox" id="c-42688402" checked=""/><div class="controls bullet"><span class="by">iforgot22</span><span>|</span><a href="#42693391">prev</a><span>|</span><a href="#42687144">next</a><span>|</span><label class="collapse" for="c-42688402">[-]</label><label class="expand" for="c-42688402">[8 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t like how this dismisses the old approach as &quot;statistics are hard for most people to understand.&quot; This algo beats A&#x2F;B testing in terms of maximizing how many visitors get the best feature. But is that really a big enough concern IRL that people are interested in optimizing it every time? Every little dynamic lever adds complexity to a system.</div><br/><div id="42693888" class="c"><input type="checkbox" id="c-42693888" checked=""/><div class="controls bullet"><span class="by">recursivecaveat</span><span>|</span><a href="#42688402">parent</a><span>|</span><a href="#42689113">next</a><span>|</span><label class="collapse" for="c-42693888">[-]</label><label class="expand" for="c-42693888">[1 more]</label></div><br/><div class="children"><div class="content">Indeed perhaps we should applaud people for choosing statistical tools that are relatively easy to use and interpret, rather than deride them for not stepping up to the lathe that they didn&#x27;t really need and we admit has lots of sharp edges.</div><br/></div></div><div id="42689113" class="c"><input type="checkbox" id="c-42689113" checked=""/><div class="controls bullet"><span class="by">rerdavies</span><span>|</span><a href="#42688402">parent</a><span>|</span><a href="#42693888">prev</a><span>|</span><a href="#42688462">next</a><span>|</span><label class="collapse" for="c-42689113">[-]</label><label class="expand" for="c-42689113">[2 more]</label></div><br/><div class="children"><div class="content">I think you missed the point. It&#x27;s not about which visitors get the best feature. It&#x27;s about how to get people to PUSH THE BUTTON!!!!! Which is kind of the opposite of the best feature. The goal is to make people do something they don&#x27;t want to do.<p>Figuring out best features is a completely different problem.</div><br/><div id="42689287" class="c"><input type="checkbox" id="c-42689287" checked=""/><div class="controls bullet"><span class="by">iforgot22</span><span>|</span><a href="#42688402">root</a><span>|</span><a href="#42689113">parent</a><span>|</span><a href="#42688462">next</a><span>|</span><label class="collapse" for="c-42689287">[-]</label><label class="expand" for="c-42689287">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t say it was the best for the user. Really the article misses this by comparing a new UI feature to a life-saving drug, but it doesn&#x27;t matter. The point is, whatever metric you&#x27;re targeting, do you use this algo or fixed group sizes?</div><br/></div></div></div></div><div id="42688462" class="c"><input type="checkbox" id="c-42688462" checked=""/><div class="controls bullet"><span class="by">randomcatuser</span><span>|</span><a href="#42688402">parent</a><span>|</span><a href="#42689113">prev</a><span>|</span><a href="#42687144">next</a><span>|</span><label class="collapse" for="c-42688462">[-]</label><label class="expand" for="c-42688462">[4 more]</label></div><br/><div class="children"><div class="content">Yeah basically. The idea is that somehow this is the data-optimal way of determining which one is the best (rather than splitting your data 50&#x2F;50 and wasting a lot of samples when you already know)<p>The caveats (perhaps not mentioned in the article) are:
- Perhaps you have many metrics you need to track&#x2F;analyze (CTR, conversion, rates on different metrics), so you can&#x27;t strictly do bandit!
- As someone mentioned below, sometimes the situation is dynamic (so having evenly sized groups helps with capturing this effect)
- Maybe some other ones I can&#x27;t think of?<p>But you can imagine this kind of auto-testing being useful... imagine AI continually pushes new variants, and it just continually learns which one is the best</div><br/><div id="42688581" class="c"><input type="checkbox" id="c-42688581" checked=""/><div class="controls bullet"><span class="by">cle</span><span>|</span><a href="#42688402">root</a><span>|</span><a href="#42688462">parent</a><span>|</span><a href="#42688564">next</a><span>|</span><label class="collapse" for="c-42688581">[-]</label><label class="expand" for="c-42688581">[2 more]</label></div><br/><div class="children"><div class="content">It still misses the biggest challenge though--defining &quot;best&quot;, and ensuring you&#x27;re actually measuring it and not something else.<p>It&#x27;s useful as long as your definition is good enough and your measurements and randomizations aren&#x27;t biased. Are you monitoring this over time to ensure that it continues to hold? If you don&#x27;t, you risk your MAB converging on something very different from what you would consider &quot;the best&quot;.<p>When it converges on the right thing, it&#x27;s better. When it converges on the wrong thing, it&#x27;s worse. Which will it do? What&#x27;s the magnitude of the upside vs downside?</div><br/><div id="42691208" class="c"><input type="checkbox" id="c-42691208" checked=""/><div class="controls bullet"><span class="by">desert_rue</span><span>|</span><a href="#42688402">root</a><span>|</span><a href="#42688581">parent</a><span>|</span><a href="#42688564">next</a><span>|</span><label class="collapse" for="c-42691208">[-]</label><label class="expand" for="c-42691208">[1 more]</label></div><br/><div class="children"><div class="content">Are you saying that it may do something like improve click-the-button conversion but lead to less sales overall?</div><br/></div></div></div></div><div id="42688564" class="c"><input type="checkbox" id="c-42688564" checked=""/><div class="controls bullet"><span class="by">iforgot22</span><span>|</span><a href="#42688402">root</a><span>|</span><a href="#42688462">parent</a><span>|</span><a href="#42688581">prev</a><span>|</span><a href="#42687144">next</a><span>|</span><label class="collapse" for="c-42688564">[-]</label><label class="expand" for="c-42688564">[1 more]</label></div><br/><div class="children"><div class="content">Facebook or YouTube might already be using an algo like this or AI to push variants, but for each billion user product, there are probably thousands of smaller products that don&#x27;t need something this automated.</div><br/></div></div></div></div></div></div><div id="42687144" class="c"><input type="checkbox" id="c-42687144" checked=""/><div class="controls bullet"><span class="by">__MatrixMan__</span><span>|</span><a href="#42688402">prev</a><span>|</span><a href="#42694103">next</a><span>|</span><label class="collapse" for="c-42687144">[-]</label><label class="expand" for="c-42687144">[1 more]</label></div><br/><div class="children"><div class="content">After this:<p>&gt; hundreds of the brightest minds of modern civilization have been hard at work not curing cancer. Instead, they have been refining techniques for getting you and me to click on banner ads<p>I was really hoping this would slowly develop into a statistical technique couched in terms of ad optimization but actually settling in on something you might call ATCG testing (e.g. the biostatistics methods that one would indeed use to cure cancer).</div><br/></div></div><div id="42694103" class="c"><input type="checkbox" id="c-42694103" checked=""/><div class="controls bullet"><span class="by">jwr</span><span>|</span><a href="#42687144">prev</a><span>|</span><a href="#42687949">next</a><span>|</span><label class="collapse" for="c-42694103">[-]</label><label class="expand" for="c-42694103">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using a Bernoulli bandit for many years now. Like the original author, I am not quite sure why more people don&#x27;t use it — for all practical purposes, if you have stuff to do, it is superior to simple A&#x2F;B testing in every way. The &quot;set it and forget it&quot; feature is really nice.<p>Another thing that I noticed while writing the code (and took advantage of) is that it is insanely scalable in a distributed system, using HyperLogLog and Bloom filters. I may or may not have totally over-engineered my code to take advantage of that, even though my site gets ridiculously low click numbers :-)</div><br/></div></div><div id="42687949" class="c"><input type="checkbox" id="c-42687949" checked=""/><div class="controls bullet"><span class="by">royal-fig</span><span>|</span><a href="#42694103">prev</a><span>|</span><a href="#42686449">next</a><span>|</span><label class="collapse" for="c-42687949">[-]</label><label class="expand" for="c-42687949">[1 more]</label></div><br/><div class="children"><div class="content">If multi-arm bandits has piqued your curiosity, we recently added support for it to our feature flagging and experimentation platform, GrowthBook.<p>We talk about it here: <a href="https:&#x2F;&#x2F;blog.growthbook.io&#x2F;introducing-multi-armed-bandits-in-growthbook&#x2F;">https:&#x2F;&#x2F;blog.growthbook.io&#x2F;introducing-multi-armed-bandits-i...</a></div><br/></div></div><div id="42686449" class="c"><input type="checkbox" id="c-42686449" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#42687949">prev</a><span>|</span><a href="#42687060">next</a><span>|</span><label class="collapse" for="c-42686449">[-]</label><label class="expand" for="c-42686449">[3 more]</label></div><br/><div class="children"><div class="content">Nothing shows on this page without JavaScript except for the header and a grey background. A bit strange for a blog.</div><br/><div id="42686945" class="c"><input type="checkbox" id="c-42686945" checked=""/><div class="controls bullet"><span class="by">lelandfe</span><span>|</span><a href="#42686449">parent</a><span>|</span><a href="#42686479">next</a><span>|</span><label class="collapse" for="c-42686945">[-]</label><label class="expand" for="c-42686945">[1 more]</label></div><br/><div class="children"><div class="content">Pre-CSS grid masonry layout. Author hides the content with CSS, and JS reveals it, to avoid a flash.<p>CSS to make it noscript friendly: `.main { visibility: visible !important; max-width: 710px; }`</div><br/></div></div><div id="42686479" class="c"><input type="checkbox" id="c-42686479" checked=""/><div class="controls bullet"><span class="by">forgetfreeman</span><span>|</span><a href="#42686449">parent</a><span>|</span><a href="#42686945">prev</a><span>|</span><a href="#42687060">next</a><span>|</span><label class="collapse" for="c-42686479">[-]</label><label class="expand" for="c-42686479">[1 more]</label></div><br/><div class="children"><div class="content">Utterly unremarkable for 2012 though.</div><br/></div></div></div></div><div id="42687060" class="c"><input type="checkbox" id="c-42687060" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#42686449">prev</a><span>|</span><a href="#42687075">next</a><span>|</span><label class="collapse" for="c-42687060">[-]</label><label class="expand" for="c-42687060">[15 more]</label></div><br/><div class="children"><div class="content">No, multi-armed bandit doesn&#x27;t &quot;beat&quot; A&#x2F;B testing, nor does it beat it &quot;every time&quot;.<p>Statistical significance is statistical significance, end of story. If you want to show that option B is better than A, then you need to test B enough times.<p>It doesn&#x27;t matter if you test it half the time (in the simplest A&#x2F;B) or 10% of the time (as suggested in the article). If you do it 10% of the time, it&#x27;s just going to take you five times longer.<p>And A&#x2F;B testing can handle multiple options just fine, contrary to the post. The name &quot;A&#x2F;B&quot; suggests two, but you&#x27;re free to use more, and this is extremely common. It&#x27;s still called &quot;A&#x2F;B testing&quot;.<p>Generally speaking, you want to find the best option and then <i>remove the other ones</i> because they&#x27;re suboptimal and code cruft. The author suggests <i>always</i> keeping 10% exploring other options. But if you already know they&#x27;re worse, that&#x27;s just making your product worse for those 10% of users.</div><br/><div id="42688034" class="c"><input type="checkbox" id="c-42688034" checked=""/><div class="controls bullet"><span class="by">LPisGood</span><span>|</span><a href="#42687060">parent</a><span>|</span><a href="#42688380">next</a><span>|</span><label class="collapse" for="c-42688034">[-]</label><label class="expand" for="c-42688034">[13 more]</label></div><br/><div class="children"><div class="content">Multi-arm bandit does beat A&#x2F;B testing in the sense that standard A&#x2F;B testing does not seek to maximize reward during the testing period, MAB does. MAB also generalizes better to testing many things than A&#x2F;B testing.</div><br/><div id="42688461" class="c"><input type="checkbox" id="c-42688461" checked=""/><div class="controls bullet"><span class="by">cle</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688034">parent</a><span>|</span><a href="#42688216">next</a><span>|</span><label class="collapse" for="c-42688461">[-]</label><label class="expand" for="c-42688461">[3 more]</label></div><br/><div class="children"><div class="content">This is a double-edged sword. There are often cases in real-world systems where the &quot;reward&quot; the MAB maximizes is biased by eligibility issues, system caching, bugs, etc. If this happens, your MAB has the potential to converge on the worst possible experience for your users, something a static treatment allocation won&#x27;t do.</div><br/><div id="42688504" class="c"><input type="checkbox" id="c-42688504" checked=""/><div class="controls bullet"><span class="by">LPisGood</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688461">parent</a><span>|</span><a href="#42688216">next</a><span>|</span><label class="collapse" for="c-42688504">[-]</label><label class="expand" for="c-42688504">[2 more]</label></div><br/><div class="children"><div class="content">I haven’t seen these particular shortcomings before, but I certainly agree that if your data is bad, this ML approach will also be bad.<p>Can you share some more details about your experiences with those particular types of failures?</div><br/><div id="42688888" class="c"><input type="checkbox" id="c-42688888" checked=""/><div class="controls bullet"><span class="by">cle</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688504">parent</a><span>|</span><a href="#42688216">next</a><span>|</span><label class="collapse" for="c-42688888">[-]</label><label class="expand" for="c-42688888">[1 more]</label></div><br/><div class="children"><div class="content">Sure! A really simple (and common) example would be a setup w&#x2F; treatment A and treatment B, your code does &quot;if session_assignment == A .... else .... B&quot; .  In the else branch you do something that for whatever reason causes misbehavior (perhaps it sometimes crashes or throws an exception or uses a buffer that drops records under high load to protect availability). That&#x27;s suprisingly common. Or perhaps you were hashing on the wrong key to generate session assignments--ex you accidentally used an ID that expires after 24 hours of inactivity...now only highly active people get correctly sampled.<p>Another common one I saw was due to different systems handling different treatments, and there being caching discrepancies between the two, like esp in a MAB where allocations are constantly changing, if one system has a much longer TTL than the other then you might see allocation lags for one treatment and not the other, biasing the data. Or perhaps one system deploys much more frequently and the load balancer draining doesn&#x27;t wait for records to finish uploading before it kills the process.<p>The most subtle ones were eligibility biases, where one treatment might cause users to drop out of an experiment entirely. Like if you have a signup form and you want to measure long-term retention, and one treatment causes some cohorts to not complete the signup entirely.<p>There are definitely mitigations for these issues, like you can monitor the expected vs. actual allocations and alert if they go out-of-whack. That has its own set of problems and statistics though.</div><br/></div></div></div></div></div></div><div id="42688216" class="c"><input type="checkbox" id="c-42688216" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688034">parent</a><span>|</span><a href="#42688461">prev</a><span>|</span><a href="#42692120">next</a><span>|</span><label class="collapse" for="c-42688216">[-]</label><label class="expand" for="c-42688216">[7 more]</label></div><br/><div class="children"><div class="content">No -- you can&#x27;t have your cake and eat it too.<p>You get <i>zero</i> benefits from MAB over A&#x2F;B if you simply end your A&#x2F;B test once you&#x27;ve achieved statistical significance and pick the best option. Which is what any efficient A&#x2F;B test does -- there no reason to have any fixed &quot;testing period&quot; beyond what is needed to achieve statistical significance.<p>While, to the contrary, the MAB described in the article does <i>not</i> maximize reward -- as I explained in my previous comment. Because the post&#x27;s version runs indefinitely, it has <i>worse</i> long-term reward because it continues to test inferior options long after they&#x27;ve been proven worse. If you leave it running, you&#x27;re harming yourself.<p>And I have no idea what you mean by MAB &quot;generalizing&quot; more. But it doesn&#x27;t matter if it&#x27;s worse to begin with.<p>(Also, it&#x27;s a huge red flag that the post doesn&#x27;t even <i>mention</i> statistical significance.)</div><br/><div id="42688355" class="c"><input type="checkbox" id="c-42688355" checked=""/><div class="controls bullet"><span class="by">LPisGood</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688216">parent</a><span>|</span><a href="#42692120">next</a><span>|</span><label class="collapse" for="c-42688355">[-]</label><label class="expand" for="c-42688355">[6 more]</label></div><br/><div class="children"><div class="content">&gt; you can&#x27;t have your cake and eat it too<p>I disagree. There is a vast array of literature on solving the MAB problem that may as well be grouped into a bin called “how to optimally strike a balance between having one’s cake and eating it too.”<p>The optimization techniques to solve MAB problem seek to optimize reward by giving the right balance of exploration and exploitation. In other words, these techniques attempt to determine the optimal way to strike a balance between exploring if another option is better and exploiting the option currently predicted to be best.<p>There is a strong reason this literature doesn’t start and end with: “just do A&#x2F;B testing, there is no better approach”</div><br/><div id="42688566" class="c"><input type="checkbox" id="c-42688566" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688355">parent</a><span>|</span><a href="#42688701">next</a><span>|</span><label class="collapse" for="c-42688566">[-]</label><label class="expand" for="c-42688566">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not talking about the literature -- I&#x27;m talking about the extremely simplistic and sub-optimal procedure described in the post.<p>If you want to get sophisticated, MAB properly done is essentially just A&#x2F;B testing with optimal strategies for deciding when to end individual A&#x2F;B tests, or balancing tests optimally for a limited number of trials. But again, it doesn&#x27;t &quot;beat&quot; A&#x2F;B testing -- it <i>is</i> A&#x2F;B testing in that sense.<p>And that&#x27;s what I mean. You can&#x27;t magically increase your reward while simultaneously getting statistically significant results. Either your results are significant to a desired level or not, and there&#x27;s no getting around the number of samples you need to achieve that.</div><br/><div id="42688772" class="c"><input type="checkbox" id="c-42688772" checked=""/><div class="controls bullet"><span class="by">LPisGood</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688566">parent</a><span>|</span><a href="#42688701">next</a><span>|</span><label class="collapse" for="c-42688772">[-]</label><label class="expand" for="c-42688772">[1 more]</label></div><br/><div class="children"><div class="content">I am talking about the literature which solves MAB in a variety of ways, including the one in the post.<p>&gt; MAB properly done is essentially just A&#x2F;B testing<p>Words are only useful insofar as their meanings invoke ideas, and in my experience absolutely no one thinks of other MAB strategies when someone talks about A&#x2F;B testing.<p>Sure, you can classify A&#x2F;B testing as one extremely suboptimal approach to solving MAB problem. This classification doesn’t help much though, because the other MAB techniques do “magically increase the rewards” compared this simple technique.</div><br/></div></div></div></div><div id="42688701" class="c"><input type="checkbox" id="c-42688701" checked=""/><div class="controls bullet"><span class="by">cauch</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688355">parent</a><span>|</span><a href="#42688566">prev</a><span>|</span><a href="#42692120">next</a><span>|</span><label class="collapse" for="c-42688701">[-]</label><label class="expand" for="c-42688701">[3 more]</label></div><br/><div class="children"><div class="content">Another way of seeing the situation: let run your MAB solution for a while. Orange has been tested 17 times and blue has been tested 12 times. This is exactly equivalent of doing a A&#x2F;B testing where you display 1 time the orange button to 17 persons and 1 time the blue button to 12 persons.<p>The trick is to find the exact best number of test for each color so that we have good statistical significance. MAB does not do that well, as you cannot easily force testing an option that was bad when this option did not get enough trial to have a good statistical significance (imagine you have 10 colors and the color orange first score 0&#x2F;1. It will take a very long while before this color will be re-tested quite significantly: you need to first fall into the 10%, but then you still have ~10% to randomly pick this color and not one of the other). With A&#x2F;B testing, you can do a power analysis before hand (or whenever during) to know when to stop.<p>Literature does not start with &quot;just do A&#x2F;B testing&quot; because it is not the same problem. In MAB, your goal is not to demonstrate that one is bad, it&#x27;s to do your own decision when faced with a fixed situation.</div><br/><div id="42688822" class="c"><input type="checkbox" id="c-42688822" checked=""/><div class="controls bullet"><span class="by">LPisGood</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688701">parent</a><span>|</span><a href="#42692120">next</a><span>|</span><label class="collapse" for="c-42688822">[-]</label><label class="expand" for="c-42688822">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The trick is to find the exact best number of test for each color so that we have good statistical significance<p>Yes, A&#x2F;B testing will force through enough trials to get statistical significance(it is definitely a “exploration first strategy), but in many cases, you care about maximizing reward as well, in particular during testing. A&#x2F;B testing does very poorly at balancing exploitation with exploitation in general.<p>This is especially true if the situation is dynamic. Will you A&#x2F;B test forever in case something has changed and give up that long term loss in reward value?</div><br/><div id="42690241" class="c"><input type="checkbox" id="c-42690241" checked=""/><div class="controls bullet"><span class="by">cauch</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688822">parent</a><span>|</span><a href="#42692120">next</a><span>|</span><label class="collapse" for="c-42690241">[-]</label><label class="expand" for="c-42690241">[1 more]</label></div><br/><div class="children"><div class="content">But the proposed MAB system does not even propose a method to know when this system needs to be stopped (and remove all the choices except the best one).<p>With the A&#x2F;B testing, you can do power analysis whenever you want, including in the middle of the experiment. It will just be an iterative adjustment that converges.<p>In fact, you can even run on all possibilities in advance (if A get 1% and B get 1%, how many A and B do I need, if A get 2% and B get 1%, if A get 3% and B get 1%, ...) and it will give you the exact boundaries to stop for any configurations before even running the experiment. You will just have to stop trialing option A as soon as option A crosses the already decided significance threshold for A.<p>So, no, the A&#x2F;B testing will never run forever. And A&#x2F;B testing will always be better than the MAB solution, because you will have a better way to stop trying a bad solution as soon as you have crossed the threshold you decided is enough to consider it&#x27;s a bad solution.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42692120" class="c"><input type="checkbox" id="c-42692120" checked=""/><div class="controls bullet"><span class="by">ertdfgcvb</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42688034">parent</a><span>|</span><a href="#42688216">prev</a><span>|</span><a href="#42688380">next</a><span>|</span><label class="collapse" for="c-42692120">[-]</label><label class="expand" for="c-42692120">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that the point of testing (to not maximize reward but rather wait and collect data)? It sounds like maximizing reward during the experiment period can bias the results</div><br/><div id="42692975" class="c"><input type="checkbox" id="c-42692975" checked=""/><div class="controls bullet"><span class="by">LPisGood</span><span>|</span><a href="#42687060">root</a><span>|</span><a href="#42692120">parent</a><span>|</span><a href="#42688380">next</a><span>|</span><label class="collapse" for="c-42692975">[-]</label><label class="expand" for="c-42692975">[1 more]</label></div><br/><div class="children"><div class="content">The great thing is that you can do both.</div><br/></div></div></div></div></div></div></div></div><div id="42687075" class="c"><input type="checkbox" id="c-42687075" checked=""/><div class="controls bullet"><span class="by">jbentley1</span><span>|</span><a href="#42687060">prev</a><span>|</span><a href="#42694020">next</a><span>|</span><label class="collapse" for="c-42687075">[-]</label><label class="expand" for="c-42687075">[5 more]</label></div><br/><div class="children"><div class="content">Multi-armed bandits make a big assumption that effectiveness is static over time. What can happen is that if they tip traffic slightly towards option B at a time when effectiveness is higher (maybe a sale just started) B will start to overwhelmingly look like a winner and get locked in that state.<p>You can solve this with propensity scores, but it is more complicated to implement and you need to log every interaction.</div><br/><div id="42688055" class="c"><input type="checkbox" id="c-42688055" checked=""/><div class="controls bullet"><span class="by">LPisGood</span><span>|</span><a href="#42687075">parent</a><span>|</span><a href="#42694020">next</a><span>|</span><label class="collapse" for="c-42688055">[-]</label><label class="expand" for="c-42688055">[4 more]</label></div><br/><div class="children"><div class="content">This objection is mentioned specifically in the post.<p>You can add a forgetting factor for older results.</div><br/><div id="42688403" class="c"><input type="checkbox" id="c-42688403" checked=""/><div class="controls bullet"><span class="by">randomcatuser</span><span>|</span><a href="#42687075">root</a><span>|</span><a href="#42688055">parent</a><span>|</span><a href="#42693202">next</a><span>|</span><label class="collapse" for="c-42688403">[-]</label><label class="expand" for="c-42688403">[2 more]</label></div><br/><div class="children"><div class="content">This seems like a fudge factor though. Some things are changed bc you act on them! (e.g. recommendation systems that are biased towards more popular content). So having dynamic groups makes the data harder to analyze</div><br/><div id="42688474" class="c"><input type="checkbox" id="c-42688474" checked=""/><div class="controls bullet"><span class="by">LPisGood</span><span>|</span><a href="#42687075">root</a><span>|</span><a href="#42688403">parent</a><span>|</span><a href="#42693202">next</a><span>|</span><label class="collapse" for="c-42688474">[-]</label><label class="expand" for="c-42688474">[1 more]</label></div><br/><div class="children"><div class="content">A standard formulation of MAB problem assumes that acting will impact the rewards, and this forgetting factor approach is one which allows for that and still attempts to find the currently most exploitable lever.</div><br/></div></div></div></div><div id="42693202" class="c"><input type="checkbox" id="c-42693202" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#42687075">root</a><span>|</span><a href="#42688055">parent</a><span>|</span><a href="#42688403">prev</a><span>|</span><a href="#42694020">next</a><span>|</span><label class="collapse" for="c-42693202">[-]</label><label class="expand" for="c-42693202">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a different problem. In jbentley1&#x27;s scenario, A could be better, but this algorithm will choose B.</div><br/></div></div></div></div></div></div><div id="42694020" class="c"><input type="checkbox" id="c-42694020" checked=""/><div class="controls bullet"><span class="by">orasis</span><span>|</span><a href="#42687075">prev</a><span>|</span><a href="#42691213">next</a><span>|</span><label class="collapse" for="c-42694020">[-]</label><label class="expand" for="c-42694020">[1 more]</label></div><br/><div class="children"><div class="content">Check out improve.ai if you want to see this taken to the next level. We combined Thompson Sampling with XGBoost to build a multi-armed bandit that learns to choose the best arm across <i>context</i>. MIT license.</div><br/></div></div><div id="42691213" class="c"><input type="checkbox" id="c-42691213" checked=""/><div class="controls bullet"><span class="by">bartread</span><span>|</span><a href="#42694020">prev</a><span>|</span><a href="#42686346">next</a><span>|</span><label class="collapse" for="c-42691213">[-]</label><label class="expand" for="c-42691213">[2 more]</label></div><br/><div class="children"><div class="content">Interesting post - certainly I can see myself wanting to tinker with Epsilon greedy - but the comments at the bottom are pretty off the chain, and many not in a good way. No auth commenting is certainly a brave decision in 2024.</div><br/><div id="42692806" class="c"><input type="checkbox" id="c-42692806" checked=""/><div class="controls bullet"><span class="by">codazoda</span><span>|</span><a href="#42691213">parent</a><span>|</span><a href="#42686346">next</a><span>|</span><label class="collapse" for="c-42692806">[-]</label><label class="expand" for="c-42692806">[1 more]</label></div><br/><div class="children"><div class="content">I wish I could make this work without the mess though. Maybe an AI moderator could throw some of these away. Not that we need AI for everything, but I don’t have time to edit comments on my blog.</div><br/></div></div></div></div><div id="42686346" class="c"><input type="checkbox" id="c-42686346" checked=""/><div class="controls bullet"><span class="by">asdasdsddd</span><span>|</span><a href="#42691213">prev</a><span>|</span><a href="#42686366">next</a><span>|</span><label class="collapse" for="c-42686346">[-]</label><label class="expand" for="c-42686346">[3 more]</label></div><br/><div class="children"><div class="content">Multi arm bandits are fine but their limited to tests where its ok to switch users between arms frequently and tests that have more power</div><br/><div id="42686758" class="c"><input type="checkbox" id="c-42686758" checked=""/><div class="controls bullet"><span class="by">tantalor</span><span>|</span><a href="#42686346">parent</a><span>|</span><a href="#42686366">next</a><span>|</span><label class="collapse" for="c-42686758">[-]</label><label class="expand" for="c-42686758">[2 more]</label></div><br/><div class="children"><div class="content">&gt; where its ok to switch users between arms frequently<p>It&#x27;s not hard to keep track of which arm any given user was exposed to in the first run, and then repeat it.</div><br/><div id="42687087" class="c"><input type="checkbox" id="c-42687087" checked=""/><div class="controls bullet"><span class="by">asdasdsddd</span><span>|</span><a href="#42686346">root</a><span>|</span><a href="#42686758">parent</a><span>|</span><a href="#42686366">next</a><span>|</span><label class="collapse" for="c-42687087">[-]</label><label class="expand" for="c-42687087">[1 more]</label></div><br/><div class="children"><div class="content">There are often product limitations</div><br/></div></div></div></div></div></div><div id="42686366" class="c"><input type="checkbox" id="c-42686366" checked=""/><div class="controls bullet"><span class="by">85392_school</span><span>|</span><a href="#42686346">prev</a><span>|</span><a href="#42692886">next</a><span>|</span><label class="collapse" for="c-42686366">[-]</label><label class="expand" for="c-42686366">[1 more]</label></div><br/><div class="children"><div class="content">Previously discussed:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=11437114">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=11437114</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=4040022">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=4040022</a></div><br/></div></div><div id="42692886" class="c"><input type="checkbox" id="c-42692886" checked=""/><div class="controls bullet"><span class="by">rmacqueen</span><span>|</span><a href="#42686366">prev</a><span>|</span><a href="#42692761">next</a><span>|</span><label class="collapse" for="c-42692886">[-]</label><label class="expand" for="c-42692886">[1 more]</label></div><br/><div class="children"><div class="content">How do you ensure the same user always gets the same treatment, even on subsequent visits to the site? You need the bucket sizes to be consistent for consistent hashing.</div><br/></div></div><div id="42692761" class="c"><input type="checkbox" id="c-42692761" checked=""/><div class="controls bullet"><span class="by">spmartin823</span><span>|</span><a href="#42692886">prev</a><span>|</span><a href="#42692099">next</a><span>|</span><label class="collapse" for="c-42692761">[-]</label><label class="expand" for="c-42692761">[2 more]</label></div><br/><div class="children"><div class="content">Why would I introduce randomness where I don&#x27;t have to? I&#x27;d regret this as soon as I had to debug something related to it.</div><br/><div id="42694522" class="c"><input type="checkbox" id="c-42694522" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#42692761">parent</a><span>|</span><a href="#42692099">next</a><span>|</span><label class="collapse" for="c-42694522">[-]</label><label class="expand" for="c-42694522">[1 more]</label></div><br/><div class="children"><div class="content">Regular A&#x2F;B testing has the same kind of randomness. But this is more complex for other reasons.</div><br/></div></div></div></div><div id="42692099" class="c"><input type="checkbox" id="c-42692099" checked=""/><div class="controls bullet"><span class="by">ertdfgcvb</span><span>|</span><a href="#42692761">prev</a><span>|</span><a href="#42686821">next</a><span>|</span><label class="collapse" for="c-42692099">[-]</label><label class="expand" for="c-42692099">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a layman. Isn&#x27;t MAB changing the experiment parameters while the experiment is still running? That sounds like an easy way towards biased experiment results</div><br/><div id="42692209" class="c"><input type="checkbox" id="c-42692209" checked=""/><div class="controls bullet"><span class="by">tjbai</span><span>|</span><a href="#42692099">parent</a><span>|</span><a href="#42686821">next</a><span>|</span><label class="collapse" for="c-42692209">[-]</label><label class="expand" for="c-42692209">[1 more]</label></div><br/><div class="children"><div class="content">From a purely technical definition of bias (difference in expected value of the estimator and the true value), MAB is not biased because &quot;changing the experiment parameters&quot; is just dynamically allocating a different sample size to each of the estimators, so the estimator still converges to the correct value.<p>You are correct that this setup can potentially mislead you, but this is because you might end up getting estimators with high variance. So, you might mistakenly see some early promising results for experiment group A and greedily assign all the requests to that group, even though it is not guaranteed that A is actually better than B.<p>This is the famous exploration-exploitation dilemma—should you maximize conversions by diverting everyone to group A or still try to collect more data from group B?</div><br/></div></div></div></div><div id="42686821" class="c"><input type="checkbox" id="c-42686821" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#42692099">prev</a><span>|</span><a href="#42687215">next</a><span>|</span><label class="collapse" for="c-42686821">[-]</label><label class="expand" for="c-42686821">[1 more]</label></div><br/><div class="children"><div class="content">If your aim is to evaluate an effect size of your treatment because you want to know whether it’s significant, you can’t do what the article advises.</div><br/></div></div><div id="42687215" class="c"><input type="checkbox" id="c-42687215" checked=""/><div class="controls bullet"><span class="by">JensRantil</span><span>|</span><a href="#42686821">prev</a><span>|</span><a href="#42689335">next</a><span>|</span><label class="collapse" for="c-42687215">[-]</label><label class="expand" for="c-42687215">[1 more]</label></div><br/><div class="children"><div class="content">Yep. I implemented this as a Java library a while ago (and other stuff): <a href="https:&#x2F;&#x2F;github.com&#x2F;JensRantil&#x2F;java-canary-tools">https:&#x2F;&#x2F;github.com&#x2F;JensRantil&#x2F;java-canary-tools</a> Basically, I think feature flags and A&#x2F;B tests aren&#x27;t always needed to roll out experiments.</div><br/></div></div><div id="42689335" class="c"><input type="checkbox" id="c-42689335" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#42687215">prev</a><span>|</span><a href="#42688344">next</a><span>|</span><label class="collapse" for="c-42689335">[-]</label><label class="expand" for="c-42689335">[3 more]</label></div><br/><div class="children"><div class="content">This is fine as long as your users don&#x27;t mind your site randomly changing all the time.</div><br/><div id="42689614" class="c"><input type="checkbox" id="c-42689614" checked=""/><div class="controls bullet"><span class="by">data-ottawa</span><span>|</span><a href="#42689335">parent</a><span>|</span><a href="#42691195">next</a><span>|</span><label class="collapse" for="c-42689614">[-]</label><label class="expand" for="c-42689614">[1 more]</label></div><br/><div class="children"><div class="content">That’s also a problem for AB testing and solvable (to a degree) by caching assignments</div><br/></div></div><div id="42691195" class="c"><input type="checkbox" id="c-42691195" checked=""/><div class="controls bullet"><span class="by">ryan-duve</span><span>|</span><a href="#42689335">parent</a><span>|</span><a href="#42689614">prev</a><span>|</span><a href="#42688344">next</a><span>|</span><label class="collapse" for="c-42691195">[-]</label><label class="expand" for="c-42691195">[1 more]</label></div><br/><div class="children"><div class="content">This can be addressed with some variant of<p><pre><code>    random.seed(hash(user_id))

</code></pre>
I think the bigger problem is handling the fact that not all users click through the same number of times.</div><br/></div></div></div></div><div id="42688344" class="c"><input type="checkbox" id="c-42688344" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#42689335">prev</a><span>|</span><a href="#42686392">next</a><span>|</span><label class="collapse" for="c-42688344">[-]</label><label class="expand" for="c-42688344">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Statistics are hard for most people to understand.</i><p>True, but that&#x27;s exactly what statistics helps with, though also hard to understand. :)</div><br/><div id="42694530" class="c"><input type="checkbox" id="c-42694530" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#42688344">parent</a><span>|</span><a href="#42686392">next</a><span>|</span><label class="collapse" for="c-42694530">[-]</label><label class="expand" for="c-42694530">[1 more]</label></div><br/><div class="children"><div class="content">People are hard for statistics to understand.</div><br/></div></div></div></div><div id="42686392" class="c"><input type="checkbox" id="c-42686392" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#42688344">prev</a><span>|</span><a href="#42686967">next</a><span>|</span><label class="collapse" for="c-42686392">[-]</label><label class="expand" for="c-42686392">[3 more]</label></div><br/><div class="children"><div class="content">&quot;People distrust things that they do not understand, and they especially distrust machine learning algorithms, even if they are simple.&quot;<p>How times have changed :)</div><br/><div id="42687187" class="c"><input type="checkbox" id="c-42687187" checked=""/><div class="controls bullet"><span class="by">saintfire</span><span>|</span><a href="#42686392">parent</a><span>|</span><a href="#42686967">next</a><span>|</span><label class="collapse" for="c-42687187">[-]</label><label class="expand" for="c-42687187">[2 more]</label></div><br/><div class="children"><div class="content">Just had to anthropomorphize machine learning.</div><br/><div id="42691068" class="c"><input type="checkbox" id="c-42691068" checked=""/><div class="controls bullet"><span class="by">iforgot22</span><span>|</span><a href="#42686392">root</a><span>|</span><a href="#42687187">parent</a><span>|</span><a href="#42686967">next</a><span>|</span><label class="collapse" for="c-42691068">[-]</label><label class="expand" for="c-42691068">[1 more]</label></div><br/><div class="children"><div class="content">More like, had to make it really good</div><br/></div></div></div></div></div></div><div id="42686967" class="c"><input type="checkbox" id="c-42686967" checked=""/><div class="controls bullet"><span class="by">HeliumHydride</span><span>|</span><a href="#42686392">prev</a><span>|</span><a href="#42689986">next</a><span>|</span><label class="collapse" for="c-42686967">[-]</label><label class="expand" for="c-42686967">[2 more]</label></div><br/><div class="children"><div class="content">The &quot;20&quot; is missing from the title.</div><br/><div id="42687228" class="c"><input type="checkbox" id="c-42687228" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#42686967">parent</a><span>|</span><a href="#42689986">next</a><span>|</span><label class="collapse" for="c-42687228">[-]</label><label class="expand" for="c-42687228">[1 more]</label></div><br/><div class="children"><div class="content">I believe hacker news automatically truncates the number at the beginning of titles</div><br/></div></div></div></div><div id="42689986" class="c"><input type="checkbox" id="c-42689986" checked=""/><div class="controls bullet"><span class="by">fitsumbelay</span><span>|</span><a href="#42686967">prev</a><span>|</span><a href="#42690151">next</a><span>|</span><label class="collapse" for="c-42689986">[-]</label><label class="expand" for="c-42689986">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only read the first paragraph so bear with me but I&#x27;m not understanding the reasoning behind &quot;A&#x2F;B testing drugs is bad because only half of the sample can potentially benefit&quot; when the whole point is to delineate the gots and got-nots ...</div><br/><div id="42690092" class="c"><input type="checkbox" id="c-42690092" checked=""/><div class="controls bullet"><span class="by">atombender</span><span>|</span><a href="#42689986">parent</a><span>|</span><a href="#42690151">next</a><span>|</span><label class="collapse" for="c-42690092">[-]</label><label class="expand" for="c-42690092">[3 more]</label></div><br/><div class="children"><div class="content">If the drug is effective and safe, then one half of the patients lost out on the benefit. You are intentionally &quot;sacrificing&quot; the control arm.<p>(Of course, the whole point is that the benefit and safety are not certain, so I think the term &quot;sacrifice&quot; used in the article is misleading.)</div><br/><div id="42690438" class="c"><input type="checkbox" id="c-42690438" checked=""/><div class="controls bullet"><span class="by">kridsdale1</span><span>|</span><a href="#42689986">root</a><span>|</span><a href="#42690092">parent</a><span>|</span><a href="#42690151">next</a><span>|</span><label class="collapse" for="c-42690438">[-]</label><label class="expand" for="c-42690438">[2 more]</label></div><br/><div class="children"><div class="content">And the control group is also sacrificed from potentially deadly side effects.</div><br/><div id="42690735" class="c"><input type="checkbox" id="c-42690735" checked=""/><div class="controls bullet"><span class="by">rogerrogerr</span><span>|</span><a href="#42689986">root</a><span>|</span><a href="#42690438">parent</a><span>|</span><a href="#42690151">next</a><span>|</span><label class="collapse" for="c-42690735">[-]</label><label class="expand" for="c-42690735">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is they usually do small trials early where they figure out if there are deadly side effects, and then do larger effectiveness trials once there’s determined to be minimal danger. So being in the control group is probably a negative thing on average.</div><br/></div></div></div></div></div></div></div></div><div id="42690151" class="c"><input type="checkbox" id="c-42690151" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42689986">prev</a><span>|</span><a href="#42691769">next</a><span>|</span><label class="collapse" for="c-42690151">[-]</label><label class="expand" for="c-42690151">[1 more]</label></div><br/><div class="children"><div class="content">If you only keep your entire site static while test one variable change at a time, it could be statistically significant, other wise if your flow changes some  where while you do this algo, it may be misleading you into a color and then under perform because you&#x27;ve made a change else where before users get to this page.</div><br/></div></div><div id="42691769" class="c"><input type="checkbox" id="c-42691769" checked=""/><div class="controls bullet"><span class="by">groby_b</span><span>|</span><a href="#42690151">prev</a><span>|</span><label class="collapse" for="c-42691769">[-]</label><label class="expand" for="c-42691769">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a statistically valid approach. Technically correct, the best kind of correct.<p>Meanwhile, if your users get presented a different button whenever they come by, because the MAB is still pursuing its hill climbing, they&#x27;ll rightfully accuse you of having extremely crappy UX. (And, sure, you can have MAB with user stickiness, but now you <i>do</i> need to talk about sampling bias)<p>And MAB hill climb doesn&#x27;t work at all if you want to measure the <i>long-term</i> reward of a variation. You have no idea if the orange button has long-term retention impact. There are sure situations where you&#x27;d like to know.<p>Yes, it&#x27;s a neat technique to have in your repertoire, but like any given technique, it&#x27;s not the answer &quot;every time&quot;.</div><br/><div id="42694542" class="c"><input type="checkbox" id="c-42694542" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#42691769">parent</a><span>|</span><label class="collapse" for="c-42694542">[-]</label><label class="expand" for="c-42694542">[1 more]</label></div><br/><div class="children"><div class="content">A&#x2F;B testing has the same problem unless you figure out how to treat the same user the same way each time. But yeah I generally don&#x27;t give much credence to an assertion like this that isn&#x27;t based on a real-world experience. It&#x27;s not even like &quot;this algo might help,&quot; it&#x27;s &quot;will beat A&#x2F;B testing every time.&quot;</div><br/></div></div></div></div></div></div></div></div></div></body></html>