<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706518866943" as="style"/><link rel="stylesheet" href="styles.css?v=1706518866943"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers">Eagle 7B: Soaring past Transformers</a> <span class="domain">(<a href="https://blog.rwkv.com">blog.rwkv.com</a>)</span></div><div class="subtext"><span>guybedo</span> | <span>46 comments</span></div><br/><div><div id="39173025" class="c"><input type="checkbox" id="c-39173025" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#39173120">next</a><span>|</span><label class="collapse" for="c-39173025">[-]</label><label class="expand" for="c-39173025">[10 more]</label></div><br/><div class="children"><div class="content">It’s cool that progress is being made on alternative LLM architectures, and I did upvote the link.<p>However, I found this article somewhat frustrating. Showing the quality of the model is only half of the story, but the article suddenly ends there. If people are going to be motivated to adopt an entirely different architecture, then performance and context size deserve at least as much discussion.<p>Given the linear nature being shown, it seems like the primary thing people are going to want to see is the next frontier of LLMs: context sizes of ~1M tokens. The word “context” does not even appear in this article, which is disappointing. If there were a discussion of context, it would be nice to see if it passes the passkey test.<p>The article also appears to reuse a chart from RWKV-4 showing how awesome a linear function is compared to a quadratic one, but… cool story? It’s not even clear what this chart is truly showing. Is this chart <i>only</i> showing generated tokens, or is this including prompt tokens? As I have never used RWKV, I have no idea how the prompt processing speed compares to the token generation speed. Prompt processing speed has been a big problem for Mixtral, for example.<p>As a reader, I want to see a couple of actual examples of X prompt tokens + Y generated tokens, and the tokens&#x2F;s of X and Y for RKWV-5 and for Mistral on the same hardware. On the Mistral side, it is trivial to collect this information in llama.cpp, but I don’t know how the tooling is for RWKV.</div><br/><div id="39173277" class="c"><input type="checkbox" id="c-39173277" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#39173025">parent</a><span>|</span><a href="#39173197">next</a><span>|</span><label class="collapse" for="c-39173277">[-]</label><label class="expand" for="c-39173277">[1 more]</label></div><br/><div class="children"><div class="content">These models don&#x27;t have a fixed context size and are progressively fine-tuned for longer and longer contexts.  The context length also doesn&#x27;t impact inference cost.<p>Another aspect of performance is not just how well does the trained model perform, but is it data efficient (performance per token trained)? The comparison with Pythia (an open GPT) is shown in the article.<p>The rwkv4 paper is quite detailed and has examples of prompt and responses on the last few pages<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.13048" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.13048</a><p>And iirc rwkv5 is very similar to retnet which is detailed here<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.08621" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.08621</a><p><i>Edit</i> now that I thought more about, the data efficiency seems like a highly important aspect given their noble goal to be fully multi lingual.  This is fairly interesting theoretically as well and for other applications where abundance of data is not a given</div><br/></div></div><div id="39173197" class="c"><input type="checkbox" id="c-39173197" checked=""/><div class="controls bullet"><span class="by">Harrisonv</span><span>|</span><a href="#39173025">parent</a><span>|</span><a href="#39173277">prev</a><span>|</span><a href="#39173243">next</a><span>|</span><label class="collapse" for="c-39173197">[-]</label><label class="expand" for="c-39173197">[1 more]</label></div><br/><div class="children"><div class="content">For linear transformers, the current metric is &quot;perfect token recall&quot;, the ability for the model to recall a randomized sequence of data. You can find the limit of a particular model architecture by training a model of a particular size to echo randomized data, and I believe this was touched on in the zoo-ology paper.<p>This doesnt prevent the model from retaining sequences or information beyond this metric, as information can easily be compressed in the state, but it anything within that window can be perfectly recalled by the model.<p>Internal testing has placed the value for Eagle around the 2.5k ptr[perfect token recall] mark, while community fine tunes done on the partial checkpoints for long distance information gathering and memorization have been shown to easily dwarf that.<p>prompt processing speed benefits from the same gemm optimizations as standard transformers, with the extra benefit of those gemm optimizations working for batch inference as well (no need for vllm as memory allocation is static per agent)</div><br/></div></div><div id="39173243" class="c"><input type="checkbox" id="c-39173243" checked=""/><div class="controls bullet"><span class="by">Szpadel</span><span>|</span><a href="#39173025">parent</a><span>|</span><a href="#39173197">prev</a><span>|</span><a href="#39173120">next</a><span>|</span><label class="collapse" for="c-39173243">[-]</label><label class="expand" for="c-39173243">[7 more]</label></div><br/><div class="children"><div class="content">RWKV does not have context size, or in other way do look at it, it does have infinite one.<p>As far as I understand this, there is internal state that holds new information while reading input, later information can overwrite previous ones with is arguably human like behaviour.</div><br/><div id="39173597" class="c"><input type="checkbox" id="c-39173597" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39173025">root</a><span>|</span><a href="#39173243">parent</a><span>|</span><a href="#39173396">next</a><span>|</span><label class="collapse" for="c-39173597">[-]</label><label class="expand" for="c-39173597">[2 more]</label></div><br/><div class="children"><div class="content">One of three things has to be true. Either:<p>a) this is false<p>b) perfect recall is false (ie. as the internal state is overwritten, you lose information about previous entries in the context)<p>c) the inference time scales by the context length.<p>It’s not possible to have perfect recall over an arbitrary length in fixed time.<p>Not hard. <i>Totally not possible at all</i><p>That would mean you can scan an infinite amount of data perfectly in fixed time.<p>So… Hrm… this kind of claim rings some kind of alarm bells, when it’s combined with this kind of sweeping announcement.<p>It seems to good to true; either it’s not that good, or the laws of the universe no longer hold true.</div><br/><div id="39173682" class="c"><input type="checkbox" id="c-39173682" checked=""/><div class="controls bullet"><span class="by">Harrisonv</span><span>|</span><a href="#39173025">root</a><span>|</span><a href="#39173597">parent</a><span>|</span><a href="#39173396">next</a><span>|</span><label class="collapse" for="c-39173682">[-]</label><label class="expand" for="c-39173682">[1 more]</label></div><br/><div class="children"><div class="content">perfect recall is often a function of the architecture allowing for data to bleed through linkages. you can increase the perfect token recall through dialated wavenet structures, or, in the case of v5, the use of multi-head linear attention creates multiple pathways where information can skip forward in time</div><br/></div></div></div></div><div id="39173396" class="c"><input type="checkbox" id="c-39173396" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#39173025">root</a><span>|</span><a href="#39173243">parent</a><span>|</span><a href="#39173597">prev</a><span>|</span><a href="#39173518">next</a><span>|</span><label class="collapse" for="c-39173396">[-]</label><label class="expand" for="c-39173396">[2 more]</label></div><br/><div class="children"><div class="content">If later input overwrites previous input in the internal state, it means the model does have a limit to how much input it can &quot;remember&quot; at any given time and that limit is less than infinite.</div><br/><div id="39173606" class="c"><input type="checkbox" id="c-39173606" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39173025">root</a><span>|</span><a href="#39173396">parent</a><span>|</span><a href="#39173518">next</a><span>|</span><label class="collapse" for="c-39173606">[-]</label><label class="expand" for="c-39173606">[1 more]</label></div><br/><div class="children"><div class="content">You can think of it like your own memory. Can you remember a very important thing from 10 years ago? Can you remember every single thing since then? Some things will remain for basically infinite period, some will have a more limited scope.</div><br/></div></div></div></div><div id="39173518" class="c"><input type="checkbox" id="c-39173518" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#39173025">root</a><span>|</span><a href="#39173243">parent</a><span>|</span><a href="#39173396">prev</a><span>|</span><a href="#39173372">next</a><span>|</span><label class="collapse" for="c-39173518">[-]</label><label class="expand" for="c-39173518">[1 more]</label></div><br/><div class="children"><div class="content">In principle it has no context size limit, but (last time I checked) in practice there is one for implementation reasons.</div><br/></div></div><div id="39173372" class="c"><input type="checkbox" id="c-39173372" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#39173025">root</a><span>|</span><a href="#39173243">parent</a><span>|</span><a href="#39173518">prev</a><span>|</span><a href="#39173120">next</a><span>|</span><label class="collapse" for="c-39173372">[-]</label><label class="expand" for="c-39173372">[1 more]</label></div><br/><div class="children"><div class="content">There’s a difference between the computation requirements of long context lengths and the accuracy of the model on long context length tasks.</div><br/></div></div></div></div></div></div><div id="39173120" class="c"><input type="checkbox" id="c-39173120" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39173025">prev</a><span>|</span><a href="#39172950">next</a><span>|</span><label class="collapse" for="c-39173120">[-]</label><label class="expand" for="c-39173120">[9 more]</label></div><br/><div class="children"><div class="content">This shows the model architecture, be it transformer, Mamba, SSM or RWKV - doesn&#x27;t really matter when compared to the impact of the training set. We&#x27;re spending too much time debating models when we should be talking about language data, a reservoir of human experience won at great sacrifice by humanity.<p>And the same data when used to train humans creates modern capable people. Alone, without society and language, we would be mere shadows of ourselves. What does it say when AI acquires so many capabilities from language data? maybe intelligence was not centered in the brain. It&#x27;s a social process.</div><br/><div id="39173534" class="c"><input type="checkbox" id="c-39173534" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39173120">parent</a><span>|</span><a href="#39173184">next</a><span>|</span><label class="collapse" for="c-39173534">[-]</label><label class="expand" for="c-39173534">[1 more]</label></div><br/><div class="children"><div class="content">I agree that on balance, we should spend more effort on data than modeling, but it is just not true that modeling doesn&#x27;t matter. Transformer-2023 is different from Trnasformer-2020 and cumulative improvement is significant. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752</a> did such benchmark.<p>If choice between Transformer and RWKV doesn&#x27;t seem to matter to you, the only reason is that while Transformer-2020 evolved to Transformer-2023, RWKV-v1 (which is from 2021) also evolved to RWKV-v5. If you use Transformer-2020 or RWKV-v1 today you will feel the difference.</div><br/></div></div><div id="39173184" class="c"><input type="checkbox" id="c-39173184" checked=""/><div class="controls bullet"><span class="by">blackoil</span><span>|</span><a href="#39173120">parent</a><span>|</span><a href="#39173534">prev</a><span>|</span><a href="#39173169">next</a><span>|</span><label class="collapse" for="c-39173184">[-]</label><label class="expand" for="c-39173184">[1 more]</label></div><br/><div class="children"><div class="content">&gt; maybe intelligence was not centered in the brain. It&#x27;s a social process.<p>Is that controversial? We are stand on the shoulder of giants before us and that is why we insist on training younglings for couple of decades on past learnings before they are believed to be of any useful. Even the smartest person won&#x27;t survive long if dropped in 10000 BC.</div><br/></div></div><div id="39173169" class="c"><input type="checkbox" id="c-39173169" checked=""/><div class="controls bullet"><span class="by">jack_pp</span><span>|</span><a href="#39173120">parent</a><span>|</span><a href="#39173184">prev</a><span>|</span><a href="#39172950">next</a><span>|</span><label class="collapse" for="c-39173169">[-]</label><label class="expand" for="c-39173169">[6 more]</label></div><br/><div class="children"><div class="content">Of course it is in the brain, the brain created and evolved the language as a very powerful tool. If intelligence was in the language then other animals would be as intelligent as us</div><br/><div id="39173391" class="c"><input type="checkbox" id="c-39173391" checked=""/><div class="controls bullet"><span class="by">klipt</span><span>|</span><a href="#39173120">root</a><span>|</span><a href="#39173169">parent</a><span>|</span><a href="#39172950">next</a><span>|</span><label class="collapse" for="c-39173391">[-]</label><label class="expand" for="c-39173391">[5 more]</label></div><br/><div class="children"><div class="content">Scientific advancement requires both brains and knowledge transfer over generations.<p>&quot;If I have seen further, it is by standing on the shoulders of giants.&quot;</div><br/><div id="39173694" class="c"><input type="checkbox" id="c-39173694" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#39173120">root</a><span>|</span><a href="#39173391">parent</a><span>|</span><a href="#39173589">prev</a><span>|</span><a href="#39172950">next</a><span>|</span><label class="collapse" for="c-39173694">[-]</label><label class="expand" for="c-39173694">[3 more]</label></div><br/><div class="children"><div class="content">Knowledge transfer over generations is a function of the brain.<p>Other species have much more limited ability to transfer knowledge intergenerationally, and that is because the human brain&#x27;s capability for symbolic language is much more advanced than other animals&#x27;, who are not able to encode knowledge nearly as efficiently.</div><br/><div id="39173738" class="c"><input type="checkbox" id="c-39173738" checked=""/><div class="controls bullet"><span class="by">klipt</span><span>|</span><a href="#39173120">root</a><span>|</span><a href="#39173694">parent</a><span>|</span><a href="#39172950">next</a><span>|</span><label class="collapse" for="c-39173738">[-]</label><label class="expand" for="c-39173738">[2 more]</label></div><br/><div class="children"><div class="content">The point is it&#x27;s a function of many connected brains, not just one brain.</div><br/><div id="39174065" class="c"><input type="checkbox" id="c-39174065" checked=""/><div class="controls bullet"><span class="by">jack_pp</span><span>|</span><a href="#39173120">root</a><span>|</span><a href="#39173738">parent</a><span>|</span><a href="#39172950">next</a><span>|</span><label class="collapse" for="c-39174065">[-]</label><label class="expand" for="c-39174065">[1 more]</label></div><br/><div class="children"><div class="content">Sure but that&#x27;s still only possible for the human brain, other species brains aren&#x27;t capable of encoding knowledge and using that to collaborate with other members.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39172950" class="c"><input type="checkbox" id="c-39172950" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39173120">prev</a><span>|</span><a href="#39172989">next</a><span>|</span><label class="collapse" for="c-39172950">[-]</label><label class="expand" for="c-39172950">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We are releasing RWKV-v5 Eagle 7B, licensed as Apache 2.0 license, under the Linux Foundation, and can be used personally or commercially without restrictions<p>great on the team to actually set up the right incentives for testing and adoption.</div><br/></div></div><div id="39172989" class="c"><input type="checkbox" id="c-39172989" checked=""/><div class="controls bullet"><span class="by">karterk</span><span>|</span><a href="#39172950">prev</a><span>|</span><a href="#39173511">next</a><span>|</span><label class="collapse" for="c-39172989">[-]</label><label class="expand" for="c-39172989">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting how all focus is now primarily on decoder-only next-token-prediction models. Encoders (BERT, encoder of T5) are still useful for generating embedding for tasks like retrieval or classification. While there is a lot of work on fine-tuning BERT and T5 for such tasks, it would be nice to see more research on better pre-training architectures for embedding use cases.</div><br/></div></div><div id="39173511" class="c"><input type="checkbox" id="c-39173511" checked=""/><div class="controls bullet"><span class="by">127361</span><span>|</span><a href="#39172989">prev</a><span>|</span><a href="#39174127">next</a><span>|</span><label class="collapse" for="c-39173511">[-]</label><label class="expand" for="c-39173511">[5 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve joined the Linux Foundation, does that mean the models are going to be eventually censored to satisfy the foundation&#x27;s AI safety policies? That includes ensuring the models don&#x27;t generate content that&#x27;s non-inclusive or against diversity policies?</div><br/><div id="39173639" class="c"><input type="checkbox" id="c-39173639" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39173511">parent</a><span>|</span><a href="#39173666">next</a><span>|</span><label class="collapse" for="c-39173639">[-]</label><label class="expand" for="c-39173639">[1 more]</label></div><br/><div class="children"><div class="content">Downvoted, because it&#x27;s a very trolly way to ask this. Especially given the foundation doesn&#x27;t have an AI safety policy from what I&#x27;ve seen. Let&#x27;s be better than this...</div><br/></div></div><div id="39173666" class="c"><input type="checkbox" id="c-39173666" checked=""/><div class="controls bullet"><span class="by">pico_creator</span><span>|</span><a href="#39173511">parent</a><span>|</span><a href="#39173639">prev</a><span>|</span><a href="#39173636">next</a><span>|</span><label class="collapse" for="c-39173666">[-]</label><label class="expand" for="c-39173666">[1 more]</label></div><br/><div class="children"><div class="content">Currently the main policy is only around copyright - and nothing about AI safety: <a href="https:&#x2F;&#x2F;www.linuxfoundation.org&#x2F;legal&#x2F;generative-ai" rel="nofollow">https:&#x2F;&#x2F;www.linuxfoundation.org&#x2F;legal&#x2F;generative-ai</a><p>Also in the full power of opensource, if LF really force something the group disagree with, we will just fork<p>All the other alignment policies, are optional for groups to opt-in<p>So I would not worry so much about that - the group already has a plan in event we need to leave the Linux Foundation - for example: If USA regulates AI training (since LF is registered under USA)</div><br/></div></div><div id="39173636" class="c"><input type="checkbox" id="c-39173636" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#39173511">parent</a><span>|</span><a href="#39173666">prev</a><span>|</span><a href="#39174127">next</a><span>|</span><label class="collapse" for="c-39173636">[-]</label><label class="expand" for="c-39173636">[2 more]</label></div><br/><div class="children"><div class="content">It is trivial to fine tune any model (whether a base model or an aligned model) to your preferred output preferences <i>as long as you have access to the model weights</i>.</div><br/><div id="39173668" class="c"><input type="checkbox" id="c-39173668" checked=""/><div class="controls bullet"><span class="by">Al-Khwarizmi</span><span>|</span><a href="#39173511">root</a><span>|</span><a href="#39173636">parent</a><span>|</span><a href="#39174127">next</a><span>|</span><label class="collapse" for="c-39173668">[-]</label><label class="expand" for="c-39173668">[1 more]</label></div><br/><div class="children"><div class="content">Not trivial for the general public at all, and furthermore, you need much more memory for finetuning than for inference, often making it infeasible for many machine&#x2F;model combinations.</div><br/></div></div></div></div></div></div><div id="39174127" class="c"><input type="checkbox" id="c-39174127" checked=""/><div class="controls bullet"><span class="by">dizhn</span><span>|</span><a href="#39173511">prev</a><span>|</span><a href="#39173171">next</a><span>|</span><label class="collapse" for="c-39174127">[-]</label><label class="expand" for="c-39174127">[1 more]</label></div><br/><div class="children"><div class="content">FYI a lot of people are asking questions which one of the project members has been answering on Reddit the last few days.<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;user&#x2F;PicoCreator&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;user&#x2F;PicoCreator&#x2F;</a></div><br/></div></div><div id="39173171" class="c"><input type="checkbox" id="c-39173171" checked=""/><div class="controls bullet"><span class="by">ComplexSystems</span><span>|</span><a href="#39174127">prev</a><span>|</span><a href="#39173368">next</a><span>|</span><label class="collapse" for="c-39173171">[-]</label><label class="expand" for="c-39173171">[1 more]</label></div><br/><div class="children"><div class="content">Can someone knowledgeable about this stuff maybe explain what it means? What is the context and how does this compare to the usual transformer models? I don&#x27;t get how to interpret some of these benchmarks. It looks like it&#x27;s as good as Mistral 7B&#x2F;mistral-tiny?</div><br/></div></div><div id="39173368" class="c"><input type="checkbox" id="c-39173368" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#39173171">prev</a><span>|</span><a href="#39173127">next</a><span>|</span><label class="collapse" for="c-39173368">[-]</label><label class="expand" for="c-39173368">[1 more]</label></div><br/><div class="children"><div class="content">Forgive me for not searching around enough but so far I am not sure about:<p>- how much RAM is needed<p>- how many tokens per second with CPU only, like a typical VM&#x2F;VPS for example</div><br/></div></div><div id="39173127" class="c"><input type="checkbox" id="c-39173127" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#39173368">prev</a><span>|</span><a href="#39173416">next</a><span>|</span><label class="collapse" for="c-39173127">[-]</label><label class="expand" for="c-39173127">[1 more]</label></div><br/><div class="children"><div class="content">My experiments with RWKV-4 showed good inference speed but suuper slow tokenization speed; I’m not sure if this was RWKV specific or implementation specific: it was some time ago. Any guidance on this for rwkv-5?</div><br/></div></div><div id="39173416" class="c"><input type="checkbox" id="c-39173416" checked=""/><div class="controls bullet"><span class="by">_hl_</span><span>|</span><a href="#39173127">prev</a><span>|</span><a href="#39173558">next</a><span>|</span><label class="collapse" for="c-39173416">[-]</label><label class="expand" for="c-39173416">[1 more]</label></div><br/><div class="children"><div class="content">Attention is, after all, not what you need.</div><br/></div></div><div id="39173558" class="c"><input type="checkbox" id="c-39173558" checked=""/><div class="controls bullet"><span class="by">Harrisonv</span><span>|</span><a href="#39173416">prev</a><span>|</span><a href="#39173676">next</a><span>|</span><label class="collapse" for="c-39173558">[-]</label><label class="expand" for="c-39173558">[3 more]</label></div><br/><div class="children"><div class="content">Try rwkv-demo-api.recursal.ai if you want to try it and dont want to wait for gradio</div><br/><div id="39174143" class="c"><input type="checkbox" id="c-39174143" checked=""/><div class="controls bullet"><span class="by">zurfer</span><span>|</span><a href="#39173558">parent</a><span>|</span><a href="#39173676">next</a><span>|</span><label class="collapse" for="c-39174143">[-]</label><label class="expand" for="c-39174143">[2 more]</label></div><br/><div class="children"><div class="content">Thank you! I&#x27;m not experienced with 7B models.<p>3 things stand out to me:<p>- it&#x27;s absolutely not useable for the kind of use cases I solve with GPT-4 (code generation, information retrieval)<p>- it could technically swallow a 50 page PDF, but it&#x27;s not able to answer questions about it (inference speed was good, but content was garbage)<p>- it is ok for chatting and translations (how is your day?)</div><br/><div id="39174167" class="c"><input type="checkbox" id="c-39174167" checked=""/><div class="controls bullet"><span class="by">dizhn</span><span>|</span><a href="#39173558">root</a><span>|</span><a href="#39174143">parent</a><span>|</span><a href="#39173676">next</a><span>|</span><label class="collapse" for="c-39174167">[-]</label><label class="expand" for="c-39174167">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a foundation model not fine tuned for anything. Might that be the reason it&#x27;s not performing so well?</div><br/></div></div></div></div></div></div><div id="39173676" class="c"><input type="checkbox" id="c-39173676" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39173558">prev</a><span>|</span><a href="#39173132">next</a><span>|</span><label class="collapse" for="c-39173676">[-]</label><label class="expand" for="c-39173676">[4 more]</label></div><br/><div class="children"><div class="content">Has anybody else noticed the map indicating that there are barely any fluent English speakers outside of the &quot;English-speaking&quot; countries? Or is the threshold for fluency so high that no place even in Europe except the Ireland and the UK qualify at all?</div><br/><div id="39174022" class="c"><input type="checkbox" id="c-39174022" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#39173676">parent</a><span>|</span><a href="#39173858">next</a><span>|</span><label class="collapse" for="c-39174022">[-]</label><label class="expand" for="c-39174022">[1 more]</label></div><br/><div class="children"><div class="content">English is an official language is Malta and about 70% of the population is classed as &quot;advanced&quot; in English[0]. That only adds up to 280k people though, you&#x27;d probably find as many native-level speakers in any big European country, though not at the same density.<p>[0] <a href="https:&#x2F;&#x2F;nso.gov.mt&#x2F;wp-content&#x2F;uploads&#x2F;Skills-Preliminary.pdf" rel="nofollow">https:&#x2F;&#x2F;nso.gov.mt&#x2F;wp-content&#x2F;uploads&#x2F;Skills-Preliminary.pdf</a></div><br/></div></div><div id="39173858" class="c"><input type="checkbox" id="c-39173858" checked=""/><div class="controls bullet"><span class="by">airspresso</span><span>|</span><a href="#39173676">parent</a><span>|</span><a href="#39174022">prev</a><span>|</span><a href="#39173132">next</a><span>|</span><label class="collapse" for="c-39173858">[-]</label><label class="expand" for="c-39173858">[2 more]</label></div><br/><div class="children"><div class="content">This caught my eye as well. I interpreted it as the map only shows countries that have English as primary language. But the article was not precise enough about this.<p>I applaud them for focusing on multilingual performance though, as that is an important area of NLP which still has lots of room for improvement.</div><br/><div id="39174010" class="c"><input type="checkbox" id="c-39174010" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39173676">root</a><span>|</span><a href="#39173858">parent</a><span>|</span><a href="#39173132">next</a><span>|</span><label class="collapse" for="c-39174010">[-]</label><label class="expand" for="c-39174010">[1 more]</label></div><br/><div class="children"><div class="content">I highly welcome the effort as well*, but I don&#x27;t see why they would have to mistake first-language ability for fluency to argue for that. The difference is vast and relevant: anyone with good English reading and writing skills can take advantage of a model and might prefer it over a worse model in their native language.<p>*: Just sceptical whether there&#x27;s enough content out there which isn&#x27;t just (often badly or too straightforwardly) translated from English. Not an issue for the languages with let&#x27;s say &gt;10 Mio. speakers, but for everything smaller.</div><br/></div></div></div></div></div></div><div id="39173132" class="c"><input type="checkbox" id="c-39173132" checked=""/><div class="controls bullet"><span class="by">adt</span><span>|</span><a href="#39173676">prev</a><span>|</span><a href="#39172970">next</a><span>|</span><label class="collapse" for="c-39173132">[-]</label><label class="expand" for="c-39173132">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;lifearchitect.ai&#x2F;models-table&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lifearchitect.ai&#x2F;models-table&#x2F;</a></div><br/><div id="39173689" class="c"><input type="checkbox" id="c-39173689" checked=""/><div class="controls bullet"><span class="by">black_puppydog</span><span>|</span><a href="#39173132">parent</a><span>|</span><a href="#39172970">next</a><span>|</span><label class="collapse" for="c-39173689">[-]</label><label class="expand" for="c-39173689">[1 more]</label></div><br/><div class="children"><div class="content">IIUC this model type makes the &quot;ALScore&quot; column completely pointless, because the quality of results isn&#x27;t related to the number of weights in the same way as for regular transformers.</div><br/></div></div></div></div><div id="39172970" class="c"><input type="checkbox" id="c-39172970" checked=""/><div class="controls bullet"><span class="by">nickthesick</span><span>|</span><a href="#39173132">prev</a><span>|</span><a href="#39173969">next</a><span>|</span><label class="collapse" for="c-39172970">[-]</label><label class="expand" for="c-39172970">[3 more]</label></div><br/><div class="children"><div class="content">Is it possible to try this out on something like llama.cpp? Does the different architecture make a difference there?</div><br/><div id="39173098" class="c"><input type="checkbox" id="c-39173098" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#39172970">parent</a><span>|</span><a href="#39172982">next</a><span>|</span><label class="collapse" for="c-39173098">[-]</label><label class="expand" for="c-39173098">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s <a href="https:&#x2F;&#x2F;github.com&#x2F;saharNooby&#x2F;rwkv.cpp">https:&#x2F;&#x2F;github.com&#x2F;saharNooby&#x2F;rwkv.cpp</a>, which related-ish[0] to ggml&#x2F;llama.cpp<p>[0]: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;846">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;846</a></div><br/></div></div></div></div><div id="39173969" class="c"><input type="checkbox" id="c-39173969" checked=""/><div class="controls bullet"><span class="by">ReptileMan</span><span>|</span><a href="#39172970">prev</a><span>|</span><label class="collapse" for="c-39173969">[-]</label><label class="expand" for="c-39173969">[2 more]</label></div><br/><div class="children"><div class="content">Their map showing distribution of English speaking people is just terrible - I am fairly sure that there is at least one percent speaking English in India, Western Europe, Eastern Europe, Russia and China.</div><br/><div id="39174030" class="c"><input type="checkbox" id="c-39174030" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39173969">parent</a><span>|</span><label class="collapse" for="c-39174030">[-]</label><label class="expand" for="c-39174030">[1 more]</label></div><br/><div class="children"><div class="content">It must set a terribly high threshold (like language certificate holders or graduates of English-speaking schools) or actually report the percentage of native speakers. But one only has to be fluent enough to write chat messages to use a text model!</div><br/></div></div></div></div></div></div></div></div></div></body></html>