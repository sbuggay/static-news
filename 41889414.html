<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1729414851290" as="style"/><link rel="stylesheet" href="styles.css?v=1729414851290"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-engineers-build-new-algorithm-for-ai-processing-replace-complex-floating-point-multiplication-with-integer-addition">AI engineers claim new algorithm reduces AI power consumption by 95%</a> <span class="domain">(<a href="https://www.tomshardware.com">www.tomshardware.com</a>)</span></div><div class="subtext"><span>ferriswil</span> | <span>114 comments</span></div><br/><div><div id="41893178" class="c"><input type="checkbox" id="c-41893178" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#41889903">next</a><span>|</span><label class="collapse" for="c-41893178">[-]</label><label class="expand" for="c-41893178">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a very crude approximation, e.g. 1.75 * 2.5 == 3 (although it seems better as the numbers get closer to 0).<p>I tried implementing this for AVX512 with tinyBLAS in llamafile.<p><pre><code>    inline __m512 lmul512(__m512 x, __m512 y) {
        __m512i sign_mask = _mm512_set1_epi32(0x80000000);
        __m512i exp_mask = _mm512_set1_epi32(0x7F800000);
        __m512i mant_mask = _mm512_set1_epi32(0x007FFFFF);
        __m512i exp_bias = _mm512_set1_epi32(127);
        __m512i x_bits = _mm512_castps_si512(x);
        __m512i y_bits = _mm512_castps_si512(y);
        __m512i sign_x = _mm512_and_si512(x_bits, sign_mask);
        __m512i sign_y = _mm512_and_si512(y_bits, sign_mask);
        __m512i exp_x = _mm512_srli_epi32(_mm512_and_si512(x_bits, exp_mask), 23);
        __m512i exp_y = _mm512_srli_epi32(_mm512_and_si512(y_bits, exp_mask), 23);
        __m512i mant_x = _mm512_and_si512(x_bits, mant_mask);
        __m512i mant_y = _mm512_and_si512(y_bits, mant_mask);
        __m512i sign_result = _mm512_xor_si512(sign_x, sign_y);
        __m512i exp_result = _mm512_sub_epi32(_mm512_add_epi32(exp_x, exp_y), exp_bias);
        __m512i mant_result = _mm512_srli_epi32(_mm512_add_epi32(mant_x, mant_y), 1);
        __m512i result_bits = _mm512_or_si512(
            _mm512_or_si512(sign_result, _mm512_slli_epi32(exp_result, 23)), mant_result);
        return _mm512_castsi512_ps(result_bits);
    }
</code></pre>
Then I used it for Llama-3.2-3B-Instruct.F16.gguf and it outputted jibberish. So you would probably have to train and design your model specifically to use this multiplication approximation in order for it to work. Or maybe I&#x27;d have to tune the model so that only certain layers and&#x2F;or operations use the approximation. However the speed was decent. Prefill only dropped from 850 tokens per second to 200 tok&#x2F;sec on my threadripper. Prediction speed was totally unaffected, staying at 34 tok&#x2F;sec. I like how the code above generates vpternlog ops. So if anyone ever designs an LLM architecture and releases weights on Hugging Face that use this algorithm, we&#x27;ll be able to run them reasonably fast without special hardware.</div><br/><div id="41893810" class="c"><input type="checkbox" id="c-41893810" checked=""/><div class="controls bullet"><span class="by">raluk</span><span>|</span><a href="#41893178">parent</a><span>|</span><a href="#41889903">next</a><span>|</span><label class="collapse" for="c-41893810">[-]</label><label class="expand" for="c-41893810">[1 more]</label></div><br/><div class="children"><div class="content">Your kernel seems to be incorrect for 1.75 * 2.5.
From paper we have 1.75 == (1+0.75)*2^0 for 2.5 == (1+0.25)*2^1 so result is 
(1+0.75+0.25+2^-4)*2^1 == 4.125 (correct result is 4.375)</div><br/></div></div></div></div><div id="41889903" class="c"><input type="checkbox" id="c-41889903" checked=""/><div class="controls bullet"><span class="by">djoldman</span><span>|</span><a href="#41893178">prev</a><span>|</span><a href="#41890110">next</a><span>|</span><label class="collapse" for="c-41889903">[-]</label><label class="expand" for="c-41889903">[28 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2410.00907" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2410.00907</a><p>ABSTRACT<p>Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication (L-Mul) algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by elementwise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8 e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8 e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8 e4m3 as accumulation precision in both fine-tuning and inference.</div><br/><div id="41890324" class="c"><input type="checkbox" id="c-41890324" checked=""/><div class="controls bullet"><span class="by">onlyrealcuzzo</span><span>|</span><a href="#41889903">parent</a><span>|</span><a href="#41892025">next</a><span>|</span><label class="collapse" for="c-41890324">[-]</label><label class="expand" for="c-41890324">[26 more]</label></div><br/><div class="children"><div class="content">Does this mean you can train efficiently without GPUs?<p>Presumably there will be a lot of interest.</div><br/><div id="41890353" class="c"><input type="checkbox" id="c-41890353" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890324">parent</a><span>|</span><a href="#41892025">next</a><span>|</span><label class="collapse" for="c-41890353">[-]</label><label class="expand" for="c-41890353">[25 more]</label></div><br/><div class="children"><div class="content">No. But it does potentially mean that either current or future-tweaked GPUs could run a lot more efficiently -- meaning much faster or with much less energy consumption.<p>You still need the GPU parallelism though.</div><br/><div id="41893598" class="c"><input type="checkbox" id="c-41893598" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890353">parent</a><span>|</span><a href="#41890621">next</a><span>|</span><label class="collapse" for="c-41893598">[-]</label><label class="expand" for="c-41893598">[1 more]</label></div><br/><div class="children"><div class="content">This is still amazing work, imagine running chungus models on a single 3090.</div><br/></div></div><div id="41890621" class="c"><input type="checkbox" id="c-41890621" checked=""/><div class="controls bullet"><span class="by">fuzzfactor</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890353">parent</a><span>|</span><a href="#41893598">prev</a><span>|</span><a href="#41892025">next</a><span>|</span><label class="collapse" for="c-41890621">[-]</label><label class="expand" for="c-41890621">[23 more]</label></div><br/><div class="children"><div class="content">I had a feeling it had to be something like massive waste due to a misguided feature of the algorithms that shouldn&#x27;t have been there in the first place.<p>Once the &quot;math is done&quot; quite likely it would have paid off better than most investments for the top people to have spent a few short years working with grossly underpowered hardware until they could come up with amazing results there before scaling up.  Rather than grossly overpowered hardware before there was even deep understanding of the underlying processes.<p>When you think about it, what we have seen from the latest ultra-high-powered &quot;thinking&quot; machines is truly so impressive.  But if  you are trying to fool somebody into believing that it&#x27;s a real person it&#x27;s still not &quot;quite&quot; there.<p>Maybe a good benchmark would be to take a regular PC, and without reliance on AI just pull out all the stops and put all the effort into fakery itself.  No holds barred, any trick you can think of.  See what the electronics is capable of this way.  There are some smart engineers, this would only take a few years but looks like it would have been a lot more affordable.<p>Then with the same hardware if an AI alternative is not as convincing, something has got to be wrong.<p>It&#x27;s good to find out this type of thing before you go overboard.<p>Regardless of speed or power, I never could have gotten an 8-bit computer to match the output of a 32-bit floating-point algorithm by using floating-point myself.  Integers all the way and place the decimal where it&#x27;s supposed to be when you&#x27;re done.<p>Once it&#x27;s really figured out, how do you think it would feel being the one paying the electric bills up until now?</div><br/><div id="41890824" class="c"><input type="checkbox" id="c-41890824" checked=""/><div class="controls bullet"><span class="by">jimmaswell</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890621">parent</a><span>|</span><a href="#41892039">next</a><span>|</span><label class="collapse" for="c-41890824">[-]</label><label class="expand" for="c-41890824">[16 more]</label></div><br/><div class="children"><div class="content">Faster progress was absolutely worth it. Spending years agonizing over theory to save a bit of electric would have been a massive disservice to the world.</div><br/><div id="41891732" class="c"><input type="checkbox" id="c-41891732" checked=""/><div class="controls bullet"><span class="by">rossjudson</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890824">parent</a><span>|</span><a href="#41890834">next</a><span>|</span><label class="collapse" for="c-41891732">[-]</label><label class="expand" for="c-41891732">[13 more]</label></div><br/><div class="children"><div class="content">You&#x27;re sort of presuming that LLMs are going to be a massive <i>service</i> to the world there, aren&#x27;t you?  I think the jury is still out on that one.</div><br/><div id="41891845" class="c"><input type="checkbox" id="c-41891845" checked=""/><div class="controls bullet"><span class="by">jimmaswell</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41891732">parent</a><span>|</span><a href="#41890834">next</a><span>|</span><label class="collapse" for="c-41891845">[-]</label><label class="expand" for="c-41891845">[12 more]</label></div><br/><div class="children"><div class="content">They already have been. Even just in programming, even just Copilot has been a life changing productivity booster.</div><br/><div id="41892532" class="c"><input type="checkbox" id="c-41892532" checked=""/><div class="controls bullet"><span class="by">recursive</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41891845">parent</a><span>|</span><a href="#41891944">next</a><span>|</span><label class="collapse" for="c-41892532">[-]</label><label class="expand" for="c-41892532">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using copilot for several months.  If I could figure out a way to measure its impact on my productivity, I&#x27;d probably see a single digit percentage boost in &quot;productivity&quot;.  This is not life-changing for me.  And for some tasks, it&#x27;s actually worse than nothing.  As in, I spend time feeding it a task, and it just completely fails to do anything useful.</div><br/><div id="41892918" class="c"><input type="checkbox" id="c-41892918" checked=""/><div class="controls bullet"><span class="by">jimmaswell</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892532">parent</a><span>|</span><a href="#41891944">next</a><span>|</span><label class="collapse" for="c-41892918">[-]</label><label class="expand" for="c-41892918">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using it for over a year I think. I don&#x27;t often feed it tasks with comments so much as go about things the same as usual and let it autocomplete. The time and cognitive load saved adds up massively. I&#x27;ve had to go without it for a bit while my workplace gets its license in order for the corporate version and the personal version has an issue with the proxy, and it&#x27;s been agonizing going without it again. I almost forgot how much it sucks having to jump to google every other minute, and it was easy to start to take for granted how much context copilot was letting me not have to hold onto in my head. It really lets me work on the problem as opposed to being mired in immaterial details. It feels like I&#x27;m at least 2x slower overall without it.</div><br/><div id="41893539" class="c"><input type="checkbox" id="c-41893539" checked=""/><div class="controls bullet"><span class="by">atq2119</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892918">parent</a><span>|</span><a href="#41893474">next</a><span>|</span><label class="collapse" for="c-41893539">[-]</label><label class="expand" for="c-41893539">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I almost forgot how much it sucks having to jump to google every other minute<p>Even allowing for some hyperbole, your programming experience is extremely different from mine. Looking anything up outside the IDE, let alone via Google, is by far the exception for me rather than the rule.<p>I&#x27;ve long suspected that this kind of difference explains a lot of the difference in how Copilot is perceived.</div><br/></div></div><div id="41893474" class="c"><input type="checkbox" id="c-41893474" checked=""/><div class="controls bullet"><span class="by">rockskon</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892918">parent</a><span>|</span><a href="#41893539">prev</a><span>|</span><a href="#41891944">next</a><span>|</span><label class="collapse" for="c-41893474">[-]</label><label class="expand" for="c-41893474">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know about you but LLMs spit out garbage nonsense frequent enough that I can&#x27;t trust their output in <i>any</i> context I cannot personally verify the validity of.</div><br/></div></div></div></div></div></div><div id="41891944" class="c"><input type="checkbox" id="c-41891944" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41891845">parent</a><span>|</span><a href="#41892532">prev</a><span>|</span><a href="#41892396">next</a><span>|</span><label class="collapse" for="c-41891944">[-]</label><label class="expand" for="c-41891944">[6 more]</label></div><br/><div class="children"><div class="content">Are you sure it’s a life changing productivity booster? Sometimes I look at my projects and wonder how would I explain it to an LLM what this code should have done if it didn’t exist yet. Must be a shitton of boilerplate programming for copilot to be a life-changing experience.</div><br/><div id="41892560" class="c"><input type="checkbox" id="c-41892560" checked=""/><div class="controls bullet"><span class="by">AYBABTME</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41891944">parent</a><span>|</span><a href="#41892396">next</a><span>|</span><label class="collapse" for="c-41892560">[-]</label><label class="expand" for="c-41892560">[5 more]</label></div><br/><div class="children"><div class="content">You haven&#x27;t used them enough. Everytime an LLM reduces my search from 1min to 5s, the LLM pays.<p>Just summary features: save me 20min of reading a transcript, turn it into 20s. That&#x27;s a huge enabler.</div><br/><div id="41894036" class="c"><input type="checkbox" id="c-41894036" checked=""/><div class="controls bullet"><span class="by">andrei_says_</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892560">parent</a><span>|</span><a href="#41893233">next</a><span>|</span><label class="collapse" for="c-41894036">[-]</label><label class="expand" for="c-41894036">[1 more]</label></div><br/><div class="children"><div class="content">My experience with overviews is that they are often subtly or not so subtly inaccurate. LLMs not understanding meaning or intent carries risk of misrepresentation.</div><br/></div></div><div id="41893233" class="c"><input type="checkbox" id="c-41893233" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892560">parent</a><span>|</span><a href="#41894036">prev</a><span>|</span><a href="#41892901">next</a><span>|</span><label class="collapse" for="c-41893233">[-]</label><label class="expand" for="c-41893233">[1 more]</label></div><br/><div class="children"><div class="content">Overviews aren’t code though. In code, for me, they don’t pass 80&#x2F;20 tests well enough, sometimes even on simple cases. (You get 50-80% of an existing function&#x2F;block with some important context prepended and a comment, let it write the rest and check if it succeeds). It doesn’t mean that LLMs are useless. Or that I am antillamist or a denier - I’m actually an enthusiast. But this specific claim I hear often and don’t find true. Maybe true for repetitive code in boring environments where typing and remembering formats&#x2F;params over and over is the main issue. Not in actual code.<p>If I paste the actual non-trivial code, it starts deviating fast. And it isn’t too complex, it’s just less like “parallel sort two arrays” and more like “wait for an image on a screenshot by execing scrot (with no sound) repeatedly and passing the result to this detect-cv2.py script and use all matching options described in this ts type, get stdout json as in this ts type, and if there’s a match, wait for the specified anim timeout and test again to get the settled match coords after an animation finishes; throw after a total timeout”. Not a rocket science, pretty dumb shit, but right there they fall flat and start imagining things, heavily.<p>I guess it shines if you ask it to make an html form, but I couldn’t call that life-changing unless I had to make these damn forms all day.</div><br/></div></div><div id="41892901" class="c"><input type="checkbox" id="c-41892901" checked=""/><div class="controls bullet"><span class="by">mecsred</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892560">parent</a><span>|</span><a href="#41893233">prev</a><span>|</span><a href="#41892396">next</a><span>|</span><label class="collapse" for="c-41892901">[-]</label><label class="expand" for="c-41892901">[2 more]</label></div><br/><div class="children"><div class="content">If 20 mins of informations can legitimately be condensed into 20 seconds, it sounds like the original wasn&#x27;t worth reading in the first place. Could have skipped the llm entirely.</div><br/><div id="41893262" class="c"><input type="checkbox" id="c-41893262" checked=""/><div class="controls bullet"><span class="by">bostik</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892901">parent</a><span>|</span><a href="#41892396">next</a><span>|</span><label class="collapse" for="c-41893262">[-]</label><label class="expand" for="c-41893262">[1 more]</label></div><br/><div class="children"><div class="content">I upvoted you, because I think you have a valid point. The tone is unnecessarily aggressive though.<p>Effective and information-dense communication is <i>really</i> hard. That doesn&#x27;t mean we should just accept the useless fluff surrounding the actual information and&#x2F;or analysis. People could learn a lot from the Ignoble Prize ceremony&#x27;s 24&#x2F;7 presentation model.<p>Sadly, it seems we are heading towards a future where you may need an LLM to distill the relevant information out of a sea of noise.</div><br/></div></div></div></div></div></div></div></div><div id="41892396" class="c"><input type="checkbox" id="c-41892396" checked=""/><div class="controls bullet"><span class="by">giraffe_lady</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41891845">parent</a><span>|</span><a href="#41891944">prev</a><span>|</span><a href="#41890834">next</a><span>|</span><label class="collapse" for="c-41892396">[-]</label><label class="expand" for="c-41892396">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Even just in programming&quot; the jury is still out. None of my coworkers using these are noticeably more productive than the ones who don&#x27;t. Outside of programming no one gives a shit except scammers and hype chasers.</div><br/></div></div></div></div></div></div><div id="41890834" class="c"><input type="checkbox" id="c-41890834" checked=""/><div class="controls bullet"><span class="by">BolexNOLA</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890824">parent</a><span>|</span><a href="#41891732">prev</a><span>|</span><a href="#41892039">next</a><span>|</span><label class="collapse" for="c-41890834">[-]</label><label class="expand" for="c-41890834">[2 more]</label></div><br/><div class="children"><div class="content">“A bit”?</div><br/><div id="41891112" class="c"><input type="checkbox" id="c-41891112" checked=""/><div class="controls bullet"><span class="by">bartread</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890834">parent</a><span>|</span><a href="#41892039">next</a><span>|</span><label class="collapse" for="c-41891112">[-]</label><label class="expand" for="c-41891112">[1 more]</label></div><br/><div class="children"><div class="content">Yes, a large amount for - in the grand scheme of things - a short period of time (i.e., a quantity of energy usage in an intense spike that will be dwarfed by energy usage over time) can accurately be described as “a bit”.<p>Of course, the impact is that AI will continue to become cheaper to use, and induced demand will continue the feedback loop driving the market as a result.</div><br/></div></div></div></div></div></div><div id="41892039" class="c"><input type="checkbox" id="c-41892039" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890621">parent</a><span>|</span><a href="#41890824">prev</a><span>|</span><a href="#41892366">next</a><span>|</span><label class="collapse" for="c-41892039">[-]</label><label class="expand" for="c-41892039">[2 more]</label></div><br/><div class="children"><div class="content">This is a bit like recommending to skip vacuum tubes, think hard and invent transistors.</div><br/><div id="41892316" class="c"><input type="checkbox" id="c-41892316" checked=""/><div class="controls bullet"><span class="by">fuzzfactor</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892039">parent</a><span>|</span><a href="#41892366">next</a><span>|</span><label class="collapse" for="c-41892316">[-]</label><label class="expand" for="c-41892316">[1 more]</label></div><br/><div class="children"><div class="content">This is kind of thought-provoking.<p>That is a good correlation when you think about how much more energy-efficient transistors are than vacuum tubes.<p>Vacuum tube computers were a thing for a while, but it was more out of desperation than systematic intellectual progress.<p>OTOH you could look at the present accomplishments like it was throwing more vacuum tubes at a problem that can not be adequately addressed that way.<p>What turned out to be a solid-state solution was a completely different approach from the ground up.<p>To the extent a more power-saving technique <i>using the same hardware</i> is only a matter of different software approaches, that would be something that realistically could have been accomplished before so much energy was expended.<p>Even though I&#x27;ve always thought application-specific circuits would be what really helps ML and AI a lot, and that would end up not being the exact same hardware at all.<p>If power is truly being wasted enough to <i>start</i> rearing its ugly head, somebody should be able to figure out how to fix it before it gets out-of-hand.<p>Ironically enough with my experience using vacuum tubes, I&#x27;ve felt that there were some serious losses in technology when the research momentum involved was so rapidly abandoned in favor of &quot;solid-state everything&quot; at any cost.<p>Maybe it is a good idea to abandon the energy-intensive approaches, as soon as anything completely different that&#x27;s the least bit promising can barely be seen by a gifted visionary to have a glimmer of potential.</div><br/></div></div></div></div><div id="41892366" class="c"><input type="checkbox" id="c-41892366" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890621">parent</a><span>|</span><a href="#41892039">prev</a><span>|</span><a href="#41891053">next</a><span>|</span><label class="collapse" for="c-41892366">[-]</label><label class="expand" for="c-41892366">[2 more]</label></div><br/><div class="children"><div class="content">This comment lives in a fictional world where there is a singular group that could have collectively acted counterfactually. In the real world any actor that individually went this route would have gone bankrupt while the others collected money by showing actual results even if ineffeciently earned.</div><br/><div id="41892732" class="c"><input type="checkbox" id="c-41892732" checked=""/><div class="controls bullet"><span class="by">newyankee</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41892366">parent</a><span>|</span><a href="#41891053">next</a><span>|</span><label class="collapse" for="c-41892732">[-]</label><label class="expand" for="c-41892732">[1 more]</label></div><br/><div class="children"><div class="content">Also it is likely that the rise of LLMs gave many researchers in allied fields the impetus to tackle with the problems that are relevant to making it more efficient and people stumbled upon a solution hiding there.<p>The momentum with LLMs and allied technology may last till it keeps on improving even by a few percentage points and keeps shattering human created new benchmarks every few months</div><br/></div></div></div></div><div id="41891053" class="c"><input type="checkbox" id="c-41891053" checked=""/><div class="controls bullet"><span class="by">pcl</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41890621">parent</a><span>|</span><a href="#41892366">prev</a><span>|</span><a href="#41892025">next</a><span>|</span><label class="collapse" for="c-41891053">[-]</label><label class="expand" for="c-41891053">[2 more]</label></div><br/><div class="children"><div class="content">Isn’t this paper pretty much about spending a few short years to improve the performance? Or are you  arguing that the same people who made breakthroughs over the last few years should have also done the optimization work?</div><br/><div id="41891328" class="c"><input type="checkbox" id="c-41891328" checked=""/><div class="controls bullet"><span class="by">fuzzfactor</span><span>|</span><a href="#41889903">root</a><span>|</span><a href="#41891053">parent</a><span>|</span><a href="#41892025">next</a><span>|</span><label class="collapse" for="c-41891328">[-]</label><label class="expand" for="c-41891328">[1 more]</label></div><br/><div class="children"><div class="content">&gt;the same people who made breakthroughs over the last few years should have also done the optimization work<p>I never thought it would be ideal if it was otherwise, so I guess so.<p>When I first considered neural nets from state-of-the art vendors to assist with some non-linguistic situations over 30 years ago, it wasn&#x27;t quite ready for prime time and I could accept that.<p>I just don&#x27;t have generic situations all the time which would benefit me, so it&#x27;s clearly my problems that have the deficiencies ;\<p>What&#x27;s being done now with all the resources being thrown at it is highly impressive, and gaining all the time, no doubt about it.  It&#x27;s nice to know there are people that can afford it.<p>I truly look forward to more progress, and this may be the previously unreached milestone I have been detecting that might be a big one.<p>Still not good enough for what I need yet so far though.  And I can accept that as easily as ever.<p>That&#x27;s why I put up my estimation that not all of those 30+ years has been spent without agonizing over something ;)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41892025" class="c"><input type="checkbox" id="c-41892025" checked=""/><div class="controls bullet"><span class="by">etcd</span><span>|</span><a href="#41889903">parent</a><span>|</span><a href="#41890324">prev</a><span>|</span><a href="#41890110">next</a><span>|</span><label class="collapse" for="c-41892025">[-]</label><label class="expand" for="c-41892025">[1 more]</label></div><br/><div class="children"><div class="content">I feel like I have seen this idea a few times but don&#x27;t recall where but stuff posted via HN.<p>Here <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591</a> but even before that. It is possibly one of those obvious ideas to people steeped in this.<p>To me intuitively using floats to make ultimatelty boolean like decisions seems wasteful but that seemed like the way it had to be to have diffetentiable algorithms.</div><br/></div></div></div></div><div id="41890110" class="c"><input type="checkbox" id="c-41890110" checked=""/><div class="controls bullet"><span class="by">kayo_20211030</span><span>|</span><a href="#41889903">prev</a><span>|</span><a href="#41889818">next</a><span>|</span><label class="collapse" for="c-41890110">[-]</label><label class="expand" for="c-41890110">[20 more]</label></div><br/><div class="children"><div class="content">Extraordinary claims require extraordinary evidence.
Maybe it&#x27;s possible, but consider that some really smart people, in many different groups, have been working diligently in this space for quite a while; so claims of 95% savings on energy costs _with equivalent performance_ is in the extraordinary category. Of course, we&#x27;ll see when the tide goes out.</div><br/><div id="41890379" class="c"><input type="checkbox" id="c-41890379" checked=""/><div class="controls bullet"><span class="by">manquer</span><span>|</span><a href="#41890110">parent</a><span>|</span><a href="#41890322">next</a><span>|</span><label class="collapse" for="c-41890379">[-]</label><label class="expand" for="c-41890379">[4 more]</label></div><br/><div class="children"><div class="content">It is a click bait headline the claim itself is not extraordinary.
the preprint from arxiv was posted here some time back .<p>The 95% gains is specifically only for multiplication operations, inference is compute light and memory heavy in the first place so the actual gains would be far less smaller .<p>Tech journalism (all journalism really) can hardly be trusted to publish grounded news with the focus on clicks and revenue they need to survive.</div><br/><div id="41892643" class="c"><input type="checkbox" id="c-41892643" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890379">parent</a><span>|</span><a href="#41890560">next</a><span>|</span><label class="collapse" for="c-41892643">[-]</label><label class="expand" for="c-41892643">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Tech journalism (all journalism really) can hardly be trusted to publish grounded news with the focus on clicks and revenue they need to survive.<p>Right now the only way to gain real knowledge is actually to read comments of those articles.</div><br/></div></div><div id="41890560" class="c"><input type="checkbox" id="c-41890560" checked=""/><div class="controls bullet"><span class="by">kayo_20211030</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890379">parent</a><span>|</span><a href="#41892643">prev</a><span>|</span><a href="#41891862">next</a><span>|</span><label class="collapse" for="c-41890560">[-]</label><label class="expand" for="c-41890560">[1 more]</label></div><br/><div class="children"><div class="content">Thank you. That makes sense.</div><br/></div></div><div id="41891862" class="c"><input type="checkbox" id="c-41891862" checked=""/><div class="controls bullet"><span class="by">rob_c</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890379">parent</a><span>|</span><a href="#41890560">prev</a><span>|</span><a href="#41890322">next</a><span>|</span><label class="collapse" for="c-41891862">[-]</label><label class="expand" for="c-41891862">[1 more]</label></div><br/><div class="children"><div class="content">Bingo,<p>We have a winner. Glad that came from someone not in my lectures on ML network design<p>Honestly, thanks for beeting me to this comment</div><br/></div></div></div></div><div id="41890322" class="c"><input type="checkbox" id="c-41890322" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41890110">parent</a><span>|</span><a href="#41890379">prev</a><span>|</span><a href="#41890352">next</a><span>|</span><label class="collapse" for="c-41890322">[-]</label><label class="expand" for="c-41890322">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this claim is extraordinary.  Nothing proposed is mathematically impossible or even unlikely, just a pain in the ass to test (lots of retraining, fine tuning etc, and those operations are expensive when you dont have already massively parallel hardware available, otherwise you&#x27;re ASIC&#x2F;FPGAing for something with a huge investment risk)<p>If I could have a SWAG at it I would say a low resolution model like llama-2 would probably be just fine (llama-2 quantizes without too much headache) but a higher resolution model like llama-3 probably not so much, not without massive retraining anyways.</div><br/></div></div><div id="41890352" class="c"><input type="checkbox" id="c-41890352" checked=""/><div class="controls bullet"><span class="by">Randor</span><span>|</span><a href="#41890110">parent</a><span>|</span><a href="#41890322">prev</a><span>|</span><a href="#41890280">next</a><span>|</span><label class="collapse" for="c-41890352">[-]</label><label class="expand" for="c-41890352">[7 more]</label></div><br/><div class="children"><div class="content">The energy claims up to ~70% can be verified. The inference implementation is here:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;BitNet">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;BitNet</a></div><br/><div id="41890548" class="c"><input type="checkbox" id="c-41890548" checked=""/><div class="controls bullet"><span class="by">kayo_20211030</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890352">parent</a><span>|</span><a href="#41891150">next</a><span>|</span><label class="collapse" for="c-41890548">[-]</label><label class="expand" for="c-41890548">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not an AI person, in any technical sense. The savings being claimed, and I assume verified, are on ARM and x86 chips. The piece doesn&#x27;t mention swapping mult to add, and a 1-bit LLM is, well, a 1-bit LLM.<p>Also,<p>&gt; Additionally, it reduces energy consumption by 55.4% to 70.0%<p>With humility, I don&#x27;t know what that means. It seems like some dubious math with percentages.</div><br/><div id="41891000" class="c"><input type="checkbox" id="c-41891000" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890548">parent</a><span>|</span><a href="#41890656">next</a><span>|</span><label class="collapse" for="c-41891000">[-]</label><label class="expand" for="c-41891000">[1 more]</label></div><br/><div class="children"><div class="content">Not every instruction on a CPU or GPU uses the same amount of power. So if you could rewrite your algorithm to use more power efficient instructions (even if you technically use more of them), you can save overall power draw.<p>That said, time to market has been more important than any cares of efficiency for some time. Now and in the future, there is more of a focus on it as the expenses in equipment and power have really grown.</div><br/></div></div><div id="41890656" class="c"><input type="checkbox" id="c-41890656" checked=""/><div class="controls bullet"><span class="by">Randor</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890548">parent</a><span>|</span><a href="#41891000">prev</a><span>|</span><a href="#41891150">next</a><span>|</span><label class="collapse" for="c-41890656">[-]</label><label class="expand" for="c-41890656">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t know what that means. It seems like some dubious math with percentages.<p>I would start by downloading a 1.58 model such as: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;HF1BitLLM&#x2F;Llama3-8B-1.58-100B-tokens" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;HF1BitLLM&#x2F;Llama3-8B-1.58-100B-tokens</a><p>Run the non-quantized version of the model on your 3090&#x2F;4090 gpu and observe the power draw. Then load the 1.58 model and observe the power usage. Sure, the numbers have a wide range because there are many gpu&#x2F;npu to make the comparison.</div><br/><div id="41890880" class="c"><input type="checkbox" id="c-41890880" checked=""/><div class="controls bullet"><span class="by">kayo_20211030</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890656">parent</a><span>|</span><a href="#41891150">next</a><span>|</span><label class="collapse" for="c-41890880">[-]</label><label class="expand" for="c-41890880">[1 more]</label></div><br/><div class="children"><div class="content">Good one!</div><br/></div></div></div></div></div></div><div id="41891150" class="c"><input type="checkbox" id="c-41891150" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890352">parent</a><span>|</span><a href="#41890548">prev</a><span>|</span><a href="#41890280">next</a><span>|</span><label class="collapse" for="c-41891150">[-]</label><label class="expand" for="c-41891150">[2 more]</label></div><br/><div class="children"><div class="content">How does the liked article relate to BitNet at all? It&#x27;s about the “addition is all you need” paper which AFAIK is unrelated.</div><br/><div id="41892383" class="c"><input type="checkbox" id="c-41892383" checked=""/><div class="controls bullet"><span class="by">Randor</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41891150">parent</a><span>|</span><a href="#41890280">next</a><span>|</span><label class="collapse" for="c-41892383">[-]</label><label class="expand" for="c-41892383">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I get what you&#x27;re saying but both are challenging the current MatMul methods. The L-Mul paper claims &quot;a power savings of 95%&quot; and that is the thread topic. Bitnet proves that at least 70% is possible by getting rid of MatMul.</div><br/></div></div></div></div></div></div><div id="41890280" class="c"><input type="checkbox" id="c-41890280" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#41890110">parent</a><span>|</span><a href="#41890352">prev</a><span>|</span><a href="#41890428">next</a><span>|</span><label class="collapse" for="c-41890280">[-]</label><label class="expand" for="c-41890280">[1 more]</label></div><br/><div class="children"><div class="content">They’ve been working on unrelated problems like structure of the network or how to build networks with better results. There have been people working on improving the efficiency of the low-level math operations and this is the culmination of those groups. Figuring this stuff out isn’t super easy.</div><br/></div></div><div id="41890428" class="c"><input type="checkbox" id="c-41890428" checked=""/><div class="controls bullet"><span class="by">kayo_20211030</span><span>|</span><a href="#41890110">parent</a><span>|</span><a href="#41890280">prev</a><span>|</span><a href="#41890702">next</a><span>|</span><label class="collapse" for="c-41890428">[-]</label><label class="expand" for="c-41890428">[2 more]</label></div><br/><div class="children"><div class="content">re: all above&#x2F;below comments. It&#x27;s still an extraordinary claim.<p>I&#x27;m not claiming it&#x27;s not possible, nor am I claiming that it&#x27;s not true, or, at least, honest.<p>But, there will need to be evidence that using real machines, and using real energy an _equivalent performance_ is achievable. A defense that &quot;there are no suitable chips&quot; is a bit disingenuous. If the 95% savings actually has legs some smart chip manufacturer will do the math and make the chips. If it&#x27;s correct, that chip making firm will make a fortune. If it&#x27;s not, they won&#x27;t.</div><br/><div id="41891512" class="c"><input type="checkbox" id="c-41891512" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890428">parent</a><span>|</span><a href="#41890702">next</a><span>|</span><label class="collapse" for="c-41891512">[-]</label><label class="expand" for="c-41891512">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If the 95% savings actually has legs some smart chip manufacturer will do the math and make the chips<p>Terrible logic.  By a similar logic we wouldn&#x27;t be using python for machine learning at all, for example (or x86 for compute).  Yet here we are.</div><br/></div></div></div></div><div id="41890702" class="c"><input type="checkbox" id="c-41890702" checked=""/><div class="controls bullet"><span class="by">stefan_</span><span>|</span><a href="#41890110">parent</a><span>|</span><a href="#41890428">prev</a><span>|</span><a href="#41889818">next</a><span>|</span><label class="collapse" for="c-41890702">[-]</label><label class="expand" for="c-41890702">[4 more]</label></div><br/><div class="children"><div class="content">I mean, all these smart people would rather pay NVIDIA all their money than make AMD viable. And yet they tell us its all MatMul.</div><br/><div id="41893117" class="c"><input type="checkbox" id="c-41893117" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890702">parent</a><span>|</span><a href="#41890897">next</a><span>|</span><label class="collapse" for="c-41893117">[-]</label><label class="expand" for="c-41893117">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not their job to make AMD viable, it&#x27;s AMD&#x27;s job to make AMD viable. NVIDIA didn&#x27;t get their position for free, they spent a decade refining CUDA and its tooling before GPU-based crypto and AI kicked off.</div><br/></div></div><div id="41890897" class="c"><input type="checkbox" id="c-41890897" checked=""/><div class="controls bullet"><span class="by">kayo_20211030</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890702">parent</a><span>|</span><a href="#41893117">prev</a><span>|</span><a href="#41889818">next</a><span>|</span><label class="collapse" for="c-41890897">[-]</label><label class="expand" for="c-41890897">[2 more]</label></div><br/><div class="children"><div class="content">Both companies are doing pretty well. Why don&#x27;t you think AMD is viable?</div><br/><div id="41891173" class="c"><input type="checkbox" id="c-41891173" checked=""/><div class="controls bullet"><span class="by">nelup20</span><span>|</span><a href="#41890110">root</a><span>|</span><a href="#41890897">parent</a><span>|</span><a href="#41889818">next</a><span>|</span><label class="collapse" for="c-41891173">[-]</label><label class="expand" for="c-41891173">[1 more]</label></div><br/><div class="children"><div class="content">AMD&#x27;s ROCm just isn&#x27;t there yet compared to Nvidia&#x27;s CUDA. I tried it on Linux with my AMD GPU and couldn&#x27;t get things working. AFAIK on Windows it&#x27;s even worse.</div><br/></div></div></div></div></div></div></div></div><div id="41889818" class="c"><input type="checkbox" id="c-41889818" checked=""/><div class="controls bullet"><span class="by">_aavaa_</span><span>|</span><a href="#41890110">prev</a><span>|</span><a href="#41889747">next</a><span>|</span><label class="collapse" for="c-41889818">[-]</label><label class="expand" for="c-41889818">[2 more]</label></div><br/><div class="children"><div class="content">Original discussion of the preprint: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591</a></div><br/><div id="41891595" class="c"><input type="checkbox" id="c-41891595" checked=""/><div class="controls bullet"><span class="by">codethief</span><span>|</span><a href="#41889818">parent</a><span>|</span><a href="#41889747">next</a><span>|</span><label class="collapse" for="c-41891595">[-]</label><label class="expand" for="c-41891595">[1 more]</label></div><br/><div class="children"><div class="content">Ahh, there it is! I was sure we had discussed this paper before.</div><br/></div></div></div></div><div id="41889747" class="c"><input type="checkbox" id="c-41889747" checked=""/><div class="controls bullet"><span class="by">remexre</span><span>|</span><a href="#41889818">prev</a><span>|</span><a href="#41892974">next</a><span>|</span><label class="collapse" for="c-41889747">[-]</label><label class="expand" for="c-41889747">[10 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this just taking advantage of &quot;log(x) + log(y) = log(xy)&quot;? The IEEE754 floating-point representation stores floats as sign, mantissa, and exponent -- ignore the first two (you quantitized anyway, right?), and the exponent is just an integer storing log() of the float.</div><br/><div id="41890236" class="c"><input type="checkbox" id="c-41890236" checked=""/><div class="controls bullet"><span class="by">mota7</span><span>|</span><a href="#41889747">parent</a><span>|</span><a href="#41889800">next</a><span>|</span><label class="collapse" for="c-41890236">[-]</label><label class="expand" for="c-41890236">[5 more]</label></div><br/><div class="children"><div class="content">Not quite: It&#x27;s taking advantage of (1+a)(1+b) = 1 + a + b + ab.
And where a and b are both small-ish, ab is really small and can just be ignored.<p>So it turns the (1+a)(1+b) into 1+a+b. Which is definitely not the same! But it turns out, machine guessing apparently doesn&#x27;t care much about the difference.</div><br/><div id="41890382" class="c"><input type="checkbox" id="c-41890382" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#41889747">root</a><span>|</span><a href="#41890236">parent</a><span>|</span><a href="#41890513">next</a><span>|</span><label class="collapse" for="c-41890382">[-]</label><label class="expand" for="c-41890382">[2 more]</label></div><br/><div class="children"><div class="content">You might then as well replace the multiplication by the addition in the original network. In that case you&#x27;re not even approximating anything.<p>Am I missing something?</div><br/><div id="41893129" class="c"><input type="checkbox" id="c-41893129" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#41889747">root</a><span>|</span><a href="#41890382">parent</a><span>|</span><a href="#41890513">next</a><span>|</span><label class="collapse" for="c-41893129">[-]</label><label class="expand" for="c-41893129">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re applying that simplification to the exponent bits of an 8 bit float. The range is so small that the approximation to multiplication is going to be pretty close.</div><br/></div></div></div></div><div id="41890513" class="c"><input type="checkbox" id="c-41890513" checked=""/><div class="controls bullet"><span class="by">tommiegannert</span><span>|</span><a href="#41889747">root</a><span>|</span><a href="#41890236">parent</a><span>|</span><a href="#41890382">prev</a><span>|</span><a href="#41892121">next</a><span>|</span><label class="collapse" for="c-41890513">[-]</label><label class="expand" for="c-41890513">[1 more]</label></div><br/><div class="children"><div class="content">Plus the 2^-l(m) correction term.<p>Feels like multiplication shouldn&#x27;t be needed for convergence, just monotonicity? I wonder how well it would perform if the model was actually trained the same way.</div><br/></div></div><div id="41892121" class="c"><input type="checkbox" id="c-41892121" checked=""/><div class="controls bullet"><span class="by">dsv3099i</span><span>|</span><a href="#41889747">root</a><span>|</span><a href="#41890236">parent</a><span>|</span><a href="#41890513">prev</a><span>|</span><a href="#41889800">next</a><span>|</span><label class="collapse" for="c-41892121">[-]</label><label class="expand" for="c-41892121">[1 more]</label></div><br/><div class="children"><div class="content">This trick is used a ton when doing hand calculation in engineering as well.  It can save a lot of work.<p>You&#x27;re going to have tolerance on the result anyway, so what&#x27;s a little more error. :)</div><br/></div></div></div></div><div id="41889800" class="c"><input type="checkbox" id="c-41889800" checked=""/><div class="controls bullet"><span class="by">convolvatron</span><span>|</span><a href="#41889747">parent</a><span>|</span><a href="#41890236">prev</a><span>|</span><a href="#41892974">next</a><span>|</span><label class="collapse" for="c-41889800">[-]</label><label class="expand" for="c-41889800">[4 more]</label></div><br/><div class="children"><div class="content">yes. and the next question is &#x27;ok, how do we add&#x27;</div><br/><div id="41889991" class="c"><input type="checkbox" id="c-41889991" checked=""/><div class="controls bullet"><span class="by">kps</span><span>|</span><a href="#41889747">root</a><span>|</span><a href="#41889800">parent</a><span>|</span><a href="#41889877">next</a><span>|</span><label class="collapse" for="c-41889991">[-]</label><label class="expand" for="c-41889991">[1 more]</label></div><br/><div class="children"><div class="content">Yes. I haven&#x27;t yet read this paper to see what exactly it says is new, but I&#x27;ve definitely seen log-based representations under development before now. (<i>More</i> log-based than the regular floating-point exponent, that is. I don&#x27;t actually know the argument behind the exponent-and-mantissa form that&#x27;s been pretty much universal even before IEEE754, other than that it mimics decimal scientific notation.)</div><br/></div></div><div id="41889877" class="c"><input type="checkbox" id="c-41889877" checked=""/><div class="controls bullet"><span class="by">dietr1ch</span><span>|</span><a href="#41889747">root</a><span>|</span><a href="#41889800">parent</a><span>|</span><a href="#41889991">prev</a><span>|</span><a href="#41892974">next</a><span>|</span><label class="collapse" for="c-41889877">[-]</label><label class="expand" for="c-41889877">[2 more]</label></div><br/><div class="children"><div class="content">I guess that if the bulk of the computation goes into the multiplications, you can work in the log-space and simply sum, and when the time comes to actually do a sum on the original space you can go back and sum.</div><br/><div id="41890126" class="c"><input type="checkbox" id="c-41890126" checked=""/><div class="controls bullet"><span class="by">a-loup-e</span><span>|</span><a href="#41889747">root</a><span>|</span><a href="#41889877">parent</a><span>|</span><a href="#41892974">next</a><span>|</span><label class="collapse" for="c-41890126">[-]</label><label class="expand" for="c-41890126">[1 more]</label></div><br/><div class="children"><div class="content">Not sure how well that would work if you&#x27;re often adding bias after every layer</div><br/></div></div></div></div></div></div></div></div><div id="41892974" class="c"><input type="checkbox" id="c-41892974" checked=""/><div class="controls bullet"><span class="by">Art9681</span><span>|</span><a href="#41889747">prev</a><span>|</span><a href="#41890638">next</a><span>|</span><label class="collapse" for="c-41892974">[-]</label><label class="expand" for="c-41892974">[1 more]</label></div><br/><div class="children"><div class="content">In the end the power consumption means the current models that are &quot;good enough&quot; will fit a much smaller compute budget such as edge devices. However, enthusiasts are still going to want the best hardware they can afford because inevitably, everyone will want to maximize the size and intelligence of a model they can run. So we&#x27;re just going to scale. This might bring a GPT-4 level to edge devices, but we are still going to want to run what might resemble a GPT-5&#x2F;6 model on the best hardware possible at the time. So don&#x27;t throw away your GPU&#x27;s yet. This will bring capabilities to mass market, but your high end GPU will still scale the solution n-fold and youll be able to run models with disregard to the energy savings promoted in the headline.<p>In other sensationalized words: &quot;AI engineers can claim new algorithm allows them to fit GPT-5 in an RTX5090 running at 600 watts.&quot;</div><br/></div></div><div id="41890638" class="c"><input type="checkbox" id="c-41890638" checked=""/><div class="controls bullet"><span class="by">GistNoesis</span><span>|</span><a href="#41892974">prev</a><span>|</span><a href="#41893327">next</a><span>|</span><label class="collapse" for="c-41890638">[-]</label><label class="expand" for="c-41890638">[4 more]</label></div><br/><div class="children"><div class="content">Does <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jevons_paradox" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jevons_paradox</a> apply in this case ?</div><br/><div id="41891906" class="c"><input type="checkbox" id="c-41891906" checked=""/><div class="controls bullet"><span class="by">mattxxx</span><span>|</span><a href="#41890638">parent</a><span>|</span><a href="#41890949">next</a><span>|</span><label class="collapse" for="c-41891906">[-]</label><label class="expand" for="c-41891906">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s interesting.<p>Obviously, energy cost creates a barrier to entry, so reduction of cost reduces the barrier to entry... which adds more players... which increases demand.</div><br/></div></div><div id="41890949" class="c"><input type="checkbox" id="c-41890949" checked=""/><div class="controls bullet"><span class="by">narrator</span><span>|</span><a href="#41890638">parent</a><span>|</span><a href="#41891906">prev</a><span>|</span><a href="#41891105">next</a><span>|</span><label class="collapse" for="c-41890949">[-]</label><label class="expand" for="c-41890949">[1 more]</label></div><br/><div class="children"><div class="content">Of course.  Jevons paradox always applies.</div><br/></div></div><div id="41891105" class="c"><input type="checkbox" id="c-41891105" checked=""/><div class="controls bullet"><span class="by">gosub100</span><span>|</span><a href="#41890638">parent</a><span>|</span><a href="#41890949">prev</a><span>|</span><a href="#41893327">next</a><span>|</span><label class="collapse" for="c-41891105">[-]</label><label class="expand" for="c-41891105">[1 more]</label></div><br/><div class="children"><div class="content">Not necessarily a bad thing: this might give the AI charlatans enough time to actually make something useful.</div><br/></div></div></div></div><div id="41893327" class="c"><input type="checkbox" id="c-41893327" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#41890638">prev</a><span>|</span><a href="#41891092">next</a><span>|</span><label class="collapse" for="c-41893327">[-]</label><label class="expand" for="c-41893327">[1 more]</label></div><br/><div class="children"><div class="content">So couldn&#x27;t you design a GPU that uses or supports this algorithm to use the same power, but use bigger models, better models, or do more work?</div><br/></div></div><div id="41891092" class="c"><input type="checkbox" id="c-41891092" checked=""/><div class="controls bullet"><span class="by">didgetmaster</span><span>|</span><a href="#41893327">prev</a><span>|</span><a href="#41890686">next</a><span>|</span><label class="collapse" for="c-41891092">[-]</label><label class="expand" for="c-41891092">[13 more]</label></div><br/><div class="children"><div class="content">Maybe I am just a natural skeptic, but whenever I see a headline that says &#x27;method x reduces y by z%&#x27;; but when you read the text it instead says that optimizing some step &#x27;could potentially reduce y by up to z%&#x27;; I am suspicious.<p>Why not publish some actual benchmarks that prove your claim in even a few special cases?</div><br/><div id="41891234" class="c"><input type="checkbox" id="c-41891234" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#41891092">parent</a><span>|</span><a href="#41891162">next</a><span>|</span><label class="collapse" for="c-41891234">[-]</label><label class="expand" for="c-41891234">[1 more]</label></div><br/><div class="children"><div class="content">Well, one, because the headline isn&#x27;t from the researchers, its from a popular press report (not even the one posted here, originally, this is secondary reporting of another popular press piece) and isn&#x27;t what the paper claims so it would be odd for the paper&#x27;s authors to conduct benchmarks to justify it. (And, no, even the &quot;up to 95%&quot; isn&#x27;t from the paper, the cost savings are cited per operation depending on operation and the precision the operation is conducted at, are as high as 97.3%, are based on research already done establishing the energy cost of math operations on modern compute hardware, but no end-to-end cost savings claim is made.)<p>And, two, because the actual energy cost savings claimed aren&#x27;t even the experimental question -- the energy cost differences between various operations on modern hardware have been established in other research, the experimental issue here was whether the mathematical technique that enables using the lower energy cost operations performs competitively on output quality with existing implementations when substituted in for LLM inference.</div><br/></div></div><div id="41891162" class="c"><input type="checkbox" id="c-41891162" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#41891092">parent</a><span>|</span><a href="#41891234">prev</a><span>|</span><a href="#41891545">next</a><span>|</span><label class="collapse" for="c-41891162">[-]</label><label class="expand" for="c-41891162">[2 more]</label></div><br/><div class="children"><div class="content">OTOH you have a living proof that an amazingly huge neural network can work on 20W of power, so expecting multiple orders of magnitude in power consumption reduction is not unreasonable.</div><br/><div id="41892082" class="c"><input type="checkbox" id="c-41892082" checked=""/><div class="controls bullet"><span class="by">etcd</span><span>|</span><a href="#41891092">root</a><span>|</span><a href="#41891162">parent</a><span>|</span><a href="#41891545">next</a><span>|</span><label class="collapse" for="c-41892082">[-]</label><label class="expand" for="c-41892082">[1 more]</label></div><br/><div class="children"><div class="content">Mitochondria are all you need.<p>Should be able to go more efficient as the brain has other constraints such as working at 36.7 degrees C etc.</div><br/></div></div></div></div><div id="41891545" class="c"><input type="checkbox" id="c-41891545" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#41891092">parent</a><span>|</span><a href="#41891162">prev</a><span>|</span><a href="#41891148">next</a><span>|</span><label class="collapse" for="c-41891545">[-]</label><label class="expand" for="c-41891545">[4 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;BitNet">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;BitNet</a><p>&quot;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of 1.37x to 5.07x on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by 55.4% to 70.0%, further boosting overall efficiency. On x86 CPUs, speedups range from 2.37x to 6.17x with energy reductions between 71.9% to 82.2%. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. More details will be provided soon.&quot;</div><br/><div id="41891612" class="c"><input type="checkbox" id="c-41891612" checked=""/><div class="controls bullet"><span class="by">jdiez17</span><span>|</span><a href="#41891092">root</a><span>|</span><a href="#41891545">parent</a><span>|</span><a href="#41891148">next</a><span>|</span><label class="collapse" for="c-41891612">[-]</label><label class="expand" for="c-41891612">[3 more]</label></div><br/><div class="children"><div class="content">Damn. Seems almost too good to be true. Let’s see where this goes in two weeks.</div><br/><div id="41891735" class="c"><input type="checkbox" id="c-41891735" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#41891092">root</a><span>|</span><a href="#41891612">parent</a><span>|</span><a href="#41891148">next</a><span>|</span><label class="collapse" for="c-41891735">[-]</label><label class="expand" for="c-41891735">[2 more]</label></div><br/><div class="children"><div class="content">Intel and AMD will be extremely happy.<p>Nvidia will be very unhappy.</div><br/><div id="41892139" class="c"><input type="checkbox" id="c-41892139" checked=""/><div class="controls bullet"><span class="by">l11r</span><span>|</span><a href="#41891092">root</a><span>|</span><a href="#41891735">parent</a><span>|</span><a href="#41891148">next</a><span>|</span><label class="collapse" for="c-41892139">[-]</label><label class="expand" for="c-41892139">[1 more]</label></div><br/><div class="children"><div class="content">Their GPU will still be needed to do training. As far as I understand this will improve only interference performance and efficiency.</div><br/></div></div></div></div></div></div></div></div><div id="41891148" class="c"><input type="checkbox" id="c-41891148" checked=""/><div class="controls bullet"><span class="by">TheRealPomax</span><span>|</span><a href="#41891092">parent</a><span>|</span><a href="#41891545">prev</a><span>|</span><a href="#41890686">next</a><span>|</span><label class="collapse" for="c-41891148">[-]</label><label class="expand" for="c-41891148">[5 more]</label></div><br/><div class="children"><div class="content">Because as disappointing as modern life is, you need clickbait headlines to drive traffic. You did the right thing by reading the article though, that&#x27;s where the information is, not the title.</div><br/><div id="41891215" class="c"><input type="checkbox" id="c-41891215" checked=""/><div class="controls bullet"><span class="by">phtrivier</span><span>|</span><a href="#41891092">root</a><span>|</span><a href="#41891148">parent</a><span>|</span><a href="#41891868">next</a><span>|</span><label class="collapse" for="c-41891215">[-]</label><label class="expand" for="c-41891215">[3 more]</label></div><br/><div class="children"><div class="content">Fair enough, but then I want a way to penalize publishers for abusing clickbait. There is no &quot;unread&quot; button, and there is no way to unsubscribe to advertisement-based sites.<p>Even on sites that have a &quot;Like &#x2F; Don&#x27;t like&quot; button, my understanding is that clicking &quot;Don&#x27;t like&quot; is a form of &quot;engagement&quot;, that the suggestion algorithm are going to reward.<p>Give me a button that says &quot;this article was a scam&quot;, and have the publisher give the advertisement money back. Of better yet, give the advertisement money to charity &#x2F; public services &#x2F; whatever.<p>Take a cut of the money being transfered, charge the publishers for being able to get a &quot;clickbait free&quot; green mark if they implement the scheme.<p>Track the kind of articles that generate the most clickbait-angry comment. Sell back the data.<p>There might a business model.</div><br/><div id="41891295" class="c"><input type="checkbox" id="c-41891295" checked=""/><div class="controls bullet"><span class="by">NineStarPoint</span><span>|</span><a href="#41891092">root</a><span>|</span><a href="#41891215">parent</a><span>|</span><a href="#41891868">next</a><span>|</span><label class="collapse" for="c-41891295">[-]</label><label class="expand" for="c-41891295">[2 more]</label></div><br/><div class="children"><div class="content">I doubt there’s a business model there because who is going to opt in to a scheme that loses them money?<p>What could work is social media giving people an easy button to block links to specific websites from appearing in their feed, or something along those lines. It’s a nice user feature, and having every clickbait article be a chance  someone will choose to never see your website again could actually reign in some of the nonsense.</div><br/><div id="41893884" class="c"><input type="checkbox" id="c-41893884" checked=""/><div class="controls bullet"><span class="by">phtrivier</span><span>|</span><a href="#41891092">root</a><span>|</span><a href="#41891295">parent</a><span>|</span><a href="#41891868">next</a><span>|</span><label class="collapse" for="c-41893884">[-]</label><label class="expand" for="c-41893884">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I doubt there’s a business model there because who is going to opt in to a scheme that loses them money?<p>Agreed, of course.<p>In a reasonable world, that could be considered part of the basic, law mandated requirements. It would be blurry and subject to interpretation to decide what is clickbait or not, just like libel or defamation - good thing we&#x27;re only a few hundred years away from someone reinventing a device to handle that,  called &quot;independent judges&quot;.<p>In the meantime, I suppose you would have to bring some &quot;unreasonable&quot; thing to it, like &quot;brands like to have green logos on their sites to brag&quot; ?<p>&gt; What could work is social media giving people an easy button to block links to specific websites from appearing in their feed, or something along those lines.<p>I completely agree. It&#x27;s a feature they have had the technology to implement such a thing since forever, and they&#x27;ve decided against it since forever.<p>However I wonder if that&#x27;s something a browser extension could handle ? A merge of AdBlock and &quot;saved you a click&quot; that displays the &quot;boring&quot; content of the link when you hoveron a clickbaity link ?</div><br/></div></div></div></div></div></div><div id="41891868" class="c"><input type="checkbox" id="c-41891868" checked=""/><div class="controls bullet"><span class="by">keybored</span><span>|</span><a href="#41891092">root</a><span>|</span><a href="#41891148">parent</a><span>|</span><a href="#41891215">prev</a><span>|</span><a href="#41890686">next</a><span>|</span><label class="collapse" for="c-41891868">[-]</label><label class="expand" for="c-41891868">[1 more]</label></div><br/><div class="children"><div class="content">Headlines: what can they do, they need that for the traffic<p>Reader: do the moral thing and read the article, not just the title<p>How is that balanced.</div><br/></div></div></div></div></div></div><div id="41890686" class="c"><input type="checkbox" id="c-41890686" checked=""/><div class="controls bullet"><span class="by">panosv</span><span>|</span><a href="#41891092">prev</a><span>|</span><a href="#41890412">next</a><span>|</span><label class="collapse" for="c-41890686">[-]</label><label class="expand" for="c-41890686">[1 more]</label></div><br/><div class="children"><div class="content">Lemurian Labs looks like it&#x27;s doing something similar: <a href="https:&#x2F;&#x2F;www.lemurianlabs.com&#x2F;technology" rel="nofollow">https:&#x2F;&#x2F;www.lemurianlabs.com&#x2F;technology</a> They use the Logarithmic Number System (LNS)</div><br/></div></div><div id="41890412" class="c"><input type="checkbox" id="c-41890412" checked=""/><div class="controls bullet"><span class="by">idiliv</span><span>|</span><a href="#41890686">prev</a><span>|</span><a href="#41890372">next</a><span>|</span><label class="collapse" for="c-41890412">[-]</label><label class="expand" for="c-41890412">[1 more]</label></div><br/><div class="children"><div class="content">Duplicate, posted on October 9: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591</a></div><br/></div></div><div id="41890178" class="c"><input type="checkbox" id="c-41890178" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#41890372">prev</a><span>|</span><a href="#41890578">next</a><span>|</span><label class="collapse" for="c-41890178">[-]</label><label class="expand" for="c-41890178">[1 more]</label></div><br/><div class="children"><div class="content">I’m looking forward to Bitnet adaptation. MS just released a tool for it similar to llamacpp. Really hoping major models get retrained for it.</div><br/></div></div><div id="41890578" class="c"><input type="checkbox" id="c-41890578" checked=""/><div class="controls bullet"><span class="by">asicsarecool</span><span>|</span><a href="#41890178">prev</a><span>|</span><a href="#41892664">next</a><span>|</span><label class="collapse" for="c-41890578">[-]</label><label class="expand" for="c-41890578">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t assume this isn&#x27;t already in place at the main AI companies</div><br/></div></div><div id="41892664" class="c"><input type="checkbox" id="c-41892664" checked=""/><div class="controls bullet"><span class="by">tartakovsky</span><span>|</span><a href="#41890578">prev</a><span>|</span><a href="#41890056">next</a><span>|</span><label class="collapse" for="c-41892664">[-]</label><label class="expand" for="c-41892664">[1 more]</label></div><br/><div class="children"><div class="content">original paper:   <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591</a></div><br/></div></div><div id="41890056" class="c"><input type="checkbox" id="c-41890056" checked=""/><div class="controls bullet"><span class="by">robomartin</span><span>|</span><a href="#41892664">prev</a><span>|</span><a href="#41893183">next</a><span>|</span><label class="collapse" for="c-41890056">[-]</label><label class="expand" for="c-41890056">[4 more]</label></div><br/><div class="children"><div class="content">I posted this about a week ago:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41816598">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41816598</a><p>This has been done for decades in digital circuits, FPGA’s, Digital Signal Processing, etc. Floating point is both resource and power intensive and using FP without the use of dedicated FP processing hardware is something that has been avoided and done without for decades unless absolutely necessary.</div><br/><div id="41890812" class="c"><input type="checkbox" id="c-41890812" checked=""/><div class="controls bullet"><span class="by">fidotron</span><span>|</span><a href="#41890056">parent</a><span>|</span><a href="#41890498">next</a><span>|</span><label class="collapse" for="c-41890812">[-]</label><label class="expand" for="c-41890812">[1 more]</label></div><br/><div class="children"><div class="content">Right, the ML people are learning, slowly, about the importance of optimizing for silicon simplicity, not just reduction of symbols in linear algebra.<p>Their rediscovery of fixed point was bad enough but the “omg if we represent poses as quaternions everything works better” makes any game engine dev for the last 30 years explode.</div><br/></div></div><div id="41890498" class="c"><input type="checkbox" id="c-41890498" checked=""/><div class="controls bullet"><span class="by">ausbah</span><span>|</span><a href="#41890056">parent</a><span>|</span><a href="#41890812">prev</a><span>|</span><a href="#41890331">next</a><span>|</span><label class="collapse" for="c-41890498">[-]</label><label class="expand" for="c-41890498">[1 more]</label></div><br/><div class="children"><div class="content">a lot of things in the ML research space are rebranding an old concept w a new name as “novel”</div><br/></div></div><div id="41890331" class="c"><input type="checkbox" id="c-41890331" checked=""/><div class="controls bullet"><span class="by">ujikoluk</span><span>|</span><a href="#41890056">parent</a><span>|</span><a href="#41890498">prev</a><span>|</span><a href="#41893183">next</a><span>|</span><label class="collapse" for="c-41890331">[-]</label><label class="expand" for="c-41890331">[1 more]</label></div><br/><div class="children"><div class="content">Explain more for the uninitiated please.</div><br/></div></div></div></div><div id="41893183" class="c"><input type="checkbox" id="c-41893183" checked=""/><div class="controls bullet"><span class="by">nprateem</span><span>|</span><a href="#41890056">prev</a><span>|</span><a href="#41892760">next</a><span>|</span><label class="collapse" for="c-41893183">[-]</label><label class="expand" for="c-41893183">[1 more]</label></div><br/><div class="children"><div class="content">Is it the one where you delete 95% of user accounts?</div><br/></div></div><div id="41892760" class="c"><input type="checkbox" id="c-41892760" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41893183">prev</a><span>|</span><a href="#41891209">next</a><span>|</span><label class="collapse" for="c-41892760">[-]</label><label class="expand" for="c-41892760">[1 more]</label></div><br/><div class="children"><div class="content">This sounds similar to someone saying room temp super conductor was discovered</div><br/></div></div><div id="41891209" class="c"><input type="checkbox" id="c-41891209" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#41892760">prev</a><span>|</span><a href="#41891129">next</a><span>|</span><label class="collapse" for="c-41891209">[-]</label><label class="expand" for="c-41891209">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think algorithms will change energy consumption. There is always max capacity needed in terms of computing. If tomorrow a new algorithm increases the performance 4 times, we will just have 4 times more computing.</div><br/></div></div><div id="41891129" class="c"><input type="checkbox" id="c-41891129" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41891209">prev</a><span>|</span><a href="#41892087">next</a><span>|</span><label class="collapse" for="c-41891129">[-]</label><label class="expand" for="c-41891129">[1 more]</label></div><br/><div class="children"><div class="content">Related: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41784591</a> 10 days ago</div><br/></div></div><div id="41892087" class="c"><input type="checkbox" id="c-41892087" checked=""/><div class="controls bullet"><span class="by">neuroelectron</span><span>|</span><a href="#41891129">prev</a><span>|</span><a href="#41890765">next</a><span>|</span><label class="collapse" for="c-41892087">[-]</label><label class="expand" for="c-41892087">[1 more]</label></div><br/><div class="children"><div class="content">Nobody is interested in this because nobody wants less capex.</div><br/></div></div><div id="41890765" class="c"><input type="checkbox" id="c-41890765" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#41892087">prev</a><span>|</span><a href="#41890624">next</a><span>|</span><label class="collapse" for="c-41890765">[-]</label><label class="expand" for="c-41890765">[1 more]</label></div><br/><div class="children"><div class="content">Here is the Microsoft implementation:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;BitNet">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;BitNet</a></div><br/></div></div><div id="41890624" class="c"><input type="checkbox" id="c-41890624" checked=""/><div class="controls bullet"><span class="by">hello_computer</span><span>|</span><a href="#41890765">prev</a><span>|</span><a href="#41892460">next</a><span>|</span><label class="collapse" for="c-41890624">[-]</label><label class="expand" for="c-41890624">[2 more]</label></div><br/><div class="children"><div class="content">How does this differ from Cussen &amp; Ullman?<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.01415" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.01415</a></div><br/><div id="41892483" class="c"><input type="checkbox" id="c-41892483" checked=""/><div class="controls bullet"><span class="by">selimthegrim</span><span>|</span><a href="#41890624">parent</a><span>|</span><a href="#41892460">next</a><span>|</span><label class="collapse" for="c-41892483">[-]</label><label class="expand" for="c-41892483">[1 more]</label></div><br/><div class="children"><div class="content">Cussen is an HN poster incidentally.</div><br/></div></div></div></div><div id="41892460" class="c"><input type="checkbox" id="c-41892460" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#41890624">prev</a><span>|</span><a href="#41891196">next</a><span>|</span><label class="collapse" for="c-41892460">[-]</label><label class="expand" for="c-41892460">[1 more]</label></div><br/><div class="children"><div class="content">As a rule, compute only takes less than 10% of all energy. 90% is data movement.</div><br/></div></div><div id="41891196" class="c"><input type="checkbox" id="c-41891196" checked=""/><div class="controls bullet"><span class="by">greenthrow</span><span>|</span><a href="#41892460">prev</a><span>|</span><a href="#41890285">next</a><span>|</span><label class="collapse" for="c-41891196">[-]</label><label class="expand" for="c-41891196">[1 more]</label></div><br/><div class="children"><div class="content">The trend of hyping up papers too early on is eroding people&#x27;s faith in science due to poor journalism failing to explain that this is theoretical. The outlets that do this should pay the price but they don&#x27;t, because almost every outlet does it.</div><br/></div></div><div id="41890285" class="c"><input type="checkbox" id="c-41890285" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#41891196">prev</a><span>|</span><a href="#41890580">next</a><span>|</span><label class="collapse" for="c-41890285">[-]</label><label class="expand" for="c-41890285">[6 more]</label></div><br/><div class="children"><div class="content">The ultimate “you’re doing it wrong”.<p>For he sake of the climate and environment it would be nice to be true.<p>Bad news for Nvidia. “Sell your stock” bad.<p>Does it come with a demonstration?</div><br/><div id="41890991" class="c"><input type="checkbox" id="c-41890991" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#41890285">parent</a><span>|</span><a href="#41890507">next</a><span>|</span><label class="collapse" for="c-41890991">[-]</label><label class="expand" for="c-41890991">[1 more]</label></div><br/><div class="children"><div class="content">Bad news for Nvidia how?  Even ignoring that the power savings are only on one type of instruction, 20x less power doesn&#x27;t mean it runs 20x faster.  You still need big fat GPUs.<p>If this increases integer demand and decreases floating point demand, that moderately changes future product design and doesn&#x27;t do much else.</div><br/></div></div><div id="41890507" class="c"><input type="checkbox" id="c-41890507" checked=""/><div class="controls bullet"><span class="by">mouse_</span><span>|</span><a href="#41890285">parent</a><span>|</span><a href="#41890991">prev</a><span>|</span><a href="#41890492">next</a><span>|</span><label class="collapse" for="c-41890507">[-]</label><label class="expand" for="c-41890507">[2 more]</label></div><br/><div class="children"><div class="content">Hypothetically, if this is true and simple as the headline implies -- AI using 95% less power doesn&#x27;t mean AI will use 95% less power, it means we will do 20x more AI. As long as it&#x27;s the current fad, we will throw as much power and resources at this as we can physically produce, because our economy depends on constant, accelerating growth.</div><br/><div id="41892105" class="c"><input type="checkbox" id="c-41892105" checked=""/><div class="controls bullet"><span class="by">etcd</span><span>|</span><a href="#41890285">root</a><span>|</span><a href="#41890507">parent</a><span>|</span><a href="#41890492">next</a><span>|</span><label class="collapse" for="c-41892105">[-]</label><label class="expand" for="c-41892105">[1 more]</label></div><br/><div class="children"><div class="content">True. A laptop power pack wattage is probably pretty much unchanged  over 30 years for example.</div><br/></div></div></div></div><div id="41890492" class="c"><input type="checkbox" id="c-41890492" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#41890285">parent</a><span>|</span><a href="#41890507">prev</a><span>|</span><a href="#41892401">next</a><span>|</span><label class="collapse" for="c-41890492">[-]</label><label class="expand" for="c-41890492">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Bad news for Nvidia. “Sell your stock” bad.<p>People say this but then the fastest and most-used implementation of these optimizations is always written in CUDA. If this turns out to not be a hoax, I wouldn&#x27;t be surprised to see Nvidia prices <i>jump</i> in correlation.</div><br/></div></div><div id="41892401" class="c"><input type="checkbox" id="c-41892401" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#41890285">parent</a><span>|</span><a href="#41890492">prev</a><span>|</span><a href="#41890580">next</a><span>|</span><label class="collapse" for="c-41892401">[-]</label><label class="expand" for="c-41892401">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t reduced power consumption for an unfulfilled demand mean more demand for Nvida as they now need more chips to max out amount of power usage to capacity? (As concentration tends to be the more efficient way.)</div><br/></div></div></div></div><div id="41890580" class="c"><input type="checkbox" id="c-41890580" checked=""/><div class="controls bullet"><span class="by">DesiLurker</span><span>|</span><a href="#41890285">prev</a><span>|</span><a href="#41889797">next</a><span>|</span><label class="collapse" for="c-41890580">[-]</label><label class="expand" for="c-41890580">[1 more]</label></div><br/><div class="children"><div class="content">validity of the claim aside, why dont they say reduces by 20 times instead of 95%. its much better perspective of a fraction when fraction is tiny.</div><br/></div></div><div id="41890862" class="c"><input type="checkbox" id="c-41890862" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41889797">prev</a><span>|</span><label class="collapse" for="c-41890862">[-]</label><label class="expand" for="c-41890862">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if someone has feed this entire &quot;problem&quot; into the latest Chat GPT-01 (the new model with reasoning capability), and just fed it in all the code for a Multilayer Perceptron and then given it the task&#x2F;prompt of finding ways to implement the same network using only integer operations.<p>Surely even the OpenAI devs must have done this like the minute they got done training that model, right? I wonder if they&#x27;d even admit it was an AI that came up with the solution rather than just publishing it, and taking credit. haha.</div><br/><div id="41891120" class="c"><input type="checkbox" id="c-41891120" checked=""/><div class="controls bullet"><span class="by">chx</span><span>|</span><a href="#41890862">parent</a><span>|</span><label class="collapse" for="c-41891120">[-]</label><label class="expand" for="c-41891120">[3 more]</label></div><br/><div class="children"><div class="content">You are imaging LLMs are capable of much more than they actually are. Here&#x27;s the <i>only</i> thing they are good for.<p><a href="https:&#x2F;&#x2F;hachyderm.io&#x2F;@inthehands&#x2F;112006855076082650" rel="nofollow">https:&#x2F;&#x2F;hachyderm.io&#x2F;@inthehands&#x2F;112006855076082650</a><p>&gt; You might be surprised to learn that I actually think LLMs have the potential to be not only fun but genuinely useful. “Show me some bullshit that would be typical in this context” can be a genuinely helpful question to have answered, in code and in natural language — for brainstorming, for seeing common conventions in an unfamiliar context, for having something crappy to react to.<p>&gt; Alas, that does not remotely resemble how people are pitching this technology.</div><br/><div id="41893002" class="c"><input type="checkbox" id="c-41893002" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41890862">root</a><span>|</span><a href="#41891120">parent</a><span>|</span><label class="collapse" for="c-41893002">[-]</label><label class="expand" for="c-41893002">[2 more]</label></div><br/><div class="children"><div class="content">No, I&#x27;m not imagining things. You are, however, imaging (incorrectly) that I&#x27;m not an expert with AI who&#x27;s already seen superhuman performance out of LLM prompts in the vast majority of every software development question I&#x27;ve ever asked them, starting all the way back at GPT-3.5.</div><br/><div id="41893148" class="c"><input type="checkbox" id="c-41893148" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#41890862">root</a><span>|</span><a href="#41893002">parent</a><span>|</span><label class="collapse" for="c-41893148">[-]</label><label class="expand" for="c-41893148">[1 more]</label></div><br/><div class="children"><div class="content">So, where are all of your world changing innovations driven by these superhuman capabilities?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>