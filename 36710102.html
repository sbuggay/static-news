<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1689325260011" as="style"/><link rel="stylesheet" href="styles.css?v=1689325260011"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.uber.com/blog/neural-networks-jpeg/">Faster neural networks straight from JPEG (2018)</a> <span class="domain">(<a href="https://www.uber.com">www.uber.com</a>)</span></div><div class="subtext"><span>Anon84</span> | <span>88 comments</span></div><br/><div><div id="36713278" class="c"><input type="checkbox" id="c-36713278" checked=""/><div class="controls bullet"><span class="by">mochomocha</span><span>|</span><a href="#36710360">next</a><span>|</span><label class="collapse" for="c-36713278">[-]</label><label class="expand" for="c-36713278">[39 more]</label></div><br/><div class="children"><div class="content">I was doing the same thing at Netflix around the same time as a 20% research project. Training GANs end2end directly in JPEG coeffs space (and then rebuild a JPEG from the generated coeffs using libjpeg to get an image). The pitch was that it not only worked, but you could get fast training by representing each JPEG block as a dense + sparse vector (dense for the low DCT coeffs, sparse for the high ones since they&#x27;re ~all zeros) and using a neural network library with fast ops on sparse data.<p>Training on pixels is inefficient. Why have your first layers of CNNs relearn what&#x27;s already smartly encoded in the JPEG bits in the first place before it&#x27;s blown into a bloated  height x width x 3 float matrix?</div><br/><div id="36713470" class="c"><input type="checkbox" id="c-36713470" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#36713278">parent</a><span>|</span><a href="#36713691">next</a><span>|</span><label class="collapse" for="c-36713470">[-]</label><label class="expand" for="c-36713470">[27 more]</label></div><br/><div class="children"><div class="content">As an AI armchair quarterback, I&#x27;ve always held the opinion that the image ML space has a counter-productive bias towards not pre-processing images stemming from a mixture of test purity and academic hubris. &quot;Look what this method can learn from raw data completely independently!&quot; makes for a nice paper. So, they stick with sRGB inputs rather than doing basic classic transforms like converting to YUV420. Everyone learns from the papers, so that&#x27;s assumed to be the standard practice.</div><br/><div id="36715384" class="c"><input type="checkbox" id="c-36715384" checked=""/><div class="controls bullet"><span class="by">fxtentacle</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713470">parent</a><span>|</span><a href="#36714543">next</a><span>|</span><label class="collapse" for="c-36715384">[-]</label><label class="expand" for="c-36715384">[1 more]</label></div><br/><div class="children"><div class="content">In my experience, staying close to the storage format is very useful because it allows the neural network to correctly deal with clipped&#x2F;saturated values. If your file is saved in sRGB and you train in sRGB, then when something turns to 0 or 255, the AI can handle it as a special case because most likely it was too bright or too dark for your sensor to capture accurately. If you first transform to a different color space, that clear clip boundary gets lost.<p>Also, I would prefer sRGB oder RGB because it more closely matches the human vision system. That said, the RGB to YUV transformation is effectively a matrix multiplication, so if you use conv features like everyone then you can merge it into your weights for free.</div><br/></div></div><div id="36714543" class="c"><input type="checkbox" id="c-36714543" checked=""/><div class="controls bullet"><span class="by">CodesInChaos</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713470">parent</a><span>|</span><a href="#36715384">prev</a><span>|</span><a href="#36716426">next</a><span>|</span><label class="collapse" for="c-36714543">[-]</label><label class="expand" for="c-36714543">[6 more]</label></div><br/><div class="children"><div class="content">Typical CNNs can learn a linear transformation of the input at no cost. Since YUV is such a linear transformation of RGB, there is no benefit in converting to it beforehand.</div><br/><div id="36714830" class="c"><input type="checkbox" id="c-36714830" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36714543">parent</a><span>|</span><a href="#36716426">next</a><span>|</span><label class="collapse" for="c-36714830">[-]</label><label class="expand" for="c-36714830">[5 more]</label></div><br/><div class="children"><div class="content">How is there not a cost associated with forcing the machine to learn how to do something that we already have a simple, deterministic algorithm for? Won&#x27;t some engineer need to double check a few things with regard to the AI&#x27;s idea of color space transform?</div><br/><div id="36714960" class="c"><input type="checkbox" id="c-36714960" checked=""/><div class="controls bullet"><span class="by">CodesInChaos</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36714830">parent</a><span>|</span><a href="#36714890">next</a><span>|</span><label class="collapse" for="c-36714960">[-]</label><label class="expand" for="c-36714960">[3 more]</label></div><br/><div class="children"><div class="content">You could probably derive some smart initialization for the first layer of a NN based on domain knowledge (color spaces, sobel filters, etc.). But since this is such a small part of what the NN has to learn, I expect this to result in a small improvement in training time and have no effect on final performance and accuracy, so it&#x27;s unlikely to be worth the complexity of developing such a feature.</div><br/><div id="36716413" class="c"><input type="checkbox" id="c-36716413" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36714960">parent</a><span>|</span><a href="#36714890">next</a><span>|</span><label class="collapse" for="c-36716413">[-]</label><label class="expand" for="c-36716413">[2 more]</label></div><br/><div class="children"><div class="content">Absolutely this.<p>Seems like on HN people are still learning &#x27;the bitter lesson&#x27;.</div><br/><div id="36717853" class="c"><input type="checkbox" id="c-36717853" checked=""/><div class="controls bullet"><span class="by">CSSer</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36716413">parent</a><span>|</span><a href="#36714890">next</a><span>|</span><label class="collapse" for="c-36717853">[-]</label><label class="expand" for="c-36717853">[1 more]</label></div><br/><div class="children"><div class="content">Amdahl’s law?</div><br/></div></div></div></div></div></div><div id="36714890" class="c"><input type="checkbox" id="c-36714890" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36714830">parent</a><span>|</span><a href="#36714960">prev</a><span>|</span><a href="#36716426">next</a><span>|</span><label class="collapse" for="c-36714890">[-]</label><label class="expand" for="c-36714890">[1 more]</label></div><br/><div class="children"><div class="content">Your instincts are correct. Training is faster, more stable, and more efficient that way. In certain cases it &quot;pretty much is irrelevant&quot; but the advantages of the strategy of modelling the knowns and training only on the unknowns becomes starkly apparent when doing e.g. sensor fusion or other ML tasks on physical systems.</div><br/></div></div></div></div></div></div><div id="36716426" class="c"><input type="checkbox" id="c-36716426" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713470">parent</a><span>|</span><a href="#36714543">prev</a><span>|</span><a href="#36714393">next</a><span>|</span><label class="collapse" for="c-36716426">[-]</label><label class="expand" for="c-36716426">[9 more]</label></div><br/><div class="children"><div class="content">In deep ML, people are pretty familiar with the bitter lesson and don&#x27;t want to waste time on this.</div><br/><div id="36716681" class="c"><input type="checkbox" id="c-36716681" checked=""/><div class="controls bullet"><span class="by">Llamamoe</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36716426">parent</a><span>|</span><a href="#36719222">next</a><span>|</span><label class="collapse" for="c-36716681">[-]</label><label class="expand" for="c-36716681">[7 more]</label></div><br/><div class="children"><div class="content">The bitter lesson is about not trying to encode impossible-to-formalize conceptual knowledge, not avoiding data efficiency and the need to scale the model up to ever higher parameter counts.<p>If we followed this logic, we&#x27;d be training LLMs on character-level UTF-32 and just letting it figure everything out by itself, while needing two orders of magnitude bigger contexts and parameter counts.</div><br/><div id="36716974" class="c"><input type="checkbox" id="c-36716974" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36716681">parent</a><span>|</span><a href="#36719590">next</a><span>|</span><label class="collapse" for="c-36716974">[-]</label><label class="expand" for="c-36716974">[1 more]</label></div><br/><div class="children"><div class="content">Converting from RGB to YUV is absolutely subject to the bitter lesson because it is trying to generalize from a representation that we have seen works for some classical methods and hard code that knowledge in to the AI which could easily learn (and will anyways) a more useful representation for itself.<p>&gt; LLMs on character-level UTF-32 and just letting it figure everything out by itself, while needing two orders of magnitude bigger contexts and parameter counts.<p>This was tried extensively and honestly it is probably <i>still</i> too early to proclaim the demise of this approach. It&#x27;s also completely different - you&#x27;re conflating a representation that literally changes the number of forward passes you have to do (ie. the amount of computation - what the bitter lesson is about) vs. one that (at most) would just require stacking on a few layers or so.<p>A better example for your point (imo) would be audio recognition, where we pre-transform from wave amplitudes into log mel spectrogram for ingestion by the model. I think this will ultimately fall to the bitter lesson as well though.<p>Also a key difference is that you are proposing going from methods that <i>already work</i> to try to inject more classical knowledge into them. It is oftentimes the case that you&#x27;ll have an intermediary fusion between deep + classical, but not if you already have working fully deep methods.</div><br/></div></div><div id="36719590" class="c"><input type="checkbox" id="c-36719590" checked=""/><div class="controls bullet"><span class="by">ummonk</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36716681">parent</a><span>|</span><a href="#36716974">prev</a><span>|</span><a href="#36717317">next</a><span>|</span><label class="collapse" for="c-36719590">[-]</label><label class="expand" for="c-36719590">[2 more]</label></div><br/><div class="children"><div class="content">Heck why even go that far? Given how much texts we have in scanned books, just feed it scans of the books and let it dedicate a bunch of layers to learning OCR.</div><br/><div id="36719895" class="c"><input type="checkbox" id="c-36719895" checked=""/><div class="controls bullet"><span class="by">philipphutterer</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36719590">parent</a><span>|</span><a href="#36717317">next</a><span>|</span><label class="collapse" for="c-36719895">[-]</label><label class="expand" for="c-36719895">[1 more]</label></div><br/><div class="children"><div class="content">Or given the number of unscanned books, even just give it the controls for a book scanner, the books and probably some robot arms. Then let it figure out the scanning first in some layers. Shouldn&#x27;t be that hard.</div><br/></div></div></div></div><div id="36717317" class="c"><input type="checkbox" id="c-36717317" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36716681">parent</a><span>|</span><a href="#36719590">prev</a><span>|</span><a href="#36719222">next</a><span>|</span><label class="collapse" for="c-36717317">[-]</label><label class="expand" for="c-36717317">[3 more]</label></div><br/><div class="children"><div class="content">This will absolutely be the case N doublings of Moore&#x27;s law from here. Tokens are information loss.</div><br/><div id="36718944" class="c"><input type="checkbox" id="c-36718944" checked=""/><div class="controls bullet"><span class="by">d110af5ccf</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36717317">parent</a><span>|</span><a href="#36719272">next</a><span>|</span><label class="collapse" for="c-36718944">[-]</label><label class="expand" for="c-36718944">[1 more]</label></div><br/><div class="children"><div class="content">Information loss, or the result of useful computation? VAEs exist after all.</div><br/></div></div><div id="36719272" class="c"><input type="checkbox" id="c-36719272" checked=""/><div class="controls bullet"><span class="by">Aerbil313</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36717317">parent</a><span>|</span><a href="#36718944">prev</a><span>|</span><a href="#36719222">next</a><span>|</span><label class="collapse" for="c-36719272">[-]</label><label class="expand" for="c-36719272">[1 more]</label></div><br/><div class="children"><div class="content">Keep in mind Moore&#x27;s law is coming to its end.</div><br/></div></div></div></div></div></div><div id="36719222" class="c"><input type="checkbox" id="c-36719222" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36716426">parent</a><span>|</span><a href="#36716681">prev</a><span>|</span><a href="#36714393">next</a><span>|</span><label class="collapse" for="c-36719222">[-]</label><label class="expand" for="c-36719222">[1 more]</label></div><br/><div class="children"><div class="content">If you want to play by the bitter lesson, why don&#x27;t you just feed the raw JPEG bits into your neural network?</div><br/></div></div></div></div><div id="36714393" class="c"><input type="checkbox" id="c-36714393" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713470">parent</a><span>|</span><a href="#36716426">prev</a><span>|</span><a href="#36719065">next</a><span>|</span><label class="collapse" for="c-36714393">[-]</label><label class="expand" for="c-36714393">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also probably because a lot of knowledge about how jpeg works is tied up signal processing books that usually front load a bunch of mathematics as opposed to ML which often needs a bit of mathematical intuition but in practice is usually empirical.</div><br/><div id="36715160" class="c"><input type="checkbox" id="c-36715160" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36714393">parent</a><span>|</span><a href="#36719065">next</a><span>|</span><label class="collapse" for="c-36715160">[-]</label><label class="expand" for="c-36715160">[3 more]</label></div><br/><div class="children"><div class="content">I was trying to learn how the discrete cosine transform works, so I looked up some code in an open source program. The code said it copied it verbatim from a book from the 90s. I looked up the book and the book said it copied it verbatim from a paper from the 1970s.</div><br/><div id="36716171" class="c"><input type="checkbox" id="c-36716171" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36715160">parent</a><span>|</span><a href="#36719065">next</a><span>|</span><label class="collapse" for="c-36716171">[-]</label><label class="expand" for="c-36716171">[2 more]</label></div><br/><div class="children"><div class="content">The code is irrelevant, in the eyes of the theoretician anyway.</div><br/><div id="36717205" class="c"><input type="checkbox" id="c-36717205" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36716171">parent</a><span>|</span><a href="#36719065">next</a><span>|</span><label class="collapse" for="c-36717205">[-]</label><label class="expand" for="c-36717205">[1 more]</label></div><br/><div class="children"><div class="content">I am theoretically challenged.</div><br/></div></div></div></div></div></div></div></div><div id="36719065" class="c"><input type="checkbox" id="c-36719065" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713470">parent</a><span>|</span><a href="#36714393">prev</a><span>|</span><a href="#36713790">next</a><span>|</span><label class="collapse" for="c-36719065">[-]</label><label class="expand" for="c-36719065">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; rather than doing basic classic transforms like converting to YUV420</i><p>What will converting to YUV420 achieve though, except for 4:2:0 chroma subsampling? YUV has little basis in human perception to begin with, it&#x27;s a color television legacy model used for compression. There are much better models if you want to extract the perceptual information from the picture.</div><br/></div></div><div id="36713790" class="c"><input type="checkbox" id="c-36713790" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713470">parent</a><span>|</span><a href="#36719065">prev</a><span>|</span><a href="#36718852">next</a><span>|</span><label class="collapse" for="c-36713790">[-]</label><label class="expand" for="c-36713790">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s something that surprises me too, given the preprocessing applied to NLP.</div><br/><div id="36716635" class="c"><input type="checkbox" id="c-36716635" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713790">parent</a><span>|</span><a href="#36714827">next</a><span>|</span><label class="collapse" for="c-36716635">[-]</label><label class="expand" for="c-36716635">[2 more]</label></div><br/><div class="children"><div class="content">We used to do a lot more preprocessing to NLP, like stemming, removing stop words, or even adding grammar information (NP, VP, etc.).
Now we just do basic tokenization. The rest turned out to be irrelevant or even counter productive.</div><br/><div id="36716821" class="c"><input type="checkbox" id="c-36716821" checked=""/><div class="controls bullet"><span class="by">kbelder</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36716635">parent</a><span>|</span><a href="#36714827">next</a><span>|</span><label class="collapse" for="c-36716821">[-]</label><label class="expand" for="c-36716821">[1 more]</label></div><br/><div class="children"><div class="content">But also, that basic tokenization is essential; training it on a raw ascii stream would be much less efficient.  There is a sweet spot of processing &amp; abstraction that should be aimed for.</div><br/></div></div></div></div><div id="36714827" class="c"><input type="checkbox" id="c-36714827" checked=""/><div class="controls bullet"><span class="by">CodesInChaos</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713790">parent</a><span>|</span><a href="#36716635">prev</a><span>|</span><a href="#36718852">next</a><span>|</span><label class="collapse" for="c-36714827">[-]</label><label class="expand" for="c-36714827">[1 more]</label></div><br/><div class="children"><div class="content">Short text representations (via good tokenization) significantly reduces the computational cost of a transformer (need to generate fewer tokens for the same output length, and need fewer tokens to represent the same window size). I think these combine to n^3 scaling (n^2 from window size and n from output size).<p>For images it&#x27;s not clear to me if there are any preprocessing methods that do a lot better than resizing the image to a smaller resolution (which is commonly done already).</div><br/></div></div></div></div><div id="36718852" class="c"><input type="checkbox" id="c-36718852" checked=""/><div class="controls bullet"><span class="by">smrtinsert</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713470">parent</a><span>|</span><a href="#36713790">prev</a><span>|</span><a href="#36713691">next</a><span>|</span><label class="collapse" for="c-36718852">[-]</label><label class="expand" for="c-36718852">[1 more]</label></div><br/><div class="children"><div class="content">End to end ml models are the goal regardless of efficiency just like software engs aim for higher level interfaces</div><br/></div></div></div></div><div id="36713691" class="c"><input type="checkbox" id="c-36713691" checked=""/><div class="controls bullet"><span class="by">johntb86</span><span>|</span><a href="#36713278">parent</a><span>|</span><a href="#36713470">prev</a><span>|</span><a href="#36715366">next</a><span>|</span><label class="collapse" for="c-36713691">[-]</label><label class="expand" for="c-36713691">[4 more]</label></div><br/><div class="children"><div class="content">I would worry that the fixed, non-overlapping block nature of a JPEG would reduce translation invariance - shift an image by 4 pixels and the DCT coefficients may look very different. People have been doing a lot of work to try to reduce the dependence of the image on the actual pixel coordinates - see for example <a href="https:&#x2F;&#x2F;research.nvidia.com&#x2F;publication&#x2F;2021-12_alias-free-generative-adversarial-networks" rel="nofollow noreferrer">https:&#x2F;&#x2F;research.nvidia.com&#x2F;publication&#x2F;2021-12_alias-free-g...</a></div><br/><div id="36713912" class="c"><input type="checkbox" id="c-36713912" checked=""/><div class="controls bullet"><span class="by">DougBTX</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713691">parent</a><span>|</span><a href="#36715175">next</a><span>|</span><label class="collapse" for="c-36713912">[-]</label><label class="expand" for="c-36713912">[1 more]</label></div><br/><div class="children"><div class="content">On the other hand, ViT uses non-overlapping patches anyway, so the impact may be minor. Example code: <a href="https:&#x2F;&#x2F;nn.labml.ai&#x2F;transformers&#x2F;vit&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;nn.labml.ai&#x2F;transformers&#x2F;vit&#x2F;index.html</a></div><br/></div></div><div id="36715175" class="c"><input type="checkbox" id="c-36715175" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713691">parent</a><span>|</span><a href="#36713912">prev</a><span>|</span><a href="#36715366">next</a><span>|</span><label class="collapse" for="c-36715175">[-]</label><label class="expand" for="c-36715175">[2 more]</label></div><br/><div class="children"><div class="content">Does JPEG-2000 fix that? From what I gathered, it doesn&#x27;t use blocks.</div><br/><div id="36716061" class="c"><input type="checkbox" id="c-36716061" checked=""/><div class="controls bullet"><span class="by">mochomocha</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36715175">parent</a><span>|</span><a href="#36715366">next</a><span>|</span><label class="collapse" for="c-36716061">[-]</label><label class="expand" for="c-36716061">[1 more]</label></div><br/><div class="children"><div class="content">JPEG-2000 uses wavelets as a decomposition basis as opposed to DCT which in theory makes it possible to treat the whole image as a single block while ensuring high compression. In practice though tiles are used, I would guess to improve on memory and compute parallelism.</div><br/></div></div></div></div></div></div><div id="36715366" class="c"><input type="checkbox" id="c-36715366" checked=""/><div class="controls bullet"><span class="by">vladimirralev</span><span>|</span><a href="#36713278">parent</a><span>|</span><a href="#36713691">prev</a><span>|</span><a href="#36714019">next</a><span>|</span><label class="collapse" for="c-36715366">[-]</label><label class="expand" for="c-36715366">[1 more]</label></div><br/><div class="children"><div class="content">In naive ML scenarios you are right. You can think of JPEG as an input embedding. One of many. The JPEG&#x2F;spectral embedding is useful because it already provides miniature variational encoding that &quot;makes sense&quot; in terms of translation, sharpness, color, scale and texture.<p>But with clever ML you can design better variational characteristics such as rotation or nonlinear thing like faces, fingers, projections and abstract objects.<p>Further JPEG encoding&#x2F;decoding will be an obstacle for many architectures that require gradients going back and forth between pixel space and JPEG in order to do evaluation steps and loss functions based on the pixel space (which would be superior). Not to mention if you need human feedback in generative scenarios to retouch the output and run training steps on the changed pixels.<p>And finally, there are already picture and video embeddings that are gradient-friendly and reusable.</div><br/></div></div><div id="36714019" class="c"><input type="checkbox" id="c-36714019" checked=""/><div class="controls bullet"><span class="by">sharemywin</span><span>|</span><a href="#36713278">parent</a><span>|</span><a href="#36715366">prev</a><span>|</span><a href="#36713843">next</a><span>|</span><label class="collapse" for="c-36714019">[-]</label><label class="expand" for="c-36714019">[1 more]</label></div><br/><div class="children"><div class="content">I wondered awhile back about using EGA or CGA for doing image recognition and or for stable diffusion. seems like there should be more than enough image data for that resolution and color depth(16 colors).</div><br/></div></div><div id="36713843" class="c"><input type="checkbox" id="c-36713843" checked=""/><div class="controls bullet"><span class="by">luckystarr</span><span>|</span><a href="#36713278">parent</a><span>|</span><a href="#36714019">prev</a><span>|</span><a href="#36719309">next</a><span>|</span><label class="collapse" for="c-36713843">[-]</label><label class="expand" for="c-36713843">[2 more]</label></div><br/><div class="children"><div class="content">I remember I&#x27;ve heard somewhere that our retina encodes the visuals it receives into a compressed signal before forwarding it to the visual cortex. If true, this may actually be how it&#x27;s done &quot;for real&quot;. ;)</div><br/><div id="36717427" class="c"><input type="checkbox" id="c-36717427" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#36713278">root</a><span>|</span><a href="#36713843">parent</a><span>|</span><a href="#36719309">next</a><span>|</span><label class="collapse" for="c-36717427">[-]</label><label class="expand" for="c-36717427">[1 more]</label></div><br/><div class="children"><div class="content">The retina does a <i>lot</i> of processing.</div><br/></div></div></div></div><div id="36719309" class="c"><input type="checkbox" id="c-36719309" checked=""/><div class="controls bullet"><span class="by">NL807</span><span>|</span><a href="#36713278">parent</a><span>|</span><a href="#36713843">prev</a><span>|</span><a href="#36718483">next</a><span>|</span><label class="collapse" for="c-36719309">[-]</label><label class="expand" for="c-36719309">[1 more]</label></div><br/><div class="children"><div class="content">Seems like you are using similar tricks what compressed sensing people do, work with data in a sparser domain.</div><br/></div></div><div id="36718483" class="c"><input type="checkbox" id="c-36718483" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#36713278">parent</a><span>|</span><a href="#36719309">prev</a><span>|</span><a href="#36716093">next</a><span>|</span><label class="collapse" for="c-36718483">[-]</label><label class="expand" for="c-36718483">[1 more]</label></div><br/><div class="children"><div class="content">Fascinating.<p>How did this approach handle the same image being encoded in different ways by different JPEG libraries?  Or just with different quality settings?</div><br/></div></div><div id="36716093" class="c"><input type="checkbox" id="c-36716093" checked=""/><div class="controls bullet"><span class="by">23B1</span><span>|</span><a href="#36713278">parent</a><span>|</span><a href="#36718483">prev</a><span>|</span><a href="#36710360">next</a><span>|</span><label class="collapse" for="c-36716093">[-]</label><label class="expand" for="c-36716093">[1 more]</label></div><br/><div class="children"><div class="content">This is really interesting. Have you published your research anywhere that I could read?</div><br/></div></div></div></div><div id="36710360" class="c"><input type="checkbox" id="c-36710360" checked=""/><div class="controls bullet"><span class="by">goldemerald</span><span>|</span><a href="#36713278">prev</a><span>|</span><a href="#36710641">next</a><span>|</span><label class="collapse" for="c-36710360">[-]</label><label class="expand" for="c-36710360">[10 more]</label></div><br/><div class="children"><div class="content">For those interested, a modern version (vision transformers) was just published this year at CVPR <a href="https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content&#x2F;CVPR2023&#x2F;html&#x2F;Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content&#x2F;CVPR2023&#x2F;html&#x2F;Park_RGB...</a></div><br/><div id="36714231" class="c"><input type="checkbox" id="c-36714231" checked=""/><div class="controls bullet"><span class="by">jcjohns</span><span>|</span><a href="#36710360">parent</a><span>|</span><a href="#36712122">next</a><span>|</span><label class="collapse" for="c-36714231">[-]</label><label class="expand" for="c-36714231">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m one of the authors of this CVPR paper -- cool to see our work mentioned on HN!<p>The Uber paper from 2018 is one that has been floating around in the back of my head for a while. Decoding DCT to RGB is essentially an 8x8 stride 8 convolution -- it seems wasteful to perform this operation on CPU for data loading, then immediately pass the resulting decoded RGB into convolution layers that probably learn similar filters as those used during DCT decoding anyway.<p>Compared to the earlier Uber paper, our CVPR paper makes two big advances:<p>(1) Cleaner architecture: The Uber paper uses a CNN, while we use a ViT. It&#x27;s kind of awkward to modify an existing CNN architecture to accept DCT instead of RGB since the grayscale data is 8x lower resolution than RGB, and the color information is 16x lower than RGB. With a CNN, you need to add extra layers to deal with the downsampled input, and use some kind of fusion mechanism to fuse the luma&#x2F;chroma data of different resolution. With a ViT it&#x27;s very straightforward to accept DCT input; you only need to change the patch embedding layer, and the body of the network is unchanged.<p>(2) Data augmentation: The original Uber paper only showed speedup during inference. During training they need to perform data augmentation, so convert DCT to RGB, augment in RGB, then convert back to DCT to feed the augmented data to the model. This means that their approach will be <i>slower</i> during training vs an RGB model. In our paper we show to to perform all standard image augmentations directly in DCT, so we can get speedups during both training and inference.<p>Happy to answer any questions about the project!</div><br/><div id="36714656" class="c"><input type="checkbox" id="c-36714656" checked=""/><div class="controls bullet"><span class="by">inoop</span><span>|</span><a href="#36710360">root</a><span>|</span><a href="#36714231">parent</a><span>|</span><a href="#36712122">next</a><span>|</span><label class="collapse" for="c-36714656">[-]</label><label class="expand" for="c-36714656">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Decoding DCT to RGB is essentially an 8x8 stride 8 convolution -- it seems wasteful to perform this operation on CPU for data loading<p>Then why not do it on the GPU? Feels like exactly the sort of thing it was designed to do.<p>Or alternatively, use nvjpeg?</div><br/><div id="36715116" class="c"><input type="checkbox" id="c-36715116" checked=""/><div class="controls bullet"><span class="by">jcjohns</span><span>|</span><a href="#36710360">root</a><span>|</span><a href="#36714656">parent</a><span>|</span><a href="#36715159">next</a><span>|</span><label class="collapse" for="c-36715116">[-]</label><label class="expand" for="c-36715116">[1 more]</label></div><br/><div class="children"><div class="content">This makes sense in theory, but is hard to get working in practice.<p>We tried using nvjpeg to do JPEG decoding on GPU as a additional baseline, but using it as a drop-in replacement to a standard training pipeline gives huge slowdowns for a few reasons:<p>(1) Batching: nvjpeg isn&#x27;t batched; you need to decode one at a time in a loop. This is slow but could in principle be improved with a better GPU decoder.<p>(2) Concurrent data loading &#x2F; model execution: In a standard training pipeline, the CPU is loading and augmenting data on CPU for the next batch in parallel with the model running forward &#x2F; backward on the current batch. Using the GPU for decoding blocks it from running the model concurrently. If you were careful I think you could probably find a way to interleave JPEG decoding and model execution on the GPU, but it&#x27;s not straightforward. Just naively swapping out to use nvjpeg in a standard PyTorch training pipeline gives very bad performance.<p>(3) Data augmentation: If you do DCT -&gt; RGB decoding on the GPU, then you have to think about how and where to do data augmentation. You can augment in DCT either on CPU or on GPU; however DCT augmentation tends to be more expensive than RGB augmentation (especially for resize operations), so if you are already going through the trouble of decoding to RGB then it&#x27;s probably much cheaper to augment in RGB. If you augment in RGB on GPU, then you are blocking parallel model execution for both JPEG decoding and augmentation, and problem (2) gets even worse. If you do RGB augmentation on CPU, you end up with and extra GPU -&gt; CPU -&gt; GPU round trip on every model iteration which again reduces performance.</div><br/></div></div><div id="36715159" class="c"><input type="checkbox" id="c-36715159" checked=""/><div class="controls bullet"><span class="by">arketyp</span><span>|</span><a href="#36710360">root</a><span>|</span><a href="#36714656">parent</a><span>|</span><a href="#36715116">prev</a><span>|</span><a href="#36712122">next</a><span>|</span><label class="collapse" for="c-36715159">[-]</label><label class="expand" for="c-36715159">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m just a low tier ML engineer, but I&#x27;d say you generally want to avoid splitting GPU resources over many libraries, to the extent it&#x27;s even practically possible.</div><br/><div id="36719470" class="c"><input type="checkbox" id="c-36719470" checked=""/><div class="controls bullet"><span class="by">22c</span><span>|</span><a href="#36710360">root</a><span>|</span><a href="#36715159">parent</a><span>|</span><a href="#36712122">next</a><span>|</span><label class="collapse" for="c-36719470">[-]</label><label class="expand" for="c-36719470">[1 more]</label></div><br/><div class="children"><div class="content">Could you parallelise your parallel processors? ie. offload this work to a separate, (perhaps not even as beefy) GPU.<p>Akin to streamers having one GPU that they use for gaming and a second GPU used for encoding their stream.</div><br/></div></div></div></div></div></div></div></div><div id="36712122" class="c"><input type="checkbox" id="c-36712122" checked=""/><div class="controls bullet"><span class="by">w-m</span><span>|</span><a href="#36710360">parent</a><span>|</span><a href="#36714231">prev</a><span>|</span><a href="#36711881">next</a><span>|</span><label class="collapse" for="c-36712122">[-]</label><label class="expand" for="c-36712122">[2 more]</label></div><br/><div class="children"><div class="content">Ha, I remember the poster from the conference, it was quite crowded when I passed by. This one seemed to have a big focus on data augmentation in the DCT space. I was asking myself (and the author) whether you couldn’t eke out a little more efficiency by trying to quantize your network similarly to the default JPEG quantization table. As I understood, currently all weights are quantized uniformly, which does not make sense when your inputs are heavily quantized, does it? Maybe I should dive a little deeper into the Uber paper, they were focusing a bit more in the quantization part. Sorry if I’m talking nonsense, this is absolutely not my area, but I found the topic captivating.</div><br/><div id="36712216" class="c"><input type="checkbox" id="c-36712216" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#36710360">root</a><span>|</span><a href="#36712122">parent</a><span>|</span><a href="#36711881">next</a><span>|</span><label class="collapse" for="c-36712216">[-]</label><label class="expand" for="c-36712216">[1 more]</label></div><br/><div class="children"><div class="content">There is some work on using JPEG style DCT blocks for quantization: <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1109&#x2F;ISCA45697.2020.00075" rel="nofollow noreferrer">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1109&#x2F;ISCA45697.2020.00075</a><p>(Disclaimer, not mine, but a friends work)</div><br/></div></div></div></div><div id="36711881" class="c"><input type="checkbox" id="c-36711881" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#36710360">parent</a><span>|</span><a href="#36712122">prev</a><span>|</span><a href="#36713786">next</a><span>|</span><label class="collapse" for="c-36711881">[-]</label><label class="expand" for="c-36711881">[1 more]</label></div><br/><div class="children"><div class="content">Thank you. First published 2022-11-29 on arxiv [0] and updated one month ago.<p>Interesting line: &quot;With these two improvements -- ViT and data augmentation -- we show that our ViT-Ti model achieves up to 39.2% faster training and 17.9% faster inference with no accuracy loss compared to the RGB counterpart.&quot;<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.16421v2" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.16421v2</a></div><br/></div></div><div id="36713786" class="c"><input type="checkbox" id="c-36713786" checked=""/><div class="controls bullet"><span class="by">m3affan</span><span>|</span><a href="#36710360">parent</a><span>|</span><a href="#36711881">prev</a><span>|</span><a href="#36710641">next</a><span>|</span><label class="collapse" for="c-36713786">[-]</label><label class="expand" for="c-36713786">[1 more]</label></div><br/><div class="children"><div class="content">Transformers are the taking the cake in the Deep Learning community</div><br/></div></div></div></div><div id="36710641" class="c"><input type="checkbox" id="c-36710641" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#36710360">prev</a><span>|</span><a href="#36720663">next</a><span>|</span><label class="collapse" for="c-36710641">[-]</label><label class="expand" for="c-36710641">[10 more]</label></div><br/><div class="children"><div class="content">&gt; Accuracy gains are due primarily to the specific use of a DCT representation, which turns out to work curiously well for image classification.<p>It would seem quantization is a useful tool for any sort of NN-style application.<p>If the expected output is intended to be human-like, why not feed it information that a typical human could not distinguish from a lossless representation? Seems like a simple game of expectations and information theory.</div><br/><div id="36711830" class="c"><input type="checkbox" id="c-36711830" checked=""/><div class="controls bullet"><span class="by">kridsdale3</span><span>|</span><a href="#36710641">parent</a><span>|</span><a href="#36720663">next</a><span>|</span><label class="collapse" for="c-36711830">[-]</label><label class="expand" for="c-36711830">[9 more]</label></div><br/><div class="children"><div class="content">That&#x27;s kind of the key theory behind why JPEG (and other lossy encodings) work at all. A perfect being would see a JPEG next to a PNG or TIFF and find the first repugnantly error-ridden.<p>But we tend to ignore high-frequency data&#x27;s specifics most of the time, so it psychologically works.<p>I often wonder though, what do my cat and dog hear when I&#x27;m playing compressed music? Does it sounds like a muddy phone call to them?</div><br/><div id="36712166" class="c"><input type="checkbox" id="c-36712166" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#36710641">root</a><span>|</span><a href="#36711830">parent</a><span>|</span><a href="#36712137">next</a><span>|</span><label class="collapse" for="c-36712166">[-]</label><label class="expand" for="c-36712166">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Does it sounds like a muddy phone call to them?<p>Likely no.<p>Audio is decidedly less &quot;compressible&quot; in human perceptual terms. The brain is amazingly skilled at detecting time delay and frequency deviations, so this perceptual baseline likely extends (mostly) to your pets.<p>You can fool the eyes a lot more easily. You can take away 50%+ or more of the color information before even a skilled artist will start noticing.</div><br/><div id="36713050" class="c"><input type="checkbox" id="c-36713050" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#36710641">root</a><span>|</span><a href="#36712166">parent</a><span>|</span><a href="#36712313">next</a><span>|</span><label class="collapse" for="c-36713050">[-]</label><label class="expand" for="c-36713050">[3 more]</label></div><br/><div class="children"><div class="content">There are real differences in audio perception, though. Frequency range and sensitivity to different frequencies is a big difference in other animals; I would expect cats (who chase rodents, which often have very high pitched or even ultrasonic vocalizations) to be more sensitive to high frequencies than humans, and thus low passed &#x2F; low sample rate audio could sounds &#x27;bad.&#x27;<p>Another aspect is time resolution. Song birds can have 2-4x the time resolution of human hearing, which helps distinguish sounds in their very fast, complex calls. This may lead to better perception of artifacts in lossy coding schemes, but it&#x27;s hard to say for sure.<p>Edit: reference on cat hearing:
<a href="https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;4066516" rel="nofollow noreferrer">https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;4066516</a><p>The hearing range of the cat for sounds of 70 dB SPL extends from 48 Hz to 85 kHz, giving it one of the broadest hearing ranges among mammals.</div><br/><div id="36717758" class="c"><input type="checkbox" id="c-36717758" checked=""/><div class="controls bullet"><span class="by">xenadu02</span><span>|</span><a href="#36710641">root</a><span>|</span><a href="#36713050">parent</a><span>|</span><a href="#36712313">next</a><span>|</span><label class="collapse" for="c-36717758">[-]</label><label class="expand" for="c-36717758">[2 more]</label></div><br/><div class="children"><div class="content">True but hearing is logarithmic in both volume and frequency domains. Double the power does not equate to anything near double the loudness. Similarly each doubling of frequency is only one octave higher. Hearing up to 80khz doesn&#x27;t mean hearing 4x more than humans... 10 octaves for humans, 12 octaves for cats. In a musical sense it probably isn&#x27;t noticeable.</div><br/><div id="36719990" class="c"><input type="checkbox" id="c-36719990" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#36710641">root</a><span>|</span><a href="#36717758">parent</a><span>|</span><a href="#36712313">next</a><span>|</span><label class="collapse" for="c-36719990">[-]</label><label class="expand" for="c-36719990">[1 more]</label></div><br/><div class="children"><div class="content">The extreme upper limit of human hearing is around 20khz, so cats really are hearing things that we don&#x27;t, and for good reasons.<p>Sensitivity to different frequency ranges is more or less independent of anything else. Birds have heightened frequency response in the range they vocalize in, which helps them hear others if their species. Same for us; we vocalize at relatively low frequencies, so most of our hearing ability is focused on that range. There is also a range below which we don&#x27;t hear: infrasound, which is utilized by elephants.<p>Logarithmic perception is certainly real, but the tuning of  which frequency ranges an animal is more or less sensitive to is certainly species dependent.</div><br/></div></div></div></div></div></div><div id="36712313" class="c"><input type="checkbox" id="c-36712313" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#36710641">root</a><span>|</span><a href="#36712166">parent</a><span>|</span><a href="#36713050">prev</a><span>|</span><a href="#36712137">next</a><span>|</span><label class="collapse" for="c-36712313">[-]</label><label class="expand" for="c-36712313">[1 more]</label></div><br/><div class="children"><div class="content">while work has been done to characterize frequency sensitivity across species (which does vary quite a bit, especially in the higher ranges (&gt;20khz)), i haven&#x27;t seen any work that has been done to explore frequency domain perceptual masking curves in a cross species context.<p>since some species use their auditory systems for spatial localization, i would guess that the perceptual system would be totally different in those contexts.</div><br/></div></div></div></div><div id="36712137" class="c"><input type="checkbox" id="c-36712137" checked=""/><div class="controls bullet"><span class="by">jdiff</span><span>|</span><a href="#36710641">root</a><span>|</span><a href="#36711830">parent</a><span>|</span><a href="#36712166">prev</a><span>|</span><a href="#36720663">next</a><span>|</span><label class="collapse" for="c-36712137">[-]</label><label class="expand" for="c-36712137">[3 more]</label></div><br/><div class="children"><div class="content">No, audio compression doesn&#x27;t filter out high frequencies, that&#x27;s just what computer audio as a whole does. And I don&#x27;t think there&#x27;s enough of those high frequency components in what humans typically record for a cat or dog to notice the difference. As far as compression, the tricks that work on us should work on them.</div><br/><div id="36713590" class="c"><input type="checkbox" id="c-36713590" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#36710641">root</a><span>|</span><a href="#36712137">parent</a><span>|</span><a href="#36712378">next</a><span>|</span><label class="collapse" for="c-36713590">[-]</label><label class="expand" for="c-36713590">[1 more]</label></div><br/><div class="children"><div class="content">the early xing mp3 codec famously cut everything off above 18khz, but that was out of spec. :)<p>instead perceptual audio compression typically filters out frequencies that neighbor other frequencies with lots of power. deleting these neighbors is called perceptual masking and to the best of my knowledge, we do not actually know if it works the same way in animal auditory systems.</div><br/></div></div><div id="36712378" class="c"><input type="checkbox" id="c-36712378" checked=""/><div class="controls bullet"><span class="by">LordDragonfang</span><span>|</span><a href="#36710641">root</a><span>|</span><a href="#36712137">parent</a><span>|</span><a href="#36713590">prev</a><span>|</span><a href="#36720663">next</a><span>|</span><label class="collapse" for="c-36712378">[-]</label><label class="expand" for="c-36712378">[1 more]</label></div><br/><div class="children"><div class="content">&gt;MP3 compression works by reducing (or approximating) the accuracy of certain components of sound that are considered (by psychoacoustic analysis) to be beyond the hearing capabilities of most humans.<p>-via Wikipedia<p>This holds true for most other audio compression as well.<p>Now, it&#x27;s true that max recording frequency is bounded by sample rate via the Nyquist theorem, but that doesn&#x27;t mean we&#x27;re incapable of recording at higher fidelity - we just don&#x27;t bother most of the time, because on consumer hardware it&#x27;s going to be filtered out eventually anyway (or just not reproduced well enough, due to low-quality physical hardware). Recording studios will regularly produce masters that far exceed that normal hearing range though.</div><br/></div></div></div></div></div></div></div></div><div id="36720663" class="c"><input type="checkbox" id="c-36720663" checked=""/><div class="controls bullet"><span class="by">jacobgorm</span><span>|</span><a href="#36710641">prev</a><span>|</span><a href="#36720309">next</a><span>|</span><label class="collapse" for="c-36720663">[-]</label><label class="expand" for="c-36720663">[1 more]</label></div><br/><div class="children"><div class="content">I’ve tried this idea in the past and found it not very useful in practice. It breaks when you want to add image augmentation during training, and JPEG is anyway a pretty lousy format for storing training samples if you care about saving space or retaining image quality.</div><br/></div></div><div id="36720309" class="c"><input type="checkbox" id="c-36720309" checked=""/><div class="controls bullet"><span class="by">ulrikrasmussen</span><span>|</span><a href="#36720663">prev</a><span>|</span><a href="#36710480">next</a><span>|</span><label class="collapse" for="c-36720309">[-]</label><label class="expand" for="c-36720309">[1 more]</label></div><br/><div class="children"><div class="content">The quantization used for JPEG is optimized to throw away information in the frequency space that doesn&#x27;t matter much to human perception, but I wonder if that is also optimal for training neutral networks?<p>Also, as far as I know, the human eye doesn&#x27;t process images in blocks. I also wonder how blockless encoders such as JPEG 2000 would fare in this approach.</div><br/></div></div><div id="36710480" class="c"><input type="checkbox" id="c-36710480" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#36720309">prev</a><span>|</span><a href="#36719758">next</a><span>|</span><label class="collapse" for="c-36710480">[-]</label><label class="expand" for="c-36710480">[11 more]</label></div><br/><div class="children"><div class="content">Add &quot;(2018)&quot;</div><br/><div id="36711712" class="c"><input type="checkbox" id="c-36711712" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#36710480">parent</a><span>|</span><a href="#36710739">next</a><span>|</span><label class="collapse" for="c-36711712">[-]</label><label class="expand" for="c-36711712">[1 more]</label></div><br/><div class="children"><div class="content">I was wondering about &quot;jpeg turned 25 in 2017.&quot; Seemed like peculiar phrasing.</div><br/></div></div><div id="36710739" class="c"><input type="checkbox" id="c-36710739" checked=""/><div class="controls bullet"><span class="by">readthenotes1</span><span>|</span><a href="#36710480">parent</a><span>|</span><a href="#36711712">prev</a><span>|</span><a href="#36719758">next</a><span>|</span><label class="collapse" for="c-36710739">[-]</label><label class="expand" for="c-36710739">[9 more]</label></div><br/><div class="children"><div class="content">Back when Uber was thinking it could make self driving taxis</div><br/><div id="36710786" class="c"><input type="checkbox" id="c-36710786" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#36710480">root</a><span>|</span><a href="#36710739">parent</a><span>|</span><a href="#36719758">next</a><span>|</span><label class="collapse" for="c-36710786">[-]</label><label class="expand" for="c-36710786">[8 more]</label></div><br/><div class="children"><div class="content">Uber now has a deal with Waymo - so in effect they are still trying to do self-driving taxi&#x27;s, but now via a complex business relationship.</div><br/><div id="36711088" class="c"><input type="checkbox" id="c-36711088" checked=""/><div class="controls bullet"><span class="by">capableweb</span><span>|</span><a href="#36710480">root</a><span>|</span><a href="#36710786">parent</a><span>|</span><a href="#36719758">next</a><span>|</span><label class="collapse" for="c-36711088">[-]</label><label class="expand" for="c-36711088">[7 more]</label></div><br/><div class="children"><div class="content">Ah, just like I &quot;make hamburgers&quot; when I go through the McDonals drive-through, although it&#x27;s via a business transaction.</div><br/><div id="36711546" class="c"><input type="checkbox" id="c-36711546" checked=""/><div class="controls bullet"><span class="by">ianlevesque</span><span>|</span><a href="#36710480">root</a><span>|</span><a href="#36711088">parent</a><span>|</span><a href="#36711906">next</a><span>|</span><label class="collapse" for="c-36711546">[-]</label><label class="expand" for="c-36711546">[5 more]</label></div><br/><div class="children"><div class="content">God this website is cynical. Licensing technology from another company to commercialize it is completely legitimate.</div><br/><div id="36711951" class="c"><input type="checkbox" id="c-36711951" checked=""/><div class="controls bullet"><span class="by">wpietri</span><span>|</span><a href="#36710480">root</a><span>|</span><a href="#36711546">parent</a><span>|</span><a href="#36719984">next</a><span>|</span><label class="collapse" for="c-36711951">[-]</label><label class="expand" for="c-36711951">[3 more]</label></div><br/><div class="children"><div class="content">That is legitimate, but that&#x27;s not the point here. The point is Uber&#x27;s hubris. A hubris very useful in pumping up its stock price ahead of an IPO. If they had quietly planned to license it from the get-go, nobody would have mentioned it.</div><br/><div id="36712336" class="c"><input type="checkbox" id="c-36712336" checked=""/><div class="controls bullet"><span class="by">ianlevesque</span><span>|</span><a href="#36710480">root</a><span>|</span><a href="#36711951">parent</a><span>|</span><a href="#36719984">next</a><span>|</span><label class="collapse" for="c-36712336">[-]</label><label class="expand" for="c-36712336">[2 more]</label></div><br/><div class="children"><div class="content">Uber literally killed people trying to develop their own first. Tesla continues to do so. I wish everyone would license Waymo instead.</div><br/><div id="36713785" class="c"><input type="checkbox" id="c-36713785" checked=""/><div class="controls bullet"><span class="by">vajrabum</span><span>|</span><a href="#36710480">root</a><span>|</span><a href="#36712336">parent</a><span>|</span><a href="#36719984">next</a><span>|</span><label class="collapse" for="c-36713785">[-]</label><label class="expand" for="c-36713785">[1 more]</label></div><br/><div class="children"><div class="content">Any reason to prefer Waymo over Cruise? I saw an driverless Cruise taxi in SF just the other day. And are they any other competitors still in this space worth watching?</div><br/></div></div></div></div></div></div><div id="36719984" class="c"><input type="checkbox" id="c-36719984" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#36710480">root</a><span>|</span><a href="#36711546">parent</a><span>|</span><a href="#36711951">prev</a><span>|</span><a href="#36711906">next</a><span>|</span><label class="collapse" for="c-36719984">[-]</label><label class="expand" for="c-36719984">[1 more]</label></div><br/><div class="children"><div class="content">They had the Uber ATG group - they laid everyone off and gave up on their AV research. How is that cynical? That&#x27;s a fact.</div><br/></div></div></div></div><div id="36711906" class="c"><input type="checkbox" id="c-36711906" checked=""/><div class="controls bullet"><span class="by">jjk166</span><span>|</span><a href="#36710480">root</a><span>|</span><a href="#36711088">parent</a><span>|</span><a href="#36711546">prev</a><span>|</span><a href="#36719758">next</a><span>|</span><label class="collapse" for="c-36711906">[-]</label><label class="expand" for="c-36711906">[1 more]</label></div><br/><div class="children"><div class="content">More like making hamburgers by buying frozen burger patties at the supermarket.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36719758" class="c"><input type="checkbox" id="c-36719758" checked=""/><div class="controls bullet"><span class="by">namjh</span><span>|</span><a href="#36710480">prev</a><span>|</span><a href="#36712763">next</a><span>|</span><label class="collapse" for="c-36719758">[-]</label><label class="expand" for="c-36719758">[1 more]</label></div><br/><div class="children"><div class="content">Might be related: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.16421" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.16421</a>
It&#x27;s a paper about directly learning via JPEG encodings which works well with visusal transformers&#x27; patch mechanism.</div><br/></div></div><div id="36712763" class="c"><input type="checkbox" id="c-36712763" checked=""/><div class="controls bullet"><span class="by">watersb</span><span>|</span><a href="#36719758">prev</a><span>|</span><a href="#36720426">next</a><span>|</span><label class="collapse" for="c-36712763">[-]</label><label class="expand" for="c-36712763">[1 more]</label></div><br/><div class="children"><div class="content">IIRC, “neural network” style systems design started in the realm of computer vision with “Perceptrons”:<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Perceptrons_(book)Perceptrons" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Perceptrons_(book)Perceptrons</a> = <a href="https:&#x2F;&#x2F;g.co&#x2F;kgs&#x2F;8Un4eW" rel="nofollow noreferrer">https:&#x2F;&#x2F;g.co&#x2F;kgs&#x2F;8Un4eW</a><p>Makes sense that image processing would be a good fit in some cases.</div><br/></div></div><div id="36720426" class="c"><input type="checkbox" id="c-36720426" checked=""/><div class="controls bullet"><span class="by">shreyshnaccount</span><span>|</span><a href="#36712763">prev</a><span>|</span><a href="#36717859">next</a><span>|</span><label class="collapse" for="c-36720426">[-]</label><label class="expand" for="c-36720426">[1 more]</label></div><br/><div class="children"><div class="content">The next wave of ai magic will come from domain expertise.</div><br/></div></div><div id="36717859" class="c"><input type="checkbox" id="c-36717859" checked=""/><div class="controls bullet"><span class="by">waynecochran</span><span>|</span><a href="#36720426">prev</a><span>|</span><a href="#36711736">next</a><span>|</span><label class="collapse" for="c-36717859">[-]</label><label class="expand" for="c-36717859">[1 more]</label></div><br/><div class="children"><div class="content">Since convolution in the spatial domain is simply multiplication in the frequency domain it seems that this would be wayyy faster.</div><br/></div></div><div id="36711736" class="c"><input type="checkbox" id="c-36711736" checked=""/><div class="controls bullet"><span class="by">wodenokoto</span><span>|</span><a href="#36717859">prev</a><span>|</span><a href="#36711126">next</a><span>|</span><label class="collapse" for="c-36711736">[-]</label><label class="expand" for="c-36711736">[1 more]</label></div><br/><div class="children"><div class="content">That seems just in line with an earlier front page submission that used knn and gzipped text for categorization.</div><br/></div></div><div id="36711126" class="c"><input type="checkbox" id="c-36711126" checked=""/><div class="controls bullet"><span class="by">naillo</span><span>|</span><a href="#36711736">prev</a><span>|</span><a href="#36717184">next</a><span>|</span><label class="collapse" for="c-36711126">[-]</label><label class="expand" for="c-36711126">[3 more]</label></div><br/><div class="children"><div class="content">Finally someone who did this, I&#x27;ve always thought this was a low hanging fruit. I wonder if you could make interesting and quickly trained diffusion models with this trick.</div><br/><div id="36711633" class="c"><input type="checkbox" id="c-36711633" checked=""/><div class="controls bullet"><span class="by">waldarbeiter</span><span>|</span><a href="#36711126">parent</a><span>|</span><a href="#36711872">next</a><span>|</span><label class="collapse" for="c-36711633">[-]</label><label class="expand" for="c-36711633">[1 more]</label></div><br/><div class="children"><div class="content">Note that it is from 2018. As someone here already mentioned there is a paper that applies the same idea to Vision Transformers published this year [1].<p>[1] <a href="https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content&#x2F;CVPR2023&#x2F;papers&#x2F;Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content&#x2F;CVPR2023&#x2F;papers&#x2F;Park_R...</a></div><br/></div></div><div id="36711872" class="c"><input type="checkbox" id="c-36711872" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#36711126">parent</a><span>|</span><a href="#36711633">prev</a><span>|</span><a href="#36717184">next</a><span>|</span><label class="collapse" for="c-36711872">[-]</label><label class="expand" for="c-36711872">[1 more]</label></div><br/><div class="children"><div class="content">Sorta of? Latent diffusion models operate in a compressed latent space, which is just a richer &#x2F; learnable representation than DCT.</div><br/></div></div></div></div><div id="36717184" class="c"><input type="checkbox" id="c-36717184" checked=""/><div class="controls bullet"><span class="by">kevmo314</span><span>|</span><a href="#36711126">prev</a><span>|</span><a href="#36710552">next</a><span>|</span><label class="collapse" for="c-36717184">[-]</label><label class="expand" for="c-36717184">[1 more]</label></div><br/><div class="children"><div class="content">A lot of generative audio works very similar these days: it&#x27;s much faster to predict and generate a codebook than a raw waveform.</div><br/></div></div><div id="36710552" class="c"><input type="checkbox" id="c-36710552" checked=""/><div class="controls bullet"><span class="by">kunalgupta</span><span>|</span><a href="#36717184">prev</a><span>|</span><a href="#36711565">next</a><span>|</span><label class="collapse" for="c-36710552">[-]</label><label class="expand" for="c-36710552">[2 more]</label></div><br/><div class="children"><div class="content">see also: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;goodside&#x2F;status&#x2F;1679358632431853568?s=46&amp;t=YuSD72_CTtyLX4szIUa5JQ" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;goodside&#x2F;status&#x2F;1679358632431853568?s=46...</a></div><br/></div></div><div id="36711565" class="c"><input type="checkbox" id="c-36711565" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#36710552">prev</a><span>|</span><a href="#36713202">next</a><span>|</span><label class="collapse" for="c-36711565">[-]</label><label class="expand" for="c-36711565">[1 more]</label></div><br/><div class="children"><div class="content">i guess an interesting question is: can you coax a network into learning a better perceptual compression transform than the dcts in jpeg?</div><br/></div></div><div id="36713202" class="c"><input type="checkbox" id="c-36713202" checked=""/><div class="controls bullet"><span class="by">rullelito</span><span>|</span><a href="#36711565">prev</a><span>|</span><a href="#36710922">next</a><span>|</span><label class="collapse" for="c-36713202">[-]</label><label class="expand" for="c-36713202">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard of this long before 2018, was this really seen as novel in 2018?</div><br/></div></div></div></div></div></div></div></body></html>