<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695632463156" as="style"/><link rel="stylesheet" href="styles.css?v=1695632463156"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.hopsworks.ai/dictionary/two-tower-embedding-model">Two-Tower Embedding Model</a>Â <span class="domain">(<a href="https://www.hopsworks.ai">www.hopsworks.ai</a>)</span></div><div class="subtext"><span>jamesblonde</span> | <span>12 comments</span></div><br/><div><div id="37639614" class="c"><input type="checkbox" id="c-37639614" checked=""/><div class="controls bullet"><span class="by">joewferrara</span><span>|</span><a href="#37639754">next</a><span>|</span><label class="collapse" for="c-37639614">[-]</label><label class="expand" for="c-37639614">[4 more]</label></div><br/><div class="children"><div class="content">This is a very cool concept. The example given of Bytedance combining the text embedder ALBERT with the a transformer image embedder, to make an embedder that can do image and text at the same time to get an interaction score is fascinating. I had not heard of being able to combine unrelated embedders before, and I want to know more examples. A quick google search found this in depth blog article of how their using two-tower embeddings at Uber, which was written recently (this past July)<p><a href="https:&#x2F;&#x2F;www.uber.com&#x2F;blog&#x2F;innovative-recommendation-applications-using-two-tower-embeddings&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.uber.com&#x2F;blog&#x2F;innovative-recommendation-applicat...</a></div><br/><div id="37639767" class="c"><input type="checkbox" id="c-37639767" checked=""/><div class="controls bullet"><span class="by">pkage</span><span>|</span><a href="#37639614">parent</a><span>|</span><a href="#37639731">next</a><span>|</span><label class="collapse" for="c-37639767">[-]</label><label class="expand" for="c-37639767">[2 more]</label></div><br/><div class="children"><div class="content">Combining embeddings is the backbone of multimodal LLMs, such as InstructBLIP[0] or LLaVA[1]. Those architectures take the output tokens from a (frozen) vision transformer and train a very small projection layer between the output token space of the ViT and the input space of the LLM.<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.06500" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.06500</a>
[1] <a href="https:&#x2F;&#x2F;llava-vl.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;llava-vl.github.io&#x2F;</a></div><br/><div id="37641087" class="c"><input type="checkbox" id="c-37641087" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37639614">root</a><span>|</span><a href="#37639767">parent</a><span>|</span><a href="#37639731">next</a><span>|</span><label class="collapse" for="c-37641087">[-]</label><label class="expand" for="c-37641087">[1 more]</label></div><br/><div class="children"><div class="content">I wonder when Dalle3 is released if OpenAi will release any technical documents about the integration with gpt4. Their approach might be similar.</div><br/></div></div></div></div><div id="37639731" class="c"><input type="checkbox" id="c-37639731" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#37639614">parent</a><span>|</span><a href="#37639767">prev</a><span>|</span><a href="#37639754">next</a><span>|</span><label class="collapse" for="c-37639731">[-]</label><label class="expand" for="c-37639731">[1 more]</label></div><br/><div class="children"><div class="content">The two modalities that will be combined first will be the most profitable modalities :) - Uber combine products with user-query-history-context. 
Images and text are two other modalities that are getting traction.<p>I am looking at combining financial transactions and suspicious accounts. You need ground truth data that combines the two modalities - that&#x27;s the starting point.</div><br/></div></div></div></div><div id="37639754" class="c"><input type="checkbox" id="c-37639754" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37639614">prev</a><span>|</span><a href="#37640464">next</a><span>|</span><label class="collapse" for="c-37639754">[-]</label><label class="expand" for="c-37639754">[6 more]</label></div><br/><div class="children"><div class="content">I do not want to be too much of a downer, but is there something keeping us from just using the traditional verbiage we&#x27;ve used in the community for years and call it &quot;projecting into a shared latent space&quot; (or something bland but descriptive that a researcher like me could quickly latch onto, like &#x27;separate key and query encoders&#x27;)?<p>I understand that proprietary names are necessary to sell ideas, architectures, etc, but projecting things into the same latent space is an....old concept, that, like the old Ecclesiastes verse, come up in new and unique ways&#x2F;applications. Though there is nothing new under the sun, indeed.<p>Please forgive this stodgy young person. Frippery and grumpiness are a deep skill of mine, and I apply it to my own DL research as well. I do not want to discourage the author from writing more pieces, explaining concepts is I believe a great trend to have in a community.<p>Thank you and curious for anyone&#x27;s thoughts. &lt;3 :) :&#x27;)))) &lt;3</div><br/><div id="37640933" class="c"><input type="checkbox" id="c-37640933" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#37639754">parent</a><span>|</span><a href="#37639812">next</a><span>|</span><label class="collapse" for="c-37640933">[-]</label><label class="expand" for="c-37640933">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Two-tower&quot; or &quot;dual-encoder&quot; is an established terminology, many years old.</div><br/></div></div><div id="37639812" class="c"><input type="checkbox" id="c-37639812" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#37639754">parent</a><span>|</span><a href="#37640933">prev</a><span>|</span><a href="#37639834">next</a><span>|</span><label class="collapse" for="c-37639812">[-]</label><label class="expand" for="c-37639812">[2 more]</label></div><br/><div class="children"><div class="content">You are correct that we have two modalities projecting into a shared latent space. However, that doesn&#x27;t convey anything about how to jointly train the embedding models, does it? That&#x27;s what the two tower embedding model shows - how to jointly train the models using distance functions like cosine distance.<p>On an aside, others, as esteemed as Yann Lecun, are talking about learning in a shared latent space between 2 modalities, but use different names - joint embeddings, because it&#x27;s a different method.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12698" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12698</a></div><br/><div id="37640368" class="c"><input type="checkbox" id="c-37640368" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#37639754">root</a><span>|</span><a href="#37639812">parent</a><span>|</span><a href="#37639834">next</a><span>|</span><label class="collapse" for="c-37640368">[-]</label><label class="expand" for="c-37640368">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is the LeCun uses the term joint embedding as an alternate name for siamese networks. It does not necessarily imply multimodality and he has used it mostly in the context of SSL for images.</div><br/></div></div></div></div><div id="37639834" class="c"><input type="checkbox" id="c-37639834" checked=""/><div class="controls bullet"><span class="by">isaacfung</span><span>|</span><a href="#37639754">parent</a><span>|</span><a href="#37639812">prev</a><span>|</span><a href="#37640422">next</a><span>|</span><label class="collapse" for="c-37639834">[-]</label><label class="expand" for="c-37639834">[1 more]</label></div><br/><div class="children"><div class="content">Two tower isn&#x27;t a new term afaik. It&#x27;s often used in the context of recommendation systems where the two modalities are users&#x2F;query and items.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;creyesp&#x2F;Awesome-recsys">https:&#x2F;&#x2F;github.com&#x2F;creyesp&#x2F;Awesome-recsys</a></div><br/></div></div><div id="37640422" class="c"><input type="checkbox" id="c-37640422" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#37639754">parent</a><span>|</span><a href="#37639834">prev</a><span>|</span><a href="#37640464">next</a><span>|</span><label class="collapse" for="c-37640422">[-]</label><label class="expand" for="c-37640422">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t heard the term before either, but it&#x27;s mentioned at least by Google too so not entirely unheard of in the multimodal training context. Even so I also think this could have been conveyed without using that term. Two towers and even twin towers as seen in some places sounds exceedingly marketingy. But as others have pointed out, this may have been a popular term in recommendation systems before the rise of deep learning multimodality.<p><a href="https:&#x2F;&#x2F;blog.research.google&#x2F;2022&#x2F;06&#x2F;limoe-learning-multiple-modalities-with.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.research.google&#x2F;2022&#x2F;06&#x2F;limoe-learning-multiple...</a></div><br/></div></div></div></div><div id="37640464" class="c"><input type="checkbox" id="c-37640464" checked=""/><div class="controls bullet"><span class="by">choeger</span><span>|</span><a href="#37639754">prev</a><span>|</span><label class="collapse" for="c-37640464">[-]</label><label class="expand" for="c-37640464">[1 more]</label></div><br/><div class="children"><div class="content">Soo, if and when this &quot;AI&quot; is applied to suggestions, will it stop to suggest washing machines to me after I just bought one the other day in the same shop? Or the same article I just read this morning? As long as this very simply case is not covered, I consider these algorithms cow manure.</div><br/></div></div></div></div></div></div></div></body></html>