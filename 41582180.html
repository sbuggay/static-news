<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726736465026" as="style"/><link rel="stylesheet" href="styles.css?v=1726736465026"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ictnlp/LLaMA-Omni">Llama 3.1 Omni Model</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>taikon</span> | <span>28 comments</span></div><br/><div><div id="41585650" class="c"><input type="checkbox" id="c-41585650" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#41586733">next</a><span>|</span><label class="collapse" for="c-41585650">[-]</label><label class="expand" for="c-41585650">[9 more]</label></div><br/><div class="children"><div class="content">Can this play sounds that can&#x27;t be represented in text?   Ie. &quot;make the noise a chicken makes&quot;</div><br/><div id="41589711" class="c"><input type="checkbox" id="c-41589711" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#41585650">parent</a><span>|</span><a href="#41589263">next</a><span>|</span><label class="collapse" for="c-41589711">[-]</label><label class="expand" for="c-41589711">[1 more]</label></div><br/><div class="children"><div class="content">Bwaaaaaak bwakbwakbwak</div><br/></div></div><div id="41589263" class="c"><input type="checkbox" id="c-41589263" checked=""/><div class="controls bullet"><span class="by">DrSiemer</span><span>|</span><a href="#41585650">parent</a><span>|</span><a href="#41589711">prev</a><span>|</span><a href="#41587567">next</a><span>|</span><label class="collapse" for="c-41589263">[-]</label><label class="expand" for="c-41589263">[1 more]</label></div><br/><div class="children"><div class="content">Almost certainly not. Sounds like an old school vocoder, made to produce human speech and nothing else.</div><br/></div></div><div id="41587567" class="c"><input type="checkbox" id="c-41587567" checked=""/><div class="controls bullet"><span class="by">evilduck</span><span>|</span><a href="#41585650">parent</a><span>|</span><a href="#41589263">prev</a><span>|</span><a href="#41588603">next</a><span>|</span><label class="collapse" for="c-41587567">[-]</label><label class="expand" for="c-41587567">[1 more]</label></div><br/><div class="children"><div class="content">If it can create sounds associated with any nonphonetic word spellings, I can&#x27;t see why it would struggle with onomatopoeia.</div><br/></div></div><div id="41588603" class="c"><input type="checkbox" id="c-41588603" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#41585650">parent</a><span>|</span><a href="#41587567">prev</a><span>|</span><a href="#41587309">next</a><span>|</span><label class="collapse" for="c-41588603">[-]</label><label class="expand" for="c-41588603">[1 more]</label></div><br/><div class="children"><div class="content">Can it understand those sounds? And distinguish correct and incorrect pronunciation of words and accents?</div><br/></div></div><div id="41587309" class="c"><input type="checkbox" id="c-41587309" checked=""/><div class="controls bullet"><span class="by">8note</span><span>|</span><a href="#41585650">parent</a><span>|</span><a href="#41588603">prev</a><span>|</span><a href="#41586143">next</a><span>|</span><label class="collapse" for="c-41587309">[-]</label><label class="expand" for="c-41587309">[1 more]</label></div><br/><div class="children"><div class="content">As in, a cluck?<p>But can it both say the word cluck, and make a clicking sound?</div><br/></div></div><div id="41586143" class="c"><input type="checkbox" id="c-41586143" checked=""/><div class="controls bullet"><span class="by">hansenliang</span><span>|</span><a href="#41585650">parent</a><span>|</span><a href="#41587309">prev</a><span>|</span><a href="#41586733">next</a><span>|</span><label class="collapse" for="c-41586143">[-]</label><label class="expand" for="c-41586143">[3 more]</label></div><br/><div class="children"><div class="content">asking the real questions</div><br/><div id="41586165" class="c"><input type="checkbox" id="c-41586165" checked=""/><div class="controls bullet"><span class="by">indigodaddy</span><span>|</span><a href="#41585650">root</a><span>|</span><a href="#41586143">parent</a><span>|</span><a href="#41586733">next</a><span>|</span><label class="collapse" for="c-41586165">[-]</label><label class="expand" for="c-41586165">[2 more]</label></div><br/><div class="children"><div class="content">Very interesting question actually</div><br/></div></div></div></div></div></div><div id="41586733" class="c"><input type="checkbox" id="c-41586733" checked=""/><div class="controls bullet"><span class="by">twoodfin</span><span>|</span><a href="#41585650">prev</a><span>|</span><a href="#41585421">next</a><span>|</span><label class="collapse" for="c-41586733">[-]</label><label class="expand" for="c-41586733">[8 more]</label></div><br/><div class="children"><div class="content">I’m not clear on the virtues or potential of a model like this over a pure text model using STT&#x2F;TTS to achieve similar results.<p>Is the idea that as these models grow in sophistication they can properly interpret (or produce) inflection, cadence, emotion that’s lost in TTS?</div><br/><div id="41588516" class="c"><input type="checkbox" id="c-41588516" checked=""/><div class="controls bullet"><span class="by">a2128</span><span>|</span><a href="#41586733">parent</a><span>|</span><a href="#41589570">next</a><span>|</span><label class="collapse" for="c-41588516">[-]</label><label class="expand" for="c-41588516">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a lot of data loss and guessing with STT&#x2F;TTS.<p>An STT model might misrecognize a word, but an audio LLM may understand the true word because of the broad context. A TTS model needs to guess the inflection and it can get it completely wrong, but an audio LLM could understand how to talk naturally and with what tone (e.g. use a higher tone if it&#x27;s interjecting)<p>Speaking of interjection, an STT&#x2F;TTS system will never interject because it relies on VAD and heuristics to guess when to start talking or when to stop, and generally the rule is to only talk after the user stopped talking. An audio LLM could learn how to conversate naturally, avoid taking up too much conversation time or even talk with a group of people.<p>An audio LLM could also produce music or sounds or tell you what the song is when you hum it. There&#x27;s a lot of new possibility<p>I say &quot;could learn&quot; for most of this because it requires good training data, but from my understanding most of these are currently just trained with normal text datasets synthetically turned into voice with TTS, so they are effectively no better than a normal STT&#x2F;TTS system; it&#x27;s a good way to prove an architecture but it doesn&#x27;t demonstrate the full capabilities</div><br/><div id="41588624" class="c"><input type="checkbox" id="c-41588624" checked=""/><div class="controls bullet"><span class="by">langcss</span><span>|</span><a href="#41586733">root</a><span>|</span><a href="#41588516">parent</a><span>|</span><a href="#41589570">next</a><span>|</span><label class="collapse" for="c-41588624">[-]</label><label class="expand" for="c-41588624">[2 more]</label></div><br/><div class="children"><div class="content">You need a lot more power. I found gpt4o struggles doing basic OCR of printed text by hallucinating alot, while tesseract engine (old skool) gets it perfect. You need the model to be powerful enough to do everything.<p>You can work around this by the way by sending the output through a checking stage.<p>So picture -&gt; gpt4o -&gt; out1, picture -&gt; tesseract -&gt; out2, out1,out2 -&gt; llm.<p>Might work for sound too.</div><br/><div id="41589356" class="c"><input type="checkbox" id="c-41589356" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#41586733">root</a><span>|</span><a href="#41588624">parent</a><span>|</span><a href="#41589570">next</a><span>|</span><label class="collapse" for="c-41589356">[-]</label><label class="expand" for="c-41589356">[1 more]</label></div><br/><div class="children"><div class="content">Speech is inherently easier to represent  as a sequence of tokens than a high-resolution image.<p>Best speech to text is already NN transformer based anyway, so in theory it&#x27;s only better to use a combined model</div><br/></div></div></div></div></div></div><div id="41589570" class="c"><input type="checkbox" id="c-41589570" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#41586733">parent</a><span>|</span><a href="#41588516">prev</a><span>|</span><a href="#41587340">next</a><span>|</span><label class="collapse" for="c-41589570">[-]</label><label class="expand" for="c-41589570">[1 more]</label></div><br/><div class="children"><div class="content">Lots has been written on this subject, check out OpenAI’s papers on -O for example.<p>Latency is a big one due to batching. You can’t really interrupt the agent, which makes actual conversation more clunky. And yes, multimodal has better understanding. (I haven’t seen analysis of perception of emotions, has anyone seen analysis of this capability for GPT-O?)</div><br/></div></div><div id="41587340" class="c"><input type="checkbox" id="c-41587340" checked=""/><div class="controls bullet"><span class="by">Reubend</span><span>|</span><a href="#41586733">parent</a><span>|</span><a href="#41589570">prev</a><span>|</span><a href="#41588125">next</a><span>|</span><label class="collapse" for="c-41587340">[-]</label><label class="expand" for="c-41587340">[1 more]</label></div><br/><div class="children"><div class="content">Essentially, there&#x27;s data loss from audio -&gt; text. Sometimes that loss is unimportant, but sometimes it meaningfully improves output quality.<p>However, there are some other potential fringe benefits here: improving the latency of replies, improving speaker diarization, and reacting to pauses better for conversations.</div><br/></div></div><div id="41588125" class="c"><input type="checkbox" id="c-41588125" checked=""/><div class="controls bullet"><span class="by">bubaumba</span><span>|</span><a href="#41586733">parent</a><span>|</span><a href="#41587340">prev</a><span>|</span><a href="#41586762">next</a><span>|</span><label class="collapse" for="c-41588125">[-]</label><label class="expand" for="c-41588125">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I’m not clear on the virtues or potential of a model like this over a pure text model<p>you can&#x27;t put pure text with keyboard on a robot. it will become a wheeled computer.<p>actually this is a cool thing as a companion &#x2F; assistant.</div><br/></div></div><div id="41586762" class="c"><input type="checkbox" id="c-41586762" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41586733">parent</a><span>|</span><a href="#41588125">prev</a><span>|</span><a href="#41585421">next</a><span>|</span><label class="collapse" for="c-41586762">[-]</label><label class="expand" for="c-41586762">[1 more]</label></div><br/><div class="children"><div class="content">Really<p>Yeah that&#x27;s the point. Without punctuation, no one can tell what inflection my &quot;really&quot; above should have, but even if it&#x27;d been &quot;Really?&quot; or &quot;Really!&quot;, there&#x27;s still room for interpretation. With a bet on voice interfaces needing a Google moment (wherein, prior to Google, search was crap) to truely become successful (by interpreting and creating inflection, cadence, emotion, as you mentioned), creating such a model makes a lot of sense.</div><br/></div></div></div></div><div id="41585421" class="c"><input type="checkbox" id="c-41585421" checked=""/><div class="controls bullet"><span class="by">dingdingdang</span><span>|</span><a href="#41586733">prev</a><span>|</span><a href="#41585634">next</a><span>|</span><label class="collapse" for="c-41585421">[-]</label><label class="expand" for="c-41585421">[1 more]</label></div><br/><div class="children"><div class="content">Does any of the model-runners support this? Ollama, LM Studio, llama.cpp?</div><br/></div></div><div id="41585634" class="c"><input type="checkbox" id="c-41585634" checked=""/><div class="controls bullet"><span class="by">LorenDB</span><span>|</span><a href="#41585421">prev</a><span>|</span><a href="#41587318">next</a><span>|</span><label class="collapse" for="c-41585634">[-]</label><label class="expand" for="c-41585634">[2 more]</label></div><br/><div class="children"><div class="content">The TTS voice in the demo clip sounds remarkably like Ellen McLain (Valve voice actor).<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Ellen_McLain" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Ellen_McLain</a></div><br/><div id="41588155" class="c"><input type="checkbox" id="c-41588155" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#41585634">parent</a><span>|</span><a href="#41587318">next</a><span>|</span><label class="collapse" for="c-41588155">[-]</label><label class="expand" for="c-41588155">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like it&#x27;s trained on LJ Speech dataset, which is one of the best datasets and very commonly used</div><br/></div></div></div></div><div id="41587318" class="c"><input type="checkbox" id="c-41587318" checked=""/><div class="controls bullet"><span class="by">cuuupid</span><span>|</span><a href="#41585634">prev</a><span>|</span><a href="#41584130">next</a><span>|</span><label class="collapse" for="c-41587318">[-]</label><label class="expand" for="c-41587318">[1 more]</label></div><br/><div class="children"><div class="content">Wish there was training or finetuning code, as finetuning voices seems like a key requirement for any commercial use.</div><br/></div></div><div id="41584130" class="c"><input type="checkbox" id="c-41584130" checked=""/><div class="controls bullet"><span class="by">nickthegreek</span><span>|</span><a href="#41587318">prev</a><span>|</span><a href="#41585244">next</a><span>|</span><label class="collapse" for="c-41584130">[-]</label><label class="expand" for="c-41584130">[1 more]</label></div><br/><div class="children"><div class="content">The speed looks very nice. I just recently setup LMStudio + AnythingLLM to try out local voice chat and its still a little slower than I&#x27;d like but the PiperTTS voices are nicer than this.</div><br/></div></div><div id="41585244" class="c"><input type="checkbox" id="c-41585244" checked=""/><div class="controls bullet"><span class="by">opdahl</span><span>|</span><a href="#41584130">prev</a><span>|</span><a href="#41587319">next</a><span>|</span><label class="collapse" for="c-41585244">[-]</label><label class="expand" for="c-41585244">[4 more]</label></div><br/><div class="children"><div class="content">Any demos showcasing it’s performance?</div><br/><div id="41585356" class="c"><input type="checkbox" id="c-41585356" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#41585244">parent</a><span>|</span><a href="#41586195">next</a><span>|</span><label class="collapse" for="c-41585356">[-]</label><label class="expand" for="c-41585356">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s one on Huggingface <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;ICTNLP&#x2F;Llama-3.1-8B-Omni" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;ICTNLP&#x2F;Llama-3.1-8B-Omni</a></div><br/><div id="41585419" class="c"><input type="checkbox" id="c-41585419" checked=""/><div class="controls bullet"><span class="by">opdahl</span><span>|</span><a href="#41585244">root</a><span>|</span><a href="#41585356">parent</a><span>|</span><a href="#41586195">next</a><span>|</span><label class="collapse" for="c-41585419">[-]</label><label class="expand" for="c-41585419">[1 more]</label></div><br/><div class="children"><div class="content">Thank you.<p>Obviously it doesn’t sound human but that’s extremely impressive for an 8B model. Compared to the Moshi model also on the front page now, this model seems to be more coherent, but maybe less conversational?</div><br/></div></div></div></div><div id="41586195" class="c"><input type="checkbox" id="c-41586195" checked=""/><div class="controls bullet"><span class="by">twobitshifter</span><span>|</span><a href="#41585244">parent</a><span>|</span><a href="#41585356">prev</a><span>|</span><a href="#41587319">next</a><span>|</span><label class="collapse" for="c-41586195">[-]</label><label class="expand" for="c-41586195">[1 more]</label></div><br/><div class="children"><div class="content">There is a demo video on the page</div><br/></div></div></div></div><div id="41587319" class="c"><input type="checkbox" id="c-41587319" checked=""/><div class="controls bullet"><span class="by">aussieguy1234</span><span>|</span><a href="#41585244">prev</a><span>|</span><label class="collapse" for="c-41587319">[-]</label><label class="expand" for="c-41587319">[1 more]</label></div><br/><div class="children"><div class="content">Not bad for 3 days training, voice output quality needs some work, it will be interesting to see what effect more training will have.</div><br/></div></div></div></div></div></div></div></body></html>