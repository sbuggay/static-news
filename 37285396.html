<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693213263719" as="style"/><link rel="stylesheet" href="styles.css?v=1693213263719"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2308.08742">PMET: Precise Model Editing in a Transformer</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>PaulHoule</span> | <span>12 comments</span></div><br/><div><div id="37286064" class="c"><input type="checkbox" id="c-37286064" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#37287404">next</a><span>|</span><label class="collapse" for="c-37286064">[-]</label><label class="expand" for="c-37286064">[2 more]</label></div><br/><div class="children"><div class="content">Fyi, Meng et al 2022 [1] is pretty much required reading in order to understand this paper<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.05262" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.05262</a></div><br/><div id="37286132" class="c"><input type="checkbox" id="c-37286132" checked=""/><div class="controls bullet"><span class="by">lucidrains</span><span>|</span><a href="#37286064">parent</a><span>|</span><a href="#37287404">next</a><span>|</span><label class="collapse" for="c-37286132">[-]</label><label class="expand" for="c-37286132">[1 more]</label></div><br/><div class="children"><div class="content">Yannic did a great interview with the authors some time ago <a href="https:&#x2F;&#x2F;youtu.be&#x2F;_NMQyOu2HTo" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;_NMQyOu2HTo</a></div><br/></div></div></div></div><div id="37287404" class="c"><input type="checkbox" id="c-37287404" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#37286064">prev</a><span>|</span><a href="#37287044">next</a><span>|</span><label class="collapse" for="c-37287404">[-]</label><label class="expand" for="c-37287404">[1 more]</label></div><br/><div class="children"><div class="content">This may drop the cost and significantly increase the feasibility for government &#x2F; court mandated changes &#x2F; censoring &#x2F; edits to models.</div><br/></div></div><div id="37287044" class="c"><input type="checkbox" id="c-37287044" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#37287404">prev</a><span>|</span><label class="collapse" for="c-37287044">[-]</label><label class="expand" for="c-37287044">[8 more]</label></div><br/><div class="children"><div class="content">The PRC would doubtless have an interest in precisely removing all knowledge of certain historical facts from LLMs within China.</div><br/><div id="37287316" class="c"><input type="checkbox" id="c-37287316" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37287044">parent</a><span>|</span><a href="#37287248">next</a><span>|</span><label class="collapse" for="c-37287316">[-]</label><label class="expand" for="c-37287316">[6 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just one application.<p>One of the worst problems of LLMs at this point in time is keeping them updated.<p>For instance ChatGPT should be able to talk about the Superbowl in 1984 when the Chicago Bears trounced the New England Patriots (I remember it well because I grew up in New England!) but I couldn&#x27;t expect it to have anything to say about the (other kind of football) game I saw yesterday where West Ham beat Brighton because nothing about the later game is in the training set.<p>This problem just gets worse as time passes and the world continues to change.  Bing&#x27;s chatbot works around this for my soccer example by running a conventional query and then having the LLM summarize it which gave a pretty good summary of the game but when I asked it pointed questions about this particular game such &quot;Who had the most possession?&quot; which was relevant because it was really lopsided in the direction of the losing team,  it fell down,  it seemed to be working off structured statistics that didn&#x27;t have this data as opposed to media reports of the game which surely would have noticed that.<p>With current technology they will need to rebuild the whole thing one day which will (1) be crazy expensive and (2) will break all the document vectors that people have saved from the model which will be a big problem for anybody using systems like LangChain or doing embedding-based similarity search.<p>There&#x27;s a lot of need for some ability to update an LLM incrementally and not wreck it&#x27;s performance and this kind of research points to one path to that.</div><br/><div id="37287909" class="c"><input type="checkbox" id="c-37287909" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#37287044">root</a><span>|</span><a href="#37287316">parent</a><span>|</span><a href="#37288112">next</a><span>|</span><label class="collapse" for="c-37287909">[-]</label><label class="expand" for="c-37287909">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Super_Bowl_XVIII" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Super_Bowl_XVIII</a></div><br/><div id="37288278" class="c"><input type="checkbox" id="c-37288278" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37287044">root</a><span>|</span><a href="#37287909">parent</a><span>|</span><a href="#37288112">next</a><span>|</span><label class="collapse" for="c-37288278">[-]</label><label class="expand" for="c-37288278">[1 more]</label></div><br/><div class="children"><div class="content">Crap I got the year wrong...  It was 1986<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Super_Bowl_XX" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Super_Bowl_XX</a></div><br/></div></div></div></div><div id="37288112" class="c"><input type="checkbox" id="c-37288112" checked=""/><div class="controls bullet"><span class="by">DennisP</span><span>|</span><a href="#37287044">root</a><span>|</span><a href="#37287316">parent</a><span>|</span><a href="#37287909">prev</a><span>|</span><a href="#37287248">next</a><span>|</span><label class="collapse" for="c-37288112">[-]</label><label class="expand" for="c-37288112">[3 more]</label></div><br/><div class="children"><div class="content">How do you save a document vector and do similarity search with it?</div><br/><div id="37288287" class="c"><input type="checkbox" id="c-37288287" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37287044">root</a><span>|</span><a href="#37288112">parent</a><span>|</span><a href="#37288299">next</a><span>|</span><label class="collapse" for="c-37288287">[-]</label><label class="expand" for="c-37288287">[1 more]</label></div><br/><div class="children"><div class="content">There is this<p><a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;chatgpt-retrieval-plugin">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;chatgpt-retrieval-plugin</a><p>I just use SBERT which has models I can run locally<p><a href="https:&#x2F;&#x2F;sbert.net&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;sbert.net&#x2F;</a></div><br/></div></div><div id="37288299" class="c"><input type="checkbox" id="c-37288299" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#37287044">root</a><span>|</span><a href="#37288112">parent</a><span>|</span><a href="#37288287">prev</a><span>|</span><a href="#37287248">next</a><span>|</span><label class="collapse" for="c-37288299">[-]</label><label class="expand" for="c-37288299">[1 more]</label></div><br/><div class="children"><div class="content">You encode your document with some kind of embedding, eg HuggingFace Sentence Transformers: <a href="https:&#x2F;&#x2F;www.sbert.net&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sbert.net&#x2F;</a> (probably most commonly used) or OpenAI Embeddings: <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings&#x2F;what-are-embeddings" rel="nofollow noreferrer">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings&#x2F;what-are-...</a> and then use a vector database (Elastic, Postgres, FAISS or whatever) to do a similarity  search.</div><br/></div></div></div></div></div></div><div id="37287248" class="c"><input type="checkbox" id="c-37287248" checked=""/><div class="controls bullet"><span class="by">quantum_state</span><span>|</span><a href="#37287044">parent</a><span>|</span><a href="#37287316">prev</a><span>|</span><label class="collapse" for="c-37287248">[-]</label><label class="expand" for="c-37287248">[1 more]</label></div><br/><div class="children"><div class="content">they could just use it without publishing the paper … wonder what the reason could be …</div><br/></div></div></div></div></div></div></div></div></div></body></html>