<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1727946065570" as="style"/><link rel="stylesheet" href="styles.css?v=1727946065570"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/slashml/amd_inference">AMD GPU Inference</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>fazkan</span> | <span>84 comments</span></div><br/><div><div id="41722860" class="c"><input type="checkbox" id="c-41722860" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#41725780">next</a><span>|</span><label class="collapse" for="c-41722860">[-]</label><label class="expand" for="c-41722860">[10 more]</label></div><br/><div class="children"><div class="content">For inference, if you have a supported card (or probably architecture if you are on Linux and can use HSA_OVERRIDE_GFX_VERSION), then  you can probably run anything with (upstream) PyTorch and transformers. Also, compiling llama.cpp is has been pretty trouble-free for me for at least a year.<p>(If you are on Windows, there is usually a win-hip binary of llama.cpp in the project&#x27;s releases or if things totally refuse to work, you can use the Vulkan build as a (less performant) fallback).<p>Having more options can&#x27;t hurt, but ROCm 5.4.2 is almost 2 years old, and things have come a long way since then, so I&#x27;m curious about this being published freshly today, in October 2024.<p>BTW, I recently went through and updated my compatibility doc (focused on RDNA3) w&#x2F; ROCm 6.2 for those interested. A lot has changed just in the past few months (upstream bitsandbytes, upstream xformers, and Triton-based Flash Attention): <a href="https:&#x2F;&#x2F;llm-tracker.info&#x2F;howto&#x2F;AMD-GPUs" rel="nofollow">https:&#x2F;&#x2F;llm-tracker.info&#x2F;howto&#x2F;AMD-GPUs</a></div><br/><div id="41723493" class="c"><input type="checkbox" id="c-41723493" checked=""/><div class="controls bullet"><span class="by">woodrowbarlow</span><span>|</span><a href="#41722860">parent</a><span>|</span><a href="#41725458">next</a><span>|</span><label class="collapse" for="c-41723493">[-]</label><label class="expand" for="c-41723493">[1 more]</label></div><br/><div class="children"><div class="content">i also have been playing with inference on the amd 7900xtx, and i agree. there are no hoops to jump through these days. just make sure to install the rocm version of torch (if using a1111 or similar, don&#x27;t trust requirements.txt), as shown clearly on the pytorch homepage. obsidian is a similar story. hip is straightforward, at least on arch and ubuntu (fedora still requires some twiddling, though). i didn&#x27;t realize xformers is also functional! that&#x27;s good news.</div><br/></div></div><div id="41725458" class="c"><input type="checkbox" id="c-41725458" checked=""/><div class="controls bullet"><span class="by">qamononep</span><span>|</span><a href="#41722860">parent</a><span>|</span><a href="#41723493">prev</a><span>|</span><a href="#41723714">next</a><span>|</span><label class="collapse" for="c-41725458">[-]</label><label class="expand" for="c-41725458">[5 more]</label></div><br/><div class="children"><div class="content">It would be great if you included a section on running with Docker on Linux. The only one that worked out of the box was Ollama, and it had an example.
<a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;docker.md">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;docker.md</a><p>has a docker image but no examples to run it
<a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;docs&#x2F;docker.md">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;docs&#x2F;dock...</a><p>has a docker image but no examples to run it
<a href="https:&#x2F;&#x2F;github.com&#x2F;LostRuins&#x2F;koboldcpp?tab=readme-ov-file#docker">https:&#x2F;&#x2F;github.com&#x2F;LostRuins&#x2F;koboldcpp?tab=readme-ov-file#do...</a><p>docker image was broken for me on 7800xt running rhel9
<a href="https:&#x2F;&#x2F;github.com&#x2F;Atinoda&#x2F;text-generation-webui-docker">https:&#x2F;&#x2F;github.com&#x2F;Atinoda&#x2F;text-generation-webui-docker</a></div><br/><div id="41725573" class="c"><input type="checkbox" id="c-41725573" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41722860">root</a><span>|</span><a href="#41725458">parent</a><span>|</span><a href="#41723714">next</a><span>|</span><label class="collapse" for="c-41725573">[-]</label><label class="expand" for="c-41725573">[4 more]</label></div><br/><div class="children"><div class="content">good feedback thanks, would you be able to open an issue</div><br/><div id="41725825" class="c"><input type="checkbox" id="c-41725825" checked=""/><div class="controls bullet"><span class="by">qamononep</span><span>|</span><a href="#41722860">root</a><span>|</span><a href="#41725573">parent</a><span>|</span><a href="#41723714">next</a><span>|</span><label class="collapse" for="c-41725825">[-]</label><label class="expand" for="c-41725825">[3 more]</label></div><br/><div class="children"><div class="content">this repo?
<a href="https:&#x2F;&#x2F;github.com&#x2F;AUGMXNT&#x2F;llm-tracker.info-vault&#x2F;issues">https:&#x2F;&#x2F;github.com&#x2F;AUGMXNT&#x2F;llm-tracker.info-vault&#x2F;issues</a></div><br/><div id="41726208" class="c"><input type="checkbox" id="c-41726208" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#41722860">root</a><span>|</span><a href="#41725825">parent</a><span>|</span><a href="#41726280">next</a><span>|</span><label class="collapse" for="c-41726208">[-]</label><label class="expand" for="c-41726208">[1 more]</label></div><br/><div class="children"><div class="content">I think fazkan was confused about which repo you were talking about. For the llm-tracker doc, that&#x27;s something I maintain. It&#x27;s based on stuff I test but if you want to submit a PR or issue w&#x2F; info in a way that I can verify then I&#x27;m happy to add a Docker section.</div><br/></div></div><div id="41726280" class="c"><input type="checkbox" id="c-41726280" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41722860">root</a><span>|</span><a href="#41725825">parent</a><span>|</span><a href="#41726208">prev</a><span>|</span><a href="#41723714">next</a><span>|</span><label class="collapse" for="c-41726280">[-]</label><label class="expand" for="c-41726280">[1 more]</label></div><br/><div class="children"><div class="content">haha, I was a bit confused, but I was referring to this one <a href="https:&#x2F;&#x2F;github.com&#x2F;slashml&#x2F;amd_inference">https:&#x2F;&#x2F;github.com&#x2F;slashml&#x2F;amd_inference</a>. But the comment applies to other repos as well, do open issues in them, helps the maintainers prioritize features.</div><br/></div></div></div></div></div></div></div></div><div id="41723714" class="c"><input type="checkbox" id="c-41723714" checked=""/><div class="controls bullet"><span class="by">conshama</span><span>|</span><a href="#41722860">parent</a><span>|</span><a href="#41725458">prev</a><span>|</span><a href="#41725780">next</a><span>|</span><label class="collapse" for="c-41723714">[-]</label><label class="expand" for="c-41723714">[3 more]</label></div><br/><div class="children"><div class="content">related: <a href="https:&#x2F;&#x2F;www.nonbios.ai&#x2F;post&#x2F;deploying-large-405b-models-in-full-precision-on-runpod" rel="nofollow">https:&#x2F;&#x2F;www.nonbios.ai&#x2F;post&#x2F;deploying-large-405b-models-in-f...</a><p>tldr: uses the latest rocm 6.2 to run full precision inference for llama 405b on a single node 8 x MI300x AMD GPU<p>How mature do you think Rocm 6.2-AMD stack is compared to Nvidia ?</div><br/><div id="41723748" class="c"><input type="checkbox" id="c-41723748" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41722860">root</a><span>|</span><a href="#41723714">parent</a><span>|</span><a href="#41725780">next</a><span>|</span><label class="collapse" for="c-41723748">[-]</label><label class="expand" for="c-41723748">[2 more]</label></div><br/><div class="children"><div class="content">this uses vllm?</div><br/><div id="41727768" class="c"><input type="checkbox" id="c-41727768" checked=""/><div class="controls bullet"><span class="by">conshama</span><span>|</span><a href="#41722860">root</a><span>|</span><a href="#41723748">parent</a><span>|</span><a href="#41725780">next</a><span>|</span><label class="collapse" for="c-41727768">[-]</label><label class="expand" for="c-41727768">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div></div></div></div></div></div></div><div id="41725780" class="c"><input type="checkbox" id="c-41725780" checked=""/><div class="controls bullet"><span class="by">tcdent</span><span>|</span><a href="#41722860">prev</a><span>|</span><a href="#41728358">next</a><span>|</span><label class="collapse" for="c-41725780">[-]</label><label class="expand" for="c-41725780">[2 more]</label></div><br/><div class="children"><div class="content">The rise of generated slop ml libraries is staggering.<p>This library is 50% print statements. And where it does branch, it doesn&#x27;t even need to.<p>Defines two environment variables and sets two flags on torch.</div><br/><div id="41727704" class="c"><input type="checkbox" id="c-41727704" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#41725780">parent</a><span>|</span><a href="#41728358">next</a><span>|</span><label class="collapse" for="c-41727704">[-]</label><label class="expand" for="c-41727704">[1 more]</label></div><br/><div class="children"><div class="content">I also had to go to therapy to cure myself of the misunderstanding that data scientists and machine learning folks are software engineers, and expecting the same work product from those disparate audiences only raises your blood pressure<p>Expectation management is a huge part of any team&#x2F;organization, I think</div><br/></div></div></div></div><div id="41728358" class="c"><input type="checkbox" id="c-41728358" checked=""/><div class="controls bullet"><span class="by">danielEM</span><span>|</span><a href="#41725780">prev</a><span>|</span><a href="#41726049">next</a><span>|</span><label class="collapse" for="c-41728358">[-]</label><label class="expand" for="c-41728358">[1 more]</label></div><br/><div class="children"><div class="content">It has been like 8 months since I got Ryzen 8700G with NPU just for the purpose of inferencing NN, and so far only acceleration I&#x27;m getting is through vulkan on iGPU, not NPU (I&#x27;m using Linux only). On the bright side, with 64GB of RAM had no isues with trying models over 32GB. Kudos to llama.cpp for supporting vulkan backend!</div><br/></div></div><div id="41726049" class="c"><input type="checkbox" id="c-41726049" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#41728358">prev</a><span>|</span><a href="#41718440">next</a><span>|</span><label class="collapse" for="c-41726049">[-]</label><label class="expand" for="c-41726049">[5 more]</label></div><br/><div class="children"><div class="content">On Ubuntu 24.04 (and Debian UnstableÂ¹), the OS-provided packages should be able to get llama.cpp running on ROCm on just about any discrete AMD GPU from Vega onwardsÂ²Â³â´. No docker or HSA_OVERRIDE_GFX_VERSION required. The performance might not be ideal in every caseâµ, but I&#x27;ve tested a wide variety of cards:<p><pre><code>    # install dependencies
    sudo apt -y update
    sudo apt -y upgrade
    sudo apt -y install git wget hipcc libhipblas-dev librocblas-dev cmake build-essential

    # ensure you have permissions by adding yourself to the video and render groups
    sudo usermod -aG video,render $USER
    # log out and then log back in to apply the group changes
    # you can run `rocminfo` and look for your GPU in the output to check everything is working thus far

    # download a model, build llama.cpp, and run it
    wget https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;dolphin-2.2.1-mistral-7B-GGUF&#x2F;resolve&#x2F;main&#x2F;dolphin-2.2.1-mistral-7b.Q5_K_M.gguf?download=true -O dolphin-2.2.1-mistral-7b.Q5_K_M.gguf
    git clone https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp.git
    cd llama.cpp
    git checkout b3267
    HIPCXX=clang-17 cmake -H. -Bbuild -DGGML_HIPBLAS=ON -DCMAKE_HIP_ARCHITECTURES=&quot;gfx803;gfx900;gfx906;gfx908;gfx90a;gfx1010;gfx1030;gfx1100;gfx1101;gfx1102&quot; -DCMAKE_BUILD_TYPE=Release
    make -j16 -C build
    build&#x2F;bin&#x2F;llama-cli -ngl 32 --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -m ..&#x2F;dolphin-2.2.1-mistral-7b.Q5_K_M.gguf --prompt &quot;Once upon a time&quot;
</code></pre>
I&#x27;d suggest RDNA 3, MI200 and MI300 users should probably use the AMD-provided ROCm packages for improved performance. Users that need PyTorch should also use the AMD-provided ROCm packages, as PyTorch has some dependencies that are not available from the system packages. Still, you can&#x27;t beat the ease of installation or the compatibility with older hardware provided by the OS packages.<p>Â¹ <a href="https:&#x2F;&#x2F;lists.debian.org&#x2F;debian-ai&#x2F;2024&#x2F;07&#x2F;msg00002.html" rel="nofollow">https:&#x2F;&#x2F;lists.debian.org&#x2F;debian-ai&#x2F;2024&#x2F;07&#x2F;msg00002.html</a>
Â² Not including MI300 because that released too close to the Ubuntu 24.04 launch.
Â³ Pre-Vega architectures might work, but have known bugs for some applications.
â´ Vega and RDNA 2 APUs might work with Linux 6.10+ installed. I&#x27;m in the process of testing that.
âµ The version of rocBLAS that comes with Ubuntu 24.04 is a bit old and therefore lacks some optimizations for RDNA 3. It&#x27;s also missing some MI200 optimizations.</div><br/><div id="41726601" class="c"><input type="checkbox" id="c-41726601" checked=""/><div class="controls bullet"><span class="by">mindcrime</span><span>|</span><a href="#41726049">parent</a><span>|</span><a href="#41726785">next</a><span>|</span><label class="collapse" for="c-41726601">[-]</label><label class="expand" for="c-41726601">[1 more]</label></div><br/><div class="children"><div class="content">I was able to install (AMD provided) ROCm and Ollama on Ubuntu 22.04.5 with an RX 7900 XTX with no real problems to speak of, and I can execute LLMs using Ollama on ROCm just fine. Take that FWIW.</div><br/></div></div><div id="41726785" class="c"><input type="checkbox" id="c-41726785" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#41726049">parent</a><span>|</span><a href="#41726601">prev</a><span>|</span><a href="#41718440">next</a><span>|</span><label class="collapse" for="c-41726785">[-]</label><label class="expand" for="c-41726785">[3 more]</label></div><br/><div class="children"><div class="content">are there AMD cards with more than 24GB VRAM on the market right now at consumer friendly prices?</div><br/><div id="41727856" class="c"><input type="checkbox" id="c-41727856" checked=""/><div class="controls bullet"><span class="by">coolspot</span><span>|</span><a href="#41726049">root</a><span>|</span><a href="#41726785">parent</a><span>|</span><a href="#41727053">next</a><span>|</span><label class="collapse" for="c-41727856">[-]</label><label class="expand" for="c-41727856">[1 more]</label></div><br/><div class="children"><div class="content">Amazon prices:<p>$3,600 - 61 TFLOPS - AMD Radeon Pro W7900<p>$4,200 - 38.7 TFLOPS - NVidia RTX A6000 48GB Ampere<p>$7,200 - 91.1 TFLOPS - NVidia RTX A6000 48GB Ada</div><br/></div></div><div id="41727053" class="c"><input type="checkbox" id="c-41727053" checked=""/><div class="controls bullet"><span class="by">mindcrime</span><span>|</span><a href="#41726049">root</a><span>|</span><a href="#41726785">parent</a><span>|</span><a href="#41727856">prev</a><span>|</span><a href="#41718440">next</a><span>|</span><label class="collapse" for="c-41727053">[-]</label><label class="expand" for="c-41727053">[1 more]</label></div><br/><div class="children"><div class="content">It sort of depends on how you define &quot;consumer friendly prices&quot;. AFAIK, in the $1000 - &quot;slightly over or under $1000&quot; range, 24GB is all you can get. But there are Radeon Pro boards with 32GB or 48GB of RAM for various prices between around $2000 to about $3500. So not &quot;cheap&quot; but possibly within reach for a serious hobbyist who doesn&#x27;t mind spending a little bit more.</div><br/></div></div></div></div></div></div><div id="41718440" class="c"><input type="checkbox" id="c-41718440" checked=""/><div class="controls bullet"><span class="by">a2128</span><span>|</span><a href="#41726049">prev</a><span>|</span><a href="#41726739">next</a><span>|</span><label class="collapse" for="c-41718440">[-]</label><label class="expand" for="c-41718440">[2 more]</label></div><br/><div class="children"><div class="content">It seems to use an old, 2 year old version of ROCm (5.4.2) which I&#x27;m doubtful would support my RX 7900 XTX. I personally found it easiest to just use the latest `rocm&#x2F;pytorch` image and run what I need from there</div><br/><div id="41725594" class="c"><input type="checkbox" id="c-41725594" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#41718440">parent</a><span>|</span><a href="#41726739">next</a><span>|</span><label class="collapse" for="c-41725594">[-]</label><label class="expand" for="c-41725594">[1 more]</label></div><br/><div class="children"><div class="content">The RX 7900 XTX (gfx1100) was first enabled in the math libraries (e.g. rocBLAS) for ROCm 5.4, but I don&#x27;t think the AI libraries (e.g. MIOpen) had it enabled until ROCm 5.5. I believe the performance improved significantly in later releases, as well.</div><br/></div></div></div></div><div id="41726739" class="c"><input type="checkbox" id="c-41726739" checked=""/><div class="controls bullet"><span class="by">rglullis</span><span>|</span><a href="#41718440">prev</a><span>|</span><a href="#41723214">next</a><span>|</span><label class="collapse" for="c-41726739">[-]</label><label class="expand" for="c-41726739">[1 more]</label></div><br/><div class="children"><div class="content">So, this is all I needed to add to NixOS workstation:<p><pre><code>     hardware.graphics.enable = true;

     services.ollama = {
     enable = true;
     acceleration = &quot;rocm&quot;;
     environmentVariables = {
       ROC_ENABLE_PRE_VEGA = &quot;1&quot;;
       HSA_OVERRIDE_GFX_VERSION = &quot;11.0.0&quot;;
     };
   };</code></pre></div><br/></div></div><div id="41723214" class="c"><input type="checkbox" id="c-41723214" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#41726739">prev</a><span>|</span><a href="#41723428">next</a><span>|</span><label class="collapse" for="c-41723214">[-]</label><label class="expand" for="c-41723214">[14 more]</label></div><br/><div class="children"><div class="content">I almost tried to install AMD rocm a while ago after discovering the simplicity of llamafile.<p><pre><code>  sudo apt install rocm

  Summary:
    Upgrading: 0, Installing: 203, Removing: 0, Not Upgrading: 0
    Download size: 2,369 MB &#x2F; 2,371 MB
    Space needed: 35.7 GB &#x2F; 822 GB available
</code></pre>
I don&#x27;t understand how 36 GB can be justified for what amounts to a GPU driver.</div><br/><div id="41724067" class="c"><input type="checkbox" id="c-41724067" checked=""/><div class="controls bullet"><span class="by">atq2119</span><span>|</span><a href="#41723214">parent</a><span>|</span><a href="#41725267">next</a><span>|</span><label class="collapse" for="c-41724067">[-]</label><label class="expand" for="c-41724067">[6 more]</label></div><br/><div class="children"><div class="content">So no doubt modern software is ridiculously bloated, but ROCm isn&#x27;t just a GPU driver. It includes all sorts of tools and libraries as well.<p>By comparison, if you go and download the CUDA toolkit as a single file, you get a download file that&#x27;s over 4GB, so quite a bit larger than the download size you quoted. I haven&#x27;t checked how much that expands to (it seems the ROCm install has a lot of redundancy given how well it compresses), but the point is, you get something that seems insanely large either way.</div><br/><div id="41724590" class="c"><input type="checkbox" id="c-41724590" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41724067">parent</a><span>|</span><a href="#41725267">next</a><span>|</span><label class="collapse" for="c-41724590">[-]</label><label class="expand" for="c-41724590">[5 more]</label></div><br/><div class="children"><div class="content">I suspected that, but any binaries being that large just seems wrong, I mean the whole thing is 35 time larger than my entire OS install.<p>Do you know what is included in ROCm that could be so big? Does it include training datasets or something?</div><br/><div id="41726074" class="c"><input type="checkbox" id="c-41726074" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41724590">parent</a><span>|</span><a href="#41725308">next</a><span>|</span><label class="collapse" for="c-41726074">[-]</label><label class="expand" for="c-41726074">[2 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s the big files in my &#x2F;opt&#x2F;rocm&#x2F;lib which is most of it:<p><pre><code>  4.8G hipblaslt
  1.6G libdevice_conv_operations.a
  2.0G libdevice_gemm_operations.a
  1.4G libMIOpen.so.1.0.60200
  1.1G librocblas.so.4.2.60200
  1.6G librocsolver.so.0.2.60200
  1.4G librocsparse.so.1.0.60200
  1.5G llvm
  3.5G rocblas
  2.0G rocfft
</code></pre>
The biggest one just to pick on one is hipblaslt is &quot;a library that provides general matrix-matrix operations. It has a flexible API that extends functionalities beyond a traditional BLAS library, such as adding flexibility to matrix data layouts, input types, compute types, and algorithmic implementations and heuristics.&quot; <a href="https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;hipBLASLt">https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;hipBLASLt</a><p>There are mostly GPU kernels that by themselves aren&#x27;t so big, but for every single operation x every single supported graphics architecture, eg:<p><pre><code>  304K TensileLibrary_SS_SS_UA_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx942.co
  24K TensileLibrary_SS_SS_UA_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx942.dat
  240K TensileLibrary_SS_SS_UA_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx942.co
  20K TensileLibrary_SS_SS_UA_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx942.dat
  344K TensileLibrary_SS_SS_UA_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx942.co
  24K TensileLibrary_SS_SS_UA_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx942.dat</code></pre></div><br/><div id="41726937" class="c"><input type="checkbox" id="c-41726937" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41726074">parent</a><span>|</span><a href="#41725308">next</a><span>|</span><label class="collapse" for="c-41726937">[-]</label><label class="expand" for="c-41726937">[1 more]</label></div><br/><div class="children"><div class="content">Ok so like four of those files literally just do matrix multiplications</div><br/></div></div></div></div><div id="41725308" class="c"><input type="checkbox" id="c-41725308" checked=""/><div class="controls bullet"><span class="by">skirmish</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41724590">parent</a><span>|</span><a href="#41726074">prev</a><span>|</span><a href="#41725267">next</a><span>|</span><label class="collapse" for="c-41725308">[-]</label><label class="expand" for="c-41725308">[2 more]</label></div><br/><div class="children"><div class="content">My understanding is that ROCm contains all included kernels for each supported architecture, so it would have (made up):<p><pre><code>  -- matrix multiply 2048x2048 for Navi 31,
  -- same for Navi 32,
  -- same for Navi 33,
  -- same for Navi 21,
  -- same for Navi 22,
  -- same for Navi 23,
  -- same for Navi 24, etc.
  -- matrix multiply 4096x4096 for Navi 31,
  -- ...</code></pre></div><br/><div id="41726136" class="c"><input type="checkbox" id="c-41726136" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41725308">parent</a><span>|</span><a href="#41725267">next</a><span>|</span><label class="collapse" for="c-41726136">[-]</label><label class="expand" for="c-41726136">[1 more]</label></div><br/><div class="children"><div class="content">Correct. Although, you wouldn&#x27;t find Navi 22, 23 or 24 in the list because those particular architectures are not supported. Instead, you&#x27;d see Vega 10, Vega 20, Arcturus, Aldebaran, Aqua Vanjaram and sometimes Polaris.<p>We&#x27;re working on a few different strategies to reduce the binary size. It will get worse before it gets better, but I think you can expect significant improvements in the future. There are lots of ways to slim the libraries down.</div><br/></div></div></div></div></div></div></div></div><div id="41725267" class="c"><input type="checkbox" id="c-41725267" checked=""/><div class="controls bullet"><span class="by">steeve</span><span>|</span><a href="#41723214">parent</a><span>|</span><a href="#41724067">prev</a><span>|</span><a href="#41725064">next</a><span>|</span><label class="collapse" for="c-41725267">[-]</label><label class="expand" for="c-41725267">[2 more]</label></div><br/><div class="children"><div class="content">You can look us up at <a href="https:&#x2F;&#x2F;github.com&#x2F;zml&#x2F;zml">https:&#x2F;&#x2F;github.com&#x2F;zml&#x2F;zml</a>, we fix that.</div><br/><div id="41725547" class="c"><input type="checkbox" id="c-41725547" checked=""/><div class="controls bullet"><span class="by">andyferris</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41725267">parent</a><span>|</span><a href="#41725064">next</a><span>|</span><label class="collapse" for="c-41725547">[-]</label><label class="expand" for="c-41725547">[1 more]</label></div><br/><div class="children"><div class="content">Wait, looking at that link I don&#x27;t see how it avoids downloading CUDA or ROCM. Do you use MLIR to compile to GPU without using the vendor provided tooling at all?</div><br/></div></div></div></div><div id="41725064" class="c"><input type="checkbox" id="c-41725064" checked=""/><div class="controls bullet"><span class="by">burnte</span><span>|</span><a href="#41723214">parent</a><span>|</span><a href="#41725267">prev</a><span>|</span><a href="#41723601">next</a><span>|</span><label class="collapse" for="c-41725064">[-]</label><label class="expand" for="c-41725064">[1 more]</label></div><br/><div class="children"><div class="content">CPU drivers are complete OSes that run on the GPUs now.</div><br/></div></div><div id="41723601" class="c"><input type="checkbox" id="c-41723601" checked=""/><div class="controls bullet"><span class="by">greenavocado</span><span>|</span><a href="#41723214">parent</a><span>|</span><a href="#41725064">prev</a><span>|</span><a href="#41723428">next</a><span>|</span><label class="collapse" for="c-41723601">[-]</label><label class="expand" for="c-41723601">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not just you; AMD manages to completely shit-up the Linux kernel with their drivers: <a href="https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;AMD-5-Million-Lines" rel="nofollow">https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;AMD-5-Million-Lines</a></div><br/><div id="41723819" class="c"><input type="checkbox" id="c-41723819" checked=""/><div class="controls bullet"><span class="by">striking</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41723601">parent</a><span>|</span><a href="#41723745">next</a><span>|</span><label class="collapse" for="c-41723819">[-]</label><label class="expand" for="c-41723819">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Of course, much of that is auto-generated header files... A large portion of it with AMD continuing to introduce new auto-generated header files with each new generation&#x2F;version of a given block. These verbose header files has been AMD&#x27;s alternative to creating exhaustive public documentation on their GPUs that they were once known for.</div><br/><div id="41724013" class="c"><input type="checkbox" id="c-41724013" checked=""/><div class="controls bullet"><span class="by">NekkoDroid</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41723819">parent</a><span>|</span><a href="#41723745">next</a><span>|</span><label class="collapse" for="c-41724013">[-]</label><label class="expand" for="c-41724013">[1 more]</label></div><br/><div class="children"><div class="content">There have been talks about moving those headers to a separate repo and only including the needed headers upstream[1]<p>[1]: <a href="https:&#x2F;&#x2F;gitlab.freedesktop.org&#x2F;drm&#x2F;amd&#x2F;-&#x2F;issues&#x2F;3636" rel="nofollow">https:&#x2F;&#x2F;gitlab.freedesktop.org&#x2F;drm&#x2F;amd&#x2F;-&#x2F;issues&#x2F;3636</a></div><br/></div></div></div></div><div id="41723745" class="c"><input type="checkbox" id="c-41723745" checked=""/><div class="controls bullet"><span class="by">anthk</span><span>|</span><a href="#41723214">root</a><span>|</span><a href="#41723601">parent</a><span>|</span><a href="#41723819">prev</a><span>|</span><a href="#41723428">next</a><span>|</span><label class="collapse" for="c-41723745">[-]</label><label class="expand" for="c-41723745">[1 more]</label></div><br/><div class="children"><div class="content">OpenBSD, too.</div><br/></div></div></div></div></div></div><div id="41723428" class="c"><input type="checkbox" id="c-41723428" checked=""/><div class="controls bullet"><span class="by">freeqaz</span><span>|</span><a href="#41723214">prev</a><span>|</span><a href="#41727346">next</a><span>|</span><label class="collapse" for="c-41723428">[-]</label><label class="expand" for="c-41723428">[12 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the best bang-for-your-buck AMD GPU these days? I just bought 2 used 3090s for $750ish refurb&#x27;d on eBay. Curious what others are using for running LLMs locally.</div><br/><div id="41727921" class="c"><input type="checkbox" id="c-41727921" checked=""/><div class="controls bullet"><span class="by">snep</span><span>|</span><a href="#41723428">parent</a><span>|</span><a href="#41723470">next</a><span>|</span><label class="collapse" for="c-41727921">[-]</label><label class="expand" for="c-41727921">[1 more]</label></div><br/><div class="children"><div class="content">I bought an MI100 recently for $650. 32GB of HBM2 and it performs around around 0-5% faster than a 3090 on the default flash attention 2 benchmarks. Performance on actual applications can be mixed though, as many are not well optimised for CDNA&#x27;s matrix cores -  even where work has been done for RDNA, which is not that often, it doesn&#x27;t necessarily carry over. It&#x27;s also frustrating when efforts to improve performance get turned back by maintainers: llama.cpp closing PR for flash attention on AMD because the requisite (header-only) lib is supposedly adding an unneeded dependency (<a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;7011">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;7011</a>).<p>There&#x27;s also a few tricks&#x2F;updates I&#x27;d like to try which may improve performance, e.g. hipblaslt support being added next rocm release - of course these are &quot;maybes&quot;.<p>To give you a rough idea of practical performance, default SDXL with xformers is around 4.5-5it&#x2F;s (between 3090 and 4090 from my understanding), and exllamav2 with qwen 72B at 3bpw is around 7t&#x2F;s (slower than a 3090, though a 3090 has to use a lower precision to fit).<p>As others have pointed out, I can&#x27;t really see what this project offers for AMD users over existing options like llama.cpp, exllamav2, mlc-ai, etc. Most projects work relatively easily these days.</div><br/></div></div><div id="41723470" class="c"><input type="checkbox" id="c-41723470" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#41723428">parent</a><span>|</span><a href="#41727921">prev</a><span>|</span><a href="#41726881">next</a><span>|</span><label class="collapse" for="c-41723470">[-]</label><label class="expand" for="c-41723470">[5 more]</label></div><br/><div class="children"><div class="content">Personal experience: It&#x27;s not even worth it. AMD (i)GPU breaks with every pytorch, ROCm, xformers, or ollama updates. You&#x27;ll sleep more compfortably at night.</div><br/><div id="41725423" class="c"><input type="checkbox" id="c-41725423" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#41723428">root</a><span>|</span><a href="#41723470">parent</a><span>|</span><a href="#41723918">next</a><span>|</span><label class="collapse" for="c-41725423">[-]</label><label class="expand" for="c-41725423">[2 more]</label></div><br/><div class="children"><div class="content">When dealing with ROCM, it&#x27;s critical that once you have a working configuration, you freeze everything in place (except your application). Docker is one way to achieve this if your host machine is subject to kernel or package updates</div><br/><div id="41728712" class="c"><input type="checkbox" id="c-41728712" checked=""/><div class="controls bullet"><span class="by">horsebridge</span><span>|</span><a href="#41723428">root</a><span>|</span><a href="#41725423">parent</a><span>|</span><a href="#41723918">next</a><span>|</span><label class="collapse" for="c-41728712">[-]</label><label class="expand" for="c-41728712">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really have any problem with ROCm these days, although I only use system packages. It used to be quite wonky though, and I&#x27;ve totally given up on custom ROCm installs.</div><br/></div></div></div></div><div id="41723918" class="c"><input type="checkbox" id="c-41723918" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41723428">root</a><span>|</span><a href="#41723470">parent</a><span>|</span><a href="#41725423">prev</a><span>|</span><a href="#41726876">next</a><span>|</span><label class="collapse" for="c-41723918">[-]</label><label class="expand" for="c-41723918">[1 more]</label></div><br/><div class="children"><div class="content">Thats our observation, which is why we wrote the scripts ourselves that way we can control the dependencies at least.</div><br/></div></div><div id="41726876" class="c"><input type="checkbox" id="c-41726876" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#41723428">root</a><span>|</span><a href="#41723470">parent</a><span>|</span><a href="#41723918">prev</a><span>|</span><a href="#41726881">next</a><span>|</span><label class="collapse" for="c-41726876">[-]</label><label class="expand" for="c-41726876">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not the experience I have. I&#x27;ve been using ollama for 6 months on mine and never had any issues with ROCm breaking.</div><br/></div></div></div></div><div id="41726881" class="c"><input type="checkbox" id="c-41726881" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#41723428">parent</a><span>|</span><a href="#41723470">prev</a><span>|</span><a href="#41724469">next</a><span>|</span><label class="collapse" for="c-41726881">[-]</label><label class="expand" for="c-41726881">[1 more]</label></div><br/><div class="children"><div class="content">I got my radeon pro vii for â¬300 new. Was not a bad deal IMO especially since it comes with HBM2 and has the same memory bandwidth as the 4090 (1TB&#x2F;s). It&#x27;s got only 16GB though.</div><br/></div></div><div id="41724469" class="c"><input type="checkbox" id="c-41724469" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#41723428">parent</a><span>|</span><a href="#41726881">prev</a><span>|</span><a href="#41727346">next</a><span>|</span><label class="collapse" for="c-41724469">[-]</label><label class="expand" for="c-41724469">[4 more]</label></div><br/><div class="children"><div class="content">Probably the 7900xtx. $1k for 24GB of RAM.</div><br/><div id="41725471" class="c"><input type="checkbox" id="c-41725471" checked=""/><div class="controls bullet"><span class="by">freeqaz</span><span>|</span><a href="#41723428">root</a><span>|</span><a href="#41724469">parent</a><span>|</span><a href="#41727346">next</a><span>|</span><label class="collapse" for="c-41725471">[-]</label><label class="expand" for="c-41725471">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s about the same price as a 3090 and it&#x27;s also 24GB. Are they faster at inference?</div><br/><div id="41725532" class="c"><input type="checkbox" id="c-41725532" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#41723428">root</a><span>|</span><a href="#41725471">parent</a><span>|</span><a href="#41725623">next</a><span>|</span><label class="collapse" for="c-41725532">[-]</label><label class="expand" for="c-41725532">[1 more]</label></div><br/><div class="children"><div class="content">I doubt it, but the 3090 is a four year old card which means it might have a lot of mileage from the previous owner. A lot of them are from mining rigs.</div><br/></div></div><div id="41725623" class="c"><input type="checkbox" id="c-41725623" checked=""/><div class="controls bullet"><span class="by">sipjca</span><span>|</span><a href="#41723428">root</a><span>|</span><a href="#41725471">parent</a><span>|</span><a href="#41725532">prev</a><span>|</span><a href="#41727346">next</a><span>|</span><label class="collapse" for="c-41725623">[-]</label><label class="expand" for="c-41725623">[1 more]</label></div><br/><div class="children"><div class="content">it is not, at least in llama.cpp&#x2F;llamafile<p><a href="https:&#x2F;&#x2F;benchmarks.andromeda.computer&#x2F;compare" rel="nofollow">https:&#x2F;&#x2F;benchmarks.andromeda.computer&#x2F;compare</a></div><br/></div></div></div></div></div></div></div></div><div id="41727346" class="c"><input type="checkbox" id="c-41727346" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#41723428">prev</a><span>|</span><a href="#41722902">next</a><span>|</span><label class="collapse" for="c-41727346">[-]</label><label class="expand" for="c-41727346">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re interested in how much the AMD graphics cards cost compared to the NVidia ones, I have <a href="https:&#x2F;&#x2F;gpuquicklist.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gpuquicklist.com&#x2F;</a> which gives you a quick table view of lowest prices available on Amazon that I can find. &lt;&#x2F; selfpromotion&gt;</div><br/></div></div><div id="41722902" class="c"><input type="checkbox" id="c-41722902" checked=""/><div class="controls bullet"><span class="by">leonheld</span><span>|</span><a href="#41727346">prev</a><span>|</span><a href="#41723340">next</a><span>|</span><label class="collapse" for="c-41722902">[-]</label><label class="expand" for="c-41722902">[10 more]</label></div><br/><div class="children"><div class="content">People use &quot;Docker-based&quot; all the time but what they mean is that they ship $SOFTWARE in a Docker image.<p>&quot;Docker-based&quot; reads, to me, as if you were doing Inference on AMD cards with Docker somehow, which doesn&#x27;t make sense.</div><br/><div id="41723192" class="c"><input type="checkbox" id="c-41723192" checked=""/><div class="controls bullet"><span class="by">a_vanderbilt</span><span>|</span><a href="#41722902">parent</a><span>|</span><a href="#41723263">next</a><span>|</span><label class="collapse" for="c-41723192">[-]</label><label class="expand" for="c-41723192">[1 more]</label></div><br/><div class="children"><div class="content">You can do inference from a Docker container, just as you&#x27;d do it with NVidia. OpenAI runs a K8s cluster doing this. I have personally only worked with NVidia, but the docs are present for AMD too.<p>Like anything AI and AMD, you need the right card(s) and rocm version along with sheer dumb luck to get it working. AMD has Docker images with rocm support, so you could merge your app in with that as the base layer. Just pass through the GPU to the container and you should get it working.<p>It might just be the software in a Docker image, but it removes a variable I would otherwise have to worry about during deployment. It literally is inference on AMD with Docker, if that&#x27;s what you meant.</div><br/></div></div><div id="41723263" class="c"><input type="checkbox" id="c-41723263" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#41722902">parent</a><span>|</span><a href="#41723192">prev</a><span>|</span><a href="#41723757">next</a><span>|</span><label class="collapse" for="c-41723263">[-]</label><label class="expand" for="c-41723263">[2 more]</label></div><br/><div class="children"><div class="content">Docker became part of the standard toolkit for ML because deploying Python that links to underlying system libraries is a gong show unless you ship that layer too.</div><br/><div id="41725067" class="c"><input type="checkbox" id="c-41725067" checked=""/><div class="controls bullet"><span class="by">tannhaeuser</span><span>|</span><a href="#41722902">root</a><span>|</span><a href="#41723263">parent</a><span>|</span><a href="#41723757">next</a><span>|</span><label class="collapse" for="c-41725067">[-]</label><label class="expand" for="c-41725067">[1 more]</label></div><br/><div class="children"><div class="content">Even Docker doesn&#x27;t guarantee reproducible results due to sensitivity towards host GPU drivers, and ML frontends&#x2F;integrations bringing their own &quot;helpful&quot; newby-friendly all-in-one dependency checks and updater services.</div><br/></div></div></div></div><div id="41723757" class="c"><input type="checkbox" id="c-41723757" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41722902">parent</a><span>|</span><a href="#41723263">prev</a><span>|</span><a href="#41724782">next</a><span>|</span><label class="collapse" for="c-41723757">[-]</label><label class="expand" for="c-41723757">[1 more]</label></div><br/><div class="children"><div class="content">you can mount a specific device to docker. If you read the script, we are mounting GPUs<p><a href="https:&#x2F;&#x2F;github.com&#x2F;slashml&#x2F;amd_inference&#x2F;blob&#x2F;main&#x2F;run-docker-amd.sh">https:&#x2F;&#x2F;github.com&#x2F;slashml&#x2F;amd_inference&#x2F;blob&#x2F;main&#x2F;run-docke...</a></div><br/></div></div><div id="41724782" class="c"><input type="checkbox" id="c-41724782" checked=""/><div class="controls bullet"><span class="by">steeve</span><span>|</span><a href="#41722902">parent</a><span>|</span><a href="#41723757">prev</a><span>|</span><a href="#41723287">next</a><span>|</span><label class="collapse" for="c-41724782">[-]</label><label class="expand" for="c-41724782">[3 more]</label></div><br/><div class="children"><div class="content">Hi, we (ZML), fix that: <a href="https:&#x2F;&#x2F;github.com&#x2F;zml&#x2F;zml">https:&#x2F;&#x2F;github.com&#x2F;zml&#x2F;zml</a></div><br/><div id="41724819" class="c"><input type="checkbox" id="c-41724819" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41722902">root</a><span>|</span><a href="#41724782">parent</a><span>|</span><a href="#41723287">next</a><span>|</span><label class="collapse" for="c-41724819">[-]</label><label class="expand" for="c-41724819">[2 more]</label></div><br/><div class="children"><div class="content">This is pretty cool. Is there a document that shows which AMD drivers are supported out of the box?</div><br/><div id="41725187" class="c"><input type="checkbox" id="c-41725187" checked=""/><div class="controls bullet"><span class="by">steeve</span><span>|</span><a href="#41722902">root</a><span>|</span><a href="#41724819">parent</a><span>|</span><a href="#41723287">next</a><span>|</span><label class="collapse" for="c-41725187">[-]</label><label class="expand" for="c-41725187">[1 more]</label></div><br/><div class="children"><div class="content">We are in line with ROCm 6.2 support. We actually just opened a PR to bump to 6.2.2: <a href="https:&#x2F;&#x2F;github.com&#x2F;zml&#x2F;zml&#x2F;pull&#x2F;39">https:&#x2F;&#x2F;github.com&#x2F;zml&#x2F;zml&#x2F;pull&#x2F;39</a></div><br/></div></div></div></div></div></div><div id="41723287" class="c"><input type="checkbox" id="c-41723287" checked=""/><div class="controls bullet"><span class="by">jeffhuys</span><span>|</span><a href="#41722902">parent</a><span>|</span><a href="#41724782">prev</a><span>|</span><a href="#41725753">next</a><span>|</span><label class="collapse" for="c-41723287">[-]</label><label class="expand" for="c-41723287">[1 more]</label></div><br/><div class="children"><div class="content">Why doesnât it make sense? You can talk to devices from a Docker container - you just have to attach it.</div><br/></div></div><div id="41725753" class="c"><input type="checkbox" id="c-41725753" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#41722902">parent</a><span>|</span><a href="#41723287">prev</a><span>|</span><a href="#41723340">next</a><span>|</span><label class="collapse" for="c-41725753">[-]</label><label class="expand" for="c-41725753">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, they&#x27;re using docker to wrap up the software packages, which is what Docker is used for. I don&#x27;t understand why that confuses you or what you think Docker is otherwise used for.</div><br/></div></div></div></div><div id="41723340" class="c"><input type="checkbox" id="c-41723340" checked=""/><div class="controls bullet"><span class="by">sandGorgon</span><span>|</span><a href="#41722902">prev</a><span>|</span><a href="#41723929">next</a><span>|</span><label class="collapse" for="c-41723340">[-]</label><label class="expand" for="c-41723340">[1 more]</label></div><br/><div class="children"><div class="content">is anyone using the new HX370 based laptops for any LLM work ? i mean the ipex-llm  libraries of Intel&#x27;s new Lunar Lake is already supporting Llama 3.2 (<a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;articles&#x2F;technical&#x2F;intel-ai-solutions-support-the-new-llama-3-2-model.html" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;articles&#x2F;t...</a>), but AMD&#x27;s new Zen5 chips dont seem to be much active here.</div><br/></div></div><div id="41723929" class="c"><input type="checkbox" id="c-41723929" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#41723340">prev</a><span>|</span><a href="#41725747">next</a><span>|</span><label class="collapse" for="c-41723929">[-]</label><label class="expand" for="c-41723929">[5 more]</label></div><br/><div class="children"><div class="content">Does it work with an APU? I just put 64GB in my system and gonna drop in a 5700G. Will that be enough? SFF inference if so.</div><br/><div id="41728263" class="c"><input type="checkbox" id="c-41728263" checked=""/><div class="controls bullet"><span class="by">Chordless</span><span>|</span><a href="#41723929">parent</a><span>|</span><a href="#41724303">next</a><span>|</span><label class="collapse" for="c-41728263">[-]</label><label class="expand" for="c-41728263">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m able to run Ollama and llama.cpp on my Ryzen 4600G APU following this guide:
<a href="https:&#x2F;&#x2F;agieverywhere.com&#x2F;apuguide&#x2F;AMDAPU&#x2F;APU_Linux" rel="nofollow">https:&#x2F;&#x2F;agieverywhere.com&#x2F;apuguide&#x2F;AMDAPU&#x2F;APU_Linux</a><p>Your APU should be similar, just faster.<p>There are some magic environment variables you want to set to get ROCM to work with this technically unsupported APU:
HSA_OVERRIDE_GFX_VERSION=9.0.0
HSA_ENABLE_SDMA=0<p>Performance is not great, but <i>slightly</i> better than running inference on the CPU, with the bonus that your CPU is essentially free for other tasks even while running LLMs.</div><br/></div></div><div id="41724303" class="c"><input type="checkbox" id="c-41724303" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#41723929">parent</a><span>|</span><a href="#41728263">prev</a><span>|</span><a href="#41724850">next</a><span>|</span><label class="collapse" for="c-41724303">[-]</label><label class="expand" for="c-41724303">[2 more]</label></div><br/><div class="children"><div class="content">The integrated GPU of the 5700G uses old architecture from 2017, this one: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Radeon_RX_Vega_series" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Radeon_RX_Vega_series</a> Pretty sure it does not support ROCm.<p>BTW if you just want to play with a local LLM, you can try my old port of Mistral: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;tree&#x2F;master&#x2F;Mistral&#x2F;MistralChat">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;tree&#x2F;master&#x2F;Mistral&#x2F;Mistral...</a> Unlike CUDA or ROCm my port is based on Direct3D 11 GPU API, runs on all GPUs regardless of the brand.</div><br/><div id="41724866" class="c"><input type="checkbox" id="c-41724866" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41723929">root</a><span>|</span><a href="#41724303">parent</a><span>|</span><a href="#41724850">next</a><span>|</span><label class="collapse" for="c-41724866">[-]</label><label class="expand" for="c-41724866">[1 more]</label></div><br/><div class="children"><div class="content">@Const-me according to this it should work, <a href="https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;ROCm&#x2F;issues&#x2F;2216">https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;ROCm&#x2F;issues&#x2F;2216</a></div><br/></div></div></div></div><div id="41724850" class="c"><input type="checkbox" id="c-41724850" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41723929">parent</a><span>|</span><a href="#41724303">prev</a><span>|</span><a href="#41725747">next</a><span>|</span><label class="collapse" for="c-41724850">[-]</label><label class="expand" for="c-41724850">[1 more]</label></div><br/><div class="children"><div class="content">haven&#x27;t tested it, but it should according to 
<a href="https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;ROCm&#x2F;issues&#x2F;2216">https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;ROCm&#x2F;issues&#x2F;2216</a><p>You just need to update the version check here<p><a href="https:&#x2F;&#x2F;github.com&#x2F;slashml&#x2F;amd_inference&#x2F;blob&#x2F;4b9ec069c4b2accc764eaa677eb43e2d1d5ff1f3&#x2F;src&#x2F;amd_setup.py#L11">https:&#x2F;&#x2F;github.com&#x2F;slashml&#x2F;amd_inference&#x2F;blob&#x2F;4b9ec069c4b2ac...</a><p>feel free to open an issue, with the requirements and we will test it.</div><br/></div></div></div></div><div id="41726680" class="c"><input type="checkbox" id="c-41726680" checked=""/><div class="controls bullet"><span class="by">kn100</span><span>|</span><a href="#41725747">prev</a><span>|</span><a href="#41723513">next</a><span>|</span><label class="collapse" for="c-41726680">[-]</label><label class="expand" for="c-41726680">[1 more]</label></div><br/><div class="children"><div class="content">Sad that RDNA2 cards aren&#x27;t supported. Not even that old!</div><br/></div></div><div id="41723513" class="c"><input type="checkbox" id="c-41723513" checked=""/><div class="controls bullet"><span class="by">stefan_</span><span>|</span><a href="#41726680">prev</a><span>|</span><a href="#41722855">next</a><span>|</span><label class="collapse" for="c-41723513">[-]</label><label class="expand" for="c-41723513">[2 more]</label></div><br/><div class="children"><div class="content">This seems to be some AI generated wrapper around a wrapper of a wrapper.<p>&gt;     # Other AMD-specific optimizations can be added here<p>&gt;     # For example, you might want to set specific flags or use AMD-optimized libraries<p>What are we doing here, then?</div><br/><div id="41723930" class="c"><input type="checkbox" id="c-41723930" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41723513">parent</a><span>|</span><a href="#41722855">next</a><span>|</span><label class="collapse" for="c-41723930">[-]</label><label class="expand" for="c-41723930">[1 more]</label></div><br/><div class="children"><div class="content">its just a big requirements file, and a dockerfile :) the rest are mostly helper scripts.</div><br/></div></div></div></div><div id="41722855" class="c"><input type="checkbox" id="c-41722855" checked=""/><div class="controls bullet"><span class="by">ashirviskas</span><span>|</span><a href="#41723513">prev</a><span>|</span><a href="#41723424">next</a><span>|</span><label class="collapse" for="c-41722855">[-]</label><label class="expand" for="c-41722855">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m all for having more open source projects, but I do not see how it can be useful in this ecosystem, especially for people with newer AMD GPUs (not supported in this project) which are already supported in most popular projects?</div><br/><div id="41723609" class="c"><input type="checkbox" id="c-41723609" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41722855">parent</a><span>|</span><a href="#41723424">next</a><span>|</span><label class="collapse" for="c-41723609">[-]</label><label class="expand" for="c-41723609">[1 more]</label></div><br/><div class="children"><div class="content">Just something that, we found helpful, support for new architectures is just a package update. This is more of a cookie cutter</div><br/></div></div></div></div><div id="41723424" class="c"><input type="checkbox" id="c-41723424" checked=""/><div class="controls bullet"><span class="by">white_waluigi</span><span>|</span><a href="#41722855">prev</a><span>|</span><a href="#41723376">next</a><span>|</span><label class="collapse" for="c-41723424">[-]</label><label class="expand" for="c-41723424">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this just a wrapper for huggingface-transformers?</div><br/><div id="41723598" class="c"><input type="checkbox" id="c-41723598" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41723424">parent</a><span>|</span><a href="#41723376">next</a><span>|</span><label class="collapse" for="c-41723598">[-]</label><label class="expand" for="c-41723598">[1 more]</label></div><br/><div class="children"><div class="content">yes, but handles all the dependencies for AMD architecture. So technically its just a requirements file :). Author of the repo above.</div><br/></div></div></div></div><div id="41723376" class="c"><input type="checkbox" id="c-41723376" checked=""/><div class="controls bullet"><span class="by">dhruvdh</span><span>|</span><a href="#41723424">prev</a><span>|</span><a href="#41724542">next</a><span>|</span><label class="collapse" for="c-41723376">[-]</label><label class="expand" for="c-41723376">[4 more]</label></div><br/><div class="children"><div class="content">Why would you use this over vLLM?</div><br/><div id="41723780" class="c"><input type="checkbox" id="c-41723780" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41723376">parent</a><span>|</span><a href="#41724542">next</a><span>|</span><label class="collapse" for="c-41723780">[-]</label><label class="expand" for="c-41723780">[3 more]</label></div><br/><div class="children"><div class="content">we have vllm in certin production instances, it is a pain for most non-nvidia related architectures. A bit of digging around and we realized that most of it is just a wrapper on top of pytorch function calls. If we can do away with batch processing with vllm supports, we can be good, this is what we did here.</div><br/><div id="41724901" class="c"><input type="checkbox" id="c-41724901" checked=""/><div class="controls bullet"><span class="by">dhruvdh</span><span>|</span><a href="#41723376">root</a><span>|</span><a href="#41723780">parent</a><span>|</span><a href="#41724542">next</a><span>|</span><label class="collapse" for="c-41724901">[-]</label><label class="expand" for="c-41724901">[2 more]</label></div><br/><div class="children"><div class="content">Batching is how you get ~350 tokens&#x2F;sec on Qwen 14b on vLLM (7900XTX). By running 15 requests at once.<p>Also, there is a Dockerfile.rocm at the root of vLLM&#x27;s repo. How is it a pain?</div><br/><div id="41725306" class="c"><input type="checkbox" id="c-41725306" checked=""/><div class="controls bullet"><span class="by">fazkan</span><span>|</span><a href="#41723376">root</a><span>|</span><a href="#41724901">parent</a><span>|</span><a href="#41724542">next</a><span>|</span><label class="collapse" for="c-41725306">[-]</label><label class="expand" for="c-41725306">[1 more]</label></div><br/><div class="children"><div class="content">driver mismatch issues, we mostly use publicly available instances, so the drivers change as the instances change, according to their base image. Not saying it won&#x27;t work, but it was more painful to figure out vllm, than to write a simple inference script and do it ourselves.</div><br/></div></div></div></div></div></div></div></div><div id="41724542" class="c"><input type="checkbox" id="c-41724542" checked=""/><div class="controls bullet"><span class="by">BaculumMeumEst</span><span>|</span><a href="#41723376">prev</a><span>|</span><a href="#41726775">next</a><span>|</span><label class="collapse" for="c-41724542">[-]</label><label class="expand" for="c-41724542">[3 more]</label></div><br/><div class="children"><div class="content">how about they follow up 7900 XTX with a card that actually has some VRAM</div><br/><div id="41725349" class="c"><input type="checkbox" id="c-41725349" checked=""/><div class="controls bullet"><span class="by">skirmish</span><span>|</span><a href="#41724542">parent</a><span>|</span><a href="#41726775">next</a><span>|</span><label class="collapse" for="c-41725349">[-]</label><label class="expand" for="c-41725349">[2 more]</label></div><br/><div class="children"><div class="content">They prefer you pay $3,600 for AMD Radeon Pro W7900, 48GB VRAM.</div><br/><div id="41726585" class="c"><input type="checkbox" id="c-41726585" checked=""/><div class="controls bullet"><span class="by">anthonix1</span><span>|</span><a href="#41724542">root</a><span>|</span><a href="#41725349">parent</a><span>|</span><a href="#41726775">next</a><span>|</span><label class="collapse" for="c-41726585">[-]</label><label class="expand" for="c-41726585">[1 more]</label></div><br/><div class="children"><div class="content">... which also has a much lower power cap</div><br/></div></div></div></div></div></div><div id="41726775" class="c"><input type="checkbox" id="c-41726775" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#41724542">prev</a><span>|</span><a href="#41723680">next</a><span>|</span><label class="collapse" for="c-41726775">[-]</label><label class="expand" for="c-41726775">[1 more]</label></div><br/><div class="children"><div class="content">Does it work with GGUF files?</div><br/></div></div><div id="41723188" class="c"><input type="checkbox" id="c-41723188" checked=""/><div class="controls bullet"><span class="by">khurdula</span><span>|</span><a href="#41723680">prev</a><span>|</span><label class="collapse" for="c-41723188">[-]</label><label class="expand" for="c-41723188">[2 more]</label></div><br/><div class="children"><div class="content">Are we supposed to use AMD GPUs for this to work? Or Does it work on any GPU?</div><br/><div id="41723307" class="c"><input type="checkbox" id="c-41723307" checked=""/><div class="controls bullet"><span class="by">karamanolev</span><span>|</span><a href="#41723188">parent</a><span>|</span><label class="collapse" for="c-41723307">[-]</label><label class="expand" for="c-41723307">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This project provides a Docker-based inference engine for running Large Language Models (LLMs) on AMD GPUs.<p>First sentence of the README in the repo. Was it somehow unclear?</div><br/></div></div></div></div></div></div></div></div></div></body></html>