<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685523667010" as="style"/><link rel="stylesheet" href="styles.css?v=1685523667010"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking">A Mechanistic Interpretability Analysis of Grokking</a>Â <span class="domain">(<a href="https://www.alignmentforum.org">www.alignmentforum.org</a>)</span></div><div class="subtext"><span>og_kalu</span> | <span>39 comments</span></div><br/><div><div id="36134022" class="c"><input type="checkbox" id="c-36134022" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#36133650">next</a><span>|</span><label class="collapse" for="c-36134022">[-]</label><label class="expand" for="c-36134022">[1 more]</label></div><br/><div class="children"><div class="content">The algorithm learned here actually makes a lot of sense when you spend more time understanding how transformers typically work.<p>Namely: once you include layer normalization your model is more or less forced to find ways to represent absolute quantities in a way that won&#x27;t be normalized away and a great way to achieve this is to... store things as rotations of a unit tensor! With that as your primitive, it&#x27;s fairly natural to rotate around a circle to compute modular addition.<p>I&#x27;d be curious to explore if a different algorithm is learned if one were to stop normalizing at various points. I wouldn&#x27;t be surprised if a large hurdle to mechanistic interpretability turns out to be that the models have learned complicated rotations in non-obvious coordinate spaces that are tricky to identify after the fact.</div><br/></div></div><div id="36133650" class="c"><input type="checkbox" id="c-36133650" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36134022">prev</a><span>|</span><a href="#36135653">next</a><span>|</span><label class="collapse" for="c-36133650">[-]</label><label class="expand" for="c-36133650">[15 more]</label></div><br/><div class="children"><div class="content">Why has the term grokking been formalized like this? Honest question. Why can&#x27;t we stick with normal words like &quot;learning&quot; or &quot;understanding&quot;? It just seems like a slow creep of needless semantic complexity in an already esoteric and niche space.</div><br/><div id="36133781" class="c"><input type="checkbox" id="c-36133781" checked=""/><div class="controls bullet"><span class="by">ska</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36133841">next</a><span>|</span><label class="collapse" for="c-36133781">[-]</label><label class="expand" for="c-36133781">[4 more]</label></div><br/><div class="children"><div class="content">The term &quot;grokking&quot; has been used for something like &quot;deep understanding&quot; for many decades now, after it&#x27;s introduction in an early 1960&#x27;s novel.  Perhaps not falling into completely general use, but popular within a large swath of technical communities, certainly.<p>I&#x27;m not sure it&#x27;s particularly well targeted here, but they are going for something deeper than &quot;learning&quot; which already has a standard usage in the field.</div><br/><div id="36134004" class="c"><input type="checkbox" id="c-36134004" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#36133650">root</a><span>|</span><a href="#36133781">parent</a><span>|</span><a href="#36133841">next</a><span>|</span><label class="collapse" for="c-36134004">[-]</label><label class="expand" for="c-36134004">[3 more]</label></div><br/><div class="children"><div class="content">But this paper isn&#x27;t using &quot;Grok&quot; in that standard slang fashion but in a very specific fashion, which I think the gp can reasonably complain about.</div><br/><div id="36134329" class="c"><input type="checkbox" id="c-36134329" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#36133650">root</a><span>|</span><a href="#36134004">parent</a><span>|</span><a href="#36134521">next</a><span>|</span><label class="collapse" for="c-36134329">[-]</label><label class="expand" for="c-36134329">[1 more]</label></div><br/><div class="children"><div class="content">How would using common words suggested by ggp avoid this issue? It&#x27;s a usual occurrence for terms of art to be repurposed words having somewhat related usages elsewhere. It all sounds like an empty complaint.</div><br/></div></div><div id="36134521" class="c"><input type="checkbox" id="c-36134521" checked=""/><div class="controls bullet"><span class="by">boyka</span><span>|</span><a href="#36133650">root</a><span>|</span><a href="#36134004">parent</a><span>|</span><a href="#36134329">prev</a><span>|</span><a href="#36133841">next</a><span>|</span><label class="collapse" for="c-36134521">[-]</label><label class="expand" for="c-36134521">[1 more]</label></div><br/><div class="children"><div class="content">Grokking in ML has been around for quite a while. First heard the term being used at a conference in 2017.</div><br/></div></div></div></div></div></div><div id="36133841" class="c"><input type="checkbox" id="c-36133841" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36133781">prev</a><span>|</span><a href="#36133717">next</a><span>|</span><label class="collapse" for="c-36133841">[-]</label><label class="expand" for="c-36133841">[1 more]</label></div><br/><div class="children"><div class="content">The term classifies a different type of characteristic of the learning trajectory, so just using &#x27;learning&#x27; would be missing that distinction. There needs to be a separate term for &#x27;long stretch of epochs for which there is a large, almost perfect, separation between train and test accuracy&#x27; ... and I would hate to write that over and over again, so there&#x27;s a single term for it.
In some respects, it&#x27;s maybe problematic for causing a namespace conflict for grokking in this sense, vs grokking in the more &#x27;normal, human&#x27; sense.</div><br/></div></div><div id="36133717" class="c"><input type="checkbox" id="c-36133717" checked=""/><div class="controls bullet"><span class="by">urthor</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36133841">prev</a><span>|</span><a href="#36133736">next</a><span>|</span><label class="collapse" for="c-36133717">[-]</label><label class="expand" for="c-36133717">[2 more]</label></div><br/><div class="children"><div class="content">Because if you&#x27;ve spent 80 hours a week researching, you want some fun in the 85th hour of your work week spent writing the research up.</div><br/><div id="36135821" class="c"><input type="checkbox" id="c-36135821" checked=""/><div class="controls bullet"><span class="by">twic</span><span>|</span><a href="#36133650">root</a><span>|</span><a href="#36133717">parent</a><span>|</span><a href="#36133736">next</a><span>|</span><label class="collapse" for="c-36135821">[-]</label><label class="expand" for="c-36135821">[1 more]</label></div><br/><div class="children"><div class="content">A stronger hypothesis: the main goal of most AI researchers is to make other AI researchers think they are cool, and using a word from classic science fiction helps do that.</div><br/></div></div></div></div><div id="36133736" class="c"><input type="checkbox" id="c-36133736" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36133717">prev</a><span>|</span><a href="#36135760">next</a><span>|</span><label class="collapse" for="c-36133736">[-]</label><label class="expand" for="c-36133736">[1 more]</label></div><br/><div class="children"><div class="content">Grokking comes from Heinlein, and it is more than a synonym for both of those words.<p>Good intro here: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Grok" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Grok</a></div><br/></div></div><div id="36135760" class="c"><input type="checkbox" id="c-36135760" checked=""/><div class="controls bullet"><span class="by">avgcorrection</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36133736">prev</a><span>|</span><a href="#36133698">next</a><span>|</span><label class="collapse" for="c-36135760">[-]</label><label class="expand" for="c-36135760">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, this gratuitous use of fancy words induces cognitive load for me.</div><br/></div></div><div id="36133698" class="c"><input type="checkbox" id="c-36133698" checked=""/><div class="controls bullet"><span class="by">dkarl</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36135760">prev</a><span>|</span><a href="#36134795">next</a><span>|</span><label class="collapse" for="c-36133698">[-]</label><label class="expand" for="c-36133698">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m going to guess that in AI those terms have been so cheapened by now that they are useless.</div><br/></div></div><div id="36134795" class="c"><input type="checkbox" id="c-36134795" checked=""/><div class="controls bullet"><span class="by">mellosouls</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36133698">prev</a><span>|</span><a href="#36134796">next</a><span>|</span><label class="collapse" for="c-36134795">[-]</label><label class="expand" for="c-36134795">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also problematic because it implies a <i>subjective</i> &quot;feeling&quot; that we associate with sentience (a property of AGI) - not AI systems.</div><br/></div></div><div id="36134796" class="c"><input type="checkbox" id="c-36134796" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36134795">prev</a><span>|</span><a href="#36134414">next</a><span>|</span><label class="collapse" for="c-36134796">[-]</label><label class="expand" for="c-36134796">[1 more]</label></div><br/><div class="children"><div class="content">because, in highly technical research area, there are zillons of somewhat similar effects. Some are interesting, some are not.<p>They need a word for each interesting effect. Sometimes they make up new words, sometime they reuse a rarely used word</div><br/></div></div><div id="36134414" class="c"><input type="checkbox" id="c-36134414" checked=""/><div class="controls bullet"><span class="by">breck</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36134796">prev</a><span>|</span><a href="#36133863">next</a><span>|</span><label class="collapse" for="c-36134414">[-]</label><label class="expand" for="c-36134414">[1 more]</label></div><br/><div class="children"><div class="content">Calling these spikes &quot;epiphanies&quot; might be a better term. I think grok is nice b&#x2F;c it&#x27;s only 4 letters.</div><br/></div></div><div id="36133863" class="c"><input type="checkbox" id="c-36133863" checked=""/><div class="controls bullet"><span class="by">billfruit</span><span>|</span><a href="#36133650">parent</a><span>|</span><a href="#36134414">prev</a><span>|</span><a href="#36135653">next</a><span>|</span><label class="collapse" for="c-36133863">[-]</label><label class="expand" for="c-36133863">[1 more]</label></div><br/><div class="children"><div class="content">Or &#x27;comprehension&#x27;.</div><br/></div></div></div></div><div id="36135653" class="c"><input type="checkbox" id="c-36135653" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36133650">prev</a><span>|</span><a href="#36133012">next</a><span>|</span><label class="collapse" for="c-36135653">[-]</label><label class="expand" for="c-36135653">[5 more]</label></div><br/><div class="children"><div class="content">Repurposing language to apply it as analogous labels is bad.<p>As a term of art it really has specific meaning.<p>Casual reading outside the cognoscenti will misunderstand it to have literal value as &quot;hallucinating&quot; and &quot;learning&quot; already are.<p>Don&#x27;t do this. Grokking does not imply intelligence or even understanding. There is no necessary introspection or inductive quality.</div><br/><div id="36135742" class="c"><input type="checkbox" id="c-36135742" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#36135653">parent</a><span>|</span><a href="#36133012">next</a><span>|</span><label class="collapse" for="c-36135742">[-]</label><label class="expand" for="c-36135742">[4 more]</label></div><br/><div class="children"><div class="content">Sure ok, there&#x27;s a bit of namespace conflict now, happens quite a lot.  I also don&#x27;t love it, but it&#x27;s there now, and it&#x27;s really not <i>the worst</i> name.
It&#x27;s a good area though, and the paper seems interesting.
Do you have any <i>real</i> problems with it?</div><br/><div id="36135779" class="c"><input type="checkbox" id="c-36135779" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36135653">root</a><span>|</span><a href="#36135742">parent</a><span>|</span><a href="#36133012">next</a><span>|</span><label class="collapse" for="c-36135779">[-]</label><label class="expand" for="c-36135779">[3 more]</label></div><br/><div class="children"><div class="content">My only complaint is that it adds to the set of core concept-labels which imply things about AI which aren&#x27;t correct.</div><br/><div id="36135973" class="c"><input type="checkbox" id="c-36135973" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#36135653">root</a><span>|</span><a href="#36135779">parent</a><span>|</span><a href="#36133012">next</a><span>|</span><label class="collapse" for="c-36135973">[-]</label><label class="expand" for="c-36135973">[2 more]</label></div><br/><div class="children"><div class="content">Probably most words for which the origin is known started out as a metaphor.<p>Examples:<p>hallucinate - mid 17th century (in the sense âbe deceived, have illusionsâ): from Latin hallucinat- âgone astray in thoughtâ, from the verb hallucinari, from Greek alussein âbe uneasy or distraughtâ.<p>induction - late Middle English: from Latin inductio(n- ), from the verb inducere âlead intoâ<p>Wikipedia:<p>&gt; Metaphor: Change based on similarity between concepts, e.g., mouse &quot;rodent&quot; â &quot;computer device&quot;.<p>&gt; In historical onomasiology or in historical linguistics, a metaphor is defined as a semantic change based on a similarity in form or function between the original concept and the target concept named by a word.<p>&gt; In cognitive linguistics, conceptual metaphor, or cognitive metaphor, refers to the understanding of one idea, or conceptual domain, in terms of another. An example of this is the understanding of quantity in terms of directionality (e.g. &quot;the price of peace is rising&quot;) or the understanding of time in terms of money (e.g. &quot;I spent time at work today&quot;).</div><br/><div id="36136098" class="c"><input type="checkbox" id="c-36136098" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36135653">root</a><span>|</span><a href="#36135973">parent</a><span>|</span><a href="#36133012">next</a><span>|</span><label class="collapse" for="c-36136098">[-]</label><label class="expand" for="c-36136098">[1 more]</label></div><br/><div class="children"><div class="content">I get this, but I worry that the metaphorical quality of hallucinate implies belief in consciousness. It may have surface qualities analogous to what we call hallucination in conscious beings, but to backwards intuit because we decide this AI is &quot;hallucinating&quot; implies it&#x27;s alive worries me.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36133012" class="c"><input type="checkbox" id="c-36133012" checked=""/><div class="controls bullet"><span class="by">wwarner</span><span>|</span><a href="#36135653">prev</a><span>|</span><a href="#36133821">next</a><span>|</span><label class="collapse" for="c-36133012">[-]</label><label class="expand" for="c-36133012">[1 more]</label></div><br/><div class="children"><div class="content">I really believe phase changes in models are going to tell us a lot about what theyâve learned. Though it has to be said that seeing that sharp improvement in a model during training is the exception rather than the rule â normally you just see gradual improvement until youâve reached sufficient accuracy and then decide to stop training. So itâs not clear to me that every highly accurate model will contain a phase change.</div><br/></div></div><div id="36133821" class="c"><input type="checkbox" id="c-36133821" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#36133012">prev</a><span>|</span><a href="#36133337">next</a><span>|</span><label class="collapse" for="c-36133821">[-]</label><label class="expand" for="c-36133821">[1 more]</label></div><br/><div class="children"><div class="content">I have been looking into this as well.<p>I&#x27;m strongly convinced that Chu duality or similar (i.e. the duality between states and transition between states is at play). At first, the model learns only the states but over time it learns the transformations.<p>Ironically, this is a similar process to QTF renormalization.</div><br/></div></div><div id="36133337" class="c"><input type="checkbox" id="c-36133337" checked=""/><div class="controls bullet"><span class="by">siddiqi123</span><span>|</span><a href="#36133821">prev</a><span>|</span><a href="#36133124">next</a><span>|</span><label class="collapse" for="c-36133337">[-]</label><label class="expand" for="c-36133337">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t talk about human analogs of grokking, and that makes sense for a technical paper like this. Nonetheless, grokking also seems to happen in humans, and everybody has had &quot;Aha!&quot; moments before. Can you maybe comment a bit on the relation to human learning? It seems clear that human grokking is not a process that purely depends on the number of training samples seen but also on the availability of hypotheses. People grok faster if you provide them with symbolic descriptions of what goes on. What are your thoughts on the representation and transfer of the resulting structure, e.g., via language&#x2F;token streams?</div><br/></div></div><div id="36133124" class="c"><input type="checkbox" id="c-36133124" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#36133337">prev</a><span>|</span><a href="#36133979">next</a><span>|</span><label class="collapse" for="c-36133124">[-]</label><label class="expand" for="c-36133124">[8 more]</label></div><br/><div class="children"><div class="content">Not really a characterization of this work (I like mechanistic interpretability even though I consider it a detour in the long run), but of the submission title (edit: it was &quot;Tiny Transformer trained for addition learns bizarre addition algorithm&quot; at the point of this comment being posted). It&#x27;s bad. I suspect it&#x27;s informed by Rob Miles&#x27; characterization of this algorithm as &quot;bizarre and inhuman&quot;[1]<p>AI safety&#x2F;alignment discourse as a whole is so incredibly bad. It&#x27;s autodidactic and weird in the bad sense, weird like people who think they are brave iconoclasts but just haven&#x27;t the curiosity and humility to learn the basics of the field are weird. Add some money from Dustin Moskovitz to Open Philanthropy to Robert Miles&#x27; propaganda firehose on youtube, add policy lobbying and buying bankrupt crypto podcasters, and you have the worst that could come from the culture of public intellectualism. The notions they have inserted into this potentially vital field, like inner&#x2F;outer misalignment and &quot;mesaoptimization&quot; [2] (where simple overfitting is the real and more productive explanation) are deeply misleading, technically illiterate and stoking public fears in directions we should care about the least, siphoning resources and attention from real issues informed by specifics of contemporary ML (chiefly data and curriculum engineering). They&#x27;re close to putting a lid on LLM development â because they&#x27;re getting spooked by some vague sense of growing &quot;capabilities&quot;; even though LLMs are evidently a vastly better approach to safe-yet-capable-general-AI than anything any of their thought leaders have ever come up with in decades of SIAI&#x2F;MIRI&#x2F;Lesswrong procrastination. (For one example of a decent takedown of their approach I recommend [3]).<p>Why is this algorithm bizarre, or even inhuman? Could anyone show me the specific functional connectivity graph and activation functions in my brain that I use when doing addition, with the circuit going through spatial-quantitative cortex and phonological loop, populations querying cached results in long-term memory, individual numbers marked as interesting or banal by the highest-level associative units? Would it look anything like a neat and sensible algorithm we&#x27;d come up with using a global top-down representation of the solution target space?<p>By this standard, humans are inhuman, humans are shoggoths, as they should be, because emergent data-driven structures in general-purpose substrate generally do not resolve to anything like provable optimality, even if they get very close in performance. This framing adds nothing but clicks and insecurity.<p>1. <a href="https:&#x2F;&#x2F;twitter.com&#x2F;robertskmiles&#x2F;status&#x2F;1663534255249453056" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;robertskmiles&#x2F;status&#x2F;1663534255249453056</a><p>2. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=zkbPdEHEyEI">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=zkbPdEHEyEI</a><p>3. <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;wAczufCpMdaamF9fy&#x2F;my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#The_difficulty_of_alignment" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;wAczufCpMdaamF9fy&#x2F;my-objecti...</a></div><br/><div id="36133252" class="c"><input type="checkbox" id="c-36133252" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36133124">parent</a><span>|</span><a href="#36133181">next</a><span>|</span><label class="collapse" for="c-36133252">[-]</label><label class="expand" for="c-36133252">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think being bizarre or not adds much to the fear of alignment. Whether it is or not, the issue remains the same, i.e we are not the ones controlling the learning process. There isn&#x27;t really any solace in the other option (surprisingly human) in terms of alignment fears.<p>Let me put it this way. Unaligned intelligence is dangerous. You don&#x27;t need a Sci-fi book to realize this, just a history one.<p>Our history is replete with examples of the slightly more technologically advanced group decimating their competition.<p>Humanity is directly responsible for the extinction of hundreds of species. Not because of any particular malice or offense, simply that our goals didn&#x27;t align with the interest of any of those species.<p>Just looking at Humans alone, Unaligned intelligence is potentially catastrophic even when the advantage gulf is fairly low.<p>When the advantage gulf is large, it is potentially genocidal.<p>So personally, &quot;Human SuperIntelligence&quot; doesn&#x27;t exactly warm the heart.</div><br/><div id="36134739" class="c"><input type="checkbox" id="c-36134739" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#36133124">root</a><span>|</span><a href="#36133252">parent</a><span>|</span><a href="#36133181">next</a><span>|</span><label class="collapse" for="c-36134739">[-]</label><label class="expand" for="c-36134739">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Whether it is or not, the issue remains the same, i.e we are not the ones controlling the learning process<p>I think this is just uninspected, vague intuition. What does it mean to control the learning process? No, we control the data, the deterministic learning rule, weights and activations, every step of the way is controllable â it&#x27;s just it&#x27;s intractable to control it all. Moreover, if it were tractable, would we know what to do? How would that be different from the problem of programming an AI from scratch?<p>&gt; There isn&#x27;t really any solace in the other option (surprisingly human) in terms of alignment fears.<p>If there weren&#x27;t, I suppose major AI doomers (Yudkowsky, Leahy, Besinger etc.) wouldn&#x27;t have been putting such an emphasis in their Lovecraft-inspired rhetoric on the alienness and inscrutability of those evil matrices of floating point numbers. No, the idea that human mental architecture is safer (and that DL does not approximate it) is very much at the core of alignment fears. E.g. Besinger on why he expects AGI Ruin [1]: &quot;We&#x27;re building &quot;AI&quot; in the sense of building powerful general search processes (and search processes for search processes), not building &quot;AI&quot; in the sense of building friendly ~humans but in silicon â¦ The key differences between humans and &quot;things that are more easily approximated as random search processes than as humans-plus-a-bit-of-noise&quot; lies in lots of complicated machinery in the human brain. â¦ This doesn&#x27;t mean the problem is unsolvable; but it means that you either need to reproduce that internal machinery, in a lot of detail, in AI, or you need to build some new kind of machinery thatâs safe for reasons other than the specific reasons humans are safe.&quot;<p>&gt; Let me put it this way. Unaligned intelligence is dangerous.<p>I would ask you not to condescend but I have learned that this is an impossible request with the AI risk crowd, because you never really encounter pushback in your para-academic hothouse and so come to believe that, indeed, pretty basic intuitions are knockdown arguments only silly people could dismiss. You define intelligence in a certain way that has nothing to do with how you identify it in the wild. The definition is something like &quot;intelligence is an optimization process&quot;; the thing under consideration can be an LLM or a diffusion model that seems really good at its job. You mean to say &quot;processes shaping real-world outcomes that are not optimizing for outcomes people consider good are dangerous&quot;. This is true but trivial. The onus is on you to tie this consideration to intelligence in general or to &quot;capabilities&quot; of arbitrarily powerful ML models.<p>1. <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;eaDCgdkbsfGqpWazi&#x2F;the-basic-reasons-i-expect-agi-ruin" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;eaDCgdkbsfGqpWazi&#x2F;the-basic-...</a></div><br/></div></div></div></div><div id="36133181" class="c"><input type="checkbox" id="c-36133181" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#36133124">parent</a><span>|</span><a href="#36133252">prev</a><span>|</span><a href="#36133704">next</a><span>|</span><label class="collapse" for="c-36133181">[-]</label><label class="expand" for="c-36133181">[3 more]</label></div><br/><div class="children"><div class="content">How can the title of the submission (which is just the title of the article) be informed by Miles&#x27; description of _the same article_ (which he posted today, nearly a year after the article was posted)?</div><br/><div id="36133213" class="c"><input type="checkbox" id="c-36133213" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36133124">root</a><span>|</span><a href="#36133181">parent</a><span>|</span><a href="#36133241">next</a><span>|</span><label class="collapse" for="c-36133213">[-]</label><label class="expand" for="c-36133213">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t personally change it but the title of the post was different when i originally submitted. Didn&#x27;t call it inhuman though, just bizarre which it is...depending on your frame of reference.</div><br/></div></div><div id="36133241" class="c"><input type="checkbox" id="c-36133241" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36133124">root</a><span>|</span><a href="#36133181">parent</a><span>|</span><a href="#36133213">prev</a><span>|</span><a href="#36133704">next</a><span>|</span><label class="collapse" for="c-36133241">[-]</label><label class="expand" for="c-36133241">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Tiny Transformer trained for addition learns bizarre addition algorithm&quot; was the title.</div><br/></div></div></div></div><div id="36133704" class="c"><input type="checkbox" id="c-36133704" checked=""/><div class="controls bullet"><span class="by">jimrandomh</span><span>|</span><a href="#36133124">parent</a><span>|</span><a href="#36133181">prev</a><span>|</span><a href="#36134451">next</a><span>|</span><label class="collapse" for="c-36133704">[-]</label><label class="expand" for="c-36133704">[1 more]</label></div><br/><div class="children"><div class="content">&gt; buying bankrupt crypto podcasters<p>I haven&#x27;t heard of anything like that happening; what is this referring to? Are you claiming that a podcast was secretly sponsored rather than natural?</div><br/></div></div><div id="36134451" class="c"><input type="checkbox" id="c-36134451" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#36133124">parent</a><span>|</span><a href="#36133704">prev</a><span>|</span><a href="#36133979">next</a><span>|</span><label class="collapse" for="c-36134451">[-]</label><label class="expand" for="c-36134451">[1 more]</label></div><br/><div class="children"><div class="content">That youtube video is basically a crime against humanity. I&#x27;m being hyperbolic, but only slightly. I have never seen such blatant misrepresentation of results used to &quot;verify&quot; entirely unrelated hypotheses. Anyone with even a mild understanding of deep learning can spot this; but clearly the audience for these videos is _not_ people with deep learning experience. Rather, it is the followers of the cult of alignment.<p>At least one aspect of why this is so popular has to be that deep learning has now gotten sufficiently close to the popular conceptions of what a &quot;general AI&quot; should look like according to science fiction. As such, layman are using priors from e.g. Asimov (which deliberately simplifies things for poetic irony and &quot;violations of the laws&quot;) such that they feel like they are actually already equipped to handle these relatively advanced topics.</div><br/></div></div></div></div><div id="36133979" class="c"><input type="checkbox" id="c-36133979" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#36133124">prev</a><span>|</span><a href="#36133139">next</a><span>|</span><label class="collapse" for="c-36133979">[-]</label><label class="expand" for="c-36133979">[1 more]</label></div><br/><div class="children"><div class="content">Here, &quot;Grokking&quot; seems to be defined as a quick shift in the degree of generalization of a model. I&#x27;m not sure this is in any way an interesting phenomena. The original paper doesn&#x27;t seem to be referenced [1] and I&#x27;d speculate the interest in the &quot;alignment&quot; community.<p>[1] <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?cites=4466239674951044045&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en" rel="nofollow">https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?cites=4466239674951044045...</a></div><br/></div></div><div id="36133139" class="c"><input type="checkbox" id="c-36133139" checked=""/><div class="controls bullet"><span class="by">sukilot</span><span>|</span><a href="#36133979">prev</a><span>|</span><a href="#36133630">next</a><span>|</span><label class="collapse" for="c-36133139">[-]</label><label class="expand" for="c-36133139">[2 more]</label></div><br/><div class="children"><div class="content">Implemented addition via:<p><pre><code>      cos(wx) sin (wy) + sin(wx) cos(wy)

</code></pre>
But how did it compute <i>that</i> intermediate addition?</div><br/><div id="36133439" class="c"><input type="checkbox" id="c-36133439" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#36133139">parent</a><span>|</span><a href="#36133630">next</a><span>|</span><label class="collapse" for="c-36133439">[-]</label><label class="expand" for="c-36133439">[1 more]</label></div><br/><div class="children"><div class="content">What it learned to do was modular addition, not addition.</div><br/></div></div></div></div><div id="36133630" class="c"><input type="checkbox" id="c-36133630" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36133139">prev</a><span>|</span><label class="collapse" for="c-36133630">[-]</label><label class="expand" for="c-36133630">[3 more]</label></div><br/><div class="children"><div class="content">I think people should read at least slightly reputable ML papers instead of AI papers by crazy autodidactic AI experts who are making up all of their terms and won&#x27;t stop telling you it&#x27;s going to take over the world because of some math they did once.</div><br/><div id="36136066" class="c"><input type="checkbox" id="c-36136066" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#36133630">parent</a><span>|</span><a href="#36133947">next</a><span>|</span><label class="collapse" for="c-36136066">[-]</label><label class="expand" for="c-36136066">[1 more]</label></div><br/><div class="children"><div class="content">tell me you&#x27;re posting from an armchair without telling me you&#x27;re posting from an armchair</div><br/></div></div><div id="36133947" class="c"><input type="checkbox" id="c-36133947" checked=""/><div class="controls bullet"><span class="by">nschiefer</span><span>|</span><a href="#36133630">parent</a><span>|</span><a href="#36136066">prev</a><span>|</span><label class="collapse" for="c-36133947">[-]</label><label class="expand" for="c-36133947">[1 more]</label></div><br/><div class="children"><div class="content">Good news for readers who prefer reputable ML papers: this work was accepted to ICLR 2023! <a href="https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=9XFSbDPmdW" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=9XFSbDPmdW</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>