<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718701258927" as="style"/><link rel="stylesheet" href="styles.css?v=1718701258927"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt">Getting 50% (SoTA) on Arc-AGI with GPT-4o</a> <span class="domain">(<a href="https://redwoodresearch.substack.com">redwoodresearch.substack.com</a>)</span></div><div class="subtext"><span>tomduncalf</span> | <span>129 comments</span></div><br/><div><div id="40712282" class="c"><input type="checkbox" id="c-40712282" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#40715123">next</a><span>|</span><label class="collapse" for="c-40712282">[-]</label><label class="expand" for="c-40712282">[11 more]</label></div><br/><div class="children"><div class="content">(ARC Prize co-founder here).<p>Ryan&#x27;s work is legitimately interesting and novel &quot;LLM reasoning&quot; research! The core idea:<p>&gt; get GPT-4o to generate around 8,000 python programs which attempt to implement the transformation, select a program which is right on all the examples (usually there are 3 examples), and then submit the output this function produces when applied to the additional test input(s)<p>Roughly, he&#x27;s implemented an outer loop and using 4o to sample reasoning traces&#x2F;programs from training data and test. Hybrid DL + program synthesis approaches are solutions we&#x27;d love to see more of.<p>A couple important notes:<p>1. this result is on the public eval set vs private set (ARC Prize $).<p>2. the current private set SOTA ~35% solution also performed ~50% on the public set. so this new result <i>might</i> be SOTA but hasn&#x27;t been validated or scrutinized yet.<p>All said, I do expect verified public set results to flow down to the private set over time. We&#x27;ll be publishing all the SOTA scores and open source reproductions here once available: <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;leaderboard" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;leaderboard</a><p>EDIT: also, congrats and kudos to Ryan for achieving this and putting the effort in to document and share his approach. we hope to inspire more frontier AI research sharing like this</div><br/><div id="40715482" class="c"><input type="checkbox" id="c-40715482" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40714116">next</a><span>|</span><label class="collapse" for="c-40715482">[-]</label><label class="expand" for="c-40715482">[1 more]</label></div><br/><div class="children"><div class="content">Ah, give it a rest. That&#x27;s not &quot;frontier AI research&quot;, neither is it any kind of reasoning. It&#x27;s the dumbest of the dumb possible generate-and-test approach that spams a fire hose of Python programs until it hits one that works. And still it gets only 50% on the public eval.<p>How many thousands of Python programs does a human need to solve a single ARC task? That&#x27;s what you get with reasoning: you don&#x27;t need oodles of compute and boodles of sampling.<p>And I&#x27;m sorry to be so mean, but ARC is a farce. It&#x27;s supposed to be a test for AGI but its only defense from a big data approach (what Francois calls &quot;memorisation&quot;) is that there are few examples provided. That doesn&#x27;t make the tasks hard to solve with memorisation it just makes it hard for a human researcher to find enough examples to solve with memorisation. Like almost every other AI-IQ test before it, ARC is testing for the wrong thing, with the wrong assumptions. See the Winograd Schema Challenge (but not yet the Bongard problems).</div><br/></div></div><div id="40714116" class="c"><input type="checkbox" id="c-40714116" checked=""/><div class="controls bullet"><span class="by">refreshingdrink</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40715482">prev</a><span>|</span><a href="#40715353">next</a><span>|</span><label class="collapse" for="c-40714116">[-]</label><label class="expand" for="c-40714116">[1 more]</label></div><br/><div class="children"><div class="content">Also worth nothing that Ryan mentions<p>&gt; In addition to iterating on the training set, I also did a small amount of iteration on a 100 problem subset of the public test set<p>and<p>&gt; it&#x27;s unfortunate that these sets aren’t IID: it makes iteration harder and more confusing<p>It’s not unfortunate: generalizing beyond the training distribution is a crucial part of intelligence that ARC is trying to measure! Among other reasons, developing with test-set data is a bad practice in ML because it hides the difficulty this challenge. Even worse, writing about a bunch of tricks that help results on this subset is extending the test-set leakage the blog post&#x27;s readers. This is why I&#x27;m glad the ARC Prize has a truly hidden test set</div><br/></div></div><div id="40715353" class="c"><input type="checkbox" id="c-40715353" checked=""/><div class="controls bullet"><span class="by">jd115</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40714116">prev</a><span>|</span><a href="#40715468">next</a><span>|</span><label class="collapse" for="c-40715353">[-]</label><label class="expand" for="c-40715353">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me a bit of Genetic Programming as proposed by John Holland, John Koza, etc. Ever since GPT came out, I&#x27;ve been thinking of ways to combine that original idea with LLMs in some way that would accelerate the process with a more &quot;intelligent&quot; selection.</div><br/></div></div><div id="40715468" class="c"><input type="checkbox" id="c-40715468" checked=""/><div class="controls bullet"><span class="by">lelanthran</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40715353">prev</a><span>|</span><a href="#40712673">next</a><span>|</span><label class="collapse" for="c-40715468">[-]</label><label class="expand" for="c-40715468">[1 more]</label></div><br/><div class="children"><div class="content">Maybe I am missing something, but to me this looks like &quot;Let&#x27;s brute-force on the training data&quot;.<p>I mean, generating tens of thousands of possible solutions, to find one that works does not, to me, signify AGI.<p>After all, the human solving these problem doesn&#x27;t make 10k attempts before getting a solution, do they?<p>The approach here, due to brute force, can&#x27;t really scale: if a random solution to a <i>very simple</i> problem has a 1&#x2F;10k chance of being right, you can&#x27;t scale this up to non-trivial problems without exponentially increasing the computational power used. Hence, I feel this is brute-force.</div><br/></div></div><div id="40712673" class="c"><input type="checkbox" id="c-40712673" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40715468">prev</a><span>|</span><a href="#40714428">next</a><span>|</span><label class="collapse" for="c-40712673">[-]</label><label class="expand" for="c-40712673">[2 more]</label></div><br/><div class="children"><div class="content">Do you have any perspectives to share on Ryan&#x27;s observation of a potential scaling law for these tasks and his comment that &quot;ARC-AGI will be one benchmark among many that just gets solved by scale&quot;?</div><br/><div id="40714361" class="c"><input type="checkbox" id="c-40714361" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#40712282">root</a><span>|</span><a href="#40712673">parent</a><span>|</span><a href="#40714428">next</a><span>|</span><label class="collapse" for="c-40714361">[-]</label><label class="expand" for="c-40714361">[1 more]</label></div><br/><div class="children"><div class="content">ARC isn&#x27;t perfect and I hope ARC is not the last AGI benchmark. I&#x27;ve spoken with a few other benchmark creators looking to emulate ARC&#x27;s novelty in other domains, so I think we&#x27;ll see more. The evolution of AGI benchmarks likely needs to evolve alongside the tech -- humans have to design these tasks today to ensure novelty but should expect that to shift.<p>One core idea we&#x27;ve been advocating with ARC is that pure LLM scaling (parameters...) is insufficient to achieve AGI. Something new is needed. And OPs approach using a novel outer loop is one cool demonstration of this.</div><br/></div></div></div></div><div id="40714428" class="c"><input type="checkbox" id="c-40714428" checked=""/><div class="controls bullet"><span class="by">sriku</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40712673">prev</a><span>|</span><a href="#40714245">next</a><span>|</span><label class="collapse" for="c-40714428">[-]</label><label class="expand" for="c-40714428">[1 more]</label></div><br/><div class="children"><div class="content">Part of the challenge I understood to be learning priors from the training set that can then be applied to an extended private test set. This approach doesn&#x27;t seem to do any such &quot;learning&quot; on the go. So, supposing it accomplished 85% on the private test set, would it be construed to have won the prize with &quot;we have AGI&quot; being trumpeted out?</div><br/></div></div><div id="40714245" class="c"><input type="checkbox" id="c-40714245" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40714428">prev</a><span>|</span><a href="#40712907">next</a><span>|</span><label class="collapse" for="c-40714245">[-]</label><label class="expand" for="c-40714245">[1 more]</label></div><br/><div class="children"><div class="content">There are similarities to the approach in this paper (though they trained a model from scratch): <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.07062" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.07062</a><p>How well would an LLM trained with a huge number of examples do on this test? Essentially with enough attention, Goodhart&#x27;s law will take over.</div><br/></div></div><div id="40712907" class="c"><input type="checkbox" id="c-40712907" checked=""/><div class="controls bullet"><span class="by">Nimitz14</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40714245">prev</a><span>|</span><a href="#40713440">next</a><span>|</span><label class="collapse" for="c-40712907">[-]</label><label class="expand" for="c-40712907">[1 more]</label></div><br/><div class="children"><div class="content">Ah that&#x27;s an important detail about public v private. Makes it a nice result but nearly as impressive as initially stated.</div><br/></div></div><div id="40713440" class="c"><input type="checkbox" id="c-40713440" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40712282">parent</a><span>|</span><a href="#40712907">prev</a><span>|</span><a href="#40715123">next</a><span>|</span><label class="collapse" for="c-40713440">[-]</label><label class="expand" for="c-40713440">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of the AlphaCode approach.<p>Why do you say it&#x27;s sampling programs from &quot;training data&quot;? With that choice of words, you&#x27;re rhetorically assuming the conclusion.<p>If he only sampled 20 programs, instead of 8000, will we still say the programs came from &quot;training data&quot;, or will we say it&#x27;s genuine OOD generalization? At what point do we attribute the intelligence to the LLM itself instead of the outer loop?<p>This isn&#x27;t meant to be facetious. Because clearly, if the N programs sampled is very large, it&#x27;s easy to get the right solution with little intelligence by relying on luck. But as N gets small the LLM has to be intelligent and capable of OOD generalization, assuming the benchmark is good.</div><br/></div></div></div></div><div id="40715123" class="c"><input type="checkbox" id="c-40715123" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#40712282">prev</a><span>|</span><a href="#40714152">next</a><span>|</span><label class="collapse" for="c-40715123">[-]</label><label class="expand" for="c-40715123">[1 more]</label></div><br/><div class="children"><div class="content">The article jumps to the conclusion that &quot;Given that current LLMs can perform decently well on ARC-AGI&quot; after having used multiple hand-crafted tricks to get to these results, including &quot;I also did a small amount of iteration on a 100 problem subset of the public test set&quot; which is hidden in the middle of the article and not mentioned in the bullet list at the top.<p>Adding the close-to ad-hominem attack on Francois Chollet with the comics at the beginning (Francois never claimed to be a neuro-symbolic believer), this work does a significant disservice to the community.</div><br/></div></div><div id="40714152" class="c"><input type="checkbox" id="c-40714152" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#40715123">prev</a><span>|</span><a href="#40712326">next</a><span>|</span><label class="collapse" for="c-40714152">[-]</label><label class="expand" for="c-40714152">[16 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll say what a lot of people seem to be denying. GPT-4 is an AGI, just a very bad one. Even GPT-1 was an AGI. There isn&#x27;t a hard boundary between non AGI and AGI. A lot of people wish there was so they imagine absolutes regarding LLM&#x27;s like &quot;they cannot create anything new&quot; or something like that. Just think: we consider humans a general intelligence, but obviously wouldn&#x27;t consider an embryo or infant a general intelligence. So at what point does a human go from not generally intelligent to generally intelligent? And I don&#x27;t mean an age or brain size, I mean suite of testable abilities.<p>Intelligence is an ability that is naturally gradual and emerges over many domains. It is a collection of tools via which general abstractive principles can be applied, not a singular universally applicable ability to think in abstractions. GPT-4, compared to a human, is a very very small brain trained for the single purpose of textual thinking with some image capabilities. Claiming that ARC is the absolute market of general intelligence fails to account for the big picture of what intelligence is.</div><br/><div id="40714191" class="c"><input type="checkbox" id="c-40714191" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#40714152">parent</a><span>|</span><a href="#40714565">next</a><span>|</span><label class="collapse" for="c-40714191">[-]</label><label class="expand" for="c-40714191">[2 more]</label></div><br/><div class="children"><div class="content">This seems to be so broad a definition as to no longer mean anything useful.<p>People in general are interested in capabilities or economic impact, and GPT-2 cleared no notable thresholds in those regards.<p>I prefer the exact opposite approach: let’s use a strict definition, and have levels to make it really explicit what we are talking about.<p>Here is a good one:<p>“Levels of AGI for Operationalizing Progress on the Path to AGI”<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.02462" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.02462</a></div><br/><div id="40714948" class="c"><input type="checkbox" id="c-40714948" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40714191">parent</a><span>|</span><a href="#40714565">next</a><span>|</span><label class="collapse" for="c-40714948">[-]</label><label class="expand" for="c-40714948">[1 more]</label></div><br/><div class="children"><div class="content">People will never agree on this. We&#x27;ve known about the concept of intelligence for much, much longer than computers have been around and we still don&#x27;t have a common definition or set of rules to check. That also makes things like consciousness and death pretty hard to define in medicine, leading to inconsistent rules across jurisdictions. For AGI in particular, I guarantee you that no matter which test gets beaten, the majority of humanity will always just move the goalpost and claim it&#x27;s not &quot;real&quot; AGI because &quot;reasons.&quot; Because the opposite would mean they have to admit that they are now the lesser intelligence on the planet.</div><br/></div></div></div></div><div id="40714565" class="c"><input type="checkbox" id="c-40714565" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40714152">parent</a><span>|</span><a href="#40714191">prev</a><span>|</span><a href="#40715248">next</a><span>|</span><label class="collapse" for="c-40714565">[-]</label><label class="expand" for="c-40714565">[6 more]</label></div><br/><div class="children"><div class="content">&gt; GPT-4 is an AGI, just a very bad one.<p>Then stop selling it as a tool to replace humans. A fast moving car breaking through a barrier and flying off the cliff could be called &quot;an airborne means of transportation, just a very bad one&quot; yet nobody is suggesting it should replace school busses if only we could add longer wings to it. What the LLM community refuses to see is that there is a limit to the patience and the financing the rest of the world will grant you before you&#x27;re told, &quot;it doesn&#x27;t work mate.&quot;<p>&gt; So at what point does a human go from not generally intelligent to generally intelligent?<p>Developmental psychology would be a good place to start looking for answers to this question. Also, forgetting scientific approach and going with common sense, we do not allow young humans to operate complex machinery, decide who is allowed to become a doctor, or go to jail. Intelligence is something that is not equally distributed across the human population and some of us never have much of it, yet we function and have a role in society. Our behaviour, choices, preferences, opinions are not just based on our intelligence, but often on our past experiences and circumstances. It is also not the sole quality we use to compare ourselves against each other. A not very intelligent person is capable of making the right choices (an otherwise obedient soldier refusing to press the button and blow up a building full of children); similarly, a highly intelligent person can become a hard-to-find serial criminal (a gynecologist impregnating his patients).<p>What intelligent and creative people hold against LLMs is not that they replace them, but that they replace them with a shit version of them relegating thousands of years of human progress and creativity to the dustbin of the models and layers of tweaks to the output that still produce unreliable crap. I think the person who wrote this sign summed it up best <a href="https:&#x2F;&#x2F;x.com&#x2F;gvanrossum&#x2F;status&#x2F;1802378022361911711" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;gvanrossum&#x2F;status&#x2F;1802378022361911711</a></div><br/><div id="40714937" class="c"><input type="checkbox" id="c-40714937" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40714565">parent</a><span>|</span><a href="#40714760">next</a><span>|</span><label class="collapse" for="c-40714937">[-]</label><label class="expand" for="c-40714937">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What the LLM community refuses to see is that there is a limit to the patience and the financing the rest of the world will grant you before you&#x27;re told, &quot;it doesn&#x27;t work mate.&quot;<p>The point about LLM&#x27;s is they may have a lot of drawbacks right now but they&#x27;re improving at a rapid pace. They already are very useful. There are hundreds of stories coming out of companies effectively leveraging them to replace workers in many natural-language related tasks. They&#x27;re far more useful than a car that goes off a cliff.<p>Nobody more useful than an LLM is being effectively replaced by an LLM. Those few companies that jump the gun too early are suffering for it.<p>&gt;That sign<p>We already have dishwashers and washing machines. Companies are working on making humanoid robots that can do those things, it&#x27;s just that it&#x27;s harder to develop a fully-fledged embodied humanoid than it is to create the diffusion models and LLM&#x27;s being used today. It&#x27;s not some conspiracy to let AI do all the fun stuff first.<p>Nobody is preventing anyone from making art or writing poetry. If someone finds value in AI art or writing, either you have to accept that they weren&#x27;t the audience member you wanted, or you have to accept that your ability to be creative is a learnable algorithm same as anything else.</div><br/></div></div><div id="40714760" class="c"><input type="checkbox" id="c-40714760" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40714565">parent</a><span>|</span><a href="#40714937">prev</a><span>|</span><a href="#40715248">next</a><span>|</span><label class="collapse" for="c-40714760">[-]</label><label class="expand" for="c-40714760">[4 more]</label></div><br/><div class="children"><div class="content">In response to the sign: then learn to code or make art that is better than AI art.<p>It&#x27;s an existential complaint. &quot;Why won&#x27;t the nerds make something for meeeee.&quot; Do it yourself. Make that robot.<p>Sucks to think that you&#x27;re not that special. Most art isn&#x27;t. Most music isn&#x27;t. Any honest artist will agree. Most professional artists are graphic designers, not brilliant once in a generation visionaries. It&#x27;s the new excuse for starving artists. AI or no, they&#x27;d still be unsuccessful. That&#x27;s the way it&#x27;s always been.</div><br/><div id="40714852" class="c"><input type="checkbox" id="c-40714852" checked=""/><div class="controls bullet"><span class="by">earthnail</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40714760">parent</a><span>|</span><a href="#40714951">next</a><span>|</span><label class="collapse" for="c-40714852">[-]</label><label class="expand" for="c-40714852">[2 more]</label></div><br/><div class="children"><div class="content">While that is 100% true, the real problem is that the risk of finding out whether you can make special art has significantly increased. Previously if you didn’t make it as an artist, you could still earn money with other art related tasks - in graphics, many became illustrators. In music, people made music for ads.<p>That plan B is now going away, and a music career will be much more like a sports career: either you make it in football, or you need to find another career where your football skills won’t be very useful.<p>That is obviously scary for many.</div><br/><div id="40714927" class="c"><input type="checkbox" id="c-40714927" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40714852">parent</a><span>|</span><a href="#40714951">next</a><span>|</span><label class="collapse" for="c-40714927">[-]</label><label class="expand" for="c-40714927">[1 more]</label></div><br/><div class="children"><div class="content">Artists who make it usually have a legend, a story to tell or be told by their friends, associates, agents, publishers, gallerists, etc. That story has a human dimension that touches the rest of us and we somehow connect to it. Van Gogh cut of his ear, we still keep talking about it and wondering why? There is nothing AI can tell us about itself, its &quot;art&quot;. The artistic struggle with AI is not about expressing your vision on a canvas in a way that makes others feel what you want them to feel but about forcing it to generate something it is incapable of generating or programmed not to generate. We got to the point where we are given crayons programmed to not draw the things others do not want them to draw or to draw HR-approved version of what the artist wants to draw. The future is now and it&#x27;s shit.</div><br/></div></div></div></div><div id="40714951" class="c"><input type="checkbox" id="c-40714951" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40714760">parent</a><span>|</span><a href="#40714852">prev</a><span>|</span><a href="#40715248">next</a><span>|</span><label class="collapse" for="c-40714951">[-]</label><label class="expand" for="c-40714951">[1 more]</label></div><br/><div class="children"><div class="content">The sign reads (paraphrased): “I want AI to do my dishes so I can do art. Not do art so I can do my dishes”<p>Your response is “learn to art” 
“The nerds dont owe you anything.”
“Most of You would be unsuccessful anyway”<p>You brought in absolutely unrelated items.<p>1) Learn art - that is baked into what the Sign is saying. There is no Terminal Point for being an artist.<p>2) Nerds dont… - Where nerds come in as a class for this conversation?<p>2.1) if you can speak for all nerds, please note that I sure as heck dont want Warhammer 40k, I want Star Trek.<p>3) Most would be unsuccessful - so what?<p>Are they happy practicing their craft? Do they have the choice to spend their time on those pursuits and enrich their lives, and share their joys with others around them?</div><br/></div></div></div></div></div></div><div id="40715248" class="c"><input type="checkbox" id="c-40715248" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#40714152">parent</a><span>|</span><a href="#40714565">prev</a><span>|</span><a href="#40715346">next</a><span>|</span><label class="collapse" for="c-40715248">[-]</label><label class="expand" for="c-40715248">[2 more]</label></div><br/><div class="children"><div class="content">The definition of AGI that i am familiar with is that it can do all (digital) tasks a human can do <i>at the level of an average human</i>.<p>As long as this level hasn&#x27;t been achieved in all domains, it isn&#x27;t AGI.</div><br/><div id="40715391" class="c"><input type="checkbox" id="c-40715391" checked=""/><div class="controls bullet"><span class="by">lupusreal</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40715248">parent</a><span>|</span><a href="#40715346">next</a><span>|</span><label class="collapse" for="c-40715391">[-]</label><label class="expand" for="c-40715391">[1 more]</label></div><br/><div class="children"><div class="content">This seems like a problematic standard.  For one, it&#x27;s very fuzzy.  A human who is top 51% and one who&#x27;s top 49% are very similar to each other and could probably swap places depending on how they&#x27;re feeling that day; there&#x27;s nothing fundamentally different going on in their heads.  Even at the further ends of the scale, humans have essentially the same kind of brains and minds as each other, some more capable than others but still all belonging to the same category of thinking things.  Your AGI definition bifurcates the human population into those that possess general intelligence and those who don&#x27;t, but this seems hard to justify.  At least, hard to justify when drawn there.  If you put the line at profound mental retardation where a person can no longer function in society, that would make more sense.  A slightly below average human may not be exceptional in any regard but I think they still possess what must be regarded as <i>general</i> intelligence.<p>Furthermore, you&#x27;re counting cases where humans do things the computer cannot but ignoring cases where the computer does things humans cannot.  For instance, I doubt any human alive, let alone <i>average humans</i> can give reasonable explanations for short snippets of computer code in as many languages as GPT-4o, or formulate poetry in as many styles on arbitrary topics, or rattle off esoteric trivia and opinions about obscure historic topics, ....  I think you get the point.  It has already surpassed average human abilities in many categories of intellectually challenging tasks, but with your definition if it fails at even one task an average human can do, then it lacks &quot;general intelligence.&quot;<p>I suggest that your definition is one for &quot;AHI&quot; (Average Human Intelligence), not one for &quot;AGI&quot; (Artificial <i>General</i> Intelligence.)</div><br/></div></div></div></div><div id="40715346" class="c"><input type="checkbox" id="c-40715346" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#40714152">parent</a><span>|</span><a href="#40715248">prev</a><span>|</span><a href="#40715384">next</a><span>|</span><label class="collapse" for="c-40715346">[-]</label><label class="expand" for="c-40715346">[1 more]</label></div><br/><div class="children"><div class="content">Totally agree. So does Peter Norvig: <a href="https:&#x2F;&#x2F;www.noemamag.com&#x2F;artificial-general-intelligence-is-already-here" rel="nofollow">https:&#x2F;&#x2F;www.noemamag.com&#x2F;artificial-general-intelligence-is-...</a></div><br/></div></div><div id="40715384" class="c"><input type="checkbox" id="c-40715384" checked=""/><div class="controls bullet"><span class="by">lucianbr</span><span>|</span><a href="#40714152">parent</a><span>|</span><a href="#40715346">prev</a><span>|</span><a href="#40714189">next</a><span>|</span><label class="collapse" for="c-40715384">[-]</label><label class="expand" for="c-40715384">[1 more]</label></div><br/><div class="children"><div class="content">&quot;A car is a plane, just a very bad one. It can do the cruising down the runway part, only the flying part is missing&quot;.</div><br/></div></div><div id="40714189" class="c"><input type="checkbox" id="c-40714189" checked=""/><div class="controls bullet"><span class="by">blharr</span><span>|</span><a href="#40714152">parent</a><span>|</span><a href="#40715384">prev</a><span>|</span><a href="#40712326">next</a><span>|</span><label class="collapse" for="c-40714189">[-]</label><label class="expand" for="c-40714189">[3 more]</label></div><br/><div class="children"><div class="content">The &quot;general&quot; part of AGI implies it should be capable across all types of different tasks. I would definitely call it real Artificial Intelligence, but it&#x27;s not general by any means.</div><br/><div id="40714598" class="c"><input type="checkbox" id="c-40714598" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40714189">parent</a><span>|</span><a href="#40712326">next</a><span>|</span><label class="collapse" for="c-40714598">[-]</label><label class="expand" for="c-40714598">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s capable of <i>attempting</i> all types of different tasks. That is a novel capability on its own. We&#x27;re used to GPT&#x27;s amusing failures at this point, so we forget that there is absolutely no input you could hand to a chess program that would get it to try and play checkers.<p>Not so with GPT. It will try, and fail, but that it tries at all was unimaginable five years ago.</div><br/><div id="40715160" class="c"><input type="checkbox" id="c-40715160" checked=""/><div class="controls bullet"><span class="by">dahart</span><span>|</span><a href="#40714152">root</a><span>|</span><a href="#40714598">parent</a><span>|</span><a href="#40712326">next</a><span>|</span><label class="collapse" for="c-40715160">[-]</label><label class="expand" for="c-40715160">[1 more]</label></div><br/><div class="children"><div class="content">Its amusing to me how the very language used to describe GPT anthropomorphizes it. GPT wont “attempt” or “try” anything on its own without a human telling it what to try, it has no agenda, no will, no agency, no self-reflection, no initiative, no fear, and no desire. It’s all A and no I.</div><br/></div></div></div></div></div></div></div></div><div id="40712326" class="c"><input type="checkbox" id="c-40712326" checked=""/><div class="controls bullet"><span class="by">asperous</span><span>|</span><a href="#40714152">prev</a><span>|</span><a href="#40712008">next</a><span>|</span><label class="collapse" for="c-40712326">[-]</label><label class="expand" for="c-40712326">[21 more]</label></div><br/><div class="children"><div class="content">Having tons of people employ human ingenuity to manipulate existing LLMs into passing this one benchmark kind of defeats the purpose of testing for &quot;AGI&quot;. The author points this out as it&#x27;s more of a pattern matching test.<p>Though on the other hand figuring out which manipulations are effective does teach us something. And I think most problems boil down to pattern matching, creating a true, easily testable AGI test may be tough.</div><br/><div id="40713120" class="c"><input type="checkbox" id="c-40713120" checked=""/><div class="controls bullet"><span class="by">worstspotgain</span><span>|</span><a href="#40712326">parent</a><span>|</span><a href="#40712503">next</a><span>|</span><label class="collapse" for="c-40713120">[-]</label><label class="expand" for="c-40713120">[12 more]</label></div><br/><div class="children"><div class="content">Let me play devil&#x27;s advocate for a second. Let&#x27;s suppose that with LLMs, we&#x27;ve actually invented an AGI machine that also happens to produce useful textual responses to a prompt.<p>This would sound more far-fetched if we knew exactly how they work, bit-by-bit. We&#x27;ve been training them statistically, via the data-for-code tradeoff. The question is not yet satisfactorily answered.<p>In this hypothetical, for every accusation that an LLM passes a test because it&#x27;s been coached to do so, there&#x27;s a counter that it was designed for &quot;excessively human&quot; AGI to begin with, maybe even that it was designed for the unconscious purpose of having humans pass it preferentially. The attorney for the hypothetical AGI in the LLM would argue that there are tons of &quot;LLM AGI&quot; problems it can solve that a human would struggle with.<p>Fundamentally, the tests are only useful insofar as they let us improve AI. The evaluation of novel approaches to pass them like this one should err in the approaches&#x27; favor, IMO. A &#x27;gotcha&#x27; test is the least-useful kind.</div><br/><div id="40713521" class="c"><input type="checkbox" id="c-40713521" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40713120">parent</a><span>|</span><a href="#40712503">next</a><span>|</span><label class="collapse" for="c-40713521">[-]</label><label class="expand" for="c-40713521">[11 more]</label></div><br/><div class="children"><div class="content">There’s every reason to believe that AGI is meaningfully different from LLMs because humans do not take anywhere near this amount of training data to create inferences (that and executive planning and creative problem solving are clear weak spots in LLMs)</div><br/><div id="40713651" class="c"><input type="checkbox" id="c-40713651" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40713521">parent</a><span>|</span><a href="#40714011">next</a><span>|</span><label class="collapse" for="c-40713651">[-]</label><label class="expand" for="c-40713651">[6 more]</label></div><br/><div class="children"><div class="content">&gt;There’s every reason to believe that AGI is meaningfully different from LLMs because humans do not take anywhere near this amount of training data to create inferences<p>The human brain is millions of years of brute force evolution in the making. Comparing it to a transformer or any other ANN really which essentially start from scratch relatively speaking doesn&#x27;t mean much.</div><br/><div id="40713702" class="c"><input type="checkbox" id="c-40713702" checked=""/><div class="controls bullet"><span class="by">infgeoax</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40713651">parent</a><span>|</span><a href="#40714011">next</a><span>|</span><label class="collapse" for="c-40713702">[-]</label><label class="expand" for="c-40713702">[5 more]</label></div><br/><div class="children"><div class="content">Plus it&#x27;s unclear if the amount of data used to &quot;train&quot; a human brain is really less than what GPT4 used. Imagine all the inputs from all the senses of a human over a lifetime: the sound, light, touches, interactions with peers, etc.</div><br/><div id="40714140" class="c"><input type="checkbox" id="c-40714140" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40713702">parent</a><span>|</span><a href="#40714993">next</a><span>|</span><label class="collapse" for="c-40714140">[-]</label><label class="expand" for="c-40714140">[3 more]</label></div><br/><div class="children"><div class="content">But that is of little help when you want to train an LLM to do the job at your company. A human requires just a little bit of tutorials and help, an LLM still require an unknown amount of data to get up to speed since we haven&#x27;t reached that level yet.</div><br/><div id="40714356" class="c"><input type="checkbox" id="c-40714356" checked=""/><div class="controls bullet"><span class="by">infgeoax</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40714140">parent</a><span>|</span><a href="#40714993">next</a><span>|</span><label class="collapse" for="c-40714356">[-]</label><label class="expand" for="c-40714356">[2 more]</label></div><br/><div class="children"><div class="content">Yeah humans can generalize much faster than LLM with far fewer &quot;examples&quot; running on sandwiches and coffee.</div><br/><div id="40714614" class="c"><input type="checkbox" id="c-40714614" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40714356">parent</a><span>|</span><a href="#40714993">next</a><span>|</span><label class="collapse" for="c-40714614">[-]</label><label class="expand" for="c-40714614">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Yeah humans can generalize much faster than LLM with far fewer &quot;examples&quot; running on sandwiches and coffee.<p>This isn&#x27;t really true. If you give an LLM a large prompt detailing a new spoken language, programming language or logical framework with a couple examples, and ask it to do something with it, it&#x27;ll probably do a lot better at it than if you just let an average human read the same prompt and do the same task.</div><br/></div></div></div></div></div></div><div id="40714993" class="c"><input type="checkbox" id="c-40714993" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40713702">parent</a><span>|</span><a href="#40714140">prev</a><span>|</span><a href="#40714011">next</a><span>|</span><label class="collapse" for="c-40714993">[-]</label><label class="expand" for="c-40714993">[1 more]</label></div><br/><div class="children"><div class="content">Don’t forget all the lifetimes of all ancestors as well. A lot of our intelligence is something we are born with and a result of many millions of years of evolution.</div><br/></div></div></div></div></div></div><div id="40714011" class="c"><input type="checkbox" id="c-40714011" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40713521">parent</a><span>|</span><a href="#40713651">prev</a><span>|</span><a href="#40712503">next</a><span>|</span><label class="collapse" for="c-40714011">[-]</label><label class="expand" for="c-40714011">[4 more]</label></div><br/><div class="children"><div class="content">How many attempts have there been for humans to solve math or science outstanding problems? We&#x27;re also kind of spamming with ideas until one works out</div><br/><div id="40714044" class="c"><input type="checkbox" id="c-40714044" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40714011">parent</a><span>|</span><a href="#40712503">next</a><span>|</span><label class="collapse" for="c-40714044">[-]</label><label class="expand" for="c-40714044">[3 more]</label></div><br/><div class="children"><div class="content">I’ll give you as much time as you want with an LLM and am 100% sure that it won’t solve a single outstanding complex math problem.</div><br/><div id="40714251" class="c"><input type="checkbox" id="c-40714251" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40714044">parent</a><span>|</span><a href="#40712503">next</a><span>|</span><label class="collapse" for="c-40714251">[-]</label><label class="expand" for="c-40714251">[2 more]</label></div><br/><div class="children"><div class="content">I can say the same about myself, and I would probably consider myself generally intelligent.</div><br/><div id="40714383" class="c"><input type="checkbox" id="c-40714383" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40714251">parent</a><span>|</span><a href="#40712503">next</a><span>|</span><label class="collapse" for="c-40714383">[-]</label><label class="expand" for="c-40714383">[1 more]</label></div><br/><div class="children"><div class="content">There’s a meaningful difference between a silicon intelligence and an organic one. Every silicon intelligence is closer to an equally smart clone whereas organic ones have much more variance (not to mention different training).<p>Anyway, my point was that humans butter direct their energy than randomly spamming ideas, at least with the innovation of the scientific method. But an LLM struggles deeply to perform reasoning.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40712503" class="c"><input type="checkbox" id="c-40712503" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#40712326">parent</a><span>|</span><a href="#40713120">prev</a><span>|</span><a href="#40712555">next</a><span>|</span><label class="collapse" for="c-40712503">[-]</label><label class="expand" for="c-40712503">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps if we don’t know how to create an evaluation that can’t be “gamed” it tells us something about how special our intelligence really is?</div><br/><div id="40715393" class="c"><input type="checkbox" id="c-40715393" checked=""/><div class="controls bullet"><span class="by">lucianbr</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40712503">parent</a><span>|</span><a href="#40712555">next</a><span>|</span><label class="collapse" for="c-40715393">[-]</label><label class="expand" for="c-40715393">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know how to create a liver, or test one, so what does that say about my liver? Pretty much nothing.</div><br/></div></div></div></div><div id="40712555" class="c"><input type="checkbox" id="c-40712555" checked=""/><div class="controls bullet"><span class="by">opdahl</span><span>|</span><a href="#40712326">parent</a><span>|</span><a href="#40712503">prev</a><span>|</span><a href="#40712632">next</a><span>|</span><label class="collapse" for="c-40712555">[-]</label><label class="expand" for="c-40712555">[4 more]</label></div><br/><div class="children"><div class="content">Wouldn’t the real AGI test be that an AI would be able to do what the author did here and write this blog post?</div><br/><div id="40712730" class="c"><input type="checkbox" id="c-40712730" checked=""/><div class="controls bullet"><span class="by">atroche</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40712555">parent</a><span>|</span><a href="#40712632">next</a><span>|</span><label class="collapse" for="c-40712730">[-]</label><label class="expand" for="c-40712730">[3 more]</label></div><br/><div class="children"><div class="content">Yep, but a float is more useful than a bool for tracking progress, especially if you want to answer questions like &quot;how soon can we expect (drivers&#x2F;customer support staff&#x2F;programmers) to lose their jobs?&quot;<p>Hard to find the right float but worth trying I think.</div><br/><div id="40713241" class="c"><input type="checkbox" id="c-40713241" checked=""/><div class="controls bullet"><span class="by">opdahl</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40712730">parent</a><span>|</span><a href="#40712632">next</a><span>|</span><label class="collapse" for="c-40713241">[-]</label><label class="expand" for="c-40713241">[2 more]</label></div><br/><div class="children"><div class="content">I agree, but it does seem a bit strange that you are allowed to &quot;custom-fit&quot; an AI program to solve a specific benchmark. Shouldn&#x27;t there be some sort of rule that for something to be AGI it should work as &quot;off-the-shelf&quot; as possible?</div><br/><div id="40713415" class="c"><input type="checkbox" id="c-40713415" checked=""/><div class="controls bullet"><span class="by">soist</span><span>|</span><a href="#40712326">root</a><span>|</span><a href="#40713241">parent</a><span>|</span><a href="#40712632">next</a><span>|</span><label class="collapse" for="c-40713415">[-]</label><label class="expand" for="c-40713415">[1 more]</label></div><br/><div class="children"><div class="content">If OpenAI had an embedded python interpreter or for that matter an interpreter for lambda calculus or some other equally universal Turing machine then this approach would work but there are no LLMs with embedded symbolic interpreters. LLMs currently are essentially probability distributions based on a training corpus and do not have any symbolic reasoning capabilities. There is no backtracking, for example, like in Prolog.</div><br/></div></div></div></div></div></div></div></div><div id="40712632" class="c"><input type="checkbox" id="c-40712632" checked=""/><div class="controls bullet"><span class="by">sheeshkebab</span><span>|</span><a href="#40712326">parent</a><span>|</span><a href="#40712555">prev</a><span>|</span><a href="#40713156">next</a><span>|</span><label class="collapse" for="c-40712632">[-]</label><label class="expand" for="c-40712632">[1 more]</label></div><br/><div class="children"><div class="content">Show me a test and I’ll show you a neural network that passes it… used to be an saying.</div><br/></div></div><div id="40713156" class="c"><input type="checkbox" id="c-40713156" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#40712326">parent</a><span>|</span><a href="#40712632">prev</a><span>|</span><a href="#40712008">next</a><span>|</span><label class="collapse" for="c-40713156">[-]</label><label class="expand" for="c-40713156">[1 more]</label></div><br/><div class="children"><div class="content">its LLM grade school. let them cook, train these things to match utility in our world. I&#x27;m not married to the &quot;AGI&quot; goal if there is other utility along the way.</div><br/></div></div></div></div><div id="40712008" class="c"><input type="checkbox" id="c-40712008" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#40712326">prev</a><span>|</span><a href="#40713305">next</a><span>|</span><label class="collapse" for="c-40712008">[-]</label><label class="expand" for="c-40712008">[9 more]</label></div><br/><div class="children"><div class="content">Very cool. When GPT-4 first came out I tried some very naive approaches using JSON representations on the puzzles [0], [1]. GPT-4 did &quot;okay&quot;, but in some cases it felt like it was falling for the classic LLM issue of saying all the right things but then then failing to grasp some critical bit of logic and missing the solution entirely.<p>At the time I noticed that many of the ARC problems rely on visual-spatial priors that are &quot;obvious&quot; when viewing the grids, but become less so when transmuted to some other representation. Many of them rely on some kind of symmetry, counting, or the very human bias to assume a velocity or continued movement when seeing particular patterns.<p>I had always thought maybe multimodality was key: the model needs to have similar priors around grounded physical spaces and movement to be able to do well. I&#x27;m not sure the OP really fleshes this line of thinking out, brute forcing python solutions is a very &quot;non human&quot; approach.<p>[0] <a href="https:&#x2F;&#x2F;x.com&#x2F;eatpraydiehard&#x2F;status&#x2F;1632671307254099968" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;eatpraydiehard&#x2F;status&#x2F;1632671307254099968</a><p>[1] <a href="https:&#x2F;&#x2F;x.com&#x2F;eatpraydiehard&#x2F;status&#x2F;1632683214329479169" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;eatpraydiehard&#x2F;status&#x2F;1632683214329479169</a></div><br/><div id="40712644" class="c"><input type="checkbox" id="c-40712644" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40712008">parent</a><span>|</span><a href="#40713305">next</a><span>|</span><label class="collapse" for="c-40712644">[-]</label><label class="expand" for="c-40712644">[8 more]</label></div><br/><div class="children"><div class="content">&gt; brute forcing python solutions is a very &quot;non human&quot; approach.<p>ARC-AGI has odd features that leave me flummoxed by the naming and the attendant prize money and hype.<p>It is one singular task and frankly I <i>strongly</i> suspect someone could beat it within 30 days[1], in an unsatisfying way, as you note.<p>There&#x27;s so much alpha that can be pieced together from here, ex. the last couple Google papers use the 1M context to do *500-shot*, i.e. 500 question answer examples. IIRC most recent showed raising travelling-salesman problem solve rate from 3 to 35%.<p>[1] I pre-registered this via a Twitter post, about 48 hours ago, i.e. before this result was announced.</div><br/><div id="40714197" class="c"><input type="checkbox" id="c-40714197" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40712008">root</a><span>|</span><a href="#40712644">parent</a><span>|</span><a href="#40712736">next</a><span>|</span><label class="collapse" for="c-40714197">[-]</label><label class="expand" for="c-40714197">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this is &quot;non-satisfying&quot; at all.<p>Program synthesis has been mentioned as a promising approach by François Chollet, and that&#x27;s exactly what this is.<p>The place I find slightly unsatisfying is this:<p>&gt; Sample vast, vast numbers of completions (~5,000 per problem) from GPT-4o.<p>&gt; Take the most promising 12 completions for each problem, and then try to fix each by showing GPT-4o what this program actually outputs on the examples, and then asking GPT-4o to revise the code to make it correct. We sample ~3,000 completions that attempt to fix per problem in total across these 12 starting implementations.<p>I&#x27;d been tossing around a MCTS idea similar to AlphaGo, based on the idea that the end transformation is a series of sub-transformations. I feel like this could work well alongside the GPT-4o completion catalog. (This isn&#x27;t an original observation or anything)</div><br/><div id="40714631" class="c"><input type="checkbox" id="c-40714631" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#40712008">root</a><span>|</span><a href="#40714197">parent</a><span>|</span><a href="#40712736">next</a><span>|</span><label class="collapse" for="c-40714631">[-]</label><label class="expand" for="c-40714631">[1 more]</label></div><br/><div class="children"><div class="content">Classic, I&#x27;ve been doing the same, writing an alphazero for the transformation part. What seems _much_ harder is picking a decent set of transformations&#x2F;concepts to work with, or more generally automating that process. Maybe you&#x27;re right that LLMs could help there!</div><br/></div></div></div></div><div id="40712736" class="c"><input type="checkbox" id="c-40712736" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#40712008">root</a><span>|</span><a href="#40712644">parent</a><span>|</span><a href="#40714197">prev</a><span>|</span><a href="#40713305">next</a><span>|</span><label class="collapse" for="c-40712736">[-]</label><label class="expand" for="c-40712736">[5 more]</label></div><br/><div class="children"><div class="content">The private test set has been available to crack for almost four years now. There was also a monetary prize competition run last year.<p>In your opinion, what has changed that would accelerate a solution to the next 30 days?</div><br/><div id="40713193" class="c"><input type="checkbox" id="c-40713193" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40712008">root</a><span>|</span><a href="#40712736">parent</a><span>|</span><a href="#40713305">next</a><span>|</span><label class="collapse" for="c-40713193">[-]</label><label class="expand" for="c-40713193">[4 more]</label></div><br/><div class="children"><div class="content">Prize money meant people would more cleverly strain the rule that &quot;the private test set stays private, no GPT4o, Claude etc.&quot;, as shown by the TFA.<p>This sort of idea would then be shared openly on new sites, creating more attempts. Fallout I did not anticipate was getting widespread attentional on general tech news sites, and then getting public comment from a prize co-founder confirming it was acceptable.</div><br/><div id="40713751" class="c"><input type="checkbox" id="c-40713751" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#40712008">root</a><span>|</span><a href="#40713193">parent</a><span>|</span><a href="#40713305">next</a><span>|</span><label class="collapse" for="c-40713751">[-]</label><label class="expand" for="c-40713751">[3 more]</label></div><br/><div class="children"><div class="content">It seems like you don’t understand the rules of the competition. Entries don’t have access to the internet. The OP acknowledges in their post that this is not eligible for the prize. The HN comment from the prize co-founder specifically says the OP’s claims haven’t been scrutinized. (implicit: they won’t be for the prize set unless the OP submits with an open LLM implementation)<p>There is a plan for a “public” leaderboard, but it currently has no entries, so we don’t actually know what the SOTA for the unrestrained version is. [1]<p>The general idea - test time augmentation - is what the current private set SOTA uses. [2] Generating more examples via transforming the samples is not a new idea.<p>Really, it seems like all the publicity has just gotten a bunch of armchair software architects coming up with 1-4 year-old ideas thinking they are geniuses.<p>[1] <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;leaderboard" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;leaderboard</a><p>[2] <a href="https:&#x2F;&#x2F;lab42.global&#x2F;community-interview-jack-cole&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lab42.global&#x2F;community-interview-jack-cole&#x2F;</a></div><br/><div id="40714174" class="c"><input type="checkbox" id="c-40714174" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40712008">root</a><span>|</span><a href="#40713751">parent</a><span>|</span><a href="#40713305">next</a><span>|</span><label class="collapse" for="c-40714174">[-]</label><label class="expand" for="c-40714174">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It seems like you don’t understand the rules of the competition.<p>I don&#x27;t think you &quot;don&#x27;t understand&quot; anything :) I&#x27;d ask you, politely, to consider that when you&#x27;re replying to other people in the future.<p>Better to bring to interactions the prior that your interlocutor is a presumably intelligent individual who can have a different interpretation of the same facts, than decide they just don&#x27;t get it. The second is a quite lonely path.<p>&gt; Entries don’t have access to the internet.<p>Correct. Per TFA, cofounder, Chollet, then me: this <i>is an offline solution</i>: the solution is the Python program found by an LLM.<p>&gt; The HN comment from the prize co-founder specifically says the OP’s claims haven’t been scrutinized.<p>Objection: relevancy? Is your claim here that it might be false so we shouldn&#x27;t be discussing it at all?<p>&gt; (implicit: they won’t be for the prize set unless the OP submits with an open LLM implementation)<p>I don&#x27;t know what this means, &quot;open LLM implementation&quot; is either a term of art I don&#x27;t recognize, or a misunderstanding of the situation.<p>I do assume you read the article, so I&#x27;m not trying to talk down to you, but to clarify:<p>The <i>solution is the Python program</i>, not the LLM prompts that iterated on a Python program. A common thread that would describe the confusing experience of reading your comment phrased aggressively and disputing everything up until you agree with me: your observations assume I assume the solution requires a cloud-based LLM to run. As noted above, it doesn&#x27;t, which is also the thrust of my comment: they found a way to skirt what I thought the rules are, and the co-founder and Chollett have embraced it, publicly.<p>&gt; There is a plan for a “public” leaderboard, but it currently has no entries, so we don’t actually know what the SOTA for the unrestrained version is. [1]<p>This was false before you posted, when I checked this morning, and it was false as early as 4 days ago, June 14th, we can confirm via archive.is. (prefix the URL you provided with archive.is&#x2F; to check for yourself)<p>&gt; The general idea - test time augmentation - is what the current private set SOTA uses. [2] Generating more examples via transforming the samples is not a new idea.<p>Did anyone claim it was?<p>&gt; Really, it seems like all the publicity has just gotten a bunch of armchair software architects coming up with 1-4 year-old ideas thinking they are geniuses.<p>I don&#x27;t know what this means other than you&#x27;re upset, but yes, sounds like both you and I agree that having an LLM generate Python programs isn&#x27;t quite what we&#x27;d thought would be an AGI solution in the eyes of Chollet.<p>Alas, here we are.</div><br/><div id="40714210" class="c"><input type="checkbox" id="c-40714210" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40712008">root</a><span>|</span><a href="#40714174">parent</a><span>|</span><a href="#40713305">next</a><span>|</span><label class="collapse" for="c-40714210">[-]</label><label class="expand" for="c-40714210">[1 more]</label></div><br/><div class="children"><div class="content">(Not the OP)<p>&gt;&gt; (implicit: they won’t be for the prize set unless the OP submits with an open LLM implementation)<p>&gt; The solution is the Python program, not the LLM prompts that iterated on a Python program. A common thread that would describe the confusing experience of reading your comment phrased aggressively and disputing everything up until you agree with me: your observations assume I assume the solution requires a cloud-based LLM to run. As noted above, it doesn&#x27;t, which is also the thrust of my comment: they found a way to skirt what I thought the rules are, and the co-founder and Chollett have embraced it, publicly.<p>I think the implication is that solutions that use an LLM via an API won&#x27;t be eligible (the &quot;no internet&quot; rule).<p>This seems obvious to solve: can use GPT4 to generate catalogs in advance and a lesser, local LLM with good code abilities to select them.<p>I don&#x27;t see why this skirts any rules you think were implied and I&#x27;m puzzled why you think it does.<p>&gt; sounds like both you and I agree that having an LLM generate Python programs isn&#x27;t quite what we&#x27;d thought would be an AGI solution in the eyes of Chollet.<p>&gt; Alas, here we are.<p>Chollet noted that program synthesis was a promising approach, so it&#x27;s not surprising to me that a program synthesis approach that also uses an LLM is effective.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40713305" class="c"><input type="checkbox" id="c-40713305" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#40712008">prev</a><span>|</span><a href="#40713480">next</a><span>|</span><label class="collapse" for="c-40713305">[-]</label><label class="expand" for="c-40713305">[1 more]</label></div><br/><div class="children"><div class="content">To me the big take-aways here are:<p>1) Most of the heavy lifting is being done by search. We&#x27;re talking about having the LLM generate <i>thousands</i> of candidate solutions, and they&#x27;re mostly bad enough that &quot;just pick the ones that get kinda close on the examples&quot; is a meaningful operation.<p>2) More samples improves performance despite the fact that GPT-4o&#x27;s vision is not capable of parsing the inputs. I&#x27;m curious how much performance would degrade if you shuffled the images passed to the model (but used the correct images when evaluating which candidates to keep).<p>3) It&#x27;s definitely true that the LLM has to be giving you something more than random programs. At the very least, the LLM knows how to craft parsimonious programs that are more likely to be the solution. It may be that it&#x27;s providing more than that, but it&#x27;s not clear to me exactly how much information on the correct search space is coming from the hand-crafted examples in the prompt.<p>Overall, the work to get this far is very impressive, but it doesn&#x27;t really move the needle for me on whether GPT-4 can do ARC puzzles. It does, however, show me that search is surprisingly powerful on this task.</div><br/></div></div><div id="40713480" class="c"><input type="checkbox" id="c-40713480" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#40713305">prev</a><span>|</span><a href="#40713006">next</a><span>|</span><label class="collapse" for="c-40713480">[-]</label><label class="expand" for="c-40713480">[1 more]</label></div><br/><div class="children"><div class="content">Seems that Arc-AGI is more flawed rather than GPT-4o is more AGI.<p>Maybe a AI version of Hanlons Razor. Never attribute to AGI what could be easily explained by being in the training set.</div><br/></div></div><div id="40713006" class="c"><input type="checkbox" id="c-40713006" checked=""/><div class="controls bullet"><span class="by">badrunaway</span><span>|</span><a href="#40713480">prev</a><span>|</span><a href="#40715339">next</a><span>|</span><label class="collapse" for="c-40713006">[-]</label><label class="expand" for="c-40713006">[3 more]</label></div><br/><div class="children"><div class="content">When we talk about system 2; is it possible that [generating large number of programs; evaluating them of the task; choosing top K outcomes; feeding it back to Neural net] can act as system 2 for a AGI? Isn&#x27;t that how we think intelligently as well- by making lot of hypothesis internally and evaluating them - and updating our model?</div><br/><div id="40713140" class="c"><input type="checkbox" id="c-40713140" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#40713006">parent</a><span>|</span><a href="#40713161">next</a><span>|</span><label class="collapse" for="c-40713140">[-]</label><label class="expand" for="c-40713140">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s more like humans are a chaotic choir of subsystems all doing their thing and tossing up their directives until some sort of &quot;win&quot; happens or the volume is loud enough in some direction that it then gets reverse engineered into a &quot;thought&quot;. But yes.</div><br/></div></div><div id="40713161" class="c"><input type="checkbox" id="c-40713161" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#40713006">parent</a><span>|</span><a href="#40713140">prev</a><span>|</span><a href="#40715339">next</a><span>|</span><label class="collapse" for="c-40713161">[-]</label><label class="expand" for="c-40713161">[1 more]</label></div><br/><div class="children"><div class="content">Possibly<p>I think we need those pieces, and also a piece for determining hypotheses in an efficient manner. Monte Carlo Tree Search could be that piece. Probabilistically choose a node to search, and then backpropagate the probabilities back to the root node.</div><br/></div></div></div></div><div id="40715339" class="c"><input type="checkbox" id="c-40715339" checked=""/><div class="controls bullet"><span class="by">comfortabledoug</span><span>|</span><a href="#40713006">prev</a><span>|</span><a href="#40712154">next</a><span>|</span><label class="collapse" for="c-40715339">[-]</label><label class="expand" for="c-40715339">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad someone else finally said it, those born blind cannot possibly have AGI!<p>&#x2F;sarcasm :D</div><br/></div></div><div id="40712154" class="c"><input type="checkbox" id="c-40712154" checked=""/><div class="controls bullet"><span class="by">rgbrgb</span><span>|</span><a href="#40715339">prev</a><span>|</span><a href="#40712174">next</a><span>|</span><label class="collapse" for="c-40712154">[-]</label><label class="expand" for="c-40712154">[9 more]</label></div><br/><div class="children"><div class="content">&gt; 50% accuracy on the public test set for ARC-AGI by having GPT-4o<p>Isn&#x27;t the public test set public on github and therefore GPT-4o trained on it?</div><br/><div id="40712472" class="c"><input type="checkbox" id="c-40712472" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#40712154">parent</a><span>|</span><a href="#40712401">next</a><span>|</span><label class="collapse" for="c-40712472">[-]</label><label class="expand" for="c-40712472">[3 more]</label></div><br/><div class="children"><div class="content">In this case I don&#x27;t think having seen the Arc set would help much in writing and selecting python scripts to solve the test cases.  (Unless someone else has tried this approach before and _their_ results are in the training data.)<p>It will be good to see the private set results though.</div><br/><div id="40713721" class="c"><input type="checkbox" id="c-40713721" checked=""/><div class="controls bullet"><span class="by">Truth_In_Lies</span><span>|</span><a href="#40712154">root</a><span>|</span><a href="#40712472">parent</a><span>|</span><a href="#40712793">next</a><span>|</span><label class="collapse" for="c-40713721">[-]</label><label class="expand" for="c-40713721">[1 more]</label></div><br/><div class="children"><div class="content">Yes, someone has tried <a href="https:&#x2F;&#x2F;iprc-dip.github.io&#x2F;DARC&#x2F;" rel="nofollow">https:&#x2F;&#x2F;iprc-dip.github.io&#x2F;DARC&#x2F;</a></div><br/></div></div><div id="40712793" class="c"><input type="checkbox" id="c-40712793" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#40712154">root</a><span>|</span><a href="#40712472">parent</a><span>|</span><a href="#40713721">prev</a><span>|</span><a href="#40712401">next</a><span>|</span><label class="collapse" for="c-40712793">[-]</label><label class="expand" for="c-40712793">[1 more]</label></div><br/><div class="children"><div class="content">Public discussions of solutions to the public test set will presumably have somewhat similar analogies and&#x2F;or embeddings to aspects of the python programs that solve them.</div><br/></div></div></div></div><div id="40712401" class="c"><input type="checkbox" id="c-40712401" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40712154">parent</a><span>|</span><a href="#40712472">prev</a><span>|</span><a href="#40712174">next</a><span>|</span><label class="collapse" for="c-40712401">[-]</label><label class="expand" for="c-40712401">[5 more]</label></div><br/><div class="children"><div class="content">I keep seeing this comment all over the place. Just because something exists 1 time in the training data doesn&#x27;t mean it can just regurgitate that. That&#x27;s not how training works. An LLM is not a knowledge database.</div><br/><div id="40713177" class="c"><input type="checkbox" id="c-40713177" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#40712154">root</a><span>|</span><a href="#40712401">parent</a><span>|</span><a href="#40712453">next</a><span>|</span><label class="collapse" for="c-40713177">[-]</label><label class="expand" for="c-40713177">[1 more]</label></div><br/><div class="children"><div class="content">It could exist many times. People can fork and clone the repo. People are likely to copy the examples and share them online.</div><br/></div></div><div id="40712453" class="c"><input type="checkbox" id="c-40712453" checked=""/><div class="controls bullet"><span class="by">adroniser</span><span>|</span><a href="#40712154">root</a><span>|</span><a href="#40712401">parent</a><span>|</span><a href="#40713177">prev</a><span>|</span><a href="#40712174">next</a><span>|</span><label class="collapse" for="c-40712453">[-]</label><label class="expand" for="c-40712453">[3 more]</label></div><br/><div class="children"><div class="content">And yet it doesn&#x27;t rule out that it can&#x27;t. See new york times lawsuit</div><br/><div id="40712544" class="c"><input type="checkbox" id="c-40712544" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40712154">root</a><span>|</span><a href="#40712453">parent</a><span>|</span><a href="#40712174">next</a><span>|</span><label class="collapse" for="c-40712544">[-]</label><label class="expand" for="c-40712544">[2 more]</label></div><br/><div class="children"><div class="content">From old pieces of articles that are quoted all over the internet? That&#x27;s not surprising.</div><br/><div id="40714639" class="c"><input type="checkbox" id="c-40714639" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40712154">root</a><span>|</span><a href="#40712544">parent</a><span>|</span><a href="#40712174">next</a><span>|</span><label class="collapse" for="c-40714639">[-]</label><label class="expand" for="c-40714639">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s still sufficient for both The Times and for it to be a potential problem in this case.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40712174" class="c"><input type="checkbox" id="c-40712174" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#40712154">prev</a><span>|</span><a href="#40712005">next</a><span>|</span><label class="collapse" for="c-40712174">[-]</label><label class="expand" for="c-40712174">[29 more]</label></div><br/><div class="children"><div class="content">The Arc stuff just felt intuitively wrong as soon as I heard it. I don&#x27;t find any of Chollet&#x27;s critiques of LLMs to be convincing. It&#x27;s almost as if he&#x27;s being overly negative about them to make a point or something to push back against all the unbridled optimism. The problem is, the optimism really seems to be justified, and the rate of improvement of LLMs in the past 12 months has been nothing short of astonishing.<p>So it&#x27;s not at all surprising to me to see Arc already being mostly solved using existing models, just with different prompting techniques and some tool usage. At some point, the naysayers about LLMs are going to have to confront the problem that, if they are right about LLMs not really thinking&#x2F;understanding&#x2F;being sentient, then a very large percentage of people living today are also not thinking&#x2F;understanding&#x2F;sentient!</div><br/><div id="40712233" class="c"><input type="checkbox" id="c-40712233" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40712352">next</a><span>|</span><label class="collapse" for="c-40712233">[-]</label><label class="expand" for="c-40712233">[2 more]</label></div><br/><div class="children"><div class="content">Actually the solution being discussed here is the one that Chollet mentioned in his interview with Dwarkesh, and only bolsters his case.<p>The LLM isn&#x27;t doing the reasoning here, it&#x27;s just pattern matching the before&#x2F;after diff and generating thousands of Python programs. The actual reasoning is done by an agentic like loop wrapped around the LLM, as described in the linked blog.</div><br/><div id="40713162" class="c"><input type="checkbox" id="c-40713162" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712233">parent</a><span>|</span><a href="#40712352">next</a><span>|</span><label class="collapse" for="c-40713162">[-]</label><label class="expand" for="c-40713162">[1 more]</label></div><br/><div class="children"><div class="content">When you peer into the soul of the machine it delicately resolves to `while(1){...}`. All Hail The REPL.</div><br/></div></div></div></div><div id="40712352" class="c"><input type="checkbox" id="c-40712352" checked=""/><div class="controls bullet"><span class="by">traject_</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40712233">prev</a><span>|</span><a href="#40712431">next</a><span>|</span><label class="collapse" for="c-40712352">[-]</label><label class="expand" for="c-40712352">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s almost as if he&#x27;s being overly negative about them to make a point or something to push back against all the unbridled optimism.<p>I don&#x27;t think it is like that but rather Chollet wants to see stronger neuroplasticity 
 in these models. I think there is a divide between the effectiveness of existing AI models versus their ability to be autonomous, robust and consistently learn from unanticipated problems.<p>My guess is Chollet wants to see something more similar to biological organisms especially mammals or birds in their level of autonomous nature. I think people underestimate the degree of novel problems birds and mammals alone face in just simply navigating their environment and it is the comparison here that LLMs, for now at least, seem lacking.<p>So when he says LLMs are not sentient, he&#x27;s asking to consider the novel problems animals let alone humans have to face in navigating their environment. This is especially apparent in young children but declines as we age and gain experience&#x2F;lose a sense of novelty.</div><br/><div id="40713640" class="c"><input type="checkbox" id="c-40713640" checked=""/><div class="controls bullet"><span class="by">infgeoax</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712352">parent</a><span>|</span><a href="#40712431">next</a><span>|</span><label class="collapse" for="c-40713640">[-]</label><label class="expand" for="c-40713640">[1 more]</label></div><br/><div class="children"><div class="content">Agree. When I first saw ARC, my reaction was this could possibly be the kind of problem that gives us evolutionary pressure.</div><br/></div></div></div></div><div id="40712431" class="c"><input type="checkbox" id="c-40712431" checked=""/><div class="controls bullet"><span class="by">TacticalCoder</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40712352">prev</a><span>|</span><a href="#40712304">next</a><span>|</span><label class="collapse" for="c-40712431">[-]</label><label class="expand" for="c-40712431">[11 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t find any of Chollet&#x27;s critiques of LLMs to be convincing. It&#x27;s almost as if he&#x27;s being overly negative about them to make a point or something to push back against all the unbridled optimism.<p>Chollet published his paper <i>On the measure of intelligence</i> in 2019. In Internet time that is a lifetime before the LLM hype started.</div><br/><div id="40712888" class="c"><input type="checkbox" id="c-40712888" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712431">parent</a><span>|</span><a href="#40712651">next</a><span>|</span><label class="collapse" for="c-40712888">[-]</label><label class="expand" for="c-40712888">[2 more]</label></div><br/><div class="children"><div class="content">From Chollet&#x27;s perspective, the LLM hype started well before, with at least GPT-2 half a year before his paper, and he spent plenty of time mocking GPT-2 on Twitter before he came up with ARC as a rebuttal.</div><br/><div id="40713697" class="c"><input type="checkbox" id="c-40713697" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712888">parent</a><span>|</span><a href="#40712651">next</a><span>|</span><label class="collapse" for="c-40713697">[-]</label><label class="expand" for="c-40713697">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a very convincing rebuttal considering that GPT-3 and GPT-4 came out after ARC but made no significant progress on it. He seemingly had the single most accurate and verifiable prediction of anyone in the world (in 2019) about <i>exactly</i> what type of tasks scaled LLMs would be bad at.</div><br/></div></div></div></div><div id="40712651" class="c"><input type="checkbox" id="c-40712651" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712431">parent</a><span>|</span><a href="#40712888">prev</a><span>|</span><a href="#40712304">next</a><span>|</span><label class="collapse" for="c-40712651">[-]</label><label class="expand" for="c-40712651">[8 more]</label></div><br/><div class="children"><div class="content">Einstein, infamously, couldn&#x27;t really make much progress with quantum physics, even though he invented the precursors (ex. Brownian motion). Your world model is hard to update.</div><br/><div id="40713512" class="c"><input type="checkbox" id="c-40713512" checked=""/><div class="controls bullet"><span class="by">imperfect_light</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712651">parent</a><span>|</span><a href="#40713624">next</a><span>|</span><label class="collapse" for="c-40713512">[-]</label><label class="expand" for="c-40713512">[6 more]</label></div><br/><div class="children"><div class="content">A bit of a stretch given that Chollet is a researcher in deep learning and transformers and his criticism is that memorization (training LLMs on lots and lots of problems) doesn&#x27;t equate to AGI.</div><br/><div id="40713693" class="c"><input type="checkbox" id="c-40713693" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40713512">parent</a><span>|</span><a href="#40713624">next</a><span>|</span><label class="collapse" for="c-40713693">[-]</label><label class="expand" for="c-40713693">[5 more]</label></div><br/><div class="children"><div class="content">&gt; A bit of a stretch<p>Is that true?<p>C.f. what we&#x27;re discussing<p>He&#x27;s actively encouraging using <i>LLMs</i> to solve his benchmark, called ARC <i>AGI</i>.<p>8 hours ago, from Chollet, re: TFA<p>&quot;The best solution to fight combinatorial explosion is to leverage intuition over the structure of program space, provided by a deep learning model. For instance, <i>you can use a LLM to sample a program</i>...&quot;<p>Source: <a href="https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1802801425514410275" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1802801425514410275</a></div><br/><div id="40713793" class="c"><input type="checkbox" id="c-40713793" checked=""/><div class="controls bullet"><span class="by">imperfect_light</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40713693">parent</a><span>|</span><a href="#40713624">next</a><span>|</span><label class="collapse" for="c-40713793">[-]</label><label class="expand" for="c-40713793">[4 more]</label></div><br/><div class="children"><div class="content">The stretch was in reference to comparing Chollet to Einstein.  Chollet clearly understands LLMs (and transformers and deep learning), he simply doesn&#x27;t believe they are sufficient for AGI.</div><br/><div id="40714132" class="c"><input type="checkbox" id="c-40714132" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40713793">parent</a><span>|</span><a href="#40713624">next</a><span>|</span><label class="collapse" for="c-40714132">[-]</label><label class="expand" for="c-40714132">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what you mean, it&#x27;s a straightforward analogy, but yes, that&#x27;s right, except for the part where he&#x27;s heralding this news by telling people the LLM is an underexplored solution space for a possible solution to his AGI benchmark he made to disprove LLMs are AGI.<p>I don&#x27;t mean to offend, but to be really straightforward: he&#x27;s the one saying it&#x27;s possible they might be AGI now. I&#x27;m as flummoxed as you, but I think its hiding the ball to file it under &quot;he doesn&#x27;t mean what he&#x27;s saying, because he doesn&#x27;t believe LLMs can ever be AGI.&quot; The only steelman for that is playing at: AGI-my-benchmark, which I say is for AGI, is not the AGI I mean</div><br/><div id="40714424" class="c"><input type="checkbox" id="c-40714424" checked=""/><div class="controls bullet"><span class="by">imperfect_light</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40714132">parent</a><span>|</span><a href="#40713624">next</a><span>|</span><label class="collapse" for="c-40714424">[-]</label><label class="expand" for="c-40714424">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re reading a whole lot into a tweet, in his interview with 
Dwarkesh Patel he says, about 20 different times, that scaling LLMs (as they are currently conceived) won&#x27;t lead to AGI.</div><br/><div id="40714964" class="c"><input type="checkbox" id="c-40714964" checked=""/><div class="controls bullet"><span class="by">anoncareer0212</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40714424">parent</a><span>|</span><a href="#40713624">next</a><span>|</span><label class="collapse" for="c-40714964">[-]</label><label class="expand" for="c-40714964">[1 more]</label></div><br/><div class="children"><div class="content">You keep changing topics so I don&#x27;t get it either, I can attest it&#x27;s not a fringe view that the situation is interesting, seen it discussed several times today by unrelated people.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40713624" class="c"><input type="checkbox" id="c-40713624" checked=""/><div class="controls bullet"><span class="by">infgeoax</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712651">parent</a><span>|</span><a href="#40713512">prev</a><span>|</span><a href="#40712304">next</a><span>|</span><label class="collapse" for="c-40713624">[-]</label><label class="expand" for="c-40713624">[1 more]</label></div><br/><div class="children"><div class="content">But it&#x27;s his EPR paper inspired the Bell&#x27;s inequality and pushed the field further. Yes he was wrong about how reality works, but still he asked the right question.</div><br/></div></div></div></div></div></div><div id="40712304" class="c"><input type="checkbox" id="c-40712304" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40712431">prev</a><span>|</span><a href="#40712290">next</a><span>|</span><label class="collapse" for="c-40712304">[-]</label><label class="expand" for="c-40712304">[4 more]</label></div><br/><div class="children"><div class="content">a) 50% result is not solving the problem. Especially when the implementation is brute forcing the problem and is against the spirit of ARC.<p>b) He is not being overly negative of LLMs. In fact he believes they will play a role in any AGI system.<p>c) OpenAI CTO has publicly said that ChatGPT 5 will not be significantly better than existing models. So the rate of improvements you believe in simply doesn&#x27;t match reality.</div><br/><div id="40712459" class="c"><input type="checkbox" id="c-40712459" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712304">parent</a><span>|</span><a href="#40714049">next</a><span>|</span><label class="collapse" for="c-40712459">[-]</label><label class="expand" for="c-40712459">[1 more]</label></div><br/><div class="children"><div class="content">For the record, a lot of problems might turn out to be like this, where we figure out a brute force approach that stands in for human creativity.</div><br/></div></div><div id="40714049" class="c"><input type="checkbox" id="c-40714049" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712304">parent</a><span>|</span><a href="#40712459">prev</a><span>|</span><a href="#40714041">next</a><span>|</span><label class="collapse" for="c-40714049">[-]</label><label class="expand" for="c-40714049">[1 more]</label></div><br/><div class="children"><div class="content">Skeptical about (c), source please. She did say they don&#x27;t have anything much better than GPT-4o currently, but GPT-5 likely only started training recently.</div><br/></div></div></div></div><div id="40712290" class="c"><input type="checkbox" id="c-40712290" checked=""/><div class="controls bullet"><span class="by">Smaug123</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40712304">prev</a><span>|</span><a href="#40714220">next</a><span>|</span><label class="collapse" for="c-40712290">[-]</label><label class="expand" for="c-40712290">[1 more]</label></div><br/><div class="children"><div class="content">&gt; a very large percentage of people living today are also not thinking&#x2F;understanding&#x2F;sentient<p>This isn&#x27;t <i>that</i> big a bullet to bite (<a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;4AHXDwcGab5PhKhHT&#x2F;humans-who-are-not-concentrating-are-not-general" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;4AHXDwcGab5PhKhHT&#x2F;humans-who...</a> comes from well before ChatGPT&#x27;s launch), and I myself am inclined to bite it. System 1 alone does not a general intelligence make, although the article is extremely interesting in asking the question &quot;is System 1 plus Python enough for a general intelligence?&quot;. But it&#x27;s not a very relevant philosophical point, because Chollet&#x27;s position is consistent with humans being obsoleted and&#x2F;or driven extinct whether or not the LLMs are &quot;general intelligences&quot;.<p>His position is that training LLMs results in an ever-larger number of learned algorithms and no ability to construct <i>new</i> algorithms. This is consistent with the possibility that, after some threshold of size and training, the LLM has learned every algorithm it needs to supplant humans in (say) 99.9% of cases. (It would definitely be going out with a whimper rather than a bang, on that hypothesis, to be out-competed by something that _really is_ just a gigantic lookup table!)</div><br/></div></div><div id="40714220" class="c"><input type="checkbox" id="c-40714220" checked=""/><div class="controls bullet"><span class="by">Lockal</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40712290">prev</a><span>|</span><a href="#40712713">next</a><span>|</span><label class="collapse" for="c-40714220">[-]</label><label class="expand" for="c-40714220">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a big jump in generalization that bruteforcing 4 colors in 9x9 grids with 8000 programs has anything near to what sentient human can do.<p>Back in the days similar generalization was used for Deep Blue chess computer. Computer won in 1997, but the AGI abyss is still as big.</div><br/></div></div><div id="40712713" class="c"><input type="checkbox" id="c-40712713" checked=""/><div class="controls bullet"><span class="by">biophysboy</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40714220">prev</a><span>|</span><a href="#40712385">next</a><span>|</span><label class="collapse" for="c-40712713">[-]</label><label class="expand" for="c-40712713">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think he&#x27;s as critical as you say. He just views LLMs as the product of intelligence rather than intelligence itself. LLM fans will say this is a false distinction, I guess.<p>His definition of intelligence is interesting: something that can quickly achieve tasks with few priors or experience. I also think the idea of using human &quot;Core Knowledge&quot; priors is a clever way to make a test.</div><br/></div></div><div id="40712385" class="c"><input type="checkbox" id="c-40712385" checked=""/><div class="controls bullet"><span class="by">adroniser</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40712713">prev</a><span>|</span><a href="#40713491">next</a><span>|</span><label class="collapse" for="c-40712385">[-]</label><label class="expand" for="c-40712385">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how the point about the typical human is relevant. Either you can reason or you can&#x27;t, the ARC test is supposed to be an objective way to measure this. Clearly a vanilla LLM currently cannot do this, and somehow an expert crafting a super-specific prompt is supposed to be impressive.</div><br/><div id="40712781" class="c"><input type="checkbox" id="c-40712781" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40712385">parent</a><span>|</span><a href="#40713491">next</a><span>|</span><label class="collapse" for="c-40712781">[-]</label><label class="expand" for="c-40712781">[1 more]</label></div><br/><div class="children"><div class="content">The point is that if you have some test of whether an AI is intelligent that the vast majority of living humans would fail or do worse on than gpt4-o (let alone future LLMs) then it’s not a very persuasive argument.</div><br/></div></div></div></div><div id="40713491" class="c"><input type="checkbox" id="c-40713491" checked=""/><div class="controls bullet"><span class="by">imperfect_light</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40712385">prev</a><span>|</span><a href="#40713110">next</a><span>|</span><label class="collapse" for="c-40713491">[-]</label><label class="expand" for="c-40713491">[1 more]</label></div><br/><div class="children"><div class="content">Did you listen to what Chollet said?  How much of LLM improvements are due to enlarging the training sets to cover more problems and how much is due to any emergent properties?</div><br/></div></div><div id="40713110" class="c"><input type="checkbox" id="c-40713110" checked=""/><div class="controls bullet"><span class="by">lassoiat</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40713491">prev</a><span>|</span><a href="#40712465">next</a><span>|</span><label class="collapse" for="c-40713110">[-]</label><label class="expand" for="c-40713110">[2 more]</label></div><br/><div class="children"><div class="content">I am a chatGPT fan boy and have been quite impressed by 4o but I will really be impressed when it stops inventing aspects of python libraries that don&#x27;t exists and instead just tells me it doesn&#x27;t exist.<p>It literally just did this for me 15 minutes ago. You can&#x27;t talk about AGI when it is this easy to push it over the edge into something it doesn&#x27;t know.<p>Paper references have got better the last 12 months but just this week it made up both a book and paper for me that do not exist. The authors exist and they did not write what it said they did.<p>It is very interesting if you ask &quot;do you understand your responses?&quot; sometimes it will say yes and sometimes it will so no not like a human understands.<p>We should forget about AGI until it can at least say it doesn&#x27;t know something. It is hardly a sign of intelligence in humans to make up answers to questions you don&#x27;t know.</div><br/><div id="40713818" class="c"><input type="checkbox" id="c-40713818" checked=""/><div class="controls bullet"><span class="by">motoxpro</span><span>|</span><a href="#40712174">root</a><span>|</span><a href="#40713110">parent</a><span>|</span><a href="#40712465">next</a><span>|</span><label class="collapse" for="c-40713818">[-]</label><label class="expand" for="c-40713818">[1 more]</label></div><br/><div class="children"><div class="content">Every time you’re wrong and you disagree with someone who is right you are inventing things that don’t exist.<p>Unless you’re saying you have never held on to a wrong opinion that was at some point proven to be wrong?</div><br/></div></div></div></div><div id="40712465" class="c"><input type="checkbox" id="c-40712465" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40712174">parent</a><span>|</span><a href="#40713110">prev</a><span>|</span><a href="#40712005">next</a><span>|</span><label class="collapse" for="c-40712465">[-]</label><label class="expand" for="c-40712465">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I agree. We have reached the end of LLMs. LLMs are infallible and require no further improvement. Anyone who points out shortcomings of current architectures and training approaches should be ignored as a naysayer. Anyone who proposes a solution to perceived flaws is a crank trying to fix something that was never broken. Everyone knows humans are incapable of internal monologues or visualization and vocalisation. Humans don&#x27;t actually move their lips to speak to produce a sound that can be interpreted by a speaker of the same language, they produce universally understood tokens encoding objective reality and the fact that they use the local language is merely a habit that is hard to break out of.</div><br/></div></div></div></div><div id="40712005" class="c"><input type="checkbox" id="c-40712005" checked=""/><div class="controls bullet"><span class="by">traject_</span><span>|</span><a href="#40712174">prev</a><span>|</span><a href="#40712635">next</a><span>|</span><label class="collapse" for="c-40712005">[-]</label><label class="expand" for="c-40712005">[2 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t actually know if it is SOTA, the previous SOTA solution also got around the same on the evaluation set.</div><br/><div id="40712275" class="c"><input type="checkbox" id="c-40712275" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#40712005">parent</a><span>|</span><a href="#40712635">next</a><span>|</span><label class="collapse" for="c-40712275">[-]</label><label class="expand" for="c-40712275">[1 more]</label></div><br/><div class="children"><div class="content">Yeah and GPT4o was potentially trained on this test set and if the tried to hold it out it was still likely trained on discussions of the problems.</div><br/></div></div></div></div><div id="40712635" class="c"><input type="checkbox" id="c-40712635" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#40712005">prev</a><span>|</span><a href="#40712792">next</a><span>|</span><label class="collapse" for="c-40712635">[-]</label><label class="expand" for="c-40712635">[2 more]</label></div><br/><div class="children"><div class="content">François Chollet says LLMs do not learn in-context. But Geoff Hinton says LLMs&#x27; few-shot learning compares quite favorably with people!<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=QWWgr2rN45o&amp;t=46m20s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=QWWgr2rN45o&amp;t=46m20s</a><p>The truth is in the middle, I think. They learn in-context, but not as well as humans.<p>The approach in the article hides the unreliability of current LLMs by generating thousands of programs, and still the results aren&#x27;t human-level. (This is impressive work though -- I&#x27;m not criticizing it.)</div><br/><div id="40714144" class="c"><input type="checkbox" id="c-40714144" checked=""/><div class="controls bullet"><span class="by">hackpert</span><span>|</span><a href="#40712635">parent</a><span>|</span><a href="#40712792">next</a><span>|</span><label class="collapse" for="c-40714144">[-]</label><label class="expand" for="c-40714144">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure how to quantify how quickly or well humans learn in-context (if you know of any work on this I&#x27;d love to read it!)<p>In general, there is too much fluff and confusion floating around about what these models are and are not capable of (regardless of the training mechanism.) I think more people need to read Song Mei&#x27;s lovely slides[1] and related work by others. These slides are the best exposition I&#x27;ve found of neat ideas around ICL that researchers have been aware of for a while.<p>[1] <a href="https:&#x2F;&#x2F;www.stat.berkeley.edu&#x2F;~songmei&#x2F;Presentation&#x2F;Algorithm_approx_BSJC.pdf" rel="nofollow">https:&#x2F;&#x2F;www.stat.berkeley.edu&#x2F;~songmei&#x2F;Presentation&#x2F;Algorith...</a></div><br/></div></div></div></div><div id="40712792" class="c"><input type="checkbox" id="c-40712792" checked=""/><div class="controls bullet"><span class="by">TheDudeMan</span><span>|</span><a href="#40712635">prev</a><span>|</span><a href="#40711986">next</a><span>|</span><label class="collapse" for="c-40712792">[-]</label><label class="expand" for="c-40712792">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Vision is an especially large weakness.&quot;<p>But you can have GPT write code to reliably convert the image grid into a textual representation, right?  And code to convert back to image and auto-verify.</div><br/><div id="40713429" class="c"><input type="checkbox" id="c-40713429" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40712792">parent</a><span>|</span><a href="#40711986">next</a><span>|</span><label class="collapse" for="c-40713429">[-]</label><label class="expand" for="c-40713429">[2 more]</label></div><br/><div class="children"><div class="content">GPT-4o might not have been trained on a sufficiently large amount of visual data to develop advanced spatial intelligence. Perhaps it needs to see a lot more images, or perhaps it needs to be trained differently (e.g. predict the next frame in a video). I suspect SORA has more spatial intelligence internally than 4o.</div><br/></div></div></div></div><div id="40712167" class="c"><input type="checkbox" id="c-40712167" checked=""/><div class="controls bullet"><span class="by">bashfulpup</span><span>|</span><a href="#40711986">prev</a><span>|</span><a href="#40713656">next</a><span>|</span><label class="collapse" for="c-40712167">[-]</label><label class="expand" for="c-40712167">[9 more]</label></div><br/><div class="children"><div class="content">I looked at the website and have no idea how Arc is supposed to be AGI.<p>Can someone explain?</div><br/><div id="40712815" class="c"><input type="checkbox" id="c-40712815" checked=""/><div class="controls bullet"><span class="by">TheDudeMan</span><span>|</span><a href="#40712167">parent</a><span>|</span><a href="#40712975">next</a><span>|</span><label class="collapse" for="c-40712815">[-]</label><label class="expand" for="c-40712815">[7 more]</label></div><br/><div class="children"><div class="content">It is necessary but not sufficient.<p>If you can&#x27;t do ARC, you aren&#x27;t general enough.  But even if you can do ARC, you still might not be general enough.</div><br/><div id="40712927" class="c"><input type="checkbox" id="c-40712927" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#40712167">root</a><span>|</span><a href="#40712815">parent</a><span>|</span><a href="#40712975">next</a><span>|</span><label class="collapse" for="c-40712927">[-]</label><label class="expand" for="c-40712927">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also possible that you are an AGI and simply cannot pass ARC.</div><br/><div id="40713114" class="c"><input type="checkbox" id="c-40713114" checked=""/><div class="controls bullet"><span class="by">TheDudeMan</span><span>|</span><a href="#40712167">root</a><span>|</span><a href="#40712927">parent</a><span>|</span><a href="#40712975">next</a><span>|</span><label class="collapse" for="c-40713114">[-]</label><label class="expand" for="c-40713114">[5 more]</label></div><br/><div class="children"><div class="content">How so?  If there is a task that humans can do but the AI cannot, I would not call it AGI.  But that&#x27;s just my definition.</div><br/><div id="40713172" class="c"><input type="checkbox" id="c-40713172" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#40712167">root</a><span>|</span><a href="#40713114">parent</a><span>|</span><a href="#40712975">next</a><span>|</span><label class="collapse" for="c-40713172">[-]</label><label class="expand" for="c-40713172">[4 more]</label></div><br/><div class="children"><div class="content">Yeah but if my brother can&#x27;t pass it, that doesn&#x27;t mean he is NOT human.</div><br/><div id="40713646" class="c"><input type="checkbox" id="c-40713646" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40712167">root</a><span>|</span><a href="#40713172">parent</a><span>|</span><a href="#40714594">next</a><span>|</span><label class="collapse" for="c-40713646">[-]</label><label class="expand" for="c-40713646">[1 more]</label></div><br/><div class="children"><div class="content">Could he pass it if he was educated to do the task from birth? Human level intelligence includes being able to be educated, the ML models we have done so far can&#x27;t be educated so have to match the level of educated humans to compare.<p>General intelligence as we know it requires ability to receive education.</div><br/></div></div><div id="40714594" class="c"><input type="checkbox" id="c-40714594" checked=""/><div class="controls bullet"><span class="by">TheDudeMan</span><span>|</span><a href="#40712167">root</a><span>|</span><a href="#40713172">parent</a><span>|</span><a href="#40713646">prev</a><span>|</span><a href="#40713654">next</a><span>|</span><label class="collapse" for="c-40714594">[-]</label><label class="expand" for="c-40714594">[1 more]</label></div><br/><div class="children"><div class="content">I said AGI.  I did not say human.</div><br/></div></div><div id="40713654" class="c"><input type="checkbox" id="c-40713654" checked=""/><div class="controls bullet"><span class="by">infgeoax</span><span>|</span><a href="#40712167">root</a><span>|</span><a href="#40713172">parent</a><span>|</span><a href="#40714594">prev</a><span>|</span><a href="#40712975">next</a><span>|</span><label class="collapse" for="c-40713654">[-]</label><label class="expand" for="c-40713654">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t chatGPT already proven to be smarter than many of us in many ways?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40712975" class="c"><input type="checkbox" id="c-40712975" checked=""/><div class="controls bullet"><span class="by">biophysboy</span><span>|</span><a href="#40712167">parent</a><span>|</span><a href="#40712815">prev</a><span>|</span><a href="#40713656">next</a><span>|</span><label class="collapse" for="c-40712975">[-]</label><label class="expand" for="c-40712975">[1 more]</label></div><br/><div class="children"><div class="content">Its not a test of AGI. It tests whether you possess innate human capacities: rudimentary arithmetic &amp; geometry, etc. Most of the problems were created manually. The original paper states that they limited the test to innate human priors to make the scope well-defined.</div><br/></div></div></div></div><div id="40713656" class="c"><input type="checkbox" id="c-40713656" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#40712167">prev</a><span>|</span><a href="#40712864">next</a><span>|</span><label class="collapse" for="c-40713656">[-]</label><label class="expand" for="c-40713656">[3 more]</label></div><br/><div class="children"><div class="content">Can we be sure GPT-4o hasn’t been trained on the public test set?</div><br/><div id="40713748" class="c"><input type="checkbox" id="c-40713748" checked=""/><div class="controls bullet"><span class="by">ivalm</span><span>|</span><a href="#40713656">parent</a><span>|</span><a href="#40712864">next</a><span>|</span><label class="collapse" for="c-40713748">[-]</label><label class="expand" for="c-40713748">[2 more]</label></div><br/><div class="children"><div class="content">It has been trained on public test set as it’s on github</div><br/><div id="40714301" class="c"><input type="checkbox" id="c-40714301" checked=""/><div class="controls bullet"><span class="by">free_bip</span><span>|</span><a href="#40713656">root</a><span>|</span><a href="#40713748">parent</a><span>|</span><a href="#40712864">next</a><span>|</span><label class="collapse" for="c-40714301">[-]</label><label class="expand" for="c-40714301">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t necessarily true, OpenAI isn&#x27;t open about how their data cleaning process works.</div><br/></div></div></div></div></div></div><div id="40712864" class="c"><input type="checkbox" id="c-40712864" checked=""/><div class="controls bullet"><span class="by">gibsonf1</span><span>|</span><a href="#40713656">prev</a><span>|</span><a href="#40712143">next</a><span>|</span><label class="collapse" for="c-40712864">[-]</label><label class="expand" for="c-40712864">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t 50% kind of a failing grade?</div><br/><div id="40713447" class="c"><input type="checkbox" id="c-40713447" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40712864">parent</a><span>|</span><a href="#40712143">next</a><span>|</span><label class="collapse" for="c-40713447">[-]</label><label class="expand" for="c-40713447">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s only been a week since the million dollar prize was offered. Let&#x27;s see what the SOTA is in a month.</div><br/></div></div></div></div><div id="40712143" class="c"><input type="checkbox" id="c-40712143" checked=""/><div class="controls bullet"><span class="by">greatpostman</span><span>|</span><a href="#40712864">prev</a><span>|</span><label class="collapse" for="c-40712143">[-]</label><label class="expand" for="c-40712143">[4 more]</label></div><br/><div class="children"><div class="content">You know you’re approaching AGI when creating benchmarks gets difficult. This is only just beginning</div><br/><div id="40713061" class="c"><input type="checkbox" id="c-40713061" checked=""/><div class="controls bullet"><span class="by">Slyfox33</span><span>|</span><a href="#40712143">parent</a><span>|</span><a href="#40712761">next</a><span>|</span><label class="collapse" for="c-40713061">[-]</label><label class="expand" for="c-40713061">[1 more]</label></div><br/><div class="children"><div class="content">Benchmarks being difficult to create has no connection to something being agi.</div><br/></div></div><div id="40712761" class="c"><input type="checkbox" id="c-40712761" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#40712143">parent</a><span>|</span><a href="#40713061">prev</a><span>|</span><label class="collapse" for="c-40712761">[-]</label><label class="expand" for="c-40712761">[2 more]</label></div><br/><div class="children"><div class="content">Alternatively, society has no common understanding of what AGI means.</div><br/><div id="40712937" class="c"><input type="checkbox" id="c-40712937" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#40712143">root</a><span>|</span><a href="#40712761">parent</a><span>|</span><label class="collapse" for="c-40712937">[-]</label><label class="expand" for="c-40712937">[1 more]</label></div><br/><div class="children"><div class="content">Which is why the first &quot;artificial&quot; sentient beings will likely go through the wringer that humanity historically put other &quot;sub-human&quot; beings through.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>