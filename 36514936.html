<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688029269275" as="style"/><link rel="stylesheet" href="styles.css?v=1688029269275"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.salesforceairesearch.com/xgen/">XGen-7B, a new 7B foundational model trained on up to 8K length for 1.5T tokens</a>Â <span class="domain">(<a href="https://blog.salesforceairesearch.com">blog.salesforceairesearch.com</a>)</span></div><div class="subtext"><span>bratao</span> | <span>82 comments</span></div><br/><div><div id="36515401" class="c"><input type="checkbox" id="c-36515401" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36515200">next</a><span>|</span><label class="collapse" for="c-36515401">[-]</label><label class="expand" for="c-36515401">[6 more]</label></div><br/><div class="children"><div class="content">&gt; The training recipe and model architecture follow LLaMA<p>This is huge.<p>MPT and Falcon are cool, but the inference runtimes and various tooling is mostly optimized for LLaMA. If this is a drop-in replacement for 7B, it&#x27;s going to catch on much faster than any other small model.</div><br/><div id="36517221" class="c"><input type="checkbox" id="c-36517221" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36515401">parent</a><span>|</span><a href="#36516030">next</a><span>|</span><label class="collapse" for="c-36517221">[-]</label><label class="expand" for="c-36517221">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also OpenLLaMA, which has a 13B version as well and is a straight drop-in (except for code generation due to multiple space tokenization: 
<a href="https:&#x2F;&#x2F;github.com&#x2F;openlm-research&#x2F;open_llama#update-06152023">https:&#x2F;&#x2F;github.com&#x2F;openlm-research&#x2F;open_llama#update-0615202...</a>).<p>XGen-7B is probably the superior 7B model, it&#x27;s trained on more tokens and a longer default sequence length (although both presumably can adopt SuperHOT (Position Interpolation) to extend context), but larger models still probably perform better on an absolute basis.</div><br/></div></div><div id="36516030" class="c"><input type="checkbox" id="c-36516030" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#36515401">parent</a><span>|</span><a href="#36517221">prev</a><span>|</span><a href="#36515200">next</a><span>|</span><label class="collapse" for="c-36516030">[-]</label><label class="expand" for="c-36516030">[4 more]</label></div><br/><div class="children"><div class="content">It seems to use a different tokenizer than LLaMA, though the neural network architecture is the same.</div><br/><div id="36516190" class="c"><input type="checkbox" id="c-36516190" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36515401">root</a><span>|</span><a href="#36516030">parent</a><span>|</span><a href="#36516417">next</a><span>|</span><label class="collapse" for="c-36516190">[-]</label><label class="expand" for="c-36516190">[2 more]</label></div><br/><div class="children"><div class="content">The tokenizer appears to be the original GPT-2 tokenizer, with some curious added tokens: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Salesforce&#x2F;xgen-7b-8k-base&#x2F;blob&#x2F;5e1ad70389cda911e995522368165a7cc4b3da21&#x2F;tokenization_xgen.py#L40" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Salesforce&#x2F;xgen-7b-8k-base&#x2F;blob&#x2F;5e1ad...</a></div><br/><div id="36516554" class="c"><input type="checkbox" id="c-36516554" checked=""/><div class="controls bullet"><span class="by">imrehg</span><span>|</span><a href="#36515401">root</a><span>|</span><a href="#36516190">parent</a><span>|</span><a href="#36516417">next</a><span>|</span><label class="collapse" for="c-36516554">[-]</label><label class="expand" for="c-36516554">[1 more]</label></div><br/><div class="children"><div class="content">Those look all programming related tokens, and I think they should relate to their focus of improving this model&#x27;s code generation capabilities, and adding quite a bit of data related to that (BigCode Starcoder in the Second stage in pre-training <a href="https:&#x2F;&#x2F;blog.salesforceairesearch.com&#x2F;xgen&#x2F;#pre-training-data" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.salesforceairesearch.com&#x2F;xgen&#x2F;#pre-training-dat...</a> )</div><br/></div></div></div></div><div id="36516417" class="c"><input type="checkbox" id="c-36516417" checked=""/><div class="controls bullet"><span class="by">imrehg</span><span>|</span><a href="#36515401">root</a><span>|</span><a href="#36516030">parent</a><span>|</span><a href="#36516190">prev</a><span>|</span><a href="#36515200">next</a><span>|</span><label class="collapse" for="c-36516417">[-]</label><label class="expand" for="c-36516417">[1 more]</label></div><br/><div class="children"><div class="content">Looks like this is the limitation why it cannot just be plugged into llama.cpp (doesn&#x27;t have a saved tokeniser model, and I&#x27;m not sure how would one go about creating one). Otherwise it would be cool to try it out, the metrics in the article are promising to have something running locally on an M1 Mac...</div><br/></div></div></div></div></div></div><div id="36515200" class="c"><input type="checkbox" id="c-36515200" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#36515401">prev</a><span>|</span><a href="#36516814">next</a><span>|</span><label class="collapse" for="c-36515200">[-]</label><label class="expand" for="c-36515200">[40 more]</label></div><br/><div class="children"><div class="content">From all the experimentation I&#x27;ve done, 7B parameter models just don&#x27;t seem to be able to produce useful output reliably enough for my use cases.<p>What use cases do people have for these smaller LLM&#x27;s?</div><br/><div id="36515386" class="c"><input type="checkbox" id="c-36515386" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36515231">next</a><span>|</span><label class="collapse" for="c-36515386">[-]</label><label class="expand" for="c-36515386">[15 more]</label></div><br/><div class="children"><div class="content">7B LLaMA is a terrible general purpose model, but the finetunes are pretty good at very specific roles, like dialogue&#x2F;roleplay, a dungeon master bot or even code completion.<p>The metrics are good though, perhaps placing this closer to 13B.<p>And 8K context is <i>huge</i>. When you can stuff that much example text in, it gives the model more to &quot;latch onto,&quot; and its also the point where you would start worrying about RAM&#x2F;VRAM consumption for a ~13B model.</div><br/><div id="36516059" class="c"><input type="checkbox" id="c-36516059" checked=""/><div class="controls bullet"><span class="by">ttt3ts</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515386">parent</a><span>|</span><a href="#36516952">next</a><span>|</span><label class="collapse" for="c-36516059">[-]</label><label class="expand" for="c-36516059">[1 more]</label></div><br/><div class="children"><div class="content">Do you know a dataset for fine-tuning roleplaying?</div><br/></div></div><div id="36516952" class="c"><input type="checkbox" id="c-36516952" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515386">parent</a><span>|</span><a href="#36516059">prev</a><span>|</span><a href="#36515704">next</a><span>|</span><label class="collapse" for="c-36516952">[-]</label><label class="expand" for="c-36516952">[3 more]</label></div><br/><div class="children"><div class="content">You must have missed the memo... It&#x27;s now super easy to extend the context of 2k llama models to 8k, 16k, or even 32k with just a small fine tune and a tweak to the code.<p>You still need the memory to be able to go that high, but it&#x27;s totally doable.</div><br/><div id="36517030" class="c"><input type="checkbox" id="c-36517030" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36516952">parent</a><span>|</span><a href="#36515704">next</a><span>|</span><label class="collapse" for="c-36517030">[-]</label><label class="expand" for="c-36517030">[2 more]</label></div><br/><div class="children"><div class="content">What memo&#x2F;paper does this?</div><br/><div id="36517133" class="c"><input type="checkbox" id="c-36517133" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36517030">parent</a><span>|</span><a href="#36515704">next</a><span>|</span><label class="collapse" for="c-36517133">[-]</label><label class="expand" for="c-36517133">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.15595" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.15595</a></div><br/></div></div></div></div></div></div><div id="36515704" class="c"><input type="checkbox" id="c-36515704" checked=""/><div class="controls bullet"><span class="by">aryamaan</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515386">parent</a><span>|</span><a href="#36516952">prev</a><span>|</span><a href="#36515231">next</a><span>|</span><label class="collapse" for="c-36515704">[-]</label><label class="expand" for="c-36515704">[10 more]</label></div><br/><div class="children"><div class="content">would appreciate resources on fine tuning</div><br/><div id="36517666" class="c"><input type="checkbox" id="c-36517666" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515704">parent</a><span>|</span><a href="#36516048">next</a><span>|</span><label class="collapse" for="c-36517666">[-]</label><label class="expand" for="c-36517666">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a good writeup that goes into more depth than most of the READMEs on fine-tuning: <a href="https:&#x2F;&#x2F;erichartford.com&#x2F;uncensored-models" rel="nofollow noreferrer">https:&#x2F;&#x2F;erichartford.com&#x2F;uncensored-models</a></div><br/></div></div><div id="36516048" class="c"><input type="checkbox" id="c-36516048" checked=""/><div class="controls bullet"><span class="by">ttt3ts</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515704">parent</a><span>|</span><a href="#36517666">prev</a><span>|</span><a href="#36515827">next</a><span>|</span><label class="collapse" for="c-36516048">[-]</label><label class="expand" for="c-36516048">[6 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;stackllama" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;stackllama</a><p>You can easily finetune 7B or 15B LORA model with that on consumer GPUs.</div><br/><div id="36516083" class="c"><input type="checkbox" id="c-36516083" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36516048">parent</a><span>|</span><a href="#36515827">next</a><span>|</span><label class="collapse" for="c-36516083">[-]</label><label class="expand" for="c-36516083">[5 more]</label></div><br/><div class="children"><div class="content">That blog post demonstrates that it&#x27;s not &quot;easily&quot; finetuneable, just possible to finetune. There&#x27;s <i>many</i> technical considerations even beyond hardware (dataset formatting, training hyperparameter nuances) that do not make it accessible to the newbie experimenting with LLMs.<p>It&#x27;s a rabbithole, and unfortunately there&#x27;s no good shortcuts.</div><br/><div id="36516166" class="c"><input type="checkbox" id="c-36516166" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36516083">parent</a><span>|</span><a href="#36515827">next</a><span>|</span><label class="collapse" for="c-36516166">[-]</label><label class="expand" for="c-36516166">[4 more]</label></div><br/><div class="children"><div class="content">Why have there been thousands of overnight AI&#x2F;GPT startups and products in the last few months and NOT a single simple intuitive &quot;fine tuning wizard&quot; app? That seems like such an obvious glaring gap.</div><br/><div id="36516232" class="c"><input type="checkbox" id="c-36516232" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36516166">parent</a><span>|</span><a href="#36517138">next</a><span>|</span><label class="collapse" for="c-36516232">[-]</label><label class="expand" for="c-36516232">[2 more]</label></div><br/><div class="children"><div class="content">Because the ChatGPT API (and analogous competitors) is cheap enough that it&#x27;s both faster and more cost effective to just use it instead instead of using your own model, with maybe some shenanigans to handle its shortcomings without increasing cost much if at all.. And that was before gpt-3.5-turbo-0613, which dropped the price more and is about 2-3x faster.<p>There are startups that do finetuning on your own data, but with zero hints on how to preprocess your data and <i>absurd</i> costs (both upfront training and GPUs for serving inference) that&#x27;s it&#x27;s extremely difficult to argue from a customer business perspective compared to just using an API.</div><br/></div></div><div id="36517138" class="c"><input type="checkbox" id="c-36517138" checked=""/><div class="controls bullet"><span class="by">shapefrog</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36516166">parent</a><span>|</span><a href="#36516232">prev</a><span>|</span><a href="#36515827">next</a><span>|</span><label class="collapse" for="c-36517138">[-]</label><label class="expand" for="c-36517138">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why have there been thousands of overnight AI&#x2F;GPT startups and products in the last few months and NOT a single simple intuitive &quot;fine tuning wizard&quot; app?<p>Vapourware GPT startup inc is valued at $2bn the afternoon after you form the company and buy your first macbook.<p>Actual usage of Ai, fine tuning etc. I can offer you $100,000 for 30% of your company if you can demonstrate a fully working product.</div><br/></div></div></div></div></div></div></div></div><div id="36515827" class="c"><input type="checkbox" id="c-36515827" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515704">parent</a><span>|</span><a href="#36516048">prev</a><span>|</span><a href="#36515960">next</a><span>|</span><label class="collapse" for="c-36515827">[-]</label><label class="expand" for="c-36515827">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, I dunno. I think most people are using lit-llama or EasyLM (on TPUs) for finetuning?<p>QLORA is the gold standard for more affordable training.<p>As for datasets, just look at the open datasets the best-in-class models are using, like Vicuna or <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;NousResearch&#x2F;Nous-Hermes-13b" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;NousResearch&#x2F;Nous-Hermes-13b</a><p>Some model datasets like Manticore, Chronos or the infamous Pygmalion are more &quot;secretive,&quot; but you can find the dataset gathering scripts on Github or in community chats.</div><br/></div></div><div id="36515960" class="c"><input type="checkbox" id="c-36515960" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515704">parent</a><span>|</span><a href="#36515827">prev</a><span>|</span><a href="#36515231">next</a><span>|</span><label class="collapse" for="c-36515960">[-]</label><label class="expand" for="c-36515960">[1 more]</label></div><br/><div class="children"><div class="content">This and&#x2F;or text-generation-webui training doc are a good place to start.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;zetavg&#x2F;LLaMA-LoRA-Tuner">https:&#x2F;&#x2F;github.com&#x2F;zetavg&#x2F;LLaMA-LoRA-Tuner</a></div><br/></div></div></div></div></div></div><div id="36515231" class="c"><input type="checkbox" id="c-36515231" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36515386">prev</a><span>|</span><a href="#36515216">next</a><span>|</span><label class="collapse" for="c-36515231">[-]</label><label class="expand" for="c-36515231">[5 more]</label></div><br/><div class="children"><div class="content">&gt; What use cases do people have for these smaller LLM&#x27;s?<p>None. Training a functionally useless model and releasing it is a great way to demonstrate that your company is hip and current. That way when prospective clients ask about AI you can vaguely gesture at some model that you released and say you employ cutting edge AI experts.</div><br/><div id="36515706" class="c"><input type="checkbox" id="c-36515706" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515231">parent</a><span>|</span><a href="#36515216">next</a><span>|</span><label class="collapse" for="c-36515706">[-]</label><label class="expand" for="c-36515706">[4 more]</label></div><br/><div class="children"><div class="content">yep, thatâs why itâs free.<p>If it was <i>good</i>, then theyâd charge for it.<p>â¦what they (and everyone) is gonna do is play with smallish models to iterate on the process for relatively small  expense and earn karma.<p>Then pay big $$$ to make a really good model for internal use and&#x2F;or an api that people have to pay for.<p>Tldr; itâs free. Itâs by sales force. You should expect it to be a) crippled and b) a loss leader for a paid product.<p>Not judging; itâs a fair strategy. Just saying: salesforce is not a company that just gives hundreds of thousands of dollars away for nothing.<p>If you want a good free open model, youâre kidding yourself if you think a corporate giant is going to kiss you on the head and give it to your for free.</div><br/><div id="36515904" class="c"><input type="checkbox" id="c-36515904" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515706">parent</a><span>|</span><a href="#36515884">next</a><span>|</span><label class="collapse" for="c-36515904">[-]</label><label class="expand" for="c-36515904">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If you want a good free open model, youâre kidding yourself if you think a corporate giant is going to kiss you on the head and give it to your for free.<p>Yep! That makes sense!<p>I would love to be the CEO of the company that <i>does</i> give away an actually useful model and little forehead kisses though. The amount of goodwill that one would generate from that would be astronomical and training costs are getting so low that nearly any company with enough cash could do it.<p>I look forward to waking up and hearing that the Nabisco&#x2F;Canadian Tire&#x2F;A&amp;W usefully-tuned model is revolutionizing the economy and seeing the infinite amount of good press that it would generate.</div><br/><div id="36516590" class="c"><input type="checkbox" id="c-36516590" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515904">parent</a><span>|</span><a href="#36515884">next</a><span>|</span><label class="collapse" for="c-36516590">[-]</label><label class="expand" for="c-36516590">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI did this when releasing Whisper, but I mostly hear sneers about how they&#x27;re not really open, and no gratitude for the &quot;little kiss&quot;. Given that, I don&#x27;t know that as CEO, I&#x27;d be very benevolent.</div><br/></div></div></div></div><div id="36515884" class="c"><input type="checkbox" id="c-36515884" checked=""/><div class="controls bullet"><span class="by">AuryGlenz</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515706">parent</a><span>|</span><a href="#36515904">prev</a><span>|</span><a href="#36515216">next</a><span>|</span><label class="collapse" for="c-36515884">[-]</label><label class="expand" for="c-36515884">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, Stable Diffusion (especially the upcoming SDXL) are good <i>and</i> free.</div><br/></div></div></div></div></div></div><div id="36515216" class="c"><input type="checkbox" id="c-36515216" checked=""/><div class="controls bullet"><span class="by">jimmyl02</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36515231">prev</a><span>|</span><a href="#36515394">next</a><span>|</span><label class="collapse" for="c-36515216">[-]</label><label class="expand" for="c-36515216">[9 more]</label></div><br/><div class="children"><div class="content">The main use case is that it&#x27;s probably the only size consumers can run on their personal devices. If you don&#x27;t want your data going into an external platform like OpenAI it&#x27;s the only solution even if it&#x27;s not very usuable.</div><br/><div id="36515664" class="c"><input type="checkbox" id="c-36515664" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515216">parent</a><span>|</span><a href="#36515256">next</a><span>|</span><label class="collapse" for="c-36515664">[-]</label><label class="expand" for="c-36515664">[4 more]</label></div><br/><div class="children"><div class="content">You can run big models on the cloud yourself, or with a 3090&#x2F;4090 quantized.<p>You don&#x27;t have to go to openai.</div><br/><div id="36515715" class="c"><input type="checkbox" id="c-36515715" checked=""/><div class="controls bullet"><span class="by">aryamaan</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515664">parent</a><span>|</span><a href="#36515256">next</a><span>|</span><label class="collapse" for="c-36515715">[-]</label><label class="expand" for="c-36515715">[3 more]</label></div><br/><div class="children"><div class="content">Whatâre some models and hardware combos we can run now? I am avoiding to go to OpenAI with my officeâs stuff and can use some gpu(s)</div><br/><div id="36515793" class="c"><input type="checkbox" id="c-36515793" checked=""/><div class="controls bullet"><span class="by">MINIMAN10000</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515715">parent</a><span>|</span><a href="#36515862">next</a><span>|</span><label class="collapse" for="c-36515793">[-]</label><label class="expand" for="c-36515793">[1 more]</label></div><br/><div class="children"><div class="content">You would just need a computer which can fit 2 3090s in order to run those to run something like TheBloke&#x2F;airoboros-65B-gpt4-1.3-GPTQ<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;wiki&#x2F;models&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;wiki&#x2F;models&#x2F;</a> gives you a list of VRAM requirements to load the model into GPU VRAM. the more VRAM the computer has, the larger the model you can load in, thus making 3090s the current consumer grade king due to price to max VRAM.<p>This being said however most models are LLAMA based which all fall under that specific research license.<p>So following the rules, you would be limited to a subset of models which are foundational models which allow for commercial use</div><br/></div></div><div id="36515862" class="c"><input type="checkbox" id="c-36515862" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515715">parent</a><span>|</span><a href="#36515793">prev</a><span>|</span><a href="#36515256">next</a><span>|</span><label class="collapse" for="c-36515862">[-]</label><label class="expand" for="c-36515862">[1 more]</label></div><br/><div class="children"><div class="content">I can easily run LLaMA 13B on my 6GB VRAM&#x2F;16GB RAM laptop using llama.cpp (specifically Kobold.cpp as the frontend).<p>I can <i>barely</i> run 33B, but anything more than 800 context and I oom. But it would run very comfortably on a bigger GPU or a 24GB+ laptop.<p>Theoretically some phones can comfortably handle 13B on mlc-llm though in practice its not really implemented yet.</div><br/></div></div></div></div></div></div><div id="36515256" class="c"><input type="checkbox" id="c-36515256" checked=""/><div class="controls bullet"><span class="by">jahewson</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515216">parent</a><span>|</span><a href="#36515664">prev</a><span>|</span><a href="#36515394">next</a><span>|</span><label class="collapse" for="c-36515256">[-]</label><label class="expand" for="c-36515256">[4 more]</label></div><br/><div class="children"><div class="content">These are constraints but not a use case.</div><br/><div id="36515308" class="c"><input type="checkbox" id="c-36515308" checked=""/><div class="controls bullet"><span class="by">verwkljdslfklj</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515256">parent</a><span>|</span><a href="#36515394">next</a><span>|</span><label class="collapse" for="c-36515308">[-]</label><label class="expand" for="c-36515308">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t it obvious they&#x27;re referring to the use case of using a model given those constraints?</div><br/><div id="36515374" class="c"><input type="checkbox" id="c-36515374" checked=""/><div class="controls bullet"><span class="by">roughly</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515308">parent</a><span>|</span><a href="#36515380">next</a><span>|</span><label class="collapse" for="c-36515374">[-]</label><label class="expand" for="c-36515374">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but I think the responder is wondering if there are useable use cases for that - like, what can you actually Do with that model. Iâm in the same boat - I donât want to ship my data to openai, I do want to run local, so Iâd love to hear what other folks are Doing with models of that size.</div><br/></div></div><div id="36515380" class="c"><input type="checkbox" id="c-36515380" checked=""/><div class="controls bullet"><span class="by">sidibe</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515308">parent</a><span>|</span><a href="#36515374">prev</a><span>|</span><a href="#36515394">next</a><span>|</span><label class="collapse" for="c-36515380">[-]</label><label class="expand" for="c-36515380">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Use case of using&quot; :)</div><br/></div></div></div></div></div></div></div></div><div id="36515394" class="c"><input type="checkbox" id="c-36515394" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36515216">prev</a><span>|</span><a href="#36515408">next</a><span>|</span><label class="collapse" for="c-36515394">[-]</label><label class="expand" for="c-36515394">[3 more]</label></div><br/><div class="children"><div class="content">The use case is people who got some fear of missing out on AI and also they are not capable of seeing the difference between the model outputs themselves. Maybe you will say I&#x27;m elitist but it&#x27;s disturbing to me how many people literally can&#x27;t tell the difference between gpt 3 and gpt 4 outputs, it&#x27;s like the &quot;It&#x27;s the same picture&quot; meme. Maybe that makes me a hipster gpt connoisseur elitist that I can recognize and have opinions about those differences between versions and I hate using the small ones and I think only the very biggest ones are good.</div><br/><div id="36515663" class="c"><input type="checkbox" id="c-36515663" checked=""/><div class="controls bullet"><span class="by">ipaddr</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515394">parent</a><span>|</span><a href="#36515408">next</a><span>|</span><label class="collapse" for="c-36515663">[-]</label><label class="expand" for="c-36515663">[2 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t make you an elitist hipster.  It puts you in a category of most people on earth.  An elist hipster  would swear by a custom smaller parameter model finetuned by a wizard for specific tasks and have a spell book of models.</div><br/><div id="36515817" class="c"><input type="checkbox" id="c-36515817" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515663">parent</a><span>|</span><a href="#36515408">next</a><span>|</span><label class="collapse" for="c-36515817">[-]</label><label class="expand" for="c-36515817">[1 more]</label></div><br/><div class="children"><div class="content">Right, that&#x27;s the Way of the Hipster</div><br/></div></div></div></div></div></div><div id="36515408" class="c"><input type="checkbox" id="c-36515408" checked=""/><div class="controls bullet"><span class="by">steppi</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36515394">prev</a><span>|</span><a href="#36515326">next</a><span>|</span><label class="collapse" for="c-36515408">[-]</label><label class="expand" for="c-36515408">[1 more]</label></div><br/><div class="children"><div class="content">I did some old school NLP before, but donât really work in the field anymore. As a generative model. As a general purpose generative model, maybe this isnât very useful. As a foundation to make models to perform text classification and information extraction tasks, this could be very useful. For these kinds of tasks, you can still get good results with the classic bag of words type approaches people were using 30 years ago even. I remember when transformers first came out, limitations in sequence size made them unusable for some classification tasks.</div><br/></div></div><div id="36515326" class="c"><input type="checkbox" id="c-36515326" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36515408">prev</a><span>|</span><a href="#36516867">next</a><span>|</span><label class="collapse" for="c-36515326">[-]</label><label class="expand" for="c-36515326">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re only modestly worse than text-davinci-003 in my experience, and you can finetune them cheaply to do simple tasks e.g. triage human input and decide where to send it. But yeah, if you can afford to run a larger model or pay OpenAI for gpt4 calls, that&#x27;s gonna work a lot better.<p>If you&#x27;re relying on prompting for the 7B models IMO you&#x27;re gonna have a bad time â they&#x27;re mostly toys at that: interesting output but not consistently useful. But finetuning gets better results, and it&#x27;s cheap to finetune.</div><br/></div></div><div id="36516867" class="c"><input type="checkbox" id="c-36516867" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36515326">prev</a><span>|</span><a href="#36515785">next</a><span>|</span><label class="collapse" for="c-36516867">[-]</label><label class="expand" for="c-36516867">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had very good results with even a 3.5 B model (Fastchat-T5) for retrieval augmented generation (aka putting information into the context window and letting the model rephrase it).</div><br/></div></div><div id="36515785" class="c"><input type="checkbox" id="c-36515785" checked=""/><div class="controls bullet"><span class="by">makestuff</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36516867">prev</a><span>|</span><a href="#36515251">next</a><span>|</span><label class="collapse" for="c-36515785">[-]</label><label class="expand" for="c-36515785">[1 more]</label></div><br/><div class="children"><div class="content">I feel like this would work well as a local LLM for a home assistant type setup. Fully local instead of having Alexa send everything to the cloud.</div><br/></div></div><div id="36515251" class="c"><input type="checkbox" id="c-36515251" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#36515200">parent</a><span>|</span><a href="#36515785">prev</a><span>|</span><a href="#36516814">next</a><span>|</span><label class="collapse" for="c-36515251">[-]</label><label class="expand" for="c-36515251">[3 more]</label></div><br/><div class="children"><div class="content">Bert had lots of use cases and this one is supposedly stronger model.</div><br/><div id="36515265" class="c"><input type="checkbox" id="c-36515265" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515251">parent</a><span>|</span><a href="#36516814">next</a><span>|</span><label class="collapse" for="c-36515265">[-]</label><label class="expand" for="c-36515265">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used BERT in a number of production apps, it feels like a very apples to oranges comparison given how the AI landscape has changed since BERT&#x27;s release.</div><br/><div id="36515289" class="c"><input type="checkbox" id="c-36515289" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#36515200">root</a><span>|</span><a href="#36515265">parent</a><span>|</span><a href="#36516814">next</a><span>|</span><label class="collapse" for="c-36515289">[-]</label><label class="expand" for="c-36515289">[1 more]</label></div><br/><div class="children"><div class="content">Then looks like you know already where smaller and weaker models are useful.</div><br/></div></div></div></div></div></div></div></div><div id="36516814" class="c"><input type="checkbox" id="c-36516814" checked=""/><div class="controls bullet"><span class="by">profsummergig</span><span>|</span><a href="#36515200">prev</a><span>|</span><a href="#36515436">next</a><span>|</span><label class="collapse" for="c-36516814">[-]</label><label class="expand" for="c-36516814">[9 more]</label></div><br/><div class="children"><div class="content">If someone could elucidate on what these phrases signify, I&#x27;d be very grateful:<p>1) 7B foundational model<p>2) 8K length<p>3) 1.5T tokens</div><br/><div id="36517243" class="c"><input type="checkbox" id="c-36517243" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#36516814">parent</a><span>|</span><a href="#36517678">next</a><span>|</span><label class="collapse" for="c-36517243">[-]</label><label class="expand" for="c-36517243">[1 more]</label></div><br/><div class="children"><div class="content">A (overly) simplified explanation:<p>- 7B means 7 billions parameters.<p>- 8K length means the size of input&#x2F;output is 8K tokens.<p>- 1.5T tokens mean the training set has 1.5T tokens.<p>A: What&#x27;s a parameter?<p>Q: More parameters your model has, more complex relationship it can represent. For example let&#x27;s say you have a function f(x). This is a 2-parameter model:<p>f(x) = ax + b<p>This is a 4 parameter model:<p>f(x) = ax^3 + bx^2 + cx + d<p>As you can see as the number of parameters grows, the function is able to represent more complex relationship between f(x) and x.<p>A: What&#x27;s a token?<p>Token is a way to encode text, like ASCII or Unicode. Unlike Unicode, tokenizor usually favors common combinations of alphabets. For example, &quot;the&quot; is a single token for GPT-3 tokenizor, but &quot;eht&quot; is two tokens (e and ht).<p>* Note that the number of parameters is more like an &quot;upper limit&quot; of the model&#x27;s capabilities. If your a, b, c, d are just random shit, it&#x27;s still a 4-parameter model, but it&#x27;s still useless. The whole concept of &quot;training&quot; is just &quot;finding the best parameters&quot;.</div><br/></div></div><div id="36517678" class="c"><input type="checkbox" id="c-36517678" checked=""/><div class="controls bullet"><span class="by">knaik94</span><span>|</span><a href="#36516814">parent</a><span>|</span><a href="#36517243">prev</a><span>|</span><a href="#36517524">next</a><span>|</span><label class="collapse" for="c-36517678">[-]</label><label class="expand" for="c-36517678">[1 more]</label></div><br/><div class="children"><div class="content">1) 7B foundational model means this is a base model that has not been fine-tuned on the &quot;prompt: response:&quot; structure, and it has 7 billion weights&#x2F;biases.<p>2) Currently every model that can run locally was trained with a 2K context size. It&#x27;s a hard limit on prompt length. There have been recent advances with [A] position interpolation, but those methods explore fine-tuning&#x2F;loras. This base model was trained with 8k sequences.<p>3. 1.5T tokens is the size of the total training corpus. Training cost and time scale increase with training size. [B]<p>A. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.15595" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.15595</a><p>B. <a href="https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;the-ai-brick-wall-a-practical-limit" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;the-ai-brick-wall-a-practical...</a>  (Jan 2023)</div><br/></div></div><div id="36517524" class="c"><input type="checkbox" id="c-36517524" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36516814">parent</a><span>|</span><a href="#36517678">prev</a><span>|</span><a href="#36516897">next</a><span>|</span><label class="collapse" for="c-36517524">[-]</label><label class="expand" for="c-36517524">[1 more]</label></div><br/><div class="children"><div class="content">Here are some high level answers:<p>&quot;7B&quot; refers to the number of parameters or weights for a model. For a specific model, the versions with more parameters take more compute power to train and perform better.<p>A foundational model is the part of a ML model that is &quot;pretrained&quot; on a massive data set (and usually is the bulk of the compute cost). This is usually considered the &quot;raw&quot; model after which it is fine-tuned for specific tasks (turned into a chatbot).<p>&quot;8K length&quot; refers to the Context Window length (in tokens). This is basically an LLM&#x27;s short term memory - you can think of it as its attention span and what it can generate reasonable output for.<p>&quot;1.5T tokens&quot; refers to the size of the corpus of the training set.<p>In general Wikipedia (or I suppose ChatGPT 4&#x2F;Bing Chat with Web Browsing) is a decent enough place to start reading&#x2F;asking basic questions. I&#x27;d recommend starting here: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Large_language_model" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Large_language_model</a> and finding the related concepts.<p>For those going deeper, there are lot of general resources lists like <a href="https:&#x2F;&#x2F;github.com&#x2F;Hannibal046&#x2F;Awesome-LLM">https:&#x2F;&#x2F;github.com&#x2F;Hannibal046&#x2F;Awesome-LLM</a> or <a href="https:&#x2F;&#x2F;github.com&#x2F;Mooler0410&#x2F;LLMsPracticalGuide">https:&#x2F;&#x2F;github.com&#x2F;Mooler0410&#x2F;LLMsPracticalGuide</a> or one I like, <a href="https:&#x2F;&#x2F;sebastianraschka.com&#x2F;blog&#x2F;2023&#x2F;llm-reading-list.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;sebastianraschka.com&#x2F;blog&#x2F;2023&#x2F;llm-reading-list.html</a> (there are a bajillion of these and you&#x27;ll find more once you get a grasp on the terms you want to surf for). Almost everything is published on arXiv, and most is fairly readable even as a layman.<p>For non-ML programmers looking to get up to speed, I feel like Karpathy&#x27;s Zero to Hero&#x2F;nanoGPT or Jay Mody&#x27;s picoGPT <a href="https:&#x2F;&#x2F;jaykmody.com&#x2F;blog&#x2F;gpt-from-scratch&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;jaykmody.com&#x2F;blog&#x2F;gpt-from-scratch&#x2F;</a> are alternative&#x2F;maybe a better way to understand the basic concepts on a practical level.</div><br/></div></div><div id="36516863" class="c"><input type="checkbox" id="c-36516863" checked=""/><div class="controls bullet"><span class="by">going_ham</span><span>|</span><a href="#36516814">parent</a><span>|</span><a href="#36516897">prev</a><span>|</span><a href="#36515436">next</a><span>|</span><label class="collapse" for="c-36516863">[-]</label><label class="expand" for="c-36516863">[4 more]</label></div><br/><div class="children"><div class="content">1. The trained model has 7B parameters or weights for each neuron.<p>2. It can handle upto 8k tokens. Tokens are usually some representation for a word. If your tokens are characters then, &quot;h&quot;, &quot;e&quot;, &quot;y&quot; represent 3 tokens for hey. Most of the algos use byte pair encoding. For example &quot;hand-le&quot; has two tokens &quot;hand&quot; and &quot;le&quot;. This is a very crud example which is enough to give the gist but is not accurate. You can look into byte pair encoding for more details.<p>3. The token size 1.5T token means they have huge variations for input and output. Simply put, it was trained on large data corpus.<p>I hope this simplifies it. You can research further if you are interested! Hope it helps!</div><br/><div id="36517018" class="c"><input type="checkbox" id="c-36517018" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36516814">root</a><span>|</span><a href="#36516863">parent</a><span>|</span><a href="#36517065">next</a><span>|</span><label class="collapse" for="c-36517018">[-]</label><label class="expand" for="c-36517018">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t just post ChatGPT answers as comments on hackernews.<p>This one doesn&#x27;t even make any sense. Of course it doesn&#x27;t have 7B parameters _per_ neuron.</div><br/><div id="36517083" class="c"><input type="checkbox" id="c-36517083" checked=""/><div class="controls bullet"><span class="by">johanvts</span><span>|</span><a href="#36516814">root</a><span>|</span><a href="#36517018">parent</a><span>|</span><a href="#36517065">next</a><span>|</span><label class="collapse" for="c-36517083">[-]</label><label class="expand" for="c-36517083">[1 more]</label></div><br/><div class="children"><div class="content">Donât be too critical dr. Ahle. Maybe itâs a new single-neuron architecture.</div><br/></div></div></div></div><div id="36517065" class="c"><input type="checkbox" id="c-36517065" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#36516814">root</a><span>|</span><a href="#36516863">parent</a><span>|</span><a href="#36517018">prev</a><span>|</span><a href="#36515436">next</a><span>|</span><label class="collapse" for="c-36517065">[-]</label><label class="expand" for="c-36517065">[1 more]</label></div><br/><div class="children"><div class="content">This is a good example on why StackOverflow banned ChatGPT-generated answers.</div><br/></div></div></div></div></div></div><div id="36515436" class="c"><input type="checkbox" id="c-36515436" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36516814">prev</a><span>|</span><a href="#36515205">next</a><span>|</span><label class="collapse" for="c-36515436">[-]</label><label class="expand" for="c-36515436">[1 more]</label></div><br/><div class="children"><div class="content">Also, their metric table is very interesting. It shows Falcon 7B and OpenLlama 7B much less favorably than other evaluations (including the HuggingFace leaderboard, which I am kinda suspicious of), and instruct benchmarks like that aren&#x27;t seen as much.</div><br/></div></div><div id="36515205" class="c"><input type="checkbox" id="c-36515205" checked=""/><div class="controls bullet"><span class="by">DanAtC</span><span>|</span><a href="#36515436">prev</a><span>|</span><a href="#36516271">next</a><span>|</span><label class="collapse" for="c-36515205">[-]</label><label class="expand" for="c-36515205">[10 more]</label></div><br/><div class="children"><div class="content">I have no idea what any of these words mean, but I&#x27;d like to. Can someone point me in the direction of an &quot;AI for Dipshits&quot;?</div><br/><div id="36515432" class="c"><input type="checkbox" id="c-36515432" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#36515205">parent</a><span>|</span><a href="#36515229">next</a><span>|</span><label class="collapse" for="c-36515432">[-]</label><label class="expand" for="c-36515432">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an open source language model with seven billion parameters (a measure of its complexity), and a longer than typical sequence length of 8K, which allows the provision of more context when querying the model. For example, it allows you to better generate text in someone else&#x27;s voice by providing a longer example of their work.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Foundation_models" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Foundation_models</a></div><br/><div id="36515753" class="c"><input type="checkbox" id="c-36515753" checked=""/><div class="controls bullet"><span class="by">sacnoradhq</span><span>|</span><a href="#36515205">root</a><span>|</span><a href="#36515432">parent</a><span>|</span><a href="#36515698">next</a><span>|</span><label class="collapse" for="c-36515753">[-]</label><label class="expand" for="c-36515753">[2 more]</label></div><br/><div class="children"><div class="content">The number of parameters seems meaningless when the training sets are dogshit and hardened old gum chipped from the shoes of Gregslist and FleaBay.<p>OTOH, there ought to be a construction in the form of an web app that can pinch out nonrepetitive, coherent ~100 page trashy romance novels in the style of any author given name with open source or specific text(s) or transcripts with enough original input volume: Churchill, The Unabomber, psycho happy kindergarten child development IEP manual writer, The Dude, Walter (agro gun nut), Bob Ross, Grace Hopper, Ayn Rand, LBJ, The Dalai LaMa%, Hitler, Kanye (Ye), Bhad Bhabie, and the King James Bible. Ethical and generational safety features be damned; it&#x27;d be generating fucking^2 art for hilarious entertainment purposes. How does one stretch training input to something that might involve human&#x2F;computer output validation to discard sticking on repetitive nonsense?<p>% <i>He never saw that one coming</i> Ow^(3 + i).</div><br/><div id="36515868" class="c"><input type="checkbox" id="c-36515868" checked=""/><div class="controls bullet"><span class="by">scubbo</span><span>|</span><a href="#36515205">root</a><span>|</span><a href="#36515753">parent</a><span>|</span><a href="#36515698">next</a><span>|</span><label class="collapse" for="c-36515868">[-]</label><label class="expand" for="c-36515868">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any recommendations for cryptocoins?</div><br/></div></div></div></div><div id="36515698" class="c"><input type="checkbox" id="c-36515698" checked=""/><div class="controls bullet"><span class="by">luxuryballs</span><span>|</span><a href="#36515205">root</a><span>|</span><a href="#36515432">parent</a><span>|</span><a href="#36515753">prev</a><span>|</span><a href="#36515229">next</a><span>|</span><label class="collapse" for="c-36515698">[-]</label><label class="expand" for="c-36515698">[1 more]</label></div><br/><div class="children"><div class="content">dang so this means that just like my 30yo 4GB hard drive is to my 4GB RAM phoneâ¦ we ainât seen nothing yet if weâre still counting these metrics?</div><br/></div></div></div></div><div id="36515229" class="c"><input type="checkbox" id="c-36515229" checked=""/><div class="controls bullet"><span class="by">seeknotfind</span><span>|</span><a href="#36515205">parent</a><span>|</span><a href="#36515432">prev</a><span>|</span><a href="#36516271">next</a><span>|</span><label class="collapse" for="c-36515229">[-]</label><label class="expand" for="c-36515229">[5 more]</label></div><br/><div class="children"><div class="content">Why not ask the AIs themselves? They are pretty good at explaining this type of thing.</div><br/><div id="36515532" class="c"><input type="checkbox" id="c-36515532" checked=""/><div class="controls bullet"><span class="by">deedree</span><span>|</span><a href="#36515205">root</a><span>|</span><a href="#36515229">parent</a><span>|</span><a href="#36515338">next</a><span>|</span><label class="collapse" for="c-36515532">[-]</label><label class="expand" for="c-36515532">[3 more]</label></div><br/><div class="children"><div class="content">But if you don&#x27;t know a certain amount about a subject already you won&#x27;t know when it&#x27;s lying to you. That would probably be the case here.</div><br/><div id="36516277" class="c"><input type="checkbox" id="c-36516277" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#36515205">root</a><span>|</span><a href="#36515532">parent</a><span>|</span><a href="#36515338">next</a><span>|</span><label class="collapse" for="c-36516277">[-]</label><label class="expand" for="c-36516277">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s basically what an interactive dialogue with Wikipedia would look like, which is still a darned useful thing.</div><br/><div id="36517284" class="c"><input type="checkbox" id="c-36517284" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36515205">root</a><span>|</span><a href="#36516277">parent</a><span>|</span><a href="#36515338">next</a><span>|</span><label class="collapse" for="c-36517284">[-]</label><label class="expand" for="c-36517284">[1 more]</label></div><br/><div class="children"><div class="content">If Wikipedia had the conviction and focus of a 4 year old. But yeah, while tricky, it&#x27;s still usually net positive.</div><br/></div></div></div></div></div></div><div id="36515338" class="c"><input type="checkbox" id="c-36515338" checked=""/><div class="controls bullet"><span class="by">dcre</span><span>|</span><a href="#36515205">root</a><span>|</span><a href="#36515229">parent</a><span>|</span><a href="#36515532">prev</a><span>|</span><a href="#36516271">next</a><span>|</span><label class="collapse" for="c-36515338">[-]</label><label class="expand" for="c-36515338">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a joke but it&#x27;s true.</div><br/></div></div></div></div></div></div><div id="36516271" class="c"><input type="checkbox" id="c-36516271" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36515205">prev</a><span>|</span><a href="#36515162">next</a><span>|</span><label class="collapse" for="c-36516271">[-]</label><label class="expand" for="c-36516271">[2 more]</label></div><br/><div class="children"><div class="content">Per the validation perplexity chart shown, the 8K length model performs better than the 4K length model even at &lt;4K length, so why are they even offering the 4K model if the 8K is strictly better?</div><br/><div id="36517815" class="c"><input type="checkbox" id="c-36517815" checked=""/><div class="controls bullet"><span class="by">numeri</span><span>|</span><a href="#36516271">parent</a><span>|</span><a href="#36515162">next</a><span>|</span><label class="collapse" for="c-36517815">[-]</label><label class="expand" for="c-36517815">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say for research purposes. HackerNews seems to tend to mostly represent the LLM consumer viewpoint, but these waves of models being released are honestly more interesting from a research than a user point of view. As a LLM user, you&#x27;re (vast generalization&#x2F;simplification) really just interested in the best of N models, but as a researcher, I&#x27;m super interested in each model&#x27;s performance and analyzing the reasons for any differences.<p>With this model (and they say this in the blog post), they were testing the hypothesis that training on a longer context size would provide more performance at the same parameter count&#x2F;inference FLOPs. From a quick perusal of their post, it looks like this was true, and we should train all future models with as long of a context size as we can afford.</div><br/></div></div></div></div><div id="36515162" class="c"><input type="checkbox" id="c-36515162" checked=""/><div class="controls bullet"><span class="by">foolfoolz</span><span>|</span><a href="#36516271">prev</a><span>|</span><label class="collapse" for="c-36515162">[-]</label><label class="expand" for="c-36515162">[13 more]</label></div><br/><div class="children"><div class="content">when will the llm race peak? have we peaked already?</div><br/><div id="36515207" class="c"><input type="checkbox" id="c-36515207" checked=""/><div class="controls bullet"><span class="by">DantesKite</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36515495">next</a><span>|</span><label class="collapse" for="c-36515207">[-]</label><label class="expand" for="c-36515207">[2 more]</label></div><br/><div class="children"><div class="content">If youâre curious, you can check the progress of many open source LLMâs and how they perform on various evals here:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderboard" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderb...</a></div><br/><div id="36517126" class="c"><input type="checkbox" id="c-36517126" checked=""/><div class="controls bullet"><span class="by">entrep</span><span>|</span><a href="#36515162">root</a><span>|</span><a href="#36515207">parent</a><span>|</span><a href="#36515495">next</a><span>|</span><label class="collapse" for="c-36517126">[-]</label><label class="expand" for="c-36517126">[1 more]</label></div><br/><div class="children"><div class="content">With proprietary models:
<a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?leaderboard" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?leaderboard</a></div><br/></div></div></div></div><div id="36515495" class="c"><input type="checkbox" id="c-36515495" checked=""/><div class="controls bullet"><span class="by">bratao</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36515207">prev</a><span>|</span><a href="#36515488">next</a><span>|</span><label class="collapse" for="c-36515495">[-]</label><label class="expand" for="c-36515495">[1 more]</label></div><br/><div class="children"><div class="content">I think we are only at the beginning. Call it first generation. LLaMA was even missing many improvements that existed before it (xPos, Multi-Query Attention, Blockwise Parallel Transformer)<p>Many researchers are improving very fast, and I would bet that soon we will see more efficient LLMs.</div><br/></div></div><div id="36515488" class="c"><input type="checkbox" id="c-36515488" checked=""/><div class="controls bullet"><span class="by">sacnoradhq</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36515495">prev</a><span>|</span><a href="#36517526">next</a><span>|</span><label class="collapse" for="c-36515488">[-]</label><label class="expand" for="c-36515488">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think we need anymore high-flying search engines, do we?<p><i>salesforce air esearch</i> ;)</div><br/></div></div><div id="36517526" class="c"><input type="checkbox" id="c-36517526" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36515488">prev</a><span>|</span><a href="#36515261">next</a><span>|</span><label class="collapse" for="c-36517526">[-]</label><label class="expand" for="c-36517526">[1 more]</label></div><br/><div class="children"><div class="content">In terms of Gartner hype cycle,  we are at the peak of inflated expectations for llms   I reckon.</div><br/></div></div><div id="36515261" class="c"><input type="checkbox" id="c-36515261" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36517526">prev</a><span>|</span><a href="#36515188">next</a><span>|</span><label class="collapse" for="c-36515261">[-]</label><label class="expand" for="c-36515261">[1 more]</label></div><br/><div class="children"><div class="content">Weâre still in the early innings.</div><br/></div></div><div id="36515188" class="c"><input type="checkbox" id="c-36515188" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36515261">prev</a><span>|</span><a href="#36515566">next</a><span>|</span><label class="collapse" for="c-36515188">[-]</label><label class="expand" for="c-36515188">[1 more]</label></div><br/><div class="children"><div class="content">Long way from that</div><br/></div></div><div id="36515566" class="c"><input type="checkbox" id="c-36515566" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36515188">prev</a><span>|</span><a href="#36515238">next</a><span>|</span><label class="collapse" for="c-36515566">[-]</label><label class="expand" for="c-36515566">[1 more]</label></div><br/><div class="children"><div class="content">When it&#x27;s superhuman.</div><br/></div></div><div id="36515238" class="c"><input type="checkbox" id="c-36515238" checked=""/><div class="controls bullet"><span class="by">rasz</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36515566">prev</a><span>|</span><a href="#36515761">next</a><span>|</span><label class="collapse" for="c-36515238">[-]</label><label class="expand" for="c-36515238">[3 more]</label></div><br/><div class="children"><div class="content">Nobody doing xB models is participating in any AI races, at this point those are useless toys with garbage output.</div><br/><div id="36515558" class="c"><input type="checkbox" id="c-36515558" checked=""/><div class="controls bullet"><span class="by">happycube</span><span>|</span><a href="#36515162">root</a><span>|</span><a href="#36515238">parent</a><span>|</span><a href="#36515761">next</a><span>|</span><label class="collapse" for="c-36515558">[-]</label><label class="expand" for="c-36515558">[2 more]</label></div><br/><div class="children"><div class="content">How far we&#x27;ve come from GPT2...</div><br/><div id="36517304" class="c"><input type="checkbox" id="c-36517304" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36515162">root</a><span>|</span><a href="#36515558">parent</a><span>|</span><a href="#36515761">next</a><span>|</span><label class="collapse" for="c-36517304">[-]</label><label class="expand" for="c-36517304">[1 more]</label></div><br/><div class="children"><div class="content">Biggest models crossed the threshold between glorified Markov chain and proto-intelligence, so sure enough, expectations shoot up.</div><br/></div></div></div></div></div></div><div id="36515761" class="c"><input type="checkbox" id="c-36515761" checked=""/><div class="controls bullet"><span class="by">phillnom</span><span>|</span><a href="#36515162">parent</a><span>|</span><a href="#36515238">prev</a><span>|</span><label class="collapse" for="c-36515761">[-]</label><label class="expand" for="c-36515761">[1 more]</label></div><br/><div class="children"><div class="content">To quote Dennis Reynolds, it hasn&#x27;t even begun to peak.</div><br/></div></div></div></div></div></div></div></div></div></body></html>