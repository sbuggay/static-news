<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1723885262457" as="style"/><link rel="stylesheet" href="styles.css?v=1723885262457"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://mpmisko.github.io/2024/ai-fundamentals-energy-based-models/">Fun times with energy-based models</a> <span class="domain">(<a href="https://mpmisko.github.io">mpmisko.github.io</a>)</span></div><div class="subtext"><span>mpmisko</span> | <span>12 comments</span></div><br/><div><div id="41271550" class="c"><input type="checkbox" id="c-41271550" checked=""/><div class="controls bullet"><span class="by">blt</span><span>|</span><a href="#41272797">next</a><span>|</span><label class="collapse" for="c-41271550">[-]</label><label class="expand" for="c-41271550">[2 more]</label></div><br/><div class="children"><div class="content">If the author is reading: In the proof, at the end of Step 6, it&#x27;s confusing that the &quot;uv&quot; term of the integration by parts is suddenly given a range from -∞ to ∞, as if we had previously assumed x ∈ ℝ. But elsewhere in the article, including the examples, we have higher-dimensional x&#x27;s. I suggest to either 1) include the full multidimensional version from the paper, or 2) explicitly mention that this is the simple 1d case, the same result holds in ℝ^n, and refer to the paper.</div><br/><div id="41272527" class="c"><input type="checkbox" id="c-41272527" checked=""/><div class="controls bullet"><span class="by">cshimmin</span><span>|</span><a href="#41271550">parent</a><span>|</span><a href="#41272797">next</a><span>|</span><label class="collapse" for="c-41272527">[-]</label><label class="expand" for="c-41272527">[1 more]</label></div><br/><div class="children"><div class="content">In the multidimensional case of integration by parts, the limit of integration is understood to be the (d-1 dimensional) boundary _at infinity _, so writing +&#x2F;-inf is a reasonable shorthand. In almost all cases, it’s used when the term uv can be assumed to be zero (or at least constant) at this boundary.</div><br/></div></div></div></div><div id="41272797" class="c"><input type="checkbox" id="c-41272797" checked=""/><div class="controls bullet"><span class="by">stubbi</span><span>|</span><a href="#41271550">prev</a><span>|</span><a href="#41271027">next</a><span>|</span><label class="collapse" for="c-41272797">[-]</label><label class="expand" for="c-41272797">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. Since I studied them during my Masters I feel in the longer term EBMs will be the way forward for AI</div><br/></div></div><div id="41271027" class="c"><input type="checkbox" id="c-41271027" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#41272797">prev</a><span>|</span><a href="#41271735">next</a><span>|</span><label class="collapse" for="c-41271027">[-]</label><label class="expand" for="c-41271027">[7 more]</label></div><br/><div class="children"><div class="content">I think the rationale for using tricks like score matching and contrastive divergence deserves a mention: the partition function is computationally expensive.<p>Since we&#x27;re on the subject, what are EBMs good for today?</div><br/><div id="41271493" class="c"><input type="checkbox" id="c-41271493" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#41271027">parent</a><span>|</span><a href="#41271428">next</a><span>|</span><label class="collapse" for="c-41271493">[-]</label><label class="expand" for="c-41271493">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re good for reinforcement learning. E.g. Cicero uses piKL which samples according to<p>p ∝ anchor_policy * exp(utility &#x2F; temperature)<p>The utility is exactly the same as &quot;energy&quot;. The article ignores entropy, but you can add in entropy regularization e.g. in soft actor-critic.</div><br/></div></div><div id="41271428" class="c"><input type="checkbox" id="c-41271428" checked=""/><div class="controls bullet"><span class="by">mpmisko</span><span>|</span><a href="#41271027">parent</a><span>|</span><a href="#41271493">prev</a><span>|</span><a href="#41271528">next</a><span>|</span><label class="collapse" for="c-41271428">[-]</label><label class="expand" for="c-41271428">[1 more]</label></div><br/><div class="children"><div class="content">EBMs show up all over the place, apparently even your classifier is an EBM :) (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.03263" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.03263</a>).</div><br/></div></div><div id="41271528" class="c"><input type="checkbox" id="c-41271528" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41271027">parent</a><span>|</span><a href="#41271428">prev</a><span>|</span><a href="#41271482">next</a><span>|</span><label class="collapse" for="c-41271528">[-]</label><label class="expand" for="c-41271528">[1 more]</label></div><br/><div class="children"><div class="content">Also: you are free to model p(x) without worrying about normalization, something that would be required to maximize likelihood.</div><br/></div></div><div id="41271380" class="c"><input type="checkbox" id="c-41271380" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41271027">parent</a><span>|</span><a href="#41271482">prev</a><span>|</span><a href="#41271735">next</a><span>|</span><label class="collapse" for="c-41271380">[-]</label><label class="expand" for="c-41271380">[2 more]</label></div><br/><div class="children"><div class="content">This paper lists the benefits in the introduction: <a href="https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2019&#x2F;file&#x2F;378a063b8fdb1db941e34f4bde584c7d-Paper.pdf" rel="nofollow">https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2019&#x2F;file&#x2F;3...</a><p>- Simplicity and Stability: An EBM is the only object that needs to be trained and designed. Separate networks are not tuned to ensure balance.<p>- Sharing of Statistical Strength: Since the EBM is the only trained object, it requires fewer model parameters than approaches that use multiple networks.<p>- Adaptive Computation Time: Implicit sample generation is an iterative stochastic optimization process, which allows for a trade-off between generation quality and computation time.<p>- VAEs and flow-based models are bound by the manifold structure
of the prior distribution and consequently have issues modelling discontinuous data manifolds, often assigning probability mass to areas unwarranted by the data. EBMs avoid this issue by directly modelling particular regions as high or lower energy.<p>- Compositionality: If we think of energy functions as costs for a certain goals or constraints, summation of two or more energies corresponds to satisfying all their goals or constraints.</div><br/><div id="41271511" class="c"><input type="checkbox" id="c-41271511" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#41271027">root</a><span>|</span><a href="#41271380">parent</a><span>|</span><a href="#41271735">next</a><span>|</span><label class="collapse" for="c-41271511">[-]</label><label class="expand" for="c-41271511">[1 more]</label></div><br/><div class="children"><div class="content">As far as I can tell, flow-based models are bound by the exact same requirements as energy based models (flow = diffusion&#x2F;normalizing flow&#x2F;flow-matching models). But they&#x27;re absolutely right about VAEs. Those are a memetic virus that need to die off in favor of more theoretically grounded encoders.</div><br/></div></div></div></div></div></div><div id="41271735" class="c"><input type="checkbox" id="c-41271735" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#41271027">prev</a><span>|</span><label class="collapse" for="c-41271735">[-]</label><label class="expand" for="c-41271735">[1 more]</label></div><br/><div class="children"><div class="content">We are working on a startup that is revisiting the math that underlies EBMs. If you want to work with us or invest, check out these links<p><a href="http:&#x2F;&#x2F;traceoid.ai" rel="nofollow">http:&#x2F;&#x2F;traceoid.ai</a><p><a href="https:&#x2F;&#x2F;x.com&#x2F;adamnemecek1&#x2F;status&#x2F;1822727041399328839" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;adamnemecek1&#x2F;status&#x2F;1822727041399328839</a></div><br/></div></div></div></div></div></div></div></body></html>