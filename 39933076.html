<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712307649518" as="style"/><link rel="stylesheet" href="styles.css?v=1712307649518"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://research.myshell.ai/jetmoe">JetMoE: Reaching LLaMA2 performance with 0.1M dollars</a> <span class="domain">(<a href="https://research.myshell.ai">research.myshell.ai</a>)</span></div><div class="subtext"><span>gyre007</span> | <span>87 comments</span></div><br/><div><div id="39934483" class="c"><input type="checkbox" id="c-39934483" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#39934253">next</a><span>|</span><label class="collapse" for="c-39934483">[-]</label><label class="expand" for="c-39934483">[41 more]</label></div><br/><div class="children"><div class="content">&gt; JetMoE-8B is trained with less than $ 0.1 million1 cost but outperforms LLaMA2-7B from Meta AI, who has multi-billion-dollar training resources. LLM training can be much cheaper than people generally thought.<p>They want you to read this as &quot;we spent $100k compared to Meta&#x27;s spending billions&quot;, but that&#x27;s not actually what this says. It says that they spent $100k and Meta <i>has the resources</i> to spend billions if they wanted to.<p>We don&#x27;t know what Facebook spent on training LLaMA 2, but they say that it took them 184320 A100-80GB GPU-hours to train the 7B model [0]. AWS charges $14.46&#x2F;hour for an instance that has 8 of those [1], which amounts to $1.81&#x2F;GPU&#x2F;hr.<p>At that rate and assuming they paid something resembling AWS&#x27;s list price, LLaMA 2 7B cost ~$333k. That&#x27;s more than $100k, but not by orders of magnitude, and it&#x27;s likely that Facebook wasn&#x27;t paying the full price AWS is charging today.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama&#x2F;blob&#x2F;main&#x2F;MODEL_CARD.md#hardware-and-software">https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama&#x2F;blob&#x2F;main&#x2F;MODEL_CARD.md#...</a><p>[1] <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;p4&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;p4&#x2F;</a></div><br/><div id="39934873" class="c"><input type="checkbox" id="c-39934873" checked=""/><div class="controls bullet"><span class="by">DalasNoin</span><span>|</span><a href="#39934483">parent</a><span>|</span><a href="#39934805">next</a><span>|</span><label class="collapse" for="c-39934873">[-]</label><label class="expand" for="c-39934873">[12 more]</label></div><br/><div class="children"><div class="content">This entire difference can be explained due to their double mixture of experts architecture. So only 1&#x2F;4 MLP and attention blocks are used at any time. Maybe this should be the headline, Moe reduces compute by a factor of 4 without losing accuracy. But this is already known. Still interesting to see a smaller Moe model. This could be the ideal size for many local applications.</div><br/><div id="39936471" class="c"><input type="checkbox" id="c-39936471" checked=""/><div class="controls bullet"><span class="by">Centigonal</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934873">parent</a><span>|</span><a href="#39936605">next</a><span>|</span><label class="collapse" for="c-39936471">[-]</label><label class="expand" for="c-39936471">[6 more]</label></div><br/><div class="children"><div class="content">MoE reduces compute cost for inference at scale, but not for training. You still have to train the whole model (plus the router)</div><br/><div id="39938395" class="c"><input type="checkbox" id="c-39938395" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936471">parent</a><span>|</span><a href="#39937015">next</a><span>|</span><label class="collapse" for="c-39938395">[-]</label><label class="expand" for="c-39938395">[1 more]</label></div><br/><div class="children"><div class="content">It’s absolutely beneficial when training because the forward pass and back propagation is still only on the neurons that were activated.<p>The Mistral guys specifically mention that training speed (due to not needing as much compute) was one of the reasons Mixtral was released so soon after Mistal 7b.</div><br/></div></div><div id="39937015" class="c"><input type="checkbox" id="c-39937015" checked=""/><div class="controls bullet"><span class="by">rileyphone</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936471">parent</a><span>|</span><a href="#39938395">prev</a><span>|</span><a href="#39936605">next</a><span>|</span><label class="collapse" for="c-39937015">[-]</label><label class="expand" for="c-39937015">[4 more]</label></div><br/><div class="children"><div class="content">With an MoE you only need to train a smaller model which you can then combine into an x8 and finetune&#x2F;train the router. Mistral used their 7B base to make Mixtral, Qwen&#x27;s new MoE uses their 1.8B model upscaled to 2.7B, pretty sure Grok also trained a smaller model first.</div><br/><div id="39937699" class="c"><input type="checkbox" id="c-39937699" checked=""/><div class="controls bullet"><span class="by">phree_radical</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39937015">parent</a><span>|</span><a href="#39939730">next</a><span>|</span><label class="collapse" for="c-39937699">[-]</label><label class="expand" for="c-39937699">[2 more]</label></div><br/><div class="children"><div class="content">Very incorrect!  The &quot;8x7b&quot; in the name regularly confuses people into some similar conclusion, but there are not eight 7b &quot;experts&quot; in Mixtral 8x.  It&#x27;s more apt to think of all 256 FFN&#x27;s as the &quot;experts,&quot; as each expert FFN on a given layer has no relation to the expert FFN&#x27;s on other layers.  You need to train them all within the MoE architecture, as combining existing models (&quot;clown car MoE&quot;) works, but isn&#x27;t gaining anything from the architecture&#x2F;sparsity</div><br/><div id="39940182" class="c"><input type="checkbox" id="c-39940182" checked=""/><div class="controls bullet"><span class="by">epups</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39937699">parent</a><span>|</span><a href="#39939730">next</a><span>|</span><label class="collapse" for="c-39940182">[-]</label><label class="expand" for="c-39940182">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, could you expand on this a bit further? Are you saying that for a MoE, you want to train the exact same model, and then just finetune the feed forward networks differently for each of them? And you&#x27;re saying that separately training 8 different models would not be efficient - do we have evidence for that?</div><br/></div></div></div></div><div id="39939730" class="c"><input type="checkbox" id="c-39939730" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39937015">parent</a><span>|</span><a href="#39937699">prev</a><span>|</span><a href="#39936605">next</a><span>|</span><label class="collapse" for="c-39939730">[-]</label><label class="expand" for="c-39939730">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re only correct about Qwen&#x27;s MoE. I presume that Chinese model builders feel more pressure to be efficient about using their GPU time because of sanctions.</div><br/></div></div></div></div></div></div><div id="39936605" class="c"><input type="checkbox" id="c-39936605" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934873">parent</a><span>|</span><a href="#39936471">prev</a><span>|</span><a href="#39938724">next</a><span>|</span><label class="collapse" for="c-39936605">[-]</label><label class="expand" for="c-39936605">[4 more]</label></div><br/><div class="children"><div class="content">If MoEs are that good, we know GPT-4 is, than why not train very specific MoEs. One part of MoE could be a perfect Math model which  can actually calculate 2+2. Wouldn&#x27;t models like these be better in general?</div><br/><div id="39936681" class="c"><input type="checkbox" id="c-39936681" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936605">parent</a><span>|</span><a href="#39938724">next</a><span>|</span><label class="collapse" for="c-39936681">[-]</label><label class="expand" for="c-39936681">[3 more]</label></div><br/><div class="children"><div class="content">Keeping it short: &quot;Not even wrong&quot;, in the Pauli sense.<p>- People hear &quot;mixture of experts&quot; and they think &quot;N specialists&quot; - but ex. think how much know you need to know to autocomplete &quot;Two plus two is &quot;<p>- Fundamental thing of ML is you define functions and give it data, and the more data you give it to the better. Once youre at &quot;I will simply give it the training data needed to be good enough at the task and wall off that part of the implementation&quot; you&#x27;re outside ML and have a chicken and egg problem<p>- We don&#x27;t know GPT-4 is MoE<p>- MoE in practice is fundamentally about trading off runtime vs. static size properties to gain inference speed. I.e. 7x8 stored and picking 7x2 at runtime means youre somewhere between 7x2 and 7x3 in quality, inference at 7x2 speed, and have to train and store and load 7x8. You don&#x27;t reach for it to increase quality, you reach for it to increase inference speed at the expense of inference ram and total model size.</div><br/><div id="39937929" class="c"><input type="checkbox" id="c-39937929" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936681">parent</a><span>|</span><a href="#39938724">next</a><span>|</span><label class="collapse" for="c-39937929">[-]</label><label class="expand" for="c-39937929">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We don&#x27;t know GPT-4 is MoE<p>Didn&#x27;t Yampleg&#x27;s tweet &#x2F; leak confirm this one? I mean, he could be wrong about this, but I thought the consensus was on it being true by now.<p>(Copy of the removed tweets at <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;mlscaling&#x2F;comments&#x2F;14wcy7m&#x2F;gpt4s_details_are_leaked&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;mlscaling&#x2F;comments&#x2F;14wcy7m&#x2F;gpt4s_de...</a> )</div><br/><div id="39938018" class="c"><input type="checkbox" id="c-39938018" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39937929">parent</a><span>|</span><a href="#39938724">next</a><span>|</span><label class="collapse" for="c-39938018">[-]</label><label class="expand" for="c-39938018">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just a dude retweeting a substack. I wouldn&#x27;t bet against it* but I wouldn&#x27;t bet on it either. His tweet would have just linked to the article in the top comment.<p>* I used to crusade against this rumor because the only source is that article, and people repeating that source. But I imagine it&#x27;s a no-brainer given they have enough users that they essentially get a throughput bump &#x27;for free&#x27; even if the model weights are huge, i.e. better to utilize as much GPU ram as you can muster, the cost of needing more GPU ram is offset by the cost of being able to run multiple inference against the model all the time anyway</div><br/></div></div></div></div></div></div></div></div><div id="39938724" class="c"><input type="checkbox" id="c-39938724" checked=""/><div class="controls bullet"><span class="by">brianarbuckle</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934873">parent</a><span>|</span><a href="#39936605">prev</a><span>|</span><a href="#39934805">next</a><span>|</span><label class="collapse" for="c-39938724">[-]</label><label class="expand" for="c-39938724">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. In inference, I could even imagine a 4-bit conversion for certain edge devices.</div><br/></div></div></div></div><div id="39934805" class="c"><input type="checkbox" id="c-39934805" checked=""/><div class="controls bullet"><span class="by">nuz</span><span>|</span><a href="#39934483">parent</a><span>|</span><a href="#39934873">prev</a><span>|</span><a href="#39935059">next</a><span>|</span><label class="collapse" for="c-39934805">[-]</label><label class="expand" for="c-39934805">[24 more]</label></div><br/><div class="children"><div class="content">Meta has their own data centers so they definitely didn&#x27;t pay the equivalent to what AWS costs</div><br/><div id="39934837" class="c"><input type="checkbox" id="c-39934837" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934805">parent</a><span>|</span><a href="#39936046">next</a><span>|</span><label class="collapse" for="c-39934837">[-]</label><label class="expand" for="c-39934837">[17 more]</label></div><br/><div class="children"><div class="content">Good point, although it&#x27;s possible that with the extreme price of GPUs that it cost <i>more</i> to train by buying hardware than it would to rent.  For example it might take two to three years before the GPUs are paid for by customers.</div><br/><div id="39934934" class="c"><input type="checkbox" id="c-39934934" checked=""/><div class="controls bullet"><span class="by">greenavocado</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934837">parent</a><span>|</span><a href="#39939760">next</a><span>|</span><label class="collapse" for="c-39934934">[-]</label><label class="expand" for="c-39934934">[13 more]</label></div><br/><div class="children"><div class="content">Linux reserved cost of p3.16xlarge is $146,362.0800 annually. On-demand cost is $214,444.8000 annually<p>I am pretty damn sure I could build a 8 GPU Intel Xeon E5-2686 v4 (Broadwell) (that&#x27;s what Amazon uses - it&#x27;s $30 to $75 on eBay) server for less than that and come out ahead on electricity even at full throttle. RTX 4090 are just under $2000 each on eBay.<p>8 GPU × $2000 (RTX 4090) + $1000 (for the rest of the computer) = $17,000<p>If pulling 2kW continuously at 15 cents per kW*hr for 1 year that&#x27;s 2000 watts × 365 days × (0.15&#x2F;(kW×hr)) or $2,628<p>In total the computer will cost $19,628 if you throw it in the dumpster at the end of each calendar year of using it.<p>If you stack internet cost of $200 a month on top, that&#x27;s $2400 a year, which raises your annual cost to: $22,028<p>This is still $124,334 cheaper per year than one AWS 8-GPU server if you fully depreciate your own hardware at the end of year 1 to $0.<p>I could hire an engineer in America to babysit it with the money left over.</div><br/><div id="39935947" class="c"><input type="checkbox" id="c-39935947" checked=""/><div class="controls bullet"><span class="by">OtherShrezzing</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934934">parent</a><span>|</span><a href="#39936680">next</a><span>|</span><label class="collapse" for="c-39935947">[-]</label><label class="expand" for="c-39935947">[8 more]</label></div><br/><div class="children"><div class="content">Are consumer grade RTX 4090 cards going to be suitable for running full tilt 24&#x2F;7 for a year? Those things are fine to stress on the latest game for a few hours at a time, but would probably cause some defects from significant heat stress after just a few days at 100%.<p>This is inconsequential when you&#x27;re playing Overwatch for a few hours a night and a frame drops now and again. If you&#x27;re training an iteratively developed LLM though, physical defects could propagate into huge deficiencies in the final model.</div><br/><div id="39936240" class="c"><input type="checkbox" id="c-39936240" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39935947">parent</a><span>|</span><a href="#39936034">next</a><span>|</span><label class="collapse" for="c-39936240">[-]</label><label class="expand" for="c-39936240">[3 more]</label></div><br/><div class="children"><div class="content">Yep absolutely, crypto miners have been doing it for years.<p>I still think it would be impractical at scale because they are so much more hot and power hungry than the datacenter cards, and you would be lucky to score one or two if you’re on a wait list.</div><br/><div id="39937220" class="c"><input type="checkbox" id="c-39937220" checked=""/><div class="controls bullet"><span class="by">eek2121</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936240">parent</a><span>|</span><a href="#39937645">next</a><span>|</span><label class="collapse" for="c-39937220">[-]</label><label class="expand" for="c-39937220">[1 more]</label></div><br/><div class="children"><div class="content">Except you can absolutely obtain 4090s today, while enterprise hardware is (was? haven&#x27;t looked at the data) recently, which is the exact opposite scenario you mentioned.<p>I&#x27;m actually really surprised that you can still buy 4090s for under $2,000 (cheapest available I saw was $1,800 new and I only took 30 seconds to look), but you can usually sell certain models for quite a bit more. For example, my used 4090 FE is currently worth more than I paid for it.<p>I&#x27;ve played with AI, and while admittedly I&#x27;ve not done anything super serious, I can tell you that both the 3090 and 4090 are more than capable of performing. Tie them with a power efficient AMD CPU and you have something that can be competitive with enterprise (somewhat).<p>I&#x27;ve seen the pricing of &quot;cloud&quot; offerings and I&#x27;ve toyed with the idea of creating an &quot;AI Cloud&quot; because I have access to really fast internet and super cheap electricity, but I haven&#x27;t executed because I&#x27;m most certainly not a salesperson. I do, however, know enough about marketing that one should not target price, so there is that...</div><br/></div></div><div id="39937645" class="c"><input type="checkbox" id="c-39937645" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936240">parent</a><span>|</span><a href="#39937220">prev</a><span>|</span><a href="#39936034">next</a><span>|</span><label class="collapse" for="c-39937645">[-]</label><label class="expand" for="c-39937645">[1 more]</label></div><br/><div class="children"><div class="content">You could under-volt or watt-limit a bit and lose just a fraction of FLOPS for much less heat&#x2F;power though, depending on the workload</div><br/></div></div></div></div><div id="39936034" class="c"><input type="checkbox" id="c-39936034" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39935947">parent</a><span>|</span><a href="#39936240">prev</a><span>|</span><a href="#39936680">next</a><span>|</span><label class="collapse" for="c-39936034">[-]</label><label class="expand" for="c-39936034">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think they&#x27;d become a fire hazard, but it is true that one would likely pick something else for this application.<p>Having said that, switching to something like the Tesla V100-SXM2-16GB wouldn&#x27;t cost that much more.<p>TBH, I&#x27;m shocked at how many people treat Amazon as the first choice for this stuff. Much of it isn&#x27;t even what most would consider a &quot;production&quot; workload. You are paying for a lot of enterprise-readiness that you don&#x27;t need for training.</div><br/><div id="39936701" class="c"><input type="checkbox" id="c-39936701" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936034">parent</a><span>|</span><a href="#39939050">next</a><span>|</span><label class="collapse" for="c-39936701">[-]</label><label class="expand" for="c-39936701">[2 more]</label></div><br/><div class="children"><div class="content">If you wanted to finetune a Mixtral 8x7B, what would you use?</div><br/><div id="39937499" class="c"><input type="checkbox" id="c-39937499" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936701">parent</a><span>|</span><a href="#39939050">next</a><span>|</span><label class="collapse" for="c-39937499">[-]</label><label class="expand" for="c-39937499">[1 more]</label></div><br/><div class="children"><div class="content">Given the relative availability, I&#x27;d probably try to do it with a couple of rtx4090s on tensordock.</div><br/></div></div></div></div><div id="39939050" class="c"><input type="checkbox" id="c-39939050" checked=""/><div class="controls bullet"><span class="by">pdntspa</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936034">parent</a><span>|</span><a href="#39936701">prev</a><span>|</span><a href="#39936680">next</a><span>|</span><label class="collapse" for="c-39939050">[-]</label><label class="expand" for="c-39939050">[1 more]</label></div><br/><div class="children"><div class="content">&gt; TBH, I&#x27;m shocked at how many people treat Amazon as the first choice for this stuff<p>You can thank Amazon&#x27;s legions of salespeople for that, particularly the end of year junket in Las Vegas where attendees are so pampered that about the only thing they won&#x27;t do is suck your dick<p>Oh, yeah, they&#x27;ll also yell at you on stage if you complain about their UI</div><br/></div></div></div></div></div></div><div id="39936680" class="c"><input type="checkbox" id="c-39936680" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934934">parent</a><span>|</span><a href="#39935947">prev</a><span>|</span><a href="#39935350">next</a><span>|</span><label class="collapse" for="c-39936680">[-]</label><label class="expand" for="c-39936680">[1 more]</label></div><br/><div class="children"><div class="content">Though this comparison is really only relevant for a couple of machines. Beyond that, at this cost, if you pay AWS list prices &quot;at scale&quot; you&#x27;re doing something very wrong.<p>Don&#x27;t get me wrong - I&#x27;ve frequently argued that AWS is price gouging and relying on peoples lack of understanding of how the devops costs of running your own works out, but it doesn&#x27;t take a huge budget before this calculation will look very different (still cheaper to own your own, though).</div><br/></div></div><div id="39935350" class="c"><input type="checkbox" id="c-39935350" checked=""/><div class="controls bullet"><span class="by">ghshephard</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934934">parent</a><span>|</span><a href="#39936680">prev</a><span>|</span><a href="#39936312">next</a><span>|</span><label class="collapse" for="c-39935350">[-]</label><label class="expand" for="c-39935350">[2 more]</label></div><br/><div class="children"><div class="content">An A100-80 GPU  goes for about $20K each.</div><br/><div id="39935374" class="c"><input type="checkbox" id="c-39935374" checked=""/><div class="controls bullet"><span class="by">greenavocado</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39935350">parent</a><span>|</span><a href="#39936312">next</a><span>|</span><label class="collapse" for="c-39935374">[-]</label><label class="expand" for="c-39935374">[1 more]</label></div><br/><div class="children"><div class="content">The instances in question use Tesla V100-SXM2-16GB</div><br/></div></div></div></div><div id="39936312" class="c"><input type="checkbox" id="c-39936312" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934934">parent</a><span>|</span><a href="#39935350">prev</a><span>|</span><a href="#39939760">next</a><span>|</span><label class="collapse" for="c-39936312">[-]</label><label class="expand" for="c-39936312">[1 more]</label></div><br/><div class="children"><div class="content">You can build old Xeon based but only has 40 lane PCIe. For training 8 GPUs how do you push data fast? I’m using 7000 series Epyc for this to get 128 lanes. Have you built this kind of machine? You see good speed with 40 lane? Curious because then I can use old Tyan motherboard which comes in full case with good layout for multi GPU. Epyc based I have to use riser and custom frame which is painful.<p>New Tyan more costly but great case layout.</div><br/></div></div></div></div><div id="39939760" class="c"><input type="checkbox" id="c-39939760" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934837">parent</a><span>|</span><a href="#39934934">prev</a><span>|</span><a href="#39935172">next</a><span>|</span><label class="collapse" for="c-39939760">[-]</label><label class="expand" for="c-39939760">[1 more]</label></div><br/><div class="children"><div class="content">Since the GPUs can be rented out afterwards, they amortize <i>very</i> quickly with prices in the order of $1&#x2F;h.</div><br/></div></div><div id="39934883" class="c"><input type="checkbox" id="c-39934883" checked=""/><div class="controls bullet"><span class="by">fleischhauf</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934837">parent</a><span>|</span><a href="#39935172">prev</a><span>|</span><a href="#39936046">next</a><span>|</span><label class="collapse" for="c-39934883">[-]</label><label class="expand" for="c-39934883">[1 more]</label></div><br/><div class="children"><div class="content">I think AWS prices scale with hardware price</div><br/></div></div></div></div><div id="39936046" class="c"><input type="checkbox" id="c-39936046" checked=""/><div class="controls bullet"><span class="by">packetslave</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39934805">parent</a><span>|</span><a href="#39934837">prev</a><span>|</span><a href="#39935059">next</a><span>|</span><label class="collapse" for="c-39936046">[-]</label><label class="expand" for="c-39936046">[6 more]</label></div><br/><div class="children"><div class="content">Meta also doesn&#x27;t pay AWS anywhere near retail price for instances.</div><br/><div id="39936444" class="c"><input type="checkbox" id="c-39936444" checked=""/><div class="controls bullet"><span class="by">KMnO4</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936046">parent</a><span>|</span><a href="#39935059">next</a><span>|</span><label class="collapse" for="c-39936444">[-]</label><label class="expand" for="c-39936444">[5 more]</label></div><br/><div class="children"><div class="content">Why is this the case? Even AWS internal pays the same AWS prices as everyone else</div><br/><div id="39936739" class="c"><input type="checkbox" id="c-39936739" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936444">parent</a><span>|</span><a href="#39938310">next</a><span>|</span><label class="collapse" for="c-39936739">[-]</label><label class="expand" for="c-39936739">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m less surprised if AWS internally pays AWS list prices, because that&#x27;s just internal accounting. From the even relatively small AWS customers I know, none of them needed to get very far into the 6 digits per year spend before a couple of quiet mentions to their account manager that they were reviewing other options was enough to get steep discounts.<p>Add in lots of credits, and if you pay list price, you&#x27;re being taken to the cleaners..<p>I&#x27;ve done contract work for clients to be ready to migrate both as part of maximising credits and as part of negotiating posture, and the savings can be enormous (though it&#x27;d still usually be cheaper to use managed servers).</div><br/></div></div><div id="39938310" class="c"><input type="checkbox" id="c-39938310" checked=""/><div class="controls bullet"><span class="by">packetslave</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936444">parent</a><span>|</span><a href="#39936739">prev</a><span>|</span><a href="#39936583">next</a><span>|</span><label class="collapse" for="c-39938310">[-]</label><label class="expand" for="c-39938310">[1 more]</label></div><br/><div class="children"><div class="content">Couple of reasons:  Meta&#x27;s annual spend with AWS is large enough that they&#x27;ll have a negotiated blanket discount that takes a fixed percentage off the top of their monthly spend. This is very common for larger AWS customers, not just Meta.<p>For instances specifically, any planned usage will be using either reserved instances or at minimum a compute savings plan (CSP) that drops the hourly rate dramatically in exchange for a committed number of instance hours, with or without an upfront payment.<p>Finally, there may be a negotiated rate for specific instance types built into the contract. Again, common for very large customers.<p>source:  I was on one of the cloud-related infrastructure teams (left in early 2022). I have no idea about their spend (or discounts) today, but two years ago it was enough that Andy Jassy would meet 1:1 with Mark to &quot;discuss the relationship&quot;.</div><br/></div></div><div id="39936583" class="c"><input type="checkbox" id="c-39936583" checked=""/><div class="controls bullet"><span class="by">elcomet</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936444">parent</a><span>|</span><a href="#39938310">prev</a><span>|</span><a href="#39935059">next</a><span>|</span><label class="collapse" for="c-39936583">[-]</label><label class="expand" for="c-39936583">[2 more]</label></div><br/><div class="children"><div class="content">They have their own data centers, they don&#x27;t use AWS</div><br/><div id="39938875" class="c"><input type="checkbox" id="c-39938875" checked=""/><div class="controls bullet"><span class="by">packetslave</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39936583">parent</a><span>|</span><a href="#39935059">next</a><span>|</span><label class="collapse" for="c-39938875">[-]</label><label class="expand" for="c-39938875">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t use AWS <i>to run the big three apps</i> (FB, Insta, WhatsApp). They very much use them for other things. FB data center machines tend to be highly-specialized and optimized for running the apps, not general-purpose compute.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39935059" class="c"><input type="checkbox" id="c-39935059" checked=""/><div class="controls bullet"><span class="by">dannyw</span><span>|</span><a href="#39934483">parent</a><span>|</span><a href="#39934805">prev</a><span>|</span><a href="#39936784">next</a><span>|</span><label class="collapse" for="c-39935059">[-]</label><label class="expand" for="c-39935059">[1 more]</label></div><br/><div class="children"><div class="content">Any company as big as Meta have teams working on optimisation (eg optimised kernels), usually with direct engagement with NVIDIA engineers.<p>These kind of things are usually only selectively shared.</div><br/></div></div><div id="39936784" class="c"><input type="checkbox" id="c-39936784" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#39934483">parent</a><span>|</span><a href="#39935059">prev</a><span>|</span><a href="#39939570">next</a><span>|</span><label class="collapse" for="c-39936784">[-]</label><label class="expand" for="c-39936784">[1 more]</label></div><br/><div class="children"><div class="content">I’ll agree with your general point even though there are some subtleties to FBNY.<p>More important, we let an awful lot of self-promotion from the big guys slide around here.<p>I can live with the guys and gals doing this on a shoestring getting a little of that sweet hype love. This seems pretty legit.</div><br/></div></div><div id="39939570" class="c"><input type="checkbox" id="c-39939570" checked=""/><div class="controls bullet"><span class="by">rvba</span><span>|</span><a href="#39934483">parent</a><span>|</span><a href="#39936784">prev</a><span>|</span><a href="#39934253">next</a><span>|</span><label class="collapse" for="c-39939570">[-]</label><label class="expand" for="c-39939570">[2 more]</label></div><br/><div class="children"><div class="content">Training data costs zero?<p>What is the training data anwyay? Books? (Is it legal to use non public domain, wait whom am I kidding) Reddit posts scrapped from the site? (Without clean up there are lots of bad subreddits) Wikipedia?<p>Of course I understand why labour costs + employee cost related costs are ignored.</div><br/><div id="39939841" class="c"><input type="checkbox" id="c-39939841" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39934483">root</a><span>|</span><a href="#39939570">parent</a><span>|</span><a href="#39934253">next</a><span>|</span><label class="collapse" for="c-39939841">[-]</label><label class="expand" for="c-39939841">[1 more]</label></div><br/><div class="children"><div class="content">It costs more the more you care about squeaky clean training data. Of course you get a better model in return.<p>ChatGPT used a crawl of the internet and patches things up with alignment and DPO. Big boys like Microsoft might have deals with publishers to get textbooks in bulk.<p>Contents from sites with moderation can be filtered using the platform&#x27;s mechanism, e.g., only include text with a certain length and count of upvotes.<p>LLMs can be used to generate and filter data as well. Humans have been used to do this, they might have to do this less in the future. Mostly to review what the LLMs are suggesting.</div><br/></div></div></div></div></div></div><div id="39934253" class="c"><input type="checkbox" id="c-39934253" checked=""/><div class="controls bullet"><span class="by">plufz</span><span>|</span><a href="#39934483">prev</a><span>|</span><a href="#39934734">next</a><span>|</span><label class="collapse" for="c-39934253">[-]</label><label class="expand" for="c-39934253">[8 more]</label></div><br/><div class="children"><div class="content">You’ve been in tech for too long when 1 million USD is your smallest unit.</div><br/><div id="39934601" class="c"><input type="checkbox" id="c-39934601" checked=""/><div class="controls bullet"><span class="by">noodlesUK</span><span>|</span><a href="#39934253">parent</a><span>|</span><a href="#39935333">next</a><span>|</span><label class="collapse" for="c-39934601">[-]</label><label class="expand" for="c-39934601">[5 more]</label></div><br/><div class="children"><div class="content">I wonder why they decided to call it 0.1M USD rather than 100k USD. For many of us, a million dollars is a large amount of money, even for a business.</div><br/><div id="39934823" class="c"><input type="checkbox" id="c-39934823" checked=""/><div class="controls bullet"><span class="by">plufz</span><span>|</span><a href="#39934253">root</a><span>|</span><a href="#39934601">parent</a><span>|</span><a href="#39936274">next</a><span>|</span><label class="collapse" for="c-39934823">[-]</label><label class="expand" for="c-39934823">[1 more]</label></div><br/><div class="children"><div class="content">I’m sure they had their reasons, but all I can see is the Simpsons meme with Mr Burns at the ATM saying “What’s the smallest amount of money I can think of? A thousand dollars.” ;)</div><br/></div></div><div id="39936274" class="c"><input type="checkbox" id="c-39936274" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#39934253">root</a><span>|</span><a href="#39934601">parent</a><span>|</span><a href="#39934823">prev</a><span>|</span><a href="#39936093">next</a><span>|</span><label class="collapse" for="c-39936274">[-]</label><label class="expand" for="c-39936274">[1 more]</label></div><br/><div class="children"><div class="content">100k isn’t worth anywhere near what it used to due to inflation. It might get you a nice pickup truck or a kitchen remodel. If your business is doing research and can’t spend then it’s more of a hobby than a business.</div><br/></div></div><div id="39936093" class="c"><input type="checkbox" id="c-39936093" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#39934253">root</a><span>|</span><a href="#39934601">parent</a><span>|</span><a href="#39936274">prev</a><span>|</span><a href="#39934785">next</a><span>|</span><label class="collapse" for="c-39936093">[-]</label><label class="expand" for="c-39936093">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s to imply that it costs other people in the millions, but they did it for only 0.1 million, which is a small number of millions. Just a rhetorical trick.</div><br/></div></div><div id="39934785" class="c"><input type="checkbox" id="c-39934785" checked=""/><div class="controls bullet"><span class="by">mattlondon</span><span>|</span><a href="#39934253">root</a><span>|</span><a href="#39934601">parent</a><span>|</span><a href="#39936093">prev</a><span>|</span><a href="#39935333">next</a><span>|</span><label class="collapse" for="c-39934785">[-]</label><label class="expand" for="c-39934785">[1 more]</label></div><br/><div class="children"><div class="content">Same reason things are x.99 - the 0.1 decimal &quot;feels&quot; smaller than seeing 100,000 - &quot;holy fuck!&quot; Etc</div><br/></div></div></div></div><div id="39935333" class="c"><input type="checkbox" id="c-39935333" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#39934253">parent</a><span>|</span><a href="#39934601">prev</a><span>|</span><a href="#39934506">next</a><span>|</span><label class="collapse" for="c-39935333">[-]</label><label class="expand" for="c-39935333">[1 more]</label></div><br/><div class="children"><div class="content">Well, it&#x27;s interesting to think about how much has been invested into BigModel companies (Anthropic, Perplexity, OpenAI) when it&#x27;s very rapidly becoming commoditized.</div><br/></div></div></div></div><div id="39934734" class="c"><input type="checkbox" id="c-39934734" checked=""/><div class="controls bullet"><span class="by">antimatter15</span><span>|</span><a href="#39934253">prev</a><span>|</span><a href="#39935157">next</a><span>|</span><label class="collapse" for="c-39934734">[-]</label><label class="expand" for="c-39934734">[4 more]</label></div><br/><div class="children"><div class="content">It looks like Llama 2 7B took 184,320 A100-80GB GPU-hours to train[1]. This one says it used a 96×H100 GPU cluster for 2 weeks, for 32,256 hours. That&#x27;s 17.5% of the number of hours, but H100s are faster than A100s [2] and FP16&#x2F;bfloat16 performance is ~3x better.<p>If they had tried to replicate Llama 2 identically with their hardware setup, it&#x27;d cost a little bit less than twice their MoE model.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama&#x2F;blob&#x2F;main&#x2F;MODEL_CARD.md#hardware-and-software">https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama&#x2F;blob&#x2F;main&#x2F;MODEL_CARD.md#...</a><p>[2] <a href="https:&#x2F;&#x2F;blog.ori.co&#x2F;choosing-between-nvidia-h100-vs-a100-performance-and-costs-considerations" rel="nofollow">https:&#x2F;&#x2F;blog.ori.co&#x2F;choosing-between-nvidia-h100-vs-a100-per...</a></div><br/><div id="39935099" class="c"><input type="checkbox" id="c-39935099" checked=""/><div class="controls bullet"><span class="by">anizan</span><span>|</span><a href="#39934734">parent</a><span>|</span><a href="#39935157">next</a><span>|</span><label class="collapse" for="c-39935099">[-]</label><label class="expand" for="c-39935099">[3 more]</label></div><br/><div class="children"><div class="content">They mention the cost was ~80,000k USD so for 32,256 hours it comes to ~2.48$ an hour.
Amazing how cost effective the compute actually is.</div><br/><div id="39937062" class="c"><input type="checkbox" id="c-39937062" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39934734">root</a><span>|</span><a href="#39935099">parent</a><span>|</span><a href="#39935157">next</a><span>|</span><label class="collapse" for="c-39937062">[-]</label><label class="expand" for="c-39937062">[2 more]</label></div><br/><div class="children"><div class="content">I was paying $1.1 for A100 hour more than a year ago. $2.48 is crazy expensive.</div><br/><div id="39937594" class="c"><input type="checkbox" id="c-39937594" checked=""/><div class="controls bullet"><span class="by">anizan</span><span>|</span><a href="#39934734">root</a><span>|</span><a href="#39937062">parent</a><span>|</span><a href="#39935157">next</a><span>|</span><label class="collapse" for="c-39937594">[-]</label><label class="expand" for="c-39937594">[1 more]</label></div><br/><div class="children"><div class="content">It was for a 96 X H100 cluster. Their provider was exabits.ai which bills itself as a decentralised computing marketplace.</div><br/></div></div></div></div></div></div></div></div><div id="39935157" class="c"><input type="checkbox" id="c-39935157" checked=""/><div class="controls bullet"><span class="by">vertis</span><span>|</span><a href="#39934734">prev</a><span>|</span><a href="#39934864">next</a><span>|</span><label class="collapse" for="c-39935157">[-]</label><label class="expand" for="c-39935157">[5 more]</label></div><br/><div class="children"><div class="content">It might be equivalent to LLaMA2 but it&#x27;s still not capable of even simple reasoning:<p>&gt; If two apples cost 2 dollars and 1 apple costs 1.20, what is the discount you&#x27;re getting<p>&gt; To calculate the discount, you need to compare the original price of the apples to the price after the discount.<p>&gt; The original price for two apples is: 2 apples * $1.20&#x2F;apple = $2.40<p>&gt; The price for one apple after the discount is: 1 apple * $2.00&#x2F;apple = $2.00<p>&gt; Now, let&#x27;s calculate the discount for one apple:
&gt; Original price for one apple - Discounted price for one apple = Discount amount
&gt; $2.40 - $2.00 = $0.40<p>&gt; The discount for one apple is $0.40. Since you&#x27;re buying two apples, you&#x27;ll get the discount on both:
&gt; Discount amount for two apples = 2 * $0.40 = $0.80<p>&gt; So, the discount you&#x27;re getting for two apples is $0.80.</div><br/><div id="39937629" class="c"><input type="checkbox" id="c-39937629" checked=""/><div class="controls bullet"><span class="by">ravetcofx</span><span>|</span><a href="#39935157">parent</a><span>|</span><a href="#39936131">next</a><span>|</span><label class="collapse" for="c-39937629">[-]</label><label class="expand" for="c-39937629">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m kind of impressed it was able to do basic math even if the reasoning isn&#x27;t correct. That seems like an impressive emergent behavior for a small cheap model like this.</div><br/><div id="39939971" class="c"><input type="checkbox" id="c-39939971" checked=""/><div class="controls bullet"><span class="by">vertis</span><span>|</span><a href="#39935157">root</a><span>|</span><a href="#39937629">parent</a><span>|</span><a href="#39936131">next</a><span>|</span><label class="collapse" for="c-39939971">[-]</label><label class="expand" for="c-39939971">[1 more]</label></div><br/><div class="children"><div class="content">Llama2:7b makes the same mistakes. It&#x27;s not until you use something like Mixtral or Llama2:13b that it actually gets the correct results (in my one example).<p>Interestingly Llama2:13b objects that there is no discount until I clarify: &quot;the discount you&#x27;re getting [with 2 apples]&quot;<p>It&#x27;s not just math though it&#x27;s any kind of complex reasoning and ambiguity. Comparing to humans is always complex, but humans for the most part wouldn&#x27;t balk at me asking what discount you&#x27;re getting without specifying that it&#x27;s the 2 apples that have the discount in this example. A more advanced model often states the assumptions.<p>There are lots of nuances in this question as well. I&#x27;m still paying 80c more than buying one apple, so I should only buy two apples if I would use two apples.</div><br/></div></div></div></div><div id="39936131" class="c"><input type="checkbox" id="c-39936131" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#39935157">parent</a><span>|</span><a href="#39937629">prev</a><span>|</span><a href="#39936472">next</a><span>|</span><label class="collapse" for="c-39936131">[-]</label><label class="expand" for="c-39936131">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if the reasoning capability of a moe model is limited by the max size of the individual experts?</div><br/></div></div><div id="39936472" class="c"><input type="checkbox" id="c-39936472" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39935157">parent</a><span>|</span><a href="#39936131">prev</a><span>|</span><a href="#39934864">next</a><span>|</span><label class="collapse" for="c-39936472">[-]</label><label class="expand" for="c-39936472">[1 more]</label></div><br/><div class="children"><div class="content">The simple fact that they did not list the current SOTA for the size class in their comparison table tells you all you need to know about their confidence. And listing Gemma-2B is like shooting fish in a barrel, might as well also put RedPajama on there.<p>It&#x27;s good to see MoE being attempted at the smaller sizes, and it may scale well downwards as well given their results. But regardless, 1.25T is very little training data compared to the 6T that Mistral 7B received and even that makes it barely usable and likely not yet saturated. Before it, the sub-13B size class was considered basically an academic exercise.</div><br/></div></div></div></div><div id="39934864" class="c"><input type="checkbox" id="c-39934864" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#39935157">prev</a><span>|</span><a href="#39935351">next</a><span>|</span><label class="collapse" for="c-39934864">[-]</label><label class="expand" for="c-39934864">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been out of academia for a bit, but in my day 100k USD would <i>not</i> have been considered academia-friendly in my neck of the woods...</div><br/><div id="39935145" class="c"><input type="checkbox" id="c-39935145" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#39934864">parent</a><span>|</span><a href="#39935351">next</a><span>|</span><label class="collapse" for="c-39935145">[-]</label><label class="expand" for="c-39935145">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s about the cost of 1 grad student year including all overhead, I believe. It&#x27;s definitely far less than what many physics and biology labs spend on equipment in a year.<p>I mean, you&#x27;re an idiot of a PI if you have $500K&#x2F;year of grants and spend it on 5 students and no compute.</div><br/></div></div></div></div><div id="39935351" class="c"><input type="checkbox" id="c-39935351" checked=""/><div class="controls bullet"><span class="by">operator-name</span><span>|</span><a href="#39934864">prev</a><span>|</span><a href="#39935836">next</a><span>|</span><label class="collapse" for="c-39935351">[-]</label><label class="expand" for="c-39935351">[3 more]</label></div><br/><div class="children"><div class="content">Sadly no matter what kind of prompting that I try it fails the sisters question:<p>&gt; Sally (a girl) has three brothers. Each of her brothers has two sisters. How many sisters does Sally have?<p>Sally (a girl) has three brothers. Each of her brothers has two sisters. How many sisters does Sally have?<p>Sally has two sisters. Here&#x27;s the reasoning:
Sally has three brothers.
Each of her brothers has two sisters.
Since her brothers are the ones who have two sisters each, and we know there are three brothers, we can conclude that each brother has two sisters.
Therefore, there are two sisters in total (one for each brother).
So, Sally has two sisters.<p><a href="https:&#x2F;&#x2F;www.lepton.ai&#x2F;playground&#x2F;chat?model=jetmoe-8b-chat" rel="nofollow">https:&#x2F;&#x2F;www.lepton.ai&#x2F;playground&#x2F;chat?model=jetmoe-8b-chat</a></div><br/><div id="39936545" class="c"><input type="checkbox" id="c-39936545" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39935351">parent</a><span>|</span><a href="#39935836">next</a><span>|</span><label class="collapse" for="c-39936545">[-]</label><label class="expand" for="c-39936545">[2 more]</label></div><br/><div class="children"><div class="content">Not exactly something you&#x27;d expect a model of this size to solve, Mixtral fails it too (if you switch the names&#x2F;genders that is, since it&#x27;s contaminated with a few versions). It does at least indicate that their training data might indeed be as clean as they say.</div><br/><div id="39940033" class="c"><input type="checkbox" id="c-39940033" checked=""/><div class="controls bullet"><span class="by">vertis</span><span>|</span><a href="#39935351">root</a><span>|</span><a href="#39936545">parent</a><span>|</span><a href="#39935836">next</a><span>|</span><label class="collapse" for="c-39940033">[-]</label><label class="expand" for="c-39940033">[1 more]</label></div><br/><div class="children"><div class="content">More complex models fail this question, so it would be exceptional if it passed it:<p>ollama run llama2:13b<p>&gt;&gt;&gt; Sally (a girl) has three brothers. Each of her brothers has two sisters. How many sisters does Sally have?<p>This is a classic lateral thinking puzzle that requires you to think outside the box and challenge your 
assumptions. The answer is not what you might expect at first glance.<p>To solve this puzzle, we need to break it down step by step:<p>1. Sally has three brothers.
2. Each of her brothers has two sisters.<p>So, let&#x27;s count the number of sisters that each of Sally&#x27;s brothers has:<p>Brother 1: 2 sisters
Brother 2: 2 sisters
Brother 3: 2 sisters<p>Now, let&#x27;s add up all the sisters that Sally has:<p>Sister 1 + Sister 2 + Sister 3 + Sister 4 + Sister 5 = 2 + 2 + 2 + 2 + 2 = 8<p>Therefore, Sally has 8 sisters.</div><br/></div></div></div></div></div></div><div id="39935836" class="c"><input type="checkbox" id="c-39935836" checked=""/><div class="controls bullet"><span class="by">patrick-fitz</span><span>|</span><a href="#39935351">prev</a><span>|</span><a href="#39935032">next</a><span>|</span><label class="collapse" for="c-39935836">[-]</label><label class="expand" for="c-39935836">[3 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, looking at the cheapest price for a H100 that I could find online.<p>Lambda Reserved Cloud [1] starts at $1.89 per H100 per hour.<p>It could be possible to get the cost down to a lower amount:<p>$1.89 * 96GPUs * 24hours * 14days = ~$61k<p>1 - <a href="https:&#x2F;&#x2F;lambdalabs.com&#x2F;deep-learning&#x2F;servers&#x2F;hyperplane" rel="nofollow">https:&#x2F;&#x2F;lambdalabs.com&#x2F;deep-learning&#x2F;servers&#x2F;hyperplane</a></div><br/><div id="39936602" class="c"><input type="checkbox" id="c-39936602" checked=""/><div class="controls bullet"><span class="by">avrionov</span><span>|</span><a href="#39935836">parent</a><span>|</span><a href="#39939407">next</a><span>|</span><label class="collapse" for="c-39936602">[-]</label><label class="expand" for="c-39936602">[1 more]</label></div><br/><div class="children"><div class="content">This is the price of training if nothing fails.</div><br/></div></div><div id="39939407" class="c"><input type="checkbox" id="c-39939407" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#39935836">parent</a><span>|</span><a href="#39936602">prev</a><span>|</span><a href="#39935032">next</a><span>|</span><label class="collapse" for="c-39939407">[-]</label><label class="expand" for="c-39939407">[1 more]</label></div><br/><div class="children"><div class="content">It also depends on the interconnect speed. If you don&#x27;t have fast enough interconnect between the machines, you won&#x27;t get linear speedup with N gpus.</div><br/></div></div></div></div><div id="39935032" class="c"><input type="checkbox" id="c-39935032" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39935836">prev</a><span>|</span><a href="#39934791">next</a><span>|</span><label class="collapse" for="c-39935032">[-]</label><label class="expand" for="c-39935032">[7 more]</label></div><br/><div class="children"><div class="content">At $DAY_JOB nowadays we run 128x H100 runs without thinking twice nowadays. Only takes a few days to train a small-ish LLM with that to test out some ideas.</div><br/><div id="39936557" class="c"><input type="checkbox" id="c-39936557" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39935032">parent</a><span>|</span><a href="#39935345">next</a><span>|</span><label class="collapse" for="c-39936557">[-]</label><label class="expand" for="c-39936557">[2 more]</label></div><br/><div class="children"><div class="content">echo $DAY_JOB</div><br/><div id="39938760" class="c"><input type="checkbox" id="c-39938760" checked=""/><div class="controls bullet"><span class="by">agravier</span><span>|</span><a href="#39935032">root</a><span>|</span><a href="#39936557">parent</a><span>|</span><a href="#39935345">next</a><span>|</span><label class="collapse" for="c-39938760">[-]</label><label class="expand" for="c-39938760">[1 more]</label></div><br/><div class="children"><div class="content">Also `echo $NIGHT_JOB`, I&#x27;m curious like that.<p>To preempt other queries, maybe just paste the `set` output.</div><br/></div></div></div></div><div id="39935345" class="c"><input type="checkbox" id="c-39935345" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#39935032">parent</a><span>|</span><a href="#39936557">prev</a><span>|</span><a href="#39934791">next</a><span>|</span><label class="collapse" for="c-39935345">[-]</label><label class="expand" for="c-39935345">[4 more]</label></div><br/><div class="children"><div class="content">Where are they hosted?</div><br/><div id="39935466" class="c"><input type="checkbox" id="c-39935466" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39935032">root</a><span>|</span><a href="#39935345">parent</a><span>|</span><a href="#39934791">next</a><span>|</span><label class="collapse" for="c-39935466">[-]</label><label class="expand" for="c-39935466">[3 more]</label></div><br/><div class="children"><div class="content">AWS and GCP both.</div><br/><div id="39935592" class="c"><input type="checkbox" id="c-39935592" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#39935032">root</a><span>|</span><a href="#39935466">parent</a><span>|</span><a href="#39934791">next</a><span>|</span><label class="collapse" for="c-39935592">[-]</label><label class="expand" for="c-39935592">[2 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, what leads you to train models from the ground up rather than fine tuning existing models?</div><br/><div id="39935666" class="c"><input type="checkbox" id="c-39935666" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39935032">root</a><span>|</span><a href="#39935592">parent</a><span>|</span><a href="#39934791">next</a><span>|</span><label class="collapse" for="c-39935666">[-]</label><label class="expand" for="c-39935666">[1 more]</label></div><br/><div class="children"><div class="content">We do both. You can’t just fine tune if you’re trying a different model architecture, or even change some of the hyperparameters on an existing one. Every now and again you might be able to reuse some of the weights, but that’s about it. That’s part of the reason research is so incredibly expensive and time consuming in this field. I bet that $80k is only a fraction of the overall cost for the model described in the article, too.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39934791" class="c"><input type="checkbox" id="c-39934791" checked=""/><div class="controls bullet"><span class="by">davidcollantes</span><span>|</span><a href="#39935032">prev</a><span>|</span><a href="#39934655">next</a><span>|</span><label class="collapse" for="c-39934791">[-]</label><label class="expand" for="c-39934791">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t wait for the GGUF to play with it. I tried the demo (<a href="https:&#x2F;&#x2F;www.lepton.ai&#x2F;playground&#x2F;chat?model=jetmoe-8b-chat" rel="nofollow">https:&#x2F;&#x2F;www.lepton.ai&#x2F;playground&#x2F;chat?model=jetmoe-8b-chat</a>), and the results were very good!</div><br/></div></div><div id="39934655" class="c"><input type="checkbox" id="c-39934655" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#39934791">prev</a><span>|</span><a href="#39934165">next</a><span>|</span><label class="collapse" for="c-39934655">[-]</label><label class="expand" for="c-39934655">[2 more]</label></div><br/><div class="children"><div class="content">Anyone got a ballpark figure for what Meta spent on Llama 2 training for the 7B model?</div><br/></div></div><div id="39934165" class="c"><input type="checkbox" id="c-39934165" checked=""/><div class="controls bullet"><span class="by">helloericsf</span><span>|</span><a href="#39934655">prev</a><span>|</span><a href="#39934432">next</a><span>|</span><label class="collapse" for="c-39934165">[-]</label><label class="expand" for="c-39934165">[1 more]</label></div><br/><div class="children"><div class="content">X thread: <a href="https:&#x2F;&#x2F;x.com&#x2F;qinzytech&#x2F;status&#x2F;1775916338822709755?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;qinzytech&#x2F;status&#x2F;1775916338822709755?s=20</a></div><br/></div></div><div id="39934432" class="c"><input type="checkbox" id="c-39934432" checked=""/><div class="controls bullet"><span class="by">throwitaway222</span><span>|</span><a href="#39934165">prev</a><span>|</span><a href="#39935101">next</a><span>|</span><label class="collapse" for="c-39934432">[-]</label><label class="expand" for="c-39934432">[1 more]</label></div><br/><div class="children"><div class="content">This stuff is just going to keep getting pushed down.</div><br/></div></div><div id="39935101" class="c"><input type="checkbox" id="c-39935101" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#39934432">prev</a><span>|</span><a href="#39937781">next</a><span>|</span><label class="collapse" for="c-39935101">[-]</label><label class="expand" for="c-39935101">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It only uses public datasets for training, and the code is open-sourced<p>Looking at the repo, there is no training or data processing code.</div><br/><div id="39936180" class="c"><input type="checkbox" id="c-39936180" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#39935101">parent</a><span>|</span><a href="#39937781">next</a><span>|</span><label class="collapse" for="c-39936180">[-]</label><label class="expand" for="c-39936180">[2 more]</label></div><br/><div class="children"><div class="content">It was trained with &quot;1 trillion tokens from large-scale open-source pretraining datasets, including RefinedWeb, Pile, Github data, etc.&quot;<p>I guess it is good that they mentioned some of it, but yeah, that isn&#x27;t exceptionally helpful when making claims of it being 100% open source.<p>I&#x27;m not sure why they feel the need to be so secretive if all of the sources are open.</div><br/><div id="39939039" class="c"><input type="checkbox" id="c-39939039" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#39935101">root</a><span>|</span><a href="#39936180">parent</a><span>|</span><a href="#39937781">next</a><span>|</span><label class="collapse" for="c-39939039">[-]</label><label class="expand" for="c-39939039">[1 more]</label></div><br/><div class="children"><div class="content">&quot;etc.&quot; is the most important part here. There is NL SFT and code SFT data which guessing by the names are instruction data very likely from GPT-4. It is known in finetuning community that training with GPT-4 data is the easiest way of improving the model. If that&#x27;s the case base JetMoE should be compared to finetuned llama, not base llama.</div><br/></div></div></div></div></div></div><div id="39937781" class="c"><input type="checkbox" id="c-39937781" checked=""/><div class="controls bullet"><span class="by">gavin_guo</span><span>|</span><a href="#39935101">prev</a><span>|</span><a href="#39934431">next</a><span>|</span><label class="collapse" for="c-39937781">[-]</label><label class="expand" for="c-39937781">[1 more]</label></div><br/><div class="children"><div class="content">model <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;jetmoe&#x2F;jetmoe-8b" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;jetmoe&#x2F;jetmoe-8b</a></div><br/></div></div><div id="39934431" class="c"><input type="checkbox" id="c-39934431" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#39937781">prev</a><span>|</span><a href="#39934933">next</a><span>|</span><label class="collapse" for="c-39934431">[-]</label><label class="expand" for="c-39934431">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m skeptical, expect data contamination was the reason for high benchmark scores.</div><br/><div id="39934946" class="c"><input type="checkbox" id="c-39934946" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#39934431">parent</a><span>|</span><a href="#39934933">next</a><span>|</span><label class="collapse" for="c-39934946">[-]</label><label class="expand" for="c-39934946">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. IBM especially has a history of fudging the numbers on reports like this. Research puts together reports which are aggressively p-hacked and ensembled and overfit, and then sales uses those reports to boondoggle clients into using IBM.</div><br/></div></div></div></div><div id="39934933" class="c"><input type="checkbox" id="c-39934933" checked=""/><div class="controls bullet"><span class="by">barkingcat</span><span>|</span><a href="#39934431">prev</a><span>|</span><label class="collapse" for="c-39934933">[-]</label><label class="expand" for="c-39934933">[1 more]</label></div><br/><div class="children"><div class="content">This kind of assumption is super deceptive.<p>The Facebook budget includes money to pay off people they&#x27;ve ripped off (in private settlements) and money for lawyers to shield the developers so they can feel free to rip off copyrighted content without having to pay personal penalty or be imprisoned for infringement. It also includes the price of buying lobbyists to alter laws to let this practice continue.<p>Also, unless the authors work inside Facebook, they have no idea how much Facebook spent on training that model specifically.</div><br/></div></div></div></div></div></div></div></body></html>