<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1710579650748" as="style"/><link rel="stylesheet" href="styles.css?v=1710579650748"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ollama.com/blog/amd-preview">Ollama now supports AMD graphics cards</a> <span class="domain">(<a href="https://ollama.com">ollama.com</a>)</span></div><div class="subtext"><span>tosh</span> | <span>201 comments</span></div><br/><div><div id="39719199" class="c"><input type="checkbox" id="c-39719199" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39719695">next</a><span>|</span><label class="collapse" for="c-39719199">[-]</label><label class="expand" for="c-39719199">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty funny to see this blog post, when I have been running Ollama on my AMD RX 6650 for weeks :D<p>They have shipped ROCm containers since 0.1.27 (21 days ago). This blog post seems to be published along with the latest release, 0.1.29. I wonder what they actually changed in this release with regards to AMD support.<p>Also: see this issue[0] that I made where I worked through running Ollama on an AMD card that they don&#x27;t &quot;officially&quot; support yet. It&#x27;s just a matter of setting an environment variable.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2870">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2870</a><p>Edit: I did notice one change, now the starcoder2[1] model works now. Before that would crash[2].<p>[1] <a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;starcoder2">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;starcoder2</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2953">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2953</a></div><br/><div id="39720741" class="c"><input type="checkbox" id="c-39720741" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#39719199">parent</a><span>|</span><a href="#39723368">next</a><span>|</span><label class="collapse" for="c-39720741">[-]</label><label class="expand" for="c-39720741">[2 more]</label></div><br/><div class="children"><div class="content">While the PRs went in slightly earlier, much of the time was spent on testing the integrations, and working with AMD directly to resolve issues.<p>There were issues that we resolved prior to cutting the release, and many reported by the community as well.</div><br/><div id="39721629" class="c"><input type="checkbox" id="c-39721629" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39719199">root</a><span>|</span><a href="#39720741">parent</a><span>|</span><a href="#39723368">next</a><span>|</span><label class="collapse" for="c-39721629">[-]</label><label class="expand" for="c-39721629">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for clarifying and thanks for the great work you do!</div><br/></div></div></div></div><div id="39723368" class="c"><input type="checkbox" id="c-39723368" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#39719199">parent</a><span>|</span><a href="#39720741">prev</a><span>|</span><a href="#39719569">next</a><span>|</span><label class="collapse" for="c-39723368">[-]</label><label class="expand" for="c-39723368">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they wanted to wait for bug reports for 21 days before publishing a popular blog post about it..?</div><br/></div></div><div id="39719569" class="c"><input type="checkbox" id="c-39719569" checked=""/><div class="controls bullet"><span class="by">throwaway5959</span><span>|</span><a href="#39719199">parent</a><span>|</span><a href="#39723368">prev</a><span>|</span><a href="#39719695">next</a><span>|</span><label class="collapse" for="c-39719569">[-]</label><label class="expand" for="c-39719569">[2 more]</label></div><br/><div class="children"><div class="content">I mean, it was 21 days ago. What’s the difference?</div><br/><div id="39719595" class="c"><input type="checkbox" id="c-39719595" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#39719199">root</a><span>|</span><a href="#39719569">parent</a><span>|</span><a href="#39719695">next</a><span>|</span><label class="collapse" for="c-39719595">[-]</label><label class="expand" for="c-39719595">[1 more]</label></div><br/><div class="children"><div class="content">2 versions, apparently</div><br/></div></div></div></div></div></div><div id="39719695" class="c"><input type="checkbox" id="c-39719695" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39719199">prev</a><span>|</span><a href="#39720228">next</a><span>|</span><label class="collapse" for="c-39719695">[-]</label><label class="expand" for="c-39719695">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m thrilled to see support for RX 6800&#x2F;6800 XT &#x2F; 6900 XT.  I bought one of those for an outrageous amount during the post-covid shortage in hopes that I could use it for ML stuff, and thus far it hasn&#x27;t been very successful, which is a shame because it&#x27;s a beast of a card!<p>Many thanks to ollama project and llama.cpp!</div><br/><div id="39720133" class="c"><input type="checkbox" id="c-39720133" checked=""/><div class="controls bullet"><span class="by">mey</span><span>|</span><a href="#39719695">parent</a><span>|</span><a href="#39720228">next</a><span>|</span><label class="collapse" for="c-39720133">[-]</label><label class="expand" for="c-39720133">[4 more]</label></div><br/><div class="children"><div class="content">Sad to see that the cut off is just after 6700 XT which is what is in my desktop.  They indicate more devices are coming, hopefully that includes some of the more modern all in one chips with RDNA 2&#x2F;3 from AMD as well.</div><br/><div id="39720155" class="c"><input type="checkbox" id="c-39720155" checked=""/><div class="controls bullet"><span class="by">mey</span><span>|</span><a href="#39719695">root</a><span>|</span><a href="#39720133">parent</a><span>|</span><a href="#39723527">next</a><span>|</span><label class="collapse" for="c-39720155">[-]</label><label class="expand" for="c-39720155">[1 more]</label></div><br/><div class="children"><div class="content">It appears that the cut off lines up with HIP SDK support from AMD, <a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;projects&#x2F;install-on-windows&#x2F;en&#x2F;latest&#x2F;reference&#x2F;system-requirements.html" rel="nofollow">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;projects&#x2F;install-on-windows&#x2F;en&#x2F;lat...</a></div><br/></div></div><div id="39723527" class="c"><input type="checkbox" id="c-39723527" checked=""/><div class="controls bullet"><span class="by">bavell</span><span>|</span><a href="#39719695">root</a><span>|</span><a href="#39720133">parent</a><span>|</span><a href="#39720155">prev</a><span>|</span><a href="#39721079">next</a><span>|</span><label class="collapse" for="c-39723527">[-]</label><label class="expand" for="c-39723527">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using my 6750XT for more than a year now on all sorts of AI projects. Takes a little research and a few env vars but no need to wait for &quot;official&quot; support most of the time.</div><br/></div></div><div id="39721079" class="c"><input type="checkbox" id="c-39721079" checked=""/><div class="controls bullet"><span class="by">throawayonthe</span><span>|</span><a href="#39719695">root</a><span>|</span><a href="#39720133">parent</a><span>|</span><a href="#39723527">prev</a><span>|</span><a href="#39720228">next</a><span>|</span><label class="collapse" for="c-39721079">[-]</label><label class="expand" for="c-39721079">[1 more]</label></div><br/><div class="children"><div class="content">I’ve already been using ollama with my 6700xt just fine, you just have to set some env variable to make rocm work “unoficially”<p>The linked page says they will support more soon, so i’m guessing this will just be integrated</div><br/></div></div></div></div></div></div><div id="39720228" class="c"><input type="checkbox" id="c-39720228" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39719695">prev</a><span>|</span><a href="#39718917">next</a><span>|</span><label class="collapse" for="c-39720228">[-]</label><label class="expand" for="c-39720228">[57 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure why Ollama garners so much attention. It has limited value - used for only experimenting with models + cannot support more than 1 model at a time. It&#x27;s not meant for production deployments. Granted that it makes the experimentation process super easy but for something that relies on llama.cpp completely and whose main value proposition is easy model management I&#x27;m not sure it deserves the brouhaha people are giving it.<p>Edit: what do you do after the initial experimentation? you need to deploy these models eventually to production. I&#x27;m not even talking about giving credit to llama.cpp, just mentioning that this product is gaining disproportionate attention and kudos compared to the value it delivers. Not denying that it&#x27;s a great product.</div><br/><div id="39722041" class="c"><input type="checkbox" id="c-39722041" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39720668">next</a><span>|</span><label class="collapse" for="c-39722041">[-]</label><label class="expand" for="c-39722041">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s not meant for production deployments.<p>I am probably not the demographics you expect. I don’t do “production” in that sense, but I have ollama running quite often when I am working, as I use it for RAG and as a fancy knowledge extraction engine. It is <i>incredibly</i> useful:<p>- I can test a lot of models by just pulling them (very useful as progress is very fast),<p>- using their command line is trivial,<p>- the fact that it keeps running in the background means that it starts once every few days and stays out of the way,<p>- it integrates nicely with langchain (and a host of other libraries), which means that it is easy to set up some sophisticated process and abstract away the LLM itself.<p>&gt; what do you do after the initial experimentation?<p>I just keep using it. And for now, I keep tweaking my scripts but I expect them to stabilise at some point, because I use these models to do some real work, and this work is not monkeying about with LLMs.<p>&gt; I&#x27;m not even talking about giving credit to llama.cpp, just mentioning that this product is gaining disproportionate attention and kudos compared to the value it delivers.<p>For me, there is nothing that comes close in terms of integration and convenience. The value it delivers is great, because it enables me to do some useful work without wasting time worrying about lower-level architecture details. Again, I am probably not in the demographics you have in mind (I am not a CS person and my programming is usually limited to HPC), but ollama is very useful to me. Its reputation is completely deserved, as far as I am concerned.</div><br/><div id="39723726" class="c"><input type="checkbox" id="c-39723726" checked=""/><div class="controls bullet"><span class="by">rkwz</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39722041">parent</a><span>|</span><a href="#39720668">next</a><span>|</span><label class="collapse" for="c-39723726">[-]</label><label class="expand" for="c-39723726">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I use it for RAG and as a fancy knowledge extraction engine<p>Curious, can you share more details about your usecase?</div><br/></div></div></div></div><div id="39720668" class="c"><input type="checkbox" id="c-39720668" checked=""/><div class="controls bullet"><span class="by">cbhl</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39722041">prev</a><span>|</span><a href="#39720283">next</a><span>|</span><label class="collapse" for="c-39720668">[-]</label><label class="expand" for="c-39720668">[1 more]</label></div><br/><div class="children"><div class="content">In my opinion, pre-built binaries and an easy-to-use front-end are things that should exist and are valid as a separate project unto themselves (see, e.g., HandBrake vs ffmpeg).<p>Using the name of the authors or the project you&#x27;re building on can also read like an endorsement, which is not _necessarily_ desirable for the original authors (it can lead to ollama bugs being reported against llama.cpp instead of to the ollama devs and other forms of support request toil). Consider the third clause of BSD 3-Clause for an example used in other projects (although llama.cpp is licensed under MIT).</div><br/></div></div><div id="39720283" class="c"><input type="checkbox" id="c-39720283" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39720668">prev</a><span>|</span><a href="#39720604">next</a><span>|</span><label class="collapse" for="c-39720283">[-]</label><label class="expand" for="c-39720283">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Granted that it makes the experimentation process super easy<p>That&#x27;s the answer to your question. It may have less space than a Zune, but the average person doesn&#x27;t care about technically superior alternatives that are much harder to use.</div><br/><div id="39722142" class="c"><input type="checkbox" id="c-39722142" checked=""/><div class="controls bullet"><span class="by">thejohnconway</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39720283">parent</a><span>|</span><a href="#39720604">next</a><span>|</span><label class="collapse" for="c-39722142">[-]</label><label class="expand" for="c-39722142">[1 more]</label></div><br/><div class="children"><div class="content">*Nomad<p>And lame.</div><br/></div></div></div></div><div id="39720604" class="c"><input type="checkbox" id="c-39720604" checked=""/><div class="controls bullet"><span class="by">nerdix</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39720283">prev</a><span>|</span><a href="#39720281">next</a><span>|</span><label class="collapse" for="c-39720604">[-]</label><label class="expand" for="c-39720604">[22 more]</label></div><br/><div class="children"><div class="content">The answer to your question is:<p>ollama run mixtral<p>That&#x27;s it. You&#x27;re running a local LLM. I have no clue how to run llama.cpp<p>I got Stable Diffusion running and I wish there was something like ollama for it. It was painful.</div><br/><div id="39721600" class="c"><input type="checkbox" id="c-39721600" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39720604">parent</a><span>|</span><a href="#39721651">next</a><span>|</span><label class="collapse" for="c-39721600">[-]</label><label class="expand" for="c-39721600">[1 more]</label></div><br/><div class="children"><div class="content">On a mac, <a href="https:&#x2F;&#x2F;drawthings.ai" rel="nofollow">https:&#x2F;&#x2F;drawthings.ai</a> is the ollama of Stable Diffusion.</div><br/></div></div><div id="39721651" class="c"><input type="checkbox" id="c-39721651" checked=""/><div class="controls bullet"><span class="by">ghurtado</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39720604">parent</a><span>|</span><a href="#39721600">prev</a><span>|</span><a href="#39723846">next</a><span>|</span><label class="collapse" for="c-39721651">[-]</label><label class="expand" for="c-39721651">[1 more]</label></div><br/><div class="children"><div class="content">For me, ComfyUI made the process of installing and playing with SD about as simple as a Windows installer.</div><br/></div></div><div id="39723846" class="c"><input type="checkbox" id="c-39723846" checked=""/><div class="controls bullet"><span class="by">icelain</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39720604">parent</a><span>|</span><a href="#39721651">prev</a><span>|</span><a href="#39721775">next</a><span>|</span><label class="collapse" for="c-39723846">[-]</label><label class="expand" for="c-39723846">[1 more]</label></div><br/><div class="children"><div class="content">Check out EasyDiffusion.</div><br/></div></div><div id="39721775" class="c"><input type="checkbox" id="c-39721775" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39720604">parent</a><span>|</span><a href="#39723846">prev</a><span>|</span><a href="#39720281">next</a><span>|</span><label class="collapse" for="c-39721775">[-]</label><label class="expand" for="c-39721775">[18 more]</label></div><br/><div class="children"><div class="content">The README is pretty clear, albeit it talks about a lot of optional steps you don’t need, but it’s essentially gonna be something like:<p><pre><code>   git clone https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp.git
   cd llama.cpp
   make
   wget https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Mixtral-8x7B-v0.1-GGUF&#x2F;resolve&#x2F;main&#x2F;mixtral-8x7b-v0.1.Q4_K_M.gguf?download=true
   .&#x2F;main -m .&#x2F;mixtral-8x7b-v0.1.Q4_K_M.gguf -n 128</code></pre></div><br/><div id="39721880" class="c"><input type="checkbox" id="c-39721880" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721775">parent</a><span>|</span><a href="#39722090">next</a><span>|</span><label class="collapse" for="c-39721880">[-]</label><label class="expand" for="c-39721880">[13 more]</label></div><br/><div class="children"><div class="content">This shows the value ollama provides<p>I only need to know the model name and then run a single command</div><br/><div id="39723076" class="c"><input type="checkbox" id="c-39723076" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721880">parent</a><span>|</span><a href="#39724116">next</a><span>|</span><label class="collapse" for="c-39723076">[-]</label><label class="expand" for="c-39723076">[1 more]</label></div><br/><div class="children"><div class="content">The first 3 steps GP provided are literally just the steps for installation. The &quot;value&quot; you mentioned is just a packaged installer (or, in the case of Linux, apparently a `curl | sh` -- and I&#x27;d much prefer the git clone version).<p>On multiple occasions I&#x27;ve been modifying llama.cpp code directly and recompiling for my own purposes. If you&#x27;re using ollama on the command line, I&#x27;d say having the option to easily do that is much more useful than saving a couple commands upon installation.</div><br/></div></div><div id="39724116" class="c"><input type="checkbox" id="c-39724116" checked=""/><div class="controls bullet"><span class="by">read_if_gay_</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721880">parent</a><span>|</span><a href="#39723076">prev</a><span>|</span><a href="#39721962">next</a><span>|</span><label class="collapse" for="c-39724116">[-]</label><label class="expand" for="c-39724116">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Hacker News</div><br/></div></div><div id="39721962" class="c"><input type="checkbox" id="c-39721962" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721880">parent</a><span>|</span><a href="#39724116">prev</a><span>|</span><a href="#39721923">next</a><span>|</span><label class="collapse" for="c-39721962">[-]</label><label class="expand" for="c-39721962">[7 more]</label></div><br/><div class="children"><div class="content">It should be fairly obvious that one can find alternative models and use them in the above command too.<p>Look, I’m not arguing that a prebuilt binary that handles model downloading has <i>no</i> value over a source build and manually pulling down gguf files. I just want to dispel some of the mystery.<p>Local LLM execution doesn’t require some mysterious voodoo that can only be done by installing and running a server runtime. It’s just something you can do by running code that loads a model file into memory and feeds tokens to it.<p>More <i>programmers</i> should be looking at llama.cpp language bindings than at Ollama’s implementation of the openAI api.</div><br/><div id="39722739" class="c"><input type="checkbox" id="c-39722739" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721962">parent</a><span>|</span><a href="#39721981">next</a><span>|</span><label class="collapse" for="c-39722739">[-]</label><label class="expand" for="c-39722739">[1 more]</label></div><br/><div class="children"><div class="content">There are 5 commands in that README two comments up, 4 can reasonably fail (I&#x27;ll give cd high marks for reliability). `make` especially is a minefield and usually involves a half-hour of searching the internet and figuring out which dependencies are a problem today. And that is all assuming someone is comfortable with compiled languages. I&#x27;d hazard most devs these days are from JS land and don&#x27;t know how to debug make.<p>Finding the correct model weights is also a challenge in my experience, there are a lot of alternatives and it is often difficult to figure out what the differences are and whether they matter.<p>The README is clear that I&#x27;m probably about to lose an hour debugging if I follow it. It might be one of those rare cases where it works first time but that is the exception not the rule.</div><br/></div></div><div id="39721981" class="c"><input type="checkbox" id="c-39721981" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721962">parent</a><span>|</span><a href="#39722739">prev</a><span>|</span><a href="#39721923">next</a><span>|</span><label class="collapse" for="c-39721981">[-]</label><label class="expand" for="c-39721981">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;d rather focus on building on top of of LLMs than going lower level<p>Ollama makes that super easy. I tried llama.cpp first and hit build issues. Ollama worked out of the box</div><br/><div id="39721994" class="c"><input type="checkbox" id="c-39721994" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721981">parent</a><span>|</span><a href="#39721923">next</a><span>|</span><label class="collapse" for="c-39721994">[-]</label><label class="expand" for="c-39721994">[4 more]</label></div><br/><div class="children"><div class="content">Sure.<p>Just be aware that there’s a lot of expressive difference between building on top of an HTTP API vs on top of a direct interface to the token sampler and model state.</div><br/><div id="39722040" class="c"><input type="checkbox" id="c-39722040" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721994">parent</a><span>|</span><a href="#39721923">next</a><span>|</span><label class="collapse" for="c-39722040">[-]</label><label class="expand" for="c-39722040">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m aware, I don&#x27;t need that amount of sophistication yet.<p>Python seems to be the way to go deeper though. Is there a good reason I should be aware of to pick llama.cpp over python?</div><br/><div id="39722187" class="c"><input type="checkbox" id="c-39722187" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39722040">parent</a><span>|</span><a href="#39721923">next</a><span>|</span><label class="collapse" for="c-39722187">[-]</label><label class="expand" for="c-39722187">[2 more]</label></div><br/><div class="children"><div class="content">Python’s as good a choice as any for the application layer. You’re either going to be using PyTorch or llama-cpp-python to get the CUDA stuff working - both rely on native compiled C&#x2F;C++ code to access GPUs and manage memory at the scale needed for LLMs. I’m not actually up to speed on the current state of the game there but my understanding is that llama.cpp’s less generic approach has allowed it to focus on specifically optimizing performance of llama-style LLMs.</div><br/><div id="39722281" class="c"><input type="checkbox" id="c-39722281" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39722187">parent</a><span>|</span><a href="#39721923">next</a><span>|</span><label class="collapse" for="c-39722281">[-]</label><label class="expand" for="c-39722281">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen more of the model fiddling, like logits restrictions and layer dropping, implemented in python, which is why I ask<p>Most of AI has centralized around Python, I see more of my code moving that way, like how I&#x27;m using LlamaIndex as my primary interface now, which supports ollama and many more model loaders &#x2F; APIs</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39721923" class="c"><input type="checkbox" id="c-39721923" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721880">parent</a><span>|</span><a href="#39721962">prev</a><span>|</span><a href="#39722090">next</a><span>|</span><label class="collapse" for="c-39721923">[-]</label><label class="expand" for="c-39721923">[3 more]</label></div><br/><div class="children"><div class="content">And what will you do after trying it? Sure, you saved a few mins in trying out a model or models. What next?</div><br/><div id="39721954" class="c"><input type="checkbox" id="c-39721954" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721923">parent</a><span>|</span><a href="#39723719">next</a><span>|</span><label class="collapse" for="c-39721954">[-]</label><label class="expand" for="c-39721954">[1 more]</label></div><br/><div class="children"><div class="content">I focus on building the application rather than figuring out someone else preferred method for how I should work?<p>I use Docker Compose locally, Kubernetes in the cloud<p>I run in hot-reload locally, I build for production<p>I often nuke my database locally, but I run it HA in production<p>It is very rare to use the same technology locally (or the same way) as in production</div><br/></div></div><div id="39723719" class="c"><input type="checkbox" id="c-39723719" checked=""/><div class="controls bullet"><span class="by">ramblerman</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721923">parent</a><span>|</span><a href="#39721954">prev</a><span>|</span><a href="#39722090">next</a><span>|</span><label class="collapse" for="c-39723719">[-]</label><label class="expand" for="c-39723719">[1 more]</label></div><br/><div class="children"><div class="content">Relax. Not everything in this world was built exactly for you. You almost seem to have a problem with this.</div><br/></div></div></div></div></div></div><div id="39722090" class="c"><input type="checkbox" id="c-39722090" checked=""/><div class="controls bullet"><span class="by">ies7</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721775">parent</a><span>|</span><a href="#39721880">prev</a><span>|</span><a href="#39721812">next</a><span>|</span><label class="collapse" for="c-39722090">[-]</label><label class="expand" for="c-39722090">[1 more]</label></div><br/><div class="children"><div class="content">For us this may like a walk in the park.<p>For non technical people there is a possibility their os don&#x27;t have git, wget and c++ compiler (especially in windows)<p>This is just like dropbox case years ago.</div><br/></div></div><div id="39721812" class="c"><input type="checkbox" id="c-39721812" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721775">parent</a><span>|</span><a href="#39722090">prev</a><span>|</span><a href="#39723376">next</a><span>|</span><label class="collapse" for="c-39721812">[-]</label><label class="expand" for="c-39721812">[1 more]</label></div><br/><div class="children"><div class="content">Last time I tried llama.cpp I got errors when running make that were way too time consuming to bother tracking down.<p>It&#x27;s probably a simple build if everything is how it wants it, but it wasn&#x27;t in my machine, while running ollama was.</div><br/></div></div><div id="39723376" class="c"><input type="checkbox" id="c-39723376" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721775">parent</a><span>|</span><a href="#39721812">prev</a><span>|</span><a href="#39722051">next</a><span>|</span><label class="collapse" for="c-39723376">[-]</label><label class="expand" for="c-39723376">[1 more]</label></div><br/><div class="children"><div class="content">This will likely build a version without GPU acceleration, I think?</div><br/></div></div><div id="39722051" class="c"><input type="checkbox" id="c-39722051" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721775">parent</a><span>|</span><a href="#39723376">prev</a><span>|</span><a href="#39720281">next</a><span>|</span><label class="collapse" for="c-39722051">[-]</label><label class="expand" for="c-39722051">[1 more]</label></div><br/><div class="children"><div class="content">Compared to “ollama pull mixtral”?
And then actually using the thing is easier as well.</div><br/></div></div></div></div></div></div><div id="39720281" class="c"><input type="checkbox" id="c-39720281" checked=""/><div class="controls bullet"><span class="by">reustle</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39720604">prev</a><span>|</span><a href="#39720290">next</a><span>|</span><label class="collapse" for="c-39720281">[-]</label><label class="expand" for="c-39720281">[2 more]</label></div><br/><div class="children"><div class="content">Even for just running a model locally, Ollama provided a much simpler &quot;one click install&quot; earlier than most tools. That in itself is worth the support.</div><br/><div id="39721468" class="c"><input type="checkbox" id="c-39721468" checked=""/><div class="controls bullet"><span class="by">Aka456</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39720281">parent</a><span>|</span><a href="#39720290">next</a><span>|</span><label class="collapse" for="c-39721468">[-]</label><label class="expand" for="c-39721468">[1 more]</label></div><br/><div class="children"><div class="content">Koboldcpp is also very, very good, plug and play, very complet web UI, nice little api with sse text streaming, vulkan accelerated, have an AMD fork...</div><br/></div></div></div></div><div id="39720290" class="c"><input type="checkbox" id="c-39720290" checked=""/><div class="controls bullet"><span class="by">davidhariri</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39720281">prev</a><span>|</span><a href="#39721927">next</a><span>|</span><label class="collapse" for="c-39720290">[-]</label><label class="expand" for="c-39720290">[1 more]</label></div><br/><div class="children"><div class="content">As it turns out, making it faster and better to manage things tends to get people’s attention. I think it’s well deserved.</div><br/></div></div><div id="39721927" class="c"><input type="checkbox" id="c-39721927" checked=""/><div class="controls bullet"><span class="by">ecnahc515</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39720290">prev</a><span>|</span><a href="#39721510">next</a><span>|</span><label class="collapse" for="c-39721927">[-]</label><label class="expand" for="c-39721927">[1 more]</label></div><br/><div class="children"><div class="content">Ollama is the Docker of LLMs. Ollama made it _very_ easy to run LLMs locally. This is surprisingly not as easy as it seems, and incredibly useful.</div><br/></div></div><div id="39721510" class="c"><input type="checkbox" id="c-39721510" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39721927">prev</a><span>|</span><a href="#39721125">next</a><span>|</span><label class="collapse" for="c-39721510">[-]</label><label class="expand" for="c-39721510">[1 more]</label></div><br/><div class="children"><div class="content">Interestingly, Ollama is not popular at all in the &quot;localllama&quot; community (which also extends to related discords and repos).<p>And I think thats because of capabilities... Ollama is somewhat restrictive compared to other frontends. I have a littany of reasons I personally wouldn&#x27;t run it over exui or koboldcpp, both for performance and output quality.<p>This is a necessity of being stable and one-click though.</div><br/></div></div><div id="39721125" class="c"><input type="checkbox" id="c-39721125" checked=""/><div class="controls bullet"><span class="by">Karrot_Kream</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39721510">prev</a><span>|</span><a href="#39721148">next</a><span>|</span><label class="collapse" for="c-39721125">[-]</label><label class="expand" for="c-39721125">[11 more]</label></div><br/><div class="children"><div class="content">I mean, it takes something difficult like an LLM and makes it easy to run. It&#x27;s bound to get attention. If you&#x27;ve tried to get other models like BERT based models to run you&#x27;ll realize just how big the usability gains are running ollama than anything else in the space.<p>If the question you&#x27;re asking is why so many folks are focused on experimentation instead of productionizing these models, then I see where you&#x27;re coming from. There&#x27;s the question of how much LLMs are actually being used in prod scenarios right now as opposed to just excited people chucking things at them; that maybe LLMs are more just fun playthings than tools for production. But in my experience as HN has gotten bigger, the number of posters talking about productionizing anything has really gone down. I suspect the userbase has become more broadly &quot;interested in software&quot; rather than &quot;ships production facing code&quot; and the enthusiasm in these comments reflects those interests.<p>FWIW we use some LLMs in production and we do <i>not</i> use ollama at all. Our prod story is very different than what folks are talking about here and I&#x27;d love to have a thread that focuses more on language model prod deployments.</div><br/><div id="39721334" class="c"><input type="checkbox" id="c-39721334" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721125">parent</a><span>|</span><a href="#39721148">next</a><span>|</span><label class="collapse" for="c-39721334">[-]</label><label class="expand" for="c-39721334">[10 more]</label></div><br/><div class="children"><div class="content">Well you would be one of the few hundred people on the planet doing that. With local LLMs we&#x27;re just trying to create a way for everyone else to use AI that doesn&#x27;t require sharing all their data with them. First thing everyone asks for of course is how to turn the open source local llms into their own online service.</div><br/><div id="39721568" class="c"><input type="checkbox" id="c-39721568" checked=""/><div class="controls bullet"><span class="by">Karrot_Kream</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721334">parent</a><span>|</span><a href="#39721884">next</a><span>|</span><label class="collapse" for="c-39721568">[-]</label><label class="expand" for="c-39721568">[5 more]</label></div><br/><div class="children"><div class="content">Ollama&#x27;s purpose and usefulness is clear. I don&#x27;t think anyone is disputing that nor the large usability gains ollama has driven. At least I&#x27;m not.<p>As far as being one of the few hundred on the planet, well yeah that&#x27;s why I&#x27;m on HN. There&#x27;s tons of publications and subreddits and fora for generic tech conversation. I come here because I want to talk about the unknowns.</div><br/><div id="39722096" class="c"><input type="checkbox" id="c-39722096" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721568">parent</a><span>|</span><a href="#39721884">next</a><span>|</span><label class="collapse" for="c-39722096">[-]</label><label class="expand" for="c-39722096">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I come here because I want to talk about the unknowns.<p>Your knowns an are unknowns to some people and vice versa. This is a great strength of HN; on a whole lot of subject you’ll find people ranging from enthusiastic to expert. There are probably subreddits or discord servers tailored to narrow niches and that’s cool, but HN is not that. They are complementary, if anything. In contrast, HN is much more interesting and with a much better S&#x2F;N ratio than generic tech subreddits, it’s not even comparable.</div><br/><div id="39722190" class="c"><input type="checkbox" id="c-39722190" checked=""/><div class="controls bullet"><span class="by">Karrot_Kream</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39722096">parent</a><span>|</span><a href="#39721884">next</a><span>|</span><label class="collapse" for="c-39722190">[-]</label><label class="expand" for="c-39722190">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using this site since 2008. This is my second account from 2009. HN very much used to be a tech niche site. I realize that for newer users to HN, the appeal is like r&#x2F;programming or r&#x2F;technology but with a more text oriented interface or higher SNR or whatever but this is a shift in audience and there are folks like I on this site who still want to use it for niche content.<p>There are still threads where people do discuss gory details, even if the topics aren&#x27;t technical. A lot of the mapping articles on the site bring out folks with deep knowledge about mapping stacks. Alternate energy threads do it too. It can be like that for LLMs also, but the user base has to want this site to be more than just Yet Another Tech News Aggregator thread.<p>For me as of late I&#x27;ve come to realize that the current audience wants YATNE more than they want deep discussion here and so I modulate my time here accordingly. The LLM threads bring me here because experts like jart chime in.</div><br/><div id="39722413" class="c"><input type="checkbox" id="c-39722413" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39722190">parent</a><span>|</span><a href="#39722453">prev</a><span>|</span><a href="#39721884">next</a><span>|</span><label class="collapse" for="c-39722413">[-]</label><label class="expand" for="c-39722413">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve been using this site since 2008. This is my second account from 2009.<p>I did not really like HN back in the day because it felt too startup-y, but maybe I got a wrong impression. I much preferred Ars Technica and their forum (now, Ars is much less compelling).<p>&gt; For me as of late I&#x27;ve come to realize that the current audience wants YATNE more than they want deep discussion here and so I modulate my time here accordingly.<p>I think it depends on the stories. Different subjects have different demographics, and I have almost completely stopped reading physics stuff because it is way too Reddit-like (and full of confident people asserting embarrassingly wrong facts). I can see how you could feel about fields closer to your interests being dumbed down by over-confident non-specialists.<p>There are still good, highly technical discussions, but it is true that the home page is a bit limited and inefficient to find them.</div><br/></div></div></div></div></div></div></div></div><div id="39721884" class="c"><input type="checkbox" id="c-39721884" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721334">parent</a><span>|</span><a href="#39721568">prev</a><span>|</span><a href="#39721148">next</a><span>|</span><label class="collapse" for="c-39721884">[-]</label><label class="expand" for="c-39721884">[4 more]</label></div><br/><div class="children"><div class="content">Few hundred on the planet? Are you kidding me? We&#x27;re asking enterprises to run LLMs on-premise (I&#x27;m intentionally discounting the cloud scenario where the traffic rates are much higher). That&#x27;s way more than a hundred and sorry to break it to you that Ollama is just not going to cut it.</div><br/><div id="39721920" class="c"><input type="checkbox" id="c-39721920" checked=""/><div class="controls bullet"><span class="by">Karrot_Kream</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721884">parent</a><span>|</span><a href="#39721148">next</a><span>|</span><label class="collapse" for="c-39721920">[-]</label><label class="expand" for="c-39721920">[3 more]</label></div><br/><div class="children"><div class="content">No need to be angry about this. Tech folks should be discussing this collectively and collaboratively. There&#x27;s space for everything from local models running on smartphones all the way up to OpenAI style industrialized models. Back when social networks were first coming out, I used to read lots of comments about deploying and running distributed systems. I remember reading early incident reports about hotspotting and consistent hashing and TTL problems and such. We need to foster more of that kind of conversation for LMs. Sadly right now Xitter seems to be the best place for that.</div><br/><div id="39722007" class="c"><input type="checkbox" id="c-39722007" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721920">parent</a><span>|</span><a href="#39721148">next</a><span>|</span><label class="collapse" for="c-39722007">[-]</label><label class="expand" for="c-39722007">[2 more]</label></div><br/><div class="children"><div class="content">Not angry. Having a discussion :-). It just amazes me how the HN crowd is more than happy with just trying out a model on their machine and calling it a day and not seeing the real picture ahead. Let ignore perf concerns for a moment. Let&#x27;s say I want to run it on a shared server in the enterprise network so that any application can make use of it. Each application might want to use a model of their choosing. Ollama will unload&#x2F;load&#x2F;unload models as each new request arrives. Not sure if folks here are realizing this :-)</div><br/><div id="39722674" class="c"><input type="checkbox" id="c-39722674" checked=""/><div class="controls bullet"><span class="by">theshackleford</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39722007">parent</a><span>|</span><a href="#39721148">next</a><span>|</span><label class="collapse" for="c-39722674">[-]</label><label class="expand" for="c-39722674">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Not sure if folks here are realizing this :-)<p>I’m not sure you’re capable of understanding that your needs and requirements are just that, yours.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39721148" class="c"><input type="checkbox" id="c-39721148" checked=""/><div class="controls bullet"><span class="by">evilduck</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39721125">prev</a><span>|</span><a href="#39720346">next</a><span>|</span><label class="collapse" for="c-39721148">[-]</label><label class="expand" for="c-39721148">[2 more]</label></div><br/><div class="children"><div class="content">I am 100% uninterested in your production deployment of rent seeking behavior for tools and models I can run myself. Ollama empowers me to do more of that easier. That’s why it’s popular.</div><br/><div id="39721806" class="c"><input type="checkbox" id="c-39721806" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721148">parent</a><span>|</span><a href="#39720346">next</a><span>|</span><label class="collapse" for="c-39721806">[-]</label><label class="expand" for="c-39721806">[1 more]</label></div><br/><div class="children"><div class="content">OP’s point is more that Ollama isn’t what’s doing the empowering. Llama.cpp is.</div><br/></div></div></div></div><div id="39720346" class="c"><input type="checkbox" id="c-39720346" checked=""/><div class="controls bullet"><span class="by">vikramkr</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39721148">prev</a><span>|</span><a href="#39720375">next</a><span>|</span><label class="collapse" for="c-39720346">[-]</label><label class="expand" for="c-39720346">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s nice for personal use which is what I think it was built for, has some nice frontend options too. The tooling around it is nice, and there are projects building in rag etc. I don&#x27;t think people are intending to deploy days services through these tools</div><br/></div></div><div id="39720375" class="c"><input type="checkbox" id="c-39720375" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39720346">prev</a><span>|</span><a href="#39721531">next</a><span>|</span><label class="collapse" for="c-39720375">[-]</label><label class="expand" for="c-39720375">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like you are dismissing Ollama as a &quot;toy&quot;.<p>Refer:<p><a href="https:&#x2F;&#x2F;paulgraham.com&#x2F;startupideas.html" rel="nofollow">https:&#x2F;&#x2F;paulgraham.com&#x2F;startupideas.html</a></div><br/></div></div><div id="39721531" class="c"><input type="checkbox" id="c-39721531" checked=""/><div class="controls bullet"><span class="by">elwebmaster</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39720375">prev</a><span>|</span><a href="#39720977">next</a><span>|</span><label class="collapse" for="c-39721531">[-]</label><label class="expand" for="c-39721531">[1 more]</label></div><br/><div class="children"><div class="content">You are not making any sense. I am running ollama and Open WebUI (which takes care of auth) in production.</div><br/></div></div><div id="39720977" class="c"><input type="checkbox" id="c-39720977" checked=""/><div class="controls bullet"><span class="by">airocker</span><span>|</span><a href="#39720228">parent</a><span>|</span><a href="#39721531">prev</a><span>|</span><a href="#39718917">next</a><span>|</span><label class="collapse" for="c-39720977">[-]</label><label class="expand" for="c-39720977">[8 more]</label></div><br/><div class="children"><div class="content">More than one is easy: put it behind a load balancer. Put one ollama in one container or one port.</div><br/><div id="39721902" class="c"><input type="checkbox" id="c-39721902" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39720977">parent</a><span>|</span><a href="#39721665">next</a><span>|</span><label class="collapse" for="c-39721902">[-]</label><label class="expand" for="c-39721902">[3 more]</label></div><br/><div class="children"><div class="content">FWIW Ollama has no concurrency support even though llama.cpp&#x27;s server component (the thing that Ollama actually uses) supports it. Besides, you can&#x27;t have more than 1 model running. Unloading and loading models is not free. Again, there&#x27;s a lot more and really much of the real optimization work is not in Ollama; it&#x27;s in llama.cpp which is completely ignored in this equation.</div><br/><div id="39721967" class="c"><input type="checkbox" id="c-39721967" checked=""/><div class="controls bullet"><span class="by">airocker</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721902">parent</a><span>|</span><a href="#39721665">next</a><span>|</span><label class="collapse" for="c-39721967">[-]</label><label class="expand" for="c-39721967">[2 more]</label></div><br/><div class="children"><div class="content">Thanks! Great to know. I did not know llama.cpp could do this. It should be pretty straight forward to support, not sure why they would not do it.</div><br/><div id="39723149" class="c"><input type="checkbox" id="c-39723149" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721967">parent</a><span>|</span><a href="#39721665">next</a><span>|</span><label class="collapse" for="c-39723149">[-]</label><label class="expand" for="c-39723149">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure their primary focus right now is to gain as much mindshare as possible and they seem to be doing a great job of it.  If you look at the following GitHub metrics:<p><a href="https:&#x2F;&#x2F;devboard.gitsense.com&#x2F;ggerganov?r=ggerganov%2Fllama.cpp&amp;nb=true" rel="nofollow">https:&#x2F;&#x2F;devboard.gitsense.com&#x2F;ggerganov?r=ggerganov%2Fllama....</a><p><a href="https:&#x2F;&#x2F;devboard.gitsense.com&#x2F;ollama?r=ollama%2Follama&amp;nb=true" rel="nofollow">https:&#x2F;&#x2F;devboard.gitsense.com&#x2F;ollama?r=ollama%2Follama&amp;nb=tr...</a><p>The number of people engaging with ollama is twice that of llama.cpp.  And there hasn&#x27;t been a dip in people engaging with Ollama in the past 6 months.  However, what I do find interesting with regards to these two projects is the number of merged pull requests.  If you click on the &quot;Groups&quot; tab and look at &quot;Hooray&quot;, you can see llama.cpp had 72 contributors with one or more merged pull requests vs 25 for Ollama.<p>For Ollama, people are certainly more interested in commenting and raising issues.  Compare this to llama.cpp, where the number of people contributing code changes is double that of Ollama.<p>I know llama.cpp is VC funded and if they don&#x27;t focus on make using llama.cpp as easy to use as Ollama, they may find themselves doing all the hard stuff with Ollama reaping all the benefits.<p>Full Disclosure:  The tool that I used is mine.</div><br/></div></div></div></div></div></div><div id="39721665" class="c"><input type="checkbox" id="c-39721665" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39720977">parent</a><span>|</span><a href="#39721902">prev</a><span>|</span><a href="#39718917">next</a><span>|</span><label class="collapse" for="c-39721665">[-]</label><label class="expand" for="c-39721665">[4 more]</label></div><br/><div class="children"><div class="content">That is still one model per instance of Ollama, right?</div><br/><div id="39721758" class="c"><input type="checkbox" id="c-39721758" checked=""/><div class="controls bullet"><span class="by">airocker</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721665">parent</a><span>|</span><a href="#39718917">next</a><span>|</span><label class="collapse" for="c-39721758">[-]</label><label class="expand" for="c-39721758">[3 more]</label></div><br/><div class="children"><div class="content">yes, not sure you can do better than that. You cannot still have one instance of LLM in (GPU) memory answer two queries at one time.</div><br/><div id="39721907" class="c"><input type="checkbox" id="c-39721907" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721758">parent</a><span>|</span><a href="#39718917">next</a><span>|</span><label class="collapse" for="c-39721907">[-]</label><label class="expand" for="c-39721907">[2 more]</label></div><br/><div class="children"><div class="content">Of course, you can support concurrent requests. But Ollama doesn&#x27;t support it and it&#x27;s not meant for this purpose and that&#x27;s perfectly ok. That&#x27;s not the point though. For fast&#x2F;perf scenarios, you&#x27;re better off with vllm.</div><br/><div id="39721937" class="c"><input type="checkbox" id="c-39721937" checked=""/><div class="controls bullet"><span class="by">airocker</span><span>|</span><a href="#39720228">root</a><span>|</span><a href="#39721907">parent</a><span>|</span><a href="#39718917">next</a><span>|</span><label class="collapse" for="c-39721937">[-]</label><label class="expand" for="c-39721937">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! This is great to know.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39718917" class="c"><input type="checkbox" id="c-39718917" checked=""/><div class="controls bullet"><span class="by">sofixa</span><span>|</span><a href="#39720228">prev</a><span>|</span><a href="#39720983">next</a><span>|</span><label class="collapse" for="c-39718917">[-]</label><label class="expand" for="c-39718917">[10 more]</label></div><br/><div class="children"><div class="content">This is great news. The more projects do this, the less of a moat CUDA is, and the less of a competitive advantage Nvidia has.</div><br/><div id="39719093" class="c"><input type="checkbox" id="c-39719093" checked=""/><div class="controls bullet"><span class="by">anonymous-panda</span><span>|</span><a href="#39718917">parent</a><span>|</span><a href="#39719138">next</a><span>|</span><label class="collapse" for="c-39719093">[-]</label><label class="expand" for="c-39719093">[8 more]</label></div><br/><div class="children"><div class="content">What does performance look like?</div><br/><div id="39719111" class="c"><input type="checkbox" id="c-39719111" checked=""/><div class="controls bullet"><span class="by">sevagh</span><span>|</span><a href="#39718917">root</a><span>|</span><a href="#39719093">parent</a><span>|</span><a href="#39719410">next</a><span>|</span><label class="collapse" for="c-39719111">[-]</label><label class="expand" for="c-39719111">[6 more]</label></div><br/><div class="children"><div class="content">Spoiler alert: not good enough to break CUDA&#x27;s moat</div><br/><div id="39719469" class="c"><input type="checkbox" id="c-39719469" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#39718917">root</a><span>|</span><a href="#39719111">parent</a><span>|</span><a href="#39719565">next</a><span>|</span><label class="collapse" for="c-39719469">[-]</label><label class="expand" for="c-39719469">[1 more]</label></div><br/><div class="children"><div class="content">This is not CUDA&#x27;s moat. That is on the R&amp;D&#x2F;training side.<p>Inference side is partly about performance, but mostly about cost per token.<p>And given that there has been a ton of standardization around LLaMA architectures, AMD&#x2F;ROCm can target this much more easily, and still take a nice chunk of the inference market for non-SOTA models.</div><br/></div></div><div id="39719565" class="c"><input type="checkbox" id="c-39719565" checked=""/><div class="controls bullet"><span class="by">bornfreddy</span><span>|</span><a href="#39718917">root</a><span>|</span><a href="#39719111">parent</a><span>|</span><a href="#39719469">prev</a><span>|</span><a href="#39719410">next</a><span>|</span><label class="collapse" for="c-39719565">[-]</label><label class="expand" for="c-39719565">[4 more]</label></div><br/><div class="children"><div class="content">Not sure why you&#x27;re downvoted, but as far as I&#x27;ve heard AMD cards can&#x27;t beat 4090 - yet.<p>Still, I think AMD will catch or overtake NVidia in hardware soon, but software is a bigger problem. Hopefully the opensource strategy will pay off for them.</div><br/><div id="39719859" class="c"><input type="checkbox" id="c-39719859" checked=""/><div class="controls bullet"><span class="by">arein3</span><span>|</span><a href="#39718917">root</a><span>|</span><a href="#39719565">parent</a><span>|</span><a href="#39720499">next</a><span>|</span><label class="collapse" for="c-39719859">[-]</label><label class="expand" for="c-39719859">[1 more]</label></div><br/><div class="children"><div class="content">Really hope so, maybe this time will catch and last<p>Usually when corps open source stuff to get adoption, they stuff the adopters after they gain enough market share and the cycle repeats again</div><br/></div></div><div id="39720499" class="c"><input type="checkbox" id="c-39720499" checked=""/><div class="controls bullet"><span class="by">nerdix</span><span>|</span><a href="#39718917">root</a><span>|</span><a href="#39719565">parent</a><span>|</span><a href="#39719859">prev</a><span>|</span><a href="#39719410">next</a><span>|</span><label class="collapse" for="c-39720499">[-]</label><label class="expand" for="c-39720499">[2 more]</label></div><br/><div class="children"><div class="content">A RTX 4090 is about twice the price of and 50%-ish faster than AMD&#x27;s most expensive consumer card so I&#x27;m not sure anyone really expects it to ever surpass a 4090.<p>A 7900 XTX beating a RTX 4080 at inference is probably a more realistic goal though I&#x27;m not sure how they compare right now.</div><br/><div id="39721703" class="c"><input type="checkbox" id="c-39721703" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39718917">root</a><span>|</span><a href="#39720499">parent</a><span>|</span><a href="#39719410">next</a><span>|</span><label class="collapse" for="c-39721703">[-]</label><label class="expand" for="c-39721703">[1 more]</label></div><br/><div class="children"><div class="content">The 4080 is $1k for 16gb of VRAM, and the 7900 is $1k for 24gb of VRAM. Unless you&#x27;re constantly hammering it with requests, the extra speed you may get with CUDA on a 4080 is basically irrelevant when you can run much better models at a reasonable speed.</div><br/></div></div></div></div></div></div></div></div><div id="39719410" class="c"><input type="checkbox" id="c-39719410" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39718917">root</a><span>|</span><a href="#39719093">parent</a><span>|</span><a href="#39719111">prev</a><span>|</span><a href="#39719138">next</a><span>|</span><label class="collapse" for="c-39719410">[-]</label><label class="expand" for="c-39719410">[1 more]</label></div><br/><div class="children"><div class="content">I get 35tps on Mistral:7b-Instruct-Q6_K with my 6650 XT.</div><br/></div></div></div></div><div id="39719138" class="c"><input type="checkbox" id="c-39719138" checked=""/><div class="controls bullet"><span class="by">ixaxaar</span><span>|</span><a href="#39718917">parent</a><span>|</span><a href="#39719093">prev</a><span>|</span><a href="#39720983">next</a><span>|</span><label class="collapse" for="c-39719138">[-]</label><label class="expand" for="c-39719138">[1 more]</label></div><br/><div class="children"><div class="content">Hey I did, and sorry for the self promo,<p>Please check out <a href="https:&#x2F;&#x2F;github.com&#x2F;geniusrise">https:&#x2F;&#x2F;github.com&#x2F;geniusrise</a> - tool for running llms and other stuff, behaves like docker compose, works with whatever is supported by underlying engines:<p>Huggingface - MPS, cuda
VLLM - cuda, ROCm
llama.cpp, whisper.cpp - cuda, mps, rocm<p>Also coming up integration with spark (TorchDistributor), kafka and airflow.</div><br/></div></div></div></div><div id="39720983" class="c"><input type="checkbox" id="c-39720983" checked=""/><div class="controls bullet"><span class="by">jjice</span><span>|</span><a href="#39718917">prev</a><span>|</span><a href="#39719173">next</a><span>|</span><label class="collapse" for="c-39720983">[-]</label><label class="expand" for="c-39720983">[1 more]</label></div><br/><div class="children"><div class="content">Just downloaded this and gave it a go. I have no experience with running any local models, but this just worked out of the box on my 7600 on Ubuntu 22. This is fantastic.</div><br/></div></div><div id="39719173" class="c"><input type="checkbox" id="c-39719173" checked=""/><div class="controls bullet"><span class="by">KronisLV</span><span>|</span><a href="#39720983">prev</a><span>|</span><a href="#39720014">next</a><span>|</span><label class="collapse" for="c-39719173">[-]</label><label class="expand" for="c-39719173">[19 more]</label></div><br/><div class="children"><div class="content">Feels like all of this local LLM stuff is definitely pushing people in the direction of getting new hardware, since nothing like RX 570&#x2F;580 or other older cards sees support.<p>On one hand, the hardware nowadays is better and more powerful, but on the other, the initial version of CUDA came out in 2007 and ROCm in 2016. You&#x27;d think that compute on GPUs wouldn&#x27;t require the latest cards.</div><br/><div id="39719408" class="c"><input type="checkbox" id="c-39719408" checked=""/><div class="controls bullet"><span class="by">mysteria</span><span>|</span><a href="#39719173">parent</a><span>|</span><a href="#39719518">next</a><span>|</span><label class="collapse" for="c-39719408">[-]</label><label class="expand" for="c-39719408">[1 more]</label></div><br/><div class="children"><div class="content">The Ollama backend llama.cpp definitely supports those older cards with the OpenCL and Vulkan backends, though performance is worse than ROCm or CUDA. In their Vulkan thread for instance I see people getting it working with Polaris and even Hawaii cards.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;2059">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;2059</a><p>Personally I just run it on CPU and several tokens&#x2F;s is good enough for my purposes.</div><br/></div></div><div id="39719518" class="c"><input type="checkbox" id="c-39719518" checked=""/><div class="controls bullet"><span class="by">bradley13</span><span>|</span><a href="#39719173">parent</a><span>|</span><a href="#39719408">prev</a><span>|</span><a href="#39719384">next</a><span>|</span><label class="collapse" for="c-39719518">[-]</label><label class="expand" for="c-39719518">[1 more]</label></div><br/><div class="children"><div class="content">No new hardware needed. I was shocked that Mixtral runs well on my laptop, which has a so-so mobile GPU. Mixtral isn&#x27;t hugely fast, but definitely good enough!</div><br/></div></div><div id="39719384" class="c"><input type="checkbox" id="c-39719384" checked=""/><div class="controls bullet"><span class="by">hugozap</span><span>|</span><a href="#39719173">parent</a><span>|</span><a href="#39719518">prev</a><span>|</span><a href="#39720781">next</a><span>|</span><label class="collapse" for="c-39719384">[-]</label><label class="expand" for="c-39719384">[13 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a happy user of Mistral on my Mac Air M1.</div><br/><div id="39719462" class="c"><input type="checkbox" id="c-39719462" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719384">parent</a><span>|</span><a href="#39719474">next</a><span>|</span><label class="collapse" for="c-39719462">[-]</label><label class="expand" for="c-39719462">[7 more]</label></div><br/><div class="children"><div class="content">How many gbs of RAM do you have in your M1 machine?</div><br/><div id="39719527" class="c"><input type="checkbox" id="c-39719527" checked=""/><div class="controls bullet"><span class="by">hugozap</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719462">parent</a><span>|</span><a href="#39719474">next</a><span>|</span><label class="collapse" for="c-39719527">[-]</label><label class="expand" for="c-39719527">[6 more]</label></div><br/><div class="children"><div class="content">8gb</div><br/><div id="39719592" class="c"><input type="checkbox" id="c-39719592" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719527">parent</a><span>|</span><a href="#39719474">next</a><span>|</span><label class="collapse" for="c-39719592">[-]</label><label class="expand" for="c-39719592">[5 more]</label></div><br/><div class="children"><div class="content">Thanks, wow, amazing that you can already run a small model with so little ram. I need to buy a new laptop, guess more than 16 gb on a macbook isn&#x27;t really needed</div><br/><div id="39721231" class="c"><input type="checkbox" id="c-39721231" checked=""/><div class="controls bullet"><span class="by">evilduck</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719592">parent</a><span>|</span><a href="#39720075">next</a><span>|</span><label class="collapse" for="c-39721231">[-]</label><label class="expand" for="c-39721231">[1 more]</label></div><br/><div class="children"><div class="content">I use several LLM models locally for chat UIs and IDE autocompletions like copilot (continue.dev).<p>Between Teams, Chrome, VS Code, Outlook, and now LLMs my RAM usage sits around 20-22GB. 16GB will be a bottleneck to utility.</div><br/></div></div><div id="39720075" class="c"><input type="checkbox" id="c-39720075" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719592">parent</a><span>|</span><a href="#39721231">prev</a><span>|</span><a href="#39720439">next</a><span>|</span><label class="collapse" for="c-39720075">[-]</label><label class="expand" for="c-39720075">[1 more]</label></div><br/><div class="children"><div class="content">I would advise getting as much RAM as you possibly can. You can&#x27;t upgrade later, so get as much as you can afford.<p>Mine is 64GB, and my memory pressure goes into the red when running a quantized 70B model with a dozen Chrome tabs open.</div><br/></div></div><div id="39720439" class="c"><input type="checkbox" id="c-39720439" checked=""/><div class="controls bullet"><span class="by">TylerE</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719592">parent</a><span>|</span><a href="#39720075">prev</a><span>|</span><a href="#39720149">next</a><span>|</span><label class="collapse" for="c-39720439">[-]</label><label class="expand" for="c-39720439">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve run LLMs and some of the various image models on my M1 Studio 32GB without issue. Not as fast as my old 3080 card, but considering the Mac all in has about a 5th the power draw, it&#x27;s a <i>lot</i> closer than I expected. I&#x27;m not sure of the exact details but there is clearly some secret sauce that allows it to leverage the onboard NN hardware.</div><br/></div></div><div id="39720149" class="c"><input type="checkbox" id="c-39720149" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719592">parent</a><span>|</span><a href="#39720439">prev</a><span>|</span><a href="#39719474">next</a><span>|</span><label class="collapse" for="c-39720149">[-]</label><label class="expand" for="c-39720149">[1 more]</label></div><br/><div class="children"><div class="content">Mistral is _very_ small when quantized.<p>I’d still go with 16gbs</div><br/></div></div></div></div></div></div></div></div><div id="39719474" class="c"><input type="checkbox" id="c-39719474" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719384">parent</a><span>|</span><a href="#39719462">prev</a><span>|</span><a href="#39720781">next</a><span>|</span><label class="collapse" for="c-39719474">[-]</label><label class="expand" for="c-39719474">[5 more]</label></div><br/><div class="children"><div class="content">Is it easy to set this up?</div><br/><div id="39719521" class="c"><input type="checkbox" id="c-39719521" checked=""/><div class="controls bullet"><span class="by">hugozap</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719474">parent</a><span>|</span><a href="#39719495">next</a><span>|</span><label class="collapse" for="c-39719521">[-]</label><label class="expand" for="c-39719521">[1 more]</label></div><br/><div class="children"><div class="content">It is, it doesn&#x27;t require any setup.<p>After installation:<p>&gt; ollama run mistral:latest</div><br/></div></div><div id="39719495" class="c"><input type="checkbox" id="c-39719495" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719474">parent</a><span>|</span><a href="#39719521">prev</a><span>|</span><a href="#39720781">next</a><span>|</span><label class="collapse" for="c-39719495">[-]</label><label class="expand" for="c-39719495">[3 more]</label></div><br/><div class="children"><div class="content">Super easy. You can just head down to <a href="https:&#x2F;&#x2F;lmstudio.ai" rel="nofollow">https:&#x2F;&#x2F;lmstudio.ai</a> and pick up an app that lets you play around. It&#x27;s not particularly advanced, but it works pretty well.<p>It&#x27;s mostly optimized for M-series silicon, but it also technically works on Windows, and isn&#x27;t too difficult to trick into working on Linux either.</div><br/><div id="39719517" class="c"><input type="checkbox" id="c-39719517" checked=""/><div class="controls bullet"><span class="by">glial</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719495">parent</a><span>|</span><a href="#39720781">next</a><span>|</span><label class="collapse" for="c-39719517">[-]</label><label class="expand" for="c-39719517">[2 more]</label></div><br/><div class="children"><div class="content">Also, <a href="https:&#x2F;&#x2F;jan.ai" rel="nofollow">https:&#x2F;&#x2F;jan.ai</a> is open source and worth trying out too.</div><br/><div id="39719576" class="c"><input type="checkbox" id="c-39719576" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#39719173">root</a><span>|</span><a href="#39719517">parent</a><span>|</span><a href="#39720781">next</a><span>|</span><label class="collapse" for="c-39719576">[-]</label><label class="expand" for="c-39719576">[1 more]</label></div><br/><div class="children"><div class="content">Looks super cool, though it seems to be missing a good chunk of features, like the ability to change the prompt format. (Just installed it myself to check out all the options.) All the other missing stuff I can see though is stuff that LM Studio doesn&#x27;t have either (such as a notebook mode). If it has a good chat mode then that&#x27;s good enough for most!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39720781" class="c"><input type="checkbox" id="c-39720781" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39719173">parent</a><span>|</span><a href="#39719384">prev</a><span>|</span><a href="#39721759">next</a><span>|</span><label class="collapse" for="c-39720781">[-]</label><label class="expand" for="c-39720781">[1 more]</label></div><br/><div class="children"><div class="content">The compatibility matrix is quite complex for both AMD and NVIDIA graphics cards, and completely agree: there is a lot of work to do, but the hope is to gracefully fall back to older cards.. they still speed up inference quite a bit when they do work!</div><br/></div></div><div id="39721759" class="c"><input type="checkbox" id="c-39721759" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39719173">parent</a><span>|</span><a href="#39720781">prev</a><span>|</span><a href="#39720698">next</a><span>|</span><label class="collapse" for="c-39721759">[-]</label><label class="expand" for="c-39721759">[1 more]</label></div><br/><div class="children"><div class="content">My 1080ti runs ok even this 47B model: <a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;dolphin-mixtral">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;dolphin-mixtral</a></div><br/></div></div><div id="39720698" class="c"><input type="checkbox" id="c-39720698" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#39719173">parent</a><span>|</span><a href="#39721759">prev</a><span>|</span><a href="#39720014">next</a><span>|</span><label class="collapse" for="c-39720698">[-]</label><label class="expand" for="c-39720698">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp added first class support for the RX 580 by implementing the vulkan backend. There are some issues on older kernel amdgpu code where a llm process VRAM is never reloaded if it gets kicked out to GTT (in 5.x kernels) but overall it&#x27;s much faster than the clBLAST opencl implementation.</div><br/></div></div></div></div><div id="39720014" class="c"><input type="checkbox" id="c-39720014" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#39719173">prev</a><span>|</span><a href="#39719094">next</a><span>|</span><label class="collapse" for="c-39720014">[-]</label><label class="expand" for="c-39720014">[4 more]</label></div><br/><div class="children"><div class="content">Here is the commit that added ROCm support to llama.cpp back in August:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;commit&#x2F;6bbc598a632560cb45dd2c51ad403bda8723b629">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;commit&#x2F;6bbc598a632560...</a></div><br/><div id="39723405" class="c"><input type="checkbox" id="c-39723405" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#39720014">parent</a><span>|</span><a href="#39719094">next</a><span>|</span><label class="collapse" for="c-39723405">[-]</label><label class="expand" for="c-39723405">[3 more]</label></div><br/><div class="children"><div class="content">Yep, and it deserves the credit! He who writes the cuda kernel (or translates it) controls the spice.<p>I had wrapped this and had it working in Ollama months ago as well: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;pull&#x2F;814">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;pull&#x2F;814</a>. I don&#x27;t use Ollama anymore, but I really like the way they handle device memory allocation dynamically, I think they were the first to do this well.</div><br/><div id="39723587" class="c"><input type="checkbox" id="c-39723587" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#39720014">root</a><span>|</span><a href="#39723405">parent</a><span>|</span><a href="#39719094">next</a><span>|</span><label class="collapse" for="c-39723587">[-]</label><label class="expand" for="c-39723587">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious about both:<p>- what&#x27;s special about the memory allocation, and how might it help me?<p>- what are you now using instead of ollama?</div><br/><div id="39723669" class="c"><input type="checkbox" id="c-39723669" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#39720014">root</a><span>|</span><a href="#39723587">parent</a><span>|</span><a href="#39719094">next</a><span>|</span><label class="collapse" for="c-39723669">[-]</label><label class="expand" for="c-39723669">[1 more]</label></div><br/><div class="children"><div class="content">Ollama does a nice job of looking at how much VRAM the card has and tuning the number of gpu layers offloaded. Before that, I mainly just had to guess. It&#x27;s still a heuristic, but I thought that was neat.<p>I&#x27;m mainly just using llama.cpp as a native library now, mainly for the direct access to more of llama&#x27;s data structures, and because I have a sort of unique sampler setup.</div><br/></div></div></div></div></div></div></div></div><div id="39719094" class="c"><input type="checkbox" id="c-39719094" checked=""/><div class="controls bullet"><span class="by">reilly3000</span><span>|</span><a href="#39720014">prev</a><span>|</span><a href="#39718962">next</a><span>|</span><label class="collapse" for="c-39719094">[-]</label><label class="expand" for="c-39719094">[31 more]</label></div><br/><div class="children"><div class="content">I’m curious as to how they pulled this off. OpenCL isn’t that common in the wild relative to Cuda. Hopefully it can become robust and widespread soon enough. I personally succumbed to the pressure and spent a relative fortune on a 4090 but wish I had some choice in the matter.</div><br/><div id="39719124" class="c"><input type="checkbox" id="c-39719124" checked=""/><div class="controls bullet"><span class="by">Apofis</span><span>|</span><a href="#39719094">parent</a><span>|</span><a href="#39719898">next</a><span>|</span><label class="collapse" for="c-39719124">[-]</label><label class="expand" for="c-39719124">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised they didn&#x27;t speak about the implementation at all. Anyone got more intel?</div><br/><div id="39719167" class="c"><input type="checkbox" id="c-39719167" checked=""/><div class="controls bullet"><span class="by">harwoodr</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719124">parent</a><span>|</span><a href="#39719142">next</a><span>|</span><label class="collapse" for="c-39719167">[-]</label><label class="expand" for="c-39719167">[4 more]</label></div><br/><div class="children"><div class="content">ROCm:  <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;commit&#x2F;6c5ccb11f993ccc88c4761b8c31e0fefcbc1900f">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;commit&#x2F;6c5ccb11f993ccc88c47...</a></div><br/><div id="39719824" class="c"><input type="checkbox" id="c-39719824" checked=""/><div class="controls bullet"><span class="by">skipants</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719167">parent</a><span>|</span><a href="#39719142">next</a><span>|</span><label class="collapse" for="c-39719824">[-]</label><label class="expand" for="c-39719824">[3 more]</label></div><br/><div class="children"><div class="content">Another giveaway that it&#x27;s ROCm is that it doesn&#x27;t support the 5700 series...<p>I&#x27;m really salty because I &quot;upgraded&quot; to a 5700XT from a Nvidia GTX 1070 and can&#x27;t do AI on the GPU anymore, purely because the software is unsupported.<p>But, as a dev, I suppose I should feel some empathy that there&#x27;s probably some really difficult problem causing 5700XT to be unsupported by ROCm.</div><br/><div id="39720250" class="c"><input type="checkbox" id="c-39720250" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719824">parent</a><span>|</span><a href="#39719142">next</a><span>|</span><label class="collapse" for="c-39720250">[-]</label><label class="expand" for="c-39720250">[2 more]</label></div><br/><div class="children"><div class="content">I wrote a bunch of openmp code on a 5700XT a couple of years ago, if you&#x27;re building from source it&#x27;ll probably run fine</div><br/></div></div></div></div></div></div><div id="39719142" class="c"><input type="checkbox" id="c-39719142" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719124">parent</a><span>|</span><a href="#39719167">prev</a><span>|</span><a href="#39719158">next</a><span>|</span><label class="collapse" for="c-39719142">[-]</label><label class="expand" for="c-39719142">[6 more]</label></div><br/><div class="children"><div class="content">They&#x27;re open source and based on llama.cpp so nothings secret.<p>My money, looking at nothing, would be on one of the two Vulkan backends added in Jan&#x2F;Feb.<p>I continue to be flummoxed by a mostly-programmer-forum treating ollama like a magical new commercial entity breaking new ground.<p>It&#x27;s a CLI wrapper around llama.cpp so you don&#x27;t have to figure out how to compile it</div><br/><div id="39719990" class="c"><input type="checkbox" id="c-39719990" checked=""/><div class="controls bullet"><span class="by">washadjeffmad</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719142">parent</a><span>|</span><a href="#39719158">next</a><span>|</span><label class="collapse" for="c-39719990">[-]</label><label class="expand" for="c-39719990">[5 more]</label></div><br/><div class="children"><div class="content">I tried it recently and couldn&#x27;t figure out why it existed. It&#x27;s just a very feature limited app that doesn&#x27;t require you to know anything or be able to read a model card to &quot;do AI&quot;.<p>And that more or less answered it.</div><br/><div id="39720137" class="c"><input type="checkbox" id="c-39720137" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719990">parent</a><span>|</span><a href="#39720152">next</a><span>|</span><label class="collapse" for="c-39720137">[-]</label><label class="expand" for="c-39720137">[3 more]</label></div><br/><div class="children"><div class="content">It’s because most devs nowadays are new devs and probably aren’t very familiar with native compilation.<p>So compiling the correct version of llama.cpp for their hardware is confusing.<p>Compound that with everyone’s relative inexperience with configuring any given model and you have prime grounds for a simple tool to exist.<p>That’s what ollama and their Modelfiles accomplish.</div><br/><div id="39720266" class="c"><input type="checkbox" id="c-39720266" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39720137">parent</a><span>|</span><a href="#39720394">next</a><span>|</span><label class="collapse" for="c-39720266">[-]</label><label class="expand" for="c-39720266">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just because it&#x27;s convenient. I wrote a rich text editor front end for llama.cpp and I originally wrote a quick go web server with streaming using the go bindings, but now I just use ollama because it&#x27;s just simpler and the workflow for pulling down models with their registry and packaging new ones in containers is simpler. Also most people who want to play around with local models aren&#x27;t developers at all.</div><br/></div></div><div id="39720394" class="c"><input type="checkbox" id="c-39720394" checked=""/><div class="controls bullet"><span class="by">mypalmike</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39720137">parent</a><span>|</span><a href="#39720266">prev</a><span>|</span><a href="#39720152">next</a><span>|</span><label class="collapse" for="c-39720394">[-]</label><label class="expand" for="c-39720394">[1 more]</label></div><br/><div class="children"><div class="content">Eh, I&#x27;ve been building native code for decades and hit quite a few roadblocks trying to get llama.cpp building with cuda support on my Ubuntu box. Library version issues and such. Ended up down a rabbit hole related to codenames for the various Nvidia architectures... It&#x27;s a project on hold for now.<p>Weirdly, the Python bindings built without issue with pip.</div><br/></div></div></div></div><div id="39720152" class="c"><input type="checkbox" id="c-39720152" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719990">parent</a><span>|</span><a href="#39720137">prev</a><span>|</span><a href="#39719158">next</a><span>|</span><label class="collapse" for="c-39720152">[-]</label><label class="expand" for="c-39720152">[1 more]</label></div><br/><div class="children"><div class="content">Edited it out of my original comment because I didn&#x27;t want to seem ranty&#x2F;angry&#x2F;like I have some personal vendatta, as opposed to just being extremely puzzled, but it legit took me months to realize it wasn&#x27;t a GUI because of how it&#x27;s discussed on HN, i.e. as key to democratizing, as a large, unique, entity, etc.<p>Hadn&#x27;t thought about it recently. After seeing it again here, and being gobsmacked by the # of genuine, earnest, comments assuming there&#x27;s extensive independent development of large pieces going on in it, I&#x27;m going with:<p>- &quot;The puzzled feeling you have is simply because llama.cpp is a challenge on the best of days, you need to know a lot to get to fully accelerated on ye average MacBook. and technical users don&#x27;t want a GUI for an LLM, they want a way to call an API, so that&#x27;s why there isn&#x27;t content extalling the virtues of GPT4All*. So TL;DR you&#x27;re old and have been on computer too much :P&quot;<p>but I legit don&#x27;t know and still can&#x27;t figure it out.<p>* picked them because they&#x27;re the most recent example of a genuinely democratizing tool that goes far beyond llama.cpp and also makes large contributions back to llama.cpp, ex. GPT4All landed 1 of the 2 vulkan backends</div><br/></div></div></div></div></div></div><div id="39719158" class="c"><input type="checkbox" id="c-39719158" checked=""/><div class="controls bullet"><span class="by">j33zusjuice</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719124">parent</a><span>|</span><a href="#39719142">prev</a><span>|</span><a href="#39719898">next</a><span>|</span><label class="collapse" for="c-39719158">[-]</label><label class="expand" for="c-39719158">[1 more]</label></div><br/><div class="children"><div class="content">Ahhhh, I see what you did there.</div><br/></div></div></div></div><div id="39719898" class="c"><input type="checkbox" id="c-39719898" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#39719094">parent</a><span>|</span><a href="#39719124">prev</a><span>|</span><a href="#39719605">next</a><span>|</span><label class="collapse" for="c-39719898">[-]</label><label class="expand" for="c-39719898">[4 more]</label></div><br/><div class="children"><div class="content">It would serve Nvidia right if their insistence on only running CUDA workloads on their hardware results in adoption of ROCm&#x2F;OpenCL.</div><br/><div id="39720079" class="c"><input type="checkbox" id="c-39720079" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719898">parent</a><span>|</span><a href="#39719973">next</a><span>|</span><label class="collapse" for="c-39720079">[-]</label><label class="expand" for="c-39720079">[2 more]</label></div><br/><div class="children"><div class="content">You can use OpenCL just fine on Nvidia, but CUDA is just a superior compute programming model overall (both in features and design.) Pretty much every vendor offers something superior to OpenCL (HIP, OneAPI, etc), because it simply isn&#x27;t very nice to use.</div><br/><div id="39720858" class="c"><input type="checkbox" id="c-39720858" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39720079">parent</a><span>|</span><a href="#39719973">next</a><span>|</span><label class="collapse" for="c-39720858">[-]</label><label class="expand" for="c-39720858">[1 more]</label></div><br/><div class="children"><div class="content">I suppose that&#x27;s about right. The implementors are busy building on a path to profit and much less concerned about any sort-of lock-in or open standards--that comes much later in the cycle.</div><br/></div></div></div></div><div id="39719973" class="c"><input type="checkbox" id="c-39719973" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719898">parent</a><span>|</span><a href="#39720079">prev</a><span>|</span><a href="#39719605">next</a><span>|</span><label class="collapse" for="c-39719973">[-]</label><label class="expand" for="c-39719973">[1 more]</label></div><br/><div class="children"><div class="content">OpenCL is fine on Nvidia Hardware. Of course it&#x27;s a second class citizen next to CUDA, but then again everything is a second class citizen on AMD hardware.</div><br/></div></div></div></div><div id="39719605" class="c"><input type="checkbox" id="c-39719605" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39719094">parent</a><span>|</span><a href="#39719898">prev</a><span>|</span><a href="#39719966">next</a><span>|</span><label class="collapse" for="c-39719605">[-]</label><label class="expand" for="c-39719605">[4 more]</label></div><br/><div class="children"><div class="content">OpenCL is as dead as OpenGL and the inference implementations that exist are very unperformant. The only real options are CUDA, ROCm, Vulkan and CPU. And Vulkan is a proper pain too, takes forver to build compute shaders and has to do so for each model. It only makes sense on Intel Arc since there&#x27;s nothing else there.</div><br/><div id="39719808" class="c"><input type="checkbox" id="c-39719808" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719605">parent</a><span>|</span><a href="#39719858">next</a><span>|</span><label class="collapse" for="c-39719808">[-]</label><label class="expand" for="c-39719808">[1 more]</label></div><br/><div class="children"><div class="content">SYCL is a fairly direct successor to the OpenCL model and is not quite dead, Intel seems to be betting on it more than others.</div><br/></div></div><div id="39719858" class="c"><input type="checkbox" id="c-39719858" checked=""/><div class="controls bullet"><span class="by">mpreda</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719605">parent</a><span>|</span><a href="#39719808">prev</a><span>|</span><a href="#39719900">next</a><span>|</span><label class="collapse" for="c-39719858">[-]</label><label class="expand" for="c-39719858">[1 more]</label></div><br/><div class="children"><div class="content">ROCm includes OpenCL. And it&#x27;s a very performant OpenCL implementation.</div><br/></div></div><div id="39719900" class="c"><input type="checkbox" id="c-39719900" checked=""/><div class="controls bullet"><span class="by">taminka</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719605">parent</a><span>|</span><a href="#39719858">prev</a><span>|</span><a href="#39719966">next</a><span>|</span><label class="collapse" for="c-39719900">[-]</label><label class="expand" for="c-39719900">[1 more]</label></div><br/><div class="children"><div class="content">why though? except for apple, most vendors still actively support it and newer versions of OpenCL are released…</div><br/></div></div></div></div><div id="39719966" class="c"><input type="checkbox" id="c-39719966" checked=""/><div class="controls bullet"><span class="by">shmerl</span><span>|</span><a href="#39719094">parent</a><span>|</span><a href="#39719605">prev</a><span>|</span><a href="#39719405">next</a><span>|</span><label class="collapse" for="c-39719966">[-]</label><label class="expand" for="c-39719966">[1 more]</label></div><br/><div class="children"><div class="content">May be Vulkan compute? But yeah, interesting how.</div><br/></div></div><div id="39719405" class="c"><input type="checkbox" id="c-39719405" checked=""/><div class="controls bullet"><span class="by">programmarchy</span><span>|</span><a href="#39719094">parent</a><span>|</span><a href="#39719966">prev</a><span>|</span><a href="#39718962">next</a><span>|</span><label class="collapse" for="c-39719405">[-]</label><label class="expand" for="c-39719405">[9 more]</label></div><br/><div class="children"><div class="content">Apple killed off OpenCL for their platforms when they created Metal which was disappointing. Sounds like ROCm will keep it alive but the fragmentation sucks. Gotta support CUDA, OpenCL, and Metal now to be cross-platform.</div><br/><div id="39719805" class="c"><input type="checkbox" id="c-39719805" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719405">parent</a><span>|</span><a href="#39718962">next</a><span>|</span><label class="collapse" for="c-39719805">[-]</label><label class="expand" for="c-39719805">[8 more]</label></div><br/><div class="children"><div class="content">What is OpenCL? AMD GPUs support CUDA. It&#x27;s called HIP. You just need a bunch of #define statements like this:<p><pre><code>    #ifndef __HIP__
    #include &lt;cuda_fp16.h&gt;
    #include &lt;cuda_runtime.h&gt;
    #else
    #include &lt;hip&#x2F;hip_fp16.h&gt;
    #include &lt;hip&#x2F;hip_runtime.h&gt;
    #define cudaSuccess hipSuccess
    #define cudaStream_t hipStream_t
    #define cudaGetLastError hipGetLastError
    #endif
</code></pre>
Then your CUDA code works on AMD.</div><br/><div id="39723186" class="c"><input type="checkbox" id="c-39723186" checked=""/><div class="controls bullet"><span class="by">programmarchy</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719805">parent</a><span>|</span><a href="#39720549">next</a><span>|</span><label class="collapse" for="c-39723186">[-]</label><label class="expand" for="c-39723186">[1 more]</label></div><br/><div class="children"><div class="content">OpenCL is a Khronos open spec for GPU compute, and what you’d use on Apple platforms before Metal compute shaders and CoreML were released. If you wanted to run early ML models on Apple hardware, it was an option. There was an OpenCL backend for torch, for example.</div><br/></div></div><div id="39720549" class="c"><input type="checkbox" id="c-39720549" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39719805">parent</a><span>|</span><a href="#39723186">prev</a><span>|</span><a href="#39718962">next</a><span>|</span><label class="collapse" for="c-39720549">[-]</label><label class="expand" for="c-39720549">[6 more]</label></div><br/><div class="children"><div class="content">Can you explain why nobody knows this trick, for some values of “nobody”?</div><br/><div id="39720722" class="c"><input type="checkbox" id="c-39720722" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39720549">parent</a><span>|</span><a href="#39721171">next</a><span>|</span><label class="collapse" for="c-39720722">[-]</label><label class="expand" for="c-39720722">[1 more]</label></div><br/><div class="children"><div class="content">No idea. My best guess is their background is in graphics and games rather than machine learning. When CUDA is all you&#x27;ve ever known, you try just a little harder to find a way to keep using it elsewhere.</div><br/></div></div><div id="39721171" class="c"><input type="checkbox" id="c-39721171" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39720549">parent</a><span>|</span><a href="#39720722">prev</a><span>|</span><a href="#39718962">next</a><span>|</span><label class="collapse" for="c-39721171">[-]</label><label class="expand" for="c-39721171">[4 more]</label></div><br/><div class="children"><div class="content">People know; it just hasn&#x27;t been reliable.</div><br/><div id="39721424" class="c"><input type="checkbox" id="c-39721424" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39721171">parent</a><span>|</span><a href="#39718962">next</a><span>|</span><label class="collapse" for="c-39721424">[-]</label><label class="expand" for="c-39721424">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s not reliable about it? On Linux hipcc is about as easy to use as gcc. On Windows it&#x27;s a little janky because hipcc is a perl script and there&#x27;s no perl interpreter I&#x27;ll admit. I&#x27;m otherwise happy with it though. It&#x27;d be nice if they had a shell script installer like NVIDIA, so I could use an OS that isn&#x27;t a 2 year old Ubuntu. I own 2 XTX cards but I&#x27;m actually switching back to NVIDIA on my main workstation for that reason alone. GPUs shouldn&#x27;t be choosing winners in the OS world. The lack of a profiler is also a source of frustration. I think the smart thing to do is to develop on NVIDIA and then distribute to AMD. I hope things change though and I plan to continue doing everything I can do to support AMD since I badly want to see more balance in this space.</div><br/><div id="39721477" class="c"><input type="checkbox" id="c-39721477" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39721424">parent</a><span>|</span><a href="#39718962">next</a><span>|</span><label class="collapse" for="c-39721477">[-]</label><label class="expand" for="c-39721477">[2 more]</label></div><br/><div class="children"><div class="content">The compilation toolchain may be reliable but then you get kernel panics at runtime.</div><br/><div id="39721493" class="c"><input type="checkbox" id="c-39721493" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39719094">root</a><span>|</span><a href="#39721477">parent</a><span>|</span><a href="#39718962">next</a><span>|</span><label class="collapse" for="c-39721493">[-]</label><label class="expand" for="c-39721493">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard geohot is upset about that. I haven&#x27;t tortured any of my AMD cards enough to run into that issue yet. Do you know how to make it happen?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39718962" class="c"><input type="checkbox" id="c-39718962" checked=""/><div class="controls bullet"><span class="by">Symmetry</span><span>|</span><a href="#39719094">prev</a><span>|</span><a href="#39719127">next</a><span>|</span><label class="collapse" for="c-39718962">[-]</label><label class="expand" for="c-39718962">[3 more]</label></div><br/><div class="children"><div class="content">Given the price of top line NVidia cards, if they can be had at all, there&#x27;s got to be a lot of effort going on behind the scenes to improve AMD support in various places.</div><br/><div id="39723210" class="c"><input type="checkbox" id="c-39723210" checked=""/><div class="controls bullet"><span class="by">jeff-davis</span><span>|</span><a href="#39718962">parent</a><span>|</span><a href="#39719046">next</a><span>|</span><label class="collapse" for="c-39723210">[-]</label><label class="expand" for="c-39723210">[1 more]</label></div><br/><div class="children"><div class="content">What are the barriers to doing so?</div><br/></div></div></div></div><div id="39719127" class="c"><input type="checkbox" id="c-39719127" checked=""/><div class="controls bullet"><span class="by">simon83</span><span>|</span><a href="#39718962">prev</a><span>|</span><a href="#39723849">next</a><span>|</span><label class="collapse" for="c-39719127">[-]</label><label class="expand" for="c-39719127">[5 more]</label></div><br/><div class="children"><div class="content">Does anyone know how the AMD consumer GPU support on Linux has been implemented? Must use something else than ROCm I assume? Because ROCm only supports the 7900 XTX on Linux[1], while on Windows[2] support is from RX 6600 and upwards.<p>[1]: <a href="https:&#x2F;&#x2F;rocblas.readthedocs.io&#x2F;en&#x2F;rocm-6.0.0&#x2F;about&#x2F;compatibility&#x2F;linux-support.html#linux-supported-gpus" rel="nofollow">https:&#x2F;&#x2F;rocblas.readthedocs.io&#x2F;en&#x2F;rocm-6.0.0&#x2F;about&#x2F;compatibi...</a>
[2]: <a href="https:&#x2F;&#x2F;rocblas.readthedocs.io&#x2F;en&#x2F;rocm-6.0.0&#x2F;about&#x2F;compatibility&#x2F;windows-support.html#windows-supported-gpus" rel="nofollow">https:&#x2F;&#x2F;rocblas.readthedocs.io&#x2F;en&#x2F;rocm-6.0.0&#x2F;about&#x2F;compatibi...</a></div><br/><div id="39719174" class="c"><input type="checkbox" id="c-39719174" checked=""/><div class="controls bullet"><span class="by">Symmetry</span><span>|</span><a href="#39719127">parent</a><span>|</span><a href="#39719351">next</a><span>|</span><label class="collapse" for="c-39719174">[-]</label><label class="expand" for="c-39719174">[1 more]</label></div><br/><div class="children"><div class="content">The newest release, 6.0.2, supports a number of other cards[1] and in general people are able to get a lot more cards to work than are officially supported.  My 7900 XT worked on 6.0.0 for instance.<p>[1]<a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;projects&#x2F;install-on-linux&#x2F;en&#x2F;latest&#x2F;reference&#x2F;system-requirements.html" rel="nofollow">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;projects&#x2F;install-on-linux&#x2F;en&#x2F;lates...</a></div><br/></div></div><div id="39719351" class="c"><input type="checkbox" id="c-39719351" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#39719127">parent</a><span>|</span><a href="#39719174">prev</a><span>|</span><a href="#39723849">next</a><span>|</span><label class="collapse" for="c-39719351">[-]</label><label class="expand" for="c-39719351">[3 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t read too much into support. It&#x27;s more in terms of business&#x2F;warranty&#x2F;promises than what can actually do things<p>I&#x27;ve had a 6900XT since launch and this is the first I&#x27;m hearing <i>&quot;unsupported&quot;</i>, having played with ROCM plenty over the years with Fedora Linux.<p>I think, at most, it&#x27;s taken a couple key environment variables</div><br/><div id="39719642" class="c"><input type="checkbox" id="c-39719642" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#39719127">root</a><span>|</span><a href="#39719351">parent</a><span>|</span><a href="#39723849">next</a><span>|</span><label class="collapse" for="c-39719642">[-]</label><label class="expand" for="c-39719642">[2 more]</label></div><br/><div class="children"><div class="content">How hard would it be for AMD just to document the levels of support of different cards the way NVIDIA does with their &quot;compute capability&quot; numbers ?!<p>I&#x27;m not sure what is worse from AMD - the ML software support they provide for their cards, or the utterly crap documentation.<p>How about one page documenting AMD&#x27;s software stack compared to NVIDIA, one page documenting what ML frameworks support AMD cards, and another documenting &quot;compute capability&quot; type numbers to define the capabilities of different cards.</div><br/><div id="39719758" class="c"><input type="checkbox" id="c-39719758" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39719127">root</a><span>|</span><a href="#39719642">parent</a><span>|</span><a href="#39723849">next</a><span>|</span><label class="collapse" for="c-39719758">[-]</label><label class="expand" for="c-39719758">[1 more]</label></div><br/><div class="children"><div class="content">And <i>almost</i> looks like they&#x27;re deliberately trying to not win any market share.<p>It&#x27;s as if the CEO is mates with NVidias CEO and has an unwritten agreement not to try too hard to topple the applecart...<p>Oh wait...   They&#x27;re cousins!</div><br/></div></div></div></div></div></div></div></div><div id="39723849" class="c"><input type="checkbox" id="c-39723849" checked=""/><div class="controls bullet"><span class="by">VadimPR</span><span>|</span><a href="#39719127">prev</a><span>|</span><a href="#39719339">next</a><span>|</span><label class="collapse" for="c-39723849">[-]</label><label class="expand" for="c-39723849">[1 more]</label></div><br/><div class="children"><div class="content">Ollama runs really, really slow on my MBP for Mistral - as in just a few tokens a second and it takes a long while before it starts giving a result. Anyone else run into this?<p>I&#x27;ve seen that it seems to be related to the amount of system memory available when ollama is started (??) however LM Studio does not have such issues.</div><br/></div></div><div id="39719339" class="c"><input type="checkbox" id="c-39719339" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39723849">prev</a><span>|</span><a href="#39719780">next</a><span>|</span><label class="collapse" for="c-39719339">[-]</label><label class="expand" for="c-39719339">[2 more]</label></div><br/><div class="children"><div class="content">Can anyone (maybe ollama contributors) explain to me the relationship between llama.cpp and ollama?<p>I always thought that ollama basically just was a wrapper (i.e. not much changes to inference code, and only built on top) around llama.cpp, but this makes it seem like it is more than that?</div><br/><div id="39719383" class="c"><input type="checkbox" id="c-39719383" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39719339">parent</a><span>|</span><a href="#39719780">next</a><span>|</span><label class="collapse" for="c-39719383">[-]</label><label class="expand" for="c-39719383">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp also supports AMD cards: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp?tab=readme-ov-file#hipblas">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp?tab=readme-ov-file#hi...</a></div><br/></div></div></div></div><div id="39719780" class="c"><input type="checkbox" id="c-39719780" checked=""/><div class="controls bullet"><span class="by">airocker</span><span>|</span><a href="#39719339">prev</a><span>|</span><a href="#39721664">next</a><span>|</span><label class="collapse" for="c-39719780">[-]</label><label class="expand" for="c-39719780">[3 more]</label></div><br/><div class="children"><div class="content">I heard &quot;Nvidia for LLM today is similar to how Sun Microsystems was for the web&quot;</div><br/><div id="39720577" class="c"><input type="checkbox" id="c-39720577" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#39719780">parent</a><span>|</span><a href="#39721664">next</a><span>|</span><label class="collapse" for="c-39720577">[-]</label><label class="expand" for="c-39720577">[2 more]</label></div><br/><div class="children"><div class="content">... for a very brief period of time until Linux servers and other options caught up.</div><br/><div id="39721877" class="c"><input type="checkbox" id="c-39721877" checked=""/><div class="controls bullet"><span class="by">airocker</span><span>|</span><a href="#39719780">root</a><span>|</span><a href="#39720577">parent</a><span>|</span><a href="#39721664">next</a><span>|</span><label class="collapse" for="c-39721877">[-]</label><label class="expand" for="c-39721877">[1 more]</label></div><br/><div class="children"><div class="content">OS was possibly much more complicated to write at that time than CUDA is to write today. And competition is too strong. It might be more briefer than even Sun.</div><br/></div></div></div></div></div></div><div id="39721664" class="c"><input type="checkbox" id="c-39721664" checked=""/><div class="controls bullet"><span class="by">OtomotO</span><span>|</span><a href="#39719780">prev</a><span>|</span><a href="#39719100">next</a><span>|</span><label class="collapse" for="c-39721664">[-]</label><label class="expand" for="c-39721664">[2 more]</label></div><br/><div class="children"><div class="content">Hm, fooocus manages to run, but for Ollama I get:<p>&gt;time=2024-03-16T00:11:07.993+01:00 level=WARN source=amd_linux.go:50 msg=&quot;ollama &gt;recommends running the <a href="https:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;support&#x2F;linux-drivers" rel="nofollow">https:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;support&#x2F;linux-drivers</a>: amdgpu version file &gt;missing: &#x2F;sys&#x2F;module&#x2F;amdgpu&#x2F;version stat &#x2F;sys&#x2F;module&#x2F;amdgpu&#x2F;version: no such file or &gt;directory&quot;
&gt;time=2024-03-16T00:11:07.993+01:00 level=INFO source=amd_linux.go:85 msg=&quot;detected amdgpu &gt;versions [gfx1031]&quot;
&gt;time=2024-03-16T00:11:07.996+01:00 level=WARN source=amd_linux.go:339 msg=&quot;amdgpu &gt;detected, but no compatible rocm library found.  Either install rocm v6, or follow manual &gt;install instructions at <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;linux.md#manual-&gt;install" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;linux.md#man...</a>&quot;
&gt;time=2024-03-16T00:11:07.996+01:00 level=WARN source=amd_linux.go:96 msg=&quot;unable to verify &gt;rocm library, will use cpu: no suitable rocm found, falling back to CPU&quot;
&gt;time=2024-03-16T00:11:07.996+01:00 level=INFO source=routes.go:1105 msg=&quot;no GPU detected&quot;<p>Need to check how to install rocm on arch again... have done it once, a few moons back, but alas...</div><br/><div id="39721763" class="c"><input type="checkbox" id="c-39721763" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39721664">parent</a><span>|</span><a href="#39719100">next</a><span>|</span><label class="collapse" for="c-39721763">[-]</label><label class="expand" for="c-39721763">[1 more]</label></div><br/><div class="children"><div class="content">Ah, this is probably from missing ROCm libraries. The dynamic libraries are available as one of the release assets (warning: it&#x27;s about 4GB expanded) <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;releases&#x2F;tag&#x2F;v0.1.29">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;releases&#x2F;tag&#x2F;v0.1.29</a> – dropping them in the same directory as the `ollama` binary should work.</div><br/></div></div></div></div><div id="39719100" class="c"><input type="checkbox" id="c-39719100" checked=""/><div class="controls bullet"><span class="by">deadalus</span><span>|</span><a href="#39721664">prev</a><span>|</span><a href="#39720027">next</a><span>|</span><label class="collapse" for="c-39719100">[-]</label><label class="expand" for="c-39719100">[10 more]</label></div><br/><div class="children"><div class="content">I wish AMD did well in the Stable Diffusion front because AMD is never greedy on VRAM. The 4060Ti 16GB(minimum required for Stable Diffusion in 2024) starts at $450.<p>AMD with ROCm is decent on Linux but pretty bad on Windows.</div><br/><div id="39724244" class="c"><input type="checkbox" id="c-39724244" checked=""/><div class="controls bullet"><span class="by">wastewastewaste</span><span>|</span><a href="#39719100">parent</a><span>|</span><a href="#39719314">next</a><span>|</span><label class="collapse" for="c-39724244">[-]</label><label class="expand" for="c-39724244">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need 16gb, literally majority of people don&#x27;t have that and use 8gb and up. especially with forge</div><br/></div></div><div id="39719314" class="c"><input type="checkbox" id="c-39719314" checked=""/><div class="controls bullet"><span class="by">Adverblessly</span><span>|</span><a href="#39719100">parent</a><span>|</span><a href="#39724244">prev</a><span>|</span><a href="#39719169">next</a><span>|</span><label class="collapse" for="c-39719314">[-]</label><label class="expand" for="c-39719314">[1 more]</label></div><br/><div class="children"><div class="content">I run A1111, ComfyUI and kohya-ss on an AMD (6900XT which has 16GB, the minimum required for Stable Diffusion in 2024 ;)), though on Linux. Is it a Windows specific Issue for you?<p>Edit to add: Though apparently I still don&#x27;t run ollama on AMD since it seems to disagree with my setup.</div><br/></div></div><div id="39719169" class="c"><input type="checkbox" id="c-39719169" checked=""/><div class="controls bullet"><span class="by">choilive</span><span>|</span><a href="#39719100">parent</a><span>|</span><a href="#39719314">prev</a><span>|</span><a href="#39720027">next</a><span>|</span><label class="collapse" for="c-39719169">[-]</label><label class="expand" for="c-39719169">[7 more]</label></div><br/><div class="children"><div class="content">They bump up VRAM because they can&#x27;t compete on raw compute.</div><br/><div id="39719668" class="c"><input type="checkbox" id="c-39719668" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39719100">root</a><span>|</span><a href="#39719169">parent</a><span>|</span><a href="#39719481">next</a><span>|</span><label class="collapse" for="c-39719668">[-]</label><label class="expand" for="c-39719668">[2 more]</label></div><br/><div class="children"><div class="content">Or rather Nvidia is purposefully restricting VRAM to avoid gaming cards canibalizing their supremely profitable professional&#x2F;server cards. AMD has no relevant server cards, so they have no reason to hold back on VRAM in consumer cards</div><br/><div id="39724064" class="c"><input type="checkbox" id="c-39724064" checked=""/><div class="controls bullet"><span class="by">karolist</span><span>|</span><a href="#39719100">root</a><span>|</span><a href="#39719668">parent</a><span>|</span><a href="#39719481">next</a><span>|</span><label class="collapse" for="c-39724064">[-]</label><label class="expand" for="c-39724064">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia released consumer RTX 3090 with 24GB VRAM in Sep 2020, AMDs flagship release in that same month was 6900 XT with 16GB VRAM. Who is being restrictive here exactly?</div><br/></div></div></div></div><div id="39719481" class="c"><input type="checkbox" id="c-39719481" checked=""/><div class="controls bullet"><span class="by">risho</span><span>|</span><a href="#39719100">root</a><span>|</span><a href="#39719169">parent</a><span>|</span><a href="#39719668">prev</a><span>|</span><a href="#39720592">next</a><span>|</span><label class="collapse" for="c-39719481">[-]</label><label class="expand" for="c-39719481">[3 more]</label></div><br/><div class="children"><div class="content">it doesn&#x27;t matter how much compute you have if you don&#x27;t have enough vram to run the model.</div><br/><div id="39719701" class="c"><input type="checkbox" id="c-39719701" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39719100">root</a><span>|</span><a href="#39719481">parent</a><span>|</span><a href="#39720272">next</a><span>|</span><label class="collapse" for="c-39719701">[-]</label><label class="expand" for="c-39719701">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. My friend was telling me that I was making a mistake for getting a 7900 XTX to run language models, when the fact of the matter is the cheapest NVIDIA card with 24 GB of VRAM is over 50% more expensive than the 7900 XTX. Running a high quality model at like 80 tps is way more important to me than running a way lower quality model at like 120 tps.</div><br/></div></div></div></div><div id="39720592" class="c"><input type="checkbox" id="c-39720592" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#39719100">root</a><span>|</span><a href="#39719169">parent</a><span>|</span><a href="#39719481">prev</a><span>|</span><a href="#39720027">next</a><span>|</span><label class="collapse" for="c-39720592">[-]</label><label class="expand" for="c-39720592">[1 more]</label></div><br/><div class="children"><div class="content">They lag on software a lot more than the lag on silicon.</div><br/></div></div></div></div></div></div><div id="39720027" class="c"><input type="checkbox" id="c-39720027" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39719100">prev</a><span>|</span><a href="#39719724">next</a><span>|</span><label class="collapse" for="c-39720027">[-]</label><label class="expand" for="c-39720027">[2 more]</label></div><br/><div class="children"><div class="content">Does this work with integrated Radeon graphics? If so it might be worth getting one of those Ryzen mini PCs to use as a local LLM server.</div><br/><div id="39720146" class="c"><input type="checkbox" id="c-39720146" checked=""/><div class="controls bullet"><span class="by">stebalien</span><span>|</span><a href="#39720027">parent</a><span>|</span><a href="#39719724">next</a><span>|</span><label class="collapse" for="c-39720146">[-]</label><label class="expand" for="c-39720146">[1 more]</label></div><br/><div class="children"><div class="content">Not yet: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2637">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2637</a></div><br/></div></div></div></div><div id="39719724" class="c"><input type="checkbox" id="c-39719724" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#39720027">prev</a><span>|</span><a href="#39719530">next</a><span>|</span><label class="collapse" for="c-39719724">[-]</label><label class="expand" for="c-39719724">[1 more]</label></div><br/><div class="children"><div class="content">Curious to see if this will work on APUs. Have a 7840HS to test, will give it a go ASAP.</div><br/></div></div><div id="39719530" class="c"><input type="checkbox" id="c-39719530" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39719724">prev</a><span>|</span><a href="#39719407">next</a><span>|</span><label class="collapse" for="c-39719530">[-]</label><label class="expand" for="c-39719530">[1 more]</label></div><br/><div class="children"><div class="content">If anyone wants to run some benchmarks on MI300x, ping me.</div><br/></div></div><div id="39719407" class="c"><input type="checkbox" id="c-39719407" checked=""/><div class="controls bullet"><span class="by">kodarna</span><span>|</span><a href="#39719530">prev</a><span>|</span><a href="#39721579">next</a><span>|</span><label class="collapse" for="c-39719407">[-]</label><label class="expand" for="c-39719407">[3 more]</label></div><br/><div class="children"><div class="content">I wonder why they aren&#x27;t supporting RX 6750 XT and lower yet, are there architectural differences between these and RX 6800+?</div><br/><div id="39719475" class="c"><input type="checkbox" id="c-39719475" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39719407">parent</a><span>|</span><a href="#39719578">next</a><span>|</span><label class="collapse" for="c-39719475">[-]</label><label class="expand" for="c-39719475">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t support it, but it works if you set an environment variable.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2870#issuecomment-1975477251">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2870#issuecomment-19...</a></div><br/></div></div><div id="39719578" class="c"><input type="checkbox" id="c-39719578" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#39719407">parent</a><span>|</span><a href="#39719475">prev</a><span>|</span><a href="#39721579">next</a><span>|</span><label class="collapse" for="c-39719578">[-]</label><label class="expand" for="c-39719578">[1 more]</label></div><br/><div class="children"><div class="content">Those are Navi 22&#x2F;23&#x2F;24 GPUs while the RX 6800+ GPUs are Navi 21. They have different ISAs... however, the ISAs are identical in all but name.<p>LLVM has recently introduced a unified ISA for all RDNA 2 GPUs (gfx10.3-generic), so the need for the environment variable workaround mentioned in the other comment should eventually disappear.</div><br/></div></div></div></div><div id="39721579" class="c"><input type="checkbox" id="c-39721579" checked=""/><div class="controls bullet"><span class="by">haolez</span><span>|</span><a href="#39719407">prev</a><span>|</span><a href="#39722520">next</a><span>|</span><label class="collapse" for="c-39721579">[-]</label><label class="expand" for="c-39721579">[2 more]</label></div><br/><div class="children"><div class="content">Is there an equivalent to ollama or gpt4all for Android? I&#x27;d like to host my model somewhere and talk to it via app.</div><br/><div id="39721903" class="c"><input type="checkbox" id="c-39721903" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#39721579">parent</a><span>|</span><a href="#39722520">next</a><span>|</span><label class="collapse" for="c-39721903">[-]</label><label class="expand" for="c-39721903">[1 more]</label></div><br/><div class="children"><div class="content">At that point, it sounds like your after an API endpoint for any model. Lots of solutions out there, depends more on your hosting</div><br/></div></div></div></div><div id="39722520" class="c"><input type="checkbox" id="c-39722520" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#39721579">prev</a><span>|</span><a href="#39721633">next</a><span>|</span><label class="collapse" for="c-39722520">[-]</label><label class="expand" for="c-39722520">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, Ollama.<p>(Sorry, could not resist.)</div><br/></div></div><div id="39721633" class="c"><input type="checkbox" id="c-39721633" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#39722520">prev</a><span>|</span><a href="#39723601">next</a><span>|</span><label class="collapse" for="c-39721633">[-]</label><label class="expand" for="c-39721633">[1 more]</label></div><br/><div class="children"><div class="content">Any info on the performance? How does it compare to 4080&#x2F;4090?</div><br/></div></div><div id="39723601" class="c"><input type="checkbox" id="c-39723601" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#39721633">prev</a><span>|</span><a href="#39721719">next</a><span>|</span><label class="collapse" for="c-39723601">[-]</label><label class="expand" for="c-39723601">[1 more]</label></div><br/><div class="children"><div class="content">Does it also support AMD APUs?</div><br/></div></div><div id="39721719" class="c"><input type="checkbox" id="c-39721719" checked=""/><div class="controls bullet"><span class="by">h4x0rr</span><span>|</span><a href="#39723601">prev</a><span>|</span><a href="#39721716">next</a><span>|</span><label class="collapse" for="c-39721719">[-]</label><label class="expand" for="c-39721719">[1 more]</label></div><br/><div class="children"><div class="content">No support for my rx 5700xt :(</div><br/></div></div><div id="39719404" class="c"><input type="checkbox" id="c-39719404" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39721716">prev</a><span>|</span><a href="#39720444">next</a><span>|</span><label class="collapse" for="c-39719404">[-]</label><label class="expand" for="c-39719404">[9 more]</label></div><br/><div class="children"><div class="content">Wow, that&#x27;s a huge feature. Thank you, guys. By the way, does anyone have a preferred case where they can put 4 AMD 7900XTX? There&#x27;s a lot of motherboards and CPUs that support 128 lanes. It&#x27;s the physical arrangement that I have trouble with.</div><br/><div id="39721782" class="c"><input type="checkbox" id="c-39721782" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#39719404">parent</a><span>|</span><a href="#39719432">next</a><span>|</span><label class="collapse" for="c-39721782">[-]</label><label class="expand" for="c-39721782">[3 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need 128 lanes.  8x PCIe3 is more than enough, so for 4 cards that&#x27;s 32.   Most CPUs have about 40lanes.  If you are not doing much that would be more than sufficient.  Buy a PCIe riser.  Go to amazon and search for it, a 16x to 16x PCIe riser.    They go for about $25-$30 often about 20-30cm.  If you want really long one, you can get one from China a 60cm for about the same price, you just have to wait for 3 weeks.  That&#x27;s what I did.  Stuffing all those in a case is often difficult, so you have to go open rig.  Either have the cables running out your computer and figuring out a way to support the cards while keeping them cool or just buy a small $20-$30 open rig frame.</div><br/><div id="39723665" class="c"><input type="checkbox" id="c-39723665" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39719404">root</a><span>|</span><a href="#39721782">parent</a><span>|</span><a href="#39721975">next</a><span>|</span><label class="collapse" for="c-39723665">[-]</label><label class="expand" for="c-39723665">[1 more]</label></div><br/><div class="children"><div class="content">Most CPUs have about 20 lanes (plus a link to the chipset).<p>On the one hand, they will be gen 4 or 5, so they&#x27;re the equivalent of 40-80 gen 3 lanes.<p>On the other hand, you can only split them up if you have a motherboard that supports bifurcation.  If you buy the wrong model, you&#x27;re stuck dedicating the equivalent of 64 gen 3 lanes to a single card.<p>Edit: Actually, looking into it further, current Intel desktop processors will only run their lanes as 16(+4) or 8+8(+4).  You can kind of make 4 cards work by using chipset-fed slots, but that sucks.  You could also get a PCIe switch but those are very expensive.  AMD will do 4+4+4+4(+4) on the right boards.</div><br/></div></div><div id="39721975" class="c"><input type="checkbox" id="c-39721975" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39719404">root</a><span>|</span><a href="#39721782">parent</a><span>|</span><a href="#39723665">prev</a><span>|</span><a href="#39719432">next</a><span>|</span><label class="collapse" for="c-39721975">[-]</label><label class="expand" for="c-39721975">[1 more]</label></div><br/><div class="children"><div class="content">Ah ha, that&#x27;s the part I was curious about. I was wondering if I could keep everything cool open rig. I&#x27;m waiting for the stuff to arrive: risers, board, CPU, GPUs. And I&#x27;ve been putting it off because I wasn&#x27;t sure how about the case. All right then, open rig frame. Thank you!</div><br/></div></div></div></div><div id="39719432" class="c"><input type="checkbox" id="c-39719432" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#39719404">parent</a><span>|</span><a href="#39721782">prev</a><span>|</span><a href="#39720444">next</a><span>|</span><label class="collapse" for="c-39719432">[-]</label><label class="expand" for="c-39719432">[5 more]</label></div><br/><div class="children"><div class="content">Used crypto mining parts not available any more?</div><br/><div id="39719894" class="c"><input type="checkbox" id="c-39719894" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#39719404">root</a><span>|</span><a href="#39719432">parent</a><span>|</span><a href="#39720444">next</a><span>|</span><label class="collapse" for="c-39719894">[-]</label><label class="expand" for="c-39719894">[4 more]</label></div><br/><div class="children"><div class="content">Crypto mining didn&#x27;t require significant bandwidth to the card. Mining-oriented motherboards typically only provisioned a single lane of PCIe to each card, and often used anemic host CPUs (like Celeron embedded parts).</div><br/><div id="39723396" class="c"><input type="checkbox" id="c-39723396" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#39719404">root</a><span>|</span><a href="#39719894">parent</a><span>|</span><a href="#39720053">next</a><span>|</span><label class="collapse" for="c-39723396">[-]</label><label class="expand" for="c-39723396">[1 more]</label></div><br/><div class="children"><div class="content">Does LLM inference require significant bandwidth to the card? You have to get the model into VRAM, but that&#x27;s a fixed startup cost, not a per-output-token cost.</div><br/></div></div><div id="39720053" class="c"><input type="checkbox" id="c-39720053" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39719404">root</a><span>|</span><a href="#39719894">parent</a><span>|</span><a href="#39723396">prev</a><span>|</span><a href="#39720444">next</a><span>|</span><label class="collapse" for="c-39720053">[-]</label><label class="expand" for="c-39720053">[2 more]</label></div><br/><div class="children"><div class="content">Exactly. They&#x27;d use PCIe x1 to PCIe x16 risers with power adapters. These require high-bandwidth.</div><br/><div id="39721054" class="c"><input type="checkbox" id="c-39721054" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#39719404">root</a><span>|</span><a href="#39720053">parent</a><span>|</span><a href="#39720444">next</a><span>|</span><label class="collapse" for="c-39721054">[-]</label><label class="expand" for="c-39721054">[1 more]</label></div><br/><div class="children"><div class="content">Oh. Shows I wasn&#x27;t into that.<p>I did once work with a crypto case, but yes, it was one motherboard with a lot of wifis and we still didn&#x27;t need the pcie lanes.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39720444" class="c"><input type="checkbox" id="c-39720444" checked=""/><div class="controls bullet"><span class="by">observationist</span><span>|</span><a href="#39719404">prev</a><span>|</span><label class="collapse" for="c-39720444">[-]</label><label class="expand" for="c-39720444">[18 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a thing somewhat conspicuous in its absence - why isn&#x27;t llama.cpp more directly credited and thanked for providing the base technology powering this tool?<p>All the other cool &quot;run local&quot; software seems to have the appropriate level of credit. You can find llama.cpp references in the code, being set up in a kind of &quot;as is&quot; fashion such that it might be OK as far as MIT licensing goes, but it seems kind of petty to have no shout out or thank you anywhere in the repository or blog or ollama website.<p>GPT4All - <a href="https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html</a><p>LM Studio - <a href="https:&#x2F;&#x2F;lmstudio.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmstudio.ai&#x2F;</a><p>Both of these projects credit and attribute appropriately, Ollama seems to bend over backwards so they don&#x27;t have to?</div><br/><div id="39720542" class="c"><input type="checkbox" id="c-39720542" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39720444">parent</a><span>|</span><a href="#39724006">next</a><span>|</span><label class="collapse" for="c-39720542">[-]</label><label class="expand" for="c-39720542">[6 more]</label></div><br/><div class="children"><div class="content">ollama has made a lot of nice contributions of their own. It&#x27;s a good look to give a hat tip to the great work llama.cpp is also doing, but they&#x27;re strictly speaking not required to do that in their advertising any more than llama.cpp is required to give credit to Google Brain, and I think that&#x27;s because llama.cpp has pulled off tricks in the execution that Brain never could have accomplished, just as ollama has had great success focusing on things that wouldn&#x27;t make sense for llama.cpp. Besides everyone who wants to know what&#x27;s up can read the source code, research papers, etc. then make their own judgements about who&#x27;s who. It&#x27;s all in the open.</div><br/><div id="39722913" class="c"><input type="checkbox" id="c-39722913" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#39720444">root</a><span>|</span><a href="#39720542">parent</a><span>|</span><a href="#39722093">next</a><span>|</span><label class="collapse" for="c-39722913">[-]</label><label class="expand" for="c-39722913">[2 more]</label></div><br/><div class="children"><div class="content">Newton only gave credits to &quot;Gods&quot;. He said he stands on shoulders of Gods or something like that. But he never mentioned which Gods in particular, did he?</div><br/><div id="39723456" class="c"><input type="checkbox" id="c-39723456" checked=""/><div class="controls bullet"><span class="by">aragilar</span><span>|</span><a href="#39720444">root</a><span>|</span><a href="#39722913">parent</a><span>|</span><a href="#39722093">next</a><span>|</span><label class="collapse" for="c-39723456">[-]</label><label class="expand" for="c-39723456">[1 more]</label></div><br/><div class="children"><div class="content">Giants, not Gods, and it&#x27;s debated whether it was actually an insult or not: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Isaac_Newton#Personality" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Isaac_Newton#Personality</a></div><br/></div></div></div></div><div id="39722093" class="c"><input type="checkbox" id="c-39722093" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39720444">root</a><span>|</span><a href="#39720542">parent</a><span>|</span><a href="#39722913">prev</a><span>|</span><a href="#39724006">next</a><span>|</span><label class="collapse" for="c-39722093">[-]</label><label class="expand" for="c-39722093">[3 more]</label></div><br/><div class="children"><div class="content">Like what? What contributions has ollama made to llama.cpp? It&#x27;s not a big deal or problem. It just hasnt.<p>And they could have very, very, easily, there&#x27;s a server, sitting right there.<p>They chose not to.<p>That&#x27;s fine. But it&#x27;s a choice.<p>The rest I chalk it up to inexperience and being busy.</div><br/><div id="39723815" class="c"><input type="checkbox" id="c-39723815" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#39720444">root</a><span>|</span><a href="#39722093">parent</a><span>|</span><a href="#39723799">next</a><span>|</span><label class="collapse" for="c-39723815">[-]</label><label class="expand" for="c-39723815">[1 more]</label></div><br/><div class="children"><div class="content">OP never said &quot;contributions to llama.cpp&quot;, the comment just said just &quot;contributions&quot; which I read as &quot;own developments&quot;.</div><br/></div></div><div id="39723799" class="c"><input type="checkbox" id="c-39723799" checked=""/><div class="controls bullet"><span class="by">varjag</span><span>|</span><a href="#39720444">root</a><span>|</span><a href="#39722093">parent</a><span>|</span><a href="#39723815">prev</a><span>|</span><a href="#39724006">next</a><span>|</span><label class="collapse" for="c-39723799">[-]</label><label class="expand" for="c-39723799">[1 more]</label></div><br/><div class="children"><div class="content">Where the name does come from?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>