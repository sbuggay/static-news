<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700902853467" as="style"/><link rel="stylesheet" href="styles.css?v=1700902853467"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://elifesciences.org/articles/89736">Eleven strategies for making reproducible research the norm</a>Â <span class="domain">(<a href="https://elifesciences.org">elifesciences.org</a>)</span></div><div class="subtext"><span>Tomte</span> | <span>14 comments</span></div><br/><div><div id="38411656" class="c"><input type="checkbox" id="c-38411656" checked=""/><div class="controls bullet"><span class="by">rowyourboat</span><span>|</span><a href="#38412170">next</a><span>|</span><label class="collapse" for="c-38411656">[-]</label><label class="expand" for="c-38411656">[7 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the main problem that academics are measured by the number of publications they publish, and reproductions of existing studies aren&#x27;t published by the main journals, thus there is little incentive to try and reproduce findings? I never thought this was a problem of ability.</div><br/><div id="38412268" class="c"><input type="checkbox" id="c-38412268" checked=""/><div class="controls bullet"><span class="by">empiko</span><span>|</span><a href="#38411656">parent</a><span>|</span><a href="#38412179">next</a><span>|</span><label class="collapse" for="c-38412268">[-]</label><label class="expand" for="c-38412268">[1 more]</label></div><br/><div class="children"><div class="content">Another issue is that making things reproducible costs you time and that is exactly what most researchers do not have. For example, many ML papers have code that is just a barely working Jupyter notebook. To make it reproducible you would have to create a reproducible environment, package the data, and prepare scripts that would rerun all the experiments you have done. That can take several weeks, but it will not increase the chance of acceptance for your paper at all.</div><br/></div></div><div id="38412179" class="c"><input type="checkbox" id="c-38412179" checked=""/><div class="controls bullet"><span class="by">geokon</span><span>|</span><a href="#38411656">parent</a><span>|</span><a href="#38412268">prev</a><span>|</span><a href="#38411878">next</a><span>|</span><label class="collapse" for="c-38412179">[-]</label><label class="expand" for="c-38412179">[2 more]</label></div><br/><div class="children"><div class="content">If you fail to reproduce some important research then I think that would absolutely get published. (see the recent superconductor drama)<p>So if you feel some impactful work is suspicious .. I think disproving it would absolutely be incentivised<p>If you show its actually correct.. Well then usually it&#x27;s not that hard to push the envelope a bit further and say something new. That happens all the time</div><br/><div id="38412243" class="c"><input type="checkbox" id="c-38412243" checked=""/><div class="controls bullet"><span class="by">ttymck</span><span>|</span><a href="#38411656">root</a><span>|</span><a href="#38412179">parent</a><span>|</span><a href="#38411878">next</a><span>|</span><label class="collapse" for="c-38412243">[-]</label><label class="expand" for="c-38412243">[1 more]</label></div><br/><div class="children"><div class="content">It seems to me, instead of funding a new college or traditional research institute, some benefactor ought to fund a &quot;research reproduction institute&quot;, dedicated to identifying and reproducing suspicious publications.</div><br/></div></div></div></div><div id="38411878" class="c"><input type="checkbox" id="c-38411878" checked=""/><div class="controls bullet"><span class="by">reignbeaux</span><span>|</span><a href="#38411656">parent</a><span>|</span><a href="#38412179">prev</a><span>|</span><a href="#38412170">next</a><span>|</span><label class="collapse" for="c-38411878">[-]</label><label class="expand" for="c-38411878">[3 more]</label></div><br/><div class="children"><div class="content">Unfortunately this is only part of the problem. Even studies on ML that use public datasets, which are the kinds of studies that when code is shared should be very easy to reproduce, are often surprisingly hard to repeat. Sometimes only parts of the code are published, the code has a lot of bugs (who knows why? Added intentionally?), the code is very badly documented, or the exact libraries are not specified properly.<p>And this is in a field where everything is based on code, where in principle reproducibility is easy. Go into materials science or chemistry and try to synthesize something following a published paper and you get all sorts of problems. Different equipment, different temperature, not all steps documented, ... Reproducing experimental findings can take you months.</div><br/><div id="38412149" class="c"><input type="checkbox" id="c-38412149" checked=""/><div class="controls bullet"><span class="by">sigtstp</span><span>|</span><a href="#38411656">root</a><span>|</span><a href="#38411878">parent</a><span>|</span><a href="#38411973">next</a><span>|</span><label class="collapse" for="c-38412149">[-]</label><label class="expand" for="c-38412149">[1 more]</label></div><br/><div class="children"><div class="content">It still largely comes down to incentives from what I&#x27;ve seen. A lot of times all anyone (from the researcher to the reviewer) cares about is the paper. Journals don&#x27;t check that code actually works, and a lot of researchers don&#x27;t spend time on preparing their code. They feel there&#x27;s no need, since they now got a new article on their CV. It&#x27;s true that they may not have the skills and experience to produce good code they can share (depending on the area), but often 1) there&#x27;s no time to prep code since they&#x27;ve got 3 other projects going on and a crazy work pace 2) the code is seen as something incidental and secondary - what matters to them is the figures and results 3) some groups want to milk a topic for a few papers so they&#x27;re guarding their code and data. Luckily at least plenty of journals demand access to data or even making it public.</div><br/></div></div><div id="38411973" class="c"><input type="checkbox" id="c-38411973" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#38411656">root</a><span>|</span><a href="#38411878">parent</a><span>|</span><a href="#38412149">prev</a><span>|</span><a href="#38412170">next</a><span>|</span><label class="collapse" for="c-38411973">[-]</label><label class="expand" for="c-38411973">[1 more]</label></div><br/><div class="children"><div class="content">Page limits certainly don&#x27;t help!</div><br/></div></div></div></div></div></div><div id="38412170" class="c"><input type="checkbox" id="c-38412170" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#38411656">prev</a><span>|</span><a href="#38411967">next</a><span>|</span><label class="collapse" for="c-38412170">[-]</label><label class="expand" for="c-38412170">[1 more]</label></div><br/><div class="children"><div class="content">the root of the issue is trust in published science. if people focus too much on reproducibility, then you will get people trying to game the system to publish reproducible but still misleading&#x2F;cherrypicked science</div><br/></div></div><div id="38411967" class="c"><input type="checkbox" id="c-38411967" checked=""/><div class="controls bullet"><span class="by">fsckboy</span><span>|</span><a href="#38412170">prev</a><span>|</span><a href="#38411982">next</a><span>|</span><label class="collapse" for="c-38411967">[-]</label><label class="expand" for="c-38411967">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not entirely important (theoretically) that research be reproducible, but it would be important that the attempt at reproduction is made in a timely way. In many cases today, it&#x27;s not so easy to set up the experiments in the first place to attempt it.</div><br/></div></div><div id="38411982" class="c"><input type="checkbox" id="c-38411982" checked=""/><div class="controls bullet"><span class="by">oddmiral</span><span>|</span><a href="#38411967">prev</a><span>|</span><a href="#38411337">next</a><span>|</span><label class="collapse" for="c-38411982">[-]</label><label class="expand" for="c-38411982">[1 more]</label></div><br/><div class="children"><div class="content">12. Create a journal for reproduced papers only.</div><br/></div></div><div id="38411337" class="c"><input type="checkbox" id="c-38411337" checked=""/><div class="controls bullet"><span class="by">alanbernstein</span><span>|</span><a href="#38411982">prev</a><span>|</span><a href="#38411416">next</a><span>|</span><label class="collapse" for="c-38411337">[-]</label><label class="expand" for="c-38411337">[2 more]</label></div><br/><div class="children"><div class="content">&gt; An alternative approach is to encourage students tnduct replication studies, evidence synthesis, or meta-research as part of graduate theses.<p>Why not <i>require</i> it?</div><br/></div></div></div></div></div></div></div></body></html>