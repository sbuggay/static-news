<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726736465026" as="style"/><link rel="stylesheet" href="styles.css?v=1726736465026"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://qwenlm.github.io/blog/qwen2.5/">Qwen2.5: A Party of Foundation Models</a>Â <span class="domain">(<a href="https://qwenlm.github.io">qwenlm.github.io</a>)</span></div><div class="subtext"><span>apsec112</span> | <span>32 comments</span></div><br/><div><div id="41585944" class="c"><input type="checkbox" id="c-41585944" checked=""/><div class="controls bullet"><span class="by">jcoc611</span><span>|</span><a href="#41584901">next</a><span>|</span><label class="collapse" for="c-41585944">[-]</label><label class="expand" for="c-41585944">[6 more]</label></div><br/><div class="children"><div class="content">Probably an ignorant question, but could someone explain why the Context Length is much larger than the Generation Length?</div><br/><div id="41586055" class="c"><input type="checkbox" id="c-41586055" checked=""/><div class="controls bullet"><span class="by">dacox</span><span>|</span><a href="#41585944">parent</a><span>|</span><a href="#41586034">next</a><span>|</span><label class="collapse" for="c-41586055">[-]</label><label class="expand" for="c-41586055">[4 more]</label></div><br/><div class="children"><div class="content">When doing inference for an LLM, there are two stages.<p>The first phase is referred to as &quot;prefill&quot;, where the input is processed to create the KV Cache.<p>After that phase, the &quot;decode&quot; phase is called auto-regressively. Each decode yields one new token.<p>This post on [Inference Memory Requirements](<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;llama31#inference-memory-requirements" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;llama31#inference-memory-require...</a>) is quite good.<p>These two phases have pretty different performance characteristics - prefill can really maximize GPU memory. For long contexts, its can be nigh impossible to do it all in a single pass - frameworks like vLLM use a technique called &quot;chunked prefill&quot;.<p>The decode phase is compute intensive, but tends not to maximize GPU memory.<p>If you are serving these models, you really want to be able to have larger batch sizes during inference, which can only really come with scale - for a smaller app, you won&#x27;t want to make the user wait that long.<p>So, long contexts only have to be processed _once_ per inference, which is basically a scheduling problem.<p>But the number of decode passes scales linearly with the output length. If it was unlimited, you could get some requests just _always_ present in an inference batch, reducing throughput for everyone.</div><br/><div id="41588031" class="c"><input type="checkbox" id="c-41588031" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#41585944">root</a><span>|</span><a href="#41586055">parent</a><span>|</span><a href="#41586692">next</a><span>|</span><label class="collapse" for="c-41588031">[-]</label><label class="expand" for="c-41588031">[1 more]</label></div><br/><div class="children"><div class="content">Decode speed is generally memory bandwidth bound. Prefill is typically arithmetic bound. This is the reason for mixed batches (both decode and prefill) - it let&#x27;s you saturate both memory and arithmetic.<p>Chunked prefill is for minimizing latency for decode entries in the same batch. It&#x27;s not needed if you have only one request - in that case it&#x27;s the fastest to just prefill in one chunk.<p>I&#x27;m pretty sure the sibling comment is right about different length limits - it&#x27;s because of training and model talking nonsense if you let too long.</div><br/></div></div><div id="41586692" class="c"><input type="checkbox" id="c-41586692" checked=""/><div class="controls bullet"><span class="by">easygenes</span><span>|</span><a href="#41585944">root</a><span>|</span><a href="#41586055">parent</a><span>|</span><a href="#41588031">prev</a><span>|</span><a href="#41586172">next</a><span>|</span><label class="collapse" for="c-41586692">[-]</label><label class="expand" for="c-41586692">[1 more]</label></div><br/><div class="children"><div class="content">It is also a training issue. The model has to be trained to reinforce longer outputs, which has a quadratic train-time cost and requires suitable long-context response training data.</div><br/></div></div><div id="41586172" class="c"><input type="checkbox" id="c-41586172" checked=""/><div class="controls bullet"><span class="by">jcoc611</span><span>|</span><a href="#41585944">root</a><span>|</span><a href="#41586055">parent</a><span>|</span><a href="#41586692">prev</a><span>|</span><a href="#41586034">next</a><span>|</span><label class="collapse" for="c-41586172">[-]</label><label class="expand" for="c-41586172">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a great explanation, thank you!</div><br/></div></div></div></div></div></div><div id="41584901" class="c"><input type="checkbox" id="c-41584901" checked=""/><div class="controls bullet"><span class="by">freeqaz</span><span>|</span><a href="#41585944">prev</a><span>|</span><a href="#41584295">next</a><span>|</span><label class="collapse" for="c-41584901">[-]</label><label class="expand" for="c-41584901">[7 more]</label></div><br/><div class="children"><div class="content">32B is a nice size for 2x 3090s. That comfortably fits on the GPU with minimal quantization and still leaves extra memory for the long context length.<p>70B is just a littttle rough trying to run without offloading some layers to the CPU.</div><br/><div id="41586329" class="c"><input type="checkbox" id="c-41586329" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#41584901">parent</a><span>|</span><a href="#41584295">next</a><span>|</span><label class="collapse" for="c-41586329">[-]</label><label class="expand" for="c-41586329">[6 more]</label></div><br/><div class="children"><div class="content">70B+ models typically run great with my MacBook&#x27;s 96GB of (V)RAM. I want a Mac Studio to run e.g. llama-405B, but I can&#x27;t justify the marginal model quality ROI for like $7k or whatever. (But I waaant iiit!)</div><br/><div id="41586602" class="c"><input type="checkbox" id="c-41586602" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#41584901">root</a><span>|</span><a href="#41586329">parent</a><span>|</span><a href="#41587213">next</a><span>|</span><label class="collapse" for="c-41586602">[-]</label><label class="expand" for="c-41586602">[2 more]</label></div><br/><div class="children"><div class="content">You can get refurbished Mac Studio M1 Ultra with 128GB VRAM for ~ $3k on ebay. M1 ultra has 800GB&#x2F;s memory bandwidth, same as the M2 ultra.<p>Not sure if 128GB VRAM is enough for running 405b (maybe at 3-bit quant?), but it seems to offer great value for running 70B models at 8-bit.</div><br/><div id="41588920" class="c"><input type="checkbox" id="c-41588920" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#41584901">root</a><span>|</span><a href="#41586602">parent</a><span>|</span><a href="#41587213">next</a><span>|</span><label class="collapse" for="c-41588920">[-]</label><label class="expand" for="c-41588920">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I would want the 192GB Mac for attempting such hefty models. But I have such basic bitch needs that 405B is overkill haha.</div><br/></div></div></div></div><div id="41587213" class="c"><input type="checkbox" id="c-41587213" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#41584901">root</a><span>|</span><a href="#41586329">parent</a><span>|</span><a href="#41586602">prev</a><span>|</span><a href="#41587029">next</a><span>|</span><label class="collapse" for="c-41587213">[-]</label><label class="expand" for="c-41587213">[2 more]</label></div><br/><div class="children"><div class="content">&gt; run great<p>How many tokens&#x2F;second is that approx?<p>For reference, Qwen 2.5 32B on CPU (5950X) with GPU offloading (to RTX 3090ti) gets about 8.5 token&#x2F;s, while 14B (fully on GPU) gets about ~64 tokens&#x2F;s.</div><br/><div id="41588903" class="c"><input type="checkbox" id="c-41588903" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#41584901">root</a><span>|</span><a href="#41587213">parent</a><span>|</span><a href="#41587029">next</a><span>|</span><label class="collapse" for="c-41588903">[-]</label><label class="expand" for="c-41588903">[1 more]</label></div><br/><div class="children"><div class="content">For 70B models, I usually get 15-25 t&#x2F;s on my laptop. Obviously that heavily depends on which quant, context length, etc. I usually roll with q5s, since the loss is so minuscule.</div><br/></div></div></div></div><div id="41587029" class="c"><input type="checkbox" id="c-41587029" checked=""/><div class="controls bullet"><span class="by">BaculumMeumEst</span><span>|</span><a href="#41584901">root</a><span>|</span><a href="#41586329">parent</a><span>|</span><a href="#41587213">prev</a><span>|</span><a href="#41584295">next</a><span>|</span><label class="collapse" for="c-41587029">[-]</label><label class="expand" for="c-41587029">[1 more]</label></div><br/><div class="children"><div class="content">what quant are you running for that rig? i&#x27;ve been running q4, not sure if I can bump that up to q5 across the board (or if it&#x27;s worth it in general)</div><br/></div></div></div></div></div></div><div id="41584295" class="c"><input type="checkbox" id="c-41584295" checked=""/><div class="controls bullet"><span class="by">Flux159</span><span>|</span><a href="#41584901">prev</a><span>|</span><a href="#41583832">next</a><span>|</span><label class="collapse" for="c-41584295">[-]</label><label class="expand" for="c-41584295">[8 more]</label></div><br/><div class="children"><div class="content">It would be nice to have comparisons to Claude 3.5 for the coder model, only comparing to open source models isnât super helpful because I would want to compare to the model Iâm currently using for development work.</div><br/><div id="41584360" class="c"><input type="checkbox" id="c-41584360" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41584295">parent</a><span>|</span><a href="#41584498">next</a><span>|</span><label class="collapse" for="c-41584360">[-]</label><label class="expand" for="c-41584360">[3 more]</label></div><br/><div class="children"><div class="content">Aider will probably have some numbers at <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;leaderboards&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;leaderboards&#x2F;</a></div><br/><div id="41584616" class="c"><input type="checkbox" id="c-41584616" checked=""/><div class="controls bullet"><span class="by">Deathmax</span><span>|</span><a href="#41584295">root</a><span>|</span><a href="#41584360">parent</a><span>|</span><a href="#41584498">next</a><span>|</span><label class="collapse" for="c-41584616">[-]</label><label class="expand" for="c-41584616">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve posted their own run of the Aider benchmark [1] if you want to compare, it achieved 57.1%.<p>[1]: <a href="https:&#x2F;&#x2F;qianwen-res.oss-cn-beijing.aliyuncs.com&#x2F;Qwen2.5&#x2F;Qwen2.5-Coder&#x2F;qwen2.5-coder-instruct.jpg" rel="nofollow">https:&#x2F;&#x2F;qianwen-res.oss-cn-beijing.aliyuncs.com&#x2F;Qwen2.5&#x2F;Qwen...</a></div><br/><div id="41589673" class="c"><input type="checkbox" id="c-41589673" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#41584295">root</a><span>|</span><a href="#41584616">parent</a><span>|</span><a href="#41584498">next</a><span>|</span><label class="collapse" for="c-41589673">[-]</label><label class="expand" for="c-41589673">[1 more]</label></div><br/><div class="children"><div class="content">Oof. I&#x27;m really not sure why companies keep releasing these mini coding models; 57.1% is worse than gpt-3.5-turbo, and running it locally will be slower than OpenAI&#x27;s API. I guess you could use it if you took your laptop into the woods, but with such poor coding ability, would you even want to?<p>The Qwen2.5-72B model seems to do pretty well on coding benchmarks, though â although no word about Aider yet.</div><br/></div></div></div></div></div></div><div id="41584498" class="c"><input type="checkbox" id="c-41584498" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#41584295">parent</a><span>|</span><a href="#41584360">prev</a><span>|</span><a href="#41586289">next</a><span>|</span><label class="collapse" for="c-41584498">[-]</label><label class="expand" for="c-41584498">[3 more]</label></div><br/><div class="children"><div class="content">Here is a comparison of the prompt &quot;I want to create a basic Flight simulator in Bevy and Rust. Help me figure out the core properties I need for take off, in air flight and landing&quot; between Claude Sonnet 3.5 and Qwen2.5-14B-Instruct-Q4_K_M.gguf:<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;victorb&#x2F;7749e76f7c27674f3ae36d791e20140a" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;victorb&#x2F;7749e76f7c27674f3ae36d791e20...</a><p>AFAIK, there isn&#x27;t any (micro)benchmark comparisons out yet.</div><br/><div id="41585398" class="c"><input type="checkbox" id="c-41585398" checked=""/><div class="controls bullet"><span class="by">yourMadness</span><span>|</span><a href="#41584295">root</a><span>|</span><a href="#41584498">parent</a><span>|</span><a href="#41586289">next</a><span>|</span><label class="collapse" for="c-41585398">[-]</label><label class="expand" for="c-41585398">[2 more]</label></div><br/><div class="children"><div class="content">14B with Q4_K_M quantization is about 9 GB.<p>Remarkable that it is at all comparable to Sonnet 3.5</div><br/><div id="41585842" class="c"><input type="checkbox" id="c-41585842" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#41584295">root</a><span>|</span><a href="#41585398">parent</a><span>|</span><a href="#41586289">next</a><span>|</span><label class="collapse" for="c-41585842">[-]</label><label class="expand" for="c-41585842">[1 more]</label></div><br/><div class="children"><div class="content">Comparable, I guess. But the result is a lot worse compared to Sonnet for sure. Parts of the example code doesn&#x27;t make much sense. Meanwhile Sonnet seems to have the latest API of Bevy considered, and mostly makes sense.</div><br/></div></div></div></div></div></div><div id="41586289" class="c"><input type="checkbox" id="c-41586289" checked=""/><div class="controls bullet"><span class="by">Sn0wCoder</span><span>|</span><a href="#41584295">parent</a><span>|</span><a href="#41584498">prev</a><span>|</span><a href="#41583832">next</a><span>|</span><label class="collapse" for="c-41586289">[-]</label><label class="expand" for="c-41586289">[1 more]</label></div><br/><div class="children"><div class="content">This might be what you are asking for...
<a href="https:&#x2F;&#x2F;qwenlm.github.io&#x2F;blog&#x2F;qwen2.5-coder&#x2F;" rel="nofollow">https:&#x2F;&#x2F;qwenlm.github.io&#x2F;blog&#x2F;qwen2.5-coder&#x2F;</a><p>Ctrl F - Code Reasoning:</div><br/></div></div></div></div><div id="41583832" class="c"><input type="checkbox" id="c-41583832" checked=""/><div class="controls bullet"><span class="by">ekojs</span><span>|</span><a href="#41584295">prev</a><span>|</span><a href="#41585805">next</a><span>|</span><label class="collapse" for="c-41583832">[-]</label><label class="expand" for="c-41583832">[1 more]</label></div><br/><div class="children"><div class="content">Actually really impressive. They went up from 7T tokens to 18T tokens. Curious to see how they perform after finetuning.</div><br/></div></div><div id="41585805" class="c"><input type="checkbox" id="c-41585805" checked=""/><div class="controls bullet"><span class="by">cateye</span><span>|</span><a href="#41583832">prev</a><span>|</span><a href="#41583552">next</a><span>|</span><label class="collapse" for="c-41585805">[-]</label><label class="expand" for="c-41585805">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; we are inspired by the recent advancements in reinforcement learning (e.g., o1)</i><p>It is interesting to see what the future will bring when models incorporate chain of thought approaches and whether o1 will get outperformed by open source models.</div><br/></div></div><div id="41583552" class="c"><input type="checkbox" id="c-41583552" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41585805">prev</a><span>|</span><a href="#41585856">next</a><span>|</span><label class="collapse" for="c-41583552">[-]</label><label class="expand" for="c-41583552">[7 more]</label></div><br/><div class="children"><div class="content">&gt;our latest large-scale dataset, encompassing up to 18 trillion tokens<p>I remember when GPT-3 was trained on 300B tokens.</div><br/><div id="41583909" class="c"><input type="checkbox" id="c-41583909" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41583552">parent</a><span>|</span><a href="#41585856">next</a><span>|</span><label class="collapse" for="c-41583909">[-]</label><label class="expand" for="c-41583909">[6 more]</label></div><br/><div class="children"><div class="content">and was considered too dangerous to be released publicly.</div><br/><div id="41584282" class="c"><input type="checkbox" id="c-41584282" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#41583552">root</a><span>|</span><a href="#41583909">parent</a><span>|</span><a href="#41586129">next</a><span>|</span><label class="collapse" for="c-41584282">[-]</label><label class="expand" for="c-41584282">[1 more]</label></div><br/><div class="children"><div class="content">they are dangerous... for folks who need to scrape the web for low background tokens to train their transformers.</div><br/></div></div><div id="41586129" class="c"><input type="checkbox" id="c-41586129" checked=""/><div class="controls bullet"><span class="by">abc-1</span><span>|</span><a href="#41583552">root</a><span>|</span><a href="#41583909">parent</a><span>|</span><a href="#41584282">prev</a><span>|</span><a href="#41584118">next</a><span>|</span><label class="collapse" for="c-41586129">[-]</label><label class="expand" for="c-41586129">[1 more]</label></div><br/><div class="children"><div class="content">Nobody ever really believed this, the truth is rarely in vogue.</div><br/></div></div><div id="41584118" class="c"><input type="checkbox" id="c-41584118" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41583552">root</a><span>|</span><a href="#41583909">parent</a><span>|</span><a href="#41586129">prev</a><span>|</span><a href="#41585856">next</a><span>|</span><label class="collapse" for="c-41584118">[-]</label><label class="expand" for="c-41584118">[3 more]</label></div><br/><div class="children"><div class="content">The larger GPT-2s were also considered too dangerous to release publicly at first.</div><br/><div id="41584774" class="c"><input type="checkbox" id="c-41584774" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41583552">root</a><span>|</span><a href="#41584118">parent</a><span>|</span><a href="#41585856">next</a><span>|</span><label class="collapse" for="c-41584774">[-]</label><label class="expand" for="c-41584774">[2 more]</label></div><br/><div class="children"><div class="content">I remember be very understanding of it too after seeing the incredible (but absolutely terrible in retrospect) outputs.</div><br/><div id="41584985" class="c"><input type="checkbox" id="c-41584985" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41583552">root</a><span>|</span><a href="#41584774">parent</a><span>|</span><a href="#41585856">next</a><span>|</span><label class="collapse" for="c-41584985">[-]</label><label class="expand" for="c-41584985">[1 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t really compelled at the time, nothing has changed.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>