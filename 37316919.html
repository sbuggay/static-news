<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693386064689" as="style"/><link rel="stylesheet" href="styles.css?v=1693386064689"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://buildkite.com/blog/applying-sre-principles-to-cicd">Applying SRE Principles to CI/CD</a> <span class="domain">(<a href="https://buildkite.com">buildkite.com</a>)</span></div><div class="subtext"><span>mooreds</span> | <span>22 comments</span></div><br/><div><div id="37319290" class="c"><input type="checkbox" id="c-37319290" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#37319235">next</a><span>|</span><label class="collapse" for="c-37319290">[-]</label><label class="expand" for="c-37319290">[1 more]</label></div><br/><div class="children"><div class="content">If I understand the article correctly, the author offers SLOs as a way to pressure the management to allocate resources to fix CI problems.  This would work under assumption that the management has resources to spare or would be able to divert those resources from other departments towards fixing CI problems.<p>And, sometimes this will probably work.  But, I can easily imagine a situation where I, as a CI personnel come to my manager and tell her that we&#x27;ve burned past our 87% SLO objective, and get an immediate response that starting today our SLO objective is 77%.<p>In my experience, QA (and therefore CI tests) are the first chunk of the budget allocated to overall development that is being subtracted from if any subtraction is to take place.  Very few companies bet on the quality of their software as a sale&#x27;s driver.  Most will probably fire the whole QA department and throw away all tests, if times were tough instead of trying to allocate more resources to software quality when hitting some percentage of test failures.</div><br/></div></div><div id="37319235" class="c"><input type="checkbox" id="c-37319235" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#37319290">prev</a><span>|</span><a href="#37317549">next</a><span>|</span><label class="collapse" for="c-37319235">[-]</label><label class="expand" for="c-37319235">[1 more]</label></div><br/><div class="children"><div class="content">Skip your flaky tests should be a religion. There&#x27;s nothing else I feel as strongly about regarding CI optimization. If a test is flaky, it gets immediately skipped. Even if you&#x27;re working on a fix, it&#x27;s skipped until you solve it, if you ever do. Most of your CI problems can start to be solved by applying this simple rule.<p>How do you know if it&#x27;s flaky? You keep a count and any time a test fails&#x2F;recovers 3 times it gets skipped, even if there&#x27;s weeks between failures. You can make it more complex for little gain, but I&#x27;ve found this system will have teams actually prioritize fixing important tests, but mostly it has proven that many &quot;important tests to keep even if they are flaky&quot; never were actually important or end up getting re-written in different ways later on.</div><br/></div></div><div id="37317549" class="c"><input type="checkbox" id="c-37317549" checked=""/><div class="controls bullet"><span class="by">blurker</span><span>|</span><a href="#37319235">prev</a><span>|</span><a href="#37317527">next</a><span>|</span><label class="collapse" for="c-37317549">[-]</label><label class="expand" for="c-37317549">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Back when I was a junior developer, there was a smoke test in our pipeline that never passed. I recall asking, “Why is this test failing?” The Senior Developer I was pairing with answered, “Ohhh, that one, yeah it hardly ever passes.” From that moment on, every time I saw a CI failure, I wondered: “Is this a flaky test, or a genuine failure?”<p>This is a really key insight. It erodes trust in the entire test suite and will lead to false negatives. If I couldn&#x27;t get the time budget to fix the test, I&#x27;d delete it. I think a flaky test is worse than nothing.</div><br/><div id="37318082" class="c"><input type="checkbox" id="c-37318082" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#37317549">parent</a><span>|</span><a href="#37317527">next</a><span>|</span><label class="collapse" for="c-37318082">[-]</label><label class="expand" for="c-37318082">[6 more]</label></div><br/><div class="children"><div class="content">&quot;Normalisation of Deviance&quot; is a concept that will change the way you look at the world once you learn to recognise it. It&#x27;s made famous by Richard Feynman&#x27;s report about the Challenger disaster, where he said that NASA management had started accepting recurring mission-critical failures as <i>normal issues</i> and ignored them.<p>My favourite one is: Pick a server or a piece of enterprise software and go take a look at its logs. If it&#x27;s doing anything interesting at all, it&#x27;ll be full of errors. There&#x27;s a decent chance that those errors are being ignored by everyone responsible for the system, because they&#x27;re &quot;the usual errors&quot;.<p>I&#x27;ve seen this go as far as cluster nodes crashing multiple times per day and rebooting over and over, causing mass fail-over events of services. That was written up as &quot;the system is usually this slow&quot;, in the sense of &quot;there is nothing we can do about it.&quot;<p>It&#x27;s not slow! It&#x27;s <i>broken!</i></div><br/><div id="37318640" class="c"><input type="checkbox" id="c-37318640" checked=""/><div class="controls bullet"><span class="by">ertian</span><span>|</span><a href="#37317549">root</a><span>|</span><a href="#37318082">parent</a><span>|</span><a href="#37318582">next</a><span>|</span><label class="collapse" for="c-37318640">[-]</label><label class="expand" for="c-37318640">[2 more]</label></div><br/><div class="children"><div class="content">Oof, yes. I used to be an SRE at Google, with oncall responsibility for dozens of servers maintained by a dozen or so dev teams.<p>Trying to track down issues with requests that crossed or interacted with 10-15 services, when _all_ those services had logs full of &#x27;normal&#x27; errors (that the devs had learned to ignore) was...pretty brutal. I don&#x27;t know how many hours I wasted chasing red herrings while debugging ongoing prod issues.</div><br/><div id="37318784" class="c"><input type="checkbox" id="c-37318784" checked=""/><div class="controls bullet"><span class="by">m_mueller</span><span>|</span><a href="#37317549">root</a><span>|</span><a href="#37318640">parent</a><span>|</span><a href="#37318582">next</a><span>|</span><label class="collapse" for="c-37318784">[-]</label><label class="expand" for="c-37318784">[1 more]</label></div><br/><div class="children"><div class="content">we&#x27;re using AWS X-ray for this purpose, i.e. a service is always passing on and logging the X-ray identifier generated at first entry into the system. pretty helpful for this purpose. And yes, there should be consistent log handling &#x2F; monitoring. Depending on service we differ between error log level (=expected user errors) and critical error level (makes our monitor go red).</div><br/></div></div></div></div><div id="37318582" class="c"><input type="checkbox" id="c-37318582" checked=""/><div class="controls bullet"><span class="by">gregmac</span><span>|</span><a href="#37317549">root</a><span>|</span><a href="#37318082">parent</a><span>|</span><a href="#37318640">prev</a><span>|</span><a href="#37317527">next</a><span>|</span><label class="collapse" for="c-37318582">[-]</label><label class="expand" for="c-37318582">[3 more]</label></div><br/><div class="children"><div class="content">I agree with what you&#x27;re saying, but this is a bad example:<p>&gt; Pick a server or a piece of enterprise software and go take a look at its logs. If it&#x27;s doing anything interesting at all, it&#x27;ll be full of errors.<p>It&#x27;s true, but IME those &quot;errors&quot; are mostly worth ignoring. Developers, in general, are really bad at logging, and so most logs are full of useless noise. Doubly so for most &quot;enterprise software&quot;.<p>The trouble is context. Eg: &quot;malformed email address&quot; is indeed an error that prevents the email process from sending a message, so it&#x27;s common that someone will put in a log.Error() call for that. In many cases though, that&#x27;s just a user problem. The system operator isn&#x27;t going to and in fact <i>can&#x27;t</i> address it. &quot;Email server unreachable&quot; on the other hand is definitely an error the operator should care about.<p>I still haven&#x27;t actually done it yet, but someday I want to rename that call to log.PageEntireDevTeamAt3AM() and see what happens to log quality..</div><br/><div id="37318706" class="c"><input type="checkbox" id="c-37318706" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#37317549">root</a><span>|</span><a href="#37318582">parent</a><span>|</span><a href="#37317527">next</a><span>|</span><label class="collapse" for="c-37318706">[-]</label><label class="expand" for="c-37318706">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The trouble is context. Eg: &quot;malformed email address&quot; is indeed an error that prevents the email process from sending a message<p>I’m sure you didn’t quite mean it as literal as I’m going to take it and I’m sorry for that. Any process that gets as far as attempting to send an email to something that isn’t a valid e-mail address is, however, an issue that should not be ignored in my opinion.<p>If your e-mail sending process can’t expect valid input then it should validate its input and not cause an error. Of course this is caused by saving invalid e-mail addresses as e-mail addresses in the first place which in it self shows that you’re in trouble, because that means you have to validate everything everywhere because you can’t trust anything. And so on. I’m obviously not disagreeing with your premise. It’s easy to imagine why it would happen and also why it would in fact end up in the “error.log”, but it’s really not an ignorable issue. Or it can be, and it likely is in a lot of places but that’s exactly GPS point isn’t it? That a culture which allows that will eventually cause the spaceship to crash.<p>I think we as a society are far too cool with IT errors in general. I recently went to an appointment where they had some digital parking system where you’d enter your license plate. Only the system was down and the receptionist was like “don’t worry, when the system is down they can’t hand out tickets”. Which is all well and good unless you’re damaged by working in digitalisation and can’t help but do the mental math on just how much money that is costing the parking service. It’s not just the system that’s down, it’s also the entire fleet of parking patrol people who have to sit around and wait for it to get to work. It’s the support phones being hammered and so on. And we just collectively shrug it off because that’s just how IT works “teehee”. I realise this example is probably not the best, considering it’s parking services, but it’s like that everywhere isn’t it?</div><br/><div id="37319287" class="c"><input type="checkbox" id="c-37319287" checked=""/><div class="controls bullet"><span class="by">blcknight</span><span>|</span><a href="#37317549">root</a><span>|</span><a href="#37318706">parent</a><span>|</span><a href="#37317527">next</a><span>|</span><label class="collapse" for="c-37319287">[-]</label><label class="expand" for="c-37319287">[1 more]</label></div><br/><div class="children"><div class="content">I disagree about extensive validating of email addresses. This is why: <a href="https:&#x2F;&#x2F;davidcel.is&#x2F;articles&#x2F;stop-validating-email-addresses-with-regex&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;davidcel.is&#x2F;articles&#x2F;stop-validating-email-addresses...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="37317527" class="c"><input type="checkbox" id="c-37317527" checked=""/><div class="controls bullet"><span class="by">solatic</span><span>|</span><a href="#37317549">prev</a><span>|</span><a href="#37318887">next</a><span>|</span><label class="collapse" for="c-37317527">[-]</label><label class="expand" for="c-37317527">[8 more]</label></div><br/><div class="children"><div class="content">If you have so many flaky tests, and their flakiness is so intractable, that you actually need to come up with SLOs for handling the flakiness to negotiate being allowed to address the flakiness in the test base, then quite frankly, you should be looking for another job, one where you can actually go ahead and just fix these things and get back to shipping value to customers, instead of one where you play bureaucratic games with management that cares neither about craftsmanship nor about getting value out the door as quickly as possible.</div><br/><div id="37319207" class="c"><input type="checkbox" id="c-37319207" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#37317527">parent</a><span>|</span><a href="#37317888">next</a><span>|</span><label class="collapse" for="c-37319207">[-]</label><label class="expand" for="c-37319207">[1 more]</label></div><br/><div class="children"><div class="content">This is a bunch of wishful thinking...<p>Some software projects, let&#x27;s call them &quot;integration projects&quot; use third-party software they can do nothing about.  And it just doesn&#x27;t work well.  But you have to use it in testing because... well, you are the integrator.  The users have already accepted the fact that the software you are integrating doesn&#x27;t work well, so, it&#x27;s &quot;all good&quot;, except it makes it very hard to distinguish between failures that need to be addressed and those that don&#x27;t.<p>Just to give you one example of this situation: JupyterLab is an absolute pile of garbage in terms of how it&#x27;s programmed.  For example, the side-bar of the interface doesn&#x27;t load properly quite often, and you need to click on &quot;reload&quot; button few times to get it to show up.  Suppose now you are the integrator which provides some features that are supposed to be exposed to the user through JupyterLab interface.  Well, what can you do? -- Yes.  Nothing.  Just suck it up.  You can manipulate the threshold for how many times you will retry reloading the interface, but you absolutely have to have a threshold because sometimes the interface will never load (because of some other reason), and you will be stalling the test pipeline if you don&#x27;t let this test fail.<p>In general, the larger the SUT, the more &quot;foreign&quot; components it has, the harder it is to predict the test behavior, and the more flaky the tests are.<p>But this isn&#x27;t the only source of test flakiness.  Hardware is another source.  Especially in embedded software that has to be tested on hardware that the software company has limited access to (think something like Smart TV, where the TV vendor provides some means of accessing the OS running on the TV set, but they deliberately limit the access in such a way as to prevent the SW company from getting access to the proprietary bits installed by the vendor).  So, sometimes things will fail.  And you wouldn&#x27;t know why and wouldn&#x27;t be able to discover (as in, if you tried to break into vendor&#x27;s part of the software, they&#x27;d sue you).</div><br/></div></div><div id="37317888" class="c"><input type="checkbox" id="c-37317888" checked=""/><div class="controls bullet"><span class="by">Swizec</span><span>|</span><a href="#37317527">parent</a><span>|</span><a href="#37319207">prev</a><span>|</span><a href="#37318027">next</a><span>|</span><label class="collapse" for="c-37317888">[-]</label><label class="expand" for="c-37317888">[5 more]</label></div><br/><div class="children"><div class="content">Every flakey test is a production failure that really happens to some users sometimes. How often is just a matter of scale.<p>A 1 in 10,000 failure can be a daily annoyance for your users even with just 100 daily active users who each make 10 actions on your app. At “internet scale” a 1 in 10k error frustrates a user every few seconds.<p>If your tests are so flaky that you need SLOs … your poor users …</div><br/><div id="37318392" class="c"><input type="checkbox" id="c-37318392" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#37317527">root</a><span>|</span><a href="#37317888">parent</a><span>|</span><a href="#37318386">next</a><span>|</span><label class="collapse" for="c-37318392">[-]</label><label class="expand" for="c-37318392">[1 more]</label></div><br/><div class="children"><div class="content">Depending on the problem domain, a lot of times the flakiness is in the way the test is written and not the code under test. You have to judge the relative cost of tracking down and solving that.</div><br/></div></div><div id="37318386" class="c"><input type="checkbox" id="c-37318386" checked=""/><div class="controls bullet"><span class="by">zarroboogs</span><span>|</span><a href="#37317527">root</a><span>|</span><a href="#37317888">parent</a><span>|</span><a href="#37318392">prev</a><span>|</span><a href="#37318112">next</a><span>|</span><label class="collapse" for="c-37318386">[-]</label><label class="expand" for="c-37318386">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Every flakey test is a production failure that really happens to some users sometimes. How often is just a matter of scale.<p>Not really, the one I’ve seen most often is a shared resource failing - for example a Gitlab Runner not handling a new VM for a DB so tests fail etc</div><br/></div></div><div id="37318112" class="c"><input type="checkbox" id="c-37318112" checked=""/><div class="controls bullet"><span class="by">drewcoo</span><span>|</span><a href="#37317527">root</a><span>|</span><a href="#37317888">parent</a><span>|</span><a href="#37318386">prev</a><span>|</span><a href="#37318027">next</a><span>|</span><label class="collapse" for="c-37318112">[-]</label><label class="expand" for="c-37318112">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Every flakey test is a production failure that really happens to some users sometimes<p>No. Tests can be flakey when the code they&#x27;re testing is not. Every flakey test is not a customer problem.<p>Training developers to ignore flakey tests also trains them to ignore real failures. Potentially many of them. Any of those might be real customer problems.</div><br/><div id="37318814" class="c"><input type="checkbox" id="c-37318814" checked=""/><div class="controls bullet"><span class="by">m3ll0</span><span>|</span><a href="#37317527">root</a><span>|</span><a href="#37318112">parent</a><span>|</span><a href="#37318027">next</a><span>|</span><label class="collapse" for="c-37318814">[-]</label><label class="expand" for="c-37318814">[1 more]</label></div><br/><div class="children"><div class="content">This! Flaky tests become a customer problem when developers learn to ignore them!
You gotta weed them out.</div><br/></div></div></div></div></div></div></div></div><div id="37318887" class="c"><input type="checkbox" id="c-37318887" checked=""/><div class="controls bullet"><span class="by">Sparkyte</span><span>|</span><a href="#37317527">prev</a><span>|</span><a href="#37318538">next</a><span>|</span><label class="collapse" for="c-37318887">[-]</label><label class="expand" for="c-37318887">[2 more]</label></div><br/><div class="children"><div class="content">So I got in a discussion with someone why we need to test post-deployment. I was like because your environments are different you want to eliminate a failure point even if you&#x27;ve tested at build. You can make everything as closely similar to each other as possible but you want to eliminate the failures of a bad configuration, schema or integration. I once was at a place where someone deployed something and just let produce malformed data because a schema wasn&#x27;t applied to the transformation process of the workers. You know what would have solved that? Pos deploy testing. How does is this in an automated pipeline, you automate the test.</div><br/><div id="37319120" class="c"><input type="checkbox" id="c-37319120" checked=""/><div class="controls bullet"><span class="by">tnolet</span><span>|</span><a href="#37318887">parent</a><span>|</span><a href="#37318538">next</a><span>|</span><label class="collapse" for="c-37319120">[-]</label><label class="expand" for="c-37319120">[1 more]</label></div><br/><div class="children"><div class="content">With risk of tooting my own horn too much, this is exactly why I started my company <a href="https:&#x2F;&#x2F;checklyhq.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;checklyhq.com</a><p>We approach it a bit different: we blur the lines between E2E testing and production monitoring. You run an E2E test and promote it to a monitor that runs around the clock.<p>It&#x27;s quite powerful. Just an E2E test that logs into your production environment after deploy and then every 10 minutes will catch a ton of catastrophic bugs.<p>You can also trigger them in CI or right after production deployment.<p>Big fat disclaimer: I&#x27;m a founder and CTO.</div><br/></div></div></div></div><div id="37318538" class="c"><input type="checkbox" id="c-37318538" checked=""/><div class="controls bullet"><span class="by">jedilance</span><span>|</span><a href="#37318887">prev</a><span>|</span><a href="#37318954">next</a><span>|</span><label class="collapse" for="c-37318538">[-]</label><label class="expand" for="c-37318538">[1 more]</label></div><br/><div class="children"><div class="content">Off topic but I liked the presentation template a lot.</div><br/></div></div><div id="37318954" class="c"><input type="checkbox" id="c-37318954" checked=""/><div class="controls bullet"><span class="by">sharts</span><span>|</span><a href="#37318538">prev</a><span>|</span><label class="collapse" for="c-37318954">[-]</label><label class="expand" for="c-37318954">[1 more]</label></div><br/><div class="children"><div class="content">This is obvious. So of course most people will never do it.</div><br/></div></div></div></div></div></div></div></body></html>