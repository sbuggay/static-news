<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693990869238" as="style"/><link rel="stylesheet" href="styles.css?v=1693990869238"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.fast.ai/posts/2023-09-04-learning-jumps/">Can LLMs learn from a single example?</a> <span class="domain">(<a href="https://www.fast.ai">www.fast.ai</a>)</span></div><div class="subtext"><span>jdkee</span> | <span>73 comments</span></div><br/><div><div id="37400003" class="c"><input type="checkbox" id="c-37400003" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37402419">next</a><span>|</span><label class="collapse" for="c-37400003">[-]</label><label class="expand" for="c-37400003">[28 more]</label></div><br/><div class="children"><div class="content">Thank you for posting this to HN! :D<p>I&#x27;m one of the authors of this post -- Johno &amp; I found it really interesting looking into this curious issue of rapid memorization from LLMs. I&#x27;ve been working with neural nets for 30 years, and fine-tuning language models since 2017, and this behavior is most surprising to me! Other folks have seen it in LLMs too, although I haven&#x27;t seen a analysis of this kind before (although we might have missed something).<p>Let me know if you have any questions or thoughts.</div><br/><div id="37400120" class="c"><input type="checkbox" id="c-37400120" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37400003">parent</a><span>|</span><a href="#37400548">next</a><span>|</span><label class="collapse" for="c-37400120">[-]</label><label class="expand" for="c-37400120">[15 more]</label></div><br/><div class="children"><div class="content">In the palm-e paper (<a href="https:&#x2F;&#x2F;palm-e.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;palm-e.github.io&#x2F;</a>), when they try to unfreeze and train the LLM on new image data only, there is expectedly a lot of CF on NLP tasks but very interestingly, the effect diminishes greatly with the scale of the LLM prior to training.<p>From an average -87.3% performance drop on the 12B model to -61.6% on the 84B model then just -3.9% on the 562B model. Felt like we were just shy of an insight breakthrough here.<p>Is avoiding CF potentially just a matter of sheer scale ?</div><br/><div id="37400137" class="c"><input type="checkbox" id="c-37400137" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400120">parent</a><span>|</span><a href="#37400177">next</a><span>|</span><label class="collapse" for="c-37400137">[-]</label><label class="expand" for="c-37400137">[13 more]</label></div><br/><div class="children"><div class="content">I think our experiments actually <i>don&#x27;t</i> show catastrophic forgetting! The accuracy does <i>not</i> decrease as loss gets worse -- it&#x27;s simply getting over-confident.<p>So I&#x27;m not even sure we&#x27;re showing any problem to solve here -- it might be more of a opportunity, in fact!</div><br/><div id="37401778" class="c"><input type="checkbox" id="c-37401778" checked=""/><div class="controls bullet"><span class="by">vvrm</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400137">parent</a><span>|</span><a href="#37401500">next</a><span>|</span><label class="collapse" for="c-37401778">[-]</label><label class="expand" for="c-37401778">[3 more]</label></div><br/><div class="children"><div class="content">I have been training a natural intelligence model for 3 years now and she still doesn’t get nuance. Things are either good or bad in her book: nothing in between. My plan is to let her train with binary good&#x2F;bad labels till the age of 5 and then start smoothing the labels after that. Wonder if that works for your AI.</div><br/><div id="37402030" class="c"><input type="checkbox" id="c-37402030" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401778">parent</a><span>|</span><a href="#37402655">next</a><span>|</span><label class="collapse" for="c-37402030">[-]</label><label class="expand" for="c-37402030">[1 more]</label></div><br/><div class="children"><div class="content">This took a couple reads, but it’s funny. The bad news is that I’ve been training mine for 17 years and nuance is still something that needs more training.</div><br/></div></div><div id="37402655" class="c"><input type="checkbox" id="c-37402655" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401778">parent</a><span>|</span><a href="#37402030">prev</a><span>|</span><a href="#37401500">next</a><span>|</span><label class="collapse" for="c-37402655">[-]</label><label class="expand" for="c-37402655">[1 more]</label></div><br/><div class="children"><div class="content">in my mind I&#x27;ve built an &#x27;emotional engine&#x27; to add nuance to models understanding, take something like Plutchik&#x27;s wheel of emotions and create a high quality multi-modal dataset based on that structure, given our current technology takes inspiration from the brain, it would seem like having discrete models specialising in particular aspects of &#x27;intelligence&#x27; that are then organised into a mixture of experts is an interesting area to explore, and perhaps more accessible as smaller models require less resources.</div><br/></div></div></div></div><div id="37401500" class="c"><input type="checkbox" id="c-37401500" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400137">parent</a><span>|</span><a href="#37401778">prev</a><span>|</span><a href="#37400166">next</a><span>|</span><label class="collapse" for="c-37401500">[-]</label><label class="expand" for="c-37401500">[1 more]</label></div><br/><div class="children"><div class="content">Awesome investigative work, what&#x27;s the opportunity though, I don&#x27;t get it</div><br/></div></div><div id="37400166" class="c"><input type="checkbox" id="c-37400166" checked=""/><div class="controls bullet"><span class="by">Yenrabbit</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400137">parent</a><span>|</span><a href="#37401500">prev</a><span>|</span><a href="#37401389">next</a><span>|</span><label class="collapse" for="c-37400166">[-]</label><label class="expand" for="c-37400166">[3 more]</label></div><br/><div class="children"><div class="content">It does start getting worse at some point right?</div><br/><div id="37401301" class="c"><input type="checkbox" id="c-37401301" checked=""/><div class="controls bullet"><span class="by">minihat</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400166">parent</a><span>|</span><a href="#37400214">next</a><span>|</span><label class="collapse" for="c-37401301">[-]</label><label class="expand" for="c-37401301">[1 more]</label></div><br/><div class="children"><div class="content">Cross-entropy loss can start getting worse due to the model becoming less calibrated, even as the classification accuracy continues to improve. I first heard that here: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.04599" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.04599</a><p>Is this &#x27;overconfidence&#x27; the leading explanation as to why LLMs continue to show qualitative improvement even after their test loss levels off?</div><br/></div></div><div id="37400214" class="c"><input type="checkbox" id="c-37400214" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400166">parent</a><span>|</span><a href="#37401301">prev</a><span>|</span><a href="#37401389">next</a><span>|</span><label class="collapse" for="c-37400214">[-]</label><label class="expand" for="c-37400214">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure eventually it would, but we haven&#x27;t gotten to that point yet in our training.</div><br/></div></div></div></div><div id="37401389" class="c"><input type="checkbox" id="c-37401389" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400137">parent</a><span>|</span><a href="#37400166">prev</a><span>|</span><a href="#37400177">next</a><span>|</span><label class="collapse" for="c-37401389">[-]</label><label class="expand" for="c-37401389">[5 more]</label></div><br/><div class="children"><div class="content">Plz eli5 catastrophic forgetting,<p>I assume this means losing all the energy and compute input for a model to know, perform, infer on inputs already indexed(?) (What is the proper term here?)<p>But is this the premise -you lose all prior investment of resource to a (I don&#x27;t know the term for an AI archetype of knowledge) {btw, I love the embedded etymology of knowledge<p>&quot;The ledger of things that we KNOW&quot;}</div><br/><div id="37401482" class="c"><input type="checkbox" id="c-37401482" checked=""/><div class="controls bullet"><span class="by">tarvaina</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401389">parent</a><span>|</span><a href="#37400177">next</a><span>|</span><label class="collapse" for="c-37401482">[-]</label><label class="expand" for="c-37401482">[4 more]</label></div><br/><div class="children"><div class="content">Suppose we have trained a model to perform a certain set of tasks. Later we would want to teach it a new task. Catastrophic forgetting means that teaching it a new task makes it unlearn some or all of its earlier tasks.<p>It occurs because training changes the weights of the model. The earlier set of weights was good for the previous tasks. The new set of weights is only good for the new task. Usually special care must be taken to overcome catastrophic forgetting.</div><br/><div id="37402468" class="c"><input type="checkbox" id="c-37402468" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401482">parent</a><span>|</span><a href="#37401501">next</a><span>|</span><label class="collapse" for="c-37402468">[-]</label><label class="expand" for="c-37402468">[1 more]</label></div><br/><div class="children"><div class="content">I think some cases CF would be even good eg you want llm that produces only valid json data as output.</div><br/></div></div><div id="37401501" class="c"><input type="checkbox" id="c-37401501" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401482">parent</a><span>|</span><a href="#37402468">prev</a><span>|</span><a href="#37400177">next</a><span>|</span><label class="collapse" for="c-37401501">[-]</label><label class="expand" for="c-37401501">[2 more]</label></div><br/><div class="children"><div class="content">Can it be taught &quot;contextual matrices&quot; where by it builds a new layer of construct but preserves the other, then cross learns between parameters or something (sorry for my poor lexicon, I&#x27;m wet-learning :-)<p>But imagine all LLMs in a macro view like a sponge entity</div><br/><div id="37402267" class="c"><input type="checkbox" id="c-37402267" checked=""/><div class="controls bullet"><span class="by">tinco</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401501">parent</a><span>|</span><a href="#37400177">next</a><span>|</span><label class="collapse" for="c-37402267">[-]</label><label class="expand" for="c-37402267">[1 more]</label></div><br/><div class="children"><div class="content">We wouldn&#x27;t know how to construct those matrices because we don&#x27;t know where in the layers what knowledge is represented. One thing that helps a little bit is freezing the lower layers, so at least the model won&#x27;t forget its most fundamental knowledge.<p>Note that the only reason that things are catastrophically forgotten, is that the original examples are not shown again. If the model learns in a single shot, there might simply be no time to show both the old and the new examples. I don&#x27;t think it would have a significant effect or else we&#x27;d know about this effect a lot sooner (i.e. the training of these LLM&#x27;s would get less effective from a certain point)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37400177" class="c"><input type="checkbox" id="c-37400177" checked=""/><div class="controls bullet"><span class="by">Yenrabbit</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400120">parent</a><span>|</span><a href="#37400137">prev</a><span>|</span><a href="#37400548">next</a><span>|</span><label class="collapse" for="c-37400177">[-]</label><label class="expand" for="c-37400177">[1 more]</label></div><br/><div class="children"><div class="content">Ooh interesting, thanks for sharing!</div><br/></div></div></div></div><div id="37400548" class="c"><input type="checkbox" id="c-37400548" checked=""/><div class="controls bullet"><span class="by">jwuphysics</span><span>|</span><a href="#37400003">parent</a><span>|</span><a href="#37400120">prev</a><span>|</span><a href="#37400461">next</a><span>|</span><label class="collapse" for="c-37400548">[-]</label><label class="expand" for="c-37400548">[3 more]</label></div><br/><div class="children"><div class="content">Hi Jeremy, always a fan of your work! Just a technical note since it falls under my domain of expertise (astronomy) -- the example about MOND described here should actually have choice (E) as the correct answer!</div><br/><div id="37400625" class="c"><input type="checkbox" id="c-37400625" checked=""/><div class="controls bullet"><span class="by">jwuphysics</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400548">parent</a><span>|</span><a href="#37400862">next</a><span>|</span><label class="collapse" for="c-37400625">[-]</label><label class="expand" for="c-37400625">[1 more]</label></div><br/><div class="children"><div class="content">In terms of the actual article -- really nice finding. Or I guess, nice set of experiments to decipher what lots of LLM researchers have been finding!<p>I&#x27;ve noticed somewhat similar behavior while training graph neural networks to model physical systems, except that it takes way longer than a single epoch to get there. Or course, there&#x27;s no pretending involved with my GNNs, but the models do have very constrained representations, so once they start to figure out how to represent the physics at hand, the loss plummets dramatically.</div><br/></div></div><div id="37400862" class="c"><input type="checkbox" id="c-37400862" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37400548">parent</a><span>|</span><a href="#37400625">prev</a><span>|</span><a href="#37400461">next</a><span>|</span><label class="collapse" for="c-37400862">[-]</label><label class="expand" for="c-37400862">[1 more]</label></div><br/><div class="children"><div class="content">As it happens I dug into this question in some detail a couple of weeks ago when analysing the dataset, including carefully reading the wikipedia page which the question comes from. AFAICT both D and E are kinda correct, but E isn&#x27;t quite right because MOND doesn&#x27;t entirely &quot;eliminate the observed missing baryonic mass&quot;, but rather just reduces it from a factor of 10 to 2.<p>Is that not correct? (Of course I fully accept your expertise in this matter and this is just my curiosity, not trying to tell you you&#x27;re wrong!)</div><br/></div></div></div></div><div id="37400461" class="c"><input type="checkbox" id="c-37400461" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37400003">parent</a><span>|</span><a href="#37400548">prev</a><span>|</span><a href="#37401886">next</a><span>|</span><label class="collapse" for="c-37400461">[-]</label><label class="expand" for="c-37400461">[1 more]</label></div><br/><div class="children"><div class="content">What is the base model? I think that was a big oversight to leave that out and attribute this to LLMs in general.<p>Although I am not a researcher, it is obvious to me that not all LLMs are the same architecture, and I think that even ones with similar architecture can evolve to functionally operate quite differently on the same inputs.<p>Yet most articles seem to refer to LLMs as if they were just one architecture and model.</div><br/></div></div><div id="37401886" class="c"><input type="checkbox" id="c-37401886" checked=""/><div class="controls bullet"><span class="by">armatav</span><span>|</span><a href="#37400003">parent</a><span>|</span><a href="#37400461">prev</a><span>|</span><a href="#37400489">next</a><span>|</span><label class="collapse" for="c-37401886">[-]</label><label class="expand" for="c-37401886">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if you could perform inference, highlight the weights that were most used during that inference, grab the hottest 20%, freeze the rest of the model, and perform backpropagation solely on those to allow for more of this sort of rapid memorization behavior closer to the end user.<p>Like online learning in a way. But you do it during inference time.<p>There’s no way the entire model actually needs to be touched for something like “sky color is:” and “blue”.</div><br/><div id="37401888" class="c"><input type="checkbox" id="c-37401888" checked=""/><div class="controls bullet"><span class="by">armatav</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401886">parent</a><span>|</span><a href="#37400489">next</a><span>|</span><label class="collapse" for="c-37401888">[-]</label><label class="expand" for="c-37401888">[3 more]</label></div><br/><div class="children"><div class="content">In fact I bet you could update like one or two neurons for certain concepts, and then transplant those neurons to another LLM to give it some idea of it. Like a literal brain transplant but for concepts.</div><br/><div id="37401897" class="c"><input type="checkbox" id="c-37401897" checked=""/><div class="controls bullet"><span class="by">armatav</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401888">parent</a><span>|</span><a href="#37401922">next</a><span>|</span><label class="collapse" for="c-37401897">[-]</label><label class="expand" for="c-37401897">[1 more]</label></div><br/><div class="children"><div class="content">And you could identify these neurons using dropout techniques and repetitively querying the model against them.<p>Drop a set of neurons and there’s no change? Probably doesn’t contain the “sky color” concept.<p>Drop a set of neurons and the model freaks out, definitely conceptual neurons.<p>Rinse and repeat to find the distilled pattern across all the neurons.<p>You could train an LLM against the neuron graph to do this for you.</div><br/></div></div><div id="37401922" class="c"><input type="checkbox" id="c-37401922" checked=""/><div class="controls bullet"><span class="by">niemandhier</span><span>|</span><a href="#37400003">root</a><span>|</span><a href="#37401888">parent</a><span>|</span><a href="#37401897">prev</a><span>|</span><a href="#37400489">next</a><span>|</span><label class="collapse" for="c-37401922">[-]</label><label class="expand" for="c-37401922">[1 more]</label></div><br/><div class="children"><div class="content">Many neurons are polysynthactic, that makes interventions like the proposed difficult.</div><br/></div></div></div></div></div></div><div id="37400489" class="c"><input type="checkbox" id="c-37400489" checked=""/><div class="controls bullet"><span class="by">n9Mtq4</span><span>|</span><a href="#37400003">parent</a><span>|</span><a href="#37401886">prev</a><span>|</span><a href="#37400549">next</a><span>|</span><label class="collapse" for="c-37400489">[-]</label><label class="expand" for="c-37400489">[1 more]</label></div><br/><div class="children"><div class="content">Very cool. This came up in a huggingface transformers issue a while ago and we also determined memorization to be the likely reason. It&#x27;s nice to see someone else reach the same conclusion.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;18730">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;18730</a></div><br/></div></div><div id="37400549" class="c"><input type="checkbox" id="c-37400549" checked=""/><div class="controls bullet"><span class="by">ScoutOrgo</span><span>|</span><a href="#37400003">parent</a><span>|</span><a href="#37400489">prev</a><span>|</span><a href="#37400953">next</a><span>|</span><label class="collapse" for="c-37400549">[-]</label><label class="expand" for="c-37400549">[1 more]</label></div><br/><div class="children"><div class="content">Hey Jeremy, it seems like you could calculate exactly how much a model learns in a single step by calculating the loss for a batch a second time (with no_grad) after the loss is calculated the first time and gradients are updated. This seems like it could produce interesting outputs when graphing the difference of first and second losses at the batch or observation&#x2F;question level.</div><br/></div></div><div id="37400953" class="c"><input type="checkbox" id="c-37400953" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#37400003">parent</a><span>|</span><a href="#37400549">prev</a><span>|</span><a href="#37400311">next</a><span>|</span><label class="collapse" for="c-37400953">[-]</label><label class="expand" for="c-37400953">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, but you should show the example as concrete evidence, rather than hand waving arguments based on loss curves “evidence”.</div><br/></div></div><div id="37400311" class="c"><input type="checkbox" id="c-37400311" checked=""/><div class="controls bullet"><span class="by">azg123</span><span>|</span><a href="#37400003">parent</a><span>|</span><a href="#37400953">prev</a><span>|</span><a href="#37402419">next</a><span>|</span><label class="collapse" for="c-37400311">[-]</label><label class="expand" for="c-37400311">[1 more]</label></div><br/><div class="children"><div class="content">Super interesting! Another area that I&#x27;ve seen these types of loss curves are recommendation models: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2209.06053.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2209.06053.pdf</a></div><br/></div></div></div></div><div id="37402419" class="c"><input type="checkbox" id="c-37402419" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37400003">prev</a><span>|</span><a href="#37400591">next</a><span>|</span><label class="collapse" for="c-37402419">[-]</label><label class="expand" for="c-37402419">[1 more]</label></div><br/><div class="children"><div class="content">Do people really use the phrase “over confident” in this way? It is very misleading.<p>What is happening is called “over fitting”.<p>Think of data as dots. A model that generalizes well will create as simple of a function as possible that fits the training data points pretty well.<p>But keep training and parameters will often get very large, creating huge up and down swings in the function curve, far outside the actual data values, in order to pass through the training data points exactly.<p>So it’s technically a better fit to the training data, but it is now a crazy function, often producing extreme outputs on new data. Practically a worst case lack of generalization.<p><i>Thus, “over fitting”.</i><p>And “over fitting” isn’t the same as “memorization”. Large models can memorize small datasets without over fitting.  They have so many parameters, it takes few changes to fit the training data. At which time, learning stops at an otherwise random function, and generalization is never achieved.<p><i>That case is called “underdetermined”.</i><p>There are models that produce both outputs and confidences (essentially predict their own error standard deviation per output, based on the input).<p><i>So “over confident” can mean a model that predicted high confidence (low error deviation) inaccurately.</i></div><br/></div></div><div id="37400591" class="c"><input type="checkbox" id="c-37400591" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37402419">prev</a><span>|</span><a href="#37400802">next</a><span>|</span><label class="collapse" for="c-37400591">[-]</label><label class="expand" for="c-37400591">[5 more]</label></div><br/><div class="children"><div class="content">Was this not sort of the clear implication of the fact that most LLMs are currently only being trained with one epoch?<p>ie. if they are only being trained from one epoch, there is clear overfitting concerns just by doing even a second pass in the data.<p>It does seem somewhat contrary to the findings of this paper [0] that found that old data was as good as new for at least 4 epochs.<p>[0]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.16264" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.16264</a></div><br/><div id="37400985" class="c"><input type="checkbox" id="c-37400985" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#37400591">parent</a><span>|</span><a href="#37400867">next</a><span>|</span><label class="collapse" for="c-37400985">[-]</label><label class="expand" for="c-37400985">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Was this not sort of the clear implication of the fact that most LLMs are currently only being trained with one epoch?<p>Slight nit: Many public LLMs are trained for at least slightly over one epoch, and usually several epochs on particular subsets of the data (like wikipedia).</div><br/></div></div><div id="37400867" class="c"><input type="checkbox" id="c-37400867" checked=""/><div class="controls bullet"><span class="by">computerex</span><span>|</span><a href="#37400591">parent</a><span>|</span><a href="#37400985">prev</a><span>|</span><a href="#37400869">next</a><span>|</span><label class="collapse" for="c-37400867">[-]</label><label class="expand" for="c-37400867">[1 more]</label></div><br/><div class="children"><div class="content">They are not being trained only on 1 epoch. They are trained on multiple epochs for high quality data. Also Meta team with llama show that simply training more, more tokens, continues to reduce loss.</div><br/></div></div><div id="37400869" class="c"><input type="checkbox" id="c-37400869" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37400591">parent</a><span>|</span><a href="#37400867">prev</a><span>|</span><a href="#37400802">next</a><span>|</span><label class="collapse" for="c-37400869">[-]</label><label class="expand" for="c-37400869">[2 more]</label></div><br/><div class="children"><div class="content">&lt;deleted&gt;</div><br/><div id="37401066" class="c"><input type="checkbox" id="c-37401066" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37400591">root</a><span>|</span><a href="#37400869">parent</a><span>|</span><a href="#37400802">next</a><span>|</span><label class="collapse" for="c-37401066">[-]</label><label class="expand" for="c-37401066">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s the exact paper i link in my comment :)</div><br/></div></div></div></div></div></div><div id="37400802" class="c"><input type="checkbox" id="c-37400802" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#37400591">prev</a><span>|</span><a href="#37402396">next</a><span>|</span><label class="collapse" for="c-37400802">[-]</label><label class="expand" for="c-37400802">[4 more]</label></div><br/><div class="children"><div class="content">I see similar loss curves when training ViTs (from scratch), which has always bothered me but I had bigger concerns so never delved too deep into it.  The only difference is that I see the training loss go _up_ during each epoch.  The cliffs between epochs are large enough that training loss goes down overall and validation loss keeps going down the whole time as well.  The model gets close-ish to SoTA so I guess it&#x27;s &quot;normal&quot;.<p>I haven&#x27;t trained convnets at this scale so I&#x27;m not sure if similar behavior has been seen there, but you&#x27;d think someone would have mentioned it at some point.  So perhaps these strange loss curves are a feature of Transformer based models in particular?</div><br/><div id="37401118" class="c"><input type="checkbox" id="c-37401118" checked=""/><div class="controls bullet"><span class="by">lIIllIIllIIllII</span><span>|</span><a href="#37400802">parent</a><span>|</span><a href="#37400843">next</a><span>|</span><label class="collapse" for="c-37401118">[-]</label><label class="expand" for="c-37401118">[1 more]</label></div><br/><div class="children"><div class="content">The original article mentioned LLMs needing powerful abstractions<p>this is basically the case with transformer networks, which is apparent when learning from scratch. The model seems to be going basically nowhere and totally useless until suddenly, at some random point after a bunch of learning cycles the weights find some minimum on the error surface and bam, suddenly the model can do things properly. And it&#x27;s because the transformer has learned an abstraction that works for all of the input data in an attentional sense (think how you scan a sentence when reading). Not the best explanation but its from memory from a post I saw on HN a while back</div><br/></div></div><div id="37400843" class="c"><input type="checkbox" id="c-37400843" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37400802">parent</a><span>|</span><a href="#37401118">prev</a><span>|</span><a href="#37401408">next</a><span>|</span><label class="collapse" for="c-37400843">[-]</label><label class="expand" for="c-37400843">[1 more]</label></div><br/><div class="children"><div class="content">Oh wow yeah - I&#x27;ve also seen other people&#x27;s training loss curves like that, going up during each epoch and then jumping down at the end of the epoch. I&#x27;ve never experienced that myself, and have no idea what&#x27;s causing it!</div><br/></div></div><div id="37401408" class="c"><input type="checkbox" id="c-37401408" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37400802">parent</a><span>|</span><a href="#37400843">prev</a><span>|</span><a href="#37402396">next</a><span>|</span><label class="collapse" for="c-37401408">[-]</label><label class="expand" for="c-37401408">[1 more]</label></div><br/><div class="children"><div class="content">even in the first epoch the loss goes up? that seems.. odd</div><br/></div></div></div></div><div id="37402396" class="c"><input type="checkbox" id="c-37402396" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#37400802">prev</a><span>|</span><a href="#37400343">next</a><span>|</span><label class="collapse" for="c-37402396">[-]</label><label class="expand" for="c-37402396">[1 more]</label></div><br/><div class="children"><div class="content">If you find this interesting, checkout also &quot;Mass Editing Memory in a Transformer&quot; [1] and &quot;Locating and Editing Factual Associations in GPT&quot; [2].<p>[1] <a href="https:&#x2F;&#x2F;memit.baulab.info&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;memit.baulab.info&#x2F;</a>
[2] <a href="https:&#x2F;&#x2F;rome.baulab.info&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;rome.baulab.info&#x2F;</a></div><br/></div></div><div id="37400343" class="c"><input type="checkbox" id="c-37400343" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#37402396">prev</a><span>|</span><a href="#37402038">next</a><span>|</span><label class="collapse" for="c-37400343">[-]</label><label class="expand" for="c-37400343">[9 more]</label></div><br/><div class="children"><div class="content">If this holds true, this would support the idea that much smaller, human curated datasets will be of much higher value than synthetic datasets generated by LLMs</div><br/><div id="37400711" class="c"><input type="checkbox" id="c-37400711" checked=""/><div class="controls bullet"><span class="by">rsrsrs86</span><span>|</span><a href="#37400343">parent</a><span>|</span><a href="#37401317">next</a><span>|</span><label class="collapse" for="c-37400711">[-]</label><label class="expand" for="c-37400711">[1 more]</label></div><br/><div class="children"><div class="content">Whichever has the most information wins. When the information has structure you can heavily exploit it for generating synthetic data. For this I point you to Apple Sim. It’s a repository of 3D models for interiors. You can generate many layers of information by controlling the renderer and then use it on real photos. That’s done all over images so vectorial spaces are pretty natural for embeddings. You don’t need to add much structure algebraically speaking.<p>If your domain is heavily algebraic, you might even be able to generate correct examples arbitrarily, which is a situation I recommend anyone to be in.</div><br/></div></div><div id="37401317" class="c"><input type="checkbox" id="c-37401317" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#37400343">parent</a><span>|</span><a href="#37400711">prev</a><span>|</span><a href="#37400448">next</a><span>|</span><label class="collapse" for="c-37401317">[-]</label><label class="expand" for="c-37401317">[2 more]</label></div><br/><div class="children"><div class="content">I doubt it.  If anything, ULMFiT era AI has finally killed the need for human curated data.  ChatGPT 4 is already being used as an oracle model that everyday AI models are trained off of.  A truly gargantuan oracle model will obviate all but the smallest of human input.</div><br/><div id="37401722" class="c"><input type="checkbox" id="c-37401722" checked=""/><div class="controls bullet"><span class="by">tellarin</span><span>|</span><a href="#37400343">root</a><span>|</span><a href="#37401317">parent</a><span>|</span><a href="#37400448">next</a><span>|</span><label class="collapse" for="c-37401722">[-]</label><label class="expand" for="c-37401722">[1 more]</label></div><br/><div class="children"><div class="content">GPT4 relies heavily on human curated data. Both for specific domains and for instruction following. Any new model that tries to go beyond it will also likely rely on such data.</div><br/></div></div></div></div><div id="37400448" class="c"><input type="checkbox" id="c-37400448" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#37400343">parent</a><span>|</span><a href="#37401317">prev</a><span>|</span><a href="#37401292">next</a><span>|</span><label class="collapse" for="c-37400448">[-]</label><label class="expand" for="c-37400448">[2 more]</label></div><br/><div class="children"><div class="content">I assume there is a value metric that balances quantity with quantity that may be exploitable in our mid-gains period of understanding the tech behavior -- meaning potential gains from synthetic data. That said, I also expect no-free-lunch to kick in at some point, and synthetic data doesn&#x27;t always pay attention to the data generating process for outliers.</div><br/><div id="37400722" class="c"><input type="checkbox" id="c-37400722" checked=""/><div class="controls bullet"><span class="by">rsrsrs86</span><span>|</span><a href="#37400343">root</a><span>|</span><a href="#37400448">parent</a><span>|</span><a href="#37401292">next</a><span>|</span><label class="collapse" for="c-37400722">[-]</label><label class="expand" for="c-37400722">[1 more]</label></div><br/><div class="children"><div class="content">You will find active learning interesting. It starts by attributing a value to each point in your domain that it learns to match the expected gain in some performance metric.<p>This metric can be learned so it’s okay if it’s really hard to specify.</div><br/></div></div></div></div><div id="37401292" class="c"><input type="checkbox" id="c-37401292" checked=""/><div class="controls bullet"><span class="by">cuuupid</span><span>|</span><a href="#37400343">parent</a><span>|</span><a href="#37400448">prev</a><span>|</span><a href="#37400822">next</a><span>|</span><label class="collapse" for="c-37401292">[-]</label><label class="expand" for="c-37401292">[1 more]</label></div><br/><div class="children"><div class="content">Google reached that conclusion ~2 years ago but has yet to show significant results, key word above being curated</div><br/></div></div><div id="37400822" class="c"><input type="checkbox" id="c-37400822" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#37400343">parent</a><span>|</span><a href="#37401292">prev</a><span>|</span><a href="#37400379">next</a><span>|</span><label class="collapse" for="c-37400822">[-]</label><label class="expand" for="c-37400822">[1 more]</label></div><br/><div class="children"><div class="content">Why are we only able to theorize about these things? Why can&#x27;t we get know and why these things work?</div><br/></div></div><div id="37400379" class="c"><input type="checkbox" id="c-37400379" checked=""/><div class="controls bullet"><span class="by">RhysU</span><span>|</span><a href="#37400343">parent</a><span>|</span><a href="#37400822">prev</a><span>|</span><a href="#37402038">next</a><span>|</span><label class="collapse" for="c-37400379">[-]</label><label class="expand" for="c-37400379">[1 more]</label></div><br/><div class="children"><div class="content">This is not surprising in the context of our wetware: &quot;Jane sees Spot run. Run, Spot, Run.&quot;</div><br/></div></div></div></div><div id="37402038" class="c"><input type="checkbox" id="c-37402038" checked=""/><div class="controls bullet"><span class="by">deyiao</span><span>|</span><a href="#37400343">prev</a><span>|</span><a href="#37400337">next</a><span>|</span><label class="collapse" for="c-37402038">[-]</label><label class="expand" for="c-37402038">[1 more]</label></div><br/><div class="children"><div class="content">I often observe similar phenomenna in CNN related reserch. which indicate that the model indeed can learn from a single example, but sadly, this requires the dataset to be randomly distributed, In real-world applications, new data does not meet this requirement.</div><br/></div></div><div id="37400337" class="c"><input type="checkbox" id="c-37400337" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#37402038">prev</a><span>|</span><a href="#37401677">next</a><span>|</span><label class="collapse" for="c-37400337">[-]</label><label class="expand" for="c-37400337">[6 more]</label></div><br/><div class="children"><div class="content">Does anyone know if LLMs have been used to augment their own training data?<p>I wonder what would happen if you trained an LLM on a little input but then had it generate a lot of synthetic input added to the training data. I think of it as &quot;dreaming&quot;. This seems like it would just add noise, but LLMs are able to improve their output by augmenting their own context (by &quot;thinking out loud&quot;), maybe they can do the same with their own training data?</div><br/><div id="37401008" class="c"><input type="checkbox" id="c-37401008" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#37400337">parent</a><span>|</span><a href="#37400413">next</a><span>|</span><label class="collapse" for="c-37401008">[-]</label><label class="expand" for="c-37401008">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s effectively what RLHF is; a means for LLMs to self train on their own output exclusively by using a small human curated dataset as guidance as to what a &quot;good&quot; and &quot;bad&quot; output is.</div><br/></div></div><div id="37400413" class="c"><input type="checkbox" id="c-37400413" checked=""/><div class="controls bullet"><span class="by">muxator</span><span>|</span><a href="#37400337">parent</a><span>|</span><a href="#37401008">prev</a><span>|</span><a href="#37400873">next</a><span>|</span><label class="collapse" for="c-37400413">[-]</label><label class="expand" for="c-37400413">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting that this conclusion is the exact opposite of a sibling comment, which proposes that a small, human-curated corpus may be more effective than big, synthetic datasets.</div><br/><div id="37400438" class="c"><input type="checkbox" id="c-37400438" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#37400337">root</a><span>|</span><a href="#37400413">parent</a><span>|</span><a href="#37400873">next</a><span>|</span><label class="collapse" for="c-37400438">[-]</label><label class="expand" for="c-37400438">[1 more]</label></div><br/><div class="children"><div class="content">I have no &quot;conclusion&quot;. I&#x27;m just wondering.</div><br/></div></div></div></div><div id="37400873" class="c"><input type="checkbox" id="c-37400873" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37400337">parent</a><span>|</span><a href="#37400413">prev</a><span>|</span><a href="#37400687">next</a><span>|</span><label class="collapse" for="c-37400873">[-]</label><label class="expand" for="c-37400873">[1 more]</label></div><br/><div class="children"><div class="content">Yes, a lot of recent research uses LLM outputs as training data, and it&#x27;s been an extremely successful line of work.</div><br/></div></div><div id="37400687" class="c"><input type="checkbox" id="c-37400687" checked=""/><div class="controls bullet"><span class="by">rsrsrs86</span><span>|</span><a href="#37400337">parent</a><span>|</span><a href="#37400873">prev</a><span>|</span><a href="#37401677">next</a><span>|</span><label class="collapse" for="c-37400687">[-]</label><label class="expand" for="c-37400687">[1 more]</label></div><br/><div class="children"><div class="content">You can find the answer by trying the following: generate random data according to a model, fit a linear regression (or any other distribution), sample from the distribution, add it as to the training set.</div><br/></div></div></div></div><div id="37401677" class="c"><input type="checkbox" id="c-37401677" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#37400337">prev</a><span>|</span><a href="#37401472">next</a><span>|</span><label class="collapse" for="c-37401677">[-]</label><label class="expand" for="c-37401677">[1 more]</label></div><br/><div class="children"><div class="content">I found the title misleading.<p>Isn&#x27;t learning from a single example desirable, while memorizing undesirable in the context of training? The former is the goal we&#x27;re aiming for in order to match how animals learn, while the latter a failure mode that happens often. The article shows a case of unexplained memorizing, not of learning, right?</div><br/></div></div><div id="37401472" class="c"><input type="checkbox" id="c-37401472" checked=""/><div class="controls bullet"><span class="by">klft</span><span>|</span><a href="#37401677">prev</a><span>|</span><a href="#37401442">next</a><span>|</span><label class="collapse" for="c-37401472">[-]</label><label class="expand" for="c-37401472">[4 more]</label></div><br/><div class="children"><div class="content">GPT-4 (I haven&#x27;t really tested other models) is surprisingly adept at &quot;learning&quot; from examples provided as part of the prompt. This could be due to the same underlying mechanism.</div><br/><div id="37401489" class="c"><input type="checkbox" id="c-37401489" checked=""/><div class="controls bullet"><span class="by">bathtub365</span><span>|</span><a href="#37401472">parent</a><span>|</span><a href="#37402328">next</a><span>|</span><label class="collapse" for="c-37401489">[-]</label><label class="expand" for="c-37401489">[2 more]</label></div><br/><div class="children"><div class="content">I’ve found the opposite in trying to get it to play Wordle. It’ll repeatedly forget things it’s seemingly learned within the same session, all the while confident in its correctness.</div><br/><div id="37401893" class="c"><input type="checkbox" id="c-37401893" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#37401472">root</a><span>|</span><a href="#37401489">parent</a><span>|</span><a href="#37402328">next</a><span>|</span><label class="collapse" for="c-37401893">[-]</label><label class="expand" for="c-37401893">[1 more]</label></div><br/><div class="children"><div class="content">What approach are you using to get the LLM to split words into individual letters?</div><br/></div></div></div></div><div id="37402328" class="c"><input type="checkbox" id="c-37402328" checked=""/><div class="controls bullet"><span class="by">cypress66</span><span>|</span><a href="#37401472">parent</a><span>|</span><a href="#37401489">prev</a><span>|</span><a href="#37401442">next</a><span>|</span><label class="collapse" for="c-37402328">[-]</label><label class="expand" for="c-37402328">[1 more]</label></div><br/><div class="children"><div class="content">Not really. That&#x27;s called few shot learner.<p>It&#x27;s basically unrelated to what happens during training, which is using gradients.</div><br/></div></div></div></div><div id="37401442" class="c"><input type="checkbox" id="c-37401442" checked=""/><div class="controls bullet"><span class="by">justanotherjoe</span><span>|</span><a href="#37401472">prev</a><span>|</span><a href="#37401346">next</a><span>|</span><label class="collapse" for="c-37401442">[-]</label><label class="expand" for="c-37401442">[2 more]</label></div><br/><div class="children"><div class="content">isn&#x27;t it highly dependent on what is your one epoch of data? if there are a lot of repetitions of similar concepts in there then can you say it&#x27;s learning from one example?</div><br/><div id="37401846" class="c"><input type="checkbox" id="c-37401846" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37401442">parent</a><span>|</span><a href="#37401346">next</a><span>|</span><label class="collapse" for="c-37401846">[-]</label><label class="expand" for="c-37401846">[1 more]</label></div><br/><div class="children"><div class="content">If it was due to repetition there wouldn&#x27;t be those sudden cliffs after each epoch.</div><br/></div></div></div></div><div id="37401346" class="c"><input type="checkbox" id="c-37401346" checked=""/><div class="controls bullet"><span class="by">spit2wind</span><span>|</span><a href="#37401442">prev</a><span>|</span><a href="#37400436">next</a><span>|</span><label class="collapse" for="c-37401346">[-]</label><label class="expand" for="c-37401346">[2 more]</label></div><br/><div class="children"><div class="content">What are the axis labels on the graphs?</div><br/><div id="37401841" class="c"><input type="checkbox" id="c-37401841" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#37401346">parent</a><span>|</span><a href="#37400436">next</a><span>|</span><label class="collapse" for="c-37401841">[-]</label><label class="expand" for="c-37401841">[1 more]</label></div><br/><div class="children"><div class="content">Cross entropy loss vs batch number</div><br/></div></div></div></div><div id="37400436" class="c"><input type="checkbox" id="c-37400436" checked=""/><div class="controls bullet"><span class="by">rafaelero</span><span>|</span><a href="#37401346">prev</a><span>|</span><a href="#37400517">next</a><span>|</span><label class="collapse" for="c-37400436">[-]</label><label class="expand" for="c-37400436">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s intriguing. But what I want to see is if that one example can change the whole web of knowledge previously established. So, for example, if we finetune the model with a sentence like &quot;Scientists discovered that a type of antigen can make a host immune to HIV&quot; will it then be able to infer that &quot;mRNA vaccines are a valid preventive approach to AIDS since they may be able to express a type of resistance known to make hosts immune to HIV&quot;?</div><br/></div></div><div id="37400517" class="c"><input type="checkbox" id="c-37400517" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#37400436">prev</a><span>|</span><a href="#37402784">next</a><span>|</span><label class="collapse" for="c-37400517">[-]</label><label class="expand" for="c-37400517">[5 more]</label></div><br/><div class="children"><div class="content">Does this mean it is now computationally efficient to have the model learn&#x2F;memorize information on the fly, say the current chat context, as part of the model weights? One shot encoding (something the hippocampus is very good at) allows us to build experiences into retrievable memories tied into semantic concepts we&#x27;ve previously learned..in fact it gets better the more rich our semantic conceptualization of events become from childhood into adulthood.<p>If memorization of events in llm is accelerated because of- these deep semantic frameworks, then does this provide a path towards long context windows?</div><br/><div id="37400987" class="c"><input type="checkbox" id="c-37400987" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#37400517">parent</a><span>|</span><a href="#37400710">next</a><span>|</span><label class="collapse" for="c-37400987">[-]</label><label class="expand" for="c-37400987">[1 more]</label></div><br/><div class="children"><div class="content">Maybe, but there are a lot of unknowns. Does the &quot;memorization on the fly&quot; come with catastrophic forgetting of other information? How does one control for memorizing recent stuff vs. remembering older stuff?</div><br/></div></div><div id="37400710" class="c"><input type="checkbox" id="c-37400710" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37400517">parent</a><span>|</span><a href="#37400987">prev</a><span>|</span><a href="#37402784">next</a><span>|</span><label class="collapse" for="c-37400710">[-]</label><label class="expand" for="c-37400710">[3 more]</label></div><br/><div class="children"><div class="content">Beginner here, so just musing:<p>I like the idea. You would need your own mutable copy of the model, which is usually huge. And you need to backprop so there is a bit more computation. It might be doable for a local model that is smaller than GPT3.5&#x2F;4.<p>You also need to decide what is worth memorizing long term vs short term.</div><br/><div id="37401163" class="c"><input type="checkbox" id="c-37401163" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#37400517">root</a><span>|</span><a href="#37400710">parent</a><span>|</span><a href="#37402784">next</a><span>|</span><label class="collapse" for="c-37401163">[-]</label><label class="expand" for="c-37401163">[2 more]</label></div><br/><div class="children"><div class="content">&gt; own mutable copy of the model, which is usually huge<p>It could just be the diff against the main model or similar.</div><br/><div id="37401357" class="c"><input type="checkbox" id="c-37401357" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37400517">root</a><span>|</span><a href="#37401163">parent</a><span>|</span><a href="#37402784">next</a><span>|</span><label class="collapse" for="c-37401357">[-]</label><label class="expand" for="c-37401357">[1 more]</label></div><br/><div class="children"><div class="content">But if you have say 50bn weights, and you run backprop, you are going to update most of the weights (except the dropout ones, but which ones drop out changes on every token I think). This means you need 50bn deltas. It might compress, but if you do then you need extra compute to do that.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>