<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1694595675911" as="style"/><link rel="stylesheet" href="styles.css?v=1694595675911"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://thepalindrome.org/p/how-large-that-number-in-the-law">How large is that number in the Law of Large Numbers?</a> <span class="domain">(<a href="https://thepalindrome.org">thepalindrome.org</a>)</span></div><div class="subtext"><span>sebg</span> | <span>65 comments</span></div><br/><div><div id="37481525" class="c"><input type="checkbox" id="c-37481525" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#37484519">next</a><span>|</span><label class="collapse" for="c-37481525">[-]</label><label class="expand" for="c-37481525">[30 more]</label></div><br/><div class="children"><div class="content">My statistics class at high school level taught the following:<p>The number of samples you need is very difficult to calculate correctly, requiring deep analysis of standard deviations and variances.<p>But surprisingly, you can simply know you&#x27;ve reached large number status when over 10 items exist in each category.<p>---------<p>Ex: when doing a heads vs tails coin flip experiment, you likely have a large number once you have over 10 heads and over 10 tails. No matter how biased the coin is.<p>Or in this &#x27;Lotto ticket&#x27; example, you have a large number of samples after gathering enough data to find over 10 Jackpot winners.</div><br/><div id="37482264" class="c"><input type="checkbox" id="c-37482264" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#37481525">parent</a><span>|</span><a href="#37488676">next</a><span>|</span><label class="collapse" for="c-37482264">[-]</label><label class="expand" for="c-37482264">[4 more]</label></div><br/><div class="children"><div class="content">Very cool rule.<p>I think you can justify it by approximating each category as an independent Poisson distribution. Then for each such processes the variance equals the mean. So once you have 10 successes in a bin, you have evidence of a probably good estimate for the arrival rate of that category. The book &quot;The Probabilistic Method&quot; calls a related idea &quot;the Poisson paradigm.&quot;<p>(10 a nice round number where the standard deviation is below the mean)</div><br/><div id="37488268" class="c"><input type="checkbox" id="c-37488268" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37482264">parent</a><span>|</span><a href="#37488676">next</a><span>|</span><label class="collapse" for="c-37488268">[-]</label><label class="expand" for="c-37488268">[3 more]</label></div><br/><div class="children"><div class="content">Small proviso: this is only true for a reasonable number of categories (or you run into repeated experiment problems).</div><br/><div id="37491217" class="c"><input type="checkbox" id="c-37491217" checked=""/><div class="controls bullet"><span class="by">hgsgm</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37488268">parent</a><span>|</span><a href="#37488676">next</a><span>|</span><label class="collapse" for="c-37491217">[-]</label><label class="expand" for="c-37491217">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s a reasonable number of categories? 10?</div><br/><div id="37491534" class="c"><input type="checkbox" id="c-37491534" checked=""/><div class="controls bullet"><span class="by">dredmorbius</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37491217">parent</a><span>|</span><a href="#37488676">next</a><span>|</span><label class="collapse" for="c-37491534">[-]</label><label class="expand" for="c-37491534">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s defined by the phenomenon you&#x27;re investigating.<p>In the case of six-sided dice, there are precisely six categories, ideally with even odds of occurrence.  With the lottery jackpot given, there are eight categories, with highly asymmetric probabilities and values.<p>In real-world cases, you might be trying to distinguish two cases (treatment and control in a medical experiment), between multiple particles or isotopes (say, with physics or chemistry), amongst different political divisions (countries, states or provinces, counties, cities, or other), between political parties or candidates (which raises interesting questions over which and&#x2F;or how many to include in consideration, in turn dependent on voting procedures, overall popularity, and impacts of non-winning candidates or parties on others), on multiple products, or on different behavioural characteristics in some domain (e.g., highly-active, occasionally-active, and lurking participants in online fora).<p>There are times when categories are well and unambiguously defined.  Others in which where you choose to draw divisions (say, in generational groups, or wealth or income brackets) is highly arbitrary.  Even where there are a large number of potential categories, choosing some limited number for specific analysis (2, 3, 5, 10, etc.) and lumping the remaining into &quot;other&quot; may provide clearer insights and fewer distractions than choosing a large number of divisions.[1]  In other cases, a very small number of <i>individuals</i> may account for an overwhelming majority of <i>activity</i> or <i>outcome</i>.  I&#x27;d strongly argue that in this case, the analysis might be somewhat poorly focused, and that activities and outcomes rather than individuals are of greater interest.[2]<p>What&#x27;s key is to <i>match your sampling and sample sizes to the phenomenon being studied</i>.<p>________________________________<p>Notes:<p>1. Power law distribution &#x2F; Zipf functions often mean that a very small number of participants has highly disproportionate impact or significance.<p>2. This is often the flip side of power law distributions.  If we look at all book titles, there are a huge number of individual items to consider; there are roughly 300k annual English-language &quot;traditional&quot; publications, and over 1 million &quot;nontraditional&quot; (self-published, or publish-on-demand) titles.  But if your focus is instead titles by percentage of revenue or number of sales, a top-n analysis (5, 10, 20, etc.) often captures much of the activity, frequently well over half.  This is typical of any informational good:  music, cinema, blogs, social media posts, etc.</div><br/></div></div></div></div></div></div></div></div><div id="37488676" class="c"><input type="checkbox" id="c-37488676" checked=""/><div class="controls bullet"><span class="by">NelsonMinar</span><span>|</span><a href="#37481525">parent</a><span>|</span><a href="#37482264">prev</a><span>|</span><a href="#37492255">next</a><span>|</span><label class="collapse" for="c-37488676">[-]</label><label class="expand" for="c-37488676">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a neat rule of thumb; is there a simple statistical argument for why 10 is the (not very large) number?</div><br/></div></div><div id="37492255" class="c"><input type="checkbox" id="c-37492255" checked=""/><div class="controls bullet"><span class="by">fiddlerwoaroof</span><span>|</span><a href="#37481525">parent</a><span>|</span><a href="#37488676">prev</a><span>|</span><a href="#37483188">next</a><span>|</span><label class="collapse" for="c-37492255">[-]</label><label class="expand" for="c-37492255">[1 more]</label></div><br/><div class="children"><div class="content">Does this work the other way? I.e. “you have enough buckets if adding one more puts the number of samples in the smallest bucket below 10?”</div><br/></div></div><div id="37483188" class="c"><input type="checkbox" id="c-37483188" checked=""/><div class="controls bullet"><span class="by">tgv</span><span>|</span><a href="#37481525">parent</a><span>|</span><a href="#37492255">prev</a><span>|</span><a href="#37481694">next</a><span>|</span><label class="collapse" for="c-37483188">[-]</label><label class="expand" for="c-37483188">[21 more]</label></div><br/><div class="children"><div class="content">For heads or tails, that leaves a very large margin. In approx. 1 in 20 trials, you&#x27;ll end up with a 10-20 split.</div><br/><div id="37483371" class="c"><input type="checkbox" id="c-37483371" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37483188">parent</a><span>|</span><a href="#37481694">next</a><span>|</span><label class="collapse" for="c-37483371">[-]</label><label class="expand" for="c-37483371">[20 more]</label></div><br/><div class="children"><div class="content">Yeah, 95% confidence ratio (or approximately two standard deviations) is pretty standard with regards to statistical tests.<p>You gotta draw the line somewhere. At high-school statistics level, its basically universally drawn at the 95% confidence level. If you wanna draw new lines elsewhere, you gotta make new rules yourself and recalculate all the rules of thumb.</div><br/><div id="37484566" class="c"><input type="checkbox" id="c-37484566" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37483371">parent</a><span>|</span><a href="#37481694">next</a><span>|</span><label class="collapse" for="c-37484566">[-]</label><label class="expand" for="c-37484566">[19 more]</label></div><br/><div class="children"><div class="content">I remember my high school AP Psychology teacher mocking p=0.05 as practically meaningless. In retrospect it&#x27;s funny for a psychologist to say that, but I guess it was because he was from the more empirically minded behaviorist cognitive school and from time to time they have done actual rigorous experiments[1] (in rodents).<p>[1] For example as described by Feynman in Cargo Cult Science.</div><br/><div id="37486278" class="c"><input type="checkbox" id="c-37486278" checked=""/><div class="controls bullet"><span class="by">lisper</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37484566">parent</a><span>|</span><a href="#37487432">next</a><span>|</span><label class="collapse" for="c-37486278">[-]</label><label class="expand" for="c-37486278">[17 more]</label></div><br/><div class="children"><div class="content">The problem is two-fold:<p>1.  p=0.05 means that one result in 20 is going to be the result of chance.<p>2.  It&#x27;s generally pretty easy (especially in psychology) to do 20 experiments, cherry-pick -- and publish! -- the p=0.05 result, and throw away the others.<p>The result is that <i>published</i> p=0.05 results are much <i>more</i> likely than 1 in 20 to be the result of chance.</div><br/><div id="37489352" class="c"><input type="checkbox" id="c-37489352" checked=""/><div class="controls bullet"><span class="by">Viliam1234</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37486278">parent</a><span>|</span><a href="#37486624">next</a><span>|</span><label class="collapse" for="c-37489352">[-]</label><label class="expand" for="c-37489352">[9 more]</label></div><br/><div class="children"><div class="content">&gt; p=0.05 means that one result in 20 is going to be the result of chance.<p>You made the same mistake most people make here: you turned the arrow of the implication. It is not &quot;successful experiment implies chance (probability 5%)&quot; but &quot;chance implies successful experiment (probability 5%)&quot;.<p>What does that mean in practice? Imagine a hypothetical scientist that is fundamentally confused about something important, so <i>all</i> hypotheses they generate are false. Yet, using p=0.05, 5% of those hypotheses will be &quot;confirmed experimentally&quot;. In that case, it is not 5% of the &quot;experimentally confirmed&quot; hypotheses that are wrong -- it is full 100%. Even without any cherry-picking.<p>The problem is not that p=0.05 is too high. The problem is, it doesn&#x27;t actually mean what most people believe it means.</div><br/><div id="37489627" class="c"><input type="checkbox" id="c-37489627" checked=""/><div class="controls bullet"><span class="by">lisper</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37489352">parent</a><span>|</span><a href="#37490794">next</a><span>|</span><label class="collapse" for="c-37489627">[-]</label><label class="expand" for="c-37489627">[6 more]</label></div><br/><div class="children"><div class="content">I think we&#x27;re actually in violent agreement here, but I just wasn&#x27;t precise enough.  Let me try again:<p><pre><code>    p=0.05 means that one POSITIVE result in 20 is going to be the result of chance and not causality
</code></pre>
In other words: if I have some kind of intervention or treatment, and that intervention or treatment produces some result in a test group relative to a control group with p=0.05, then the odds of getting that result simply by chance and not because the treatment or intervention actually had an effect are 5%.<p>The practical effect of this is that there are two different ways of getting a p=0.05 result:<p>1.  Find a treatment or intervention that actually works or<p>2.  Test ~20 different (useless) interventions.  Or test one useless intervention ~20 times.<p>A single p=0.05 result in isolation is useless because there is no way to know which of the two methods produced it.<p>This is why replication is so important.  The odds of getting a p=0.05 result by chance is 5%.  But the odds of getting TWO of them in sequential trials is 0.25%, and the odds of a positive result being the result of pure chance decrease exponentially with each subsequent replication.</div><br/><div id="37490560" class="c"><input type="checkbox" id="c-37490560" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37489627">parent</a><span>|</span><a href="#37490794">next</a><span>|</span><label class="collapse" for="c-37490560">[-]</label><label class="expand" for="c-37490560">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Let me try again:<p>&gt; p=0.05 means that one POSITIVE result in 20 is going to be the result of chance and not causality<p>No, you still didn&#x27;t get it. In the example above, a full 100% of positive results, 20 out of every 20, are the result of chance and not causality.<p>Your followup discussion is better, but your statement at the top doesn&#x27;t work.<p>(Note also that there is an interaction between p-threshold and sample size which guarantees that, if you&#x27;re investigating an effect that your sample size is not large enough to detect, any statistically significant result that you get will be several times stronger than the actual effect. They&#x27;re also quite likely to have the wrong sign.)</div><br/><div id="37491156" class="c"><input type="checkbox" id="c-37491156" checked=""/><div class="controls bullet"><span class="by">lisper</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37490560">parent</a><span>|</span><a href="#37490794">next</a><span>|</span><label class="collapse" for="c-37491156">[-]</label><label class="expand" for="c-37491156">[4 more]</label></div><br/><div class="children"><div class="content">&gt; No, you still didn&#x27;t get it. In the example above, a full 100% of positive results, 20 out of every 20, are the result of chance and not causality.<p>Yep, you&#x27;re right.  I do think I understand this, but rendering it into words is turning out to be surprisingly challenging.<p>Let me try this one more time: p=0.05 means that there is a 5% chance that any one particular positive result is due to chance.  If you test a false hypothesis repeatedly, or test multiple false hypotheses, then 5% of the time you will get false positives (at p=0.05).<p>However...<p>&gt; Imagine a hypothetical scientist that is fundamentally confused about something important, so all hypotheses they generate are false. Yet, using p=0.05, 5% of those hypotheses will be &quot;confirmed experimentally&quot;.  In that case, it is not 5% of the &quot;experimentally confirmed&quot; hypotheses that are wrong -- it is full 100%.<p>This is not wrong, but it&#x27;s a little misleading because you are <i>presuming</i> that all of the hypotheses being tested are false.  If we&#x27;re testing a hypothesis it&#x27;s generally because we don&#x27;t know whether or not it&#x27;s true; we&#x27;re trying to find out.  That&#x27;s why it&#x27;s important to think of a positive result not as &quot;confirmed experimentally&quot; but rather as &quot;not ruled out by this particular experimental result&quot;.  It is only after failing to rule something out by <i>multiple</i> experiments that we can start to call it &quot;confirmed&quot;.  And nothing is ever 100% confirmed -- at best it is &quot;not ruled out by the evidence so far&quot;.</div><br/><div id="37493366" class="c"><input type="checkbox" id="c-37493366" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37491156">parent</a><span>|</span><a href="#37491285">next</a><span>|</span><label class="collapse" for="c-37493366">[-]</label><label class="expand" for="c-37493366">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I do think I understand this, but rendering it into words is turning out to be surprisingly challenging.<p>A p-value of .05 means that, under the assumption that the null hypothesis you specified is true, you just observed a result which lies at the 5th percentile of the outcome space, sorted along some metric (usually &quot;extremity of outcome&quot;). That is to say, out of all possible outcomes, only 5% of them are as &quot;extreme&quot; as, or more &quot;extreme&quot; than, the outcome you observed.<p>It doesn&#x27;t tell you anything about the odds that any result is due to chance. It tells you how often the null hypothesis gives you a result that is &quot;similar&quot;, by some definition, to the result you observed.</div><br/></div></div><div id="37491285" class="c"><input type="checkbox" id="c-37491285" checked=""/><div class="controls bullet"><span class="by">hgsgm</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37491156">parent</a><span>|</span><a href="#37493366">prev</a><span>|</span><a href="#37490794">next</a><span>|</span><label class="collapse" for="c-37491285">[-]</label><label class="expand" for="c-37491285">[2 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t simply ignore the base rate, even if you don&#x27;t know it.<p>In a purely random world, 5% of experiments are false positives, at p=0.05. None are true positives.<p>In a well ordered world with brilliant hypotheses, there are no false positives.<p>If more than 5% of experiments show positive results at p=0.05, some of them are probably true, so you can try to replicate them with lower p.<p>p=0.05 is a filter for &quot;worth trying to replicate&quot; (but even that is modulated by cost of replication vs value of result).<p>The crisis in science is largely that people confuse &quot;publishable&quot; with &quot;probably true&quot;. Anything &quot;probably better then random guessing&quot; is publishable to help other researchers, but that doesn&#x27;t mean it&#x27;s probably true.</div><br/><div id="37491370" class="c"><input type="checkbox" id="c-37491370" checked=""/><div class="controls bullet"><span class="by">lisper</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37491285">parent</a><span>|</span><a href="#37490794">next</a><span>|</span><label class="collapse" for="c-37491370">[-]</label><label class="expand" for="c-37491370">[1 more]</label></div><br/><div class="children"><div class="content">&gt; p=0.05 is a filter for &quot;worth trying to replicate&quot;<p>Yes, I think that is an excellent way to put it.<p>&gt; The crisis in science is largely that people confuse &quot;publishable&quot; with &quot;probably true&quot;.<p>I would put it slightly differently: people conflate &quot;published in a top-tier peer-reviewed journal&quot; with &quot;true beyond reasonable dispute&quot;.  They also conflate &quot;not published in a top-tier peer-reviewed journal&quot; with &quot;almost certainly false.&quot;<p>But I think we&#x27;re in substantial agreement here.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37490794" class="c"><input type="checkbox" id="c-37490794" checked=""/><div class="controls bullet"><span class="by">majormajor</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37489352">parent</a><span>|</span><a href="#37489627">prev</a><span>|</span><a href="#37486624">next</a><span>|</span><label class="collapse" for="c-37490794">[-]</label><label class="expand" for="c-37490794">[2 more]</label></div><br/><div class="children"><div class="content">&gt; What does that mean in practice? Imagine a hypothetical scientist that is fundamentally confused about something important, so all hypotheses they generate are false. Yet, using p=0.05, 5% of those hypotheses will be &quot;confirmed experimentally&quot;. In that case, it is not 5% of the &quot;experimentally confirmed&quot; hypotheses that are wrong -- it is full 100%. Even without any cherry-picking.<p>Well, that&#x27;s example is also introducing dependence, which is a tricky thing of course whenever we talk about chance and stats.<p>But there&#x27;s also another issue - a statement like &quot;5% of positive published results are by chance since we have a p&lt;=0.05 standard&quot; treats every set of results as if p=0.05, whereas some of them are considerably lower anyway. Though the point of bad actors cherry-picking to screw up the data also comes into play here.<p>(And of course, fully independent things in life are much harder to find than one might think at first.)</div><br/><div id="37491485" class="c"><input type="checkbox" id="c-37491485" checked=""/><div class="controls bullet"><span class="by">stkdump</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37490794">parent</a><span>|</span><a href="#37486624">next</a><span>|</span><label class="collapse" for="c-37491485">[-]</label><label class="expand" for="c-37491485">[1 more]</label></div><br/><div class="children"><div class="content">I agree that the point about the &#x27;confused scientist&#x27; is important, even if that itself is not stated clearly enough. Here is my own reading:<p>Imagine that a scientist is making experiments of the form: Does observable variable A correlate with observable variable B? Now imagine that there are billions of observable variables and almost all of them are not correlated. And imagine that there is no better way to come up with plausible correlations to test than randomly picking variables. Then it will take a very long time and a very large number of experiments to find a pair that is truly correlated. It will be inevitable that most positive results are bogus.</div><br/></div></div></div></div></div></div><div id="37486624" class="c"><input type="checkbox" id="c-37486624" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37486278">parent</a><span>|</span><a href="#37489352">prev</a><span>|</span><a href="#37487432">next</a><span>|</span><label class="collapse" for="c-37486624">[-]</label><label class="expand" for="c-37486624">[7 more]</label></div><br/><div class="children"><div class="content">So run a meta-study upon the results published by a set of authors and double-check to make sure that their results are normally distributed across the p-values associated with their studies.<p>These problems are solved problems in the scientific community. Just announce that regular meta-studies will be done, expectations for authors to be normally distributed is published, and publicly show off the meta-study.<p>-------------<p>In any case, the discussion point you&#x27;re making is well beyond the high-school level needed for a general education. If someone needs to run their own experiment (A&#x2F;B testing upon their website) and cannot afford a proper set of tests&#x2F;statistics, they should instead rely upon high-school level heuristics to design their personal studies.<p>This isn&#x27;t a level of study about analyzing other people&#x27;s results and finding flaws in other people&#x27;s (possibly maliciously seeded) results. This is a heuristic about how to run your own experiments and how to prove something to yourself at a 95% confidence level. If you want to get published in the scientific community, the level of rigor is much higher of course, but no one tries to publish a scientific paper on just a high school education (which is where I was aiming my original comment at).</div><br/><div id="37487499" class="c"><input type="checkbox" id="c-37487499" checked=""/><div class="controls bullet"><span class="by">lisper</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37486624">parent</a><span>|</span><a href="#37490616">next</a><span>|</span><label class="collapse" for="c-37487499">[-]</label><label class="expand" for="c-37487499">[3 more]</label></div><br/><div class="children"><div class="content">First, I was specifically responding to this:<p>&gt; I remember my high school AP Psychology teacher mocking p=0.05 as practically meaningless.<p>and trying to explain why the OP&#x27;s teacher was probably right.<p>Second:<p>&gt; So run a meta-study upon the results published by a set of authors and double-check to make sure that their results are normally distributed across the p-values associated with their studies.<p>That won&#x27;t work, especially if you only run the meta-study on published results because it is all but impossible to get negative results published.  Authors don&#x27;t need to cherry-pick, the peer-review system does it for them.<p>&gt; These problems are solved problems in the scientific community.<p>No, they aren&#x27;t.  These are social and political problems, not mathematical ones.  And the scientific community is pretty bad at solving those.<p>&gt; the discussion point you&#x27;re making is well beyond the high-school level needed for a general education<p>I strongly disagree.  I think everyone needs to understand this so they can approach scientific claims with an appropriate level of skepticism.  Understanding how the sausage is made is essential to understanding science.<p>And BTW, I am not some crazy anti-vaxxer climate-change denialist flat-earther.  I was an academic researcher for 15 years -- in a STEM field, not psychology, and even <i>that</i> was sufficiently screwed up to make me change my career.  I have advocated for science and the scientific method for decades.  It&#x27;s not science that&#x27;s broken, it&#x27;s the academic peer-review system, which is essentially unchanged since it was invented in the 19th century.  <i>That</i> is what needs to change.  And that has nothing to do with math and everything to do with politics and economics.</div><br/><div id="37492715" class="c"><input type="checkbox" id="c-37492715" checked=""/><div class="controls bullet"><span class="by">trashtester</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37487499">parent</a><span>|</span><a href="#37490616">next</a><span>|</span><label class="collapse" for="c-37492715">[-]</label><label class="expand" for="c-37492715">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s not science that&#x27;s broken, it&#x27;s the academic peer-review system, which is essentially unchanged since it was invented in the 19th century.<p>In my experience, it&#x27;s not even this. Rather, it is that outside of STEM, very, very few people truly understand hypothesis testing.<p>At least in my experience, even basic concepts, as &quot;falsify the null-hypothesis&quot; is surprisingly hard, even with presumably intelligent people, such as MD&#x27;s in PHd programmes.<p>They will still tend to believe that a &quot;significant&quot; result is proof of an effect, and often even believe it proves that the effect is causal with the direction they prefer.<p>At some point, stats just becomes a set of arcane conjurations for an entire field. At that point, the field as a whole tends to lose their ability to follow the scientific method and turns into something resembling a cult or clergy.</div><br/><div id="37492769" class="c"><input type="checkbox" id="c-37492769" checked=""/><div class="controls bullet"><span class="by">lisper</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37492715">parent</a><span>|</span><a href="#37490616">next</a><span>|</span><label class="collapse" for="c-37492769">[-]</label><label class="expand" for="c-37492769">[1 more]</label></div><br/><div class="children"><div class="content">FWIW, I got through a Ph.D. program in CS without ever having to take a stats course.  I took probability theory, which is related, but not the same thing.  I had to figure out stats on my own.  So yes, I think you&#x27;re absolutely right, but it&#x27;s not just &quot;outside of STEM&quot; -- sometimes it&#x27;s inside of STEM too.</div><br/></div></div></div></div></div></div><div id="37490616" class="c"><input type="checkbox" id="c-37490616" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37486624">parent</a><span>|</span><a href="#37487499">prev</a><span>|</span><a href="#37487118">next</a><span>|</span><label class="collapse" for="c-37490616">[-]</label><label class="expand" for="c-37490616">[1 more]</label></div><br/><div class="children"><div class="content">&gt; and double-check to make sure that their results are normally distributed across the p-values associated with their studies<p>What is the distribution of a set of results over a set of p-values?<p>If you mean that you should check to make sure that the p-values themselves are normally distributed... wouldn&#x27;t that be wrong? Assuming all hypotheses are false, p-values should be uniformly distributed. Assuming some hypotheses can sometimes be true, there&#x27;s not a lot you can say about the appropriate distribution of p-values - it would depend on how often hypotheses are correct, and how strong the effects are.</div><br/></div></div><div id="37487118" class="c"><input type="checkbox" id="c-37487118" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37486624">parent</a><span>|</span><a href="#37490616">prev</a><span>|</span><a href="#37487432">next</a><span>|</span><label class="collapse" for="c-37487118">[-]</label><label class="expand" for="c-37487118">[2 more]</label></div><br/><div class="children"><div class="content">There’s a professor of Human Evolutionary Biology at Harvard who only has a high school diploma[1]. Needless to say he’s been published and cited many times over.<p>[1] <a href="https:&#x2F;&#x2F;theconversation.com&#x2F;profiles&#x2F;louis-liebenberg-1226800" rel="nofollow noreferrer">https:&#x2F;&#x2F;theconversation.com&#x2F;profiles&#x2F;louis-liebenberg-122680...</a></div><br/><div id="37489402" class="c"><input type="checkbox" id="c-37489402" checked=""/><div class="controls bullet"><span class="by">withinboredom</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37487118">parent</a><span>|</span><a href="#37487432">next</a><span>|</span><label class="collapse" for="c-37489402">[-]</label><label class="expand" for="c-37489402">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know whether you&#x27;re mocking them or being supportive of them or just stating a fact. Either way, education level has no bearing on subject knowledge. I know more about how computers, compilers, and software algorithms work than most post-docs and professors that I&#x27;ve run into in those subjects.<p>Am I smarter than them? Nope. Do I know as many fancy big words as them? Nope. Do I care about results and communicating complex topics to normal people? Yep. Do I care more about making the company money than chasing some bug-bear to go on my resume? Yep.<p>I fucking hate school and have no desire to ever go back. I can&#x27;t put up with the bullshit, so I dropped out; I just never stopped studying and I don&#x27;t need a piece of paper to affirm that fact.</div><br/></div></div></div></div></div></div></div></div><div id="37487432" class="c"><input type="checkbox" id="c-37487432" checked=""/><div class="controls bullet"><span class="by">tgv</span><span>|</span><a href="#37481525">root</a><span>|</span><a href="#37484566">parent</a><span>|</span><a href="#37486278">prev</a><span>|</span><a href="#37481694">next</a><span>|</span><label class="collapse" for="c-37487432">[-]</label><label class="expand" for="c-37487432">[1 more]</label></div><br/><div class="children"><div class="content">The observation above is simply true. If you toss a coin 30 times, there&#x27;s about a 5% chance that you&#x27;ll end up with 10-20 ratio or one more extreme.<p>NHST testing inverts the probability logic, makes the 5% holy, and skims over the high probability of finding something that is not equal to a specific value. That procedure is then used for theory confirmation, while it was (in another form) meant for falsification. Everything is wrong about it, even if the experimental method is flawless. Hence the reproducibility crisis.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37484519" class="c"><input type="checkbox" id="c-37484519" checked=""/><div class="controls bullet"><span class="by">nathell</span><span>|</span><a href="#37481525">prev</a><span>|</span><a href="#37485661">next</a><span>|</span><label class="collapse" for="c-37484519">[-]</label><label class="expand" for="c-37484519">[6 more]</label></div><br/><div class="children"><div class="content">Tangential: syntax-highlighting math! This is the first time I’ve seen it. Not yet sure what I think about it, but I can definitely see the allure.</div><br/><div id="37484999" class="c"><input type="checkbox" id="c-37484999" checked=""/><div class="controls bullet"><span class="by">tetha</span><span>|</span><a href="#37484519">parent</a><span>|</span><a href="#37485574">next</a><span>|</span><label class="collapse" for="c-37484999">[-]</label><label class="expand" for="c-37484999">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I like it. I used this as a tutor in more finicky exercises when it becomes really important to keep 2-3 very similar, but different things apart. It takes a bit of dexterity, but you can switch fluently between 3 different whiteboard markers held in one hand while writing, haha.<p>I am kind of wondering if a semantic highlighting makes sense as well. You often end up with some implicit assignment of lowercase latin, uppercase latin, lowercase greek letters and such for certain meanings. Kinematic - xyzt for position in time, T_i(I_i) for the quaternion or transformation representing a certain joint of a robot.</div><br/></div></div><div id="37485574" class="c"><input type="checkbox" id="c-37485574" checked=""/><div class="controls bullet"><span class="by">rendaw</span><span>|</span><a href="#37484519">parent</a><span>|</span><a href="#37484999">prev</a><span>|</span><a href="#37484735">next</a><span>|</span><label class="collapse" for="c-37485574">[-]</label><label class="expand" for="c-37485574">[2 more]</label></div><br/><div class="children"><div class="content">Pedant-man on the scene: this is just highlighting since the highlighting isn&#x27;t derived from syntax.</div><br/><div id="37490049" class="c"><input type="checkbox" id="c-37490049" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#37484519">root</a><span>|</span><a href="#37485574">parent</a><span>|</span><a href="#37484735">next</a><span>|</span><label class="collapse" for="c-37490049">[-]</label><label class="expand" for="c-37490049">[1 more]</label></div><br/><div class="children"><div class="content">Which makes me wonder what <i>that</i> would look like and would it be helpful?<p>But there&#x27;s already such complex and varied typography in math I wonder if it would be kind of redundant. E.g. you don&#x27;t need matching parentheses to be colored when they already come in different sets of matching heights.</div><br/></div></div></div></div><div id="37484735" class="c"><input type="checkbox" id="c-37484735" checked=""/><div class="controls bullet"><span class="by">Tachyooon</span><span>|</span><a href="#37484519">parent</a><span>|</span><a href="#37485574">prev</a><span>|</span><a href="#37490995">next</a><span>|</span><label class="collapse" for="c-37484735">[-]</label><label class="expand" for="c-37484735">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easy on the eyes and it can make reading lots of equations less awkward if done correctly. I remember finding out this was possible while I was working on an assignment in Latex - it looked amazing.<p>It takes a little bit of work to colour in equations but I hope more people start doing it (including me, I&#x27;d forgotten about it for a while)</div><br/></div></div><div id="37490995" class="c"><input type="checkbox" id="c-37490995" checked=""/><div class="controls bullet"><span class="by">andrewprock</span><span>|</span><a href="#37484519">parent</a><span>|</span><a href="#37484735">prev</a><span>|</span><a href="#37485661">next</a><span>|</span><label class="collapse" for="c-37490995">[-]</label><label class="expand" for="c-37490995">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been around for a long time (centuries?). In most textbooks, you&#x27;ll get different semantics for italic and bold faces. Modern textbooks with color printing often use color in semantic ways.</div><br/></div></div></div></div><div id="37485661" class="c"><input type="checkbox" id="c-37485661" checked=""/><div class="controls bullet"><span class="by">gloryless</span><span>|</span><a href="#37484519">prev</a><span>|</span><a href="#37486770">next</a><span>|</span><label class="collapse" for="c-37485661">[-]</label><label class="expand" for="c-37485661">[2 more]</label></div><br/><div class="children"><div class="content">This kind of intuition is why a high school level statistics or probability class seems so so valuable. I know not everyone will use the math per se, but the concepts apply to everyday life and are really hard to just grasp without having been taught it at some point.</div><br/><div id="37486575" class="c"><input type="checkbox" id="c-37486575" checked=""/><div class="controls bullet"><span class="by">zodmaner</span><span>|</span><a href="#37485661">parent</a><span>|</span><a href="#37486770">next</a><span>|</span><label class="collapse" for="c-37486575">[-]</label><label class="expand" for="c-37486575">[1 more]</label></div><br/><div class="children"><div class="content">The sad thing is, having a mandatory high school level statistics &amp; probability class alone is not enough, you&#x27;ll also need a good curriculum and a competent teacher to go along with it. Otherwise, it wouldn&#x27;t work: a bad curriculum taught badly by a unmotivated or unqualified teacher will almost always fail to teach the intuition, or, even worse, alienates students from the materials.</div><br/></div></div></div></div><div id="37486770" class="c"><input type="checkbox" id="c-37486770" checked=""/><div class="controls bullet"><span class="by">alexb_</span><span>|</span><a href="#37485661">prev</a><span>|</span><a href="#37484010">next</a><span>|</span><label class="collapse" for="c-37486770">[-]</label><label class="expand" for="c-37486770">[6 more]</label></div><br/><div class="children"><div class="content">If you had a gambling game that was simply &quot;heads or tails, even money&quot;, you would expect over a Large Number of trials that you would get 0. But once you observe exactly one trial, the expected value because +1 or -1 unit. We know this is always going to happen one way or the other. Why then, does the bell curve of &quot;expected value&quot; for this game not have two peaks, at 1 and -1? Why does it peak at 0 instead?<p>What I&#x27;m asking about, I know I&#x27;m wrong about - I just want to know how I can derive that for myself.</div><br/><div id="37488818" class="c"><input type="checkbox" id="c-37488818" checked=""/><div class="controls bullet"><span class="by">munchbunny</span><span>|</span><a href="#37486770">parent</a><span>|</span><a href="#37486899">next</a><span>|</span><label class="collapse" for="c-37488818">[-]</label><label class="expand" for="c-37488818">[1 more]</label></div><br/><div class="children"><div class="content">The intuitive explanation is that the effect of a single sample on the average diminishes as you take more samples. So, hand-waving a bit, let&#x27;s assume it&#x27;s true that over a large number of trials you would expect the average to converge to 0. You just tossed a coin and got heads, so you&#x27;re at +1. The average of (1 + 0*n)&#x2F;(n+1) still goes to 0 as n grows bigger and bigger.<p>That skips over the distinction between &quot;average&quot; and &quot;probability distribution&quot;, but those are nuances are probably better left for a proof of the central limit theorem.</div><br/></div></div><div id="37486899" class="c"><input type="checkbox" id="c-37486899" checked=""/><div class="controls bullet"><span class="by">ineptech</span><span>|</span><a href="#37486770">parent</a><span>|</span><a href="#37488818">prev</a><span>|</span><a href="#37484010">next</a><span>|</span><label class="collapse" for="c-37486899">[-]</label><label class="expand" for="c-37486899">[4 more]</label></div><br/><div class="children"><div class="content">&quot;The expected value of a random variable with a finite number of outcomes is a weighted average of all possible outcomes.&quot; -- <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Expected_value" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Expected_value</a></div><br/><div id="37486953" class="c"><input type="checkbox" id="c-37486953" checked=""/><div class="controls bullet"><span class="by">alexb_</span><span>|</span><a href="#37486770">root</a><span>|</span><a href="#37486899">parent</a><span>|</span><a href="#37484010">next</a><span>|</span><label class="collapse" for="c-37486953">[-]</label><label class="expand" for="c-37486953">[3 more]</label></div><br/><div class="children"><div class="content">That makes sense, I was always thinking of it as &quot;Given an infinite number of trials...&quot;</div><br/><div id="37491564" class="c"><input type="checkbox" id="c-37491564" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37486770">root</a><span>|</span><a href="#37486953">parent</a><span>|</span><a href="#37488499">next</a><span>|</span><label class="collapse" for="c-37491564">[-]</label><label class="expand" for="c-37491564">[1 more]</label></div><br/><div class="children"><div class="content">That would be the frequentist interpretation. A Bayesian would say that probability is to be interpreted in belief that an outcome will occur. Neither is really right or wrong, it depends on what you&#x27;re modeling.
 If we’re analyzing some kind of heavily repeated task (e.g., a sordid night of us glued to the blackjack table where we play a lot of hands or data transmission over a noisy cable), a frequentist interpretation might feel more sense. However if you’re talking about the probability of a candidate winning an election, you could take a Bayesian view where the probability asserts a confidence in an outcome. A radical frequentist would take umbrage with an event that only happens once.
However, I suppose, depending on your election rules and model (e.g., a direct democracy), you could interpret the election winner in a frequentist manner: the probability of winning is the rate at which people vote for the candidate. For a more complicated system I’m not sure the frequentist view is as easily justified.<p>However to answer your question more directly, the expected value is just another name for the average or mean of a random variable. In this case, the variable is your profit. Assume we’re betting a dollar per toss on coin flips and I win if it’s heads (everyone knows heads always wins, right?). The expected value is probability of heads * 1 - probability of tails * 1. If the coin is fair, the probabilities are the same so the expected value is zero.<p>Aside: sequences of random variables that are “fair bets” are called martingales and are incredibly useful. It’s a fair bet because, given all prior knowledge of the value of the variable thus far, the expected value of the next value you witness is the current value of the variable. You could imagine looking at a history of stock values. Given all that information, it’s a martingale (and thus a fair bet) if given that information your expected profit from investing is 0.</div><br/></div></div><div id="37488499" class="c"><input type="checkbox" id="c-37488499" checked=""/><div class="controls bullet"><span class="by">ineptech</span><span>|</span><a href="#37486770">root</a><span>|</span><a href="#37486953">parent</a><span>|</span><a href="#37491564">prev</a><span>|</span><a href="#37484010">next</a><span>|</span><label class="collapse" for="c-37488499">[-]</label><label class="expand" for="c-37488499">[1 more]</label></div><br/><div class="children"><div class="content">Whether&#x2F;when its better to think in terms of &quot;X has a 37% chance of happening in a single trial&quot; vs &quot;If you ran a lot of trials, X would happen in 37% of them&quot; is kind of a fraught topic that I can&#x27;t say much about, but you might find this interesting: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Probability_interpretations" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Probability_interpretations</a></div><br/></div></div></div></div></div></div></div></div><div id="37484010" class="c"><input type="checkbox" id="c-37484010" checked=""/><div class="controls bullet"><span class="by">derbOac</span><span>|</span><a href="#37486770">prev</a><span>|</span><a href="#37482273">next</a><span>|</span><label class="collapse" for="c-37484010">[-]</label><label class="expand" for="c-37484010">[1 more]</label></div><br/><div class="children"><div class="content">The biggest problem for real processes is knowing whether in fact x ~ i.i.d., with regard to time as well as individual observations.</div><br/></div></div><div id="37482273" class="c"><input type="checkbox" id="c-37482273" checked=""/><div class="controls bullet"><span class="by">yafbum</span><span>|</span><a href="#37484010">prev</a><span>|</span><a href="#37492743">next</a><span>|</span><label class="collapse" for="c-37482273">[-]</label><label class="expand" for="c-37482273">[9 more]</label></div><br/><div class="children"><div class="content">Stats class role of thumb: if you need to calculate the relative probability of two outcomes, you can get to within about 10% once you get 100 samples of each outcome (so, need more samples overall if the distribution is skewed).</div><br/><div id="37482469" class="c"><input type="checkbox" id="c-37482469" checked=""/><div class="controls bullet"><span class="by">gear54rus</span><span>|</span><a href="#37482273">parent</a><span>|</span><a href="#37492743">next</a><span>|</span><label class="collapse" for="c-37482469">[-]</label><label class="expand" for="c-37482469">[8 more]</label></div><br/><div class="children"><div class="content">Its interesting that even in this thread the 2 answers differ by an order of magnitude lol</div><br/><div id="37493930" class="c"><input type="checkbox" id="c-37493930" checked=""/><div class="controls bullet"><span class="by">Keirmot</span><span>|</span><a href="#37482273">root</a><span>|</span><a href="#37482469">parent</a><span>|</span><a href="#37483256">next</a><span>|</span><label class="collapse" for="c-37493930">[-]</label><label class="expand" for="c-37493930">[1 more]</label></div><br/><div class="children"><div class="content">I think both answers are referencing the Central Limit Theorem, that states [simplified] that once you get over 30 samples for each independent variable, you will get a normal distribution.</div><br/></div></div><div id="37483256" class="c"><input type="checkbox" id="c-37483256" checked=""/><div class="controls bullet"><span class="by">koolba</span><span>|</span><a href="#37482273">root</a><span>|</span><a href="#37482469">parent</a><span>|</span><a href="#37493930">prev</a><span>|</span><a href="#37482636">next</a><span>|</span><label class="collapse" for="c-37483256">[-]</label><label class="expand" for="c-37483256">[1 more]</label></div><br/><div class="children"><div class="content">Just apply it recursively. Let’s get 100 samples of comments suggesting the number of samples to use. Then average those.</div><br/></div></div><div id="37482636" class="c"><input type="checkbox" id="c-37482636" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#37482273">root</a><span>|</span><a href="#37482469">parent</a><span>|</span><a href="#37483256">prev</a><span>|</span><a href="#37482555">next</a><span>|</span><label class="collapse" for="c-37482636">[-]</label><label class="expand" for="c-37482636">[1 more]</label></div><br/><div class="children"><div class="content">FWIW, the threshold I learned was 20 in each bucket, so now you have 3 answers.</div><br/></div></div><div id="37482555" class="c"><input type="checkbox" id="c-37482555" checked=""/><div class="controls bullet"><span class="by">gipp</span><span>|</span><a href="#37482273">root</a><span>|</span><a href="#37482469">parent</a><span>|</span><a href="#37482636">prev</a><span>|</span><a href="#37492743">next</a><span>|</span><label class="collapse" for="c-37482555">[-]</label><label class="expand" for="c-37482555">[4 more]</label></div><br/><div class="children"><div class="content">Eh it&#x27;s really the same rule, just applying a different threshold.</div><br/><div id="37482812" class="c"><input type="checkbox" id="c-37482812" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#37482273">root</a><span>|</span><a href="#37482555">parent</a><span>|</span><a href="#37492743">next</a><span>|</span><label class="collapse" for="c-37482812">[-]</label><label class="expand" for="c-37482812">[3 more]</label></div><br/><div class="children"><div class="content">The problem is that the sensitivity to the number growth is supposed to be exponential. So if you need 100 samples for &quot;within 10% of the value&quot;, then 10 samples should give you almost completely random behavior.<p>In reality, it depends on your actual distribution, but the OP from this thread here is unreasonably conservative for something described as a &quot;rule of thumb&quot;. Almost always, if you have at least 10 of every category, you can already discover every interesting thing that a rule of thumb will allow. And you probably could go with less. But if you want precision, you can&#x27;t get it with rules of thumb.</div><br/><div id="37484111" class="c"><input type="checkbox" id="c-37484111" checked=""/><div class="controls bullet"><span class="by">CaptainNegative</span><span>|</span><a href="#37482273">root</a><span>|</span><a href="#37482812">parent</a><span>|</span><a href="#37492743">next</a><span>|</span><label class="collapse" for="c-37484111">[-]</label><label class="expand" for="c-37484111">[2 more]</label></div><br/><div class="children"><div class="content">The dependence on sample size is not exponential, it&#x27;s sublinear. The heuristic rate of convergence to keep in mind is the square root of the sample size, i.e. getting 10x more samples shrinks the margin of error (in a multiplicative sense) by sqrt(10) ≈ 3ish.<p>The exponential bit applies to the probability densities as a function of the bounds themselves, i.e. how likely you are to fall x units away from the mean typically decreases exponentially with (some polynomial in) x.<p>Of course, this is all assuming a whole bunch of standard conditions on the data you&#x27;re looking at (independence, identically distributed, bounded variance, etc.) and may not hold if these are violated.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37492743" class="c"><input type="checkbox" id="c-37492743" checked=""/><div class="controls bullet"><span class="by">blt</span><span>|</span><a href="#37482273">prev</a><span>|</span><a href="#37491664">next</a><span>|</span><label class="collapse" for="c-37492743">[-]</label><label class="expand" for="c-37492743">[1 more]</label></div><br/><div class="children"><div class="content">In learning theory there is focus on &quot;non-asymptotic&quot; results. Instead of only showing that our method converges on the right answer in the limit of infinite data, we must show <i>how fast</i> it converges.</div><br/></div></div><div id="37491664" class="c"><input type="checkbox" id="c-37491664" checked=""/><div class="controls bullet"><span class="by">pid-1</span><span>|</span><a href="#37492743">prev</a><span>|</span><a href="#37484100">next</a><span>|</span><label class="collapse" for="c-37491664">[-]</label><label class="expand" for="c-37491664">[1 more]</label></div><br/><div class="children"><div class="content">One way to gain intuition on why the LLN might work faster, slower or not at all is to write the mean equation in rescursive form (What&#x27;s the next expected value estimation, given a new sample and the current expected value estimation?).</div><br/></div></div><div id="37484100" class="c"><input type="checkbox" id="c-37484100" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#37491664">prev</a><span>|</span><a href="#37487471">next</a><span>|</span><label class="collapse" for="c-37484100">[-]</label><label class="expand" for="c-37484100">[2 more]</label></div><br/><div class="children"><div class="content">Curious how people are ‘applying’ the Law of Large Numbers in a way that needs this advice to be tacked on?<p>&gt; Always keep the speed of convergence in mind when applying the law of large numbers.<p>Any ‘application’ of the LLN basically amounts to replacing some probalistic number derived from a bunch of random samples with the <i>expected value</i> of that number… and tacking on ‘for sufficiently large <i>n</i>’ as a caveat to your subsequent conclusions.<p>Figuring out whether, in practical cases, you will have a sufficiently large <i>n</i> that the conclusion is valid is a necessary step in the analysis.</div><br/><div id="37485376" class="c"><input type="checkbox" id="c-37485376" checked=""/><div class="controls bullet"><span class="by">LudwigNagasena</span><span>|</span><a href="#37484100">parent</a><span>|</span><a href="#37487471">next</a><span>|</span><label class="collapse" for="c-37485376">[-]</label><label class="expand" for="c-37485376">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Figuring out whether, in practical cases, you will have a sufficiently large n that the conclusion is valid is a necessary step in the analysis.<p>The econometrics textbook I studied has more words “asymptotic” in it than there are pages. Oftentimes it’s impractical or even theoretically intractable to derive finite sample properties (and thus to answer when n is <i>really</i> large enough).</div><br/></div></div></div></div><div id="37487471" class="c"><input type="checkbox" id="c-37487471" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#37484100">prev</a><span>|</span><a href="#37482497">next</a><span>|</span><label class="collapse" for="c-37487471">[-]</label><label class="expand" for="c-37487471">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This means that on average, we’ll need a fifty million times larger sample for the sample average to be as close to the true average as in the case of dice rolls.<p>This is &quot;as close&quot; in an absolute sense, right?<p>If I take into account that the lottery value is 20x larger, and I&#x27;m targeting relative accuracy, then I need 2.5 million times as many samples?</div><br/></div></div><div id="37482497" class="c"><input type="checkbox" id="c-37482497" checked=""/><div class="controls bullet"><span class="by">wodenokoto</span><span>|</span><a href="#37487471">prev</a><span>|</span><a href="#37492807">next</a><span>|</span><label class="collapse" for="c-37482497">[-]</label><label class="expand" for="c-37482497">[1 more]</label></div><br/><div class="children"><div class="content">I really like how the plots and graphics look. Is it the library by 3blue1brown? (Is it manim, it’s called?)</div><br/></div></div><div id="37492807" class="c"><input type="checkbox" id="c-37492807" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#37482497">prev</a><span>|</span><a href="#37489378">next</a><span>|</span><label class="collapse" for="c-37492807">[-]</label><label class="expand" for="c-37492807">[1 more]</label></div><br/><div class="children"><div class="content">About three fifty.<p>(No, just joking. Actually 42 plus or minus.)</div><br/></div></div><div id="37489378" class="c"><input type="checkbox" id="c-37489378" checked=""/><div class="controls bullet"><span class="by">crdrost</span><span>|</span><a href="#37492807">prev</a><span>|</span><a href="#37488014">next</a><span>|</span><label class="collapse" for="c-37489378">[-]</label><label class="expand" for="c-37489378">[1 more]</label></div><br/><div class="children"><div class="content">If the blog author is reading, some notes for improvement:<p>- Your odds calculation is likely wrong. You assumed from the word &quot;odds&quot; that &quot;odds ratio&quot; was meant, (Odds=3 meaning &quot;odds 3:1 against&quot; corresponding to p=25%) but the phrase is &quot;approximate odds 1 in X&quot; (Odds=3 meaning &quot;odds of 1 in 3 to win&quot; meaning 33%) and recalculating results in the remarkably exact expected value of $80 which seems intentional?<p>- You phrase things in terms of variances, people will think more in terms of standard deviations. So 3.5 ± 1.7 vs $80 ± $12,526.<p>- Note that you try to make a direct comparison between those two but the two are in fact incomparable. The most direct comparison might be to subtract 1 from the die roll and multiply by $32, so that you have a 1&#x2F;6 chance of winning $0, 1&#x2F;6 of winning $32, ... 1&#x2F;6 of winning $160. So then we have $80 ± $55 vs $80 ± $12,526. Then instead of saying you&#x27;d need 50 <i>million</i> more lottery tickets you&#x27;d actually say you need about 50 <i>thousand</i> more. This is closer to the &quot;right ballpark&quot; where you can tell that the whole lottery is expected to sell about 10,200,000 tickets on a good day.<p>- But where an article like this should really go is, &quot;what are you using the numbers for?&quot;. In the case of the Texas lottery this is actually a strong constraint, they have to make sure that they make a &quot;profit&quot; (like, it&#x27;s not a real profit, it probably goes to schools or something) on most lotteries, so you&#x27;re actually trying to ensure that 5 sigma or so is less than the bias. So you&#x27;ve got a competition between $20 · <i>n</i> and 5 · $12,526 · √(<i>n</i>), or √(<i>n</i>) = 12526&#x2F;4, <i>n</i> = 9.8 million. So that&#x27;s what the Texas Lottery is targeting, right? So then we would calculate that the equivalent number of people that should play in the &quot;roll a die linear lottery&quot; we&#x27;ve constructed is 187, call it an even 200, if 200 people pay $100 for a lottery ticket on the linear lottery then we can pretty much always pay out even on a really bad day.<p>- So the 50,000x number that is actually correct is basically just saying that we can run a much smaller lottery, 50,000 times smaller, with that payoff structure. And there&#x27;s something nice about phrasing it this way.<p>- To really get &quot;law of large numbers&quot; we should <i>actually</i> probably be looking at how much these distributions deviate from Gaussian, rather than complaining that the Gaussian is too wide? You can account for a wide Gaussian in a number of ways. But probably we want to take the cube root of the 3d cumulant, for example, try to argue when it &quot;vanishes&quot;? Except given the symmetry the 3rd cumulant for the die is probably 0 so you might need to go out to the 4th cumulant for the die -- and this might give a better explanation for the die converging more rapidly in &quot;shape&quot; to the mean, it doesn&#x27;t just come close faster, it also becomes a Gaussian significantly faster because the payoff structure is symmetric about the mean.</div><br/></div></div><div id="37488014" class="c"><input type="checkbox" id="c-37488014" checked=""/><div class="controls bullet"><span class="by">causality0</span><span>|</span><a href="#37489378">prev</a><span>|</span><a href="#37482501">next</a><span>|</span><label class="collapse" for="c-37488014">[-]</label><label class="expand" for="c-37488014">[1 more]</label></div><br/><div class="children"><div class="content">Am I the only one unreasonably annoyed that his graphs don&#x27;t match the description of his rolls?</div><br/></div></div></div></div></div></div></div></body></html>