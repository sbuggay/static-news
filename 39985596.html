<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712739667824" as="style"/><link rel="stylesheet" href="styles.css?v=1712739667824"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://aider.chat/2024/04/09/gpt-4-turbo.html">GPT-4 Turbo with Vision is a step backwards for coding</a> <span class="domain">(<a href="https://aider.chat">aider.chat</a>)</span></div><div class="subtext"><span>anotherpaulg</span> | <span>37 comments</span></div><br/><div><div id="39987614" class="c"><input type="checkbox" id="c-39987614" checked=""/><div class="controls bullet"><span class="by">tedsanders</span><span>|</span><a href="#39988433">next</a><span>|</span><label class="collapse" for="c-39987614">[-]</label><label class="expand" for="c-39987614">[3 more]</label></div><br/><div class="children"><div class="content">Interestingly, GPT-4 Turbo with Vision is at the top of the LiveCodeBench Leaderboard: <a href="https:&#x2F;&#x2F;livecodebench.github.io&#x2F;leaderboard.html" rel="nofollow">https:&#x2F;&#x2F;livecodebench.github.io&#x2F;leaderboard.html</a><p>(GPT-4 Turbo with Vision has a knowledge cutoff of Dec 2023, so filter to Jan 2024+ to minimize the chance of contamination.)<p>In general, my take is that each model has its own personality, which can cause it to do better or worse on different sorts of tasks. From evaluating many LLMs, I&#x27;ve found that it&#x27;s almost never the case that one model is better than an another at everything. When an eval only has a certain type of problem (e.g., only edits to long codebases, or only short self-contained competition problems), it&#x27;s not clear how homogeneously its performance rankings will generalize to other coding tasks. Unfortunately, if you&#x27;re a developer using an LLM API, the best thing to do is to test all of the models from all the providers to see which works best for your use case.<p>(I work at OpenAI, so feel free to discount my opinions as much as you like.)</div><br/><div id="39988327" class="c"><input type="checkbox" id="c-39988327" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39987614">parent</a><span>|</span><a href="#39987726">next</a><span>|</span><label class="collapse" for="c-39988327">[-]</label><label class="expand" for="c-39988327">[1 more]</label></div><br/><div class="children"><div class="content">As a user, I basically just care about a minimum baseline of competence... which most models do well enough on.  But then I want the model to &quot;just give me the code&quot;.  I switched to Claude, and canceled my chatgpt subscription because the amount of placeholders and just general &quot;laziness&quot; in chatgpt was terrible.<p>Using Claude was a breath of fresh air.  I asked for some code, I got the entire code.</div><br/></div></div><div id="39987726" class="c"><input type="checkbox" id="c-39987726" checked=""/><div class="controls bullet"><span class="by">anshumankmr</span><span>|</span><a href="#39987614">parent</a><span>|</span><a href="#39988327">prev</a><span>|</span><a href="#39988433">next</a><span>|</span><label class="collapse" for="c-39987726">[-]</label><label class="expand" for="c-39987726">[1 more]</label></div><br/><div class="children"><div class="content">Hi Ted, since I have been using GPT 4 pretty much every day, I have a few questions about the performance, We had been using 1106 preview for several months to generate SQL queries for  a project, but one fine day in February, it stopped responding and it used to respond like so &quot;As a language model, I do not have the ability to generate queries etc...&quot;. This lasted for a few hours. Anyway, switching to 0125-preview which helped us immediately resolve the problem. We have been using that for whenever we have code generation related tasks unless we are doing FAQ stuff (where GPT 3.5 Turbo was good enough).<p>However, off late, I am noticing some really inconsistent behaviours in 0125-preview where it responds inconsistently for certain problems, ie one time it works with a detailed prompt and other time it doesn&#x27;t. I know these models are predicting the next most likely token which is not always deterministic.<p>So I was hoping for the ability to fine tune GPT 4 Turbo some time soon. Is that on the roadmap for Open AI?</div><br/></div></div></div></div><div id="39988433" class="c"><input type="checkbox" id="c-39988433" checked=""/><div class="controls bullet"><span class="by">jeswin</span><span>|</span><a href="#39987614">prev</a><span>|</span><a href="#39988398">next</a><span>|</span><label class="collapse" for="c-39988433">[-]</label><label class="expand" for="c-39988433">[1 more]</label></div><br/><div class="children"><div class="content">A big limitation with GPT4 Turbo (and Claude 3) for coding is the output token size. The only way to overcome the 4k limitation is by generating a file (if it fits), and feeding it back to generate the second and so on.<p>For this reason, GPT4-32k is my preferred model for codegen. I wish there are cheaper options.</div><br/></div></div><div id="39988398" class="c"><input type="checkbox" id="c-39988398" checked=""/><div class="controls bullet"><span class="by">pr337h4m</span><span>|</span><a href="#39988433">prev</a><span>|</span><a href="#39987590">next</a><span>|</span><label class="collapse" for="c-39988398">[-]</label><label class="expand" for="c-39988398">[1 more]</label></div><br/><div class="children"><div class="content">How hard could it be to let ChatGPT Plus users choose model versions? (especially when older versions are accessible through the API)</div><br/></div></div><div id="39987590" class="c"><input type="checkbox" id="c-39987590" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#39988398">prev</a><span>|</span><a href="#39987447">next</a><span>|</span><label class="collapse" for="c-39987590">[-]</label><label class="expand" for="c-39987590">[9 more]</label></div><br/><div class="children"><div class="content">Good thing Claude&#x27;s a massive step forward.</div><br/><div id="39988137" class="c"><input type="checkbox" id="c-39988137" checked=""/><div class="controls bullet"><span class="by">gorbypark</span><span>|</span><a href="#39987590">parent</a><span>|</span><a href="#39988370">next</a><span>|</span><label class="collapse" for="c-39988137">[-]</label><label class="expand" for="c-39988137">[4 more]</label></div><br/><div class="children"><div class="content">I had my Anthropic account banned (presumably) because I was testing out the vision capabilities and took a photo of a Japanese kitchen knife and asked it to &quot;translate the characters on the knife into English&quot;.  This wasn&#x27;t a Claude Pro account, but an API account, so it&#x27;s extra weird because what if I had some product based off the API, and an end user asked&#x2F;searched for something taboo..does my entire business get taken offline?  Good thing this was just a test account with like $10 in credit on it.  They haven&#x27;t responded to my &quot;account suspension appeal&quot; which is just a google form to enter your email address, not even a box to enter any details.<p>Anyways, Claude 3 Opus is pretty great for coding (I think better in most cases than the GPT4-Turbo previews) but I&#x27;m a bit weary of Anthropic now.</div><br/><div id="39988424" class="c"><input type="checkbox" id="c-39988424" checked=""/><div class="controls bullet"><span class="by">remoroid</span><span>|</span><a href="#39987590">root</a><span>|</span><a href="#39988137">parent</a><span>|</span><a href="#39988364">next</a><span>|</span><label class="collapse" for="c-39988424">[-]</label><label class="expand" for="c-39988424">[1 more]</label></div><br/><div class="children"><div class="content">I just tried to make an account<p>1. Asks me to enter my phone number and sends me a code<p>2. Enter code<p>3. Asks me to enter email and get code<p>4. Enter code<p>5. Redirects to asking me to enter phone number, but my number is already used now<p>6. My account is automatically banned</div><br/></div></div><div id="39988364" class="c"><input type="checkbox" id="c-39988364" checked=""/><div class="controls bullet"><span class="by">egeozcan</span><span>|</span><a href="#39987590">root</a><span>|</span><a href="#39988137">parent</a><span>|</span><a href="#39988424">prev</a><span>|</span><a href="#39988370">next</a><span>|</span><label class="collapse" for="c-39988364">[-]</label><label class="expand" for="c-39988364">[2 more]</label></div><br/><div class="children"><div class="content">I guess you can always use AI to detect inappropriate content from users... oh wait.<p>Seriously though, I understand that these mostly play to the enterprise market where even a hint of anything remotely &quot;unsafe&quot; needs to be shut down and deleted but why can&#x27;t they allow us to turn off the strict filtering like Google does? Why can Google offer &quot;unsafe&quot; content (in a limited fashion but it&#x27;s FINE) but LLM providers can&#x27;t?<p>Lack of competition?</div><br/><div id="39988419" class="c"><input type="checkbox" id="c-39988419" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#39987590">root</a><span>|</span><a href="#39988364">parent</a><span>|</span><a href="#39988370">next</a><span>|</span><label class="collapse" for="c-39988419">[-]</label><label class="expand" for="c-39988419">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not an LLM provider problem. It&#x27;s an Anthropic&#x2F;Google culture problem. OpenAI would very likely not have any problems with a request like that, but Claude has struggled with an absurdly misaligned sense of ethics from the start.<p>Note that Google is a big investor into Anthropic, and Anthropic was created because a bunch of OpenAI people thought OpenAI wasn&#x27;t being woke <i>enough</i> and quit as a consequence. So it&#x27;s not a surprise that it&#x27;s a lot more extremist than other model vendors.<p>That&#x27;s one reason why Aider doesn&#x27;t recommend you use it, even though in some ways it&#x27;s slightly better at coding. Claude Opus will routinely refuse ordinary coding requests due to its misalignment, whereas GPT-4 will not. That better reliability more than makes up for any difference in skill or speed.</div><br/></div></div></div></div></div></div><div id="39988370" class="c"><input type="checkbox" id="c-39988370" checked=""/><div class="controls bullet"><span class="by">lucaspiller</span><span>|</span><a href="#39987590">parent</a><span>|</span><a href="#39988137">prev</a><span>|</span><a href="#39987902">next</a><span>|</span><label class="collapse" for="c-39988370">[-]</label><label class="expand" for="c-39988370">[1 more]</label></div><br/><div class="children"><div class="content">Is there a good alternative available in the EU? Anthropic announced it was available in the EU last month, but it seems now that they&#x27;ve changed their mind.<p><a href="https:&#x2F;&#x2F;www.anthropic.com&#x2F;claude-ai-locations" rel="nofollow">https:&#x2F;&#x2F;www.anthropic.com&#x2F;claude-ai-locations</a></div><br/></div></div><div id="39987902" class="c"><input type="checkbox" id="c-39987902" checked=""/><div class="controls bullet"><span class="by">Zetobal</span><span>|</span><a href="#39987590">parent</a><span>|</span><a href="#39988370">prev</a><span>|</span><a href="#39987447">next</a><span>|</span><label class="collapse" for="c-39987902">[-]</label><label class="expand" for="c-39987902">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worthless until they open up the api for private use.</div><br/><div id="39987932" class="c"><input type="checkbox" id="c-39987932" checked=""/><div class="controls bullet"><span class="by">rfw300</span><span>|</span><a href="#39987590">root</a><span>|</span><a href="#39987902">parent</a><span>|</span><a href="#39987447">next</a><span>|</span><label class="collapse" for="c-39987932">[-]</label><label class="expand" for="c-39987932">[2 more]</label></div><br/><div class="children"><div class="content">I’ve been using the Claude 3 API since the models were announced. I believe it’s generally available (though capacity constrained &amp; rate limited at present).</div><br/><div id="39988236" class="c"><input type="checkbox" id="c-39988236" checked=""/><div class="controls bullet"><span class="by">098799</span><span>|</span><a href="#39987590">root</a><span>|</span><a href="#39987932">parent</a><span>|</span><a href="#39987447">next</a><span>|</span><label class="collapse" for="c-39988236">[-]</label><label class="expand" for="c-39988236">[1 more]</label></div><br/><div class="children"><div class="content">You do have to give them the company name though (however inconsequential that is)</div><br/></div></div></div></div></div></div></div></div><div id="39987447" class="c"><input type="checkbox" id="c-39987447" checked=""/><div class="controls bullet"><span class="by">macrolime</span><span>|</span><a href="#39987590">prev</a><span>|</span><a href="#39985597">next</a><span>|</span><label class="collapse" for="c-39987447">[-]</label><label class="expand" for="c-39987447">[6 more]</label></div><br/><div class="children"><div class="content">Another thing I have noticed is that if you use ChatGPT and it at some points uses Bing to look up something, it becomes super lazy afterwards, going from page long responses on average to a single paragraph.</div><br/><div id="39988514" class="c"><input type="checkbox" id="c-39988514" checked=""/><div class="controls bullet"><span class="by">kgabis</span><span>|</span><a href="#39987447">parent</a><span>|</span><a href="#39987514">next</a><span>|</span><label class="collapse" for="c-39988514">[-]</label><label class="expand" for="c-39988514">[1 more]</label></div><br/><div class="children"><div class="content">So the more advanced the AI, the more human-like it becomes. Senior Programmer level AI will spend all computing resources browsing memes.</div><br/></div></div><div id="39987514" class="c"><input type="checkbox" id="c-39987514" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39987447">parent</a><span>|</span><a href="#39988514">prev</a><span>|</span><a href="#39987479">next</a><span>|</span><label class="collapse" for="c-39987514">[-]</label><label class="expand" for="c-39987514">[1 more]</label></div><br/><div class="children"><div class="content">It probably has to do with the extended context window. Keeping websites in there is kind of a hassle. But I actually consider that a feature, not a bug. If I have ChatGPT use the internet, I don&#x27;t want a full page answer - especially not on the relatively slow GPT4. It&#x27;s also a hassle if you&#x27;re unsure about the validity of the output. In that case I might as well browse myself. Just give me a short preview so I can either start searching on my own or ask more questions.</div><br/></div></div><div id="39987479" class="c"><input type="checkbox" id="c-39987479" checked=""/><div class="controls bullet"><span class="by">Amivit</span><span>|</span><a href="#39987447">parent</a><span>|</span><a href="#39987514">prev</a><span>|</span><a href="#39987488">next</a><span>|</span><label class="collapse" for="c-39987479">[-]</label><label class="expand" for="c-39987479">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x2F;should make a custom GPT that isn&#x27;t allowed to use Bing. Works much better that way</div><br/></div></div><div id="39987488" class="c"><input type="checkbox" id="c-39987488" checked=""/><div class="controls bullet"><span class="by">e6quisitory</span><span>|</span><a href="#39987447">parent</a><span>|</span><a href="#39987479">prev</a><span>|</span><a href="#39988112">next</a><span>|</span><label class="collapse" for="c-39987488">[-]</label><label class="expand" for="c-39987488">[1 more]</label></div><br/><div class="children"><div class="content">Use ChatGPT Classic.</div><br/></div></div><div id="39988112" class="c"><input type="checkbox" id="c-39988112" checked=""/><div class="controls bullet"><span class="by">barfbagginus</span><span>|</span><a href="#39987447">parent</a><span>|</span><a href="#39987488">prev</a><span>|</span><a href="#39985597">next</a><span>|</span><label class="collapse" for="c-39988112">[-]</label><label class="expand" for="c-39988112">[1 more]</label></div><br/><div class="children"><div class="content">If answer is too lazy, you can tell it to elaborate. However, repairing a lazy context is sometimes slow and unreliable.<p>To avoid that, use backtracking and up the pressure for detailed answers. Then consider taking the least lazy of 2 or 3 samples.<p>A good prompt for detailed answers is Critique Of Though, an enhanced chain of thought technique. You ask for a search  and a detailed response with simple sections including analysis, critique and key assumptions.<p>It will expend more tokens, get more ideas out, and achieve higher accuracy. It will also be less lazy and more liable to recover from laziness or mistakes.<p>TLDR; if GPT4 is being lazy, backtrack and request a detailed multi section critical analysis.</div><br/></div></div></div></div><div id="39985597" class="c"><input type="checkbox" id="c-39985597" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#39987447">prev</a><span>|</span><a href="#39988096">next</a><span>|</span><label class="collapse" for="c-39985597">[-]</label><label class="expand" for="c-39985597">[8 more]</label></div><br/><div class="children"><div class="content">OpenAI just released GPT-4 Turbo with Vision and it performs worse on aider’s coding benchmark suites than all the previous GPT-4 models. In particular, it seems much more prone to “lazy coding” than the GPT-4 Turbo preview models.</div><br/><div id="39987005" class="c"><input type="checkbox" id="c-39987005" checked=""/><div class="controls bullet"><span class="by">jimmywetnips</span><span>|</span><a href="#39985597">parent</a><span>|</span><a href="#39987337">next</a><span>|</span><label class="collapse" for="c-39987005">[-]</label><label class="expand" for="c-39987005">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve definitely run into this personally. But even even I explicitly tell it to not skip implementation and to generate fully functional code, it says that it understands but continues right into omitting things again.<p>It was honestly shocking because we&#x27;re so used to it understanding our commands that a blatant disregard like that made me seriously wonder what kind of laziness layer they added</div><br/><div id="39987323" class="c"><input type="checkbox" id="c-39987323" checked=""/><div class="controls bullet"><span class="by">lyu07282</span><span>|</span><a href="#39985597">root</a><span>|</span><a href="#39987005">parent</a><span>|</span><a href="#39987285">next</a><span>|</span><label class="collapse" for="c-39987323">[-]</label><label class="expand" for="c-39987323">[2 more]</label></div><br/><div class="children"><div class="content">I suspect they might be worried it could reproduce copyrighted code in certain circumstances, so their solution was to condition the model to never produce large continuous chunks of code. It was a very noticeable change across the board.</div><br/><div id="39987387" class="c"><input type="checkbox" id="c-39987387" checked=""/><div class="controls bullet"><span class="by">s-lambert</span><span>|</span><a href="#39985597">root</a><span>|</span><a href="#39987323">parent</a><span>|</span><a href="#39987285">next</a><span>|</span><label class="collapse" for="c-39987387">[-]</label><label class="expand" for="c-39987387">[1 more]</label></div><br/><div class="children"><div class="content">I thought it would be for performance, since it doesn&#x27;t output all of the code then each reply is shorter&#x2F;quicker. Although you can still ask it to generate more of the code but that introduces some latency so there&#x27;s less overall load.</div><br/></div></div></div></div><div id="39987285" class="c"><input type="checkbox" id="c-39987285" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#39985597">root</a><span>|</span><a href="#39987005">parent</a><span>|</span><a href="#39987323">prev</a><span>|</span><a href="#39987191">next</a><span>|</span><label class="collapse" for="c-39987285">[-]</label><label class="expand" for="c-39987285">[1 more]</label></div><br/><div class="children"><div class="content">The laziness layer seems to be to be an assistant but not a replacement or doing the task.</div><br/></div></div><div id="39987191" class="c"><input type="checkbox" id="c-39987191" checked=""/><div class="controls bullet"><span class="by">_giorgio_</span><span>|</span><a href="#39985597">root</a><span>|</span><a href="#39987005">parent</a><span>|</span><a href="#39987285">prev</a><span>|</span><a href="#39987337">next</a><span>|</span><label class="collapse" for="c-39987191">[-]</label><label class="expand" for="c-39987191">[1 more]</label></div><br/><div class="children"><div class="content">They should offer different models at this point.<p>This laziness occurs over and over, so why bother with omniscience.</div><br/></div></div></div></div><div id="39987337" class="c"><input type="checkbox" id="c-39987337" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#39985597">parent</a><span>|</span><a href="#39987005">prev</a><span>|</span><a href="#39987168">next</a><span>|</span><label class="collapse" for="c-39987337">[-]</label><label class="expand" for="c-39987337">[1 more]</label></div><br/><div class="children"><div class="content">Really appreciate the thoroughness you apply to evaluating models for use with Aider. Did you adjust the prompt at all for the newer models?</div><br/></div></div><div id="39987168" class="c"><input type="checkbox" id="c-39987168" checked=""/><div class="controls bullet"><span class="by">memothon</span><span>|</span><a href="#39985597">parent</a><span>|</span><a href="#39987337">prev</a><span>|</span><a href="#39988096">next</a><span>|</span><label class="collapse" for="c-39987168">[-]</label><label class="expand" for="c-39987168">[1 more]</label></div><br/><div class="children"><div class="content">Thanks again for running all these benchmarks with model releases. They are really helpful to track progress!</div><br/></div></div></div></div><div id="39988096" class="c"><input type="checkbox" id="c-39988096" checked=""/><div class="controls bullet"><span class="by">klohto</span><span>|</span><a href="#39985597">prev</a><span>|</span><a href="#39987188">next</a><span>|</span><label class="collapse" for="c-39988096">[-]</label><label class="expand" for="c-39988096">[1 more]</label></div><br/><div class="children"><div class="content">I would be curious to see if the results improve by using DSPy to improve your prompts (and also reevaluate which prompts work better on the newest model).</div><br/></div></div><div id="39987188" class="c"><input type="checkbox" id="c-39987188" checked=""/><div class="controls bullet"><span class="by">kingkongjaffa</span><span>|</span><a href="#39988096">prev</a><span>|</span><a href="#39987286">next</a><span>|</span><label class="collapse" for="c-39987188">[-]</label><label class="expand" for="c-39987188">[3 more]</label></div><br/><div class="children"><div class="content">Its not just for coding, the base “gpt-4” model seems better than the latest preview model<p><a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;continuous-model-upgrades" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;continuous-model-upg...</a></div><br/><div id="39987278" class="c"><input type="checkbox" id="c-39987278" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#39987188">parent</a><span>|</span><a href="#39987223">next</a><span>|</span><label class="collapse" for="c-39987278">[-]</label><label class="expand" for="c-39987278">[1 more]</label></div><br/><div class="children"><div class="content">I wish base gpt-4 was available in the chat product, miss it.</div><br/></div></div><div id="39987223" class="c"><input type="checkbox" id="c-39987223" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#39987188">parent</a><span>|</span><a href="#39987278">prev</a><span>|</span><a href="#39987286">next</a><span>|</span><label class="collapse" for="c-39987223">[-]</label><label class="expand" for="c-39987223">[1 more]</label></div><br/><div class="children"><div class="content">The -turbo models in the past have been much worse too. gpt-3.5-turbo is way way worse than text-davinci-003 (gpt-3.5).<p>The -turbos are correspondingly priced. gpt-4-turbo is ~1&#x2F;3 price of gpt-4, 6.6x more expensive than gpt-3.5-turbo-instruct and 20x gpt-3.5-turbo.</div><br/></div></div></div></div><div id="39987286" class="c"><input type="checkbox" id="c-39987286" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#39987188">prev</a><span>|</span><a href="#39988116">next</a><span>|</span><label class="collapse" for="c-39987286">[-]</label><label class="expand" for="c-39987286">[2 more]</label></div><br/><div class="children"><div class="content">If it was possible to hook into token selection process (kind of like JSON restricted grammar but using custom scripts), then it would be possible to detect that GPT-4 is about to add &quot;# impement code here&quot; and then we could force it select a different set of tokens which would make GPT4 generate a proper method body.</div><br/><div id="39988452" class="c"><input type="checkbox" id="c-39988452" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#39987286">parent</a><span>|</span><a href="#39988116">next</a><span>|</span><label class="collapse" for="c-39988452">[-]</label><label class="expand" for="c-39988452">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s called guidance and the problem is that it has to be done carefully or else you&#x27;ll just get rephrasings that work around the block.<p>I think a better approach is multi-pass coding along with fine-tuning or prompting to use a particular form of TODO comment. Aider can already do a form of fake &quot;fill in the middle&quot; by making it emit diffs. If it notices that some code has been filled out lazily, it could go back and ask it to do the next chunk of work. Given that large tasks are normally split up into small tasks by programmers anyway, this seems like a natural approach that is required for scaling up regardless.</div><br/></div></div></div></div><div id="39988116" class="c"><input type="checkbox" id="c-39988116" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#39987286">prev</a><span>|</span><label class="collapse" for="c-39988116">[-]</label><label class="expand" for="c-39988116">[2 more]</label></div><br/><div class="children"><div class="content">I posted this before, I&#x27;ll post this again: GPT getting lazier is not an objectively bad thing. I don&#x27;t copy code that it generates but ask it about more high level concepts more often, and have to instruct it not to generate imports and other boilerplate code. In most cases, this “lazy” generation saves time and tokens and is exactly what I need.</div><br/><div id="39988401" class="c"><input type="checkbox" id="c-39988401" checked=""/><div class="controls bullet"><span class="by">Simon321</span><span>|</span><a href="#39988116">parent</a><span>|</span><label class="collapse" for="c-39988401">[-]</label><label class="expand" for="c-39988401">[1 more]</label></div><br/><div class="children"><div class="content">Yes that&#x27;s true, but if you ask to give the full code specifically it should do do</div><br/></div></div></div></div></div></div></div></div></div></body></html>