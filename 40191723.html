<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714381266442" as="style"/><link rel="stylesheet" href="styles.css?v=1714381266442"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ollama/ollama/releases/tag/v0.1.33-rc5">Ollama v0.1.33 with Llama 3, Phi 3, and Qwen 110B</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>ashvardanian</span> | <span>47 comments</span></div><br/><div><div id="40194890" class="c"><input type="checkbox" id="c-40194890" checked=""/><div class="controls bullet"><span class="by">wiktor-k</span><span>|</span><a href="#40193020">next</a><span>|</span><label class="collapse" for="c-40194890">[-]</label><label class="expand" for="c-40194890">[2 more]</label></div><br/><div class="children"><div class="content">Ollama is simply great! I was quite surprised how easy it is to integrate through their API. A simple chat using Ollama + llama3 is less than 40 lines of TypeScript: <a href="https:&#x2F;&#x2F;github.com&#x2F;wiktor-k&#x2F;llama-chat">https:&#x2F;&#x2F;github.com&#x2F;wiktor-k&#x2F;llama-chat</a></div><br/><div id="40195570" class="c"><input type="checkbox" id="c-40195570" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#40194890">parent</a><span>|</span><a href="#40193020">next</a><span>|</span><label class="collapse" for="c-40195570">[-]</label><label class="expand" for="c-40195570">[1 more]</label></div><br/><div class="children"><div class="content">Nice! Would there be a way to do that streaming, with streaming voice input too?</div><br/></div></div></div></div><div id="40193020" class="c"><input type="checkbox" id="c-40193020" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#40194890">prev</a><span>|</span><a href="#40195901">next</a><span>|</span><label class="collapse" for="c-40193020">[-]</label><label class="expand" for="c-40193020">[10 more]</label></div><br/><div class="children"><div class="content">I wonder if Ollama will or plans to have other &quot;Supported backends&quot; than llama.cpp. It&#x27;s listed on the very last line of their readme as if the llama.cpp dependency is just incidental and a very minor detail rather than Ollama as a deployment mechanism for llama.cpp and gguf based models.</div><br/><div id="40193565" class="c"><input type="checkbox" id="c-40193565" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#40193020">parent</a><span>|</span><a href="#40195664">next</a><span>|</span><label class="collapse" for="c-40193565">[-]</label><label class="expand" for="c-40193565">[2 more]</label></div><br/><div class="children"><div class="content">Yes, we are also looking at integrating MLX [1] which is optimized for Apple Silicon and built by an amazing team of individuals, a few of which were behind the original Torch [2] project. There&#x27;s also TensorRT-LLM [3] by Nvidia optimized for their recent hardware.<p>All of this of course acknowledging that llama.cpp is an incredible project with competitive performance and support for almost any platform.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ml-explore&#x2F;mlx">https:&#x2F;&#x2F;github.com&#x2F;ml-explore&#x2F;mlx</a><p>[2] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Torch_(machine_learning)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Torch_(machine_learning)</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;TensorRT-LLM">https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;TensorRT-LLM</a></div><br/><div id="40194734" class="c"><input type="checkbox" id="c-40194734" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#40193020">root</a><span>|</span><a href="#40193565">parent</a><span>|</span><a href="#40195664">next</a><span>|</span><label class="collapse" for="c-40194734">[-]</label><label class="expand" for="c-40194734">[1 more]</label></div><br/><div class="children"><div class="content">MLX and TensorRT would be really nice!</div><br/></div></div></div></div><div id="40195664" class="c"><input type="checkbox" id="c-40195664" checked=""/><div class="controls bullet"><span class="by">sh79</span><span>|</span><a href="#40193020">parent</a><span>|</span><a href="#40193565">prev</a><span>|</span><a href="#40193223">next</a><span>|</span><label class="collapse" for="c-40195664">[-]</label><label class="expand" for="c-40195664">[1 more]</label></div><br/><div class="children"><div class="content">Their behaviour around llama.cpp acknowledgement is very shady. Until the very recent, there was no mention of llama.cpp in their README at all and now it&#x27;s tucked away all the way down. Compare that to the originally proposed PR for example: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;pull&#x2F;3700">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;pull&#x2F;3700</a></div><br/></div></div><div id="40193223" class="c"><input type="checkbox" id="c-40193223" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#40193020">parent</a><span>|</span><a href="#40195664">prev</a><span>|</span><a href="#40193595">next</a><span>|</span><label class="collapse" for="c-40193223">[-]</label><label class="expand" for="c-40193223">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think they will move away from llama.cpp until they are forced to.  The number of people contributing to llama.cpp is quite significant [1] and it wouldn&#x27;t make sense to use another backend given how quickly llama.cpp is iterating and growing.<p>[1] <a href="https:&#x2F;&#x2F;devboard.gitsense.com&#x2F;ggerganov?r=ggerganov%2Fllama.cpp&amp;nb=true&amp;comments=ignore&amp;issues=ignore" rel="nofollow">https:&#x2F;&#x2F;devboard.gitsense.com&#x2F;ggerganov?r=ggerganov%2Fllama....</a><p>Full disclosure: This is my tool</div><br/><div id="40193686" class="c"><input type="checkbox" id="c-40193686" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40193020">root</a><span>|</span><a href="#40193223">parent</a><span>|</span><a href="#40193595">next</a><span>|</span><label class="collapse" for="c-40193686">[-]</label><label class="expand" for="c-40193686">[4 more]</label></div><br/><div class="children"><div class="content"><i>ghost of christmas future</i><p>The chance onnx becomes significantly relevant here went from 1% to 15% this week. They&#x27;re demo&#x27;ing ~2x faster inference with Phi-3. There&#x27;s been fits and starts on LLMs in ONNX for a year, but, with Wintel&#x27;s AI PC™ push, and all the constituent parts in place (4 bit quants! adaptive quants!), I&#x27;d put very good money on it.</div><br/><div id="40194075" class="c"><input type="checkbox" id="c-40194075" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#40193020">root</a><span>|</span><a href="#40193686">parent</a><span>|</span><a href="#40193595">next</a><span>|</span><label class="collapse" for="c-40194075">[-]</label><label class="expand" for="c-40194075">[3 more]</label></div><br/><div class="children"><div class="content">So you are saying Ollama is a strong MS acquisition in the future if onnx works out.</div><br/><div id="40194495" class="c"><input type="checkbox" id="c-40194495" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40193020">root</a><span>|</span><a href="#40194075">parent</a><span>|</span><a href="#40193595">next</a><span>|</span><label class="collapse" for="c-40194495">[-]</label><label class="expand" for="c-40194495">[2 more]</label></div><br/><div class="children"><div class="content">no, ONNX is a Microsoft project, I don&#x27;t know why people know what Ollama is and I don&#x27;t think they will in a year</div><br/><div id="40194563" class="c"><input type="checkbox" id="c-40194563" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#40193020">root</a><span>|</span><a href="#40194495">parent</a><span>|</span><a href="#40193595">next</a><span>|</span><label class="collapse" for="c-40194563">[-]</label><label class="expand" for="c-40194563">[1 more]</label></div><br/><div class="children"><div class="content">I know it is a Microsoft Project.  My reasoning is, if Ollama supports ONNX and if it can provide performance on par or better than llama.cpp, it would make sense for Microsoft to acquire Ollama for distribution reasons.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40193595" class="c"><input type="checkbox" id="c-40193595" checked=""/><div class="controls bullet"><span class="by">Cheer2171</span><span>|</span><a href="#40193020">parent</a><span>|</span><a href="#40193223">prev</a><span>|</span><a href="#40195901">next</a><span>|</span><label class="collapse" for="c-40193595">[-]</label><label class="expand" for="c-40193595">[1 more]</label></div><br/><div class="children"><div class="content">It is open source, so if you want to see this in ollama, pull requests are welcome. :)</div><br/></div></div></div></div><div id="40195901" class="c"><input type="checkbox" id="c-40195901" checked=""/><div class="controls bullet"><span class="by">stephen37</span><span>|</span><a href="#40193020">prev</a><span>|</span><a href="#40193138">next</a><span>|</span><label class="collapse" for="c-40195901">[-]</label><label class="expand" for="c-40195901">[1 more]</label></div><br/><div class="children"><div class="content">I love working with Ollama, I was really surprised at how easy it is to build a simple RAG system with it. For example: <a href="https:&#x2F;&#x2F;github.com&#x2F;stephen37&#x2F;ollama_local_rag">https:&#x2F;&#x2F;github.com&#x2F;stephen37&#x2F;ollama_local_rag</a></div><br/></div></div><div id="40193138" class="c"><input type="checkbox" id="c-40193138" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#40195901">prev</a><span>|</span><a href="#40192730">next</a><span>|</span><label class="collapse" for="c-40193138">[-]</label><label class="expand" for="c-40193138">[3 more]</label></div><br/><div class="children"><div class="content">I actually just benchmarked Llama3 70B coding with aider, and it did quite well. It scored similar to GPT 3.5.<p>You can use Llama3 70B with aider via Ollama [0]. It&#x27;s also available for free via Groq [1] (with rate limits). And OpenRouter has it available [2] for low cost on their paid api.<p>[0] <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;llms.html#ollama" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;llms.html#ollama</a><p>[1] <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;llms.html#groq" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;llms.html#groq</a><p>[2] <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;llms.html#openrouter" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;llms.html#openrouter</a></div><br/><div id="40194012" class="c"><input type="checkbox" id="c-40194012" checked=""/><div class="controls bullet"><span class="by">typpo</span><span>|</span><a href="#40193138">parent</a><span>|</span><a href="#40194856">next</a><span>|</span><label class="collapse" for="c-40194012">[-]</label><label class="expand" for="c-40194012">[1 more]</label></div><br/><div class="children"><div class="content">Paul&#x27;s benchmarks are excellent and they&#x27;re the first thing I look for to get a sense of a new model performance :)<p>For those looking to create their own benchmarks, promptfoo[0] is one way to do this locally:<p><pre><code>  prompts:
    - &quot;Write this in Python 3: {{ask}}&quot;
  
  providers:
    - ollama:chat:llama3:8b
    - ollama:chat:phi3
    - ollama:chat:qwen:7b
    
  tests:
    - vars:
        ask: a function to determine if a number is prime
    - vars:
        ask: a function to split a restaurant bill given individual contributions and shared items
</code></pre>
Jumping in because I&#x27;m a big believer in (1) local LLMs, and (2) evals specific to individual use cases.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;typpo&#x2F;promptfoo">https:&#x2F;&#x2F;github.com&#x2F;typpo&#x2F;promptfoo</a></div><br/></div></div><div id="40194856" class="c"><input type="checkbox" id="c-40194856" checked=""/><div class="controls bullet"><span class="by">jkh1</span><span>|</span><a href="#40193138">parent</a><span>|</span><a href="#40194012">prev</a><span>|</span><a href="#40192730">next</a><span>|</span><label class="collapse" for="c-40194856">[-]</label><label class="expand" for="c-40194856">[1 more]</label></div><br/><div class="children"><div class="content">Another benchmark: <a href="https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2024.04.19.590278v2.full.pdf" rel="nofollow">https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2024.04.19.590278v2....</a></div><br/></div></div></div></div><div id="40192730" class="c"><input type="checkbox" id="c-40192730" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#40193138">prev</a><span>|</span><a href="#40192725">next</a><span>|</span><label class="collapse" for="c-40192730">[-]</label><label class="expand" for="c-40192730">[4 more]</label></div><br/><div class="children"><div class="content">[Why] do models require a new version? It can already take arbitrary gguf; I assumed they just had a registry online</div><br/><div id="40192760" class="c"><input type="checkbox" id="c-40192760" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#40192730">parent</a><span>|</span><a href="#40192841">next</a><span>|</span><label class="collapse" for="c-40192760">[-]</label><label class="expand" for="c-40192760">[2 more]</label></div><br/><div class="children"><div class="content">They do, and I was using the &quot;new&quot; models before the update. Perhaps there is tuning or bug fixes for them? Or they just want to confirm that these are supported. There are some new models that do have different architectures, so sometimes an update is necessary.</div><br/><div id="40193705" class="c"><input type="checkbox" id="c-40193705" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40192730">root</a><span>|</span><a href="#40192760">parent</a><span>|</span><a href="#40192841">next</a><span>|</span><label class="collapse" for="c-40193705">[-]</label><label class="expand" for="c-40193705">[1 more]</label></div><br/><div class="children"><div class="content">Phi 3 has a unique architecture that needed some additions to llama.cpp&#x27;s conversion script. Also Phi 3 is an absolute mess, there&#x27;s no reliable way to latch on to when it&#x27;s done writing a message and no one wants to admit it, people are patching around it instead.<p>ex. I <i>could</i> condition on &quot;\n\n&lt;|assistant|&gt;||&lt;|system|&gt;||&lt;|user&gt;&quot;, but it&#x27;d still be wrong.<p>Pretty much everything Phi 3 feels like it needed to all come out within 48 hours a month too early. The ONNX genai library doesn&#x27;t work on Mac, at all, the mobile SDKs don&#x27;t support it...sigh</div><br/></div></div></div></div><div id="40192841" class="c"><input type="checkbox" id="c-40192841" checked=""/><div class="controls bullet"><span class="by">FieryTransition</span><span>|</span><a href="#40192730">parent</a><span>|</span><a href="#40192760">prev</a><span>|</span><a href="#40192725">next</a><span>|</span><label class="collapse" for="c-40192841">[-]</label><label class="expand" for="c-40192841">[1 more]</label></div><br/><div class="children"><div class="content">Because the way they are quantized takes time to get bug-free when new architectures are released. If a model was quantized with a known bug in the quantizer, then it effectively makes those quantized versions buggy and they need to be requantized with a new version of llamacpp which has this fixed.</div><br/></div></div></div></div><div id="40192725" class="c"><input type="checkbox" id="c-40192725" checked=""/><div class="controls bullet"><span class="by">thedatamonger</span><span>|</span><a href="#40192730">prev</a><span>|</span><label class="collapse" for="c-40192725">[-]</label><label class="expand" for="c-40192725">[26 more]</label></div><br/><div class="children"><div class="content">this looks very awesome.  can someone tell me why there is no chatter about this?  is there something else out there that blows this out of the water in terms of ease of use and access to sample many LLM&#x27;s ?</div><br/><div id="40192939" class="c"><input type="checkbox" id="c-40192939" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#40192725">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40192939">[-]</label><label class="expand" for="c-40192939">[19 more]</label></div><br/><div class="children"><div class="content">HN isnt really the best space for LLM news - r&#x2F;LocalLlama and twitter are much better.  I think HN has some cultural issues with “AI” news</div><br/><div id="40193067" class="c"><input type="checkbox" id="c-40193067" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40192939">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193067">[-]</label><label class="expand" for="c-40193067">[18 more]</label></div><br/><div class="children"><div class="content">Hmm I don&#x27;t think so. Most comments are pretty positive.<p>I think the articles are just not really upvoted unless it&#x27;s really big news, makes sense because HN is for more than just AI.<p>But I don&#x27;t think it&#x27;s anti-AI like most people here would be pretty anti-cryptocurrency (and for good reason IMO)</div><br/><div id="40193298" class="c"><input type="checkbox" id="c-40193298" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193067">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193298">[-]</label><label class="expand" for="c-40193298">[17 more]</label></div><br/><div class="children"><div class="content">I didn’t upvote it because I don’t use Ollama. To experiment with LLMs I use Huggingface. Does Ollama provide something I cannot get with Huggingface?</div><br/><div id="40194877" class="c"><input type="checkbox" id="c-40194877" checked=""/><div class="controls bullet"><span class="by">jkh1</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193298">parent</a><span>|</span><a href="#40194298">next</a><span>|</span><label class="collapse" for="c-40194877">[-]</label><label class="expand" for="c-40194877">[1 more]</label></div><br/><div class="children"><div class="content">Running locally is sometimes necessary, e.g. you don&#x27;t want to send sensitive data to any random third party server.</div><br/></div></div><div id="40194298" class="c"><input type="checkbox" id="c-40194298" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193298">parent</a><span>|</span><a href="#40194877">prev</a><span>|</span><a href="#40193361">next</a><span>|</span><label class="collapse" for="c-40194298">[-]</label><label class="expand" for="c-40194298">[2 more]</label></div><br/><div class="children"><div class="content">Ollama provides a web server with API that just works out of the box, which is great when you want to integrate multiple applications (potentially distributed on smaller edge devices) with LLMs that run on a single beefy machine.<p>In my home I have a large gaming rig that sometimes runs Ollama+Open WebUI, then I also have a bunch of other services running on a smaller server and a Raspberry Pi which reach out to Ollama for their LLM inference needs.</div><br/><div id="40194489" class="c"><input type="checkbox" id="c-40194489" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40194298">parent</a><span>|</span><a href="#40193361">next</a><span>|</span><label class="collapse" for="c-40194489">[-]</label><label class="expand" for="c-40194489">[1 more]</label></div><br/><div class="children"><div class="content">Sure, maybe it’s better for niche use cases like yours.<p>HF is the biggest provider of llms, and I guess I haven’t run into it’s limitations yet.</div><br/></div></div></div></div><div id="40193361" class="c"><input type="checkbox" id="c-40193361" checked=""/><div class="controls bullet"><span class="by">gertop</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193298">parent</a><span>|</span><a href="#40194298">prev</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193361">[-]</label><label class="expand" for="c-40193361">[13 more]</label></div><br/><div class="children"><div class="content">Hugging face is a model repository.<p>Ollama allows you to run those models.<p>Different things.</div><br/><div id="40193395" class="c"><input type="checkbox" id="c-40193395" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193361">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193395">[-]</label><label class="expand" for="c-40193395">[12 more]</label></div><br/><div class="children"><div class="content">I run models using HF just fine. I mean I’m using HF transformers repo, which gets models from HF hub.<p>Or do you mean commercial deployment of models for inference?</div><br/><div id="40193422" class="c"><input type="checkbox" id="c-40193422" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193395">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193422">[-]</label><label class="expand" for="c-40193422">[11 more]</label></div><br/><div class="children"><div class="content">Are you talking about the Hugging Face Python libraries, the Hugging Face hosted inference APIs, the Hugging Face web interfaces, the Hugging Face iPhone app, Hugging Face Spaces (hosted Docker environments with GPU access) or something else?</div><br/><div id="40193435" class="c"><input type="checkbox" id="c-40193435" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193422">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193435">[-]</label><label class="expand" for="c-40193435">[10 more]</label></div><br/><div class="children"><div class="content">I updated my comment above: I’m using HF transformers repo, which gets models from HF hub.</div><br/><div id="40193451" class="c"><input type="checkbox" id="c-40193451" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193435">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193451">[-]</label><label class="expand" for="c-40193451">[9 more]</label></div><br/><div class="children"><div class="content">Do you have an NVIDIA GPU? I have not had much luck with the transformers library on a Mac.</div><br/><div id="40193494" class="c"><input type="checkbox" id="c-40193494" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193451">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193494">[-]</label><label class="expand" for="c-40193494">[8 more]</label></div><br/><div class="children"><div class="content">Of course. I thought Nvidia GPUs are pretty much a must have to play with DL models.</div><br/><div id="40193629" class="c"><input type="checkbox" id="c-40193629" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193494">parent</a><span>|</span><a href="#40193536">next</a><span>|</span><label class="collapse" for="c-40193629">[-]</label><label class="expand" for="c-40193629">[2 more]</label></div><br/><div class="children"><div class="content">Ollama supports many radeons now. And I guess llama.cpp does too, after all it&#x27;s what ollama uses as backend.</div><br/><div id="40193662" class="c"><input type="checkbox" id="c-40193662" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193629">parent</a><span>|</span><a href="#40193536">next</a><span>|</span><label class="collapse" for="c-40193662">[-]</label><label class="expand" for="c-40193662">[1 more]</label></div><br/><div class="children"><div class="content">PyTorch (the underlying framework of HF) supports AMD as well, though I haven’t tried it.</div><br/></div></div></div></div><div id="40193536" class="c"><input type="checkbox" id="c-40193536" checked=""/><div class="controls bullet"><span class="by">objektif</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193494">parent</a><span>|</span><a href="#40193629">prev</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193536">[-]</label><label class="expand" for="c-40193536">[5 more]</label></div><br/><div class="children"><div class="content">Well being able to run these models on CPU was pretty much the revolutionary part of llama.cpp.</div><br/><div id="40193577" class="c"><input type="checkbox" id="c-40193577" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193536">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40193577">[-]</label><label class="expand" for="c-40193577">[4 more]</label></div><br/><div class="children"><div class="content">I can run them on CPU - HF uses plain Pytorch code - fully supported on CPU.</div><br/><div id="40194152" class="c"><input type="checkbox" id="c-40194152" checked=""/><div class="controls bullet"><span class="by">tmostak</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40193577">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40194152">[-]</label><label class="expand" for="c-40194152">[3 more]</label></div><br/><div class="children"><div class="content">But it&#x27;s likely to be much slower than what you&#x27;d get with a backend like llama.cpp on CPU (particularly if you&#x27;re running on a Mac, but I think on Linux as well), as well as not supporting features like CPU offloading.</div><br/><div id="40194292" class="c"><input type="checkbox" id="c-40194292" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40194152">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40194292">[-]</label><label class="expand" for="c-40194292">[2 more]</label></div><br/><div class="children"><div class="content">Are there benchmarks? 2x speed up would not be enough for me to return to c++ hell, but 5x might be, in some circumstances.</div><br/><div id="40195745" class="c"><input type="checkbox" id="c-40195745" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#40192725">root</a><span>|</span><a href="#40194292">parent</a><span>|</span><a href="#40192852">next</a><span>|</span><label class="collapse" for="c-40195745">[-]</label><label class="expand" for="c-40195745">[1 more]</label></div><br/><div class="children"><div class="content">I think the biggest selling point of ollama (llama.cpp) are quantizations, for a slight hit (with q8 or q4) in quality you can get a significant performance boost.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40192852" class="c"><input type="checkbox" id="c-40192852" checked=""/><div class="controls bullet"><span class="by">chadsix</span><span>|</span><a href="#40192725">parent</a><span>|</span><a href="#40192939">prev</a><span>|</span><a href="#40192856">next</a><span>|</span><label class="collapse" for="c-40192852">[-]</label><label class="expand" for="c-40192852">[1 more]</label></div><br/><div class="children"><div class="content">Ollama is really organized - it relies on llama but the UX and organization it provides makes it legit. We recently made a one-click wizard to run Open WebUI and Ollama together, self hosted and remotely accessible but locally hosted [1]<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ipv6rslimited&#x2F;cloudseeder">https:&#x2F;&#x2F;github.com&#x2F;ipv6rslimited&#x2F;cloudseeder</a></div><br/></div></div><div id="40192856" class="c"><input type="checkbox" id="c-40192856" checked=""/><div class="controls bullet"><span class="by">FieryTransition</span><span>|</span><a href="#40192725">parent</a><span>|</span><a href="#40192852">prev</a><span>|</span><a href="#40193622">next</a><span>|</span><label class="collapse" for="c-40192856">[-]</label><label class="expand" for="c-40192856">[1 more]</label></div><br/><div class="children"><div class="content">I use a mix of using llamacpp directly via my own python bindings and using it via llamacpp-python for function calling and full control over parameters and loading, but otherwise ollama is just great for ease of use. There&#x27;s really not a reason not to use it, if just want to load gguf models and don&#x27;t have any intricate requirements.</div><br/></div></div><div id="40193622" class="c"><input type="checkbox" id="c-40193622" checked=""/><div class="controls bullet"><span class="by">Cheer2171</span><span>|</span><a href="#40192725">parent</a><span>|</span><a href="#40192856">prev</a><span>|</span><a href="#40193121">next</a><span>|</span><label class="collapse" for="c-40193622">[-]</label><label class="expand" for="c-40193622">[1 more]</label></div><br/><div class="children"><div class="content">Why do you think there is no chatter about this? There have been hundreds of posts about ollama on HN. This is a point release of an already well known project.</div><br/></div></div><div id="40193121" class="c"><input type="checkbox" id="c-40193121" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#40192725">parent</a><span>|</span><a href="#40193622">prev</a><span>|</span><a href="#40193342">next</a><span>|</span><label class="collapse" for="c-40193121">[-]</label><label class="expand" for="c-40193121">[1 more]</label></div><br/><div class="children"><div class="content">I can recommend LM Studio and Msty if you&#x27;re looking for something with an integrated UX.</div><br/></div></div><div id="40193342" class="c"><input type="checkbox" id="c-40193342" checked=""/><div class="controls bullet"><span class="by">gertop</span><span>|</span><a href="#40192725">parent</a><span>|</span><a href="#40193121">prev</a><span>|</span><a href="#40192776">next</a><span>|</span><label class="collapse" for="c-40193342">[-]</label><label class="expand" for="c-40193342">[1 more]</label></div><br/><div class="children"><div class="content">LM Studio is a lot more user friendly, probably the easiest UI to use out there.  No terminal nonsense, no manual to read. Just double click and chat. It even explains to you what the model names mean (eg diff between Q4_1 Q4_K Q4_K_M... For whatever reason all the other tools assume you know what it means).<p>Built-in model recommendations are also handy.<p>Very friendly tool!<p>However it&#x27;s not open-source.</div><br/></div></div><div id="40192776" class="c"><input type="checkbox" id="c-40192776" checked=""/><div class="controls bullet"><span class="by">throw03172019</span><span>|</span><a href="#40192725">parent</a><span>|</span><a href="#40193342">prev</a><span>|</span><label class="collapse" for="c-40192776">[-]</label><label class="expand" for="c-40192776">[1 more]</label></div><br/><div class="children"><div class="content">Lola a has been brought up many times on HN. It’s a great tool!</div><br/></div></div></div></div></div></div></div></div></div></body></html>