<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1722762051907" as="style"/><link rel="stylesheet" href="styles.css?v=1722762051907"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.theverge.com/2024/8/3/24212518/nvidia-ai-chip-delay-blackwell-b200-microsoft-amazon-google-openai-meta-artificial-intelligence">Nvidia reportedly delays its next AI chip due to a design flaw</a> <span class="domain">(<a href="https://www.theverge.com">www.theverge.com</a>)</span></div><div class="subtext"><span>mgh2</span> | <span>53 comments</span></div><br/><div><div id="41150602" class="c"><input type="checkbox" id="c-41150602" checked=""/><div class="controls bullet"><span class="by">TheAlchemist</span><span>|</span><a href="#41151564">next</a><span>|</span><label class="collapse" for="c-41150602">[-]</label><label class="expand" for="c-41150602">[25 more]</label></div><br/><div class="children"><div class="content">I like to think (hope) that the next breakthrough will come not from these huge clusters, but from somebody tinkering with new ideas on a small local system.<p>I also wonder - is the compute the main limiting factor today ? Let&#x27;s imagine there is an unlimited number of NVidia chips available right now and energy is cheap - would using a cluster x100 of current biggest one result in a significant improvement ? My naive intuition is that not really.</div><br/><div id="41150681" class="c"><input type="checkbox" id="c-41150681" checked=""/><div class="controls bullet"><span class="by">Mehdi2277</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41150712">next</a><span>|</span><label class="collapse" for="c-41150681">[-]</label><label class="expand" for="c-41150681">[5 more]</label></div><br/><div class="children"><div class="content">My experience working on ml at couple faang like companies is gpus actually tend to be too fast compute wise and often models are unable to come close to theoretical nvidia flops numbers. In that very frequently bottlenecks from profiling are elsewhere. It is very easy to have your data reading code be bottleneck. I have seen some models where our networking was bottleneck and could not keep up with the compute and we had adjust model architecture in ways to reduce amount of data transferred in training steps across the cluster. Or maybe you have gpu memory bandwidth as bottleneck. Key idea in flash attention work is optimizing attention kernels to lower amount of vram usage and stick to smaller&#x2F;faster sram. This is valuable work, but is also kind of work that is pretty rare engineer I have worked with would have cuda kernel experience to create custom efficient kernels. Some of the models I train use a lot of sparse tensors as features and tensorflow’s sparse gpu kernel is rather bad with many operations either falling back to cpu or sometimes I have had gpu sparse kernel that was slower than cpu equivalent kernel. Several times densifying and padding tensors with large fraction of 0’s was faster than using sparse kernel.<p>I’m sure a few companies&#x2F;models are optimized enough to fit ideal case but it’s rare.<p>Edit: Another aspect of this is nature of model architecture that are good today is very hardware driven. Major advantage of transformers over recurrent lstm models is training efficiency on gpu. The gap in training efficiency is much more dramatic with gpu than cpu for these two architectures. Similarly other architectures with sequential components like tree structured&#x2F;recursive dynamic models tend to fit badly for gpu performance wise.</div><br/><div id="41150708" class="c"><input type="checkbox" id="c-41150708" checked=""/><div class="controls bullet"><span class="by">TheAlchemist</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41150681">parent</a><span>|</span><a href="#41150712">next</a><span>|</span><label class="collapse" for="c-41150708">[-]</label><label class="expand" for="c-41150708">[4 more]</label></div><br/><div class="children"><div class="content">Interesting, thanks.<p>Let me reframe the question - assume it&#x27;s not only 100x GPUs, but all the performance bottlenecks you&#x27;ve mentioned are also solved or accelerated x100.<p>What kind of improvement would we observe, given the current state of the models and knowledge ?</div><br/><div id="41150740" class="c"><input type="checkbox" id="c-41150740" checked=""/><div class="controls bullet"><span class="by">Mehdi2277</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41150708">parent</a><span>|</span><a href="#41150712">next</a><span>|</span><label class="collapse" for="c-41150740">[-]</label><label class="expand" for="c-41150740">[3 more]</label></div><br/><div class="children"><div class="content">If I assume you mean LLM like models similar to chatgpt that is pretty debated in the community. Several years ago many people in ML community believed we were at plateau and that throwing more compute&#x2F;money would not give significant improvements. Well then LLMs did much better than expected as they scaled up and continue to iterate now on various benchmarks.<p>So are we now at performance plateau? I know people at openai like places that think AGI is likely in next 3-5 years and is mostly scaling up context&#x2F;performance&#x2F;a few other key bets away. I know others who think that is unlikely in next few decades.<p>My personal view is I would expect 100x speed up to make ML used even more broadly and to allow more companies outside big players to have there own foundation models tuned for their use cases or other specialized domain models outside language modeling. Even now I still see tabular datasets (recommender systems, pricing models, etc) as most common to work in industry jobs. As for impact 100x compute will have for leading models like openai&#x2F;anthropic I honestly have little confidence what will happen.<p>The rest of this is very speculative and not sure of, but my personal gut is we still need other algorithmic improvements like better ways to represent storing memory that models can later query&#x2F;search for, but honestly part of that is just math&#x2F;cs background in me not wanting everything to end up being hardware problem. Other part is I’m doubtful human like intelligence is so compute expensive and we can’t find more cost efficient ways for models to learn but maybe our nervous system is just much faster at parallel computation?</div><br/><div id="41151532" class="c"><input type="checkbox" id="c-41151532" checked=""/><div class="controls bullet"><span class="by">HdS84</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41150740">parent</a><span>|</span><a href="#41151294">next</a><span>|</span><label class="collapse" for="c-41151532">[-]</label><label class="expand" for="c-41151532">[1 more]</label></div><br/><div class="children"><div class="content">The human brain manages to work with 0.3 kWh per day - even if we say all of that is used for training &quot;models&quot; and for twenty years that&#x27;s only 2200kwh - much less then what chat needed to train (500mwh?). So there are obviously lots of thinks we can do to improve efficiency. On the other hand, our brains hat hundreds of millions of years to be optimized for energy consumption.</div><br/></div></div><div id="41151294" class="c"><input type="checkbox" id="c-41151294" checked=""/><div class="controls bullet"><span class="by">chii</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41150740">parent</a><span>|</span><a href="#41151532">prev</a><span>|</span><a href="#41150712">next</a><span>|</span><label class="collapse" for="c-41151294">[-]</label><label class="expand" for="c-41151294">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s some speculation that there are higher horizons to the training, as explained in this video: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Nvb_4Jj5kBo" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Nvb_4Jj5kBo</a><p>the term for it is &quot;grokking&quot;, amusingly. There&#x27;s some indication that we are actually undertraining by 10x</div><br/></div></div></div></div></div></div></div></div><div id="41150712" class="c"><input type="checkbox" id="c-41150712" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41150681">prev</a><span>|</span><a href="#41150626">next</a><span>|</span><label class="collapse" for="c-41150712">[-]</label><label class="expand" for="c-41150712">[6 more]</label></div><br/><div class="children"><div class="content">&gt; I also wonder - is the compute the main limiting factor today ?<p>So much of this is a blackbox that you have to wing it for lots of things and try stuff. The more compute you have, the more YOLO runs you can do.<p>For example, the research team I work with has about 1&#x2F;20th the compute that a Google researcher has. This gives them a massive advantage because they can afford to train lots of random ideas and see what kind of advantage they get. We have to be much, much more measured and predict our outcomes better.</div><br/><div id="41151043" class="c"><input type="checkbox" id="c-41151043" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41150712">parent</a><span>|</span><a href="#41151809">next</a><span>|</span><label class="collapse" for="c-41151043">[-]</label><label class="expand" for="c-41151043">[4 more]</label></div><br/><div class="children"><div class="content">&gt; the research team I work with has about 1&#x2F;20th the compute that a Google researcher has. This gives them a massive advantage<p>The search space could easily be so large that a less disciplined approach might yield fewer useful outcomes even with the compute advantage. Being forced to be focused and disciplined might actually be a big advantage to you.</div><br/><div id="41151438" class="c"><input type="checkbox" id="c-41151438" checked=""/><div class="controls bullet"><span class="by">jahewson</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41151043">parent</a><span>|</span><a href="#41151809">next</a><span>|</span><label class="collapse" for="c-41151438">[-]</label><label class="expand" for="c-41151438">[3 more]</label></div><br/><div class="children"><div class="content">It’s hard to be disciplined about a black box though. That’s one reason why we’re all speeding off at a thousand miles per hour on transformers - the architecture works, why try other things?</div><br/><div id="41151591" class="c"><input type="checkbox" id="c-41151591" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41151438">parent</a><span>|</span><a href="#41151809">next</a><span>|</span><label class="collapse" for="c-41151591">[-]</label><label class="expand" for="c-41151591">[2 more]</label></div><br/><div class="children"><div class="content">whoever comes up with the next trick could win big.</div><br/><div id="41151985" class="c"><input type="checkbox" id="c-41151985" checked=""/><div class="controls bullet"><span class="by">jack_pp</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41151591">parent</a><span>|</span><a href="#41151809">next</a><span>|</span><label class="collapse" for="c-41151985">[-]</label><label class="expand" for="c-41151985">[1 more]</label></div><br/><div class="children"><div class="content">Could, or meta makes you irrelevant with by reproducing your trick and giving it away for free</div><br/></div></div></div></div></div></div></div></div><div id="41151809" class="c"><input type="checkbox" id="c-41151809" checked=""/><div class="controls bullet"><span class="by">TheAlchemist</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41150712">parent</a><span>|</span><a href="#41151043">prev</a><span>|</span><a href="#41150626">next</a><span>|</span><label class="collapse" for="c-41151809">[-]</label><label class="expand" for="c-41151809">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the process of deciding what stuff you will try ?</div><br/></div></div></div></div><div id="41150626" class="c"><input type="checkbox" id="c-41150626" checked=""/><div class="controls bullet"><span class="by">cbanek</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41150712">prev</a><span>|</span><a href="#41151902">next</a><span>|</span><label class="collapse" for="c-41150626">[-]</label><label class="expand" for="c-41150626">[4 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re right.  It does seem like the models need exponentially larger datasets to get linear improvements, which are now in the realm of having to buy them from large social media companies.  The next breakthrough will probably being doing something different rather than doing it slightly better.</div><br/><div id="41151612" class="c"><input type="checkbox" id="c-41151612" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41150626">parent</a><span>|</span><a href="#41151902">next</a><span>|</span><label class="collapse" for="c-41151612">[-]</label><label class="expand" for="c-41151612">[3 more]</label></div><br/><div class="children"><div class="content">given that language timeseries itself turned out to store some form of intelligence, I wonder how much more of the human mind is trained on video-input&#x2F;proprioception&#x2F;action timeseries. i.e. make robots that make small decisions and actions, train on their experience, do more complicated actions, train on those experiences - there&#x27;s your 10x, 100x, e^x training tokens, save the language for fine tuning. language as a specific task of a general, world-interacting robot.</div><br/><div id="41152045" class="c"><input type="checkbox" id="c-41152045" checked=""/><div class="controls bullet"><span class="by">cbanek</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41151612">parent</a><span>|</span><a href="#41151777">next</a><span>|</span><label class="collapse" for="c-41152045">[-]</label><label class="expand" for="c-41152045">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I mean this already exists in the human brain, for example we&#x27;re more likely to be surprised by vision lower to the ground than in the sky.  The reason? Snakes.  And a lot of animals have a startle reaction to snakes as it&#x27;s a common problem out there in the world.</div><br/></div></div><div id="41151777" class="c"><input type="checkbox" id="c-41151777" checked=""/><div class="controls bullet"><span class="by">khimaros</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41151612">parent</a><span>|</span><a href="#41152045">prev</a><span>|</span><a href="#41151902">next</a><span>|</span><label class="collapse" for="c-41151777">[-]</label><label class="expand" for="c-41151777">[1 more]</label></div><br/><div class="children"><div class="content">apparently this is part of their plan with the Tesla Optimus robots.</div><br/></div></div></div></div></div></div><div id="41151902" class="c"><input type="checkbox" id="c-41151902" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41150626">prev</a><span>|</span><a href="#41151238">next</a><span>|</span><label class="collapse" for="c-41151902">[-]</label><label class="expand" for="c-41151902">[1 more]</label></div><br/><div class="children"><div class="content">&gt; is the compute the main limiting factor today ?<p>I think it depends on the framing of the question, especially how you define compute.<p>- No, compute is not the limiting factor. The limiting factor is poorly optimized software (there&#x27;s a joke: &quot;10 years of hardware advancements have been entirely undone by 10 years of software advancements&quot;)<p>- No, compute is not the limiting factor. The limiting factor is that the electron is too big and the speed of light is too slow.<p>If we&#x27;re talking about ML, then no, compute is not the limiting factor.<p>At least if we&#x27;re define compute as the number of FLOPs we can process and not in terms of algorithm or resultant abilities. Though I&#x27;ll admit that I&#x27;m an outlier in this respect[0]. But I think it is worth recognizing that we now have exaflop machines, that use tens of megawatts of energy, and they pale in comparison to what a 3 lb piece of fat and meat that only uses a handful of watts. In fact, our exascale computers aren&#x27;t even seemingly sufficient to simulate far smaller and far less intelligent creatures. Certainly scale is a factor (we do see this pattern in apes too), but clearly there is more. And I think it should be obvious that scale isn&#x27;t all you need, since we&#x27;re the only ones. If it was that simple, we should see it more often. And if scale is indeed all we need, well we neither do we have an idea of how much scale that actually is nor does it mean that this is the best path forward as that scale may be ludicrously large. But what we do know, is that incredible feats can be done with what would constitute a rounding error to current scales (let alone future). I think we just want to believe this is the path forward because if it is, then there is a clear direction. But if it isn&#x27;t, then we have to admit that we&#x27;re still lost. But I think the problem is that we think that there&#x27;s a problem in being lost. Or that we think that admitting we&#x27;re lost somehow undermines or rejects the progress that we have made. But research is all about exploring the unknown. If you aren&#x27;t at least a little lost, well then you&#x27;re not exploring, you&#x27;re reading a map. But the irony in this is that &quot;scale is all you need&quot; denies a lot of significant advancements we&#x27;ve made. Many smaller models perform far better that previously, and this is not due to knowledge transfer from larger models. Just look at any leaderboard, they aren&#x27;t size is not the determining factor.<p>So I&#x27;d argue that if you want to advance AI, you should focus on smaller models. After all, smaller models are far easier to scale than larger models. They&#x27;re also far easier to analyze and interpret, which is what gives us more information on how to lighten the way forward. But also don&#x27;t expect a smaller model that is more successful to immediately be better than larger models. I far too often see a mistake even by reviewers&#x2F;experts, where a method is dismissed because it was developed by some poor grad student with limited compute and did not unilaterally defeat the big models. Of course that doesn&#x27;t mean the proposed methods are better, but that&#x27;s orthogonal to what I&#x27;m arguing.<p>[0] Obviously I&#x27;m not alone. Yann is a clear believer and it&#x27;s why he&#x27;s looking at JEPA models (I don&#x27;t think this will be enough but I think it is better). And Collet became more well known (at least outside the ML research community) and is a clear dissenter.</div><br/></div></div><div id="41151238" class="c"><input type="checkbox" id="c-41151238" checked=""/><div class="controls bullet"><span class="by">maciejgryka</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41151902">prev</a><span>|</span><a href="#41151211">next</a><span>|</span><label class="collapse" for="c-41151238">[-]</label><label class="expand" for="c-41151238">[1 more]</label></div><br/><div class="children"><div class="content">Yes, compute is absolutely the limiting factor today. Not only because the space of hyperparameters is huge and having more compute would make it easier&#x2F;possible to explore. But also, weirdly, because inference becomes increasingly important for training, which means even more compute! A lot of work these days goes into getting better data and it turns out that using an existing large model to create&#x2F;curate data for you works really well.</div><br/></div></div><div id="41151211" class="c"><input type="checkbox" id="c-41151211" checked=""/><div class="controls bullet"><span class="by">vinnyvichy</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41151238">prev</a><span>|</span><a href="#41150658">next</a><span>|</span><label class="collapse" for="c-41151211">[-]</label><label class="expand" for="c-41151211">[1 more]</label></div><br/><div class="children"><div class="content">Exactly this!<p>On another thread*, I mentioned that the XC2064 FPGA design is the paradigmatic  example of an optimized minimal &quot;hardware API&quot;, that, if you were interested in making new kinds of logic and memory-- okay you are prob thinking of higher level designs, but I want to throw out a relevant memorable example ASAP<p>*<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41149882">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41149882</a></div><br/></div></div><div id="41150658" class="c"><input type="checkbox" id="c-41150658" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41151211">prev</a><span>|</span><a href="#41151480">next</a><span>|</span><label class="collapse" for="c-41150658">[-]</label><label class="expand" for="c-41150658">[1 more]</label></div><br/><div class="children"><div class="content">Main limiting factor? Hard to know.<p>But throw 100x more compute to an LLM (through an increase of both parameters and tokens) and it will undoubtedly beat current models.</div><br/></div></div><div id="41151480" class="c"><input type="checkbox" id="c-41151480" checked=""/><div class="controls bullet"><span class="by">Flomlo</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41150658">prev</a><span>|</span><a href="#41150796">next</a><span>|</span><label class="collapse" for="c-41151480">[-]</label><label class="expand" for="c-41151480">[1 more]</label></div><br/><div class="children"><div class="content">Bigger and a lot faster means a lot more research and experimentation.<p>There are probably plenty of undiscovered architectures out there which might be the next thing.<p>It is quite a problem if every experiment takes days weeks or month.</div><br/></div></div><div id="41150796" class="c"><input type="checkbox" id="c-41150796" checked=""/><div class="controls bullet"><span class="by">shmatt</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41151480">prev</a><span>|</span><a href="#41151427">next</a><span>|</span><label class="collapse" for="c-41150796">[-]</label><label class="expand" for="c-41150796">[1 more]</label></div><br/><div class="children"><div class="content">There generally would be a limit we would reach to probabilistic syllable generators. We would reach some max workable context window, minimize hallucinations, etc.<p>But still they would be extremely useful, even if very different than AGI</div><br/></div></div><div id="41151427" class="c"><input type="checkbox" id="c-41151427" checked=""/><div class="controls bullet"><span class="by">dev1ycan</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41150796">prev</a><span>|</span><a href="#41150652">next</a><span>|</span><label class="collapse" for="c-41151427">[-]</label><label class="expand" for="c-41151427">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a few videos by Sabine&#x2F;Anton and also now PBS about Microtubules and conciousness, it&#x27;s starting to gain ground and If I were an AI company right now I&#x27;d be having cold sweat.<p>Video of reference (check especially the ending summary where they calculate more or less how much would be necessary just to replicate a human mind&#x27;s way of achieving conciousness if microtubules in fact contribute towards it):<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xa2Kpkksf3k" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xa2Kpkksf3k</a></div><br/><div id="41151620" class="c"><input type="checkbox" id="c-41151620" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41150602">root</a><span>|</span><a href="#41151427">parent</a><span>|</span><a href="#41150652">next</a><span>|</span><label class="collapse" for="c-41151620">[-]</label><label class="expand" for="c-41151620">[1 more]</label></div><br/><div class="children"><div class="content">the argument for quantum consciousness is quite interesting: 
1. consciousness is really weird.<p>2. quantum is really weird.<p>3. what if the two were really connected?<p>edit: oh god, I just had a horrible thought: the universe is a simulation, and the reason consciousness requires &quot;quantum&quot; spookiness is because it&#x27;s running on a different computer and plugged in as an adhoc skill.</div><br/></div></div></div></div><div id="41150652" class="c"><input type="checkbox" id="c-41150652" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#41150602">parent</a><span>|</span><a href="#41151427">prev</a><span>|</span><a href="#41151564">next</a><span>|</span><label class="collapse" for="c-41150652">[-]</label><label class="expand" for="c-41150652">[1 more]</label></div><br/><div class="children"><div class="content">Working on it <a href="https:&#x2F;&#x2F;x.com&#x2F;adamnemecek1&#x2F;status&#x2F;1814071171127935176" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;adamnemecek1&#x2F;status&#x2F;1814071171127935176</a></div><br/></div></div></div></div><div id="41151564" class="c"><input type="checkbox" id="c-41151564" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#41150602">prev</a><span>|</span><a href="#41150655">next</a><span>|</span><label class="collapse" for="c-41151564">[-]</label><label class="expand" for="c-41151564">[3 more]</label></div><br/><div class="children"><div class="content">Just make smaller chips with more VRAM. 
Then consumer PCs could run local models much easier on top of their server GPU market.</div><br/><div id="41151606" class="c"><input type="checkbox" id="c-41151606" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#41151564">parent</a><span>|</span><a href="#41150655">next</a><span>|</span><label class="collapse" for="c-41151606">[-]</label><label class="expand" for="c-41151606">[2 more]</label></div><br/><div class="children"><div class="content">They use RAM capacity for segmentation.<p>A 48GB card costs more than 2 24GB card and an H100 costs as much as 30 2GB cards.<p>Meanwhile production costs are basically linear on top of the shared base.<p>So your comment really just says “drop your profit margin massively”.</div><br/><div id="41151834" class="c"><input type="checkbox" id="c-41151834" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#41151564">root</a><span>|</span><a href="#41151606">parent</a><span>|</span><a href="#41150655">next</a><span>|</span><label class="collapse" for="c-41151834">[-]</label><label class="expand" for="c-41151834">[1 more]</label></div><br/><div class="children"><div class="content">I agree with you. It would be good for the consumer though.</div><br/></div></div></div></div></div></div><div id="41150655" class="c"><input type="checkbox" id="c-41150655" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#41151564">prev</a><span>|</span><a href="#41151135">next</a><span>|</span><label class="collapse" for="c-41150655">[-]</label><label class="expand" for="c-41150655">[1 more]</label></div><br/><div class="children"><div class="content">The real risk is that... 3 month delay turns into 6 months easily. That said there will be some relief rally at some point into Fed cut hopes</div><br/></div></div><div id="41151135" class="c"><input type="checkbox" id="c-41151135" checked=""/><div class="controls bullet"><span class="by">breadwinner</span><span>|</span><a href="#41150655">prev</a><span>|</span><a href="#41150717">next</a><span>|</span><label class="collapse" for="c-41151135">[-]</label><label class="expand" for="c-41151135">[5 more]</label></div><br/><div class="children"><div class="content">The next big thing that will drive demand for Nvidia chips is AI Search (<a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;searchgpt-prototype&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;searchgpt-prototype&#x2F;</a>). To avoid being obsoleted Google and Microsoft Bing have to spend big on Nvidia hardware, and when Nvidia releases newer chips with lower power consumption becomes available OpenAI, Google and Microsoft will be forced to  lap it up.</div><br/><div id="41151554" class="c"><input type="checkbox" id="c-41151554" checked=""/><div class="controls bullet"><span class="by">djtango</span><span>|</span><a href="#41151135">parent</a><span>|</span><a href="#41151165">next</a><span>|</span><label class="collapse" for="c-41151554">[-]</label><label class="expand" for="c-41151554">[1 more]</label></div><br/><div class="children"><div class="content">Google has been sitting on AI search for a long time - I recall people saying it simply wasn&#x27;t cost effective to do AI powered search because the way AI costs scale. Has something changed on this front or is it just an arms race?</div><br/></div></div><div id="41151165" class="c"><input type="checkbox" id="c-41151165" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41151135">parent</a><span>|</span><a href="#41151554">prev</a><span>|</span><a href="#41150717">next</a><span>|</span><label class="collapse" for="c-41151165">[-]</label><label class="expand" for="c-41151165">[3 more]</label></div><br/><div class="children"><div class="content">Google has TPUs and Microsoft is buying AMD GPUs so I wouldn&#x27;t say they <i>have</i> to buy Nvidia.</div><br/><div id="41151531" class="c"><input type="checkbox" id="c-41151531" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#41151135">root</a><span>|</span><a href="#41151165">parent</a><span>|</span><a href="#41150717">next</a><span>|</span><label class="collapse" for="c-41151531">[-]</label><label class="expand" for="c-41151531">[2 more]</label></div><br/><div class="children"><div class="content">AMD GPUs are still mainly used only for inference.<p>Anyone who wants to use CUDA continues to be forced to buy Nvidia hardware.</div><br/><div id="41151909" class="c"><input type="checkbox" id="c-41151909" checked=""/><div class="controls bullet"><span class="by">dleink</span><span>|</span><a href="#41151135">root</a><span>|</span><a href="#41151531">parent</a><span>|</span><a href="#41150717">next</a><span>|</span><label class="collapse" for="c-41151909">[-]</label><label class="expand" for="c-41151909">[1 more]</label></div><br/><div class="children"><div class="content">How far along are efforts to run CUDA on other hardware?</div><br/></div></div></div></div></div></div></div></div><div id="41150717" class="c"><input type="checkbox" id="c-41150717" checked=""/><div class="controls bullet"><span class="by">notarealllama</span><span>|</span><a href="#41151135">prev</a><span>|</span><a href="#41150924">next</a><span>|</span><label class="collapse" for="c-41150717">[-]</label><label class="expand" for="c-41150717">[1 more]</label></div><br/><div class="children"><div class="content">I am looking forward to consumer &#x2F; producer grade Tensor Processing Units and holding off on a desktop &#x2F; server purchase until we maybe see something like this.<p>With inference tasks it&#x27;d be nice to have something not as performance heavy and instead like 24gb vram or more.<p>I know there are tricks in CUDA to allow access to system ram as a proxy but limited success in duplicating this across various setups.</div><br/></div></div><div id="41150924" class="c"><input type="checkbox" id="c-41150924" checked=""/><div class="controls bullet"><span class="by">linotype</span><span>|</span><a href="#41150717">prev</a><span>|</span><a href="#41151556">next</a><span>|</span><label class="collapse" for="c-41150924">[-]</label><label class="expand" for="c-41150924">[8 more]</label></div><br/><div class="children"><div class="content">Does anyone know when the next GPU (RTX) is expected? Seems like it’s also delayed.</div><br/><div id="41150954" class="c"><input type="checkbox" id="c-41150954" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41150924">parent</a><span>|</span><a href="#41151556">next</a><span>|</span><label class="collapse" for="c-41150954">[-]</label><label class="expand" for="c-41150954">[7 more]</label></div><br/><div class="children"><div class="content">Rumors say Q4 or Q1. It kind of doesn&#x27;t matter if no one can afford it.</div><br/><div id="41150971" class="c"><input type="checkbox" id="c-41150971" checked=""/><div class="controls bullet"><span class="by">samspenc</span><span>|</span><a href="#41150924">root</a><span>|</span><a href="#41150954">parent</a><span>|</span><a href="#41151314">next</a><span>|</span><label class="collapse" for="c-41150971">[-]</label><label class="expand" for="c-41150971">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think Nvidia cares much about the mid or lower tier of GPUs, especially consumer GPUs. Sure they manufacture them to stay competitive in that area, but I don&#x27;t think they care too much about it.<p>Their core revenue comes from - in decreasing order - really expensive data center GPUs, somewhat expensive workstation &#x2F; enterprise GPUs, and finally, expensive top-of-the-line consumer GPUs like the 3090 &#x2F; 4090 (and upcoming rumored 5090) which they can mark up higher than the lower tiers.<p>I&#x27;m sure that Nvidia knows that even though consumers &#x2F; gamers will complain, the companies and hobbyists buying them will have no choice and be willing to pay for that power and extra performance and VRAM, whether it be for creatives (3D or game creators) or AI &#x2F; ML &#x2F; LLM hobbyists trying to GenAI at home.</div><br/><div id="41151016" class="c"><input type="checkbox" id="c-41151016" checked=""/><div class="controls bullet"><span class="by">cdshn</span><span>|</span><a href="#41150924">root</a><span>|</span><a href="#41150971">parent</a><span>|</span><a href="#41151314">next</a><span>|</span><label class="collapse" for="c-41151016">[-]</label><label class="expand" for="c-41151016">[3 more]</label></div><br/><div class="children"><div class="content">But if the ex-SGI employees that started Nvidia have any sense, they&#x27;ll stick to the consumer market to ensure no upstart can build an enterprise computing empire off of gamers.</div><br/><div id="41151595" class="c"><input type="checkbox" id="c-41151595" checked=""/><div class="controls bullet"><span class="by">gregw2</span><span>|</span><a href="#41150924">root</a><span>|</span><a href="#41151016">parent</a><span>|</span><a href="#41151102">next</a><span>|</span><label class="collapse" for="c-41151595">[-]</label><label class="expand" for="c-41151595">[1 more]</label></div><br/><div class="children"><div class="content">Ex-Sun guys started it actually. The SGI guys joined later.</div><br/></div></div><div id="41151102" class="c"><input type="checkbox" id="c-41151102" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#41150924">root</a><span>|</span><a href="#41151016">parent</a><span>|</span><a href="#41151595">prev</a><span>|</span><a href="#41151314">next</a><span>|</span><label class="collapse" for="c-41151102">[-]</label><label class="expand" for="c-41151102">[1 more]</label></div><br/><div class="children"><div class="content">Just a piece of history of SGI:<p>When I went to school for Softimage in 1997 - my favorite thing was building with MetaClay -- I built an amazing Human Figure on an O2 SGI box -- and sadly I lost my dat backup years later - which I would have been able to bring into blender these days....<p>Metaclay in 1997 was unreal on SGI...<p>Now its in the Alien that is Free Blender...<p>(Softimage was[ $25,000 a license at the time](<a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;9gaeeQl.jpeg" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;9gaeeQl.jpeg</a>) - and an [SGI O2 was 15K and an Octane was 47K](<a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;aVQATfV.jpeg" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;aVQATfV.jpeg</a>)<p><a href="https:&#x2F;&#x2F;forums.sgi.sh&#x2F;index.php?threads&#x2F;sgi-hardware-and-software-price-lists-1995-1998.345&#x2F;" rel="nofollow">https:&#x2F;&#x2F;forums.sgi.sh&#x2F;index.php?threads&#x2F;sgi-hardware-and-sof...</a><p>---<p>So after going to school for 3D when it was only taught in 5 schools in the nation at the time - and you can see what a simple rig cost...<p>I later spent $1,600 on an Evans and Sutherland 32MB Full-Length ATX video card that could handle OpenGL... and run NT and Softimage...</div><br/></div></div></div></div></div></div><div id="41151314" class="c"><input type="checkbox" id="c-41151314" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#41150924">root</a><span>|</span><a href="#41150954">parent</a><span>|</span><a href="#41150971">prev</a><span>|</span><a href="#41151434">next</a><span>|</span><label class="collapse" for="c-41151314">[-]</label><label class="expand" for="c-41151314">[1 more]</label></div><br/><div class="children"><div class="content">It does matter a bit re: the affordability of the used market.</div><br/></div></div><div id="41151434" class="c"><input type="checkbox" id="c-41151434" checked=""/><div class="controls bullet"><span class="by">dev1ycan</span><span>|</span><a href="#41150924">root</a><span>|</span><a href="#41150954">parent</a><span>|</span><a href="#41151314">prev</a><span>|</span><a href="#41151556">next</a><span>|</span><label class="collapse" for="c-41151434">[-]</label><label class="expand" for="c-41151434">[1 more]</label></div><br/><div class="children"><div class="content">To be fair I think they&#x27;re delaying it primarily because any new memory increase would make it less likely that Chinese consumers buy the memory cut version of it which would inevitably be similar in performance to the generation they already have available, at least right now they have a good enough product over there that Chinese consumers can keep buying.<p>I personally (not in China) bought the 4080 Super and honestly regret the investment already, right now it&#x27;s a pretty bad time to be spending money with the upcoming recession, especially for products not good enough to run your own competitively performing local LLMs on</div><br/></div></div></div></div></div></div><div id="41151556" class="c"><input type="checkbox" id="c-41151556" checked=""/><div class="controls bullet"><span class="by">lowbloodsugar</span><span>|</span><a href="#41150924">prev</a><span>|</span><a href="#41151015">next</a><span>|</span><label class="collapse" for="c-41151556">[-]</label><label class="expand" for="c-41151556">[1 more]</label></div><br/><div class="children"><div class="content">Translation: “There’s no competition. You sops can keep paying ridiculous amounts for our old tech, and we’ll just sit on this new one until it’s needed to compete.”</div><br/></div></div><div id="41151015" class="c"><input type="checkbox" id="c-41151015" checked=""/><div class="controls bullet"><span class="by">meroes</span><span>|</span><a href="#41151556">prev</a><span>|</span><a href="#41151303">next</a><span>|</span><label class="collapse" for="c-41151015">[-]</label><label class="expand" for="c-41151015">[1 more]</label></div><br/><div class="children"><div class="content">Cover for AI winter?</div><br/></div></div><div id="41151303" class="c"><input type="checkbox" id="c-41151303" checked=""/><div class="controls bullet"><span class="by">paulproteus</span><span>|</span><a href="#41151015">prev</a><span>|</span><a href="#41150689">next</a><span>|</span><label class="collapse" for="c-41151303">[-]</label><label class="expand" for="c-41151303">[1 more]</label></div><br/><div class="children"><div class="content">What if the design flaw makes AIs unsafe? This would be a fun sci-fi piece.</div><br/></div></div><div id="41150689" class="c"><input type="checkbox" id="c-41150689" checked=""/><div class="controls bullet"><span class="by">bzmrgonz</span><span>|</span><a href="#41151303">prev</a><span>|</span><a href="#41151281">next</a><span>|</span><label class="collapse" for="c-41150689">[-]</label><label class="expand" for="c-41150689">[1 more]</label></div><br/><div class="children"><div class="content">I think they&#x27;re calling the bug or design flaw &quot;intel sabo&quot;.</div><br/></div></div><div id="41151281" class="c"><input type="checkbox" id="c-41151281" checked=""/><div class="controls bullet"><span class="by">xyst</span><span>|</span><a href="#41150689">prev</a><span>|</span><a href="#41150587">next</a><span>|</span><label class="collapse" for="c-41151281">[-]</label><label class="expand" for="c-41151281">[4 more]</label></div><br/><div class="children"><div class="content">Intel chips are defective. AMD has a delay due to late game QA concerns (or playing release games). Now NVDA “ai chips” are faulty.<p>AI bust is coming</div><br/><div id="41151444" class="c"><input type="checkbox" id="c-41151444" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#41151281">parent</a><span>|</span><a href="#41150587">next</a><span>|</span><label class="collapse" for="c-41151444">[-]</label><label class="expand" for="c-41151444">[3 more]</label></div><br/><div class="children"><div class="content">Delays? We just took delivery of 128 MI300x GPUs about a month after paying for them.<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;AMD_MI300&#x2F;comments&#x2F;1dishr2&#x2F;comment&#x2F;lajzs62&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;AMD_MI300&#x2F;comments&#x2F;1dishr2&#x2F;comment&#x2F;...</a><p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;AMD_MI300&#x2F;comments&#x2F;1eir05k&#x2F;hot_aisle_receives_their_first_shipment_of_dell&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;AMD_MI300&#x2F;comments&#x2F;1eir05k&#x2F;hot_aisl...</a></div><br/><div id="41151506" class="c"><input type="checkbox" id="c-41151506" checked=""/><div class="controls bullet"><span class="by">edward28</span><span>|</span><a href="#41151281">root</a><span>|</span><a href="#41151444">parent</a><span>|</span><a href="#41150587">next</a><span>|</span><label class="collapse" for="c-41151506">[-]</label><label class="expand" for="c-41151506">[2 more]</label></div><br/><div class="children"><div class="content">He is talking about zen5 consumer chips.</div><br/><div id="41151522" class="c"><input type="checkbox" id="c-41151522" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#41151281">root</a><span>|</span><a href="#41151506">parent</a><span>|</span><a href="#41150587">next</a><span>|</span><label class="collapse" for="c-41151522">[-]</label><label class="expand" for="c-41151522">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the clarification. I was confused because I&#x27;ve heard people say that about MI300x as well.</div><br/></div></div></div></div></div></div></div></div><div id="41150587" class="c"><input type="checkbox" id="c-41150587" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#41151281">prev</a><span>|</span><label class="collapse" for="c-41150587">[-]</label><label class="expand" for="c-41150587">[1 more]</label></div><br/><div class="children"><div class="content">Given how spooky AI is getting I&#x27;m kind of relieved. Hopefully there really is a flaw, if not then extra spooky.<p>AI-Made Bioweapons Are Washington’s Latest Security Obsession (<a href="https:&#x2F;&#x2F;archive.ph&#x2F;oROPO" rel="nofollow">https:&#x2F;&#x2F;archive.ph&#x2F;oROPO</a>)</div><br/></div></div></div></div></div></div></div></body></html>