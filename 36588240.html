<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688547651855" as="style"/><link rel="stylesheet" href="styles.css?v=1688547651855"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/openzfs/zfs/pull/13392">ZFS 2.2.0 (RC): Block Cloning merged</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>turrini</span> | <span>127 comments</span></div><br/><div><div id="36589298" class="c"><input type="checkbox" id="c-36589298" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#36591457">next</a><span>|</span><label class="collapse" for="c-36589298">[-]</label><label class="expand" for="c-36589298">[1 more]</label></div><br/><div class="children"><div class="content">Also see this issue: <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;405">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;405</a><p>&gt; It is in FreeBSD main branch now, but disabled by default just to be safe till after 14.0 released, where it will be included. Can be enabled with loader tunable there.<p>&gt; more code is needed on the ZFS side for Linux integration. A few people are looking at it AFAIK.</div><br/></div></div><div id="36591457" class="c"><input type="checkbox" id="c-36591457" checked=""/><div class="controls bullet"><span class="by">ludde</span><span>|</span><a href="#36589298">prev</a><span>|</span><a href="#36589289">next</a><span>|</span><label class="collapse" for="c-36591457">[-]</label><label class="expand" for="c-36591457">[5 more]</label></div><br/><div class="children"><div class="content">A M A Z I N G<p>Have been looking forward to this for years!<p>This is so much better than automatically doing dedup and the RAM overhead that entails.<p>Doing offline&#x2F;RAM+in memory dedup size optimizations seem like a really good optimization path. In the spirit of also paying only what you use and not the rest.<p>Edit: What&#x27;s the RAM overhead of this? Is it ~64B per 128kB deduped block or what&#x27;s the magnitude of things?</div><br/><div id="36592882" class="c"><input type="checkbox" id="c-36592882" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#36591457">parent</a><span>|</span><a href="#36589289">next</a><span>|</span><label class="collapse" for="c-36592882">[-]</label><label class="expand" for="c-36592882">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Edit: What&#x27;s the RAM overhead of this? Is it ~64B per 128kB deduped block or what&#x27;s the magnitude of things?<p>No real memory impact.  There&#x27;s a regions table that uses 128k of memory per terabyte of total storage (and may be a bit more in the future).  So for your 10 petabyte pool using deduping, you&#x27;d better have an extra gigabyte of RAM.<p>But erasing files can potentially be twice as expensive in IOPS, even if not deduped.  They try to prevent this.</div><br/><div id="36593880" class="c"><input type="checkbox" id="c-36593880" checked=""/><div class="controls bullet"><span class="by">ludde</span><span>|</span><a href="#36591457">root</a><span>|</span><a href="#36592882">parent</a><span>|</span><a href="#36589289">next</a><span>|</span><label class="collapse" for="c-36593880">[-]</label><label class="expand" for="c-36593880">[3 more]</label></div><br/><div class="children"><div class="content">If I manually deduplicate&#x2F;copy a 1GB file, how many such offset&#x2F;refcount tuples would be created in RAM? Just one for the whole file, or one per underlying 128kB block?</div><br/><div id="36596460" class="c"><input type="checkbox" id="c-36596460" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#36591457">root</a><span>|</span><a href="#36593880">parent</a><span>|</span><a href="#36594019">next</a><span>|</span><label class="collapse" for="c-36596460">[-]</label><label class="expand" for="c-36596460">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t go in ram.  You have one 64 bit refcount number for every 64MB of storage irrespective of how much is deduplicated; this is a reduction factor of 8M, or 128k per terabyte stored.<p>This largely exists so you can avoid doing extra work on free.</div><br/></div></div><div id="36594019" class="c"><input type="checkbox" id="c-36594019" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36591457">root</a><span>|</span><a href="#36593880">parent</a><span>|</span><a href="#36596460">prev</a><span>|</span><a href="#36589289">next</a><span>|</span><label class="collapse" for="c-36594019">[-]</label><label class="expand" for="c-36594019">[1 more]</label></div><br/><div class="children"><div class="content">n.b. I am not pjd, this is from memory and may be wrong.<p>The answer to that is messy, but basically, there&#x27;s a table that should be kept in memory for &quot;is there anything reflinked at all in this logical range on disk&quot;, and that covers large spans, so how many entries would depend on how contiguous your data logically was on disk; the actual precise mapping list per-vdev doesn&#x27;t need to be kept continuously in memory, just the more coarse table, so that saves you a fair bit on memory requirements.</div><br/></div></div></div></div></div></div></div></div><div id="36589289" class="c"><input type="checkbox" id="c-36589289" checked=""/><div class="controls bullet"><span class="by">lockhouse</span><span>|</span><a href="#36591457">prev</a><span>|</span><a href="#36588582">next</a><span>|</span><label class="collapse" for="c-36589289">[-]</label><label class="expand" for="c-36589289">[65 more]</label></div><br/><div class="children"><div class="content">Anyone here using ZFS in production these days?  If so what OS and implementation?  What have been your experiences or gotchas you experienced?</div><br/><div id="36589637" class="c"><input type="checkbox" id="c-36589637" checked=""/><div class="controls bullet"><span class="by">trws</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589424">next</a><span>|</span><label class="collapse" for="c-36589637">[-]</label><label class="expand" for="c-36589637">[3 more]</label></div><br/><div class="children"><div class="content">I’m not a filesystem admin, but we at LLNL use OpenZFS as the storage layer for all of our Lustre file systems in production, including using raid-z for resilience in each pool (order 100 disks each), and have for most of a decade. That combined with improvements in Lustre have taken the rate of data loss or need to clear large scale shared file systems down to nearly zero. There’s a reason we spend as many engineer hours as we do maintaining it, it’s worth it.<p>LLNL openzfs project: <a href="https:&#x2F;&#x2F;computing.llnl.gov&#x2F;projects&#x2F;openzfs" rel="nofollow noreferrer">https:&#x2F;&#x2F;computing.llnl.gov&#x2F;projects&#x2F;openzfs</a>
Old presentation from intel with info on what was one of our bigger deployments in 2016 (~50pb): <a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;dam&#x2F;www&#x2F;public&#x2F;us&#x2F;en&#x2F;documents&#x2F;presentation&#x2F;benefits-zfs-for-lustre-deployments.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;dam&#x2F;www&#x2F;public&#x2F;us&#x2F;en&#x2F;documents...</a></div><br/><div id="36593095" class="c"><input type="checkbox" id="c-36593095" checked=""/><div class="controls bullet"><span class="by">muxator</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589637">parent</a><span>|</span><a href="#36589424">next</a><span>|</span><label class="collapse" for="c-36593095">[-]</label><label class="expand" for="c-36593095">[2 more]</label></div><br/><div class="children"><div class="content">If I&#x27;m not mistaken the linux port of ZFS that later became OpenZFS started at LLNL and was a port from FreeBSD (it may have been release ~9).<p>I believe it was called ZFS On Linux or something like that.<p>Nice how things have evolved: from FreeBSD to linux and back. In my mind this has always been a very inspiring example of a public institution working for the public good.</div><br/><div id="36593350" class="c"><input type="checkbox" id="c-36593350" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36593095">parent</a><span>|</span><a href="#36589424">next</a><span>|</span><label class="collapse" for="c-36593350">[-]</label><label class="expand" for="c-36593350">[1 more]</label></div><br/><div class="children"><div class="content">FreeBSD had its own ZFS port.<p>ZoL, if my ancient memory serves, was at LLNL, not based on the FreeBSD port (if you go _very_ far back in the commit history you can see Brian rebasing against OpenSolaris revisions), but like 2 or 3 different orgs originally announced Linux ports at the same time and then all pooled together, since originally only one of the three was going to have a POSIX layer (the other two didn&#x27;t need a working POSIX filesystem layer). (I&#x27;m not actually sure how much came of this collaboration, I just remember being very amused when within the span of a week or two, three different orgs announced ports, looked at each other, and went &quot;...wait.&quot;)<p>Then for a while people developed on either the FreeBSD port, the illumos fork called OpenZFS, or the Linux port, but because (among other reasons) a bunch of development kept happening on the Linux port, it became the defacto upstream and got renamed &quot;OpenZFS&quot;, and then FreeBSD more or less got a fresh port from the OpenZFS codebase that is now what it&#x27;s based on.<p>The macOS port got a fresh sync against that codebase recently and is slowly trying to merge in, and then from there, ???</div><br/></div></div></div></div></div></div><div id="36589424" class="c"><input type="checkbox" id="c-36589424" checked=""/><div class="controls bullet"><span class="by">drewg123</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589637">prev</a><span>|</span><a href="#36597608">next</a><span>|</span><label class="collapse" for="c-36589424">[-]</label><label class="expand" for="c-36589424">[34 more]</label></div><br/><div class="children"><div class="content">We use ZFS in production for the non-content filesystems on a large, ever increasing, percentage of our Netflix Open Connect CDN nodes, replacing geom&#x27;s gmirror.  We had one gotcha (caught on a very limited set of canaries) where a buggy bootloader crashed part-way through boot, leaving the ZFS &quot;bootonce&quot; stuff in a funky state requiring manual recovery (the nodes with gmirror were fine, and fell back to the old image without fuss).   This has since been fixed.<p>Note that we <i>do not</i> use ZFS for content, since it is incompatible with efficient use of sendfile (both because there is no async handler for ZFS, so no async sendfile, and because the ARC is not integrated with the page cache, so content would require an extra memory copy to be served).</div><br/><div id="36589913" class="c"><input type="checkbox" id="c-36589913" checked=""/><div class="controls bullet"><span class="by">bakul</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589424">parent</a><span>|</span><a href="#36591252">next</a><span>|</span><label class="collapse" for="c-36589913">[-]</label><label class="expand" for="c-36589913">[23 more]</label></div><br/><div class="children"><div class="content">Is integrating ARC with the page cache a lost cause? If not, may be Netflix can fund it!</div><br/><div id="36590475" class="c"><input type="checkbox" id="c-36590475" checked=""/><div class="controls bullet"><span class="by">postmodest</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589913">parent</a><span>|</span><a href="#36591252">next</a><span>|</span><label class="collapse" for="c-36590475">[-]</label><label class="expand" for="c-36590475">[22 more]</label></div><br/><div class="children"><div class="content">I would expect page cache to be one of those platform-dependent things that prevent OpenZFS from doing that. Especially on Linux, and especially because AFAIK the Linux version has the most eyes on it.</div><br/><div id="36591491" class="c"><input type="checkbox" id="c-36591491" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36590475">parent</a><span>|</span><a href="#36593040">next</a><span>|</span><label class="collapse" for="c-36591491">[-]</label><label class="expand" for="c-36591491">[20 more]</label></div><br/><div class="children"><div class="content">My understanding, not having dug into it, is that it&#x27;s possible but just work nobody has done yet, though I&#x27;m not sure what the relevant interfaces are in the Linux kernel.<p>One thing that makes the interfaces in Linux much messier than the FreeBSD ones is that a lot of the core functionality you might like to leverage in Linux (workqueues, basically anything more complicated than just calling kmalloc, and of course any SIMD save&#x2F;restore state, to name three examples I&#x27;ve stumbled over recently) are marked EXPORT_SYMBOL_GPL or just entirely not exported in newer releases, so you get to reimplement the wheel for those, whereas on FreeBSD it&#x27;s trivial to just use their implementations of such things and shim them to the Solaris-ish interfaces the non-platform-specific code expects.<p>So that makes the Linux-specific code a lot heavier, because upstream is actively hostile.</div><br/><div id="36596742" class="c"><input type="checkbox" id="c-36596742" checked=""/><div class="controls bullet"><span class="by">5e92cb50239222b</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36591491">parent</a><span>|</span><a href="#36592266">next</a><span>|</span><label class="collapse" for="c-36596742">[-]</label><label class="expand" for="c-36596742">[2 more]</label></div><br/><div class="children"><div class="content">Sun has picked the license specifically to make the project incompatible with the Linux kernel. If you start out with hostility, what treatment do you expect in return?</div><br/><div id="36597621" class="c"><input type="checkbox" id="c-36597621" checked=""/><div class="controls bullet"><span class="by">depr</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36596742">parent</a><span>|</span><a href="#36592266">next</a><span>|</span><label class="collapse" for="c-36597621">[-]</label><label class="expand" for="c-36597621">[1 more]</label></div><br/><div class="children"><div class="content">Surely you have a source for that that isn&#x27;t someone else&#x27;s comment.</div><br/></div></div></div></div><div id="36592266" class="c"><input type="checkbox" id="c-36592266" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36591491">parent</a><span>|</span><a href="#36596742">prev</a><span>|</span><a href="#36592633">next</a><span>|</span><label class="collapse" for="c-36592266">[-]</label><label class="expand" for="c-36592266">[13 more]</label></div><br/><div class="children"><div class="content">&gt; SIMD save&#x2F;restore state<p>I wish someone would come in and convince the kernel devs that &quot;hey, if you want EXPORT_SYMBOL_GPL to have legal weight in a copyleft sense then you can&#x27;t just slap it onto interfaces for political reasons&quot;</div><br/><div id="36592459" class="c"><input type="checkbox" id="c-36592459" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592266">parent</a><span>|</span><a href="#36592525">next</a><span>|</span><label class="collapse" for="c-36592459">[-]</label><label class="expand" for="c-36592459">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think they care about it having legal weight, that ship sailed long ago when they started advocating for just slapping SYMBOL_GPL on things out of spite; I think they care about excluding people from using their software.<p>IMO Linus should stop being half-and-half about it and either mark everything SYMBOL_GPL and see how well that goes or stop this nonsense.</div><br/><div id="36596670" class="c"><input type="checkbox" id="c-36596670" checked=""/><div class="controls bullet"><span class="by">5e92cb50239222b</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592459">parent</a><span>|</span><a href="#36592580">next</a><span>|</span><label class="collapse" for="c-36596670">[-]</label><label class="expand" for="c-36596670">[1 more]</label></div><br/><div class="children"><div class="content">We see how well &quot;including people into using your software&quot; works out for pushover licenses almost every week now, FreeBSD itself being the prime example, having lost three major companies one after another from its list of users, sponsors, and developers a couple of years ago. Not that there were many to begin with. What Linus and the gang are doing have been working out great over the long term (much better than any competition, at least).</div><br/></div></div><div id="36592580" class="c"><input type="checkbox" id="c-36592580" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592459">parent</a><span>|</span><a href="#36596670">prev</a><span>|</span><a href="#36592525">next</a><span>|</span><label class="collapse" for="c-36592580">[-]</label><label class="expand" for="c-36592580">[4 more]</label></div><br/><div class="children"><div class="content">I just don&#x27;t understand why they&#x27;re so anti-ZFS. I want my data to survive, please...</div><br/><div id="36592693" class="c"><input type="checkbox" id="c-36592693" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592580">parent</a><span>|</span><a href="#36596330">next</a><span>|</span><label class="collapse" for="c-36592693">[-]</label><label class="expand" for="c-36592693">[1 more]</label></div><br/><div class="children"><div class="content">My impression is that some of the Linux kernel devs are anti-anything that&#x27;s not GPL-compatible, of any sort, regardless of the particulars.<p>Linus himself also made remarks about ZFS at one point that were pretty...hostile. [1] [2]<p>&gt; The fact is, the whole point of the GPL is that you&#x27;re being &quot;paid&quot; in terms of tit-for-tat: we give source code to you for free, but we want source code improvements back. If you don&#x27;t do that but instead say &quot;I think this is _legal_, but I&#x27;m not going to help you&quot; you certainly don&#x27;t get any help from us.<p>&gt; So things that are outside the kernel tree simply do not matter to us. They get absolutely zero attention. We simply don&#x27;t care. It&#x27;s that simple.<p>&gt;  And things that don&#x27;t do that &quot;give back&quot; have no business talking about us being assholes when we don&#x27;t care about them.<p>&gt; See?<p>Note that there&#x27;s at least one unfixed Linux kernel bug that was found by OpenZFS users, reproducible without using OpenZFS in any way, reported with a patch, and ignored. [3]<p>So &quot;not giving back&quot; is a dubious claim.<p>[1] - <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2020&#x2F;01&#x2F;linus-torvalds-zfs-statements-arent-right-heres-the-straight-dope&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2020&#x2F;01&#x2F;linus-torvalds-zfs-s...</a><p>[2] - <a href="https:&#x2F;&#x2F;www.realworldtech.com&#x2F;forum&#x2F;?threadid=189711&amp;curpostid=189959" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.realworldtech.com&#x2F;forum&#x2F;?threadid=189711&amp;curpost...</a><p>[3] - <a href="https:&#x2F;&#x2F;bugzilla.kernel.org&#x2F;show_bug.cgi?id=212295" rel="nofollow noreferrer">https:&#x2F;&#x2F;bugzilla.kernel.org&#x2F;show_bug.cgi?id=212295</a></div><br/></div></div><div id="36596330" class="c"><input type="checkbox" id="c-36596330" checked=""/><div class="controls bullet"><span class="by">zokula</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592580">parent</a><span>|</span><a href="#36592693">prev</a><span>|</span><a href="#36592525">next</a><span>|</span><label class="collapse" for="c-36596330">[-]</label><label class="expand" for="c-36596330">[2 more]</label></div><br/><div class="children"><div class="content">ZFS was made to be Linux hostile, the kernel devs are just returning the favor.</div><br/><div id="36597537" class="c"><input type="checkbox" id="c-36597537" checked=""/><div class="controls bullet"><span class="by">olgeni</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36596330">parent</a><span>|</span><a href="#36592525">next</a><span>|</span><label class="collapse" for="c-36597537">[-]</label><label class="expand" for="c-36597537">[1 more]</label></div><br/><div class="children"><div class="content">Was ifconfig made to be hostile too? :D<p>(&#x2F;s)</div><br/></div></div></div></div></div></div></div></div><div id="36592525" class="c"><input type="checkbox" id="c-36592525" checked=""/><div class="controls bullet"><span class="by">colonwqbang</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592266">parent</a><span>|</span><a href="#36592459">prev</a><span>|</span><a href="#36592633">next</a><span>|</span><label class="collapse" for="c-36592525">[-]</label><label class="expand" for="c-36592525">[6 more]</label></div><br/><div class="children"><div class="content">Why don&#x27;t you think it has legal weight? Or did you mean something else?<p>As far as know the point of EXPORT_SYMBOL_GPL was to push back on companies like Nvidia who wanted to exploit loopholes in the GPL. That seems to me like a reasonable objective.<p>Relevant Torvalds quote: <a href="https:&#x2F;&#x2F;yarchive.net&#x2F;comp&#x2F;linux&#x2F;export_symbol_gpl.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;yarchive.net&#x2F;comp&#x2F;linux&#x2F;export_symbol_gpl.html</a></div><br/><div id="36592578" class="c"><input type="checkbox" id="c-36592578" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592525">parent</a><span>|</span><a href="#36592599">next</a><span>|</span><label class="collapse" for="c-36592578">[-]</label><label class="expand" for="c-36592578">[3 more]</label></div><br/><div class="children"><div class="content">Sure, and that alone isn&#x27;t an unreasonable premise - as he says, intent matters.<p>But if you&#x27;re marking interfaces as GPL-only, or implementing taint detection that means if you use a non-SYMBOL_GPL kernel symbol which calls a GPL-only function it treats the non-SYMBOL_GPL symbol as GPL-only and blocks your linking, it gets a bit out of hand.<p>Building the kernel with certain kernel options makes modules like OpenZFS or OpenAFS not link because of that taint propagation - because things like the lockdep checker turn uninfringing calls into infringing ones.<p>Or a little while ago, there was a change which broke building on PPC because a change made a non-SYMBOL_GPL call on POWER into a SYMBOL_GPL one indirectly, and when the original author was contacted, he sent a patch reverting the changed symbol, and GregKH refused to pull it into stable, suggesting distros could carry it if they wanted to. (Of course, he had happily merged a change into -stable earlier that just implemented more aggressive GPL tainting and thereby broke things like the aforementioned...)</div><br/><div id="36592646" class="c"><input type="checkbox" id="c-36592646" checked=""/><div class="controls bullet"><span class="by">PlutoIsAPlanet</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592578">parent</a><span>|</span><a href="#36592599">next</a><span>|</span><label class="collapse" for="c-36592646">[-]</label><label class="expand" for="c-36592646">[2 more]</label></div><br/><div class="children"><div class="content">The Linux kernel has never supported out of tree modules like how ZFS works out of tree.<p>All ZFS needs to do is just have one of Oracles many lawyers say &quot;CDDL is compatible with GPL&quot;. Yet, they Oracle don&#x27;t.</div><br/><div id="36592746" class="c"><input type="checkbox" id="c-36592746" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592646">parent</a><span>|</span><a href="#36592599">next</a><span>|</span><label class="collapse" for="c-36592746">[-]</label><label class="expand" for="c-36592746">[1 more]</label></div><br/><div class="children"><div class="content">&quot;All.&quot;<p>It&#x27;s explicitly not compatible with GPL, though. It has clauses that are more restrictive than GPL, and IIRC some people who contributed to the OpenZFS project did so explicitly without allowing later CDDL license revisions, which removes Oracle&#x27;s ability to say CDDL-2 or whatever is GPL-compatible.<p>So even if someone rolled up dumptrucks of cash and convinced Oracle that  everything was great, they don&#x27;t have all the control needed to do that.</div><br/></div></div></div></div></div></div><div id="36592599" class="c"><input type="checkbox" id="c-36592599" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592525">parent</a><span>|</span><a href="#36592578">prev</a><span>|</span><a href="#36592633">next</a><span>|</span><label class="collapse" for="c-36592599">[-]</label><label class="expand" for="c-36592599">[2 more]</label></div><br/><div class="children"><div class="content">To have legal weight, it has to be a signal that you&#x27;re implementing something that is derivative of kernel code.  That&#x27;s the directly stated intent of EXPORT_SYMBOL_GPL.<p>But &quot;call an opaque function that saves SIMD state&quot; is obviously not derivative of the kernel code in any way.  The more exports that get badly marked this way, the more EXPORT_SYMBOL_GPL becomes indistinguishable from EXPORT_SYMBOL.</div><br/><div id="36592979" class="c"><input type="checkbox" id="c-36592979" checked=""/><div class="controls bullet"><span class="by">colonwqbang</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592599">parent</a><span>|</span><a href="#36592633">next</a><span>|</span><label class="collapse" for="c-36592979">[-]</label><label class="expand" for="c-36592979">[1 more]</label></div><br/><div class="children"><div class="content">I see it as just a kind of &quot;warranty void if seal broken&quot;. Don&#x27;t do this or you <i>may</i> be in violation of the GPL. Maybe a legal court in $country would find in your favour (I&#x27;m not convinced it&#x27;s as clear cut as you imply). Maybe they would find that you willfully infringed, despite the kernel devs clearly warning you not to do it.<p>The main &quot;legal effect&quot; I see is that you are not willing to take that risk, just like Oracle isn&#x27;t.</div><br/></div></div></div></div></div></div></div></div><div id="36592633" class="c"><input type="checkbox" id="c-36592633" checked=""/><div class="controls bullet"><span class="by">bakul</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36591491">parent</a><span>|</span><a href="#36592266">prev</a><span>|</span><a href="#36593040">next</a><span>|</span><label class="collapse" for="c-36592633">[-]</label><label class="expand" for="c-36592633">[4 more]</label></div><br/><div class="children"><div class="content">I suspect the underlying issues for not unifying the two have more to do with the ZFS design than anything to do with Linux. It may be the codebase is far too large at this stage to make such a fundamental change.</div><br/><div id="36592933" class="c"><input type="checkbox" id="c-36592933" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592633">parent</a><span>|</span><a href="#36593040">next</a><span>|</span><label class="collapse" for="c-36592933">[-]</label><label class="expand" for="c-36592933">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think so. The memory management stuff is pretty well abstracted; on FBSD it just glues into UMA pretty transparently, it&#x27;s just on Linux there&#x27;s a lot of machinery for implementing our own little cache allocating because Linux&#x27;s kernel cache allocator is very limited in what sizes it will give you, and sometimes ZFS wants 16M (not necessarily contiguous) regions because someone said they wanted 16M records.<p>The ZoL project lead said at one point there were a variety of reasons this wasn&#x27;t initially done for the Linux integration [1], but that it was worth taking another look at since that was a decade ago now. Having looked at the Linux memory subsystems recently for various reasons, I would suspect the limiting factor is that almost all the Linux memory management functions that involve details beyond &quot;give me X pages&quot; are SYMBOL_GPL, so I suspect we couldn&#x27;t access whatever functionality would be needed to do this.<p>I could be wrong, though, as I wasn&#x27;t looking at the code for that specific purpose, so I might have missed functionality that would provide this.<p>[1] - <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;10255#issuecomment-620886713">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;10255#issuecomment-620...</a></div><br/><div id="36593134" class="c"><input type="checkbox" id="c-36593134" checked=""/><div class="controls bullet"><span class="by">bakul</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592933">parent</a><span>|</span><a href="#36593040">next</a><span>|</span><label class="collapse" for="c-36593134">[-]</label><label class="expand" for="c-36593134">[2 more]</label></div><br/><div class="children"><div class="content">Behlendorf&#x27;s comment in that thread seems to be talking about linux integration. My point was this is an older issue, going back to the Sun days. See for instance this thread in where McVoy complains about the same issue! <a href="https:&#x2F;&#x2F;www.tuhs.org&#x2F;pipermail&#x2F;tuhs&#x2F;2021-February&#x2F;023013.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tuhs.org&#x2F;pipermail&#x2F;tuhs&#x2F;2021-February&#x2F;023013.htm...</a></div><br/><div id="36593277" class="c"><input type="checkbox" id="c-36593277" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36593134">parent</a><span>|</span><a href="#36593040">next</a><span>|</span><label class="collapse" for="c-36593277">[-]</label><label class="expand" for="c-36593277">[1 more]</label></div><br/><div class="children"><div class="content">That seems more like it&#x27;s complaining about it not being the actual page cache, not it not being counted as &quot;cache&quot;, which is a larger set in at least Linux than just the page cache itself.<p>But sure, it&#x27;s certainly an older issue, and given that the ABD rework happened, I wouldn&#x27;t put anything past being &quot;feasible&quot; if the benefits were great enough.<p>(Look at the O_DIRECT zvol rework stuff that&#x27;s pending (I believe not merged) for how a more cut-through memory model could be done, though that has all the tradeoffs you might expect of skipping the abstractions ZFS uses to minimize the ability of applications to poke holes in the abstraction model and violate consistency, I believe...)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36593040" class="c"><input type="checkbox" id="c-36593040" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36590475">parent</a><span>|</span><a href="#36591491">prev</a><span>|</span><a href="#36591252">next</a><span>|</span><label class="collapse" for="c-36593040">[-]</label><label class="expand" for="c-36593040">[1 more]</label></div><br/><div class="children"><div class="content">Could the linux integration use dax[0] to bypass the page cache and go straight to ARC?<p>[0] <a href="https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;filesystems&#x2F;dax.txt" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;filesystems&#x2F;dax.txt</a></div><br/></div></div></div></div></div></div><div id="36591252" class="c"><input type="checkbox" id="c-36591252" checked=""/><div class="controls bullet"><span class="by">xmodem</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589424">parent</a><span>|</span><a href="#36589913">prev</a><span>|</span><a href="#36589947">next</a><span>|</span><label class="collapse" for="c-36591252">[-]</label><label class="expand" for="c-36591252">[1 more]</label></div><br/><div class="children"><div class="content">If you can share, what type of non-content data do the nodes store? Is this just OS+application+logs?</div><br/></div></div><div id="36589947" class="c"><input type="checkbox" id="c-36589947" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589424">parent</a><span>|</span><a href="#36591252">prev</a><span>|</span><a href="#36590147">next</a><span>|</span><label class="collapse" for="c-36589947">[-]</label><label class="expand" for="c-36589947">[5 more]</label></div><br/><div class="children"><div class="content">This is amazing. A detail of Netflix that, I a plebe, wouldn’t know if not for this site.</div><br/><div id="36590126" class="c"><input type="checkbox" id="c-36590126" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589947">parent</a><span>|</span><a href="#36590147">next</a><span>|</span><label class="collapse" for="c-36590126">[-]</label><label class="expand" for="c-36590126">[4 more]</label></div><br/><div class="children"><div class="content">Actually, Drew&#x27;s presentations about Netflix, FreeBSD, ZFS, saturating high-bandwidth network adapters, etc. are legendary and have been posted far and wide. But having him available to answer questions on HN just takes it to a whole &#x27;nother level.</div><br/><div id="36591051" class="c"><input type="checkbox" id="c-36591051" checked=""/><div class="controls bullet"><span class="by">drewg123</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36590126">parent</a><span>|</span><a href="#36590147">next</a><span>|</span><label class="collapse" for="c-36591051">[-]</label><label class="expand" for="c-36591051">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re making me blush..  But, to set the record straight: I actually know very little about ZFS, beyond basic user&#x2F;admin knowledge (from having run it for ~15 years). I&#x27;ve never spoken about it, and other members of the team I work for at Netflix are far more knowledgeable about ZFS, and are the ones who have managed the conversion of our fleet to ZFS for non-content partitions.</div><br/><div id="36592217" class="c"><input type="checkbox" id="c-36592217" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36591051">parent</a><span>|</span><a href="#36591892">next</a><span>|</span><label class="collapse" for="c-36592217">[-]</label><label class="expand" for="c-36592217">[1 more]</label></div><br/><div class="children"><div class="content">I’ve devoured your FreeBSD networking presentations but I guess I must have confused a post about tracking down a ZFS bug in production written by someone else with all the other content you’ve produced.<p>Back to the topic at hand, it’s actually scary how few software expose control over whether or not sendfile is used, assuming support is only a matter of OS and kernel version but not taking into account filesystem limitations. I ran into a terrible Samba on FreeBSD bug (shares remotely disconnected and connections reset with moderate levels of concurrent ro access from even a single client) that I ultimately tracked down to sendfile being enabled in the (default?) config - so it wasn’t just the expected “performance requirements not being met” with sendfile on ZFS but even other reliability issues (almost certainly exposing a different underlying bug, tbh). Imagine if Samba didn’t have a tubeable to set&#x2F;override sendfile support, though.</div><br/></div></div><div id="36591892" class="c"><input type="checkbox" id="c-36591892" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36591051">parent</a><span>|</span><a href="#36592217">prev</a><span>|</span><a href="#36590147">next</a><span>|</span><label class="collapse" for="c-36591892">[-]</label><label class="expand" for="c-36591892">[1 more]</label></div><br/><div class="children"><div class="content">Have they ever blogged or spoke at conferences about it? I soak up all that content -- least I try to.</div><br/></div></div></div></div></div></div></div></div><div id="36590147" class="c"><input type="checkbox" id="c-36590147" checked=""/><div class="controls bullet"><span class="by">throw0101c</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589424">parent</a><span>|</span><a href="#36589947">prev</a><span>|</span><a href="#36589580">next</a><span>|</span><label class="collapse" for="c-36590147">[-]</label><label class="expand" for="c-36590147">[2 more]</label></div><br/><div class="children"><div class="content">Any use of boot environments for easy(er?) rollbacks of OS updates?</div><br/><div id="36590989" class="c"><input type="checkbox" id="c-36590989" checked=""/><div class="controls bullet"><span class="by">drewg123</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36590147">parent</a><span>|</span><a href="#36589580">next</a><span>|</span><label class="collapse" for="c-36590989">[-]</label><label class="expand" for="c-36590989">[1 more]</label></div><br/><div class="children"><div class="content">Yes.  That&#x27;s the bootonce thing I was talking about.  When we update the OS, we set the &quot;bootonce&quot; flag via bectl activate -t to ensure we fall back to the previous BE if the current BE is borked and not bootable.   This is the same functionality we had by keeping a primary and secondary root partition in geom a and toggling the bootable partition via the bootonce flag in gpart.</div><br/></div></div></div></div><div id="36589580" class="c"><input type="checkbox" id="c-36589580" checked=""/><div class="controls bullet"><span class="by">lifty</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589424">parent</a><span>|</span><a href="#36590147">prev</a><span>|</span><a href="#36597608">next</a><span>|</span><label class="collapse" for="c-36589580">[-]</label><label class="expand" for="c-36589580">[2 more]</label></div><br/><div class="children"><div class="content">what do you use for the content filesystem?</div><br/><div id="36589685" class="c"><input type="checkbox" id="c-36589685" checked=""/><div class="controls bullet"><span class="by">drewg123</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589580">parent</a><span>|</span><a href="#36597608">next</a><span>|</span><label class="collapse" for="c-36589685">[-]</label><label class="expand" for="c-36589685">[1 more]</label></div><br/><div class="children"><div class="content">FreeBSD&#x27;s UFS</div><br/></div></div></div></div></div></div><div id="36597608" class="c"><input type="checkbox" id="c-36597608" checked=""/><div class="controls bullet"><span class="by">opk</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589424">prev</a><span>|</span><a href="#36597542">next</a><span>|</span><label class="collapse" for="c-36597608">[-]</label><label class="expand" for="c-36597608">[1 more]</label></div><br/><div class="children"><div class="content">My workplace has been using ZFS for years. I&#x27;ve yet to see an alternative come close to being compelling. We use OpenZFS on all our RHEL servers and even workstations with XFS for root partitions and smaller clients. Our primary file server is Solaris 11 but is coming to the end of its long life. The plan is to use FreeBSD on the replacement. ZFS has become so valuable to us that it alone can be a major factor in directing our other choices.<p>Is nice to see it advancing in useful ways. Hopefully this leads to offline dedup without the runtime memory costs.</div><br/></div></div><div id="36597542" class="c"><input type="checkbox" id="c-36597542" checked=""/><div class="controls bullet"><span class="by">mnw21cam</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36597608">prev</a><span>|</span><a href="#36596476">next</a><span>|</span><label class="collapse" for="c-36597542">[-]</label><label class="expand" for="c-36597542">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, a few years ago I steered our university research group towards using ZFS for a couple of new servers with 1.5PB storage each (2.0PB before raidz2), on RHEL (would have preferred Debian). Absolutely no problems with the system. The IT guys were a bit doubtful whether it would work well, for instance they insisted that XFS would be better at managing parallel requests, but when the system happily saturates a 10Gb ethernet with plenty of bandwidth to spare I don&#x27;t think there&#x27;s a problem. We have had ZFS flag up one hard drive so far that was returning false data, which was then hot-swapped out. Other filesystems would probably have just suffered data corruption without us knowing it.</div><br/></div></div><div id="36596476" class="c"><input type="checkbox" id="c-36596476" checked=""/><div class="controls bullet"><span class="by">mgoetzke</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36597542">prev</a><span>|</span><a href="#36589408">next</a><span>|</span><label class="collapse" for="c-36596476">[-]</label><label class="expand" for="c-36596476">[1 more]</label></div><br/><div class="children"><div class="content">We use it with SmartOS on hypervisors running customer Bhyve machines. ZFS (and smartos) works really really well. 
.<p>The fact how I can replicate live data of arbitrary size (many TB sized filesystems) in small chunks to other hosts every minute greatly increased my deep sleep quality. Of course databases with multi-host write etc are nice, but in our use case, all customers are rather small with just lots and lots of files (medical and otherwise), the database itself is rather small and doesn&#x27;t need replication.<p>Best thing, on the receiver side of the backup ZFS ensures due to its architecture that the diff is directly applied on top of the existing filesystem, while in normal differential backups one might find out months or years later that one diff snapshot was damaged in transfer or is not accessible.<p>zfs scrubbing with S.M.A.R.T monitoring also helps a lot to ensure drive quality over time.<p># Gotchas<p>ZFS:<p>- There is no undo etc, this is unix, so beware of wrong commands.
- ZFS Scrubbing can be stopped (it does sometimes affect io speed), but ZFS resilvering cannot. This can lead to performance issues. 
- There must be enough RAM for the caching to work well and synchronous workloads do well with good write cache drives (ZIL)
- Data usage patterns should fit well with the Append Log schema of ZFS. E.g databases such as LevelDB worked really well. Others are not slow, but need a good ZIL more then when the pattern fits.<p>SmartOS: Some minor gotchas with how vmadm deletes zfs filesystems or in general with SmartOS, e.g when having too many snapshots, but everything quite predictable.</div><br/></div></div><div id="36589408" class="c"><input type="checkbox" id="c-36589408" checked=""/><div class="controls bullet"><span class="by">noaheverett</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36596476">prev</a><span>|</span><a href="#36590085">next</a><span>|</span><label class="collapse" for="c-36589408">[-]</label><label class="expand" for="c-36589408">[4 more]</label></div><br/><div class="children"><div class="content">Running in production for about 3 years with Ubuntu 20.04 &#x2F; zfs 0.8.3. ZFS is being used as the datastore for a cluster of LXD&#x2F;LXC instances over multiple physical hosts. I have the OS setup on its own dedicated drive and ZFS striped&#x2F;cloned over 4 NVMe drives.<p>No gotchas &#x2F; issues, works well, easy to setup.<p>I am looking forward to the Direct IO speed improvements for NVMe drives with <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;pull&#x2F;10018">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;pull&#x2F;10018</a><p>edit: one thing I forgot to mention is, when creating your pool make sure to import your drives by ID (zpool import -d &#x2F;dev&#x2F;disk&#x2F;by-id&#x2F; &lt;poolname&gt;) instead of name in case name assignments change somehow [1]<p>[1] <a href="https:&#x2F;&#x2F;superuser.com&#x2F;questions&#x2F;1732532&#x2F;zfs-disk-drive-letter-changed-how-to-reimport-by-id" rel="nofollow noreferrer">https:&#x2F;&#x2F;superuser.com&#x2F;questions&#x2F;1732532&#x2F;zfs-disk-drive-lette...</a></div><br/><div id="36590163" class="c"><input type="checkbox" id="c-36590163" checked=""/><div class="controls bullet"><span class="by">throw0101c</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589408">parent</a><span>|</span><a href="#36594160">next</a><span>|</span><label class="collapse" for="c-36590163">[-]</label><label class="expand" for="c-36590163">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I am looking forward to the Direct IO speed improvements for NVMe drives with <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;pull&#x2F;10018">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;pull&#x2F;10018</a></i><p>See also &quot;Scaling ZFS for NVMe&quot; by Allan Jude at EuroBSDcon 2022:<p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=v8sl8gj9UnA">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=v8sl8gj9UnA</a></div><br/><div id="36590967" class="c"><input type="checkbox" id="c-36590967" checked=""/><div class="controls bullet"><span class="by">noaheverett</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36590163">parent</a><span>|</span><a href="#36594160">next</a><span>|</span><label class="collapse" for="c-36590967">[-]</label><label class="expand" for="c-36590967">[1 more]</label></div><br/><div class="children"><div class="content">Sweet, I appreciate the link!</div><br/></div></div></div></div><div id="36594160" class="c"><input type="checkbox" id="c-36594160" checked=""/><div class="controls bullet"><span class="by">doctor_eval</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589408">parent</a><span>|</span><a href="#36590163">prev</a><span>|</span><a href="#36590085">next</a><span>|</span><label class="collapse" for="c-36594160">[-]</label><label class="expand" for="c-36594160">[1 more]</label></div><br/><div class="children"><div class="content">Sorry if this is a newbie question - but what are you using to share the ZFS file system between physical hosts?<p>I’ve been out of this particular game for a long time.</div><br/></div></div></div></div><div id="36590085" class="c"><input type="checkbox" id="c-36590085" checked=""/><div class="controls bullet"><span class="by">Drybones</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589408">prev</a><span>|</span><a href="#36593143">next</a><span>|</span><label class="collapse" for="c-36590085">[-]</label><label class="expand" for="c-36590085">[2 more]</label></div><br/><div class="children"><div class="content">We use ZFS on every server we deploy<p>We typically use Proxmox. It’s a convenient node host setup and usually has a very up to date zfs and it’s stable<p>I just wouldn’t use the Proxmox web ui for zfs configuration. It doesn’t have up to date options. Always configure zfs on the cli</div><br/><div id="36597247" class="c"><input type="checkbox" id="c-36597247" checked=""/><div class="controls bullet"><span class="by">tlamponi</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36590085">parent</a><span>|</span><a href="#36593143">next</a><span>|</span><label class="collapse" for="c-36597247">[-]</label><label class="expand" for="c-36597247">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I just wouldn’t use the Proxmox web ui for zfs configuration. It doesn’t have up to date options.<p>What options are missing for you?<p>Would be great if you could open an enhancement request over at <a href="https:&#x2F;&#x2F;bugzilla.proxmox.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;bugzilla.proxmox.com&#x2F;</a> for tracking this (no promises on (immediate) implementation though).</div><br/></div></div></div></div><div id="36593143" class="c"><input type="checkbox" id="c-36593143" checked=""/><div class="controls bullet"><span class="by">benlivengood</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36590085">prev</a><span>|</span><a href="#36596326">next</a><span>|</span><label class="collapse" for="c-36593143">[-]</label><label class="expand" for="c-36593143">[1 more]</label></div><br/><div class="children"><div class="content">&quot;production&quot; at home on Debian 11, previously on FreeBSD 10-13.  The weirdest gotcha has been related to sending encrypted raw snapshots to remote machines[0],[1].   These have been the first instabilities I had with ZFS in roughly 15 years around the filesystem since switching to native encryption this year.  Native encryption seems to be barely stable for production use; no actual data corruption but automatic synchronization (I use znapzend) was breaking frequently.  Recent kernel updates fixed my problem although some of the bug reports are still open.  I only moved on from FreeBSD because of more familiarity with Linux.<p>A slightly annoying property of snapshots and clones is the inability to fully re-root a tree of snapshots, e.g. permanently split a clone from its original source and allow first-class send&#x2F;receive from that clone.  The snapshot which originated the clone needs to stick around forever[2].  This prevents a typical virtual machine imagine process of keeping a base image up to date over time that VMs can be cloned from when desired and eventually removing the storage used by the original base image after e.g. several OS upgrades.<p>I don&#x27;t have any big performance requirements and most file storage is throughput based on spinning disks which can easily saturate the gigabit network.<p>I also use ZFS on my laptop&#x27;s SSD under Ubuntu with about 1GB&#x2F;s performance and no shortage of IOPS and the ability to send snapshots off to the backup system which is pretty nice.  Ubuntu is going backwards on support for ZFS and native encryption uses a hacky intermediate key under LUKS, but it works.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;12014">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;12014</a>
[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;12594">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;12594</a>
[2]<a href="https:&#x2F;&#x2F;serverfault.com&#x2F;questions&#x2F;265779&#x2F;split-a-zfs-clone" rel="nofollow noreferrer">https:&#x2F;&#x2F;serverfault.com&#x2F;questions&#x2F;265779&#x2F;split-a-zfs-clone</a></div><br/></div></div><div id="36596326" class="c"><input type="checkbox" id="c-36596326" checked=""/><div class="controls bullet"><span class="by">Datagenerator</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36593143">prev</a><span>|</span><a href="#36589446">next</a><span>|</span><label class="collapse" for="c-36596326">[-]</label><label class="expand" for="c-36596326">[1 more]</label></div><br/><div class="children"><div class="content">In production since the first Release of FreeBSD in our Research company. We layered our storage to disconnect hardware Lifecycles from OS. The spinning disks are pooled together using ScaleIO and provided to FreeBSD on ESX as physical devices which live forever through each EOL cycle of the hardware every five up to eight years. We have been serving ten PetaBytes for 15 years without hiccups. We expose SMB3&#x2F;NFS4 and S3 and have nightly DR to mirror sites using ZFS binary difference Send and Receive cronjobs which are fully automated and monitored. Basically the ScaleIO SDS servers can be replaced during daytime while the upper layer keeps going on for HPC and thousands of other jobs. Since hardware provides more capacity after each Lifecycle we can increase the ZFS pool on the fly and have the most profound 24&#x2F;7 storage environment without any major interruptions for 15 YEARS and counting. Our partners are having multiple outages due Lustre and other problems, my 2c</div><br/></div></div><div id="36589446" class="c"><input type="checkbox" id="c-36589446" checked=""/><div class="controls bullet"><span class="by">nightfly</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36596326">prev</a><span>|</span><a href="#36592794">next</a><span>|</span><label class="collapse" for="c-36589446">[-]</label><label class="expand" for="c-36589446">[1 more]</label></div><br/><div class="children"><div class="content">Yes, Ubuntu 20.04 and 22.04. But we&#x27;ve been running ZFS in some form or other for 10+ years. ACL support not as good&#x2F;easy to use as Solaris&#x2F;FreeBSD. Not having weird pathological performance issues with kernel memory allocation like we had with FreeBSD though. Sometimes we have issues with automatic pool import on boot, so that&#x27;s something to be careful with. The tooling is great though, and we&#x27;ve never had catastrophic failure that was due to ZFS, only due to failing hardware.</div><br/></div></div><div id="36592794" class="c"><input type="checkbox" id="c-36592794" checked=""/><div class="controls bullet"><span class="by">enneff</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589446">prev</a><span>|</span><a href="#36590416">next</a><span>|</span><label class="collapse" for="c-36592794">[-]</label><label class="expand" for="c-36592794">[3 more]</label></div><br/><div class="children"><div class="content">I use ZFS on Debian for my home file server. The setup is just a tiny NUC with a couple of large USB hard drives, mirrored with ZFS. I’ve had drives fail and painlessly replaced and resilvered them. This is easily the most hassle free file storage setup I’ve owned; been going strong over 10 years now with little to no maintenance.</div><br/><div id="36594424" class="c"><input type="checkbox" id="c-36594424" checked=""/><div class="controls bullet"><span class="by">rsaxvc</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36592794">parent</a><span>|</span><a href="#36594303">prev</a><span>|</span><a href="#36590416">next</a><span>|</span><label class="collapse" for="c-36594424">[-]</label><label class="expand" for="c-36594424">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much same setup here except I used a Lacie atom NAS, added a USB stick to the motherboard for booting.</div><br/></div></div></div></div><div id="36590416" class="c"><input type="checkbox" id="c-36590416" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36592794">prev</a><span>|</span><a href="#36589984">next</a><span>|</span><label class="collapse" for="c-36590416">[-]</label><label class="expand" for="c-36590416">[2 more]</label></div><br/><div class="children"><div class="content">TrueNAS (www.truenas.com) uses ZFS for the storage layer across it&#x27;s product range (storage software).<p>They have both FreeBSD and Linux based stuff, targeting different use cases.</div><br/><div id="36595855" class="c"><input type="checkbox" id="c-36595855" checked=""/><div class="controls bullet"><span class="by">explorer83</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36590416">parent</a><span>|</span><a href="#36589984">next</a><span>|</span><label class="collapse" for="c-36595855">[-]</label><label class="expand" for="c-36595855">[1 more]</label></div><br/><div class="children"><div class="content">I use TrueNAS (FreeBSD) version. ZFS makes keeping data snapshots and external backups a fairly easy process. It really cut down on server data management time. The only issue I found with it is there was a now long patched SMB bug a few years ago in the TrueNAS software that caused my storage pool to become corrupted after a large transfer. And there was no way for me to recover the data. It was all lost despite the disks still spinning and presumably still having most and the 1 and 0s recoverable.</div><br/></div></div></div></div><div id="36589984" class="c"><input type="checkbox" id="c-36589984" checked=""/><div class="controls bullet"><span class="by">crest</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36590416">prev</a><span>|</span><a href="#36589769">next</a><span>|</span><label class="collapse" for="c-36589984">[-]</label><label class="expand" for="c-36589984">[1 more]</label></div><br/><div class="children"><div class="content">I use it as my default file system on FreeBSD. It was rough in FreeBSD 7.x (around 2009), but starting with FreeBSD 8.x it has been rock solid to this day. The only gotcha (which the documentation warns about) has been that automatic block level deduplication is only useful in a few special applications and has a large main memory overhead unless you can accept terrible performance for normal operations (e.g. a bandwidth limited offsite backup).</div><br/></div></div><div id="36589769" class="c"><input type="checkbox" id="c-36589769" checked=""/><div class="controls bullet"><span class="by">SkyMarshal</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589984">prev</a><span>|</span><a href="#36589844">next</a><span>|</span><label class="collapse" for="c-36589769">[-]</label><label class="expand" for="c-36589769">[1 more]</label></div><br/><div class="children"><div class="content">Not in production, but using ZoL on my personal workstations.  <a href="https:&#x2F;&#x2F;zfsonlinux.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;zfsonlinux.org&#x2F;</a><p>Some discussion: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;NixOS&#x2F;comments&#x2F;ops0n0&#x2F;big_shoutout_to_the_nixos_team_for_the_excellent&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;NixOS&#x2F;comments&#x2F;ops0n0&#x2F;big_shoutout_...</a></div><br/></div></div><div id="36589844" class="c"><input type="checkbox" id="c-36589844" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589769">prev</a><span>|</span><a href="#36593996">next</a><span>|</span><label class="collapse" for="c-36589844">[-]</label><label class="expand" for="c-36589844">[2 more]</label></div><br/><div class="children"><div class="content">The gotcha on Proxmox is that you can&#x27;t do swapfiles on ZFS, so if your swap isn&#x27;t made big enough when installing and you format everything as ZFS you have to live with it or do odd workarounds.</div><br/><div id="36597318" class="c"><input type="checkbox" id="c-36597318" checked=""/><div class="controls bullet"><span class="by">tlamponi</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36589844">parent</a><span>|</span><a href="#36593996">next</a><span>|</span><label class="collapse" for="c-36597318">[-]</label><label class="expand" for="c-36597318">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a gotcha on Proxmox projects but a gotcha everywhere, SWAP on ZFS needs a lot of special care to avoid that it needs to allocate memory during swapping memory out (e.g., if low on memory), causing hard-freezes.<p>It can be made somewhat[0] stable with using no (or a simple compression like ZLE), avoiding log devices and caches it can work, but it&#x27;s way simpler and guaranteed stable to just use a separate partition.<p>[0] even setting the myriad of options there still exist reports about hangs, like <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;7734#issuecomment-416706195">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;7734#issuecomment-4167...</a></div><br/></div></div></div></div><div id="36593996" class="c"><input type="checkbox" id="c-36593996" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589844">prev</a><span>|</span><a href="#36591770">next</a><span>|</span><label class="collapse" for="c-36593996">[-]</label><label class="expand" for="c-36593996">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t use ZFS as a filesystem, we use it to aggregate multiple EBS into a single pool, and present it to... stock Ubuntu (one found in AWS marketplace) as a single big block device.<p>We also use ZFS for testing of our own block device.  My biggest problems with it are related to this activity.  Hot-plugging or removing block devices from the pool often leads to unrepeatable pool, and the tests have to be scrambled because the whole system needs to be rebooted.</div><br/></div></div><div id="36591770" class="c"><input type="checkbox" id="c-36591770" checked=""/><div class="controls bullet"><span class="by">DvdGiessen</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36593996">prev</a><span>|</span><a href="#36594883">next</a><span>|</span><label class="collapse" for="c-36591770">[-]</label><label class="expand" for="c-36591770">[2 more]</label></div><br/><div class="children"><div class="content">In production on SmartOS (illumos) servers running applications and VM&#x27;s, on TrueNAS and plain FreeBSD for various storage and backups, and on a few Linux-based workstations. Using mirrors and raidz2 depending on the needs of the machines.<p>We&#x27;ve successfully survived numerous disk failures (a broken batch of HDD&#x27;s giving all kinds of small read errors, an SSD that completely failed and disappeared, etc), and were in most cases able to replace them without a second of downtime (would have been all cases if not for disks placed in hard-to-reach places, now only a few minutes downtime to physically swap the disk).<p>Snapshots work perfectly as well. Systems are set up to automatically make snapshots using [1], on boot, on a timer, and right before potentially dangerous operations such as package manager commands as well. I&#x27;ve rolled back after botched OS updates without problems; after a reboot the machine was back in it&#x27;s old state. Also rolled back a live system a few times after a broken package update, restoring the filesystem state without any issues. Easily accessing old versions of a file is an added bonus which has been helpful a few times.<p>Send&#x2F;receive is ideal for backups. We are able to send snapshots between machines, even across different OSes, without issues. We&#x27;ve also moved entire pools from one OS to another without problems.<p>Knowing we have automatic snapshots and external backups configured also allows me to be very liberal with giving root access to inexperienced people to various (non-critical) machines, knowing that if anything breaks it will always be easy to roll back, and encouraging them to learn by experimenting a bit, to the point where we can even diff between snapshots to inspect what changed and learn from that.<p>Biggest gotchas so far have been on my personal Arch Linux setup, where the out-of-tree nature of ZFS has caused some issues like a incompatible kernel being installed, the ZFS module failing to compile, and my workstation subsequently being unable to boot. But even that was solved by my entire system running on ZFS: a single rollback from my bootloader [2] and all was back the way it was before.<p>Having good tooling set up definitely helped a lot. My monkey brain has the tendency to think &quot;surely I got it right this time, so no need to make a snapshot before trying out X!&quot;, especially when experimenting on my own workstation. Automating snapshots using a systemd timer and hooks added to my package manager saved me a number of times.<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;psy0rz&#x2F;zfs_autobackup">https:&#x2F;&#x2F;github.com&#x2F;psy0rz&#x2F;zfs_autobackup</a>
[2]: <a href="https:&#x2F;&#x2F;zfsbootmenu.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;zfsbootmenu.org&#x2F;</a></div><br/><div id="36594030" class="c"><input type="checkbox" id="c-36594030" checked=""/><div class="controls bullet"><span class="by">csdvrx</span><span>|</span><a href="#36589289">root</a><span>|</span><a href="#36591770">parent</a><span>|</span><a href="#36594883">next</a><span>|</span><label class="collapse" for="c-36594030">[-]</label><label class="expand" for="c-36594030">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Systems are set up to automatically make snapshots<p>I do that with sqlite to keep a selection of snapshots from the last hours, days etc.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;csdvrx&#x2F;zfs-autosnapshot">https:&#x2F;&#x2F;github.com&#x2F;csdvrx&#x2F;zfs-autosnapshot</a></div><br/></div></div></div></div><div id="36594883" class="c"><input type="checkbox" id="c-36594883" checked=""/><div class="controls bullet"><span class="by">cyrnel</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36591770">prev</a><span>|</span><a href="#36589791">next</a><span>|</span><label class="collapse" for="c-36594883">[-]</label><label class="expand" for="c-36594883">[1 more]</label></div><br/><div class="children"><div class="content">I use it in Kubernetes via <a href="https:&#x2F;&#x2F;github.com&#x2F;openebs&#x2F;zfs-localpv">https:&#x2F;&#x2F;github.com&#x2F;openebs&#x2F;zfs-localpv</a><p>The PersistentVolume API is a nice way to divvy up a shared resource across different teams, and using ZFS for that gives us the snapshotting, deduplication, and compression for free. For our workloads, it benchmarked faster than XFS so it was a no-brainer.</div><br/></div></div><div id="36589791" class="c"><input type="checkbox" id="c-36589791" checked=""/><div class="controls bullet"><span class="by">unixhero</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36594883">prev</a><span>|</span><a href="#36592977">next</a><span>|</span><label class="collapse" for="c-36589791">[-]</label><label class="expand" for="c-36589791">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Using latest ZFS ZFS On Linux distrib on Debian. Using Proxmox. Never had and problems ever, ever.</div><br/></div></div><div id="36592977" class="c"><input type="checkbox" id="c-36592977" checked=""/><div class="controls bullet"><span class="by">quags</span><span>|</span><a href="#36589289">parent</a><span>|</span><a href="#36589791">prev</a><span>|</span><a href="#36588582">next</a><span>|</span><label class="collapse" for="c-36592977">[-]</label><label class="expand" for="c-36592977">[1 more]</label></div><br/><div class="children"><div class="content">I have been using zfs for years from ubuntu 18. Easy snapshots, monitoring, choices for raid levels, and ability to very easily copy a dataset remotely with resume or incremental support is awesome. I mainly use it for kvm systems each with their own dataset. Coming from mdadm + lvm from my previous set up is night and day for doing snapshots and backups. I do not use zfs on root for ubuntu instead I do a raid1 software set up for the os and then a zfs set up on other disks - zfs on root was the only gotcha. For FreeBSD zfs on root works fine.</div><br/></div></div></div></div><div id="36588582" class="c"><input type="checkbox" id="c-36588582" checked=""/><div class="controls bullet"><span class="by">miohtama</span><span>|</span><a href="#36589289">prev</a><span>|</span><a href="#36591435">next</a><span>|</span><label class="collapse" for="c-36588582">[-]</label><label class="expand" for="c-36588582">[13 more]</label></div><br/><div class="children"><div class="content">What are applications that benefit from block cloning?</div><br/><div id="36588779" class="c"><input type="checkbox" id="c-36588779" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#36588582">parent</a><span>|</span><a href="#36588754">next</a><span>|</span><label class="collapse" for="c-36588779">[-]</label><label class="expand" for="c-36588779">[1 more]</label></div><br/><div class="children"><div class="content">Excited, because in addition to ref copies&#x2F;clones, httm will use this feature, if available (I&#x27;ve already done some work to implement), for its `--roll-forward` operation, and for faster file recoveries from snapshots [0].<p>As I understand it, there will be no need to copy any data from the same dataset, and <i>this includes all snapshots</i>.  Blocks written to the live dataset can just be references to the underlying blocks, and no additional space will need be used.<p>Imagine being able to continuously switch a file or a dataset back to a previous state extremely quickly without a heavy weight clone, or a rollback, etc.<p>Right now, httm simply diff copies the blocks for file recovery and roll-forward.  For further details, see the man page entry for `--roll-forward`, and the link to the httm GitHub below:<p><pre><code>    --roll-forward=&quot;snap_name&quot;

    traditionally &#x27;zfs rollback&#x27; is a destructive operation, whereas httm roll-forward is non-destructive.  httm will copy only the blocks and file metadata that have changed since a specified snapshot, from that snapshot, to its live dataset.  httm will also take two precautionary snapshots, one before and one after the copy.  Should the roll forward fail for any reason, httm will roll back to the pre-execution state.  Note: This is a ZFS only option which requires super user privileges.
</code></pre>
[0]: <a href="https:&#x2F;&#x2F;github.com&#x2F;kimono-koans&#x2F;httm">https:&#x2F;&#x2F;github.com&#x2F;kimono-koans&#x2F;httm</a></div><br/></div></div><div id="36588754" class="c"><input type="checkbox" id="c-36588754" checked=""/><div class="controls bullet"><span class="by">vovin</span><span>|</span><a href="#36588582">parent</a><span>|</span><a href="#36588779">prev</a><span>|</span><a href="#36588954">next</a><span>|</span><label class="collapse" for="c-36588754">[-]</label><label class="expand" for="c-36588754">[3 more]</label></div><br/><div class="children"><div class="content">This is huge.
One practical application is fast recovery of a file from past snapshot without using any additional space. I use ZFS dataset for my vCenter datastore (storing my vmdk files). In case of need to launch a clone from a past state one could use a block cloning to bring past vmdk file without the need to actually copy the file - it saves both space and time to make such clone.</div><br/><div id="36591884" class="c"><input type="checkbox" id="c-36591884" checked=""/><div class="controls bullet"><span class="by">bithavoc</span><span>|</span><a href="#36588582">root</a><span>|</span><a href="#36588754">parent</a><span>|</span><a href="#36588954">next</a><span>|</span><label class="collapse" for="c-36591884">[-]</label><label class="expand" for="c-36591884">[2 more]</label></div><br/><div class="children"><div class="content">Can you elaborate a bit more on how you use ZFS with vCenter? How do you mount it?</div><br/><div id="36594272" class="c"><input type="checkbox" id="c-36594272" checked=""/><div class="controls bullet"><span class="by">redundantly</span><span>|</span><a href="#36588582">root</a><span>|</span><a href="#36591884">parent</a><span>|</span><a href="#36588954">next</a><span>|</span><label class="collapse" for="c-36594272">[-]</label><label class="expand" for="c-36594272">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re likely using a NAS (e.g., FreeNAS&#x2F;TrueNAS) that uses ZFS for the underlying storage then sharing that storage by either NFS or iSCSI with their vSphere cluster. In some rare cases FC is being used instead of NFS or iSCSI.</div><br/></div></div></div></div></div></div><div id="36588954" class="c"><input type="checkbox" id="c-36588954" checked=""/><div class="controls bullet"><span class="by">philsnow</span><span>|</span><a href="#36588582">parent</a><span>|</span><a href="#36588754">prev</a><span>|</span><a href="#36588737">next</a><span>|</span><label class="collapse" for="c-36588954">[-]</label><label class="expand" for="c-36588954">[3 more]</label></div><br/><div class="children"><div class="content">It seems kind of like hard linking but with copy-on-write for the underlying data, so you&#x27;ll get near-instant file copies and writing into the middle of them will also be near-instant.<p>All of this happens under the covers already if you have dedup turned on, but this allows utilities (gnu cp might be taught to opportunistically and transparently use the new clone zfs syscalls, because there is no downside and only upside) and applications to tell zfs that &quot;these blocks are going to be the same as those&quot; without zfs needing to hash all the new blocks and compare them.<p>Aditionally, for finer control, ranges of blocks can be cloned, not just entire files.<p>I can&#x27;t tell from the github issue, can this manual dedup &#x2F; block cloning be turned on if you&#x27;re not already using dedup on a dataset?  Last time I set up zfs, I was warned that dedup took gobs of memory, so I didn&#x27;t turn it on.</div><br/><div id="36591125" class="c"><input type="checkbox" id="c-36591125" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36588582">root</a><span>|</span><a href="#36588954">parent</a><span>|</span><a href="#36589046">next</a><span>|</span><label class="collapse" for="c-36591125">[-]</label><label class="expand" for="c-36591125">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s orthogonal to dedup being on or off, and as someone else said, it&#x27;s more or less the same underlying semantics you would expect from cp --reflink anywhere.<p>Also, as mentioned, on Linux, it&#x27;s not wired up with any interface to be used at all right now.</div><br/></div></div><div id="36589046" class="c"><input type="checkbox" id="c-36589046" checked=""/><div class="controls bullet"><span class="by">nabla9</span><span>|</span><a href="#36588582">root</a><span>|</span><a href="#36588954">parent</a><span>|</span><a href="#36591125">prev</a><span>|</span><a href="#36588737">next</a><span>|</span><label class="collapse" for="c-36589046">[-]</label><label class="expand" for="c-36589046">[1 more]</label></div><br/><div class="children"><div class="content">Gnu cp --reflink.<p>&gt;When  --reflink[=always]  is specified, perform a lightweight copy, where the data blocks are copied only when modified. If this is not possible the copy fails, or if --reflink=auto is specified, fall back to  a  standard  copy.   Use  --reflink=never to ensure a standard copy is performed.&quot;</div><br/></div></div></div></div><div id="36588737" class="c"><input type="checkbox" id="c-36588737" checked=""/><div class="controls bullet"><span class="by">thrill</span><span>|</span><a href="#36588582">parent</a><span>|</span><a href="#36588954">prev</a><span>|</span><a href="#36589502">next</a><span>|</span><label class="collapse" for="c-36588737">[-]</label><label class="expand" for="c-36588737">[1 more]</label></div><br/><div class="children"><div class="content">FTFA: &quot;Block Cloning allows to clone a file (or a subset of its blocks) into another (or the same) file by just creating additional references to the data blocks without copying the data itself. Block Cloning can be described as a fast, manual deduplication.&quot;</div><br/></div></div><div id="36589502" class="c"><input type="checkbox" id="c-36589502" checked=""/><div class="controls bullet"><span class="by">danudey</span><span>|</span><a href="#36588582">parent</a><span>|</span><a href="#36588737">prev</a><span>|</span><a href="#36593100">next</a><span>|</span><label class="collapse" for="c-36589502">[-]</label><label class="expand" for="c-36589502">[1 more]</label></div><br/><div class="children"><div class="content">As others have said: block cloning (the underlying technology that enables copy-on-write) allows you to &#x27;copy&#x27; a file without reading all of the data and re-writing it.<p>For example, if you have a 1 GB file and you want to make a copy of it, you need to read the whole file (all at once or in parts) and then write the whole new file (all at once or in parts). This results in 1 GB of reads and 1 GB of writes. Obviously the slower (or more overloaded) your storage media is, the longer this takes.<p>With block cloning, you simply tell the OS &quot;I want this file A to be a copy of this file B&quot; and it creates a new &quot;file&quot; that references all the blocks in the old &quot;file&quot;. Given that a &quot;file&quot; on a filesystem is just a list of blocks that make up the data in that file, you can create a new &quot;file&quot; which has pointers to the same blocks as the old &quot;file&quot;. This is a simple system call (or a few system calls), and as such isn&#x27;t much more intensive than simply renaming a file instead of copying it.<p>At my previous job we did builds for our software. This required building the BIOS, kernel, userspace, generating the UI, and so on. These builds required pulling down 10+ GB of git repositories (the git data itself, the checkout, the LFS binary files, external vendor SDKs), and then a large amount of build artifacts on top of that. We also needed to do this build for 80-100 different product models, for both release and debug versions. This meant 200+ copies of the source code alone (not to mention build artifacts and intermediate products), and because of disk space limitations this meant we had to dramatically reduce the number of concurrent builds we could run. The solution we came up with was something like:<p>1. Check out the source code<p>2. Create an overlayfs filesystem to mount into each build space<p>3. Do the build<p>4. Tear down the overlayfs filesystem<p>This was problematic if we weren&#x27;t able to mount the filesystem, if we weren&#x27;t able to unmount the filesystem (because of hanging file descriptors or processes), and so on. Lots of moving parts, lots of `sudo` commands in the scripts, and so on.<p>Copy-on-write would have solved this for us by accomplishing the same thing; we could simply do the following:<p>1. Check out the source code<p>2. Have each build process simply `cp -R --reflink=always source&#x2F; build_root&#x2F;`; this would be instantaneous and use no new disk space.<p>3. Do the build<p>4. `rm -rf build_root`<p>Fewer moving parts, no root access required, generally simpler all around.</div><br/></div></div><div id="36593100" class="c"><input type="checkbox" id="c-36593100" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#36588582">parent</a><span>|</span><a href="#36589502">prev</a><span>|</span><a href="#36589081">next</a><span>|</span><label class="collapse" for="c-36593100">[-]</label><label class="expand" for="c-36593100">[1 more]</label></div><br/><div class="children"><div class="content">Any copy command. On-demand deduplication managed by userspace.<p><a href="https:&#x2F;&#x2F;man7.org&#x2F;linux&#x2F;man-pages&#x2F;man2&#x2F;ioctl_fideduperange.2.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;man7.org&#x2F;linux&#x2F;man-pages&#x2F;man2&#x2F;ioctl_fideduperange.2....</a> <a href="https:&#x2F;&#x2F;man7.org&#x2F;linux&#x2F;man-pages&#x2F;man2&#x2F;copy_file_range.2.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;man7.org&#x2F;linux&#x2F;man-pages&#x2F;man2&#x2F;copy_file_range.2.html</a> <a href="https:&#x2F;&#x2F;github.com&#x2F;markfasheh&#x2F;duperemove">https:&#x2F;&#x2F;github.com&#x2F;markfasheh&#x2F;duperemove</a></div><br/></div></div><div id="36589081" class="c"><input type="checkbox" id="c-36589081" checked=""/><div class="controls bullet"><span class="by">aardvark179</span><span>|</span><a href="#36588582">parent</a><span>|</span><a href="#36593100">prev</a><span>|</span><a href="#36588953">next</a><span>|</span><label class="collapse" for="c-36589081">[-]</label><label class="expand" for="c-36589081">[1 more]</label></div><br/><div class="children"><div class="content">It can be a really convenient way to snapshot something if you can arrange some point at which everything is synced to disk. Get to that point, make your new files that start sharing all their blocks, and then let your main db process (or whatever) continue on as normal.</div><br/></div></div><div id="36588953" class="c"><input type="checkbox" id="c-36588953" checked=""/><div class="controls bullet"><span class="by">ikiris</span><span>|</span><a href="#36588582">parent</a><span>|</span><a href="#36589081">prev</a><span>|</span><a href="#36591435">next</a><span>|</span><label class="collapse" for="c-36588953">[-]</label><label class="expand" for="c-36588953">[1 more]</label></div><br/><div class="children"><div class="content">I think the big piece is native overlayfs so k8 setups get a bit simpler.</div><br/></div></div></div></div><div id="36591435" class="c"><input type="checkbox" id="c-36591435" checked=""/><div class="controls bullet"><span class="by">rossmohax</span><span>|</span><a href="#36588582">prev</a><span>|</span><a href="#36591573">next</a><span>|</span><label class="collapse" for="c-36591435">[-]</label><label class="expand" for="c-36591435">[2 more]</label></div><br/><div class="children"><div class="content">Does ZFS or any other FS offer special operations which DB engine like RocksDB, SQLite or PostgreSQL could benefit from if they decided to target that FS specifically?</div><br/><div id="36591865" class="c"><input type="checkbox" id="c-36591865" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#36591435">parent</a><span>|</span><a href="#36591573">next</a><span>|</span><label class="collapse" for="c-36591865">[-]</label><label class="expand" for="c-36591865">[1 more]</label></div><br/><div class="children"><div class="content">Internally, ZFS is kinda like an object store[1], and there was a project trying to expose the ZFS internals through an object store API rather than through a filesystem API.<p>Sadly I can&#x27;t seem to find the presentation or recall the name of the project.<p>On the other hand, looking at for example RocksDB[2]:<p><i>File system operations are not atomic, and are susceptible to inconsistencies in the event of system failure. Even with journaling turned on, file systems do not guarantee consistency on unclean restart. POSIX file system does not support atomic batching of operations either. Hence, it is not possible to rely on metadata embedded in RocksDB datastore files to reconstruct the last consistent state of the RocksDB on restart. RocksDB has a built-in mechanism to overcome these limitations of POSIX file system [...]</i><p>ZFS <i>does</i> provide atomic operations internally[1], so if exposed it seems something like RocksDB could take advantage of that and forego all the complexity mentioned above.<p>How much that would help I don&#x27;t know though, but seems potentially interesting at first glance.<p>[1]: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;MsY-BafQgj4?t=442" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;MsY-BafQgj4?t=442</a><p>[2]: <a href="https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;rocksdb&#x2F;wiki&#x2F;MANIFEST">https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;rocksdb&#x2F;wiki&#x2F;MANIFEST</a></div><br/></div></div></div></div><div id="36591573" class="c"><input type="checkbox" id="c-36591573" checked=""/><div class="controls bullet"><span class="by">dark-star</span><span>|</span><a href="#36591435">prev</a><span>|</span><a href="#36590452">next</a><span>|</span><label class="collapse" for="c-36591573">[-]</label><label class="expand" for="c-36591573">[3 more]</label></div><br/><div class="children"><div class="content">Wow, I was under the impression that this had long been implemented already (as it&#x27;s already in btrfs and other commercial file systems)<p>Awesome!</div><br/><div id="36592201" class="c"><input type="checkbox" id="c-36592201" checked=""/><div class="controls bullet"><span class="by">mgerdts</span><span>|</span><a href="#36591573">parent</a><span>|</span><a href="#36590452">next</a><span>|</span><label class="collapse" for="c-36592201">[-]</label><label class="expand" for="c-36592201">[2 more]</label></div><br/><div class="children"><div class="content">It has been in the Solaris version of zfs for a long time as well. This came a few years after the Oracle-imposed fork.<p><a href="https:&#x2F;&#x2F;blogs.oracle.com&#x2F;solaris&#x2F;post&#x2F;reflink3c-what-is-it-why-do-i-care-and-how-can-i-use-it" rel="nofollow noreferrer">https:&#x2F;&#x2F;blogs.oracle.com&#x2F;solaris&#x2F;post&#x2F;reflink3c-what-is-it-w...</a></div><br/><div id="36593993" class="c"><input type="checkbox" id="c-36593993" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#36591573">root</a><span>|</span><a href="#36592201">parent</a><span>|</span><a href="#36590452">next</a><span>|</span><label class="collapse" for="c-36593993">[-]</label><label class="expand" for="c-36593993">[1 more]</label></div><br/><div class="children"><div class="content">reflink is implemented on Oracle Solaris; how it works internally I don&#x27;t think Oracle has ever commented, since implementing post-facto dedup on a filesystem that assumes data once written is unmodified is a bit Spicy(tm) - you can look at the BRT talk from the OpenZFS dev summit or other talks about it at the leadership meetings to get some idea why it&#x27;s so exciting.</div><br/></div></div></div></div></div></div><div id="36590452" class="c"><input type="checkbox" id="c-36590452" checked=""/><div class="controls bullet"><span class="by">uvatbc</span><span>|</span><a href="#36591573">prev</a><span>|</span><a href="#36595885">next</a><span>|</span><label class="collapse" for="c-36590452">[-]</label><label class="expand" for="c-36590452">[1 more]</label></div><br/><div class="children"><div class="content">Technically, yes: through the use of Truenas that gives us API access to iscsi on ZFS.</div><br/></div></div><div id="36595885" class="c"><input type="checkbox" id="c-36595885" checked=""/><div class="controls bullet"><span class="by">KirillPanov</span><span>|</span><a href="#36590452">prev</a><span>|</span><a href="#36589700">next</a><span>|</span><label class="collapse" for="c-36595885">[-]</label><label class="expand" for="c-36595885">[2 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t btrfs have this like a ~decade ago?</div><br/><div id="36596164" class="c"><input type="checkbox" id="c-36596164" checked=""/><div class="controls bullet"><span class="by">LeoPanthera</span><span>|</span><a href="#36595885">parent</a><span>|</span><a href="#36589700">next</a><span>|</span><label class="collapse" for="c-36596164">[-]</label><label class="expand" for="c-36596164">[1 more]</label></div><br/><div class="children"><div class="content">Yes, although be careful, because running &quot;defragment&quot; will un-clone and therefore duplicate all your identical blocks.</div><br/></div></div></div></div><div id="36589700" class="c"><input type="checkbox" id="c-36589700" checked=""/><div class="controls bullet"><span class="by">GauntletWizard</span><span>|</span><a href="#36595885">prev</a><span>|</span><a href="#36588790">next</a><span>|</span><label class="collapse" for="c-36589700">[-]</label><label class="expand" for="c-36589700">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Note: currently it is not possible to clone blocks between encrypted datasets, even if those datasets use the same encryption key (this includes snapshots of encrypted datasets). Cloning blocks between datasets that use the same keys should be possible and should be implemented in the future.<p>Once this is ready, I am going to subdivide my user homedir much more than it already is. The biggest obstacle in the way of this has been that it would waste a bunch of space until the snapshots were done rolling over, which for me is a long time (I keep weekly snapshots of my homedir for a year).</div><br/><div id="36590108" class="c"><input type="checkbox" id="c-36590108" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#36589700">parent</a><span>|</span><a href="#36588790">next</a><span>|</span><label class="collapse" for="c-36590108">[-]</label><label class="expand" for="c-36590108">[4 more]</label></div><br/><div class="children"><div class="content">Is there a benefit to breaking up your home directory?</div><br/><div id="36590689" class="c"><input type="checkbox" id="c-36590689" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#36589700">root</a><span>|</span><a href="#36590108">parent</a><span>|</span><a href="#36590429">next</a><span>|</span><label class="collapse" for="c-36590689">[-]</label><label class="expand" for="c-36590689">[1 more]</label></div><br/><div class="children"><div class="content">Not sure why you&#x27;d want to do that to your home directory usually, but it depends on what you store in it and how you use it, really.<p>In general, breaking up a filesystem into multiple ones in ZFS is mostly useful for making filesystem management more fine-grained, as a filesystem&#x2F;dataset in ZFS is the unit of management for most properties and operations (snapshots, clones, compression and checksum algorithms, quotas, encryption, dedup, send&#x2F;recv, ditto copies, etc) as well as their inheritance and space accounting.<p>In terms of filesystem management, there aren&#x27;t many downsides to breaking up a filesystem (within reason), as most properties and the most common operations can be shared between all sub-filesystems if they are part of the same inherited tree (which doesn&#x27;t necessarily have to correspond to the mountpoint tree!).<p>As far as I know, the major downsides by far were that 1) you couldn&#x27;t quickly move a file from one dataset to another, i.e. `mv` would be forced to do a full copy of the file contents rather than just do a cheap rename, and 2) in terms of disk space, moving a file between filesystems would be equivalent to copying the file and deleting the original, which could be terrible if you use snapshots as it would lead to an additional space consumption of a full new file&#x27;s worth of disk space.<p>In principle, both of these downsides should be fixed with this new block cloning feature and AFAIU the only tradeoffs would be some amount of increased overhead when freeing data (which should be zero overhead if you don&#x27;t have many of these cloned blocks being shared anymore), and the low maturity of this code (i.e. higher chance of running into bugs) due to being so new.</div><br/></div></div><div id="36590429" class="c"><input type="checkbox" id="c-36590429" checked=""/><div class="controls bullet"><span class="by">GauntletWizard</span><span>|</span><a href="#36589700">root</a><span>|</span><a href="#36590108">parent</a><span>|</span><a href="#36590689">prev</a><span>|</span><a href="#36588790">next</a><span>|</span><label class="collapse" for="c-36590429">[-]</label><label class="expand" for="c-36590429">[2 more]</label></div><br/><div class="children"><div class="content">Controlling the rate and location of snapshots, mostly. I&#x27;ve broken out some kinds of datasets (video archives) but not others historically (music). It doesn&#x27;t matter that much, but I want to split some more chunks out.</div><br/><div id="36590623" class="c"><input type="checkbox" id="c-36590623" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#36589700">root</a><span>|</span><a href="#36590429">parent</a><span>|</span><a href="#36588790">next</a><span>|</span><label class="collapse" for="c-36590623">[-]</label><label class="expand" for="c-36590623">[1 more]</label></div><br/><div class="children"><div class="content">Fair enough. I&#x27;ve personally slowly moved to a smaller number of filesystems, but if you&#x27;re actually handling snapshots differently per-area then it makes sense (indeed, one of the reasons I&#x27;m consolidating is the realization that <i>personally</i> I&#x27;m almost never going to snapshot&#x2F;restore things separately).</div><br/></div></div></div></div></div></div></div></div><div id="36588790" class="c"><input type="checkbox" id="c-36588790" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#36589700">prev</a><span>|</span><a href="#36593759">next</a><span>|</span><label class="collapse" for="c-36588790">[-]</label><label class="expand" for="c-36588790">[13 more]</label></div><br/><div class="children"><div class="content">Do Btrfs or ext4 offer this?</div><br/><div id="36588962" class="c"><input type="checkbox" id="c-36588962" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#36588790">parent</a><span>|</span><a href="#36588992">next</a><span>|</span><label class="collapse" for="c-36588962">[-]</label><label class="expand" for="c-36588962">[8 more]</label></div><br/><div class="children"><div class="content">This feature is basically the same as what underpins the reflink feature that btrfs has supported approximately forever and xfs has supported for at least several years.</div><br/><div id="36589150" class="c"><input type="checkbox" id="c-36589150" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36588962">parent</a><span>|</span><a href="#36588992">next</a><span>|</span><label class="collapse" for="c-36589150">[-]</label><label class="expand" for="c-36589150">[7 more]</label></div><br/><div class="children"><div class="content">Does anyone know whether btrfs or XFS support reflinks from snapshot datasets?</div><br/><div id="36589467" class="c"><input type="checkbox" id="c-36589467" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36589150">parent</a><span>|</span><a href="#36589544">next</a><span>|</span><label class="collapse" for="c-36589467">[-]</label><label class="expand" for="c-36589467">[1 more]</label></div><br/><div class="children"><div class="content">I can confirm BTRFS yes, but note that source and destination need to be on the same mount point before kernel 5.18</div><br/></div></div><div id="36589544" class="c"><input type="checkbox" id="c-36589544" checked=""/><div class="controls bullet"><span class="by">danudey</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36589150">parent</a><span>|</span><a href="#36589467">prev</a><span>|</span><a href="#36589532">next</a><span>|</span><label class="collapse" for="c-36589544">[-]</label><label class="expand" for="c-36589544">[4 more]</label></div><br/><div class="children"><div class="content">XFS doesn&#x27;t have snapshot support, so the short answer there is no.</div><br/><div id="36589938" class="c"><input type="checkbox" id="c-36589938" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36589544">parent</a><span>|</span><a href="#36589532">next</a><span>|</span><label class="collapse" for="c-36589938">[-]</label><label class="expand" for="c-36589938">[3 more]</label></div><br/><div class="children"><div class="content">Shows what I know about XFS.  Thanks!</div><br/><div id="36592687" class="c"><input type="checkbox" id="c-36592687" checked=""/><div class="controls bullet"><span class="by">PlutoIsAPlanet</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36589938">parent</a><span>|</span><a href="#36592544">next</a><span>|</span><label class="collapse" for="c-36592687">[-]</label><label class="expand" for="c-36592687">[1 more]</label></div><br/><div class="children"><div class="content">You can get psuedo-snapshots on XFS with a tool like <a href="https:&#x2F;&#x2F;github.com&#x2F;aravindavk&#x2F;reflink-snapshot">https:&#x2F;&#x2F;github.com&#x2F;aravindavk&#x2F;reflink-snapshot</a><p>But, it still has to duplicate metadata which depending on the amount of files may cause inconsistency in the snapshot.</div><br/></div></div><div id="36592544" class="c"><input type="checkbox" id="c-36592544" checked=""/><div class="controls bullet"><span class="by">plq</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36589938">parent</a><span>|</span><a href="#36592687">prev</a><span>|</span><a href="#36589532">next</a><span>|</span><label class="collapse" for="c-36592544">[-]</label><label class="expand" for="c-36592544">[1 more]</label></div><br/><div class="children"><div class="content">This is only a tangent given we are talking about snapshots and reflink, but just wanted to mention that LVM has snapshots, so if you need XFS snapshots, create the XFS filesystem on top of an LVM logical volume.</div><br/></div></div></div></div></div></div><div id="36589532" class="c"><input type="checkbox" id="c-36589532" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36589150">parent</a><span>|</span><a href="#36589544">prev</a><span>|</span><a href="#36588992">next</a><span>|</span><label class="collapse" for="c-36589532">[-]</label><label class="expand" for="c-36589532">[1 more]</label></div><br/><div class="children"><div class="content">XFS doesn&#x27;t have native snapshot support, though?</div><br/></div></div></div></div></div></div><div id="36588992" class="c"><input type="checkbox" id="c-36588992" checked=""/><div class="controls bullet"><span class="by">dsr_</span><span>|</span><a href="#36588790">parent</a><span>|</span><a href="#36588962">prev</a><span>|</span><a href="#36588910">next</a><span>|</span><label class="collapse" for="c-36588992">[-]</label><label class="expand" for="c-36588992">[2 more]</label></div><br/><div class="children"><div class="content">You can get a similar effect on top of any file system that supports hard links with rdfind ( <a href="https:&#x2F;&#x2F;rdfind.pauldreik.se&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;rdfind.pauldreik.se&#x2F;</a> ) -- but it&#x27;s pretty slow.<p>The Arch wiki says:<p>&quot;Tools dedicated to deduplicate a Btrfs formatted partition include duperemove, bees, bedup and btrfs-dedup. One may also want to merely deduplicate data on a file based level instead using e.g. rmlint, jdupes or dduper-git. For an overview of available features of those programs and additional information, have a look at the upstream Wiki entry.<p>Furthermore, Btrfs developers are working on inband (also known as synchronous or inline) deduplication, meaning deduplication done when writing new data to the filesystem. Currently, it is still an experiment which is developed out-of-tree. Users willing to test the new feature should read the appropriate kernel wiki page.&quot;</div><br/><div id="36590281" class="c"><input type="checkbox" id="c-36590281" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36588992">parent</a><span>|</span><a href="#36588910">next</a><span>|</span><label class="collapse" for="c-36590281">[-]</label><label class="expand" for="c-36590281">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You can get a similar effect on top of any file system that supports hard links with rdfind ( <a href="https:&#x2F;&#x2F;rdfind.pauldreik.se&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;rdfind.pauldreik.se&#x2F;</a> ) -- but it&#x27;s pretty slow.<p>It&#x27;s a similar effect only if you don&#x27;t modify the files, I think.<p>If you &quot;clone&quot; a file with a hard link and you modify the contents of one copy, the other copy would also be equally modified.<p>As far as I understand this wouldn&#x27;t happen with this type of block cloning: each copy of the file would be completely separate, except that they may (transparently) share data blocks on disk.</div><br/></div></div></div></div><div id="36588910" class="c"><input type="checkbox" id="c-36588910" checked=""/><div class="controls bullet"><span class="by">thrtythreeforty</span><span>|</span><a href="#36588790">parent</a><span>|</span><a href="#36588992">prev</a><span>|</span><a href="#36593759">next</a><span>|</span><label class="collapse" for="c-36588910">[-]</label><label class="expand" for="c-36588910">[2 more]</label></div><br/><div class="children"><div class="content">Btrfs yes, ext4 no (but I believe xfs does).<p>This should end up being exposed through cp --reflink=always, so you could look up filesystem support for that.</div><br/><div id="36589514" class="c"><input type="checkbox" id="c-36589514" checked=""/><div class="controls bullet"><span class="by">danudey</span><span>|</span><a href="#36588790">root</a><span>|</span><a href="#36588910">parent</a><span>|</span><a href="#36593759">next</a><span>|</span><label class="collapse" for="c-36589514">[-]</label><label class="expand" for="c-36589514">[1 more]</label></div><br/><div class="children"><div class="content">XFS does, I&#x27;ve used it for specifically this feature before.</div><br/></div></div></div></div></div></div><div id="36593759" class="c"><input type="checkbox" id="c-36593759" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#36588790">prev</a><span>|</span><a href="#36590303">next</a><span>|</span><label class="collapse" for="c-36593759">[-]</label><label class="expand" for="c-36593759">[2 more]</label></div><br/><div class="children"><div class="content">Finally! This is exactly what I want for handling photos and media on my home server. We have phone&#x27;s dumping their camera rolls onto the machine via Syncthing, but the easiest way to share content safely is just to make copies of the file and go &quot;storage is cheap&quot;. But the reality has always been closer to &quot;storage should handle deduplication&quot; - with this feature landing that can finally be a reality.<p>EDIT: Not to mention, this should make ZFS best-in-class for handling container filesystems. Deduplicating all the files in the individual layers means you can pretty much dispense with awkwardly trying to maintain inheritance trees.</div><br/><div id="36594667" class="c"><input type="checkbox" id="c-36594667" checked=""/><div class="controls bullet"><span class="by">thinkloop</span><span>|</span><a href="#36593759">parent</a><span>|</span><a href="#36590303">next</a><span>|</span><label class="collapse" for="c-36594667">[-]</label><label class="expand" for="c-36594667">[1 more]</label></div><br/><div class="children"><div class="content">Would you mind expanding on that - what mechanism&#x2F;app on the phone would share the files?</div><br/></div></div></div></div><div id="36590303" class="c"><input type="checkbox" id="c-36590303" checked=""/><div class="controls bullet"><span class="by">Pxtl</span><span>|</span><a href="#36593759">prev</a><span>|</span><label class="collapse" for="c-36590303">[-]</label><label class="expand" for="c-36590303">[14 more]</label></div><br/><div class="children"><div class="content">Filesystem-level de-duplication is scary as hell as a concept, but also sounds amazing, especially doing it at copy-time so you don&#x27;t have to opt-in to scanning to deduplicate.  Is this common in filesystems?  Or is ZFS striking out new ground here?  I&#x27;m not really an under-the-OS-hood kinda guy.</div><br/><div id="36591898" class="c"><input type="checkbox" id="c-36591898" checked=""/><div class="controls bullet"><span class="by">wrs</span><span>|</span><a href="#36590303">parent</a><span>|</span><a href="#36590349">next</a><span>|</span><label class="collapse" for="c-36591898">[-]</label><label class="expand" for="c-36591898">[2 more]</label></div><br/><div class="children"><div class="content">MacOS and BTRFS have had it for several years. In fact I believe it’s the default behavior when copying a file in MacOS using the Finder (you have to specify `cp -c` in shell).</div><br/><div id="36592462" class="c"><input type="checkbox" id="c-36592462" checked=""/><div class="controls bullet"><span class="by">nijave</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36591898">parent</a><span>|</span><a href="#36590349">next</a><span>|</span><label class="collapse" for="c-36592462">[-]</label><label class="expand" for="c-36592462">[1 more]</label></div><br/><div class="children"><div class="content">Windows Server has had dedupe for at least 10 years, too</div><br/></div></div></div></div><div id="36590349" class="c"><input type="checkbox" id="c-36590349" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#36590303">parent</a><span>|</span><a href="#36591898">prev</a><span>|</span><label class="collapse" for="c-36590349">[-]</label><label class="expand" for="c-36590349">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Filesystem-level de-duplication is scary as hell as a concept<p>What&#x27;s scary about it? You have to track references, but it doesn&#x27;t seem <i>that</i> hard compared to everything else going on in ZFS et al.<p>&gt; Is this common in filesystems? Or is ZFS striking out new ground here?
At least BTRFS does approximately the same.</div><br/><div id="36590415" class="c"><input type="checkbox" id="c-36590415" checked=""/><div class="controls bullet"><span class="by">cesarb</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590349">parent</a><span>|</span><a href="#36590962">next</a><span>|</span><label class="collapse" for="c-36590415">[-]</label><label class="expand" for="c-36590415">[7 more]</label></div><br/><div class="children"><div class="content">&gt; &gt; Filesystem-level de-duplication is scary as hell as a concept<p>&gt; What&#x27;s scary about it?<p>It&#x27;s scary because there&#x27;s only one copy when you might have expected two. A single bad block could lose both &quot;copies&quot; at once.</div><br/><div id="36590926" class="c"><input type="checkbox" id="c-36590926" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590415">parent</a><span>|</span><a href="#36591894">next</a><span>|</span><label class="collapse" for="c-36590926">[-]</label><label class="expand" for="c-36590926">[1 more]</label></div><br/><div class="children"><div class="content">Disks die all the time anyway. If you want to keep your data, you should have at least two-disk redundancy. In which case bad blocks won&#x27;t kill anything.</div><br/></div></div><div id="36591894" class="c"><input type="checkbox" id="c-36591894" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590415">parent</a><span>|</span><a href="#36590926">prev</a><span>|</span><a href="#36590545">next</a><span>|</span><label class="collapse" for="c-36591894">[-]</label><label class="expand" for="c-36591894">[1 more]</label></div><br/><div class="children"><div class="content">Besides what all the others have mentioned, you can force ZFS to keep up to 3 copies of data blocks on a dataset. ZFS uses this internally for important metadata and will try to spread them around to maximize the chance of recovery,
though don&#x27;t rely on this feature alone for redundancy.</div><br/></div></div><div id="36590545" class="c"><input type="checkbox" id="c-36590545" checked=""/><div class="controls bullet"><span class="by">grepfru_it</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590415">parent</a><span>|</span><a href="#36591894">prev</a><span>|</span><a href="#36593717">next</a><span>|</span><label class="collapse" for="c-36590545">[-]</label><label class="expand" for="c-36590545">[1 more]</label></div><br/><div class="children"><div class="content">The file system metadata is redundant and on a correctly configured ZFS system your error correction is isolated and can be redundant as well</div><br/></div></div><div id="36593717" class="c"><input type="checkbox" id="c-36593717" checked=""/><div class="controls bullet"><span class="by">stormking</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590415">parent</a><span>|</span><a href="#36590545">prev</a><span>|</span><a href="#36591628">next</a><span>|</span><label class="collapse" for="c-36593717">[-]</label><label class="expand" for="c-36593717">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what data checksumming and mirroring is for.</div><br/></div></div><div id="36591628" class="c"><input type="checkbox" id="c-36591628" checked=""/><div class="controls bullet"><span class="by">phpisthebest</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590415">parent</a><span>|</span><a href="#36593717">prev</a><span>|</span><a href="#36590526">next</a><span>|</span><label class="collapse" for="c-36591628">[-]</label><label class="expand" for="c-36591628">[1 more]</label></div><br/><div class="children"><div class="content">3-2-1..<p>3 Copies<p>2 Media<p>1 offsite.<p>If you follow that then you would have no fear of data loss.  if you are putting 2 copies on the same filesystem you are already doing backups wrongs</div><br/></div></div><div id="36590526" class="c"><input type="checkbox" id="c-36590526" checked=""/><div class="controls bullet"><span class="by">ForkMeOnTinder</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590415">parent</a><span>|</span><a href="#36591628">prev</a><span>|</span><a href="#36590962">next</a><span>|</span><label class="collapse" for="c-36590526">[-]</label><label class="expand" for="c-36590526">[1 more]</label></div><br/><div class="children"><div class="content">Copying a file isn&#x27;t great protection against bad blocks. Modern SSDs, when they fail, tend to fail catastrophically (the whole device dies all at once), rather than dying one block at a time. If you care about the data, back it up on a separate piece of hardware.</div><br/></div></div></div></div><div id="36590962" class="c"><input type="checkbox" id="c-36590962" checked=""/><div class="controls bullet"><span class="by">Pxtl</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590349">parent</a><span>|</span><a href="#36590415">prev</a><span>|</span><label class="collapse" for="c-36590962">[-]</label><label class="expand" for="c-36590962">[3 more]</label></div><br/><div class="children"><div class="content">&gt; What&#x27;s scary about it?<p>Just that I&#x27;m trusting the OS to re-duplicate it at block level on file write.  The idea that block by block you&#x27;ve got &quot;okay, this block is shared by files XYZ, this next block is unique to file Z, then the next block is back to XYZ... oh we&#x27;re editing that one?  Then it&#x27;s a new block that&#x27;s now unique to file Z too&quot;.<p>I guess I&#x27;m not used to trusting filesystems to do anything but dumb write and read.  I know they abstract away a crapload of amazing complexity in reality, I&#x27;m just used to thinking of them as dumb bags of bits.</div><br/><div id="36597665" class="c"><input type="checkbox" id="c-36597665" checked=""/><div class="controls bullet"><span class="by">tonoto</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590962">parent</a><span>|</span><a href="#36592318">next</a><span>|</span><label class="collapse" for="c-36597665">[-]</label><label class="expand" for="c-36597665">[1 more]</label></div><br/><div class="children"><div class="content">ZFS is COW with checksums so it wouldn&#x27;t edit the same blocks and there is the possibility for sending snapshots to another pool (which may&#x2F;may not use deduplication).
Although, deduplication comes with a performance cost. I had all my photos spread out on various disks and external media, sometimes extra copies (as I did not trust certain disks) - if I remember it correctly I went from 3.6T to 2.2T usage by consolidating all my photos to a deduplicated pool. All fine, but the zpool wanted way more RAM and felt slower than my other pool. 
After I removed duplicates (with help of <a href="https:&#x2F;&#x2F;github.com&#x2F;sahib&#x2F;rmlint">https:&#x2F;&#x2F;github.com&#x2F;sahib&#x2F;rmlint</a> ), I migrated my photos to an ordinary zpool instead.</div><br/></div></div><div id="36592318" class="c"><input type="checkbox" id="c-36592318" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36590303">root</a><span>|</span><a href="#36590962">parent</a><span>|</span><a href="#36597665">prev</a><span>|</span><label class="collapse" for="c-36592318">[-]</label><label class="expand" for="c-36592318">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re on ZFS you&#x27;re probably using snapshots, so all that work is already happening.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>