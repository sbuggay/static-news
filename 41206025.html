<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1723280461035" as="style"/><link rel="stylesheet" href="styles.css?v=1723280461035"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/">Grace Hopper, Nvidia&#x27;s Halfway APU</a> <span class="domain">(<a href="https://chipsandcheese.com">chipsandcheese.com</a>)</span></div><div class="subtext"><span>PaulHoule</span> | <span>21 comments</span></div><br/><div><div id="41208115" class="c"><input type="checkbox" id="c-41208115" checked=""/><div class="controls bullet"><span class="by">erulabs</span><span>|</span><a href="#41206764">next</a><span>|</span><label class="collapse" for="c-41208115">[-]</label><label class="expand" for="c-41208115">[1 more]</label></div><br/><div class="children"><div class="content">If AI remains in the cloud, nvidia wins. But I can’t help but think that if AI becomes “self-hosted”, if we return to a world where people own their own machines, AMDs APUs and interconnect technology will be absolutely dominant. Training may still be Nvidias wheelhouse, but for a single device able to do all the things (inference, rendering, and computing), AMD, at least currently, would seem to be the winner. I’d love someone more knowledgeable in AI scaling to correct me here though.<p>Maybe that’s all far enough afield to make the current state of things irrelevant?</div><br/></div></div><div id="41206764" class="c"><input type="checkbox" id="c-41206764" checked=""/><div class="controls bullet"><span class="by">MobiusHorizons</span><span>|</span><a href="#41208115">prev</a><span>|</span><a href="#41206392">next</a><span>|</span><label class="collapse" for="c-41206764">[-]</label><label class="expand" for="c-41206764">[7 more]</label></div><br/><div class="children"><div class="content">I am really surprised to see the performance of the CPU and especially the latency characteristics are so poor. The article alludes to the design likely being tuned for specific workloads, which seems like a good explanation. But I can&#x27;t help wonder if throughput at the cost of high memory latency is just not a good strategy for CPUs even with the excellent branch predictors and clever OOO work that modern CPUs bring to the table. Is this a bad take? Are we just not seeing the intended use-case where this thing really shines compared to anything else?</div><br/><div id="41207892" class="c"><input type="checkbox" id="c-41207892" checked=""/><div class="controls bullet"><span class="by">freeqaz</span><span>|</span><a href="#41206764">parent</a><span>|</span><a href="#41207461">next</a><span>|</span><label class="collapse" for="c-41207892">[-]</label><label class="expand" for="c-41207892">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the point of having the GPU on die for this? Are they expecting people to deploy one of these nodes without dedicated GPUs? It has a ton of NVLink connections which makes me think that these will often be deployed alongside GPUs which feels weird.<p>The flip side of this is if the GPU can access the main system memory then I could see this being useful for loading big models with much more efficient &quot;offloading&quot; of layers. Even though bandwidth between GPU-&gt;LPDDR5 is going to be slow, it&#x27;s still faster than what traditional PCI-E would allow.<p>The caveat here is that I imagine these machines are $$$ and enterprise only. If something like this was brought to the consumer market though I think it would be very enticing.<p>(If anybody from AMD is reading this, I feel like an architecture like this would be awesome to have. I would love to run Llama 3.1 405b at home and today I see zero path towards doing that for any &quot;reasonable&quot; amount of money (&lt;$10k?).)<p>Edit: It&#x27;s at the bottom of the article. These are designed to be meshed together via NVLink into one big cluster.<p>Makes sense. I&#x27;m really curious how the system RAM would be used in LLM training scenarios, or if these boxes are going to be used for totally different tasks that I have little context into.</div><br/></div></div><div id="41207461" class="c"><input type="checkbox" id="c-41207461" checked=""/><div class="controls bullet"><span class="by">tonyarkles</span><span>|</span><a href="#41206764">parent</a><span>|</span><a href="#41207892">prev</a><span>|</span><a href="#41206942">next</a><span>|</span><label class="collapse" for="c-41207461">[-]</label><label class="expand" for="c-41207461">[1 more]</label></div><br/><div class="children"><div class="content">We’re using the Orin AGX for edge ML. Not the same setup (Ampere) but it’s a similar situation. The GPU is excellent for what we need it to do, but the CPU cores are painful. We’re lucky… the CPUs aren’t great but there’s 12 of them and we can get away with carefully pipelining our data flows across multiple threads to get the throughput we need even though some individual stage latencies aren’t what we’d like.</div><br/></div></div><div id="41206942" class="c"><input type="checkbox" id="c-41206942" checked=""/><div class="controls bullet"><span class="by">edward28</span><span>|</span><a href="#41206764">parent</a><span>|</span><a href="#41207461">prev</a><span>|</span><a href="#41206798">next</a><span>|</span><label class="collapse" for="c-41206942">[-]</label><label class="expand" for="c-41206942">[2 more]</label></div><br/><div class="children"><div class="content">These CPUs are intended just to run miscellaneous tasks, such as loading AI models or running the cluster operating system. They don&#x27;t need to be performant, just efficient, as the GPU does all the heavy lifting.
NVIDIA also provides an option to swap the grace chips out with an x86 chip, which could deliver better performance depending on the remaining power budget though.</div><br/><div id="41207213" class="c"><input type="checkbox" id="c-41207213" checked=""/><div class="controls bullet"><span class="by">MobiusHorizons</span><span>|</span><a href="#41206764">root</a><span>|</span><a href="#41206942">parent</a><span>|</span><a href="#41206798">next</a><span>|</span><label class="collapse" for="c-41207213">[-]</label><label class="expand" for="c-41207213">[1 more]</label></div><br/><div class="children"><div class="content">If this is all there is to it, why do they have the high frequency and high l3 cache? Those seem to be optimizing for something, not just a “good enough” configuration for a part that is not the bottleneck</div><br/></div></div></div></div><div id="41206798" class="c"><input type="checkbox" id="c-41206798" checked=""/><div class="controls bullet"><span class="by">p1necone</span><span>|</span><a href="#41206764">parent</a><span>|</span><a href="#41206942">prev</a><span>|</span><a href="#41206392">next</a><span>|</span><label class="collapse" for="c-41206798">[-]</label><label class="expand" for="c-41206798">[2 more]</label></div><br/><div class="children"><div class="content">This kind of hardware makes sense for video games, and I guess GPU heavy workloads like AI might be similar? Most games have middling compute requirements but will take as much GPU power as you can give them if you&#x27;re trying to run at high resolutions&#x2F;settings. Although getting smooth gameplay at very high frame rates (~120hz+) does need a decent CPU in a lot of games.<p>Look at how atrocious the CPUs were in the PS4&#x2F;Xbone generation for an example of this.</div><br/><div id="41206848" class="c"><input type="checkbox" id="c-41206848" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41206764">root</a><span>|</span><a href="#41206798">parent</a><span>|</span><a href="#41206392">next</a><span>|</span><label class="collapse" for="c-41206848">[-]</label><label class="expand" for="c-41206848">[1 more]</label></div><br/><div class="children"><div class="content">Grace Hopper was not designed for games though.</div><br/></div></div></div></div></div></div><div id="41206392" class="c"><input type="checkbox" id="c-41206392" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#41206764">prev</a><span>|</span><a href="#41207976">next</a><span>|</span><label class="collapse" for="c-41206392">[-]</label><label class="expand" for="c-41206392">[9 more]</label></div><br/><div class="children"><div class="content">Irrelevant, but the intro reminded me that nvidia also used to dabble in chipsets like nforce, back when there was supplier variety in such.</div><br/><div id="41207196" class="c"><input type="checkbox" id="c-41207196" checked=""/><div class="controls bullet"><span class="by">MegaDeKay</span><span>|</span><a href="#41206392">parent</a><span>|</span><a href="#41207410">next</a><span>|</span><label class="collapse" for="c-41207196">[-]</label><label class="expand" for="c-41207196">[1 more]</label></div><br/><div class="children"><div class="content">One place you&#x27;ll find said chipset is in the OG XBox, where they provided the Southbridge &quot;MCPX&quot; chip as well as the GPU.<p><a href="https:&#x2F;&#x2F;classic.copetti.org&#x2F;writings&#x2F;consoles&#x2F;xbox&#x2F;#io" rel="nofollow">https:&#x2F;&#x2F;classic.copetti.org&#x2F;writings&#x2F;consoles&#x2F;xbox&#x2F;#io</a></div><br/></div></div><div id="41207410" class="c"><input type="checkbox" id="c-41207410" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#41206392">parent</a><span>|</span><a href="#41207196">prev</a><span>|</span><a href="#41206952">next</a><span>|</span><label class="collapse" for="c-41207410">[-]</label><label class="expand" for="c-41207410">[2 more]</label></div><br/><div class="children"><div class="content">I think that stopped when intel said nvidia couldn&#x27;t produce chipsets for some cpu architecture they were coming out with.<p>I don&#x27;t know if this was market savvy or a footshoot that made their ecosystem weaker.</div><br/><div id="41207823" class="c"><input type="checkbox" id="c-41207823" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41206392">root</a><span>|</span><a href="#41207410">parent</a><span>|</span><a href="#41206952">next</a><span>|</span><label class="collapse" for="c-41207823">[-]</label><label class="expand" for="c-41207823">[1 more]</label></div><br/><div class="children"><div class="content">The transition point was when Intel moved the DRAM controller and PCIe root complex onto the CPU die, merging in the northbridge and leaving the southbridge as the only separate part of the chipset. The disappearance of the Front Side Bus meant Intel platforms no longer had a good place for an integrated GPU other than on the CPU package itself, and it was years before Intel&#x27;s iGPUs caught up to the Nvidia 9400M iGPU.<p>In principle, Nvidia could have made chipsets for Intel&#x27;s newer platforms where the southbridge connects to the CPU over what is essentially four lanes of PCIe, but Intel locked out third parties from that market. But there wasn&#x27;t much room for Nvidia to provide any significant advantages over Intel&#x27;s own chipsets, except perhaps by undercutting some of Intel&#x27;s product segmentation.<p>(On the AMD side, the DRAM controller was on the CPU starting in 2003, but there was still a separate northbridge for providing AGP&#x2F;PCIe, with a relatively high-speed HyperTransport link to the CPU. AMD dropped HT starting with their APUs in 2011 and the rest of the desktop processors starting with the introduction of the Ryzen family.)</div><br/></div></div></div></div><div id="41206952" class="c"><input type="checkbox" id="c-41206952" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#41206392">parent</a><span>|</span><a href="#41207410">prev</a><span>|</span><a href="#41207976">next</a><span>|</span><label class="collapse" for="c-41206952">[-]</label><label class="expand" for="c-41206952">[5 more]</label></div><br/><div class="children"><div class="content">SoundStorm vs Dolby is such a turning point story. Nvidia had a 5 billion op&#x2F;s DSP and Dolby digital encoding on that chipset. Computers were coming into their own as powerful universal systems that could do anything.<p>Then Dolby cancelled the license. To this day you still need very fancy sound cards or exotic motherboards to be able to output good surround sound to a large number of av receivers. There are some open DTS standards that Linux can do too, dunno about windows&#x2F;Mac.<p>But it just felt like we slid so far down, that Dolby went &amp; made everything so much worse.<p>(Media software can do Dolby pass-through to let the high quality sound files through, yes. But this means you can&#x27;t do any effect processing, like audio normalization&#x2F;compression for example. And if you are playing games your amp may be getting only basic low quality surround surround, not the good many channel stuff.)</div><br/><div id="41208102" class="c"><input type="checkbox" id="c-41208102" checked=""/><div class="controls bullet"><span class="by">izacus</span><span>|</span><a href="#41206392">root</a><span>|</span><a href="#41206952">parent</a><span>|</span><a href="#41207434">next</a><span>|</span><label class="collapse" for="c-41208102">[-]</label><label class="expand" for="c-41208102">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m bit confused about your last paragraph - what&#x27;s low quality about Dolby Atmos &#x2F; DTS:X output you get for games these days?</div><br/></div></div><div id="41207434" class="c"><input type="checkbox" id="c-41207434" checked=""/><div class="controls bullet"><span class="by">throwaway81523</span><span>|</span><a href="#41206392">root</a><span>|</span><a href="#41206952">parent</a><span>|</span><a href="#41208102">prev</a><span>|</span><a href="#41206980">next</a><span>|</span><label class="collapse" for="c-41207434">[-]</label><label class="expand" for="c-41207434">[2 more]</label></div><br/><div class="children"><div class="content">Do you mean AC3?  Ffmpeg has been able to do that since forever.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dolby_Digital" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dolby_Digital</a></div><br/><div id="41207585" class="c"><input type="checkbox" id="c-41207585" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#41206392">root</a><span>|</span><a href="#41207434">parent</a><span>|</span><a href="#41206980">next</a><span>|</span><label class="collapse" for="c-41207585">[-]</label><label class="expand" for="c-41207585">[1 more]</label></div><br/><div class="children"><div class="content">Theres some debate about what patents apply, but even Dolby had to admit defeat as of 2017. So yes, a 640kbit&#x2F;s 6 channel format is available for encoding on ffmpeg &amp; some others.<p>I don&#x27;t know if games are smart enough to use this?<p>It also feels like a very low bar. It&#x27;s not awful bitrate for 6 channels but neither is it great. It&#x27;s not a pitiful number of channels but again neither is it great.<p>Last &amp; most crucially, just because one piece of software can emit ac3 doesn&#x27;t make it particularly useful for a system. I should be able to have multiple different apps doing surround sound, sending notifications to back channels or panning sounds as I prefer. Yes ffmpeg can encode 5.1 media audio to an AVR but that doesn&#x27;t really substitute for an actual surround system.<p>This is more a software problem, now that the 5.1 AC3 patents are expired. And there have been some stacks in the past where this worked on Linux for example. But it seems like modern hardware (with a Sound Open Firmware) has changed a bit and PipeWire needs to come up with a new way of doing ac3&#x2F;a52 encoding. <a href="https:&#x2F;&#x2F;gitlab.freedesktop.org&#x2F;pipewire&#x2F;pipewire&#x2F;-&#x2F;issues&#x2F;3253" rel="nofollow">https:&#x2F;&#x2F;gitlab.freedesktop.org&#x2F;pipewire&#x2F;pipewire&#x2F;-&#x2F;issues&#x2F;32...</a></div><br/></div></div></div></div></div></div></div></div><div id="41207976" class="c"><input type="checkbox" id="c-41207976" checked=""/><div class="controls bullet"><span class="by">bmacho</span><span>|</span><a href="#41206392">prev</a><span>|</span><a href="#41207343">next</a><span>|</span><label class="collapse" for="c-41207976">[-]</label><label class="expand" for="c-41207976">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The first signs of trouble appeared when vi, a simple text editor, took more than several seconds to load.<p>Can it run vi?</div><br/></div></div><div id="41207343" class="c"><input type="checkbox" id="c-41207343" checked=""/><div class="controls bullet"><span class="by">dagmx</span><span>|</span><a href="#41207976">prev</a><span>|</span><a href="#41207754">next</a><span>|</span><label class="collapse" for="c-41207343">[-]</label><label class="expand" for="c-41207343">[1 more]</label></div><br/><div class="children"><div class="content">The article talks about the difference in the pre-fetcher between the two neoverse setups (Graviton and Grace Hopper). However isn’t the prefetcher part of the core design in neoverse? How would they differ?</div><br/></div></div><div id="41207754" class="c"><input type="checkbox" id="c-41207754" checked=""/><div class="controls bullet"><span class="by">rkwasny</span><span>|</span><a href="#41207343">prev</a><span>|</span><label class="collapse" for="c-41207754">[-]</label><label class="expand" for="c-41207754">[1 more]</label></div><br/><div class="children"><div class="content">Yeah so I also benchmarked GH200 yesterday and I am also a bit puzzled TBH:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;mag-&#x2F;gpu_benchmark">https:&#x2F;&#x2F;github.com&#x2F;mag-&#x2F;gpu_benchmark</a></div><br/></div></div></div></div></div></div></div></body></html>