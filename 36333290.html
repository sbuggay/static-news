<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686819663727" as="style"/><link rel="stylesheet" href="styles.css?v=1686819663727"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://shishirpatil.github.io/gorilla/">Gorilla: Large Language Model Connected with APIs</a>Â <span class="domain">(<a href="https://shishirpatil.github.io">shishirpatil.github.io</a>)</span></div><div class="subtext"><span>throwaway888abc</span> | <span>39 comments</span></div><br/><div><div id="36337919" class="c"><input type="checkbox" id="c-36337919" checked=""/><div class="controls bullet"><span class="by">sfriedr</span><span>|</span><a href="#36334612">next</a><span>|</span><label class="collapse" for="c-36337919">[-]</label><label class="expand" for="c-36337919">[1 more]</label></div><br/><div class="children"><div class="content">Congratulation, great paper! It should have been put on HN earlier ;)<p>I have a few questions:<p>* you say (page 4): &quot;We then perform standard instruction finetuning on the
base LLaMA-7B model&quot; Could you perhaps provide a reference to the _exact_ finetuning approach you used? I&#x27;m afraid different groups of people have a different notion of &quot;standart&quot; (see for example pages 131-155 from <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.08575" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.08575</a> for various fine-tuning approaches) and without knowing exactly how fine-tuning was carried out, it can be very difficult reproduce your research and results exactly.<p>* the idea of using AST Sub-Tree Matching is nice. Could you please let me know which function in which file from your GitHub repository this is implemented in?<p>Again, great job on publishing this paper!<p>---<p>Best regards,<p>friederrr.org</div><br/></div></div><div id="36334612" class="c"><input type="checkbox" id="c-36334612" checked=""/><div class="controls bullet"><span class="by">shishirpatil</span><span>|</span><a href="#36337919">prev</a><span>|</span><a href="#36334646">next</a><span>|</span><label class="collapse" for="c-36334612">[-]</label><label class="expand" for="c-36334612">[4 more]</label></div><br/><div class="children"><div class="content">Hi HN, I&#x27;m one of the lead authors on the paper! Gorilla is an open source effort and we would love to hear from the community. Let us know if you have any questions or suggestions!!</div><br/><div id="36335416" class="c"><input type="checkbox" id="c-36335416" checked=""/><div class="controls bullet"><span class="by">gsharma</span><span>|</span><a href="#36334612">parent</a><span>|</span><a href="#36334839">next</a><span>|</span><label class="collapse" for="c-36335416">[-]</label><label class="expand" for="c-36335416">[2 more]</label></div><br/><div class="children"><div class="content">Congratulations on shipping! This is pretty cool. Building next-gen Zapier on top of this would be a great use of this.<p>There is a finite number of public APIs (100K?) which keeps the problem manageable. IMO, adding support for custom&#x2F;private APIs (something like OpenAI functions) will make this a very powerful tool.</div><br/><div id="36336985" class="c"><input type="checkbox" id="c-36336985" checked=""/><div class="controls bullet"><span class="by">shishirpatil</span><span>|</span><a href="#36334612">root</a><span>|</span><a href="#36335416">parent</a><span>|</span><a href="#36334839">next</a><span>|</span><label class="collapse" for="c-36336985">[-]</label><label class="expand" for="c-36336985">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the kind words @gsharma! Private APIs is something we are definitely thinking about.</div><br/></div></div></div></div><div id="36334839" class="c"><input type="checkbox" id="c-36334839" checked=""/><div class="controls bullet"><span class="by">joshuanapoli</span><span>|</span><a href="#36334612">parent</a><span>|</span><a href="#36335416">prev</a><span>|</span><a href="#36334646">next</a><span>|</span><label class="collapse" for="c-36334839">[-]</label><label class="expand" for="c-36334839">[1 more]</label></div><br/><div class="children"><div class="content">Is Gorilla helpful for private APIs? Is it addressing the same need that OpenAI&#x27;s new &quot;function-calling&quot; feature?</div><br/></div></div></div></div><div id="36334646" class="c"><input type="checkbox" id="c-36334646" checked=""/><div class="controls bullet"><span class="by">absk82</span><span>|</span><a href="#36334612">prev</a><span>|</span><a href="#36335901">next</a><span>|</span><label class="collapse" for="c-36334646">[-]</label><label class="expand" for="c-36334646">[6 more]</label></div><br/><div class="children"><div class="content">Your license says it can be used commercially by anyone while Llama&#x27;s license says it can only be used for research purpose. Isn&#x27;t your license bound by the Llama usage license ?</div><br/><div id="36335223" class="c"><input type="checkbox" id="c-36335223" checked=""/><div class="controls bullet"><span class="by">shishirpatil</span><span>|</span><a href="#36334646">parent</a><span>|</span><a href="#36335945">next</a><span>|</span><label class="collapse" for="c-36335223">[-]</label><label class="expand" for="c-36335223">[1 more]</label></div><br/><div class="children"><div class="content">Yes we have three set of models. One based on llama - which you are right, cannot be used commercial. We have two additional models based on MPT-7 base and Falcon-7B which can be used commercially with no obligations!</div><br/></div></div><div id="36335945" class="c"><input type="checkbox" id="c-36335945" checked=""/><div class="controls bullet"><span class="by">danShumway</span><span>|</span><a href="#36334646">parent</a><span>|</span><a href="#36335223">prev</a><span>|</span><a href="#36334873">next</a><span>|</span><label class="collapse" for="c-36335945">[-]</label><label class="expand" for="c-36335945">[1 more]</label></div><br/><div class="children"><div class="content">&gt; while Llama&#x27;s license says it can only be used for research purpose<p>Minor nitpick, but we still don&#x27;t have a clear legal answer on whether this would be binding to people who didn&#x27;t sign that agreement, because we still don&#x27;t have a clear legal answer on whether model weights are covered by copryight.<p>That being said, it is good for projects to point out that there&#x27;s uncertainty over whether Llama can be used commercially; so I agree with the overall point.</div><br/></div></div><div id="36334873" class="c"><input type="checkbox" id="c-36334873" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#36334646">parent</a><span>|</span><a href="#36335945">prev</a><span>|</span><a href="#36335156">next</a><span>|</span><label class="collapse" for="c-36334873">[-]</label><label class="expand" for="c-36334873">[1 more]</label></div><br/><div class="children"><div class="content">AIUI it uses the Llama architecture, but not Facebook&#x27;s Llama weights. It uses MPT-7B, which was trained from scratch: <a href="https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b</a></div><br/></div></div><div id="36335156" class="c"><input type="checkbox" id="c-36335156" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#36334646">parent</a><span>|</span><a href="#36334873">prev</a><span>|</span><a href="#36335901">next</a><span>|</span><label class="collapse" for="c-36335156">[-]</label><label class="expand" for="c-36335156">[2 more]</label></div><br/><div class="children"><div class="content">the code is open source but not the weights. as far as i can tell.</div><br/><div id="36335502" class="c"><input type="checkbox" id="c-36335502" checked=""/><div class="controls bullet"><span class="by">shishirpatil</span><span>|</span><a href="#36334646">root</a><span>|</span><a href="#36335156">parent</a><span>|</span><a href="#36335901">next</a><span>|</span><label class="collapse" for="c-36335502">[-]</label><label class="expand" for="c-36335502">[1 more]</label></div><br/><div class="children"><div class="content">Hi swyx, the weights are also open sourced at <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;gorilla-llm" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;gorilla-llm</a> Let us know if you are unable to access them.</div><br/></div></div></div></div></div></div><div id="36335901" class="c"><input type="checkbox" id="c-36335901" checked=""/><div class="controls bullet"><span class="by">fareesh</span><span>|</span><a href="#36334646">prev</a><span>|</span><a href="#36336514">next</a><span>|</span><label class="collapse" for="c-36335901">[-]</label><label class="expand" for="c-36335901">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s a good&#x2F;affordable GPU to run these projects locally?<p>It seems like building anything on top of these runs into either a big GPU cost for yourself or a big compute cost if you scale for others.</div><br/><div id="36336925" class="c"><input type="checkbox" id="c-36336925" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36335901">parent</a><span>|</span><a href="#36336514">next</a><span>|</span><label class="collapse" for="c-36336925">[-]</label><label class="expand" for="c-36336925">[1 more]</label></div><br/><div class="children"><div class="content">3090 can run 30b parameter models, 2x can run 65b parameter models.<p>4090 can run the same, very slightly faster for much more money.</div><br/></div></div></div></div><div id="36336514" class="c"><input type="checkbox" id="c-36336514" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#36335901">prev</a><span>|</span><a href="#36335682">next</a><span>|</span><label class="collapse" for="c-36336514">[-]</label><label class="expand" for="c-36336514">[1 more]</label></div><br/><div class="children"><div class="content">At first I was excited -- this is the second time i&#x27;m.seeing this advertised, and we are thinking through reliable API call-out strategies for louie.ai -- but then I got confused by the paper:<p>Is this really just tested against 95 API calls, and I&#x27;m guessing largely from just a small number of libriaries like pytorch?<p>More importantly, if anywhere near true, is there any reason to (so far) use this for use cases like OpenAI&#x27;s around calling generic OpenAPI style libs (zapier scenario), known specific tools, or random python libs not in that dataset?<p>I&#x27;m really thinking 3 scenarios for our users:<p>-- python libraries we know they&#x27;ll want to use ahead of time, like pandas and pygraphistry<p>-- Same for CLI, like AWS and az, and OpenAPI from and index<p>-- Long-tail that we don&#x27;t expect, esp in python + js, so on the fly, with limited time budget for inspecting GitHub&#x2F;Google&#x2F;etc<p>So far, we generally find auto approaches too unreliable for non-hobbyists, and have to tune a bunch for each tool and database we teach louie. This line of research is def interesting to us...</div><br/></div></div><div id="36335682" class="c"><input type="checkbox" id="c-36335682" checked=""/><div class="controls bullet"><span class="by">arbuge</span><span>|</span><a href="#36336514">prev</a><span>|</span><a href="#36334680">next</a><span>|</span><label class="collapse" for="c-36335682">[-]</label><label class="expand" for="c-36335682">[5 more]</label></div><br/><div class="children"><div class="content">In the colab example it appears you are using the openai python library but with the gorilla model instead of openai&#x27;s models. That works? How do you set that up?<p><pre><code>  # Query Gorilla server 
  
  def get_gorilla_response(prompt=&quot;I would like to translate from English to French.&quot;, model=&quot;gorilla-7b-hf-v0&quot;):
  try:
    completion = openai.ChatCompletion.create(
      model=model,
      messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
    )
    return completion.choices[0].message.content
  except Exception as e:
    raise_issue(e, model, prompt)</code></pre></div><br/><div id="36335724" class="c"><input type="checkbox" id="c-36335724" checked=""/><div class="controls bullet"><span class="by">lt</span><span>|</span><a href="#36335682">parent</a><span>|</span><a href="#36334680">next</a><span>|</span><label class="collapse" for="c-36335724">[-]</label><label class="expand" for="c-36335724">[4 more]</label></div><br/><div class="children"><div class="content">they point openai.api_base to their server that implements the same API</div><br/><div id="36336395" class="c"><input type="checkbox" id="c-36336395" checked=""/><div class="controls bullet"><span class="by">OkGoDoIt</span><span>|</span><a href="#36335682">root</a><span>|</span><a href="#36335724">parent</a><span>|</span><a href="#36335744">next</a><span>|</span><label class="collapse" for="c-36336395">[-]</label><label class="expand" for="c-36336395">[2 more]</label></div><br/><div class="children"><div class="content">Thatâs clever. Do other LLM APIâs do that?</div><br/><div id="36337223" class="c"><input type="checkbox" id="c-36337223" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#36335682">root</a><span>|</span><a href="#36336395">parent</a><span>|</span><a href="#36335744">next</a><span>|</span><label class="collapse" for="c-36337223">[-]</label><label class="expand" for="c-36337223">[1 more]</label></div><br/><div class="children"><div class="content">It would take you (or gpt) 3 seconds to write an openai compatible wrapper;  the inference api is trivial for all LLMs.</div><br/></div></div></div></div><div id="36335744" class="c"><input type="checkbox" id="c-36335744" checked=""/><div class="controls bullet"><span class="by">arbuge</span><span>|</span><a href="#36335682">root</a><span>|</span><a href="#36335724">parent</a><span>|</span><a href="#36336395">prev</a><span>|</span><a href="#36334680">next</a><span>|</span><label class="collapse" for="c-36335744">[-]</label><label class="expand" for="c-36335744">[1 more]</label></div><br/><div class="children"><div class="content">Ah, I missed that. Thanks.</div><br/></div></div></div></div></div></div><div id="36334680" class="c"><input type="checkbox" id="c-36334680" checked=""/><div class="controls bullet"><span class="by">tianjunz</span><span>|</span><a href="#36335682">prev</a><span>|</span><a href="#36334649">next</a><span>|</span><label class="collapse" for="c-36334680">[-]</label><label class="expand" for="c-36334680">[1 more]</label></div><br/><div class="children"><div class="content">We named the project Gorilla cause it is an cute animal that use tools  !</div><br/></div></div><div id="36334649" class="c"><input type="checkbox" id="c-36334649" checked=""/><div class="controls bullet"><span class="by">tianjunz</span><span>|</span><a href="#36334680">prev</a><span>|</span><a href="#36334357">next</a><span>|</span><label class="collapse" for="c-36334649">[-]</label><label class="expand" for="c-36334649">[1 more]</label></div><br/><div class="children"><div class="content">Hey everyone, I am one of the authors of the Gorilla project. Super excited to see how the project grows! We have released LLaMA based, MPT based (Apache 2.0) and Falcon based (Apache 2.0) models so far. Something cooler is coming soon!</div><br/></div></div><div id="36336039" class="c"><input type="checkbox" id="c-36336039" checked=""/><div class="controls bullet"><span class="by">thisisit</span><span>|</span><a href="#36334357">prev</a><span>|</span><a href="#36335592">next</a><span>|</span><label class="collapse" for="c-36336039">[-]</label><label class="expand" for="c-36336039">[1 more]</label></div><br/><div class="children"><div class="content">Previously discussed:
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36073241">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36073241</a></div><br/></div></div><div id="36335592" class="c"><input type="checkbox" id="c-36335592" checked=""/><div class="controls bullet"><span class="by">jarulraj</span><span>|</span><a href="#36336039">prev</a><span>|</span><a href="#36334208">next</a><span>|</span><label class="collapse" for="c-36335592">[-]</label><label class="expand" for="c-36335592">[1 more]</label></div><br/><div class="children"><div class="content">Neat idea, @shishirpatil! We are developing EvaDB [1] for shipping simpler, faster, and cost-effective AI apps. Can you share your thoughts on transforming the output of the Gorilla LLM to functions in EvaDB apps -- like this function that uses the HuggingFace API -- <a href="https:&#x2F;&#x2F;evadb.readthedocs.io&#x2F;en&#x2F;stable&#x2F;source&#x2F;tutorials&#x2F;07-object-segmentation-huggingface.html#register-hugging-face-segmentation-model-as-an-user-defined-function-udf-in-evadb" rel="nofollow noreferrer">https:&#x2F;&#x2F;evadb.readthedocs.io&#x2F;en&#x2F;stable&#x2F;source&#x2F;tutorials&#x2F;07-o...</a>?<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;georgia-tech-db&#x2F;eva">https:&#x2F;&#x2F;github.com&#x2F;georgia-tech-db&#x2F;eva</a></div><br/></div></div><div id="36334208" class="c"><input type="checkbox" id="c-36334208" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#36335592">prev</a><span>|</span><a href="#36335010">next</a><span>|</span><label class="collapse" for="c-36334208">[-]</label><label class="expand" for="c-36334208">[1 more]</label></div><br/><div class="children"><div class="content">Currently released weights:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;gorilla-llm" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;gorilla-llm</a></div><br/></div></div><div id="36335010" class="c"><input type="checkbox" id="c-36335010" checked=""/><div class="controls bullet"><span class="by">random5245</span><span>|</span><a href="#36334208">prev</a><span>|</span><a href="#36333917">next</a><span>|</span><label class="collapse" for="c-36335010">[-]</label><label class="expand" for="c-36335010">[1 more]</label></div><br/><div class="children"><div class="content">Is this module raw uncensored ?</div><br/></div></div><div id="36333917" class="c"><input type="checkbox" id="c-36333917" checked=""/><div class="controls bullet"><span class="by">Aditya_Garg</span><span>|</span><a href="#36335010">prev</a><span>|</span><a href="#36334041">next</a><span>|</span><label class="collapse" for="c-36333917">[-]</label><label class="expand" for="c-36333917">[2 more]</label></div><br/><div class="children"><div class="content">Heads up, your discord link is broken</div><br/><div id="36334016" class="c"><input type="checkbox" id="c-36334016" checked=""/><div class="controls bullet"><span class="by">jeron</span><span>|</span><a href="#36333917">parent</a><span>|</span><a href="#36334041">next</a><span>|</span><label class="collapse" for="c-36334016">[-]</label><label class="expand" for="c-36334016">[1 more]</label></div><br/><div class="children"><div class="content">working for me...</div><br/></div></div></div></div><div id="36334041" class="c"><input type="checkbox" id="c-36334041" checked=""/><div class="controls bullet"><span class="by">edmundsauto</span><span>|</span><a href="#36333917">prev</a><span>|</span><a href="#36334617">next</a><span>|</span><label class="collapse" for="c-36334041">[-]</label><label class="expand" for="c-36334041">[9 more]</label></div><br/><div class="children"><div class="content">How does this compare to LangChain?</div><br/><div id="36334277" class="c"><input type="checkbox" id="c-36334277" checked=""/><div class="controls bullet"><span class="by">TechBro8615</span><span>|</span><a href="#36334041">parent</a><span>|</span><a href="#36334633">next</a><span>|</span><label class="collapse" for="c-36334277">[-]</label><label class="expand" for="c-36334277">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know how its performance compares, but its architecture is completely different: LangChain is a &quot;normal&quot; software library, but Gorilla is itself an LLM:<p>&gt; Gorilla is a LLM that can provide appropriate API calls. It is trained on three massive machine learning hub datasets: Torch Hub, TensorFlow Hub and HuggingFace. We are rapidly adding new domains, including Kubernetes, GCP, AWS, OpenAPI, and more. Zero-shot Gorilla outperforms GPT-4, Chat-GPT and Claude. Gorilla is extremely reliable, and significantly reduces hallucination errors.<p>My reading of that abstract is that it&#x27;s an LLM that outputs API calls instead of natural language (or maybe it still outputs natural language, but it can use API calls during inference? I didn&#x27;t read very far), whereas LangChain is simply a software library. In theory, you could probably get Gorilla to output LangChain &quot;API&quot; (function) calls...</div><br/><div id="36334409" class="c"><input type="checkbox" id="c-36334409" checked=""/><div class="controls bullet"><span class="by">csjh</span><span>|</span><a href="#36334041">root</a><span>|</span><a href="#36334277">parent</a><span>|</span><a href="#36334405">next</a><span>|</span><label class="collapse" for="c-36334409">[-]</label><label class="expand" for="c-36334409">[4 more]</label></div><br/><div class="children"><div class="content">Still outputs natural language.
Example from their colab:<p>Input:
I would like to translate from English to Chinese.<p>Output:<p>&lt;&lt;&lt;domain&gt;&gt;&gt;: Natural Language Processing Text2Text Generation<p>&lt;&lt;&lt;api_call&gt;&gt;&gt;: M2M100ForConditionalGeneration.from_pretrained(&#x27;facebook&#x2F;m2m100_1.2B&#x27;)<p>&lt;&lt;&lt;api_provider&gt;&gt;&gt;: Hugging Face Transformers<p>&lt;&lt;&lt;explanation&gt;&gt;&gt;: 1. Import M2M100ForConditionalGeneration and M2M100Tokenizer from the transformers library.<p>2. Load the pre-trained M2M100 model and tokenizer using the from_pretrained() method. The model is trained to translate text from English to Chinese, among other languages.<p>3. Encode the input text in English using the tokenizer.<p>4. Generate the translation using the model.generate() method.<p>5. Decode the output tokens using the tokenizer to obtain the translated text in Chinese.<p>6. Print the translated text.</div><br/><div id="36335301" class="c"><input type="checkbox" id="c-36335301" checked=""/><div class="controls bullet"><span class="by">shishirpatil</span><span>|</span><a href="#36334041">root</a><span>|</span><a href="#36334409">parent</a><span>|</span><a href="#36334405">next</a><span>|</span><label class="collapse" for="c-36335301">[-]</label><label class="expand" for="c-36335301">[3 more]</label></div><br/><div class="children"><div class="content">Hi @csjh, we trained to model to also additionally output additional context so it would be useful for a downstream task. We wrapped the API call with special decorator so it&#x27;s easier to just regex. Would you like to just have the API instead? Happy to release an API only model if there is wider interest - It&#x27;s a strictly easier task for Gorilla LLM :)</div><br/><div id="36335596" class="c"><input type="checkbox" id="c-36335596" checked=""/><div class="controls bullet"><span class="by">jadbox</span><span>|</span><a href="#36334041">root</a><span>|</span><a href="#36335301">parent</a><span>|</span><a href="#36335439">next</a><span>|</span><label class="collapse" for="c-36335596">[-]</label><label class="expand" for="c-36335596">[1 more]</label></div><br/><div class="children"><div class="content">Ideally I&#x27;d like the result to be in a clean json with a key that&#x27;s strictly the result and other keys for context information. It would reduce needing to use regex everywhere.</div><br/></div></div><div id="36335439" class="c"><input type="checkbox" id="c-36335439" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#36334041">root</a><span>|</span><a href="#36335301">parent</a><span>|</span><a href="#36335596">prev</a><span>|</span><a href="#36334405">next</a><span>|</span><label class="collapse" for="c-36335439">[-]</label><label class="expand" for="c-36335439">[1 more]</label></div><br/><div class="children"><div class="content">I would love this!</div><br/></div></div></div></div></div></div><div id="36334405" class="c"><input type="checkbox" id="c-36334405" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#36334041">root</a><span>|</span><a href="#36334277">parent</a><span>|</span><a href="#36334409">prev</a><span>|</span><a href="#36334633">next</a><span>|</span><label class="collapse" for="c-36334405">[-]</label><label class="expand" for="c-36334405">[2 more]</label></div><br/><div class="children"><div class="content">&quot;We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls&quot;<p>Sounds like it&#x27;s another LLaMA variant specifically fine tuned for API calls.</div><br/><div id="36334641" class="c"><input type="checkbox" id="c-36334641" checked=""/><div class="controls bullet"><span class="by">shishirpatil</span><span>|</span><a href="#36334041">root</a><span>|</span><a href="#36334405">parent</a><span>|</span><a href="#36334633">next</a><span>|</span><label class="collapse" for="c-36334641">[-]</label><label class="expand" for="c-36334641">[1 more]</label></div><br/><div class="children"><div class="content">Good point. This was the original release, we now also have Apache-2.0 licensed models finetuned on MPT-7B and Falcon-7B!</div><br/></div></div></div></div></div></div><div id="36334633" class="c"><input type="checkbox" id="c-36334633" checked=""/><div class="controls bullet"><span class="by">shishirpatil</span><span>|</span><a href="#36334041">parent</a><span>|</span><a href="#36334277">prev</a><span>|</span><a href="#36334617">next</a><span>|</span><label class="collapse" for="c-36334633">[-]</label><label class="expand" for="c-36334633">[1 more]</label></div><br/><div class="children"><div class="content">Langchain is a terrific project that tries to teach agents how to use tools using prompting. Our take on this is that prompting is not scalable if you want to pick between 1000s of APIs. So Gorilla is a LLM that can pick and write the semantically and syntactically correct API for you to call! A drop in replacement into Langchain!</div><br/></div></div></div></div></div></div></div></div></div></body></html>