<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1687078847126" as="style"/><link rel="stylesheet" href="styles.css?v=1687078847126"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://falconllm.tii.ae/">Falcon LLM – A 40B Model</a> <span class="domain">(<a href="https://falconllm.tii.ae">falconllm.tii.ae</a>)</span></div><div class="subtext"><span>Risyandi94</span> | <span>31 comments</span></div><br/><div><div id="36378227" class="c"><input type="checkbox" id="c-36378227" checked=""/><div class="controls bullet"><span class="by">LeoPanthera</span><span>|</span><a href="#36377415">next</a><span>|</span><label class="collapse" for="c-36378227">[-]</label><label class="expand" for="c-36378227">[3 more]</label></div><br/><div class="children"><div class="content">&gt; to remove machine generated text and <i>adult content</i><p>Why are tech companies so puritanical? Adult content is not immoral.</div><br/><div id="36378274" class="c"><input type="checkbox" id="c-36378274" checked=""/><div class="controls bullet"><span class="by">comfypotato</span><span>|</span><a href="#36378227">parent</a><span>|</span><a href="#36378262">next</a><span>|</span><label class="collapse" for="c-36378274">[-]</label><label class="expand" for="c-36378274">[1 more]</label></div><br/><div class="children"><div class="content">Whether or not it’s immoral is opinion-based. It’s probably typically in order to not alienate those who think it is. Effectively a business decision even in the context of an open model.</div><br/></div></div><div id="36378262" class="c"><input type="checkbox" id="c-36378262" checked=""/><div class="controls bullet"><span class="by">0898</span><span>|</span><a href="#36378227">parent</a><span>|</span><a href="#36378274">prev</a><span>|</span><a href="#36377415">next</a><span>|</span><label class="collapse" for="c-36378262">[-]</label><label class="expand" for="c-36378262">[1 more]</label></div><br/><div class="children"><div class="content">TII is based in the United Arab Emirates.</div><br/></div></div></div></div><div id="36377415" class="c"><input type="checkbox" id="c-36377415" checked=""/><div class="controls bullet"><span class="by">elahieh</span><span>|</span><a href="#36378227">prev</a><span>|</span><a href="#36376733">next</a><span>|</span><label class="collapse" for="c-36377415">[-]</label><label class="expand" for="c-36377415">[6 more]</label></div><br/><div class="children"><div class="content">Is there a guide out there for dummies on how to try a ChatGPT like instance of this on a VM cheaply? eg pay $1 or $2 an hour for a point and click experience with the instruct version of this. A docker image perhaps.<p>Reading posts on r&#x2F;LocalLLAMA is people’s trial and error experiences, quite random.</div><br/><div id="36378349" class="c"><input type="checkbox" id="c-36378349" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36377415">parent</a><span>|</span><a href="#36377581">next</a><span>|</span><label class="collapse" for="c-36378349">[-]</label><label class="expand" for="c-36378349">[1 more]</label></div><br/><div class="children"><div class="content">For Falcon specifically, this is easy, it&#x27;s embedded here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;falcon#demo" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;falcon#demo</a> or you can access the demo here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;falcon-chat" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;falcon-chat</a><p>I just tested both and it&#x27;s pretty zippy (faster than AMD&#x27;s recent live MI300 demo).<p>For llama-based models, recently I&#x27;ve been using <a href="https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllama">https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllama</a> a lot. It has a Dockerfile&#x2F;docker-compose.yml so it should be pretty easy to get going. llama.cpp is the other easy one and the most recent updates put it&#x27;s CUDA support only about 25% slower and generally is a simple `make` with a flag depending on which GPU you support you want and has basically no dependencies.<p>Also, here&#x27;s a Colab notebook that should let shows you run up to 13b quantized models (12G RAM, 80G disk, Tesla T4 16G) for free: <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1QzFsWru1YLnTVK77itWEASCPnIH7IDPo" rel="nofollow noreferrer">https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1QzFsWru1YLnTVK77itW...</a> (for Falcon, replace w&#x2F; Koboldcpp or ctransformers)</div><br/></div></div><div id="36377581" class="c"><input type="checkbox" id="c-36377581" checked=""/><div class="controls bullet"><span class="by">joshka</span><span>|</span><a href="#36377415">parent</a><span>|</span><a href="#36378349">prev</a><span>|</span><a href="#36377503">next</a><span>|</span><label class="collapse" for="c-36377581">[-]</label><label class="expand" for="c-36377581">[1 more]</label></div><br/><div class="children"><div class="content">Take a look at youtube vids for this. Mainly because you&#x27;re going to see people show all the steps when presenting instead of skipping them when talking about what they did. E.g. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=KenORQDCXV0">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=KenORQDCXV0</a></div><br/></div></div><div id="36377503" class="c"><input type="checkbox" id="c-36377503" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#36377415">parent</a><span>|</span><a href="#36377581">prev</a><span>|</span><a href="#36376733">next</a><span>|</span><label class="collapse" for="c-36377503">[-]</label><label class="expand" for="c-36377503">[3 more]</label></div><br/><div class="children"><div class="content">A small cheap VPS won’t have the compute or RAM to run these. The best way (and the intent) is to run it locally. A fast box with at least 32GiB of RAM (or VRAM for a GPU) can run many of the models that work with llama.cpp. For this 40G model you will need more like 48GiB of RAM.<p>Apple Silicon is pretty good for local models due to the unified CPU&#x2F;GPU memory but a gaming PC is probably the most cost effective option.<p>If you want to just play around and don’t have a box big enough then temporarily renting one at Hetzner or OVH is pretty cost effective.</div><br/><div id="36377683" class="c"><input type="checkbox" id="c-36377683" checked=""/><div class="controls bullet"><span class="by">gardnr</span><span>|</span><a href="#36377415">root</a><span>|</span><a href="#36377503">parent</a><span>|</span><a href="#36378013">next</a><span>|</span><label class="collapse" for="c-36377683">[-]</label><label class="expand" for="c-36377683">[1 more]</label></div><br/><div class="children"><div class="content">Falcon doesn&#x27;t work in llama.cpp yet: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1602">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1602</a></div><br/></div></div><div id="36378013" class="c"><input type="checkbox" id="c-36378013" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#36377415">root</a><span>|</span><a href="#36377503">parent</a><span>|</span><a href="#36377683">prev</a><span>|</span><a href="#36376733">next</a><span>|</span><label class="collapse" for="c-36378013">[-]</label><label class="expand" for="c-36378013">[1 more]</label></div><br/><div class="children"><div class="content">They said 1 or 2 bucks an hour. You can get an A100 for that.</div><br/></div></div></div></div></div></div><div id="36376733" class="c"><input type="checkbox" id="c-36376733" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#36377415">prev</a><span>|</span><a href="#36377201">next</a><span>|</span><label class="collapse" for="c-36376733">[-]</label><label class="expand" for="c-36376733">[6 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t do so great on the leaderboards: <a href="https:&#x2F;&#x2F;tatsu-lab.github.io&#x2F;alpaca_eval&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tatsu-lab.github.io&#x2F;alpaca_eval&#x2F;</a></div><br/><div id="36376781" class="c"><input type="checkbox" id="c-36376781" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#36376733">parent</a><span>|</span><a href="#36377201">next</a><span>|</span><label class="collapse" for="c-36376781">[-]</label><label class="expand" for="c-36376781">[5 more]</label></div><br/><div class="children"><div class="content">Depends on the benchmark.  Does well on other metrics when compared to open models.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderboard" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderb...</a></div><br/><div id="36378092" class="c"><input type="checkbox" id="c-36378092" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#36376733">root</a><span>|</span><a href="#36376781">parent</a><span>|</span><a href="#36376904">next</a><span>|</span><label class="collapse" for="c-36378092">[-]</label><label class="expand" for="c-36378092">[2 more]</label></div><br/><div class="children"><div class="content">That leaderboard is incorrect<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderboard&#x2F;discussions&#x2F;63" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderb...</a></div><br/><div id="36378386" class="c"><input type="checkbox" id="c-36378386" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36376733">root</a><span>|</span><a href="#36378092">parent</a><span>|</span><a href="#36376904">next</a><span>|</span><label class="collapse" for="c-36378386">[-]</label><label class="expand" for="c-36378386">[1 more]</label></div><br/><div class="children"><div class="content">Actually it&#x27;s HF&#x27;s leaderboard that&#x27;s bugged. Falcon is only on top since their MMLU scores are bugged across all LLaMA models: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderboard&#x2F;discussions&#x2F;63" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderb...</a></div><br/></div></div></div></div><div id="36376904" class="c"><input type="checkbox" id="c-36376904" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#36376733">root</a><span>|</span><a href="#36376781">parent</a><span>|</span><a href="#36378092">prev</a><span>|</span><a href="#36377201">next</a><span>|</span><label class="collapse" for="c-36376904">[-]</label><label class="expand" for="c-36376904">[2 more]</label></div><br/><div class="children"><div class="content">Given that HellaSwag performance seems to correlate with reasoning ability more than other benchmarks, Falcon certainly look promising! Hopefully this is a clean result and not the product of dataset contamination.</div><br/><div id="36378049" class="c"><input type="checkbox" id="c-36378049" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#36376733">root</a><span>|</span><a href="#36376904">parent</a><span>|</span><a href="#36377201">next</a><span>|</span><label class="collapse" for="c-36378049">[-]</label><label class="expand" for="c-36378049">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve given it a try, to having a chat is good, to follow langchain prompts it&#x27;s not.<p>I guess it depends on the type of work you want to extract from it.</div><br/></div></div></div></div></div></div></div></div><div id="36377201" class="c"><input type="checkbox" id="c-36377201" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#36376733">prev</a><span>|</span><a href="#36376831">next</a><span>|</span><label class="collapse" for="c-36377201">[-]</label><label class="expand" for="c-36377201">[5 more]</label></div><br/><div class="children"><div class="content">Worth noting that according to the initial press release, they&#x27;re also working on Falcon 180B, which would be the largest (and likely most effective) open source model by far.</div><br/><div id="36377298" class="c"><input type="checkbox" id="c-36377298" checked=""/><div class="controls bullet"><span class="by">jumpCastle</span><span>|</span><a href="#36377201">parent</a><span>|</span><a href="#36376831">next</a><span>|</span><label class="collapse" for="c-36377298">[-]</label><label class="expand" for="c-36377298">[4 more]</label></div><br/><div class="children"><div class="content">Not by far, there&#x27;s bloom and opt if you count it.</div><br/><div id="36377352" class="c"><input type="checkbox" id="c-36377352" checked=""/><div class="controls bullet"><span class="by">lordofgibbons</span><span>|</span><a href="#36377201">root</a><span>|</span><a href="#36377298">parent</a><span>|</span><a href="#36377882">next</a><span>|</span><label class="collapse" for="c-36377352">[-]</label><label class="expand" for="c-36377352">[2 more]</label></div><br/><div class="children"><div class="content">Those weren&#x27;t open source though.<p>OPT is non-commercial, and BLOOM had this extremely deranged OpenRAIL license which includes user hostile things like forced updates, and other weird restrictions.</div><br/><div id="36377695" class="c"><input type="checkbox" id="c-36377695" checked=""/><div class="controls bullet"><span class="by">gardnr</span><span>|</span><a href="#36377201">root</a><span>|</span><a href="#36377352">parent</a><span>|</span><a href="#36377882">next</a><span>|</span><label class="collapse" for="c-36377695">[-]</label><label class="expand" for="c-36377695">[1 more]</label></div><br/><div class="children"><div class="content">Falcon was intially release under a weird modified license. It looks like they changed it to Apache May 31st: <a href="https:&#x2F;&#x2F;www.tii.ae&#x2F;news&#x2F;uaes-falcon-40b-now-royalty-free" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tii.ae&#x2F;news&#x2F;uaes-falcon-40b-now-royalty-free</a></div><br/></div></div></div></div><div id="36377882" class="c"><input type="checkbox" id="c-36377882" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#36377201">root</a><span>|</span><a href="#36377298">parent</a><span>|</span><a href="#36377352">prev</a><span>|</span><a href="#36376831">next</a><span>|</span><label class="collapse" for="c-36377882">[-]</label><label class="expand" for="c-36377882">[1 more]</label></div><br/><div class="children"><div class="content">Bloom and OPT are weak, predictably so. Nobody uses them (except for research sometimes, and even then rarely). It doesn&#x27;t make sense to look for hardware to run a 176B model when a 13B outperforms it across the board.</div><br/></div></div></div></div></div></div><div id="36376831" class="c"><input type="checkbox" id="c-36376831" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#36377201">prev</a><span>|</span><a href="#36376112">next</a><span>|</span><label class="collapse" for="c-36376831">[-]</label><label class="expand" for="c-36376831">[2 more]</label></div><br/><div class="children"><div class="content">17 days ago: 
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36145185">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36145185</a></div><br/><div id="36377423" class="c"><input type="checkbox" id="c-36377423" checked=""/><div class="controls bullet"><span class="by">kytazo</span><span>|</span><a href="#36376831">parent</a><span>|</span><a href="#36376112">next</a><span>|</span><label class="collapse" for="c-36377423">[-]</label><label class="expand" for="c-36377423">[1 more]</label></div><br/><div class="children"><div class="content">Not everyone always catches up with all the news, its important enough for a report if you ask me.</div><br/></div></div></div></div><div id="36376112" class="c"><input type="checkbox" id="c-36376112" checked=""/><div class="controls bullet"><span class="by">Risyandi94</span><span>|</span><a href="#36376831">prev</a><span>|</span><a href="#36377050">next</a><span>|</span><label class="collapse" for="c-36376112">[-]</label><label class="expand" for="c-36376112">[1 more]</label></div><br/><div class="children"><div class="content">Falcon LLM is a foundational large language model (LLM) with 40 billion parameters trained on one trillion tokens. TII has now released Falcon LLM – a 40B model.</div><br/></div></div><div id="36377050" class="c"><input type="checkbox" id="c-36377050" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#36376112">prev</a><span>|</span><label class="collapse" for="c-36377050">[-]</label><label class="expand" for="c-36377050">[7 more]</label></div><br/><div class="children"><div class="content">Has anybody gotten this running on consumer hardware ala llama or is that not in the cards?</div><br/><div id="36377336" class="c"><input type="checkbox" id="c-36377336" checked=""/><div class="controls bullet"><span class="by">orost</span><span>|</span><a href="#36377050">parent</a><span>|</span><a href="#36377196">next</a><span>|</span><label class="collapse" for="c-36377336">[-]</label><label class="expand" for="c-36377336">[1 more]</label></div><br/><div class="children"><div class="content">Experimental Falcon inference via ggml (so on CPU): <a href="https:&#x2F;&#x2F;github.com&#x2F;cmp-nct&#x2F;ggllm.cpp">https:&#x2F;&#x2F;github.com&#x2F;cmp-nct&#x2F;ggllm.cpp</a><p>It has problems but it does work</div><br/></div></div><div id="36377196" class="c"><input type="checkbox" id="c-36377196" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#36377050">parent</a><span>|</span><a href="#36377336">prev</a><span>|</span><a href="#36377176">next</a><span>|</span><label class="collapse" for="c-36377196">[-]</label><label class="expand" for="c-36377196">[2 more]</label></div><br/><div class="children"><div class="content">llama.cpp just got Falcon support (not yet merged), so you could run it on just RAM. Not too fast though.</div><br/><div id="36377719" class="c"><input type="checkbox" id="c-36377719" checked=""/><div class="controls bullet"><span class="by">gardnr</span><span>|</span><a href="#36377050">root</a><span>|</span><a href="#36377196">parent</a><span>|</span><a href="#36377176">next</a><span>|</span><label class="collapse" for="c-36377719">[-]</label><label class="expand" for="c-36377719">[1 more]</label></div><br/><div class="children"><div class="content">In case anyone wants to follow along: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1602">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1602</a></div><br/></div></div></div></div><div id="36377176" class="c"><input type="checkbox" id="c-36377176" checked=""/><div class="controls bullet"><span class="by">bestcoder69</span><span>|</span><a href="#36377050">parent</a><span>|</span><a href="#36377196">prev</a><span>|</span><label class="collapse" for="c-36377176">[-]</label><label class="expand" for="c-36377176">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only seen people mention that it runs really slow, even on like A100s.</div><br/><div id="36377873" class="c"><input type="checkbox" id="c-36377873" checked=""/><div class="controls bullet"><span class="by">danieldk</span><span>|</span><a href="#36377050">root</a><span>|</span><a href="#36377176">parent</a><span>|</span><a href="#36378251">next</a><span>|</span><label class="collapse" for="c-36377873">[-]</label><label class="expand" for="c-36377873">[1 more]</label></div><br/><div class="children"><div class="content">There are no big differences compared to other LLM architecturally. The largest differences compared to NeoX are: no biases in linear layers, shared heads for the key and value representations (but not query).<p>Of course, it has 40B parameters, but there is also a 7B parameter version. The primary issue is that the current upstream version (on Huggingface) hasn&#x27;t implemented key-value caching correctly. KV caching is needed to bring the complexity down from O(n^3) to O(n^2). The issues are: (1) their implementation uses Torch&#x27; scaled dot-product attention, which uses incorrect causal masks when the query&#x2F;key sizes are not the same (which it the case when generating with a cache). (2) They don&#x27;t index the rotary embeddings correctly when using key-value cache, so the rotary embedding of the first token is used for all generated tokens. Together, this causes the model to output garbage and it only works when using it without KV caching, making it very slow.<p>However, this is not a property of the model and they will probably fix this soon. E.g. the transformer library that we are currently developing supports Falcon with key-value caching and it the speed is on-par with other models of the same size:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;curated-transformers&#x2F;blob&#x2F;main&#x2F;curated_transformers&#x2F;models&#x2F;refined_web_model&#x2F;layer.py">https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;curated-transformers&#x2F;blob&#x2F;main&#x2F;...</a><p>(This is a correct implementation of the decoder layer.)</div><br/></div></div><div id="36378251" class="c"><input type="checkbox" id="c-36378251" checked=""/><div class="controls bullet"><span class="by">kbrkbr</span><span>|</span><a href="#36377050">root</a><span>|</span><a href="#36377176">parent</a><span>|</span><a href="#36377873">prev</a><span>|</span><label class="collapse" for="c-36378251">[-]</label><label class="expand" for="c-36378251">[1 more]</label></div><br/><div class="children"><div class="content">I tried it using oobabooga&#x27;s webui side by side with Alpaca 65B loaded in 4 bit on the same AWS instance with 64GB of VRAM.<p>While Alpaca produced 3 tokens&#x2F;sec, Falcon produced 0.17 tokens&#x2F;sec.<p>So it is very slow with the current tooling still.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>