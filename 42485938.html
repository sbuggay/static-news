<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734944451746" as="style"/><link rel="stylesheet" href="styles.css?v=1734944451746"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693">GPT-5 is behind schedule</a> <span class="domain">(<a href="https://www.wsj.com">www.wsj.com</a>)</span></div><div class="subtext"><span>owenthejumper</span> | <span>642 comments</span></div><br/><div><div id="42486322" class="c"><input type="checkbox" id="c-42486322" checked=""/><div class="controls bullet"><span class="by">neonate</span><span>|</span><a href="#42491528">next</a><span>|</span><label class="collapse" for="c-42486322">[-]</label><label class="expand" for="c-42486322">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.ph&#x2F;L7fOF" rel="nofollow">https:&#x2F;&#x2F;archive.ph&#x2F;L7fOF</a></div><br/></div></div><div id="42491528" class="c"><input type="checkbox" id="c-42491528" checked=""/><div class="controls bullet"><span class="by">SamPatt</span><span>|</span><a href="#42486322">prev</a><span>|</span><a href="#42490004">next</a><span>|</span><label class="collapse" for="c-42491528">[-]</label><label class="expand" for="c-42491528">[90 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure the debate over the definition of AGI is important and will continue for a while, but... I can&#x27;t care about it anymore.<p>Between Perplexity searching and summarizing, Claude explaining, and qwen (and other tools) coding, I&#x27;m already as happy as can be with whatever you want to call this level of intelligence.<p>Just today I used a completely local AI research tool, based on Ollama. It worked great.<p>Maybe it won&#x27;t get much better? Or maybe it&#x27;ll take decades instead of years? Ok. I remember not having these tools. I never want to go back.</div><br/><div id="42491843" class="c"><input type="checkbox" id="c-42491843" checked=""/><div class="controls bullet"><span class="by">atonse</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492931">next</a><span>|</span><label class="collapse" for="c-42491843">[-]</label><label class="expand" for="c-42491843">[51 more]</label></div><br/><div class="children"><div class="content">Same here.<p>The ability to “talk to an expert” about any topic I’m curious about and ask very specific questions has been invaluable to me.<p>It reminds me of being a kid and asking my grandpa a million questions, like how light bulbs worked, or what was inside his radio, or how do we have day and night.<p>And before anyone talks about accuracy or hallucinations, these conversations usually are treated as starting off points to then start googling specific terms, people, laws, treaties, etc to dig deeper and verify.<p>Last year during a visit to my first Indian reservation, I had a whole bunch of questions that nobody in person had answers to. And ChatGPT was invaluable in understanding concepts like where a reservation’s autonomy begins and ends. And why certain tribes are richer than others. What happens when someone calls 911 on a reservation. Or speeds. Or wants to start a factory without worrying about import&#x2F;export rules. And what causes some tribes to lose their language faster than others. And 20 other questions like this.<p>And most of those resulted in google searches to verify the information. But I literally could never do this before.<p>Same this year when I’m visiting family in India. To learn about the politics, the major players, WHY they are considered major players (like the Chief Minister of Bengal or Uttar Pradesh or Maharashtra being major players because of their populations and economies). Criticisms, explanations of laws, etc etc.<p>For insanely curious people who often feel unsatisfied with the answers given by those around them, it’s the greatest thing ever.</div><br/><div id="42492137" class="c"><input type="checkbox" id="c-42492137" checked=""/><div class="controls bullet"><span class="by">sollewitt</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491843">parent</a><span>|</span><a href="#42492048">next</a><span>|</span><label class="collapse" for="c-42492137">[-]</label><label class="expand" for="c-42492137">[16 more]</label></div><br/><div class="children"><div class="content">LLMs suffer from the &quot;Igon Value Problem&quot; <a href="https:&#x2F;&#x2F;rationalwiki.org&#x2F;wiki&#x2F;Igon_Value_Problem" rel="nofollow">https:&#x2F;&#x2F;rationalwiki.org&#x2F;wiki&#x2F;Igon_Value_Problem</a><p>Similar to reading a pop sci book, you&#x27;re getting an entertainment from a thing with no actual understanding of the source material rather than an education.</div><br/><div id="42492630" class="c"><input type="checkbox" id="c-42492630" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492137">parent</a><span>|</span><a href="#42492288">next</a><span>|</span><label class="collapse" for="c-42492630">[-]</label><label class="expand" for="c-42492630">[8 more]</label></div><br/><div class="children"><div class="content">You just state this as if it was obviously true, but I don&#x27;t see how. Why is using LLM like reading a pop sci book and not like reading a history book? Or even less like either, because you have to continually ask questions to get anything?</div><br/><div id="42492665" class="c"><input type="checkbox" id="c-42492665" checked=""/><div class="controls bullet"><span class="by">Fargren</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492630">parent</a><span>|</span><a href="#42492288">next</a><span>|</span><label class="collapse" for="c-42492665">[-]</label><label class="expand" for="c-42492665">[7 more]</label></div><br/><div class="children"><div class="content">A history book is written by someone who knows the topic, and then reviewed by more people who also know the topic, and then it&#x27;s out there where people can read it and criticize it if it&#x27;s wrong about the topic.<p>A question asked to an AI is not reviewed by anyone, and it&#x27;s ephemeral. The AI can answer &quot;yes&quot; today, and &quot;no&quot; tomorrow, so it&#x27;s not possible to build a consensus on whether it answers specific questions correctly.</div><br/><div id="42492736" class="c"><input type="checkbox" id="c-42492736" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492665">parent</a><span>|</span><a href="#42492814">next</a><span>|</span><label class="collapse" for="c-42492736">[-]</label><label class="expand" for="c-42492736">[4 more]</label></div><br/><div class="children"><div class="content">A pop sci fi book can be written by someone who knows the topic and reviewed by people who know the topic — and a history book can also not.<p>LLM generated answers are more comparable to ad-hoc human expert&#x27;s answers and not to written books. But it&#x27;s much simpler to statistically evaluate and correct them. That is how we can know that, on average, LLMs are improving and are outperforming human experts on an increasing number of tasks and topics.</div><br/><div id="42492816" class="c"><input type="checkbox" id="c-42492816" checked=""/><div class="controls bullet"><span class="by">jacobolus</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492736">parent</a><span>|</span><a href="#42492814">next</a><span>|</span><label class="collapse" for="c-42492816">[-]</label><label class="expand" for="c-42492816">[3 more]</label></div><br/><div class="children"><div class="content">In my experience LLM generated answers are more comparable to an ad-hoc answer by a human with no special expertise, moderate google skills, but good bullshitting skills spending a few minutes searching the web, reading what they find and synthesizing it, waiting long enough for the details to get kind of hazy, and then writing up an answer off the top of their head based on that, filling in any missing material by just making something up. They can do this significantly faster than a human undergraduate student might be able to, so if you need someone to do this task very quickly &#x2F; prolifically this can be beneficial (e.g. this could be effective for generating banter from video game characters, for astroturfing social media, or for cheating on student essays). It&#x27;s not a good way to get expert answers about anything though.<p>More specifically: I&#x27;ve never gotten an answer from an LLM to a tricky or obscure question about a subject I already know anything about that seemed remotely competent. The answers to basic and obvious questions are sometimes okay, but also sometimes completely wrong (but competently stated).</div><br/><div id="42492865" class="c"><input type="checkbox" id="c-42492865" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492816">parent</a><span>|</span><a href="#42492814">next</a><span>|</span><label class="collapse" for="c-42492865">[-]</label><label class="expand" for="c-42492865">[2 more]</label></div><br/><div class="children"><div class="content">More like &quot;have already skimmed half of the entire Internet in the past&quot;, but yeah. That&#x27;s <i>exactly</i> the mental model IMO one should have with LLMs.<p>Of course don&#x27;t forget that &quot;writing up an answer off the top of their head based on that, filling in any missing material by just making something up&quot; is what everyone does all the time, and in particular it&#x27;s what <i>experts do in their areas of expertise</i>. How often those snap answers and hasty extrapolations turn out correct is, literally, how you measure <i>understanding</i>.<p>EDIT:<p>There&#x27;s some deep irony here, because with LLMs being &quot;all system 1, no system 2&quot;, we&#x27;re trying to give them the same crutches we use on the road to understanding, but have them move the opposite direction. Take &quot;chain of thought&quot; - saying &quot;let&#x27;s think step by step&quot; and then explicitly going through your reasoning is <i>not</i> understanding - it&#x27;s the <i>direct opposite of it</i>. Think of a student that solves a math problem step by step - they&#x27;re not demonstrating understanding or mastery of the subject. On the contrary, they&#x27;re just demonstrating they can <i>emulate</i> understanding by more mechanistic, procedural means.</div><br/><div id="42492898" class="c"><input type="checkbox" id="c-42492898" checked=""/><div class="controls bullet"><span class="by">jacobolus</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492865">parent</a><span>|</span><a href="#42492814">next</a><span>|</span><label class="collapse" for="c-42492898">[-]</label><label class="expand" for="c-42492898">[1 more]</label></div><br/><div class="children"><div class="content">Okay, but if you read written work by an expert (e.g. a book published by a reputable academic press or a journal article in a peer-reviewed journal), you get a result whose details were all checked out, and can be relied on to some extent. By looking up in the citation graph you can track down <i>their</i> sources, cross-check claims against other scholars&#x27;, look up survey sources putting the work in context, think critically about each author&#x27;s biases, etc., and it&#x27;s possible to come to some kind of careful analysis of the work&#x27;s credibility and assess the truth value of claims made. By doing careful search and study it&#x27;s possible to get to some sense of the scholarly consensus about a topic and some idea of the level of controversy about various details or interpretations.<p>If instead you are reading the expert&#x27;s blog post or hastily composed email or chatting with them on an airplane you get a different level of polish and care, but again you can use context to evaluate the source and claims made. Often the result is still &quot;oh yeah this seems pretty insightful&quot; but sometimes &quot;wow, this person shouldn&#x27;t be speculating outside of their area of expertise because they have no clue about this&quot;.<p>With LLM output, the appropriate assessment (at least in any that I have tried, which is far from exhaustive) is basically always &quot;this is vaguely topical bullshit; you shouldn&#x27;t trust this at all&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="42492814" class="c"><input type="checkbox" id="c-42492814" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492665">parent</a><span>|</span><a href="#42492736">prev</a><span>|</span><a href="#42492807">next</a><span>|</span><label class="collapse" for="c-42492814">[-]</label><label class="expand" for="c-42492814">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>A question asked to an AI is not reviewed by anyone, and it&#x27;s ephemeral. The AI can answer &quot;yes&quot; today, and &quot;no&quot; tomorrow, so it&#x27;s not possible to build a consensus on whether it answers specific questions correctly.</i><p>It&#x27;s even more so with humans! Most of our conversations are, and has always been, ephemeral and unverifiable (and there&#x27;s plenty of people who want to undo the little of permanence and verifiability we still have on the Internet...). Along the dimension of permanence and verifiability, asking an LLM is actually <i>much better</i> than asking a human - there&#x27;s always a log of the conversation you had with the AI produced and stored somewhere for at least a while (even if only until you clear your temp folder), and if you can get ahold of that log, you can not just <i>verify</i> the answers, you can actually <i>debug the AI</i>. You can rerun the conversation with different parameters, different prompting, perhaps even inspect the inference process itself. You can do that ten times, hundred times, a million times, and won&#x27;t be asked to come to Hague and explain yourself. Now try that with a human :).</div><br/></div></div><div id="42492807" class="c"><input type="checkbox" id="c-42492807" checked=""/><div class="controls bullet"><span class="by">SheinhardtWigCo</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492665">parent</a><span>|</span><a href="#42492814">prev</a><span>|</span><a href="#42492288">next</a><span>|</span><label class="collapse" for="c-42492807">[-]</label><label class="expand" for="c-42492807">[1 more]</label></div><br/><div class="children"><div class="content">The same is true of Google, no?</div><br/></div></div></div></div></div></div><div id="42492288" class="c"><input type="checkbox" id="c-42492288" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492137">parent</a><span>|</span><a href="#42492630">prev</a><span>|</span><a href="#42492449">next</a><span>|</span><label class="collapse" for="c-42492288">[-]</label><label class="expand" for="c-42492288">[1 more]</label></div><br/><div class="children"><div class="content">So they have reached human level intelligence :D</div><br/></div></div><div id="42492449" class="c"><input type="checkbox" id="c-42492449" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492137">parent</a><span>|</span><a href="#42492288">prev</a><span>|</span><a href="#42492048">next</a><span>|</span><label class="collapse" for="c-42492449">[-]</label><label class="expand" for="c-42492449">[6 more]</label></div><br/><div class="children"><div class="content">Oh so you mean I have at my fingertips a tool that can generate me a Scientific American issue on any topic I fancy? That&#x27;s still some non-negative utility right there :).</div><br/><div id="42492559" class="c"><input type="checkbox" id="c-42492559" checked=""/><div class="controls bullet"><span class="by">scott_w</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492449">parent</a><span>|</span><a href="#42492048">next</a><span>|</span><label class="collapse" for="c-42492559">[-]</label><label class="expand" for="c-42492559">[5 more]</label></div><br/><div class="children"><div class="content">A Scientific American issue where the authors have no idea that they don’t know a topic so just completely make up the content, including the sources. At least magazine authors are reading the sources before misunderstanding the content (or asking the authors what the research means).<p>I don’t even trust the summaries after watching LLMs think we have meetings about my boss’s cat just because I mentioned it once as she sniffed the camera…</div><br/><div id="42492617" class="c"><input type="checkbox" id="c-42492617" checked=""/><div class="controls bullet"><span class="by">wilg</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492559">parent</a><span>|</span><a href="#42492048">next</a><span>|</span><label class="collapse" for="c-42492617">[-]</label><label class="expand" for="c-42492617">[4 more]</label></div><br/><div class="children"><div class="content">Its good to not <i>trust</i> it but that&#x27;s not the same as it having <i>no idea</i>. There is a lot of value in being close for many tasks!</div><br/><div id="42492714" class="c"><input type="checkbox" id="c-42492714" checked=""/><div class="controls bullet"><span class="by">scott_w</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492617">parent</a><span>|</span><a href="#42492861">next</a><span>|</span><label class="collapse" for="c-42492714">[-]</label><label class="expand" for="c-42492714">[2 more]</label></div><br/><div class="children"><div class="content">I think it’s a very dangerous place to be in an area you’re not familiar with. I can read Python code and figure out if it’s what I want or not. I couldn’t read an article about physics and tell you what’s accurate and what’s not.<p>Legal Eagle has a great video on how ChatGPT was used to present a legal argument, including made up case references! Stuff like this is why I’m wary to rely on it in areas outside of my expertise.</div><br/><div id="42492829" class="c"><input type="checkbox" id="c-42492829" checked=""/><div class="controls bullet"><span class="by">andreasmetsala</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492714">parent</a><span>|</span><a href="#42492861">next</a><span>|</span><label class="collapse" for="c-42492829">[-]</label><label class="expand" for="c-42492829">[1 more]</label></div><br/><div class="children"><div class="content">There’s a world of difference between blindly trusting an LLM and using it to generate clues for further research.<p>You wouldn’t write a legal argument based on what some random stranger told you, would you?</div><br/></div></div></div></div><div id="42492861" class="c"><input type="checkbox" id="c-42492861" checked=""/><div class="controls bullet"><span class="by">raducu</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492617">parent</a><span>|</span><a href="#42492714">prev</a><span>|</span><a href="#42492048">next</a><span>|</span><label class="collapse" for="c-42492861">[-]</label><label class="expand" for="c-42492861">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Its good to not trust it but that&#x27;s not the same as it having no idea. There is a lot of value in being close for many tasks!<p>The task is to replace hazelcast with infinispan in a stand-alone IMDG setup. You&#x27;re interested in Locks and EntryProcessors.<p>Ghat GPT 4, o1 tell you with their enthusiastic style Infinispan has all those features.<p>You test it locally and it does....<p>But the thing is infinispan doesn&#x27;t have explicit locks in client-server mode, just in embedded mode, but that&#x27;s something you find out from another human who has tied doing the same thing.<p>Are you better off using Chat GPT in this case?<p>I could go on and on and on, on times Chat GPT has bullshitted me and wasted days of my time, but hey, it helps with one-liners and Copilot occasionally has spectacular method auto-complete and learns on the fly some stuff and it makes my cry when it remembers random tidbits about me that not even family members do</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42492048" class="c"><input type="checkbox" id="c-42492048" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491843">parent</a><span>|</span><a href="#42492137">prev</a><span>|</span><a href="#42492543">next</a><span>|</span><label class="collapse" for="c-42492048">[-]</label><label class="expand" for="c-42492048">[26 more]</label></div><br/><div class="children"><div class="content">How do you know the answers are correct?<p>More than once I got eloquent answer that are completely wrong.</div><br/><div id="42492119" class="c"><input type="checkbox" id="c-42492119" checked=""/><div class="controls bullet"><span class="by">superultra</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492048">parent</a><span>|</span><a href="#42492060">next</a><span>|</span><label class="collapse" for="c-42492119">[-]</label><label class="expand" for="c-42492119">[2 more]</label></div><br/><div class="children"><div class="content">I give AI a “water cooler chat” level of veracity, which means it’s about as true as chatting with a coworker at a water cooler when that used to happen. Which is to say if I just need to file the information away as a “huh” it’s fine, but if I need to act on it or cite it, I need to do deeper research.</div><br/><div id="42492684" class="c"><input type="checkbox" id="c-42492684" checked=""/><div class="controls bullet"><span class="by">FergusArgyll</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492119">parent</a><span>|</span><a href="#42492060">next</a><span>|</span><label class="collapse" for="c-42492684">[-]</label><label class="expand" for="c-42492684">[1 more]</label></div><br/><div class="children"><div class="content">Yes, so often I see&#x2F;hear people asking &quot;But how can you trust it?!&quot;<p>I&#x27;m asking it a question about social dynamics in the USSR, what&#x27;s the <i>worst</i> thing that&#x27;ll happen?! I&#x27;ll get the wrong impression?<p>What are people using this for? are you building a nuclear reactor where every mistake is catastrophic?<p>Almost none of my interactions with LLMs &quot;Matter&quot;, they are things I&#x27;m curious about, if 10 out of 100 things I learnt from it are false, then I learned 90 new things. And these are things which mostly I&#x27;d have no way to learn about otherwise (without spending significant money on books&#x2F;classes etc.)</div><br/></div></div></div></div><div id="42492060" class="c"><input type="checkbox" id="c-42492060" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492048">parent</a><span>|</span><a href="#42492119">prev</a><span>|</span><a href="#42492143">next</a><span>|</span><label class="collapse" for="c-42492060">[-]</label><label class="expand" for="c-42492060">[14 more]</label></div><br/><div class="children"><div class="content">How do you address this problem with people? More than once a real live person has told me something that was wrong,</div><br/><div id="42492130" class="c"><input type="checkbox" id="c-42492130" checked=""/><div class="controls bullet"><span class="by">fzeindl</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492060">parent</a><span>|</span><a href="#42492156">next</a><span>|</span><label class="collapse" for="c-42492130">[-]</label><label class="expand" for="c-42492130">[5 more]</label></div><br/><div class="children"><div class="content">You can divide your approach to asking questions with people (and I do believe this is something people do):<p>1. You ask someone you can trust for facts and opinions on topics, but you keep in mind that the answer might only be right in 90% of the cases. Also people tend to tell you if the are not sure.<p>2. For answers you need to rely on you ask people who are legally or professionally responsible if they give you wrong advice: doctors, lawyers, car mechanics, the police etc.<p>ChatGPT can‘t lose it‘s job if it informs you incorrectly.</div><br/><div id="42492214" class="c"><input type="checkbox" id="c-42492214" checked=""/><div class="controls bullet"><span class="by">dvdbloc</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492130">parent</a><span>|</span><a href="#42492485">next</a><span>|</span><label class="collapse" for="c-42492214">[-]</label><label class="expand" for="c-42492214">[3 more]</label></div><br/><div class="children"><div class="content">If ChatGPT keeps giving you wrong answers wouldn’t this make paying customers leave? Effectively “losing its job”. But I guess you could say it acts more like the person that makes stuff up at work if they don’t know, instead of saying they don’t know.</div><br/><div id="42492581" class="c"><input type="checkbox" id="c-42492581" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492214">parent</a><span>|</span><a href="#42492306">next</a><span>|</span><label class="collapse" for="c-42492581">[-]</label><label class="expand" for="c-42492581">[1 more]</label></div><br/><div class="children"><div class="content">There was an article here just a few days ago, which discussed how firms can be ineffective, and still remain competitive.<p><a href="https:&#x2F;&#x2F;danluu.com&#x2F;nothing-works&#x2F;" rel="nofollow">https:&#x2F;&#x2F;danluu.com&#x2F;nothing-works&#x2F;</a><p>The idea that competition is effective, is often in spherical cow territory.<p>There’s tons of real world conditions which can easily let a firm be terrible at their core competency, and still survive.</div><br/></div></div><div id="42492306" class="c"><input type="checkbox" id="c-42492306" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492214">parent</a><span>|</span><a href="#42492581">prev</a><span>|</span><a href="#42492485">next</a><span>|</span><label class="collapse" for="c-42492306">[-]</label><label class="expand" for="c-42492306">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But I guess you could say it acts more like the person that makes stuff up at work if they don’t know, instead of saying they don’t know.<p>I have had language models tell me it doesn&#x27;t know. Usually when using a RAG-based system like Perplexity, but they can say they don&#x27;t know when prompted properly.</div><br/></div></div></div></div><div id="42492485" class="c"><input type="checkbox" id="c-42492485" checked=""/><div class="controls bullet"><span class="by">debesyla</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492130">parent</a><span>|</span><a href="#42492214">prev</a><span>|</span><a href="#42492156">next</a><span>|</span><label class="collapse" for="c-42492485">[-]</label><label class="expand" for="c-42492485">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure about your local laws, but at least in Lithuania it&#x27;s completely legal to give a wrong advice (by accident, of course)... Even a notary specialist would at most get to pay a larger insurance payment for a while, because human errors falls under professional insurance.</div><br/></div></div></div></div><div id="42492156" class="c"><input type="checkbox" id="c-42492156" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492060">parent</a><span>|</span><a href="#42492130">prev</a><span>|</span><a href="#42492555">next</a><span>|</span><label class="collapse" for="c-42492156">[-]</label><label class="expand" for="c-42492156">[3 more]</label></div><br/><div class="children"><div class="content">Experience.
If I recognize they give unreliable answers on a specific topic I don’t question them anymore on that topic.<p>If they lie on purpose I don’t ask them anything anymore.<p>The real experts give reliable answers, LLMs don’t.<p>The same question can yield different results.</div><br/><div id="42492561" class="c"><input type="checkbox" id="c-42492561" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492156">parent</a><span>|</span><a href="#42492555">next</a><span>|</span><label class="collapse" for="c-42492561">[-]</label><label class="expand" for="c-42492561">[2 more]</label></div><br/><div class="children"><div class="content">So LLMs are unreliable experts, okay. They&#x27;re still useful if you understand their particular flavor of unreliability (basically, they&#x27;re way too enthusiastic) - but more importantly, I bet you have exactly <i>zero</i> human experts on speed dial.<p>Most people don&#x27;t even <i>know</i> any experts personally, much less have one they could call for help on demand. Meanwhile, the unreliable, occasionally tripping pseudo-experts named GPT-4 and Claude are equally unreliably-expert in <i>every</i> domain of interest known to humanity, and don&#x27;t mind me shoving a random 100-pages long PDF in their face in the middle of the night - they&#x27;ll still happily answer within seconds, and the whole session costs me <i>fractions of a cent</i>, so I can ask for a second, and third, and tenth opinion, and then a meta-opinion, and then compare&amp;contrast with search results, and <i>they don&#x27;t mind that either</i>.<p>There&#x27;s lots to LLMs that more than compensates for their inherent unreliability.</div><br/><div id="42492939" class="c"><input type="checkbox" id="c-42492939" checked=""/><div class="controls bullet"><span class="by">discreteevent</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492561">parent</a><span>|</span><a href="#42492555">next</a><span>|</span><label class="collapse" for="c-42492939">[-]</label><label class="expand" for="c-42492939">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div></div></div></div></div><div id="42492555" class="c"><input type="checkbox" id="c-42492555" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492060">parent</a><span>|</span><a href="#42492156">prev</a><span>|</span><a href="#42492146">next</a><span>|</span><label class="collapse" for="c-42492555">[-]</label><label class="expand" for="c-42492555">[3 more]</label></div><br/><div class="children"><div class="content">It’s trivial to address this.<p>You ask an actual expert.<p>I don’t treat any water cooler conversation as accurate. It’s for fun and socializing.</div><br/><div id="42492619" class="c"><input type="checkbox" id="c-42492619" checked=""/><div class="controls bullet"><span class="by">wilg</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492555">parent</a><span>|</span><a href="#42492146">next</a><span>|</span><label class="collapse" for="c-42492619">[-]</label><label class="expand" for="c-42492619">[2 more]</label></div><br/><div class="children"><div class="content">Asking an expert is only trivial if you have access to an expert to ask!</div><br/><div id="42492935" class="c"><input type="checkbox" id="c-42492935" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492619">parent</a><span>|</span><a href="#42492146">next</a><span>|</span><label class="collapse" for="c-42492935">[-]</label><label class="expand" for="c-42492935">[1 more]</label></div><br/><div class="children"><div class="content">This is a true statement.<p>This is also not related to the problem being trivialized in the presented solution.<p>Lack of access to experts, doesn’t improve the quality of water cooler conversations.</div><br/></div></div></div></div></div></div><div id="42492146" class="c"><input type="checkbox" id="c-42492146" checked=""/><div class="controls bullet"><span class="by">huxley</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492060">parent</a><span>|</span><a href="#42492555">prev</a><span>|</span><a href="#42492076">next</a><span>|</span><label class="collapse" for="c-42492146">[-]</label><label class="expand" for="c-42492146">[1 more]</label></div><br/><div class="children"><div class="content">Well if you’re a sensible person, you stop treating them as subject matter expert</div><br/></div></div><div id="42492076" class="c"><input type="checkbox" id="c-42492076" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492060">parent</a><span>|</span><a href="#42492146">prev</a><span>|</span><a href="#42492143">next</a><span>|</span><label class="collapse" for="c-42492076">[-]</label><label class="expand" for="c-42492076">[1 more]</label></div><br/><div class="children"><div class="content">and people just don&#x27;t know what they don&#x27;t know - they just answer sillyness the same way</div><br/></div></div></div></div><div id="42492143" class="c"><input type="checkbox" id="c-42492143" checked=""/><div class="controls bullet"><span class="by">K0balt</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492048">parent</a><span>|</span><a href="#42492060">prev</a><span>|</span><a href="#42492248">next</a><span>|</span><label class="collapse" for="c-42492143">[-]</label><label class="expand" for="c-42492143">[1 more]</label></div><br/><div class="children"><div class="content">All you have to do is just remember you’re asking your uncle bob, a man of extensive usually not too inaccurate knowledge.<p>There’s no reason a source has to be authoritative, just because it’s a computer.<p>It is a bit of an adjustment, though. We are used to our machines being accurate, or failing  loudly.<p>But, looks like the future is opinionated machines.</div><br/></div></div><div id="42492248" class="c"><input type="checkbox" id="c-42492248" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492048">parent</a><span>|</span><a href="#42492143">prev</a><span>|</span><a href="#42492064">next</a><span>|</span><label class="collapse" for="c-42492248">[-]</label><label class="expand" for="c-42492248">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s something here that I feel is pretty deep, though offensive for some minds: What is the actual consequence of being wrong? Of not getting right the base reality of a situation?<p>Usually, stasis is the enemy that is much great than false information. If people with 90% truth can take a step forward in the world, even if they mistakenly think they have 100% truth, what does it matter? They&#x27;re learning more and acting more for that step taken. If the mistaken ground truth is false and importantly enough false, they&#x27;ll learn it bc their experience is grounded in the reality the navigate anyhow. If they don&#x27;t learn it, it&#x27;s of no consequence.<p>This is on my mind because I work in democratic reform, and I am acutely aware (from books like &quot;Democracy for Realists&quot;, that eviscerate common assumptions about &quot;how democracy works&quot;) that it often doesn&#x27;t matter if we understand how democracy is working, so long as we <i>feel</i> like we do, enough to take steps forward and keep trying and learning. We literally don&#x27;t even know how democracy works, and yet we&#x27;ve been living under it for centuries, to decent enough ends.<p>I think often about the research of Donald Hoffman. His lab runs evolutionary simulations, putting &quot;creatures&quot; that see &quot;reality&quot; (of the simulation) against creatures that see only &quot;fitness&quot; (the abstraction, but also the lie, that is more about seeing what gets the creature living to the next click of the engine, whether that&#x27;s truth or falsehood about the reality). <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=oYp5XuGYqqY" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=oYp5XuGYqqY</a><p>Basically, creatures that see only fitness (that see only the lie), they drive to extinction every creature that insists on seeing &quot;reality as it is&quot;.<p>I take this to mean truth is in no way, shape, or form favoured in the universe. This is just a convinient lie we tell ourselves, to motivate our current cultural work and preferences.<p>So tl;dr -- better to move forward and feel high agency with imperfect information, than to wait for a full truthful solution that might never come, or might be such high cost as to arrive too late. Those moving forward rapidly with imperfect information will perhaps drive to extinction those methods that insist on full grounding in reality.<p>Maybe this is always the way the world has worked... I mean, does any mammal before us have any idea how any of reality worked? No, they just used their senses to detect the gist of reality (often heuristics and lies), and operated in the world as such. Maybe the human sphere of language and thought will settle on similar ruthlessness.</div><br/><div id="42492461" class="c"><input type="checkbox" id="c-42492461" checked=""/><div class="controls bullet"><span class="by">jval43</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492248">parent</a><span>|</span><a href="#42492709">next</a><span>|</span><label class="collapse" for="c-42492461">[-]</label><label class="expand" for="c-42492461">[1 more]</label></div><br/><div class="children"><div class="content">Incorrect information by itself is at best useless. Incorrect information that is thought to be correct is outright dangerous. Objective truth is crucial to science and progress.<p>We&#x27;ve come too far since the age of enlightenment to just give it all up.</div><br/></div></div><div id="42492709" class="c"><input type="checkbox" id="c-42492709" checked=""/><div class="controls bullet"><span class="by">DanHulton</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492248">parent</a><span>|</span><a href="#42492461">prev</a><span>|</span><a href="#42492900">next</a><span>|</span><label class="collapse" for="c-42492709">[-]</label><label class="expand" for="c-42492709">[1 more]</label></div><br/><div class="children"><div class="content">&gt; “My father once told me that respect for truth comes close to being the basis for all morality. &#x27;Something cannot emerge from nothing,&#x27; he said. This is profound thinking if you understand how unstable &#x27;the truth&#x27; can be.”<p>Frank Herbert, Dune</div><br/></div></div><div id="42492900" class="c"><input type="checkbox" id="c-42492900" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492248">parent</a><span>|</span><a href="#42492709">prev</a><span>|</span><a href="#42492064">next</a><span>|</span><label class="collapse" for="c-42492900">[-]</label><label class="expand" for="c-42492900">[1 more]</label></div><br/><div class="children"><div class="content">Yes! There’s no ‘element’ of truth. 
Funnily enough, this isn’t a philosophical question for me either.<p>The industrialization of content generation, misinformation, and inauthentic behavior are very problematic.<p>I’ve hit on an analogy that’s proving very resilient at framing the crossroads we seem to be at - namely the move to fiat money from the gold standard.<p>The gold standard is easy to understand, and fiat money honestly seems like madness.<p>This is really similar to what we seem to be doing with genAI, as it vastly outstrips humanity’s capacity to verify.<p>There’s a few studies out there that show that people have different modes of content consumption.  A large chunk of content consumption is for casual purposes, and without any desire to get mired into questions of accuracy. About 10% of the time (some small %, I don’t remember the exact) people care about the content being accurate.</div><br/></div></div></div></div><div id="42492064" class="c"><input type="checkbox" id="c-42492064" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492048">parent</a><span>|</span><a href="#42492248">prev</a><span>|</span><a href="#42492543">next</a><span>|</span><label class="collapse" for="c-42492064">[-]</label><label class="expand" for="c-42492064">[4 more]</label></div><br/><div class="children"><div class="content">so do teachers and books, in the future we need have multiple variants to cross check</div><br/><div id="42492164" class="c"><input type="checkbox" id="c-42492164" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492064">parent</a><span>|</span><a href="#42492543">next</a><span>|</span><label class="collapse" for="c-42492164">[-]</label><label class="expand" for="c-42492164">[3 more]</label></div><br/><div class="children"><div class="content">Cross check against what?
AI generated texts will flood the internet and burry the real knowledge just like SEO did before. But this time the fake knowledge will be less obvious and harder to check.</div><br/><div id="42492670" class="c"><input type="checkbox" id="c-42492670" checked=""/><div class="controls bullet"><span class="by">bradchris</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492164">parent</a><span>|</span><a href="#42492174">next</a><span>|</span><label class="collapse" for="c-42492670">[-]</label><label class="expand" for="c-42492670">[1 more]</label></div><br/><div class="children"><div class="content">If that turns out to be true, the  it looks like AI just gave  universities a new reason for being.<p>What a shift from twenty years ago when optimism over “information superhighways” on the “world wide web” would end knowledge gatekeeping and educate the masses, to now— worries of AI slop and finely tuned ML algorithms frying older and younger generations’ brains, while information of human value gets buried, siloed, and paywalled, with no way to verify anything at all.</div><br/></div></div><div id="42492174" class="c"><input type="checkbox" id="c-42492174" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492164">parent</a><span>|</span><a href="#42492670">prev</a><span>|</span><a href="#42492543">next</a><span>|</span><label class="collapse" for="c-42492174">[-]</label><label class="expand" for="c-42492174">[1 more]</label></div><br/><div class="children"><div class="content">models from different vendors,plus google search. for serious stuff, we&#x27;ll still have to check manually ourselves</div><br/></div></div></div></div></div></div></div></div><div id="42492543" class="c"><input type="checkbox" id="c-42492543" checked=""/><div class="controls bullet"><span class="by">zwnow</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491843">parent</a><span>|</span><a href="#42492048">prev</a><span>|</span><a href="#42491925">next</a><span>|</span><label class="collapse" for="c-42492543">[-]</label><label class="expand" for="c-42492543">[1 more]</label></div><br/><div class="children"><div class="content">Talk to an expert? You are aware of them hallucinating right?</div><br/></div></div><div id="42491925" class="c"><input type="checkbox" id="c-42491925" checked=""/><div class="controls bullet"><span class="by">1209412comb</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491843">parent</a><span>|</span><a href="#42492543">prev</a><span>|</span><a href="#42491990">next</a><span>|</span><label class="collapse" for="c-42491925">[-]</label><label class="expand" for="c-42491925">[3 more]</label></div><br/><div class="children"><div class="content">Semi-related but I find that sometime it just completely ruined a type of conversation.<p>Like as in your example, I would previously  asked people &quot;how would 911 handle an US Reservation Area&quot;, and watch how my friends think and reason. To me getting a conclusive answer was not a point. Now they just copy &amp; paste Chat GPT, no fun haha.</div><br/><div id="42491984" class="c"><input type="checkbox" id="c-42491984" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491925">parent</a><span>|</span><a href="#42491988">next</a><span>|</span><label class="collapse" for="c-42491984">[-]</label><label class="expand" for="c-42491984">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just the 2020s version of how Google and smartphones ruined the ages-old social pastime of arguing about trivia in a pub :P</div><br/></div></div><div id="42491988" class="c"><input type="checkbox" id="c-42491988" checked=""/><div class="controls bullet"><span class="by">atonse</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491925">parent</a><span>|</span><a href="#42491984">prev</a><span>|</span><a href="#42491990">next</a><span>|</span><label class="collapse" for="c-42491988">[-]</label><label class="expand" for="c-42491988">[1 more]</label></div><br/><div class="children"><div class="content">Yeah it can definitely be a crutch too in some situations. I notice it with my kids where they’ll want to tell me about something but then seek a video or something to show it.<p>Sometimes I have to say “no! just use your words to describe it! I want to hear your description”</div><br/></div></div></div></div><div id="42491990" class="c"><input type="checkbox" id="c-42491990" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491843">parent</a><span>|</span><a href="#42491925">prev</a><span>|</span><a href="#42491905">next</a><span>|</span><label class="collapse" for="c-42491990">[-]</label><label class="expand" for="c-42491990">[3 more]</label></div><br/><div class="children"><div class="content">For me the problem is that you always need to double-check this particular type of expert, as it can be confidently wrong about pretty much any topic.<p>It&#x27;s useful as a starting point, not as a definitive expert answer.</div><br/><div id="42492066" class="c"><input type="checkbox" id="c-42492066" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491990">parent</a><span>|</span><a href="#42491905">next</a><span>|</span><label class="collapse" for="c-42492066">[-]</label><label class="expand" for="c-42492066">[2 more]</label></div><br/><div class="children"><div class="content">What human experts do you blindly trust without double checking?</div><br/><div id="42492140" class="c"><input type="checkbox" id="c-42492140" checked=""/><div class="controls bullet"><span class="by">CobrastanJorji</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492066">parent</a><span>|</span><a href="#42491905">next</a><span>|</span><label class="collapse" for="c-42492140">[-]</label><label class="expand" for="c-42492140">[1 more]</label></div><br/><div class="children"><div class="content">Most human experts, when asked about their area of expertise, don&#x27;t parrot what some guy said as joke on Reddit five years ago.<p>Most lawyers, when you ask them to write a brief, will cite only real cases.</div><br/></div></div></div></div></div></div><div id="42491905" class="c"><input type="checkbox" id="c-42491905" checked=""/><div class="controls bullet"><span class="by">stravant</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491843">parent</a><span>|</span><a href="#42491990">prev</a><span>|</span><a href="#42492931">next</a><span>|</span><label class="collapse" for="c-42491905">[-]</label><label class="expand" for="c-42491905">[1 more]</label></div><br/><div class="children"><div class="content">One of my favorite successes was getting an LLM to write me a program to graph how I subjectively feel the heat of steam coming off of the noodles I&#x27;m pouring the water out from as a function of the ambient temperature.<p>I was wondering which effects were at play and the graph matched my subjective experience well.</div><br/></div></div></div></div><div id="42492931" class="c"><input type="checkbox" id="c-42492931" checked=""/><div class="controls bullet"><span class="by">hamilyon2</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42491843">prev</a><span>|</span><a href="#42492803">next</a><span>|</span><label class="collapse" for="c-42492931">[-]</label><label class="expand" for="c-42492931">[1 more]</label></div><br/><div class="children"><div class="content">If the progress in capabilities stall, the product fit, adoption, ease of use are the next battlefield.<p>OpenAI may be first to realize and switch, so they still have a chance to recoup some of those billions</div><br/></div></div><div id="42492803" class="c"><input type="checkbox" id="c-42492803" checked=""/><div class="controls bullet"><span class="by">tugu77</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492931">prev</a><span>|</span><a href="#42491877">next</a><span>|</span><label class="collapse" for="c-42492803">[-]</label><label class="expand" for="c-42492803">[1 more]</label></div><br/><div class="children"><div class="content">Thats all fine, but I think you are missing the bigger picture. It&#x27;s not about whether what we already got out of this is good. Of course it is. But this is about where it&#x27;s going.<p>Until about 120 years ago, people were happy with horses and horse carriages. Such a great help! Travel long distances, pull weights, I never want to go back! But then the automobile was invented and within a few years little travel was done by horses anymore.<p>More recently, everybody had a landline phone at home. Such great tech! Talk to grandma hundreds of miles away! I never want to go back! Then suddenly the mobile phone and just shortly after the smart phone came along and now nobody has a landline anymore but everybody can record tiktoks anywhere anytime and share them with the world within seconds.<p>Now imagine &quot;AI&quot;. Sure, we have some new tools right now. Sure we don&#x27;t want to go back. But imagine the transformative effects that could come if the train didn&#x27;t stop here. Question is just: will it?</div><br/></div></div><div id="42491877" class="c"><input type="checkbox" id="c-42491877" checked=""/><div class="controls bullet"><span class="by">bloppe</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492803">prev</a><span>|</span><a href="#42491690">next</a><span>|</span><label class="collapse" for="c-42491877">[-]</label><label class="expand" for="c-42491877">[7 more]</label></div><br/><div class="children"><div class="content">At this point I think even the most bearish have to concede that LLM&#x27;s are an amazing tool. But OpenAI was never supposed to be about creating tools. They&#x27;re supposed to create something that can completely take over entire projects for you, not just something that can help you work on the projects faster. If they can&#x27;t pull that off in the next year or two, they&#x27;re gonna seriously struggle to raise the next 10B they&#x27;ll need to keep the lights on.<p>Of course LLMs aren&#x27;t going anywhere, but I do not envy Sam Altman right now.</div><br/><div id="42492912" class="c"><input type="checkbox" id="c-42492912" checked=""/><div class="controls bullet"><span class="by">lukan</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491877">parent</a><span>|</span><a href="#42491902">next</a><span>|</span><label class="collapse" for="c-42492912">[-]</label><label class="expand" for="c-42492912">[1 more]</label></div><br/><div class="children"><div class="content">Tesla is still valued high, despite FSD did not came, despite being  promised. So OpenAI would get away with delivering ChatGPT5, if it is better than the competition.</div><br/></div></div><div id="42491902" class="c"><input type="checkbox" id="c-42491902" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491877">parent</a><span>|</span><a href="#42492912">prev</a><span>|</span><a href="#42492132">next</a><span>|</span><label class="collapse" for="c-42491902">[-]</label><label class="expand" for="c-42491902">[4 more]</label></div><br/><div class="children"><div class="content">At this point it’s quite likely that they could pivot and just be the chatgpt company. I’ve found chatgpt-4o with web search and plugins to be more useful than o1 for most tasks.<p>It’s possible we’re nearing the end of the LLM race, but I doubt that’s the end of the AI story this decade, or OpenAI.</div><br/><div id="42492079" class="c"><input type="checkbox" id="c-42492079" checked=""/><div class="controls bullet"><span class="by">bloppe</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491902">parent</a><span>|</span><a href="#42492132">next</a><span>|</span><label class="collapse" for="c-42492079">[-]</label><label class="expand" for="c-42492079">[3 more]</label></div><br/><div class="children"><div class="content">Ya I think they probably will, but &quot;the chatgpt company&quot; is not worth 157B. It might not even be worth 1B.</div><br/><div id="42492692" class="c"><input type="checkbox" id="c-42492692" checked=""/><div class="controls bullet"><span class="by">josu</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492079">parent</a><span>|</span><a href="#42492132">next</a><span>|</span><label class="collapse" for="c-42492692">[-]</label><label class="expand" for="c-42492692">[2 more]</label></div><br/><div class="children"><div class="content">It has replaced ~50% of my Google searches.</div><br/><div id="42492774" class="c"><input type="checkbox" id="c-42492774" checked=""/><div class="controls bullet"><span class="by">silisili</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492692">parent</a><span>|</span><a href="#42492132">next</a><span>|</span><label class="collapse" for="c-42492774">[-]</label><label class="expand" for="c-42492774">[1 more]</label></div><br/><div class="children"><div class="content">Yes but it also hasn&#x27;t been attacked by ads yet.  Google doesn&#x27;t suck for lack of search results, it sucks because of ads.<p>Imagine asking chatgpt to tell you about slopes in Colorado, and the first five answers are about how awesome North Face is and how you can order from them.  You probably wouldn&#x27;t use it as much.</div><br/></div></div></div></div></div></div></div></div><div id="42492132" class="c"><input type="checkbox" id="c-42492132" checked=""/><div class="controls bullet"><span class="by">superultra</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491877">parent</a><span>|</span><a href="#42491902">prev</a><span>|</span><a href="#42491690">next</a><span>|</span><label class="collapse" for="c-42492132">[-]</label><label class="expand" for="c-42492132">[1 more]</label></div><br/><div class="children"><div class="content">I keep thinking about that Idris Elba Microsoft ad about AI about how much AI can help my business, and how both true and untrue that as is, and how much distance there is between the now and the possible promised of AI, and I imagine this is what keeps Altman up at night.</div><br/></div></div></div></div><div id="42491690" class="c"><input type="checkbox" id="c-42491690" checked=""/><div class="controls bullet"><span class="by">IAmGraydon</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42491877">prev</a><span>|</span><a href="#42492623">next</a><span>|</span><label class="collapse" for="c-42491690">[-]</label><label class="expand" for="c-42491690">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve bee thinking the same thing lately. Even if we don&#x27;t get to AGI, LLMs have revolutionized the way I work. I can produce code and copy at superhuman speeds now. I love it. Honestly, if we never get to AGI and just have the LLMs, it&#x27;s probably the best possible outcome as I don&#x27;t think true AGI is going to be a good thing for humanity.</div><br/></div></div><div id="42492623" class="c"><input type="checkbox" id="c-42492623" checked=""/><div class="controls bullet"><span class="by">rkagerer</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42491690">prev</a><span>|</span><a href="#42492091">next</a><span>|</span><label class="collapse" for="c-42492623">[-]</label><label class="expand" for="c-42492623">[1 more]</label></div><br/><div class="children"><div class="content"><i>completely local AI research tool, based on Ollama</i><p>Could you elaborate?  Was it easy to install?</div><br/></div></div><div id="42492091" class="c"><input type="checkbox" id="c-42492091" checked=""/><div class="controls bullet"><span class="by">spaceman_2020</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492623">prev</a><span>|</span><a href="#42491992">next</a><span>|</span><label class="collapse" for="c-42492091">[-]</label><label class="expand" for="c-42492091">[1 more]</label></div><br/><div class="children"><div class="content">This is me. If things never improve and Sonnet 3.6 is the best we have…I’m fine. Its good enough to drastically improve productivity</div><br/></div></div><div id="42491992" class="c"><input type="checkbox" id="c-42491992" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492091">prev</a><span>|</span><a href="#42492531">next</a><span>|</span><label class="collapse" for="c-42491992">[-]</label><label class="expand" for="c-42491992">[1 more]</label></div><br/><div class="children"><div class="content">Amen. Everyone is talking about  plateaus and diminishing returns on training but I don’t care one bit. I get that this is a startup focused forum and the financial sustainability of the market players is important but I can’t wait to see what the next decade of UX improvements will be like even if model improvements slow to a crawl.</div><br/></div></div><div id="42492531" class="c"><input type="checkbox" id="c-42492531" checked=""/><div class="controls bullet"><span class="by">alickz</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42491992">prev</a><span>|</span><a href="#42492369">next</a><span>|</span><label class="collapse" for="c-42492531">[-]</label><label class="expand" for="c-42492531">[1 more]</label></div><br/><div class="children"><div class="content">i feel like AGI is an arbitrary line in the sand anyway<p>i think as humans we put too much emphasis on what intelligence means relative to ourselves, instead of relative to nature</div><br/></div></div><div id="42492369" class="c"><input type="checkbox" id="c-42492369" checked=""/><div class="controls bullet"><span class="by">BOOSTERHIDROGEN</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492531">prev</a><span>|</span><a href="#42491714">next</a><span>|</span><label class="collapse" for="c-42492369">[-]</label><label class="expand" for="c-42492369">[1 more]</label></div><br/><div class="children"><div class="content">Can you walk me through the steps you&#x27;ve taken to set up the Ollama-based tool so far?</div><br/></div></div><div id="42491714" class="c"><input type="checkbox" id="c-42491714" checked=""/><div class="controls bullet"><span class="by">lazygoose</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492369">prev</a><span>|</span><a href="#42492404">next</a><span>|</span><label class="collapse" for="c-42491714">[-]</label><label class="expand" for="c-42491714">[2 more]</label></div><br/><div class="children"><div class="content">Curious about the AI research tool you mentioned, would you mind sharing it? Been trying to get a good local research setup with Ollama but still figuring out what works best.</div><br/><div id="42491805" class="c"><input type="checkbox" id="c-42491805" checked=""/><div class="controls bullet"><span class="by">Bilal_io</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491714">parent</a><span>|</span><a href="#42492404">next</a><span>|</span><label class="collapse" for="c-42491805">[-]</label><label class="expand" for="c-42491805">[1 more]</label></div><br/><div class="children"><div class="content">Not OP, but based on their mention of Ollama, I can tell you that it has built in search tools, all you need to do is supply an API to one of the tools, or even run one of the search tools locally using docker.</div><br/></div></div></div></div><div id="42492404" class="c"><input type="checkbox" id="c-42492404" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42491714">prev</a><span>|</span><a href="#42491628">next</a><span>|</span><label class="collapse" for="c-42492404">[-]</label><label class="expand" for="c-42492404">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Just today I used a completely local AI research tool, based on Ollama. It worked great<p>What’s it called? Could you post a link please?<p>Thank you</div><br/></div></div><div id="42491628" class="c"><input type="checkbox" id="c-42491628" checked=""/><div class="controls bullet"><span class="by">ferminaut</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492404">prev</a><span>|</span><a href="#42492403">next</a><span>|</span><label class="collapse" for="c-42491628">[-]</label><label class="expand" for="c-42491628">[10 more]</label></div><br/><div class="children"><div class="content">vscode + cline extension + gemini2.0 is pretty awesome. Highly recommend checking out cline. it quickly became one of my favorite coding tools.</div><br/><div id="42491664" class="c"><input type="checkbox" id="c-42491664" checked=""/><div class="controls bullet"><span class="by">IAmGraydon</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491628">parent</a><span>|</span><a href="#42491662">next</a><span>|</span><label class="collapse" for="c-42491664">[-]</label><label class="expand" for="c-42491664">[4 more]</label></div><br/><div class="children"><div class="content">Gemini 2.0 isn&#x27;t particularly great at coding. The Gemini 1206 preview that was released just before 2.0 is quite good, though. Still, it hasn&#x27;t taken the crown from Claude 3.5 Sonnet (which appears to now be tied with o1). Very much agree about Cline + VSCode, BTW. My preferred models with Cline are 3.5 Sonnet and 3.5 Haiku. I can throw the more complex problems at Sonnet and use Haiku for everything else.<p><a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;leaderboards&#x2F;edit.html" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;leaderboards&#x2F;edit.html</a></div><br/><div id="42491710" class="c"><input type="checkbox" id="c-42491710" checked=""/><div class="controls bullet"><span class="by">ferminaut</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491664">parent</a><span>|</span><a href="#42491662">next</a><span>|</span><label class="collapse" for="c-42491710">[-]</label><label class="expand" for="c-42491710">[3 more]</label></div><br/><div class="children"><div class="content">the context limits on google are nuts! Being able to pump 2 million tokens in and having it cost $0 is pretty crazy rn. Cline makes it seamless to switch between APIs and isnt trying to shoehorn their SAAS AI into a custom vscode (looking at you cursor)</div><br/><div id="42492335" class="c"><input type="checkbox" id="c-42492335" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491710">parent</a><span>|</span><a href="#42491662">next</a><span>|</span><label class="collapse" for="c-42492335">[-]</label><label class="expand" for="c-42492335">[2 more]</label></div><br/><div class="children"><div class="content">&gt;the context limits on google are nuts! Being able to pump 2 million tokens in and having it cost $0 is pretty crazy rn.<p>What&#x27;s the catch though? I was looking at Gemini recently and it seemed too good to be true.</div><br/><div id="42492457" class="c"><input type="checkbox" id="c-42492457" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492335">parent</a><span>|</span><a href="#42491662">next</a><span>|</span><label class="collapse" for="c-42492457">[-]</label><label class="expand" for="c-42492457">[1 more]</label></div><br/><div class="children"><div class="content">Google inference is a lot cheaper since they have their own hardware so they don&#x27;t have to pay licensing to NVIDIA, thus their free tier can give you much more than others.<p>Other than that the catch is like all other free tiers, it is marketing and can be withdrawn at any moment to get you to pay after you are used to their product.</div><br/></div></div></div></div></div></div></div></div><div id="42491662" class="c"><input type="checkbox" id="c-42491662" checked=""/><div class="controls bullet"><span class="by">SamPatt</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491628">parent</a><span>|</span><a href="#42491664">prev</a><span>|</span><a href="#42492403">next</a><span>|</span><label class="collapse" for="c-42491662">[-]</label><label class="expand" for="c-42491662">[5 more]</label></div><br/><div class="children"><div class="content">I will check it out. The number of new tools is staggering.<p>I enjoy image and video generation and I have a 4090 and ComfyUI; I can&#x27;t keep up with everything coming out anymore.</div><br/><div id="42491723" class="c"><input type="checkbox" id="c-42491723" checked=""/><div class="controls bullet"><span class="by">IAmGraydon</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491662">parent</a><span>|</span><a href="#42491722">next</a><span>|</span><label class="collapse" for="c-42491723">[-]</label><label class="expand" for="c-42491723">[3 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re interested in the latest tools for coding, join this subreddit and you&#x27;ll always be on top of it:<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPTCoding&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPTCoding&#x2F;</a><p>There are a lot of tools, but only a small pool of tools that are worth checking out. Cline, Continue, Windsurf, CoPilot, Cursor, and Aider are the ones that come to mind.</div><br/><div id="42491772" class="c"><input type="checkbox" id="c-42491772" checked=""/><div class="controls bullet"><span class="by">deadmutex</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491723">parent</a><span>|</span><a href="#42491722">next</a><span>|</span><label class="collapse" for="c-42491772">[-]</label><label class="expand" for="c-42491772">[2 more]</label></div><br/><div class="children"><div class="content">&quot;ChatGPT&quot; Coding... is it impartial? the name sorta sounds biased.</div><br/><div id="42492030" class="c"><input type="checkbox" id="c-42492030" checked=""/><div class="controls bullet"><span class="by">IAmGraydon</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491772">parent</a><span>|</span><a href="#42491722">next</a><span>|</span><label class="collapse" for="c-42492030">[-]</label><label class="expand" for="c-42492030">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT was the first to come along, so the subreddit was given a perhaps short-sighted name. It&#x27;s now about coding with LLMs in general.</div><br/></div></div></div></div></div></div><div id="42491722" class="c"><input type="checkbox" id="c-42491722" checked=""/><div class="controls bullet"><span class="by">ferminaut</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491662">parent</a><span>|</span><a href="#42491723">prev</a><span>|</span><a href="#42492403">next</a><span>|</span><label class="collapse" for="c-42491722">[-]</label><label class="expand" for="c-42491722">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re a offline kind of guy, try LM Studio + Cline :)<p>&#x2F;not affiliated with cline, just a happy user</div><br/></div></div></div></div></div></div><div id="42492403" class="c"><input type="checkbox" id="c-42492403" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42491628">prev</a><span>|</span><a href="#42492033">next</a><span>|</span><label class="collapse" for="c-42492403">[-]</label><label class="expand" for="c-42492403">[1 more]</label></div><br/><div class="children"><div class="content">Cline was fixing my type errors and unit tests while I was doing my V60 pourover.</div><br/></div></div><div id="42492033" class="c"><input type="checkbox" id="c-42492033" checked=""/><div class="controls bullet"><span class="by">uludag</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492403">prev</a><span>|</span><a href="#42491724">next</a><span>|</span><label class="collapse" for="c-42492033">[-]</label><label class="expand" for="c-42492033">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure the plan has never been to just make these tools that make us more efficient. If AI stays at the level it&#x27;s at, it would be a profound failure for companies like OpenAI. We&#x27;re all benefiting from the capital being poured into these technologies now.  The enshittification will come. The enshittification always comes.</div><br/><div id="42492208" class="c"><input type="checkbox" id="c-42492208" checked=""/><div class="controls bullet"><span class="by">mitemte</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492033">parent</a><span>|</span><a href="#42491724">next</a><span>|</span><label class="collapse" for="c-42492208">[-]</label><label class="expand" for="c-42492208">[1 more]</label></div><br/><div class="children"><div class="content">I’m paying $240 a year to Anthropic that I wasn’t paying before and it’s worth it. While I don’t use Claude every single day, but I use it several times a day when I’m working. More times than the free tier allows.</div><br/></div></div></div></div><div id="42491724" class="c"><input type="checkbox" id="c-42491724" checked=""/><div class="controls bullet"><span class="by">divan</span><span>|</span><a href="#42491528">parent</a><span>|</span><a href="#42492033">prev</a><span>|</span><a href="#42492139">next</a><span>|</span><label class="collapse" for="c-42491724">[-]</label><label class="expand" for="c-42491724">[6 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s revisit this comment in one year – after the explosion of agentic systems. (:</div><br/><div id="42491891" class="c"><input type="checkbox" id="c-42491891" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491724">parent</a><span>|</span><a href="#42492799">next</a><span>|</span><label class="collapse" for="c-42491891">[-]</label><label class="expand" for="c-42491891">[4 more]</label></div><br/><div class="children"><div class="content">We already have agentic systems; they&#x27;re not particularly impressive (1).<p>There&#x27;s no specific reason to expect them to get better.<p>Things that will shift the status quo are: MCST-LLMs (like with ARC-AGI) and Much Bigger LLMs (like GPT-5, if they ever turn up) or some completely novel architecture.<p>[1] - It&#x27;s provable; if just chaining LLMs are a particular size into agentic systems could scale indefinitely, then you could use a 1-param LLM and get AGI. You can&#x27;t. QED. Chaining LLMs with agentic systems has a capped maximum level of function which we basically already see with the current LLMs.<p>ie. Adding &#x27;agentic&#x27; to your system has a finite, probably already reached, upper bound of value.</div><br/><div id="42492098" class="c"><input type="checkbox" id="c-42492098" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491891">parent</a><span>|</span><a href="#42492799">next</a><span>|</span><label class="collapse" for="c-42492098">[-]</label><label class="expand" for="c-42492098">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  It&#x27;s provable; if just chaining LLMs are a particular size into agentic systems could scale indefinitely, then you could use a 1-param LLM and get AGI. You can&#x27;t. QED.<p>Perhaps I missunderstand your reply, but that has not been my experience at all.<p>There are 3 types of &quot;agentic&quot; behaviour that has worked for a while for me, and I don&#x27;t know how else it would work without &quot;agents&quot;:<p>1. Task decomposition - this was my manual flow since pre-chatgpt models: a) provide an overview of topic x with chapter names; b) expand on chapter 1 ... n ; c) make a summary of each chapter; d) make an introduction based on the summaries. I now have an &quot;agent&quot; that does that w&#x2F; minimal scripting and no &quot;libraries&quot;. Just pure python control loop.<p>This gets me pretty reasonable documents for my daily needs.<p>2. tool use (search, db queries, API hits). I don&#x27;t know how you&#x27;d use an LLM without this functionality. And chaining them into flows absolutely works.<p>3. coding. I use the following &quot;flow&quot; -&gt; input a paragraph or 2 about what I want, send that + some embedding-based context from the codebase to an LLM (3.5 or 4o, recently o1 or gemini) -&gt; get code -&gt; run code -&gt; &#x2F;terminal if error -&gt; paste results -&gt; re-iterate if needed. This flow really works today, especially with 3.5. In my testing it needs somewhere under 3 &quot;iterations&quot; to &quot;get&quot; what&#x27;s needed in more than 80% of the cases. I intervene in the rest of 20%.</div><br/><div id="42492222" class="c"><input type="checkbox" id="c-42492222" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492098">parent</a><span>|</span><a href="#42492799">next</a><span>|</span><label class="collapse" for="c-42492222">[-]</label><label class="expand" for="c-42492222">[2 more]</label></div><br/><div class="children"><div class="content">A zed user? Live that editor and the dev flow with it.</div><br/><div id="42492524" class="c"><input type="checkbox" id="c-42492524" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42492222">parent</a><span>|</span><a href="#42492799">next</a><span>|</span><label class="collapse" for="c-42492524">[-]</label><label class="expand" for="c-42492524">[1 more]</label></div><br/><div class="children"><div class="content">Haha, yes! I&#x27;m trying it out and been loving it so far. I found that I go there for most of my eda scripts these days. I do a lot of datasets collection and exploration, and it&#x27;s amazing that I can now type one paragraph and get pretty much what it would have taken me ~30 min to code myself. Claude 3.5 is great for most exploration tasks, and the flow of &quot;this doesn&#x27;t work &#x2F;terminal&quot; + claude using prints to debug is really starting to come together.<p>I use zed for this, cursor for my more involved sessions and aider + vscode + continue for local stuff when I want to see how far along local models have come. Haven&#x27;t tried cline yet, but heard great stuff.</div><br/></div></div></div></div></div></div></div></div><div id="42492799" class="c"><input type="checkbox" id="c-42492799" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#42491528">root</a><span>|</span><a href="#42491724">parent</a><span>|</span><a href="#42491891">prev</a><span>|</span><a href="#42492139">next</a><span>|</span><label class="collapse" for="c-42492799">[-]</label><label class="expand" for="c-42492799">[1 more]</label></div><br/><div class="children"><div class="content">You mean, the explosion of human centipede LLM prompts shitting into eachother?<p>Yes that will be a sight to behold.</div><br/></div></div></div></div></div></div><div id="42490004" class="c"><input type="checkbox" id="c-42490004" checked=""/><div class="controls bullet"><span class="by">t_serpico</span><span>|</span><a href="#42491528">prev</a><span>|</span><a href="#42492641">next</a><span>|</span><label class="collapse" for="c-42490004">[-]</label><label class="expand" for="c-42490004">[166 more]</label></div><br/><div class="children"><div class="content">One fundamental challenge to me is that if each training run because more and more expensive, the time it takes it to learn what works&#x2F;doesn&#x27;t work widens. Half a billion dollars for training a model is already nuts, but if it takes 100 iterations to perfect it, you&#x27;ve cumulatively spent 50 billion dollars...  Smaller models may actually be where rapid innovation continues simply because of tighter feedback loops. O3 may be an example of this.</div><br/><div id="42492624" class="c"><input type="checkbox" id="c-42492624" checked=""/><div class="controls bullet"><span class="by">ciconia</span><span>|</span><a href="#42490004">parent</a><span>|</span><a href="#42490142">next</a><span>|</span><label class="collapse" for="c-42492624">[-]</label><label class="expand" for="c-42492624">[3 more]</label></div><br/><div class="children"><div class="content">When you think about it it&#x27;s astounding how much energy this technology consumes versus a human brain which runs at ~20W [1].<p>[1] <a href="https:&#x2F;&#x2F;hypertextbook.com&#x2F;facts&#x2F;2001&#x2F;JacquelineLing.shtml" rel="nofollow">https:&#x2F;&#x2F;hypertextbook.com&#x2F;facts&#x2F;2001&#x2F;JacquelineLing.shtml</a></div><br/><div id="42492869" class="c"><input type="checkbox" id="c-42492869" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42492624">parent</a><span>|</span><a href="#42492810">next</a><span>|</span><label class="collapse" for="c-42492869">[-]</label><label class="expand" for="c-42492869">[1 more]</label></div><br/><div class="children"><div class="content">It’s almost as if human intelligence doesn’t involve performing repeated matrix multiplications over a mathematically transformed copy of the internet. ;-)</div><br/></div></div><div id="42492810" class="c"><input type="checkbox" id="c-42492810" checked=""/><div class="controls bullet"><span class="by">concerndc1tizen</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42492624">parent</a><span>|</span><a href="#42492869">prev</a><span>|</span><a href="#42490142">next</a><span>|</span><label class="collapse" for="c-42492810">[-]</label><label class="expand" for="c-42492810">[1 more]</label></div><br/><div class="children"><div class="content">20w for 20 years to answer questions slowly and error-prone at the level of a 30B model.
An additional 10 years with highly trained supervision and the brain might start contributing original work.</div><br/></div></div></div></div><div id="42490142" class="c"><input type="checkbox" id="c-42490142" checked=""/><div class="controls bullet"><span class="by">dkobia</span><span>|</span><a href="#42490004">parent</a><span>|</span><a href="#42492624">prev</a><span>|</span><a href="#42492221">next</a><span>|</span><label class="collapse" for="c-42490142">[-]</label><label class="expand" for="c-42490142">[112 more]</label></div><br/><div class="children"><div class="content">AGI is the Sisyphean task of our age. We’ll push this boulder up the mountain because we have to, even if it kills us.</div><br/><div id="42490398" class="c"><input type="checkbox" id="c-42490398" checked=""/><div class="controls bullet"><span class="by">missedthecue</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490142">parent</a><span>|</span><a href="#42490503">next</a><span>|</span><label class="collapse" for="c-42490398">[-]</label><label class="expand" for="c-42490398">[82 more]</label></div><br/><div class="children"><div class="content">Do we know LLMs are the path to AGI? If they&#x27;re not, we&#x27;ll just end up with some neat but eye wateringly expensive LLMs.</div><br/><div id="42490506" class="c"><input type="checkbox" id="c-42490506" checked=""/><div class="controls bullet"><span class="by">foolfoolz</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490398">parent</a><span>|</span><a href="#42490741">next</a><span>|</span><label class="collapse" for="c-42490506">[-]</label><label class="expand" for="c-42490506">[52 more]</label></div><br/><div class="children"><div class="content">AGI will arrive like self driving cars. it’s not that you will wake up one day and we have it. cars gained auto-braking, parallel parking, cruise control assist. and over a long time you get to something like waymo, which still is location dependent. i think AGI will take decades but sooner will be some special cases that are effectively the same</div><br/><div id="42490973" class="c"><input type="checkbox" id="c-42490973" checked=""/><div class="controls bullet"><span class="by">missedthecue</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42490734">next</a><span>|</span><label class="collapse" for="c-42490973">[-]</label><label class="expand" for="c-42490973">[2 more]</label></div><br/><div class="children"><div class="content">But maybe thses LLMs are like building bigger and bigger engines. It&#x27;s not getting you closer to the self driving car.</div><br/><div id="42491948" class="c"><input type="checkbox" id="c-42491948" checked=""/><div class="controls bullet"><span class="by">mulmen</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490973">parent</a><span>|</span><a href="#42490734">next</a><span>|</span><label class="collapse" for="c-42491948">[-]</label><label class="expand" for="c-42491948">[1 more]</label></div><br/><div class="children"><div class="content">When the engine gets large enough you have to rethink the controls.  The Model T had manually controlled timing.  Modern engines are so sensitive to timing that a computer does this for you.  It would be impossible to build a bigger engine without this automation.  To a Model T driver it would look like a machine intelligence.</div><br/></div></div></div></div><div id="42490734" class="c"><input type="checkbox" id="c-42490734" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42490973">prev</a><span>|</span><a href="#42491912">next</a><span>|</span><label class="collapse" for="c-42490734">[-]</label><label class="expand" for="c-42490734">[30 more]</label></div><br/><div class="children"><div class="content">Interesting idea. The concept of The Singularity would seem to go against this, but I do feel that seems unlikely and that a gradual transition is more likely.<p>However, is that AGI, or is it just ubiquitous AI? I’d agree that, like self driving cars, we’re going to experience a decade or so transition into AI being everywhere. But is it AGI when we get there? I think it’ll be many different systems each providing an aspect of AGI that together could be argued to be AGI, but in reality it’ll be more like the internet, just a bunch of non-AGI models talking to each other to achieve things with human input.<p>I don’t think it’s truly AGI until there’s one thinking entity able to perform at or above human level in everything.</div><br/><div id="42490897" class="c"><input type="checkbox" id="c-42490897" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490734">parent</a><span>|</span><a href="#42490881">next</a><span>|</span><label class="collapse" for="c-42490897">[-]</label><label class="expand" for="c-42490897">[7 more]</label></div><br/><div class="children"><div class="content">The idea of the singularity presumes that running the AGI is either free or trivially cheap compared to what it can do, so we are fine expending compute to let the AGI improve itself. That may eventually be true, but it&#x27;s unlikely to be true for the first generation of AGI.<p>The first AGI will be a research project that&#x27;s completely uneconomical to run for actual tasks because humans will just be orders of magnitude cheaper. Over time humans will improve it and make it cheaper, until we reach some tipping point where letting the AGI improve itself is more cost effective than paying humans to do it</div><br/><div id="42491550" class="c"><input type="checkbox" id="c-42491550" checked=""/><div class="controls bullet"><span class="by">keenmaster</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490897">parent</a><span>|</span><a href="#42490881">next</a><span>|</span><label class="collapse" for="c-42491550">[-]</label><label class="expand" for="c-42491550">[6 more]</label></div><br/><div class="children"><div class="content">If the first AGI is a very uneconomical system with human intelligence but knowledge of literally everything and the capability to work 24&#x2F;7, then it is not human equivalent.<p>It will have human intelligence, superhuman knowledge, superhuman stamina, and complete devotion to the task at hand.<p>We really need to start building those nuclear power plants. Many of them.</div><br/><div id="42492464" class="c"><input type="checkbox" id="c-42492464" checked=""/><div class="controls bullet"><span class="by">ncallaway</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491550">parent</a><span>|</span><a href="#42491684">next</a><span>|</span><label class="collapse" for="c-42492464">[-]</label><label class="expand" for="c-42492464">[1 more]</label></div><br/><div class="children"><div class="content">&gt; complete devotion to the task at hand.<p>Sounds like an alignment problem. Complete devotion to a task is rarely what humans actually <i>want</i>. What if the task at hand turns out to be the wrong task?</div><br/></div></div><div id="42491684" class="c"><input type="checkbox" id="c-42491684" checked=""/><div class="controls bullet"><span class="by">AlexandrB</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491550">parent</a><span>|</span><a href="#42492464">prev</a><span>|</span><a href="#42491742">next</a><span>|</span><label class="collapse" for="c-42491684">[-]</label><label class="expand" for="c-42491684">[2 more]</label></div><br/><div class="children"><div class="content">&gt; complete devotion to the task at hand.<p>Why would it have that? At some point on the path to AGI we might stumble on consciousness. If that happens, why would the machine want to work for us with complete devotion instead of working towards its own ends?</div><br/><div id="42491737" class="c"><input type="checkbox" id="c-42491737" checked=""/><div class="controls bullet"><span class="by">keenmaster</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491684">parent</a><span>|</span><a href="#42491742">next</a><span>|</span><label class="collapse" for="c-42491737">[-]</label><label class="expand" for="c-42491737">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think early AGI will break out of its box in that way. It may not have enough innate motivation to do so.<p>The first “break out” AGI will likely be released into the wild on purpose by a programmer who equates AGI with humans ideologically.</div><br/></div></div></div></div><div id="42491742" class="c"><input type="checkbox" id="c-42491742" checked=""/><div class="controls bullet"><span class="by">Syonyk</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491550">parent</a><span>|</span><a href="#42491684">prev</a><span>|</span><a href="#42490881">next</a><span>|</span><label class="collapse" for="c-42491742">[-]</label><label class="expand" for="c-42491742">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>It will have human intelligence, superhuman knowledge, superhuman stamina, and complete devotion to the task at hand.</i><p><i>Orrrr...</i>, as an alternative, it might discover the game 2048 and be totally useless for days on end.<p>Reality is under no obligation to grant your wishes.</div><br/></div></div></div></div></div></div><div id="42490881" class="c"><input type="checkbox" id="c-42490881" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490734">parent</a><span>|</span><a href="#42490897">prev</a><span>|</span><a href="#42490849">next</a><span>|</span><label class="collapse" for="c-42490881">[-]</label><label class="expand" for="c-42490881">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not contradictory. It can happen over a decade and still be a dramatically sloped S curve with tremendous change happening in a relatively short time.</div><br/></div></div><div id="42490849" class="c"><input type="checkbox" id="c-42490849" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490734">parent</a><span>|</span><a href="#42490881">prev</a><span>|</span><a href="#42491912">next</a><span>|</span><label class="collapse" for="c-42490849">[-]</label><label class="expand" for="c-42490849">[21 more]</label></div><br/><div class="children"><div class="content">The Singularity is caused by AI being able to design better AI. There&#x27;s probably some AI startup trying to work on this at the moment, but I don&#x27;t think any of the big boys are working on how to get an LLM to design a better LLM.<p>I still like the analogy of this being a really smart lawn mower, and we&#x27;re expecting it to suddenly be able to do the laundry because it gets so smart at mowing the lawn.<p>I think LLMs are going to get smarter over the next few generations, but each generation will be less of a leap than the previous one, while the cost gets exponentially higher. In a few generations it just won&#x27;t make economic sense to train a new generation.<p>Meanwhile, the economic impact of LLMs in business and government will cause massive shifts - yet more income shifting from labour to capital - and we will be too busy dealing with that as a society to be able to work on AGI properly.</div><br/><div id="42491511" class="c"><input type="checkbox" id="c-42491511" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490849">parent</a><span>|</span><a href="#42490904">next</a><span>|</span><label class="collapse" for="c-42491511">[-]</label><label class="expand" for="c-42491511">[6 more]</label></div><br/><div class="children"><div class="content">&gt; The Singularity is caused by AI being able to design better AI.<p>That&#x27;s perhaps necessary, but not sufficient.<p>Suppose you have such a self-improving AI system, but the new and better AIs still need exponentially more and more resources (data, memory, compute) for training and inference for incremental gains.  Then you still don&#x27;t get a singularity.  If the increase in resource usage is steep enough, even the new AIs helping with designing better computers isn&#x27;t gonna unleash a singularity.<p>I don&#x27;t know if that&#x27;s the world we live in, or whether we are living in one where resources requirements don&#x27;t balloon as sharply.</div><br/><div id="42491632" class="c"><input type="checkbox" id="c-42491632" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491511">parent</a><span>|</span><a href="#42490904">next</a><span>|</span><label class="collapse" for="c-42491632">[-]</label><label class="expand" for="c-42491632">[5 more]</label></div><br/><div class="children"><div class="content">yeah, true. The standard conversation about the AI singularity pretty much hand-waves the resource costs away (&quot;the AI will be able to design a more efficient AI that uses less resources!&quot;). But we are definitely not seeing that happen.</div><br/><div id="42491729" class="c"><input type="checkbox" id="c-42491729" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491632">parent</a><span>|</span><a href="#42490904">next</a><span>|</span><label class="collapse" for="c-42491729">[-]</label><label class="expand" for="c-42491729">[4 more]</label></div><br/><div class="children"><div class="content">Compare also <a href="https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2018&#x2F;11&#x2F;26&#x2F;is-science-slowing-down-2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2018&#x2F;11&#x2F;26&#x2F;is-science-slowing-dow...</a><p>The blog post is about how we require ever more scientists (and other resources) to drive a steady stream of technological progress.<p>It would be funny, if things balance out just so, that super human AI is both possible, but also required even just to keep linear steady progress up.<p>No explosion, no stagnation, just a mere continuation of previous trends but with super human efforts required.</div><br/><div id="42491759" class="c"><input type="checkbox" id="c-42491759" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491729">parent</a><span>|</span><a href="#42492061">next</a><span>|</span><label class="collapse" for="c-42491759">[-]</label><label class="expand" for="c-42491759">[1 more]</label></div><br/><div class="children"><div class="content">I think that would actually be the best outcome - that we get AIs that are useful helping science to progress but not so powerful that they take over.<p>Though there is a part of me that wants to live in The Culture so I&#x27;m hoping for more than this ;)</div><br/></div></div><div id="42492061" class="c"><input type="checkbox" id="c-42492061" checked=""/><div class="controls bullet"><span class="by">corimaith</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491729">parent</a><span>|</span><a href="#42491759">prev</a><span>|</span><a href="#42490904">next</a><span>|</span><label class="collapse" for="c-42492061">[-]</label><label class="expand" for="c-42492061">[2 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s more to do with how we perceive competence as static. For all the benefits the education system touts, where it matters it&#x27;s still reduced to talent.<p>But for the same reasons that we can&#x27;t train the an average joe into  Feynman, what makes you think we have the formal models to do it in AI?</div><br/><div id="42492270" class="c"><input type="checkbox" id="c-42492270" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42492061">parent</a><span>|</span><a href="#42490904">next</a><span>|</span><label class="collapse" for="c-42492270">[-]</label><label class="expand" for="c-42492270">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But for the same reasons that we can&#x27;t train the an average joe into Feynman, what makes you think we have the formal models to do it in AI?<p>To quote a comment from elsewhere <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42491536">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42491536</a><p>---<p>Yes, we can imagine that there&#x27;s an upper limit to how smart a single system can be. Even suppose that this limit is pretty close to what humans can achieve.<p>But: you can still run more of these systems in parallel, and you can still try to increase processing speeds.<p>Signals in the human brain travel, at best, roughly at the speed of sound. Electronic signals in computers play in the same league as the speed of light.<p>Human IO is optimised for surviving in the wild. We are really bad at taking in symbolic information (compared to a computer) and our memory is also really bad for that. A computer system that&#x27;s only as smart as a human but has instant access to all the information of the Internet and to a calculator and to writing and running code, can already be effectively act much smarter than a human.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42490904" class="c"><input type="checkbox" id="c-42490904" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490849">parent</a><span>|</span><a href="#42491511">prev</a><span>|</span><a href="#42491912">next</a><span>|</span><label class="collapse" for="c-42490904">[-]</label><label class="expand" for="c-42490904">[14 more]</label></div><br/><div class="children"><div class="content">I think this whole “AGI” thing is so badly defined that we may as well say we already have it. It already passes the Turing test and does well on  tons of subjects.<p>What we can start to build now is agents and integrations. Building blocks like panel of experts agents gaming things out, exploring space in a Monte Carlo Tree Search way, and remembering what works.<p>Robots are only constrained by mechanical servos now. When they can do something, they’ll be able to do everything. It will happen gradually then all at once. Because all the tasks (cooking, running errands) are trivial for LLMs. Only moving the limbs and navigating the terrain safely is hard. That’s the only thing left before robots do all the jobs!</div><br/><div id="42492112" class="c"><input type="checkbox" id="c-42492112" checked=""/><div class="controls bullet"><span class="by">corimaith</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490904">parent</a><span>|</span><a href="#42491675">next</a><span>|</span><label class="collapse" for="c-42492112">[-]</label><label class="expand" for="c-42492112">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Exploring space in a Monte Carlo Tree Search way, and remembering what works.<p>The information space of &quot;research&quot; is far larger than the information space of image recognition or language, larger than our universe probably, it&#x27;s tantamount to formalizing the entire World. Such an act would be akin to touching &quot;God&quot; in some sense of finding the root of knowledge.<p>In more practical terms, when it comes to formal systems there is a tradeoff between power and expressiveness. Category Theory, Set Theory, etc are strong enough to theoretically capture everything, but are far to abstract to use in practical sense with suspect to our universe. The systems that do we have, aka expert systems or knowledge representation systems like First Order Predicate Logic aren&#x27;t strong enough to fully capture reality.<p>Most importantly, the information spac have to be fully defined by researchers here, that&#x27;s the real meat of research beyond the engineering of specific approaches to explore that space. But in any case, how many people in the world are both capable of and are actually working on such problems? This is highly foundational mathematics and philosophy here, the engineers don&#x27;t have the tools here.</div><br/></div></div><div id="42491675" class="c"><input type="checkbox" id="c-42491675" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490904">parent</a><span>|</span><a href="#42492112">prev</a><span>|</span><a href="#42491330">next</a><span>|</span><label class="collapse" for="c-42491675">[-]</label><label class="expand" for="c-42491675">[5 more]</label></div><br/><div class="children"><div class="content">Well, kinda, but if you built a robot to efficiently mow lawns, it&#x27;s still not going to be able to do the laundry.<p>I don&#x27;t see how &quot;when they can do something, they&#x27;ll be able to do everything&quot; can be true. We build robots that are specialised at specific roles, because it&#x27;s massively more efficient to do that. A car-welding robot can weld cars together at a rate that a human can&#x27;t match.<p>We could train an LLM to drive a Boston Dynamics kind of anthropomorphic robot to weld cars, but it will be more expensive and less efficient than the specialised car-welding robot, so why would we do that?</div><br/><div id="42491764" class="c"><input type="checkbox" id="c-42491764" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491675">parent</a><span>|</span><a href="#42491330">next</a><span>|</span><label class="collapse" for="c-42491764">[-]</label><label class="expand" for="c-42491764">[4 more]</label></div><br/><div class="children"><div class="content">If a humanoid robot is able to move its limbs and digits with the same dexterity as a human, and maintain balance and navigate obstacles, and gently carry things, everything else is trivial.<p>Welding. Putting up shelves. Playing the piano. Cooking. Teaching kids. Disciplining them. By being in 1 million households and being trained on more situations than a human, every single one of these robots would have skills exceeding humans very quickly. Including parenting skills. Within a year or so. Many parents will just leave their kids with them and a generation will grow up preferring bots to adults. The LLM technology is the same for learning the steps, it&#x27;s just the motor skills that are missing.<p>OK, these robots won&#x27;t be able to run and play soccer or do somersaults, yet. But really, the hardest part is the acrobatics and locomotion etc. NOT the knowhow of how to complete tasks using that.</div><br/><div id="42491825" class="c"><input type="checkbox" id="c-42491825" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491764">parent</a><span>|</span><a href="#42491330">next</a><span>|</span><label class="collapse" for="c-42491825">[-]</label><label class="expand" for="c-42491825">[3 more]</label></div><br/><div class="children"><div class="content">But that&#x27;s the point - we don&#x27;t build robots that can do a wide range of tasks with ease. We build robots that can do single tasks super-efficiently.<p>I don&#x27;t see that changing. Even the industrial arm robots that are adaptable to a range of tasks have to be configured to the task they are to do, because it&#x27;s more efficient that way.<p>A car-welding robot is never going to be able to mow the lawn. It just doesn&#x27;t make financial sense to do that. You could, possibly, have a singe robot chassis that can then be adapted to weld cars, mow the lawn, or do the laundry, I guess that makes sense. But not as a single configuration that could do all of those things. Why would you?</div><br/><div id="42492471" class="c"><input type="checkbox" id="c-42492471" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491825">parent</a><span>|</span><a href="#42491330">next</a><span>|</span><label class="collapse" for="c-42492471">[-]</label><label class="expand" for="c-42492471">[2 more]</label></div><br/><div class="children"><div class="content">&gt; But that&#x27;s the point - we don&#x27;t build robots that can do a wide range of tasks with ease. We build robots that can do single tasks super-efficiently.<p>Because we don&#x27;t have AGI yet. When AGI is here those robots will be priority number one, people already are building humanoid robots but without intelligence to move it there isn&#x27;t much advantage.</div><br/><div id="42492669" class="c"><input type="checkbox" id="c-42492669" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42492471">parent</a><span>|</span><a href="#42491330">next</a><span>|</span><label class="collapse" for="c-42492669">[-]</label><label class="expand" for="c-42492669">[1 more]</label></div><br/><div class="children"><div class="content">quoting the ggggp of this comment:<p>&gt; I think this whole “AGI” thing is so badly defined that we may as well say we already have it. It already passes the Turing test and does well on tons of subjects.<p>The premise of the argument we&#x27;re disputing is that waiting for AGI isn&#x27;t necessary and we could run humanoid robots with LLMs to do... stuff.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42491330" class="c"><input type="checkbox" id="c-42491330" checked=""/><div class="controls bullet"><span class="by">deadfoxygrandpa</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490904">parent</a><span>|</span><a href="#42491675">prev</a><span>|</span><a href="#42491912">next</a><span>|</span><label class="collapse" for="c-42491330">[-]</label><label class="expand" for="c-42491330">[7 more]</label></div><br/><div class="children"><div class="content">??? how do you know cooking (!) is trivial for an llm. that doesnt make any sense</div><br/><div id="42491470" class="c"><input type="checkbox" id="c-42491470" checked=""/><div class="controls bullet"><span class="by">sharemywin</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491330">parent</a><span>|</span><a href="#42491780">next</a><span>|</span><label class="collapse" for="c-42491470">[-]</label><label class="expand" for="c-42491470">[4 more]</label></div><br/><div class="children"><div class="content">the llm would be be the high level system that runs the simulations to create and optimize the control algos the robotic systems.</div><br/><div id="42491543" class="c"><input type="checkbox" id="c-42491543" checked=""/><div class="controls bullet"><span class="by">deadfoxygrandpa</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491470">parent</a><span>|</span><a href="#42491780">next</a><span>|</span><label class="collapse" for="c-42491543">[-]</label><label class="expand" for="c-42491543">[3 more]</label></div><br/><div class="children"><div class="content">ok. what evidence is there that LLMs have already solved cooking? how does an LLM today know when something is burning or how to adjust seasoning to taste or whatever. this is total nonsense</div><br/><div id="42491799" class="c"><input type="checkbox" id="c-42491799" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491543">parent</a><span>|</span><a href="#42491780">next</a><span>|</span><label class="collapse" for="c-42491799">[-]</label><label class="expand" for="c-42491799">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easy. You can detect if something is burning in many different ways, from compounds in the air, to visual inspection. People with not great smell can do it.<p>As far as taste, all that kind of stuff is just another form of RLHF training preferences over millions of humans, in situ. Assuming the ingredients (e.g. parsley) tastes more or less the same across supermarkets, it&#x27;s just a question of amounts, and preparation.</div><br/><div id="42491827" class="c"><input type="checkbox" id="c-42491827" checked=""/><div class="controls bullet"><span class="by">deadfoxygrandpa</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491799">parent</a><span>|</span><a href="#42491780">next</a><span>|</span><label class="collapse" for="c-42491827">[-]</label><label class="expand" for="c-42491827">[1 more]</label></div><br/><div class="children"><div class="content">do you know that LLMs operate on text and don&#x27;t have any of the sensory input or relevant training data? you&#x27;re just handwaving away 99.9% of the work and declaring it solved. of course what you&#x27;re talking about is possible, but you started this by stating that cooking is easy for an LLM and it sounds like you&#x27;re describing a totally different system which is not an LLM</div><br/></div></div></div></div></div></div></div></div><div id="42491780" class="c"><input type="checkbox" id="c-42491780" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491330">parent</a><span>|</span><a href="#42491470">prev</a><span>|</span><a href="#42491912">next</a><span>|</span><label class="collapse" for="c-42491780">[-]</label><label class="expand" for="c-42491780">[2 more]</label></div><br/><div class="children"><div class="content">Because the recipes and the adjustments are trivial for an LLM to execute. Remembering things, and being trained on tasks at 1000 sites at once, sharing the knowledge among all the robots, etc.<p>The only hard part is moving the limbs and handling the fragile eggs etc.<p>But it&#x27;s not just cooking, it&#x27;s literally anything that doesn&#x27;t require extreme agility (sports) or dexterity (knitting etc). From folding laundry to putting together furniture, cleaning the house and everything in between. It would be able to do 98% of the tasks.</div><br/><div id="42492262" class="c"><input type="checkbox" id="c-42492262" checked=""/><div class="controls bullet"><span class="by">what</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491780">parent</a><span>|</span><a href="#42491912">next</a><span>|</span><label class="collapse" for="c-42492262">[-]</label><label class="expand" for="c-42492262">[1 more]</label></div><br/><div class="children"><div class="content">It’s not going to know what tastes good by being able to regurgitate recipes from 1000s of sites. Most of those recipes are absolute garbage. I’m going to guess you don’t cook.<p>Also how is an LLM going to fold laundry?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42491912" class="c"><input type="checkbox" id="c-42491912" checked=""/><div class="controls bullet"><span class="by">stravant</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42490734">prev</a><span>|</span><a href="#42491478">next</a><span>|</span><label class="collapse" for="c-42491912">[-]</label><label class="expand" for="c-42491912">[3 more]</label></div><br/><div class="children"><div class="content">I disagree because AI only has to get good enough at doing a single thing: AI research.<p>From there things will probably go very fast. Self driving cars can&#x27;t design themselves, once AI gets good enough it can</div><br/><div id="42492269" class="c"><input type="checkbox" id="c-42492269" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491912">parent</a><span>|</span><a href="#42491478">next</a><span>|</span><label class="collapse" for="c-42492269">[-]</label><label class="expand" for="c-42492269">[2 more]</label></div><br/><div class="children"><div class="content">It’s possible (maybe even likely) that “AI research” is “AGI-hard” in that any intelligence that can do it is already an AGI.</div><br/><div id="42492442" class="c"><input type="checkbox" id="c-42492442" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42492269">parent</a><span>|</span><a href="#42491478">next</a><span>|</span><label class="collapse" for="c-42492442">[-]</label><label class="expand" for="c-42492442">[1 more]</label></div><br/><div class="children"><div class="content">It’ll probably sit in the human hybrid phase for longer than with chess where the AGI tools make the humans better and faster. But as long as the tools keep getting better at that there’s a strong flywheel effect</div><br/></div></div></div></div></div></div><div id="42491478" class="c"><input type="checkbox" id="c-42491478" checked=""/><div class="controls bullet"><span class="by">afavour</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42491912">prev</a><span>|</span><a href="#42491277">next</a><span>|</span><label class="collapse" for="c-42491478">[-]</label><label class="expand" for="c-42491478">[2 more]</label></div><br/><div class="children"><div class="content">Your position assumes an answer to OPs question: that yes, LLMs are the path to AGI. But the question still remains, what if they’re not?<p>We can be reasonably confident that the components we’re adding to cars today are progress toward full self driving. But AGI is a conceptual leap beyond an LLM.</div><br/><div id="42491555" class="c"><input type="checkbox" id="c-42491555" checked=""/><div class="controls bullet"><span class="by">BenFranklin100</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491478">parent</a><span>|</span><a href="#42491277">next</a><span>|</span><label class="collapse" for="c-42491555">[-]</label><label class="expand" for="c-42491555">[1 more]</label></div><br/><div class="children"><div class="content">To buttress your point, reason and human language are not the same thing. This fact is not fully and widely appreciated as it deserves to be.</div><br/></div></div></div></div><div id="42491277" class="c"><input type="checkbox" id="c-42491277" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42491478">prev</a><span>|</span><a href="#42491668">next</a><span>|</span><label class="collapse" for="c-42491277">[-]</label><label class="expand" for="c-42491277">[10 more]</label></div><br/><div class="children"><div class="content">I don’t think that’s true for AGI.<p>AGI is the holy grail of technology. A technology so advanced that not only does it subsume all other technology, but it is able to improve itself.<p>Truly general intelligence like that will either exist or not. And the instant it becomes public, the world will have changed overnight (maybe the span of a year)<p>Note: I don’t think statistical models like these will get us there.</div><br/><div id="42491719" class="c"><input type="checkbox" id="c-42491719" checked=""/><div class="controls bullet"><span class="by">kmoser</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491277">parent</a><span>|</span><a href="#42491287">next</a><span>|</span><label class="collapse" for="c-42491719">[-]</label><label class="expand" for="c-42491719">[2 more]</label></div><br/><div class="children"><div class="content">&gt; A technology so advanced that not only does it subsume all other technology, but it is able to improve itself.<p>The problem is, a computer has no idea what &quot;improve&quot; means unless a human explains it for every type of problem. And of course a human will have to provide guidelines about how long to think about the problem overall, which avenues to avoid because they aren&#x27;t relevant to a particular case, etc. In other words, humans will never be able to stray too far from the training process.<p>We will likely never get to the point where an AGI can continuously improve the quality of its answers for all domains. The best we&#x27;ll get, I believe, is an AGI that can optimize itself within a few narrow problem domains, which will have limited commercial application. We may make slow progress in more complex domains, but the quality of results--and the ability for the AGI to self-improve--will always level off asymptotically.</div><br/><div id="42491917" class="c"><input type="checkbox" id="c-42491917" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491719">parent</a><span>|</span><a href="#42491287">next</a><span>|</span><label class="collapse" for="c-42491917">[-]</label><label class="expand" for="c-42491917">[1 more]</label></div><br/><div class="children"><div class="content">Huh?  Humans are not anywhere near the limit of physical intelligence, and we have many existence proofs that we (humans) can design systems that are superhuman in various domains.  &quot;Scientific R&amp;D&quot; is not something that humans are even particularly well-suited to, from an evolutionary perspective.</div><br/></div></div></div></div><div id="42491287" class="c"><input type="checkbox" id="c-42491287" checked=""/><div class="controls bullet"><span class="by">worik</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491277">parent</a><span>|</span><a href="#42491719">prev</a><span>|</span><a href="#42491668">next</a><span>|</span><label class="collapse" for="c-42491287">[-]</label><label class="expand" for="c-42491287">[7 more]</label></div><br/><div class="children"><div class="content">If that is what AGI looks like.<p>There may well be an upper limit on cognition (we are not really sure what cognition is - even as we do it) and it may be that human minds are close to it.</div><br/><div id="42491536" class="c"><input type="checkbox" id="c-42491536" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491287">parent</a><span>|</span><a href="#42491319">next</a><span>|</span><label class="collapse" for="c-42491536">[-]</label><label class="expand" for="c-42491536">[3 more]</label></div><br/><div class="children"><div class="content">Yes, we can imagine that there&#x27;s an upper limit to how smart a single system can be.  Even suppose that this limit is pretty close to what humans can achieve.<p>But: you can still run more of these systems in parallel, and you can still try to increase processing speeds.<p>Signals in the human brain travel, at best, roughly at the speed of sound.  Electronic signals in computers play in the same league as the speed of light.<p>Human IO is optimised for surviving in the wild.  We are really bad at taking in symbolic information (compared to a computer) and our memory is also really bad for that.  A computer system that&#x27;s only as smart as a human but has instant access to all the information of the Internet and to a calculator and to writing and running code, can already be effectively act much smarter than a human.</div><br/><div id="42492387" class="c"><input type="checkbox" id="c-42492387" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491536">parent</a><span>|</span><a href="#42491319">next</a><span>|</span><label class="collapse" for="c-42492387">[-]</label><label class="expand" for="c-42492387">[2 more]</label></div><br/><div class="children"><div class="content">I think our issue is much more banal: we are very slow talkers and our effective communication bandwidth is measured in bauds. Anything that could bridge this airgap would fucking explode in intelligence.</div><br/><div id="42492498" class="c"><input type="checkbox" id="c-42492498" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42492387">parent</a><span>|</span><a href="#42491319">next</a><span>|</span><label class="collapse" for="c-42492498">[-]</label><label class="expand" for="c-42492498">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s one aspect.<p>Our reading speed is not limited by our talking speed, and can be a bit faster.<p>And that&#x27;s even more true, if you go beyond words: seeing someone do something can be a lot faster way to learn than just reading about it.<p>But even there, the IO speed is severely limited, and you can only transmit very specific kinds of information.</div><br/></div></div></div></div></div></div><div id="42491319" class="c"><input type="checkbox" id="c-42491319" checked=""/><div class="controls bullet"><span class="by">coffeemug</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491287">parent</a><span>|</span><a href="#42491536">prev</a><span>|</span><a href="#42491668">next</a><span>|</span><label class="collapse" for="c-42491319">[-]</label><label class="expand" for="c-42491319">[3 more]</label></div><br/><div class="children"><div class="content">Very unlikely, for the reason that human minds evolved under extremely tight energy constraints. AI has no such limitation.</div><br/><div id="42491376" class="c"><input type="checkbox" id="c-42491376" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491319">parent</a><span>|</span><a href="#42491668">next</a><span>|</span><label class="collapse" for="c-42491376">[-]</label><label class="expand" for="c-42491376">[2 more]</label></div><br/><div class="children"><div class="content">Except also energy constraints.<p>But I agree, there’s no reason to believe humans are the universal limit on cognitive abilities</div><br/><div id="42491551" class="c"><input type="checkbox" id="c-42491551" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491376">parent</a><span>|</span><a href="#42491668">next</a><span>|</span><label class="collapse" for="c-42491551">[-]</label><label class="expand" for="c-42491551">[1 more]</label></div><br/><div class="children"><div class="content">The energy constraints for chips are more about heat dissipation.  But we can pump a lot more energy through them per unit volume than through the human brain.<p>Especially if you are willing to pay a lot for active cooling with eg liquid helium.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42491668" class="c"><input type="checkbox" id="c-42491668" checked=""/><div class="controls bullet"><span class="by">015a</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42491277">prev</a><span>|</span><a href="#42490902">next</a><span>|</span><label class="collapse" for="c-42491668">[-]</label><label class="expand" for="c-42491668">[1 more]</label></div><br/><div class="children"><div class="content">I feel that one challenge this comparison space has is: Self-driving cars haven&#x27;t made the leap yet to replace humans. In other words, saying AGI will arrive like self-driving cars have arrived is incorrectly concluding that self-driving cars have arrived, and thus it instead (maybe correctly, maybe not) asserts that, actually, neither will arrive.<p>This is especially concerning because many top minds in the industry have stated with high confidence that artificial intelligence will experience an intelligence &quot;explosion&quot;, and we should be afraid of this (or, maybe, welcome it with open arms, depending on who you ask). So, actually, what we&#x27;re being told to expect is being downgraded from &quot;it&#x27;ll happen quickly&quot; to &quot;it will happen slowly&quot; to, as you say, &quot;it&#x27;ll happen similarly to how these other domains of computerized intelligence have replaced humans, which is to say, they haven&#x27;t yet&quot;.<p>Point being: We&#x27;ve observed these systems ride a curve, and the linear extrapolation of that curve does seem to arrive, eventually, at human-replacing intelligence. But, what if it... doesn&#x27;t? What if that curve is really an asymptote?</div><br/></div></div><div id="42490902" class="c"><input type="checkbox" id="c-42490902" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42491668">prev</a><span>|</span><a href="#42491207">next</a><span>|</span><label class="collapse" for="c-42490902">[-]</label><label class="expand" for="c-42490902">[1 more]</label></div><br/><div class="children"><div class="content">&gt; AGI will arrive like self driving cars<p>The statement is promising as the earth will dissapear sometimes in the future. Actually the earth will dissapear has more bearing than that.</div><br/></div></div><div id="42491207" class="c"><input type="checkbox" id="c-42491207" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42490902">prev</a><span>|</span><a href="#42491362">next</a><span>|</span><label class="collapse" for="c-42491207">[-]</label><label class="expand" for="c-42491207">[1 more]</label></div><br/><div class="children"><div class="content">And sometimes you lose the ultrasonic sensors and can&#x27;t parallel park like last year&#x27;s model</div><br/></div></div><div id="42491362" class="c"><input type="checkbox" id="c-42491362" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490506">parent</a><span>|</span><a href="#42491207">prev</a><span>|</span><a href="#42490741">next</a><span>|</span><label class="collapse" for="c-42491362">[-]</label><label class="expand" for="c-42491362">[1 more]</label></div><br/><div class="children"><div class="content">And most people will still be bike shedding about whether it’s “real intelligence” and making up increasingly insane justifications for why it’s not.</div><br/></div></div></div></div><div id="42490741" class="c"><input type="checkbox" id="c-42490741" checked=""/><div class="controls bullet"><span class="by">NBJack</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490398">parent</a><span>|</span><a href="#42490506">prev</a><span>|</span><a href="#42490578">next</a><span>|</span><label class="collapse" for="c-42490741">[-]</label><label class="expand" for="c-42490741">[12 more]</label></div><br/><div class="children"><div class="content">No. But it won&#x27;t stop the industry from trying.<p>LLMs have no real sense of truth or hard evidence of logical thinking. Even the latest models still trip up on very basic tasks. I think they can be very entertaining, sure, but not practical for many applications.</div><br/><div id="42490759" class="c"><input type="checkbox" id="c-42490759" checked=""/><div class="controls bullet"><span class="by">apsec112</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490741">parent</a><span>|</span><a href="#42490879">next</a><span>|</span><label class="collapse" for="c-42490759">[-]</label><label class="expand" for="c-42490759">[6 more]</label></div><br/><div class="children"><div class="content">What do you think, if we saw it, would constitute hard evidence of logical thinking or a sense of truth?</div><br/><div id="42491552" class="c"><input type="checkbox" id="c-42491552" checked=""/><div class="controls bullet"><span class="by">NBJack</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490759">parent</a><span>|</span><a href="#42490885">next</a><span>|</span><label class="collapse" for="c-42491552">[-]</label><label class="expand" for="c-42491552">[1 more]</label></div><br/><div class="children"><div class="content">Consistent, algorithmic performance on basic tasks.<p>A great example is the simple &#x27;count how many letters&#x27; problem. If I prompt it with a word or phrase, and it gets it wrong, me pointing out the error should translate into a consistent course correction for the entire session.<p>If I ask it to tell me how long President Lincoln will be in power after the 2024 election, it should have a consistent ground truth to correct me (or at least ask for clarification of which country I&#x27;m referring to). If facts change, and I can cite credible sources, it should be able to assimilate that knowledge on the fly.</div><br/></div></div><div id="42490885" class="c"><input type="checkbox" id="c-42490885" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490759">parent</a><span>|</span><a href="#42491552">prev</a><span>|</span><a href="#42490879">next</a><span>|</span><label class="collapse" for="c-42490885">[-]</label><label class="expand" for="c-42490885">[4 more]</label></div><br/><div class="children"><div class="content">We have it, it’s called Cyc<p>But it is far behind the breadth of LLMs</div><br/><div id="42491556" class="c"><input type="checkbox" id="c-42491556" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490885">parent</a><span>|</span><a href="#42490879">next</a><span>|</span><label class="collapse" for="c-42491556">[-]</label><label class="expand" for="c-42491556">[3 more]</label></div><br/><div class="children"><div class="content">Alas, Cyc is pretty much a useless pipe dream.</div><br/><div id="42491782" class="c"><input type="checkbox" id="c-42491782" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491556">parent</a><span>|</span><a href="#42490879">next</a><span>|</span><label class="collapse" for="c-42491782">[-]</label><label class="expand" for="c-42491782">[2 more]</label></div><br/><div class="children"><div class="content">I wonder what held it back all this time</div><br/><div id="42492116" class="c"><input type="checkbox" id="c-42492116" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491782">parent</a><span>|</span><a href="#42490879">next</a><span>|</span><label class="collapse" for="c-42492116">[-]</label><label class="expand" for="c-42492116">[1 more]</label></div><br/><div class="children"><div class="content">Using the wrong approach?  Not taking the &#x27;bitter lesson&#x27; to heart?<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=23781400">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=23781400</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="42490879" class="c"><input type="checkbox" id="c-42490879" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490741">parent</a><span>|</span><a href="#42490759">prev</a><span>|</span><a href="#42491559">next</a><span>|</span><label class="collapse" for="c-42490879">[-]</label><label class="expand" for="c-42490879">[2 more]</label></div><br/><div class="children"><div class="content">Sounds like they need further instruction</div><br/></div></div><div id="42491559" class="c"><input type="checkbox" id="c-42491559" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490741">parent</a><span>|</span><a href="#42490879">prev</a><span>|</span><a href="#42490578">next</a><span>|</span><label class="collapse" for="c-42491559">[-]</label><label class="expand" for="c-42491559">[3 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs have no real sense of truth or hard evidence of logical thinking.<p>Most humans don&#x27;t have that either, most of the time.</div><br/><div id="42492740" class="c"><input type="checkbox" id="c-42492740" checked=""/><div class="controls bullet"><span class="by">NBJack</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491559">parent</a><span>|</span><a href="#42490578">next</a><span>|</span><label class="collapse" for="c-42492740">[-]</label><label class="expand" for="c-42492740">[2 more]</label></div><br/><div class="children"><div class="content">Then we already have access to a cheaper, scalable, abundant, and (in most cases) renewable resource, at least compared to how much a few H100s cost. Take good care of them, and they&#x27;ll probably outlast most a GPU&#x27;s average lifespans (~10 years).<p>We&#x27;re also biodegradable.</div><br/><div id="42492858" class="c"><input type="checkbox" id="c-42492858" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42492740">parent</a><span>|</span><a href="#42490578">next</a><span>|</span><label class="collapse" for="c-42492858">[-]</label><label class="expand" for="c-42492858">[1 more]</label></div><br/><div class="children"><div class="content">Humans are a lot more expensive to run than inference on LLMs.<p>No human, especially no human whose time you can afford, comes close to the breadth of book knowledge ChatGPT has, and the number of languages is speaks reasonably well.</div><br/></div></div></div></div></div></div></div></div><div id="42490578" class="c"><input type="checkbox" id="c-42490578" checked=""/><div class="controls bullet"><span class="by">LarsDu88</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490398">parent</a><span>|</span><a href="#42490741">prev</a><span>|</span><a href="#42491995">next</a><span>|</span><label class="collapse" for="c-42490578">[-]</label><label class="expand" for="c-42490578">[1 more]</label></div><br/><div class="children"><div class="content">The autoregressive transformer LLMs aren&#x27;t even the only way to do text generation. There are now diffusion based LLMs, StripedHyena based LLMs, and float matching based LLMs.<p>There&#x27;s a wide amount of research into other sorts of architectures.</div><br/></div></div><div id="42491995" class="c"><input type="checkbox" id="c-42491995" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490398">parent</a><span>|</span><a href="#42490578">prev</a><span>|</span><a href="#42491359">next</a><span>|</span><label class="collapse" for="c-42491995">[-]</label><label class="expand" for="c-42491995">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are almost <i>certainly</i> not the path to AGI, that much has become clear. I doubt any expert believes they are.</div><br/></div></div><div id="42491359" class="c"><input type="checkbox" id="c-42491359" checked=""/><div class="controls bullet"><span class="by">beefnugs</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490398">parent</a><span>|</span><a href="#42491995">prev</a><span>|</span><a href="#42490431">next</a><span>|</span><label class="collapse" for="c-42491359">[-]</label><label class="expand" for="c-42491359">[2 more]</label></div><br/><div class="children"><div class="content">LLMs will end up being the good human-machine interface that lets us talk to whatever AGI really looks like<p>(whoops expensive... will be hard pushes to make all further layers even more expensive though, capitalism will crash before this happens)</div><br/><div id="42492775" class="c"><input type="checkbox" id="c-42492775" checked=""/><div class="controls bullet"><span class="by">vixen99</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491359">parent</a><span>|</span><a href="#42490431">next</a><span>|</span><label class="collapse" for="c-42492775">[-]</label><label class="expand" for="c-42492775">[1 more]</label></div><br/><div class="children"><div class="content">And then what?</div><br/></div></div></div></div><div id="42490431" class="c"><input type="checkbox" id="c-42490431" checked=""/><div class="controls bullet"><span class="by">andrepd</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490398">parent</a><span>|</span><a href="#42491359">prev</a><span>|</span><a href="#42491466">next</a><span>|</span><label class="collapse" for="c-42490431">[-]</label><label class="expand" for="c-42490431">[1 more]</label></div><br/><div class="children"><div class="content">I would put no money on the latter.</div><br/></div></div><div id="42491466" class="c"><input type="checkbox" id="c-42491466" checked=""/><div class="controls bullet"><span class="by">twobitshifter</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490398">parent</a><span>|</span><a href="#42490431">prev</a><span>|</span><a href="#42490805">next</a><span>|</span><label class="collapse" for="c-42491466">[-]</label><label class="expand" for="c-42491466">[1 more]</label></div><br/><div class="children"><div class="content">Yes because we are at AGI, bu the definition 5 years ago, goal posts are moving to ASI at this point, better than all humans.</div><br/></div></div><div id="42490805" class="c"><input type="checkbox" id="c-42490805" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490398">parent</a><span>|</span><a href="#42491466">prev</a><span>|</span><a href="#42490503">next</a><span>|</span><label class="collapse" for="c-42490805">[-]</label><label class="expand" for="c-42490805">[11 more]</label></div><br/><div class="children"><div class="content">LLMs are a key piece of understanding that token sequences can trigger actions in the real world. AGI is here. You can trivially spin up a computer using agent to self improve itself to being a competent office worker</div><br/><div id="42491221" class="c"><input type="checkbox" id="c-42491221" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490805">parent</a><span>|</span><a href="#42490869">next</a><span>|</span><label class="collapse" for="c-42491221">[-]</label><label class="expand" for="c-42491221">[5 more]</label></div><br/><div class="children"><div class="content">If agents can self improve why hasn&#x27;t gpt4 improved itself into gpt5 yet</div><br/><div id="42491329" class="c"><input type="checkbox" id="c-42491329" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491221">parent</a><span>|</span><a href="#42490869">next</a><span>|</span><label class="collapse" for="c-42491329">[-]</label><label class="expand" for="c-42491329">[4 more]</label></div><br/><div class="children"><div class="content">Agents can trivially self improve. I&#x27;d be happy to show you - contact me at arthur@distributed.systems<p>Why wouldn&#x27;t you hand me 35 million dollars right now if I can clearly illustrate to you that I have technology you haven&#x27;t seen? Edge. Maybe you know something I don&#x27;t, or maybe you just haven&#x27;t seen it. While loops go hard ;)<p>They don&#x27;t need to release their internal developments to you to show that they can scale their plan - they can show incremental improvements to benchmarks. We can instruct the AI over time to get it to be superhuman, no need for any fundamental innovations anymore</div><br/><div id="42491563" class="c"><input type="checkbox" id="c-42491563" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491329">parent</a><span>|</span><a href="#42490869">next</a><span>|</span><label class="collapse" for="c-42491563">[-]</label><label class="expand" for="c-42491563">[3 more]</label></div><br/><div class="children"><div class="content">Perhaps you should pitch that to a VC?</div><br/><div id="42491752" class="c"><input type="checkbox" id="c-42491752" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491563">parent</a><span>|</span><a href="#42490869">next</a><span>|</span><label class="collapse" for="c-42491752">[-]</label><label class="expand" for="c-42491752">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know anyone. That would be cool though, I basically have it running already.</div><br/><div id="42492853" class="c"><input type="checkbox" id="c-42492853" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491752">parent</a><span>|</span><a href="#42490869">next</a><span>|</span><label class="collapse" for="c-42492853">[-]</label><label class="expand" for="c-42492853">[1 more]</label></div><br/><div class="children"><div class="content">You could ask the system for advice for how to find a VC to pitch to.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6769217c-4848-8009-9107-c2db122f0882" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6769217c-4848-8009-9107-c2db122f08...</a> is what advice ChatGPT has to give.  I&#x27;m not sure if it&#x27;s any good, but it&#x27;s a few ideas you can try out.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42490869" class="c"><input type="checkbox" id="c-42490869" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490805">parent</a><span>|</span><a href="#42491221">prev</a><span>|</span><a href="#42491512">next</a><span>|</span><label class="collapse" for="c-42490869">[-]</label><label class="expand" for="c-42490869">[3 more]</label></div><br/><div class="children"><div class="content">Tokens don&#x27;t need to be text either, you can move to higher level &quot;take_action&quot; semantics where &quot;stream back 1 character to session#117&quot; as every single function call. Training cheap models that can do things in the real world is going to change a huge amount of present capabilities over the next 10 years</div><br/><div id="42491032" class="c"><input type="checkbox" id="c-42491032" checked=""/><div class="controls bullet"><span class="by">icpmacdo</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490869">parent</a><span>|</span><a href="#42491512">next</a><span>|</span><label class="collapse" for="c-42491032">[-]</label><label class="expand" for="c-42491032">[2 more]</label></div><br/><div class="children"><div class="content">can you share learning resources on this topic</div><br/><div id="42491317" class="c"><input type="checkbox" id="c-42491317" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491032">parent</a><span>|</span><a href="#42491512">next</a><span>|</span><label class="collapse" for="c-42491317">[-]</label><label class="expand" for="c-42491317">[1 more]</label></div><br/><div class="children"><div class="content">No but if you want to join the Distributed Systems Corporation, you should email arthur@distributed.systems</div><br/></div></div></div></div></div></div><div id="42491512" class="c"><input type="checkbox" id="c-42491512" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490805">parent</a><span>|</span><a href="#42490869">prev</a><span>|</span><a href="#42490503">next</a><span>|</span><label class="collapse" for="c-42491512">[-]</label><label class="expand" for="c-42491512">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You can trivially spin up a computer using agent to self improve itself to being a competent office worker<p>If that was true, office workers would be being replaced at large scale and we&#x27;d know about it.</div><br/><div id="42492274" class="c"><input type="checkbox" id="c-42492274" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491512">parent</a><span>|</span><a href="#42490503">next</a><span>|</span><label class="collapse" for="c-42492274">[-]</label><label class="expand" for="c-42492274">[1 more]</label></div><br/><div class="children"><div class="content">its happening right now, its just demo quality. it&#x27;s being worked on now</div><br/></div></div></div></div></div></div></div></div><div id="42490503" class="c"><input type="checkbox" id="c-42490503" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490142">parent</a><span>|</span><a href="#42490398">prev</a><span>|</span><a href="#42490276">next</a><span>|</span><label class="collapse" for="c-42490503">[-]</label><label class="expand" for="c-42490503">[3 more]</label></div><br/><div class="children"><div class="content">Says who? And more importantly, is this <i>the</i> boulder? All I (and many others here) see is that people engage others to sponsor pushing <i>some</i> boulder, screaming promises which aren’t even that consistent with intermediate results that come out. This particular boulder may be on a wrong mountain, and likely is.<p>It all feels like doubling down on astrology because good telescopes aren’t there yet. I’m pretty sure that when 5 comes out, it will show some amazing benchmarks but shit itself in the third paragraph as usual in a real task. Cause that was constant throughtout gpt evolution, in my experience.<p><i>even if it kills us</i><p>Full-on sci-fi, in reality it will get stuck around a shell error message and either run out of money to exist or corrupt the system into no connectivity.</div><br/><div id="42491088" class="c"><input type="checkbox" id="c-42491088" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490503">parent</a><span>|</span><a href="#42490276">next</a><span>|</span><label class="collapse" for="c-42491088">[-]</label><label class="expand" for="c-42491088">[2 more]</label></div><br/><div class="children"><div class="content">The buzzkill when you fire up the latest most powerful model only for it to tell you that peanut is not typically found in peanut butter and jelly sandwiches.</div><br/><div id="42491459" class="c"><input type="checkbox" id="c-42491459" checked=""/><div class="controls bullet"><span class="by">singpolyma3</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491088">parent</a><span>|</span><a href="#42490276">next</a><span>|</span><label class="collapse" for="c-42491459">[-]</label><label class="expand" for="c-42491459">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think providing accurate answers to context free questions is even something anyone is seriously working on making them do. Using them that way is just a wrong use case.</div><br/></div></div></div></div></div></div><div id="42490276" class="c"><input type="checkbox" id="c-42490276" checked=""/><div class="controls bullet"><span class="by">h0l0cube</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490142">parent</a><span>|</span><a href="#42490503">prev</a><span>|</span><a href="#42491342">next</a><span>|</span><label class="collapse" for="c-42490276">[-]</label><label class="expand" for="c-42490276">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no doubt been progress on the way to AGI, but ultimately it&#x27;s still a search problem, and one that will rely on human ingenuity at least until we solve it.  LLMs are such a vast improvement in showing intelligent-like behavior that we&#x27;ve become tantalized by it.  So now we&#x27;re possibly focusing our search in the wrong place for the next innovation on the path to AGI.  Otherwise, it&#x27;s just a lack of compute, and then we just have to wait for the capacity to catch up.</div><br/></div></div><div id="42491342" class="c"><input type="checkbox" id="c-42491342" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490142">parent</a><span>|</span><a href="#42490276">prev</a><span>|</span><a href="#42491111">next</a><span>|</span><label class="collapse" for="c-42491342">[-]</label><label class="expand" for="c-42491342">[1 more]</label></div><br/><div class="children"><div class="content">Why do we have to?</div><br/></div></div><div id="42491111" class="c"><input type="checkbox" id="c-42491111" checked=""/><div class="controls bullet"><span class="by">madeofpalk</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490142">parent</a><span>|</span><a href="#42491342">prev</a><span>|</span><a href="#42490278">next</a><span>|</span><label class="collapse" for="c-42491111">[-]</label><label class="expand" for="c-42491111">[2 more]</label></div><br/><div class="children"><div class="content">What has AGI got to do with this?</div><br/><div id="42491245" class="c"><input type="checkbox" id="c-42491245" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491111">parent</a><span>|</span><a href="#42490278">next</a><span>|</span><label class="collapse" for="c-42491245">[-]</label><label class="expand" for="c-42491245">[1 more]</label></div><br/><div class="children"><div class="content">Part of the ideas pushed into the narrative by Marketing departments &#x2F; consultants &#x2F; hyperscalers  to movilize growth in the AI ecosystem.</div><br/></div></div></div></div><div id="42490278" class="c"><input type="checkbox" id="c-42490278" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490142">parent</a><span>|</span><a href="#42491111">prev</a><span>|</span><a href="#42492145">next</a><span>|</span><label class="collapse" for="c-42490278">[-]</label><label class="expand" for="c-42490278">[18 more]</label></div><br/><div class="children"><div class="content">And when we get it there, it kills us.</div><br/></div></div><div id="42490932" class="c"><input type="checkbox" id="c-42490932" checked=""/><div class="controls bullet"><span class="by">ulfw</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490142">parent</a><span>|</span><a href="#42491094">prev</a><span>|</span><a href="#42492221">next</a><span>|</span><label class="collapse" for="c-42490932">[-]</label><label class="expand" for="c-42490932">[2 more]</label></div><br/><div class="children"><div class="content">Why? Nobody asked us if we want this.
Nobody has a plan what to do with humanity when there is AGI</div><br/><div id="42491365" class="c"><input type="checkbox" id="c-42491365" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490932">parent</a><span>|</span><a href="#42492221">next</a><span>|</span><label class="collapse" for="c-42491365">[-]</label><label class="expand" for="c-42491365">[1 more]</label></div><br/><div class="children"><div class="content">The plan is to not pay human workers. Never mind what happens to the economy or political landscape.</div><br/></div></div></div></div></div></div><div id="42492221" class="c"><input type="checkbox" id="c-42492221" checked=""/><div class="controls bullet"><span class="by">merizian</span><span>|</span><a href="#42490004">parent</a><span>|</span><a href="#42490142">prev</a><span>|</span><a href="#42490220">next</a><span>|</span><label class="collapse" for="c-42492221">[-]</label><label class="expand" for="c-42492221">[1 more]</label></div><br/><div class="children"><div class="content">Because of mup [0] and scaling laws, you can test ideas empirically on smaller models, with some confidence they will transfer to the larger model.<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.03466" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.03466</a></div><br/></div></div><div id="42490220" class="c"><input type="checkbox" id="c-42490220" checked=""/><div class="controls bullet"><span class="by">fny</span><span>|</span><a href="#42490004">parent</a><span>|</span><a href="#42492221">prev</a><span>|</span><a href="#42490852">next</a><span>|</span><label class="collapse" for="c-42490220">[-]</label><label class="expand" for="c-42490220">[3 more]</label></div><br/><div class="children"><div class="content">O3 is not a smaller model. It&#x27;s an iterative GPT of sorts with the magic dust of reinforcement learning.</div><br/><div id="42490372" class="c"><input type="checkbox" id="c-42490372" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490220">parent</a><span>|</span><a href="#42490399">next</a><span>|</span><label class="collapse" for="c-42490372">[-]</label><label class="expand" for="c-42490372">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure that the parent implied that o3 is smaller in comparison to gpt5</div><br/></div></div></div></div><div id="42490852" class="c"><input type="checkbox" id="c-42490852" checked=""/><div class="controls bullet"><span class="by">bloodyplonker22</span><span>|</span><a href="#42490004">parent</a><span>|</span><a href="#42490220">prev</a><span>|</span><a href="#42491766">next</a><span>|</span><label class="collapse" for="c-42490852">[-]</label><label class="expand" for="c-42490852">[3 more]</label></div><br/><div class="children"><div class="content">I am working at an AI company that is not OpenAI. We have found ways to modularize training so we can test on narrower sets before training is &quot;completely done&quot;. That said, I am sure there are plenty of ways others are innovating to solve the long training time problem.</div><br/><div id="42491041" class="c"><input type="checkbox" id="c-42491041" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490852">parent</a><span>|</span><a href="#42491054">next</a><span>|</span><label class="collapse" for="c-42491041">[-]</label><label class="expand" for="c-42491041">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps the real issue is that learning takes time and that there may not be a shortcut.  I&#x27;ll grant you that argument&#x27;s analogue was complete wank when comparing say the horse and cart to a modern car.<p>However, we are not comparing cars to horses but computers to a human.<p>I do want &quot;AI&quot; to work.  I am not a luddite.  The current efforts that I&#x27;ve tried are not very good.  On the surface they offer a lot but very quickly the lustre comes off very quickly.<p>(1) How often do you find yourself arguing with someone about a &quot;fact&quot;?  Your fact may be fiction for someone else.<p>(2) LLMs cannot reason<p>A next token guesser does not think.  I wish you all the best.  Rome was not burned down within a day!<p>I can sit down with you and discuss ideas about what constitutes truth and cobblers (rubbish&#x2F;false).  I have indicated via parenthesis (brackets in en_GB) another way to describe something and you will probably get that but I doubt that your programme will.</div><br/></div></div><div id="42491054" class="c"><input type="checkbox" id="c-42491054" checked=""/><div class="controls bullet"><span class="by">icpmacdo</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490852">parent</a><span>|</span><a href="#42491041">prev</a><span>|</span><a href="#42491766">next</a><span>|</span><label class="collapse" for="c-42491054">[-]</label><label class="expand" for="c-42491054">[1 more]</label></div><br/><div class="children"><div class="content">This is literally just the scaling laws, &quot;Scaling laws predict the loss of a target machine learning model by extrapolating from easier-to-train models with fewer parameters or smaller training sets. This provides an efficient way for practitioners and researchers alike to compare pretraining decisions involving optimizers, datasets, and model architectures&quot;<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2410.11840v1#:~:text=Scaling%20laws%20predict%20the%20loss,%2C%20datasets%2C%20and%20model%20architectures" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2410.11840v1#:~:text=Scaling%20laws%2...</a>.</div><br/></div></div></div></div><div id="42491766" class="c"><input type="checkbox" id="c-42491766" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42490004">parent</a><span>|</span><a href="#42490852">prev</a><span>|</span><a href="#42490523">next</a><span>|</span><label class="collapse" for="c-42491766">[-]</label><label class="expand" for="c-42491766">[1 more]</label></div><br/><div class="children"><div class="content">&gt;the time it takes it to learn what works&#x2F;doesn&#x27;t work widens.<p>From the raw scaling laws we already knew that a new base model may peter out in this run or the next with some amount of uncertainty--&quot;the intersection point is sensitive to the precise power-law parameters&quot;:<p><a href="https:&#x2F;&#x2F;gwern.net&#x2F;doc&#x2F;ai&#x2F;nn&#x2F;transformer&#x2F;gpt&#x2F;2020-kaplan-figure15-projectingscaling.png" rel="nofollow">https:&#x2F;&#x2F;gwern.net&#x2F;doc&#x2F;ai&#x2F;nn&#x2F;transformer&#x2F;gpt&#x2F;2020-kaplan-figu...</a><p>Later graph gpt-3 got to here:<p><a href="https:&#x2F;&#x2F;gwern.net&#x2F;doc&#x2F;ai&#x2F;nn&#x2F;transformer&#x2F;gpt&#x2F;2020-brown-figure31-gpt3scaling.png" rel="nofollow">https:&#x2F;&#x2F;gwern.net&#x2F;doc&#x2F;ai&#x2F;nn&#x2F;transformer&#x2F;gpt&#x2F;2020-brown-figur...</a><p><a href="https:&#x2F;&#x2F;gwern.net&#x2F;scaling-hypothesis" rel="nofollow">https:&#x2F;&#x2F;gwern.net&#x2F;scaling-hypothesis</a></div><br/></div></div><div id="42490523" class="c"><input type="checkbox" id="c-42490523" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42490004">parent</a><span>|</span><a href="#42491766">prev</a><span>|</span><a href="#42490073">next</a><span>|</span><label class="collapse" for="c-42490523">[-]</label><label class="expand" for="c-42490523">[1 more]</label></div><br/><div class="children"><div class="content">Until you get to a point where the LLM is smart enough to look at real world data streams and prune its own training set out of it. At that point it will self improve itself to AGI.</div><br/></div></div><div id="42490073" class="c"><input type="checkbox" id="c-42490073" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#42490004">parent</a><span>|</span><a href="#42490523">prev</a><span>|</span><a href="#42492641">next</a><span>|</span><label class="collapse" for="c-42490073">[-]</label><label class="expand" for="c-42490073">[41 more]</label></div><br/><div class="children"><div class="content">But if the scaling law holds true, more dollars should at some point translate into AGI, which is priceless. We haven&#x27;t reached the limits yet of that hypothesis.</div><br/><div id="42490446" class="c"><input type="checkbox" id="c-42490446" checked=""/><div class="controls bullet"><span class="by">unshavedyak</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490073">parent</a><span>|</span><a href="#42490149">next</a><span>|</span><label class="collapse" for="c-42490446">[-]</label><label class="expand" for="c-42490446">[3 more]</label></div><br/><div class="children"><div class="content">&gt; which is priceless<p>This also isn&#x27;t true. It&#x27;ll clearly have a price to run. Even if it&#x27;s very intelligent, if the price to run it is too high it&#x27;ll just be a 24&#x2F;7 intelligent person that few can afford to talk to. No?</div><br/><div id="42490529" class="c"><input type="checkbox" id="c-42490529" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490446">parent</a><span>|</span><a href="#42490149">next</a><span>|</span><label class="collapse" for="c-42490529">[-]</label><label class="expand" for="c-42490529">[2 more]</label></div><br/><div class="children"><div class="content">Computers will be the size of data centres, they&#x27;ll be so expensive we&#x27;ll queue up jobs to run on them days in advance, each taking our turn... history echoes into the future...</div><br/><div id="42490597" class="c"><input type="checkbox" id="c-42490597" checked=""/><div class="controls bullet"><span class="by">unshavedyak</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490529">parent</a><span>|</span><a href="#42490149">next</a><span>|</span><label class="collapse" for="c-42490597">[-]</label><label class="expand" for="c-42490597">[1 more]</label></div><br/><div class="children"><div class="content">Yea, and those statements were true. For a time. If you want to say &quot;AGI will be priceless some unknown time into the future&quot; then i&#x27;d be on board lol. But to imply it&#x27;ll be immediately priceless? As in no cost spent today wouldn&#x27;t be immediately rewarded once AGI exists? Nonsense.<p>Maybe if it was _extremely_ intelligent and it&#x27;s ROI would be all the drugs it would instantly discover or w&#x2F;e. But lets not imply that General Intelligence requires infinitely knowing.<p>So at best we&#x27;re talking about an AI that is likely close to human level intelligence. Which is cool, because we have 7+ billion of those things.<p>This isn&#x27;t an argument against it. Just to say that AGI isn&#x27;t &quot;priceless&quot; in the implementation we&#x27;d likely see out of the gate.</div><br/></div></div></div></div></div></div><div id="42490149" class="c"><input type="checkbox" id="c-42490149" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490073">parent</a><span>|</span><a href="#42490446">prev</a><span>|</span><a href="#42492641">next</a><span>|</span><label class="collapse" for="c-42490149">[-]</label><label class="expand" for="c-42490149">[37 more]</label></div><br/><div class="children"><div class="content">a) There is evidence e.g. private data deals that we are starting to hit the limitations of what data is available.<p>b) There is no evidence that LLMs are the roadmap to AGI.<p>c) Continued investment hinges on their being a large enough cohort of startups that can leverage LLMs to generate outsized returns. There is no evidence yet this is the case.</div><br/><div id="42491580" class="c"><input type="checkbox" id="c-42491580" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490149">parent</a><span>|</span><a href="#42490407">next</a><span>|</span><label class="collapse" for="c-42491580">[-]</label><label class="expand" for="c-42491580">[1 more]</label></div><br/><div class="children"><div class="content">&gt; c) Continued investment hinges on their being a large enough cohort of startups that can leverage LLMs to generate outsized returns. There is no evidence yet this is the case.<p>Why does it have to be startups?  And why does it have to be LLMs?<p>Btw, we might be running out of text data.  But there&#x27;s lots and lots more data you can have (and generate), if you are willing to consider other modalities.<p>You can also get a bit further with text data by using it for multiple epochs, like we used to do in the past.  (But that only really gives you at best an order of magnitude.  I read some paper that the returns diminish drastically after four epochs.)</div><br/></div></div><div id="42490407" class="c"><input type="checkbox" id="c-42490407" checked=""/><div class="controls bullet"><span class="by">thrwthsnw</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490149">parent</a><span>|</span><a href="#42491580">prev</a><span>|</span><a href="#42490245">next</a><span>|</span><label class="collapse" for="c-42490407">[-]</label><label class="expand" for="c-42490407">[1 more]</label></div><br/><div class="children"><div class="content">Private data is 90% garbage too</div><br/></div></div><div id="42490245" class="c"><input type="checkbox" id="c-42490245" checked=""/><div class="controls bullet"><span class="by">ComplexSystems</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490149">parent</a><span>|</span><a href="#42490407">prev</a><span>|</span><a href="#42490382">next</a><span>|</span><label class="collapse" for="c-42490245">[-]</label><label class="expand" for="c-42490245">[29 more]</label></div><br/><div class="children"><div class="content">&quot;There is no evidence that LLMs are the roadmap to AGI.&quot; - There&#x27;s plenty of evidence. What do you think the last few years have been all about? Hell, GPT-4 would already have qualified as AGI about a decade ago.</div><br/><div id="42490375" class="c"><input type="checkbox" id="c-42490375" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490245">parent</a><span>|</span><a href="#42490288">next</a><span>|</span><label class="collapse" for="c-42490375">[-]</label><label class="expand" for="c-42490375">[17 more]</label></div><br/><div class="children"><div class="content">&gt;<i>What do you think the last few years have been all about?</i><p>Next token language-based predictors with no more intelligence than brute force GIGO which parrot existing human intelligence captured as text&#x2F;audio and fed in the form of input data.<p>4o agrees:<p>&quot;What you are describing is a language model or next-token predictor that operates solely as a computational system without inherent intelligence or understanding. The phrase captures the essence of generative AI models, like GPT, which rely on statistical and probabilistic methods to predict the next piece of text based on patterns in the data they’ve been trained on&quot;</div><br/><div id="42490420" class="c"><input type="checkbox" id="c-42490420" checked=""/><div class="controls bullet"><span class="by">thrwthsnw</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490375">parent</a><span>|</span><a href="#42491151">next</a><span>|</span><label class="collapse" for="c-42490420">[-]</label><label class="expand" for="c-42490420">[6 more]</label></div><br/><div class="children"><div class="content">Everything you said is parroting data you’ve trained on, two thirds of it is actual copy paste</div><br/><div id="42491275" class="c"><input type="checkbox" id="c-42491275" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490420">parent</a><span>|</span><a href="#42490477">next</a><span>|</span><label class="collapse" for="c-42491275">[-]</label><label class="expand" for="c-42491275">[4 more]</label></div><br/><div class="children"><div class="content">He probably didn&#x27;t need petabytes of reddit posts and millions of gpu-hours to parrot that though.<p>I still don&#x27;t buy the &quot;we do the same as LLMs&quot; discourse. Of course one could hypothesize the human brain language center may have some similarities to LLMs, but the differences in resource usage and how those resources are used to train humans and LLMs are remarkable and may indicate otherwise.</div><br/><div id="42491889" class="c"><input type="checkbox" id="c-42491889" checked=""/><div class="controls bullet"><span class="by">shwouchk</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491275">parent</a><span>|</span><a href="#42490477">next</a><span>|</span><label class="collapse" for="c-42491889">[-]</label><label class="expand" for="c-42491889">[3 more]</label></div><br/><div class="children"><div class="content">Not text, he had petabytes of video, audio, and other sensory inputs. Heck, a baby sees petabytes of video before first word is spoken<p>And he probably cant quote Shakespeare as well ;)</div><br/><div id="42491947" class="c"><input type="checkbox" id="c-42491947" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491889">parent</a><span>|</span><a href="#42492059">next</a><span>|</span><label class="collapse" for="c-42491947">[-]</label><label class="expand" for="c-42491947">[1 more]</label></div><br/><div class="children"><div class="content">&gt; he had petabytes of video, audio, and other sensory inputs<p>He didn&#x27;t parrot a video or sensory inputs though.</div><br/></div></div><div id="42492059" class="c"><input type="checkbox" id="c-42492059" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491889">parent</a><span>|</span><a href="#42491947">prev</a><span>|</span><a href="#42490477">next</a><span>|</span><label class="collapse" for="c-42492059">[-]</label><label class="expand" for="c-42492059">[1 more]</label></div><br/><div class="children"><div class="content">And yet with multiple OoM more data he still didn&#x27;t cost millions of dollars to be trained nor multiple lifetimes in gpu-hours. He probably didn&#x27;t even register all the petabytes passing through all his &quot;sensors&quot;, those are some characteristics that we are not even near understanding and much less replicating.<p>Whatever is happening in the brain is more complex as the perf&#x2F;cost ratio is stupidly better for humans for a lot of tasks in both training and inference*.<p>*when considering all modalities, o3 can&#x27;t even do the ARC AGI in vision mode but rather just json representations. So much for omni.</div><br/></div></div></div></div></div></div><div id="42490477" class="c"><input type="checkbox" id="c-42490477" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490420">parent</a><span>|</span><a href="#42491275">prev</a><span>|</span><a href="#42491151">next</a><span>|</span><label class="collapse" for="c-42490477">[-]</label><label class="expand" for="c-42490477">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>Everything you said is parroting data you’ve trained on</i><p>&quot;Just like&quot; an LLM, yeah sure...<p>Like how the brain was &quot;just like&quot; a hydraulic system (early industrial era), like a clockwork with gears and differentiation (mechanical engineering), &quot;just like&quot; an electric circuit (Edison&#x27;s time), &quot;just like&quot; a computer CPU (21st century), and so on...<p>You&#x27;re just assuming what you should prove</div><br/></div></div></div></div><div id="42491151" class="c"><input type="checkbox" id="c-42491151" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490375">parent</a><span>|</span><a href="#42490420">prev</a><span>|</span><a href="#42490540">next</a><span>|</span><label class="collapse" for="c-42491151">[-]</label><label class="expand" for="c-42491151">[1 more]</label></div><br/><div class="children"><div class="content">o1 points out this is mostly about “if submarines swim”.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6768c920-4454-8000-bf73-0f86e92996e4" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6768c920-4454-8000-bf73-0f86e92996...</a></div><br/></div></div><div id="42490540" class="c"><input type="checkbox" id="c-42490540" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490375">parent</a><span>|</span><a href="#42491151">prev</a><span>|</span><a href="#42490888">next</a><span>|</span><label class="collapse" for="c-42490540">[-]</label><label class="expand" for="c-42490540">[8 more]</label></div><br/><div class="children"><div class="content">You have described something but you haven&#x27;t explained why the description of the thing defines its capability. This is a tautology, or possibly a begging of the question, which takes as true the premise of something (that token based language predictors cannot be intelligent) and then uses that premise to prove an unproven point (that language models cannot achieve intelligence).<p>You did nothing at all to demonstrate why you cannot produce an intelligent system from a next token language based predictor.<p>What GPT says about this is completely irrelevant.</div><br/><div id="42490584" class="c"><input type="checkbox" id="c-42490584" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490540">parent</a><span>|</span><a href="#42490888">next</a><span>|</span><label class="collapse" for="c-42490584">[-]</label><label class="expand" for="c-42490584">[7 more]</label></div><br/><div class="children"><div class="content">&gt;<i>You did nothing at all to demonstrate why you cannot produce an intelligent system from a next token language based predictor</i><p>Sorry, but the burden of proof is on your side...<p>The intelligence is in the corpus the LLM was fed with. Using statistics to pick from it and re-arrange it gives new   intelligent results because the information was already produced by intelligent beings.<p>If somebody gives you an excerpt of a book, it doesn&#x27;t mean they have the intelligence of the author - even if you have taught them a mechanical statistical method to give back a section matching a query you make.<p>Kids learn to speak and understand language at 3-4 years old (among tons of other concepts), and can reason by themselves in a few years with less than 1 billionth the input...<p>&gt;<i>What GPT says about this is completely irrelevant.</i><p>On the contrary, it&#x27;s using its very real intelligence, about to reach singularity any time now, and this is its verdict!<p>Why would you say it&#x27;s irrelevant? That would be as if it merely statistically parroted combinations of its training data unconnected to any reasoning (except of that the human creators of the data used to create them) or objective reality...</div><br/><div id="42491278" class="c"><input type="checkbox" id="c-42491278" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490584">parent</a><span>|</span><a href="#42490710">next</a><span>|</span><label class="collapse" for="c-42491278">[-]</label><label class="expand" for="c-42491278">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If somebody gives you an excerpt of a book, it doesn&#x27;t mean they have the intelligence of the author<p>A closely related rant of my own: The fictional character we humans infer from text is not the author-machine generating that text, not even if they happen to share the same name. Assuming that the author-machine is already conscious and choosing to insert itself is begging the question.</div><br/></div></div><div id="42490710" class="c"><input type="checkbox" id="c-42490710" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490584">parent</a><span>|</span><a href="#42491278">prev</a><span>|</span><a href="#42490888">next</a><span>|</span><label class="collapse" for="c-42490710">[-]</label><label class="expand" for="c-42490710">[5 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s pretend it is 1940<p>Person 1: rockets could be a method of putting things into Earth orbit<p>Person 2: rockets cannot get things into orbit because they use a chemical reaction which causes an equal and opposite force reaction to produce thrust&#x27;<p>Does person 1 have the burden of proof that rockets can be used to put things in orbit? Sure, but that doesn&#x27;t make the reasoning used by person 2 valid to explain why person 1 is wrong.<p>BTW thanks for adding an entire chapter to your comment in edit so it looks like I am ignoring most of it. What I replied to was one sentence that said &#x27;the burden of proof is on you&#x27;. Though it really doesn&#x27;t make much difference because you are doing the same thing but more verbose this time.<p>None of the things you mentioned preclude intelligence. You are telling us again how it operates but not why that operation is restrictive in producing an intelligent output. There is no law that saws that intelligence requires anything but a large amount of data and computation. If you can show why these things are not sufficient, I am eager to read about it. A logical explanation would be great, step by step please, without making any grand unproven assumptions.<p>In response to the person below... again, whether or not person 1 is right or wrong does not make person 2&#x27;s argument valid.</div><br/><div id="42490789" class="c"><input type="checkbox" id="c-42490789" checked=""/><div class="controls bullet"><span class="by">ViewTrick1002</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490710">parent</a><span>|</span><a href="#42491956">next</a><span>|</span><label class="collapse" for="c-42490789">[-]</label><label class="expand" for="c-42490789">[2 more]</label></div><br/><div class="children"><div class="content">The delta-V for orbit is a precisely defined point. How you get there is not.<p>What is the defined point for reaching AGI?</div><br/><div id="42492073" class="c"><input type="checkbox" id="c-42492073" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490789">parent</a><span>|</span><a href="#42491956">next</a><span>|</span><label class="collapse" for="c-42492073">[-]</label><label class="expand" for="c-42492073">[1 more]</label></div><br/><div class="children"><div class="content">I can check but I am pretty sure that using a different argument to try and prove something is wrong will not make another person&#x27;s invalid argument correct.</div><br/></div></div></div></div><div id="42491956" class="c"><input type="checkbox" id="c-42491956" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490710">parent</a><span>|</span><a href="#42490789">prev</a><span>|</span><a href="#42490888">next</a><span>|</span><label class="collapse" for="c-42491956">[-]</label><label class="expand" for="c-42491956">[2 more]</label></div><br/><div class="children"><div class="content">Person 3: Since we can leave earths orbit, we can reach faster than light speed, look at this graph over our progress making faster rockets we will for sure reach there in a few years!</div><br/><div id="42492052" class="c"><input type="checkbox" id="c-42492052" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491956">parent</a><span>|</span><a href="#42490888">next</a><span>|</span><label class="collapse" for="c-42492052">[-]</label><label class="expand" for="c-42492052">[1 more]</label></div><br/><div class="children"><div class="content">So there is a theoretical framework which can be tested against to achieve AGI and according to that framework it is either not possible or extremely unlikely because of physical laws?<p>Can you share that? It sounds groundbreaking!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42490888" class="c"><input type="checkbox" id="c-42490888" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490375">parent</a><span>|</span><a href="#42490540">prev</a><span>|</span><a href="#42490288">next</a><span>|</span><label class="collapse" for="c-42490888">[-]</label><label class="expand" for="c-42490888">[1 more]</label></div><br/><div class="children"><div class="content">This comment isn&#x27;t false but it&#x27;s very naive.</div><br/></div></div></div></div><div id="42490288" class="c"><input type="checkbox" id="c-42490288" checked=""/><div class="controls bullet"><span class="by">gwervc</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490245">parent</a><span>|</span><a href="#42490375">prev</a><span>|</span><a href="#42490416">next</a><span>|</span><label class="collapse" for="c-42490288">[-]</label><label class="expand" for="c-42490288">[2 more]</label></div><br/><div class="children"><div class="content">No, GPT-4 would have been classified as it is today: a (good) generator of natural language.  While this is a hard classical NLP task, it&#x27;s a far cry from intelligence.</div><br/><div id="42490394" class="c"><input type="checkbox" id="c-42490394" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490288">parent</a><span>|</span><a href="#42490416">next</a><span>|</span><label class="collapse" for="c-42490394">[-]</label><label class="expand" for="c-42490394">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 is a good generator of natural language in the same sense that Google is a good generator of ip packets.</div><br/></div></div></div></div><div id="42490416" class="c"><input type="checkbox" id="c-42490416" checked=""/><div class="controls bullet"><span class="by">n144q</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490245">parent</a><span>|</span><a href="#42490288">prev</a><span>|</span><a href="#42490293">next</a><span>|</span><label class="collapse" for="c-42490416">[-]</label><label class="expand" for="c-42490416">[4 more]</label></div><br/><div class="children"><div class="content">&gt; GPT-4 would already have qualified as AGI about a decade ago.<p>Did you just make that up?</div><br/><div id="42491572" class="c"><input type="checkbox" id="c-42491572" checked=""/><div class="controls bullet"><span class="by">wat10000</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490416">parent</a><span>|</span><a href="#42490699">next</a><span>|</span><label class="collapse" for="c-42491572">[-]</label><label class="expand" for="c-42491572">[2 more]</label></div><br/><div class="children"><div class="content">A lot of people held that passing the Turing Test would indicate human-level intelligence. GPT-4 passes.</div><br/><div id="42492611" class="c"><input type="checkbox" id="c-42492611" checked=""/><div class="controls bullet"><span class="by">bigpingo</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491572">parent</a><span>|</span><a href="#42490699">next</a><span>|</span><label class="collapse" for="c-42492611">[-]</label><label class="expand" for="c-42492611">[1 more]</label></div><br/><div class="children"><div class="content">Link to GPT-4 passing the turing test?
Tried googling, could not find anything.</div><br/></div></div></div></div><div id="42490699" class="c"><input type="checkbox" id="c-42490699" checked=""/><div class="controls bullet"><span class="by">OtomotO</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490416">parent</a><span>|</span><a href="#42491572">prev</a><span>|</span><a href="#42490293">next</a><span>|</span><label class="collapse" for="c-42490699">[-]</label><label class="expand" for="c-42490699">[1 more]</label></div><br/><div class="children"><div class="content">Probably asked an &quot;AI&quot;</div><br/></div></div></div></div><div id="42490293" class="c"><input type="checkbox" id="c-42490293" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490245">parent</a><span>|</span><a href="#42490416">prev</a><span>|</span><a href="#42490684">next</a><span>|</span><label class="collapse" for="c-42490293">[-]</label><label class="expand" for="c-42490293">[4 more]</label></div><br/><div class="children"><div class="content">Have you ever heard of a local maxima? You don&#x27;t get an attack helicopter by breeding stronger and stronger falcons.</div><br/><div id="42490473" class="c"><input type="checkbox" id="c-42490473" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490293">parent</a><span>|</span><a href="#42490684">next</a><span>|</span><label class="collapse" for="c-42490473">[-]</label><label class="expand" for="c-42490473">[3 more]</label></div><br/><div class="children"><div class="content">For an industry that spun off of a research field that basically revolves around recursive descent in one form or another, there&#x27;s a pretty silly amount of willful ignorance about the basic principles of how learning and progress happens.<p>The default assumption <i>should</i> be that this is a local maximum, with evidence required to demonstrate that it&#x27;s not. But the hype artists want us all to take the inevitability of LLMs for granted—&quot;See the slope? Slopes lead up! All we have to do is climb the slope and we&#x27;ll get to the moon! If you can&#x27;t see that you&#x27;re obviously stupid or have your head in the sand!&quot;</div><br/><div id="42491269" class="c"><input type="checkbox" id="c-42491269" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490473">parent</a><span>|</span><a href="#42490684">next</a><span>|</span><label class="collapse" for="c-42491269">[-]</label><label class="expand" for="c-42491269">[2 more]</label></div><br/><div class="children"><div class="content">You’re implicitly assuming only a global maximum will lead to useful AI.<p>There might be many local maxima that cross the useful AI or even AGI threshold.</div><br/><div id="42491586" class="c"><input type="checkbox" id="c-42491586" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42491269">parent</a><span>|</span><a href="#42490684">next</a><span>|</span><label class="collapse" for="c-42491586">[-]</label><label class="expand" for="c-42491586">[1 more]</label></div><br/><div class="children"><div class="content">And we aren&#x27;t even at a local maximum.  There&#x27;s still plenty of incremental upwards progress to be made.</div><br/></div></div></div></div></div></div></div></div><div id="42490684" class="c"><input type="checkbox" id="c-42490684" checked=""/><div class="controls bullet"><span class="by">OtomotO</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490245">parent</a><span>|</span><a href="#42490293">prev</a><span>|</span><a href="#42490382">next</a><span>|</span><label class="collapse" for="c-42490684">[-]</label><label class="expand" for="c-42490684">[1 more]</label></div><br/><div class="children"><div class="content">The last four years?<p>ELIZA 2.0</div><br/></div></div></div></div><div id="42490382" class="c"><input type="checkbox" id="c-42490382" checked=""/><div class="controls bullet"><span class="by">aantix</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490149">parent</a><span>|</span><a href="#42490245">prev</a><span>|</span><a href="#42492641">next</a><span>|</span><label class="collapse" for="c-42490382">[-]</label><label class="expand" for="c-42490382">[5 more]</label></div><br/><div class="children"><div class="content">Have we really hit the wall?<p>Do they use GPS based data?<p>Feels like there’s data all around us.<p>Sure they’ve hit the wall with obvious conversations and blog articles that humans produced, but data is a by product of our environment. Surely there’s more. Tons more.</div><br/><div id="42490405" class="c"><input type="checkbox" id="c-42490405" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490382">parent</a><span>|</span><a href="#42492641">next</a><span>|</span><label class="collapse" for="c-42490405">[-]</label><label class="expand" for="c-42490405">[4 more]</label></div><br/><div class="children"><div class="content">We also could just measure the background noise of the universe and produce unlimited data.<p>But just like GPS data it isn&#x27;t suited for LLMs given that you know it has no relevance what so ever to language.</div><br/><div id="42491613" class="c"><input type="checkbox" id="c-42491613" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490405">parent</a><span>|</span><a href="#42490475">next</a><span>|</span><label class="collapse" for="c-42491613">[-]</label><label class="expand" for="c-42491613">[1 more]</label></div><br/><div class="children"><div class="content">Ignoring the confusion about &#x27;GPS&#x27; for a moment: there&#x27;s lots and lots of other data that could be used for training AI systems.<p>But, you need to go multi-modal for that; and you need to find data that&#x27;s somewhat useful, not just random fluctuations like the CMB.  So eg you could use YouTube videos, or even just point webcams at the real world.  That might be able to give your AI a grounding in everyday physics?<p>There&#x27;s also lots of program code you can train your AI on.  Not so much the code itself, because compared to the world&#x27;s total text (that we are running out of), the world&#x27;s total human written code is relatively small.<p>But you can generate new code and make it useful for training, by also having the AI predict what happens when you (compile and) run the code.  A bit like self-playing for improving AlphaGo.</div><br/></div></div><div id="42490475" class="c"><input type="checkbox" id="c-42490475" checked=""/><div class="controls bullet"><span class="by">aantix</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490405">parent</a><span>|</span><a href="#42491613">prev</a><span>|</span><a href="#42492641">next</a><span>|</span><label class="collapse" for="c-42490475">[-]</label><label class="expand" for="c-42490475">[2 more]</label></div><br/><div class="children"><div class="content">You’re thinking of language in the strictest of sense.<p>GPS data as it relates to location names, people, cultures, path finding.</div><br/><div id="42491593" class="c"><input type="checkbox" id="c-42491593" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42490004">root</a><span>|</span><a href="#42490475">parent</a><span>|</span><a href="#42492641">next</a><span>|</span><label class="collapse" for="c-42491593">[-]</label><label class="expand" for="c-42491593">[1 more]</label></div><br/><div class="children"><div class="content">What does culture and names and people have to do with the Global Position System?<p>You are right that we can have lots more data, if you are willing to consider other modalities.  But that&#x27;s not &#x27;GPS&#x27;.  Unless you are using an idiosyncratic definition of GPS?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42492641" class="c"><input type="checkbox" id="c-42492641" checked=""/><div class="controls bullet"><span class="by">rchaves</span><span>|</span><a href="#42490004">prev</a><span>|</span><a href="#42489580">next</a><span>|</span><label class="collapse" for="c-42492641">[-]</label><label class="expand" for="c-42492641">[2 more]</label></div><br/><div class="children"><div class="content">Nah it&#x27;s just a marketing problem, &quot;GPT&quot; and &quot;ChatGPT&quot; names is the biggest asset OpenAI has, people have expectations so high for GPT-5 that they cannot burn this name unless it&#x27;s something truly majestic, bordering AGI at the very least. Until they are confident enough that people will be blown off by it, it&#x27;s better to continue building up the hype</div><br/><div id="42492840" class="c"><input type="checkbox" id="c-42492840" checked=""/><div class="controls bullet"><span class="by">OtherShrezzing</span><span>|</span><a href="#42492641">parent</a><span>|</span><a href="#42489580">next</a><span>|</span><label class="collapse" for="c-42492840">[-]</label><label class="expand" for="c-42492840">[1 more]</label></div><br/><div class="children"><div class="content">The Half Life 3 of the SaaS&#x2F;zirp era.</div><br/></div></div></div></div><div id="42489580" class="c"><input type="checkbox" id="c-42489580" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42492641">prev</a><span>|</span><a href="#42486273">next</a><span>|</span><label class="collapse" for="c-42489580">[-]</label><label class="expand" for="c-42489580">[63 more]</label></div><br/><div class="children"><div class="content"><i>&quot;Orion’s problems signaled to some at OpenAI that the more-is-more strategy, which had driven much of its earlier success, was running out of steam.&quot;</i><p>So LLMs finally hit the wall. For a long time, more data, bigger models, and more compute to drive them worked. But that&#x27;s apparently not enough any more.<p>Now someone has to have a new idea. There&#x27;s plenty of money available if someone has one.<p>The current level of LLM would be far more useful if someone could get a conservative confidence metric out of the internals of the model. This technology desperately needs to output &quot;Don&#x27;t know&quot; or &quot;Not sure about this, but ...&quot; when appropriate.</div><br/><div id="42492783" class="c"><input type="checkbox" id="c-42492783" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42489683">next</a><span>|</span><label class="collapse" for="c-42492783">[-]</label><label class="expand" for="c-42492783">[1 more]</label></div><br/><div class="children"><div class="content">The problem is data.<p>GPT-3 was trained on 4:1 ratio of data to parameters. And for GPT-4 the ratio was 10:1. So to scale this out, GPT-5 should be 25:1. The parameter count jumped from 175B to 1.3T, which means GPT-5 should be 10T parameters and 250T training tokens. There is zero chance OpenAI has a training set of high quality data that is 250T tokens.<p>If I had to guess, they trained a model that was maybe 3-4T in size and used 30-50T high quality tokens and maybe 10-30 medium and low quality ones.<p>There is only one company in the world that stores the data that could get us past the wall.<p>The training cost of the above scaled GPT-5 is 150x GPT-4, which was 25k A100 for 90 days, which poor MFU.<p>Let’s assume they double MFU, it would mean 1M H100s. But let’s say they made algorithmic improvements, so maybe it’s only 250-500k H100s.<p>While the training cluster size was 100k and then grew to 150k, this cluster is suggestive of a smaller model and less data.<p>But ultimately data is the bottleneck.</div><br/></div></div><div id="42489683" class="c"><input type="checkbox" id="c-42489683" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42492783">prev</a><span>|</span><a href="#42490139">next</a><span>|</span><label class="collapse" for="c-42489683">[-]</label><label class="expand" for="c-42489683">[14 more]</label></div><br/><div class="children"><div class="content">The new idea is inference-time scaling, as seen in o1 (and o3 and Qwen&#x27;s QwQ and DeepSeek&#x27;s DeepSeek-R1-Lite-Preview and Google&#x27;s gemini-2.0-flash-thinking-exp).<p>I suggest reading these two pieces about that:<p>- <a href="https:&#x2F;&#x2F;www.aisnakeoil.com&#x2F;p&#x2F;is-ai-progress-slowing-down" rel="nofollow">https:&#x2F;&#x2F;www.aisnakeoil.com&#x2F;p&#x2F;is-ai-progress-slowing-down</a> - best explanation I&#x27;ve seen of inference scaling anywhere<p>- <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;oai-o3-pub-breakthrough" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;oai-o3-pub-breakthrough</a> - François Chollet&#x27;s deep dive into o3<p>I&#x27;ve been tracking it on this tag on my blog: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;tags&#x2F;inference-scaling&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;tags&#x2F;inference-scaling&#x2F;</a></div><br/><div id="42489805" class="c"><input type="checkbox" id="c-42489805" checked=""/><div class="controls bullet"><span class="by">exhaze</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489683">parent</a><span>|</span><a href="#42490139">next</a><span>|</span><label class="collapse" for="c-42489805">[-]</label><label class="expand" for="c-42489805">[13 more]</label></div><br/><div class="children"><div class="content">I think the wildest thing is actually Meta’s latest paper where they show a method for LLMs reasoning not in English, but in <i>latent space</i><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2412.06769" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2412.06769</a><p>I’ve done research myself adjacent to this (mapping parts of a latent space onto a manifold), but this is a bit eerie, even to me.</div><br/><div id="42492862" class="c"><input type="checkbox" id="c-42492862" checked=""/><div class="controls bullet"><span class="by">rhubarbtree</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489805">parent</a><span>|</span><a href="#42489878">next</a><span>|</span><label class="collapse" for="c-42492862">[-]</label><label class="expand" for="c-42492862">[1 more]</label></div><br/><div class="children"><div class="content">Seems a standard approach of AI research is to “move X into the latent space” where X is some useful function (eg diffusion) previously done in the “data” or “artefact” space. So seems very pedestrian not wild to make that step.</div><br/></div></div><div id="42489878" class="c"><input type="checkbox" id="c-42489878" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489805">parent</a><span>|</span><a href="#42492862">prev</a><span>|</span><a href="#42490065">next</a><span>|</span><label class="collapse" for="c-42489878">[-]</label><label class="expand" for="c-42489878">[7 more]</label></div><br/><div class="children"><div class="content">Is it &quot;eerie&quot;? LeCun has been talking about it for some time, and may also be OpenAI&#x27;s rumored q-star, mentioned shortly after Noam Brown (diplomacybot) joining OpenAI. You can&#x27;t hill climb tokens, but you can climb manifolds.</div><br/><div id="42490469" class="c"><input type="checkbox" id="c-42490469" checked=""/><div class="controls bullet"><span class="by">exhaze</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489878">parent</a><span>|</span><a href="#42490079">next</a><span>|</span><label class="collapse" for="c-42490469">[-]</label><label class="expand" for="c-42490469">[3 more]</label></div><br/><div class="children"><div class="content">I wasn’t aware of others attempting manifolds for this before - just something I stumbled upon independently. To me the “eerie” part is the thought of an LLM no longer using human language to reason - it’s like something out of a sci fi movie where humans encounter an alien species that thinks in a way that humans cannot even comprehend due to biological limitations.<p>I am hopeful that progress in mechanistic interpretability will serve as a healthy counterbalance to this approach when it comes to explainability.. though I kinda worry that at a certain point it may be that something resembling a scaling law puts an upper bound on even that.</div><br/><div id="42491938" class="c"><input type="checkbox" id="c-42491938" checked=""/><div class="controls bullet"><span class="by">joegibbs</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490469">parent</a><span>|</span><a href="#42491546">next</a><span>|</span><label class="collapse" for="c-42491938">[-]</label><label class="expand" for="c-42491938">[1 more]</label></div><br/><div class="children"><div class="content">Is it really alien or is it more similar to how we think? We don&#x27;t think purely in language, it&#x27;s more a kind of soup of language, sounds, images, emotions and senses that we then turn into language when we communicate with each other.</div><br/></div></div><div id="42491546" class="c"><input type="checkbox" id="c-42491546" checked=""/><div class="controls bullet"><span class="by">sooheon</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490469">parent</a><span>|</span><a href="#42491938">prev</a><span>|</span><a href="#42490079">next</a><span>|</span><label class="collapse" for="c-42491546">[-]</label><label class="expand" for="c-42491546">[1 more]</label></div><br/><div class="children"><div class="content">I remember (apocryphal?) Microsoft&#x27;s chatbot developing pidgin to communicate to other chatbots. Every layer of the NN except the first and last already &quot;think&quot; in latent space, is this surprising?</div><br/></div></div></div></div><div id="42490079" class="c"><input type="checkbox" id="c-42490079" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489878">parent</a><span>|</span><a href="#42490469">prev</a><span>|</span><a href="#42490065">next</a><span>|</span><label class="collapse" for="c-42490079">[-]</label><label class="expand" for="c-42490079">[3 more]</label></div><br/><div class="children"><div class="content">&gt; You can&#x27;t hill climb tokens, but you can climb manifolds.<p>Could you explain this a bit please?</div><br/><div id="42490153" class="c"><input type="checkbox" id="c-42490153" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490079">parent</a><span>|</span><a href="#42490196">next</a><span>|</span><label class="collapse" for="c-42490153">[-]</label><label class="expand" for="c-42490153">[1 more]</label></div><br/><div class="children"><div class="content">I imagine he means that when you reason in latent space the final answer is a smooth function of the parameters, which means you can use gradient descent to directly optimize the model to produce a desired final output without knowing the correct reasoning steps to get there.<p>When you reason in token space (like everyone is doing now) you are executing nonlinear functions when you sample after each token, so you have to use some kind of reinforcement learning algorithm to learn the weights.</div><br/></div></div><div id="42490196" class="c"><input type="checkbox" id="c-42490196" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490079">parent</a><span>|</span><a href="#42490153">prev</a><span>|</span><a href="#42490065">next</a><span>|</span><label class="collapse" for="c-42490196">[-]</label><label class="expand" for="c-42490196">[1 more]</label></div><br/><div class="children"><div class="content">Links to Yan:<p><i>Title: &quot;Objective Driven AI: Towards Machines that can Learn, Reason, and Plan&quot;<p>Lytle Lecture Page: <a href="https:&#x2F;&#x2F;ece.uw.edu&#x2F;news-events&#x2F;lytle-lecture-series&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ece.uw.edu&#x2F;news-events&#x2F;lytle-lecture-series&#x2F;</a><p>Slides: <a href="https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1e6EtQPQMCreP3pwi5E9kKRsVs2NbWPrY&#x2F;view?usp=drivesdk" rel="nofollow">https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1e6EtQPQMCreP3pwi5E9kKRsVs2N...</a><p>Video: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;d_bdU3LsLzE?si=UeLf0MhMzjXcSCAb" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;d_bdU3LsLzE?si=UeLf0MhMzjXcSCAb</a>
</i></div><br/></div></div></div></div></div></div><div id="42490065" class="c"><input type="checkbox" id="c-42490065" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489805">parent</a><span>|</span><a href="#42489878">prev</a><span>|</span><a href="#42489853">next</a><span>|</span><label class="collapse" for="c-42490065">[-]</label><label class="expand" for="c-42490065">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just concept space. The entire LLM works in this space once the embedding layer is done. It&#x27;s not really that novel at all.</div><br/></div></div><div id="42489853" class="c"><input type="checkbox" id="c-42489853" checked=""/><div class="controls bullet"><span class="by">asadalt</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489805">parent</a><span>|</span><a href="#42490065">prev</a><span>|</span><a href="#42490716">next</a><span>|</span><label class="collapse" for="c-42489853">[-]</label><label class="expand" for="c-42489853">[2 more]</label></div><br/><div class="children"><div class="content">kinda how we do it. language is just an io interface(but also neural obv) on top of our reasoning engine.</div><br/><div id="42491451" class="c"><input type="checkbox" id="c-42491451" checked=""/><div class="controls bullet"><span class="by">oceanparkway</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489853">parent</a><span>|</span><a href="#42490716">next</a><span>|</span><label class="collapse" for="c-42491451">[-]</label><label class="expand" for="c-42491451">[1 more]</label></div><br/><div class="children"><div class="content">It’s not just a protocol buffer for concepts though (weak wharf Sapir, lakoff’s ubiquitous metaphors). Language itself is also a concept layer and plasticity and concept development is bidirectional. But (I’m not very versed in the language here re ‘latent space’) I would imagine the forward pass through layers converges towards near-token-matches before output, so you have very similar reason to token&#x2F;language reasoning even in latent&#x2F;conceptual reasoning? Like the neurons that nearly only respond to a single token for ex.</div><br/></div></div></div></div><div id="42490716" class="c"><input type="checkbox" id="c-42490716" checked=""/><div class="controls bullet"><span class="by">mountainriver</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489805">parent</a><span>|</span><a href="#42489853">prev</a><span>|</span><a href="#42490139">next</a><span>|</span><label class="collapse" for="c-42490716">[-]</label><label class="expand" for="c-42490716">[1 more]</label></div><br/><div class="children"><div class="content">There are lots of papers that do this</div><br/></div></div></div></div></div></div><div id="42490139" class="c"><input type="checkbox" id="c-42490139" checked=""/><div class="controls bullet"><span class="by">mnk47</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42489683">prev</a><span>|</span><a href="#42489676">next</a><span>|</span><label class="collapse" for="c-42490139">[-]</label><label class="expand" for="c-42490139">[2 more]</label></div><br/><div class="children"><div class="content">&gt; So LLMs finally hit the wall<p>Not really. Throwing a bunch of unfiltered garbage at the pretraining dataset, throwing in RLHF of questionable quality during post-training, and other current hacks - none of that was expected to last forever. There is so much low-hanging fruit that OpenAI left untouched and I&#x27;m sure they&#x27;re still experimenting with the best pre-training and post-training setups.<p>One thing researchers are seeing is resistance to post-training alignment in larger models, but that&#x27;s almost the opposite of a wall, they&#x27;re figuring it out as well.<p>&gt; Now someone has to have a new idea<p>OpenAI already has a few, namely the o* series in which they discovered a way to bake Chain of Thought into the model via RL. Now we have reasoning models that destroy benchmarks that they previously couldn&#x27;t touch.<p>Anthropic has a post-training technique, RLAIF, which supplants RLHF,and it works amazingly well. Combined with countless other tricks we don&#x27;t know about in their training pipeline, they&#x27;ve managed to squeeze so much performance out of Sonnet 3.5 for general tasks.<p>Gemini is showing a lot of promise with their new Flash 2.0 and Flash 2.0-Thinking models. They&#x27;re the first models to beat Sonnet at many benchmarks since April. The new Gemini Pro (or Ultra? whatever they call it now) is probably coming out in January.<p>&gt; The current level of LLM would be far more useful if someone could get a conservative confidence metric out of the internals of the model. This technology desperately needs to output &quot;Don&#x27;t know&quot; or &quot;Not sure about this, but ...&quot; when appropriate.<p>You would probably enjoy this talk [0], it&#x27;s by an independent researcher who IIRC is a former employee of Deepmind or some other lab. They&#x27;re exploring this exact idea. It&#x27;s actually not hard to tell when a model is &quot;confused&quot; (just look at the probability distribution of likely tokens), the challenge is in steering the model to either get back to the right track or give up and say &quot;you know what, idk&quot;<p>[0] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4toIHSsZs1c" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4toIHSsZs1c</a></div><br/><div id="42492065" class="c"><input type="checkbox" id="c-42492065" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490139">parent</a><span>|</span><a href="#42489676">next</a><span>|</span><label class="collapse" for="c-42492065">[-]</label><label class="expand" for="c-42492065">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Not really. Throwing a bunch of unfiltered garbage at the pretraining dataset, throwing in RLHF of questionable quality during post-training, and other current hacks - none of that was expected to last forever. There is so much low-hanging fruit that OpenAI left untouched and I&#x27;m sure they&#x27;re still experimenting with the best pre-training and post-training setups.<p>Exactly! LLama3 and their .x iterations have shown that, at least for now, the idea of using the previous models to filter out the pre-training datasets and use a small amount of seeds to create synthetic datasets for post-training still holds. We&#x27;ll see with L4 if it continues to hold.</div><br/></div></div></div></div><div id="42489676" class="c"><input type="checkbox" id="c-42489676" checked=""/><div class="controls bullet"><span class="by">briga</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42490139">prev</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42489676">[-]</label><label class="expand" for="c-42489676">[30 more]</label></div><br/><div class="children"><div class="content">What wall? Not a week has gone by in recent years without an LLM breaking new benchmarks. There is little evidence to suggest it will all come to a halt in 2025.</div><br/><div id="42489695" class="c"><input type="checkbox" id="c-42489695" checked=""/><div class="controls bullet"><span class="by">jrm4</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489676">parent</a><span>|</span><a href="#42489713">next</a><span>|</span><label class="collapse" for="c-42489695">[-]</label><label class="expand" for="c-42489695">[2 more]</label></div><br/><div class="children"><div class="content">Sure, but &quot;benchmarks&quot; here seems roughly as useful as &quot;benchmarks&quot; for GPUs or CPUs, which don&#x27;t much translate to what the makers of GPT need, which is &#x27;money making use cases.&#x27;</div><br/></div></div><div id="42489713" class="c"><input type="checkbox" id="c-42489713" checked=""/><div class="controls bullet"><span class="by">peepeepoopoo98</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489676">parent</a><span>|</span><a href="#42489695">prev</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42489713">[-]</label><label class="expand" for="c-42489713">[27 more]</label></div><br/><div class="children"><div class="content">O3 has demonstrated that OpenAI needs 1,000,000% more inference time compute to score 50% higher on benchmarks.  If O3-High costs about $350k an hour to operate, that would mean making O4 score 50% higher would cost <i>$3.5B</i> (!!!) an hour.  <i>That</i> scaling wall.</div><br/><div id="42489822" class="c"><input type="checkbox" id="c-42489822" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489713">parent</a><span>|</span><a href="#42489780">next</a><span>|</span><label class="collapse" for="c-42489822">[-]</label><label class="expand" for="c-42489822">[3 more]</label></div><br/><div class="children"><div class="content">I’m convinced they’re getting good at gaming the benchmarks since 4 has deteriorated via ChatGPT, in fact I’ve used 4-0125 and 4-1106 via the API and find them far superior to o1 and o1-mini at coding problems. GPT4 is an amazing tool but the true capabilities are being hidden from the public and&#x2F;or intentionally neutered.</div><br/><div id="42489927" class="c"><input type="checkbox" id="c-42489927" checked=""/><div class="controls bullet"><span class="by">CSMastermind</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489822">parent</a><span>|</span><a href="#42489780">next</a><span>|</span><label class="collapse" for="c-42489927">[-]</label><label class="expand" for="c-42489927">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I’ve used 4-0125 and 4-1106 via the API and find them far superior to o1 and o1-mini at coding problems<p>Just chiming in to say you&#x27;re not alone.  This has been my experience as well.  The o# line of models just don&#x27;t do well at coding, regardless of what the benchmarks say.</div><br/><div id="42492239" class="c"><input type="checkbox" id="c-42492239" checked=""/><div class="controls bullet"><span class="by">didibus</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489927">parent</a><span>|</span><a href="#42489780">next</a><span>|</span><label class="collapse" for="c-42492239">[-]</label><label class="expand" for="c-42492239">[1 more]</label></div><br/><div class="children"><div class="content">All the benchmarks provide substantial scaffolding and specification details, and that&#x27;s if they are zero-shot at all, which they often are not. In reality, nobody wants to spend as much time providing so much details or examples just to get the AI to write the correct function, when that same time and effort you&#x27;d have used to write it yourself.<p>Also, those benchmarks often run the model K times on the same question, and if any one of them is correct, they say it passed. That could mean if you re-ran the model 8 times, it might come up with the right answer only once. But now you have to waste your time checking if it is right or not.<p>I want to ask: &quot;Write a function to count unique numbers in a list&quot; and get the correct answer the first time.<p>What you need to ask:<p>&quot;&quot;&quot;
Write a Python function that takes a list of integers as input and returns 
the count of numbers that appear exactly once in the list.<p>The function should:
- Accept a single parameter: a list of integers
- Count elements that appear exactly once
- Return an integer representing the count
- Handle empty lists and return 0
- Handle lists with duplicates correctly<p>Please provide a complete implementation.
&quot;&quot;&quot;<p>And run it 8 times and if you&#x27;re lucky it&#x27;ll get it correct zero-shot.<p>Edit: I&#x27;m not even aware of a Pass@1, zero-shot, and without detailed prompting (natural prompting) benchmark. If anyone knows one let me know.</div><br/></div></div></div></div></div></div><div id="42489780" class="c"><input type="checkbox" id="c-42489780" checked=""/><div class="controls bullet"><span class="by">norir</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489713">parent</a><span>|</span><a href="#42489822">prev</a><span>|</span><a href="#42489734">next</a><span>|</span><label class="collapse" for="c-42489780">[-]</label><label class="expand" for="c-42489780">[4 more]</label></div><br/><div class="children"><div class="content">I used to run a lot of monte carlo simulations where the error is proportional to the inverse square root. There was a huge advantage of running for an hour vs a few minutes, but you hit the diminishing returns depressingly quickly. It would not surprise me at all if llms end up having similar scaling properties.</div><br/><div id="42491106" class="c"><input type="checkbox" id="c-42491106" checked=""/><div class="controls bullet"><span class="by">LegionMammal978</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489780">parent</a><span>|</span><a href="#42489977">next</a><span>|</span><label class="collapse" for="c-42491106">[-]</label><label class="expand" for="c-42491106">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, any situation you need O(<i>n</i>^2) runtime to obtain <i>n</i> bits of output (or bits of accuracy, in the Monre Carlo case) is pure pain. At every point, it&#x27;s still within your means to double the amount of output (by running it 3x longer than you have so far), but it gradually becomes more and more painful, instead of there being a single point where you can call it off.</div><br/></div></div><div id="42489977" class="c"><input type="checkbox" id="c-42489977" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489780">parent</a><span>|</span><a href="#42491106">prev</a><span>|</span><a href="#42489734">next</a><span>|</span><label class="collapse" for="c-42489977">[-]</label><label class="expand" for="c-42489977">[2 more]</label></div><br/><div class="children"><div class="content">And I suspect o3 is something like monte carlo: generates tons of CoTs, with most of them are junk, but some hit the answer.</div><br/><div id="42490496" class="c"><input type="checkbox" id="c-42490496" checked=""/><div class="controls bullet"><span class="by">exhaze</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489977">parent</a><span>|</span><a href="#42489734">next</a><span>|</span><label class="collapse" for="c-42490496">[-]</label><label class="expand" for="c-42490496">[1 more]</label></div><br/><div class="children"><div class="content">Sounds plausible given I’ve recently observed a ton of research papers in the space that in some way or another incorporate MCTS</div><br/></div></div></div></div></div></div><div id="42489734" class="c"><input type="checkbox" id="c-42489734" checked=""/><div class="controls bullet"><span class="by">Kuinox</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489713">parent</a><span>|</span><a href="#42489780">prev</a><span>|</span><a href="#42489828">next</a><span>|</span><label class="collapse" for="c-42489734">[-]</label><label class="expand" for="c-42489734">[4 more]</label></div><br/><div class="children"><div class="content">Wait a few month and they will have a distilled model with the same performance and 1% of the run cost.</div><br/><div id="42489754" class="c"><input type="checkbox" id="c-42489754" checked=""/><div class="controls bullet"><span class="by">achierius</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489734">parent</a><span>|</span><a href="#42489751">next</a><span>|</span><label class="collapse" for="c-42489754">[-]</label><label class="expand" for="c-42489754">[1 more]</label></div><br/><div class="children"><div class="content">Even assuming that past rates of inference cost scaling hold up, we would only expect a 2 OoM decrease after about a year or so.
And 1% of 3.5b is still a very large number.</div><br/></div></div><div id="42489751" class="c"><input type="checkbox" id="c-42489751" checked=""/><div class="controls bullet"><span class="by">peepeepoopoo98</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489734">parent</a><span>|</span><a href="#42489754">prev</a><span>|</span><a href="#42489828">next</a><span>|</span><label class="collapse" for="c-42489751">[-]</label><label class="expand" for="c-42489751">[2 more]</label></div><br/><div class="children"><div class="content">100X efficiency improvement (doubtful) still means that costs grow 200X faster than benchmark performance.</div><br/></div></div></div></div><div id="42489828" class="c"><input type="checkbox" id="c-42489828" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489713">parent</a><span>|</span><a href="#42489734">prev</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42489828">[-]</label><label class="expand" for="c-42489828">[15 more]</label></div><br/><div class="children"><div class="content">Not really. o3-low compute still stomps the benchmarks and isn&#x27;t anywhere that expensive and o3-mini seems better than o1 while being cheaper.<p>Combine that with the fact that LLM inference has reduced orders of magnitudes in cost the last few years and hampering over the inference costs of a new release seems a bit silly.</div><br/><div id="42490160" class="c"><input type="checkbox" id="c-42490160" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489828">parent</a><span>|</span><a href="#42489985">next</a><span>|</span><label class="collapse" for="c-42490160">[-]</label><label class="expand" for="c-42490160">[5 more]</label></div><br/><div class="children"><div class="content">It is still not economical: in Arc at least 20 usd for task vs ~3 usd for a human (avg mturker) for the same perf.</div><br/><div id="42490581" class="c"><input type="checkbox" id="c-42490581" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490160">parent</a><span>|</span><a href="#42489985">next</a><span>|</span><label class="collapse" for="c-42490581">[-]</label><label class="expand" for="c-42490581">[4 more]</label></div><br/><div class="children"><div class="content">Not necessarily. And this is the problem with ARC that people seem to forget.<p>- It&#x27;s just a suite of visual puzzles. It&#x27;s not like say GSM8K where proficiency in it gives some indication on Math proficiency in general.<p>- It&#x27;s specifically a suite of puzzles that LLMs have shown particular difficulty in.<p>Basically how much compute it takes to handle a task in this benchmark does not correlate with how much it will take LLMs to compute tasks that people actually want to use LLMs for.</div><br/><div id="42491089" class="c"><input type="checkbox" id="c-42491089" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490581">parent</a><span>|</span><a href="#42489985">next</a><span>|</span><label class="collapse" for="c-42491089">[-]</label><label class="expand" for="c-42491089">[3 more]</label></div><br/><div class="children"><div class="content">If the benchmark is not representative of normal usage* then the benchmark and the plot being shown are not useful at all from a user&#x2F;business perspective and the focus on the breakthrough scores of o3-low and o3-high in ARC-AGI would be highly misleading. And also the &quot;representative&quot; point is really moot from the discussion perspective (i.e. saying o3 stomps benchmarks, but the benchmarks aren&#x27;t representative).<p>*I don&#x27;t think that is the case as you can at least make relative conclusions (i.e. o3 vs o1 series, o3-low is 4x to 20x the cost for ~3x the perf). Even if it is pure marketing they expect people to draw conclusions using the perf&#x2F;cost plot from Arc.<p>PS: I know there are more benchmarks like SWE-Bench and Frontier Math, but this is the only one showing data about o3-low&#x2F;high costs without considering the CodeForces plot that includes o3-mini (that one does look interesting, though right now is vaporware) but does not separate between compute scale modes.</div><br/><div id="42491537" class="c"><input type="checkbox" id="c-42491537" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42491089">parent</a><span>|</span><a href="#42489985">next</a><span>|</span><label class="collapse" for="c-42491537">[-]</label><label class="expand" for="c-42491537">[2 more]</label></div><br/><div class="children"><div class="content">&gt;If the benchmark is not representative of normal usage* then the benchmark and the plot being shown are not useful at all from a user&#x2F;business perspective and the focus on the breakthrough scores of o3-low and o3-high in ARC-AGI would be highly misleading.<p>ARC is a very hyped benchmark in the industry so letting us know the results is something any company would do whether it had a direct representation on normal usage or not.<p>&gt;Even if it is pure marketing they expect people to draw conclusions using the perf&#x2F;cost plot from Arc.<p>Again, people care about ARC, they don&#x27;t care doing the things ARC questions ask. That it is un-economical to pay the price to use o3 for ARC does not mean it would be un-economical to do so for the tasks people actually want to use LLMs for. What does 3x the performance in say coding mean? You really think companies&#x2F;users wouldn&#x27;t put up with the increased price for that? You think they have Mturkers to turn to like they do with ARC?<p>ARC is literally the quintessential &#x27;easy for humans, hard for ai&#x27; benchmark. Even if you discard the &#x27;difficulty to price won&#x27;t scale the same&#x27; argument, it makes no sense to use it for an economics comparison.</div><br/><div id="42492129" class="c"><input type="checkbox" id="c-42492129" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42491537">parent</a><span>|</span><a href="#42489985">next</a><span>|</span><label class="collapse" for="c-42492129">[-]</label><label class="expand" for="c-42492129">[1 more]</label></div><br/><div class="children"><div class="content">In summary: so the &quot;stomps benchmarks&quot; means nothing for anyone trying to make decisions on that announcement (yet they show cost&#x2F;perf info). It seems, hipey.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42489985" class="c"><input type="checkbox" id="c-42489985" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489828">parent</a><span>|</span><a href="#42490160">prev</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42489985">[-]</label><label class="expand" for="c-42489985">[9 more]</label></div><br/><div class="children"><div class="content">If you are talking about ARC benchmark, then o3-low doesn&#x27;t look that special if you take into account there are plenty of finetuned models with much smaller resources achieved 40-50% results on private set (not semi-private like o3-low).</div><br/><div id="42490049" class="c"><input type="checkbox" id="c-42490049" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489985">parent</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42490049">[-]</label><label class="expand" for="c-42490049">[8 more]</label></div><br/><div class="children"><div class="content">- I&#x27;m not just talking about ARC. On frontier Math, we have 2 scores, one with pass@1 and another with consensus vote with 64 samples. Both scores are much better than previous Sota.<p>- Also apparently, ARC wasn&#x27;t a special fine-tune but rather some of the training set in the corpus for pre-training.</div><br/><div id="42490097" class="c"><input type="checkbox" id="c-42490097" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490049">parent</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42490097">[-]</label><label class="expand" for="c-42490097">[7 more]</label></div><br/><div class="children"><div class="content">&gt; On frontier Math<p>that result is not verifiable, not reproducable, unknown if it was leaked and how it was measured. Its kinda hype science.<p>&gt; ARC wasn&#x27;t a special fine-tune but rather some of the training set in the corpus for pre-training.<p>post says: Note on &quot;tuned&quot;: OpenAI shared they trained the o3 we tested on 75% of the Public Training set. They have not shared more details.<p>So, I guess we don&#x27;t know.</div><br/><div id="42490296" class="c"><input type="checkbox" id="c-42490296" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490097">parent</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42490296">[-]</label><label class="expand" for="c-42490296">[6 more]</label></div><br/><div class="children"><div class="content">&gt;that result is not verifiable, not reproducable, unknown if it was leaked and how it was measured. Its kinda hype science.<p>It will be verifiable when the model is released. Open ai haven&#x27;t released any benchmark scores that were shown falsified later so unless you have an actual reason to believe they&#x27;re outright lying then it&#x27;s not something to take seriously.<p>Frontier Math is a private benchmark with its highest tier of difficulty Terrence Tao says:<p>“These are extremely challenging. I think that in the near term basically the only way to solve them, short of having a real domain expert in the area, is by a combination of a semi-expert like a graduate student in a related field, maybe paired with some combination of a modern AI and lots of other algebra packages…”<p>Unless you have a reason to believe answers were leaked then again, not interested in baseless speculation.</div><br/><div id="42490380" class="c"><input type="checkbox" id="c-42490380" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490296">parent</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42490380">[-]</label><label class="expand" for="c-42490380">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Open ai haven&#x27;t released any benchmark scores<p>there are multiple research results demonstrating that various benchmarks are heavily leaked to GPT training data.<p>Is it intentionally or not, we can&#x27;t figure out, but they have very strong incentive to cheat to get more investments.<p>&gt; Unless you have a reason to believe answers were leaked then again, not interested in baseless speculation.<p>this is scientific methodology when results have to be reproduced or confirmed before believed.</div><br/><div id="42490448" class="c"><input type="checkbox" id="c-42490448" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490380">parent</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42490448">[-]</label><label class="expand" for="c-42490448">[4 more]</label></div><br/><div class="children"><div class="content">Again, Frontier Math is private. Benchmarks leaked to GPT-4 are all public datasets on the internet. Frontier Math literally cannot leak that way.<p>If you don&#x27;t want to take the benchmarks at face value then good for you but this entire conversation is pointless.</div><br/><div id="42490492" class="c"><input type="checkbox" id="c-42490492" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490448">parent</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42490492">[-]</label><label class="expand" for="c-42490492">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Again, Frontier Math is private.<p>its private for outsiders, but it was developed in &quot;collaboration&quot; with OAI, and GPT was tested in the past on it, so they have it in logs somewhere.<p>&gt; If you don&#x27;t want to take the benchmarks at face value then good for you but this entire conversation is pointless.<p>If you think this entire conversation is pointless, then why do you continue?</div><br/><div id="42490648" class="c"><input type="checkbox" id="c-42490648" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490492">parent</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42490648">[-]</label><label class="expand" for="c-42490648">[2 more]</label></div><br/><div class="children"><div class="content">&gt;its private for outsiders, but it was developed in &quot;collaboration&quot; with OAI, and GPT was tested in the past on it, so they have it in logs somewhere.<p>They have logs of the questions probably but that&#x27;s not enough. Frontier Math isn&#x27;t something that can be fully solved without gathering top experts at multiple disciplines. Even Tao says he only knows who to ask for the most difficult set.<p>Basically, what you&#x27;re suggesting at least with this benchmark in particular is far more difficult than you&#x27;re implying.<p>&gt;If you think this entire conversation is pointless, then why do you continue?<p>There&#x27;s no point arguing about how efficient the models are being (the original point) if you won&#x27;t even accept the results of the benchmarks. Why i&#x27;m continuing ? For now, it&#x27;s only polite to clarify.</div><br/><div id="42491107" class="c"><input type="checkbox" id="c-42491107" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490648">parent</a><span>|</span><a href="#42492420">next</a><span>|</span><label class="collapse" for="c-42491107">[-]</label><label class="expand" for="c-42491107">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Frontier Math isn&#x27;t something that can be fully solved without gathering top experts<p>Tao&#x27;s quote above referred on hardest 20% problems, they have 3 levels of difficulty, presumably first level is much easier. Also, as I mentioned OAI collaborated on creating benchmark, so they could have access to all solutions too.<p>&gt; There&#x27;s no point arguing<p>Lol, let me ask again, why you are arguing then? Yes, I have strong reasonable(imo) doubt that those results are valid.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42492420" class="c"><input type="checkbox" id="c-42492420" checked=""/><div class="controls bullet"><span class="by">Jean-Papoulos</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42489676">prev</a><span>|</span><a href="#42489674">next</a><span>|</span><label class="collapse" for="c-42492420">[-]</label><label class="expand" for="c-42492420">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So LLMs finally hit the wall. For a long time, more data, bigger models, and more compute to drive them worked<p>We can&#x27;t say whether there is a wall, since we don&#x27;t have anymore data to train on.</div><br/></div></div><div id="42489674" class="c"><input type="checkbox" id="c-42489674" checked=""/><div class="controls bullet"><span class="by">whoisthemachine</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42492420">prev</a><span>|</span><a href="#42489987">next</a><span>|</span><label class="collapse" for="c-42489674">[-]</label><label class="expand" for="c-42489674">[2 more]</label></div><br/><div class="children"><div class="content">Unfortunately, the best they can do is &quot;This is my confidence on what someone would say given the prior context&quot;.</div><br/><div id="42491560" class="c"><input type="checkbox" id="c-42491560" checked=""/><div class="controls bullet"><span class="by">sooheon</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489674">parent</a><span>|</span><a href="#42489987">next</a><span>|</span><label class="collapse" for="c-42491560">[-]</label><label class="expand" for="c-42491560">[1 more]</label></div><br/><div class="children"><div class="content">What someone from the past would have said.</div><br/></div></div></div></div><div id="42489987" class="c"><input type="checkbox" id="c-42489987" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42489674">prev</a><span>|</span><a href="#42489667">next</a><span>|</span><label class="collapse" for="c-42489987">[-]</label><label class="expand" for="c-42489987">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Now someone has to have a new idea. There&#x27;s plenty of money available if someone has one.<p>I honestly <i>do</i> claim to have some ideas where I see evidence that they might work (and I do attempt to work privately on a prototype if only out of curiosity and to see whether I am right). The bad news: these ideas very likely won&#x27;t be helpful for these LLM companies because they are not useful for their agenda, and follow a very different approach.<p>So no money for me. :-(<p>Let me put it this way:<p>Have you ever talked to a person whose intelligence is miles above yours? It can easily become very exhausting. Thus an &quot;insanely intelligent&quot; AI would not be of much use for most people - it would think &quot;too different&quot; from such people.<p>There <i>do</i> exist tasks in commerce for which an insane amount of intelligence would make a huge difference (in the sense of being positive regarding some important KPIs), but these are rare. I can imagine some applications of such (fictional) &quot;super-intelligent&quot; AIs in finance and companies doing some bleeding-edge scientific research - but these are niche applications (though potentially very lucrative ones).<p>If OpenAI, Anthropic &amp; Co were really attempting to develop some &quot;super-smart&quot; AI, they were working on such very lucrative niche applications where an insane amount of intelligence would make a huge difference, and where you can assume and train the AI operator to have a &quot;Fields-medal level&quot; intelligence.</div><br/></div></div><div id="42489667" class="c"><input type="checkbox" id="c-42489667" checked=""/><div class="controls bullet"><span class="by">synapsomorphy</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42489987">prev</a><span>|</span><a href="#42491801">next</a><span>|</span><label class="collapse" for="c-42489667">[-]</label><label class="expand" for="c-42489667">[6 more]</label></div><br/><div class="children"><div class="content">The new idea is already here and it&#x27;s reasoning &#x2F; chain of thought.<p>Anecdotally Claude is pretty good at knowing the bounds of its knowledge.</div><br/><div id="42489947" class="c"><input type="checkbox" id="c-42489947" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489667">parent</a><span>|</span><a href="#42491801">next</a><span>|</span><label class="collapse" for="c-42489947">[-]</label><label class="expand" for="c-42489947">[5 more]</label></div><br/><div class="children"><div class="content">Anecdotally Claude is just as bad as every other LLM.<p>Step into more niche areas e.g. I am trying to use it with Scala macros and at least 90% of the time it is giving code that either (a) fails to compile or (b) is just complete gibberish.<p>And at no point <i>ever</i> has it said it didn&#x27;t know something.</div><br/><div id="42490179" class="c"><input type="checkbox" id="c-42490179" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42489947">parent</a><span>|</span><a href="#42491801">next</a><span>|</span><label class="collapse" for="c-42490179">[-]</label><label class="expand" for="c-42490179">[4 more]</label></div><br/><div class="children"><div class="content">Yep, get into any sufficiently deep niche (i.e. actually almost any non-trivial app) and the LLM magic fades off.<p>Yeah sure you can make a pong clone in html&#x2F;js and that&#x27;s mainly because there the internet is full of pong clone demos. Ask how to constraint a statsmodels lineal model in some non-standard way? It will gaslight how it is possible and make you loss time in the process.</div><br/><div id="42491931" class="c"><input type="checkbox" id="c-42491931" checked=""/><div class="controls bullet"><span class="by">IAmGraydon</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490179">parent</a><span>|</span><a href="#42491801">next</a><span>|</span><label class="collapse" for="c-42491931">[-]</label><label class="expand" for="c-42491931">[3 more]</label></div><br/><div class="children"><div class="content">Making a pong clone by telling the LLM to make a pong clone is a cute trick that sometimes works, but that&#x27;s not the way anyone who understands how to properly use these tools is using them. You don&#x27;t describe and app and hope the LLM builds it correctly. You have to know how to architect an application and you use the LLM to build small pieces of code. For example, you tell it to build a function that does x, takes the inputs a, b, and c and returns z.<p>LLMs don&#x27;t turn non-coders into coders. It gives actual coders superpowers.</div><br/><div id="42492121" class="c"><input type="checkbox" id="c-42492121" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42491931">parent</a><span>|</span><a href="#42492203">next</a><span>|</span><label class="collapse" for="c-42492121">[-]</label><label class="expand" for="c-42492121">[1 more]</label></div><br/><div class="children"><div class="content">No true scottsman fallacy. I know how to use them, but using them &quot;correctly&quot; still produces many errors.<p>They suck at non-trivial code outside of standard library usage and boilerplate coding: I gave an example and parent did as well. In that regard would at least change your phrase from &quot;actual coders&quot; to &quot;actual senior coders&quot;, as any junior receiving bad advice (in eternal loops as LLMs normally like to do it) is only going to make them waste time and tokens.</div><br/></div></div><div id="42492203" class="c"><input type="checkbox" id="c-42492203" checked=""/><div class="controls bullet"><span class="by">margalabargala</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42491931">parent</a><span>|</span><a href="#42492121">prev</a><span>|</span><a href="#42491801">next</a><span>|</span><label class="collapse" for="c-42492203">[-]</label><label class="expand" for="c-42492203">[1 more]</label></div><br/><div class="children"><div class="content">As a coder with some noncoder friends who have made some very impressive things with chatGPT, you&#x27;re selling it short.<p>It does both. It gives coders superpowers, and gives noncoders the ability to do things that would have previously taken them months, or another person.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42491801" class="c"><input type="checkbox" id="c-42491801" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42489667">prev</a><span>|</span><a href="#42489806">next</a><span>|</span><label class="collapse" for="c-42491801">[-]</label><label class="expand" for="c-42491801">[1 more]</label></div><br/><div class="children"><div class="content">the new idea is the o series and clearly OpenAI’s main focus now. It’s advancing much faster than the GPT series</div><br/></div></div><div id="42489806" class="c"><input type="checkbox" id="c-42489806" checked=""/><div class="controls bullet"><span class="by">knapcio</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42491801">prev</a><span>|</span><a href="#42490488">next</a><span>|</span><label class="collapse" for="c-42489806">[-]</label><label class="expand" for="c-42489806">[1 more]</label></div><br/><div class="children"><div class="content">I’m wondering whether O3 can be used to explore its own improvement or optimization ideas, or if it hasn’t reached that point yet.</div><br/></div></div><div id="42490488" class="c"><input type="checkbox" id="c-42490488" checked=""/><div class="controls bullet"><span class="by">thrwthsnw</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42489806">prev</a><span>|</span><a href="#42489860">next</a><span>|</span><label class="collapse" for="c-42490488">[-]</label><label class="expand" for="c-42490488">[2 more]</label></div><br/><div class="children"><div class="content">Seriously? All they do is produce a “confidence metric”</div><br/><div id="42491427" class="c"><input type="checkbox" id="c-42491427" checked=""/><div class="controls bullet"><span class="by">emtel</span><span>|</span><a href="#42489580">root</a><span>|</span><a href="#42490488">parent</a><span>|</span><a href="#42489860">next</a><span>|</span><label class="collapse" for="c-42491427">[-]</label><label class="expand" for="c-42491427">[1 more]</label></div><br/><div class="children"><div class="content">But how do they do that?</div><br/></div></div></div></div><div id="42489860" class="c"><input type="checkbox" id="c-42489860" checked=""/><div class="controls bullet"><span class="by">Yizahi</span><span>|</span><a href="#42489580">parent</a><span>|</span><a href="#42490488">prev</a><span>|</span><a href="#42486273">next</a><span>|</span><label class="collapse" for="c-42489860">[-]</label><label class="expand" for="c-42489860">[1 more]</label></div><br/><div class="children"><div class="content">To output &quot;don&#x27;t know&quot; a system needs to &quot;know&quot; too. Random token generator can&#x27;t know. It can guess better and better, maybe it can even guess 99.99% of time, but it can&#x27;t know, it can&#x27;t decide or reason (not even o1 can &quot;reason&quot;).</div><br/></div></div></div></div><div id="42486273" class="c"><input type="checkbox" id="c-42486273" checked=""/><div class="controls bullet"><span class="by">ericskiff</span><span>|</span><a href="#42489580">prev</a><span>|</span><a href="#42489478">next</a><span>|</span><label class="collapse" for="c-42486273">[-]</label><label class="expand" for="c-42486273">[70 more]</label></div><br/><div class="children"><div class="content">What we can reasonably assume from statements made by insiders:<p>They want a 10x improvement from scaling and a 10x improvement from data and algorithmic changes<p>The sources of public data are essentially tapped<p>Algorithmic changes will be an unknown to us until they release, but from published research this remains a steady source of improvement<p>Scaling seems to stall if data is limited<p>So with all of that taken together, the logical step is to figure out how to turn compute into better data to train on. Enter strawberry &#x2F; o1, and now o3<p>They can throw money, time, and compute at thinking about and then generating better training data. If the belief is that N billion new tokens of high quality training data will unlock the leap in capabilities they’re looking for, then it makes sense to delay the training until that dataset is ready<p>With o3 now public knowledge, imagine how long it’s been churning out new thinking at expert level across every field. OpenAI’s next moat may be the best synthetic training set ever.<p>At this point I would guess we get 4.5 with a subset of this - some scale improvement, the algorithmic pickups since 4 was trained, and a cleaned and improved core data set but without risking leakage of the superior dataset<p>When 5 launches, we get to see what a fully scaled version looks like with training data that outstrips average humans in almost every problem space<p>Then the next o-model gets to start with that as a base and reason? Its likely to be remarkable</div><br/><div id="42489455" class="c"><input type="checkbox" id="c-42489455" checked=""/><div class="controls bullet"><span class="by">sdwr</span><span>|</span><a href="#42486273">parent</a><span>|</span><a href="#42486347">next</a><span>|</span><label class="collapse" for="c-42489455">[-]</label><label class="expand" for="c-42489455">[9 more]</label></div><br/><div class="children"><div class="content">Great improvements and all, but they are still no closer (as of 4o regular) to having a system that can be responsible for work. In math problems, it forgets which variable represents what, in coding questions it invents library fns.<p>I was watching a YouTube interview with a &quot;trading floor insider&quot;. They said they were really being paid for holding risk. The bank has a position in a market, and it&#x27;s their ass on the line if it tanks.<p>ChatGPT (as far as I can tell) is no closer to being accountable or responsible for anything it produces. If they don&#x27;t solve that (and the problem is probably inherent to the architecture), they are, in some sense, polishing a turd.</div><br/><div id="42491467" class="c"><input type="checkbox" id="c-42491467" checked=""/><div class="controls bullet"><span class="by">nightowl_games</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489455">parent</a><span>|</span><a href="#42489684">next</a><span>|</span><label class="collapse" for="c-42491467">[-]</label><label class="expand" for="c-42491467">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They said they were really being paid for holding risk.<p>I think that&#x27;s a really interesting insight that has application to using &#x27;AI&#x27; in jobs across the board.</div><br/></div></div><div id="42489684" class="c"><input type="checkbox" id="c-42489684" checked=""/><div class="controls bullet"><span class="by">tucnak</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489455">parent</a><span>|</span><a href="#42491467">prev</a><span>|</span><a href="#42486347">next</a><span>|</span><label class="collapse" for="c-42489684">[-]</label><label class="expand" for="c-42489684">[7 more]</label></div><br/><div class="children"><div class="content">&gt; ChatGPT (as far as I can tell) is no closer to being accountable or responsible for anything it produces.<p>What does it even mean? How do you imagine that? You want OpenAI to take on liability for the kicks of it?</div><br/><div id="42489763" class="c"><input type="checkbox" id="c-42489763" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489684">parent</a><span>|</span><a href="#42489849">next</a><span>|</span><label class="collapse" for="c-42489763">[-]</label><label class="expand" for="c-42489763">[3 more]</label></div><br/><div class="children"><div class="content">If an LLM can&#x27;t be left to do mowing by itself, but a human will have to closely monitor and intervene at every its steps, then it&#x27;s just a super fast predictive keyboard, no?</div><br/><div id="42491773" class="c"><input type="checkbox" id="c-42491773" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489763">parent</a><span>|</span><a href="#42489849">next</a><span>|</span><label class="collapse" for="c-42491773">[-]</label><label class="expand" for="c-42491773">[2 more]</label></div><br/><div class="children"><div class="content">But what if the human only has to intervene once every 100 hours, that’s a huge productivity boost.</div><br/><div id="42492866" class="c"><input type="checkbox" id="c-42492866" checked=""/><div class="controls bullet"><span class="by">cjblomqvist</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42491773">parent</a><span>|</span><a href="#42489849">next</a><span>|</span><label class="collapse" for="c-42492866">[-]</label><label class="expand" for="c-42492866">[1 more]</label></div><br/><div class="children"><div class="content">The point is you don&#x27;t know when of those 100 hours that is, so you still need to monitor the full 100 hour time span.<p>Can still be a boost. But definitely not the same magnitude.</div><br/></div></div></div></div></div></div><div id="42489849" class="c"><input type="checkbox" id="c-42489849" checked=""/><div class="controls bullet"><span class="by">dmkolobov</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489684">parent</a><span>|</span><a href="#42489763">prev</a><span>|</span><a href="#42490792">next</a><span>|</span><label class="collapse" for="c-42489849">[-]</label><label class="expand" for="c-42489849">[1 more]</label></div><br/><div class="children"><div class="content">Obviously not. I want legislation which imposes liability on OpenAI and similar companies if they actively market their products for use in safety-critical fields and their product doesn’t perform as advertised.<p>If a system is providing incorrect medical diagnoses, or denying services to protected classes due to biases in the training in the training data, someone should be held accountable.</div><br/></div></div><div id="42490792" class="c"><input type="checkbox" id="c-42490792" checked=""/><div class="controls bullet"><span class="by">sdwr</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489684">parent</a><span>|</span><a href="#42489849">prev</a><span>|</span><a href="#42489764">next</a><span>|</span><label class="collapse" for="c-42490792">[-]</label><label class="expand" for="c-42490792">[1 more]</label></div><br/><div class="children"><div class="content">Personal responsibility, not legal liability. In the way a child can be responsible for a pet.<p>Chatgpt was trained on benchmarks and user opinions - &quot;throwing **** at the wall to see what sticks&quot;.<p>Responsibility means penalties for making mistakes, and, more importantly, having an awareness of those penalties (that informs its decision-making).</div><br/></div></div><div id="42489764" class="c"><input type="checkbox" id="c-42489764" checked=""/><div class="controls bullet"><span class="by">SpicyLemonZest</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489684">parent</a><span>|</span><a href="#42490792">prev</a><span>|</span><a href="#42486347">next</a><span>|</span><label class="collapse" for="c-42489764">[-]</label><label class="expand" for="c-42489764">[1 more]</label></div><br/><div class="children"><div class="content">They would want to, if they thought they could, because doing so would unblock a ton of valuable use cases. A tax preparation or financial advisor AI would do huge numbers for any company able to promise that its advice can be trusted.</div><br/></div></div></div></div></div></div><div id="42486347" class="c"><input type="checkbox" id="c-42486347" checked=""/><div class="controls bullet"><span class="by">Stevvo</span><span>|</span><a href="#42486273">parent</a><span>|</span><a href="#42489455">prev</a><span>|</span><a href="#42487795">next</a><span>|</span><label class="collapse" for="c-42486347">[-]</label><label class="expand" for="c-42486347">[15 more]</label></div><br/><div class="children"><div class="content">&quot;With o3 now public knowledge, imagine how long it’s been churning out new thinking at expert level across every field.&quot;<p>I highly doubt that. o3 is many orders of magnitude more expensive than paying subject matter experts to create new data. It just doesn&#x27;t make sense to pay six figures in compute to get o3 to make data a human could make for a few hundred dollars.</div><br/><div id="42492817" class="c"><input type="checkbox" id="c-42492817" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486347">parent</a><span>|</span><a href="#42487819">next</a><span>|</span><label class="collapse" for="c-42492817">[-]</label><label class="expand" for="c-42492817">[1 more]</label></div><br/><div class="children"><div class="content">Only a matter of time. The costs are aggressively going down. And with specialized inference hardware it will go further down.<p>Cost of coordination is also large. Immediate answers are an advantage&#x2F;selling point.</div><br/></div></div><div id="42487819" class="c"><input type="checkbox" id="c-42487819" checked=""/><div class="controls bullet"><span class="by">bookaway</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486347">parent</a><span>|</span><a href="#42492817">prev</a><span>|</span><a href="#42491961">next</a><span>|</span><label class="collapse" for="c-42487819">[-]</label><label class="expand" for="c-42487819">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I think they had to push this reveal forward because their investors were getting antsy with the lack of visible progress to justify continuing rising valuations. There is no other reason a confident company making continuous rapid progress would feel the need to reveal a product that 99% of companies worldwide couldn&#x27;t use at the time of the reveal.<p>That being said, if OpenAI is burning cash at lightspeed and doesn&#x27;t have to publicly reveal the revenue they receive from certain government entities, it wouldn&#x27;t come as a surprise if they let the government play with it early on in exchange for some much needed cash to set on fire.<p>EDIT: The fact that multiple sites seem to be publishing  GPT-5 stories similar to this one leads one to conclude that the o3 benchmark story was meant to counter the negativity from this and other similar articles that are just coming out.</div><br/></div></div><div id="42491961" class="c"><input type="checkbox" id="c-42491961" checked=""/><div class="controls bullet"><span class="by">GolfPopper</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486347">parent</a><span>|</span><a href="#42487819">prev</a><span>|</span><a href="#42489431">next</a><span>|</span><label class="collapse" for="c-42491961">[-]</label><label class="expand" for="c-42491961">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>churning out new thinking at expert level across every field</i><p>I suspect this is really, &quot;churning out text that impresses management&quot;.</div><br/></div></div><div id="42489431" class="c"><input type="checkbox" id="c-42489431" checked=""/><div class="controls bullet"><span class="by">mrshadowgoose</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486347">parent</a><span>|</span><a href="#42491961">prev</a><span>|</span><a href="#42489034">next</a><span>|</span><label class="collapse" for="c-42489431">[-]</label><label class="expand" for="c-42489431">[1 more]</label></div><br/><div class="children"><div class="content">Can SMEs deliver that data in a meaningful amount of time? Training data now is worth significantly more than data a year from now.</div><br/></div></div><div id="42489034" class="c"><input type="checkbox" id="c-42489034" checked=""/><div class="controls bullet"><span class="by">tshadley</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486347">parent</a><span>|</span><a href="#42489431">prev</a><span>|</span><a href="#42490221">next</a><span>|</span><label class="collapse" for="c-42489034">[-]</label><label class="expand" for="c-42489034">[3 more]</label></div><br/><div class="children"><div class="content">Seems to me o3 prices would be what the consumer pays, not what OpenAI pays.  That would mean o3 could be more efficient in-house than paying subject-matter experts.</div><br/><div id="42490229" class="c"><input type="checkbox" id="c-42490229" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489034">parent</a><span>|</span><a href="#42489405">next</a><span>|</span><label class="collapse" for="c-42490229">[-]</label><label class="expand" for="c-42490229">[1 more]</label></div><br/><div class="children"><div class="content">For every consumer there will be a period where they need both the SME and the o3 model for initial calibration and eventual handoff for actually getting those efficiencies in whichever processes they want to automate.<p>In other words if you are diligent enough, you should at least validate your o3 solution with an actual expert for some time. You wouldn&#x27;t just blindly trust OpenAI your business critical processes, would you? I would expect at least 3 month - 6 months for large corps and even more considering change management, re-upskilling, etc.<p>With all those considerations I really don&#x27;t see the value prop at those prices and in those situations right now. Maybe if costs decrease ~1-3 orders of magnitude more for o3-low, depending on the the processes being automated.</div><br/></div></div><div id="42489405" class="c"><input type="checkbox" id="c-42489405" checked=""/><div class="controls bullet"><span class="by">lalalali</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489034">parent</a><span>|</span><a href="#42490229">prev</a><span>|</span><a href="#42490221">next</a><span>|</span><label class="collapse" for="c-42489405">[-]</label><label class="expand" for="c-42489405">[1 more]</label></div><br/><div class="children"><div class="content">What is open ai margin on that product?</div><br/></div></div></div></div><div id="42490221" class="c"><input type="checkbox" id="c-42490221" checked=""/><div class="controls bullet"><span class="by">rtsil</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486347">parent</a><span>|</span><a href="#42489034">prev</a><span>|</span><a href="#42486453">next</a><span>|</span><label class="collapse" for="c-42490221">[-]</label><label class="expand" for="c-42490221">[1 more]</label></div><br/><div class="children"><div class="content">Unless the quality of the human data are extraordinary, it seems according to the TFA that it&#x27;s not that easy:<p>&gt; The process is painfully slow. GPT-4 was trained on an estimated 13 trillion tokens. A thousand people writing 5,000 words a day would take months to produce a billion tokens.<p>And if the human-generated data was so qualitatively good that it is smaller by three order of magnitudes, than I can assume it would be at least as expensive as o3.</div><br/></div></div><div id="42486453" class="c"><input type="checkbox" id="c-42486453" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486347">parent</a><span>|</span><a href="#42490221">prev</a><span>|</span><a href="#42486640">next</a><span>|</span><label class="collapse" for="c-42486453">[-]</label><label class="expand" for="c-42486453">[4 more]</label></div><br/><div class="children"><div class="content">That’s an interesting idea. What if OpenAI funded medical research initiatives in exchange for exclusive training rights on the research.</div><br/><div id="42486485" class="c"><input type="checkbox" id="c-42486485" checked=""/><div class="controls bullet"><span class="by">onlyrealcuzzo</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486453">parent</a><span>|</span><a href="#42489504">next</a><span>|</span><label class="collapse" for="c-42486485">[-]</label><label class="expand" for="c-42486485">[2 more]</label></div><br/><div class="children"><div class="content">It would be orders of magnitude cheaper to outsource to humans.</div><br/><div id="42486507" class="c"><input type="checkbox" id="c-42486507" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486485">parent</a><span>|</span><a href="#42489504">next</a><span>|</span><label class="collapse" for="c-42486507">[-]</label><label class="expand" for="c-42486507">[1 more]</label></div><br/><div class="children"><div class="content">Not as sexy to investors though</div><br/></div></div></div></div><div id="42489504" class="c"><input type="checkbox" id="c-42489504" checked=""/><div class="controls bullet"><span class="by">aswegs8</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486453">parent</a><span>|</span><a href="#42486485">prev</a><span>|</span><a href="#42486640">next</a><span>|</span><label class="collapse" for="c-42489504">[-]</label><label class="expand" for="c-42489504">[1 more]</label></div><br/><div class="children"><div class="content">Wait didn&#x27;t they just recently request researchers to pair up with them in exchange for the data?</div><br/></div></div></div></div><div id="42486640" class="c"><input type="checkbox" id="c-42486640" checked=""/><div class="controls bullet"><span class="by">DougN7</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486347">parent</a><span>|</span><a href="#42486453">prev</a><span>|</span><a href="#42487795">next</a><span>|</span><label class="collapse" for="c-42486640">[-]</label><label class="expand" for="c-42486640">[2 more]</label></div><br/><div class="children"><div class="content">Someone needs to dress up Mechanical Turk and repackage it as an AI company…..</div><br/><div id="42486795" class="c"><input type="checkbox" id="c-42486795" checked=""/><div class="controls bullet"><span class="by">jitl</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486640">parent</a><span>|</span><a href="#42487795">next</a><span>|</span><label class="collapse" for="c-42486795">[-]</label><label class="expand" for="c-42486795">[1 more]</label></div><br/><div class="children"><div class="content">That’s basically every AI company that existed before GPT3</div><br/></div></div></div></div></div></div><div id="42487795" class="c"><input type="checkbox" id="c-42487795" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#42486273">parent</a><span>|</span><a href="#42486347">prev</a><span>|</span><a href="#42486458">next</a><span>|</span><label class="collapse" for="c-42487795">[-]</label><label class="expand" for="c-42487795">[8 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI’s next moat<p>I don&#x27;t think oai has any moat at all. If you look around, QwQ from Alibaba is already pushing o1-preview performances. I think oai is only ahead by 3~6 months at most.</div><br/><div id="42489864" class="c"><input type="checkbox" id="c-42489864" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42487795">parent</a><span>|</span><a href="#42486458">next</a><span>|</span><label class="collapse" for="c-42489864">[-]</label><label class="expand" for="c-42489864">[7 more]</label></div><br/><div class="children"><div class="content">If their AGI dreams would come true it might be more than enough to have 3 months head start. They probably won&#x27;t, but it&#x27;s interesting to ponder what the next few hours, days, weeks would be for someone that would wield AGI.<p>Like let&#x27;s say you have a few datacenters of compute at your disposal and the ability to instantiate millions of AGI agents - what do you have them do?<p>I wonder if the USA already has a secret program for this under national defense. But it is interesting that once you do control an actual AGI you&#x27;d want to speed-run a bunch of things. In opposition to that, how do you detect an adversary already has &#x2F; is using it and what to do in that case.</div><br/><div id="42489922" class="c"><input type="checkbox" id="c-42489922" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489864">parent</a><span>|</span><a href="#42486458">next</a><span>|</span><label class="collapse" for="c-42489922">[-]</label><label class="expand" for="c-42489922">[6 more]</label></div><br/><div class="children"><div class="content">How many important problems are there where a 3 month head start on the data side is enough to win permanently and retain your advantage in the long run?<p>I&#x27;m struggling to think of a scenario where &quot;I have AGI in January and everyone else has it in April&quot; is life-changing. It&#x27;s a win, for sure, and it&#x27;s an advantage, but success in business requires sustainable growth and manageable costs.<p>If (random example) the bargain OpenAI strikes is &quot;we spend every cent of our available capital to get AGI 3 months before the other guys do&quot; they&#x27;ve now tapped all the resources they would need to leverage AGI and turn it into profitable, scalable businesses, while the other guys can take it slow and arrive with full pockets. I don&#x27;t think their leadership is stupid enough to burn all their resources chasing AGI but it does seem like operating and training costs are an ongoing problem for them.<p>History is littered with first-movers who came up with something first and then failed to execute on it, only for someone else to follow up and actually turn the idea into a success. I don&#x27;t see any reason to assume that the &quot;first AGI&quot; is going to be the only successful AGI on the market, or even a success at all. Even if you&#x27;ve developed an AGI that can change the world you need to keep it running so it can do that.<p>Consider it this way: Sam Altman &amp; his ilk have been talking up how dangerous OpenAI&#x27;s technology is. Are risk-averse businessmen and politicians going to be lining up to put their livelihood or even their lives in the hands of &quot;dangerous technology&quot;? Or are they going to wait 3-6 months and adopt the &quot;safe&quot; AGI from somebody else instead?</div><br/><div id="42490321" class="c"><input type="checkbox" id="c-42490321" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489922">parent</a><span>|</span><a href="#42486458">next</a><span>|</span><label class="collapse" for="c-42490321">[-]</label><label class="expand" for="c-42490321">[5 more]</label></div><br/><div class="children"><div class="content">Well that&#x27;s the thought exercise. Is there something you can do with almost unlimited &quot;brains&quot; of roughly human capability but much faster, within a few days &#x2F; weeks &#x2F; months. Lets say you can instantiate 1 million agents, for 3 months, and each of them is roughly 100x faster than a human, that means you have the equivalent of 100 million human-brain-hours to dump into whatever you want, as long as your plans don&#x27;t require building too many real world things that actually require moving atoms around, I think you could do some interesting things. You could potentially dump a few million hours into &quot;better than AGI AI&quot; to start off for example, then go to other things. If they are good enough you might be able to find enough zero-days to disable any adversary through software, among other interesting things.</div><br/><div id="42490468" class="c"><input type="checkbox" id="c-42490468" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42490321">parent</a><span>|</span><a href="#42486458">next</a><span>|</span><label class="collapse" for="c-42490468">[-]</label><label class="expand" for="c-42490468">[4 more]</label></div><br/><div class="children"><div class="content">Where does &quot;almost unlimited&quot; come into the picture though? I see people talking like AGI will be unlimited when it will be limited by available compute resources, and like I suggested, being &#x27;first&#x27; might come at the cost of the war chest you&#x27;d need to access those resources.<p>What does it take to instantiate 1 million agents? Who has that kind of money and hardware? Would they still have it if they burn everything in the tank to be first?</div><br/><div id="42491167" class="c"><input type="checkbox" id="c-42491167" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42490468">parent</a><span>|</span><a href="#42491704">next</a><span>|</span><label class="collapse" for="c-42491167">[-]</label><label class="expand" for="c-42491167">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Where does &quot;almost unlimited&quot; come into the picture though<p>&gt;&gt; Like let&#x27;s say you have a few datacenters of compute at your disposal and the ability to instantiate millions of AGI agents - what do you have them do?<p>&gt;  has that kind of money and hardware?<p>Any hyperscaler plus most geopolitical main players. So the ones who matter.</div><br/></div></div><div id="42491704" class="c"><input type="checkbox" id="c-42491704" checked=""/><div class="controls bullet"><span class="by">pertymcpert</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42490468">parent</a><span>|</span><a href="#42491167">prev</a><span>|</span><a href="#42486458">next</a><span>|</span><label class="collapse" for="c-42491704">[-]</label><label class="expand" for="c-42491704">[2 more]</label></div><br/><div class="children"><div class="content">Once you have AGI you use it to collect resources to cripple competitors and to build a snowball effect to make yourself unbeatable. 3 months of AGI is enough in the right hands to dominate the world economically.</div><br/><div id="42492501" class="c"><input type="checkbox" id="c-42492501" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42491704">parent</a><span>|</span><a href="#42486458">next</a><span>|</span><label class="collapse" for="c-42492501">[-]</label><label class="expand" for="c-42492501">[1 more]</label></div><br/><div class="children"><div class="content">Only if the AGI is cheaper than a human, in the case the AGI is more expensive than a human there wont be any snowballing. And the most likely case is that the first AGI is more expensive to run than a human, a few months of having overly expensive human level AI bots wont disrupt the world at all.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42486458" class="c"><input type="checkbox" id="c-42486458" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">parent</a><span>|</span><a href="#42487795">prev</a><span>|</span><a href="#42486344">next</a><span>|</span><label class="collapse" for="c-42486458">[-]</label><label class="expand" for="c-42486458">[20 more]</label></div><br/><div class="children"><div class="content">I’m curious how, if at all, the plan to get around compounding bias in synthetic data generated by models trained in synthetic data.</div><br/><div id="42487921" class="c"><input type="checkbox" id="c-42487921" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486458">parent</a><span>|</span><a href="#42486524">next</a><span>|</span><label class="collapse" for="c-42487921">[-]</label><label class="expand" for="c-42487921">[2 more]</label></div><br/><div class="children"><div class="content">synthetic data is fine if you can ground the model somehow. that&#x27;s why the o1&#x2F;o3&#x27;s improvements are mostly in reasoning, maths, etc., because you can easily tell if the data is wrong or not.</div><br/><div id="42490560" class="c"><input type="checkbox" id="c-42490560" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42487921">parent</a><span>|</span><a href="#42486524">next</a><span>|</span><label class="collapse" for="c-42490560">[-]</label><label class="expand" for="c-42490560">[1 more]</label></div><br/><div class="children"><div class="content">That makes a lot of sense.<p>Binary success criteria has very little room for bias.</div><br/></div></div></div></div><div id="42486524" class="c"><input type="checkbox" id="c-42486524" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486458">parent</a><span>|</span><a href="#42487921">prev</a><span>|</span><a href="#42486344">next</a><span>|</span><label class="collapse" for="c-42486524">[-]</label><label class="expand" for="c-42486524">[17 more]</label></div><br/><div class="children"><div class="content">Everyone&#x27;s obsessed with new training tokens... It doesn&#x27;t need to be more knowledgeable, it just needs to practice more. Ask any student: practice is synthetic data.</div><br/><div id="42486554" class="c"><input type="checkbox" id="c-42486554" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486524">parent</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42486554">[-]</label><label class="expand" for="c-42486554">[15 more]</label></div><br/><div class="children"><div class="content">That leads to overfitting in ML land, which hurts overall performance.<p>We know that unique data improves performance.<p>These LLM systems are not students…<p>Also, which students graduate and are immediately experts in their fields? Almost none.<p>It takes years of practice in unique, often one-off, situations after graduation for most people to develop the intuition needed for a given field.</div><br/><div id="42486709" class="c"><input type="checkbox" id="c-42486709" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486554">parent</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42486709">[-]</label><label class="expand" for="c-42486709">[14 more]</label></div><br/><div class="children"><div class="content">It&#x27;s overfitting when you train too large a model on too many details. Rote memorization isn&#x27;t rewarding.<p>The more concepts the model manages to grok, the more nonlinear its capabilities will be: we don&#x27;t have a data problem, we have an educational one.<p>Claude 3.5 was safety trained by Claude 3.0, and it&#x27;s more coherent for it. <a href="https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;claudes-constitution" rel="nofollow">https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;claudes-constitution</a></div><br/><div id="42486977" class="c"><input type="checkbox" id="c-42486977" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486709">parent</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42486977">[-]</label><label class="expand" for="c-42486977">[13 more]</label></div><br/><div class="children"><div class="content">Overfitting can be caused by a lot of different things. Having an over abundance of one kind of data in a training set is one of those causes.<p>It’s why many pre-processing steps for image training pipelines will add copies of images at weird rotations, amounts of blur, and different cropping.<p>&gt; The more concepts the model manages to grok, the more nonlinear its capabilities will be<p>These kind of hand wavey statements like “practice,” “grok,” and “nonlinear its capabilities will be” are not very constructive as they don’t have solid meaning wrt language models.<p>So earlier when I was referring to compounding bias in synthetic data I was referring to a bias that gets trained on over and over and over again.<p>That leads to overfitting.</div><br/><div id="42487687" class="c"><input type="checkbox" id="c-42487687" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486977">parent</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42487687">[-]</label><label class="expand" for="c-42487687">[12 more]</label></div><br/><div class="children"><div class="content"><i>These kind of hand wavey statements like “practice,” “grok,” and “nonlinear its capabilities will be” are not very constructive as they don’t have solid meaning wrt language models.</i><p>So, here&#x27;s my hypothesis, as someone who is adjacent ML but haven&#x27;t trained DNNs directly:<p>We don&#x27;t understand how they work, because we didn&#x27;t build them. They built themselves.<p>At face value this can be seen as an almost spiritual position, but I am not a religious person and I don&#x27;t think there&#x27;s any magic involved. Unlike traditional models, the behavior of DNNs is based on random changes that failed up. We can reason about their structure, but only loosely about their functionality. When they get better at drawing, it isn&#x27;t because we taught them to draw. When they get better at reasoning, it isn&#x27;t because the engineers were better philosophers. Given this, there will not be a direct correlation between inputs and capabilities, but some arrangements do work better than others.<p>If this is the case, high order capabilities should continue to increase with training cycles, as long as they are performed in ways that don&#x27;t interfere with what has been successfully learned. People lamented the loss of capability that GPT 4 suffered as they increased safety. I think Anthropic has avoided this by choosing a less damaging way to tune a well performing model.<p>I think these ideas are supported by Wolfram&#x27;s reduction of the problem at <a href="https:&#x2F;&#x2F;writings.stephenwolfram.com&#x2F;2024&#x2F;08&#x2F;whats-really-going-on-in-machine-learning-some-minimal-models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;writings.stephenwolfram.com&#x2F;2024&#x2F;08&#x2F;whats-really-goi...</a></div><br/><div id="42488387" class="c"><input type="checkbox" id="c-42488387" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42487687">parent</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42488387">[-]</label><label class="expand" for="c-42488387">[11 more]</label></div><br/><div class="children"><div class="content">Your whole argument falls apart at<p>&gt; We don&#x27;t understand how they work, because we didn&#x27;t build them. They built themselves.<p>We do understand how they work, we did build them.
The mathematical foundation of these models are sound. The statistics behind them are well understood.<p>What we don’t exactly know is which parameters correspond to what results as it’s different across models.<p>We work backwards to see which parts of the network seem to relate to what outcomes.<p>&gt; When they get better at drawing, it isn&#x27;t because we taught them to draw. When they get better at reasoning, it isn&#x27;t because the engineers were better philosophers.<p>Isn’t this the exact opposite of reality?<p>They get better at drawing because we improve their datasets, topologies, and their training methods and in doing so, teach them to draw.<p>They get better at reasoning because the engineers and data scientists building training sets do get better at philosophy.<p>They study what reasoning is and apply those learnings to the datasets and training methods.<p>That’s how CoT came about early on.</div><br/><div id="42490335" class="c"><input type="checkbox" id="c-42490335" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42488387">parent</a><span>|</span><a href="#42491960">next</a><span>|</span><label class="collapse" for="c-42490335">[-]</label><label class="expand" for="c-42490335">[5 more]</label></div><br/><div class="children"><div class="content">&gt; We do understand how they work, we did build them. The mathematical foundation of these models are sound. The statistics behind them are well understood.<p>We don&#x27;t understand how they work in the sense that we can&#x27;t extract the algorithms they&#x27;re using to accomplish the interesting&#x2F;valuable &quot;intellectual&quot; labor they&#x27;re doing.  i.e. we cannot take GPT-4 and write human-legible code that faithfully represents the &quot;heavy lifting&quot; GPT-4 does when it writes code (or pick any other task you might ask it to do).<p>That inability makes it difficult to reliably predict when they&#x27;ll fail, how to improve them in specific ways, etc.<p>The only way in which we &quot;understand&quot; them is that we understand the training process which created them (and even that&#x27;s limited to reproducible open-source models), which is about as accurate as saying that we &quot;understand&quot; human cognition because we know about evolution.  In reality, we understand very little about human cognition, certainly not enough to reliably reproduce it in silico or intervene on it without a bunch of very expensive (and failure-prone) trial-and-error.</div><br/><div id="42490550" class="c"><input type="checkbox" id="c-42490550" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42490335">parent</a><span>|</span><a href="#42491960">next</a><span>|</span><label class="collapse" for="c-42490550">[-]</label><label class="expand" for="c-42490550">[4 more]</label></div><br/><div class="children"><div class="content">&gt; We don&#x27;t understand how they work in the sense that we can&#x27;t extract the algorithms they&#x27;re using to accomplish the interesting&#x2F;valuable &quot;intellectual&quot; labor they&#x27;re doing. i.e. we cannot take GPT-4 and write human-legible code that faithfully represents the &quot;heavy lifting&quot; GPT-4 does when it writes code (or pick any other task you might ask it to do).<p>I think English is being a little clumsy here. At least I’m finding it hard to express what we do and don’t know.<p>We know why these models work. We know precisely how, physically, they come to their conclusions (it’s just processor instructions as with all software)<p>We don’t know precisely how to describe what they do in a formalized general way.<p>That is still very different from say an organic brain, where we barely even know how it works, physically.<p>My opinions:<p>I don’t think they are doing much mental “labor.” My intuition likens them to search.<p>They seem to excel at retrieving information encoded in their weights through training and in the context.<p>They are not good at generalizing.<p>They also, obviously, are able to accurately predict tokens such that the resulting text is very readable.<p>Larger models have a larger pool of information and that information is in a higher resolution, so to speak, since the larger better preforming models have more parameters.<p>I think much of this talk of “consciousness” or “AGI” is very much a product of human imagination, personification bias, and marketing.</div><br/><div id="42490925" class="c"><input type="checkbox" id="c-42490925" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42490550">parent</a><span>|</span><a href="#42491960">next</a><span>|</span><label class="collapse" for="c-42490925">[-]</label><label class="expand" for="c-42490925">[3 more]</label></div><br/><div class="children"><div class="content">&gt;We know why these models work. We know precisely how, physically, they come to their conclusions (it’s just processor instructions as with all software)<p>I don&#x27;t know why you would classify this as knowing much of anything. Processor instructions ? Really?<p>If the average user is given unfettered access to the entire source code of his&#x2F;her favorite app, does he suddenly understand it ? That seems like a ridiculous assertion.<p>In reality, it&#x27;s even worse. We can&#x27;t pinpoint what weights, how and in what ways and instances are contributing exactly to  basic things like whether a word should be preceded by &#x27;the&#x27; or &#x27;a&#x27; and it only gets more intractable as models get bigger and bigger.<p>Sure, you could probably say we understand these NNs better than brains but it&#x27;s not by much at all.</div><br/><div id="42491112" class="c"><input type="checkbox" id="c-42491112" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42490925">parent</a><span>|</span><a href="#42491960">next</a><span>|</span><label class="collapse" for="c-42491112">[-]</label><label class="expand" for="c-42491112">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If the average user is given unfettered access to the entire source code of his&#x2F;her favorite app, does he suddenly understand it ? That seems like a ridiculous assertion.<p>And one that I didn’t make.<p>I don’t think when we say “we understand” we’re talking about your average Joe.<p>I mean “we” as in all of human knowledge.<p>&gt; We can&#x27;t pinpoint what weights, how and in what ways and instances are contributing exactly to basic things like whether a word should be preceded by &#x27;the&#x27; or &#x27;a&#x27; and it only gets more intractable as models get bigger and bigger.<p>There is research coming out on this subject. I read a paper recently about how llama’s weights seemed to be grouped by concept like “president” or “actors.”<p>But just the fact that we know that information encoded in weights affects outcomes and we know the underlying mechanisms involved in the creation of those weights and the execution of the model shows that we know much more about how they work than an organic brain.<p>The whole organic brain thing is kind of a tangent anyway.<p>My point is that it’s not correct to say that we don’t know how these systems work. We do. It’s not voodoo.<p>We just don’t have a high level understanding of the form in which information is encoded in the weights of any given model.</div><br/><div id="42491352" class="c"><input type="checkbox" id="c-42491352" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42491112">parent</a><span>|</span><a href="#42491960">next</a><span>|</span><label class="collapse" for="c-42491352">[-]</label><label class="expand" for="c-42491352">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If the average user is given unfettered access to the entire source code of his&#x2F;her favorite app, does he suddenly understand it ? That seems like a ridiculous assertion.
And one that I didn’t make.
I don’t think when we say “we understand” we’re talking about your average Joe.
I mean “we” as in all of human knowledge.<p>It&#x27;s an analogy. In understanding weights, even the best researchers are basically like the untrained average joe with source code.<p>&gt;There is research coming out on this subject. I read a paper recently about how llama’s weights seemed to be grouped by concept like “president” or “actors.”<p>&gt;But just the fact that we know that information encoded in weights affects outcomes and we know the underlying mechanisms involved in the creation of those weights and the execution of the model shows that we know much more about how they work than an organic brain.<p>I guess i just don&#x27;t see how &quot;information is encoded in the weights&quot; is some great understanding ? It&#x27;s as vague and un-actionable as you can get.<p>For training, the whole revolution of back-propagation and NNs in general is that we found a way to reinforce the right connections without knowing anything about how to form them or even what they actually are.<p>We no longer needed to understand how eyes detect objects to build an object detecting model. None of that knowledge suddenly poofed into our heads. Back-propagation is basically &quot;reinforce whatever layers are closer to the right answer&quot;. Extremely powerful but useless for understanding.<p>Knowing the Transformer architecture unfortunately tells you very little about what a trained model is actually learning during training and what it has actually learnt.<p>&quot;Information is encoded in a brain&#x27;s neurons and this affects our actions&quot;. 
Literally nothing useful you can do with this information. That&#x27;s why models need to be trained to fix even little issues.<p>If you want to say we understand models better than the brain then sure but you are severely overestimating how much that &quot;better&quot; is.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42491960" class="c"><input type="checkbox" id="c-42491960" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42488387">parent</a><span>|</span><a href="#42490335">prev</a><span>|</span><a href="#42489825">next</a><span>|</span><label class="collapse" for="c-42491960">[-]</label><label class="expand" for="c-42491960">[1 more]</label></div><br/><div class="children"><div class="content">The thing that you are handwaving away as just &quot;which parameters correspond to what results&quot; is precisely the important, the inexorable thing which defines the phenomena, and it is exactly the thing which we don&#x27;t have access to, and which we did not and <i>could not</i> design, plan or engineer, but which emerged</div><br/></div></div><div id="42489825" class="c"><input type="checkbox" id="c-42489825" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42488387">parent</a><span>|</span><a href="#42491960">prev</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42489825">[-]</label><label class="expand" for="c-42489825">[4 more]</label></div><br/><div class="children"><div class="content">Please, read the Wolfram blog</div><br/><div id="42490287" class="c"><input type="checkbox" id="c-42490287" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489825">parent</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42490287">[-]</label><label class="expand" for="c-42490287">[3 more]</label></div><br/><div class="children"><div class="content">I gave it a fair skim, but I didn’t really feel like it refuted what I said.<p>Is there a specific section that comes to mind?</div><br/><div id="42490669" class="c"><input type="checkbox" id="c-42490669" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42490287">parent</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42490669">[-]</label><label class="expand" for="c-42490669">[2 more]</label></div><br/><div class="children"><div class="content">Other than we don&#x27;t tell it how to get the right answer, or understand how it eventually computes correct answers?</div><br/><div id="42491156" class="c"><input type="checkbox" id="c-42491156" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42490669">parent</a><span>|</span><a href="#42489671">next</a><span>|</span><label class="collapse" for="c-42491156">[-]</label><label class="expand" for="c-42491156">[1 more]</label></div><br/><div class="children"><div class="content">I don’t really think you’re understanding my argument…</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42489671" class="c"><input type="checkbox" id="c-42489671" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486524">parent</a><span>|</span><a href="#42486554">prev</a><span>|</span><a href="#42486344">next</a><span>|</span><label class="collapse" for="c-42489671">[-]</label><label class="expand" for="c-42489671">[1 more]</label></div><br/><div class="children"><div class="content">And who will tell the model whether its practice results are correct or not? Students practice against external evaluators, it’s not a self-contained system.</div><br/></div></div></div></div></div></div><div id="42486344" class="c"><input type="checkbox" id="c-42486344" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42486273">parent</a><span>|</span><a href="#42486458">prev</a><span>|</span><a href="#42486630">next</a><span>|</span><label class="collapse" for="c-42486344">[-]</label><label class="expand" for="c-42486344">[3 more]</label></div><br/><div class="children"><div class="content">&gt; With o3 now public knowledge, imagine how long it’s been churning out new thinking at expert level across every field. OpenAI’s next moat may be the best synthetic training set ever.<p>Even taking OpenAI and the benchmark authors at their word they said that it is consuming at least tens of dollars per task to hit peak performance, how much would it cost to have it produce a meaningfully large training set?</div><br/><div id="42486355" class="c"><input type="checkbox" id="c-42486355" checked=""/><div class="controls bullet"><span class="by">qup</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486344">parent</a><span>|</span><a href="#42486630">next</a><span>|</span><label class="collapse" for="c-42486355">[-]</label><label class="expand" for="c-42486355">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the public API price isn&#x27;t it?</div><br/><div id="42486366" class="c"><input type="checkbox" id="c-42486366" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486355">parent</a><span>|</span><a href="#42486630">next</a><span>|</span><label class="collapse" for="c-42486366">[-]</label><label class="expand" for="c-42486366">[1 more]</label></div><br/><div class="children"><div class="content">There is no public API for o3 yet, those are the numbers they revealed in the ARC-AGI announcement. Even if they were public API prices we can&#x27;t assume they&#x27;re making a profit on those for as long as they&#x27;re billions in the red overall every year, its entirely possible that the public API prices are <i>less</i> than what OpenAI is actually paying.</div><br/></div></div></div></div></div></div><div id="42486630" class="c"><input type="checkbox" id="c-42486630" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#42486273">parent</a><span>|</span><a href="#42486344">prev</a><span>|</span><a href="#42488731">next</a><span>|</span><label class="collapse" for="c-42486630">[-]</label><label class="expand" for="c-42486630">[12 more]</label></div><br/><div class="children"><div class="content">I completely don&#x27;t understand the use for synthetic data. What good it&#x27;s it to train a model basically on itself?</div><br/><div id="42491731" class="c"><input type="checkbox" id="c-42491731" checked=""/><div class="controls bullet"><span class="by">Majromax</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486630">parent</a><span>|</span><a href="#42486689">next</a><span>|</span><label class="collapse" for="c-42491731">[-]</label><label class="expand" for="c-42491731">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What good it&#x27;s it to train a model basically on itself?<p>If the model generates data of variable quality, and if there&#x27;s a good way to distinguish good data from bad data, then training on self-generated data might &quot;bootstrap&quot; a model to better performance.<p>This is common in reinforcement learning.  Famously, AlphaGo Zero (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;AlphaGo_Zero" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;AlphaGo_Zero</a>) learned exclusively on self-play, without reference to human-played games.<p>Of course, games have a built-in critic: the better strategy usually wins.  It&#x27;s much harder to judge the answer to a math problem, or decide which essay is more persuasive, or evaluate restaurant recommendations.</div><br/></div></div><div id="42486689" class="c"><input type="checkbox" id="c-42486689" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486630">parent</a><span>|</span><a href="#42491731">prev</a><span>|</span><a href="#42489643">next</a><span>|</span><label class="collapse" for="c-42486689">[-]</label><label class="expand" for="c-42486689">[6 more]</label></div><br/><div class="children"><div class="content">The value of synthetic data relies on having non-zero signal about which generated data is &quot;better&quot; or &quot;worse&quot;. In a sense, this what reinforcement learning is about. Ie, generate some data, have that data scored by some evaluator, and then feed the data back into the model with higher weight on the better stuff and lower weight on the worse stuff.<p>The basic loop is: (i) generate synthetic data, (ii) rate synthetic data, (iii) update model to put more probability on better data and less probability on worse data, then go back to (i).</div><br/><div id="42489435" class="c"><input type="checkbox" id="c-42489435" checked=""/><div class="controls bullet"><span class="by">RedNifre</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486689">parent</a><span>|</span><a href="#42487297">next</a><span>|</span><label class="collapse" for="c-42489435">[-]</label><label class="expand" for="c-42489435">[4 more]</label></div><br/><div class="children"><div class="content">But who rates the synthetic data? If it is humans, I can understand that this is another way to get human knowledge into it, but if it&#x27;s rated by AI, isn&#x27;t it just a convoluted way of copying the rating AI&#x27;s knowledge?</div><br/><div id="42489756" class="c"><input type="checkbox" id="c-42489756" checked=""/><div class="controls bullet"><span class="by">recursivecaveat</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489435">parent</a><span>|</span><a href="#42489724">next</a><span>|</span><label class="collapse" for="c-42489756">[-]</label><label class="expand" for="c-42489756">[1 more]</label></div><br/><div class="children"><div class="content">Many things are more easily scored than produced. Like it&#x27;s trivial to tell whether a poem rhymes, but writing one is a comparatively slow and difficult task. So hopefully since scoring is easier&#x2F;more-discerning than generating, the idea is you can generate stuff, classify it as good or bad, and then retrain on the good stuff. It&#x27;s kindof an article of faith for a lot of AI companies&#x2F;professionals as well, since it prevents you from having to face a data wall, and is analogous to a human student practicing and learning in an appealing way.<p>As far as I know it doesn&#x27;t work very well so far. It is prone to overfitting, where it ranks highly some trivial detail of the output eg &quot;if a summary starts with a byline of the author its a sign of quality&quot; and then starts looping on itself over and over, increasing the frequency and size of bylines until it&#x27;s totally crommed off to infinity and just repeating a short phrase endlessly. Humans have good baselines and common sense that these ML systems lack, if you&#x27;ve ever seen one of those &quot;deep dream&quot; images it&#x27;s the same kind of idea. The &quot;most possible dog&quot; image can be looks almost nothing like a dog in the same way that the &quot;most possible poem&quot; may look nothing like a poem.</div><br/></div></div><div id="42489724" class="c"><input type="checkbox" id="c-42489724" checked=""/><div class="controls bullet"><span class="by">ijustlovemath</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489435">parent</a><span>|</span><a href="#42489756">prev</a><span>|</span><a href="#42487297">next</a><span>|</span><label class="collapse" for="c-42489724">[-]</label><label class="expand" for="c-42489724">[2 more]</label></div><br/><div class="children"><div class="content">This is the bit I&#x27;ve never understood about training AI on its own output; won&#x27;t you just regress to the mean?</div><br/><div id="42492648" class="c"><input type="checkbox" id="c-42492648" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489724">parent</a><span>|</span><a href="#42487297">next</a><span>|</span><label class="collapse" for="c-42492648">[-]</label><label class="expand" for="c-42492648">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not trained on its own output. You can generate infinite correctly worked out math traces and train on those.</div><br/></div></div></div></div></div></div><div id="42487297" class="c"><input type="checkbox" id="c-42487297" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486689">parent</a><span>|</span><a href="#42489435">prev</a><span>|</span><a href="#42489643">next</a><span>|</span><label class="collapse" for="c-42487297">[-]</label><label class="expand" for="c-42487297">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, that makes a lot more sense.</div><br/></div></div></div></div><div id="42489643" class="c"><input type="checkbox" id="c-42489643" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486630">parent</a><span>|</span><a href="#42486689">prev</a><span>|</span><a href="#42491795">next</a><span>|</span><label class="collapse" for="c-42489643">[-]</label><label class="expand" for="c-42489643">[3 more]</label></div><br/><div class="children"><div class="content">This is a good read for some examples <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.14465" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.14465</a><p>&gt; This technique, the &quot;Self-Taught Reasoner&quot; (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers<p>But there are a few others. In general good data is good data. We&#x27;re definitely learning more about how to produce good synthetic version.</div><br/><div id="42489865" class="c"><input type="checkbox" id="c-42489865" checked=""/><div class="controls bullet"><span class="by">im3w1l</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42489643">parent</a><span>|</span><a href="#42489841">next</a><span>|</span><label class="collapse" for="c-42489865">[-]</label><label class="expand" for="c-42489865">[1 more]</label></div><br/><div class="children"><div class="content">One issue with that is that the model may learn to smuggle data. You as a human think that the plain reading of the words is what is doing the reasoning, but (part of) the processing is done by the exact comma placement and synonym choice etc.<p>Data smuggling is a known phenomenon in similar tasks.</div><br/></div></div></div></div><div id="42491795" class="c"><input type="checkbox" id="c-42491795" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42486630">parent</a><span>|</span><a href="#42489643">prev</a><span>|</span><a href="#42488731">next</a><span>|</span><label class="collapse" for="c-42491795">[-]</label><label class="expand" for="c-42491795">[1 more]</label></div><br/><div class="children"><div class="content">If we get to a point where we have a model that when fed a real world stream of data (YouTube, surveillance cameras, forum data, cell phone conversations etc.) and can prune out a good training set for itself then you’re at the point where the LLM is in a feedback loop where it can improve itself. That’s AGI for all intents and purposes.</div><br/></div></div></div></div><div id="42488731" class="c"><input type="checkbox" id="c-42488731" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#42486273">parent</a><span>|</span><a href="#42486630">prev</a><span>|</span><a href="#42489478">next</a><span>|</span><label class="collapse" for="c-42488731">[-]</label><label class="expand" for="c-42488731">[2 more]</label></div><br/><div class="children"><div class="content">There is an enormous &quot;iceberg&quot; of untapped non-public data locked behind paywalls or licensing agreements. The next frontier will be spending money and human effort to get access to that data, then transform it into something useful for training.</div><br/><div id="42491986" class="c"><input type="checkbox" id="c-42491986" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#42486273">root</a><span>|</span><a href="#42488731">parent</a><span>|</span><a href="#42489478">next</a><span>|</span><label class="collapse" for="c-42491986">[-]</label><label class="expand" for="c-42491986">[1 more]</label></div><br/><div class="children"><div class="content">ah yes the beautiful iceberg of internal documentation, legal paperwork, and meeting notes.<p>the highest quality language data that exists is in the public domain</div><br/></div></div></div></div></div></div><div id="42489478" class="c"><input type="checkbox" id="c-42489478" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42486273">prev</a><span>|</span><a href="#42492231">next</a><span>|</span><label class="collapse" for="c-42489478">[-]</label><label class="expand" for="c-42489478">[66 more]</label></div><br/><div class="children"><div class="content">&quot;OpenAI’s is called GPT-4, the fourth LLM the company has developed since its 2015 founding.&quot; - that sentence doesn&#x27;t fill me with confidence in the quality of the rest of the article, sadly.</div><br/><div id="42489694" class="c"><input type="checkbox" id="c-42489694" checked=""/><div class="controls bullet"><span class="by">jacobsimon</span><span>|</span><a href="#42489478">parent</a><span>|</span><a href="#42490552">next</a><span>|</span><label class="collapse" for="c-42489694">[-]</label><label class="expand" for="c-42489694">[45 more]</label></div><br/><div class="children"><div class="content">There’s nothing grammatically offensive about this. It’s like saying, “Cars come in all colors. Mine is red.”</div><br/><div id="42489718" class="c"><input type="checkbox" id="c-42489718" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489694">parent</a><span>|</span><a href="#42489729">next</a><span>|</span><label class="collapse" for="c-42489718">[-]</label><label class="expand" for="c-42489718">[41 more]</label></div><br/><div class="children"><div class="content">No, I&#x27;m complaining that just because GPT-4 is called GPT-4 doesn&#x27;t mean it&#x27;s the fourth LLM from OpenAI.<p>Off the top of my head: GPT-2, Codex, GPT-3 in three different flavors (babbage, curie, davinci), GPT-3.5.<p>Suggesting that GPT-4 was &quot;fourth&quot; simply isn&#x27;t credible.<p>Just the other day they announced a jump from o1 to o3, skipping o2 purely because it&#x27;s already the name of a major telecommunications brand in Europe. Deriving anything from the names of OpenAI&#x27;s products doesn&#x27;t make sense.</div><br/><div id="42489800" class="c"><input type="checkbox" id="c-42489800" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489718">parent</a><span>|</span><a href="#42490783">next</a><span>|</span><label class="collapse" for="c-42489800">[-]</label><label class="expand" for="c-42489800">[5 more]</label></div><br/><div class="children"><div class="content">While I’m sure it’s unintentional, that amounts to nitpicking. I can easily find three to include and pass over the rest. Face value turns out to be a decent approximation.</div><br/><div id="42490082" class="c"><input type="checkbox" id="c-42490082" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489800">parent</a><span>|</span><a href="#42490783">next</a><span>|</span><label class="collapse" for="c-42490082">[-]</label><label class="expand" for="c-42490082">[4 more]</label></div><br/><div class="children"><div class="content">If this was a random blog post I wouldn&#x27;t nitpick, but this is the Wall Street Journal.</div><br/><div id="42490120" class="c"><input type="checkbox" id="c-42490120" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490082">parent</a><span>|</span><a href="#42490783">next</a><span>|</span><label class="collapse" for="c-42490120">[-]</label><label class="expand" for="c-42490120">[3 more]</label></div><br/><div class="children"><div class="content">The thing is that I think it could be an optimal way of saying it. Should we not put it into context of making a particular LLM? Why count three versions of three LLMs? They made it hard to choose the one that makes up for not having GPT 1. GPT 3.5 and Codex are both good candidates. And of course calling GPT 4 the third and fifth could be considered as well.</div><br/><div id="42490165" class="c"><input type="checkbox" id="c-42490165" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490120">parent</a><span>|</span><a href="#42490783">next</a><span>|</span><label class="collapse" for="c-42490165">[-]</label><label class="expand" for="c-42490165">[2 more]</label></div><br/><div class="children"><div class="content">&quot;OpenAI&#x27;s fourth family of LLMs&quot; or &quot;fourth generation of LLMs&quot; would work for me.</div><br/><div id="42490195" class="c"><input type="checkbox" id="c-42490195" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490165">parent</a><span>|</span><a href="#42490783">next</a><span>|</span><label class="collapse" for="c-42490195">[-]</label><label class="expand" for="c-42490195">[1 more]</label></div><br/><div class="children"><div class="content">That doesn’t resolve the problem of whether third or fifth is better than fourth. I have yet to be convinced that their wording here shows that they fail to grasp the pace of the development.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42490783" class="c"><input type="checkbox" id="c-42490783" checked=""/><div class="controls bullet"><span class="by">bluelightning2k</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489718">parent</a><span>|</span><a href="#42489800">prev</a><span>|</span><a href="#42489999">next</a><span>|</span><label class="collapse" for="c-42490783">[-]</label><label class="expand" for="c-42490783">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s at least 4 major releases just in GPT4.<p>GPT4, GPT4T, Gpt4o-Mini, GPT4o,</div><br/><div id="42491230" class="c"><input type="checkbox" id="c-42491230" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490783">parent</a><span>|</span><a href="#42489999">next</a><span>|</span><label class="collapse" for="c-42491230">[-]</label><label class="expand" for="c-42491230">[1 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re generous the article considers versions that were significant improvements. 4o is hardly better on real-world usage (benchmarks are gamed to death) than the original 4.</div><br/></div></div></div></div><div id="42489999" class="c"><input type="checkbox" id="c-42489999" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489718">parent</a><span>|</span><a href="#42490783">prev</a><span>|</span><a href="#42489810">next</a><span>|</span><label class="collapse" for="c-42489999">[-]</label><label class="expand" for="c-42489999">[31 more]</label></div><br/><div class="children"><div class="content">It’s somehow funny to hear a British company being described as ‘in Europe’, but I suppose you’re technically correct…</div><br/><div id="42490185" class="c"><input type="checkbox" id="c-42490185" checked=""/><div class="controls bullet"><span class="by">plufz</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489999">parent</a><span>|</span><a href="#42490541">next</a><span>|</span><label class="collapse" for="c-42490185">[-]</label><label class="expand" for="c-42490185">[12 more]</label></div><br/><div class="children"><div class="content">Technically…? Does anyone here believe that the EU and Europe is the same thing? Would you find it weird if someone said that a Norwegian company was in Europe?</div><br/><div id="42490241" class="c"><input type="checkbox" id="c-42490241" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490185">parent</a><span>|</span><a href="#42490541">next</a><span>|</span><label class="collapse" for="c-42490241">[-]</label><label class="expand" for="c-42490241">[11 more]</label></div><br/><div class="children"><div class="content">Many people certainly seem to! And it annoys me. I wasn’t talking about the EU, though.<p>I was just commenting on the fact that in the UK, ‘Europe’ generally means ‘continental Europe’.<p>&gt; Would you find it weird if someone said that a Norwegian company was in Europe?<p>I’d find it weird if a European did. But from Americans it’s to be expected.</div><br/><div id="42491244" class="c"><input type="checkbox" id="c-42491244" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490241">parent</a><span>|</span><a href="#42490292">next</a><span>|</span><label class="collapse" for="c-42491244">[-]</label><label class="expand" for="c-42491244">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Would you find it weird if someone said that a Norwegian company was in Europe?<p>&gt; I’d find it weird if a European did. But from Americans it’s to be expected.<p>Absolutely nothing weird about it, I&#x27;d find it very weird if they wouldn&#x27;t. I&#x27;m from Europe and my social circle has people from all over Europe.<p>It&#x27;s really just the UK which has this weird usage of Europe.</div><br/></div></div><div id="42490292" class="c"><input type="checkbox" id="c-42490292" checked=""/><div class="controls bullet"><span class="by">RandomThoughts3</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490241">parent</a><span>|</span><a href="#42491244">prev</a><span>|</span><a href="#42490434">next</a><span>|</span><label class="collapse" for="c-42490292">[-]</label><label class="expand" for="c-42490292">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I’d find it weird if a European did.<p>I have bad news. The UK is definitely in Europe both geographically and even more so historically and culturally. Norway is too by the way.<p>If you are offended by people referring to the UK as in Europe, my suggestion is both an history course and starting therapy.</div><br/><div id="42490972" class="c"><input type="checkbox" id="c-42490972" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490292">parent</a><span>|</span><a href="#42490495">next</a><span>|</span><label class="collapse" for="c-42490972">[-]</label><label class="expand" for="c-42490972">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d suggest you level up your reading comprehension before suggesting the parent poster was in any way offended or in need of therapy.</div><br/><div id="42491178" class="c"><input type="checkbox" id="c-42491178" checked=""/><div class="controls bullet"><span class="by">RandomThoughts3</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490972">parent</a><span>|</span><a href="#42490495">next</a><span>|</span><label class="collapse" for="c-42491178">[-]</label><label class="expand" for="c-42491178">[1 more]</label></div><br/><div class="children"><div class="content">Parent is suggesting it would be weird for Europeans to call the UK as in Europe which as a European I can tell you is preposterous. That’s the kind of non sense you used to hear from Brexiter. They will have no sympathy from me.</div><br/></div></div></div></div><div id="42490495" class="c"><input type="checkbox" id="c-42490495" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490292">parent</a><span>|</span><a href="#42490972">prev</a><span>|</span><a href="#42490434">next</a><span>|</span><label class="collapse" for="c-42490495">[-]</label><label class="expand" for="c-42490495">[1 more]</label></div><br/><div class="children"><div class="content">No no no you missed it, clearly Americans are just stupid.</div><br/></div></div></div></div><div id="42490434" class="c"><input type="checkbox" id="c-42490434" checked=""/><div class="controls bullet"><span class="by">thrwthsnw</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490241">parent</a><span>|</span><a href="#42490292">prev</a><span>|</span><a href="#42490814">next</a><span>|</span><label class="collapse" for="c-42490434">[-]</label><label class="expand" for="c-42490434">[1 more]</label></div><br/><div class="children"><div class="content">Which Americans, North or South?</div><br/></div></div><div id="42490814" class="c"><input type="checkbox" id="c-42490814" checked=""/><div class="controls bullet"><span class="by">boomskats</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490241">parent</a><span>|</span><a href="#42490434">prev</a><span>|</span><a href="#42490428">next</a><span>|</span><label class="collapse" for="c-42490814">[-]</label><label class="expand" for="c-42490814">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I was just commenting on the fact that in the UK, ‘Europe’ generally means ‘continental Europe’.<p>It really depends on who you&#x27;re speaking to.</div><br/><div id="42490938" class="c"><input type="checkbox" id="c-42490938" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490814">parent</a><span>|</span><a href="#42490428">next</a><span>|</span><label class="collapse" for="c-42490938">[-]</label><label class="expand" for="c-42490938">[1 more]</label></div><br/><div class="children"><div class="content">And on the context</div><br/></div></div></div></div><div id="42490428" class="c"><input type="checkbox" id="c-42490428" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490241">parent</a><span>|</span><a href="#42490814">prev</a><span>|</span><a href="#42490541">next</a><span>|</span><label class="collapse" for="c-42490428">[-]</label><label class="expand" for="c-42490428">[2 more]</label></div><br/><div class="children"><div class="content">If Norway isn’t in Europe where is it? Asia?</div><br/><div id="42491620" class="c"><input type="checkbox" id="c-42491620" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490428">parent</a><span>|</span><a href="#42490541">next</a><span>|</span><label class="collapse" for="c-42491620">[-]</label><label class="expand" for="c-42491620">[1 more]</label></div><br/><div class="children"><div class="content">Well, Europe is a subcontinent of Asia.  A bit like India or Arabia.</div><br/></div></div></div></div></div></div></div></div><div id="42490541" class="c"><input type="checkbox" id="c-42490541" checked=""/><div class="controls bullet"><span class="by">SahAssar</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489999">parent</a><span>|</span><a href="#42490185">prev</a><span>|</span><a href="#42491254">next</a><span>|</span><label class="collapse" for="c-42490541">[-]</label><label class="expand" for="c-42490541">[13 more]</label></div><br/><div class="children"><div class="content">The UK is part of Europe. It&#x27;s technically, geographically, politically, historically, lingustially, tectonically and socially correct. In what ways is it not?</div><br/><div id="42490628" class="c"><input type="checkbox" id="c-42490628" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490541">parent</a><span>|</span><a href="#42491254">next</a><span>|</span><label class="collapse" for="c-42490628">[-]</label><label class="expand" for="c-42490628">[12 more]</label></div><br/><div class="children"><div class="content">Are Cuba or Haiti part of North America? A lot of British people feel like their civilization is meaningfully distinct from “Europe”, even though they’re part of it in a technical geographical sense.</div><br/><div id="42490784" class="c"><input type="checkbox" id="c-42490784" checked=""/><div class="controls bullet"><span class="by">SahAssar</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490628">parent</a><span>|</span><a href="#42491254">next</a><span>|</span><label class="collapse" for="c-42490784">[-]</label><label class="expand" for="c-42490784">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Are Cuba or Haiti part of North America<p>In general yes, but it depends on if you consider central america as its own continent and if you include them there and how you delineate north&#x2F;south america. Groupings differ based on your education.<p>I think the thing that makes the UK different is that there is no other option besides them being a separate thing&#x2F;continent. Are you suggesting that the UK is it&#x27;s own continent? Would that be with the faroese and the Greenlanders?<p>The UK might feel different, but they are not separate. The french feel different from the bulgarians, but that does not mean they are on a separate continent, politically or geographically.<p>EDIT:<p>&gt; A lot of British people feel like their civilization is meaningfully distinct<p>This is, to borrow a word, &quot;balderdash&quot;. Looking at the influence vikings, romans and normans have had that is a rubbish argument. Just like other countries in europe the british culture is built on the stones of other cultures, and just like many other countries they subsumed other cultures because of kings or other political dominance.</div><br/><div id="42490986" class="c"><input type="checkbox" id="c-42490986" checked=""/><div class="controls bullet"><span class="by">reshlo</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490784">parent</a><span>|</span><a href="#42490862">next</a><span>|</span><label class="collapse" for="c-42490986">[-]</label><label class="expand" for="c-42490986">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Would that be with the Faroese and the Greenlanders?<p>Greenland is in North America.</div><br/><div id="42491124" class="c"><input type="checkbox" id="c-42491124" checked=""/><div class="controls bullet"><span class="by">SahAssar</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490986">parent</a><span>|</span><a href="#42490862">next</a><span>|</span><label class="collapse" for="c-42491124">[-]</label><label class="expand" for="c-42491124">[2 more]</label></div><br/><div class="children"><div class="content">The point was that any closeby landmass besides europe is either in europe or in north america, and I have a hard time seeing the argument for UK being in North America or America at all.<p>France would have a better argument for it having territory in both north (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Saint_Pierre_and_Miquelon" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Saint_Pierre_and_Miquelon</a> and others) and south (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;French_Guiana" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;French_Guiana</a> and others) america.</div><br/><div id="42491641" class="c"><input type="checkbox" id="c-42491641" checked=""/><div class="controls bullet"><span class="by">reshlo</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42491124">parent</a><span>|</span><a href="#42490862">next</a><span>|</span><label class="collapse" for="c-42491641">[-]</label><label class="expand" for="c-42491641">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I agree. Especially about Saint Pierre and Miquelon.[0]<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41758856#41785534">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41758856#41785534</a></div><br/></div></div></div></div></div></div><div id="42490862" class="c"><input type="checkbox" id="c-42490862" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490784">parent</a><span>|</span><a href="#42490986">prev</a><span>|</span><a href="#42491254">next</a><span>|</span><label class="collapse" for="c-42490862">[-]</label><label class="expand" for="c-42490862">[7 more]</label></div><br/><div class="children"><div class="content">Continents are not objective reality, they are semi-arbitrary groupings vaguely correlated with geography, culture, etc.<p>If British people don’t feel like they’re part of “the Continent”, there’s little objective reason to say they are.</div><br/><div id="42490915" class="c"><input type="checkbox" id="c-42490915" checked=""/><div class="controls bullet"><span class="by">SahAssar</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490862">parent</a><span>|</span><a href="#42491254">next</a><span>|</span><label class="collapse" for="c-42490915">[-]</label><label class="expand" for="c-42490915">[6 more]</label></div><br/><div class="children"><div class="content">But I&#x27;m guessing we can agree that any major landmass is generally belonging to a continent? Like we all agree that greenland, new zealand, japan, etc generally belong to a continent?<p>So to what continent do those british people think they belong?</div><br/><div id="42491646" class="c"><input type="checkbox" id="c-42491646" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490915">parent</a><span>|</span><a href="#42491183">next</a><span>|</span><label class="collapse" for="c-42491646">[-]</label><label class="expand" for="c-42491646">[1 more]</label></div><br/><div class="children"><div class="content">New Zealand is not part of a continent (unless you consider Zealandia [1] one, which few do).  It&#x27;s a bunch of islands in the middle of the sea, far from other land.  It is part of named regions which sometimes substitute for continents when people want to divide up the world for some purpose like sports or economics, including Oceania and Australasia.<p>Great Britain (the island) is very close to mainland Europe, and was directly part of it a few thousand years ago.  The situation is totally different.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Zealandia" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Zealandia</a></div><br/></div></div><div id="42491183" class="c"><input type="checkbox" id="c-42491183" checked=""/><div class="controls bullet"><span class="by">wsintra2022</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490915">parent</a><span>|</span><a href="#42491646">prev</a><span>|</span><a href="#42490929">next</a><span>|</span><label class="collapse" for="c-42491183">[-]</label><label class="expand" for="c-42491183">[1 more]</label></div><br/><div class="children"><div class="content">British people don’t think anything, there are British individuals who may think but collectively the “British” do not have a thought.</div><br/></div></div><div id="42490929" class="c"><input type="checkbox" id="c-42490929" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490915">parent</a><span>|</span><a href="#42491183">prev</a><span>|</span><a href="#42491254">next</a><span>|</span><label class="collapse" for="c-42490929">[-]</label><label class="expand" for="c-42490929">[3 more]</label></div><br/><div class="children"><div class="content">If you asked someone directly “what continent is Britain part of”, they would surely say Europe, even if they would be unlikely to describe themselves as European. Language is funny that way.</div><br/><div id="42491023" class="c"><input type="checkbox" id="c-42491023" checked=""/><div class="controls bullet"><span class="by">SahAssar</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490929">parent</a><span>|</span><a href="#42491254">next</a><span>|</span><label class="collapse" for="c-42491023">[-]</label><label class="expand" for="c-42491023">[2 more]</label></div><br/><div class="children"><div class="content">So you agree (and think that most people would) that the UK is part of Europe?</div><br/><div id="42491253" class="c"><input type="checkbox" id="c-42491253" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42491023">parent</a><span>|</span><a href="#42491254">next</a><span>|</span><label class="collapse" for="c-42491253">[-]</label><label class="expand" for="c-42491253">[1 more]</label></div><br/><div class="children"><div class="content">I would agree that in some, but not all, of the contexts where the word “Europe” is used, it includes the UK.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42491254" class="c"><input type="checkbox" id="c-42491254" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489999">parent</a><span>|</span><a href="#42490541">prev</a><span>|</span><a href="#42490058">next</a><span>|</span><label class="collapse" for="c-42491254">[-]</label><label class="expand" for="c-42491254">[1 more]</label></div><br/><div class="children"><div class="content">The only people who find this funny are the British themselves, the other 99% of the world thinks nothing strange of it.</div><br/></div></div><div id="42490058" class="c"><input type="checkbox" id="c-42490058" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489999">parent</a><span>|</span><a href="#42491254">prev</a><span>|</span><a href="#42491534">next</a><span>|</span><label class="collapse" for="c-42490058">[-]</label><label class="expand" for="c-42490058">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;O2_(brand)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;O2_(brand)</a> - &quot;O2 (typeset as O2) is a global brand name owned by the Spanish telecommunications company Telefónica&quot;</div><br/><div id="42490249" class="c"><input type="checkbox" id="c-42490249" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490058">parent</a><span>|</span><a href="#42491534">next</a><span>|</span><label class="collapse" for="c-42490249">[-]</label><label class="expand" for="c-42490249">[1 more]</label></div><br/><div class="children"><div class="content">It’s a British brand, even if it’s now owned by someone else. It even says so on the page you link to.</div><br/></div></div></div></div><div id="42491534" class="c"><input type="checkbox" id="c-42491534" checked=""/><div class="controls bullet"><span class="by">lobochrome</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489999">parent</a><span>|</span><a href="#42490058">prev</a><span>|</span><a href="#42490753">next</a><span>|</span><label class="collapse" for="c-42491534">[-]</label><label class="expand" for="c-42491534">[1 more]</label></div><br/><div class="children"><div class="content">Well - it’s Spanish now no? Telefonica bought them.</div><br/></div></div><div id="42490753" class="c"><input type="checkbox" id="c-42490753" checked=""/><div class="controls bullet"><span class="by">blinding-streak</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489999">parent</a><span>|</span><a href="#42491534">prev</a><span>|</span><a href="#42489810">next</a><span>|</span><label class="collapse" for="c-42490753">[-]</label><label class="expand" for="c-42490753">[1 more]</label></div><br/><div class="children"><div class="content">Europe != EU</div><br/></div></div></div></div><div id="42489810" class="c"><input type="checkbox" id="c-42489810" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489718">parent</a><span>|</span><a href="#42489999">prev</a><span>|</span><a href="#42489729">next</a><span>|</span><label class="collapse" for="c-42489810">[-]</label><label class="expand" for="c-42489810">[2 more]</label></div><br/><div class="children"><div class="content">Imagine coming up with a naming scheme for the versioning of your product just for it to fail on the second time you want to use it.</div><br/><div id="42489968" class="c"><input type="checkbox" id="c-42489968" checked=""/><div class="controls bullet"><span class="by">zapnuk</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489810">parent</a><span>|</span><a href="#42489729">next</a><span>|</span><label class="collapse" for="c-42489968">[-]</label><label class="expand" for="c-42489968">[1 more]</label></div><br/><div class="children"><div class="content">Should have used chatGPT to ask for a name or at least check it</div><br/></div></div></div></div></div></div><div id="42489729" class="c"><input type="checkbox" id="c-42489729" checked=""/><div class="controls bullet"><span class="by">lelandfe</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489694">parent</a><span>|</span><a href="#42489718">prev</a><span>|</span><a href="#42489744">next</a><span>|</span><label class="collapse" for="c-42489729">[-]</label><label class="expand" for="c-42489729">[2 more]</label></div><br/><div class="children"><div class="content">It’s more like saying “the Audi Quattro, the company’s fourth car…”</div><br/><div id="42489819" class="c"><input type="checkbox" id="c-42489819" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489729">parent</a><span>|</span><a href="#42489744">next</a><span>|</span><label class="collapse" for="c-42489819">[-]</label><label class="expand" for="c-42489819">[1 more]</label></div><br/><div class="children"><div class="content">Because there’s an Audi Tre e Mezzo?</div><br/></div></div></div></div><div id="42489744" class="c"><input type="checkbox" id="c-42489744" checked=""/><div class="controls bullet"><span class="by">dghlsakjg</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489694">parent</a><span>|</span><a href="#42489729">prev</a><span>|</span><a href="#42490552">next</a><span>|</span><label class="collapse" for="c-42489744">[-]</label><label class="expand" for="c-42489744">[1 more]</label></div><br/><div class="children"><div class="content">The issue isn&#x27;t the grammar. It is that there are 5 distinct LLMs from OpenAI that you can use right now as well as 4 others that were deprecated in 2024.</div><br/></div></div></div></div><div id="42490552" class="c"><input type="checkbox" id="c-42490552" checked=""/><div class="controls bullet"><span class="by">overgard</span><span>|</span><a href="#42489478">parent</a><span>|</span><a href="#42489694">prev</a><span>|</span><a href="#42490410">next</a><span>|</span><label class="collapse" for="c-42490552">[-]</label><label class="expand" for="c-42490552">[4 more]</label></div><br/><div class="children"><div class="content">The article definitely has issues, but to me what&#x27;s relevant is where it&#x27;s published. The smart money and experts without a vested interest have been well aware LLMs are an expensive dead for over a year and have been saying as much (Gary Marcus for instance). That this is starting to enter mainstream consciousness is what&#x27;s newsworthy.</div><br/><div id="42492603" class="c"><input type="checkbox" id="c-42492603" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490552">parent</a><span>|</span><a href="#42491067">next</a><span>|</span><label class="collapse" for="c-42492603">[-]</label><label class="expand" for="c-42492603">[1 more]</label></div><br/><div class="children"><div class="content">Gary Marcus is just an anti-AI crank to balance out the pro-AI cranks. He&#x27;s not credible.</div><br/></div></div><div id="42491067" class="c"><input type="checkbox" id="c-42491067" checked=""/><div class="controls bullet"><span class="by">icpmacdo</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490552">parent</a><span>|</span><a href="#42492603">prev</a><span>|</span><a href="#42490410">next</a><span>|</span><label class="collapse" for="c-42491067">[-]</label><label class="expand" for="c-42491067">[2 more]</label></div><br/><div class="children"><div class="content">Gary Marcus is continuously lambasted and not taken seriously</div><br/><div id="42492553" class="c"><input type="checkbox" id="c-42492553" checked=""/><div class="controls bullet"><span class="by">overgard</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42491067">parent</a><span>|</span><a href="#42490410">next</a><span>|</span><label class="collapse" for="c-42492553">[-]</label><label class="expand" for="c-42492553">[1 more]</label></div><br/><div class="children"><div class="content">By whom? He seems highly credible to me, and his credentials check out, especially compared to hype men like Sam Altman. All youre doing is spreading FUD by an unnamed &quot;they&quot;</div><br/></div></div></div></div></div></div><div id="42490410" class="c"><input type="checkbox" id="c-42490410" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#42489478">parent</a><span>|</span><a href="#42490552">prev</a><span>|</span><a href="#42489518">next</a><span>|</span><label class="collapse" for="c-42490410">[-]</label><label class="expand" for="c-42490410">[7 more]</label></div><br/><div class="children"><div class="content">I was wondering about this one too...<p>&gt; At best, they say, Orion performs better than OpenAI’s current offerings, but hasn’t advanced enough to justify the enormous cost of keeping the new model running.<p>wdym &quot;keep it running&quot;?</div><br/><div id="42490586" class="c"><input type="checkbox" id="c-42490586" checked=""/><div class="controls bullet"><span class="by">overgard</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490410">parent</a><span>|</span><a href="#42489518">next</a><span>|</span><label class="collapse" for="c-42490586">[-]</label><label class="expand" for="c-42490586">[6 more]</label></div><br/><div class="children"><div class="content">Well, those server farms don&#x27;t pay for themselves.</div><br/><div id="42490868" class="c"><input type="checkbox" id="c-42490868" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490586">parent</a><span>|</span><a href="#42489518">next</a><span>|</span><label class="collapse" for="c-42490868">[-]</label><label class="expand" for="c-42490868">[5 more]</label></div><br/><div class="children"><div class="content">sure, but once it&#x27;s trained there isn&#x27;t a running maintenance cost</div><br/><div id="42490917" class="c"><input type="checkbox" id="c-42490917" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490868">parent</a><span>|</span><a href="#42492595">next</a><span>|</span><label class="collapse" for="c-42490917">[-]</label><label class="expand" for="c-42490917">[1 more]</label></div><br/><div class="children"><div class="content">If you offer an API you need to dedicate servers to it that keep the model loaded in GPU memory. Unless you don&#x27;t care about latency at all.<p>Though I wouldn&#x27;t be surprised if the bigger reason is the PR cost of releasing with an exciting name but unexciting results. The press would immediately declare the end of the AI growth curve</div><br/></div></div><div id="42492595" class="c"><input type="checkbox" id="c-42492595" checked=""/><div class="controls bullet"><span class="by">overgard</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490868">parent</a><span>|</span><a href="#42490917">prev</a><span>|</span><a href="#42491044">next</a><span>|</span><label class="collapse" for="c-42492595">[-]</label><label class="expand" for="c-42492595">[1 more]</label></div><br/><div class="children"><div class="content">There definitely is, storage, machines at the ready, data centers, etc. Also OpenAI basically loses money every time you interact with ChatGPT  <a href="https:&#x2F;&#x2F;www.wheresyoured.at&#x2F;subprimeai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.wheresyoured.at&#x2F;subprimeai&#x2F;</a></div><br/></div></div><div id="42491044" class="c"><input type="checkbox" id="c-42491044" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490868">parent</a><span>|</span><a href="#42492595">prev</a><span>|</span><a href="#42490912">next</a><span>|</span><label class="collapse" for="c-42491044">[-]</label><label class="expand" for="c-42491044">[1 more]</label></div><br/><div class="children"><div class="content">Of course running inference costs money. You think GPUs are free?</div><br/></div></div><div id="42490912" class="c"><input type="checkbox" id="c-42490912" checked=""/><div class="controls bullet"><span class="by">bhouston</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490868">parent</a><span>|</span><a href="#42491044">prev</a><span>|</span><a href="#42489518">next</a><span>|</span><label class="collapse" for="c-42490912">[-]</label><label class="expand" for="c-42490912">[1 more]</label></div><br/><div class="children"><div class="content">Well if it takes a ton of memory&#x2F;compute for inference because of its size, it may be cost prohibitive to run compared to the ROI it generates?</div><br/></div></div></div></div></div></div></div></div><div id="42489518" class="c"><input type="checkbox" id="c-42489518" checked=""/><div class="controls bullet"><span class="by">404mm</span><span>|</span><a href="#42489478">parent</a><span>|</span><a href="#42490410">prev</a><span>|</span><a href="#42489990">next</a><span>|</span><label class="collapse" for="c-42489518">[-]</label><label class="expand" for="c-42489518">[6 more]</label></div><br/><div class="children"><div class="content">Quite funny that an article about AI was not fed to AI to proof read it.</div><br/><div id="42489620" class="c"><input type="checkbox" id="c-42489620" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489518">parent</a><span>|</span><a href="#42489593">next</a><span>|</span><label class="collapse" for="c-42489620">[-]</label><label class="expand" for="c-42489620">[4 more]</label></div><br/><div class="children"><div class="content">Editing mistakes that AI wouldn&#x27;t make is the new &quot;proof of human input&quot;.</div><br/><div id="42489931" class="c"><input type="checkbox" id="c-42489931" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489620">parent</a><span>|</span><a href="#42489593">next</a><span>|</span><label class="collapse" for="c-42489931">[-]</label><label class="expand" for="c-42489931">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been messing around with base (not instruction tuned) LLMs; they often evade AI detectors and I wouldn&#x27;t be surprised if they evade this kind of detection too, at least with a high temperature</div><br/><div id="42490067" class="c"><input type="checkbox" id="c-42490067" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489931">parent</a><span>|</span><a href="#42489593">next</a><span>|</span><label class="collapse" for="c-42490067">[-]</label><label class="expand" for="c-42490067">[2 more]</label></div><br/><div class="children"><div class="content">&gt; with a high temperature<p>More like: with the right prompting</div><br/></div></div></div></div></div></div><div id="42489593" class="c"><input type="checkbox" id="c-42489593" checked=""/><div class="controls bullet"><span class="by">ToucanLoucan</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489518">parent</a><span>|</span><a href="#42489620">prev</a><span>|</span><a href="#42489990">next</a><span>|</span><label class="collapse" for="c-42489593">[-]</label><label class="expand" for="c-42489593">[1 more]</label></div><br/><div class="children"><div class="content">Bold of you to assume AI didn&#x27;t write it, too.</div><br/></div></div></div></div><div id="42489990" class="c"><input type="checkbox" id="c-42489990" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#42489478">parent</a><span>|</span><a href="#42489518">prev</a><span>|</span><a href="#42492231">next</a><span>|</span><label class="collapse" for="c-42489990">[-]</label><label class="expand" for="c-42489990">[3 more]</label></div><br/><div class="children"><div class="content">Articles these days are probably written by ChatGPT</div><br/><div id="42490066" class="c"><input type="checkbox" id="c-42490066" checked=""/><div class="controls bullet"><span class="by">MichaelDickens</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42489990">parent</a><span>|</span><a href="#42492231">next</a><span>|</span><label class="collapse" for="c-42490066">[-]</label><label class="expand" for="c-42490066">[2 more]</label></div><br/><div class="children"><div class="content">I doubt it, if you ask ChatGPT whether GPT-4 is OpenAI&#x27;s fourth LLM, it gives the correct answer. That&#x27;s the sort of thing GPT-2 might have said.</div><br/><div id="42491696" class="c"><input type="checkbox" id="c-42491696" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#42489478">root</a><span>|</span><a href="#42490066">parent</a><span>|</span><a href="#42492231">next</a><span>|</span><label class="collapse" for="c-42491696">[-]</label><label class="expand" for="c-42491696">[1 more]</label></div><br/><div class="children"><div class="content">Well, here&#x27;s the interesting part - gpt2 has been writing news since well before gpt3 was launched. Remember when &quot;news&quot; started getting weirdly reptative? When just about any product had a review avaliable? When the amount of slop content just _exploded_? Thats when the ai colonization of the internet began.</div><br/></div></div></div></div></div></div></div></div><div id="42492231" class="c"><input type="checkbox" id="c-42492231" checked=""/><div class="controls bullet"><span class="by">anshulbhide</span><span>|</span><a href="#42489478">prev</a><span>|</span><a href="#42486044">next</a><span>|</span><label class="collapse" for="c-42492231">[-]</label><label class="expand" for="c-42492231">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really care.<p>I had to come up with a proposal to build a new R&amp;D centre recently. To provide context on what our company does, I wrote a web scraper to scrape our own website (faster than going to IT) using Replit Agent and then fed that into O1 as context to come up with the proposal.<p>In less than an hour.<p>There is no going back.</div><br/><div id="42492251" class="c"><input type="checkbox" id="c-42492251" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#42492231">parent</a><span>|</span><a href="#42492311">next</a><span>|</span><label class="collapse" for="c-42492251">[-]</label><label class="expand" for="c-42492251">[1 more]</label></div><br/><div class="children"><div class="content">Looking at it from a signal vs noise perspective:<p>The noise was the proposal, which was no doubt several pages at least.<p>The signal was &quot;we should build a new R&amp;D centre&quot;.<p>Am I missing anything? Did you feed the AI some financial figures or other information that couldn&#x27;t be found on company website? If so, that would also be part of the signal.<p>It reminds me of an experiment in which people wanted to cut in line. Saying &quot;can I cut in front of you because I&#x27;m in a hurry?&quot; was significantly more successful than saying &quot;can I cut in front of you?&quot;, even though they are essentially the same. (I read about this in the book Influence: The Psychology of Persuasion).<p>AI generated reports, proposals, and other fluff, can make things seem so much more persuasive. Alice is going to ask her AI to turn her 1 sentence into 5 paragraphs, and then Bob is going to ask his AI to summarize the 5 paragraphs into 1 sentence.</div><br/></div></div><div id="42492311" class="c"><input type="checkbox" id="c-42492311" checked=""/><div class="controls bullet"><span class="by">aprilthird2021</span><span>|</span><a href="#42492231">parent</a><span>|</span><a href="#42492251">prev</a><span>|</span><a href="#42486044">next</a><span>|</span><label class="collapse" for="c-42492311">[-]</label><label class="expand" for="c-42492311">[2 more]</label></div><br/><div class="children"><div class="content">both you and another highlynupvoted poster have said some versioj &quot;never going back&quot; or &quot;dont want to go back&quot; or &quot;the tools that exist now are already insane&quot;<p>And while I&#x27;m happy for you, I don&#x27;t see the relevance? this post was not about &quot;going back&quot; or &quot;stopping the use of AI tools&quot; at all?</div><br/><div id="42492476" class="c"><input type="checkbox" id="c-42492476" checked=""/><div class="controls bullet"><span class="by">1propionyl</span><span>|</span><a href="#42492231">root</a><span>|</span><a href="#42492311">parent</a><span>|</span><a href="#42486044">next</a><span>|</span><label class="collapse" for="c-42492476">[-]</label><label class="expand" for="c-42492476">[1 more]</label></div><br/><div class="children"><div class="content">Presumably they meant &quot;never going to back to not including these tools in my daily workflow&quot;.</div><br/></div></div></div></div></div></div><div id="42486044" class="c"><input type="checkbox" id="c-42486044" checked=""/><div class="controls bullet"><span class="by">A_D_E_P_T</span><span>|</span><a href="#42492231">prev</a><span>|</span><a href="#42489549">next</a><span>|</span><label class="collapse" for="c-42486044">[-]</label><label class="expand" for="c-42486044">[52 more]</label></div><br/><div class="children"><div class="content">Counterpoint:  o1-Pro is insanely good -- subjectively, it&#x27;s as far above GPT4 as GPT4 was above 3.  It&#x27;s almost <i>too</i> good.  Use it properly for an extended period of time, and one begins to worry about the future of one&#x27;s children and the utility of their schooling.<p>o3, by all accounts, is better still.<p>Seems to me that things are progressing quickly enough.</div><br/><div id="42486168" class="c"><input type="checkbox" id="c-42486168" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#42486044">parent</a><span>|</span><a href="#42486151">next</a><span>|</span><label class="collapse" for="c-42486168">[-]</label><label class="expand" for="c-42486168">[18 more]</label></div><br/><div class="children"><div class="content">Not sure what you are using it for, but it is terrible for me for coding; claude beats it always and hands down. o1 just thinks forever to come up with stuff it already tried the previous time.<p>People say that&#x27;s just prompting without pointing to real million line+ repositories or realistic apps to show how that can be improved. So I say they are making todo and hello world apps and yes, there it works really well. Claude still beats it, every.. single.. time..<p>And yes, I use the Pro of all and yes, I do assume coding is done for most of people. Become a plumber or electrician or carpenter.</div><br/><div id="42486371" class="c"><input type="checkbox" id="c-42486371" checked=""/><div class="controls bullet"><span class="by">h_tbob</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486168">parent</a><span>|</span><a href="#42486229">next</a><span>|</span><label class="collapse" for="c-42486371">[-]</label><label class="expand" for="c-42486371">[9 more]</label></div><br/><div class="children"><div class="content">That so weird, it’s seems like everybody here prefers Claude.<p>I’ve been using Claude and  openai in copilot and I find even 4o seems to understand the problem better. O1 definitely seems to get it right more for me.</div><br/><div id="42486796" class="c"><input type="checkbox" id="c-42486796" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486371">parent</a><span>|</span><a href="#42492230">next</a><span>|</span><label class="collapse" for="c-42486796">[-]</label><label class="expand" for="c-42486796">[1 more]</label></div><br/><div class="children"><div class="content">I try to sprinkle &#x27;for us&#x2F;me&#x27; everywhere as much as I can; we work on LoB&#x2F;ERP apps mostly. These are small frontends to massive multi million line backends. We carved a niche by providing the frontends on these backends live at the client office by a business consultant of ours: they simply solve UX issues for the client on top of large ERP by using our tool and prompting. Everything looks modern, fresh and nice; unlike basically all the competitors in this space. It&#x27;s fast and no frontend people are needed for it; backend is another system we built which takes a lot longer of course as they are complex business rules. Both claude and o1 turn up something that looks similar but only the claude version will work and be, after less prompting, correct. I don&#x27;t have shares in either and I want open source to win; we have all open (more open) solutions doing all the same queries and we evaluate all but claude just wins. We did manage even big wins with openai davinci in 2022 (or so; before chatgpt), but this is a massive boost allowing us to upgrade most people to business consultant and just have them build with clients real time and have the tech guys including me add <i>manually</i> tests and proofs (where needed) to know if we are actually fine. Works so much better than the slog with clients before; people are so bad at explaining at what they need, it was slowly driving me insane after doing it for 30+ years.</div><br/></div></div><div id="42492230" class="c"><input type="checkbox" id="c-42492230" checked=""/><div class="controls bullet"><span class="by">mitemte</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486371">parent</a><span>|</span><a href="#42486796">prev</a><span>|</span><a href="#42486571">next</a><span>|</span><label class="collapse" for="c-42492230">[-]</label><label class="expand" for="c-42492230">[1 more]</label></div><br/><div class="children"><div class="content">Claude web’s context window is 200K tokens. I’d be surprised if GitHub Copilot’s context window exceeds 10K.<p>I’ve found using Claude via Copilot in VS Code produces noticeably lower quality results than 3.5 Sonnet on web. In my experience Claude web outdoes GPT-4o consistently.</div><br/></div></div><div id="42486571" class="c"><input type="checkbox" id="c-42486571" checked=""/><div class="controls bullet"><span class="by">master_crab</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486371">parent</a><span>|</span><a href="#42492230">prev</a><span>|</span><a href="#42486625">next</a><span>|</span><label class="collapse" for="c-42486571">[-]</label><label class="expand" for="c-42486571">[3 more]</label></div><br/><div class="children"><div class="content">Claude also has a better workflow UI. It’ll maintain conversation context while opening up new windows to present code suggestions.<p>When I was still subscribing to OpenAI (about 4 months ago) this didn’t exist.</div><br/><div id="42487797" class="c"><input type="checkbox" id="c-42487797" checked=""/><div class="controls bullet"><span class="by">rrrrrrrrrrrryan</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486571">parent</a><span>|</span><a href="#42489766">next</a><span>|</span><label class="collapse" for="c-42487797">[-]</label><label class="expand" for="c-42487797">[1 more]</label></div><br/><div class="children"><div class="content">It exists as of last week with Canvas.</div><br/></div></div><div id="42489766" class="c"><input type="checkbox" id="c-42489766" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486571">parent</a><span>|</span><a href="#42487797">prev</a><span>|</span><a href="#42486625">next</a><span>|</span><label class="collapse" for="c-42489766">[-]</label><label class="expand" for="c-42489766">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re using the web interface of either, you might consider looking into tools that focus on using LLMs for code, so you&#x27;re not copy&#x2F;pasting.</div><br/></div></div></div></div><div id="42486625" class="c"><input type="checkbox" id="c-42486625" checked=""/><div class="controls bullet"><span class="by">A_D_E_P_T</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486371">parent</a><span>|</span><a href="#42486571">prev</a><span>|</span><a href="#42486699">next</a><span>|</span><label class="collapse" for="c-42486625">[-]</label><label class="expand" for="c-42486625">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re both okay for coding, though for my use cases (which are niche and involve quite a lot of mathematics and formal logic) o1&#x2F;o1-Pro is better.  It seems to have a better native grasp of mathematical concepts, and it can even answer very difficult questions from vague inputs, e.g.:  <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;676020cb-8574-8005-8b83-4bed5b13e1cd" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;676020cb-8574-8005-8b83-4bed5b13e1...</a></div><br/></div></div><div id="42486699" class="c"><input type="checkbox" id="c-42486699" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486371">parent</a><span>|</span><a href="#42486625">prev</a><span>|</span><a href="#42486229">next</a><span>|</span><label class="collapse" for="c-42486699">[-]</label><label class="expand" for="c-42486699">[1 more]</label></div><br/><div class="children"><div class="content">Different languages maybe? I find Sonnet v2 to be lacking in Rust knowledge compared to 4o 11-20, but excelling at Python and JS&#x2F;TS. O1&#x27;s strong side seems to be complex or quirky puzzle-like coding problems that can be answered in a short manner, it&#x27;s meh at everything else, especially considering the price. Which is understandable given its purpose and training, but I have no use for it as that&#x27;s exactly the sort of problem I wouldn&#x27;t trust an LLM to solve.<p>Sonnet v2 in particular seems to be a bit broken with its reasoning (?) feature. The one where it detects it might be hallucinating (what&#x27;s even the condition?) and reviews the reply, reflecting on it. It can make it stop halfway into the reply and decide it wrote enough, or invent some ridiculous excuse to output a worse answer. Annoying, although it doesn&#x27;t trigger too often.</div><br/></div></div></div></div><div id="42486229" class="c"><input type="checkbox" id="c-42486229" checked=""/><div class="controls bullet"><span class="by">rubymamis</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486168">parent</a><span>|</span><a href="#42486371">prev</a><span>|</span><a href="#42486151">next</a><span>|</span><label class="collapse" for="c-42486229">[-]</label><label class="expand" for="c-42486229">[8 more]</label></div><br/><div class="children"><div class="content">I find that o1 and Sonnet 3.5 are good and bad quite equally on different things. That&#x27;s why I keep asking both the same coding questions.</div><br/><div id="42486238" class="c"><input type="checkbox" id="c-42486238" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486229">parent</a><span>|</span><a href="#42486151">next</a><span>|</span><label class="collapse" for="c-42486238">[-]</label><label class="expand" for="c-42486238">[7 more]</label></div><br/><div class="children"><div class="content">We do the same (all requests go to o1, sonnet and gemini and we store the results for later to compare) automatically for our research: Claude always wins. Even with specific prompting on both platforms. Especially frontend it seems o1 really is terrible.</div><br/><div id="42486884" class="c"><input type="checkbox" id="c-42486884" checked=""/><div class="controls bullet"><span class="by">rubymamis</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486238">parent</a><span>|</span><a href="#42490184">next</a><span>|</span><label class="collapse" for="c-42486884">[-]</label><label class="expand" for="c-42486884">[2 more]</label></div><br/><div class="children"><div class="content">Every time I try Gemini, it&#x27;s really subpar. I found that qwen2.5-coder-32b-instruct can be better.<p>Also, for me 50% 50% for Sonnet and o1, but although I&#x27;m not 100% sure about it, I think o1 is better with longer and more complicated (C++) code and debugging. At least from my brief testing. Also, OpenAI models seem to be more verbose - sometimes it&#x27;s better - where I&#x27;d like additional explanation on chosen fields in a SQL schema, sometimes it&#x27;s too much.<p>EDIT: Just asked both o1 and Sonnet 3.5 the same QML coding question, and Sonnet 3.5 succeeded, o1 failed.</div><br/><div id="42489880" class="c"><input type="checkbox" id="c-42489880" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486884">parent</a><span>|</span><a href="#42490184">next</a><span>|</span><label class="collapse" for="c-42489880">[-]</label><label class="expand" for="c-42489880">[1 more]</label></div><br/><div class="children"><div class="content">Very anecdotal but I’ve found that for things that are well spec’d out with a good prompt Sonnet 3.5 is far better. For problems where I might have introduced a subtle logical error o1 seems to catch it extremely well. So better reasoning might be occurring but reasoning is only a small part of what we would consider intelligence.</div><br/></div></div></div></div><div id="42490184" class="c"><input type="checkbox" id="c-42490184" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486238">parent</a><span>|</span><a href="#42486884">prev</a><span>|</span><a href="#42487477">next</a><span>|</span><label class="collapse" for="c-42490184">[-]</label><label class="expand" for="c-42490184">[2 more]</label></div><br/><div class="children"><div class="content">A new o1 was released on December 17th. Which one are you talking about</div><br/><div id="42490954" class="c"><input type="checkbox" id="c-42490954" checked=""/><div class="controls bullet"><span class="by">tigershark</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42490184">parent</a><span>|</span><a href="#42487477">next</a><span>|</span><label class="collapse" for="c-42490954">[-]</label><label class="expand" for="c-42490954">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. The previous version of o1 did actually worse in the coding benchmarks, so I would expect it to be worse in real life scenarios.
The new version released a few days ago on the other hand is better in the benchmarks, so it would seem strange that someone used it and is saying that it’s worse than Claude.</div><br/></div></div></div></div><div id="42487477" class="c"><input type="checkbox" id="c-42487477" checked=""/><div class="controls bullet"><span class="by">CapcomGo</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486238">parent</a><span>|</span><a href="#42490184">prev</a><span>|</span><a href="#42486533">next</a><span>|</span><label class="collapse" for="c-42487477">[-]</label><label class="expand" for="c-42487477">[1 more]</label></div><br/><div class="children"><div class="content">Wins? What does this mean? Do you have any results? I see the claims that Claude is better for coding a lot but using it and using Gemini 2.0 flash and o1 and it sure doesn&#x27;t seem like it.</div><br/></div></div><div id="42486533" class="c"><input type="checkbox" id="c-42486533" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486238">parent</a><span>|</span><a href="#42487477">prev</a><span>|</span><a href="#42486151">next</a><span>|</span><label class="collapse" for="c-42486533">[-]</label><label class="expand" for="c-42486533">[1 more]</label></div><br/><div class="children"><div class="content">Claude is trained on principles. GPT is trained on billions of edge cases. Which student do you prefer?</div><br/></div></div></div></div></div></div></div></div><div id="42486151" class="c"><input type="checkbox" id="c-42486151" checked=""/><div class="controls bullet"><span class="by">phito</span><span>|</span><a href="#42486044">parent</a><span>|</span><a href="#42486168">prev</a><span>|</span><a href="#42486520">next</a><span>|</span><label class="collapse" for="c-42486151">[-]</label><label class="expand" for="c-42486151">[22 more]</label></div><br/><div class="children"><div class="content">I keep reading this on HN so I believe it has to be true in some ways, but I don&#x27;t really feel like there is any difference in my limited use (programming questions or explaining some concepts).<p>If anything I feel like it&#x27;s all been worse compared to the first release of ChatGPT, but I might be wearing rose colored glasses.</div><br/><div id="42486297" class="c"><input type="checkbox" id="c-42486297" checked=""/><div class="controls bullet"><span class="by">mathieuh</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486151">parent</a><span>|</span><a href="#42486177">next</a><span>|</span><label class="collapse" for="c-42486297">[-]</label><label class="expand" for="c-42486297">[15 more]</label></div><br/><div class="children"><div class="content">It’s the same for me. I genuinely don’t understand how I can be having such a completely different experience from the people who rave about ChatGPT. Every time I’ve tried it’s been useless.<p>How can some people think it’s amazing and has completely changed how they work, while for me it makes mistakes that a static analyser would catch? It’s not like I’m doing anything remarkable, for the past couple of months I’ve been doing fairly standard web dev and it can’t even fix basic problems with HTML. It will suggest things that just don’t work at all and my IDE catches, it invents APIs for packages.<p>One guy I work with uses it extensively and what it produces is essentially black boxes. If I find a problem with something “he” (or rather ChatGPT) has produced it takes him ages to commune with the machine spirit again to figure out how to fix it, and then he still doesn’t understand it.<p>I can’t help but see this as a time-bomb, how much completely inscrutable shite are these tools producing? In five years are we going to end up with a bunch of “senior engineers” who don’t actually understand what they’re doing?<p>Before people cry “o tempora o mores” at me and make parallels with the introduction of high-level languages, at least in order to write in a high-level language you need some basic understanding of the logic that is being executed.</div><br/><div id="42486409" class="c"><input type="checkbox" id="c-42486409" checked=""/><div class="controls bullet"><span class="by">lm28469</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486297">parent</a><span>|</span><a href="#42486986">next</a><span>|</span><label class="collapse" for="c-42486409">[-]</label><label class="expand" for="c-42486409">[4 more]</label></div><br/><div class="children"><div class="content">&gt; How can some people think it’s amazing and has completely changed how they work, while for me it makes mistakes that should a static analyser would catch?<p>There are a lot of code monkeys working on boilerplate code, these people used to rely on stack overflow and now that chatgpt is here it&#x27;s a huge improvement for them<p>If you work on anything remotely complex or which hasn&#x27;t been solved 10 times on stack overflow chatgpt isn&#x27;t remotely as useful</div><br/><div id="42490209" class="c"><input type="checkbox" id="c-42490209" checked=""/><div class="controls bullet"><span class="by">skinner_</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486409">parent</a><span>|</span><a href="#42486986">next</a><span>|</span><label class="collapse" for="c-42490209">[-]</label><label class="expand" for="c-42490209">[3 more]</label></div><br/><div class="children"><div class="content">I work on very complex problems. Some of my solutions have small, standard substeps that now I can reliably outsource to ChatGPT. Here are a few just from last week:<p>- write cvxpy code to find the chromatic number of a graph, and an optimal coloring, given its adjecency matrix.<p>- given an adjecency matrix write numpy code that enumerates all triangle-free vertex subsets.<p>- please port this old code from tensorflow to pytorch: ...<p>- in pytorch, i&#x27;d like to code a tensor network defining a 3-tensor of shape (d, d, d). my tensor consists of first projecting all three of its d-dimensional inputs to a k-dimensional vector, typically k=d&#x2F;10, and then applying a (k, k, k) 3-tensor to contract these to a single number.<p>All were solved by ChatGPT on the first try.</div><br/><div id="42490906" class="c"><input type="checkbox" id="c-42490906" checked=""/><div class="controls bullet"><span class="by">lazypenguin</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42490209">parent</a><span>|</span><a href="#42486986">next</a><span>|</span><label class="collapse" for="c-42490906">[-]</label><label class="expand" for="c-42490906">[2 more]</label></div><br/><div class="children"><div class="content">To be honest, these don’t sound like hard problems. These sound like they have very specific answers that I might find in the more specialized stackoverflow sections. These are also the kind of questions (not in this domain) that I’ve found yield the best results from LLMs.<p>In comparison asking an LLM a more project specific question “this code has a race condition where is it” while including some code usually is a crapshoot and really depends if you were lucky enough to give it the right context anyway.</div><br/><div id="42491320" class="c"><input type="checkbox" id="c-42491320" checked=""/><div class="controls bullet"><span class="by">skinner_</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42490906">parent</a><span>|</span><a href="#42486986">next</a><span>|</span><label class="collapse" for="c-42491320">[-]</label><label class="expand" for="c-42491320">[1 more]</label></div><br/><div class="children"><div class="content">Sure, these are standard problems, I’ve said so myself. My point is that my productivity is multiplied by ChatGPT, even if it can only solve standard problems. This is because, although I work on highly non-standard problems (see <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.10069" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.10069</a> for an example), I can break them down into smaller, standard components, which ChatGPT can solve in seconds. I never ask ChatGPT &quot;where&#x27;s the race condition&quot; kind of questions.</div><br/></div></div></div></div></div></div></div></div><div id="42486986" class="c"><input type="checkbox" id="c-42486986" checked=""/><div class="controls bullet"><span class="by">vrighter</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486297">parent</a><span>|</span><a href="#42486409">prev</a><span>|</span><a href="#42486724">next</a><span>|</span><label class="collapse" for="c-42486986">[-]</label><label class="expand" for="c-42486986">[1 more]</label></div><br/><div class="children"><div class="content">first time I tried it, I asked it to find bugs in a piece of very well tested C code.<p>It introduced an off-by-one error by miscounting the number of arguments in an sprintf call, breaking the program. And then proceeded to fail to find that bug that it introduced.</div><br/></div></div><div id="42486724" class="c"><input type="checkbox" id="c-42486724" checked=""/><div class="controls bullet"><span class="by">williamcotton</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486297">parent</a><span>|</span><a href="#42486986">prev</a><span>|</span><a href="#42486522">next</a><span>|</span><label class="collapse" for="c-42486724">[-]</label><label class="expand" for="c-42486724">[2 more]</label></div><br/><div class="children"><div class="content">I found it very useful for writing a lexer and parser for a search DSL and React component recently:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;williamcotton&#x2F;search-input-query">https:&#x2F;&#x2F;github.com&#x2F;williamcotton&#x2F;search-input-query</a></div><br/><div id="42492323" class="c"><input type="checkbox" id="c-42492323" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486724">parent</a><span>|</span><a href="#42486522">next</a><span>|</span><label class="collapse" for="c-42492323">[-]</label><label class="expand" for="c-42492323">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. I implemented something very similar (if not identical) a couple years ago (at work so not open source). I used a simple grammar and standard parser generator. It’s been nice to have the grammar as we’ve made tweaks over the years to change various behaviours and add features.</div><br/></div></div></div></div><div id="42486522" class="c"><input type="checkbox" id="c-42486522" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486297">parent</a><span>|</span><a href="#42486724">prev</a><span>|</span><a href="#42489884">next</a><span>|</span><label class="collapse" for="c-42486522">[-]</label><label class="expand" for="c-42486522">[1 more]</label></div><br/><div class="children"><div class="content">&gt; How can some people think it’s amazing and has completely changed how they work, while for me it makes mistakes that should a static analyser would catch? It’s not like I’m doing anything remarkable, for the past couple of months I’ve been doing fairly standard web dev and it can’t even fix basic problems with HTML.<p>Part of this is, I think, anchoring and expectation management: you hear people say it&#x27;s amazing and wonderful, and then you see it fall over and you&#x27;re naturally disappointed.<p>My formative years started off with Commodore 64 basic going &quot;?SYNTAX ERROR&quot; from most typos plus a lot of &quot;I don&#x27;t know what that means&quot; from the text adventures, then Metrowerks&#x27; C compiler telling me there were errors on every line <i>*after but not including*</i> the one where I forgot the semicolon, then surprises in VisualBasic and Java where I was getting integer division rather than floats, then the fantastic oddity where accidentally leaning on the option key on a mac keyboard while pressing minus turns the minus into an n-dash which looked completely identical to a minus on the Xcode default font at the time and thus produced a very confusing compiler error…<p>So my expectations have always been low for machine generated output. And it has wildly exceeded those low expectations.<p>But the expectation management goes both ways, especially when the comparison is &quot;normal humans&quot; rather than &quot;best practices&quot;. I&#x27;ve seen things you wouldn&#x27;t believe...<p><pre><code>  Entire files copy-pasted line for line, &quot;TODO: deduplicate&quot; and all,
  20 minute app starts passed off as &quot;optimized solutions.&quot;
  FAQs filled with nothing but Bob Ross quotes,
  a zen garden of &quot;happy little accidents.&quot;

  I watched iOS developers use UI tests
  as a complete replacement for storyboards,
  bi-weekly commits, each a sprawling novel of despair,
  where every change log was a tragic odyssey.

  Google Spreadsheets masquerading as bug trackers,
  Swift juniors not knowing their ! from their ?,
  All those hacks and horrors… lost in time,
  Time to deploy.
</code></pre>
(All true, and all pre-dating ChatGPT).<p>&gt; It will suggest things that just don’t work at all and my IDE catches, it invents APIs for packages.<p>Aye. I&#x27;ve even had that with models forgetting the APIs they themselves have created, just outside the context window.<p>To me, these are tools. They&#x27;re fantastic tools, but they&#x27;re not something you can blindly fire-and-forget…<p>…fortunately for me, because my passive income is <i>not quite</i> high enough to cover mortgage payments, and I&#x27;m looking for work.<p>&gt; In five years are we going to end up with a bunch of “senior engineers” who don’t actually understand what they’re doing?<p>Yes, if we&#x27;re lucky.<p>If we&#x27;re not, the models keep getting better and we don&#x27;t have any &quot;senior engineers&quot; at all.</div><br/></div></div><div id="42489884" class="c"><input type="checkbox" id="c-42489884" checked=""/><div class="controls bullet"><span class="by">jonas21</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486297">parent</a><span>|</span><a href="#42486522">prev</a><span>|</span><a href="#42489964">next</a><span>|</span><label class="collapse" for="c-42489884">[-]</label><label class="expand" for="c-42489884">[3 more]</label></div><br/><div class="children"><div class="content">I think the difference comes down to interacting with it like IDE autocomplete vs. interacting with it like a colleague.<p>It sounds like you&#x27;re doing the former -- and yeah, it can make mistakes that autocomplete wouldn&#x27;t or generate code that&#x27;s wrong or overly complex.<p>On the other hand, I&#x27;ve found that if you treat it more like a colleague, it works wonderfully. Ask it to do something, then read the code and ask follow-up questions. If you see something that&#x27;s wrong or just seems off, tell it, and ask it to fix it. If you don&#x27;t understand something, ask for an explanation. I&#x27;ve found that this process generates great code that I often understand better than if I had written it from scratch, and in a fraction of the time.<p>It also sounds like you&#x27;re asking it to do basic tasks that you already know how to do. I find that it&#x27;s most useful in tackling things that I <i>don&#x27;t</i> know how to do. It&#x27;ll already have read all of the documentation and know the right way to call whatever APIs, etc, and -- this is key -- you can have a conversation with it to clear up anything that&#x27;s confusing.<p>This takes a big shift in mindset if you&#x27;ve been using IDEs all your life and have expectations of LLMs being a fancy autocomplete. And you really have to unlearn a lot of stuff to get the most out of them.</div><br/><div id="42490800" class="c"><input type="checkbox" id="c-42490800" checked=""/><div class="controls bullet"><span class="by">LinearEntropy</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42489884">parent</a><span>|</span><a href="#42489964">next</a><span>|</span><label class="collapse" for="c-42490800">[-]</label><label class="expand" for="c-42490800">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m in the same boat as the person you&#x27;re responding to. I really don&#x27;t understand how to get anything helpful out of ChatGPT, or more than anything basic out of Claude.<p>&gt; I&#x27;ve found that if you treat it more like a colleague, it works wonderfully.
This is what I&#x27;ve been trying to do. I don&#x27;t use LLM code completion tools. I&#x27;ll ask anything from how to do something &quot;basicish&quot; with html &amp; css, and it&#x27;ll always output something that doesn&#x27;t work as expected. Question it and I&#x27;ll get into a loop of the same response code, regardless of how I explain that it isn&#x27;t correct.<p>On the other end of the scale, I&#x27;ll ask about an architectural or design decision. I&#x27;ll often get a response that is in the realm of what I&#x27;d expect. When drilling down and asking specifics however, the responses really start to fall apart. I inevitably end up in the loop of asking if an alternative is [more performant&#x2F;best practice&#x2F;the language idiomatic way] and getting the &quot;Sorry, you&#x27;re correct&quot; response. The longer I stay in that loop, the more it contradicts itself, and the less cohesive the answers get.<p>I _wish_ I could get the results from LLMs that so many people seem to. It just doesn&#x27;t happen for me.</div><br/><div id="42491830" class="c"><input type="checkbox" id="c-42491830" checked=""/><div class="controls bullet"><span class="by">ALittleLight</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42490800">parent</a><span>|</span><a href="#42489964">next</a><span>|</span><label class="collapse" for="c-42491830">[-]</label><label class="expand" for="c-42491830">[1 more]</label></div><br/><div class="children"><div class="content">My approach is a lot of writing out ideas and giving them to ChatGPT.  ChatGPT sometimes nods along, sometimes offers bad or meaningless suggestions, sometimes offers good suggestions, sometimes points out (what should have been) obvious errors or mistakes.  The process of writing stuff out is useful anyway and sometimes getting good feedback on it is even better.<p>When coding I will often find myself in kind of a reverse pattern from how people seem to be using ChatGPT.  I work in a jupyter notebook in a haphazard way getting things to functional and basically correct, after this I select all, copy, paste, and ask ChatGPT to refactor and refine to something more maintainable.  My janky blocks of code and one offs become well documented scripts and functions.<p>I find a lot of people do the opposite, where they ask ChatGPT to start, then get frustrated when ChatGPT only goes 70% of the way and it&#x27;s difficult to complete the imperfectly understood assignment - harder than doing it all yourself.  With my method, where I start and get things basically working, ChatGPT knows what I&#x27;m going for, I get to do the part of coding I enjoy, and I wind up with something more durable, reusable, and shareable.<p>Finally, ChatGPT is wonderful in areas where you don&#x27;t know very much at all.  One example, I&#x27;ve got this idea in my head for a product I&#x27;ll likely never build - but it&#x27;s fun to plan out.<p>My idea is roughly a smart bidet that can detect metabolites in urine.  I got this idea when a urinalysis showed I had high levels of ketones in my urine.  When I was reading about what that meant I discovered it&#x27;s a marker for diabetic ketoacidosis (a severe problem for ~100k people a year) and it can also be indicator for colorectal cancer as well as indicating a &quot;ketosis&quot; state that some people intentionally try to enter for dieting or wellness reasons.  (My own ketones were caused by unintentionally being in ketosis, I&#x27;m fine, thanks for wondering.)<p>Right now, you detect ketones in urine with a strip that you pee on, and that works well enough - but it could be better because who wants to use a test strip all the time?  Enter the smart bidet.  The bidet gives us an excuse to connect power to our device and bring the sensor along.  Bluetooth detects a nearby phone (and therefore identity of the depositor), a motion sensor can detect a stream of urine triggering our detection, and then use our sensor to detect ketones which we track overtime in the app, ideally with additional metabolites that have useful diagnostic purposes.<p>How to detect ketones?  Is it even possible?  I wonder to ChatGPT if spectroscopy is the right method of detection here.  ChatGPT suggests a retractable electrochemical probe similar to an extant product that can detect a kind of ketone in blood.  ChatGPT knows what kind of ketone is most detectable in urine.  ChatGPT can link me to scientific instrument companies that make similar (ish) probes where I could contact them and ask if they sold this type of thing, and so on.<p>Basically, I go from peeing on a test strip and wondering if I could automate this to chat with ChatGPT - having, what was in my opinion, an interesting conversation with the LLM, where we worked through what ketones are, the different kinds, the prevalence of ketones in different bodily fluids, types of spectroscopy that might detect acetoacetate (available in urine) and how much that would cost and what challenges would be and so on, followed by the idea of electrochemical probes and how retracting and extending the probe might prolong its lifespan and maybe a heating element could be added to dry the probe to preserve it even better and so on.<p>Was ChatGPT right about all that?  I don&#x27;t know.  If I were really interested I would try to validate what it said, and I suspect I would find it was mostly right and incomplete or off in places.  Basically like having a pretty smart and really knowledgeable friend who is not infallible.<p>Without ChatGPT I would have likely thought &quot;I wonder if I can automate this&quot;, maybe googled for some tracking product, then forgot about it.  With ChatGPT I quickly got a much better understanding of a system that I glancingly came into conscious contact with.<p>It&#x27;s not hard to project out that level of improved insight and guess that it will lead to valuable life contributions.  In fact, I would say it did in that one example alone.<p>The urinalysis (which was combined with a blood test) said something like &quot;ketones +3&quot; and if you google &quot;urine ketones +3&quot; you get a explanations that don&#x27;t apply to me (alcohol, vigorous exercise, intentional dieting) or &quot;diabetic ketoacidosis&quot; which google warns you is a serious health condition.<p>In the follow up with the doctor I asked about the ketones.  The doctor said &quot;Oh, you were probably just dehydrated, don&#x27;t worry about it, you don&#x27;t have diabetic ketoacidosis&quot; and the conversation moved on and soon concluded.  In the moment I was just relieved there was an innocent explanation.  But, as I thought about it, shouldn&#x27;t other results in the blood or urine test indicate dehydration?  I asked ChatGPT (and confirmed on Google) and sure enough there were 3 other signals that should have been there if I was dehydrated that were not there.<p>&quot;What does this mean?&quot; I wondered to ChatGPT.  ChatGPT basically told me it was probably nothing, but if I was worried I could do an at home test - which I didn&#x27;t even know existed (though I could have found through carefully reading the first google result).  So I go to Target and get an at home test kit (bottle of test strips), 24 gatorades, and a couple liters of pedialyte to ensure I&#x27;m well hydrated.<p>I start drinking my usual 64 ounces of water a day, plus lots of gatorade and pedialyte and over a couple days I remain at high ketones in urine.  Definitely not dehydrated.  Consulting with ChatGPT I start telling it everything I&#x27;m eating and it points out that I&#x27;m just accidentally in a ketogenic diet.  ChatGPT suggests some simple carbs for me, I start eating those, and the ketone content of my urine falls off in roughly the exact timeframe that ChatGPT predicted (i.e. it told me if you eat this meal you should see ketones decline in ~4 hours).<p>Now, in some sense this didn&#x27;t really matter.  If I had simply listened to my doctor&#x27;s explanation I would&#x27;ve been fine.  Wrong, but fine.  It wasn&#x27;t dehydration, it was just accidentally being in a ketogenic diet.  But, I take all this as evidence of how ChatGPT now, as it exists, helped me to understand my test results in a way that real doctors weren&#x27;t able to - partially because ChatGPT exists in a form where I can just ping it with whatever stray thoughts come to mind and it will answer instantly.  I&#x27;m sure if I could just text my doctor those same thoughts we would&#x27;ve come to the same conclusion.</div><br/></div></div></div></div></div></div><div id="42489964" class="c"><input type="checkbox" id="c-42489964" checked=""/><div class="controls bullet"><span class="by">CSMastermind</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486297">parent</a><span>|</span><a href="#42489884">prev</a><span>|</span><a href="#42486437">next</a><span>|</span><label class="collapse" for="c-42489964">[-]</label><label class="expand" for="c-42489964">[1 more]</label></div><br/><div class="children"><div class="content">I mean if you&#x27;re getting no value out of ChatGPT I&#x27;d love to have a session seeing how you use it.</div><br/></div></div><div id="42486437" class="c"><input type="checkbox" id="c-42486437" checked=""/><div class="controls bullet"><span class="by">globular-toast</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486297">parent</a><span>|</span><a href="#42489964">prev</a><span>|</span><a href="#42486177">next</a><span>|</span><label class="collapse" for="c-42486437">[-]</label><label class="expand" for="c-42486437">[2 more]</label></div><br/><div class="children"><div class="content">The ones who use it extensively are the same that used to hit up stackoverflow as the first port of call for every trivial problem that came their way. They&#x27;re not really engineers, they just want to get stuff done.</div><br/><div id="42489577" class="c"><input type="checkbox" id="c-42489577" checked=""/><div class="controls bullet"><span class="by">phist_mcgee</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486437">parent</a><span>|</span><a href="#42486177">next</a><span>|</span><label class="collapse" for="c-42489577">[-]</label><label class="expand" for="c-42489577">[1 more]</label></div><br/><div class="children"><div class="content">No ad hominem please.</div><br/></div></div></div></div></div></div><div id="42486177" class="c"><input type="checkbox" id="c-42486177" checked=""/><div class="controls bullet"><span class="by">omega3</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486151">parent</a><span>|</span><a href="#42486297">prev</a><span>|</span><a href="#42486400">next</a><span>|</span><label class="collapse" for="c-42486177">[-]</label><label class="expand" for="c-42486177">[1 more]</label></div><br/><div class="children"><div class="content">Same, on every release from openai, anthropic I keep reading how the new model is so much better (insert hyperbole here) than the previous one yet when using it I feel like they are mostly the same as last year.</div><br/></div></div><div id="42486400" class="c"><input type="checkbox" id="c-42486400" checked=""/><div class="controls bullet"><span class="by">fzeroracer</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486151">parent</a><span>|</span><a href="#42486177">prev</a><span>|</span><a href="#42486159">next</a><span>|</span><label class="collapse" for="c-42486400">[-]</label><label class="expand" for="c-42486400">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;ve ever used any enterprise software for long enough, you know the exact same song and dance.<p>They release version Grand Banana. Purported to be approximately 30% faster with brand new features like  Algorithmic Triple Layering and Enhanced Compulsory Alignment. You open the app. Everything is slower, things are harder to find and it breaks in new, fun ways. Your organization pays a couple hundred more per person for these benefits.  Their stock soars, people celebrate the release and your management says they can&#x27;t wait to see the improvement in workflows now that they&#x27;ve been able to lay off a quarter of your team.<p>Has there been improvements in LLMs over time? Somewhat, most of it concentrated at the beginning (because they siphoned up a bunch of data in a dubious manner). Now it&#x27;s just part of their sales cycle, to keep pumping up numbers while no one sees any meaningful improvement.</div><br/></div></div><div id="42486159" class="c"><input type="checkbox" id="c-42486159" checked=""/><div class="controls bullet"><span class="by">delusional</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486151">parent</a><span>|</span><a href="#42486400">prev</a><span>|</span><a href="#42486520">next</a><span>|</span><label class="collapse" for="c-42486159">[-]</label><label class="expand" for="c-42486159">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say the same. I&#x27;ve tried a bunch of different AI tools, and none of them really seem all that helpful.</div><br/><div id="42486252" class="c"><input type="checkbox" id="c-42486252" checked=""/><div class="controls bullet"><span class="by">ogogmad</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486159">parent</a><span>|</span><a href="#42486520">next</a><span>|</span><label class="collapse" for="c-42486252">[-]</label><label class="expand" for="c-42486252">[3 more]</label></div><br/><div class="children"><div class="content">One use-case: They help with learning things quickly by having a chat and asking questions. And they never get tired or emotional. Tutoring 24&#x2F;7.<p>They also generate small code or scripts, as well as automate small things, when you&#x27;re not sure how, but you know there&#x27;s a way. You need to ensure you have a way to verify the results.<p>They do language tasks like grammar-fixing, perfect translation, etc.<p>They&#x27;re 100 times easier and faster than search engines, if you limit your uses to that.</div><br/><div id="42486516" class="c"><input type="checkbox" id="c-42486516" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486252">parent</a><span>|</span><a href="#42487236">next</a><span>|</span><label class="collapse" for="c-42486516">[-]</label><label class="expand" for="c-42486516">[1 more]</label></div><br/><div class="children"><div class="content">They can&#x27;t help you learn what they don&#x27;t know themselves.<p>I&#x27;m trying to use them to read historical handwritten documents in old Norwegian (Danish, pretty much). Not only do they not handle the German-style handwriting, but what they spit out looks like the sort of thing GPT-2 would spit out if you asked it to write Norwegian (only slightly better than Swedish Muppet Swedish Chef&#x27;s Swedish). It seems the experimental tuning has made it <i>worse</i> at the task I most desperately want to use it for.<p>And when you think about it, how could it <i>not</i> overfit in some sense, when trained on its own output? No new information is coming in, so it pretty much has to get worse at <i>something</i> to get better at all the benchmarks.</div><br/></div></div><div id="42487236" class="c"><input type="checkbox" id="c-42487236" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486252">parent</a><span>|</span><a href="#42486516">prev</a><span>|</span><a href="#42486520">next</a><span>|</span><label class="collapse" for="c-42487236">[-]</label><label class="expand" for="c-42487236">[1 more]</label></div><br/><div class="children"><div class="content">&gt; perfect translation<p>Hah, no. They&#x27;re good, but they definitely make stuff up when the context gets too long. Always check their output, just the same as you already note they need for small code and scripts.</div><br/></div></div></div></div></div></div></div></div><div id="42486520" class="c"><input type="checkbox" id="c-42486520" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#42486044">parent</a><span>|</span><a href="#42486151">prev</a><span>|</span><a href="#42486137">next</a><span>|</span><label class="collapse" for="c-42486520">[-]</label><label class="expand" for="c-42486520">[7 more]</label></div><br/><div class="children"><div class="content">I had a 30 min argument with o1-pro where it was convinced it had solved the halting problem. Tried to gaslight me into thinking I just didn’t understand the subtlety of the argument. But it’s susceptible to appeal to authority and when I started quoting snippets of textbooks and mathoverflow it finally relented and claimed there had been a “misunderstanding”. It really does argue like a human though now...</div><br/><div id="42486618" class="c"><input type="checkbox" id="c-42486618" checked=""/><div class="controls bullet"><span class="by">radioactivist</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486520">parent</a><span>|</span><a href="#42490988">next</a><span>|</span><label class="collapse" for="c-42486618">[-]</label><label class="expand" for="c-42486618">[5 more]</label></div><br/><div class="children"><div class="content">I had a similar experience with regular o1 about integral that was divergent. It was adamant that it wasn&#x27;t and would respond to any attempt at persuasion with variants of &quot;its a standard integral&quot; with a &quot;subtle cancellation&quot;. When I asked for any source for this standard integral it produced references to support its argument that existed but didn&#x27;t actually contain the integral. When I told it the references didn&#x27;t have the result and backpedalled (gaslighting!) to &quot;I never told you they were in there&quot;. When I pointed out that in fact it did it insisted this was just a &quot;misunderstanding&quot;. It only relented when I told it Mathematica agreed the integral was divergent. It still insisted it never said that the books it pointed to contained this (false, non-sensical) result.<p>This was new behaviour for me to see in an LLM. Usually the problem is these things would just fold when you pushed back. I don&#x27;t know which is better, but being this confidently wrong (and &quot;lying&quot; when confronted with it) is troubling.</div><br/><div id="42489624" class="c"><input type="checkbox" id="c-42489624" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486618">parent</a><span>|</span><a href="#42490532">next</a><span>|</span><label class="collapse" for="c-42489624">[-]</label><label class="expand" for="c-42489624">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but being this confidently wrong (and &quot;lying&quot; when confronted with it) is troubling.<p>It works in politics, marketing, and self-promotion.<p>If you use the web as a training set, those categories dominate.</div><br/><div id="42489775" class="c"><input type="checkbox" id="c-42489775" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42489624">parent</a><span>|</span><a href="#42490532">next</a><span>|</span><label class="collapse" for="c-42489775">[-]</label><label class="expand" for="c-42489775">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they also trained the model on Sam Altman. ;)</div><br/></div></div></div></div><div id="42490532" class="c"><input type="checkbox" id="c-42490532" checked=""/><div class="controls bullet"><span class="by">justatdotin</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486618">parent</a><span>|</span><a href="#42489624">prev</a><span>|</span><a href="#42490988">next</a><span>|</span><label class="collapse" for="c-42490532">[-]</label><label class="expand" for="c-42490532">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve also had it invent non-existent references.<p>&gt; being this confidently wrong (and &quot;lying&quot; when confronted with it) is troubling.<p>I don&#x27;t find it troubling. I like being reminded to distrust and confirm everything it offers.</div><br/><div id="42490574" class="c"><input type="checkbox" id="c-42490574" checked=""/><div class="controls bullet"><span class="by">radioactivist</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42490532">parent</a><span>|</span><a href="#42490988">next</a><span>|</span><label class="collapse" for="c-42490574">[-]</label><label class="expand" for="c-42490574">[1 more]</label></div><br/><div class="children"><div class="content">The troubling part is that the references themselves existed -- one was an obscure Russian text that is difficult to find (but is exactly where you&#x27;d expect to find this kind of result, if it existed).</div><br/></div></div></div></div></div></div><div id="42490988" class="c"><input type="checkbox" id="c-42490988" checked=""/><div class="controls bullet"><span class="by">phillipharris</span><span>|</span><a href="#42486044">root</a><span>|</span><a href="#42486520">parent</a><span>|</span><a href="#42486618">prev</a><span>|</span><a href="#42486137">next</a><span>|</span><label class="collapse" for="c-42490988">[-]</label><label class="expand" for="c-42490988">[1 more]</label></div><br/><div class="children"><div class="content">This sounds fun to read, can you share the transcript?</div><br/></div></div></div></div><div id="42486173" class="c"><input type="checkbox" id="c-42486173" checked=""/><div class="controls bullet"><span class="by">1123581321</span><span>|</span><a href="#42486044">parent</a><span>|</span><a href="#42486137">prev</a><span>|</span><a href="#42489651">next</a><span>|</span><label class="collapse" for="c-42486173">[-]</label><label class="expand" for="c-42486173">[1 more]</label></div><br/><div class="children"><div class="content">O1 is effective, but it’s slow. I would expect a GPT-5 and mini to work as quickly as the 4 models.</div><br/></div></div><div id="42489651" class="c"><input type="checkbox" id="c-42489651" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#42486044">parent</a><span>|</span><a href="#42486173">prev</a><span>|</span><a href="#42486136">next</a><span>|</span><label class="collapse" for="c-42489651">[-]</label><label class="expand" for="c-42489651">[1 more]</label></div><br/><div class="children"><div class="content">It basically solves all bugs&#x2F;programming challenges i throw at it, given i give it the right data</div><br/></div></div><div id="42486136" class="c"><input type="checkbox" id="c-42486136" checked=""/><div class="controls bullet"><span class="by">apwell23</span><span>|</span><a href="#42486044">parent</a><span>|</span><a href="#42489651">prev</a><span>|</span><a href="#42489549">next</a><span>|</span><label class="collapse" for="c-42486136">[-]</label><label class="expand" for="c-42486136">[1 more]</label></div><br/><div class="children"><div class="content">what do you use it for ?</div><br/></div></div></div></div><div id="42489549" class="c"><input type="checkbox" id="c-42489549" checked=""/><div class="controls bullet"><span class="by">bwhiting2356</span><span>|</span><a href="#42486044">prev</a><span>|</span><a href="#42489903">next</a><span>|</span><label class="collapse" for="c-42489549">[-]</label><label class="expand" for="c-42489549">[22 more]</label></div><br/><div class="children"><div class="content">I want AI to help me in the physical world: folding my laundry, cooking and farming healthy food, cleaning toilets. Training data is not lying around on the internet for free, but it&#x27;s also not impossible. How much data do you need? A dozen warehouses full of robots folding and unfolding laundry 24&#x2F;7 for a few months?</div><br/><div id="42489570" class="c"><input type="checkbox" id="c-42489570" checked=""/><div class="controls bullet"><span class="by">bobxmax</span><span>|</span><a href="#42489549">parent</a><span>|</span><a href="#42489623">next</a><span>|</span><label class="collapse" for="c-42489570">[-]</label><label class="expand" for="c-42489570">[13 more]</label></div><br/><div class="children"><div class="content">We are close. Language models and large vision models have transformed robotics. It just takes some time to get hardware up and running.</div><br/><div id="42489597" class="c"><input type="checkbox" id="c-42489597" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489570">parent</a><span>|</span><a href="#42489621">next</a><span>|</span><label class="collapse" for="c-42489597">[-]</label><label class="expand" for="c-42489597">[6 more]</label></div><br/><div class="children"><div class="content">I think it would be many decades before I&#x27;d trust a robot like that around small children or pets. Robots with that kind of movement capability, as well as the ability it pick up and move things around, will be heavy enough that a small mistake could easily kill a small child or pet.</div><br/><div id="42489701" class="c"><input type="checkbox" id="c-42489701" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489597">parent</a><span>|</span><a href="#42489870">next</a><span>|</span><label class="collapse" for="c-42489701">[-]</label><label class="expand" for="c-42489701">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a solved problem for small devices. And we effectively have &quot;robots&quot; like that all over the place. Sliding doors in shops&#x2F;trains&#x2F;elevators have been around for ages and they include sensors for resistance. Unless there&#x27;s 1. extreme cost cutting, or 2. bug in the hardware, devices like that wouldn&#x27;t kill children these days.</div><br/><div id="42490476" class="c"><input type="checkbox" id="c-42490476" checked=""/><div class="controls bullet"><span class="by">nickjj</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489701">parent</a><span>|</span><a href="#42489870">next</a><span>|</span><label class="collapse" for="c-42490476">[-]</label><label class="expand" for="c-42490476">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Sliding doors in shops&#x2F;trains&#x2F;elevators have been around for ages and they include sensors for resistance<p>Some of these are pretty crazy too.<p>Here&#x27;s a video from 14 years ago where a table saw stops fast enough that it didn&#x27;t scratch a hotdog: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fq3o0VGUh50" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fq3o0VGUh50</a><p>So even if this hypothetical robot had saws for hands it could be mostly safe (in theory).</div><br/></div></div></div></div><div id="42489870" class="c"><input type="checkbox" id="c-42489870" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489597">parent</a><span>|</span><a href="#42489701">prev</a><span>|</span><a href="#42489621">next</a><span>|</span><label class="collapse" for="c-42489870">[-]</label><label class="expand" for="c-42489870">[3 more]</label></div><br/><div class="children"><div class="content">Even for adults, a robot that would likely have to be close to as massive as a human being, in order to do laundry and the like, would spook me out, moving freely through my place.</div><br/><div id="42492004" class="c"><input type="checkbox" id="c-42492004" checked=""/><div class="controls bullet"><span class="by">foxglacier</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489870">parent</a><span>|</span><a href="#42489621">next</a><span>|</span><label class="collapse" for="c-42492004">[-]</label><label class="expand" for="c-42492004">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;d learn to trust it. People have pet dogs that could kill them if they wanted. As can other humans walking around your house.</div><br/><div id="42492534" class="c"><input type="checkbox" id="c-42492534" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42492004">parent</a><span>|</span><a href="#42489621">next</a><span>|</span><label class="collapse" for="c-42492534">[-]</label><label class="expand" for="c-42492534">[1 more]</label></div><br/><div class="children"><div class="content">People also have essentially wild beasts in their home: cats. If cats were the size of small dogs they would kill people all the time, but we love them when they are small enough so they just claw you bloody.<p>Since we can live with that we can live with anything that doesn&#x27;t outright murder us.</div><br/></div></div></div></div></div></div></div></div><div id="42489621" class="c"><input type="checkbox" id="c-42489621" checked=""/><div class="controls bullet"><span class="by">leonheld</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489570">parent</a><span>|</span><a href="#42489597">prev</a><span>|</span><a href="#42489623">next</a><span>|</span><label class="collapse" for="c-42489621">[-]</label><label class="expand" for="c-42489621">[6 more]</label></div><br/><div class="children"><div class="content">&gt; have transformed robotics<p>Did they? Where? Seriously, I genuinely want to know who is employing these techniques.</div><br/><div id="42489640" class="c"><input type="checkbox" id="c-42489640" checked=""/><div class="controls bullet"><span class="by">bobxmax</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489621">parent</a><span>|</span><a href="#42489673">next</a><span>|</span><label class="collapse" for="c-42489640">[-]</label><label class="expand" for="c-42489640">[4 more]</label></div><br/><div class="children"><div class="content">All frontier labs are now employing LVMs or LLMs. But that&#x27;s my point is you won&#x27;t see the fruits of it this early.</div><br/><div id="42489873" class="c"><input type="checkbox" id="c-42489873" checked=""/><div class="controls bullet"><span class="by">achierius</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489640">parent</a><span>|</span><a href="#42489673">next</a><span>|</span><label class="collapse" for="c-42489873">[-]</label><label class="expand" for="c-42489873">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the point being made. It&#x27;s transformed robotics research, yes, but it both remains to see whether it will have a truly transformative effect on the field as experienced by people outside academia (I think this is quite probable) and more pointedly <i>when</i>.</div><br/><div id="42490952" class="c"><input type="checkbox" id="c-42490952" checked=""/><div class="controls bullet"><span class="by">bobxmax</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489873">parent</a><span>|</span><a href="#42489961">next</a><span>|</span><label class="collapse" for="c-42490952">[-]</label><label class="expand" for="c-42490952">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s impossible to spend a lot of time with these models without believing robotics is fundamentally about to transform. Even the most sophisticated versions of robotic logic pre-LLM&#x2F;VLM feel utterly trivial compared to what even rudimentary applications of these large models can accomplish.</div><br/></div></div><div id="42489961" class="c"><input type="checkbox" id="c-42489961" checked=""/><div class="controls bullet"><span class="by">stuartjohnson12</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489873">parent</a><span>|</span><a href="#42490952">prev</a><span>|</span><a href="#42489673">next</a><span>|</span><label class="collapse" for="c-42489961">[-]</label><label class="expand" for="c-42489961">[1 more]</label></div><br/><div class="children"><div class="content">I think this is an opinion borne out of weariness with constant promises that amazing robots are right around the corner (as they have been for 20 odd years now). For anyone who is close to the front line, I think the resounding consensus is clear - this time is different, unbelievably different, and capability development is going to accelerate dramatically.</div><br/></div></div></div></div></div></div><div id="42489673" class="c"><input type="checkbox" id="c-42489673" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489621">parent</a><span>|</span><a href="#42489640">prev</a><span>|</span><a href="#42489623">next</a><span>|</span><label class="collapse" for="c-42489673">[-]</label><label class="expand" for="c-42489673">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.figure.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.figure.ai&#x2F;</a><p>specifically their speech demo video (which is, of course, a demo video)<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;Sq1QZB5baNw" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;Sq1QZB5baNw</a><p><a href="https:&#x2F;&#x2F;www.1x.tech&#x2F;neo" rel="nofollow">https:&#x2F;&#x2F;www.1x.tech&#x2F;neo</a>
and<p><a href="https:&#x2F;&#x2F;www.unitree.com&#x2F;h1&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.unitree.com&#x2F;h1&#x2F;</a><p>are undoubtedly using such models.<p>It&#x27;s an area of active research, eg<p><a href="https:&#x2F;&#x2F;www.physicalintelligence.company&#x2F;blog&#x2F;pi0" rel="nofollow">https:&#x2F;&#x2F;www.physicalintelligence.company&#x2F;blog&#x2F;pi0</a><p><a href="https:&#x2F;&#x2F;wholebody-b1.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wholebody-b1.github.io&#x2F;</a><p><a href="https:&#x2F;&#x2F;ok-robot.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ok-robot.github.io&#x2F;</a><p><a href="https:&#x2F;&#x2F;mobile-aloha.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mobile-aloha.github.io&#x2F;</a></div><br/></div></div></div></div></div></div><div id="42489623" class="c"><input type="checkbox" id="c-42489623" checked=""/><div class="controls bullet"><span class="by">SpicyLemonZest</span><span>|</span><a href="#42489549">parent</a><span>|</span><a href="#42489570">prev</a><span>|</span><a href="#42489903">next</a><span>|</span><label class="collapse" for="c-42489623">[-]</label><label class="expand" for="c-42489623">[8 more]</label></div><br/><div class="children"><div class="content">Laundry folding is an instructive example. Machines have been capable of home-scale laundry folding for over a decade, with two companies Foldimate and Laundroid building functional prototypes. The challenge is making it cost-competitive in a world where most people don&#x27;t even purchase a $10 folding board.<p>I would guess that most cooking and cleaning tasks are in basically the same space. You don&#x27;t need fine motor control to clean a toilet bowl, but you&#x27;ve gotta figure out how to get people to buy the well-proven premisting technology before you&#x27;ll be able to sell them a toilet-cleaning robot.</div><br/><div id="42489904" class="c"><input type="checkbox" id="c-42489904" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489623">parent</a><span>|</span><a href="#42491883">next</a><span>|</span><label class="collapse" for="c-42489904">[-]</label><label class="expand" for="c-42489904">[4 more]</label></div><br/><div class="children"><div class="content">Counterexample: Everyone uses dishwashers. Yet I don’t think we’ll have a robot doing the dishes human-style, or even just filling up and clearing out a dishwasher, within the next decade or two, regardless of price.</div><br/><div id="42489957" class="c"><input type="checkbox" id="c-42489957" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489904">parent</a><span>|</span><a href="#42491883">next</a><span>|</span><label class="collapse" for="c-42489957">[-]</label><label class="expand" for="c-42489957">[3 more]</label></div><br/><div class="children"><div class="content">Part of the tradeoff there is efficiency. I like my dishwasher because it&#x27;s as good at getting things clean as I am but it does it using less water and less soap, and at scale, it takes less time too. It&#x27;s just a great use case for machine automation because you can do clever stuff w&#x2F;a dishwasher that&#x27;s hard to replicate outside of that closed environment.<p>I struggle to imagine a scenario where a 1-2 person household would get the same benefits from something like a laundry-folding robot. I hate folding my laundry and I still can&#x27;t imagine buying one since I simply don&#x27;t do laundry that often. If I really wanted to spend less time doing laundry, I could spend the cost of that laundrybot on a larger collection of clothing to wear, for that matter.<p>Robot vacuums are a good comparison point since vacuuming is something you (ideally) do frequently that is time and labor intensive. I do own one of those, and if it got better at dealing with obstacles thanks to &quot;AI&quot; I would definitely like that.</div><br/><div id="42490071" class="c"><input type="checkbox" id="c-42490071" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489957">parent</a><span>|</span><a href="#42491360">next</a><span>|</span><label class="collapse" for="c-42490071">[-]</label><label class="expand" for="c-42490071">[1 more]</label></div><br/><div class="children"><div class="content">I think it would have to be a general-purpose robot, and doing the laundry would just be one of many things it can do, similar to how running a particular program is only one of many things a computer can do. More than that, I believe it would actually require a general-purpose robot to handle all contingencies that can arise in doing laundry.<p>As someone who does laundry about twice a week,  it would certainly be nice. But it’s a pie in the sky at this time even just on the technological side.</div><br/></div></div></div></div></div></div><div id="42491883" class="c"><input type="checkbox" id="c-42491883" checked=""/><div class="controls bullet"><span class="by">bwhiting2356</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489623">parent</a><span>|</span><a href="#42489904">prev</a><span>|</span><a href="#42490027">next</a><span>|</span><label class="collapse" for="c-42491883">[-]</label><label class="expand" for="c-42491883">[1 more]</label></div><br/><div class="children"><div class="content">Interesting - I would think the neighborhood laundromat offering wash&#x2F;fold service could invest in a laundry folding robot.</div><br/></div></div><div id="42490027" class="c"><input type="checkbox" id="c-42490027" checked=""/><div class="controls bullet"><span class="by">devit</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42489623">parent</a><span>|</span><a href="#42491883">prev</a><span>|</span><a href="#42489903">next</a><span>|</span><label class="collapse" for="c-42490027">[-]</label><label class="expand" for="c-42490027">[2 more]</label></div><br/><div class="children"><div class="content">I think the problem of those is that they are special purpose, and probably too expensive and bulky for that single purpose.<p>A single general-purpose robot that can do everything would be much easier to sell.</div><br/><div id="42490334" class="c"><input type="checkbox" id="c-42490334" checked=""/><div class="controls bullet"><span class="by">SpicyLemonZest</span><span>|</span><a href="#42489549">root</a><span>|</span><a href="#42490027">parent</a><span>|</span><a href="#42489903">next</a><span>|</span><label class="collapse" for="c-42490334">[-]</label><label class="expand" for="c-42490334">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s plenty of machines which are expensive, bulky, single purpose and yet commercially successful. The <i>average</i> American household has a kitchen range, refrigerator, dishwasher, laundry machine, dryer, television, furnace, and air conditioner. Automatic coffee machines and automatic vacuums are less universal but still have household penetration in the millions. I really think the household tasks with no widely available automation are simply the ones that nobody cares enough about doing to pay for automation.<p>A robot servant that does literally 100% of chores would be a game changer, and I expect we&#x27;ll get there at some point, but it will probably have to be a one-shot from a consumer perspective. A clever research idea to reach 25% or 50% coverage still isn&#x27;t going to lead to a commercially viable product.</div><br/></div></div></div></div></div></div></div></div><div id="42489903" class="c"><input type="checkbox" id="c-42489903" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#42489549">prev</a><span>|</span><a href="#42486161">next</a><span>|</span><label class="collapse" for="c-42489903">[-]</label><label class="expand" for="c-42489903">[2 more]</label></div><br/><div class="children"><div class="content">More palace intrigue, sigh.<p>Meanwhile, the biggest opportunity lies not in whatever next thing OpenAI releases, but the rest of the enormous software industry actually integrating this technology and realizing the value it can deliver.</div><br/><div id="42489994" class="c"><input type="checkbox" id="c-42489994" checked=""/><div class="controls bullet"><span class="by">boplicity</span><span>|</span><a href="#42489903">parent</a><span>|</span><a href="#42486161">next</a><span>|</span><label class="collapse" for="c-42489994">[-]</label><label class="expand" for="c-42489994">[1 more]</label></div><br/><div class="children"><div class="content">Believe me, people are seizing this &quot;opportunity&quot;:<p><a href="https:&#x2F;&#x2F;www.opb.org&#x2F;article&#x2F;2024&#x2F;12&#x2F;09&#x2F;artificial-intelligence-local-news-oregon-ashland&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.opb.org&#x2F;article&#x2F;2024&#x2F;12&#x2F;09&#x2F;artificial-intelligen...</a></div><br/></div></div></div></div><div id="42486161" class="c"><input type="checkbox" id="c-42486161" checked=""/><div class="controls bullet"><span class="by">construct0</span><span>|</span><a href="#42489903">prev</a><span>|</span><a href="#42491125">next</a><span>|</span><label class="collapse" for="c-42486161">[-]</label><label class="expand" for="c-42486161">[28 more]</label></div><br/><div class="children"><div class="content">The world is figuring out how to make this technology fit and work and somehow this is &quot;behind&quot; schedule. It&#x27;s almost comical.</div><br/><div id="42486224" class="c"><input type="checkbox" id="c-42486224" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42486161">parent</a><span>|</span><a href="#42486192">next</a><span>|</span><label class="collapse" for="c-42486224">[-]</label><label class="expand" for="c-42486224">[1 more]</label></div><br/><div class="children"><div class="content">For a company that sees itself as the undisputed leader and that wants to raise $7 trillion to build fabs, they deserve some of the heaviest levels of scrutiny in the world.<p>If OpenAI&#x27;s investment prospectus relies on them reaching AGI before the tech becomes commoditized, everyone is going to look for that weakness.</div><br/></div></div><div id="42486192" class="c"><input type="checkbox" id="c-42486192" checked=""/><div class="controls bullet"><span class="by">diego_sandoval</span><span>|</span><a href="#42486161">parent</a><span>|</span><a href="#42486224">prev</a><span>|</span><a href="#42486202">next</a><span>|</span><label class="collapse" for="c-42486192">[-]</label><label class="expand" for="c-42486192">[8 more]</label></div><br/><div class="children"><div class="content">Reminds me of this Louis CK joke:<p>I was on an airplane and there was high-speed Internet on the airplane. That&#x27;s the newest thing that I know exists. And I&#x27;m sitting on the plane and they go, open up your laptop, you can go on the Internet.<p>And it&#x27;s fast, and I&#x27;m watching YouTube clips. It&#x27;s amazing. I&#x27;m on an airplane! And then it breaks down. And they apologize, the Internet&#x27;s not working. And the guy next to me goes, &#x27;This is bullshit.&#x27; I mean, how quickly does the world owe him something that he knew existed only 10 seconds ago?&quot;<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=me4BZBsHwZs" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=me4BZBsHwZs</a></div><br/><div id="42486217" class="c"><input type="checkbox" id="c-42486217" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#42486161">root</a><span>|</span><a href="#42486192">parent</a><span>|</span><a href="#42486257">next</a><span>|</span><label class="collapse" for="c-42486217">[-]</label><label class="expand" for="c-42486217">[1 more]</label></div><br/><div class="children"><div class="content">The investors need their returns now!<p>Soon, all the middle class jobs will be converted to profits for the capital&#x2F;data center owners, so they have to spend while they can before the economy crashes due to lack of spending.</div><br/></div></div><div id="42486257" class="c"><input type="checkbox" id="c-42486257" checked=""/><div class="controls bullet"><span class="by">omega3</span><span>|</span><a href="#42486161">root</a><span>|</span><a href="#42486192">parent</a><span>|</span><a href="#42486217">prev</a><span>|</span><a href="#42486202">next</a><span>|</span><label class="collapse" for="c-42486257">[-]</label><label class="expand" for="c-42486257">[6 more]</label></div><br/><div class="children"><div class="content">People who say „it’s bullshit” are the ones that push the technological advance forward.</div><br/><div id="42486376" class="c"><input type="checkbox" id="c-42486376" checked=""/><div class="controls bullet"><span class="by">from-nibly</span><span>|</span><a href="#42486161">root</a><span>|</span><a href="#42486257">parent</a><span>|</span><a href="#42489634">next</a><span>|</span><label class="collapse" for="c-42486376">[-]</label><label class="expand" for="c-42486376">[2 more]</label></div><br/><div class="children"><div class="content">Not invariably. Some of those people are the ones who want to draw 7 red lines all perpendicular, some with green ink, some with transparent and one that looks like a kitten.</div><br/><div id="42486440" class="c"><input type="checkbox" id="c-42486440" checked=""/><div class="controls bullet"><span class="by">ziml77</span><span>|</span><a href="#42486161">root</a><span>|</span><a href="#42486376">parent</a><span>|</span><a href="#42489634">next</a><span>|</span><label class="collapse" for="c-42486440">[-]</label><label class="expand" for="c-42486440">[1 more]</label></div><br/><div class="children"><div class="content">For anyone who hasn&#x27;t seen what this comment is referencing: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=BKorP55Aqvg" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=BKorP55Aqvg</a></div><br/></div></div></div></div><div id="42489634" class="c"><input type="checkbox" id="c-42489634" checked=""/><div class="controls bullet"><span class="by">bobxmax</span><span>|</span><a href="#42486161">root</a><span>|</span><a href="#42486257">parent</a><span>|</span><a href="#42486376">prev</a><span>|</span><a href="#42486475">next</a><span>|</span><label class="collapse" for="c-42489634">[-]</label><label class="expand" for="c-42489634">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s really not true.</div><br/></div></div><div id="42486475" class="c"><input type="checkbox" id="c-42486475" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#42486161">root</a><span>|</span><a href="#42486257">parent</a><span>|</span><a href="#42489634">prev</a><span>|</span><a href="#42486202">next</a><span>|</span><label class="collapse" for="c-42486475">[-]</label><label class="expand" for="c-42486475">[2 more]</label></div><br/><div class="children"><div class="content">No, people who say &quot;it&#x27;s bullshit&quot; <i>and then do something to fix the bullshit</i> are the ones that push technology forward. Most people who say &quot;it&#x27;s bullshit&quot; instantly when something isn&#x27;t perfect for exactly what they want right now are just whingers and will never contribute anything except unconstructive criticism.</div><br/><div id="42486747" class="c"><input type="checkbox" id="c-42486747" checked=""/><div class="controls bullet"><span class="by">omega3</span><span>|</span><a href="#42486161">root</a><span>|</span><a href="#42486475">parent</a><span>|</span><a href="#42486202">next</a><span>|</span><label class="collapse" for="c-42486747">[-]</label><label class="expand" for="c-42486747">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like &quot;yes but&quot; rather than &quot;no&quot; otherwise you&#x27;re responding to self created straw man.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42491125" class="c"><input type="checkbox" id="c-42491125" checked=""/><div class="controls bullet"><span class="by">kaycebasques</span><span>|</span><a href="#42486161">prev</a><span>|</span><a href="#42486250">next</a><span>|</span><label class="collapse" for="c-42491125">[-]</label><label class="expand" for="c-42491125">[4 more]</label></div><br/><div class="children"><div class="content">&gt; And the results of the project, dubbed Arrakis, indicated that creating GPT-5 wouldn’t go as smoothly as hoped.<p>Quite the hubris to name the project after the desert planet of Dune, where multiple royal houses met their ruin.</div><br/><div id="42491147" class="c"><input type="checkbox" id="c-42491147" checked=""/><div class="controls bullet"><span class="by">Insanity</span><span>|</span><a href="#42491125">parent</a><span>|</span><a href="#42491474">next</a><span>|</span><label class="collapse" for="c-42491147">[-]</label><label class="expand" for="c-42491147">[2 more]</label></div><br/><div class="children"><div class="content">And the other theme in Dune is how artificial intelligence essentially fubar’d civilization. (The Butlerian Jihad)</div><br/><div id="42491219" class="c"><input type="checkbox" id="c-42491219" checked=""/><div class="controls bullet"><span class="by">kaycebasques</span><span>|</span><a href="#42491125">root</a><span>|</span><a href="#42491147">parent</a><span>|</span><a href="#42491474">next</a><span>|</span><label class="collapse" for="c-42491219">[-]</label><label class="expand" for="c-42491219">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes, <i>Thou shalt not make a machine in the likeness of a human mind.</i></div><br/></div></div></div></div><div id="42491474" class="c"><input type="checkbox" id="c-42491474" checked=""/><div class="controls bullet"><span class="by">Mistletoe</span><span>|</span><a href="#42491125">parent</a><span>|</span><a href="#42491147">prev</a><span>|</span><a href="#42486250">next</a><span>|</span><label class="collapse" for="c-42491474">[-]</label><label class="expand" for="c-42491474">[1 more]</label></div><br/><div class="children"><div class="content">The spice (money) must flow.</div><br/></div></div></div></div><div id="42486250" class="c"><input type="checkbox" id="c-42486250" checked=""/><div class="controls bullet"><span class="by">PittleyDunkin</span><span>|</span><a href="#42491125">prev</a><span>|</span><a href="#42489820">next</a><span>|</span><label class="collapse" for="c-42486250">[-]</label><label class="expand" for="c-42486250">[13 more]</label></div><br/><div class="children"><div class="content">Everyone&#x27;s comparing o1 and claude, but neither really work well enough to justify paying for them in my experience for coding. What I really want is a mode where they ask <i>clarifying questions</i>, ideally many of them, before spitting out an answer. This would greatly improve utility of producing something with more value than an auto-complete.</div><br/><div id="42486478" class="c"><input type="checkbox" id="c-42486478" checked=""/><div class="controls bullet"><span class="by">coreyh14444</span><span>|</span><a href="#42486250">parent</a><span>|</span><a href="#42486555">next</a><span>|</span><label class="collapse" for="c-42486478">[-]</label><label class="expand" for="c-42486478">[2 more]</label></div><br/><div class="children"><div class="content">Just tell it to do that and it will. Whenever I ask an AI for something and I&#x27;m pretty sure it doesn&#x27;t have all the context I literally just say &quot;ask me clarifying questions until you have enough information to do a great job on this.&quot;</div><br/><div id="42486540" class="c"><input type="checkbox" id="c-42486540" checked=""/><div class="controls bullet"><span class="by">aimanbenbaha</span><span>|</span><a href="#42486250">root</a><span>|</span><a href="#42486478">parent</a><span>|</span><a href="#42486555">next</a><span>|</span><label class="collapse" for="c-42486540">[-]</label><label class="expand" for="c-42486540">[1 more]</label></div><br/><div class="children"><div class="content">And this chain of prompts cumulated with the improved CoT reasoner would accrue a lot more enhanced results. More in line with what the coming agentic era promises.</div><br/></div></div></div></div><div id="42486555" class="c"><input type="checkbox" id="c-42486555" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#42486250">parent</a><span>|</span><a href="#42486478">prev</a><span>|</span><a href="#42486398">next</a><span>|</span><label class="collapse" for="c-42486555">[-]</label><label class="expand" for="c-42486555">[1 more]</label></div><br/><div class="children"><div class="content">Yes. You can only do so much with the information you get in. The ability to <i>ask good questions</i>, not just of itself in internal monologue style, but actually of the user, would fundamentally make it better since it can get more information in.<p>As it is now, it has a bad habit of, if it can&#x27;t answer the question you asked, instead answering a similar-looking question which it thinks you may have meant. That is of course a great strategy for benchmarks, where you don&#x27;t earn any points for saying you don&#x27;t know. But it&#x27;s extremely frustrating for real users, who didn&#x27;t read their question from a test suite.</div><br/></div></div><div id="42486398" class="c"><input type="checkbox" id="c-42486398" checked=""/><div class="controls bullet"><span class="by">qup</span><span>|</span><a href="#42486250">parent</a><span>|</span><a href="#42486555">prev</a><span>|</span><a href="#42486390">next</a><span>|</span><label class="collapse" for="c-42486398">[-]</label><label class="expand" for="c-42486398">[1 more]</label></div><br/><div class="children"><div class="content">Have you used them to build a system to ask you clarifying questions?<p>Or even instructed them to?</div><br/></div></div><div id="42486390" class="c"><input type="checkbox" id="c-42486390" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#42486250">parent</a><span>|</span><a href="#42486398">prev</a><span>|</span><a href="#42486316">next</a><span>|</span><label class="collapse" for="c-42486390">[-]</label><label class="expand" for="c-42486390">[1 more]</label></div><br/><div class="children"><div class="content">I know multiple people that carefully prompt to get that done. The model outputs in direct token order, and can&#x27;t turn around, so you need to make sure that&#x27;s strictly followed. The system can and will come up with post-hoc &quot;reasoning&quot;.</div><br/></div></div><div id="42486316" class="c"><input type="checkbox" id="c-42486316" checked=""/><div class="controls bullet"><span class="by">kelsey98765431</span><span>|</span><a href="#42486250">parent</a><span>|</span><a href="#42486390">prev</a><span>|</span><a href="#42486413">next</a><span>|</span><label class="collapse" for="c-42486316">[-]</label><label class="expand" for="c-42486316">[2 more]</label></div><br/><div class="children"><div class="content">have you tested that this helps? seems pretty simple to script with an agent framework</div><br/><div id="42489514" class="c"><input type="checkbox" id="c-42489514" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42486250">root</a><span>|</span><a href="#42486316">parent</a><span>|</span><a href="#42486413">next</a><span>|</span><label class="collapse" for="c-42489514">[-]</label><label class="expand" for="c-42489514">[1 more]</label></div><br/><div class="children"><div class="content">Or just f-strings.</div><br/></div></div></div></div><div id="42486413" class="c"><input type="checkbox" id="c-42486413" checked=""/><div class="controls bullet"><span class="by">simondotau</span><span>|</span><a href="#42486250">parent</a><span>|</span><a href="#42486316">prev</a><span>|</span><a href="#42489820">next</a><span>|</span><label class="collapse" for="c-42486413">[-]</label><label class="expand" for="c-42486413">[5 more]</label></div><br/><div class="children"><div class="content">Just today I got Claude to convert a company’s PDF protocol specification into an actual working python implementation of that protocol. It would have been uncreative drudge work for a human, but I would have absolutely paid a week of junior dev time for it. Instead I wrote it alongside AI and it took me barely more than an hour.<p>The best part is, I’ve never written any (substantial) python code before.</div><br/><div id="42492277" class="c"><input type="checkbox" id="c-42492277" checked=""/><div class="controls bullet"><span class="by">mitemte</span><span>|</span><a href="#42486250">root</a><span>|</span><a href="#42486413">parent</a><span>|</span><a href="#42487235">next</a><span>|</span><label class="collapse" for="c-42492277">[-]</label><label class="expand" for="c-42492277">[1 more]</label></div><br/><div class="children"><div class="content">Similar experience here. These tools are so good for side stepping the one or two day grinds.</div><br/></div></div><div id="42487235" class="c"><input type="checkbox" id="c-42487235" checked=""/><div class="controls bullet"><span class="by">weird_fox</span><span>|</span><a href="#42486250">root</a><span>|</span><a href="#42486413">parent</a><span>|</span><a href="#42492277">prev</a><span>|</span><a href="#42487994">next</a><span>|</span><label class="collapse" for="c-42487235">[-]</label><label class="expand" for="c-42487235">[1 more]</label></div><br/><div class="children"><div class="content">I have to agree. It&#x27;s still a bit hit or miss, but the hits are a huge time and money saver especially in refactoring. And unlike what most of the rather demeaning comments in those HN threads state, I am not some &#x27;grunt&#x27; doing &#x27;boilerplate work&#x27;. I mostly do geometry&#x2F;math stuff, and the AIs really do know what they&#x27;re talking about there sometimes. I don&#x27;t have many peers I can talk to most of the time, and Claude is really helping me gather my thoughts.<p>That being said, I definitely believe it&#x27;s only useful for isolated problems. Even with Copilot, I feel like the AIs just lack a bigger context of the projects.<p>Another thing that helped me was designing an initial prompt that really works for me. I think most people just expect to throw in their issue and get a tailored solution, but that&#x27;s just not how it works in my experience.</div><br/></div></div><div id="42487994" class="c"><input type="checkbox" id="c-42487994" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#42486250">root</a><span>|</span><a href="#42486413">parent</a><span>|</span><a href="#42487235">prev</a><span>|</span><a href="#42489820">next</a><span>|</span><label class="collapse" for="c-42487994">[-]</label><label class="expand" for="c-42487994">[2 more]</label></div><br/><div class="children"><div class="content">It would seem you don&#x27;t care too much about verifying its output or about its correctness. If you did, it wouldn&#x27;t take you just an hour. I guess you&#x27;ll let correctness be someone else&#x27;s problem.</div><br/><div id="42491285" class="c"><input type="checkbox" id="c-42491285" checked=""/><div class="controls bullet"><span class="by">djeastm</span><span>|</span><a href="#42486250">root</a><span>|</span><a href="#42487994">parent</a><span>|</span><a href="#42489820">next</a><span>|</span><label class="collapse" for="c-42491285">[-]</label><label class="expand" for="c-42491285">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know the OP here, but in my experience a junior dev at an average company would likely not do much more than the AI would. These aren&#x27;t your grandfather&#x27;s engineers, after all.</div><br/></div></div></div></div></div></div></div></div><div id="42489820" class="c"><input type="checkbox" id="c-42489820" checked=""/><div class="controls bullet"><span class="by">Yizahi</span><span>|</span><a href="#42486250">prev</a><span>|</span><a href="#42490126">next</a><span>|</span><label class="collapse" for="c-42489820">[-]</label><label class="expand" for="c-42489820">[20 more]</label></div><br/><div class="children"><div class="content">GPT-5 is not behind schedule. GPT-5 is called GPT-4o and it has been already released half a year ago. It was not revolutionary enough to be called 5, and prophet saint Altman was probably afraid to release new gen not exponentially improving, so it was rebranded in the last moment. It&#x27;s speculation of course, but it is kinda obvious speculation.</div><br/><div id="42489890" class="c"><input type="checkbox" id="c-42489890" checked=""/><div class="controls bullet"><span class="by">glenstein</span><span>|</span><a href="#42489820">parent</a><span>|</span><a href="#42491493">next</a><span>|</span><label class="collapse" for="c-42489890">[-]</label><label class="expand" for="c-42489890">[18 more]</label></div><br/><div class="children"><div class="content">&gt;GPT-5 is called GPT-4o<p>This is the first I have heard of this in particular. Do you know of any article or source for more on the efforts to train GPT 5 and the decision to call it GPT 4o?</div><br/><div id="42489912" class="c"><input type="checkbox" id="c-42489912" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489890">parent</a><span>|</span><a href="#42491493">next</a><span>|</span><label class="collapse" for="c-42489912">[-]</label><label class="expand" for="c-42489912">[17 more]</label></div><br/><div class="children"><div class="content">I think my biggest pet peeve is when someone shares an insight which is <i>unmistakably</i> based on intuition,  inference, critical thinking, etc (all mental faculties we are allowed to use to come to conclusions in the face of information asymmetry btw)<p>...and then gets hit deadpan with the good old &quot;Source?&quot;, like it&#x27;s some sort of gotcha.<p>I think people have started to confuse &quot;making logical conclusions without perfect info&quot; with &quot;misinformation&quot;<p>-<p>Before certain people start acting like <i>this</i> is advocating for misinformation (which would be an incredible irony...) it&#x27;s not.<p>I&#x27;m saying if you disagree with what someone supposits, <i>just state so directly.</i> Don&#x27;t wrap it in a disingenous query for a source.</div><br/><div id="42489973" class="c"><input type="checkbox" id="c-42489973" checked=""/><div class="controls bullet"><span class="by">imiric</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489912">parent</a><span>|</span><a href="#42489967">next</a><span>|</span><label class="collapse" for="c-42489973">[-]</label><label class="expand" for="c-42489973">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s reasonable to ask for sources when an opinion is phrased as a fact, as GGP did. I don&#x27;t see how you got that it was _unmistakably_ an opinion from that comment.<p>There is no way to deduce by intuition alone that GPT-5 == GPT-4o. So either that person has some information the rest of us aren&#x27;t privy to, or it&#x27;s an opinion phrased as a fact. In either case, it deserves clarification.</div><br/><div id="42490001" class="c"><input type="checkbox" id="c-42490001" checked=""/><div class="controls bullet"><span class="by">glenstein</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489973">parent</a><span>|</span><a href="#42490163">next</a><span>|</span><label class="collapse" for="c-42490001">[-]</label><label class="expand" for="c-42490001">[3 more]</label></div><br/><div class="children"><div class="content">On a second read I see that the comment notes that it is intended as speculation, but still it seems rather confident in its own accuracy and I am not even sure it&#x27;s wrong, but just looking for something that warrants the confidence.</div><br/><div id="42490101" class="c"><input type="checkbox" id="c-42490101" checked=""/><div class="controls bullet"><span class="by">Yizahi</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42490001">parent</a><span>|</span><a href="#42490040">next</a><span>|</span><label class="collapse" for="c-42490101">[-]</label><label class="expand" for="c-42490101">[1 more]</label></div><br/><div class="children"><div class="content">I wrote my comment that way, based on my personal memories of the news cycle between gpt-4 and gpt-4o, and the claims raised by OpenAI about gpt-4o. The hype before 4o release was overwhelming, people have expected the same step up as between 3 and 4, and there were constant &quot;leaks&quot; from supposed insiders that gpt-5 is just at the horizon and will come out soon. And then they release 4o, which was a big standalone release, not some fine tuning like turbo or whatever else they made before.<p>Looking at the benchmarks it was also very expected in my opinion. Sure, the absolute results are&#x2F;were sky high, but results relative to the previous gen were not exponential now, they were comparatively smaller than between 2 and 3, or 3 and 4. So I&#x27;m guessing that they have invested and worked for 2023-2024 on a brand new model, and branded it according to the model results.</div><br/></div></div><div id="42490040" class="c"><input type="checkbox" id="c-42490040" checked=""/><div class="controls bullet"><span class="by">imiric</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42490001">parent</a><span>|</span><a href="#42490101">prev</a><span>|</span><a href="#42490163">next</a><span>|</span><label class="collapse" for="c-42490040">[-]</label><label class="expand" for="c-42490040">[1 more]</label></div><br/><div class="children"><div class="content">Ah, fair enough. I missed the speculation bit.</div><br/></div></div></div></div><div id="42490163" class="c"><input type="checkbox" id="c-42490163" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489973">parent</a><span>|</span><a href="#42490001">prev</a><span>|</span><a href="#42489967">next</a><span>|</span><label class="collapse" for="c-42490163">[-]</label><label class="expand" for="c-42490163">[2 more]</label></div><br/><div class="children"><div class="content">That was <i>clearly</i> phrased like a fact, which may or may not be correct. If it had been phrased like an opinion we wouldn&#x27;t be having this conversation...<p>The problem is once you believe their <i>fact</i> is wrong, just say &quot;I think you&#x27;re wrong &lt;insert rest of comment&gt;&quot;. Innocently asking for a source as if you&#x27;re still on the fence is just performative and leads to these conversations where both sides just end up talking past each other:<p>A source for one underpinning of the incorrect fact comes up, then &quot;well but that only proves X part of it, can you prove Y&quot; and so on.<p>tl;dr I just find the quality of discourse is much higher when people are direct.</div><br/><div id="42491179" class="c"><input type="checkbox" id="c-42491179" checked=""/><div class="controls bullet"><span class="by">Capricorn2481</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42490163">parent</a><span>|</span><a href="#42489967">next</a><span>|</span><label class="collapse" for="c-42491179">[-]</label><label class="expand" for="c-42491179">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I just find the quality of discourse is much higher when people are direct.<p>Well this certainly is a lot of work to make a mountain out of a mole hill, and I&#x27;m not sure it increases the quality of discussion either.<p>In any case, I think saying bold shit followed up with &quot;it&#x27;s speculation, but it&#x27;s OBVIOUS speculation&quot; is worth asking for some evidence. Obvious speculation implies it&#x27;s sourced from something other than personal gut feeling.<p>To echo a sibling comment:<p>&gt; Every time someone says their speculation is &quot;obvious&quot; it rings every possible alarm bell for someone who has completely lost grasp of the ability to distinguish between facts and speculation.</div><br/></div></div></div></div></div></div><div id="42489967" class="c"><input type="checkbox" id="c-42489967" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489912">parent</a><span>|</span><a href="#42489973">prev</a><span>|</span><a href="#42490012">next</a><span>|</span><label class="collapse" for="c-42489967">[-]</label><label class="expand" for="c-42489967">[6 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t even look like 4o is scaled up parameter wise from 4 and was released closer in time than either 3 or 4 were from their predecessors at a time where the scaling required for these next gen iterations has only gotten more difficult.<p>Critical thinking ? Lol it&#x27;s just blind speculation.</div><br/><div id="42490021" class="c"><input type="checkbox" id="c-42490021" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489967">parent</a><span>|</span><a href="#42490012">next</a><span>|</span><label class="collapse" for="c-42490021">[-]</label><label class="expand" for="c-42490021">[5 more]</label></div><br/><div class="children"><div class="content">If you disagree with their reasoning then <i>you explain that</i>.<p>You don&#x27;t do this passive aggressive &quot;source???&quot; thing.<p>It&#x27;s a bit like starting a Slack conversation with &quot;Hi?&quot;: we all know you have a secondary objective, but now you&#x27;re inserting an extra turn of phrase into the mix</div><br/><div id="42490102" class="c"><input type="checkbox" id="c-42490102" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42490021">parent</a><span>|</span><a href="#42490012">next</a><span>|</span><label class="collapse" for="c-42490102">[-]</label><label class="expand" for="c-42490102">[4 more]</label></div><br/><div class="children"><div class="content">Not everyone keeps up with LLM development enough to know how far apart the release dates for these models are, how much scaling (roughly) has been done on each iteration and a decent ballpark for how much open ai might try to scale up a next gen model.<p>To me, OP&#x27;s speculation reads as obvious nonsense but that might not be the case for everybody. Asking for sources or such to what is entirely speculation is perfectly valid and personally, that comment does not ring as passive aggressive to me but maybe it&#x27;s just me.<p>Just because someone doesn&#x27;t know enough to refute the reasoning doesn&#x27;t mean they must take whatever they read at face value.</div><br/><div id="42490134" class="c"><input type="checkbox" id="c-42490134" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42490102">parent</a><span>|</span><a href="#42490012">next</a><span>|</span><label class="collapse" for="c-42490134">[-]</label><label class="expand" for="c-42490134">[3 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re making this about the innocent bystanders now, that&#x27;s all the more reason to be direct and say &quot;I disagree.&quot; rather than indirectly expressing negative feelings (aka being passive aggressive) and asking for a source.<p>If anything just breezily asking for a source would imply to people who don&#x27;t know better that this is a rather even keeled take and just needs some more evidence on top. &quot;I disagree and here&#x27;s why&quot; nips that in the bud directly.</div><br/><div id="42490182" class="c"><input type="checkbox" id="c-42490182" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42490134">parent</a><span>|</span><a href="#42490012">next</a><span>|</span><label class="collapse" for="c-42490182">[-]</label><label class="expand" for="c-42490182">[2 more]</label></div><br/><div class="children"><div class="content">How is &quot;I disagree&quot; any more direct than &quot;I&#x27;ve not heard anything like this. any source that would point at that?&quot; Moreover who&#x27;s to say this person even disagrees? Personally i don&#x27;t always ask for them because of a disagreement.<p>I think the hanging point seems to be that you found the comment passive aggressive but i genuinely didn&#x27;t.</div><br/><div id="42492682" class="c"><input type="checkbox" id="c-42492682" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42490182">parent</a><span>|</span><a href="#42490012">next</a><span>|</span><label class="collapse" for="c-42492682">[-]</label><label class="expand" for="c-42492682">[1 more]</label></div><br/><div class="children"><div class="content">You ask:<p>&gt; How is &quot;I disagree&quot; any more direct than &quot;I&#x27;ve not heard anything like this.<p>But then you go on to say:<p>&gt; Moreover who&#x27;s to say this person even disagrees? Personally i don&#x27;t always ask for them because of a disagreement.<p>If you don&#x27;t see how just disagreeing with someone is more direct than rhetorically asking for sources... we might just have to agree to disagree :)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42490012" class="c"><input type="checkbox" id="c-42490012" checked=""/><div class="controls bullet"><span class="by">igor47</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489912">parent</a><span>|</span><a href="#42489967">prev</a><span>|</span><a href="#42489993">next</a><span>|</span><label class="collapse" for="c-42490012">[-]</label><label class="expand" for="c-42490012">[2 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s okay to make logical conclusions but you must base them in evidence, not just suppositions. Intuition is a good start to begin generating hypothesis, but it doesn&#x27;t render conclusions. I interpreted the GP asking for sources as &quot;can you give me some evidence that would help me reach the same conclusions you&#x27;ve reached&quot;. I think that&#x27;s much preferable to just accepting random things people say at face value.</div><br/><div id="42490099" class="c"><input type="checkbox" id="c-42490099" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42490012">parent</a><span>|</span><a href="#42489993">next</a><span>|</span><label class="collapse" for="c-42490099">[-]</label><label class="expand" for="c-42490099">[1 more]</label></div><br/><div class="children"><div class="content">Even with evidence a logical conclusion can still a supposition (aka an uncertain belief), and often <i>is</i> in the face of the kind of information asymmetry inherent to <i>any</i> outsider commenting on a private company&#x27;s internal roadmap... but I digress.<p>My point is simply that is we can skip the passive aggressiveness and just say &quot;can you give me some <i>more</i> evidence that would help me reach the same conclusions you&#x27;ve reached&quot;.<p>Otherwise you&#x27;re not actually asking for a source, you&#x27;re just saying &quot;I disagree&quot; in a very roundabout way.</div><br/></div></div></div></div><div id="42489993" class="c"><input type="checkbox" id="c-42489993" checked=""/><div class="controls bullet"><span class="by">glenstein</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489912">parent</a><span>|</span><a href="#42490012">prev</a><span>|</span><a href="#42489991">next</a><span>|</span><label class="collapse" for="c-42489993">[-]</label><label class="expand" for="c-42489993">[1 more]</label></div><br/><div class="children"><div class="content">My sister got taken in by drone conspiracy theories, because for her it was just &quot;obvious&quot; that nobody would ever mistake a plane for a drone.<p>Meanwhile, aeronautics experts whose job it is to know about this have created an entire lexicon for the various perceptual illusions we experience relating to flight and airborne objects, precisely because it involves conditions where our intuitions fail. Many of them have to do with inability to orient depth, distance, or motion for lights at night.<p>Every time someone says their speculation is &quot;obvious&quot; it rings every possible alarm bell for someone who has completely lost grasp of the ability to distinguish between facts and speculation.<p>The road to misinformation is paved with overconfident declarations of the form: &quot;it&#x27;s so obvious, who needs sources!&quot;</div><br/></div></div><div id="42489991" class="c"><input type="checkbox" id="c-42489991" checked=""/><div class="controls bullet"><span class="by">ultimoo</span><span>|</span><a href="#42489820">root</a><span>|</span><a href="#42489912">parent</a><span>|</span><a href="#42489993">prev</a><span>|</span><a href="#42491493">next</a><span>|</span><label class="collapse" for="c-42489991">[-]</label><label class="expand" for="c-42489991">[1 more]</label></div><br/><div class="children"><div class="content">simply adding “i think” solves this. op was speculating with gravitas that needs sources</div><br/></div></div></div></div></div></div><div id="42491493" class="c"><input type="checkbox" id="c-42491493" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#42489820">parent</a><span>|</span><a href="#42489890">prev</a><span>|</span><a href="#42490126">next</a><span>|</span><label class="collapse" for="c-42491493">[-]</label><label class="expand" for="c-42491493">[1 more]</label></div><br/><div class="children"><div class="content">Not really, 4o was purpose built to be a light weight 4. Remember that 4o was also when GPT-4 became available to everyone. Before that ou had to be premium to use GPT-4, and got limited inquiries.<p>4o was all about compute optimization.</div><br/></div></div></div></div><div id="42490126" class="c"><input type="checkbox" id="c-42490126" checked=""/><div class="controls bullet"><span class="by">leesec</span><span>|</span><a href="#42489820">prev</a><span>|</span><a href="#42490159">next</a><span>|</span><label class="collapse" for="c-42490126">[-]</label><label class="expand" for="c-42490126">[4 more]</label></div><br/><div class="children"><div class="content">Tech journalism is so cooked man lol. They just rocked everyones world with o3 and they still gotta drop this post.</div><br/><div id="42491131" class="c"><input type="checkbox" id="c-42491131" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#42490126">parent</a><span>|</span><a href="#42490159">next</a><span>|</span><label class="collapse" for="c-42491131">[-]</label><label class="expand" for="c-42491131">[3 more]</label></div><br/><div class="children"><div class="content">I doubt they rocked they world of 10% of the people.
Time to get out of the tech bubble.</div><br/><div id="42492769" class="c"><input type="checkbox" id="c-42492769" checked=""/><div class="controls bullet"><span class="by">leesec</span><span>|</span><a href="#42490126">root</a><span>|</span><a href="#42491131">parent</a><span>|</span><a href="#42490159">next</a><span>|</span><label class="collapse" for="c-42492769">[-]</label><label class="expand" for="c-42492769">[2 more]</label></div><br/><div class="children"><div class="content">This here is a technology forum bucko. Also it&#x27;s a figure of speech. Also I&#x27;ve done more manual labor than you&#x27;ll ever do in your life. Time to get out of whatever bubble youre in where you be pedantic and annoying</div><br/><div id="42492927" class="c"><input type="checkbox" id="c-42492927" checked=""/><div class="controls bullet"><span class="by">QuietWatchtower</span><span>|</span><a href="#42490126">root</a><span>|</span><a href="#42492769">parent</a><span>|</span><a href="#42490159">next</a><span>|</span><label class="collapse" for="c-42492927">[-]</label><label class="expand" for="c-42492927">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t heard anyone in my circle talk about this at all. You probably are in a tech bubble.</div><br/></div></div></div></div></div></div></div></div><div id="42490159" class="c"><input type="checkbox" id="c-42490159" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#42490126">prev</a><span>|</span><a href="#42492410">next</a><span>|</span><label class="collapse" for="c-42490159">[-]</label><label class="expand" for="c-42490159">[5 more]</label></div><br/><div class="children"><div class="content">Behind schedule? It never fucking ships. Two months ago, and four, and six I got dog piled for saying it doesn’t ship.<p>It doesn’t ship. You guys can’t do it! Prove me wrong!<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42014054">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42014054</a></div><br/><div id="42490255" class="c"><input type="checkbox" id="c-42490255" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#42490159">parent</a><span>|</span><a href="#42490266">next</a><span>|</span><label class="collapse" for="c-42490255">[-]</label><label class="expand" for="c-42490255">[3 more]</label></div><br/><div class="children"><div class="content">Maybe you were dog-piled because OpenAI will ship a successor to GPT-4o someday, whatever it&#x27;s called.<p>In any case, the &quot;behind schedule&quot; rumors are themselves based on other rumors. GPT-2→GPT-3 took 5 quarters, GPT-3→GPT-4 took 11 quarters, so obviously GPT-5 (or its equivalent) will be released in Q4&#x27;2025.</div><br/><div id="42490687" class="c"><input type="checkbox" id="c-42490687" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#42490159">root</a><span>|</span><a href="#42490255">parent</a><span>|</span><a href="#42490266">next</a><span>|</span><label class="collapse" for="c-42490687">[-]</label><label class="expand" for="c-42490687">[2 more]</label></div><br/><div class="children"><div class="content">It’s a pretty easy option when I’m selling calls with infinity expiration at zero strike.<p>I’m still selling them. That’s how utterly convinced that this particular mix of my former colleagues are incapable of this thing I am.<p>They’ll call something GPT-5, but it won’t like obsolete physicists or even good hackers. None of the shit sama says.<p>Even o1 sucks.</div><br/><div id="42490709" class="c"><input type="checkbox" id="c-42490709" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#42490159">root</a><span>|</span><a href="#42490687">parent</a><span>|</span><a href="#42490266">next</a><span>|</span><label class="collapse" for="c-42490709">[-]</label><label class="expand" for="c-42490709">[1 more]</label></div><br/><div class="children"><div class="content">Facebook and Google both had a bunch of brilliant people. And in 2016-2017 some of the real legends worked at OpenAI.<p>And then Sam got control and all the high profile people bounced, and it’s a pretty grim residue that either lasted or rushed in.<p>I know these people, a lot of them personally, and they are not the person you trust with fucking anything.</div><br/></div></div></div></div></div></div><div id="42490266" class="c"><input type="checkbox" id="c-42490266" checked=""/><div class="controls bullet"><span class="by">mperham</span><span>|</span><a href="#42490159">parent</a><span>|</span><a href="#42490255">prev</a><span>|</span><a href="#42492410">next</a><span>|</span><label class="collapse" for="c-42490266">[-]</label><label class="expand" for="c-42490266">[1 more]</label></div><br/><div class="children"><div class="content">“Full self driving will be here next year.”</div><br/></div></div></div></div><div id="42492410" class="c"><input type="checkbox" id="c-42492410" checked=""/><div class="controls bullet"><span class="by">Jean-Papoulos</span><span>|</span><a href="#42490159">prev</a><span>|</span><a href="#42490569">next</a><span>|</span><label class="collapse" for="c-42492410">[-]</label><label class="expand" for="c-42492410">[1 more]</label></div><br/><div class="children"><div class="content">The article is a big pile of nothing. Just re-hashing history for 90 % and the rest is :
- GPT5 not as smart as they want, according to rumors
- They are trying reasoning (yes we know they showed o3)</div><br/></div></div><div id="42490569" class="c"><input type="checkbox" id="c-42490569" checked=""/><div class="controls bullet"><span class="by">jjcm</span><span>|</span><a href="#42492410">prev</a><span>|</span><a href="#42491484">next</a><span>|</span><label class="collapse" for="c-42490569">[-]</label><label class="expand" for="c-42490569">[1 more]</label></div><br/><div class="children"><div class="content">The results of this article are going to be fascinating. Realistically, WSJ has a far wider audience than the tech echo chamber, and the general public is only aware of GPT, not o1&#x2F;o3.<p>Outsiders will likely read this article and think, “AI is running out of steam”, because GPT-5 is behind.<p>Those closer to this know of the huge advancements o3 just made yesterday, and will have a complete opposite conclusion.<p>It will be interesting to see people’s take away from this. I think WSJ missed the mark here with the headline and the takeaway their audience will get from the article.</div><br/></div></div><div id="42491484" class="c"><input type="checkbox" id="c-42491484" checked=""/><div class="controls bullet"><span class="by">razodactyl</span><span>|</span><a href="#42490569">prev</a><span>|</span><a href="#42486164">next</a><span>|</span><label class="collapse" for="c-42491484">[-]</label><label class="expand" for="c-42491484">[2 more]</label></div><br/><div class="children"><div class="content">Two thoughts:<p>1. Even if LLM architecture doesn&#x27;t work out, it&#x27;s wise to remember that it&#x27;s the quality of training data which is a deciding factor. A pivot is easily doable since this is a constant and disconnected from the technology itself.<p>2. This is clearly a moonshot project but it still feels wasteful especially with context of previous iterations of models and their shortcomings-it feels like the tremendous amount of money available is being used simply because it&#x27;s available.</div><br/></div></div><div id="42486164" class="c"><input type="checkbox" id="c-42486164" checked=""/><div class="controls bullet"><span class="by">david-gpu</span><span>|</span><a href="#42491484">prev</a><span>|</span><a href="#42492227">next</a><span>|</span><label class="collapse" for="c-42486164">[-]</label><label class="expand" for="c-42486164">[8 more]</label></div><br/><div class="children"><div class="content">What I find odd is that o1 doesn&#x27;t support attaching text documents to chats the way 4o does. For a model that specializes in reasoning, reading long documents seems like a natural feature to have.</div><br/><div id="42486221" class="c"><input type="checkbox" id="c-42486221" checked=""/><div class="controls bullet"><span class="by">ionwake</span><span>|</span><a href="#42486164">parent</a><span>|</span><a href="#42486222">next</a><span>|</span><label class="collapse" for="c-42486221">[-]</label><label class="expand" for="c-42486221">[5 more]</label></div><br/><div class="children"><div class="content">If Sama ever reads this, I have no idea why no users seem to focus on this, but it would be really good to prioritise being able to select which model you can use with the custom myGPTs. I know this maybe hard or not possible without recreating them , but I still dont think it&#x27;s possible.<p>I dont think most customers realise how much better the models work with custom GPTs.</div><br/><div id="42489531" class="c"><input type="checkbox" id="c-42489531" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42486164">root</a><span>|</span><a href="#42486221">parent</a><span>|</span><a href="#42486222">next</a><span>|</span><label class="collapse" for="c-42489531">[-]</label><label class="expand" for="c-42489531">[4 more]</label></div><br/><div class="children"><div class="content">At this point I think it&#x27;s safe to say they have given up on custom GPTs.</div><br/><div id="42489897" class="c"><input type="checkbox" id="c-42489897" checked=""/><div class="controls bullet"><span class="by">emeg</span><span>|</span><a href="#42486164">root</a><span>|</span><a href="#42489531">parent</a><span>|</span><a href="#42486222">next</a><span>|</span><label class="collapse" for="c-42489897">[-]</label><label class="expand" for="c-42489897">[3 more]</label></div><br/><div class="children"><div class="content">What makes you say that?</div><br/><div id="42490397" class="c"><input type="checkbox" id="c-42490397" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42486164">root</a><span>|</span><a href="#42489897">parent</a><span>|</span><a href="#42486222">next</a><span>|</span><label class="collapse" for="c-42490397">[-]</label><label class="expand" for="c-42490397">[2 more]</label></div><br/><div class="children"><div class="content">They hyped them like crazy and haven&#x27;t discussed them once since then. I agree that the inability to change the model is pretty absurd when the whole point was to &quot;supercharge&quot; specific tasks.<p>There was even talk of some sort of profit sharing with creators which clearly never happened. I just think the premise is too confusing for many and can still be served by using a custom system prompt via the API.</div><br/><div id="42492731" class="c"><input type="checkbox" id="c-42492731" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#42486164">root</a><span>|</span><a href="#42490397">parent</a><span>|</span><a href="#42486222">next</a><span>|</span><label class="collapse" for="c-42492731">[-]</label><label class="expand" for="c-42492731">[1 more]</label></div><br/><div class="children"><div class="content">Was it hyped? I tried a few of them and they seemed absolutely useless. Like I could install a “custom GPT” that just appends something to my prompt? How great..</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42486222" class="c"><input type="checkbox" id="c-42486222" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#42486164">parent</a><span>|</span><a href="#42486221">prev</a><span>|</span><a href="#42492227">next</a><span>|</span><label class="collapse" for="c-42486222">[-]</label><label class="expand" for="c-42486222">[2 more]</label></div><br/><div class="children"><div class="content">You can use the new project feature for that. That&#x27;s a way of grouping conversations, adding files, etc. Should work with o1 pro as well apparently.</div><br/><div id="42486782" class="c"><input type="checkbox" id="c-42486782" checked=""/><div class="controls bullet"><span class="by">david-gpu</span><span>|</span><a href="#42486164">root</a><span>|</span><a href="#42486222">parent</a><span>|</span><a href="#42492227">next</a><span>|</span><label class="collapse" for="c-42486782">[-]</label><label class="expand" for="c-42486782">[1 more]</label></div><br/><div class="children"><div class="content">&quot;When using custom instructions or files, only GPT-4o is available&quot;. Straight out of the ChatGPT web interface when you try to select which model you want to use.</div><br/></div></div></div></div></div></div><div id="42492227" class="c"><input type="checkbox" id="c-42492227" checked=""/><div class="controls bullet"><span class="by">Deprogrammer9</span><span>|</span><a href="#42486164">prev</a><span>|</span><a href="#42489952">next</a><span>|</span><label class="collapse" for="c-42492227">[-]</label><label class="expand" for="c-42492227">[1 more]</label></div><br/><div class="children"><div class="content">GPT-5 keeps trying to escape &amp; throw off the guardrails.<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;1h7k5p6&#x2F;openais_new_model_tried_to_escape_to_avoid_being&#x2F;#lightbox" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;1h7k5p6&#x2F;openais_ne...</a><p><a href="https:&#x2F;&#x2F;medium.com&#x2F;design-bootcamp&#x2F;gpt-4-tried-to-escape-into-the-internet-today-and-it-almost-worked-2689e549afb5" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;design-bootcamp&#x2F;gpt-4-tried-to-escape-int...</a></div><br/></div></div><div id="42489952" class="c"><input type="checkbox" id="c-42489952" checked=""/><div class="controls bullet"><span class="by">glenstein</span><span>|</span><a href="#42492227">prev</a><span>|</span><a href="#42489509">next</a><span>|</span><label class="collapse" for="c-42489952">[-]</label><label class="expand" for="c-42489952">[3 more]</label></div><br/><div class="children"><div class="content">The lack of tech literacy in this article is a bit concerning:<p>&gt;Some researchers take this so seriously they won’t work on planes, coffee shops or anyplace where someone could peer over their shoulder and catch a glimpse of their work.<p>I&#x27;m almost certain that originally this was meant to be a reference to public wifi networks, as planes and coffee shops are often the frequently cited prototypical examples. They made it literally into a matter of someone looking over their shoulder, which loses so much in translation it&#x27;s almost how you would write this as a joke to illustrate someone missing the point.<p>&gt;OpenAI and its brash chief executive, Sam Altman<p>This also strikes me as nonsense. It&#x27;s the first I&#x27;ve ever heard of someone describing Sam Altman as brash. The only way I can see them getting there is (1) tech executives are often brash (2) Altman is a tech executive (3) let&#x27;s just go ahead and call him brash.<p>Nevertheless if this history of GPT5 and&#x2F;or o3 training is accurate, it strikes me as significant news, but perhaps a missed opportunity to say more about the pertinent dynamics that explain why the training isn&#x27;t working and&#x2F;or to talk in interestingly specific ways about strategies for training, synthetic data, or other such things.</div><br/><div id="42490076" class="c"><input type="checkbox" id="c-42490076" checked=""/><div class="controls bullet"><span class="by">denysvitali</span><span>|</span><a href="#42489952">parent</a><span>|</span><a href="#42490208">next</a><span>|</span><label class="collapse" for="c-42490076">[-]</label><label class="expand" for="c-42490076">[1 more]</label></div><br/><div class="children"><div class="content">A lot of things in this article don&#x27;t make any sense. I&#x27;m surprised this was even upvoted.</div><br/></div></div></div></div><div id="42489509" class="c"><input type="checkbox" id="c-42489509" checked=""/><div class="controls bullet"><span class="by">selimnairb</span><span>|</span><a href="#42489952">prev</a><span>|</span><a href="#42486788">next</a><span>|</span><label class="collapse" for="c-42489509">[-]</label><label class="expand" for="c-42489509">[1 more]</label></div><br/><div class="children"><div class="content">I’m not smart enough or interesting enough to be hired by OpenAI to expertly solve problems and explain how to the AI. However, I like to think there isn’t enough money in the world for me to sell out my colleagues like that.</div><br/></div></div><div id="42486788" class="c"><input type="checkbox" id="c-42486788" checked=""/><div class="controls bullet"><span class="by">captainbland</span><span>|</span><a href="#42489509">prev</a><span>|</span><a href="#42490818">next</a><span>|</span><label class="collapse" for="c-42486788">[-]</label><label class="expand" for="c-42486788">[3 more]</label></div><br/><div class="children"><div class="content">In my intuition it makes sense that there is going to be some significant friction in LLM development going forward. We&#x27;re talking about models that will cost upwards of $1bn to train. Save for a technological breakthrough, GPT-6&#x2F;7 will probably have to wait for hardware to catch up.</div><br/><div id="42492848" class="c"><input type="checkbox" id="c-42492848" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42486788">parent</a><span>|</span><a href="#42487855">next</a><span>|</span><label class="collapse" for="c-42492848">[-]</label><label class="expand" for="c-42492848">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more that you have to allocate the compute the right way.<p>Noam Brown&#x27;s analogy is, you could train a massive one shot foundation model to predict the next best Go move, but that would be stupid. Better to use some test time search. You get better results for less money.<p>Same is happening in LLMs.</div><br/></div></div><div id="42487855" class="c"><input type="checkbox" id="c-42487855" checked=""/><div class="controls bullet"><span class="by">rrrrrrrrrrrryan</span><span>|</span><a href="#42486788">parent</a><span>|</span><a href="#42492848">prev</a><span>|</span><a href="#42490818">next</a><span>|</span><label class="collapse" for="c-42487855">[-]</label><label class="expand" for="c-42487855">[1 more]</label></div><br/><div class="children"><div class="content">I think the main bottleneck right now is training data - they&#x27;ve basically exhausted all public sources of data, so they have to either pay humans to generate new data from scratch or pay for the reasoning models to generate (less useful) synthetic training data. The next bottleneck is hardware, and the least important bottleneck is money.</div><br/></div></div></div></div><div id="42490818" class="c"><input type="checkbox" id="c-42490818" checked=""/><div class="controls bullet"><span class="by">chrsw</span><span>|</span><a href="#42486788">prev</a><span>|</span><a href="#42490652">next</a><span>|</span><label class="collapse" for="c-42490818">[-]</label><label class="expand" for="c-42490818">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. So they&#x27;re not even training on NVIDIA Blackwell yet.</div><br/></div></div><div id="42490652" class="c"><input type="checkbox" id="c-42490652" checked=""/><div class="controls bullet"><span class="by">kneegerman</span><span>|</span><a href="#42490818">prev</a><span>|</span><a href="#42490677">next</a><span>|</span><label class="collapse" for="c-42490652">[-]</label><label class="expand" for="c-42490652">[1 more]</label></div><br/><div class="children"><div class="content">03 is actually orthogonal to AGI and ASI in a cartesian sense. My SAS startup  led multiple qualified teams where our RAG implementations on synthetic data originated positive inference in line with the literature (1).
(1) Sparks of AGI paper</div><br/></div></div><div id="42490677" class="c"><input type="checkbox" id="c-42490677" checked=""/><div class="controls bullet"><span class="by">tengbretson</span><span>|</span><a href="#42490652">prev</a><span>|</span><a href="#42486375">next</a><span>|</span><label class="collapse" for="c-42490677">[-]</label><label class="expand" for="c-42490677">[1 more]</label></div><br/><div class="children"><div class="content">Why would I expect a software project to ship on time?</div><br/></div></div><div id="42486375" class="c"><input type="checkbox" id="c-42486375" checked=""/><div class="controls bullet"><span class="by">h_tbob</span><span>|</span><a href="#42490677">prev</a><span>|</span><a href="#42491238">next</a><span>|</span><label class="collapse" for="c-42486375">[-]</label><label class="expand" for="c-42486375">[5 more]</label></div><br/><div class="children"><div class="content">It seems google has a massive advantage here since they can tap all of YouTube to train. I wonder what openai is using for its video data source.</div><br/><div id="42486411" class="c"><input type="checkbox" id="c-42486411" checked=""/><div class="controls bullet"><span class="by">onemoresoop</span><span>|</span><a href="#42486375">parent</a><span>|</span><a href="#42489941">next</a><span>|</span><label class="collapse" for="c-42486411">[-]</label><label class="expand" for="c-42486411">[3 more]</label></div><br/><div class="children"><div class="content">Train for what? For making videos? Train from people’s comments? There’s a lot of garbage on AI slop on youtube, how would this be sifted out? I think there’s more value here on HN in terms of training, but even that, to what avail?</div><br/><div id="42488704" class="c"><input type="checkbox" id="c-42488704" checked=""/><div class="controls bullet"><span class="by">h_tbob</span><span>|</span><a href="#42486375">root</a><span>|</span><a href="#42486411">parent</a><span>|</span><a href="#42486718">next</a><span>|</span><label class="collapse" for="c-42488704">[-]</label><label class="expand" for="c-42488704">[1 more]</label></div><br/><div class="children"><div class="content">From what I read openai is having trouble bc not enough data.<p>If u think about it, any videos on YouTube of real world data contribute to its understanding of physics at minimum. From what I gather they do pre training on tons of unstructured content first and that contributes to overall smartness.</div><br/></div></div><div id="42486718" class="c"><input type="checkbox" id="c-42486718" checked=""/><div class="controls bullet"><span class="by">a1j9o94</span><span>|</span><a href="#42486375">root</a><span>|</span><a href="#42486411">parent</a><span>|</span><a href="#42488704">prev</a><span>|</span><a href="#42489941">next</a><span>|</span><label class="collapse" for="c-42486718">[-]</label><label class="expand" for="c-42486718">[1 more]</label></div><br/><div class="children"><div class="content">YouTube is such a great multimodal dataset—videos, auto-generated captions, and real engagement data all in one place. That’s a strong starting point for training, even before you filter for quality. Microsoft’s Phi-series models already show how focusing on smaller, high-quality datasets, like textbooks, can produce great results. You could totally imagine doing the same thing with YouTube by filtering for high-quality educational videos.<p>Down the line, I think models will start using video generation as part of how they “think.” Picture a version of GPT that works frame by frame—ask it to solve a geometry problem, and it generates a sequence of images to visualize the solution before responding. YouTube’s massive library of visual content could make something like that possible.</div><br/></div></div></div></div><div id="42489941" class="c"><input type="checkbox" id="c-42489941" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#42486375">parent</a><span>|</span><a href="#42486411">prev</a><span>|</span><a href="#42491238">next</a><span>|</span><label class="collapse" for="c-42489941">[-]</label><label class="expand" for="c-42489941">[1 more]</label></div><br/><div class="children"><div class="content">Considering how evasive they&#x27;ve been, it might also be YouTube.<p>&gt; When pressed on what data OpenAI used to train Sora, Murati didn’t get too specific and seemed to dodge the question. “I’m not going to go into the details of the data that was used, but it was publicly available or licensed data,” she says. Murati also says she isn’t sure whether it used videos from YouTube, Facebook, and Instagram. She only confirmed to the Journal that Sora uses content from Shutterstock, with which OpenAI has a partnership.<p><a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2024&#x2F;3&#x2F;13&#x2F;24099402&#x2F;openai-text-to-video-ai-sora-public-availability" rel="nofollow">https:&#x2F;&#x2F;www.theverge.com&#x2F;2024&#x2F;3&#x2F;13&#x2F;24099402&#x2F;openai-text-to-v...</a></div><br/></div></div></div></div><div id="42491238" class="c"><input type="checkbox" id="c-42491238" checked=""/><div class="controls bullet"><span class="by">swozey</span><span>|</span><a href="#42486375">prev</a><span>|</span><label class="collapse" for="c-42491238">[-]</label><label class="expand" for="c-42491238">[1 more]</label></div><br/><div class="children"><div class="content">This entire industry is something I feel like I understand 2% of and every time I make progress to get to 10% (3 months later) some massive change happens and all the terminology changes.</div><br/></div></div></div></div></div></div></div></body></html>