<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706518866943" as="style"/><link rel="stylesheet" href="styles.css?v=1706518866943"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://pypi.org/project/apple-ocr/">Easy-to-Use Apple Vision wrapper for text extraction and clustering</a> <span class="domain">(<a href="https://pypi.org">pypi.org</a>)</span></div><div class="subtext"><span>brulenaudet</span> | <span>37 comments</span></div><br/><div><div id="39170985" class="c"><input type="checkbox" id="c-39170985" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39170251">next</a><span>|</span><label class="collapse" for="c-39170985">[-]</label><label class="expand" for="c-39170985">[7 more]</label></div><br/><div class="children"><div class="content">The most interesting piece of code here IMO is the recognize() method, which demonstrates how to call Vision.VNImageRequestHandler and Vision.VNRecognizeTextRequest from Python code using pyobjc.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;louisbrulenaudet&#x2F;apple-ocr&#x2F;blob&#x2F;70c25b24bf218d4299275700229d335f79c80cba&#x2F;apple_ocr&#x2F;ocr.py#L208-L272">https:&#x2F;&#x2F;github.com&#x2F;louisbrulenaudet&#x2F;apple-ocr&#x2F;blob&#x2F;70c25b24b...</a></div><br/><div id="39173102" class="c"><input type="checkbox" id="c-39173102" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39170985">parent</a><span>|</span><a href="#39170997">next</a><span>|</span><label class="collapse" for="c-39173102">[-]</label><label class="expand" for="c-39173102">[5 more]</label></div><br/><div class="children"><div class="content">Sorry for my ignorance, but how does it work?<p>I understand that Vision is an Apple framework, but does it mean that if you are on MacOS, you can just call `import Vision` and it just works? No need to install any package&#x2F;library as a wrapper? Didn&#x27;t see anything special in setup.cfg or readme about the setup.</div><br/><div id="39173536" class="c"><input type="checkbox" id="c-39173536" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#39170985">root</a><span>|</span><a href="#39173102">parent</a><span>|</span><a href="#39173477">next</a><span>|</span><label class="collapse" for="c-39173536">[-]</label><label class="expand" for="c-39173536">[1 more]</label></div><br/><div class="children"><div class="content">1 - install_requires in setup.y adds dependency to pyobjc<p>2 - which brings the Vision package along, <a href="https:&#x2F;&#x2F;pyobjc.readthedocs.io&#x2F;en&#x2F;latest&#x2F;apinotes&#x2F;Vision.html" rel="nofollow">https:&#x2F;&#x2F;pyobjc.readthedocs.io&#x2F;en&#x2F;latest&#x2F;apinotes&#x2F;Vision.html</a></div><br/></div></div><div id="39173477" class="c"><input type="checkbox" id="c-39173477" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#39170985">root</a><span>|</span><a href="#39173102">parent</a><span>|</span><a href="#39173536">prev</a><span>|</span><a href="#39173445">next</a><span>|</span><label class="collapse" for="c-39173477">[-]</label><label class="expand" for="c-39173477">[2 more]</label></div><br/><div class="children"><div class="content">PyObjC is shipped by Apple, yes. (It even used to be installed by default.)</div><br/><div id="39173523" class="c"><input type="checkbox" id="c-39173523" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39170985">root</a><span>|</span><a href="#39173477">parent</a><span>|</span><a href="#39173445">next</a><span>|</span><label class="collapse" for="c-39173523">[-]</label><label class="expand" for="c-39173523">[1 more]</label></div><br/><div class="children"><div class="content">&gt; PyObjC<p>Ah, thanks, I totally overlooked this. This is what brings all the Apple&#x27;s Objective C API to Python.</div><br/></div></div></div></div><div id="39173445" class="c"><input type="checkbox" id="c-39173445" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39170985">root</a><span>|</span><a href="#39173102">parent</a><span>|</span><a href="#39173477">prev</a><span>|</span><a href="#39170997">next</a><span>|</span><label class="collapse" for="c-39173445">[-]</label><label class="expand" for="c-39173445">[1 more]</label></div><br/><div class="children"><div class="content">iOS too<p><a href="https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;vision&#x2F;vnrecognizetextrequest" rel="nofollow">https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;vision&#x2F;vnrecognize...</a></div><br/></div></div></div></div><div id="39170997" class="c"><input type="checkbox" id="c-39170997" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#39170985">parent</a><span>|</span><a href="#39173102">prev</a><span>|</span><a href="#39170251">next</a><span>|</span><label class="collapse" for="c-39170997">[-]</label><label class="expand" for="c-39170997">[1 more]</label></div><br/><div class="children"><div class="content">Note that Vision.VNRecognizeTextRequest is old tech that is less accurate than the newer ImageAnalyzer API from Apple.<p>These tools use Vision.VNRecognizeTextRequest because of legacy tech reasons - new APIs from Apple tend to be Swift-exclusive, while these tools are only built for ObjC<p>Choosing Vision.VNRecognizeTextRequest is choosing less accuracy than is currently available on the same platform</div><br/></div></div></div></div><div id="39170251" class="c"><input type="checkbox" id="c-39170251" checked=""/><div class="controls bullet"><span class="by">fotta</span><span>|</span><a href="#39170985">prev</a><span>|</span><a href="#39171410">next</a><span>|</span><label class="collapse" for="c-39170251">[-]</label><label class="expand" for="c-39170251">[2 more]</label></div><br/><div class="children"><div class="content">Apple Vision SDK: <a href="https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;vision" rel="nofollow">https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;vision</a></div><br/><div id="39170953" class="c"><input type="checkbox" id="c-39170953" checked=""/><div class="controls bullet"><span class="by">m12k</span><span>|</span><a href="#39170251">parent</a><span>|</span><a href="#39171410">next</a><span>|</span><label class="collapse" for="c-39170953">[-]</label><label class="expand" for="c-39170953">[1 more]</label></div><br/><div class="children"><div class="content">There was a nice blog post a year ago from someone that used this framework and a rack of iPhones to OCR images in order to build a searchable meme database: <a href="https:&#x2F;&#x2F;findthatmeme.com&#x2F;blog&#x2F;2023&#x2F;01&#x2F;08&#x2F;image-stacks-and-iphone-racks-building-an-internet-scale-meme-search-engine-Qzrz7V6T.html" rel="nofollow">https:&#x2F;&#x2F;findthatmeme.com&#x2F;blog&#x2F;2023&#x2F;01&#x2F;08&#x2F;image-stacks-and-ip...</a></div><br/></div></div></div></div><div id="39171410" class="c"><input type="checkbox" id="c-39171410" checked=""/><div class="controls bullet"><span class="by">rob</span><span>|</span><a href="#39170251">prev</a><span>|</span><a href="#39170823">next</a><span>|</span><label class="collapse" for="c-39171410">[-]</label><label class="expand" for="c-39171410">[7 more]</label></div><br/><div class="children"><div class="content">Does using this over other OCR depend on your use case?<p>I just tried this to extract the text from the back of a nutrition bar label:<p><a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;rRtuBCu.jpeg" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;rRtuBCu.jpeg</a><p>The output missed almost all of the actual ingredients under &quot;Ingredients:&quot; (see: <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;latOiqk.png" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;latOiqk.png</a>) whereas Amazon&#x27;s Textract had no issues at all (see: <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;hAVzbUR.png" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;hAVzbUR.png</a>)<p>(For Amazon Textract, I used the &quot;Layout&quot; feature and the output is a result of combining the lines under each &quot;LAYOUT_TEXT&quot; block.)<p>I also have no idea what I&#x27;m doing, so maybe the Apple one is better&#x2F;can do the same and I just don&#x27;t know how.</div><br/><div id="39173404" class="c"><input type="checkbox" id="c-39173404" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#39171410">parent</a><span>|</span><a href="#39171884">next</a><span>|</span><label class="collapse" for="c-39173404">[-]</label><label class="expand" for="c-39173404">[1 more]</label></div><br/><div class="children"><div class="content">GPT gives me:<p>Serving Size: 1 Bar (68g) • Calories: 240 • Total Fat: 5g • Saturated Fat: 1.5g • Trans Fat: 0g • Cholesterol: 0mg • Sodium: 150mg • Total Carbohydrate: 44g • Dietary Fiber: 4g • Sugars: 23g • Other Carbohydrates: 17g • Protein: 10g • Vitamin A: 30% • Vitamin C: 90% • Calcium: 25% • Iron: 10% • Vitamin D: 10% • Vitamin E: 100% • Thiamin: 10% • Riboflavin: 15% • Niacin: 15% • Vitamin B6: 20% • Folate: 20% • Vitamin B12: 15% • Biotin: 10% • Pantothenic Acid: 20% • Phosphorus: 10% • Iodine: 10% • Magnesium: 5% • Zinc: 10% • Selenium: 10% • Copper: 8% • Manganese: 10% • Chromium: 6% • 70% Organic Ingredients<p>The ingredients listed on the label are as follows:<p>Organic Brown Rice Syrup, ClifPro® (Soy Rice Crisps [Soy Protein Isolate, Rice Flour, Barley Malt Extract], Organic Roasted Soybeans, Organic Soy Flour), Organic Rolled Oats, Chocolate Chips (Dried Cane Syrup, Unsweetened Chocolate, Cocoa Butter, Lecithin, Vanilla Extract), Organic Cane Syrup, ClifCrunch® (Organic Oat Fiber, Apple Fiber, Inulin [Chicory Extract], Organic Milled Flaxseed, Organic Oat Bran), Organic Date Paste, Organic Sunflower Oil, Molasses Powder, Sea Salt, Natural Flavors, Cinnamon. Vitamins &amp; Minerals: Dicalcium Phosphate, Magnesium Oxide, Ascorbic Acid (Vit. C), DL-Alpha Tocopheryl Acetate (Vit. E), Beta Carotene (Vit. A), Niacinamide (Vit. B3), Ergocalciferol (Vit. D2), Thiamine Mononitrate (Vit. B1), Pyridoxine Hydrochloride (Vit. B6), Riboflavin (Vit. B2), Cyanocobalamin (Vit. B12).<p>ALLERGEN STATEMENT: Contains Soy. May Contain Traces of Milk, Peanuts, Wheat, and Other Tree Nuts.</div><br/></div></div><div id="39171884" class="c"><input type="checkbox" id="c-39171884" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39171410">parent</a><span>|</span><a href="#39173404">prev</a><span>|</span><a href="#39171644">next</a><span>|</span><label class="collapse" for="c-39171884">[-]</label><label class="expand" for="c-39171884">[2 more]</label></div><br/><div class="children"><div class="content">My iPhone picked up:<p>Nutrition Facts
Serving Size 1 Bar (68g)
Calories 240
Calories from Fat 45
<i>Percent Daily Values (UV) are based on a 2.000 calorie diet
Amount&#x2F;Serving
Total Fat 59
Sat. Fat 1.5g
% DV</i>
8%
8%
Amount&#x2F;Serving
Amount&#x2F;Serving
% DV*
% DV
Sodium
150mg
Potassium 230mg
7%
Insoluble Fiber 3g
Sugars 23g
Trans Fat 0g
Total Carb. 44g
15%
Other Carb. 17g
Cholest. 0mg
0%
Dietary Fiber 4g
16%
Protein 10g
20%
Vit. A 30%
• Vit. C 90% • Calcium 25% • Iron 10% • Vit. D 10% • Vit. E 100% • Vit. K 25% • Thiamin
(81) 10%
Riboflavin (82) 15% • Niacin (B3) 15% • Vit. B6 20% • Folate 20% • Vit. B12 15% • Biotin 10% • Pantothenic Acid 20%
Phosphorus 10% * lodine 10% * Magnesium 15% * Zinc 10% * Selenium 10% * Copper 8% • Manganese 10% • Chromium 6%
Oxide, DL-Alpha Tocophery Acetate (Vit. E), ferric Ortophosphate (Iron), Beto Carotene (Vit. A), Zinc Citate, Biotin, Niacinamide (Vit. B3), D-Calcium Pantothenate
(Vit: 85), Phytonodione Vit. Kl,
Manganese Gluconate, Copper Gluconate, Riboflavin (Vit. B2), Folic Acid (Vit. B9),
Sodium Selenite,
Thiomin (Vi. B1), Pyridoxion
Hvdrochloride (Vit. 86), Ergocolterol (Vit. U24, «vanocoboiomin (Vit. 812), Chromium Chioride, Potassium lodide, ALLERGEN STATEMENT: CONTAINS SOY. MAY CONTAIN CERTIFIED TRACES OF MILK PEANUTS, WHEAÏ, AND TREE NUIS. WE SOURCE INGREDIENTS THAT ARE NOT GENETICALLY ENGINEERED.
*Rainforest Allionce Certified&quot;
COCOR
เมยESSUSL 1800C1948179-55T • 2013 CUF BAR &amp; COMPANY • TRADENARKS AND REGISTERED TRADEMARES ARE ONNED BY CUF BARE COMPANY OR USED WITH REUMSSION
CERTIFIED ORGANIC DY DA!
HIGH IN PROTEIN - 0g TRANS FAT - 70% ORGANIC INGREDIENTS
ADVENTURE SAFELY: ALL RISK AND LIABILITY IS YOURS.
BEST BY
222
CLIF BAR FAMILY
FOUNDATION.ORG
clifbar.com
30J AN16G1
09: 09 H</div><br/><div id="39171949" class="c"><input type="checkbox" id="c-39171949" checked=""/><div class="controls bullet"><span class="by">veidr</span><span>|</span><a href="#39171410">root</a><span>|</span><a href="#39171884">parent</a><span>|</span><a href="#39171644">next</a><span>|</span><label class="collapse" for="c-39171949">[-]</label><label class="expand" for="c-39171949">[1 more]</label></div><br/><div class="children"><div class="content">I mean, that&#x27;s about how well I did when I looked at that image, too...</div><br/></div></div></div></div><div id="39171644" class="c"><input type="checkbox" id="c-39171644" checked=""/><div class="controls bullet"><span class="by">snailmailman</span><span>|</span><a href="#39171410">parent</a><span>|</span><a href="#39171884">prev</a><span>|</span><a href="#39171813">next</a><span>|</span><label class="collapse" for="c-39171644">[-]</label><label class="expand" for="c-39171644">[1 more]</label></div><br/><div class="children"><div class="content">For one thing, apples OCR runs entirely on device. When scanning documents, if any are sensitive I don’t want them going to a cloud service.</div><br/></div></div><div id="39171813" class="c"><input type="checkbox" id="c-39171813" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#39171410">parent</a><span>|</span><a href="#39171644">prev</a><span>|</span><a href="#39170823">next</a><span>|</span><label class="collapse" for="c-39171813">[-]</label><label class="expand" for="c-39171813">[2 more]</label></div><br/><div class="children"><div class="content">To be fair, that&#x27;s a pretty pathological image to try to OCR. :)<p>It&#x27;s wrinkled <i>and</i> not flat so lines aren&#x27;t straight, it&#x27;s got big dark shadows going across text, and very low resolution for the letterforms. Even I have an awfully hard time reading parts of it.<p>OCR is generally designed for flat documents in relatively even lighting with a decent resolution. Different OCR software may do better or worse the further you get away from those ideal conditions, but using an image like yours to compare OCR software is like trying to compare a Porsche and a Ferrari by how well they drive through a muddy jungle.</div><br/><div id="39171968" class="c"><input type="checkbox" id="c-39171968" checked=""/><div class="controls bullet"><span class="by">rob</span><span>|</span><a href="#39171410">root</a><span>|</span><a href="#39171813">parent</a><span>|</span><a href="#39170823">next</a><span>|</span><label class="collapse" for="c-39171968">[-]</label><label class="expand" for="c-39171968">[1 more]</label></div><br/><div class="children"><div class="content">Hahaha. Yeah, I figured I&#x27;d try it with a &quot;harder&quot; image for a personal project I&#x27;m working on — something you&#x27;d expect someone to take a photo of with their phone that isn&#x27;t perfect. So I&#x27;ve been playing around with all sorts of OCR stuff. I was really surprised at how well Textract was able to read the text from that dark line with the fold in the image.</div><br/></div></div></div></div></div></div><div id="39170823" class="c"><input type="checkbox" id="c-39170823" checked=""/><div class="controls bullet"><span class="by">rgovostes</span><span>|</span><a href="#39171410">prev</a><span>|</span><a href="#39170379">next</a><span>|</span><label class="collapse" for="c-39170823">[-]</label><label class="expand" for="c-39170823">[7 more]</label></div><br/><div class="children"><div class="content">&quot;Apple Vision&quot; sounds like it has something to do with the Apple Vision Pro headset, but it doesn&#x27;t. This is related to Apple&#x27;s Vision framework, which provides computer vision algorithms.</div><br/><div id="39171504" class="c"><input type="checkbox" id="c-39171504" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#39170823">parent</a><span>|</span><a href="#39172143">next</a><span>|</span><label class="collapse" for="c-39171504">[-]</label><label class="expand" for="c-39171504">[4 more]</label></div><br/><div class="children"><div class="content">Is it going to be the Microsoft Surface all over again?</div><br/><div id="39171676" class="c"><input type="checkbox" id="c-39171676" checked=""/><div class="controls bullet"><span class="by">cyclecount</span><span>|</span><a href="#39170823">root</a><span>|</span><a href="#39171504">parent</a><span>|</span><a href="#39172143">next</a><span>|</span><label class="collapse" for="c-39171676">[-]</label><label class="expand" for="c-39171676">[3 more]</label></div><br/><div class="children"><div class="content">Probably not because this product will be culturally relevant still 5 years after launch</div><br/><div id="39172266" class="c"><input type="checkbox" id="c-39172266" checked=""/><div class="controls bullet"><span class="by">jes5199</span><span>|</span><a href="#39170823">root</a><span>|</span><a href="#39171676">parent</a><span>|</span><a href="#39172143">next</a><span>|</span><label class="collapse" for="c-39172266">[-]</label><label class="expand" for="c-39172266">[2 more]</label></div><br/><div class="children"><div class="content">optimistic take</div><br/><div id="39172359" class="c"><input type="checkbox" id="c-39172359" checked=""/><div class="controls bullet"><span class="by">cyclecount</span><span>|</span><a href="#39170823">root</a><span>|</span><a href="#39172266">parent</a><span>|</span><a href="#39172143">next</a><span>|</span><label class="collapse" for="c-39172359">[-]</label><label class="expand" for="c-39172359">[1 more]</label></div><br/><div class="children"><div class="content">Apple has a very strong track record of building high-end hardware that is expensive at first targeting pro user early adopters, then iterating and reducing cost over 3-5 years, eventually becoming the dominant player in a hardware category.<p>iPhone for mobile phones, iPad for tablets, Apple Watch for wearables, AirPods for fully wireless headphones.  They have few failed product lines so this seems like a pretty safe bet.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39170379" class="c"><input type="checkbox" id="c-39170379" checked=""/><div class="controls bullet"><span class="by">theolivenbaum</span><span>|</span><a href="#39170823">prev</a><span>|</span><a href="#39172728">next</a><span>|</span><label class="collapse" for="c-39170379">[-]</label><label class="expand" for="c-39170379">[1 more]</label></div><br/><div class="children"><div class="content">Nice! We maintain one for C# too: <a href="https:&#x2F;&#x2F;www.nuget.org&#x2F;packages&#x2F;LiveTextSharp" rel="nofollow">https:&#x2F;&#x2F;www.nuget.org&#x2F;packages&#x2F;LiveTextSharp</a></div><br/></div></div><div id="39172728" class="c"><input type="checkbox" id="c-39172728" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#39170379">prev</a><span>|</span><a href="#39170389">next</a><span>|</span><label class="collapse" for="c-39172728">[-]</label><label class="expand" for="c-39172728">[1 more]</label></div><br/><div class="children"><div class="content">Anyone else&#x27;s GPT spidey senses going off at how the documentation is written?</div><br/></div></div><div id="39170389" class="c"><input type="checkbox" id="c-39170389" checked=""/><div class="controls bullet"><span class="by">bomewish</span><span>|</span><a href="#39172728">prev</a><span>|</span><a href="#39170139">next</a><span>|</span><label class="collapse" for="c-39170389">[-]</label><label class="expand" for="c-39170389">[2 more]</label></div><br/><div class="children"><div class="content">I’ve been using textra for a wrapper on the Apple vision sdk<p><a href="https:&#x2F;&#x2F;github.com&#x2F;freedmand&#x2F;textra">https:&#x2F;&#x2F;github.com&#x2F;freedmand&#x2F;textra</a><p>But this project calls torch and a bunch of other ML libs. So it’s not using Apple vision?</div><br/><div id="39170560" class="c"><input type="checkbox" id="c-39170560" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39170389">parent</a><span>|</span><a href="#39170139">next</a><span>|</span><label class="collapse" for="c-39170560">[-]</label><label class="expand" for="c-39170560">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s only using those other libs to do some simplistic statistical analysis of Apple vision&#x27;s outputs. Further down you can see it just calls VNImageRequestHandler which is basically a call from Python to Apple Vision via ObjC in order to get any actual recognitions.</div><br/></div></div></div></div><div id="39170139" class="c"><input type="checkbox" id="c-39170139" checked=""/><div class="controls bullet"><span class="by">l33t7332273</span><span>|</span><a href="#39170389">prev</a><span>|</span><a href="#39170079">next</a><span>|</span><label class="collapse" for="c-39170139">[-]</label><label class="expand" for="c-39170139">[2 more]</label></div><br/><div class="children"><div class="content">It uses python so I’m a bit concerned about speed. Are there any benchmarks for how fast this is if I want to use it for a real time application?</div><br/><div id="39170290" class="c"><input type="checkbox" id="c-39170290" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#39170139">parent</a><span>|</span><a href="#39170079">next</a><span>|</span><label class="collapse" for="c-39170290">[-]</label><label class="expand" for="c-39170290">[1 more]</label></div><br/><div class="children"><div class="content">This is a python wrapper around apple’s vision framework. If you’re concerned about speed and&#x2F;or don’t want python, then interacting directly with the vision framework is an option.</div><br/></div></div></div></div><div id="39170828" class="c"><input type="checkbox" id="c-39170828" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#39170079">prev</a><span>|</span><a href="#39154830">next</a><span>|</span><label class="collapse" for="c-39170828">[-]</label><label class="expand" for="c-39170828">[4 more]</label></div><br/><div class="children"><div class="content">Does anyone know how Apple Vision performs on Japanese vertical, right-to-left text extraction? This is my biggest need around some machine translation stuff, but I’ve found Google OCR is the only thing that can do this reliably well, but also that it’s too expensive to run for hobby projects.</div><br/><div id="39173375" class="c"><input type="checkbox" id="c-39173375" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#39170828">parent</a><span>|</span><a href="#39170977">next</a><span>|</span><label class="collapse" for="c-39173375">[-]</label><label class="expand" for="c-39173375">[2 more]</label></div><br/><div class="children"><div class="content">How well does GPT Vision read it?<p>I always used GPT-V for reading thorny documents (e.g. medieval French handwritten letters) and it would do a spectacular job, but they hobbled it at some point to tone down its enthusiasm for translation, so a lot of the time you have to tell it that it&#x27;s important to your dead grandma&#x27;s career or other tricks to get the output.</div><br/><div id="39173951" class="c"><input type="checkbox" id="c-39173951" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#39170828">root</a><span>|</span><a href="#39173375">parent</a><span>|</span><a href="#39170977">next</a><span>|</span><label class="collapse" for="c-39173951">[-]</label><label class="expand" for="c-39173951">[1 more]</label></div><br/><div class="children"><div class="content">I haven’t tried via the API because it’s too expensive for what I’m doing (hundreds of pages of high-res scans of old Japanese books), but I should test out a few pages in ChatGPT.</div><br/></div></div></div></div><div id="39170977" class="c"><input type="checkbox" id="c-39170977" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#39170828">parent</a><span>|</span><a href="#39173375">prev</a><span>|</span><a href="#39154830">next</a><span>|</span><label class="collapse" for="c-39170977">[-]</label><label class="expand" for="c-39170977">[1 more]</label></div><br/><div class="children"><div class="content">Surprisingly, it works! Vertical text is recognized as if it were normal text rotated 90° clockwise.</div><br/></div></div></div></div><div id="39154830" class="c"><input type="checkbox" id="c-39154830" checked=""/><div class="controls bullet"><span class="by">brulenaudet</span><span>|</span><a href="#39170828">prev</a><span>|</span><label class="collapse" for="c-39154830">[-]</label><label class="expand" for="c-39154830">[2 more]</label></div><br/><div class="children"><div class="content">Apple-ocr is a utility for Optical Character Recognition (OCR) that facilitates the extraction of text from images. This Python-based tool is designed to help developers, researchers, and enthusiasts in the field of text extraction and clustering. It leverages a combination of various technologies to achieve this, including the Vision framework provided by Apple.</div><br/></div></div></div></div></div></div></div></body></html>