<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718701258927" as="style"/><link rel="stylesheet" href="styles.css?v=1718701258927"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.boundaryml.com/blog/structured-output-from-llms">Every Way to Get Structured Output from LLMs</a> <span class="domain">(<a href="https://www.boundaryml.com">www.boundaryml.com</a>)</span></div><div class="subtext"><span>constantinum</span> | <span>53 comments</span></div><br/><div><div id="40714918" class="c"><input type="checkbox" id="c-40714918" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#40714872">next</a><span>|</span><label class="collapse" for="c-40714918">[-]</label><label class="expand" for="c-40714918">[6 more]</label></div><br/><div class="children"><div class="content">This is an article written by BAML that shows BAML as the best.<p>Also, BAML seems to be a commercial product with no clear pricing.<p>&gt; Our paid capabilities only start if you use Boundary Studio, which focuses on Monitoring, Collecting Feedback, and Improving your AI pipelines. Contact us for pricing details at contact_boundaryml.com</div><br/><div id="40714966" class="c"><input type="checkbox" id="c-40714966" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40714918">parent</a><span>|</span><a href="#40715039">next</a><span>|</span><label class="collapse" for="c-40714966">[-]</label><label class="expand" for="c-40714966">[1 more]</label></div><br/><div class="children"><div class="content">our paid product is still in Beta actually as we&#x27;re continuing to build it out, but BAML itself is and always will be open source (runs fully locally as well - no extra network calls).<p>in terms of parsing, I do think we&#x27;re likely the best approach as of now. Most other libraries do reprompting or rely on constraining grammars which require owning the model. Reprompting = slow + $$, constraining grammars = require owning the model. we just tried a new approach: parse the output in a more clever way.</div><br/></div></div><div id="40715039" class="c"><input type="checkbox" id="c-40715039" checked=""/><div class="controls bullet"><span class="by">martypitt</span><span>|</span><a href="#40714918">parent</a><span>|</span><a href="#40714966">prev</a><span>|</span><a href="#40715330">next</a><span>|</span><label class="collapse" for="c-40715039">[-]</label><label class="expand" for="c-40715039">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Also, BAML seems to be a commercial product with no clear pricing.<p>You&#x27;ve only presented half the story. They&#x27;re also Open Source (Apache 2.0), with code on github.<p>As you mention, some features are gated, but they seem to have a fairly solid OSS offering.</div><br/><div id="40715125" class="c"><input type="checkbox" id="c-40715125" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#40714918">root</a><span>|</span><a href="#40715039">parent</a><span>|</span><a href="#40715330">next</a><span>|</span><label class="collapse" for="c-40715125">[-]</label><label class="expand" for="c-40715125">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Their OSS offering is described in the article though. References to the paid offering are only on the landing page. My grandparent comment is the missing half to the article’s half of story.</div><br/></div></div></div></div><div id="40715330" class="c"><input type="checkbox" id="c-40715330" checked=""/><div class="controls bullet"><span class="by">joatmon-snoo</span><span>|</span><a href="#40714918">parent</a><span>|</span><a href="#40715039">prev</a><span>|</span><a href="#40714872">next</a><span>|</span><label class="collapse" for="c-40715330">[-]</label><label class="expand" for="c-40715330">[2 more]</label></div><br/><div class="children"><div class="content">Author here! I very deliberately avoided making that claim; the table is actually very unsorted right now, in no small part because all the solutions in the space satisfy a very different set of usage criteria - some folks use Python, others use TS, yet others want Golang or Java or something else; some want support for Ollama&#x2F;llama.cpp&#x2F;vLLM, others are looking for OpenAI&#x2F;Anthropic support.<p>That being said, if you have suggestions for how we can make this table more objective, we’re all ears!</div><br/><div id="40715469" class="c"><input type="checkbox" id="c-40715469" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#40714918">root</a><span>|</span><a href="#40715330">parent</a><span>|</span><a href="#40714872">next</a><span>|</span><label class="collapse" for="c-40715469">[-]</label><label class="expand" for="c-40715469">[1 more]</label></div><br/><div class="children"><div class="content">You could make it more clear that BAML is the solution you’re providing.<p>Perhaps change: “(as a player in this space, we&#x27;re a little biased!)” to: “we’re the makers of BAML.”</div><br/></div></div></div></div></div></div><div id="40714872" class="c"><input type="checkbox" id="c-40714872" checked=""/><div class="controls bullet"><span class="by">StrauXX</span><span>|</span><a href="#40714918">prev</a><span>|</span><a href="#40715444">next</a><span>|</span><label class="collapse" for="c-40714872">[-]</label><label class="expand" for="c-40714872">[6 more]</label></div><br/><div class="children"><div class="content">Did I understand the documentation for many of these libraries correctly in that they reprompt until they receive valid JSON? If so I don&#x27;t understand why one would do that when token masking is a deterministicly verifyable way to get structured output of any kind (as done by Guidance and LMQL for instance).
This is not meant to be snarky, I really am curious. Is there an upside to reprompting - aside from easier implementation.</div><br/><div id="40714988" class="c"><input type="checkbox" id="c-40714988" checked=""/><div class="controls bullet"><span class="by">frabcus</span><span>|</span><a href="#40714872">parent</a><span>|</span><a href="#40715185">next</a><span>|</span><label class="collapse" for="c-40714988">[-]</label><label class="expand" for="c-40714988">[2 more]</label></div><br/><div class="children"><div class="content">My experience with models even about a year ago is that the model has firmly decided what it thinks by the time of the last layer, so the probabilities on that layer aren&#x27;t very useful.<p>You either get the same (in this case wrong) thing differently worded, or worse you get effectively noise if the second probability is very much lower than the largest probability.<p>My guess is that applies here too. Better to let all the layers rethink the tokens, than force hallucination of eg a random letter when you don&#x27;t expect an angle bracket<p>(Edit: above is assuming using logprobs and&#x2F;or logit_bias with the OpenAI API, not some other masking technique)</div><br/><div id="40715094" class="c"><input type="checkbox" id="c-40715094" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#40714872">root</a><span>|</span><a href="#40714988">parent</a><span>|</span><a href="#40715185">next</a><span>|</span><label class="collapse" for="c-40715094">[-]</label><label class="expand" for="c-40715094">[1 more]</label></div><br/><div class="children"><div class="content">Why not apply it at an earlier layer then?</div><br/></div></div></div></div><div id="40715185" class="c"><input type="checkbox" id="c-40715185" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#40714872">parent</a><span>|</span><a href="#40714988">prev</a><span>|</span><a href="#40714984">next</a><span>|</span><label class="collapse" for="c-40715185">[-]</label><label class="expand" for="c-40715185">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t reprompting a decent technique? Considering most modern languages are LL(k), that is you need at most k tokens to parse the output (tbf these are programming language tokens not LLM tokens), with k=1 being the most common choice, would it not be reasonable to expect to only have to regenerate only a handful of tokens at most?</div><br/><div id="40715256" class="c"><input type="checkbox" id="c-40715256" checked=""/><div class="controls bullet"><span class="by">joatmon-snoo</span><span>|</span><a href="#40714872">root</a><span>|</span><a href="#40715185">parent</a><span>|</span><a href="#40714984">next</a><span>|</span><label class="collapse" for="c-40715256">[-]</label><label class="expand" for="c-40715256">[1 more]</label></div><br/><div class="children"><div class="content">Author here- yes, reprompting can work well enough if the latency hit is acceptable to you.<p>If you’re driving user-facing interactions with LLMs, though, and you’re already dealing with &gt;1min latency on the first call (as many of our current users are!), waiting for another LLM call to come back is a really frustrating thing to block your UX on.</div><br/></div></div></div></div><div id="40714984" class="c"><input type="checkbox" id="c-40714984" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40714872">parent</a><span>|</span><a href="#40715185">prev</a><span>|</span><a href="#40715444">next</a><span>|</span><label class="collapse" for="c-40714984">[-]</label><label class="expand" for="c-40714984">[1 more]</label></div><br/><div class="children"><div class="content">the main one is that most people don&#x27;t own the model. so if you use openai &#x2F; anthropic &#x2F; etc then you can&#x27;t use token masking. in that case, reprompting is pretty much the only option</div><br/></div></div></div></div><div id="40715444" class="c"><input type="checkbox" id="c-40715444" checked=""/><div class="controls bullet"><span class="by">lysecret</span><span>|</span><a href="#40714872">prev</a><span>|</span><a href="#40714738">next</a><span>|</span><label class="collapse" for="c-40715444">[-]</label><label class="expand" for="c-40715444">[1 more]</label></div><br/><div class="children"><div class="content">Huh. I&#x27;m using json response every day on my calorie counting app. Never had any issues. I thought this was a solved problem?<p>The only times 4o couldn&#x27;t parse to valid outputs was when it was legitimately confused (and I had to add some examples).</div><br/></div></div><div id="40714738" class="c"><input type="checkbox" id="c-40714738" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#40715444">prev</a><span>|</span><a href="#40714992">next</a><span>|</span><label class="collapse" for="c-40714738">[-]</label><label class="expand" for="c-40714738">[2 more]</label></div><br/><div class="children"><div class="content">Structured output should not be assumed is limited to JSON. Claude performs very well with XML, as it has been trained with it, so there&#x27;s no real need to put in extra work. Not XML as in conformant, schema-compliant XML, just XML as delimiters.<p><a href="https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;build-with-claude&#x2F;prompt-engineering&#x2F;use-xml-tags" rel="nofollow">https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;build-with-claude&#x2F;prompt-...</a><p>Give it examples and instructions in tags, ask it to output in tags, and force it to return early by completing for it. (Assistant:&lt;output&gt;).<p>When you think about it, it makes a lot of sense. Even if the output is chatty, parsing it is easy because you&#x27;re not looking for } which may or may not match an opening {, instead you&#x27;re looking for &lt;&#x2F;output&gt; which is much easier to parse for.</div><br/><div id="40714811" class="c"><input type="checkbox" id="c-40714811" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40714738">parent</a><span>|</span><a href="#40714992">next</a><span>|</span><label class="collapse" for="c-40714811">[-]</label><label class="expand" for="c-40714811">[1 more]</label></div><br/><div class="children"><div class="content">XML is also a great option, but there are a few trade offs:<p>&gt; XML is a many more tokens (much slower + $$$ for complex schemas)<p>&gt; regardless of if you&#x27;re looking for } or &lt;&#x2F;output&gt; its really a matter of &quot;does your parser work&quot;. when you have three tokens that need to be correct &quot;&lt;&#x2F;&quot; &quot;output&quot;, &quot;&gt;&quot;, the odds of a mistake are higher, instead of when you just need &quot;}&quot;.<p>That said, the parser is much easier to write, we&#x27;re actually considering supporting XML in BAML. have you found any reductions of accuracy?<p>Also, not sure if you saw this, but apparently Claude doesn&#x27;t actually prefer XML, it just happens to work well with it. Was recently new info for myself as well.
<a href="https:&#x2F;&#x2F;x.com&#x2F;alexalbert__&#x2F;status&#x2F;1778550859598807178" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;alexalbert__&#x2F;status&#x2F;1778550859598807178</a> (devrel @ Anthropic)</div><br/></div></div></div></div><div id="40714992" class="c"><input type="checkbox" id="c-40714992" checked=""/><div class="controls bullet"><span class="by">mg</span><span>|</span><a href="#40714738">prev</a><span>|</span><a href="#40715174">next</a><span>|</span><label class="collapse" for="c-40714992">[-]</label><label class="expand" for="c-40714992">[4 more]</label></div><br/><div class="children"><div class="content">The baml config files look a lot like code. For example in baml:<p><pre><code>    class Resume {
        name string
        education Education[] @description(&quot;Extract in the same order listed&quot;)
        skills string[] @description(&quot;Only include programming languages&quot;)
    }
</code></pre>
Could be expressed in Python like this:<p><pre><code>    class Resume:
        name: str
        education: List[Education] # Extract in the same order listed
        skills: List[str] # Only include programming languages
</code></pre>
Two benefits I see are that it would make the file leaner (because Python is nicely lean) and provide free parsing and syntax highlighting.<p>Is there a benefit of rolling your own DSL?</div><br/><div id="40715071" class="c"><input type="checkbox" id="c-40715071" checked=""/><div class="controls bullet"><span class="by">GeneralMayhem</span><span>|</span><a href="#40714992">parent</a><span>|</span><a href="#40715322">next</a><span>|</span><label class="collapse" for="c-40715071">[-]</label><label class="expand" for="c-40715071">[1 more]</label></div><br/><div class="children"><div class="content">Two big advantages.<p>First, if you want a declarative config with limited, domain-specific options, rolling your own DSL instead of using something as complex as Python is much, much easier to implement. You&#x27;re not actually going to be <i>running</i> the code either way, at least not in the normal way, and the Python syntax tree is pretty complicated.<p>Second, having code that looks like Python can lead your users to believe that it is in, in fact, Python. When you&#x27;re doing things like using your DSL as configuration that happens at setup time, but then actually &quot;running&quot; the resulting config later on, that can lead to people getting themselves into trouble - for instance, they might try to use `time.now()` and end up embedding the time of the config parser as a constant in their workflow definition.<p>If you want to use Python as your language, you probably want to define your &quot;DSL&quot; as a Python library, so that you can use a normal interpreter to work with it. Maybe you have library functions that return config objects, and a user&#x27;s &quot;configuration&quot; is an arbitrary Python file with a standard function name as an entry point. But then when you want to introspect over types, you probably need to start playing games with decorators, which is tricky again, and you have to be very careful to have that evaluation step return meaningful errors.<p>Starlark (<a href="https:&#x2F;&#x2F;github.com&#x2F;bazelbuild&#x2F;starlark">https:&#x2F;&#x2F;github.com&#x2F;bazelbuild&#x2F;starlark</a>) is an example of using Python-ish as a &quot;configuration&quot; language. That took an absolutely massive amount of engineering to get to be well-defined, and was only arguably worth it because they <i>wanted</i> a language that&#x27;s a loop construct away from being Turing-complete. If they had wanted a basic declarative relationship language, they probably would have used textprotos or GCL.</div><br/></div></div><div id="40715322" class="c"><input type="checkbox" id="c-40715322" checked=""/><div class="controls bullet"><span class="by">mejutoco</span><span>|</span><a href="#40714992">parent</a><span>|</span><a href="#40715071">prev</a><span>|</span><a href="#40715066">next</a><span>|</span><label class="collapse" for="c-40715322">[-]</label><label class="expand" for="c-40715322">[1 more]</label></div><br/><div class="children"><div class="content">Using Pydantic also looks very close to the DSL (trivial to translate mechanically)<p><a href="https:&#x2F;&#x2F;docs.pydantic.dev&#x2F;latest&#x2F;concepts&#x2F;models&#x2F;#dynamic-model-creation" rel="nofollow">https:&#x2F;&#x2F;docs.pydantic.dev&#x2F;latest&#x2F;concepts&#x2F;models&#x2F;#dynamic-mo...</a></div><br/></div></div><div id="40715066" class="c"><input type="checkbox" id="c-40715066" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40714992">parent</a><span>|</span><a href="#40715322">prev</a><span>|</span><a href="#40715174">next</a><span>|</span><label class="collapse" for="c-40715066">[-]</label><label class="expand" for="c-40715066">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s a great question, there&#x27;s three main benefits:<p>1. seeing the full prompt, even though that python code feels leaner, somehow you need to convert it to a prompt. a library will do that in some way, BAML has a VSCode playground to see the entire prompt + tokenization. If we had to do this off of python&#x2F;ts, we would run into the halting problem and making the playground would be much much harder.<p>2. there&#x27;s a lot of codegen we do for users, to make life easier, e.g. w&#x2F;o BAML, to now do streaming for the resume, you would have to do something like this:<p>class PartialResume:
  name: Optional[str]
  education: List[PartialEducation]
  skills: List[str]<p>and then at some point you need to reparse PartialResume -&gt; Resume, we can codegen all of that for you, and give you autocomplete, type-safety for free.<p>3. We added a lot of static analysis &#x2F; jump to definition etc to JINJA (which we use for strings), and that is much easier to navigate than f-strings.<p>4. Since its code-gen we can support all languages way easier, so prompting techniques in python work the same exact way for the same code in typescript.</div><br/></div></div></div></div><div id="40715174" class="c"><input type="checkbox" id="c-40715174" checked=""/><div class="controls bullet"><span class="by">sticksen</span><span>|</span><a href="#40714992">prev</a><span>|</span><a href="#40714954">next</a><span>|</span><label class="collapse" for="c-40715174">[-]</label><label class="expand" for="c-40715174">[2 more]</label></div><br/><div class="children"><div class="content">Seems like a marketing post to me. 
Langchain and Llama-Index are indeed used in production. Where does it state that the other libraries are used in production?</div><br/><div id="40715243" class="c"><input type="checkbox" id="c-40715243" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#40715174">parent</a><span>|</span><a href="#40714954">next</a><span>|</span><label class="collapse" for="c-40715243">[-]</label><label class="expand" for="c-40715243">[1 more]</label></div><br/><div class="children"><div class="content">My point exactly, lot of brigading on this thread</div><br/></div></div></div></div><div id="40714954" class="c"><input type="checkbox" id="c-40714954" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#40715174">prev</a><span>|</span><a href="#40715164">next</a><span>|</span><label class="collapse" for="c-40714954">[-]</label><label class="expand" for="c-40714954">[2 more]</label></div><br/><div class="children"><div class="content">You totally ignored  the Lamini JSON output mode - which is full speed and supports enums for classifiers<p><a href="https:&#x2F;&#x2F;lamini-ai.github.io&#x2F;inference&#x2F;json_output&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lamini-ai.github.io&#x2F;inference&#x2F;json_output&#x2F;</a></div><br/><div id="40714977" class="c"><input type="checkbox" id="c-40714977" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40714954">parent</a><span>|</span><a href="#40715164">next</a><span>|</span><label class="collapse" for="c-40714977">[-]</label><label class="expand" for="c-40714977">[1 more]</label></div><br/><div class="children"><div class="content">thats pretty cool! We&#x27;ll update the page after taking a look at the library!</div><br/></div></div></div></div><div id="40715164" class="c"><input type="checkbox" id="c-40715164" checked=""/><div class="controls bullet"><span class="by">intellectronica</span><span>|</span><a href="#40714954">prev</a><span>|</span><a href="#40714509">next</a><span>|</span><label class="collapse" for="c-40715164">[-]</label><label class="expand" for="c-40715164">[1 more]</label></div><br/><div class="children"><div class="content">This is the best ... no, the only way to do real software with LLMs. Nice comparison, and not surprising, Instructor is in many ways the best and most comprehensive library (not BAML). IMO Instructor is also the lightest and nicest library to use, just a thin layer on top of the API and Pydantic.</div><br/></div></div><div id="40714509" class="c"><input type="checkbox" id="c-40714509" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40715164">prev</a><span>|</span><a href="#40714816">next</a><span>|</span><label class="collapse" for="c-40714509">[-]</label><label class="expand" for="c-40714509">[4 more]</label></div><br/><div class="children"><div class="content">Hey everyone! One of the creators of BAML here! Appreciate sharing this post. For anyone interested in playing around with an interactive version of BAML online, check it out here: <a href="https:&#x2F;&#x2F;www.promptfiddle.com" rel="nofollow">https:&#x2F;&#x2F;www.promptfiddle.com</a></div><br/><div id="40714868" class="c"><input type="checkbox" id="c-40714868" checked=""/><div class="controls bullet"><span class="by">dsign</span><span>|</span><a href="#40714509">parent</a><span>|</span><a href="#40714816">next</a><span>|</span><label class="collapse" for="c-40714868">[-]</label><label class="expand" for="c-40714868">[3 more]</label></div><br/><div class="children"><div class="content">Really interesting library! In the docs, could you describe in a bit more detail which kind of JSON errors it tolerates? And which models currently work best with your parsing approach?</div><br/><div id="40714932" class="c"><input type="checkbox" id="c-40714932" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40714509">root</a><span>|</span><a href="#40714868">parent</a><span>|</span><a href="#40714816">next</a><span>|</span><label class="collapse" for="c-40714932">[-]</label><label class="expand" for="c-40714932">[2 more]</label></div><br/><div class="children"><div class="content">Thanks! We should add that to the docs haha. But the here&#x27;s a few:<p>- keys without strings<p>- coercing singular types -&gt; arrays when the response requires an array<p>- removing any prefix or suffix tags<p>- picking the best of many JSON candidates in a string<p>- unescaped newlines + quotes so &quot;afds&quot;asdf&quot; converts to &quot;afds\&quot;asdf&quot;<p>In terms of models, honestly, we tried as bad as llama2, and it seems to work in quite a few use cases</div><br/><div id="40714962" class="c"><input type="checkbox" id="c-40714962" checked=""/><div class="controls bullet"><span class="by">dsign</span><span>|</span><a href="#40714509">root</a><span>|</span><a href="#40714932">parent</a><span>|</span><a href="#40714816">next</a><span>|</span><label class="collapse" for="c-40714962">[-]</label><label class="expand" for="c-40714962">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I see myself using the library soon :-)</div><br/></div></div></div></div></div></div></div></div><div id="40714816" class="c"><input type="checkbox" id="c-40714816" checked=""/><div class="controls bullet"><span class="by">CraftingLinks</span><span>|</span><a href="#40714509">prev</a><span>|</span><a href="#40714866">next</a><span>|</span><label class="collapse" for="c-40714816">[-]</label><label class="expand" for="c-40714816">[7 more]</label></div><br/><div class="children"><div class="content">We just use openai function calls (tools) and then use Pydantic to verify the JSON. When validation fails we try the prompt again.</div><br/><div id="40715169" class="c"><input type="checkbox" id="c-40715169" checked=""/><div class="controls bullet"><span class="by">knallfrosch</span><span>|</span><a href="#40714816">parent</a><span>|</span><a href="#40714832">next</a><span>|</span><label class="collapse" for="c-40715169">[-]</label><label class="expand" for="c-40715169">[1 more]</label></div><br/><div class="children"><div class="content">Same here. I send a JSON schema along with the prompt to ChatGPT as function_call and then verify with NodeJS + Ajv against the same schema again.</div><br/></div></div><div id="40714832" class="c"><input type="checkbox" id="c-40714832" checked=""/><div class="controls bullet"><span class="by">aaronvg</span><span>|</span><a href="#40714816">parent</a><span>|</span><a href="#40715169">prev</a><span>|</span><a href="#40714866">next</a><span>|</span><label class="collapse" for="c-40714832">[-]</label><label class="expand" for="c-40714832">[5 more]</label></div><br/><div class="children"><div class="content">[Other BAML creator here!] one time we told a customer to do this to fix small json mistakes but turns out their customers don&#x27;t tolerate a +20-30s increase in latency for regenerating a long json structure.<p>We instead had to write a parser to catch small mistakes like missing commas, quotes etc, and parse content even if there&#x27;s things like reasoning in the response, like here: <a href="https:&#x2F;&#x2F;www.promptfiddle.com&#x2F;Chain-of-Thought-KcSBh" rel="nofollow">https:&#x2F;&#x2F;www.promptfiddle.com&#x2F;Chain-of-Thought-KcSBh</a></div><br/><div id="40715136" class="c"><input type="checkbox" id="c-40715136" checked=""/><div class="controls bullet"><span class="by">b2v</span><span>|</span><a href="#40714816">root</a><span>|</span><a href="#40714832">parent</a><span>|</span><a href="#40714866">next</a><span>|</span><label class="collapse" for="c-40715136">[-]</label><label class="expand" for="c-40715136">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure I understand, in the docs for the python client it says that BAML types get converted to Pydantic models, doesn&#x27;t that step include the extra latency you mentioned?</div><br/><div id="40715308" class="c"><input type="checkbox" id="c-40715308" checked=""/><div class="controls bullet"><span class="by">aaronvg</span><span>|</span><a href="#40714816">root</a><span>|</span><a href="#40715136">parent</a><span>|</span><a href="#40714866">next</a><span>|</span><label class="collapse" for="c-40715308">[-]</label><label class="expand" for="c-40715308">[3 more]</label></div><br/><div class="children"><div class="content">My bad, I think I didnt explain correctly. Basically you have two options when a &quot;,&quot; is missing (amongst other issues) in an LLM output which causes a parsing issue:<p>- retry the request, which may take 30+ secs (if your LLM outputs are really long and you&#x27;re using something like gpt4)<p>- fix the parsing issue<p>In our library we do the latter. The conversion from BAML types to Pydantic ones is a compile-time step unrelated to the problem above. That doesn&#x27;t happen at runtime.</div><br/><div id="40715362" class="c"><input type="checkbox" id="c-40715362" checked=""/><div class="controls bullet"><span class="by">b2v</span><span>|</span><a href="#40714816">root</a><span>|</span><a href="#40715308">parent</a><span>|</span><a href="#40714866">next</a><span>|</span><label class="collapse" for="c-40715362">[-]</label><label class="expand" for="c-40715362">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the clarification. How do you handle dynamic types, ie types determined at runtime?</div><br/><div id="40715395" class="c"><input type="checkbox" id="c-40715395" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40714816">root</a><span>|</span><a href="#40715362">parent</a><span>|</span><a href="#40714866">next</a><span>|</span><label class="collapse" for="c-40715395">[-]</label><label class="expand" for="c-40715395">[1 more]</label></div><br/><div class="children"><div class="content">we recently added dynamic type support with this snippet! (docs coming soon!)<p>Python:
<a href="https:&#x2F;&#x2F;github.com&#x2F;BoundaryML&#x2F;baml&#x2F;blob&#x2F;413fdf12a0c8c1ebb75c44d756312e647807068c&#x2F;integ-tests&#x2F;python&#x2F;test_functions.py#L315">https:&#x2F;&#x2F;github.com&#x2F;BoundaryML&#x2F;baml&#x2F;blob&#x2F;413fdf12a0c8c1ebb75c...</a><p>Typescript:
<a href="https:&#x2F;&#x2F;github.com&#x2F;BoundaryML&#x2F;baml&#x2F;blob&#x2F;413fdf12a0c8c1ebb75c44d756312e647807068c&#x2F;integ-tests&#x2F;typescript&#x2F;tests&#x2F;integ-tests.test.ts#L212">https:&#x2F;&#x2F;github.com&#x2F;BoundaryML&#x2F;baml&#x2F;blob&#x2F;413fdf12a0c8c1ebb75c...</a><p>Snippet:<p>async def test_dynamic():<p><pre><code>    tb = TypeBuilder()

    tb.Person.add_property(&quot;last_name&quot;, tb.string().list())

    tb.Person.add_property(&quot;height&quot;, tb.float().optional()).description(
        &quot;Height in meters&quot;
    )


    tb.Hobby.add_value(&quot;chess&quot;)

    for name, val in tb.Hobby.list_values():
        val.alias(name.lower())

    tb.Person.add_property(&quot;hobbies&quot;, tb.Hobby.type().list()).description(
        &quot;Some suggested hobbies they might be good at&quot;
    )

    # no_tb_res = await b.ExtractPeople(&quot;My name is Harrison. My hair is black and I&#x27;m 6 feet tall.&quot;)
    tb_res = await b.ExtractPeople(
        &quot;My name is Harrison. My hair is black and I&#x27;m 6 feet tall. I&#x27;m pretty good around the hoop.&quot;,
        {&quot;tb&quot;: tb},
    )

    assert len(tb_res) &gt; 0, &quot;Expected non-empty result but got empty.&quot;

    for r in tb_res:
        print(r.model_dump())</code></pre></div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40714866" class="c"><input type="checkbox" id="c-40714866" checked=""/><div class="controls bullet"><span class="by">JakaJancar</span><span>|</span><a href="#40714816">prev</a><span>|</span><a href="#40714502">next</a><span>|</span><label class="collapse" for="c-40714866">[-]</label><label class="expand" for="c-40714866">[3 more]</label></div><br/><div class="children"><div class="content">AI noob question:<p>Why do OpenAI&#x2F;Anthropic&#x2F;... not support constraining token generation? I&#x27;d imagine producing valid structured output would be at the top of their feature request lists.</div><br/><div id="40715217" class="c"><input type="checkbox" id="c-40715217" checked=""/><div class="controls bullet"><span class="by">joatmon-snoo</span><span>|</span><a href="#40714866">parent</a><span>|</span><a href="#40714901">next</a><span>|</span><label class="collapse" for="c-40715217">[-]</label><label class="expand" for="c-40715217">[1 more]</label></div><br/><div class="children"><div class="content">Author here- besides hellovai’s point about the performance bottleneck, it’s a really tricky semantic problem!<p>LLMs today are really good at producing output that satisfies the very vague metric of “this looks good to a human” but aren’t nearly as good at producing output that satisfies a complex set of syntax and schema constraints. The state space of the former is much larger than the latter, so there’s a lot more opportunity for an LLM to be successful by targeting the state space of “looks good to a human”. Plus, there’s still a lot of room for advancement in multimodality and data quality improvements.<p>Search problems, in general, deal with this too: it’s easy to provide a good search experience when there are a lot of high-quality candidates, and much harder when there are fewer, because all you have to do is return just a few of the best candidates. (This is partly why Google Drive Search has always sucked compared to Web Search- it’s really hard to guess exactly which document in a 10k-file-Drive a user is looking for, as opposed to finding something on Wikipedia&#x2F;NYTimes&#x2F;Instagram that the user might be looking for!)</div><br/></div></div><div id="40714901" class="c"><input type="checkbox" id="c-40714901" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40714866">parent</a><span>|</span><a href="#40715217">prev</a><span>|</span><a href="#40714502">next</a><span>|</span><label class="collapse" for="c-40714901">[-]</label><label class="expand" for="c-40714901">[1 more]</label></div><br/><div class="children"><div class="content">not a noob question, here&#x27;s how the LLM works:<p>```<p>prompt = &quot;...&quot;<p>output = []<p>do:<p><pre><code>  token_probabilities = call_model(prompt)

  best_token = pick_best(token_probabilities)

  if best_token == &#x27;&lt;END&gt;&#x27;:

    break

  output += best_token
</code></pre>
while true<p>return output<p>```<p>basically to support generation they would need to modify pick_best to support constraining. That would make it so they can&#x27;t optimize the hot loop at their scales. They support super broad output constraints like JSON which apply to everyone, but that leads to other issues (things like chain-of-thought&#x2F;reasoning perform way worse in structured responses).</div><br/></div></div></div></div><div id="40715098" class="c"><input type="checkbox" id="c-40715098" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#40714502">prev</a><span>|</span><a href="#40714792">next</a><span>|</span><label class="collapse" for="c-40715098">[-]</label><label class="expand" for="c-40715098">[1 more]</label></div><br/><div class="children"><div class="content">JM2C<p>1. Langchain not being used in production?<p>&gt; How out of touch is that remark? Hard pressed to find agentic framework implementation outside of Langchain&#x2F;Llamaindex.<p>2. Outlines is not expected to work with OpenAI API because it wasn&#x27;t created to do that.</div><br/></div></div><div id="40714792" class="c"><input type="checkbox" id="c-40714792" checked=""/><div class="controls bullet"><span class="by">alhaad</span><span>|</span><a href="#40715098">prev</a><span>|</span><a href="#40715234">next</a><span>|</span><label class="collapse" for="c-40714792">[-]</label><label class="expand" for="c-40714792">[5 more]</label></div><br/><div class="children"><div class="content">Are there fine tuned models that perform better for structured &#x2F; parsable outputs?</div><br/><div id="40714873" class="c"><input type="checkbox" id="c-40714873" checked=""/><div class="controls bullet"><span class="by">_flux</span><span>|</span><a href="#40714792">parent</a><span>|</span><a href="#40714875">next</a><span>|</span><label class="collapse" for="c-40714873">[-]</label><label class="expand" for="c-40714873">[3 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t the answer to that question, but llama.cpp has a feature to constrain output to the provided grammar, such as <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;grammars&#x2F;json.gbnf">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;grammars&#x2F;...</a><p>Others should really implement that as well. You still need to guide the model to produce e.g. JSON to get good results, but they will 100% guaranteed be valid per the grammar.</div><br/><div id="40715014" class="c"><input type="checkbox" id="c-40715014" checked=""/><div class="controls bullet"><span class="by">alhaad</span><span>|</span><a href="#40714792">root</a><span>|</span><a href="#40714873">parent</a><span>|</span><a href="#40714875">next</a><span>|</span><label class="collapse" for="c-40715014">[-]</label><label class="expand" for="c-40715014">[2 more]</label></div><br/><div class="children"><div class="content">Agreed that others should implement it as well but coercing llama to output results with matching grammar needs work.</div><br/><div id="40715063" class="c"><input type="checkbox" id="c-40715063" checked=""/><div class="controls bullet"><span class="by">_flux</span><span>|</span><a href="#40714792">root</a><span>|</span><a href="#40715014">parent</a><span>|</span><a href="#40714875">next</a><span>|</span><label class="collapse" for="c-40715063">[-]</label><label class="expand" for="c-40715063">[1 more]</label></div><br/><div class="children"><div class="content">What kind of work? I&#x27;ve only given it a short try before moving to Ollama that doesn&#x27;t have it, but it seemed to have worked there. (With ollama I need to use a retry system.)<p>edit: I researched a bit and apparently it can reduce performance, plus the streaming mode fails to report incorrect grammars. Overall these don&#x27;t seem like deal-breakers.</div><br/></div></div></div></div></div></div><div id="40714875" class="c"><input type="checkbox" id="c-40714875" checked=""/><div class="controls bullet"><span class="by">cpursley</span><span>|</span><a href="#40714792">parent</a><span>|</span><a href="#40714873">prev</a><span>|</span><a href="#40715234">next</a><span>|</span><label class="collapse" for="c-40714875">[-]</label><label class="expand" for="c-40714875">[1 more]</label></div><br/><div class="children"><div class="content">Fireworks.ai Firefunction is pretty good. Not GPT-level but it’s an open model.</div><br/></div></div></div></div><div id="40715234" class="c"><input type="checkbox" id="c-40715234" checked=""/><div class="controls bullet"><span class="by">zora_goron</span><span>|</span><a href="#40714792">prev</a><span>|</span><a href="#40715002">next</a><span>|</span><label class="collapse" for="c-40715234">[-]</label><label class="expand" for="c-40715234">[4 more]</label></div><br/><div class="children"><div class="content">The article mentions,<p>&gt;&gt; “you&#x27;ve tried response_format: &quot;json&quot; and function calling and been disappointed by the results”<p>Can anyone share any examples of disappointments or issues with these techniques? Overall I’ve been pretty happy with JSON mode via OpenAI API so I’m curious to hear about any drawbacks with it.</div><br/><div id="40715292" class="c"><input type="checkbox" id="c-40715292" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40715234">parent</a><span>|</span><a href="#40715280">next</a><span>|</span><label class="collapse" for="c-40715292">[-]</label><label class="expand" for="c-40715292">[1 more]</label></div><br/><div class="children"><div class="content">The main drawback is really when you attempt to do more advanced prompting techniques like chain-of-thought or reasoning.<p>forcing those parts to be json, can be hard and unnecessarily constrain the model. e.g. <a href="https:&#x2F;&#x2F;www.promptfiddle.com&#x2F;Chain-of-Thought-KcSBh" rel="nofollow">https:&#x2F;&#x2F;www.promptfiddle.com&#x2F;Chain-of-Thought-KcSBh</a><p>try pressing run tests and you&#x27;ll see what i mean! this method or doing chain of thought works a bit better</div><br/></div></div><div id="40715280" class="c"><input type="checkbox" id="c-40715280" checked=""/><div class="controls bullet"><span class="by">amake</span><span>|</span><a href="#40715234">parent</a><span>|</span><a href="#40715292">prev</a><span>|</span><a href="#40715002">next</a><span>|</span><label class="collapse" for="c-40715280">[-]</label><label class="expand" for="c-40715280">[2 more]</label></div><br/><div class="children"><div class="content">We specify an output schema (TypeScript syntax) in our system prompt, and OpenAI gets it right <i>most</i> of the time. With some regularity it will give invalid (per the schema) output like<p>- Return a single object instead of an array of objects<p>- Return an array of a single object instead of just the object<p>On the other hand I personally haven&#x27;t seen it give malformed JSON; the JSON is well-formed but not compliant with the schema we specified.</div><br/><div id="40715310" class="c"><input type="checkbox" id="c-40715310" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40715234">root</a><span>|</span><a href="#40715280">parent</a><span>|</span><a href="#40715002">next</a><span>|</span><label class="collapse" for="c-40715310">[-]</label><label class="expand" for="c-40715310">[1 more]</label></div><br/><div class="children"><div class="content">oh thats really interesting, how often do you get errors like that?<p>fyi, we actually fix those specific errors in our parser :)</div><br/></div></div></div></div></div></div><div id="40715002" class="c"><input type="checkbox" id="c-40715002" checked=""/><div class="controls bullet"><span class="by">wiradikusuma</span><span>|</span><a href="#40715234">prev</a><span>|</span><a href="#40715104">next</a><span>|</span><label class="collapse" for="c-40715002">[-]</label><label class="expand" for="c-40715002">[2 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t we ask LLM&#x2F;GenAI to summarize it and return structured output? &#x2F;sarcasm (half?)</div><br/><div id="40715145" class="c"><input type="checkbox" id="c-40715145" checked=""/><div class="controls bullet"><span class="by">hellovai</span><span>|</span><a href="#40715002">parent</a><span>|</span><a href="#40715104">next</a><span>|</span><label class="collapse" for="c-40715145">[-]</label><label class="expand" for="c-40715145">[1 more]</label></div><br/><div class="children"><div class="content">;) <a href="https:&#x2F;&#x2F;www.promptfiddle.com&#x2F;structured-summary-66myE" rel="nofollow">https:&#x2F;&#x2F;www.promptfiddle.com&#x2F;structured-summary-66myE</a> (sorry bad syntax highlighting when including baml code in baml code)<p>{
author: &quot;Sam Lijin&quot;<p>key_points: [
  &quot;Structured output from LLMs, like JSON, is a common challenge.&quot;<p><pre><code>  &quot;Existing solutions like response_format: &#x27;json&#x27; and function calling often disappoint.&quot;

  &quot;The article compares multiple frameworks designed to handle structured output.&quot;

  &quot;Handling and preventing malformed JSON is a critical concern.&quot;

  &quot;Two main techniques for this: parsing malformed JSON or constraining LLM token generation.&quot;

  &quot;Framework comparison includes details on language support, JSON handling, prompt building, control, model providers, API flavors, type definitions, and test frameworks.&quot;

  &quot;BAML is noted for its robust handling of malformed JSON using a new Rust-based parser.&quot;

  &quot;Instructor supports multiple LLM providers but has limitations on prompt control.&quot;

  &quot;Guidance, Outlines, and others apply LLM token constraints but have limitations with models like OpenAI&#x27;s.&quot;</code></pre>
]<p>take_way: &quot;Consider using frameworks that efficiently handle malformed JSON and offer prompt control to get the desired structured output from LLMs.&quot;<p>}</div><br/></div></div></div></div><div id="40715104" class="c"><input type="checkbox" id="c-40715104" checked=""/><div class="controls bullet"><span class="by">aredox</span><span>|</span><a href="#40715002">prev</a><span>|</span><label class="collapse" for="c-40715104">[-]</label><label class="expand" for="c-40715104">[1 more]</label></div><br/><div class="children"><div class="content">... Am I the only one thinking all those contorsions to get something usable are completely mental? All to get something potentially completely wrong in a subtle way?<p>Those LLM not only suck megawatts of energy and TFLOPS of compute, but they also consume heaps of brain power - all that for what, in the end? What <i>betterment</i>?</div><br/></div></div></div></div></div></div></div></body></html>