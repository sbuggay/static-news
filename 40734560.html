<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718874079701" as="style"/><link rel="stylesheet" href="styles.css?v=1718874079701"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://scottaaronson.blog/?p=710">Rosser&#x27;s Theorem via Turing Machines (2011)</a>Â <span class="domain">(<a href="https://scottaaronson.blog">scottaaronson.blog</a>)</span></div><div class="subtext"><span>dargscisyhp</span> | <span>7 comments</span></div><br/><div><div id="40735113" class="c"><input type="checkbox" id="c-40735113" checked=""/><div class="controls bullet"><span class="by">anandkulkarni</span><span>|</span><a href="#40735080">next</a><span>|</span><label class="collapse" for="c-40735113">[-]</label><label class="expand" for="c-40735113">[1 more]</label></div><br/><div class="children"><div class="content">When I TA&#x27;d computability &amp; complexity as a graduate student, I always loved giving this proof of Godel&#x27;s Theorem as an easy corollary of the Halting Problem as a homework assignment.<p>It&#x27;s beautiful, elegant, and easy to understand. I was introduced to the proof by a note in Sipser&#x27;s text.</div><br/></div></div><div id="40735080" class="c"><input type="checkbox" id="c-40735080" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#40735113">prev</a><span>|</span><a href="#40734852">next</a><span>|</span><label class="collapse" for="c-40735080">[-]</label><label class="expand" for="c-40735080">[1 more]</label></div><br/><div class="children"><div class="content">There is more than one &quot;Rosser&#x27;s theorem&quot; to wit:<p><pre><code>  In number theory, Rosser&#x27;s theorem states that the  {\displaystyle n}th prime number is greater than  log  {\displaystyle n\log n}, where log{\displaystyle \log } is the natural logarithm function. It was published by J. Barkley Rosser in 1939.
</code></pre>
(wikipedia)<p>Wiki refers to what Scott Aaronson is calling Rosser&#x27;s theorem as &quot;Rosser&#x27;s trick&quot;<p><pre><code>  Rosser&#x27;s trick uses a formula that says &quot;If this sentence is provable, there is a shorter proof of its negation&quot;.
</code></pre>
Not that wiki is the cite beyond all reproach, having more than one theorem named after you is also a good thing.</div><br/></div></div><div id="40734852" class="c"><input type="checkbox" id="c-40734852" checked=""/><div class="controls bullet"><span class="by">waldrews</span><span>|</span><a href="#40735080">prev</a><span>|</span><label class="collapse" for="c-40734852">[-]</label><label class="expand" for="c-40734852">[4 more]</label></div><br/><div class="children"><div class="content">This seems like a good place to ask - if your knowledge of (computation&#x2F;complexity) theory is at the level of the Sipser Introduction to the Theory of Computation book, what&#x27;s a good learning path to get current?</div><br/><div id="40735340" class="c"><input type="checkbox" id="c-40735340" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40734852">parent</a><span>|</span><a href="#40735468">next</a><span>|</span><label class="collapse" for="c-40735340">[-]</label><label class="expand" for="c-40735340">[1 more]</label></div><br/><div class="children"><div class="content">Disclaimer: I have no clue of a complete learning path.  But here are some suggestions you might find interesting.<p>If you want to go heavy on complexity, you could try &#x27;Combinatorial Optimization: Polyhedra and Efficiency&#x27; by Alexander Schrijver.  But it&#x27;s basically the equivalent of Knuth&#x27;s Art of Computer Programming for complexity (in terms of rigour and breadth of approach).<p>I have the three volumes of Schrijver&#x27;s book on my desk, and even occasionally look into them.  But I admit I never read the whole thing.  I love the historical context he gives, eg explaining how maximum flow &#x2F; minimum cut problems were first formally investigated in the Cold War when Western boffins wanted to work out how to most efficiently cut the Soviet rail network&#x27;s ability to move stuff to Western Europe.<p>If you want something much lighter, you could check out some of Scott Aaronson&#x27;s backlog.  Eg <a href="https:&#x2F;&#x2F;www.scottaaronson.com&#x2F;papers&#x2F;philos.pdf" rel="nofollow">https:&#x2F;&#x2F;www.scottaaronson.com&#x2F;papers&#x2F;philos.pdf</a> &#x27;Why Philosophers Should Care About Computational Complexity&#x27;.  You can follow his bibliography for more background.  Scott&#x27;s blog backlog is also good.<p>If you like randomised algorithms, you might also like &#x27;The Discrepancy Method
Randomness and Complexity&#x27; by Bernard Chazelle. It&#x27;s available for free online. Eg at <a href="https:&#x2F;&#x2F;api.pageplace.de&#x2F;preview&#x2F;DT0400.9781316047804_A25932634&#x2F;preview-9781316047804_A25932634.pdf" rel="nofollow">https:&#x2F;&#x2F;api.pageplace.de&#x2F;preview&#x2F;DT0400.9781316047804_A25932...</a><p>Apropos Bernard Chazelle, I&#x27;m working on a little paper myself.  It&#x27;s about how to simulate the outcome of a series of heap operations (like insert and delete-minimum) in O(n) time, instead of the trivial to achieve O(n log n) you get from a naive implementation.  I&#x27;m looking for some collaborators, if you are interested.</div><br/></div></div><div id="40735468" class="c"><input type="checkbox" id="c-40735468" checked=""/><div class="controls bullet"><span class="by">ccppurcell</span><span>|</span><a href="#40734852">parent</a><span>|</span><a href="#40735340">prev</a><span>|</span><label class="collapse" for="c-40735468">[-]</label><label class="expand" for="c-40735468">[2 more]</label></div><br/><div class="children"><div class="content">The next step book wise is Arora and Barak, Computational Complexity: a modern approach. Of course that book is already out of date. The breakthrough that comes to mind is Ryan Williams&#x27; circuit lower bounds. He wrote a nice explanation of the proof here: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1111.1261" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1111.1261</a> 
He defines all the key terms pretty well but probably requires a bit of &quot;maturity&quot; to understand. There are also various breakthroughs to do with interactive proofs.</div><br/><div id="40735855" class="c"><input type="checkbox" id="c-40735855" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#40734852">root</a><span>|</span><a href="#40735468">parent</a><span>|</span><label class="collapse" for="c-40735855">[-]</label><label class="expand" for="c-40735855">[1 more]</label></div><br/><div class="children"><div class="content">Not OP but while I aced my class that used Sipser&#x27;s book, nevertheless I still lacked the mathematical comfort&#x2F;maturity to do Arora and Barak. The first class started talking about &quot;random functions&quot; and I had no background for that at the time. I don&#x27;t know if some kind of advanced discrete math class would have helped fill in the gaps at the time, at least for myself. Or maybe a remedial discrete math class, I was mainly an engineer not originally intending to be a CS major.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>