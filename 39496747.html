<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708851659757" as="style"/><link rel="stylesheet" href="styles.css?v=1708851659757"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2012.00152">Every model learned by gradient descent is approximately a kernel machine (2020)</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>Anon84</span> | <span>79 comments</span></div><br/><div><div id="39497310" class="c"><input type="checkbox" id="c-39497310" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#39498914">next</a><span>|</span><label class="collapse" for="c-39497310">[-]</label><label class="expand" for="c-39497310">[57 more]</label></div><br/><div class="children"><div class="content">This connects to a suspicion I have that a lot of the most impressive results, that have caught public attention, for LLMs and Sora are due to memorization. This is not to say that they can&#x27;t generalize or mix learned patterns - I think they can do that too, which is quite impressive to a researcher, but these are demonstrated on tasks that the public might not care as much about, and the performance there isn&#x27;t always great as well.<p>Combined with opacity in train&#x2F;test splits, this suggests a type of data laundering, where the extreme is that much of Sora (for example) is regurgitating things seen in training, which was actually made perhaps by a human using a game engine, or drone video. But it&#x27;s only news because a model generated it. Of course, I don&#x27;t have strong evidence for this. But one of the most impressive parts of Sora is the level of detail, much of which is not specified by the text prompt, and still can&#x27;t fully be accounted for by our knowledge that OAI expands text prompts for image&#x2F; video generation behind the scenes. Where is this precise detail coming from exactly? I suggest it&#x27;s from memorization.</div><br/><div id="39497426" class="c"><input type="checkbox" id="c-39497426" checked=""/><div class="controls bullet"><span class="by">PheonixPharts</span><span>|</span><a href="#39497310">parent</a><span>|</span><a href="#39497546">next</a><span>|</span><label class="collapse" for="c-39497426">[-]</label><label class="expand" for="c-39497426">[20 more]</label></div><br/><div class="children"><div class="content">François Chollet, author of Keras, lays it out pretty clearly that LLMs are just memorizing [0] (there’s more links if you follow that thread).<p>I think LLMs <i>are</i> incredible, and spend most of my days working with them closely, but they are not nearly as close to “AGI” as people think primarily due to their inability to really generalize.<p>At the end of the day LLMs <i>aren’t</i> that different than old school n-gram Markov chains, except rather than working on n-grams, they’re working in a (very sophisticated) latent space. Their power is really these incredible latent languages spaces we’re still just starting to understand.<p>In all my years of tech the “AI” space is the most curious hype-bubble since the things people expect to happen are entirely out line with what is possible, while at the same time the potential of these models is still, imho, <i>underexplored</i> and largely ignored by the vast majority of people attempting to build things with them.<p>99% of the people I know working in this space are just calling APIs and trying to do some variant of code generations, where a small minority of people are really trying to figure out what’s going on in these models and what can be done with them successfully.<p>0. <a href="https:&#x2F;&#x2F;twitter.com&#x2F;fchollet&#x2F;status&#x2F;1755250582334709970" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;fchollet&#x2F;status&#x2F;1755250582334709970</a></div><br/><div id="39497840" class="c"><input type="checkbox" id="c-39497840" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497426">parent</a><span>|</span><a href="#39497597">next</a><span>|</span><label class="collapse" for="c-39497840">[-]</label><label class="expand" for="c-39497840">[6 more]</label></div><br/><div class="children"><div class="content">We can prove that transformers can do computation beyond memorization, and that they can learn it from examples, unless you think learning an algorithm is memorization.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.05217" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.05217</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.16028" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.16028</a></div><br/><div id="39498359" class="c"><input type="checkbox" id="c-39498359" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497840">parent</a><span>|</span><a href="#39498443">next</a><span>|</span><label class="collapse" for="c-39498359">[-]</label><label class="expand" for="c-39498359">[4 more]</label></div><br/><div class="children"><div class="content">Maybe there doing something we don’t really have a name for yet ? This is why it causes so much controversy.<p>Not really generalizing, not memorizing, maybe approximating ?</div><br/><div id="39498422" class="c"><input type="checkbox" id="c-39498422" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498359">parent</a><span>|</span><a href="#39498396">next</a><span>|</span><label class="collapse" for="c-39498422">[-]</label><label class="expand" for="c-39498422">[1 more]</label></div><br/><div class="children"><div class="content">There is a name for this already: associative memory. That’s how you catch a ball: you condition your memory of catching a ball with proprioceptive and visual input, much like a multimodal transformer. There’s no thinking involved - you wouldn’t be able to catch it if you had to think</div><br/></div></div><div id="39498396" class="c"><input type="checkbox" id="c-39498396" checked=""/><div class="controls bullet"><span class="by">dataflow</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498359">parent</a><span>|</span><a href="#39498422">prev</a><span>|</span><a href="#39498443">next</a><span>|</span><label class="collapse" for="c-39498396">[-]</label><label class="expand" for="c-39498396">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Vibing&quot;?</div><br/><div id="39498427" class="c"><input type="checkbox" id="c-39498427" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498396">parent</a><span>|</span><a href="#39498443">next</a><span>|</span><label class="collapse" for="c-39498427">[-]</label><label class="expand" for="c-39498427">[1 more]</label></div><br/><div class="children"><div class="content">No need to come up with any new name, we already have it:<p>1. autocompletion or<p>2. next token prediction and<p>3. (reverse) diffusion</div><br/></div></div></div></div></div></div><div id="39498443" class="c"><input type="checkbox" id="c-39498443" checked=""/><div class="controls bullet"><span class="by">dataflow</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497840">parent</a><span>|</span><a href="#39498359">prev</a><span>|</span><a href="#39497597">next</a><span>|</span><label class="collapse" for="c-39498443">[-]</label><label class="expand" for="c-39498443">[1 more]</label></div><br/><div class="children"><div class="content">I guess what constitutes memorization depends on what you consider &quot;learning an algorithm&quot;.
Memorizing to humans doesn&#x27;t really mean learning the exact input&#x2F;output pairs per se. Like a student might &quot;learn&quot; an algorithm for differentiation (d&#x2F;dx x^n = n x^(n-1)) and then differentiate 732638 x^2 just fine despite never seeing it before, but then tell you the derivative of yx^2 with respect to y is 2yx, or something. Did they really learn how to differentiate or did they just learn a common vibe around it? When teachers see that sort of regurgitation, they call it memorization, despite the input being unique from what the student had seen in the past.</div><br/></div></div></div></div><div id="39497597" class="c"><input type="checkbox" id="c-39497597" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497426">parent</a><span>|</span><a href="#39497840">prev</a><span>|</span><a href="#39498404">next</a><span>|</span><label class="collapse" for="c-39497597">[-]</label><label class="expand" for="c-39497597">[1 more]</label></div><br/><div class="children"><div class="content">A lot of the words he uses in that tweet aren&#x27;t well defined. e.g. memorization, dataset (does he mean the literal words&#x2F;tokens or any token that is close in space after embedding?), pattern, category, program. The tweet is practically meaningless. I&#x27;m not criticizing him because his blog post is nuanced and he clearly understands what he&#x27;s talking about, but that tweet almost certainly means something quite specific to him and he&#x27;s communicating quite poorly.<p>As you mention, there is a sophisticated representation of the tokens. It&#x27;s so sophisticated that one may reasonably stop calling them tokens (or, even <i>data</i>) and start calling them &quot;concepts&quot;. Now, if someone (or something) has memorized how all the concepts go together... that&#x27;s pretty darn intelligent.</div><br/></div></div><div id="39498404" class="c"><input type="checkbox" id="c-39498404" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497426">parent</a><span>|</span><a href="#39497597">prev</a><span>|</span><a href="#39497474">next</a><span>|</span><label class="collapse" for="c-39498404">[-]</label><label class="expand" for="c-39498404">[5 more]</label></div><br/><div class="children"><div class="content">Francois is not a researcher, however. LLMs aren’t just plain “memories”. They are very explicitly _associative_ memories. And it just so happens that this is mostly what our own brains do, too.<p>Actual cognition is slow and expensive for us, and we try to use it as little as possible, filling in what we can with easy, associative, low energy, near instant stuff.<p>Therein lies the reason why AI can be considered a boon for us humans. If machines took over the mundane work that just drains our energy and doesn&#x27;t add much value, we could finally have the time to actually do what they can’t - think deeply about stuff, with their help where we find our faculties lacking. Rocket for the mind, if you will, rather than a bicycle.</div><br/><div id="39498471" class="c"><input type="checkbox" id="c-39498471" checked=""/><div class="controls bullet"><span class="by">PheonixPharts</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498404">parent</a><span>|</span><a href="#39498771">next</a><span>|</span><label class="collapse" for="c-39498471">[-]</label><label class="expand" for="c-39498471">[2 more]</label></div><br/><div class="children"><div class="content">You must have a pretty impressive resume if you don’t consider François a researcher! The wikipedia would disagree with you [0], as would anyone that has had any interaction with him on the subject.<p>0. <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fran%C3%A7ois_Chollet" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fran%C3%A7ois_Chollet</a></div><br/><div id="39498606" class="c"><input type="checkbox" id="c-39498606" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498471">parent</a><span>|</span><a href="#39498771">next</a><span>|</span><label class="collapse" for="c-39498606">[-]</label><label class="expand" for="c-39498606">[1 more]</label></div><br/><div class="children"><div class="content">Try to find a single paper on Transformers or LLMs in general in Francois’ scientific output: <a href="https:&#x2F;&#x2F;dblp.org&#x2F;pid&#x2F;116&#x2F;8242.html" rel="nofollow">https:&#x2F;&#x2F;dblp.org&#x2F;pid&#x2F;116&#x2F;8242.html</a><p>Don’t get me wrong, Keras is impressive, and Francois is impressive as well. But for insight on LLMs you should probably listen to people who specialize in them.</div><br/></div></div></div></div><div id="39498771" class="c"><input type="checkbox" id="c-39498771" checked=""/><div class="controls bullet"><span class="by">djmips</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498404">parent</a><span>|</span><a href="#39498471">prev</a><span>|</span><a href="#39498756">next</a><span>|</span><label class="collapse" for="c-39498771">[-]</label><label class="expand" for="c-39498771">[1 more]</label></div><br/><div class="children"><div class="content">An AGI might be something that can harness the LLM but also self learn.</div><br/></div></div><div id="39498756" class="c"><input type="checkbox" id="c-39498756" checked=""/><div class="controls bullet"><span class="by">djmips</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498404">parent</a><span>|</span><a href="#39498771">prev</a><span>|</span><a href="#39497474">next</a><span>|</span><label class="collapse" for="c-39498756">[-]</label><label class="expand" for="c-39498756">[1 more]</label></div><br/><div class="children"><div class="content">More like AI will finally be the bicycle for the mind.</div><br/></div></div></div></div><div id="39497474" class="c"><input type="checkbox" id="c-39497474" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497426">parent</a><span>|</span><a href="#39498404">prev</a><span>|</span><a href="#39498452">next</a><span>|</span><label class="collapse" for="c-39497474">[-]</label><label class="expand" for="c-39497474">[2 more]</label></div><br/><div class="children"><div class="content">I think there’s a lot of people who got into tech in 2020.
They are new programmers and technologists and made a life change in 2020.<p>I think they were a big part of the crypto bubble. Lots of talent, hungry for that sweet startup gold, but without the technical background to really know what’s going on.<p>I believe these same groups are operating in the same way with AI. 
Recklessly bashing together APIs and cloud services to create MVPs.<p>It’s all the worst parts of startup culture concentrated.<p>Anyway, that’s why i think most of the AI space rn is just people calling APIs and acting like they discovered fire.<p>&lt;&#x2F;salty rant&gt;</div><br/></div></div><div id="39498452" class="c"><input type="checkbox" id="c-39498452" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497426">parent</a><span>|</span><a href="#39497474">prev</a><span>|</span><a href="#39497723">next</a><span>|</span><label class="collapse" for="c-39498452">[-]</label><label class="expand" for="c-39498452">[1 more]</label></div><br/><div class="children"><div class="content">What is intelligence or generalization if not &quot;sophisticated, complex latent space&quot; navigation?</div><br/></div></div><div id="39497723" class="c"><input type="checkbox" id="c-39497723" checked=""/><div class="controls bullet"><span class="by">VirusNewbie</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497426">parent</a><span>|</span><a href="#39498452">prev</a><span>|</span><a href="#39497693">next</a><span>|</span><label class="collapse" for="c-39497723">[-]</label><label class="expand" for="c-39497723">[1 more]</label></div><br/><div class="children"><div class="content">But we can trivially show that the larger models can generalize for some questions even if we verify the answer isn’t in the training set.</div><br/></div></div><div id="39497693" class="c"><input type="checkbox" id="c-39497693" checked=""/><div class="controls bullet"><span class="by">tymscar</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497426">parent</a><span>|</span><a href="#39497723">prev</a><span>|</span><a href="#39497542">next</a><span>|</span><label class="collapse" for="c-39497693">[-]</label><label class="expand" for="c-39497693">[1 more]</label></div><br/><div class="children"><div class="content">Im curios, what sort of area you think needs more research when it comes to these models</div><br/></div></div><div id="39497542" class="c"><input type="checkbox" id="c-39497542" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497426">parent</a><span>|</span><a href="#39497693">prev</a><span>|</span><a href="#39497546">next</a><span>|</span><label class="collapse" for="c-39497542">[-]</label><label class="expand" for="c-39497542">[2 more]</label></div><br/><div class="children"><div class="content">Show me a Markov chain that can thrash a 9-dan Go master.<p>&quot;But that wasn&#x27;t an LLM.&quot;<p>OK, show me a Markov chain that can write a Python program that can play Go at all.</div><br/><div id="39498584" class="c"><input type="checkbox" id="c-39498584" checked=""/><div class="controls bullet"><span class="by">xapata</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497542">parent</a><span>|</span><a href="#39497546">next</a><span>|</span><label class="collapse" for="c-39498584">[-]</label><label class="expand" for="c-39498584">[1 more]</label></div><br/><div class="children"><div class="content">Semantics, I suppose.  Those ANNs were Markov chains, in a sense.</div><br/></div></div></div></div></div></div><div id="39497546" class="c"><input type="checkbox" id="c-39497546" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#39497310">parent</a><span>|</span><a href="#39497426">prev</a><span>|</span><a href="#39497400">next</a><span>|</span><label class="collapse" for="c-39497546">[-]</label><label class="expand" for="c-39497546">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Where is this precise detail coming from exactly? I suggest it&#x27;s from memorization.<p>Where did the precise detail of the words you&#x27;re using and the concepts they represent come from? Calling it &quot;memorization&quot; is, IMO, a cheap attempt to dismiss some degree of &quot;understanding&quot; that was learned from some form of &quot;experience&quot; (aka training).</div><br/><div id="39497670" class="c"><input type="checkbox" id="c-39497670" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497546">parent</a><span>|</span><a href="#39497652">next</a><span>|</span><label class="collapse" for="c-39497670">[-]</label><label class="expand" for="c-39497670">[4 more]</label></div><br/><div class="children"><div class="content">This is a pretty standard technical term in machine learning which isn’t necessarily a pejorative but a description of behavior. Memorization is different than generalization in that it doesn’t reflect some sort of synthesis from learned knowledge but rather repeating something from training directly. It usually indicates overfitting and is distinct than responding appropriately to something that wasn’t specifically trained on, which is generalization.</div><br/><div id="39498233" class="c"><input type="checkbox" id="c-39498233" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497670">parent</a><span>|</span><a href="#39497652">next</a><span>|</span><label class="collapse" for="c-39498233">[-]</label><label class="expand" for="c-39498233">[3 more]</label></div><br/><div class="children"><div class="content">But by the standard machine learning definition (performance on train&#x2F;test split), LLMs generalize extremely well. This isn&#x27;t even in doubt.</div><br/><div id="39498676" class="c"><input type="checkbox" id="c-39498676" checked=""/><div class="controls bullet"><span class="by">winternewt</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498233">parent</a><span>|</span><a href="#39497652">next</a><span>|</span><label class="collapse" for="c-39498676">[-]</label><label class="expand" for="c-39498676">[2 more]</label></div><br/><div class="children"><div class="content">They do generalize. The claim is that the fine details are not the result from generalization, but from repeating test data verbatim. That seems consistent both with my intuitive understanding of neural networks and with the behavior I&#x27;ve observed, so I&#x27;m inclined to agree. So what does that mean? It means that while LLM:s can produce impressive output, the most impressive results that people are touting probably have a significant amount of verbatim training data in them. In other words, they&#x27;re good but not as good as they seem to be.</div><br/><div id="39498985" class="c"><input type="checkbox" id="c-39498985" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498676">parent</a><span>|</span><a href="#39497652">next</a><span>|</span><label class="collapse" for="c-39498985">[-]</label><label class="expand" for="c-39498985">[1 more]</label></div><br/><div class="children"><div class="content">Mmm the most impressive thing I see LLMs do is take a piece of unstructured input and transform it in some way - summarize, extract information as JSON, etc. This wouldn&#x27;t be possible if it were repeating training data verbatim, since it works on new novel inputs.</div><br/></div></div></div></div></div></div></div></div><div id="39497636" class="c"><input type="checkbox" id="c-39497636" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497546">parent</a><span>|</span><a href="#39497652">prev</a><span>|</span><a href="#39497400">next</a><span>|</span><label class="collapse" for="c-39497636">[-]</label><label class="expand" for="c-39497636">[2 more]</label></div><br/><div class="children"><div class="content">A quick example of what I mean is the art gallery sora video; the individual paintings are not specified in the prompt, I&#x27;m pretty sure. Where did they come from?</div><br/><div id="39498267" class="c"><input type="checkbox" id="c-39498267" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497636">parent</a><span>|</span><a href="#39497400">next</a><span>|</span><label class="collapse" for="c-39498267">[-]</label><label class="expand" for="c-39498267">[1 more]</label></div><br/><div class="children"><div class="content">That seems obvious - they were generated.  Here’s an example with text:<p>Prompt:<p>Write a short story that is comprised of a boy reading poems he wrote to his deceased loved mom, include snippets of several of the poems.<p>Response:<p>In the soft glow of the evening, with the sun dipping below the horizon, casting long shadows over the quiet cemetery, a young boy named Eli walked slowly between the rows of tombstones. He carried a small, worn notebook clutched against his chest, filled with scribbles and verses that only his heart could decipher. Today marked a year since his mother had passed away, and in his solitude, he had found refuge in writing poems that he wished he could share with her.<p>Eli stopped in front of a modest gravestone, engraved with his mother&#x27;s name, &quot;Eleanor Grace.&quot; He took a deep breath, the air heavy with the scent of fresh earth and the faint aroma of roses from a nearby bush. Opening his notebook, he began to read aloud, his voice barely above a whisper, as if he feared disturbing the peace of those resting around him.<p>&quot;The world spins on, a dizzy blur,
But here I stand, thoughts of you stir.
Your laughter, a melody in my mind,
Leaves traces of joy you&#x27;ve left behind.&quot;<p>He paused, looking up at the sky, which was painted in hues of pink and orange. Eli could almost hear his mother&#x27;s voice encouraging him, telling him to find beauty in the world, even when she was no longer by his side.<p>Turning the page, he continued:<p>&quot;In dreams, I roam vast fields of green,
Your hand in mine, a tranquil scene.
Yet, as dawn breaks, and I awake,
I face the world, my heartache to take.&quot;<p>Tears glistened in Eli&#x27;s eyes, but he pressed on, driven by a need to share his innermost feelings, his way of keeping his mother&#x27;s memory alive.<p>&quot;Among the stars, I search for you,
In every sparkle, your essence I view.
A guiding light in the darkest night,
Leading me home, making everything right.&quot;<p>He closed the notebook gently, a small smile playing on his lips despite the tears. Eli felt a warmth envelop him, a sense of his mother&#x27;s love that transcended the boundaries of life and death. He whispered a thank you to the evening breeze, imagining it carrying his words to wherever she might be.<p>As the night drew its curtain, Eli rose to leave, his heart a little lighter. He knew he would return, again and again, to share his poems, his fears, and his dreams with his mother. In this sacred space, amidst verses of longing and love, he found a connection to her that time could not erase.<p>Eli walked away, the notebook once again pressed against his chest, a bridge of words spanning the gap between him and the stars, where he knew a part of his mother would always be listening.</div><br/></div></div></div></div></div></div><div id="39497400" class="c"><input type="checkbox" id="c-39497400" checked=""/><div class="controls bullet"><span class="by">quanto</span><span>|</span><a href="#39497310">parent</a><span>|</span><a href="#39497546">prev</a><span>|</span><a href="#39498398">next</a><span>|</span><label class="collapse" for="c-39497400">[-]</label><label class="expand" for="c-39497400">[11 more]</label></div><br/><div class="children"><div class="content">Then, by your standards, where do most human occupations lie?  Plumbers, lawyers, and engineers?  Aren&#x27;t they mostly regurgitating (by your standards) what they have seen or learned before?</div><br/><div id="39497696" class="c"><input type="checkbox" id="c-39497696" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497400">parent</a><span>|</span><a href="#39497433">next</a><span>|</span><label class="collapse" for="c-39497696">[-]</label><label class="expand" for="c-39497696">[1 more]</label></div><br/><div class="children"><div class="content">Plumbers, lawyers, engineers all synthesize new responses to unfamiliar situations. Part of being a master plumber, which is a true trade craft and requires lots of training, mean being able to problem solve new complex situations that involves ducting of fluids. This isn’t just plunging toilets all day.  Furthermore humans are actually really bad at memorization, which is essentially perfect repetition of data from the training set. In fact a large part of technical trade training is learning how to minimize generalization and maximize memorization for complex skills, but maximizing generalization and minimizing memorization for complex situations where the skills are applied.</div><br/></div></div><div id="39497433" class="c"><input type="checkbox" id="c-39497433" checked=""/><div class="controls bullet"><span class="by">vasilipupkin</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497400">parent</a><span>|</span><a href="#39497696">prev</a><span>|</span><a href="#39497606">next</a><span>|</span><label class="collapse" for="c-39497433">[-]</label><label class="expand" for="c-39497433">[7 more]</label></div><br/><div class="children"><div class="content">bingo, I think it&#x27;s a weird gotcha. Most human occupations require years of training exactly for this purpose, so that humans can just regurgitate data they have seen in training.  Very few humans ever produce truly novel insights.  you wouldn&#x27;t hire a divorce lawyer for a corporate merger and vice versa</div><br/><div id="39498412" class="c"><input type="checkbox" id="c-39498412" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497433">parent</a><span>|</span><a href="#39497549">next</a><span>|</span><label class="collapse" for="c-39498412">[-]</label><label class="expand" for="c-39498412">[1 more]</label></div><br/><div class="children"><div class="content">I reakon if you’re getting paid 200k a year to write YAML it might seem like this but being a plumber is a very 3 dimensional challenge.<p>Not only is there a lot of time constraints and physical issues to work through, you’re often also dealing with logistics problems and job site politics problems too.<p>Waiting for parts, working out how to keep things going in the meantime. Equipment failures etc. it’s quite chaotic from experience.<p>There’s a science but also an art to being a good tradesmen.</div><br/></div></div><div id="39497549" class="c"><input type="checkbox" id="c-39497549" checked=""/><div class="controls bullet"><span class="by">jjmarr</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497433">parent</a><span>|</span><a href="#39498412">prev</a><span>|</span><a href="#39497493">next</a><span>|</span><label class="collapse" for="c-39497549">[-]</label><label class="expand" for="c-39497549">[1 more]</label></div><br/><div class="children"><div class="content">A (common law) lawyer has to remember and understand case law, but they also have to create analogies to explain why and how new situations are like older ones. That&#x27;s one of the main parts of being a <i>good</i> lawyer.<p>AI will be disruptive to <i>aspects</i> of professions that rely on rote memorization or research on a large corpus of data. But many of these supposedly memorization heavy professions require the ability to go beyond one&#x27;s training and extrapolate from what one remembers.</div><br/></div></div><div id="39497493" class="c"><input type="checkbox" id="c-39497493" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497433">parent</a><span>|</span><a href="#39497549">prev</a><span>|</span><a href="#39497606">next</a><span>|</span><label class="collapse" for="c-39497493">[-]</label><label class="expand" for="c-39497493">[4 more]</label></div><br/><div class="children"><div class="content">I think that’s a gross oversimplification.<p>Have you been a plumber or an electrician?<p>Different buildings require different, occasionally novel, solutions. Not all are textbook (most aren’t.)
Not all innovations are published or even widely communicated.<p>Just like in programming, the devil is in the details, otherwise we’d all be using ruby in rails for our web servers, since regurgitating how to build a CRUD app is all you’d need.</div><br/><div id="39497575" class="c"><input type="checkbox" id="c-39497575" checked=""/><div class="controls bullet"><span class="by">BoiledCabbage</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497493">parent</a><span>|</span><a href="#39497606">next</a><span>|</span><label class="collapse" for="c-39497575">[-]</label><label class="expand" for="c-39497575">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Different buildings require different, occasionally novel, solutions. Not all are textbook (most aren’t.) Not all innovations are published or even widely communicated.<p>True, but it&#x27;s mostly derivative just like LLMs. The problem isn&#x27;t &quot;AI&quot; the problem is that people hold AI to much higher standards than humans.<p>AI to them means &quot;scifi&quot;, omniscient and omnipotent. You can have AI and it still be flawed, have weaknesses and shortcomings just like people do.</div><br/><div id="39498137" class="c"><input type="checkbox" id="c-39498137" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497575">parent</a><span>|</span><a href="#39497794">next</a><span>|</span><label class="collapse" for="c-39498137">[-]</label><label class="expand" for="c-39498137">[1 more]</label></div><br/><div class="children"><div class="content">LLMs don’t come up with new derivative solutions to problems in programming land, from what I’ve seen.<p>They always try to stick to common solutions, probably because those are what they were trained on.<p>Now they may introduce me to existing concepts I didn’t know about, but I’ve yet to see any new idea come from an llm itself.<p>To your other point, LLMs aren’t beings, a flawed AI is an incorrect program or algorithm. Entertaining, maybe, useful in some contexts, sure.<p>But they’re not beings. nothing about statistical models are like people. I think it’s dangerous to use such fuzzy conflating language with regards to AI.</div><br/></div></div><div id="39497794" class="c"><input type="checkbox" id="c-39497794" checked=""/><div class="controls bullet"><span class="by">card_zero</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497575">parent</a><span>|</span><a href="#39498137">prev</a><span>|</span><a href="#39497606">next</a><span>|</span><label class="collapse" for="c-39497794">[-]</label><label class="expand" for="c-39497794">[1 more]</label></div><br/><div class="children"><div class="content">So AI is unoriginal and thick as pigshit and is therefore an accurate simulation of a human, neat argument.</div><br/></div></div></div></div></div></div></div></div><div id="39497606" class="c"><input type="checkbox" id="c-39497606" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497400">parent</a><span>|</span><a href="#39497433">prev</a><span>|</span><a href="#39498141">next</a><span>|</span><label class="collapse" for="c-39497606">[-]</label><label class="expand" for="c-39497606">[1 more]</label></div><br/><div class="children"><div class="content">I do think models with incredible memorization is sufficient for enormous applied impact. But it remains an open question of how that relates to AGI, which I think most people think needs some amount of generalization ability that we may not have at the moment, and may not know how to achieve at the moment.</div><br/></div></div><div id="39498141" class="c"><input type="checkbox" id="c-39498141" checked=""/><div class="controls bullet"><span class="by">doubloon</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497400">parent</a><span>|</span><a href="#39497606">prev</a><span>|</span><a href="#39498398">next</a><span>|</span><label class="collapse" for="c-39498141">[-]</label><label class="expand" for="c-39498141">[1 more]</label></div><br/><div class="children"><div class="content">the major part of every occupation is explaining what you are doing to the people who are paying you and hopefully building professional relationships with other human beings in your field. also professionals saying &quot;no&quot; is essential for society and civilization to function.<p>LLMs cannot do any of this, all they do is mimic.</div><br/></div></div></div></div><div id="39498398" class="c"><input type="checkbox" id="c-39498398" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#39497310">parent</a><span>|</span><a href="#39497400">prev</a><span>|</span><a href="#39498372">next</a><span>|</span><label class="collapse" for="c-39498398">[-]</label><label class="expand" for="c-39498398">[1 more]</label></div><br/><div class="children"><div class="content">Obviously it is better at stuff it&#x27;s seen in its training data, but that&#x27;s same for all humans, and likely universal for all forms of intelligence. The question about AGI and the possibility of LLM&#x27;s and such being more than regurgitation of the training set isn&#x27;t whether or not after a certain number of compute cycles that the model can model a part of the natural world or some real process with X degree of accuracy, but rather that it can reliably improve on its failures on its own. This is what humans can do.<p>No human however can spit out a video of SORA&#x27;s quality from their brain alone, and those who can require decades of training with specific video rendering tools. Ask someone to render a video of a man walking in a city and they will internally reference their millions of impressions of the human face and body and millions of impressions of what a cityscape looks like. Ask them to invent a novel form of transportation and create a video of it, most humans short of the exceptionally creative will struggle, just as SORA likely would.<p>They&#x27;ve done brain scans on chess grandmasters and found the part of the brain most active when they play is associated with memory. Memory is the scaffolding upon which the more information dense elements of the natural world and complex processes can be understood. Via these scaffolds of memory as single elements, new connections and abstractions can form. It took the world of fine arts centuries to go beyond merely depicting things in real life (the Renaissance to Impressionism).</div><br/></div></div><div id="39498372" class="c"><input type="checkbox" id="c-39498372" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#39497310">parent</a><span>|</span><a href="#39498398">prev</a><span>|</span><a href="#39498030">next</a><span>|</span><label class="collapse" for="c-39498372">[-]</label><label class="expand" for="c-39498372">[2 more]</label></div><br/><div class="children"><div class="content">LOD is something I have a difficult time comprehending the rationale for as well unless wholesale blocks are being regurgitated.<p>So if you just ask for a picture of a frog, then yes it makes sense that an adversarial model may be able to run through refinements of millions of random noise patterns until it finds one that scores high as showing a frog. But there&#x27;s no innate reason why that image should also have a relatively consistent light source or why its background should even be visually coherent, let alone photorealistic. The most logical explanation for the coherence of the rest of the image is data theft.<p>And I think this is borne out very starkly by playing with ultra fast generators like the lightning example that was on here a few days ago. The backgrounds don&#x27;t change that much from one prompt to another unless you begin to specify them.</div><br/><div id="39498455" class="c"><input type="checkbox" id="c-39498455" checked=""/><div class="controls bullet"><span class="by">tavavex</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498372">parent</a><span>|</span><a href="#39498030">next</a><span>|</span><label class="collapse" for="c-39498455">[-]</label><label class="expand" for="c-39498455">[1 more]</label></div><br/><div class="children"><div class="content">I mean, object recognition was never an intended way of solving the problem. When you ask for a &quot;frog&quot; image, the model isn&#x27;t just trying to randomly find a frog object, it slowly narrows down data to find an image that&#x27;s classified as &quot;froggy&quot;. Sure, the actual frog will be remembered as an extremely important part of a frog image, but everything else in the image is contributing too. The background could be something that frogs are photographed in, and the lighting approximates what a photograph may look like.<p>I don&#x27;t see why there&#x27;s a need to explained it with this two-tiered approach (and a &quot;data theft&quot; claim) if the explanation of how it works seems applicable to the entire system. Images in the training dataset would almost always reflect the way lighting and objects work in real life, so the model is incentivized to approximate it as closely as possible.</div><br/></div></div></div></div><div id="39498030" class="c"><input type="checkbox" id="c-39498030" checked=""/><div class="controls bullet"><span class="by">qarl</span><span>|</span><a href="#39497310">parent</a><span>|</span><a href="#39498372">prev</a><span>|</span><a href="#39497573">next</a><span>|</span><label class="collapse" for="c-39498030">[-]</label><label class="expand" for="c-39498030">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s unclear what you mean by memorization.  However I can request images for which it&#x27;s obvious that no clear precursor exists, so something original must be being created.<p>Here&#x27;s a Volkswagen Beetle made from brains:<p><a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;Mzd6UWh.png" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;Mzd6UWh.png</a></div><br/><div id="39498057" class="c"><input type="checkbox" id="c-39498057" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498030">parent</a><span>|</span><a href="#39497573">next</a><span>|</span><label class="collapse" for="c-39498057">[-]</label><label class="expand" for="c-39498057">[7 more]</label></div><br/><div class="children"><div class="content">Yes, I wrote that these models can mix learned patterns (which may be memorized).</div><br/><div id="39498367" class="c"><input type="checkbox" id="c-39498367" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498057">parent</a><span>|</span><a href="#39498081">next</a><span>|</span><label class="collapse" for="c-39498367">[-]</label><label class="expand" for="c-39498367">[1 more]</label></div><br/><div class="children"><div class="content">The key isn&#x27;t that they just &quot;mix patterns&quot;. They learn to mix higher order abstractions, which is fundamentally different from &quot;memorization&quot;.</div><br/></div></div><div id="39498081" class="c"><input type="checkbox" id="c-39498081" checked=""/><div class="controls bullet"><span class="by">qarl</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498057">parent</a><span>|</span><a href="#39498367">prev</a><span>|</span><a href="#39498351">next</a><span>|</span><label class="collapse" for="c-39498081">[-]</label><label class="expand" for="c-39498081">[4 more]</label></div><br/><div class="children"><div class="content">Right, but you implied that these were of low quality.  Can you explain why you think the brain Beetle is &quot;not great&quot;?</div><br/><div id="39498182" class="c"><input type="checkbox" id="c-39498182" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498081">parent</a><span>|</span><a href="#39498351">next</a><span>|</span><label class="collapse" for="c-39498182">[-]</label><label class="expand" for="c-39498182">[3 more]</label></div><br/><div class="children"><div class="content">The example in another comment of the horse with an astronaut suit is in a similar spirit as the brain beetle, but was harder to get it to work. In my opinion, ChatGPT seems pretty good at mixing and combining learned patterns, but definitely seems to fail at this with high enough frequency that it&#x27;s limiting. Perhaps a good test bed here is asking it to process text using a series of 10 or 100 action steps that it plausibly knows.</div><br/><div id="39498202" class="c"><input type="checkbox" id="c-39498202" checked=""/><div class="controls bullet"><span class="by">qarl</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498182">parent</a><span>|</span><a href="#39498351">next</a><span>|</span><label class="collapse" for="c-39498202">[-]</label><label class="expand" for="c-39498202">[2 more]</label></div><br/><div class="children"><div class="content">Right.  But as I said in that thread, my very first and only attempt with ChatGPT created the desired image.  Here it is again:<p><a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;6CgVqeL.png" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;6CgVqeL.png</a></div><br/><div id="39498338" class="c"><input type="checkbox" id="c-39498338" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498202">parent</a><span>|</span><a href="#39498351">next</a><span>|</span><label class="collapse" for="c-39498338">[-]</label><label class="expand" for="c-39498338">[1 more]</label></div><br/><div class="children"><div class="content">The nose pokes out past the glass of the helmet.<p>It&#x27;s these <i>basic</i> mistakes that is so incongruous between human and machine intelligence. No matter how big the model, it always makes these same type of basic confabulations, just less often.</div><br/></div></div></div></div></div></div></div></div><div id="39498351" class="c"><input type="checkbox" id="c-39498351" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39498057">parent</a><span>|</span><a href="#39498081">prev</a><span>|</span><a href="#39497573">next</a><span>|</span><label class="collapse" for="c-39498351">[-]</label><label class="expand" for="c-39498351">[1 more]</label></div><br/><div class="children"><div class="content">The mixing is the generalization</div><br/></div></div></div></div></div></div><div id="39497573" class="c"><input type="checkbox" id="c-39497573" checked=""/><div class="controls bullet"><span class="by">ramses0</span><span>|</span><a href="#39497310">parent</a><span>|</span><a href="#39498030">prev</a><span>|</span><a href="#39497598">next</a><span>|</span><label class="collapse" for="c-39497573">[-]</label><label class="expand" for="c-39497573">[4 more]</label></div><br/><div class="children"><div class="content">An interesting example that I came across in my _very_ limited interaction with the image generators was trying to get it to draw &quot;a horse wearing a space suit&quot;.<p>I _wanted_ to see ... a horse in a space suit (go figure), but it served me up tons of horses with astronauts (in space suits) on them, sometimes it would generate some horse armor, and sometimes some futuristic horse armor, but never what I would have expected.<p>I was curious whether it would draw a bubble head with a horse face inside of it, or a shaped horse helmet with a visor or something, but nope. Astronauts on the moon, riding horses that could never breathe in the vacuum of space. Space-cowboy futures denied!<p>...kindof confirming your suspicion that it can&#x27;t &quot;think&quot; about what a space suit for a horse would look like, or generate something that hasn&#x27;t been shown to it before?</div><br/><div id="39497820" class="c"><input type="checkbox" id="c-39497820" checked=""/><div class="controls bullet"><span class="by">nick12r55t1</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497573">parent</a><span>|</span><a href="#39498168">next</a><span>|</span><label class="collapse" for="c-39497820">[-]</label><label class="expand" for="c-39497820">[1 more]</label></div><br/><div class="children"><div class="content">After about 10 minutes of experimenting with Automatic1111 using freedomRedmond (I&#x27;m completely green at this btw) I got this image for ya.. <a href="https:&#x2F;&#x2F;ibb.co&#x2F;Ch73gLm" rel="nofollow">https:&#x2F;&#x2F;ibb.co&#x2F;Ch73gLm</a><p>The prompt was: 
horse fully enclosed in bulky pressure suit with transparent glass helmet for lunar EVA in vacuum, four pressure suit legs, specially shaped helped to fit horse snout. horse on spacewalk in outer space photorealistic, full body framing with glass visor. horse is on a EVA on the lunar surface. horse head must be fully enclosed in glass.<p>It only hits right on maybe 1 in 50, or fewer, generated images from this prompt though.</div><br/></div></div><div id="39498168" class="c"><input type="checkbox" id="c-39498168" checked=""/><div class="controls bullet"><span class="by">qarl</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497573">parent</a><span>|</span><a href="#39497820">prev</a><span>|</span><a href="#39497719">next</a><span>|</span><label class="collapse" for="c-39498168">[-]</label><label class="expand" for="c-39498168">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s is what I get when I type &quot;Can you create an image of a a horse wearing a space suit?&quot; into ChatGPT.  I suspect it is what you were looking for.<p><a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;6CgVqeL.png" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;6CgVqeL.png</a></div><br/></div></div><div id="39497719" class="c"><input type="checkbox" id="c-39497719" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497573">parent</a><span>|</span><a href="#39498168">prev</a><span>|</span><a href="#39497598">next</a><span>|</span><label class="collapse" for="c-39497719">[-]</label><label class="expand" for="c-39497719">[1 more]</label></div><br/><div class="children"><div class="content">&quot;it can&#x27;t think&quot;<p>Perhaps these models are just too small? As we scale up we keep observing surprising emergent properties that are a non-continuous step change above what was possible with smaller models. Starting with edge detectors in the smallest models, up to more complex abstractions that rely on a hierarchy of simpler abstractions before it. From what we&#x27;ve seen from Sora, horse in a space suit should be easily handled with today&#x27;s models. LLMs follow a similar pattern.</div><br/></div></div></div></div><div id="39497598" class="c"><input type="checkbox" id="c-39497598" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#39497310">parent</a><span>|</span><a href="#39497573">prev</a><span>|</span><a href="#39498914">next</a><span>|</span><label class="collapse" for="c-39497598">[-]</label><label class="expand" for="c-39497598">[2 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; This connects to a suspicion I have that a lot of the most impressive results, [...]a are due to memorization.<p>I completely disagree with this. Although you state your case very well.<p>I havn&#x27;t gone through the paper in great detail, but there is a missing distinction in many approaches to demystifying (de-magicifying?) models. Often there is confusion between three levels of algorithm.<p>(1) The low level &quot;blind&quot; training algorithm: Gradient descent, or similar.<p>(2) The class of input-output algorithm implicit in the choice of data, for which the model is being trained: Text continuation prediction, etc.<p>(3) The actual algorithm learned by (1) in order to do (2). I.e. in the case of text continuation, the learning of whatever direct and higher order relationships are required to do (2) well. &quot;Just&quot; learning to predict text continuations, like &quot;just&quot; learning to compress wikipedia, or any other task involving complex data often results in algorithms that are far more complex than their class of problems, like text prediction, implies.<p>Basketball is just about putting a ball in a hoop, according to some constraints. But that is only the &quot;class&quot; of algorithm. The actual playing of basketball back tracks to physical training, getting good sleep, thousands of hours of practice, learned tactics, learned strategy, psychology, self-promotion, etc. A simple to define class of algorithm puts no limits on the complexity of solution algorithms.<p>Point being, the models trained to predict text don&#x27;t &quot;just&quot; predict text. That&#x27;s just the category of algorithms they learn. The complexity, the &quot;intelligence level&quot; of what a text predictor might have to do to, to predict some non-trivial text is unlimited.<p>In this case, the paper emphasizes a correspondence between trained models and simple mappings between examples, i.e. a support vector&#x2F;kernel equivalent interpretation.<p>That may be the case. Assuming that as true (and if so, it is a great insight!), the <i>training</i> of the model still allowed the model to <i>choose</i> the best such representation. The model isn&#x27;t simply composed of training data, plus some parameters per example. It wasn&#x27;t a &quot;support vector machine&quot; design - and it shows because the model contains far fewer parameters than a standard support vector design composed of the training data would produce.<p>Finding an equivalent to a support vector machine that can perform a task, with far fewer parameters is not trivial. It requires some way to sort and sieve through all that data. To identify the ideal or even inferred &quot;examples&quot;, the raw data doesn&#x27;t highlight at all.<p>Neural models made that leap by combining gradient descent, particularly flexible&#x2F;generalizing architectures (matrix, nonlinearities, on upward), masses of raw data, and vast amounts of computing power, to do it.<p>The result may be something that after training looks like a &quot;memorizer&quot;, behaves like it just memorized the &quot;ideal&quot; examples, but it wasn&#x27;t and couldn&#x27;t have been designed by straight memorization. The model had to choose what to virtually &quot;memorize&quot; from the data.<p>The same is no doubt true of our brain. Simple algorithms learning complex things. Complex problems reduced to simple solutions. Neither is just simply &quot;predicting&quot; or &quot;memorizing&quot;.</div><br/><div id="39498025" class="c"><input type="checkbox" id="c-39498025" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#39497310">root</a><span>|</span><a href="#39497598">parent</a><span>|</span><a href="#39498914">next</a><span>|</span><label class="collapse" for="c-39498025">[-]</label><label class="expand" for="c-39498025">[1 more]</label></div><br/><div class="children"><div class="content">Very good point! I do find it incredible that LLMs can do regression with just in-context learning. This is one of my favorite examples of a task that is mind-blowing to a researcher but a meh or huh to the public.<p>The main issue is there isn&#x27;t enough evidence to say one way or another though, I think, which is a complaint of how tasks from modern large models are shared by OpenAI and Google.</div><br/></div></div></div></div></div></div><div id="39498914" class="c"><input type="checkbox" id="c-39498914" checked=""/><div class="controls bullet"><span class="by">Benjaminsen</span><span>|</span><a href="#39497310">prev</a><span>|</span><a href="#39497246">next</a><span>|</span><label class="collapse" for="c-39498914">[-]</label><label class="expand" for="c-39498914">[1 more]</label></div><br/><div class="children"><div class="content">Even if that&#x27;s the case, a sufficiently large hash table is indistinguishable from an intelligence by a lower intelligence.</div><br/></div></div><div id="39497246" class="c"><input type="checkbox" id="c-39497246" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#39498914">prev</a><span>|</span><a href="#39497388">next</a><span>|</span><label class="collapse" for="c-39497246">[-]</label><label class="expand" for="c-39497246">[7 more]</label></div><br/><div class="children"><div class="content">Afaik this was stated in my Intro to ML course. A kernel machine can do anything when the similarity function has infinite dimensions. Similarly, I think they mentioned an infinitely wide MLP is also all you need.<p>Also, this all breaks down when you introduce reinforcement learning methods.</div><br/><div id="39498187" class="c"><input type="checkbox" id="c-39498187" checked=""/><div class="controls bullet"><span class="by">nsenifty</span><span>|</span><a href="#39497246">parent</a><span>|</span><a href="#39497499">next</a><span>|</span><label class="collapse" for="c-39498187">[-]</label><label class="expand" for="c-39498187">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Afaik this was stated in my Intro to ML course.<p>Isbell?</div><br/><div id="39498232" class="c"><input type="checkbox" id="c-39498232" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#39497246">root</a><span>|</span><a href="#39498187">parent</a><span>|</span><a href="#39498588">next</a><span>|</span><label class="collapse" for="c-39498232">[-]</label><label class="expand" for="c-39498232">[1 more]</label></div><br/><div class="children"><div class="content">CS4780 at Cornell. To be clear I took the class, didn&#x27;t teach it.</div><br/></div></div><div id="39498588" class="c"><input type="checkbox" id="c-39498588" checked=""/><div class="controls bullet"><span class="by">xapata</span><span>|</span><a href="#39497246">root</a><span>|</span><a href="#39498187">parent</a><span>|</span><a href="#39498232">prev</a><span>|</span><a href="#39497499">next</a><span>|</span><label class="collapse" for="c-39498588">[-]</label><label class="expand" for="c-39498588">[1 more]</label></div><br/><div class="children"><div class="content">That was my guess, too :-)</div><br/></div></div></div></div><div id="39497499" class="c"><input type="checkbox" id="c-39497499" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39497246">parent</a><span>|</span><a href="#39498187">prev</a><span>|</span><a href="#39497388">next</a><span>|</span><label class="collapse" for="c-39497499">[-]</label><label class="expand" for="c-39497499">[3 more]</label></div><br/><div class="children"><div class="content">What do you mean breaks down?</div><br/><div id="39497721" class="c"><input type="checkbox" id="c-39497721" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#39497246">root</a><span>|</span><a href="#39497499">parent</a><span>|</span><a href="#39497661">next</a><span>|</span><label class="collapse" for="c-39497721">[-]</label><label class="expand" for="c-39497721">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no longer a concept of training examples to be close to, since it&#x27;s just going along the gradient of high reward actions in the RL environment and going away from those with low reward.</div><br/></div></div><div id="39497661" class="c"><input type="checkbox" id="c-39497661" checked=""/><div class="controls bullet"><span class="by">ultra_nick</span><span>|</span><a href="#39497246">root</a><span>|</span><a href="#39497499">parent</a><span>|</span><a href="#39497721">prev</a><span>|</span><a href="#39497388">next</a><span>|</span><label class="collapse" for="c-39497661">[-]</label><label class="expand" for="c-39497661">[1 more]</label></div><br/><div class="children"><div class="content">Breaks.<p>Methods that expect the same input to map to the same output don&#x27;t work with feedback.</div><br/></div></div></div></div></div></div><div id="39497388" class="c"><input type="checkbox" id="c-39497388" checked=""/><div class="controls bullet"><span class="by">quanto</span><span>|</span><a href="#39497246">prev</a><span>|</span><a href="#39498829">next</a><span>|</span><label class="collapse" for="c-39497388">[-]</label><label class="expand" for="c-39497388">[2 more]</label></div><br/><div class="children"><div class="content">This paper was submitted in 2020, and the fact that NN, in the most simple case, is a kernal-learning algo was known even before that.  For those who are more familiar with this subtopic, what contribution does this paper make?</div><br/><div id="39497480" class="c"><input type="checkbox" id="c-39497480" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#39497388">parent</a><span>|</span><a href="#39498829">next</a><span>|</span><label class="collapse" for="c-39497480">[-]</label><label class="expand" for="c-39497480">[1 more]</label></div><br/><div class="children"><div class="content">Discussion at the time <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25314830">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25314830</a></div><br/></div></div></div></div><div id="39498829" class="c"><input type="checkbox" id="c-39498829" checked=""/><div class="controls bullet"><span class="by">rich_sasha</span><span>|</span><a href="#39497388">prev</a><span>|</span><a href="#39497657">next</a><span>|</span><label class="collapse" for="c-39498829">[-]</label><label class="expand" for="c-39498829">[1 more]</label></div><br/><div class="children"><div class="content">I suppose every single ML method, ever, is effectively memorisation. The crux is the feature representation is builds to aid the memorisation.<p>So take OLS, or a decision tree &#x2F; forest. When you evaluate them, they look at some feature space, then <i>compare to some fitted parameters</i> (i.e. memorises result) and produce an output.<p>Methods such as SVM or Lasso are also memorisation, except with a clever feature mapping. Instead of memorising outputs for every sample, the memories them by some learned feature sunset or transformation.<p>Perhaps the same thing happens for LLMs or NNs (I&#x27;m still reading the paper) but if so, this is just the name of the game in ML, it seems.<p>Arguably that&#x27;s what we humans do too. We are capable of creative thinking, whatever that means, but 90% of our thought process seems to be cache recall, and intelligence seems to correspond well to having a large and diverse cache. Many disciplines that require creative thinking to understand, like maths or playing instruments, seem to improve on repetitive practice that makes us memorize patterns.</div><br/></div></div><div id="39497657" class="c"><input type="checkbox" id="c-39497657" checked=""/><div class="controls bullet"><span class="by">Straw</span><span>|</span><a href="#39498829">prev</a><span>|</span><a href="#39497444">next</a><span>|</span><label class="collapse" for="c-39497657">[-]</label><label class="expand" for="c-39497657">[1 more]</label></div><br/><div class="children"><div class="content">This keeps popping up but while technically true, its essentially nonsense- normally when people talk about kernel machines, the kernel doesn&#x27;t depend on the data, or at least not very much- we might use a Gaussian kernel and tune the radius or even the covariance.<p>This construction has a kernel which depends on the entire training trajectory of the neural network! So its completely unclear what&#x27;s happening, all of the interesting parts may have just moved into the kernel. So basically this tells us nothing- we can&#x27;t just add a new data point as in a kernel method, incorporating it just by adding its interaction- every new data point changes the whole training trajectory so could completely change the resulting kernel.</div><br/></div></div><div id="39497444" class="c"><input type="checkbox" id="c-39497444" checked=""/><div class="controls bullet"><span class="by">tho2iu34234</span><span>|</span><a href="#39497657">prev</a><span>|</span><a href="#39498625">next</a><span>|</span><label class="collapse" for="c-39497444">[-]</label><label class="expand" for="c-39497444">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Remark-1&quot; says it actually is not.<p>Surprised Pedro is pushing <i>this</i> as a Kernel Machine (the need for constant coefficients is a key requirement).</div><br/></div></div><div id="39498625" class="c"><input type="checkbox" id="c-39498625" checked=""/><div class="controls bullet"><span class="by">latenightcoding</span><span>|</span><a href="#39497444">prev</a><span>|</span><a href="#39497375">next</a><span>|</span><label class="collapse" for="c-39498625">[-]</label><label class="expand" for="c-39498625">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen this shared here and there, but it&#x27;s not original research, Pedro Domingos doesn&#x27;t do much of that anymore, mostly spends his time trolling on twitter an calling everybody &quot;woke&quot;</div><br/><div id="39498647" class="c"><input type="checkbox" id="c-39498647" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39498625">parent</a><span>|</span><a href="#39497375">next</a><span>|</span><label class="collapse" for="c-39498647">[-]</label><label class="expand" for="c-39498647">[1 more]</label></div><br/><div class="children"><div class="content">I did not realize he was retired.</div><br/></div></div></div></div><div id="39497375" class="c"><input type="checkbox" id="c-39497375" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#39498625">prev</a><span>|</span><a href="#39497197">next</a><span>|</span><label class="collapse" for="c-39497375">[-]</label><label class="expand" for="c-39497375">[1 more]</label></div><br/><div class="children"><div class="content">(2020)</div><br/></div></div><div id="39497197" class="c"><input type="checkbox" id="c-39497197" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#39497375">prev</a><span>|</span><a href="#39498339">next</a><span>|</span><label class="collapse" for="c-39497197">[-]</label><label class="expand" for="c-39497197">[4 more]</label></div><br/><div class="children"><div class="content">Thanks for the submission. I keep an eye out for connections to potentially cheaper and simpler methods. The new comparison reminds me of this old paper which compared them to Gaussian networks:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1711.00165" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1711.00165</a><p>For causality, I&#x27;m also keeping an eye out for ways to tie DNN research back into decision trees, esp probabilistic and nested. Or fuzzy logic. Here&#x27;s an example I saw moving probabilistic methods into decision trees:<p><a href="http:&#x2F;&#x2F;www.gatsby.ucl.ac.uk&#x2F;~balaji&#x2F;balaji-phd-thesis.pdf" rel="nofollow">http:&#x2F;&#x2F;www.gatsby.ucl.ac.uk&#x2F;~balaji&#x2F;balaji-phd-thesis.pdf</a><p>Many of these sub-fields develop separately in their own ways. Gotta wonder what recent innovations in one could be ported to another.</div><br/><div id="39497485" class="c"><input type="checkbox" id="c-39497485" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#39497197">parent</a><span>|</span><a href="#39497391">next</a><span>|</span><label class="collapse" for="c-39497485">[-]</label><label class="expand" for="c-39497485">[2 more]</label></div><br/><div class="children"><div class="content">Does a given model converge after Gaussian blurring? What does it do in the presence of noise, given the curse of dimensionality?<p>OpenCog integrates PLN and MOSES (~2005).<p>&quot;Interpretable Model-Based Hierarchical RL Using Inductive Logic Programming&quot; (2021) <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37463686">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37463686</a> :<p>&gt; <i><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Probabilistic_logic_network" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Probabilistic_logic_network</a> :</i><p>&gt;&gt; <i>The basic goal of PLN is to provide reasonably accurate probabilistic inference in a way that is compatible with both term logic and predicate logic, and scales up to operate in real time on large dynamic knowledge bases</i><p>Asmoses updates an in-RAM (*) online hypergraph with the graph relations it learns.<p>CuPy wraps CuDNN.<p>Re: <i>quantum</i> logic and quantum causal inference: 
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38721246">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38721246</a><p>From <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39255303">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39255303</a> :<p>&gt; <i>Is Quantum Logic the correct propositional logic? Is Quantum Logic a sufficient logic for all things?</i><p>A quantum ML task: Find all local and nonlocal state linkages within the presented observations<p>And then also do universal function approximation</div><br/><div id="39497800" class="c"><input type="checkbox" id="c-39497800" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#39497197">root</a><span>|</span><a href="#39497485">parent</a><span>|</span><a href="#39497391">next</a><span>|</span><label class="collapse" for="c-39497800">[-]</label><label class="expand" for="c-39497800">[1 more]</label></div><br/><div class="children"><div class="content">But also biological neurological systems;<p>Coping strategies: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Coping" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Coping</a><p>Defense Mechanisms &gt; Vaillant&#x27;s categorization &gt; Level 4: mature: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Defence_mechanism#Level_4:_mature" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Defence_mechanism#Level_4:_mat...</a><p>&quot;From Comfort Zone to Performance Management&quot; suggests that the Carnall coping cycle coincides with the TPR curve (Transforming, Performing, Reforming, adjourning); that coping with change in systems is linked with performance.<p>And Consensus; social with nonlinear feedback and technological.<p>What are the systems thinking advantages in such fields of study?<p>Systems theory &gt; See also &gt; Glossary,: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Systems_theory#See_also" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Systems_theory#See_also</a></div><br/></div></div></div></div><div id="39497391" class="c"><input type="checkbox" id="c-39497391" checked=""/><div class="controls bullet"><span class="by">yazzku</span><span>|</span><a href="#39497197">parent</a><span>|</span><a href="#39497485">prev</a><span>|</span><a href="#39498339">next</a><span>|</span><label class="collapse" for="c-39497391">[-]</label><label class="expand" for="c-39497391">[1 more]</label></div><br/><div class="children"><div class="content">This seems like the key research to me if we want any shot at preventing the technology from being locked away behind big corp API walls interfacing giant data centers. Anything that removes the bloat and mysticism from the models so they can be scaled down and run on the little guy&#x27;s computer is orders of magnitude more progress in my opinion than, e.g., increasing the token window by some epsilon.</div><br/></div></div></div></div></div></div></div></div></div></body></html>