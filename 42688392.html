<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737104455656" as="style"/><link rel="stylesheet" href="styles.css?v=1737104455656"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2501.00663">Titans: Learning to Memorize at Test Time</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>birriel</span> | <span>5 comments</span></div><br/><div><div id="42689038" class="c"><input type="checkbox" id="c-42689038" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#42733485">next</a><span>|</span><label class="collapse" for="c-42689038">[-]</label><label class="expand" for="c-42689038">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. I like the idea of a meta-mechanism that learns to update an associative memory based on how surprising the data is. The other stuff, reading memory via keys and values and selectively erasing it with gating, look pretty conventional on a first glance. Thank you for sharing this on HN. I&#x27;ve added it to my reading list.<p>EDIT: I&#x27;m reminded of this other type of associative memory: <a href="https:&#x2F;&#x2F;github.com&#x2F;glassroom&#x2F;heinsen_routing">https:&#x2F;&#x2F;github.com&#x2F;glassroom&#x2F;heinsen_routing</a>. The idea there is to compute a mixture of memories that best predicts the given input sequence. Quite frankly, I don&#x27;t remember how the whole thing works, but I do remember that it works. It&#x27;s been a while since I used it, so YMMV. In any case, it may be of interest to you.</div><br/><div id="42733912" class="c"><input type="checkbox" id="c-42733912" checked=""/><div class="controls bullet"><span class="by">testfoo11111111</span><span>|</span><a href="#42689038">parent</a><span>|</span><a href="#42733485">next</a><span>|</span><label class="collapse" for="c-42733912">[-]</label><label class="expand" for="c-42733912">[2 more]</label></div><br/><div class="children"><div class="content">there&#x27;s nothing &quot;pretty conventional&quot; about a neural memory mechanism that comes along with such solid evidence of scalability and appealing performance characteristics.<p>If neural memory was conventional, GPT4o&#x27;s memory wouldn&#x27;t be stored as plain text and prepended to prompts.<p>This paper reminds me of the Switch Transformer paper; e.g. solidifying, expanding on, and proving out an area of research that may well have a big impact on leading LLMs and the SOTA in AI.<p>Agreed the concept of surprise is very cool.</div><br/><div id="42734302" class="c"><input type="checkbox" id="c-42734302" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#42689038">root</a><span>|</span><a href="#42733912">parent</a><span>|</span><a href="#42733485">next</a><span>|</span><label class="collapse" for="c-42734302">[-]</label><label class="expand" for="c-42734302">[1 more]</label></div><br/><div class="children"><div class="content">There definitely is precedent - any  parallelizably-decodable CABAC-derived neural compression algorithm basically has a flavor of this idea at its heart - intersperse statistical state throughout your token stream so you can decouple novelty in your state space on the fly.<p>Taken to its extreme where the ‘memory’ is descriptive enough to deterministically control the decoding you get parallelism over the sequence for free as a consequence of the associativity.<p>Similar techniques are used in making video compression algorithms robust enough for low latency reconnection in online streaming in poor&#x2F;changing network conditions, or making it possible to decompress JPEGs at &gt;1GBps in parallel by exploiting the presence of ‘RESET’ tokens that indicate independent&#x2F;novel substreams.<p>That said, I do agree that this is definitely a great paper and contribution to language models though!</div><br/></div></div></div></div></div></div><div id="42733485" class="c"><input type="checkbox" id="c-42733485" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#42689038">prev</a><span>|</span><label class="collapse" for="c-42733485">[-]</label><label class="expand" for="c-42733485">[1 more]</label></div><br/><div class="children"><div class="content">Duplicate: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42718166">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42718166</a></div><br/></div></div></div></div></div></div></div></body></html>