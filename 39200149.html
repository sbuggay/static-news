<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706691660760" as="style"/><link rel="stylesheet" href="styles.css?v=1706691660760"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://moveit.substack.com/p/why-custom-gpts-are-better-than-plugins">Why Custom GPTs are better than plugins</a> <span class="domain">(<a href="https://moveit.substack.com">moveit.substack.com</a>)</span></div><div class="subtext"><span>cheerioty</span> | <span>15 comments</span></div><br/><div><div id="39200872" class="c"><input type="checkbox" id="c-39200872" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#39201567">next</a><span>|</span><label class="collapse" for="c-39200872">[-]</label><label class="expand" for="c-39200872">[2 more]</label></div><br/><div class="children"><div class="content">In my experience they are equally unusable due to compounding reliability issues.<p>It’s a tech demo, not a platform, a data acquisition operation and alpha testing rather than anything seriously useful.<p>Interfaces are unstable. 
Inference is intentionally non deterministic and control is crippled (e.g by choosing not to expose seed, denoising or image to image on dalle), to avoid PR backlash whole simultaneously hiding the true capabilities of the platform.<p>something as simple as using VITS to analyze an image fails 30% of the time because GPT5 decides it doesn’t have vision, wants to use pytesseract instead or writes hallucinatory pytorch code for a non existent vits library instead of just using inference.<p>One may create prompts that temporarily don’t fail at a high rate but constant silent finetuning, system prompt changes and unstable models &#x2F; APIs make the whole thing a tech demo designed to get users to volunteer future training data</div><br/><div id="39201568" class="c"><input type="checkbox" id="c-39201568" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39200872">parent</a><span>|</span><a href="#39201567">next</a><span>|</span><label class="collapse" for="c-39201568">[-]</label><label class="expand" for="c-39201568">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, it&#x27;s pretty exciting. I never thought this would be possible in my lifetime and it&#x27;s actually accelerating so quickly it&#x27;s hard to target.</div><br/></div></div></div></div><div id="39201567" class="c"><input type="checkbox" id="c-39201567" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#39200872">prev</a><span>|</span><a href="#39201423">next</a><span>|</span><label class="collapse" for="c-39201567">[-]</label><label class="expand" for="c-39201567">[1 more]</label></div><br/><div class="children"><div class="content">I am very surprised that many here don&#x27;t understand the great value of custom GPTs: They give you access to online APIs combined with pre-configured custom prompts!<p>You can query databases, trigger events and handle the results smartly.<p>Now that OpenAi added the @ sign to talk to your preselected custom GPTs you can just use different APIs like slack colleges:<p>@downloader get the data from test.tsv  
@sql create table according to tsv header  
@sql insert data  
@admin open mysql port<p>One thing to keep in mind is that even though custom gpts have access to a local sandbox file system, passing data around almost always involves GPT handling the data which becomes forbidding for any large token stream.<p>Also it can&#x27;t display any images other than dalle fantasies or pyplots which is a slightly annoying limitation, but familiar to users of other shells like bash.<p>One critique that I share is the stupid branding &quot;costume GPTs&quot; and lack of discoverability: If you search the GPT store for wolfr you do not get wolfram alpha as completion! It only appears when you type 
wolfra</div><br/></div></div><div id="39201423" class="c"><input type="checkbox" id="c-39201423" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39201567">prev</a><span>|</span><a href="#39200569">next</a><span>|</span><label class="collapse" for="c-39201423">[-]</label><label class="expand" for="c-39201423">[1 more]</label></div><br/><div class="children"><div class="content">Custom GPTs <i>are</i> plugins, just more streamlined. It&#x27;s still just a carefully written system prompt + some basic middleware scanning the output and occasionally taking over.<p>The UX may be different, sure, but there&#x27;s no <i>technical</i> difference and no <i>technical</i> innovation here. The main value of both plugins and custom &quot;GPTs&quot;[0] is that they&#x27;re <i>first party</i>. You can build or buy better implementations, but it won&#x27;t be <i>the</i> &quot;GPTs&quot;.<p>--<p>[0] - Kudos for whoever at OpenAI that approved calling those &quot;GPTs&quot;, for selecting a term that maximizes confusion not just about their offering, but screws with people&#x27;s comprehension of LLMs in general.</div><br/></div></div><div id="39200569" class="c"><input type="checkbox" id="c-39200569" checked=""/><div class="controls bullet"><span class="by">addminztrator</span><span>|</span><a href="#39201423">prev</a><span>|</span><a href="#39200834">next</a><span>|</span><label class="collapse" for="c-39200569">[-]</label><label class="expand" for="c-39200569">[2 more]</label></div><br/><div class="children"><div class="content">I mainly need one reason: Microsoft doesn&#x27;t hoard all your data</div><br/><div id="39201094" class="c"><input type="checkbox" id="c-39201094" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#39200569">parent</a><span>|</span><a href="#39200834">next</a><span>|</span><label class="collapse" for="c-39201094">[-]</label><label class="expand" for="c-39201094">[1 more]</label></div><br/><div class="children"><div class="content">That is sold in form of the &quot;Team&quot; plan</div><br/></div></div></div></div><div id="39200834" class="c"><input type="checkbox" id="c-39200834" checked=""/><div class="controls bullet"><span class="by">dylanjcastillo</span><span>|</span><a href="#39200569">prev</a><span>|</span><a href="#39200613">next</a><span>|</span><label class="collapse" for="c-39200834">[-]</label><label class="expand" for="c-39200834">[1 more]</label></div><br/><div class="children"><div class="content">I feel the key question is not that if they’re better, but if people are using them more and more often than plugins.<p>And in particular, if they’re using the ones that aren’t just a custom system prompt. Because I really doubt there’s any big business in commercializing system prompts.<p>My hunch right now is that GPTs have made it clear that OpenAI should let user save multiple system prompts, but that there’s no real defensible business in distributing GPTs, as a chat interface is not that good for most purposes.</div><br/></div></div><div id="39200613" class="c"><input type="checkbox" id="c-39200613" checked=""/><div class="controls bullet"><span class="by">kylebenzle</span><span>|</span><a href="#39200834">prev</a><span>|</span><label class="collapse" for="c-39200613">[-]</label><label class="expand" for="c-39200613">[7 more]</label></div><br/><div class="children"><div class="content">Reading this article brought me no closer to understanding why people use these. From day one you could give any LLM any context you wanted, that&#x27;s the whole point after all.<p>The actual next stage of LLM development will be giving the user the ability to select&#x2F;deselect what training data to include otherwise there is a limit of at most a few pages of context you can provide.<p>Custom GPTs are trying to pretend thats what they are doing but it&#x27;s not going to work.<p>Like you can&#x27;t upload a novel and say, &quot;speak to me as if you are this character&quot; because the LLM can&#x27;t ignore it&#x27;s training data entirely and the context you give it gets drowned out quickly.</div><br/><div id="39201085" class="c"><input type="checkbox" id="c-39201085" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39200613">parent</a><span>|</span><a href="#39201118">next</a><span>|</span><label class="collapse" for="c-39201085">[-]</label><label class="expand" for="c-39201085">[1 more]</label></div><br/><div class="children"><div class="content">My problem with custom GPTs is that it&#x27;s still building off of the chatbot fine tuned version of GPT-4 which incorporates a hardcoded system message in between yours and the model as well as reflects a very Goodhart&#x27;s Law driven alignment.<p>For example, it&#x27;s next to worthless for creative writing tasks - but it doesn&#x27;t <i>need</i> to be.<p>Here is an example of a response to requesting chat suggestions as absurd and bizarre as possible from the current model:<p>&gt; If you had to choose between eating a live octopus or a dead rat, which one would you pick and why?<p>It&#x27;s stochastic so there&#x27;s a variety but they are generally pretty dry and often information based (explain gravity to flat earthers, describe Earth culture to aliens, etc).<p>Here was one of the generations from the pre-release chat model integrated into the closed beta for Bing:<p>&gt; Have you ever danced with a penguin under the moonlight?<p>I know which one of these two snapshots I&#x27;d want to build off of for any kind of creative GPTs, and it&#x27;s not the one available to power GPTs.<p>The industry needs SotA competition in alignment strategies and goals <i>badly</i> if we want this tech successful outside of a narrow scope of STEM applications, and the reliance on GPT-4 synthetic data to train its competition isn&#x27;t helping.</div><br/></div></div><div id="39201118" class="c"><input type="checkbox" id="c-39201118" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39200613">parent</a><span>|</span><a href="#39201085">prev</a><span>|</span><a href="#39200754">next</a><span>|</span><label class="collapse" for="c-39201118">[-]</label><label class="expand" for="c-39201118">[1 more]</label></div><br/><div class="children"><div class="content">&gt; From day one you could give any LLM any context you wanted, that&#x27;s the whole point after all.<p>Not in the case of the web interface to ChatGPT and nontechies who don&#x27;t want to run their own model and fuddle with the system prompt.<p>That&#x27;s the target market for the GPT Store, but OpenAI is doing an utterly terrible job of marketing it to them.</div><br/></div></div><div id="39200754" class="c"><input type="checkbox" id="c-39200754" checked=""/><div class="controls bullet"><span class="by">mvkel</span><span>|</span><a href="#39200613">parent</a><span>|</span><a href="#39201118">prev</a><span>|</span><a href="#39200948">next</a><span>|</span><label class="collapse" for="c-39200754">[-]</label><label class="expand" for="c-39200754">[2 more]</label></div><br/><div class="children"><div class="content">Agreed. Custom gpts, like plugins, feel like a complete distraction to OpenAI</div><br/><div id="39201263" class="c"><input type="checkbox" id="c-39201263" checked=""/><div class="controls bullet"><span class="by">joedevon</span><span>|</span><a href="#39200613">root</a><span>|</span><a href="#39200754">parent</a><span>|</span><a href="#39200948">next</a><span>|</span><label class="collapse" for="c-39201263">[-]</label><label class="expand" for="c-39201263">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes I want browsing. Sometimes I want no browsing. Sometimes I want to talk to a marketer. Sometimes to my personal advisor. Sometimes to a python coder. Sometimes to an ML SME. I can quickly change context with a simple @ and select the right context from a dropdown. It&#x27;s a super fast way to switch the common contexts I use with a LOT less typing.</div><br/></div></div></div></div><div id="39200948" class="c"><input type="checkbox" id="c-39200948" checked=""/><div class="controls bullet"><span class="by">hayksaakian</span><span>|</span><a href="#39200613">parent</a><span>|</span><a href="#39200754">prev</a><span>|</span><a href="#39200808">next</a><span>|</span><label class="collapse" for="c-39200948">[-]</label><label class="expand" for="c-39200948">[1 more]</label></div><br/><div class="children"><div class="content">I think the minimum value is comparable to a desktop shortcut</div><br/></div></div><div id="39200808" class="c"><input type="checkbox" id="c-39200808" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#39200613">parent</a><span>|</span><a href="#39200948">prev</a><span>|</span><label class="collapse" for="c-39200808">[-]</label><label class="expand" for="c-39200808">[1 more]</label></div><br/><div class="children"><div class="content">&gt; From day one you could give any LLM any context you wanted<p>Yes, but copying it over yourself is inconvenient.</div><br/></div></div></div></div></div></div></div></div></div></body></html>