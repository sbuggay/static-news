<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735635663378" as="style"/><link rel="stylesheet" href="styles.css?v=1735635663378"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://bellard.org/ts_zip/">Ts_zip: Text Compression Using Large Language Models</a> <span class="domain">(<a href="https://bellard.org">bellard.org</a>)</span></div><div class="subtext"><span>signa11</span> | <span>59 comments</span></div><br/><div><div id="42552271" class="c"><input type="checkbox" id="c-42552271" checked=""/><div class="controls bullet"><span class="by">0x0</span><span>|</span><a href="#42553209">next</a><span>|</span><label class="collapse" for="c-42552271">[-]</label><label class="expand" for="c-42552271">[21 more]</label></div><br/><div class="children"><div class="content">This is particularly interesting as there seems to be, for decades, a general consensus that the problem of text compression is the same as the problem of artificial intelligence, for example <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hutter_Prize" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hutter_Prize</a></div><br/><div id="42557394" class="c"><input type="checkbox" id="c-42557394" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#42552271">parent</a><span>|</span><a href="#42556081">next</a><span>|</span><label class="collapse" for="c-42557394">[-]</label><label class="expand" for="c-42557394">[1 more]</label></div><br/><div class="children"><div class="content">it is true, but i think it&#x27;s only of philosophical interests. for example, in a sense our physical laws are just human&#x27;s attempt at compressing our universe.<p>the text model used here probably isn&#x27;t going to be &quot;intelligent&quot; the same way those chat-oriented LLMs are. you can probably still sample text from it, but you can actually do the same with gzip[1].<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;Futrell&#x2F;ziplm">https:&#x2F;&#x2F;github.com&#x2F;Futrell&#x2F;ziplm</a></div><br/></div></div><div id="42556081" class="c"><input type="checkbox" id="c-42556081" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#42552271">parent</a><span>|</span><a href="#42557394">prev</a><span>|</span><a href="#42555028">next</a><span>|</span><label class="collapse" for="c-42556081">[-]</label><label class="expand" for="c-42556081">[2 more]</label></div><br/><div class="children"><div class="content">&quot;It is well established that compression is essentially prediction, which effectively links compression and langauge models (Delétang et al., 2023). The source coding theory from Shannon’s information theory (Shannon, 1948) suggests that the number of bits required by an optimal entropy encoder to compress a message ... is equal to the NLL of the message given by a statistical model.&quot; (<a href="https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;&#x2F;2402.00861" rel="nofollow">https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;&#x2F;2402.00861</a>)<p>I will say again that Li et al 2024, &quot;Evaluating Large Language Models for Generalization and Robustness via Data Compression&quot;, which evaluates LLMs on their ability to predict future text, is amazing work that the field is currently sleeping on.</div><br/><div id="42556166" class="c"><input type="checkbox" id="c-42556166" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42556081">parent</a><span>|</span><a href="#42555028">next</a><span>|</span><label class="collapse" for="c-42556166">[-]</label><label class="expand" for="c-42556166">[1 more]</label></div><br/><div class="children"><div class="content">I’m not sure how this generalises to grammar based compression such as SEQUITUR for example is… incidentally LZW also is though not advertised as such.<p>Devising the minimal grammar that generates the text is  NP-hard (<a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Smallest_grammar_problem" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Smallest_grammar_problem</a>)<p>Math seems very limited when it comes to reasoning about generative grammars and their unfolding into text. Should the apparatus been there we’d probably had grammar&#x2F;prolog  based AI long ago…</div><br/></div></div></div></div><div id="42555028" class="c"><input type="checkbox" id="c-42555028" checked=""/><div class="controls bullet"><span class="by">retrac</span><span>|</span><a href="#42552271">parent</a><span>|</span><a href="#42556081">prev</a><span>|</span><a href="#42555083">next</a><span>|</span><label class="collapse" for="c-42555028">[-]</label><label class="expand" for="c-42555028">[15 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a general consensus that entropy is deeply spooky.  It pops up in physics in black holes and the heat death of the universe.  The physicist Erwin Schrodinger suggested that life itself consumes negative entropy, and others have proposed other definitions of life that are entropic.  Some definitions of intelligence also centre on entropy.<p>What to make of all that however, has anything but consensus.</div><br/><div id="42555215" class="c"><input type="checkbox" id="c-42555215" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42555028">parent</a><span>|</span><a href="#42556700">next</a><span>|</span><label class="collapse" for="c-42555215">[-]</label><label class="expand" for="c-42555215">[13 more]</label></div><br/><div class="children"><div class="content">This is all weasel words, and you&#x27;ve misspelled &quot;Schroedinger&quot;&#x2F;&quot;Schrödinger&quot;. That sort of comment might be fine for the pub, but on the internet you don&#x27;t have to say anything and if you do it may as well have some substance.<p>Entropy doesn&#x27;t just &quot;pop up&quot; with black holes. They have thermodynamic descriptions like any other physical body, and it happens to be unusual and amazing, like most of the edge cases in general relativity. You&#x27;ve probably heard of the Bekenstein-Hawking result that the entropy is proportional to the event horizon area. That&#x27;s a cool result, if you know how the entropy is defined, and what the event horizon area is, but you can&#x27;t get that from a quip. For the necessary background you&#x27;ll have to do something like reading <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Black_hole_thermodynamics" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Black_hole_thermodynamics</a> (and references therein) or even better get a textbook like Carroll&#x27;s or MTW.<p>For &quot;consuming negative entropy&quot; I recommend going to the source: <a href="https:&#x2F;&#x2F;archive.org&#x2F;details&#x2F;whatislifeothers00schr&#x2F;" rel="nofollow">https:&#x2F;&#x2F;archive.org&#x2F;details&#x2F;whatislifeothers00schr&#x2F;</a> which collects public lectures Schrödinger gave at the time.</div><br/><div id="42556262" class="c"><input type="checkbox" id="c-42556262" checked=""/><div class="controls bullet"><span class="by">endofreach</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42555215">parent</a><span>|</span><a href="#42555389">next</a><span>|</span><label class="collapse" for="c-42556262">[-]</label><label class="expand" for="c-42556262">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but on the internet you don&#x27;t have to say anything and if you do it may as well have some substance<p>Seems like we&#x27;re using different internets. Which i am glad about. I just wish mine had less of the negativity that&#x27;s coming over from yours. Guess in the end, the people on your internet realize, it&#x27;s more fun over here.<p>You could have expressed all of that with less maliciousness towards the person. Thank god, in my internet everyone can say whatever they want f they want. Because– and more people should remember this apparently– if i don&#x27;t like it, i just turn off the internet, like grandma!<p>Wish all the best to you and everyone you care about in real life. I might be just a bot. You might be. We&#x27;ll never know for certain. Don&#x27;t let some bits mess with your feels.</div><br/><div id="42557316" class="c"><input type="checkbox" id="c-42557316" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42556262">parent</a><span>|</span><a href="#42555389">next</a><span>|</span><label class="collapse" for="c-42557316">[-]</label><label class="expand" for="c-42557316">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry for for leaking negativity into your internet. I don&#x27;t think negativity is inherently undesirable, but I don&#x27;t think it&#x27;s useful to express it towards people&#x27;s selves. I meant only to criticize the comment without further implication.<p>In fact I went and got some references I really liked because I was hoping to add what I felt was missing from the discussion on entropy. My motivation in the end was to share my personal feeling of awe, and in a way that was accessible to the parent poster as well as other readers. How do you like that internet?</div><br/></div></div></div></div><div id="42555389" class="c"><input type="checkbox" id="c-42555389" checked=""/><div class="controls bullet"><span class="by">retrac</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42555215">parent</a><span>|</span><a href="#42556262">prev</a><span>|</span><a href="#42556700">next</a><span>|</span><label class="collapse" for="c-42555389">[-]</label><label class="expand" for="c-42555389">[10 more]</label></div><br/><div class="children"><div class="content">I was trying to convey a subjective and emotional experience.  Obviously I failed.<p>I hope that when you try to express awe it isn&#x27;t dismissed as weasel words.<p>I give up.  Delete my account please dang.  This site isn&#x27;t good for my mental health.</div><br/><div id="42555787" class="c"><input type="checkbox" id="c-42555787" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42555389">parent</a><span>|</span><a href="#42555515">next</a><span>|</span><label class="collapse" for="c-42555787">[-]</label><label class="expand" for="c-42555787">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I give up. Delete my account please dang. This site isn&#x27;t good for my mental health.<p>While I cannot speak to your conclusion, I can humbly suggest to not put any credence in what some rando says on the Internet.  Including myself. :-)<p><pre><code>  Far better is it to dare mighty things, to win glorious 
  triumphs, even though checkered by failure... than to rank 
  with those poor spirits who neither enjoy nor suffer much, 
  because they live in a gray twilight that knows not victory 
  nor defeat.[0]
</code></pre>
0 - <a href="https:&#x2F;&#x2F;www.brainyquote.com&#x2F;quotes&#x2F;theodore_roosevelt_103499" rel="nofollow">https:&#x2F;&#x2F;www.brainyquote.com&#x2F;quotes&#x2F;theodore_roosevelt_103499</a></div><br/></div></div><div id="42555515" class="c"><input type="checkbox" id="c-42555515" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42555389">parent</a><span>|</span><a href="#42555787">prev</a><span>|</span><a href="#42556748">next</a><span>|</span><label class="collapse" for="c-42555515">[-]</label><label class="expand" for="c-42555515">[7 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t like your comment, that&#x27;s all. I&#x27;m just one anonymous asshole, I can&#x27;t invalidate your sense of awe.<p>FWIW, I didn&#x27;t want or expect to harm your mental health.</div><br/><div id="42556120" class="c"><input type="checkbox" id="c-42556120" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42555515">parent</a><span>|</span><a href="#42556748">next</a><span>|</span><label class="collapse" for="c-42556120">[-]</label><label class="expand" for="c-42556120">[6 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; This is all weasel words, and you&#x27;ve misspelled &quot;Schroedinger&quot;&#x2F;&quot;Schrödinger&quot;. That sort of comment might be fine for the pub, but on the internet you don&#x27;t have to say anything and if you do it may as well have some substance.<p>&gt; ... I can&#x27;t invalidate your sense of awe.<p>Actually, yes.  Yes, you can.<p>And so could I, or anyone really, given sufficiently focused vitriol.<p>For example, your sentence fragment &quot;This is all weasel words&quot; is incorrect English.  &quot;This is&quot; should use the plural form &quot;These are&quot; as the subject is &quot;words&quot; and not &quot;weasel&quot;, as well as the modifier &quot;all&quot; emphasizing plurality.<p>The irony of your subsequently pointing out a spelling error and then chastising the OP for same has not been lost.</div><br/><div id="42557277" class="c"><input type="checkbox" id="c-42557277" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42556120">parent</a><span>|</span><a href="#42556255">next</a><span>|</span><label class="collapse" for="c-42557277">[-]</label><label class="expand" for="c-42557277">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;This [comment] is all weasel words.&quot;<p>The subject was &quot;this&quot;, referring to the comment.<p>By what standard of English did you reckon my post incorrect? I appreciate your effort to cheer up your parent post, and to improve my language skills, of course.<p>(I&#x27;m not the language usage police, though I am fussy about correctly rendering people&#x27;s names.)<p>I didn&#x27;t understand your gainsaying about invalidating awe. Whether or not the poster&#x27;s awe was a real and worthwhile feeling seems to me entirely independent of my opinions.<p>I find your aims admirable. However, I regret to say that for me the irony, and purpose of this comment thread, have indeed been lost.</div><br/></div></div><div id="42556255" class="c"><input type="checkbox" id="c-42556255" checked=""/><div class="controls bullet"><span class="by">WalterBright</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42556120">parent</a><span>|</span><a href="#42557277">prev</a><span>|</span><a href="#42556748">next</a><span>|</span><label class="collapse" for="c-42556255">[-]</label><label class="expand" for="c-42556255">[4 more]</label></div><br/><div class="children"><div class="content">At least 50% of posts that point out a spelling or grammatical error contain one as well.</div><br/><div id="42557326" class="c"><input type="checkbox" id="c-42557326" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42556255">parent</a><span>|</span><a href="#42556350">next</a><span>|</span><label class="collapse" for="c-42557326">[-]</label><label class="expand" for="c-42557326">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the unconditional rate of errors in posts generally? Without the prior I don&#x27;t know if whinginging about spelling or grammar makes my posts correcter or incorrecter.</div><br/></div></div><div id="42556350" class="c"><input type="checkbox" id="c-42556350" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42556255">parent</a><span>|</span><a href="#42557326">prev</a><span>|</span><a href="#42556748">next</a><span>|</span><label class="collapse" for="c-42556350">[-]</label><label class="expand" for="c-42556350">[2 more]</label></div><br/><div class="children"><div class="content">&gt; At least 50% of posts that point out a spelling or grammatical error contain one as well.<p>Quite true.  While I do not generally claim to be a grammatical wizard, I do know when I hear from one (hello Zortech-C++, it&#x27;s been too long!).<p>If you don&#x27;t mind pointing out my mistake(s) above, I would appreciate it as my goal was to exemplify the social effect of pedantic critique.  Being corrected when doing same could serve as an additional benefit.</div><br/><div id="42556707" class="c"><input type="checkbox" id="c-42556707" checked=""/><div class="controls bullet"><span class="by">WalterBright</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42556350">parent</a><span>|</span><a href="#42556748">next</a><span>|</span><label class="collapse" for="c-42556707">[-]</label><label class="expand" for="c-42556707">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s nice to hear from a ZTC++ user!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42556748" class="c"><input type="checkbox" id="c-42556748" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42555389">parent</a><span>|</span><a href="#42555515">prev</a><span>|</span><a href="#42556700">next</a><span>|</span><label class="collapse" for="c-42556748">[-]</label><label class="expand" for="c-42556748">[1 more]</label></div><br/><div class="children"><div class="content">You wouldn&#x27;t toss out your radio because it picks up a bit of static now and then, would you?  That&#x27;s all that posts like that one amount to... static.</div><br/></div></div></div></div></div></div><div id="42556700" class="c"><input type="checkbox" id="c-42556700" checked=""/><div class="controls bullet"><span class="by">pstuart</span><span>|</span><a href="#42552271">root</a><span>|</span><a href="#42555028">parent</a><span>|</span><a href="#42555215">prev</a><span>|</span><a href="#42555083">next</a><span>|</span><label class="collapse" for="c-42556700">[-]</label><label class="expand" for="c-42556700">[1 more]</label></div><br/><div class="children"><div class="content">I liked the awe you shared -- it made me want to learn more about entropy.</div><br/></div></div></div></div><div id="42555083" class="c"><input type="checkbox" id="c-42555083" checked=""/><div class="controls bullet"><span class="by">micimize</span><span>|</span><a href="#42552271">parent</a><span>|</span><a href="#42555028">prev</a><span>|</span><a href="#42552747">next</a><span>|</span><label class="collapse" for="c-42555083">[-]</label><label class="expand" for="c-42555083">[1 more]</label></div><br/><div class="children"><div class="content">In the sense I understand that comparison, or have usually seen it referred to, the compressed representation is the internal latent in a (V)AE. Still, I haven&#x27;t seen many attempts at compression that would store the latent + a delta to form lossless compression, that an AI system could then maybe use natively at high performance. Or if I have... I have not understood them.</div><br/></div></div><div id="42552747" class="c"><input type="checkbox" id="c-42552747" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#42552271">parent</a><span>|</span><a href="#42555083">prev</a><span>|</span><a href="#42553209">next</a><span>|</span><label class="collapse" for="c-42552747">[-]</label><label class="expand" for="c-42552747">[1 more]</label></div><br/><div class="children"><div class="content">I’m not sure this is strictly true. It seems more accurate to say there are deep connections between the two rather than they are theoretically equivalent problems. His work is really cool though no doubt.</div><br/></div></div></div></div><div id="42553209" class="c"><input type="checkbox" id="c-42553209" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42552271">prev</a><span>|</span><a href="#42552593">next</a><span>|</span><label class="collapse" for="c-42553209">[-]</label><label class="expand" for="c-42553209">[1 more]</label></div><br/><div class="children"><div class="content">Also worth checking out some of the author&#x27;s other compressors e.g. another one of their neural network solutions using a transformer <a href="https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;</a> holds the top spot in the Large Text Compression Benchmark. It&#x27;s ~3 orders of magnitude slower though.</div><br/></div></div><div id="42552593" class="c"><input type="checkbox" id="c-42552593" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#42553209">prev</a><span>|</span><a href="#42557216">next</a><span>|</span><label class="collapse" for="c-42552593">[-]</label><label class="expand" for="c-42552593">[14 more]</label></div><br/><div class="children"><div class="content">If I read this correctly, the largest test reported on this page is the &quot;enwik9&quot; dataset, which compresses to 213 MB with xz and only 135 MB with this method, a 78 MB difference... using a model that is 340 MB (and was probably trained on the test data).<p>No one would be impressed with saving 78 MB on compression using a 340 MB dictionary so I am not sure why this is good?<p>Please let me know if I misunderstand.</div><br/><div id="42552996" class="c"><input type="checkbox" id="c-42552996" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42552593">parent</a><span>|</span><a href="#42552656">next</a><span>|</span><label class="collapse" for="c-42552996">[-]</label><label class="expand" for="c-42552996">[5 more]</label></div><br/><div class="children"><div class="content">&gt; using a model that is 340 MB<p>&quot;The model is quantized to 8 bits per parameter and evaluated using BF16 floating point numbers&quot; means the model is stored as 1 byte per parameter even though it&#x27;s using a 2 byte type during compute. This is backed up by checking the size of  from the download which comes out as 171,363,973 bytes for the model file.<p>&gt; and was probably trained on the test data<p>This is likely a safe assumption (enwik8 is the default training set for RWKV and no mention of using other data was given) however:<p>&gt; No one would be impressed with saving 78 MB on compression using a 340 MB dictionary so I am not sure why this is good?<p>The Ts_zip+enwik9 size comes out to less than the 197,368,568 for xz+enwik9 listed in the Large Text Compression Benchmark 
 despite the large model file. Getting 20,929,618 total bytes smaller while keeping a good runtime speed is not bad and puts it decently high in the list (even when sorted by total size) despite the difference in approach. Keep in mind the top entry at 107,261,318 total bytes in the table is nncp by the same author (neural net but not LLM based) so it makes sense to keep an open mind as to why they thought this would be worth publishing.</div><br/><div id="42555813" class="c"><input type="checkbox" id="c-42555813" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42552996">parent</a><span>|</span><a href="#42554281">next</a><span>|</span><label class="collapse" for="c-42555813">[-]</label><label class="expand" for="c-42555813">[2 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t be surprised if my math was wrong but I can&#x27;t quite follow yours. ts_zip(171 MB you say)+llm-enwik9(135MB) = 306MB is still larger than xz(0.3MB)+xz-enwik9(213MB) = 213MB.</div><br/><div id="42556078" class="c"><input type="checkbox" id="c-42556078" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42555813">parent</a><span>|</span><a href="#42554281">next</a><span>|</span><label class="collapse" for="c-42556078">[-]</label><label class="expand" for="c-42556078">[1 more]</label></div><br/><div class="children"><div class="content">I done did went and copied the enwik8 value for ts_zip when doing that compare, good catch!<p>I guess that leaves the question of &quot;how well does the LLM&#x27;s predictions work for things we&#x27;re certain weren&#x27;t in the test data set&quot;. If it&#x27;s truly just the prebuilt RWKV then it is only trained on enwik8 and enwik9 is already a generalization but there&#x27;s nothing really guaranteeing that assumption. On the other hand... I can&#x27;t think of GB class open datasets of plain english to test with that aren&#x27;t already in use on the page.</div><br/></div></div></div></div><div id="42554281" class="c"><input type="checkbox" id="c-42554281" checked=""/><div class="controls bullet"><span class="by">pmayrgundter</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42552996">parent</a><span>|</span><a href="#42555813">prev</a><span>|</span><a href="#42552656">next</a><span>|</span><label class="collapse" for="c-42554281">[-]</label><label class="expand" for="c-42554281">[2 more]</label></div><br/><div class="children"><div class="content">Not following. That top entry is marked as Transformer, which does mean it&#x27;s an LLM</div><br/><div id="42555281" class="c"><input type="checkbox" id="c-42555281" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42554281">parent</a><span>|</span><a href="#42552656">next</a><span>|</span><label class="collapse" for="c-42555281">[-]</label><label class="expand" for="c-42555281">[1 more]</label></div><br/><div class="children"><div class="content">Of the two nncp uses transformers but isn&#x27;t an LLM while ts_zip doesn&#x27;t use transformers but is an LLM. Remember LLM just means large language model, it doesn&#x27;t make any assumptions about how it&#x27;s built. Similarly transformers just relate tokens according to attention, they don&#x27;t make any assumptions those tokens must represent natural language.<p>I.e. anything you can tokenize can be wrangled using a transformer, not just language. Thankfully the same author also has a handy example of this: transformer based audio compression <a href="https:&#x2F;&#x2F;bellard.org&#x2F;tsac&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bellard.org&#x2F;tsac&#x2F;</a></div><br/></div></div></div></div></div></div><div id="42552656" class="c"><input type="checkbox" id="c-42552656" checked=""/><div class="controls bullet"><span class="by">binary132</span><span>|</span><a href="#42552593">parent</a><span>|</span><a href="#42552996">prev</a><span>|</span><a href="#42552872">next</a><span>|</span><label class="collapse" for="c-42552656">[-]</label><label class="expand" for="c-42552656">[7 more]</label></div><br/><div class="children"><div class="content">If you’re compressing 100 or 100k such datasets, presuming that it is not custom tuned for this corpus, then wouldn’t you still save much more than you spend?</div><br/><div id="42553566" class="c"><input type="checkbox" id="c-42553566" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42552656">parent</a><span>|</span><a href="#42552872">next</a><span>|</span><label class="collapse" for="c-42553566">[-]</label><label class="expand" for="c-42553566">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not saying the result is completely useless, I am comparing it to the age-old technique of using a dictionary. Does this new LLM-powered technique improve upon the old dictionary technique?<p>Dictionaries also don&#x27;t require a GPU or this amount of RAM.<p>Where I assume LLMs would shine is lossy compression.</div><br/><div id="42553778" class="c"><input type="checkbox" id="c-42553778" checked=""/><div class="controls bullet"><span class="by">binary132</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42553566">parent</a><span>|</span><a href="#42553917">next</a><span>|</span><label class="collapse" for="c-42553778">[-]</label><label class="expand" for="c-42553778">[2 more]</label></div><br/><div class="children"><div class="content">Ah ok, I think we made different assumptions about whether the model was specific to the particular dataset so each one would need a new model — a dictionary is specific to the particular dataset being compressed, right?  I was thinking the LLM would be a general-purpose text compression model.</div><br/><div id="42555784" class="c"><input type="checkbox" id="c-42555784" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42553778">parent</a><span>|</span><a href="#42553917">next</a><span>|</span><label class="collapse" for="c-42555784">[-]</label><label class="expand" for="c-42555784">[1 more]</label></div><br/><div class="children"><div class="content">Not particularly. You could make a dictionary from &quot;the English web&quot;, with common character sequences found on those sites you use as input.</div><br/></div></div></div></div><div id="42553917" class="c"><input type="checkbox" id="c-42553917" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42553566">parent</a><span>|</span><a href="#42553778">prev</a><span>|</span><a href="#42552872">next</a><span>|</span><label class="collapse" for="c-42553917">[-]</label><label class="expand" for="c-42553917">[3 more]</label></div><br/><div class="children"><div class="content">I have the same question, what is the different between LLM and Dictionary in the context of compression. Can I not &quot;train&quot; a dictionary?</div><br/><div id="42554332" class="c"><input type="checkbox" id="c-42554332" checked=""/><div class="controls bullet"><span class="by">binary132</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42553917">parent</a><span>|</span><a href="#42552872">next</a><span>|</span><label class="collapse" for="c-42554332">[-]</label><label class="expand" for="c-42554332">[2 more]</label></div><br/><div class="children"><div class="content">AIUI, a dictionary is built during compression to specify the heuristics of a particular dataset and belongs to that specific dataset only.  For example, it could be a ranking of the most frequent 10 symbols in the compressed file.  That will be different for every input file.</div><br/><div id="42556159" class="c"><input type="checkbox" id="c-42556159" checked=""/><div class="controls bullet"><span class="by">mbreese</span><span>|</span><a href="#42552593">root</a><span>|</span><a href="#42554332">parent</a><span>|</span><a href="#42552872">next</a><span>|</span><label class="collapse" for="c-42556159">[-]</label><label class="expand" for="c-42556159">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; That will be different for every input file</i><p>That <i>could</i> be different for every input file, but it doesn&#x27;t have to be. It could also be a fixed dictionary. For example, ZLIB allows for a user-defined dictionary [1].<p>In this case, I&#x27;d consider the LLM to be a fixed dictionary of sorts. A very large, fixed dictionary with probabilistic return values.<p>[1] <a href="https:&#x2F;&#x2F;www.rfc-editor.org&#x2F;rfc&#x2F;rfc1950#page-9" rel="nofollow">https:&#x2F;&#x2F;www.rfc-editor.org&#x2F;rfc&#x2F;rfc1950#page-9</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="42552872" class="c"><input type="checkbox" id="c-42552872" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#42552593">parent</a><span>|</span><a href="#42552656">prev</a><span>|</span><a href="#42557216">next</a><span>|</span><label class="collapse" for="c-42552872">[-]</label><label class="expand" for="c-42552872">[1 more]</label></div><br/><div class="children"><div class="content">Notably, solutions specialized for enwik9 (specifically fx2-cmix) take up only 110 MB, including the size of the decompressor.</div><br/></div></div></div></div><div id="42552482" class="c"><input type="checkbox" id="c-42552482" checked=""/><div class="controls bullet"><span class="by">justmarc</span><span>|</span><a href="#42557216">prev</a><span>|</span><a href="#42552134">next</a><span>|</span><label class="collapse" for="c-42552482">[-]</label><label class="expand" for="c-42552482">[1 more]</label></div><br/><div class="children"><div class="content">This man is an absolute wizard, and a legend who hasn&#x27;t stopped since the fantastic LZEXE days.</div><br/></div></div><div id="42552134" class="c"><input type="checkbox" id="c-42552134" checked=""/><div class="controls bullet"><span class="by">droideqa</span><span>|</span><a href="#42552482">prev</a><span>|</span><a href="#42551964">next</a><span>|</span><label class="collapse" for="c-42552134">[-]</label><label class="expand" for="c-42552134">[1 more]</label></div><br/><div class="children"><div class="content">I have always thought compression to be an analog to intelligence. The smarter you are, the better at summarization you are.</div><br/></div></div><div id="42551964" class="c"><input type="checkbox" id="c-42551964" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42552134">prev</a><span>|</span><a href="#42553418">next</a><span>|</span><label class="collapse" for="c-42551964">[-]</label><label class="expand" for="c-42551964">[3 more]</label></div><br/><div class="children"><div class="content">Fabrice has recently extended this work into audio encoding, an area which to me seems more useful than shaving a bit more off wik8 compression rates.<p>Demo and code? Available at bellard.org as well.</div><br/><div id="42553234" class="c"><input type="checkbox" id="c-42553234" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42551964">parent</a><span>|</span><a href="#42553418">next</a><span>|</span><label class="collapse" for="c-42553234">[-]</label><label class="expand" for="c-42553234">[2 more]</label></div><br/><div class="children"><div class="content">Link for the curious <a href="https:&#x2F;&#x2F;bellard.org&#x2F;tsac&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bellard.org&#x2F;tsac&#x2F;</a><p>Has anyone done the work of comparing this to other similar extreme audio compression solutions?</div><br/></div></div></div></div><div id="42553418" class="c"><input type="checkbox" id="c-42553418" checked=""/><div class="controls bullet"><span class="by">bhouston</span><span>|</span><a href="#42551964">prev</a><span>|</span><a href="#42551833">next</a><span>|</span><label class="collapse" for="c-42553418">[-]</label><label class="expand" for="c-42553418">[3 more]</label></div><br/><div class="children"><div class="content">I believe almost all LLMs are trained using wikpedia these days.  So compressing wikipedia well without including the size of the LLM in the compression result is a bit of a cheat.  I guess one would argue it is a universal dataset representing understanding the English language and real-world relationships at this point but it is still a bit of a cheat.</div><br/><div id="42553777" class="c"><input type="checkbox" id="c-42553777" checked=""/><div class="controls bullet"><span class="by">atiedebee</span><span>|</span><a href="#42553418">parent</a><span>|</span><a href="#42551833">next</a><span>|</span><label class="collapse" for="c-42553777">[-]</label><label class="expand" for="c-42553777">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a reason compression benchmarks often times include the size of the executable when benchmarking compression ratios. Although Matt Mahoney&#x27;s large text compression benchmark[0] does currently have a transformer model at number 1.<p>[0] <a href="http:&#x2F;&#x2F;www.mattmahoney.net&#x2F;dc&#x2F;text.html" rel="nofollow">http:&#x2F;&#x2F;www.mattmahoney.net&#x2F;dc&#x2F;text.html</a></div><br/><div id="42554200" class="c"><input type="checkbox" id="c-42554200" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#42553418">root</a><span>|</span><a href="#42553777">parent</a><span>|</span><a href="#42551833">next</a><span>|</span><label class="collapse" for="c-42554200">[-]</label><label class="expand" for="c-42554200">[1 more]</label></div><br/><div class="children"><div class="content">Which is also made by the same author as ts_zip (Fabrice Bellard): <a href="https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;</a></div><br/></div></div></div></div></div></div><div id="42551833" class="c"><input type="checkbox" id="c-42551833" checked=""/><div class="controls bullet"><span class="by">Twirrim</span><span>|</span><a href="#42553418">prev</a><span>|</span><a href="#42551892">next</a><span>|</span><label class="collapse" for="c-42551833">[-]</label><label class="expand" for="c-42551833">[3 more]</label></div><br/><div class="children"><div class="content">&quot;(and hopefully decompress)&quot; is a horrifying descriptor.</div><br/><div id="42552346" class="c"><input type="checkbox" id="c-42552346" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#42551833">parent</a><span>|</span><a href="#42551852">next</a><span>|</span><label class="collapse" for="c-42552346">[-]</label><label class="expand" for="c-42552346">[1 more]</label></div><br/><div class="children"><div class="content">It adds levity to the article and also introduces the reader to the sorts of things that can go wrong if they try it at home.<p>The last paragraph highlights how they fixed one of the main pitfalls I normally see in this sort of thing, where floating-point operations are mangled in myriad ways in the name of efficiency (almost always correct for physics or whatever, but a single bit being incorrect will occasionally mangle this compression scheme).<p>Mind you, actually doing what they claimed in that last paragraph is usually painful. The easiest approaches re-implement floating-point operations in software using integer instructions, and the complexity increases from there.</div><br/></div></div><div id="42551852" class="c"><input type="checkbox" id="c-42551852" checked=""/><div class="controls bullet"><span class="by">perching_aix</span><span>|</span><a href="#42551833">parent</a><span>|</span><a href="#42552346">prev</a><span>|</span><a href="#42551892">next</a><span>|</span><label class="collapse" for="c-42551852">[-]</label><label class="expand" for="c-42551852">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re clearly just poking fun at it.</div><br/></div></div></div></div><div id="42551892" class="c"><input type="checkbox" id="c-42551892" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#42551833">prev</a><span>|</span><a href="#42553002">next</a><span>|</span><label class="collapse" for="c-42551892">[-]</label><label class="expand" for="c-42551892">[2 more]</label></div><br/><div class="children"><div class="content">Prior discussion: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37152978">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37152978</a></div><br/><div id="42552693" class="c"><input type="checkbox" id="c-42552693" checked=""/><div class="controls bullet"><span class="by">jodrellblank</span><span>|</span><a href="#42551892">parent</a><span>|</span><a href="#42553002">next</a><span>|</span><label class="collapse" for="c-42552693">[-]</label><label class="expand" for="c-42552693">[1 more]</label></div><br/><div class="children"><div class="content">Looks like it’s been updated since then; commenters in that thread are saying the decompressor needs to run on the same hardware as the compressor; now the link says:<p>&gt; “<i>The model is evaluated in a deterministic and reproducible way. Hence the result does not depend on the exact GPU or CPU model nor on the number of configured threads. This key point ensures that a compressed file can be decompressed using a different hardware or software configuration.</i>”</div><br/></div></div></div></div><div id="42552478" class="c"><input type="checkbox" id="c-42552478" checked=""/><div class="controls bullet"><span class="by">0-_-0</span><span>|</span><a href="#42553002">prev</a><span>|</span><a href="#42552689">next</a><span>|</span><label class="collapse" for="c-42552478">[-]</label><label class="expand" for="c-42552478">[1 more]</label></div><br/><div class="children"><div class="content">1 MBps is insanely fast for a method like this, it must be in the 100k tokens per second range. Probably with large batches.</div><br/></div></div><div id="42552689" class="c"><input type="checkbox" id="c-42552689" checked=""/><div class="controls bullet"><span class="by">j_juggernaut</span><span>|</span><a href="#42552478">prev</a><span>|</span><a href="#42552193">next</a><span>|</span><label class="collapse" for="c-42552689">[-]</label><label class="expand" for="c-42552689">[2 more]</label></div><br/><div class="children"><div class="content">Made a quick and dirt streamlit app to play around encrypt decrypt<p><a href="https:&#x2F;&#x2F;llmencryptdecrypt-euyfofcjh8bf2utuha2zox.streamlit.app&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llmencryptdecrypt-euyfofcjh8bf2utuha2zox.streamlit.a...</a></div><br/><div id="42554780" class="c"><input type="checkbox" id="c-42554780" checked=""/><div class="controls bullet"><span class="by">meindnoch</span><span>|</span><a href="#42552689">parent</a><span>|</span><a href="#42552193">next</a><span>|</span><label class="collapse" for="c-42554780">[-]</label><label class="expand" for="c-42554780">[1 more]</label></div><br/><div class="children"><div class="content">It is very good at decrypting the string &quot;Error&quot;.</div><br/></div></div></div></div><div id="42552193" class="c"><input type="checkbox" id="c-42552193" checked=""/><div class="controls bullet"><span class="by">mikevin</span><span>|</span><a href="#42552689">prev</a><span>|</span><a href="#42554535">next</a><span>|</span><label class="collapse" for="c-42552193">[-]</label><label class="expand" for="c-42552193">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious what the compressed text looks like. Anyone have an example?</div><br/><div id="42552507" class="c"><input type="checkbox" id="c-42552507" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#42552193">parent</a><span>|</span><a href="#42552480">next</a><span>|</span><label class="collapse" for="c-42552507">[-]</label><label class="expand" for="c-42552507">[1 more]</label></div><br/><div class="children"><div class="content">If it is within cooee of state of the art the compressed text should look like a pile of random bits.<p>If it looks like anything at all other than randomness then you can describe whatever it is that it looks like to get more compression.</div><br/></div></div><div id="42552480" class="c"><input type="checkbox" id="c-42552480" checked=""/><div class="controls bullet"><span class="by">munch117</span><span>|</span><a href="#42552193">parent</a><span>|</span><a href="#42552507">prev</a><span>|</span><a href="#42554535">next</a><span>|</span><label class="collapse" for="c-42552480">[-]</label><label class="expand" for="c-42552480">[1 more]</label></div><br/><div class="children"><div class="content">Binary goo, barely distinguishable from random data, if at all.  The arithmetic coder will make sure of that.<p>It&#x27;s the nature of compression: Any discernible pattern could have been exploited for further compression.</div><br/></div></div></div></div><div id="42554535" class="c"><input type="checkbox" id="c-42554535" checked=""/><div class="controls bullet"><span class="by">cat5e</span><span>|</span><a href="#42552193">prev</a><span>|</span><label class="collapse" for="c-42554535">[-]</label><label class="expand" for="c-42554535">[1 more]</label></div><br/><div class="children"><div class="content">Has this been attempted for raw binary? Using an NN to predict the most likely next binary string?</div><br/></div></div></div></div></div></div></div></body></html>