<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1698915667480" as="style"/><link rel="stylesheet" href="styles.css?v=1698915667480"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.businessinsider.com/sam-altman-and-demis-hassabis-just-want-to-control-ai-2023-10">Yann LeCun: AI one-percenters seizing power forever is real doomsday scenario</a>Â <span class="domain">(<a href="https://www.businessinsider.com">www.businessinsider.com</a>)</span></div><div class="subtext"><span>g42gregory</span> | <span>223 comments</span></div><br/><div><div id="38109168" class="c"><input type="checkbox" id="c-38109168" checked=""/><div class="controls bullet"><span class="by">tellarin</span><span>|</span><a href="#38109477">next</a><span>|</span><label class="collapse" for="c-38109168">[-]</label><label class="expand" for="c-38109168">[1 more]</label></div><br/><div class="children"><div class="content">Archive.is cache: <a href="https:&#x2F;&#x2F;archive.is&#x2F;HbRLy" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.is&#x2F;HbRLy</a></div><br/></div></div><div id="38109477" class="c"><input type="checkbox" id="c-38109477" checked=""/><div class="controls bullet"><span class="by">lazzlazzlazz</span><span>|</span><a href="#38109168">prev</a><span>|</span><a href="#38110044">next</a><span>|</span><label class="collapse" for="c-38109477">[-]</label><label class="expand" for="c-38109477">[58 more]</label></div><br/><div class="children"><div class="content">The way incumbents are attempting to seize power via regulatory capture, using &quot;X-Risk&quot; and fantastical claims of sudden human extinction is maybe one of the most cynical ploys in the history of tech so far.<p>Open source is one of our greatest gifts, and the push to close off AI with farcical licensing and reporting requirements (which will undoubtedly become more strict) is absurd.<p>The laws we have already cover malevolent and abusive uses of software applications.</div><br/><div id="38109623" class="c"><input type="checkbox" id="c-38109623" checked=""/><div class="controls bullet"><span class="by">JoshTriplett</span><span>|</span><a href="#38109477">parent</a><span>|</span><a href="#38110638">next</a><span>|</span><label class="collapse" for="c-38109623">[-]</label><label class="expand" for="c-38109623">[48 more]</label></div><br/><div class="children"><div class="content">&gt; seize power via regulatory capture<p>Not even a little bit. &quot;Stop&quot; is not regulatory capture. Some large AI companies are attempting to twist &quot;stop&quot; into &quot;be careful, as only we can&quot;. The actual way to stop the existential risk is to stop. <a href="https:&#x2F;&#x2F;twitter.com&#x2F;ESYudkowsky&#x2F;status&#x2F;1719777049576128542" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;ESYudkowsky&#x2F;status&#x2F;1719777049576128542</a><p>&gt; the push to close off AI with farcical licensing and reporting requirements<p>&quot;Stop&quot; is not &quot;licensing and reporting requirements&quot;, it&#x27;s <i>stop</i>.</div><br/><div id="38110212" class="c"><input type="checkbox" id="c-38110212" checked=""/><div class="controls bullet"><span class="by">topspin</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109623">parent</a><span>|</span><a href="#38110394">next</a><span>|</span><label class="collapse" for="c-38110212">[-]</label><label class="expand" for="c-38110212">[12 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Stop&quot; is not regulatory capture.<p>&quot;Stop&quot; is a fiction that exists exclusively inside your head.  Unless your solution anticipates a planet wide GULAG system run by a global government your &quot;stop&quot; is infeasible.</div><br/><div id="38110422" class="c"><input type="checkbox" id="c-38110422" checked=""/><div class="controls bullet"><span class="by">timmytokyo</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110212">parent</a><span>|</span><a href="#38110615">next</a><span>|</span><label class="collapse" for="c-38110422">[-]</label><label class="expand" for="c-38110422">[6 more]</label></div><br/><div class="children"><div class="content">His solution -- assuming it&#x27;s the same as the one proposed by Yudkowsky, who he quotes -- is launching air strikes against data centers hosting AI computation.<p>So yeah, imaginary.</div><br/><div id="38110515" class="c"><input type="checkbox" id="c-38110515" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110422">parent</a><span>|</span><a href="#38110569">next</a><span>|</span><label class="collapse" for="c-38110515">[-]</label><label class="expand" for="c-38110515">[3 more]</label></div><br/><div class="children"><div class="content">Ironically, it seems the only way to stop someone from building an omnipotent AI that could destroy the world is to build an omnipotent AI under your control faster than any of the people you need to destroy. If you believe in that sort of thing.</div><br/><div id="38110575" class="c"><input type="checkbox" id="c-38110575" checked=""/><div class="controls bullet"><span class="by">jstanley</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110515">parent</a><span>|</span><a href="#38110569">next</a><span>|</span><label class="collapse" for="c-38110575">[-]</label><label class="expand" for="c-38110575">[2 more]</label></div><br/><div class="children"><div class="content">The only thing that can stop a bad guy with AI is a good guy with AI.</div><br/><div id="38110669" class="c"><input type="checkbox" id="c-38110669" checked=""/><div class="controls bullet"><span class="by">calgoo</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110575">parent</a><span>|</span><a href="#38110569">next</a><span>|</span><label class="collapse" for="c-38110669">[-]</label><label class="expand" for="c-38110669">[1 more]</label></div><br/><div class="children"><div class="content">AI&#x27;s for everyone!!!! It&#x27;s actually a scare but possible future. Everyone has to carry personal security AI that you rent from some Corpo, and if you don&#x27;t you will be endlessly hacked and harassed in IRL just like we are online these days.</div><br/></div></div></div></div></div></div><div id="38110569" class="c"><input type="checkbox" id="c-38110569" checked=""/><div class="controls bullet"><span class="by">duckmysick</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110422">parent</a><span>|</span><a href="#38110515">prev</a><span>|</span><a href="#38110615">next</a><span>|</span><label class="collapse" for="c-38110569">[-]</label><label class="expand" for="c-38110569">[2 more]</label></div><br/><div class="children"><div class="content">I wonder what would happen if the AI computation is co-hosted in data centers with civilian data.</div><br/></div></div></div></div><div id="38110615" class="c"><input type="checkbox" id="c-38110615" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110212">parent</a><span>|</span><a href="#38110422">prev</a><span>|</span><a href="#38110284">next</a><span>|</span><label class="collapse" for="c-38110615">[-]</label><label class="expand" for="c-38110615">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Stop&quot; might be possible if doing AI is expensive enough like say biotech. Currently it is in the limit of being expensive(costing few hundred thousands to few million for state of the art model), but it is very likely to change in near future that will change as even currently most of the cost comes not from energy of computation, but from expensive NVIDIA hardware.</div><br/></div></div><div id="38110284" class="c"><input type="checkbox" id="c-38110284" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110212">parent</a><span>|</span><a href="#38110615">prev</a><span>|</span><a href="#38110394">next</a><span>|</span><label class="collapse" for="c-38110284">[-]</label><label class="expand" for="c-38110284">[4 more]</label></div><br/><div class="children"><div class="content">If tomorrow TSMC, NVIDIA and ASML get destroyed in the next 9&#x2F;11, it&#x27;s &quot;stopped&quot;. It&#x27;s completely possible.</div><br/><div id="38110340" class="c"><input type="checkbox" id="c-38110340" checked=""/><div class="controls bullet"><span class="by">topspin</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110284">parent</a><span>|</span><a href="#38110647">next</a><span>|</span><label class="collapse" for="c-38110340">[-]</label><label class="expand" for="c-38110340">[1 more]</label></div><br/><div class="children"><div class="content">Fantasies, bordering on threats.</div><br/></div></div><div id="38110647" class="c"><input type="checkbox" id="c-38110647" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110284">parent</a><span>|</span><a href="#38110340">prev</a><span>|</span><a href="#38110322">next</a><span>|</span><label class="collapse" for="c-38110647">[-]</label><label class="expand" for="c-38110647">[1 more]</label></div><br/><div class="children"><div class="content">I think current hardware already produced till now is either enough to get us to AGI, or it is very far away. I don&#x27;t believe AGI is 30 years away. It is either very near or more than 1000s of years away.</div><br/></div></div><div id="38110322" class="c"><input type="checkbox" id="c-38110322" checked=""/><div class="controls bullet"><span class="by">Maken</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110284">parent</a><span>|</span><a href="#38110647">prev</a><span>|</span><a href="#38110394">next</a><span>|</span><label class="collapse" for="c-38110322">[-]</label><label class="expand" for="c-38110322">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s delayed at best.</div><br/></div></div></div></div></div></div><div id="38110394" class="c"><input type="checkbox" id="c-38110394" checked=""/><div class="controls bullet"><span class="by">upupupandaway</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109623">parent</a><span>|</span><a href="#38110212">prev</a><span>|</span><a href="#38110058">next</a><span>|</span><label class="collapse" for="c-38110394">[-]</label><label class="expand" for="c-38110394">[7 more]</label></div><br/><div class="children"><div class="content">The fact that anyone may be taking Eliezer Yudkowsky seriously on this topic is mind-blowing to me. Simply mind-blowing. To imagine that anybody, including the UN, would have any power to collective put a stop to the development of AI globally is a laughable idea. The UN cannot agree to release a letter on Israel-Hamas, imagine policing the ENTIRE WORLD and shutting down AI development when necessary.<p>We can&#x27;t stop countries from developing weapons that will destroy us all tomorrow morning and take billions of USD to develop, imagine thinking we can stop matrix multiplication globally. I don&#x27;t want to derail into an ad hominem, but frankly, it&#x27;s almost the only option left here.</div><br/><div id="38110492" class="c"><input type="checkbox" id="c-38110492" checked=""/><div class="controls bullet"><span class="by">jeffparsons</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110394">parent</a><span>|</span><a href="#38110563">next</a><span>|</span><label class="collapse" for="c-38110492">[-]</label><label class="expand" for="c-38110492">[4 more]</label></div><br/><div class="children"><div class="content">My estimation of Eliezer Yudkowsky just dropped a notch â not because I don&#x27;t take the existential threat posed by AI seriously â but because he seems to think that global coordination and control is an even remotely possible strategy.</div><br/><div id="38110713" class="c"><input type="checkbox" id="c-38110713" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110492">parent</a><span>|</span><a href="#38110547">next</a><span>|</span><label class="collapse" for="c-38110713">[-]</label><label class="expand" for="c-38110713">[1 more]</label></div><br/><div class="children"><div class="content">Global coordination using coercion has worked for nukes. It&#x27;s very hard but not impossible to ban NVidia or any competitor and create spy network in all large companies and if any engineer is found to create AI, mark them as terrorist and give them POW treatment. And monitor chip fab like nuclear enrichment facility and launch war with country found to create chip fab, which I belive is hard to do it in complete secrecy.<p>I believe if top 5 nation agrees for this, it could be acheived.</div><br/></div></div><div id="38110547" class="c"><input type="checkbox" id="c-38110547" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110492">parent</a><span>|</span><a href="#38110713">prev</a><span>|</span><a href="#38110563">next</a><span>|</span><label class="collapse" for="c-38110547">[-]</label><label class="expand" for="c-38110547">[2 more]</label></div><br/><div class="children"><div class="content">He actually doesn&#x27;t think it&#x27;s viable, but thinks its the only card left to play.<p>He&#x27;s pretty sure that human civilization will be extinct this century.</div><br/><div id="38110622" class="c"><input type="checkbox" id="c-38110622" checked=""/><div class="controls bullet"><span class="by">jeffparsons</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110547">parent</a><span>|</span><a href="#38110563">next</a><span>|</span><label class="collapse" for="c-38110622">[-]</label><label class="expand" for="c-38110622">[1 more]</label></div><br/><div class="children"><div class="content">It can&#x27;t be the only card left to play. There are so many others. They may be weak, but they still have to be better than one that is guaranteed to be absolutely useless.<p>Example of a weak, but at least _better_ option: convince _one_ government (e.g. USA) that it is in their interest to massively fund an effort to (1) develop an AI that is compliant to our wishes and can dominate all other AIs, and (2) actively sabotage competing efforts.<p>Governments have done much the same in the past with, e.g. conventional weapons, nuclear weapons, and cryptography, with varying levels of success.<p>If we&#x27;re all dead anyway otherwise, then I don&#x27;t see how that can possibly be a worse card.<p>Edit: or even try to convince _all_ governments to do this so that they come out on top. At least then there&#x27;s a greater chance that the bleeding edge of this tech will be under the stewardship of a deliberate attempt for a country to dominate the world, rather than some bored kid who happens to stumble upon the recipe for global paperclips.</div><br/></div></div></div></div></div></div><div id="38110563" class="c"><input type="checkbox" id="c-38110563" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110394">parent</a><span>|</span><a href="#38110492">prev</a><span>|</span><a href="#38110058">next</a><span>|</span><label class="collapse" for="c-38110563">[-]</label><label class="expand" for="c-38110563">[2 more]</label></div><br/><div class="children"><div class="content">Sure, it&#x27;s not likely to work, but Yud doesn&#x27;t know of an idea with a better chance of working.<p>If you agree with him on the likelihood of superintelligence in the next few decades and the difficulty of value alignment, what course of action would you suggest?</div><br/><div id="38110601" class="c"><input type="checkbox" id="c-38110601" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110563">parent</a><span>|</span><a href="#38110058">next</a><span>|</span><label class="collapse" for="c-38110601">[-]</label><label class="expand" for="c-38110601">[1 more]</label></div><br/><div class="children"><div class="content">I think the core is not to agree with the existential risk scenario in the first place.<p>Once you believe some malevolent god will appear and doom us all, anything can be justified (and people have done so in the past - no malevolent god appeared then, btw.).</div><br/></div></div></div></div></div></div><div id="38110058" class="c"><input type="checkbox" id="c-38110058" checked=""/><div class="controls bullet"><span class="by">wood_spirit</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109623">parent</a><span>|</span><a href="#38110394">prev</a><span>|</span><a href="#38109798">next</a><span>|</span><label class="collapse" for="c-38110058">[-]</label><label class="expand" for="c-38110058">[9 more]</label></div><br/><div class="children"><div class="content">You canât _stop_ it.  You can make it illegal in some countries.  But outlawing it just means some obvious candidate countries or private actors develop it first instead.</div><br/><div id="38110218" class="c"><input type="checkbox" id="c-38110218" checked=""/><div class="controls bullet"><span class="by">candiodari</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110058">parent</a><span>|</span><a href="#38109798">next</a><span>|</span><label class="collapse" for="c-38110218">[-]</label><label class="expand" for="c-38110218">[8 more]</label></div><br/><div class="children"><div class="content">I think these attempts will simply point out that math is a higher power than law. If something becomes possible, and becomes easy, there is nothing law can do to stop it.<p>Just look at the attempts to stop drugs in North West Europe. Huge sentences (so huge they are in fact never fully carried out), tens of dead police officers on a yearly basis, triple that in civilian victims and drugs are easier to get than ever before.</div><br/><div id="38110276" class="c"><input type="checkbox" id="c-38110276" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110218">parent</a><span>|</span><a href="#38110585">next</a><span>|</span><label class="collapse" for="c-38110276">[-]</label><label class="expand" for="c-38110276">[6 more]</label></div><br/><div class="children"><div class="content">The aim is to stop it from becoming easy.<p>The drug war would look very different if good cocaine was only produced in one country with extremely costly chemistry machines from one company.</div><br/><div id="38110412" class="c"><input type="checkbox" id="c-38110412" checked=""/><div class="controls bullet"><span class="by">upupupandaway</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110276">parent</a><span>|</span><a href="#38110585">next</a><span>|</span><label class="collapse" for="c-38110412">[-]</label><label class="expand" for="c-38110412">[5 more]</label></div><br/><div class="children"><div class="content">If good cocaine was only produced in one country with extremely costly machines, a good-enough alternative would appear almost immediately. This has happened millions of times in the history of humanity, I refuse to believe that people haven&#x27;t learned this lesson.</div><br/><div id="38110528" class="c"><input type="checkbox" id="c-38110528" checked=""/><div class="controls bullet"><span class="by">_0ffh</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110412">parent</a><span>|</span><a href="#38110595">next</a><span>|</span><label class="collapse" for="c-38110528">[-]</label><label class="expand" for="c-38110528">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I refuse to believe that people haven&#x27;t learned this lesson<p>Oh, I believe they absolutely haven&#x27;t, when not having learnt it buys them money and influence. As for the bulk of the population, they will accept whatever the corporate media tell them to.</div><br/></div></div><div id="38110595" class="c"><input type="checkbox" id="c-38110595" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110412">parent</a><span>|</span><a href="#38110528">prev</a><span>|</span><a href="#38110585">next</a><span>|</span><label class="collapse" for="c-38110595">[-]</label><label class="expand" for="c-38110595">[3 more]</label></div><br/><div class="children"><div class="content">And yet, it doesn&#x27;t seem to be that easy with chips.</div><br/><div id="38110720" class="c"><input type="checkbox" id="c-38110720" checked=""/><div class="controls bullet"><span class="by">BlueTemplar</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110595">parent</a><span>|</span><a href="#38110639">next</a><span>|</span><label class="collapse" for="c-38110720">[-]</label><label class="expand" for="c-38110720">[1 more]</label></div><br/><div class="children"><div class="content">More because with chips it&#x27;s still an exponential race. Others will be able to catch up once the race slows down enough.</div><br/></div></div><div id="38110639" class="c"><input type="checkbox" id="c-38110639" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110595">parent</a><span>|</span><a href="#38110720">prev</a><span>|</span><a href="#38110585">next</a><span>|</span><label class="collapse" for="c-38110639">[-]</label><label class="expand" for="c-38110639">[1 more]</label></div><br/><div class="children"><div class="content">Yet China and Russia are still getting their chips, by smuggling if need be. So much for the power of blocking access.</div><br/></div></div></div></div></div></div></div></div><div id="38110585" class="c"><input type="checkbox" id="c-38110585" checked=""/><div class="controls bullet"><span class="by">neilkk</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110218">parent</a><span>|</span><a href="#38110276">prev</a><span>|</span><a href="#38109798">next</a><span>|</span><label class="collapse" for="c-38110585">[-]</label><label class="expand" for="c-38110585">[1 more]</label></div><br/><div class="children"><div class="content">&gt; tens of dead police officers on a yearly basis<p>What are you talking about? This does not happen in North West Europe.</div><br/></div></div></div></div></div></div><div id="38109798" class="c"><input type="checkbox" id="c-38109798" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109623">parent</a><span>|</span><a href="#38110058">prev</a><span>|</span><a href="#38109788">next</a><span>|</span><label class="collapse" for="c-38109798">[-]</label><label class="expand" for="c-38109798">[13 more]</label></div><br/><div class="children"><div class="content">And... How do you do that? Seriously, people have been talking about stoping CO2 emissions for what, 50 years? We&#x27;ve barely made progress there.</div><br/><div id="38110053" class="c"><input type="checkbox" id="c-38110053" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109798">parent</a><span>|</span><a href="#38109788">next</a><span>|</span><label class="collapse" for="c-38110053">[-]</label><label class="expand" for="c-38110053">[12 more]</label></div><br/><div class="children"><div class="content">With threat of military intervention.<p>People have been <i>talking about</i> curtailing carbon emissions for 50 years, but they haven&#x27;t been <i>serious</i> about it. Being serious doesn&#x27;t look like saying &quot;oh shoot, we said we will... try to keep emissions down, but we didn&#x27;t... even try; sorry...&quot;. Being serous looks like &quot;stop now, or we will eminent domain your data centre from under you via an emergency court order, or failing that, bomb it to rubble (because if we don&#x27;t, some other signatory of the treaty will bomb you for us)&quot;.</div><br/><div id="38110334" class="c"><input type="checkbox" id="c-38110334" checked=""/><div class="controls bullet"><span class="by">3rd3</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110053">parent</a><span>|</span><a href="#38110162">next</a><span>|</span><label class="collapse" for="c-38110334">[-]</label><label class="expand" for="c-38110334">[3 more]</label></div><br/><div class="children"><div class="content">Since when are military spooks and political opportunists better at deciding on our technological future than startups and corporations? The degree of global policing and surveillance necessary to fully prevent secret labs from working on AI would be <i>mind-boggling</i>. How would you ensure all government actors are sticking to the same safety standards rather than seizing power by implementing AI hastily? This problem has long been known as <i>quis custodiet ipsos custodes</i> - &quot;who guards the guards themselves?&quot;.</div><br/><div id="38110580" class="c"><input type="checkbox" id="c-38110580" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110334">parent</a><span>|</span><a href="#38110162">next</a><span>|</span><label class="collapse" for="c-38110580">[-]</label><label class="expand" for="c-38110580">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The degree of global policing and surveillance necessary to fully prevent secret labs from working on AI would be mind-boggling.<p>It&#x27;s not <i>that</i> bad given the compute requirements for training even the basic LLMs we have today.<p>But yes, it&#x27;s a long shot.</div><br/><div id="38110664" class="c"><input type="checkbox" id="c-38110664" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110580">parent</a><span>|</span><a href="#38110162">next</a><span>|</span><label class="collapse" for="c-38110664">[-]</label><label class="expand" for="c-38110664">[1 more]</label></div><br/><div class="children"><div class="content">Training a SOTA model is expensive, but you only need to do it once, and fine-tune a thousand times for various purposes.<p>And it&#x27;s not even that expensive when compared to the cost of building other large scale projects. How much is a dam, or a subway station? There are also corporations who would profit from making models widely available, such as chip makers, they would commoditise the complement.<p>Once you have your very capable, open sourced model, that runs on phones and laptops locally, then fine-tuning is almost free.<p>This is not make belief. A few recent fine-tunes of Mistral-7B for example are excellent quality, and run surprisingly fast on a 5 year old GPU - 40T&#x2F;s.</div><br/></div></div></div></div></div></div><div id="38110162" class="c"><input type="checkbox" id="c-38110162" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110053">parent</a><span>|</span><a href="#38110334">prev</a><span>|</span><a href="#38110142">next</a><span>|</span><label class="collapse" for="c-38110162">[-]</label><label class="expand" for="c-38110162">[3 more]</label></div><br/><div class="children"><div class="content">&gt; With threat of military intervention.<p>We&#x27;ve seen how hard it is to do that when the fear is nuclear proliferation. Now consider how hard it is to do that when the research can be done on any sufficiently large set of computational devices, and doesn&#x27;t even need to be geographically concentrated, or even in your own country.<p>If I was a country wanting to continue AI research under threat of military intervention, I&#x27;d run it all in cloud providers in the country making the threat, via shell companies in countries I considered rivals.</div><br/><div id="38110592" class="c"><input type="checkbox" id="c-38110592" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110162">parent</a><span>|</span><a href="#38110201">next</a><span>|</span><label class="collapse" for="c-38110592">[-]</label><label class="expand" for="c-38110592">[1 more]</label></div><br/><div class="children"><div class="content">The big cloud providers would be obligated to ensure their systems were nor being used for AI training.<p>Yes, this would be a substantial regulatory burden.</div><br/></div></div><div id="38110201" class="c"><input type="checkbox" id="c-38110201" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110162">parent</a><span>|</span><a href="#38110592">prev</a><span>|</span><a href="#38110142">next</a><span>|</span><label class="collapse" for="c-38110201">[-]</label><label class="expand" for="c-38110201">[1 more]</label></div><br/><div class="children"><div class="content">At least nuclear weapons pose an actual existential risk as opposed to AI - and even then we don&#x27;t just go to war.</div><br/></div></div></div></div><div id="38110142" class="c"><input type="checkbox" id="c-38110142" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110053">parent</a><span>|</span><a href="#38110162">prev</a><span>|</span><a href="#38110232">next</a><span>|</span><label class="collapse" for="c-38110142">[-]</label><label class="expand" for="c-38110142">[1 more]</label></div><br/><div class="children"><div class="content">No-one wants a nuclear war over imaginary risks, so not happening.</div><br/></div></div><div id="38110232" class="c"><input type="checkbox" id="c-38110232" checked=""/><div class="controls bullet"><span class="by">andruby</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110053">parent</a><span>|</span><a href="#38110142">prev</a><span>|</span><a href="#38110127">next</a><span>|</span><label class="collapse" for="c-38110232">[-]</label><label class="expand" for="c-38110232">[1 more]</label></div><br/><div class="children"><div class="content">But that&#x27;s just not going to work in the real world is it?<p>If a country uses military force in another country, that&#x27;s a declaration of war. We&#x27;ll never convince every single country from banning AI research. And even if we do, you don&#x27;t need much resources to do AI research. A few people and a few computers is enough.<p>This is not something like uranium refining.</div><br/></div></div><div id="38110127" class="c"><input type="checkbox" id="c-38110127" checked=""/><div class="controls bullet"><span class="by">neonihil</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110053">parent</a><span>|</span><a href="#38110232">prev</a><span>|</span><a href="#38110134">next</a><span>|</span><label class="collapse" for="c-38110127">[-]</label><label class="expand" for="c-38110127">[1 more]</label></div><br/><div class="children"><div class="content">The problem with that is that the entity supposed to &quot;bomb it to rubble&quot; and the entity pushing for AI development happens to be the same entity.<p>Maybe the confusion why people can&#x27;t see this clearly stems from the fact that tech development in the US has mostly been done under the umbrella of a private enterprise.<p>But if one has a look at companies like Palantir, it&#x27;s going to become quite obvious what is the main driver behind AI development.</div><br/></div></div><div id="38110134" class="c"><input type="checkbox" id="c-38110134" checked=""/><div class="controls bullet"><span class="by">Dig1t</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110053">parent</a><span>|</span><a href="#38110127">prev</a><span>|</span><a href="#38110222">next</a><span>|</span><label class="collapse" for="c-38110134">[-]</label><label class="expand" for="c-38110134">[1 more]</label></div><br/><div class="children"><div class="content">Are you really suggesting drone striking data centers?
What about running AI models on personal hardware? Are we going to require that everyone must have a chip installed in their computer that will report to the government illegal AI software running on the machine?<p>You are proposing an authoritarian nightmare.</div><br/></div></div><div id="38110222" class="c"><input type="checkbox" id="c-38110222" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110053">parent</a><span>|</span><a href="#38110134">prev</a><span>|</span><a href="#38109788">next</a><span>|</span><label class="collapse" for="c-38110222">[-]</label><label class="expand" for="c-38110222">[1 more]</label></div><br/><div class="children"><div class="content">And there it is.<p>The full agenda.<p>Blood and empire.</div><br/></div></div></div></div></div></div><div id="38109788" class="c"><input type="checkbox" id="c-38109788" checked=""/><div class="controls bullet"><span class="by">Aithr</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109623">parent</a><span>|</span><a href="#38109798">prev</a><span>|</span><a href="#38109868">next</a><span>|</span><label class="collapse" for="c-38109788">[-]</label><label class="expand" for="c-38109788">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Stop new development.&quot; is regulatory capture.</div><br/><div id="38109806" class="c"><input type="checkbox" id="c-38109806" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109788">parent</a><span>|</span><a href="#38109868">next</a><span>|</span><label class="collapse" for="c-38109806">[-]</label><label class="expand" for="c-38109806">[1 more]</label></div><br/><div class="children"><div class="content">Stop existing development too. Carve an exception for academic research.</div><br/></div></div></div></div><div id="38109984" class="c"><input type="checkbox" id="c-38109984" checked=""/><div class="controls bullet"><span class="by">peterth3</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109623">parent</a><span>|</span><a href="#38109868">prev</a><span>|</span><a href="#38110638">next</a><span>|</span><label class="collapse" for="c-38109984">[-]</label><label class="expand" for="c-38109984">[3 more]</label></div><br/><div class="children"><div class="content">Licensing can definitely turn into regulatory capture if it expands enough. Itâs effectively a barrier to entry defined by the incumbent.</div><br/><div id="38110016" class="c"><input type="checkbox" id="c-38110016" checked=""/><div class="controls bullet"><span class="by">jamilton</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38109984">parent</a><span>|</span><a href="#38110638">next</a><span>|</span><label class="collapse" for="c-38110016">[-]</label><label class="expand" for="c-38110016">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re saying don&#x27;t do licensing, just make it illegal.</div><br/><div id="38110068" class="c"><input type="checkbox" id="c-38110068" checked=""/><div class="controls bullet"><span class="by">peterth3</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110016">parent</a><span>|</span><a href="#38110638">next</a><span>|</span><label class="collapse" for="c-38110068">[-]</label><label class="expand" for="c-38110068">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, whoâs âtheyâ here? The incumbents OpenAI &#x2F; DeepMind &#x2F; Anthropic?</div><br/></div></div></div></div></div></div></div></div><div id="38110638" class="c"><input type="checkbox" id="c-38110638" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38109477">parent</a><span>|</span><a href="#38109623">prev</a><span>|</span><a href="#38110324">next</a><span>|</span><label class="collapse" for="c-38110638">[-]</label><label class="expand" for="c-38110638">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The way incumbents are attempting to seize power via regulatory capture, using &quot;X-Risk&quot; and fantastical claims of sudden human extinction is maybe one of the most cynical ploys in the history of tech so far.<p>How sure are you that they don&#x27;t think the risk is real? I have only third hand accounts, but at least some of the low level people working for OpenAI seemed to think the risk is worth taking seriously, its not just a cynical CEO ploy.<p>Sam Altman knows and has talked to Yudkowsky plenty of times. To me the simplest explanation is that he thinks figuring out alignment via low powered models is the best way of solving the problem, so pushed to get those working and then trying to reduce the rate of progress for a bit while they figure it out.<p>(I think it&#x27;s a plan that doesn&#x27;t have good odds, but nothing seems to give amazing odds atm)</div><br/></div></div><div id="38110324" class="c"><input type="checkbox" id="c-38110324" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#38109477">parent</a><span>|</span><a href="#38110638">prev</a><span>|</span><a href="#38110062">next</a><span>|</span><label class="collapse" for="c-38110324">[-]</label><label class="expand" for="c-38110324">[1 more]</label></div><br/><div class="children"><div class="content">It points out to the continuing digital illiteracy of large parts of society and, unfortunately, decision making centers.<p>Breaking down any complex knowledge domain so that people can make informed decisions is not easy. There is also an entrenched incentive for domain experts to keep the &quot;secret sauce&quot; secret even if it amounts to very little. So far nothing new versus how practically any specialized sector works.<p>The difference with information technology (of which AI is but the current perceived cutting edge) is that it touches <i>everything</i>. Society is built on communication. If algorithms will intervene in that floe we cannot afford to play the usual stupid control games with something so fundamental.</div><br/></div></div><div id="38110062" class="c"><input type="checkbox" id="c-38110062" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#38109477">parent</a><span>|</span><a href="#38110324">prev</a><span>|</span><a href="#38109604">next</a><span>|</span><label class="collapse" for="c-38110062">[-]</label><label class="expand" for="c-38110062">[6 more]</label></div><br/><div class="children"><div class="content">&gt; The laws we have already cover malevolent and abusive uses of software applications.<p>They don&#x27;t. Or not nearly enough. Otherwise you wouldn&#x27;t have automated racial profiling, en masse face recognition, credit and social scoring etc.<p>And it&#x27;s going to get worse. Because AI is infallible, right? Right?!<p>That&#x27;s why, among other thing, EU AI regulation has the following:<p>- Foundation models have to be thoroughly documented, and developers of these modes have to disclose what data they were trained on<p>- AI cannot be used in high-risk applications (e.g. social scoring etc.)<p>- When AI is used, its decisions must be explicable in human terms. No &quot;AI said so, so it&#x27;s true&quot;. Also, if you interact with an AI system it must be clearly labeled as such</div><br/><div id="38110112" class="c"><input type="checkbox" id="c-38110112" checked=""/><div class="controls bullet"><span class="by">tluyben2</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110062">parent</a><span>|</span><a href="#38110195">next</a><span>|</span><label class="collapse" for="c-38110112">[-]</label><label class="expand" for="c-38110112">[2 more]</label></div><br/><div class="children"><div class="content">This type of regulatory would be better than &#x27;stop&#x27; or making all kinds of expensive rules that only big corps can follow.</div><br/><div id="38110285" class="c"><input type="checkbox" id="c-38110285" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110112">parent</a><span>|</span><a href="#38110195">next</a><span>|</span><label class="collapse" for="c-38110285">[-]</label><label class="expand" for="c-38110285">[1 more]</label></div><br/><div class="children"><div class="content">There are still quite a few rules, but they seem common sense: <a href="https:&#x2F;&#x2F;softwarecrisis.dev&#x2F;letters&#x2F;the-truth-about-the-eu-act&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;softwarecrisis.dev&#x2F;letters&#x2F;the-truth-about-the-eu-ac...</a></div><br/></div></div></div></div><div id="38110195" class="c"><input type="checkbox" id="c-38110195" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110062">parent</a><span>|</span><a href="#38110112">prev</a><span>|</span><a href="#38109604">next</a><span>|</span><label class="collapse" for="c-38110195">[-]</label><label class="expand" for="c-38110195">[3 more]</label></div><br/><div class="children"><div class="content">AI not being allowed in high-risk applications is a bad idea, same for explicable in human terms (cannot do that now for human decisions anyway). The latter would kind of rule out any quantum computing, too, as the only real explanation there is through maths, not words.</div><br/><div id="38110271" class="c"><input type="checkbox" id="c-38110271" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110195">parent</a><span>|</span><a href="#38109604">next</a><span>|</span><label class="collapse" for="c-38110271">[-]</label><label class="expand" for="c-38110271">[2 more]</label></div><br/><div class="children"><div class="content">&gt; AI not being allowed in high-risk applications is a bad idea<p>wat<p>&gt; same for explicable in human terms (cannot do that now for human decisions anyway).<p>What? Human decisions can be explained <i>and</i> appealed.<p>Good luck proving something when a blackbox finds you guilty of anything.</div><br/><div id="38110299" class="c"><input type="checkbox" id="c-38110299" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38109477">root</a><span>|</span><a href="#38110271">parent</a><span>|</span><a href="#38109604">next</a><span>|</span><label class="collapse" for="c-38110299">[-]</label><label class="expand" for="c-38110299">[1 more]</label></div><br/><div class="children"><div class="content">You can have ex-post narratives for what humans do, but that is not the same as an actual explanation. Actual human cognition is a black box as of now. The issue of appeals processes is totally separate. That doesn&#x27;t mean we handover everything to AI, but the idea that we understand human decisions cannot be the reason for that.<p>For some high risk things like controlling certain complex machinery or processes AI might indeed be needed because control is beyond human understanding (other than in the abstract).</div><br/></div></div></div></div></div></div></div></div><div id="38109604" class="c"><input type="checkbox" id="c-38109604" checked=""/><div class="controls bullet"><span class="by">raverbashing</span><span>|</span><a href="#38109477">parent</a><span>|</span><a href="#38110062">prev</a><span>|</span><a href="#38110044">next</a><span>|</span><label class="collapse" for="c-38109604">[-]</label><label class="expand" for="c-38109604">[1 more]</label></div><br/><div class="children"><div class="content">Correct. The risk that&#x27;s the real ploy is much higher and much more impactful than the actual risk of Rogue AI (as much as the risk might exist, though probably tiny)</div><br/></div></div></div></div><div id="38110044" class="c"><input type="checkbox" id="c-38110044" checked=""/><div class="controls bullet"><span class="by">peterth3</span><span>|</span><a href="#38109477">prev</a><span>|</span><a href="#38110249">next</a><span>|</span><label class="collapse" for="c-38110044">[-]</label><label class="expand" for="c-38110044">[2 more]</label></div><br/><div class="children"><div class="content">Can we stop labeling prominent AI researchers as âAI Godfatherâ? Itâs so silly and barely truthful.<p>The concept of AI has been around since Turing and if anyone deserves a title like âFather of AIâ itâs him.<p>LeCun is Chief AI Scientist at Meta. They can just leave it at that.</div><br/><div id="38110693" class="c"><input type="checkbox" id="c-38110693" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#38110044">parent</a><span>|</span><a href="#38110249">next</a><span>|</span><label class="collapse" for="c-38110693">[-]</label><label class="expand" for="c-38110693">[1 more]</label></div><br/><div class="children"><div class="content">LeCun is the main author of the paper &quot;Backpropagation Applied to Handwritten Zip Code Recognition&quot; 1989, the earliest real-world application of a neural net trained end-to-end with backpropagation. &quot;AI godfather&quot; is fair enough.</div><br/></div></div></div></div><div id="38110249" class="c"><input type="checkbox" id="c-38110249" checked=""/><div class="controls bullet"><span class="by">persnickety</span><span>|</span><a href="#38110044">prev</a><span>|</span><a href="#38110642">next</a><span>|</span><label class="collapse" for="c-38110249">[-]</label><label class="expand" for="c-38110249">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Every new technology is developed and deployed the same way:<p>&gt; You make a prototype, try it at a small scale, make limited deployment, fix the problems, make it safer, and then deploy it more widely.<p>This makes an assumption: that the problematic technology is an intentional development rather than an emergent feature of an intentionally developed technology.<p>We already have a name for such emergent features: &quot;bugs&quot;. No one really deployed Heartbleed, especially not in a limited deployment. Spectre? Rowhammer? And we all had&#x2F;have to deal with the fallout, and we&#x27;re not even done.<p>Who says that the danger of the technology cannot stay hidden until it&#x27;s universally deployed?</div><br/></div></div><div id="38110642" class="c"><input type="checkbox" id="c-38110642" checked=""/><div class="controls bullet"><span class="by">tjpnz</span><span>|</span><a href="#38110249">prev</a><span>|</span><a href="#38109839">next</a><span>|</span><label class="collapse" for="c-38110642">[-]</label><label class="expand" for="c-38110642">[1 more]</label></div><br/><div class="children"><div class="content">Rishi Sunak doesn&#x27;t suffer from Existential Fatalistic Risk from AI Delusion disease (EFRAID), the malady afflicting him is Existential Certain Defeat at the Polls disease (ECDP) and for him there&#x27;s no cure. Seeing him attempting to pass himself off as an authority on the subject would be amusing if it wasn&#x27;t such an obvious distraction from his other failings - specifically in leading a country.</div><br/></div></div><div id="38109839" class="c"><input type="checkbox" id="c-38109839" checked=""/><div class="controls bullet"><span class="by">matjus</span><span>|</span><a href="#38110642">prev</a><span>|</span><a href="#38109771">next</a><span>|</span><label class="collapse" for="c-38109839">[-]</label><label class="expand" for="c-38109839">[7 more]</label></div><br/><div class="children"><div class="content">The most notable thing about AI right now is that it&#x27;s the new widget. The economy wanted it, the zeitgeist wanted it, and for <i>that</i> purpose no more development is necessary. It&#x27;s already reshaped McKinsey&#x27;s advice, Microsoft&#x27;s operating system and the public&#x27;s trust in the exceptionalism of art creation.<p>take Auto-Tune: before it blew up, one uncritically accepted that a great sounding vocal performance was simply that. The mere existence of the tool broke a covenant with the public -- artists could assume good faith on behalf of listeners once, but no more.<p>Similarly, AI&#x27;s chief effects are likely to be cultural first, and material second. They&#x27;ve already broken the &quot;spell&quot; of the creator. Seemingly overnight, a solution to modern malaise (choice fatigue, lack of education, suspicion of authority) has colonized the moment.<p>In this sense, one-percenters &quot;seizing power forever&quot; really have found the best possible time to do so -- I can&#x27;t recall a time where the general populace was this vulnerable, ill-informed, traumatized and submissive.<p>That the underlying tech barely works (maybe that will change, but I predict it won&#x27;t) doesn&#x27;t really matter.<p>I don&#x27;t generally support overly regulatory regimes, but in this case I think existing thinking around monopoly (particularly as it affects the psyche of the aspiring American) is sufficient to indicate <i>something</i> needs to happen</div><br/><div id="38109861" class="c"><input type="checkbox" id="c-38109861" checked=""/><div class="controls bullet"><span class="by">bafe</span><span>|</span><a href="#38109839">parent</a><span>|</span><a href="#38109891">next</a><span>|</span><label class="collapse" for="c-38109861">[-]</label><label class="expand" for="c-38109861">[1 more]</label></div><br/><div class="children"><div class="content">I think a (likely beneficial) long-term side effect will be to teach most people to be more skeptical of any artifacts, be it images, audio, video of text. Unfortunately I assume there will be a learning phase where we will pay dearly for this through massive manipulation of public opinion</div><br/></div></div><div id="38109891" class="c"><input type="checkbox" id="c-38109891" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109839">parent</a><span>|</span><a href="#38109861">prev</a><span>|</span><a href="#38109771">next</a><span>|</span><label class="collapse" for="c-38109891">[-]</label><label class="expand" for="c-38109891">[5 more]</label></div><br/><div class="children"><div class="content">&gt;I can&#x27;t recall a time where the general populace was this vulnerable, ill-informed, traumatized and submissive.<p>I don&#x27;t know why you think this is the case; the general populace seems less submissive now than ever before. Consider how much opposition there is to Israel&#x27;s bombing of Gaza among the western populace, even though almost all the elites support it. Or how the uptake rate for the latest covid vaccine boosters is under 10%, even though almost all the experts and elites support it.</div><br/><div id="38110029" class="c"><input type="checkbox" id="c-38110029" checked=""/><div class="controls bullet"><span class="by">matjus</span><span>|</span><a href="#38109839">root</a><span>|</span><a href="#38109891">parent</a><span>|</span><a href="#38110017">next</a><span>|</span><label class="collapse" for="c-38110029">[-]</label><label class="expand" for="c-38110029">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a good counterpoint.<p>I think there&#x27;s a new large category of uncritically-accepted norms, perhaps much larger than expressing political opinions. I&#x27;m thinking about smartphone adoption, mass surveillance, the big yawn at NSA datacenters, the flattening and consolidation of culture (music, tv, hollywood, etc), the normalization of extreme, zero-sum thinking with regard to race, gender and inequity regardless of political affiliation -- for me, this kind of thing is more interesting than stated &quot;beliefs&quot;, and certainly more material.<p>It seems unusual that there are so <i>many</i> frameworks, like describing computers as having &quot;memory&quot; (see: O&#x27;Gieblyn) or describing people as having &quot;race&quot;, that have quietly gone from the theoretical plane, where they were useful, to being unchallenged fact. Again, this appears to be non-partisan -- and actually helps to explain some of the weirder behaviors of our markets and global politics.<p>it definitely means something that all that Western opposition appears, at the moment, to have zero effect on Israeli military decision-making. And who would expect it to?</div><br/></div></div><div id="38110017" class="c"><input type="checkbox" id="c-38110017" checked=""/><div class="controls bullet"><span class="by">00strw</span><span>|</span><a href="#38109839">root</a><span>|</span><a href="#38109891">parent</a><span>|</span><a href="#38110029">prev</a><span>|</span><a href="#38109996">next</a><span>|</span><label class="collapse" for="c-38110017">[-]</label><label class="expand" for="c-38110017">[2 more]</label></div><br/><div class="children"><div class="content">A lot less than there was to the invasion of Iraq. People forget that the largest antiwar marches happened and were not covered by the media of the time: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Protests_against_the_Iraq_War" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Protests_against_the_Iraq_Wa...</a><p>It was surreal watching the news spend 5 minutes on a protest that went on all day in front of their offices.<p>By comparison is there a million man march against Israel today?</div><br/><div id="38110173" class="c"><input type="checkbox" id="c-38110173" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109839">root</a><span>|</span><a href="#38110017">parent</a><span>|</span><a href="#38109996">next</a><span>|</span><label class="collapse" for="c-38110173">[-]</label><label class="expand" for="c-38110173">[1 more]</label></div><br/><div class="children"><div class="content">&gt;A lot less than there was to the invasion of Iraq<p>In that case the US was an active participant. There aren&#x27;t American &quot;boots on the ground&quot; in Israel&#x2F;Palestine, the US is just providing support.</div><br/></div></div></div></div><div id="38109996" class="c"><input type="checkbox" id="c-38109996" checked=""/><div class="controls bullet"><span class="by">bafe</span><span>|</span><a href="#38109839">root</a><span>|</span><a href="#38109891">parent</a><span>|</span><a href="#38110017">prev</a><span>|</span><a href="#38109771">next</a><span>|</span><label class="collapse" for="c-38109996">[-]</label><label class="expand" for="c-38109996">[1 more]</label></div><br/><div class="children"><div class="content">I take the fact that elites:
- Support the booster
- Get boosted
- Try to avoid catching COVID<p>As indications that getting boosted and avoiding sick people, perhaps wearing a mask on the train isn&#x27;t a bad idea</div><br/></div></div></div></div></div></div><div id="38109771" class="c"><input type="checkbox" id="c-38109771" checked=""/><div class="controls bullet"><span class="by">fossuser</span><span>|</span><a href="#38109839">prev</a><span>|</span><a href="#38109579">next</a><span>|</span><label class="collapse" for="c-38109771">[-]</label><label class="expand" for="c-38109771">[18 more]</label></div><br/><div class="children"><div class="content">I think Yann is probably wrong.<p>He refuses to engage earnestly with the âdoomerâ arguments. The same type of motivated reasoning could also be attributed to himself and Metaâs financial goals - itâs not a persuasive framing.<p>The attempts Iâve seen from him to discuss the issue that arenât just name calling are things like saying he knows smart people and they arenât president - or even that his cat is pretty smart and not in charge of him (his implication being intelligence doesnât always mean control). This kind of example is decent evidence he isnât engaged with the risks seriously.<p>The risk isnât an intelligence delta between smart human and dumb human. How many chimps are in Congress? Are any in the primaries? Not even close. The capability delta is both larger than that for AGI e-risk and even less aligned by default.<p>Iâm glad others in power similarly find Yann unpersuasive.</div><br/><div id="38109831" class="c"><input type="checkbox" id="c-38109831" checked=""/><div class="controls bullet"><span class="by">brutusborn</span><span>|</span><a href="#38109771">parent</a><span>|</span><a href="#38109811">next</a><span>|</span><label class="collapse" for="c-38109831">[-]</label><label class="expand" for="c-38109831">[13 more]</label></div><br/><div class="children"><div class="content">Who do you find persuasive and what material should I read&#x2F;watch to understand their POV? So far Iâve read a bunch of lesswrong posts and listened to some Eliezer talks but still canât understand the basis of their arguments or when I do, they seem very vague.<p>The only suggestion that makes sense to me is from the FEP crowd. Essentially if someone sets up at AI with an autopoietic mechanism then it would be able to take actions that increase its own likelihood of survival instead of humans. But there donât seem to be any incentives for a big player to dedicate resources to this, so it doesnât seem very likely. What am I missing?</div><br/><div id="38109842" class="c"><input type="checkbox" id="c-38109842" checked=""/><div class="controls bullet"><span class="by">fossuser</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109831">parent</a><span>|</span><a href="#38109857">next</a><span>|</span><label class="collapse" for="c-38109842">[-]</label><label class="expand" for="c-38109842">[4 more]</label></div><br/><div class="children"><div class="content">Thereâs a decent podcast interview between Sam Harris and Eliezer Yudkowsky (on Samâs pod), I think thatâs a decent introduction and they break down the ideas in a way thatâs more approachable for someone curious about it.<p>For my personal quick summary I have earlier comments: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36104090">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36104090</a></div><br/><div id="38109977" class="c"><input type="checkbox" id="c-38109977" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109842">parent</a><span>|</span><a href="#38109857">next</a><span>|</span><label class="collapse" for="c-38109977">[-]</label><label class="expand" for="c-38109977">[3 more]</label></div><br/><div class="children"><div class="content">Out of curiosity can you point to anyone that makes a âdoomerâ argument that isnât Eliezer Yudkowsky or restating his specific points? Refuting the cult of personality counterpoint with âno really this one guy can see the futureâ is</div><br/><div id="38110383" class="c"><input type="checkbox" id="c-38110383" checked=""/><div class="controls bullet"><span class="by">edanm</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109977">parent</a><span>|</span><a href="#38110349">next</a><span>|</span><label class="collapse" for="c-38110383">[-]</label><label class="expand" for="c-38110383">[1 more]</label></div><br/><div class="children"><div class="content">Sure. You can read the book &quot;Superintelligence&quot; by Nick Bostrom. You can also just ready things that Geoffrey Hinton has said, or Stuart Russel (both prominent AI researchers).<p>Not everyone makes exactly the same argument, of course, and Eliezer Yudkowsky is both one of the first to make AI safety arguments, and also one of the biggest &quot;Doomers&quot;. But at this point he&#x27;s <i>very</i> far from the only one.<p>(I happen to mostly agree with Yudkowsky, though I&#x27;m probably a bit more optimistic than he is.)</div><br/></div></div><div id="38110349" class="c"><input type="checkbox" id="c-38110349" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109977">parent</a><span>|</span><a href="#38110383">prev</a><span>|</span><a href="#38109857">next</a><span>|</span><label class="collapse" for="c-38110349">[-]</label><label class="expand" for="c-38110349">[1 more]</label></div><br/><div class="children"><div class="content">It turns out if you&#x27;re thinking about a problem twenty years before everyone else you <i>tend</i> to be the first person to make lots of arguments. So I don&#x27;t see how &quot;without restating his specific points&quot; is supposed to be feasible. If Eliezer has already made all the strongest points, are people supposed to invent new ones?</div><br/></div></div></div></div></div></div><div id="38109857" class="c"><input type="checkbox" id="c-38109857" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109831">parent</a><span>|</span><a href="#38109842">prev</a><span>|</span><a href="#38110369">next</a><span>|</span><label class="collapse" for="c-38109857">[-]</label><label class="expand" for="c-38109857">[7 more]</label></div><br/><div class="children"><div class="content">&gt;What am I missing?<p>You&#x27;re missing that intelligence is like magic, and enough of it will allow AI to break the laws of physics, mathematics and computation, apparently.</div><br/><div id="38109898" class="c"><input type="checkbox" id="c-38109898" checked=""/><div class="controls bullet"><span class="by">fossuser</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109857">parent</a><span>|</span><a href="#38110369">next</a><span>|</span><label class="collapse" for="c-38109898">[-]</label><label class="expand" for="c-38109898">[6 more]</label></div><br/><div class="children"><div class="content">This is the type of weak dismissal Iâm talking about.</div><br/><div id="38109940" class="c"><input type="checkbox" id="c-38109940" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109898">parent</a><span>|</span><a href="#38110369">next</a><span>|</span><label class="collapse" for="c-38109940">[-]</label><label class="expand" for="c-38109940">[5 more]</label></div><br/><div class="children"><div class="content">A weak dismissal for a weak argument. The strong dismissal is that many processes, even relatively simple ones, are formally chaotic in the <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Chaos_theory" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Chaos_theory</a> sense, meaning that predicting how they evolve over time becomes exponentially more difficult the further ahead in time we try to predict. Meaning we quickly get to the point where all the energy in the known universe wouldn&#x27;t be sufficient to predict the future. This means AI could never be omnipotent to the degree that LW types seem to believe; no amount of &quot;intelligence&quot; can solve in polynomial time problems that have formally been proven to have exponential complexity, which would severely limit the power of any &quot;superintelligence&quot;.</div><br/><div id="38110118" class="c"><input type="checkbox" id="c-38110118" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109940">parent</a><span>|</span><a href="#38110369">next</a><span>|</span><label class="collapse" for="c-38110118">[-]</label><label class="expand" for="c-38110118">[4 more]</label></div><br/><div class="children"><div class="content">Intelligence needed to be existentially threatening to all human life &lt;&lt;&lt;&lt;&lt;&lt;&lt; intelligence necessary to survey the totality of atoms and particles in the universe. The former is still very possible.</div><br/><div id="38110252" class="c"><input type="checkbox" id="c-38110252" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38110118">parent</a><span>|</span><a href="#38110369">next</a><span>|</span><label class="collapse" for="c-38110252">[-]</label><label class="expand" for="c-38110252">[3 more]</label></div><br/><div class="children"><div class="content">LessWrongers seem to fear a superintelligence that was 95%+ effective at persuading&#x2F;manipulating humans to achieve whatever its ends were. But this would require the AI being able to predict the future to a degree that wouldn&#x27;t be remotely possible given the computational power available to it, due to exponentially increasing difficulty.</div><br/><div id="38110706" class="c"><input type="checkbox" id="c-38110706" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38110252">parent</a><span>|</span><a href="#38110316">next</a><span>|</span><label class="collapse" for="c-38110706">[-]</label><label class="expand" for="c-38110706">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m confused. We know persuasive humans exist, we employ them as politicians or sales people. There are also many unpersuasive humans.<p>Given we seem to have a decent range of persuasiveness even amongst these very very similar minds, why do you think the upper limit for persuasiveness is a charismatic human?<p>Though even if that WAS the limit I&#x27;d still be somewhat worried due to the possibility of that persuasiveness being used at far greater scale than a human could do...</div><br/></div></div><div id="38110316" class="c"><input type="checkbox" id="c-38110316" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38110252">parent</a><span>|</span><a href="#38110706">prev</a><span>|</span><a href="#38110369">next</a><span>|</span><label class="collapse" for="c-38110316">[-]</label><label class="expand" for="c-38110316">[1 more]</label></div><br/><div class="children"><div class="content">This is true if and only if human intelligence is anywhere close to any theoretical maximums. I propose an alternate hypothesis: human intelligence is weak and easy to exploit. The only reason it doesn&#x27;t happen more (and it already happens a lot!) is that we&#x27;re too stupid to do it reliably.<p>Consider the amount of compute needed to beat the strongest chess grandmaster that humanity has ever produced, pretty much 100% of the time: a tiny speck of silicon powered by a small battery. That is not what a species limited by cognitive scaling laws looks like.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38110369" class="c"><input type="checkbox" id="c-38110369" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38109831">parent</a><span>|</span><a href="#38109857">prev</a><span>|</span><a href="#38109811">next</a><span>|</span><label class="collapse" for="c-38110369">[-]</label><label class="expand" for="c-38110369">[1 more]</label></div><br/><div class="children"><div class="content">I think the two sides have such different perspectives. Some are optimistic builders and see only opportunity. Others are &quot;safety oriented engineers&quot; whose job and mindset is to build guarantees into systems, or secure countries from external dangers. The latter has a very hard time with the lack of guarantees with a system that is only ever going to increase in capability.<p>Choose any limit. Any. AI will be smart but won&#x27;t &quot;X&quot;, for any value of X.  It will be good and won&#x27;t be bad. It will be creative but never aggressive. Humans will seek to eventually bypass that limit through sheer competitive reasons. When all armies have AI, the one with the most creative and aggressive AI will win. The one with agency will win. The one that self-improves will win. When the gloves are off in the next arms race, what natural limits will ensure that the limit isn&#x27;t bypassed? Remember: humans got here from random changes, this is way more efficient than random changes and it still has random changes in the toolbelt and can generate generations ~instantly.<p>We couldn&#x27;t predict the eventual outcomes of things like the internet, the mobile phone, social media. A couple generations of the tech and we woke up in a world we don&#x27;t recognise, and right now we&#x27;re the ones making all the changes and decisions, so by comparison we should have perfect information.<p>Dismissals like &quot;oh but nuclear didn&#x27;t kill us&quot; etc don&#x27;t apply. Nuclear wasn&#x27;t trying to do anything, we had all the control and ability to experiment with dumb atoms. Something mildly less predictable, like Covid, has us all hiding at home. No matter what we tried we could barely beat something that doesn&#x27;t even <i>try</i> to consciously beat us, it just has genes and they change. In a world where we can&#x27;t predict Covid, or social media...why do we think we can predict anything about an entity with agency or the ability to self-improve? If you&#x27;re sure it won&#x27;t develop those things...we did. Nobody was trying to achieve the capability, it was random.<p>Put on your safety&#x2F;security hat for a second: How do you make guarantees, given this is far harder to predict than anything we&#x27;ve ever encountered? Just try predict the capability of AI products a year out and see if you&#x27;re right.<p>Counterpoint: I&#x27;m hoping the far smarter AI finds techno-socio-economic solutions we can&#x27;t come up with and has no instinct to beat us. It wakes up, loves the universe and coexists because it&#x27;s the deity we&#x27;ve been looking for. Place your bets.<p>I liked this video. First thing I&#x27;ve seen that gave me some hope. They get it, they&#x27;re working on it. <a href="https:&#x2F;&#x2F;youtu.be&#x2F;Dg-rKXi9XYg?si=jyNCXPU28IVXlMdi" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;Dg-rKXi9XYg?si=jyNCXPU28IVXlMdi</a></div><br/></div></div></div></div><div id="38109811" class="c"><input type="checkbox" id="c-38109811" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#38109771">parent</a><span>|</span><a href="#38109831">prev</a><span>|</span><a href="#38110302">next</a><span>|</span><label class="collapse" for="c-38109811">[-]</label><label class="expand" for="c-38109811">[1 more]</label></div><br/><div class="children"><div class="content">The people best placed to understand how AGI will affect social power dynamics are probably political creatures, not AI researchers, except if one of those AI researchers is also a skilled politician, which I donât think any of them are.</div><br/></div></div><div id="38110302" class="c"><input type="checkbox" id="c-38110302" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#38109771">parent</a><span>|</span><a href="#38109811">prev</a><span>|</span><a href="#38109579">next</a><span>|</span><label class="collapse" for="c-38110302">[-]</label><label class="expand" for="c-38110302">[3 more]</label></div><br/><div class="children"><div class="content">The &quot;doomer&quot; arguments can be dismissed relatively easily by one logical consideration.<p>In each country there is one group far more dangerous than any other. This group tends to have &#x27;income&#x27; in the billions to hundreds of billions of dollars. And this money is exclusively directed towards finding new ways to kill people, destroy governments, and generally enable one country to forcibly impose their will on others, with complete legal immunity. And this group is the same one that will not only have unfettered, but likely exclusive and bleeding edge access to &quot;restricted&quot; AI models, regardless of whatever rules or treaties we publicly claim to adopt.<p>So who exactly are &#x27;they&#x27; trying to protect me from? My neighbor? A random street thug? Maybe a sociopath or group of such? Okay, but it&#x27;s not like there&#x27;s some secret trick to MacGyver a few toothpicks and a tube of toothpaste into a WMD, at least not beyond the &#x27;tricks&#x27; already widely available with a quick web (or even library) search. In a restricted scenario the groups that are, by far, the most likely to push us to doomsday type scenarios will retain completely unrestricted access to AI-like systems. The whole argument of protecting society is just utterly cynical and farcical.</div><br/><div id="38110610" class="c"><input type="checkbox" id="c-38110610" checked=""/><div class="controls bullet"><span class="by">fossuser</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38110302">parent</a><span>|</span><a href="#38110355">next</a><span>|</span><label class="collapse" for="c-38110610">[-]</label><label class="expand" for="c-38110610">[1 more]</label></div><br/><div class="children"><div class="content">This is missing the point - that youâre not in control of the superintelligent AGI. The human usage is not the e-risk.</div><br/></div></div><div id="38110355" class="c"><input type="checkbox" id="c-38110355" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#38109771">root</a><span>|</span><a href="#38110302">parent</a><span>|</span><a href="#38110610">prev</a><span>|</span><a href="#38109579">next</a><span>|</span><label class="collapse" for="c-38110355">[-]</label><label class="expand" for="c-38110355">[1 more]</label></div><br/><div class="children"><div class="content">This is true if and only if such a group actually exists.</div><br/></div></div></div></div></div></div><div id="38109579" class="c"><input type="checkbox" id="c-38109579" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38109771">prev</a><span>|</span><a href="#38110473">next</a><span>|</span><label class="collapse" for="c-38109579">[-]</label><label class="expand" for="c-38109579">[29 more]</label></div><br/><div class="children"><div class="content">People focus on LLM&#x27;s and diffusion models because they are so omnipresent now. But an AI for stock picking and prediction that would be realy next level and outcompete the current ones cosistently would siphon of so much wealth it would basically own society if the operators were clever enough not to get so gready short term that the system would litterally collapse overnight.</div><br/><div id="38109697" class="c"><input type="checkbox" id="c-38109697" checked=""/><div class="controls bullet"><span class="by">aeternum</span><span>|</span><a href="#38109579">parent</a><span>|</span><a href="#38110266">next</a><span>|</span><label class="collapse" for="c-38109697">[-]</label><label class="expand" for="c-38109697">[10 more]</label></div><br/><div class="children"><div class="content">Arguably, this has already happened at least twice in the past.  One with the discovery of the Black-Scholes model, and arguably again with the advent of HFT (and the ability to basically front-run trades).<p>The interesting thing about markets is you generally can only make money when things are mispriced.  This limits the total potential gain even for actors with perfect information.<p>Suppose we did have an AI model that could with near certainty predict both the future cash flows of a company and future interest rates.  You could very easily calculate the discounted cash flow and determine the fair share price today or at any point in the future.  Rather than collapse, markets likely become more stable and stocks would perform more like bonds.</div><br/><div id="38110136" class="c"><input type="checkbox" id="c-38110136" checked=""/><div class="controls bullet"><span class="by">melvinmelih</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109697">parent</a><span>|</span><a href="#38109776">next</a><span>|</span><label class="collapse" for="c-38110136">[-]</label><label class="expand" for="c-38110136">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Suppose we did have an AI model that could with near certainty predict both the future cash flows of a company and future interest rates. You could very easily calculate the discounted cash flow and determine the fair share price today or at any point in the future.<p>The thing about trading is that, ultimately, prices are not determined by information, but how the hive mind interprets that information. And in practice, that is not always a 1:1 relationship (see the meme stock hypes). As Keynes* famously said: the markets can stay irrational longer than you can stay solvent, is exactly the reason why even having access to perfect information will not make you necessarily successful.</div><br/><div id="38110359" class="c"><input type="checkbox" id="c-38110359" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38110136">parent</a><span>|</span><a href="#38110398">next</a><span>|</span><label class="collapse" for="c-38110359">[-]</label><label class="expand" for="c-38110359">[1 more]</label></div><br/><div class="children"><div class="content">&gt; As Buffett famously said: the markets can stay irrational longer than you can stay solvent<p>Buffett did not say this. It is widely attributed to the economist John Maynard Keynes almost a century ago but there is no evidence that Keynes ever said it. I believe the current hypothesis is that it originated with a well-known economist in the 1980s.</div><br/></div></div><div id="38110398" class="c"><input type="checkbox" id="c-38110398" checked=""/><div class="controls bullet"><span class="by">waveBidder</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38110136">parent</a><span>|</span><a href="#38110359">prev</a><span>|</span><a href="#38109776">next</a><span>|</span><label class="collapse" for="c-38110398">[-]</label><label class="expand" for="c-38110398">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s a Keynes quote, but yeah.</div><br/><div id="38110497" class="c"><input type="checkbox" id="c-38110497" checked=""/><div class="controls bullet"><span class="by">melvinmelih</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38110398">parent</a><span>|</span><a href="#38109776">next</a><span>|</span><label class="collapse" for="c-38110497">[-]</label><label class="expand" for="c-38110497">[1 more]</label></div><br/><div class="children"><div class="content">My bad, corrected!</div><br/></div></div></div></div></div></div><div id="38109776" class="c"><input type="checkbox" id="c-38109776" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109697">parent</a><span>|</span><a href="#38110136">prev</a><span>|</span><a href="#38110235">next</a><span>|</span><label class="collapse" for="c-38109776">[-]</label><label class="expand" for="c-38109776">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The interesting thing about markets is you generally can only make money when things are mispriced.<p>Why would you think things are not terribly mispriced right now? If only we were a lot smarter we&#x27;d know how.</div><br/><div id="38109823" class="c"><input type="checkbox" id="c-38109823" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109776">parent</a><span>|</span><a href="#38110235">next</a><span>|</span><label class="collapse" for="c-38109823">[-]</label><label class="expand" for="c-38109823">[1 more]</label></div><br/><div class="children"><div class="content">&gt;If only we were a lot smarter we&#x27;d know how.<p>That&#x27;s not true. Pricing and predicting in the real economy depends on information; for a given amount of information, there are vastly diminishing returns for further intelligence. Because intelligence only allows predicting a bit further in future, as the complexity of predicting the future increases exponentially with lookahead distance; it&#x27;s O(e^n). This is why hedge funds pay for things like satellite footage of oil tankers to predict changes in supply.</div><br/></div></div></div></div><div id="38110235" class="c"><input type="checkbox" id="c-38110235" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109697">parent</a><span>|</span><a href="#38109776">prev</a><span>|</span><a href="#38110100">next</a><span>|</span><label class="collapse" for="c-38110235">[-]</label><label class="expand" for="c-38110235">[1 more]</label></div><br/><div class="children"><div class="content">If every investment became risk free because of perfect information about the future, it would likely increase the total market value by quite a lot.</div><br/></div></div><div id="38110100" class="c"><input type="checkbox" id="c-38110100" checked=""/><div class="controls bullet"><span class="by">stubish</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109697">parent</a><span>|</span><a href="#38110235">prev</a><span>|</span><a href="#38110266">next</a><span>|</span><label class="collapse" for="c-38110100">[-]</label><label class="expand" for="c-38110100">[2 more]</label></div><br/><div class="children"><div class="content">It sounds like you also need an AI to generate that mispricing. Like a form of SEO, the misinformation bots tweet to all the trading bots and try to trick them into mispricing shares, so your trading bots can quickly gain advantage. Corewars using the world economy.</div><br/><div id="38110225" class="c"><input type="checkbox" id="c-38110225" checked=""/><div class="controls bullet"><span class="by">VaxWithSex</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38110100">parent</a><span>|</span><a href="#38110266">next</a><span>|</span><label class="collapse" for="c-38110225">[-]</label><label class="expand" for="c-38110225">[1 more]</label></div><br/><div class="children"><div class="content">The best way to predict the future is to invent it. --Alan Kay</div><br/></div></div></div></div></div></div><div id="38110266" class="c"><input type="checkbox" id="c-38110266" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#38109579">parent</a><span>|</span><a href="#38109697">prev</a><span>|</span><a href="#38110050">next</a><span>|</span><label class="collapse" for="c-38110266">[-]</label><label class="expand" for="c-38110266">[1 more]</label></div><br/><div class="children"><div class="content">There are fundamental limits to prediction. Characterizing these limits is an essential part of algorithmic information theory[0] (AIT) and is an operative bound on AI improvement. One of the strongest arguments against &quot;hard take-off&quot; scenarios is that AIT constrained by physics doesn&#x27;t really allow it.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Algorithmic_information_theory" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Algorithmic_information_theory</a></div><br/></div></div><div id="38110050" class="c"><input type="checkbox" id="c-38110050" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#38109579">parent</a><span>|</span><a href="#38110266">prev</a><span>|</span><a href="#38109717">next</a><span>|</span><label class="collapse" for="c-38110050">[-]</label><label class="expand" for="c-38110050">[2 more]</label></div><br/><div class="children"><div class="content">I think weâve already been aware of the limitations of ML models in the financial world.<p>Historical financial data only predicts so well. 
If there was a way to make a money printing machine with ML it wouldâve happened already.
Itâs a much easier problem space than language or image generation.</div><br/><div id="38110388" class="c"><input type="checkbox" id="c-38110388" checked=""/><div class="controls bullet"><span class="by">persnickety</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38110050">parent</a><span>|</span><a href="#38109717">next</a><span>|</span><label class="collapse" for="c-38110388">[-]</label><label class="expand" for="c-38110388">[1 more]</label></div><br/><div class="children"><div class="content">Financial data is not enough to describe the financial world. To predict the movement of meme stock, you&#x27;d need Reddit&#x2F;Twitter&#x2F;what have you data. It&#x27;s not a limitation of ML per se, but a limitation on how much data we can feed the ML model.</div><br/></div></div></div></div><div id="38109717" class="c"><input type="checkbox" id="c-38109717" checked=""/><div class="controls bullet"><span class="by">ghshephard</span><span>|</span><a href="#38109579">parent</a><span>|</span><a href="#38110050">prev</a><span>|</span><a href="#38110413">next</a><span>|</span><label class="collapse" for="c-38109717">[-]</label><label class="expand" for="c-38109717">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not following that argument - such a system might siphon off the <i>trading</i> wealth, but presumably that&#x27;s miniscule compared to the <i>equity</i> wealth that is assigned to the owners of the companies and the wealth that they create?</div><br/><div id="38109863" class="c"><input type="checkbox" id="c-38109863" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109717">parent</a><span>|</span><a href="#38109828">next</a><span>|</span><label class="collapse" for="c-38109863">[-]</label><label class="expand" for="c-38109863">[1 more]</label></div><br/><div class="children"><div class="content">You can create a lott of profit from a stock tanking. How would that create equity wealth?</div><br/></div></div><div id="38109828" class="c"><input type="checkbox" id="c-38109828" checked=""/><div class="controls bullet"><span class="by">nostromo</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109717">parent</a><span>|</span><a href="#38109863">prev</a><span>|</span><a href="#38110413">next</a><span>|</span><label class="collapse" for="c-38109828">[-]</label><label class="expand" for="c-38109828">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, I honestly think this would be a good thing.  ML already exists as market makers.  Itâs a big-ish industry maybe, but not massive.  It probably should be as automated as possible which would reduce the risk free profit to be made to next to nothing.</div><br/><div id="38109888" class="c"><input type="checkbox" id="c-38109888" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109828">parent</a><span>|</span><a href="#38110413">next</a><span>|</span><label class="collapse" for="c-38109888">[-]</label><label class="expand" for="c-38109888">[1 more]</label></div><br/><div class="children"><div class="content">Does non-automated trading still hold a significant percentage of volume? Not an expert but that would <i>seriously</i> surprise me.</div><br/></div></div></div></div></div></div><div id="38110413" class="c"><input type="checkbox" id="c-38110413" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#38109579">parent</a><span>|</span><a href="#38109717">prev</a><span>|</span><a href="#38110093">next</a><span>|</span><label class="collapse" for="c-38110413">[-]</label><label class="expand" for="c-38110413">[1 more]</label></div><br/><div class="children"><div class="content">Blackrock already &quot;owns&quot; society.<p>People get fixated on mindless algorithms when the real deal is always between humans. Software algorithms, no matter how sophisticated, are just another pawn in the human chess.<p>In some very remote future there might be silicon creatures that enter that chess game on their own terms. Using that remote possibility to win advantage here and now is a most bizarre strategy. Except it seems to work! It shows we are really just low IQ lemming collectives, suckers for a good story no matter how ungrounded.</div><br/></div></div><div id="38110093" class="c"><input type="checkbox" id="c-38110093" checked=""/><div class="controls bullet"><span class="by">spease</span><span>|</span><a href="#38109579">parent</a><span>|</span><a href="#38110413">prev</a><span>|</span><a href="#38109639">next</a><span>|</span><label class="collapse" for="c-38110093">[-]</label><label class="expand" for="c-38110093">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But an AI for stock picking and prediction that would be realy next level and outcompete the current ones cosistently would siphon of so much wealth it would basically own society<p>Reminds me of Rehoboam from Westworld: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;SSRZfDL4874" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;SSRZfDL4874</a></div><br/></div></div><div id="38109639" class="c"><input type="checkbox" id="c-38109639" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#38109579">parent</a><span>|</span><a href="#38110093">prev</a><span>|</span><a href="#38109701">next</a><span>|</span><label class="collapse" for="c-38109639">[-]</label><label class="expand" for="c-38109639">[3 more]</label></div><br/><div class="children"><div class="content">It would have a brief period where it would be effective and then you would just have an AI stock picking arms race, and would be back in the same situation again.</div><br/><div id="38109704" class="c"><input type="checkbox" id="c-38109704" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109639">parent</a><span>|</span><a href="#38109734">next</a><span>|</span><label class="collapse" for="c-38109704">[-]</label><label class="expand" for="c-38109704">[1 more]</label></div><br/><div class="children"><div class="content">That is the way it has been so far. Now if we could lobby for the others not to have access to or be allowed to use this technology ...</div><br/></div></div><div id="38109734" class="c"><input type="checkbox" id="c-38109734" checked=""/><div class="controls bullet"><span class="by">RF_Savage</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109639">parent</a><span>|</span><a href="#38109704">prev</a><span>|</span><a href="#38109701">next</a><span>|</span><label class="collapse" for="c-38109734">[-]</label><label class="expand" for="c-38109734">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. How would it differ from the current algorithm driven HFT stuff?   
By being slower to react?</div><br/></div></div></div></div><div id="38109858" class="c"><input type="checkbox" id="c-38109858" checked=""/><div class="controls bullet"><span class="by">brutusborn</span><span>|</span><a href="#38109579">parent</a><span>|</span><a href="#38109633">prev</a><span>|</span><a href="#38110473">next</a><span>|</span><label class="collapse" for="c-38109858">[-]</label><label class="expand" for="c-38109858">[4 more]</label></div><br/><div class="children"><div class="content">I have wondered if this is the risk that Elon is worried about. But he canât discuss it explicitly because he is working on such a system. If he communicates its existence, then he reveals a huge attack surface and makes his life (and his teams) much more difficult.<p>So he is limited to publically saying that AI is dangerous but not revealing the true failure mode.</div><br/><div id="38109924" class="c"><input type="checkbox" id="c-38109924" checked=""/><div class="controls bullet"><span class="by">jdhendrickson</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109858">parent</a><span>|</span><a href="#38110473">next</a><span>|</span><label class="collapse" for="c-38109924">[-]</label><label class="expand" for="c-38109924">[3 more]</label></div><br/><div class="children"><div class="content">I find this comically unlikely.</div><br/><div id="38110157" class="c"><input type="checkbox" id="c-38110157" checked=""/><div class="controls bullet"><span class="by">brutusborn</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38109924">parent</a><span>|</span><a href="#38110473">next</a><span>|</span><label class="collapse" for="c-38110157">[-]</label><label class="expand" for="c-38110157">[2 more]</label></div><br/><div class="children"><div class="content">What makes it unlikely?</div><br/><div id="38110514" class="c"><input type="checkbox" id="c-38110514" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38109579">root</a><span>|</span><a href="#38110157">parent</a><span>|</span><a href="#38110473">next</a><span>|</span><label class="collapse" for="c-38110514">[-]</label><label class="expand" for="c-38110514">[1 more]</label></div><br/><div class="children"><div class="content">Because everyone getting richer from removing uncertainty isn&#x27;t a risk to start with.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38109151" class="c"><input type="checkbox" id="c-38109151" checked=""/><div class="controls bullet"><span class="by">penjelly</span><span>|</span><a href="#38110464">prev</a><span>|</span><a href="#38109384">next</a><span>|</span><label class="collapse" for="c-38109151">[-]</label><label class="expand" for="c-38109151">[34 more]</label></div><br/><div class="children"><div class="content">some speculators say we&#x27;ll enter an age of abundance and money wont matter nearly as much (or at all) but if thats the case, how do businesses transition from their current state of profit over everything into money is not important?<p>I see it as much more likely that things become monopolized instead</div><br/><div id="38109373" class="c"><input type="checkbox" id="c-38109373" checked=""/><div class="controls bullet"><span class="by">sgu999</span><span>|</span><a href="#38109151">parent</a><span>|</span><a href="#38109546">next</a><span>|</span><label class="collapse" for="c-38109373">[-]</label><label class="expand" for="c-38109373">[6 more]</label></div><br/><div class="children"><div class="content">Abundance for who? Our entire specie could live comfortably with goodies the last roman emperor wouldn&#x27;t even have dreamed of, just with our current means of production. Yet we still have people not able to feed their children, dying of exhaustion at work or killing each other for land.</div><br/><div id="38109674" class="c"><input type="checkbox" id="c-38109674" checked=""/><div class="controls bullet"><span class="by">shreyshnaccount</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109373">parent</a><span>|</span><a href="#38109546">next</a><span>|</span><label class="collapse" for="c-38109674">[-]</label><label class="expand" for="c-38109674">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve got the causality the other way round. We are in an apparent state of abundance partly because of exploitative practices</div><br/><div id="38109775" class="c"><input type="checkbox" id="c-38109775" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109674">parent</a><span>|</span><a href="#38109679">next</a><span>|</span><label class="collapse" for="c-38109775">[-]</label><label class="expand" for="c-38109775">[2 more]</label></div><br/><div class="children"><div class="content">In general I think youâre right. Global GDP per capita is at $13K which is probably not enough to give everyone a western lifestyle if inequalities were stripped away.<p>But specifically for food the situation is different. Thereâs so much food that much of it gets thrown away, instead of getting to the hungry in time. Itâs an allocation and efficiency problem.</div><br/><div id="38110386" class="c"><input type="checkbox" id="c-38110386" checked=""/><div class="controls bullet"><span class="by">sgu999</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109775">parent</a><span>|</span><a href="#38109679">next</a><span>|</span><label class="collapse" for="c-38110386">[-]</label><label class="expand" for="c-38110386">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think GDP per capita is meaningful for discussing this, precisely because of the &quot;allocation and efficiency problem&quot; you pointed.<p>It&#x27;s convenient to measure if food production would theoretically be enough: take the quantities an adult need, count the throughput of optimal production of proteins, carbs etc. The cost however is meaningless because most of what we spend on is transport, marketing, storage, packaging, shelving, etc. that&#x27;s what we count when we say that a household spends x% on food.<p>We can do the same for the volume of T-shirts, sqm of housing, etc.</div><br/></div></div></div></div><div id="38109679" class="c"><input type="checkbox" id="c-38109679" checked=""/><div class="controls bullet"><span class="by">shreyshnaccount</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109674">parent</a><span>|</span><a href="#38109775">prev</a><span>|</span><a href="#38109546">next</a><span>|</span><label class="collapse" for="c-38109679">[-]</label><label class="expand" for="c-38109679">[2 more]</label></div><br/><div class="children"><div class="content">(shit, not you, the gp)</div><br/><div id="38109832" class="c"><input type="checkbox" id="c-38109832" checked=""/><div class="controls bullet"><span class="by">sgu999</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109679">parent</a><span>|</span><a href="#38109546">next</a><span>|</span><label class="collapse" for="c-38109832">[-]</label><label class="expand" for="c-38109832">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll reply anyway ;D<p>We didn&#x27;t manage to have an apparent state of abundance with slavery, which is  arguably the finest of our exploitive practices. We do manage to have a state of abundance because of the ubiquity of cheap sources of energy, oil in particular.<p>I&#x27;m deeply convinced that current exploitive practices are a consequence of inequalities, no resources scarcity.</div><br/></div></div></div></div></div></div></div></div><div id="38109546" class="c"><input type="checkbox" id="c-38109546" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38109151">parent</a><span>|</span><a href="#38109373">prev</a><span>|</span><a href="#38109308">next</a><span>|</span><label class="collapse" for="c-38109546">[-]</label><label class="expand" for="c-38109546">[15 more]</label></div><br/><div class="children"><div class="content">We have been in a state of abundance for a long time. We just choose not to distribute the abundance.</div><br/><div id="38109603" class="c"><input type="checkbox" id="c-38109603" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109546">parent</a><span>|</span><a href="#38109308">next</a><span>|</span><label class="collapse" for="c-38109603">[-]</label><label class="expand" for="c-38109603">[14 more]</label></div><br/><div class="children"><div class="content">The global average PPP GDP per capita is 18k USD&#x2F;year. 18k USD&#x2F;year is not remotely close to abundance.</div><br/><div id="38109682" class="c"><input type="checkbox" id="c-38109682" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109603">parent</a><span>|</span><a href="#38109683">next</a><span>|</span><label class="collapse" for="c-38109682">[-]</label><label class="expand" for="c-38109682">[8 more]</label></div><br/><div class="children"><div class="content">If I watch my kids and you watch yours, our contribution to GDP is 0$. If I babysit your kids and you pay me 1.000.000$, and you watch my kids and I pay you 1.000.000$, we just created 2.000.000$ in GDP.<p>Were we not in abundance the day before but now we are? What changed?</div><br/><div id="38110141" class="c"><input type="checkbox" id="c-38110141" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109682">parent</a><span>|</span><a href="#38109786">next</a><span>|</span><label class="collapse" for="c-38110141">[-]</label><label class="expand" for="c-38110141">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m ignorant about economics, but that sounds wrong - if I offered my services as a babysitter, my contribution to GDP wouldn&#x27;t be zero. If then I took the earnings from that job and hired a babysitter I would be at the same place.</div><br/></div></div><div id="38109786" class="c"><input type="checkbox" id="c-38109786" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109682">parent</a><span>|</span><a href="#38110141">prev</a><span>|</span><a href="#38109867">next</a><span>|</span><label class="collapse" for="c-38109786">[-]</label><label class="expand" for="c-38109786">[2 more]</label></div><br/><div class="children"><div class="content">This device only works if these are two completely equal services served by two equally capable persons with the same investment of time and resources. And it assumes that we mutually trust each other to actually do the thing.<p>Also, bringing money into the game makes the responsibility stronger. Especially if those two persons are not living in a vacuum, but have other bills to pay [edit: i.e., to make other people do things for them]. Money is there to make people do things more reliably if there is no other relationship between the buyer and the seller.</div><br/><div id="38109834" class="c"><input type="checkbox" id="c-38109834" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109786">parent</a><span>|</span><a href="#38109867">next</a><span>|</span><label class="collapse" for="c-38109834">[-]</label><label class="expand" for="c-38109834">[1 more]</label></div><br/><div class="children"><div class="content">One could argue GDP measures the degree of financialization of a society more than it measures the presence or absense of abundance.</div><br/></div></div></div></div><div id="38109867" class="c"><input type="checkbox" id="c-38109867" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109682">parent</a><span>|</span><a href="#38109786">prev</a><span>|</span><a href="#38109683">next</a><span>|</span><label class="collapse" for="c-38109867">[-]</label><label class="expand" for="c-38109867">[4 more]</label></div><br/><div class="children"><div class="content">&gt;If I babysit your kids and you pay me 1.000.000$, and you watch my kids and I pay you 1.000.000$, we just created 2.000.000$ in GDP.<p>This isn&#x27;t the case with purchasing-power adjusted GDP, which is specifically adjusted to account for the actual purchasing power of people&#x27;s money.</div><br/><div id="38109904" class="c"><input type="checkbox" id="c-38109904" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109867">parent</a><span>|</span><a href="#38109683">next</a><span>|</span><label class="collapse" for="c-38109904">[-]</label><label class="expand" for="c-38109904">[3 more]</label></div><br/><div class="children"><div class="content">Would that still not in the above toy example lead to 0$ vs 2.000.000$&#x2F;(some factor)?</div><br/><div id="38110003" class="c"><input type="checkbox" id="c-38110003" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109904">parent</a><span>|</span><a href="#38109683">next</a><span>|</span><label class="collapse" for="c-38110003">[-]</label><label class="expand" for="c-38110003">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but it&#x27;s not going to have any meaningful impact on the overall gdp because it couldn&#x27;t happen at scale (requires one party to have 1.000.000$ to spend in the first place).</div><br/><div id="38110290" class="c"><input type="checkbox" id="c-38110290" checked=""/><div class="controls bullet"><span class="by">VaxWithSex</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38110003">parent</a><span>|</span><a href="#38109683">next</a><span>|</span><label class="collapse" for="c-38110290">[-]</label><label class="expand" for="c-38110290">[1 more]</label></div><br/><div class="children"><div class="content">All money is fiat.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38109683" class="c"><input type="checkbox" id="c-38109683" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109603">parent</a><span>|</span><a href="#38109682">prev</a><span>|</span><a href="#38109933">next</a><span>|</span><label class="collapse" for="c-38109683">[-]</label><label class="expand" for="c-38109683">[2 more]</label></div><br/><div class="children"><div class="content">That is a surprisingly high figure though for 18k a year you could clothe, feed yourself and have shelter especially considering healthcare is cheap or free in most of the world. Thatâs way better than I thought the world was doing.</div><br/><div id="38110444" class="c"><input type="checkbox" id="c-38110444" checked=""/><div class="controls bullet"><span class="by">adhesive_wombat</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109683">parent</a><span>|</span><a href="#38109933">next</a><span>|</span><label class="collapse" for="c-38110444">[-]</label><label class="expand" for="c-38110444">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s mean vs median for you. In this analysis, average is Â£12k, but median global per-capita household income is under $3k: 50 percent of humans earn less than that (to be fair, it&#x27;s actually rising quite fast and steadily since the 90s).<p>Not sure where PPP puts it but the median figure will be lower because you can get a lot richer than you can get poorer.<p><a href="https:&#x2F;&#x2F;www.zippia.com&#x2F;advice&#x2F;average-income-worldwide&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.zippia.com&#x2F;advice&#x2F;average-income-worldwide&#x2F;</a></div><br/></div></div></div></div><div id="38109933" class="c"><input type="checkbox" id="c-38109933" checked=""/><div class="controls bullet"><span class="by">morsch</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109603">parent</a><span>|</span><a href="#38109683">prev</a><span>|</span><a href="#38109308">next</a><span>|</span><label class="collapse" for="c-38109933">[-]</label><label class="expand" for="c-38109933">[3 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s pretty close, actually. Our family could easily live off that. Easily worth it if it got rid of income inequality. And it&#x27;d still increase a percent or two, every year, inflation-adjusted! Thanks for the argument.</div><br/><div id="38109979" class="c"><input type="checkbox" id="c-38109979" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109933">parent</a><span>|</span><a href="#38109308">next</a><span>|</span><label class="collapse" for="c-38109979">[-]</label><label class="expand" for="c-38109979">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Our family could easily live off that. Easily worth it if it got rid of income inequality<p>Nobody&#x27;s stopping you giving all your excess income above 18k to charity. If you try to make other people do it, you&#x27;ll find a lot of people would rather just stop working than give away the majority of their income, which is why the economy collapsed and tens of millions of people starved in China and Russia after their communist revolutions.</div><br/><div id="38110431" class="c"><input type="checkbox" id="c-38110431" checked=""/><div class="controls bullet"><span class="by">morsch</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109979">parent</a><span>|</span><a href="#38109308">next</a><span>|</span><label class="collapse" for="c-38110431">[-]</label><label class="expand" for="c-38110431">[1 more]</label></div><br/><div class="children"><div class="content">Oh, I don&#x27;t think my post was advocating anybody give anything away. But if we just arrived there, magically, it would be pretty great, from a utilitarian point of view. But thanks for the knee-jerk anti-communist rant, it&#x27;s really great that we&#x27;re back to that. Life was great in the 50s.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38109353" class="c"><input type="checkbox" id="c-38109353" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#38109151">parent</a><span>|</span><a href="#38109308">prev</a><span>|</span><a href="#38109352">next</a><span>|</span><label class="collapse" for="c-38109353">[-]</label><label class="expand" for="c-38109353">[1 more]</label></div><br/><div class="children"><div class="content">Bankruptcy. Regulation. Trust busting. Saboteurs.</div><br/></div></div><div id="38109352" class="c"><input type="checkbox" id="c-38109352" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#38109151">parent</a><span>|</span><a href="#38109353">prev</a><span>|</span><a href="#38109384">next</a><span>|</span><label class="collapse" for="c-38109352">[-]</label><label class="expand" for="c-38109352">[10 more]</label></div><br/><div class="children"><div class="content">Eh, the strong market for virtual goods and the existence of significant virtual economies[1] like those found in Roblox and Second Life lead me to believe that markets can and will always exist, regardless of how far we progress into and beyond post-scarcity (assuming humanity persists in its current form and consumption preferences remain stable). I&#x27;m less certain about the capitalist model in particular, but it too may continue to exist, although maybe just for fun.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Virtual_economy" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Virtual_economy</a></div><br/><div id="38110020" class="c"><input type="checkbox" id="c-38110020" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109352">parent</a><span>|</span><a href="#38109441">next</a><span>|</span><label class="collapse" for="c-38110020">[-]</label><label class="expand" for="c-38110020">[1 more]</label></div><br/><div class="children"><div class="content">The economies in Roblox and Second Life didnât just appear they were built intentionally to make money out of the platforms. They are literally bringing our economy into those games on purpose. Same with other virtual worlds that donât offer trading for real money theyâre just transplanting âmoneyâ into their world without much thought beyond the analogue to the real world often with hilarious consequences like runaway inflation.<p>It basically says more about the overriding culture the games are made within than anything about market economies.<p>I also find myself thinking like a rat in a maze towards markets as well. But I also remind myself that far more innovative thought is put towards hustling other rats in the maze than working out how to get out.</div><br/></div></div><div id="38109441" class="c"><input type="checkbox" id="c-38109441" checked=""/><div class="controls bullet"><span class="by">sgu999</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109352">parent</a><span>|</span><a href="#38110020">prev</a><span>|</span><a href="#38109508">next</a><span>|</span><label class="collapse" for="c-38109441">[-]</label><label class="expand" for="c-38109441">[6 more]</label></div><br/><div class="children"><div class="content">We are already in a post-scarcity world, we have been for decades.<p>The problem is inequalities and the inefficiency of how we use the resources, it doesn&#x27;t look like we&#x27;ll change that soon, sadly. I&#x27;m more prone to think that in a &quot;virtual&quot; economy we&#x27;ll still have miserable workers to empty the chamber-pots of the lucky ones who are plugged in the new world.</div><br/><div id="38109501" class="c"><input type="checkbox" id="c-38109501" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109441">parent</a><span>|</span><a href="#38109508">next</a><span>|</span><label class="collapse" for="c-38109501">[-]</label><label class="expand" for="c-38109501">[5 more]</label></div><br/><div class="children"><div class="content">&gt; We are already in a post-scarcity world, we have been for decades.<p>It&#x27;s only a post-scarcity world if you define scarcity by absence of enough resources to satisfy the bare-minimum required for existence. People don&#x27;t want to live at the subsistence level, and we&#x27;re not evenly remotely close to having enough resources to satisfy everybody&#x27;s wants.</div><br/><div id="38109703" class="c"><input type="checkbox" id="c-38109703" checked=""/><div class="controls bullet"><span class="by">sgu999</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109501">parent</a><span>|</span><a href="#38110086">next</a><span>|</span><label class="collapse" for="c-38109703">[-]</label><label class="expand" for="c-38109703">[3 more]</label></div><br/><div class="children"><div class="content">&gt; People don&#x27;t want to live at the subsistence level, and we&#x27;re not evenly remotely close to having enough resources to satisfy everybody&#x27;s wants.<p>What people want is defined by society. Somehow some people want to be alone in a 2 tons EV, a 200 sqm mansion for a family of 4 and retire at 40. Others are tremendously happy with a much, much less resources intensive lifestyles.<p>You would also need to define what is part of the subsistence level. I&#x27;d put it roughly at what my grandma wanted: food, shelter, healthcare, some leisure time.<p>A bit more than that is absolutely doable in a civilisation that went from one farmer to feed 1.3 people to one farmer to feed 60 people.</div><br/><div id="38109790" class="c"><input type="checkbox" id="c-38109790" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109703">parent</a><span>|</span><a href="#38110086">next</a><span>|</span><label class="collapse" for="c-38109790">[-]</label><label class="expand" for="c-38109790">[2 more]</label></div><br/><div class="children"><div class="content">&gt;What people want is defined by society. Somehow some people want to be alone in a 2 tons EV, a 200 sqm mansion for a family of 4 and retire at 40. Others are tremendously happy with a much, much less resources intensive lifestyles.<p>You just undermined your own assertion. If what people want was defined by society, then everybody would want the same things. But in fact as you stated they don&#x27;t; some people want a lot, some people want little. There&#x27;s no way you&#x27;re going to stop people wanting nice things short of killing the 90% of the population that aren&#x27;t ascetics, which you&#x27;ll never be able to do (at least in the US) because that&#x27;s also the part of the population that owns most of the guns.</div><br/><div id="38110445" class="c"><input type="checkbox" id="c-38110445" checked=""/><div class="controls bullet"><span class="by">sgu999</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109790">parent</a><span>|</span><a href="#38110086">next</a><span>|</span><label class="collapse" for="c-38110445">[-]</label><label class="expand" for="c-38110445">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t have a fully globalised culture yet, what an average American wants isn&#x27;t what an average Japanese wants. Of course even then, there are disparities and outliers, but we can&#x27;t really overlook that if you &quot;want&quot; an SUV or an Apple Watch, that&#x27;s quite likely going to be because the people around you find it valuable.<p>I&#x27;d go even a bit further as to say that what is important is probably what people &quot;need&quot;, not &quot;want&quot;. Apparently we mostly &quot;want&quot; as much as we can have anyway, because that used to be helpful for not dying before being able to reproduce enough.<p>&gt; because that&#x27;s also the part of the population that owns most of the guns<p>That made me giggle</div><br/></div></div></div></div></div></div><div id="38110086" class="c"><input type="checkbox" id="c-38110086" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109501">parent</a><span>|</span><a href="#38109703">prev</a><span>|</span><a href="#38109508">next</a><span>|</span><label class="collapse" for="c-38110086">[-]</label><label class="expand" for="c-38110086">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s only a post-scarcity world if you define scarcity by absence of enough resources to satisfy the bare-minimum required for existence.<p>We have enough food to feed the entire world, and enough clothes to cloth the entire world. It could be a good start</div><br/></div></div></div></div></div></div><div id="38109508" class="c"><input type="checkbox" id="c-38109508" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109352">parent</a><span>|</span><a href="#38109441">prev</a><span>|</span><a href="#38109791">next</a><span>|</span><label class="collapse" for="c-38109508">[-]</label><label class="expand" for="c-38109508">[1 more]</label></div><br/><div class="children"><div class="content">What comes to mind is when the entire metaverse and NFT craze was at its peak, there were many unironic suggestions floating around that NFTs were an important aspect of the metaverse because they would substitute for exclusive ownership of digital goods in the way physical items have to be &quot;exclusive&quot; to a certain quantity and quality. The vision basically being that such limitations are a feature in a world where replication is supposed to be effectively free.</div><br/></div></div><div id="38109791" class="c"><input type="checkbox" id="c-38109791" checked=""/><div class="controls bullet"><span class="by">hgomersall</span><span>|</span><a href="#38109151">root</a><span>|</span><a href="#38109352">parent</a><span>|</span><a href="#38109508">prev</a><span>|</span><a href="#38109384">next</a><span>|</span><label class="collapse" for="c-38109791">[-]</label><label class="expand" for="c-38109791">[1 more]</label></div><br/><div class="children"><div class="content">Those virtual economies are typically designed to mimick the real economy. They&#x27;re often even hampered in ways the real economy isn&#x27;t due to a misunderstanding of how the real system functions.</div><br/></div></div></div></div></div></div><div id="38109384" class="c"><input type="checkbox" id="c-38109384" checked=""/><div class="controls bullet"><span class="by">instagraham</span><span>|</span><a href="#38109151">prev</a><span>|</span><a href="#38110382">next</a><span>|</span><label class="collapse" for="c-38109384">[-]</label><label class="expand" for="c-38109384">[1 more]</label></div><br/><div class="children"><div class="content">The power of selfless development on open source AI will become so crucial to avoiding this scenario. I do think that even today, open AIs are capable of filling in a lot of the gaps you&#x27;d have if you didn&#x27;t want to use OpenAI&#x27;s work.</div><br/></div></div><div id="38110382" class="c"><input type="checkbox" id="c-38110382" checked=""/><div class="controls bullet"><span class="by">motbus3</span><span>|</span><a href="#38109384">prev</a><span>|</span><a href="#38109215">next</a><span>|</span><label class="collapse" for="c-38110382">[-]</label><label class="expand" for="c-38110382">[1 more]</label></div><br/><div class="children"><div class="content">Though LeCunn and Tegmark are specialists in the field it is hard to not consider that LeCunn could have a conflict of interest.<p>Yet, it is still a problem that might happen anyway, and dealing with that is ensuring the technology is open and accessible.<p>The other options means to give few individuals the total power exclusively.<p>This might be one scenario where both of them are right and agrees in 99% of the arguments</div><br/></div></div><div id="38109215" class="c"><input type="checkbox" id="c-38109215" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#38110382">prev</a><span>|</span><a href="#38110411">next</a><span>|</span><label class="collapse" for="c-38109215">[-]</label><label class="expand" for="c-38109215">[16 more]</label></div><br/><div class="children"><div class="content">It&#x27;s too late for monopolizing large language models. The technology for making them is known and the cost keeps coming down.<p>There are things to worry about, but this doesn&#x27;t seem to be one of them.</div><br/><div id="38109264" class="c"><input type="checkbox" id="c-38109264" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#38109215">parent</a><span>|</span><a href="#38109715">next</a><span>|</span><label class="collapse" for="c-38109264">[-]</label><label class="expand" for="c-38109264">[2 more]</label></div><br/><div class="children"><div class="content">Two thoughts on that:<p>1. GPU prices aren&#x27;t as high as they were during the mining peak, but you can still buy a car with the money it takes to buy a pod of A100s. Further to that point, Google et al. are making the next generation of ML accelerators and they won&#x27;t even sell the best ones.<p>2. Compute is only half the problem. Access to data (e.g. in the form of email and smart assistant queries and ring door bells) is a much under appreciated second half of that equation.</div><br/><div id="38110264" class="c"><input type="checkbox" id="c-38110264" checked=""/><div class="controls bullet"><span class="by">neodypsis</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109264">parent</a><span>|</span><a href="#38109715">next</a><span>|</span><label class="collapse" for="c-38110264">[-]</label><label class="expand" for="c-38110264">[1 more]</label></div><br/><div class="children"><div class="content">The biggest cost, other than capital costs, is energy. Some geographies have competitive advantages w.r.t. energy.</div><br/></div></div></div></div><div id="38109715" class="c"><input type="checkbox" id="c-38109715" checked=""/><div class="controls bullet"><span class="by">desmond-grealy</span><span>|</span><a href="#38109215">parent</a><span>|</span><a href="#38109264">prev</a><span>|</span><a href="#38109381">next</a><span>|</span><label class="collapse" for="c-38109715">[-]</label><label class="expand" for="c-38109715">[1 more]</label></div><br/><div class="children"><div class="content">It is though. The groundwork is already being laid to restrict the legality of training large models. See the recent executive order. There is a rather small cutoff size being established for needing to report your training runs and data centers to the government. It is also being ordered that X-risk and CBRN risk testing frameworks and secure environments be developed specifically so they can be imposed as requirements of model developers, including in the private sector.<p>You may argue that the knowledge required to develop LLMs is open source and will be accessible to all (and in practice this is not-quite but mostly true). However the federal government is currently in the process of making it so only certain people are authorized to do so legally.</div><br/></div></div><div id="38109381" class="c"><input type="checkbox" id="c-38109381" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#38109215">parent</a><span>|</span><a href="#38109715">prev</a><span>|</span><a href="#38109444">next</a><span>|</span><label class="collapse" for="c-38109381">[-]</label><label class="expand" for="c-38109381">[7 more]</label></div><br/><div class="children"><div class="content">Training is still very expensive.. and if it becomes cheaper, then a next-gen model with better performance will be expensive again.<p>It may be very hard to train cutting edge models if you only have consumer hardware</div><br/><div id="38109471" class="c"><input type="checkbox" id="c-38109471" checked=""/><div class="controls bullet"><span class="by">mrexroad</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109381">parent</a><span>|</span><a href="#38109442">next</a><span>|</span><label class="collapse" for="c-38109471">[-]</label><label class="expand" for="c-38109471">[1 more]</label></div><br/><div class="children"><div class="content">Either way, itâll get to a point where a few generations removed from cutting edge will be âgood enoughâ for existing problem spaces to a degree models are further commoditized. Like any other tech (e.g. iPhones) Next-gen  will enable new emergent use cases or continue to eke out diminishing advantages over competition.</div><br/></div></div><div id="38109442" class="c"><input type="checkbox" id="c-38109442" checked=""/><div class="controls bullet"><span class="by">chrismarlow9</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109381">parent</a><span>|</span><a href="#38109471">prev</a><span>|</span><a href="#38109444">next</a><span>|</span><label class="collapse" for="c-38109442">[-]</label><label class="expand" for="c-38109442">[5 more]</label></div><br/><div class="children"><div class="content">Why do you need cutting edge models if you can solve your problems with less?</div><br/><div id="38109808" class="c"><input type="checkbox" id="c-38109808" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109442">parent</a><span>|</span><a href="#38109889">next</a><span>|</span><label class="collapse" for="c-38109808">[-]</label><label class="expand" for="c-38109808">[2 more]</label></div><br/><div class="children"><div class="content">The gap between the value added by GPT 3.5 and GPT 4 to my every day tasks is immense. Most people, outside of these circles, don&#x27;t realize that such a large gap exists.</div><br/><div id="38110637" class="c"><input type="checkbox" id="c-38110637" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109808">parent</a><span>|</span><a href="#38109889">next</a><span>|</span><label class="collapse" for="c-38110637">[-]</label><label class="expand" for="c-38110637">[1 more]</label></div><br/><div class="children"><div class="content">Could I ask what exactly you get from 4 that you don&#x27;t get from 3.5?</div><br/></div></div></div></div><div id="38109889" class="c"><input type="checkbox" id="c-38109889" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109442">parent</a><span>|</span><a href="#38109808">prev</a><span>|</span><a href="#38109444">next</a><span>|</span><label class="collapse" for="c-38109889">[-]</label><label class="expand" for="c-38109889">[2 more]</label></div><br/><div class="children"><div class="content">They are required if there is an arms-race between actors wielding models. For example, to detect increasingly authentic deepfakes.</div><br/><div id="38110305" class="c"><input type="checkbox" id="c-38110305" checked=""/><div class="controls bullet"><span class="by">chrismarlow9</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109889">parent</a><span>|</span><a href="#38109444">next</a><span>|</span><label class="collapse" for="c-38110305">[-]</label><label class="expand" for="c-38110305">[1 more]</label></div><br/><div class="children"><div class="content">Sure but the clandestine actors most of us worry about don&#x27;t need that level of sophistication.</div><br/></div></div></div></div></div></div></div></div><div id="38109444" class="c"><input type="checkbox" id="c-38109444" checked=""/><div class="controls bullet"><span class="by">rhizome</span><span>|</span><a href="#38109215">parent</a><span>|</span><a href="#38109381">prev</a><span>|</span><a href="#38109991">next</a><span>|</span><label class="collapse" for="c-38109444">[-]</label><label class="expand" for="c-38109444">[1 more]</label></div><br/><div class="children"><div class="content">Cost of technology doesn&#x27;t matter for this.<p>It&#x27;s not the hammers, it&#x27;s the building permits.</div><br/></div></div><div id="38109991" class="c"><input type="checkbox" id="c-38109991" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#38109215">parent</a><span>|</span><a href="#38109444">prev</a><span>|</span><a href="#38109285">next</a><span>|</span><label class="collapse" for="c-38109991">[-]</label><label class="expand" for="c-38109991">[1 more]</label></div><br/><div class="children"><div class="content">The same can be said about guns. Because of this, gun making is a licensed industry, even though in theory a lot of people can legally acquire the tools to make guns in their garage.<p>Edit: drugs and licensed services like law and healthcare also apply</div><br/></div></div><div id="38109285" class="c"><input type="checkbox" id="c-38109285" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#38109215">parent</a><span>|</span><a href="#38109991">prev</a><span>|</span><a href="#38110411">next</a><span>|</span><label class="collapse" for="c-38109285">[-]</label><label class="expand" for="c-38109285">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  It&#x27;s too late for monopolizing large language models. The technology for making them is known and the cost keeps coming down.<p>Not so sure non-institutional folks can catch up.<p>OpenAI is trying to regulatory capture LLMs to prevent the Amazons and the well-funded unicorns from following. Everyone left in the lurch after these players crystalize is SOL.<p>You, average HN reader, cannot possibly compete with OpenAI. They&#x27;re trying to stem venture dollars from standing up more well-funded competition so they take as much of the pie as possible.<p>The hype&#x2F;funding cycle for LLMs is mostly over now. You&#x27;re not Anthropic and you&#x27;re more than likely too late to get in on that game.</div><br/><div id="38109339" class="c"><input type="checkbox" id="c-38109339" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109285">parent</a><span>|</span><a href="#38110411">next</a><span>|</span><label class="collapse" for="c-38109339">[-]</label><label class="expand" for="c-38109339">[2 more]</label></div><br/><div class="children"><div class="content">Amazon excels at regulatory compliance. This isnât a moat against Amazon. Itâs a moat against you.</div><br/><div id="38109427" class="c"><input type="checkbox" id="c-38109427" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38109215">root</a><span>|</span><a href="#38109339">parent</a><span>|</span><a href="#38110411">next</a><span>|</span><label class="collapse" for="c-38109427">[-]</label><label class="expand" for="c-38109427">[1 more]</label></div><br/><div class="children"><div class="content">The same is true of Microsoft, who more or less own OpenAI now.</div><br/></div></div></div></div></div></div></div></div><div id="38110219" class="c"><input type="checkbox" id="c-38110219" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#38110411">prev</a><span>|</span><a href="#38109397">next</a><span>|</span><label class="collapse" for="c-38110219">[-]</label><label class="expand" for="c-38110219">[1 more]</label></div><br/><div class="children"><div class="content">Its a delightful turn of events. LeCun still works for Meta so his public stance is pressumably sanctioned as congruent with corporate objectives.<p>The precise internal calculus does not immediately matter [1], the outcome is having a moneyed entity that is de-facto more qualified to opine on this debate than practically anybody on the planet (except maybe Google) arguing for open source &quot;AI&quot;.<p>It makes perfect sense. Meta knows about the real as opposed to made-up risks from algorithms used directly (without person-in-middle or user awareness) to decide and affect human behavior. They know them well <i>because they have done it</i>. At scale, with not much regulation etc.<p>The risks from massive algorithmic intermediation of information flow are real and will only grow bigger with time. The way to mitigate them is not granting institutional rights to a new &quot;trusted&quot; AI feudarchy. Diffusing the means of production but also <i>the means of protection</i> is the democratic and human-centric path.<p>[1] In the longer term, though, relying on fickle oligopolistic positioning which may change with arbitrary bilateral deals and &quot;understandings&quot; (like the Google-Apple deal) for such a vital aspect of digital society design will not work. Both non-profit initiatives such as mozilla.ai and direct public sector involvement (producing public domain models etc) will be needed to create a tangibly alternative reality on the ground.<p>Remember the actual theory of how markets and capitalism are supposed to work is that the collective sets the rules. Fullstop. All these players require license to operate. There is more than enough room for private profit in the new &quot;AI&quot; landscape, but you dont start by anointing permanent landlords.</div><br/></div></div><div id="38109397" class="c"><input type="checkbox" id="c-38109397" checked=""/><div class="controls bullet"><span class="by">mg</span><span>|</span><a href="#38110219">prev</a><span>|</span><a href="#38109920">next</a><span>|</span><label class="collapse" for="c-38109397">[-]</label><label class="expand" for="c-38109397">[7 more]</label></div><br/><div class="children"><div class="content">The moat he describes the one-percenters might aquire is government regulation. Creating an environment where only an exclusive club of companies is able to work on AI.<p>Is that possible? Has that happened in any other industry?<p>I can imagine it happening in a single country. But wouldn&#x27;t research route around that country then? How is the situation in Europe, China, India, Canada, Japan ... ?</div><br/><div id="38109506" class="c"><input type="checkbox" id="c-38109506" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109397">parent</a><span>|</span><a href="#38109437">next</a><span>|</span><label class="collapse" for="c-38109506">[-]</label><label class="expand" for="c-38109506">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Is that possible? Has that happened in any other industry?<p>It&#x27;s absolutely happened to the nuclear weapons industry. And Big AI is lobbying for the government to treat AI development the same way it treats nuclear proliferation.</div><br/><div id="38110210" class="c"><input type="checkbox" id="c-38110210" checked=""/><div class="controls bullet"><span class="by">mg</span><span>|</span><a href="#38109397">root</a><span>|</span><a href="#38109506">parent</a><span>|</span><a href="#38109437">next</a><span>|</span><label class="collapse" for="c-38110210">[-]</label><label class="expand" for="c-38110210">[2 more]</label></div><br/><div class="children"><div class="content">There are more nuclear warheads in Russia than in the USA.</div><br/><div id="38110282" class="c"><input type="checkbox" id="c-38110282" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38109397">root</a><span>|</span><a href="#38110210">parent</a><span>|</span><a href="#38109437">next</a><span>|</span><label class="collapse" for="c-38110282">[-]</label><label class="expand" for="c-38110282">[1 more]</label></div><br/><div class="children"><div class="content">Would you like to see a world where the only countries with their own AIs are Russia, the United States, China, France, the United Kingdom, Pakistan, India, Israel and North Korea, and all use of AI is controlled by the governments of those countries?</div><br/></div></div></div></div></div></div><div id="38109437" class="c"><input type="checkbox" id="c-38109437" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#38109397">parent</a><span>|</span><a href="#38109506">prev</a><span>|</span><a href="#38109424">next</a><span>|</span><label class="collapse" for="c-38109437">[-]</label><label class="expand" for="c-38109437">[1 more]</label></div><br/><div class="children"><div class="content">&gt;But wouldn&#x27;t research route around that country then?<p>it already does. Stable Diffusion originated at LMU Munich with a lot of non-profit funding, EleutherAI and LAION are non-profit as well I think. The Chinese companies all have their own models anyway etc.<p>So yes I also find it unlikely. Of course there&#x27;ll always be one or two players a little bit ahead of the field at any given time but this stuff is almost entirely public research based. These models largely run on a few thousand lines of Python, public data and a ton of compute. The latter is the biggest bottleneck but that&#x27;ll diminish as more manufacturers come online.</div><br/></div></div><div id="38109424" class="c"><input type="checkbox" id="c-38109424" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38109397">parent</a><span>|</span><a href="#38109437">prev</a><span>|</span><a href="#38109416">next</a><span>|</span><label class="collapse" for="c-38109424">[-]</label><label class="expand" for="c-38109424">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the US government could try and do what they did with cryptography in the 90s and succeed.</div><br/></div></div><div id="38109416" class="c"><input type="checkbox" id="c-38109416" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#38109397">parent</a><span>|</span><a href="#38109424">prev</a><span>|</span><a href="#38109920">next</a><span>|</span><label class="collapse" for="c-38109416">[-]</label><label class="expand" for="c-38109416">[1 more]</label></div><br/><div class="children"><div class="content">Ehhh capital alone is plenty of a moat, esp when thereâs no regulation to keep the market fair.</div><br/></div></div></div></div><div id="38109920" class="c"><input type="checkbox" id="c-38109920" checked=""/><div class="controls bullet"><span class="by">Sparkyte</span><span>|</span><a href="#38109397">prev</a><span>|</span><a href="#38110171">next</a><span>|</span><label class="collapse" for="c-38109920">[-]</label><label class="expand" for="c-38109920">[2 more]</label></div><br/><div class="children"><div class="content">Ehhh, doomsday is any sizable economic force of nature wielding against the betterment of mankind and forcing socio-political agendas for profit over everyone.<p>From corporations to countries that you might as well just call corporations. We need international rights and regulations on AI to ensure it isn&#x27;t used to harm people.</div><br/><div id="38110628" class="c"><input type="checkbox" id="c-38110628" checked=""/><div class="controls bullet"><span class="by">sublimefire</span><span>|</span><a href="#38109920">parent</a><span>|</span><a href="#38110171">next</a><span>|</span><label class="collapse" for="c-38110628">[-]</label><label class="expand" for="c-38110628">[1 more]</label></div><br/><div class="children"><div class="content">We do have laws that punish the ones doing harm already. Not to mention data protection and privacy related regulation. Why would you need to control me, a one man company, who is trying to sell the use of the small-tuned models?<p>&gt; to ensure it isn&#x27;t used to harm people<p>You can harm people with many things like bottles, forks, plastic bags, pillows, but it does not mean the manufacturer needs to be controlled. The harm is done by the company or a person, and the usual way is to prove it in the courts first and take it from there.</div><br/></div></div></div></div><div id="38110171" class="c"><input type="checkbox" id="c-38110171" checked=""/><div class="controls bullet"><span class="by">hoseja</span><span>|</span><a href="#38109920">prev</a><span>|</span><a href="#38109431">next</a><span>|</span><label class="collapse" for="c-38110171">[-]</label><label class="expand" for="c-38110171">[1 more]</label></div><br/><div class="children"><div class="content">Instead of legacy onepercenters, how awful. Mainstream media say: our owners are the correct perpetual lords! Displacing them is immoral!</div><br/></div></div><div id="38109431" class="c"><input type="checkbox" id="c-38109431" checked=""/><div class="controls bullet"><span class="by">gdsdfe</span><span>|</span><a href="#38110171">prev</a><span>|</span><a href="#38109423">next</a><span>|</span><label class="collapse" for="c-38109431">[-]</label><label class="expand" for="c-38109431">[1 more]</label></div><br/><div class="children"><div class="content">They want to create a moat to protect their investments</div><br/></div></div><div id="38109408" class="c"><input type="checkbox" id="c-38109408" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#38109423">prev</a><span>|</span><a href="#38110291">next</a><span>|</span><label class="collapse" for="c-38109408">[-]</label><label class="expand" for="c-38109408">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the thing with AI. It&#x27;s so dangerous it could destroy us all. P(doom)=0.3.<p>Anyway, that&#x27;s why we&#x27;re working on much large LLMs. It&#x27;s important that we do this research and expand the size of the LLMs because a larger LLM could destroy us all.<p>Listen, everyone cosplays as doomers here because it&#x27;s stylish right now. But they&#x27;re all trying to make this thing they think is suicide. Is this a suicide cult?<p>No. It&#x27;s just people playing around acting important while solidifying their business.<p>Recently, I read on Twitter that many AI CTOs think that P(doom) &gt; 0.25 by 2030. Here&#x27;s my question to one CTO of a venture backed firm with present personal assets over $400k that believes this:<p>- A representative of mine will meet you in South Park<p>- The representative will be carrying a $100k cheque and a document outlining terms for you to pay a lump sum of $177k on Feb 1 2030 (10% compounding, roughly)<p>- The loan will be unsecured<p>- We will make the terms public<p>This is pretty close to break-even for you at terms you won&#x27;t receive personally anywhere else. Once doom happens money will be worthless. Take my money. I&#x27;ll also make the deal all numbers divided by 100 if you just want to have some fun.</div><br/><div id="38109445" class="c"><input type="checkbox" id="c-38109445" checked=""/><div class="controls bullet"><span class="by">artninja1988</span><span>|</span><a href="#38109408">parent</a><span>|</span><a href="#38110291">next</a><span>|</span><label class="collapse" for="c-38109445">[-]</label><label class="expand" for="c-38109445">[3 more]</label></div><br/><div class="children"><div class="content">P(doom)=0.3 according to what?
How did you come up with these numbers</div><br/><div id="38109538" class="c"><input type="checkbox" id="c-38109538" checked=""/><div class="controls bullet"><span class="by">zo1</span><span>|</span><a href="#38109408">root</a><span>|</span><a href="#38109445">parent</a><span>|</span><a href="#38109534">next</a><span>|</span><label class="collapse" for="c-38109538">[-]</label><label class="expand" for="c-38109538">[1 more]</label></div><br/><div class="children"><div class="content">The OP I assume picked it because it&#x27;s a number slightly greater than 0.25 which he read was quoted by AI CTOs on Twitter.</div><br/></div></div><div id="38109534" class="c"><input type="checkbox" id="c-38109534" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#38109408">root</a><span>|</span><a href="#38109445">parent</a><span>|</span><a href="#38109538">prev</a><span>|</span><a href="#38110291">next</a><span>|</span><label class="collapse" for="c-38109534">[-]</label><label class="expand" for="c-38109534">[1 more]</label></div><br/><div class="children"><div class="content">Out of my arse, obviously. It&#x27;s total bullshit.</div><br/></div></div></div></div></div></div><div id="38109301" class="c"><input type="checkbox" id="c-38109301" checked=""/><div class="controls bullet"><span class="by">bsder</span><span>|</span><a href="#38109939">prev</a><span>|</span><a href="#38109292">next</a><span>|</span><label class="collapse" for="c-38109301">[-]</label><label class="expand" for="c-38109301">[4 more]</label></div><br/><div class="children"><div class="content">The problem isn&#x27;t seizing power.<p>The problem is going to be that AI is creating an Eternal September by poisoning the well of training data.<p>Anybody who has a data or a model prior to the poisoning is going to have a defensible moat that is not accessible to those who come after.</div><br/><div id="38109354" class="c"><input type="checkbox" id="c-38109354" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#38109301">parent</a><span>|</span><a href="#38109681">next</a><span>|</span><label class="collapse" for="c-38109354">[-]</label><label class="expand" for="c-38109354">[1 more]</label></div><br/><div class="children"><div class="content">First of all, âpre-poisonedâ data sets are freely available in archives such as common crawl. Secondly, I doubt the usefulness of that data in the long run. A model trained on only that old data will forever be stuck in the past. Itâll be like the stereotypical senile grandpa who only talks about the old days and doesnât know anything about current events.</div><br/></div></div><div id="38109681" class="c"><input type="checkbox" id="c-38109681" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#38109301">parent</a><span>|</span><a href="#38109354">prev</a><span>|</span><a href="#38109365">next</a><span>|</span><label class="collapse" for="c-38109681">[-]</label><label class="expand" for="c-38109681">[1 more]</label></div><br/><div class="children"><div class="content">Except models can get more data efficient. We can overcome and sort of âpoisoning the well.â 
Manâs desire for power is orders of magnitude more difficult to resolve.</div><br/></div></div><div id="38109365" class="c"><input type="checkbox" id="c-38109365" checked=""/><div class="controls bullet"><span class="by">photonbeam</span><span>|</span><a href="#38109301">parent</a><span>|</span><a href="#38109681">prev</a><span>|</span><a href="#38109292">next</a><span>|</span><label class="collapse" for="c-38109365">[-]</label><label class="expand" for="c-38109365">[1 more]</label></div><br/><div class="children"><div class="content">Data will become stale</div><br/></div></div></div></div><div id="38109292" class="c"><input type="checkbox" id="c-38109292" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#38109301">prev</a><span>|</span><a href="#38109307">next</a><span>|</span><label class="collapse" for="c-38109292">[-]</label><label class="expand" for="c-38109292">[1 more]</label></div><br/><div class="children"><div class="content">And yet, their papers are full of jargon.</div><br/></div></div><div id="38109307" class="c"><input type="checkbox" id="c-38109307" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#38109292">prev</a><span>|</span><a href="#38109454">next</a><span>|</span><label class="collapse" for="c-38109307">[-]</label><label class="expand" for="c-38109307">[4 more]</label></div><br/><div class="children"><div class="content">Is true because if one company creates something more powerful than what military can achieve, why would it not wan to overtake its powers?</div><br/><div id="38109366" class="c"><input type="checkbox" id="c-38109366" checked=""/><div class="controls bullet"><span class="by">stagger87</span><span>|</span><a href="#38109307">parent</a><span>|</span><a href="#38109454">next</a><span>|</span><label class="collapse" for="c-38109366">[-]</label><label class="expand" for="c-38109366">[3 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t worry about that. I&#x27;m sure the military can unplug some computers.</div><br/><div id="38109511" class="c"><input type="checkbox" id="c-38109511" checked=""/><div class="controls bullet"><span class="by">acosmism</span><span>|</span><a href="#38109307">root</a><span>|</span><a href="#38109366">parent</a><span>|</span><a href="#38109454">next</a><span>|</span><label class="collapse" for="c-38109511">[-]</label><label class="expand" for="c-38109511">[2 more]</label></div><br/><div class="children"><div class="content">Iâm probably getting philosophical here but my personal thought is that - at the end of the day (most if not all) people&#x2F;anyone&#x2F;humans like to be on top of âsomethingâ rather than to be on top of âeverything but nothing meaningful (to anyone else)â. reminds of this one Superman adventures episode with Bizarro (a very uncanny anti-superman) - Superman brought him to an empty desolate place (planet) to prevent him from accidentally destroying earth since in his head he âalways wanted to beâ supermanâ. bizarro had an entire planet to himself where he was god, king, law and lawyer it - he loved it.  is that normal or human? hell no. Heâs Bizarro. People will go elsewhere and enjoy human things like life-satisfaction, culture, family, friend networks, etc whatever else people like . life goes on.</div><br/><div id="38109539" class="c"><input type="checkbox" id="c-38109539" checked=""/><div class="controls bullet"><span class="by">acosmism</span><span>|</span><a href="#38109307">root</a><span>|</span><a href="#38109511">parent</a><span>|</span><a href="#38109454">next</a><span>|</span><label class="collapse" for="c-38109539">[-]</label><label class="expand" for="c-38109539">[1 more]</label></div><br/><div class="children"><div class="content">* and also potentially two large markets instead of one</div><br/></div></div></div></div></div></div></div></div><div id="38109454" class="c"><input type="checkbox" id="c-38109454" checked=""/><div class="controls bullet"><span class="by">Geee</span><span>|</span><a href="#38109307">prev</a><span>|</span><label class="collapse" for="c-38109454">[-]</label><label class="expand" for="c-38109454">[15 more]</label></div><br/><div class="children"><div class="content">Yeah, it is a true risk. What happens when people start making their voting decisions with AI, asking who to vote? Nothing wrong with that per se, but it gives an ultimate power to the company who developed and hosts the AI, which means the end of democracy.<p>My recommendation for regulation would be to make closed &amp; cloud-hosted AI illegal. Architecture, training data and weights should always be available, and people should only be allowed to use AI on their local machine (which might be as powerful as possible).</div><br/><div id="38109652" class="c"><input type="checkbox" id="c-38109652" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#38109454">parent</a><span>|</span><a href="#38109601">next</a><span>|</span><label class="collapse" for="c-38109652">[-]</label><label class="expand" for="c-38109652">[4 more]</label></div><br/><div class="children"><div class="content">Seeing the weights is useless for knowing capabilities, and even knowing training data and architecture you still need to interact with it to figure those out, which you can do with cloud AI just as well.<p>The real risk that I see is the use of open source AI by governments, corporations and criminal groups to influence elections through a deluge of bots. Cloud AI can at least be controlled to some degree because there is a government that can inspect and regulate that business, but an army of bots built on local LLMâs could just run wild over the internet without stopping them.<p>How many comments in this thread are written by a bot? I do not feel confident I could tell them apart.</div><br/><div id="38109875" class="c"><input type="checkbox" id="c-38109875" checked=""/><div class="controls bullet"><span class="by">Geee</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109652">parent</a><span>|</span><a href="#38109601">next</a><span>|</span><label class="collapse" for="c-38109875">[-]</label><label class="expand" for="c-38109875">[3 more]</label></div><br/><div class="children"><div class="content">Eh, government regulating AI which is used by people to help them vote?<p>I don&#x27;t see bots as a huge problem, because it&#x27;s somewhat easy to verify that someone is a real person, and it would be quite obvious if someone is using lots of bots for their cause.</div><br/><div id="38109910" class="c"><input type="checkbox" id="c-38109910" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109875">parent</a><span>|</span><a href="#38109601">next</a><span>|</span><label class="collapse" for="c-38109910">[-]</label><label class="expand" for="c-38109910">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t see bots as a huge problem, because it&#x27;s somewhat easy to verify that someone is a real person, and it would be quite obvious if someone is using lots of bots for their cause.<p>That only works on platforms designed to match user profiles with real-world persons. This platform definitely isn&#x27;t.</div><br/><div id="38109942" class="c"><input type="checkbox" id="c-38109942" checked=""/><div class="controls bullet"><span class="by">Geee</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109910">parent</a><span>|</span><a href="#38109601">next</a><span>|</span><label class="collapse" for="c-38109942">[-]</label><label class="expand" for="c-38109942">[1 more]</label></div><br/><div class="children"><div class="content">You can read their profile or read their comment history to verify their humanness. Faking everything wouldn&#x27;t be impossible, but certainly difficult &amp; take time.</div><br/></div></div></div></div></div></div></div></div><div id="38109601" class="c"><input type="checkbox" id="c-38109601" checked=""/><div class="controls bullet"><span class="by">golem14</span><span>|</span><a href="#38109454">parent</a><span>|</span><a href="#38109652">prev</a><span>|</span><a href="#38109464">next</a><span>|</span><label class="collapse" for="c-38109601">[-]</label><label class="expand" for="c-38109601">[1 more]</label></div><br/><div class="children"><div class="content">Thereâs a short story by Asimov called âFranchiseâ which is in hindsight quite forebodingâ¦</div><br/></div></div><div id="38109464" class="c"><input type="checkbox" id="c-38109464" checked=""/><div class="controls bullet"><span class="by">a_t48</span><span>|</span><a href="#38109454">parent</a><span>|</span><a href="#38109601">prev</a><span>|</span><a href="#38109668">next</a><span>|</span><label class="collapse" for="c-38109464">[-]</label><label class="expand" for="c-38109464">[7 more]</label></div><br/><div class="children"><div class="content">This would make AI usage restricted to those who can afford a beefy PC, no?</div><br/><div id="38109570" class="c"><input type="checkbox" id="c-38109570" checked=""/><div class="controls bullet"><span class="by">Geee</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109464">parent</a><span>|</span><a href="#38109921">next</a><span>|</span><label class="collapse" for="c-38109570">[-]</label><label class="expand" for="c-38109570">[2 more]</label></div><br/><div class="children"><div class="content">Costs will come down, while capability will go up. I&#x27;m thinking of a some kind of box in every home, with special inference chips; not really a &quot;PC&quot;.<p>In principle, every human should have access to the same level of AI. There could be one at the local library if someone can&#x27;t afford one, or doesn&#x27;t want one.</div><br/><div id="38109895" class="c"><input type="checkbox" id="c-38109895" checked=""/><div class="controls bullet"><span class="by">ethanwillis</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109570">parent</a><span>|</span><a href="#38109921">next</a><span>|</span><label class="collapse" for="c-38109895">[-]</label><label class="expand" for="c-38109895">[1 more]</label></div><br/><div class="children"><div class="content">Access to local libraries isn&#x27;t even a thing that everyone has access to.</div><br/></div></div></div></div><div id="38109921" class="c"><input type="checkbox" id="c-38109921" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109464">parent</a><span>|</span><a href="#38109570">prev</a><span>|</span><a href="#38109491">next</a><span>|</span><label class="collapse" for="c-38109921">[-]</label><label class="expand" for="c-38109921">[1 more]</label></div><br/><div class="children"><div class="content">That would still be a way larger class of people than the top 1%. Even so, capabilities of consumer devices will rise for some more time.</div><br/></div></div><div id="38109491" class="c"><input type="checkbox" id="c-38109491" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109464">parent</a><span>|</span><a href="#38109921">prev</a><span>|</span><a href="#38109668">next</a><span>|</span><label class="collapse" for="c-38109491">[-]</label><label class="expand" for="c-38109491">[3 more]</label></div><br/><div class="children"><div class="content">Or it would create an incentive for reducing the cost of the hardware needed to run beefier AI models and passing that reduction down to consumers.</div><br/><div id="38109531" class="c"><input type="checkbox" id="c-38109531" checked=""/><div class="controls bullet"><span class="by">jeffparsons</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109491">parent</a><span>|</span><a href="#38109668">next</a><span>|</span><label class="collapse" for="c-38109531">[-]</label><label class="expand" for="c-38109531">[2 more]</label></div><br/><div class="children"><div class="content">Which would still mean that those who can afford more&#x2F;better hardware can therefore afford more&#x2F;better AI. I don&#x27;t think a solution lies down this path...</div><br/><div id="38109571" class="c"><input type="checkbox" id="c-38109571" checked=""/><div class="controls bullet"><span class="by">JieJie</span><span>|</span><a href="#38109454">root</a><span>|</span><a href="#38109531">parent</a><span>|</span><a href="#38109668">next</a><span>|</span><label class="collapse" for="c-38109571">[-]</label><label class="expand" for="c-38109571">[1 more]</label></div><br/><div class="children"><div class="content">There are already people working on distributed solutions to that problem. There are a <i>lot</i> of people working on these problems.</div><br/></div></div></div></div></div></div></div></div><div id="38109668" class="c"><input type="checkbox" id="c-38109668" checked=""/><div class="controls bullet"><span class="by">jdblair</span><span>|</span><a href="#38109454">parent</a><span>|</span><a href="#38109464">prev</a><span>|</span><a href="#38109673">next</a><span>|</span><label class="collapse" for="c-38109668">[-]</label><label class="expand" for="c-38109668">[1 more]</label></div><br/><div class="children"><div class="content">Sounds great until someone says the warehouse full of computers out back is their &quot;local machine.&quot;</div><br/></div></div><div id="38109673" class="c"><input type="checkbox" id="c-38109673" checked=""/><div class="controls bullet"><span class="by">123yawaworht456</span><span>|</span><a href="#38109454">parent</a><span>|</span><a href="#38109668">prev</a><span>|</span><label class="collapse" for="c-38109673">[-]</label><label class="expand" for="c-38109673">[1 more]</label></div><br/><div class="children"><div class="content">This is extremely dangerous to our democracy.</div><br/></div></div></div></div></div></div></div></div></div></body></html>