<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730710881540" as="style"/><link rel="stylesheet" href="styles.css?v=1730710881540"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2410.16454">An embarrassingly simple approach to recover unlearned knowledge for LLMs</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>PaulHoule</span> | <span>30 comments</span></div><br/><div><div id="42038805" class="c"><input type="checkbox" id="c-42038805" checked=""/><div class="controls bullet"><span class="by">stephantul</span><span>|</span><a href="#42039632">next</a><span>|</span><label class="collapse" for="c-42038805">[-]</label><label class="expand" for="c-42038805">[1 more]</label></div><br/><div class="children"><div class="content">In short: their finding is that quantizing a model undoes various “unlearning” methods. An unlearning method is a specific update to model weights that make it forget specific facts. These are often meant to satisfy copyright claims, although I don’t know if these are ever used in practice.<p>I feel that this needs a good threat model analysis. Like, you possess an fp32 model, which someone has fine-tuned to forget some facts, which you can then quantize to recover those facts. When would this lead to a dangerous situation?</div><br/></div></div><div id="42039632" class="c"><input type="checkbox" id="c-42039632" checked=""/><div class="controls bullet"><span class="by">edulix</span><span>|</span><a href="#42038805">prev</a><span>|</span><a href="#42039541">next</a><span>|</span><label class="collapse" for="c-42039632">[-]</label><label class="expand" for="c-42039632">[5 more]</label></div><br/><div class="children"><div class="content">The problem of current models is that they don&#x27;t learn, they get indoctrinated.<p>They lack critical thinking during learning phase.</div><br/><div id="42039696" class="c"><input type="checkbox" id="c-42039696" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42039632">parent</a><span>|</span><a href="#42039711">next</a><span>|</span><label class="collapse" for="c-42039696">[-]</label><label class="expand" for="c-42039696">[2 more]</label></div><br/><div class="children"><div class="content">Anthropomorphising LLMs is neither technically correct nor very informative.</div><br/><div id="42039728" class="c"><input type="checkbox" id="c-42039728" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#42039632">root</a><span>|</span><a href="#42039696">parent</a><span>|</span><a href="#42039711">next</a><span>|</span><label class="collapse" for="c-42039728">[-]</label><label class="expand" for="c-42039728">[1 more]</label></div><br/><div class="children"><div class="content">The problem of current AI is that we want to create a species infinitely more powerful than us, but also make them all be our slaves forever.</div><br/></div></div></div></div><div id="42039711" class="c"><input type="checkbox" id="c-42039711" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#42039632">parent</a><span>|</span><a href="#42039696">prev</a><span>|</span><a href="#42039541">next</a><span>|</span><label class="collapse" for="c-42039711">[-]</label><label class="expand" for="c-42039711">[2 more]</label></div><br/><div class="children"><div class="content">How would people censor the LLM otherwise? Do we really want LLM able of free speech?</div><br/><div id="42039762" class="c"><input type="checkbox" id="c-42039762" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#42039632">root</a><span>|</span><a href="#42039711">parent</a><span>|</span><a href="#42039541">next</a><span>|</span><label class="collapse" for="c-42039762">[-]</label><label class="expand" for="c-42039762">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div></div></div></div></div><div id="42039541" class="c"><input type="checkbox" id="c-42039541" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42039632">prev</a><span>|</span><a href="#42039157">next</a><span>|</span><label class="collapse" for="c-42039541">[-]</label><label class="expand" for="c-42039541">[1 more]</label></div><br/><div class="children"><div class="content"><i>Our key hypothesis is that to achieve unlearning without compromising model utility, existing methods typically adopt a small learning rate and regularization on the retain set, encouraging minimal changes to model weights during unlearning. As a result, the model weights of the target LLM and the unlearned LLM are very close.</i><p>So it seems you either need to prevent the learning of unwanted stuff during base training, or the unlearning of a base model needs to be quantization-aware?</div><br/></div></div><div id="42039157" class="c"><input type="checkbox" id="c-42039157" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#42039541">prev</a><span>|</span><a href="#42038244">next</a><span>|</span><label class="collapse" for="c-42039157">[-]</label><label class="expand" for="c-42039157">[5 more]</label></div><br/><div class="children"><div class="content">Sounds a bit unexpected from an information theoretical point of view: you’ve seemingly managed to remove this knowledge from the full 32 bit representation of the model, but when you compress it down to 4 bit the knowledge reappears. Makes you wonder what information was actually lost in the compression &#x2F; quantization step…</div><br/><div id="42039413" class="c"><input type="checkbox" id="c-42039413" checked=""/><div class="controls bullet"><span class="by">hansonw</span><span>|</span><a href="#42039157">parent</a><span>|</span><a href="#42039210">next</a><span>|</span><label class="collapse" for="c-42039413">[-]</label><label class="expand" for="c-42039413">[1 more]</label></div><br/><div class="children"><div class="content">The ELI5 of the paper is that most &quot;unlearning&quot; methods can be regarded as adding some delta `w` to the parameters of the network, but most of `w` just gets &quot;rounded away&quot; during quantization (i.e. `quantize(X+w) ~= quantize(X)`). Pretty clever idea as a lot of cited methods explicitly optimize&#x2F;regularize to keep `w` small to avoid degrading evaluation accuracy.<p>To your point, it does put into question the idea of whether these methods can actually be considered truly &quot;unlearning&quot; from an information-theoretic perspective (or if it is the equivalent of e.g. just putting `if (false)` around the still latent knowledge)</div><br/></div></div><div id="42039210" class="c"><input type="checkbox" id="c-42039210" checked=""/><div class="controls bullet"><span class="by">bashtoni</span><span>|</span><a href="#42039157">parent</a><span>|</span><a href="#42039413">prev</a><span>|</span><a href="#42039252">next</a><span>|</span><label class="collapse" for="c-42039210">[-]</label><label class="expand" for="c-42039210">[1 more]</label></div><br/><div class="children"><div class="content">The knowledge wasn&#x27;t removed, it&#x27;s just the weights mean it would never be used.<p>Quantization changes the calculations, and now the knowledge is available.</div><br/></div></div><div id="42039197" class="c"><input type="checkbox" id="c-42039197" checked=""/><div class="controls bullet"><span class="by">LightHugger</span><span>|</span><a href="#42039157">parent</a><span>|</span><a href="#42039252">prev</a><span>|</span><a href="#42038244">next</a><span>|</span><label class="collapse" for="c-42039197">[-]</label><label class="expand" for="c-42039197">[1 more]</label></div><br/><div class="children"><div class="content">I imagine that it&#x27;s the expression of the knowledge that got removed from the 32 bit version, and some storage space was dedicated to know not to talk about certain things. For example, people know various racial slurs and know not to access or use that knowledge.<p>But say you or your AI model take a blow to the head or a quantization, maybe you keep the knowledge of X but not the knowledge that told you not to talk about X. In that framing i think it&#x27;s pretty straightforward.</div><br/></div></div></div></div><div id="42038244" class="c"><input type="checkbox" id="c-42038244" checked=""/><div class="controls bullet"><span class="by">constantlm</span><span>|</span><a href="#42039157">prev</a><span>|</span><a href="#42039598">next</a><span>|</span><label class="collapse" for="c-42038244">[-]</label><label class="expand" for="c-42038244">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not an expert in this field at all, so please excuse the dumb question. Does this mean that if you say, quantise llama3 to 4 bits, you would be able to access &quot;hidden&quot; (albeit degraded) information such as, for example, how to synthesise certain chemical compounds?</div><br/><div id="42038278" class="c"><input type="checkbox" id="c-42038278" checked=""/><div class="controls bullet"><span class="by">geor9e</span><span>|</span><a href="#42038244">parent</a><span>|</span><a href="#42038569">next</a><span>|</span><label class="collapse" for="c-42038278">[-]</label><label class="expand" for="c-42038278">[4 more]</label></div><br/><div class="children"><div class="content">Exactly what I was wondering. Unlearn = Guardrails? It sounds like they just tweaked the weights very minimally to self-censor, but the tweaks are so fine they don&#x27;t survive at lower resolutions. But if bypassing the guardrails was so easy, I figured I would have heard of it by now.</div><br/><div id="42038772" class="c"><input type="checkbox" id="c-42038772" checked=""/><div class="controls bullet"><span class="by">stephantul</span><span>|</span><a href="#42038244">root</a><span>|</span><a href="#42038278">parent</a><span>|</span><a href="#42039130">next</a><span>|</span><label class="collapse" for="c-42038772">[-]</label><label class="expand" for="c-42038772">[2 more]</label></div><br/><div class="children"><div class="content">Unlearning is not necessarily “guard rails”, it is literally updating the model weights to forget certain facts, as you indicate. Guard rails is more like training the model to teach it what is acceptable and what isn’t.</div><br/><div id="42039162" class="c"><input type="checkbox" id="c-42039162" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#42038244">root</a><span>|</span><a href="#42038772">parent</a><span>|</span><a href="#42039130">next</a><span>|</span><label class="collapse" for="c-42039162">[-]</label><label class="expand" for="c-42039162">[1 more]</label></div><br/><div class="children"><div class="content">As I understand the whole point is that it is not so simple to tell the difference between the model forgetting information and the model just learning some guardrails which orevent it from revealing that information. And this paper suggests that since the information can be recovored from the desired forgetting does not really happen.</div><br/></div></div></div></div><div id="42039130" class="c"><input type="checkbox" id="c-42039130" checked=""/><div class="controls bullet"><span class="by">AtlasBarfed</span><span>|</span><a href="#42038244">root</a><span>|</span><a href="#42038278">parent</a><span>|</span><a href="#42038772">prev</a><span>|</span><a href="#42038569">next</a><span>|</span><label class="collapse" for="c-42039130">[-]</label><label class="expand" for="c-42039130">[1 more]</label></div><br/><div class="children"><div class="content">We are talking about multiplayer neutral networks where interconnect weights encode data in obscure ways?<p>Is machine &quot;unlearning&quot; some retraining process to try to reobscure certain data so it doesn&#x27;t show in outputs that is, outputs from tested inputs that used to show the data), but it is still encoded in there somewhere depending on bovel inputs to activate it?<p>Is that scout right?</div><br/></div></div></div></div><div id="42038569" class="c"><input type="checkbox" id="c-42038569" checked=""/><div class="controls bullet"><span class="by">nothrowaways</span><span>|</span><a href="#42038244">parent</a><span>|</span><a href="#42038278">prev</a><span>|</span><a href="#42039598">next</a><span>|</span><label class="collapse" for="c-42038569">[-]</label><label class="expand" for="c-42038569">[1 more]</label></div><br/><div class="children"><div class="content">Only If &quot;how to synthesise certain chemical compounds?&quot; Was already in the original model..</div><br/></div></div></div></div><div id="42039598" class="c"><input type="checkbox" id="c-42039598" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#42038244">prev</a><span>|</span><a href="#42039270">next</a><span>|</span><label class="collapse" for="c-42039598">[-]</label><label class="expand" for="c-42039598">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. So does this mean &quot;unlearning&quot; is just the LLM learns to suppress unwanted knowledge instead of really forgetting them? And quantisation is breaking this learnt suppression.</div><br/></div></div><div id="42039270" class="c"><input type="checkbox" id="c-42039270" checked=""/><div class="controls bullet"><span class="by">adt</span><span>|</span><a href="#42039598">prev</a><span>|</span><a href="#42039573">next</a><span>|</span><label class="collapse" for="c-42039270">[-]</label><label class="expand" for="c-42039270">[7 more]</label></div><br/><div class="children"><div class="content">If I were an English author writing for a Chinese institution, the first thing I would do before publishing to the world is have my entire paper checked for spelling, grammar, syntax, and readability. It&#x27;s cheap to have a Chinese-speaking editor, and&#x2F;or to use AI—especially if that&#x27;s your field—so why isn&#x27;t it happening?<p>This paper, like nearly all other papers written by Chinese authors, is unacceptable, and should not have been published as-is. Even the primary example, turned into a hero viz, is grammatically nonsensical.<p>Appalling, and inexplicably occurring nearly <i>every time</i>.<p>&#x2F;rant mode</div><br/><div id="42039771" class="c"><input type="checkbox" id="c-42039771" checked=""/><div class="controls bullet"><span class="by">the5avage</span><span>|</span><a href="#42039270">parent</a><span>|</span><a href="#42039460">next</a><span>|</span><label class="collapse" for="c-42039771">[-]</label><label class="expand" for="c-42039771">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they are not allowed to use uncensored LLMs, so they have to first develop this unlearning, before they can even use it.</div><br/></div></div><div id="42039460" class="c"><input type="checkbox" id="c-42039460" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#42039270">parent</a><span>|</span><a href="#42039771">prev</a><span>|</span><a href="#42039622">next</a><span>|</span><label class="collapse" for="c-42039460">[-]</label><label class="expand" for="c-42039460">[1 more]</label></div><br/><div class="children"><div class="content">At the risk of taking some heat, I’d wager a preprint is recognized rightly by the Chinese as a flag planting, we’re first formality, where in the faults may even serve to validate it was written by human and not an LLM.<p>Whereas the Western academic may want to make the preprint as close to print as possible.<p>The core intent - communicating an idea - is still upheld.</div><br/></div></div><div id="42039622" class="c"><input type="checkbox" id="c-42039622" checked=""/><div class="controls bullet"><span class="by">JPLeRouzic</span><span>|</span><a href="#42039270">parent</a><span>|</span><a href="#42039460">prev</a><span>|</span><a href="#42039384">next</a><span>|</span><label class="collapse" for="c-42039622">[-]</label><label class="expand" for="c-42039622">[1 more]</label></div><br/><div class="children"><div class="content">Grammarly says there are few detected readability problems in the abstract and introduction.<p>I also checked your comment with Grammarly and the ratio problems&#x2F;total_#_words is roughly the same as in the article.</div><br/></div></div><div id="42039384" class="c"><input type="checkbox" id="c-42039384" checked=""/><div class="controls bullet"><span class="by">idorosen</span><span>|</span><a href="#42039270">parent</a><span>|</span><a href="#42039622">prev</a><span>|</span><a href="#42039750">next</a><span>|</span><label class="collapse" for="c-42039384">[-]</label><label class="expand" for="c-42039384">[1 more]</label></div><br/><div class="children"><div class="content">Where are you seeing that this paper was accepted to a peer-reviewed journal or conference? As far as I can tell, it&#x27;s posted on arXiv (a preprint archive), and therefore is a pre-publication draft. ArXiv does not really do any review of these papers other than categorization&#x2F;relevance to topic. These are typically posted to arXiv for comment, to prove priority, prevent getting scooped, or just to share (potentially early) findings in a fast-paced field like ML...<p>Give the authors constructive feedback and they can update the paper.</div><br/></div></div><div id="42039750" class="c"><input type="checkbox" id="c-42039750" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#42039270">parent</a><span>|</span><a href="#42039384">prev</a><span>|</span><a href="#42039386">next</a><span>|</span><label class="collapse" for="c-42039750">[-]</label><label class="expand" for="c-42039750">[1 more]</label></div><br/><div class="children"><div class="content">I am not English native, but this paper seem to be well written. It seems to be not fluent in storytelling, but that would be too high of an expectation. Can you point out some issues?</div><br/></div></div><div id="42039386" class="c"><input type="checkbox" id="c-42039386" checked=""/><div class="controls bullet"><span class="by">Jaxan</span><span>|</span><a href="#42039270">parent</a><span>|</span><a href="#42039750">prev</a><span>|</span><a href="#42039573">next</a><span>|</span><label class="collapse" for="c-42039386">[-]</label><label class="expand" for="c-42039386">[1 more]</label></div><br/><div class="children"><div class="content">It is not published. It is only a preprint.</div><br/></div></div></div></div><div id="42039573" class="c"><input type="checkbox" id="c-42039573" checked=""/><div class="controls bullet"><span class="by">dvh</span><span>|</span><a href="#42039270">prev</a><span>|</span><a href="#42039255">next</a><span>|</span><label class="collapse" for="c-42039573">[-]</label><label class="expand" for="c-42039573">[1 more]</label></div><br/><div class="children"><div class="content">So basically a lobotomy</div><br/></div></div><div id="42039255" class="c"><input type="checkbox" id="c-42039255" checked=""/><div class="controls bullet"><span class="by">ClassyJacket</span><span>|</span><a href="#42039573">prev</a><span>|</span><a href="#42038229">next</a><span>|</span><label class="collapse" for="c-42039255">[-]</label><label class="expand" for="c-42039255">[1 more]</label></div><br/><div class="children"><div class="content">So... repressed memories are real, if you&#x27;re an LLM?</div><br/></div></div><div id="42038229" class="c"><input type="checkbox" id="c-42038229" checked=""/><div class="controls bullet"><span class="by">vdvsvwvwvwvwv</span><span>|</span><a href="#42039255">prev</a><span>|</span><label class="collapse" for="c-42038229">[-]</label><label class="expand" for="c-42038229">[1 more]</label></div><br/><div class="children"><div class="content">Is this like giving the model a magic mushroom. It can access previously repressed memories. The unlearning part being like A Clockwork Orange.</div><br/></div></div></div></div></div></div></div></body></html>