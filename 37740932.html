<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696323664195" as="style"/><link rel="stylesheet" href="styles.css?v=1696323664195"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/mit-han-lab/streaming-llm">Efficient streaming language models with attention sinks</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>guywithabowtie</span> | <span>71 comments</span></div><br/><div><div id="37742473" class="c"><input type="checkbox" id="c-37742473" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#37741685">next</a><span>|</span><label class="collapse" for="c-37742473">[-]</label><label class="expand" for="c-37742473">[3 more]</label></div><br/><div class="children"><div class="content">I think people are misreading this work, and assuming this is equivalent to full dense-attention. This is just saying its an efficiency gain over sliding window re-computation, where instead of computing the L^2 cost over and over (T times), you can re-use a cache and maintain perplexity. I don&#x27;t think they are claiming that this allows for attending to content that was far away.<p>They tested by running concatenating and measuring -&gt; `Q A Q A Q A Q A...` not by doing  `Q Q Q Q A A A A...`<p>They also measure perplexity, showing that it produces &quot;readable text&quot; (coherent, locally viable); not that it is &quot;extracting anything&quot; from the big-triangle-gap of no-attention.<p>I think this would fail to be given a book, then write the first word of every paragraph. Or, given a book, write a 1 sentence summary of each chapter. I might be wrong, because they didn&#x27;t test tasks like this, but I&#x27;d be very very surprised.</div><br/><div id="37743273" class="c"><input type="checkbox" id="c-37743273" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#37742473">parent</a><span>|</span><a href="#37743708">next</a><span>|</span><label class="collapse" for="c-37743273">[-]</label><label class="expand" for="c-37743273">[1 more]</label></div><br/><div class="children"><div class="content">EDIT: the authors have updated the readme to add a clarified FAQ section that directly addresses this: <a href="https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;streaming-llm#faq">https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;streaming-llm#faq</a><p>Just tested it - this definitely doesn&#x27;t seem to be giving enhanced context length. It does run quickly though, can confirm it was using about 35 GB of an A100 RAM and pinned the usage for the entire duration.<p>I ran through by getting a book from project gutenberg, splitting it into paragraphs, and feeding them in paragraph by paragraph (asking it to say &quot;okay&quot; each paragraph), then at the end, asked some questions. It entirely hallucinated its answers. (also note: in the ~10 min of playing with this, i couldn&#x27;t get the base model (lmsys&#x2F;vicuna-13b-v1.3) to respond in english...)<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;bluecoconut&#x2F;9cae9e91fe3b1616ed650a96232acd58" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;bluecoconut&#x2F;9cae9e91fe3b1616ed650a96...</a></div><br/></div></div><div id="37743708" class="c"><input type="checkbox" id="c-37743708" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#37742473">parent</a><span>|</span><a href="#37743273">prev</a><span>|</span><a href="#37741685">next</a><span>|</span><label class="collapse" for="c-37743708">[-]</label><label class="expand" for="c-37743708">[1 more]</label></div><br/><div class="children"><div class="content">Correct, but to be fair to readers (like me) the use of the term &quot;infinite-length inputs&quot; is misleading.<p>Still, really interesting work.  The most salient bit is the discovery shown in Figure 2, summarized as:<p>&gt; (1) The attention maps in the first two layers (layers 0 and 1) exhibit the &quot;local&quot; pattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends to the initial token across all layers and heads.<p>&gt;  surprisingly large amount of attention score is allocated to the initial tokens, irrespective of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens “attention sinks&quot;. Despite their lack of semantic significance, they collect significant attention scores. We attribute the reason to the Softmax operation, which requires attention scores to sum up to one for all contextual tokens. Thus, even when the current query does not have a strong match in many previous tokens, the model still needs to allocate these unneeded attention values somewhere so it sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible to almost all subsequent tokens because of the autoregressive language modeling nature, making them more readily trained to serve as attention sinks.<p>StreamingLLM is basically a &quot;hack&quot; that fixes this odd behavior when we go around butchering the LLM&#x27;s attention window.<p>This actually isn&#x27;t the first time cracks have been shown in the usage of softmax and it makes me wonder if a different function might be better if we want context-length flexible LLMs.</div><br/></div></div></div></div><div id="37741685" class="c"><input type="checkbox" id="c-37741685" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#37742473">prev</a><span>|</span><a href="#37742465">next</a><span>|</span><label class="collapse" for="c-37741685">[-]</label><label class="expand" for="c-37741685">[6 more]</label></div><br/><div class="children"><div class="content">On a first quick pass, this looks so good that I&#x27;m wondering if it&#x27;s <i>too good to be true</i>!<p>But the work looks to be of decent quality and the technique is remarkably straightforward:<p>The idea is to apply attention over the first token and a sliding context window, ignoring everything in-between, in each layer.<p>By implication, each layer must be gradually shifting relevant information forward in the sequence, enabling the top layer&#x27;s ending sliding attention window to see it.<p>The only caveat I can think of is that the sliding windows won&#x27;t be able to shift all important information forward when the span of all sliding windows isn&#x27;t sufficient to span the entire sequence -- for example, when model depth × window length &lt; sequence length, if all windows have the same length.</div><br/><div id="37745612" class="c"><input type="checkbox" id="c-37745612" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37741685">parent</a><span>|</span><a href="#37742158">next</a><span>|</span><label class="collapse" for="c-37745612">[-]</label><label class="expand" for="c-37745612">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t wait for the github repo adaptation of the method!</div><br/></div></div><div id="37742158" class="c"><input type="checkbox" id="c-37742158" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37741685">parent</a><span>|</span><a href="#37745612">prev</a><span>|</span><a href="#37741919">next</a><span>|</span><label class="collapse" for="c-37742158">[-]</label><label class="expand" for="c-37742158">[3 more]</label></div><br/><div class="children"><div class="content">The end of the sequence could be padded with constant &quot;neutral&quot; values?</div><br/><div id="37742306" class="c"><input type="checkbox" id="c-37742306" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#37741685">root</a><span>|</span><a href="#37742158">parent</a><span>|</span><a href="#37741919">next</a><span>|</span><label class="collapse" for="c-37742306">[-]</label><label class="expand" for="c-37742306">[2 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t work. Imagine a sequence with 100 tokens, fed to a model with 10 layers, each with a sliding attention window spanning 5 tokens. The top layer&#x27;s final sliding window can only see 5 trailing tokens, each of which can only see 5 trailing tokens in the previous layer, and so on, for a total of 50 trailing tokens (plus the initial token) of maximum trailing context in the top layer.<p>It&#x27;s an inherent limitation of this approach.</div><br/><div id="37742434" class="c"><input type="checkbox" id="c-37742434" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37741685">root</a><span>|</span><a href="#37742306">parent</a><span>|</span><a href="#37741919">next</a><span>|</span><label class="collapse" for="c-37742434">[-]</label><label class="expand" for="c-37742434">[1 more]</label></div><br/><div class="children"><div class="content">How about neutral value padding at the other end?<p>I am having trouble visualizing this.</div><br/></div></div></div></div></div></div></div></div><div id="37742465" class="c"><input type="checkbox" id="c-37742465" checked=""/><div class="controls bullet"><span class="by">huevosabio</span><span>|</span><a href="#37741685">prev</a><span>|</span><a href="#37748414">next</a><span>|</span><label class="collapse" for="c-37742465">[-]</label><label class="expand" for="c-37742465">[3 more]</label></div><br/><div class="children"><div class="content">This seems to be largely enabled by the observation that Softmax has to add up to one. From quick a glance [1], the model tends to use the first token as a placeholder for cases when you don&#x27;t need to attend any of the prior tokens.<p>The first time I read about this issue, that Softmax is somewhat flawed, was in a HN post by Evan Miller [2] where he observes that forcing attention heads to allocate all attention to prior tokens is wrong, and we should allow them to &quot;not attend&quot; by adding one to the softmax denominator.<p>I love that they found a way to capitalize on this observation without having to retrain models. However, I wonder how the models would look like if they followed Evan&#x27;s suggestion!<p>[1] Their description of attention sinks:<p>```<p>To understand the failure of window attention, we find an interesting phenomenon of autoregressive
LLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective
of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens
“attention sinks&quot;. Despite their lack of semantic significance, they collect significant attention scores.
We attribute the reason to the Softmax operation, which requires attention scores to sum up to one
for all contextual tokens. Thus, even when the current query does not have a strong match in many
previous tokens, the model still needs to allocate these unneeded attention values somewhere so it
sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible
to almost all subsequent tokens because of the autoregressive language modeling nature, making
them more readily trained to serve as attention sinks.<p>```<p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36851494">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36851494</a></div><br/><div id="37742580" class="c"><input type="checkbox" id="c-37742580" checked=""/><div class="controls bullet"><span class="by">huevosabio</span><span>|</span><a href="#37742465">parent</a><span>|</span><a href="#37743736">next</a><span>|</span><label class="collapse" for="c-37742580">[-]</label><label class="expand" for="c-37742580">[1 more]</label></div><br/><div class="children"><div class="content">Actually, seems like they did try the suggestion out, basically by training a model with a dedicated sink token with all zeros.<p>The verdict seems to be that you still end up with other initial tokens being used as sinks, so it is better to have a dedicated sink token.</div><br/></div></div><div id="37743736" class="c"><input type="checkbox" id="c-37743736" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#37742465">parent</a><span>|</span><a href="#37742580">prev</a><span>|</span><a href="#37748414">next</a><span>|</span><label class="collapse" for="c-37743736">[-]</label><label class="expand" for="c-37743736">[1 more]</label></div><br/><div class="children"><div class="content">That was the first time I&#x27;d read about it on HN, but as pointed out on that HN post it wasn&#x27;t the first time Softmax + 1 was proposed.  And, AFAIK, it has never resulted in better performance in practice.  Maybe Softmax + 1 works better for fiddling with the attention window after training, but I don&#x27;t know if anyone has tested that at scale.</div><br/></div></div></div></div><div id="37748414" class="c"><input type="checkbox" id="c-37748414" checked=""/><div class="controls bullet"><span class="by">__rito__</span><span>|</span><a href="#37742465">prev</a><span>|</span><a href="#37742094">next</a><span>|</span><label class="collapse" for="c-37748414">[-]</label><label class="expand" for="c-37748414">[1 more]</label></div><br/><div class="children"><div class="content">Relevant: the eponymous Professor Han at MIT is teaching a TinyML course that is open to the public. See:<p>- <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37620507">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37620507</a><p>- <a href="https:&#x2F;&#x2F;efficientml.ai" rel="nofollow noreferrer">https:&#x2F;&#x2F;efficientml.ai</a></div><br/></div></div><div id="37742493" class="c"><input type="checkbox" id="c-37742493" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#37742094">prev</a><span>|</span><a href="#37742064">next</a><span>|</span><label class="collapse" for="c-37742493">[-]</label><label class="expand" for="c-37742493">[1 more]</label></div><br/><div class="children"><div class="content">I could be wrong, but I&#x27;m not sure this is about what people seem to think it is, e.g., letting LLMs reference content past the trained length<p>I think it may just be about the performance of the model with longer texts (on the things still within the context window?). It sounds like they&#x27;re arguing that the model is essentially learning to stick some baggage in the attention to the initial tokens of the text, and break when that isn&#x27;t within the window anymore for reasons I&#x27;m not sure I understand (after all, isn&#x27;t text in the middle just as good as text at the start for non instruction inputs?)</div><br/></div></div><div id="37742064" class="c"><input type="checkbox" id="c-37742064" checked=""/><div class="controls bullet"><span class="by">smeeth</span><span>|</span><a href="#37742493">prev</a><span>|</span><a href="#37742131">next</a><span>|</span><label class="collapse" for="c-37742064">[-]</label><label class="expand" for="c-37742064">[2 more]</label></div><br/><div class="children"><div class="content">Adding attention cache memory is an extremely interesting solution to this problem.<p>If anyone is curious, there was another paper [0] that came out a few days ago that made a related observation in Vision Transformers. Transformer models appear to pick tokens to store global information in - they need tokens to &quot;think&quot;. You can eek some performance improvements (and cool explanation images) by providing the model with specific tokens for this purpose.<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.16588.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.16588.pdf</a></div><br/><div id="37742534" class="c"><input type="checkbox" id="c-37742534" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37742064">parent</a><span>|</span><a href="#37742131">next</a><span>|</span><label class="collapse" for="c-37742534">[-]</label><label class="expand" for="c-37742534">[1 more]</label></div><br/><div class="children"><div class="content">It would be an interesting place to add additional units to an already trained model, to continue training and get better performance, or to fine tuning.<p>For tuning, keep the original model parameters fixed, and only let the model adjust parameters to and from new &quot;tuning&quot; cache units.<p>This would allow different tuning unit sets to be swapped in, or even used together. Foul language avoidance units + specific terminology units + be concise units, etc.<p>Mix and match tuned unit sets, like super prompts.<p>--<p>If the number of new parameters is low enough, higher order optimization (requiring higher memory) might be a possibility for very fast and effective tuning.<p>--<p>And maybe grow the sequence length, and number of units, during training. A few units for short sequences. Then increase training sequence length, add more units, continue training, and so on.<p>Perhaps some kind of performance or gradient analysis could govern cache expansion, so an arbitrary schedule is not required.</div><br/></div></div></div></div><div id="37742131" class="c"><input type="checkbox" id="c-37742131" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#37742064">prev</a><span>|</span><a href="#37741225">next</a><span>|</span><label class="collapse" for="c-37742131">[-]</label><label class="expand" for="c-37742131">[10 more]</label></div><br/><div class="children"><div class="content">My somewhat facetious take is that LLMs are trying really hard to reinvent RNNs and would do so if we just gave them the tools to do so.</div><br/><div id="37742334" class="c"><input type="checkbox" id="c-37742334" checked=""/><div class="controls bullet"><span class="by">obblekk</span><span>|</span><a href="#37742131">parent</a><span>|</span><a href="#37742452">next</a><span>|</span><label class="collapse" for="c-37742334">[-]</label><label class="expand" for="c-37742334">[1 more]</label></div><br/><div class="children"><div class="content">RNNs are the correct solution, but infeasibly expensive to run.<p>A different way to think about it is Transformer models are trying to predict which part of the RNN network is &quot;worth&quot; keeping given a resource constraint.<p>Transformers use a simple heuristic today (and this result makes the heuristic better). Just like many NP complete problems, there might be approximations that are not perfectly correct but still useful. Transformers prove that is the case for neural networks.</div><br/></div></div><div id="37742452" class="c"><input type="checkbox" id="c-37742452" checked=""/><div class="controls bullet"><span class="by">tkellogg</span><span>|</span><a href="#37742131">parent</a><span>|</span><a href="#37742334">prev</a><span>|</span><a href="#37742346">next</a><span>|</span><label class="collapse" for="c-37742452">[-]</label><label class="expand" for="c-37742452">[5 more]</label></div><br/><div class="children"><div class="content">One such project is RWKV[1]. On the open source leaderboard it lived in the middle of the board for a while, so it really is a legit approach, it&#x27;s just not hot.<p>[1]: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;rwkv" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;rwkv</a></div><br/><div id="37742640" class="c"><input type="checkbox" id="c-37742640" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37742131">root</a><span>|</span><a href="#37742452">parent</a><span>|</span><a href="#37742346">next</a><span>|</span><label class="collapse" for="c-37742640">[-]</label><label class="expand" for="c-37742640">[4 more]</label></div><br/><div class="children"><div class="content">side note - do you think the open source leaderboard is a fair representation of the diversity of OSS models?</div><br/><div id="37746069" class="c"><input type="checkbox" id="c-37746069" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37742131">root</a><span>|</span><a href="#37742640">parent</a><span>|</span><a href="#37742346">next</a><span>|</span><label class="collapse" for="c-37746069">[-]</label><label class="expand" for="c-37746069">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the best we have for large scale comparison I think. But the major problem is that the tests are public and you can easily cheat by including them in your training set and increase your score an arbitrary amount. You might even be able to do it in a way that would be difficult to detect. And it can happen by accident as well. There are already many cases of intentional and disclosed contamination in the leaderboard.<p>What&#x27;s really needed is a leaderboard based on some private test sets, where you can submit a model to be judged and some entity will run it on their own machines without disclosing the tests to you. Even that could be vulnerable to cheating if the submission process is automated and you can submit many times, as you could use the score as feedback to disclose information about the contents of the private test set. So it would take some care to run such a service, even beyond normal security concerns like securing the sandbox that submitted code runs in, etc.</div><br/><div id="37746121" class="c"><input type="checkbox" id="c-37746121" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37742131">root</a><span>|</span><a href="#37746069">parent</a><span>|</span><a href="#37742346">next</a><span>|</span><label class="collapse" for="c-37746121">[-]</label><label class="expand" for="c-37746121">[2 more]</label></div><br/><div class="children"><div class="content">Who would you trust (and verify) to run such a leaderboard?</div><br/><div id="37746394" class="c"><input type="checkbox" id="c-37746394" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37742131">root</a><span>|</span><a href="#37746121">parent</a><span>|</span><a href="#37742346">next</a><span>|</span><label class="collapse" for="c-37746394">[-]</label><label class="expand" for="c-37746394">[1 more]</label></div><br/><div class="children"><div class="content">Any of the big labs I guess. At least as long as the leaderboard didn&#x27;t become something so important that even the big labs would be incentivized to cheat on it. In that case, I don&#x27;t know.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37742263" class="c"><input type="checkbox" id="c-37742263" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37742131">parent</a><span>|</span><a href="#37742346">prev</a><span>|</span><a href="#37742344">next</a><span>|</span><label class="collapse" for="c-37742263">[-]</label><label class="expand" for="c-37742263">[1 more]</label></div><br/><div class="children"><div class="content">Yes, indeedy.<p>Many things learned over the last three decades with smaller (the current terminology is &quot;extremely tiny&quot;! :) neural networks are being revisited for these large models.</div><br/></div></div><div id="37742344" class="c"><input type="checkbox" id="c-37742344" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#37742131">parent</a><span>|</span><a href="#37742263">prev</a><span>|</span><a href="#37741225">next</a><span>|</span><label class="collapse" for="c-37742344">[-]</label><label class="expand" for="c-37742344">[1 more]</label></div><br/><div class="children"><div class="content">I think many people believe you. The main advantage of transformers over RNNs is training parallelization. RNNs are hard because training suffers from vanishing gradients and also because it&#x27;s hard to get full utilization (needs large batches to get good utilization).<p>The existence of models like RWKV indicates that there is potentially a future in training like a transformer but inferring like an RNN.</div><br/></div></div></div></div><div id="37741225" class="c"><input type="checkbox" id="c-37741225" checked=""/><div class="controls bullet"><span class="by">WhatsName</span><span>|</span><a href="#37742131">prev</a><span>|</span><a href="#37743222">next</a><span>|</span><label class="collapse" for="c-37741225">[-]</label><label class="expand" for="c-37741225">[3 more]</label></div><br/><div class="children"><div class="content">So I can let llama2 summarize books now or are there any non-obvious caveats to this approach?</div><br/><div id="37746352" class="c"><input type="checkbox" id="c-37746352" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#37741225">parent</a><span>|</span><a href="#37744186">next</a><span>|</span><label class="collapse" for="c-37746352">[-]</label><label class="expand" for="c-37746352">[1 more]</label></div><br/><div class="children"><div class="content">If you want to do that, I have a model trained specifically on a dataset of building recursive summaries. Some of my training documents are 40-50k tokens.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Tostino&#x2F;Inkbot-13B-8k-0.2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Tostino&#x2F;Inkbot-13B-8k-0.2</a><p>Just chunk your document up, and pass in the prior summary along with this chunk of text, you can mention that it is chunk X of Y if you want (which can help with how it starts the summary often).</div><br/></div></div><div id="37744186" class="c"><input type="checkbox" id="c-37744186" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#37741225">parent</a><span>|</span><a href="#37746352">prev</a><span>|</span><a href="#37743222">next</a><span>|</span><label class="collapse" for="c-37744186">[-]</label><label class="expand" for="c-37744186">[1 more]</label></div><br/><div class="children"><div class="content">No. This does nothing to the context length itself which is still a sliding window.</div><br/></div></div></div></div><div id="37743222" class="c"><input type="checkbox" id="c-37743222" checked=""/><div class="controls bullet"><span class="by">Van_Chopiszt</span><span>|</span><a href="#37741225">prev</a><span>|</span><a href="#37748418">next</a><span>|</span><label class="collapse" for="c-37743222">[-]</label><label class="expand" for="c-37743222">[3 more]</label></div><br/><div class="children"><div class="content">The authors just uploaded a FAQ section, which may clarify some of the confusions: <a href="https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;streaming-llm&#x2F;blob&#x2F;main&#x2F;README.md#faq">https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;streaming-llm&#x2F;blob&#x2F;main&#x2F;READM...</a></div><br/><div id="37743338" class="c"><input type="checkbox" id="c-37743338" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#37743222">parent</a><span>|</span><a href="#37748418">next</a><span>|</span><label class="collapse" for="c-37743338">[-]</label><label class="expand" for="c-37743338">[2 more]</label></div><br/><div class="children"><div class="content">Nice update. I think the key question they added that clarifies a lot is #3 (quoted below)<p><pre><code>    Can I input an extensive text, like a book, into StreamingLLM for summarization?

    While you can input a lengthy text, the model will only recognize the latest tokens. Thus, if a book is an input, StreamingLLM might only summarize the concluding paragraphs, which might not be very insightful. As emphasized earlier, we neither expand the LLMs&#x27; context window nor enhance their long-term memory. StreamingLLM&#x27;s strength lies in generating fluent text from recent tokens without needing a cache refresh.</code></pre></div><br/><div id="37748358" class="c"><input type="checkbox" id="c-37748358" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#37743222">root</a><span>|</span><a href="#37743338">parent</a><span>|</span><a href="#37748418">next</a><span>|</span><label class="collapse" for="c-37748358">[-]</label><label class="expand" for="c-37748358">[1 more]</label></div><br/><div class="children"><div class="content">So instead of chunks of tokens, we can input stream of tokens and then some point say &quot;LLM take a wheel&quot;. So it is very nice but not revolutionary.</div><br/></div></div></div></div></div></div><div id="37748418" class="c"><input type="checkbox" id="c-37748418" checked=""/><div class="controls bullet"><span class="by">Trapais</span><span>|</span><a href="#37743222">prev</a><span>|</span><a href="#37742026">next</a><span>|</span><label class="collapse" for="c-37748418">[-]</label><label class="expand" for="c-37748418">[1 more]</label></div><br/><div class="children"><div class="content">Looks like longformer to me. They just renamed &quot;global attention&quot; into &quot;attention sink&quot; and removed silly parts(distilled attention) and BERT parts([CLS] saw all N tokens, there is no need for BOS to see all tokens)</div><br/></div></div><div id="37742026" class="c"><input type="checkbox" id="c-37742026" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#37748418">prev</a><span>|</span><a href="#37742963">next</a><span>|</span><label class="collapse" for="c-37742026">[-]</label><label class="expand" for="c-37742026">[14 more]</label></div><br/><div class="children"><div class="content">How do any of these sliding window techniques handle instructions that are non expected and only show up at the end? For example imagine feeding a book to the model and the last sentence being the instruction “return the count of the letter m in the previous input”. A human would handle this by first letting out an exasperated sigh but then restarting the reading while counting. An LLM has no ability to loop back and re-read the input. (Ignore LLM issues with character counting for this example). It seems like to solve this problem for real the LLM needs to be able to loop and jump arbitrarily, but I’m sure that would introduce a whole new host of issues and possibly require a new architecture all together.</div><br/><div id="37746168" class="c"><input type="checkbox" id="c-37746168" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#37742026">parent</a><span>|</span><a href="#37742081">next</a><span>|</span><label class="collapse" for="c-37746168">[-]</label><label class="expand" for="c-37746168">[1 more]</label></div><br/><div class="children"><div class="content">On a similar note, I can&#x27;t wait for LLMs to digest _all_ the research papers readable enough for them and accessible, &quot;take notes&quot; in an index-suitable format&#x2F;structure, and then act similar to a human who&#x27;d done that over an obviously more limited corpus: respond to questions by translating them into relevant key words, looking them up, _skimming the contents again,_ and finding relevant information. Might not be useful, and thus necessitate further visits to the index&#x2F;library.<p>With the needed preprocessing, a LLM that can &quot;go and do some research to adequately respond&quot; could be extremely powerful.<p>We&#x27;ve spent the last ~10 millennia improving knowledge management technology to scale beyond the capacity&#x2F;time of individual brains. Let the language model use actual research on this and pre-digest, not just Bing search.
No need for it&#x27;s short term memory to remember what say piece of code did something, just tag it when reading and rely on scalable shared indexing of tags.<p>Though the more I think about it, the more it sounds like normal LLM pretraining with the knowledge index being the giant chunk of LLM weights.</div><br/></div></div><div id="37742081" class="c"><input type="checkbox" id="c-37742081" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#37742026">parent</a><span>|</span><a href="#37746168">prev</a><span>|</span><a href="#37742097">next</a><span>|</span><label class="collapse" for="c-37742081">[-]</label><label class="expand" for="c-37742081">[1 more]</label></div><br/><div class="children"><div class="content">One option would be similar to function calling, give the llm an output it can make that changes how the context is parsed. That&#x27;s a layer on top rather than changing how the llm itself works.</div><br/></div></div><div id="37742097" class="c"><input type="checkbox" id="c-37742097" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#37742026">parent</a><span>|</span><a href="#37742081">prev</a><span>|</span><a href="#37742080">next</a><span>|</span><label class="collapse" for="c-37742097">[-]</label><label class="expand" for="c-37742097">[2 more]</label></div><br/><div class="children"><div class="content">Does an LLM need to loop back to re-read its input, even in a regular (read non-sliding) context window?<p>Maybe I&#x27;m misunderstanding, but doesn&#x27;t the hidden state solve the &quot;lookup&quot; problem in this case? In the sense that the LLM needs to ingest your entire input anyway before answering, then whether your instruction is at the front or at the end carries little impact besides on attention.</div><br/><div id="37742289" class="c"><input type="checkbox" id="c-37742289" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#37742026">root</a><span>|</span><a href="#37742097">parent</a><span>|</span><a href="#37742080">next</a><span>|</span><label class="collapse" for="c-37742289">[-]</label><label class="expand" for="c-37742289">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s my understanding that in regular non-sliding window context models the llm is able to pay attention to any part of the input when generating the output. The attention head is essentially able to jump back and forward to any point in its context window. This is what differentiates the attention mechanism from other models that use token proximity as a proxy for relevance.</div><br/></div></div></div></div><div id="37742080" class="c"><input type="checkbox" id="c-37742080" checked=""/><div class="controls bullet"><span class="by">tornato7</span><span>|</span><a href="#37742026">parent</a><span>|</span><a href="#37742097">prev</a><span>|</span><a href="#37742141">next</a><span>|</span><label class="collapse" for="c-37742080">[-]</label><label class="expand" for="c-37742080">[2 more]</label></div><br/><div class="children"><div class="content">Is it so hard to ask the user to put instructions at the beginning? Claude 100K asks users to put instructions at the end.<p>Or you just use a quick model to check if there area instructions at the end and bring it to the beginning.</div><br/><div id="37746728" class="c"><input type="checkbox" id="c-37746728" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#37742026">root</a><span>|</span><a href="#37742080">parent</a><span>|</span><a href="#37742141">next</a><span>|</span><label class="collapse" for="c-37746728">[-]</label><label class="expand" for="c-37746728">[1 more]</label></div><br/><div class="children"><div class="content">The fact that people are still treating it like entirely raw text input is insane to me. If you have a document, have a separate input for the user to paste&#x2F;upload data, and then another for the user&#x27;s instruction.<p>That allows you to do things like chunk the document while leaving the rest of their instruction alone, or do a sliding window of just the document while your instruction stays static.</div><br/></div></div></div></div><div id="37742141" class="c"><input type="checkbox" id="c-37742141" checked=""/><div class="controls bullet"><span class="by">alex_duf</span><span>|</span><a href="#37742026">parent</a><span>|</span><a href="#37742080">prev</a><span>|</span><a href="#37742271">next</a><span>|</span><label class="collapse" for="c-37742141">[-]</label><label class="expand" for="c-37742141">[6 more]</label></div><br/><div class="children"><div class="content">The example seems like a weird edge case. I don&#x27;t even know if current models are capable of this in a short input.</div><br/><div id="37742324" class="c"><input type="checkbox" id="c-37742324" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#37742026">root</a><span>|</span><a href="#37742141">parent</a><span>|</span><a href="#37742312">next</a><span>|</span><label class="collapse" for="c-37742324">[-]</label><label class="expand" for="c-37742324">[1 more]</label></div><br/><div class="children"><div class="content">Ignore the specific example of counting characters, I was just quickly coming up with a situation where the instruction is at the end of the input. Here is a better example:<p>Input the full text of a novel, then ask for a minor detail (eg color of a car that is briefly mentioned in the middle of the book). Again a human can do this by flipping back to the relevant section but LLMs have no mechanism for this when using a sliding window attention scheme.<p>If the full input can fit in the context window then any LLM today would be able to extract the color of the car.</div><br/></div></div><div id="37742312" class="c"><input type="checkbox" id="c-37742312" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37742026">root</a><span>|</span><a href="#37742141">parent</a><span>|</span><a href="#37742324">prev</a><span>|</span><a href="#37742271">next</a><span>|</span><label class="collapse" for="c-37742312">[-]</label><label class="expand" for="c-37742312">[4 more]</label></div><br/><div class="children"><div class="content">I agree, even just tokenization  screws you here, I&#x27;m 95% sure. I.e. the raw input isn&#x27;t letters but one of 100K integers that represent some set of letters.<p>That being said, probably a naive take, since we&#x27;re seeing them do so much. &amp; I bet we could get it to count correctly with at least some short input, and given infinite runs, probably trivial. (I.e. for N characters, split into N inputs, for each one &quot;say true if it is an M, false otherwise,)</div><br/><div id="37742386" class="c"><input type="checkbox" id="c-37742386" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#37742026">root</a><span>|</span><a href="#37742312">parent</a><span>|</span><a href="#37742271">next</a><span>|</span><label class="collapse" for="c-37742386">[-]</label><label class="expand" for="c-37742386">[3 more]</label></div><br/><div class="children"><div class="content">I understand that, which is why I said &quot;Ignore LLM issues with character counting for this example&quot;. It was a quick example, please see my other comment with a better example.</div><br/><div id="37742450" class="c"><input type="checkbox" id="c-37742450" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37742026">root</a><span>|</span><a href="#37742386">parent</a><span>|</span><a href="#37742271">next</a><span>|</span><label class="collapse" for="c-37742450">[-]</label><label class="expand" for="c-37742450">[2 more]</label></div><br/><div class="children"><div class="content">I see, active listening + relating it to my knowledge on my end, lmk if I compressed too much:<p>you&#x27;re curious if there&#x27;s noticably worse performance if the Q is at the end of content rather than before<p>No, there&#x27;s a good paper on this somewhere with the Claude 100K, tldr it&#x27;s sort of bow-shaped, beginning and end had equally high rates but middle would suffer</div><br/><div id="37742511" class="c"><input type="checkbox" id="c-37742511" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#37742026">root</a><span>|</span><a href="#37742450">parent</a><span>|</span><a href="#37742271">next</a><span>|</span><label class="collapse" for="c-37742511">[-]</label><label class="expand" for="c-37742511">[1 more]</label></div><br/><div class="children"><div class="content">No, what I am specifically asking about is these sliding window attention techniques. As far as I understand it Claude 100K actually uses a 100k context window, and not a sliding window.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37742963" class="c"><input type="checkbox" id="c-37742963" checked=""/><div class="controls bullet"><span class="by">ilovefood</span><span>|</span><a href="#37742026">prev</a><span>|</span><a href="#37741171">next</a><span>|</span><label class="collapse" for="c-37742963">[-]</label><label class="expand" for="c-37742963">[3 more]</label></div><br/><div class="children"><div class="content">This is working relatively well, the code is really worth a read. If you run it locally, consider the open PR and install sentencepiece as well. It&#x27;s been generating text for the past 10 minutes now :D<p>Some of the instructions are ignored though so I&#x27;d be careful there, one instruction is to rewrite the previous response by &quot;starting every sentence with the letter A&quot; which is a bit of a hit or miss right now.</div><br/><div id="37743043" class="c"><input type="checkbox" id="c-37743043" checked=""/><div class="controls bullet"><span class="by">guywithabowtie</span><span>|</span><a href="#37742963">parent</a><span>|</span><a href="#37741171">next</a><span>|</span><label class="collapse" for="c-37743043">[-]</label><label class="expand" for="c-37743043">[2 more]</label></div><br/><div class="children"><div class="content">How is the content quality ?</div><br/><div id="37743343" class="c"><input type="checkbox" id="c-37743343" checked=""/><div class="controls bullet"><span class="by">ilovefood</span><span>|</span><a href="#37742963">root</a><span>|</span><a href="#37743043">parent</a><span>|</span><a href="#37741171">next</a><span>|</span><label class="collapse" for="c-37743343">[-]</label><label class="expand" for="c-37743343">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s okay I have to say. I just ran out of memory on my 4090, so I had to retry on an A100. Here&#x27;s an extract: <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;pzLfCFWt" rel="nofollow noreferrer">https:&#x2F;&#x2F;pastebin.com&#x2F;pzLfCFWt</a><p>I think something might be off with the example. Can&#x27;t wait for this stuff to work on llama.cpp. Going to try it with mistral &amp; stable lm now, thankfully tomorrow is a holiday in Germany :)</div><br/></div></div></div></div></div></div><div id="37741171" class="c"><input type="checkbox" id="c-37741171" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#37742963">prev</a><span>|</span><a href="#37741657">next</a><span>|</span><label class="collapse" for="c-37741171">[-]</label><label class="expand" for="c-37741171">[2 more]</label></div><br/><div class="children"><div class="content">Okay, what&#x27;s the downside this time?</div><br/><div id="37741645" class="c"><input type="checkbox" id="c-37741645" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#37741171">parent</a><span>|</span><a href="#37741657">next</a><span>|</span><label class="collapse" for="c-37741645">[-]</label><label class="expand" for="c-37741645">[1 more]</label></div><br/><div class="children"><div class="content">Allegedly not “efficiency or performance”, though I’m skeptical. Will dig into this later and update my comment (if I remember).</div><br/></div></div></div></div><div id="37741657" class="c"><input type="checkbox" id="c-37741657" checked=""/><div class="controls bullet"><span class="by">13years</span><span>|</span><a href="#37741171">prev</a><span>|</span><a href="#37741651">next</a><span>|</span><label class="collapse" for="c-37741657">[-]</label><label class="expand" for="c-37741657">[2 more]</label></div><br/><div class="children"><div class="content">So can it now understand and write complete applications?</div><br/><div id="37742058" class="c"><input type="checkbox" id="c-37742058" checked=""/><div class="controls bullet"><span class="by">Jeff_Brown</span><span>|</span><a href="#37741657">parent</a><span>|</span><a href="#37741651">next</a><span>|</span><label class="collapse" for="c-37742058">[-]</label><label class="expand" for="c-37742058">[1 more]</label></div><br/><div class="children"><div class="content">It seems hard to imagine, if its training has been on small chunks of text, that the model has a way of understanding a large codebase.<p>But this stuff keeps on surprising me.</div><br/></div></div></div></div><div id="37741651" class="c"><input type="checkbox" id="c-37741651" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#37741657">prev</a><span>|</span><a href="#37745575">next</a><span>|</span><label class="collapse" for="c-37741651">[-]</label><label class="expand" for="c-37741651">[1 more]</label></div><br/><div class="children"><div class="content">This is a big claim, curious to see what the caveats are.</div><br/></div></div><div id="37745575" class="c"><input type="checkbox" id="c-37745575" checked=""/><div class="controls bullet"><span class="by">arxiv_papers</span><span>|</span><a href="#37741651">prev</a><span>|</span><a href="#37741826">next</a><span>|</span><label class="collapse" for="c-37745575">[-]</label><label class="expand" for="c-37745575">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;youtu.be&#x2F;hfJIOd2WCQ0" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;hfJIOd2WCQ0</a></div><br/></div></div><div id="37741826" class="c"><input type="checkbox" id="c-37741826" checked=""/><div class="controls bullet"><span class="by">heavyarms</span><span>|</span><a href="#37745575">prev</a><span>|</span><a href="#37741708">next</a><span>|</span><label class="collapse" for="c-37741826">[-]</label><label class="expand" for="c-37741826">[2 more]</label></div><br/><div class="children"><div class="content">Having only read the abstract, I&#x27;m probably way off the mark here, but my first thought was: LLM + LSTM.</div><br/></div></div><div id="37741708" class="c"><input type="checkbox" id="c-37741708" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#37741826">prev</a><span>|</span><a href="#37742028">next</a><span>|</span><label class="collapse" for="c-37741708">[-]</label><label class="expand" for="c-37741708">[1 more]</label></div><br/><div class="children"><div class="content">I feel like information theory prevents full information retention for unlimited context lengths and finite compute, but I don&#x27;t know if we are at information theory limits to invoke this argument. Or rather, I don&#x27;t know how to make a good analysis of (bits of context information) per (bits of model parameters).</div><br/></div></div><div id="37742028" class="c"><input type="checkbox" id="c-37742028" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37741708">prev</a><span>|</span><a href="#37740933">next</a><span>|</span><label class="collapse" for="c-37742028">[-]</label><label class="expand" for="c-37742028">[1 more]</label></div><br/><div class="children"><div class="content">This looks fantastic. Also answers the relevancy of the &quot;off-by-one&quot; softmax*<p>My naive question is...does it work? But that sounds dismissive. At length:<p>It shows that the model can&#x27;t respond after a certain length versus a proposed model that does continue to respond.<p>But can a model that continues to respond retrieve information far &quot;in the past&quot;?<p>The demo video is too low-level, at least to my brain. It shows one model stops responding but the proposed one continues.<p>I spent about 5 minutes going frame by frame to see if the proposed model attempts to have to &quot;recall&quot; information from further back, but it looks like no.<p>Perfection here isn&#x27;t necessary or even possible AFAIK, i.e. I don&#x27;t expect it to recall page 1 100% accurately at page 1000. But can it recall _anything_ from it, even if it ignores it?<p>The great thing about this era and work is we can check. But I hope someone has it up in a HuggingFace space before I figure out how to run it myself. :P<p>I&#x27;m leaning no, based on the sliding window thing. It sounds like there&#x27;s 4 fixed tokens, then the last context size - 4 tokens, that&#x27;s it<p>* at the time, two camps: one, it&#x27;s some random person saying it and there&#x27;s prior art on implementations that do the off-by-one. Two, you&#x27;d be surprised how much little things go unnoticed by large groups, and do matter.</div><br/></div></div><div id="37740933" class="c"><input type="checkbox" id="c-37740933" checked=""/><div class="controls bullet"><span class="by">guywithabowtie</span><span>|</span><a href="#37742028">prev</a><span>|</span><a href="#37748149">next</a><span>|</span><label class="collapse" for="c-37740933">[-]</label><label class="expand" for="c-37740933">[5 more]</label></div><br/><div class="children"><div class="content">We introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.</div><br/><div id="37742161" class="c"><input type="checkbox" id="c-37742161" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#37740933">parent</a><span>|</span><a href="#37742247">next</a><span>|</span><label class="collapse" for="c-37742161">[-]</label><label class="expand" for="c-37742161">[3 more]</label></div><br/><div class="children"><div class="content">Sorry, what does &quot;up to 4 million tokens and more&quot; mean? It seems like a contradiction.</div><br/><div id="37742222" class="c"><input type="checkbox" id="c-37742222" checked=""/><div class="controls bullet"><span class="by">catskul2</span><span>|</span><a href="#37740933">root</a><span>|</span><a href="#37742161">parent</a><span>|</span><a href="#37742221">next</a><span>|</span><label class="collapse" for="c-37742222">[-]</label><label class="expand" for="c-37742222">[1 more]</label></div><br/><div class="children"><div class="content">Not really a contradiction so much as redundant&#x2F;poorly worded. Should have said, &quot;at least 4 million tokens&quot;.</div><br/></div></div><div id="37742221" class="c"><input type="checkbox" id="c-37742221" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#37740933">root</a><span>|</span><a href="#37742161">parent</a><span>|</span><a href="#37742222">prev</a><span>|</span><a href="#37742247">next</a><span>|</span><label class="collapse" for="c-37742221">[-]</label><label class="expand" for="c-37742221">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a reference describing what a context window for LLMs is:<p><a href="https:&#x2F;&#x2F;www.hopsworks.ai&#x2F;dictionary&#x2F;context-window-for-llms" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.hopsworks.ai&#x2F;dictionary&#x2F;context-window-for-llms</a></div><br/></div></div></div></div></div></div><div id="37742340" class="c"><input type="checkbox" id="c-37742340" checked=""/><div class="controls bullet"><span class="by">kridsdale3</span><span>|</span><a href="#37744868">prev</a><span>|</span><label class="collapse" for="c-37742340">[-]</label><label class="expand" for="c-37742340">[2 more]</label></div><br/><div class="children"><div class="content">What if my &quot;Favorite LLM&quot; is GPT4? I don&#x27;t want to use Llama or anything like that. Does this GitHub code let me use the OpenAI API and run the new memory technique on top of that?</div><br/><div id="37742365" class="c"><input type="checkbox" id="c-37742365" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#37742340">parent</a><span>|</span><label class="collapse" for="c-37742365">[-]</label><label class="expand" for="c-37742365">[1 more]</label></div><br/><div class="children"><div class="content">No, it does not</div><br/></div></div></div></div></div></div></div></div></div></body></html>