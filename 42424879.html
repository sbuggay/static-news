<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734339678010" as="style"/><link rel="stylesheet" href="styles.css?v=1734339678010"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://rish-01.github.io/blog/posts/ml_estimation/">Maximum likelihood estimation and loss functions</a> <span class="domain">(<a href="https://rish-01.github.io">rish-01.github.io</a>)</span></div><div class="subtext"><span>snprajwal</span> | <span>19 comments</span></div><br/><div><div id="42426951" class="c"><input type="checkbox" id="c-42426951" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#42428407">next</a><span>|</span><label class="collapse" for="c-42426951">[-]</label><label class="expand" for="c-42426951">[10 more]</label></div><br/><div class="children"><div class="content">Somehow I&#x27;ve never found an explanation for MLE that&#x27;s intuitive and suitable for someone who didn&#x27;t take graduate-level statistics already. I&#x27;m 100% on board with the introduction (MSE and cross-entropy make total intuitive sense; you can see how they penalize &#x27;wrongness&#x27; to an increasing degree) but in the next paragraph we jump right to:<p>&gt;Let pmodel(x;θ) be a parametric family of distributions over a space of parameters ...<p>and it&#x27;s straight to the grad-level textbook stuff that breezily assumes familiarity with advanced mathematical notation.<p>One of the reasons I loved Andrew Ng&#x27;s machine learning course so much is that it eased you into understanding the notation, terminology, and signposted things like &quot;hey this is really important&quot; vs. &quot;hey this is just a weird notational quirk that mathematicians have, don&#x27;t worry about it too much.&quot;</div><br/><div id="42427151" class="c"><input type="checkbox" id="c-42427151" checked=""/><div class="controls bullet"><span class="by">jey</span><span>|</span><a href="#42426951">parent</a><span>|</span><a href="#42427452">next</a><span>|</span><label class="collapse" for="c-42427151">[-]</label><label class="expand" for="c-42427151">[4 more]</label></div><br/><div class="children"><div class="content">This notation doesn&#x27;t require graduate-level statistics knowledge, it&#x27;s more like stuff that would be covered in a first mathematical course on probability and statistics. It&#x27;s totally practical to learn this stuff on your own from videos, books, and PDFs. First you need to get a solid conceptual grasp on probability distributions and their associated concepts like random variables, conditional probability, and joint probability. Then you&#x27;ll be ready to learn some mathematical statistics and follow along with all the notation.<p>Note that you don&#x27;t need to go deep into measure-theoretic probability or any of that stuff that requires more advanced prior education in math.</div><br/><div id="42428212" class="c"><input type="checkbox" id="c-42428212" checked=""/><div class="controls bullet"><span class="by">user_7832</span><span>|</span><a href="#42426951">root</a><span>|</span><a href="#42427151">parent</a><span>|</span><a href="#42427644">next</a><span>|</span><label class="collapse" for="c-42428212">[-]</label><label class="expand" for="c-42428212">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This notation doesn&#x27;t require graduate-level statistics knowledge, it&#x27;s more like stuff that would be covered in a first mathematical course on probability and statistics.<p>Perhaps a first course at grad level, but my engineering bachelors covered MLEs but we didn’t learn&#x2F;use any of those formal language things. I think the core mathematics (and likely other pure science) cohorts were the only people who learnt it.</div><br/><div id="42428238" class="c"><input type="checkbox" id="c-42428238" checked=""/><div class="controls bullet"><span class="by">defrost</span><span>|</span><a href="#42426951">root</a><span>|</span><a href="#42428212">parent</a><span>|</span><a href="#42427644">next</a><span>|</span><label class="collapse" for="c-42428238">[-]</label><label class="expand" for="c-42428238">[1 more]</label></div><br/><div class="children"><div class="content">I slowly transfered out of Trad. Engineering (Civil&#x2F;Mech&#x2F;Electrical&#x2F;Electronic) pretty much because Engineering Math, Chem, and Physics units were almost all &quot;learn these results and how to apply them&quot; and little to no &quot;these are the underpinings of these results&quot;.<p>It took six months for Math 100 (Maths for wanna be mathematicians) to &quot;catch up&quot; with the applications being spat out in Math 101 (Maths for people that practically use math for applications) but by the time the foundations were laid almost all the applied math in the Engineering coursework results just became &quot;an exercise for the reader&quot; to derive without need for rote memorisation.</div><br/></div></div></div></div><div id="42427644" class="c"><input type="checkbox" id="c-42427644" checked=""/><div class="controls bullet"><span class="by">PittleyDunkin</span><span>|</span><a href="#42426951">root</a><span>|</span><a href="#42427151">parent</a><span>|</span><a href="#42428212">prev</a><span>|</span><a href="#42427452">next</a><span>|</span><label class="collapse" for="c-42427644">[-]</label><label class="expand" for="c-42427644">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This notation doesn&#x27;t require graduate-level statistics knowledge, it&#x27;s more like stuff that would be covered in a first mathematical course on probability and statistics.<p>My courses definitely used different notation for the same semantics.</div><br/></div></div></div></div><div id="42427452" class="c"><input type="checkbox" id="c-42427452" checked=""/><div class="controls bullet"><span class="by">mturmon</span><span>|</span><a href="#42426951">parent</a><span>|</span><a href="#42427151">prev</a><span>|</span><a href="#42427346">next</a><span>|</span><label class="collapse" for="c-42427452">[-]</label><label class="expand" for="c-42427452">[1 more]</label></div><br/><div class="children"><div class="content">“an explanation for MLE”<p>I used to get by on, “it’s the parameters that make the data most likely”, like it says on the name. I think that’s what you are after.<p>Then I took a stats class, and I know to say, “the MLE is minimum variance within the class of asymptotically unbiased estimators” … that is, “efficient” and “asymptotically consistent“ in the jargon. (Subject to caveats.)<p>Then I took a Bayesian stats class and learned to say, “it’s minimum risk under a (improper) uniform prior.”<p>I also recall there is a general result showing that any estimator which makes the score function zero has good properties with respect to average loss. So zero’ing the score by maximizing likelihood is a good strategy. (If someone could remind me of specifics, that would be great.)<p>But perhaps Gauss had it right when he exploited the (known, but yet un-named) central limit theorem and used how easy it is to maximize the quadratic that sits atop its “e”. (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;0804.2996" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;0804.2996</a>, page 3, top). It’s so easy we had to find a justification for using it?</div><br/></div></div><div id="42427346" class="c"><input type="checkbox" id="c-42427346" checked=""/><div class="controls bullet"><span class="by">Myrmornis</span><span>|</span><a href="#42426951">parent</a><span>|</span><a href="#42427452">prev</a><span>|</span><a href="#42427263">next</a><span>|</span><label class="collapse" for="c-42427346">[-]</label><label class="expand" for="c-42427346">[1 more]</label></div><br/><div class="children"><div class="content">One initial thing to understand is that the probability mass&#x2F;density functions that you get taught in connection with standard probability distributions (binomial, Normal, etc) are functions of the <i>data values</i>: you put in a data value and the function outputs a probability (density), for some fixed parameter values.<p>At first glance likelihood functions might look the same, but you have to think of them as functions of the <i>parameters</i>; it&#x27;s the data that&#x27;s fixed now (it&#x27;s whatever you observed in your experiment). Once that&#x27;s clear, the calculus starts to makes sense -- using the derivative of the likelihood function w.r.t. the parameters to find points in parameter space that are local maxima (or directions that are uphill in parameter space etc).<p>So given a model with unknown parameters, the data set you observe gives rise to a particular likelihood function, in other words the data set gives rise to a surface over your parameter space that you can explore for maxima. Regions of parameter space where your model gives a high probability to your observed data are considered to be regions of parameter space that your data suggests might describe how reality actually is. Of course, that&#x27;s not taking into account your prior beliefs about which regions of parameter space are plausible, or whether the model was a good choice in the first place, or whether you&#x27;ve got enough data, etc.</div><br/></div></div><div id="42427263" class="c"><input type="checkbox" id="c-42427263" checked=""/><div class="controls bullet"><span class="by">TrackerFF</span><span>|</span><a href="#42426951">parent</a><span>|</span><a href="#42427346">prev</a><span>|</span><a href="#42427400">next</a><span>|</span><label class="collapse" for="c-42427263">[-]</label><label class="expand" for="c-42427263">[1 more]</label></div><br/><div class="children"><div class="content">Always found the StatQuest vid on MLE to be extremely beginner friendly. Don&#x27;t even need college stats or math understanding to get the intuition.<p>Link: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XepXtl9YKwc" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XepXtl9YKwc</a></div><br/></div></div><div id="42427400" class="c"><input type="checkbox" id="c-42427400" checked=""/><div class="controls bullet"><span class="by">abetusk</span><span>|</span><a href="#42426951">parent</a><span>|</span><a href="#42427263">prev</a><span>|</span><a href="#42428322">next</a><span>|</span><label class="collapse" for="c-42427400">[-]</label><label class="expand" for="c-42427400">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s say I&#x27;m a huckster that plays a game with you where if I roll a single six sided die and it lands on 1 you lose but you win otherwise.<p>Let&#x27;s say you have some guarantee that I&#x27;m using the same die each time and that each of the rolls are independent. We play the game ten times and 1 is rolled the first 9 out of 10 times, with a 5 being rolled on the 10th throw. Now, you know that there&#x27;s a common loaded die that can be purchased that has a weight to skew the probabilities and you further know that the loaded die rolls a 1 80% of the time and the remaining 20% spread evenly to the other values (so 4% for every other value).<p>Given a choice between the loaded die and the fair die, which is more likely?<p>The first model, call it $\theta_0$ is the fair die. The second model, with the unfair die, call it $\theta_1$.<p>The probability of the first model ($\theta_0$) is:<p>$p( 1,1,1,1,1,1,1,1,1, 5 ; \theta_0) = \frac{1}{6}^9 \cdot \frac{1}{6}$<p>(approximately .00000008269085843959)<p>The probability of the second model ($\theta_1$) is:<p>$p( 1,1,1,1,1,1,1,1,1, 5 ; \theta_1) = (0.8)^9 \cdot 0.2 \cdot \frac{1}{5}$<p>(or .00536870912000000000)<p>So we write a computer program to iterate through all the &quot;models&quot; to see which is the more likely. In this case, the iteration goes through two models.<p>The models can be Gaussians, with model parameters the mean and variance, say, or some other distribution with other parameters to choose from.<p>For some conditions on models and their parameterization, we might even be able to use more intricate methods that use calculus, gradient descent, etc. to find the MLE.<p>The MLE formalism is trying to say &quot;given the observation, which parameters fit the best&quot;. It gets more complicated because we have to talk about which distributions we&#x27;re allowing (which &quot;model&quot;) and how we parameterize them. In the above, the models are simple, just assigning different probabilities to each of the outcomes of the die rolls and we only have a choice of two parameterizations.</div><br/></div></div><div id="42428322" class="c"><input type="checkbox" id="c-42428322" checked=""/><div class="controls bullet"><span class="by">tel</span><span>|</span><a href="#42426951">parent</a><span>|</span><a href="#42427400">prev</a><span>|</span><a href="#42428407">next</a><span>|</span><label class="collapse" for="c-42428322">[-]</label><label class="expand" for="c-42428322">[1 more]</label></div><br/><div class="children"><div class="content">Generally, when we construct models we do so by defining what probability they give to the data. That&#x27;s a function that takes in your data set and returns some number, the higher the better.<p>Technically, these functions need to satisfy a bunch of properties, but those properties matter mostly for people doing the business of building and comparing models. If you just have a model someone already made for you, then &quot;the higher the better&quot; is good enough.<p>It&#x27;s also the case that these models have &quot;parameters&quot;. As a simple example, the model of a coin flip takes in &quot;heads&quot; or &quot;tails&quot; and returns a number. The higher that number, the more probable it claims that outcome to be. When we construct that model, we also choose the &quot;fairness&quot; parameter, usually setting it so that both heads and tails are equally likely.<p>So really, it&#x27;s a function both of the data and of its parameters.<p>Now, &quot;maximum likelihood estimation&quot; (MLE) is just the method where you fix the data inputs to the model to whatever your training data is and then find the parameter inputs that maximize its output. This kind of inverts the normal mechanism where you pick the parameters and then see how probable the data was.<p>Presumptively, whatever parameterization of your model makes the data the most likely <i>is</i> the parameterization that best represents your data. That doesn&#x27;t have to be true, and often is only approximately true, but that presumption is exactly what makes MLE popular.<p>Finally, it&#x27;s worth describing the origin of the name. When we look at our model after fixing the data inputs and consider it a function of its parameters instead we call that function a &quot;likelihood&quot;. This is just another name for &quot;probability&quot; except it&#x27;s used to emphasize that likelihoods don&#x27;t meet all the technical properties I skipped up above. So &quot;maximum likelihood estimation&quot; is just the process of estimating the parameters of your model by maximizing the likelihood.</div><br/></div></div></div></div><div id="42428407" class="c"><input type="checkbox" id="c-42428407" checked=""/><div class="controls bullet"><span class="by">anon946</span><span>|</span><a href="#42426951">prev</a><span>|</span><a href="#42426125">next</a><span>|</span><label class="collapse" for="c-42428407">[-]</label><label class="expand" for="c-42428407">[1 more]</label></div><br/><div class="children"><div class="content">My favorite MLE example: Suppose you walk into a bank and ask them to give you a quarter. You flip the quarter twice and get two heads. Given this experiment, what do you estimate to be the probability p of getting a heads when you flip this coin? Using MLE, you would get p = 1. In other words, this coin will always give you a heads when you flip it! (According to MLE.)</div><br/></div></div><div id="42426125" class="c"><input type="checkbox" id="c-42426125" checked=""/><div class="controls bullet"><span class="by">skzv</span><span>|</span><a href="#42428407">prev</a><span>|</span><a href="#42426919">next</a><span>|</span><label class="collapse" for="c-42426125">[-]</label><label class="expand" for="c-42426125">[2 more]</label></div><br/><div class="children"><div class="content">To bring things full circle: the cross-entropy loss is the KL divergence. So intuitively, when you&#x27;re minimizing cross-entropy loss, you&#x27;re trying to minimize the &quot;divergence&quot; between the true distribution and your model distribution.<p>This intuition really helped me understand CE loss.</div><br/><div id="42426411" class="c"><input type="checkbox" id="c-42426411" checked=""/><div class="controls bullet"><span class="by">sidr</span><span>|</span><a href="#42426125">parent</a><span>|</span><a href="#42426919">next</a><span>|</span><label class="collapse" for="c-42426411">[-]</label><label class="expand" for="c-42426411">[1 more]</label></div><br/><div class="children"><div class="content">Cross-entropy is not the KL divergence. There is an additional term in cross-entropy which is the entropy of the data distribution (i.e., independent of the model). So, you&#x27;re right in that minimizing one is equivalent to minimizing the other.<p><a href="https:&#x2F;&#x2F;stats.stackexchange.com&#x2F;questions&#x2F;357963&#x2F;what-is-the-difference-between-cross-entropy-and-kl-divergence" rel="nofollow">https:&#x2F;&#x2F;stats.stackexchange.com&#x2F;questions&#x2F;357963&#x2F;what-is-the...</a></div><br/></div></div></div></div><div id="42426919" class="c"><input type="checkbox" id="c-42426919" checked=""/><div class="controls bullet"><span class="by">sidravi1</span><span>|</span><a href="#42426125">prev</a><span>|</span><a href="#42427409">next</a><span>|</span><label class="collapse" for="c-42426919">[-]</label><label class="expand" for="c-42426919">[1 more]</label></div><br/><div class="children"><div class="content">This reminds me that David Mackay’s book and his lectures are so excellent on these topics.</div><br/></div></div><div id="42428254" class="c"><input type="checkbox" id="c-42428254" checked=""/><div class="controls bullet"><span class="by">patrick451</span><span>|</span><a href="#42427409">prev</a><span>|</span><a href="#42426803">next</a><span>|</span><label class="collapse" for="c-42428254">[-]</label><label class="expand" for="c-42428254">[1 more]</label></div><br/><div class="children"><div class="content">If you have a parametrized functions that imperfectly models a real phenomenon, of course there are errors. Why assume they are random? A better assumption is that your model is just poor. Assuming deterministic modeling errors are due to randomness has always struck me as bizarre.</div><br/></div></div><div id="42426803" class="c"><input type="checkbox" id="c-42426803" checked=""/><div class="controls bullet"><span class="by">abdljasser2</span><span>|</span><a href="#42428254">prev</a><span>|</span><a href="#42426621">next</a><span>|</span><label class="collapse" for="c-42426803">[-]</label><label class="expand" for="c-42426803">[1 more]</label></div><br/><div class="children"><div class="content">Excellent post</div><br/></div></div><div id="42426621" class="c"><input type="checkbox" id="c-42426621" checked=""/><div class="controls bullet"><span class="by">Onavo</span><span>|</span><a href="#42426803">prev</a><span>|</span><label class="collapse" for="c-42426621">[-]</label><label class="expand" for="c-42426621">[1 more]</label></div><br/><div class="children"><div class="content">The next step is ELBO — the evidence lower bound.</div><br/></div></div></div></div></div></div></div></body></html>