<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716627665238" as="style"/><link rel="stylesheet" href="styles.css?v=1716627665238"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2405.13817">Thermodynamic Natural Gradient Descent</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>jasondavies</span> | <span>29 comments</span></div><br/><div><div id="40468898" class="c"><input type="checkbox" id="c-40468898" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#40468546">next</a><span>|</span><label class="collapse" for="c-40468898">[-]</label><label class="expand" for="c-40468898">[2 more]</label></div><br/><div class="children"><div class="content">The main point of this is that natural gradient descent is a second-order method. The main GD update equation is:<p>∇̃L(θ) = F⁻¹∇L(θ)<p>which requires solving a linear system. For this, you can use the methods from the author&#x27;s previous paper [Thermodynamic Linear Algebra](<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.05660" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.05660</a>).<p>Since it&#x27;s hard to implement a full neural network on a thermodynamic computer, the paper suggests running one in parallel to a normal GPU. The GPU computes F and ∇L(θ), but offloads the linear system to the thermo computer, which runs in parallel to the digital system (Figure 1).<p>It is important to note that the &quot;Runtime vs Accuracy&quot; plot in Figure 3 uses a &quot;timing model&quot; for the TNGD algorithm, since the computer necessary to run the algorithm still doesn&#x27;t exist.</div><br/><div id="40471063" class="c"><input type="checkbox" id="c-40471063" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40468898">parent</a><span>|</span><a href="#40468546">next</a><span>|</span><label class="collapse" for="c-40471063">[-]</label><label class="expand" for="c-40471063">[1 more]</label></div><br/><div class="children"><div class="content">It really gives nice way to think about gradient descent.</div><br/></div></div></div></div><div id="40468546" class="c"><input type="checkbox" id="c-40468546" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40468898">prev</a><span>|</span><a href="#40467971">next</a><span>|</span><label class="collapse" for="c-40468546">[-]</label><label class="expand" for="c-40468546">[1 more]</label></div><br/><div class="children"><div class="content">Cool and interesting. The authors propose a hybrid digital-analog training loop that takes into account the curvature of the loss landscape (i.e., it uses second-order derivatives), and show with numerical simulations that if their method is implemented in a hybrid digital-analog physical system, each iteration in the training loop would incur computational cost that is linear in the number of parameters. I&#x27;m all for figuring out ways to let the Laws of Thermodynamics do the work of training AI models, if doing so enables us to overcome the scaling limitations and challenges of existing digital hardware and training methods.</div><br/></div></div><div id="40467971" class="c"><input type="checkbox" id="c-40467971" checked=""/><div class="controls bullet"><span class="by">stefanpie</span><span>|</span><a href="#40468546">prev</a><span>|</span><a href="#40470093">next</a><span>|</span><label class="collapse" for="c-40467971">[-]</label><label class="expand" for="c-40467971">[3 more]</label></div><br/><div class="children"><div class="content">I know they mainly present results on deep learning&#x2F;neural network training and optimization, but I wonder how easy it would be to use the same optimization framework for other classes of hard or large optimization problems. I was also curious about this when I saw posts about Extropic (<a href="https:&#x2F;&#x2F;www.extropic.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.extropic.ai&#x2F;</a>) stuff for the first time.<p>I tried looking into any public info on their website about APIs or software stack to see what&#x27;s possible beyond NN stuff to model other optimization problems. It looks like that&#x27;s not shared publicly yet.<p>There are certainly many NP-hard and large combinatorial or analytical optimization problems still out there that are worth being able to tackle with new technology. Personally, I care about problems in EDA and semiconductor design. Adiabatic quantum computing was one technology with the promise of solving optimization problems (and quantum computing is still playing out with only small-scale solutions at the moment). Hoping that these new &quot;thermodynamic computing&quot; startups also might provide some cool technology to explore these problems with.</div><br/><div id="40468824" class="c"><input type="checkbox" id="c-40468824" checked=""/><div class="controls bullet"><span class="by">kaelan123</span><span>|</span><a href="#40467971">parent</a><span>|</span><a href="#40470093">next</a><span>|</span><label class="collapse" for="c-40468824">[-]</label><label class="expand" for="c-40468824">[2 more]</label></div><br/><div class="children"><div class="content">Indeed, other solving other optimization problem is an interesting avenue. All I can say is stay tuned!</div><br/><div id="40469037" class="c"><input type="checkbox" id="c-40469037" checked=""/><div class="controls bullet"><span class="by">stefanpie</span><span>|</span><a href="#40467971">root</a><span>|</span><a href="#40468824">parent</a><span>|</span><a href="#40470093">next</a><span>|</span><label class="collapse" for="c-40469037">[-]</label><label class="expand" for="c-40469037">[1 more]</label></div><br/><div class="children"><div class="content">Hey thanks for the reply! I&#x27;m assuming your the first author on the paper; if so, the signup button on the Normal Computing website is not working at the moment (at least for me, even with ad blocker turned off).</div><br/></div></div></div></div></div></div><div id="40470093" class="c"><input type="checkbox" id="c-40470093" checked=""/><div class="controls bullet"><span class="by">rsp1984</span><span>|</span><a href="#40467971">prev</a><span>|</span><a href="#40473385">next</a><span>|</span><label class="collapse" for="c-40470093">[-]</label><label class="expand" for="c-40470093">[3 more]</label></div><br/><div class="children"><div class="content">Leveraging thermodynamics to more efficiently compute second-order updates is certainly cool and worth exploring, however specifically in the context of deep learning I remain skeptical of its usefulness.<p>We already have very efficient second-order methods running on classical hardware [1] but they are basically not being used at all in practice, as they are outperformed by ADAM and other 1st-order methods. This is because optimizing highly nonlinear loss functions, such as the ones in deep learning models, only really works with very low learning rates, regardless of whether a 1st or a 2nd order method is used. So, comparatively speaking, a 2nd order method might give you a slightly better parameter update per step but at a more-than-slightly-higher cost, so most of the time it&#x27;s simply not worth doing.<p>[1] <a href="https:&#x2F;&#x2F;andrew.gibiansky.com&#x2F;blog&#x2F;machine-learning&#x2F;hessian-free-optimization&#x2F;" rel="nofollow">https:&#x2F;&#x2F;andrew.gibiansky.com&#x2F;blog&#x2F;machine-learning&#x2F;hessian-f...</a></div><br/><div id="40470276" class="c"><input type="checkbox" id="c-40470276" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#40470093">parent</a><span>|</span><a href="#40473385">next</a><span>|</span><label class="collapse" for="c-40470276">[-]</label><label class="expand" for="c-40470276">[2 more]</label></div><br/><div class="children"><div class="content">Agreed that it&#x27;s very cool, and also about how hard it is to make second order methods worthwhile. We&#x27;re just using such huge datasets that sometimes it&#x27;s hard to even get a decent estimate of the gradient for a minibatch. Getting a useful estimate of second order information over the dataset is even harder, especially when the whole point of using minibatches is computational feasibility.</div><br/><div id="40470532" class="c"><input type="checkbox" id="c-40470532" checked=""/><div class="controls bullet"><span class="by">kaelan123</span><span>|</span><a href="#40470093">root</a><span>|</span><a href="#40470276">parent</a><span>|</span><a href="#40473385">next</a><span>|</span><label class="collapse" for="c-40470532">[-]</label><label class="expand" for="c-40470532">[1 more]</label></div><br/><div class="children"><div class="content">Those are valid points! Hessian-free (HF) optimization is a really nice method, but as you say remains costly so people don&#x27;t use it. The key idea in this paper is that if you are able to solve linear systems faster by using an analog device, the cost of a HF-like method is brought down, so the method can become competitive.<p>About the noise, it is true that the second-order information will be noisier than the gradient for a given batch size (and a lot of results out there for HF optimization are with impractically large batch sizes). In the paper we use relatively small batch sizes (eg 32 for the fine-tuning example) and show that you can still get an advantage from second-order information. Of course it would be interesting to study in more detail how noisy 2nd order information can be, and on more datasets.</div><br/></div></div></div></div></div></div><div id="40473385" class="c"><input type="checkbox" id="c-40473385" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#40470093">prev</a><span>|</span><a href="#40469351">next</a><span>|</span><label class="collapse" for="c-40473385">[-]</label><label class="expand" for="c-40473385">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s our current best guess of how animal neurons learn?</div><br/></div></div><div id="40469351" class="c"><input type="checkbox" id="c-40469351" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40473385">prev</a><span>|</span><a href="#40473136">next</a><span>|</span><label class="collapse" for="c-40469351">[-]</label><label class="expand" for="c-40469351">[1 more]</label></div><br/><div class="children"><div class="content">Not having read the paper carefully, could someone tell me what the draw is? It looks like it is going to have the same asymptotic complexity as SGD in terms of sample size, per Table 1. Given that today&#x27;s large, over-specified models have numerous, comparable extrema, is there even a need for this? I wouldn&#x27;t get out of bed unless it were sublinear.</div><br/></div></div><div id="40473136" class="c"><input type="checkbox" id="c-40473136" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#40469351">prev</a><span>|</span><a href="#40470037">next</a><span>|</span><label class="collapse" for="c-40473136">[-]</label><label class="expand" for="c-40473136">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get it, gradient descend computation is super frequent, state&#x2F;input changes all the time, you&#x27;d have to reset heat landscape very frequently, what&#x27;s the point? No way there is any potential speedup opportunity there, no?<p>If anything you could probably do something with electromagnetic fields, their interference, possibly in 3d.</div><br/></div></div><div id="40470037" class="c"><input type="checkbox" id="c-40470037" checked=""/><div class="controls bullet"><span class="by">gnarbarian</span><span>|</span><a href="#40473136">prev</a><span>|</span><a href="#40469766">next</a><span>|</span><label class="collapse" for="c-40470037">[-]</label><label class="expand" for="c-40470037">[1 more]</label></div><br/><div class="children"><div class="content">this reminds me of simulated annealing which I learned about in an AI class about a decade ago.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Simulated_annealing" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Simulated_annealing</a></div><br/></div></div><div id="40469766" class="c"><input type="checkbox" id="c-40469766" checked=""/><div class="controls bullet"><span class="by">danbmil99</span><span>|</span><a href="#40470037">prev</a><span>|</span><a href="#40468301">next</a><span>|</span><label class="collapse" for="c-40469766">[-]</label><label class="expand" for="c-40469766">[1 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t Geoffrey Hinton going on about this about a year ago?</div><br/></div></div><div id="40468301" class="c"><input type="checkbox" id="c-40468301" checked=""/><div class="controls bullet"><span class="by">G3rn0ti</span><span>|</span><a href="#40469766">prev</a><span>|</span><label class="collapse" for="c-40468301">[-]</label><label class="expand" for="c-40468301">[14 more]</label></div><br/><div class="children"><div class="content">Sounds great until<p>&gt; requires an analog thermodynamic computer<p>Wait. What?<p>Perhaps a trained physicist can comment on that. Thanks.</div><br/><div id="40468882" class="c"><input type="checkbox" id="c-40468882" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#40468301">parent</a><span>|</span><a href="#40468455">next</a><span>|</span><label class="collapse" for="c-40468882">[-]</label><label class="expand" for="c-40468882">[1 more]</label></div><br/><div class="children"><div class="content">From my understanding, this is exactly what <a href="https:&#x2F;&#x2F;extropic.ai" rel="nofollow">https:&#x2F;&#x2F;extropic.ai</a> is working on, and I wouldn&#x27;t be surprised if <a href="https:&#x2F;&#x2F;normalcomputing.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;normalcomputing.ai&#x2F;</a> (authors of the paper) is as well.</div><br/></div></div><div id="40468455" class="c"><input type="checkbox" id="c-40468455" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40468301">parent</a><span>|</span><a href="#40468882">prev</a><span>|</span><a href="#40468721">next</a><span>|</span><label class="collapse" for="c-40468455">[-]</label><label class="expand" for="c-40468455">[1 more]</label></div><br/><div class="children"><div class="content">The whole point is to leverage <i>the laws of nature</i> to train AI models, overcoming the limitations and scaling challenges of digital hardware and existing training methods.</div><br/></div></div><div id="40468721" class="c"><input type="checkbox" id="c-40468721" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40468301">parent</a><span>|</span><a href="#40468455">prev</a><span>|</span><a href="#40468444">next</a><span>|</span><label class="collapse" for="c-40468721">[-]</label><label class="expand" for="c-40468721">[1 more]</label></div><br/><div class="children"><div class="content">This could be attractive if they can build a product along those lines: tens, if not hundreds, of billions of dollars are spent yearly on numerical optimization worldwide, and if this can significantly accelerate it, it could be very profitable.</div><br/></div></div><div id="40468444" class="c"><input type="checkbox" id="c-40468444" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#40468301">parent</a><span>|</span><a href="#40468721">prev</a><span>|</span><a href="#40468440">next</a><span>|</span><label class="collapse" for="c-40468444">[-]</label><label class="expand" for="c-40468444">[5 more]</label></div><br/><div class="children"><div class="content">The paper describes it pretty well in appendix C.  A matrix of integrators is constructed with a bunch of opamps, RC time constants (using digital potentiometers, presumably) and a multichannel ADC&#x2F;DAC interface to the PC.  Essentially a dedicated differential-equation solver.<p>So it&#x27;s a combination of old-school analog computation and modern GPU-based code.  Takes longer in practice due to the overhead of interfacing with the hardware and waiting for the integrators to settle, but the authors are claiming that an optimized implementation could outperform a purely-digital solution, as I understand it, by accelerating convergence.<p>The core idea being that conventional gradient descent is a linear operation at heart, while the gradients actually being traversed are curved surfaces that have to be approximated with multiple unnecessary steps if everything is done in the digital domain.<p>The trouble, as everybody from Seymour Cray onward has learned the hard way, is that CMOS always wins in the end, simply because the financial power of an entire industry goes into optimizing it.</div><br/><div id="40468807" class="c"><input type="checkbox" id="c-40468807" checked=""/><div class="controls bullet"><span class="by">kaelan123</span><span>|</span><a href="#40468301">root</a><span>|</span><a href="#40468444">parent</a><span>|</span><a href="#40468560">next</a><span>|</span><label class="collapse" for="c-40468807">[-]</label><label class="expand" for="c-40468807">[3 more]</label></div><br/><div class="children"><div class="content">First author of the paper here. That&#x27;s it indeed! One thing is that this is entirely CMOS-compatible. You could also do something similar with optics or other platforms, but we chose electronic circuits for this reason specifically.</div><br/><div id="40472219" class="c"><input type="checkbox" id="c-40472219" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#40468301">root</a><span>|</span><a href="#40468807">parent</a><span>|</span><a href="#40468981">next</a><span>|</span><label class="collapse" for="c-40472219">[-]</label><label class="expand" for="c-40472219">[1 more]</label></div><br/><div class="children"><div class="content">Can you implement this on an FPGA?</div><br/></div></div><div id="40468981" class="c"><input type="checkbox" id="c-40468981" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#40468301">root</a><span>|</span><a href="#40468807">parent</a><span>|</span><a href="#40472219">prev</a><span>|</span><a href="#40468560">next</a><span>|</span><label class="collapse" for="c-40468981">[-]</label><label class="expand" for="c-40468981">[1 more]</label></div><br/><div class="children"><div class="content">By that remark I meant &quot;digital CMOS,&quot; in the sense of elements that store state information discretely in flip-flops or gate insulators rather than continuously with analog integrators.<p>Very cool work in any event, though!  Best of luck with the ongoing R&amp;D.</div><br/></div></div></div></div><div id="40468560" class="c"><input type="checkbox" id="c-40468560" checked=""/><div class="controls bullet"><span class="by">stefanpie</span><span>|</span><a href="#40468301">root</a><span>|</span><a href="#40468444">parent</a><span>|</span><a href="#40468807">prev</a><span>|</span><a href="#40468440">next</a><span>|</span><label class="collapse" for="c-40468560">[-]</label><label class="expand" for="c-40468560">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t realize they included details about the hardware. Lie you said these just look like analog computers, compute in memory, analog arrays, which have also made a resurgence with deep leaning.</div><br/></div></div></div></div><div id="40468440" class="c"><input type="checkbox" id="c-40468440" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#40468301">parent</a><span>|</span><a href="#40468444">prev</a><span>|</span><a href="#40469246">next</a><span>|</span><label class="collapse" for="c-40468440">[-]</label><label class="expand" for="c-40468440">[3 more]</label></div><br/><div class="children"><div class="content">I believe one example would be quantum annealers. Where &quot;programming&quot; involves setting the right initial conditions and allowing thermodynamics to bring you to an optimum via relaxation.</div><br/><div id="40468862" class="c"><input type="checkbox" id="c-40468862" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#40468301">root</a><span>|</span><a href="#40468440">parent</a><span>|</span><a href="#40469246">next</a><span>|</span><label class="collapse" for="c-40468862">[-]</label><label class="expand" for="c-40468862">[2 more]</label></div><br/><div class="children"><div class="content">Dwave[1] does compute that way already.<p>[1] <a href="https:&#x2F;&#x2F;www.dwavesys.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.dwavesys.com&#x2F;</a></div><br/><div id="40468975" class="c"><input type="checkbox" id="c-40468975" checked=""/><div class="controls bullet"><span class="by">kaelan123</span><span>|</span><a href="#40468301">root</a><span>|</span><a href="#40468862">parent</a><span>|</span><a href="#40469246">next</a><span>|</span><label class="collapse" for="c-40468975">[-]</label><label class="expand" for="c-40468975">[1 more]</label></div><br/><div class="children"><div class="content">One key difference is the system is entirely classical (not quantum) and noise-resilient (see the last appendix).</div><br/></div></div></div></div></div></div><div id="40469246" class="c"><input type="checkbox" id="c-40469246" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#40468301">parent</a><span>|</span><a href="#40468440">prev</a><span>|</span><label class="collapse" for="c-40469246">[-]</label><label class="expand" for="c-40469246">[2 more]</label></div><br/><div class="children"><div class="content">Analog computers have a lot of history. You can Google analog with neural network or differential equations to get many results. They are fast with low power, can have precision issues, and require custom, chip design.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Analog_computer" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Analog_computer</a><p>Mixed signal ASIC’s often use a mix of digital and analog blocks to get the benefits of analog. It’s especially helpful for anything that eats lots of power or to prevent that (eg mobile).</div><br/><div id="40471145" class="c"><input type="checkbox" id="c-40471145" checked=""/><div class="controls bullet"><span class="by">onecommentman</span><span>|</span><a href="#40468301">root</a><span>|</span><a href="#40469246">parent</a><span>|</span><label class="collapse" for="c-40471145">[-]</label><label class="expand" for="c-40471145">[1 more]</label></div><br/><div class="children"><div class="content">Hard to beat the string algorithm for finding shortest paths on a positive weights network (e.g. build the network out of string where topologies match and link lengths are link weights, find the origin and destination nodes&#x2F;knots of interest, grab the two nodes and pull until taut).<p>Or the spaghetti approach to finding the largest value from a list of positive values (e.g. cut dry spaghetti noodles to length for each value, bundle them together and tap the bundle on a table, the one visually sticking out the most is the largest valued element).</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>