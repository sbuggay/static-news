<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728464455038" as="style"/><link rel="stylesheet" href="styles.css?v=1728464455038"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2410.05258">Differential Transformer</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>weirdcat</span> | <span>170 comments</span></div><br/><div><div id="41779537" class="c"><input type="checkbox" id="c-41779537" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#41778200">next</a><span>|</span><label class="collapse" for="c-41779537">[-]</label><label class="expand" for="c-41779537">[13 more]</label></div><br/><div class="children"><div class="content">I feel like I&#x27;m missing a key insight here. I understand the problem that regular softmax attention struggles to approach assigning zero attention to irrelevant stuff. And I get that having this subtraction formula makes it possible to assign exactly (or near) zero attention weight without having crazy outlier activations. But it seems like it also makes it very easy to have negative attention weight (which is equivalent to having positive attention weight on the negation of your value vectors). Intuitively, it just feels like a difficult balancing act to keep all the stuff you don&#x27;t care about so close to zero.<p>But Figure 1 clearly shows that it works, so I don&#x27;t doubt that it is in fact possible. I&#x27;m just struggling to build a picture of how exactly the network accomplishes this.</div><br/><div id="41780844" class="c"><input type="checkbox" id="c-41780844" checked=""/><div class="controls bullet"><span class="by">Grosvenor</span><span>|</span><a href="#41779537">parent</a><span>|</span><a href="#41779968">next</a><span>|</span><label class="collapse" for="c-41780844">[-]</label><label class="expand" for="c-41780844">[8 more]</label></div><br/><div class="children"><div class="content">Regular softmax (and attention) has an error in it.<p>softmax should be exp()&#x2F;1+∑exp()<p>Notice the 1 added to the denominator.<p>The difference is at the negative limit, softmax can be 0, instead of some epsilon. The same could be done by adding an extra zero value in x.<p>Downside is, you have to retrain your model from scratch to fix this.</div><br/><div id="41781880" class="c"><input type="checkbox" id="c-41781880" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#41779537">root</a><span>|</span><a href="#41780844">parent</a><span>|</span><a href="#41784160">next</a><span>|</span><label class="collapse" for="c-41781880">[-]</label><label class="expand" for="c-41781880">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve tried that in a small transformer that I trained from scratch and it didn&#x27;t really make any difference. I also made a version where I made this trainable somehow, probably by replacing the 1 with a constant associated with the layer, and that didn&#x27;t make any difference either.<p>I didn&#x27;t follow Miller&#x27;s proposal quite as he wrote it though and I put the mechanism in all the layers rather than avoiding it at the end.<p>My test doesn&#x27;t absolutely rule out usefulness-- there&#x27;s always different ways of applying something, but I saw no  indication of it.</div><br/><div id="41782203" class="c"><input type="checkbox" id="c-41782203" checked=""/><div class="controls bullet"><span class="by">Grosvenor</span><span>|</span><a href="#41779537">root</a><span>|</span><a href="#41781880">parent</a><span>|</span><a href="#41784160">next</a><span>|</span><label class="collapse" for="c-41782203">[-]</label><label class="expand" for="c-41782203">[2 more]</label></div><br/><div class="children"><div class="content">I guess the next step is to see if you&#x27;re getting those mega activations as he describes.<p>A&#x2F;B test the two models and compare?<p>Would be interesting to see if these activations only show up on larger models, or they&#x27;re some relation to model size.</div><br/><div id="41783549" class="c"><input type="checkbox" id="c-41783549" checked=""/><div class="controls bullet"><span class="by">Grosvenor</span><span>|</span><a href="#41779537">root</a><span>|</span><a href="#41782203">parent</a><span>|</span><a href="#41784160">next</a><span>|</span><label class="collapse" for="c-41783549">[-]</label><label class="expand" for="c-41783549">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36871528">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36871528</a><p>Hah. Yes. It looks like they only show up in models with 6.7B parameters or more.<p>The problem can start at 125M. Small enough to test on a whim.<p>So train a model that exhibits these behaviours, then try it out.</div><br/></div></div></div></div></div></div><div id="41784160" class="c"><input type="checkbox" id="c-41784160" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#41779537">root</a><span>|</span><a href="#41780844">parent</a><span>|</span><a href="#41781880">prev</a><span>|</span><a href="#41783013">next</a><span>|</span><label class="collapse" for="c-41784160">[-]</label><label class="expand" for="c-41784160">[1 more]</label></div><br/><div class="children"><div class="content">that silly softmax1 blog post is not worth the read. no one uses it in practice<p>if you think about it, the &quot;escape hatch&quot; is the design of the entire transformer dictionary. if Key&#x2F;Query attention misaligns with Value&#x27;s weights, you get a layer head that does not attend to anything...</div><br/></div></div><div id="41783013" class="c"><input type="checkbox" id="c-41783013" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41779537">root</a><span>|</span><a href="#41780844">parent</a><span>|</span><a href="#41784160">prev</a><span>|</span><a href="#41779968">next</a><span>|</span><label class="collapse" for="c-41783013">[-]</label><label class="expand" for="c-41783013">[3 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; softmax should be exp()&#x2F;1+∑exp()
</code></pre>
You referring to Miller&#x27;s blogpost?[0] There&#x27;s not an error in attention. Adding the +1 actually makes it not attention because you no longer generate a probability distribution[1]. There&#x27;s nothing really preventing attention to have a zero in any of the entries, the thing is that you probably won&#x27;t get -inf (very large negative number) inside inner product and you&#x27;re going to have a difficult time updating those weights via gradient descent.<p>I&#x27;ve also tested it on many networks and different types of attention and I&#x27;ve yet to see a meaningful improvement (or even an improvement), even in generalization.<p>It really is the training method...<p>As to the paper, I&#x27;m also still at a big lost and honestly, if reviewing could not accept it. The results look good, but I can&#x27;t tell why and there&#x27;s some &quot;black magic&quot; going on here.<p><pre><code>  - Figure 3 has &quot;Transformer&quot; and doesn&#x27;t specify. Is this StableLM-3B-4E1T?
    - What fucking dataset is this on? Stable has a WandB link[2] for that project and I don&#x27;t see any experiment with similar (presumably entropy?) loss values (come on... this is fucking research... label your fucking graphs...)
  - Where the fuck is the ablation? (Yes, I saw Fig 6 and Sec 3.8)
    - How do I know that (assuming this is Stable) that the difference isn&#x27;t just hyperparemeters? Or worse, GPUs! (yes, number of GPUs can change results due to sharding and this changing the statistics)
    - How do I know it isn&#x27;t down to 1k warmup steps instead of 5k?
    - What about hidden size, layers, heads, or FFN size? Stable has 32&#x2F;2560&#x2F;32&#x2F;? and this has 28&#x2F;3072&#x2F;12&#x2F;8192 (these all will mess with sharding statistics too). Is the head dimension the same?
    - How do I know it isn&#x27;t down to the tokenizer?
  - What is this magic? `0.8 - 0.6 * math.exp(-0.3 * depth)`
    - Was this learned? Hand picked? This is a huge factor
    - Any information about the learned parameters? Their final values? Trajectories? 
  - The code does not seem to be the same as whats in the algos...
</code></pre>
Obviously they improved something, but there is nothing in the paper that is convincing me that it is the differential attention. There are too many parameters at play and how am I supposed to know that the difference is by the thing they are proposing. And more importantly, how much it is improved by that specific thing and not by other things.<p><pre><code>  [0] https:&#x2F;&#x2F;www.evanmiller.org&#x2F;attention-is-off-by-one.html

  [1] This is a bit convoluted but without this condition many &quot;alternative forms&quot; you see would be equivalent to other architectures like linear layers or gated units. Term is not well defined, but this really appears to be the only agreed upon aspect, even if only implicitly stated. This is a much longer conversation though. 

  [2] https:&#x2F;&#x2F;stability.wandb.io&#x2F;stability-llm&#x2F;stable-lm&#x2F;reports&#x2F;StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo

  [2.1] The config: https:&#x2F;&#x2F;github.com&#x2F;Stability-AI&#x2F;StableLM&#x2F;blob&#x2F;main&#x2F;configs&#x2F;stablelm-3b-4e1t.yml</code></pre></div><br/><div id="41783286" class="c"><input type="checkbox" id="c-41783286" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#41779537">root</a><span>|</span><a href="#41783013">parent</a><span>|</span><a href="#41779968">next</a><span>|</span><label class="collapse" for="c-41783286">[-]</label><label class="expand" for="c-41783286">[2 more]</label></div><br/><div class="children"><div class="content">I feel like that blogpost was almost just ragebait for ai researchers. It goes between calling not including the +1 an error (which to me implies it would improve training losses, which it doesn&#x27;t really <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36854613">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36854613</a>) and saying possibly it could help with some types of quantization (which could very well be true but is a much weaker statement) and the author provides basically no evidence for either.</div><br/><div id="41783430" class="c"><input type="checkbox" id="c-41783430" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41779537">root</a><span>|</span><a href="#41783286">parent</a><span>|</span><a href="#41779968">next</a><span>|</span><label class="collapse" for="c-41783430">[-]</label><label class="expand" for="c-41783430">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the stereotypical computer scientist who thinks they know something others don&#x27;t and don&#x27;t feel the need to prove their claim. Specifically when it disagrees with experts. And unsurprisingly it&#x27;s been something others have already investigated and even written about. Definitely not all CS people, but it is a stereotype many other fields believe.<p>I know he&#x27;s an economist btw. I was also surprised he got a job at anthropic a few months after. I wonder if they&#x27;re related.</div><br/></div></div></div></div></div></div></div></div><div id="41780900" class="c"><input type="checkbox" id="c-41780900" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#41779537">parent</a><span>|</span><a href="#41779968">prev</a><span>|</span><a href="#41779779">next</a><span>|</span><label class="collapse" for="c-41780900">[-]</label><label class="expand" for="c-41780900">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I&#x27;m just struggling to build a picture of how exactly the network accomplishes this.<p>I mean, intuitively it would be trivial for the model to just optimise lambda to zero during training. Then you essentially have built a vanilla transformer with an overcomplicated parameter pruning mechanism. Pruning is already pretty well established in the literature as something that works surprisingly good for reducing parameter counts up to (hold on to your papers)... about 40%. In practice the model probably doesn&#x27;t work exactly like that, but I wouldn&#x27;t be surprised if it just approximates the normal transformer in the end anyways.</div><br/></div></div><div id="41779779" class="c"><input type="checkbox" id="c-41779779" checked=""/><div class="controls bullet"><span class="by">watsonmusic</span><span>|</span><a href="#41779537">parent</a><span>|</span><a href="#41780900">prev</a><span>|</span><a href="#41778200">next</a><span>|</span><label class="collapse" for="c-41779779">[-]</label><label class="expand" for="c-41779779">[2 more]</label></div><br/><div class="children"><div class="content">negative values can enhance the expressibility</div><br/><div id="41780385" class="c"><input type="checkbox" id="c-41780385" checked=""/><div class="controls bullet"><span class="by">Jerrrrrrry</span><span>|</span><a href="#41779537">root</a><span>|</span><a href="#41779779">parent</a><span>|</span><a href="#41778200">next</a><span>|</span><label class="collapse" for="c-41780385">[-]</label><label class="expand" for="c-41780385">[1 more]</label></div><br/><div class="children"><div class="content">doubt is the seed of reason</div><br/></div></div></div></div></div></div><div id="41778200" class="c"><input type="checkbox" id="c-41778200" checked=""/><div class="controls bullet"><span class="by">aDyslecticCrow</span><span>|</span><a href="#41779537">prev</a><span>|</span><a href="#41776904">next</a><span>|</span><label class="collapse" for="c-41778200">[-]</label><label class="expand" for="c-41778200">[6 more]</label></div><br/><div class="children"><div class="content">Very clever. I like this kind of nitty-gritty detail work, and the change is small enough to be adapted easily by others. Bravo!<p>I&#x27;m a little concerned about the last sentence of the section introduction of 
&quot;2 Differential Transformer&quot;. It mentions using improvements from previous papers, but in the grammatical context, it&#x27;s unclear if this improvement is added to both the normal transformer and their diff transformer. This would otherwise sully the comparisons. It&#x27;s the &quot;main difference&quot; wording in the previous sentence that raised a flag for me.<p>Of course, a good-faith researcher would know this and may not feel the need to clarify. But you can never be too careful about some published research in this field.</div><br/><div id="41780849" class="c"><input type="checkbox" id="c-41780849" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#41778200">parent</a><span>|</span><a href="#41778608">next</a><span>|</span><label class="collapse" for="c-41780849">[-]</label><label class="expand" for="c-41780849">[3 more]</label></div><br/><div class="children"><div class="content">Yes. This looks really, really good to me. Cross the board improvements in training time, perplexity improvements per both token trained and per model size. I&#x27;m reminded of MoE architectures, in that world we&#x27;re choosing an optimal small model to process part or all of the inference job; I wonder if MoE got some of the same benefits from forcing the Transformer to distinguish between alternate possibilities.<p>In any event, I&#x27;d imagine that this will get widely adopted if the numbers hold up; like I said, this seems to be basically no downside, and should be easy to replicate.</div><br/><div id="41782800" class="c"><input type="checkbox" id="c-41782800" checked=""/><div class="controls bullet"><span class="by">f38zf5vdt</span><span>|</span><a href="#41778200">root</a><span>|</span><a href="#41780849">parent</a><span>|</span><a href="#41778608">next</a><span>|</span><label class="collapse" for="c-41782800">[-]</label><label class="expand" for="c-41782800">[2 more]</label></div><br/><div class="children"><div class="content">There is a downside, every attention layer has to effectively compute attention twice (run scaled_dot_product_attention). As scaled_dot_product_attention is usually one of the most expensive operations in training and inference of a model, it seems like networks using this may be substantially slow and perhaps should considered against larger networks with more attention layers.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;unilm&#x2F;blob&#x2F;master&#x2F;Diff-Transformer&#x2F;multihead_flashdiff_1.py#L101-L102">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;unilm&#x2F;blob&#x2F;master&#x2F;Diff-Transfor...</a></div><br/><div id="41783540" class="c"><input type="checkbox" id="c-41783540" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#41778200">root</a><span>|</span><a href="#41782800">parent</a><span>|</span><a href="#41778608">next</a><span>|</span><label class="collapse" for="c-41783540">[-]</label><label class="expand" for="c-41783540">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. This is one of those areas where edge inference needs might be different than data center: to get an 11b quality model in 7b at the cost of 30% more inference time is probably a full yes for anyone doing local inference. And let’s remember that memory bandwidth is a huge factor as well; 30% smaller equals 30% boost in memory based time costs. Anyway I’m interested in trying this out.<p>I wonder if the specific setup might be extra effective for coding tuned models as well - you get one coding transformer and one ‘bad habits&#x2F;chat&#x2F;other non coding stuff’ negative transformer.</div><br/></div></div></div></div></div></div><div id="41778608" class="c"><input type="checkbox" id="c-41778608" checked=""/><div class="controls bullet"><span class="by">Chirono</span><span>|</span><a href="#41778200">parent</a><span>|</span><a href="#41780849">prev</a><span>|</span><a href="#41776904">next</a><span>|</span><label class="collapse" for="c-41778608">[-]</label><label class="expand" for="c-41778608">[2 more]</label></div><br/><div class="children"><div class="content">The two other changes they mention have been widely adopted, and are included in at least some of the models they benchmark against. It seems they list them for completeness as changes to the original transformer architecture.</div><br/><div id="41779736" class="c"><input type="checkbox" id="c-41779736" checked=""/><div class="controls bullet"><span class="by">aDyslecticCrow</span><span>|</span><a href="#41778200">root</a><span>|</span><a href="#41778608">parent</a><span>|</span><a href="#41776904">next</a><span>|</span><label class="collapse" for="c-41779736">[-]</label><label class="expand" for="c-41779736">[1 more]</label></div><br/><div class="children"><div class="content">Nicely spotted! Then, I really look forward to seeing this method tested by others! Epic stuff.</div><br/></div></div></div></div></div></div><div id="41776904" class="c"><input type="checkbox" id="c-41776904" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#41778200">prev</a><span>|</span><a href="#41779329">next</a><span>|</span><label class="collapse" for="c-41776904">[-]</label><label class="expand" for="c-41776904">[39 more]</label></div><br/><div class="children"><div class="content">Like most things in this new world of Machine Learning, I&#x27;m really confused why this works?<p>The analogy to noise-cancelling headphones is helpful but in that case we clearly know which is signal and which is noise. Here, if we knew why would we even bother to the noise-cancelling work?</div><br/><div id="41777492" class="c"><input type="checkbox" id="c-41777492" checked=""/><div class="controls bullet"><span class="by">blackbear_</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41777981">next</a><span>|</span><label class="collapse" for="c-41777492">[-]</label><label class="expand" for="c-41777492">[25 more]</label></div><br/><div class="children"><div class="content">With a single softmax you cannot predict exactly 0, but only very small numbers. When you have a large number of values to add up, this &quot;poisons&quot; the output with a lot of irrelevant stuff (the noise mentioned in the paper).<p>To make things worse, low attention values will have very low gradient, thus needing a lot of weight updates to undo that kind of mistakes. On the other hand, subtracting the output of two softmax allows the model to predict a weight of exactly zero for some of the values, while keeping a reasonable gradient flowing through.<p>So the model already knows what is noise, but a single softmax makes it harder to exclude it.<p>Moreover, with a single softmax the output of all heads is forced to stay in the convex hull of the value vectors, whereas with this variant each head can choose its own lambda, thus shifting the &quot;range&quot; of the outputs outside the convex hull pre-determined by the values. This makes the model as a whole more expressive.</div><br/><div id="41779327" class="c"><input type="checkbox" id="c-41779327" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777492">parent</a><span>|</span><a href="#41777740">next</a><span>|</span><label class="collapse" for="c-41779327">[-]</label><label class="expand" for="c-41779327">[2 more]</label></div><br/><div class="children"><div class="content">While I don&#x27;t discount the value of this, can you expand on the meaning of your claim that it makes the model &#x27;more expressive&#x27;<p>Everything I am seeing in this paper is related to reduced size and noise, which implies a reduction in expressiveness.<p>The improvement in needle and a haystack, benchmarks on multi-hop questions of in corpus data and multishot in-context learning points to this.<p>This is a wonderful thing if robustness is more important than generality, but it doesn&#x27;t address trimming away activations that may be spurious in the general use case but may improve an individual domain specificity.<p>Context would dramatically impact what tradeoffs and more desireble, and noise is probably never desirable.  But the ability of this paper to enable bit size for inference points to a reduction in expressiveness.<p>Perhaps I am too focused on generalization?</div><br/><div id="41781003" class="c"><input type="checkbox" id="c-41781003" checked=""/><div class="controls bullet"><span class="by">blackbear_</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41779327">parent</a><span>|</span><a href="#41777740">next</a><span>|</span><label class="collapse" for="c-41781003">[-]</label><label class="expand" for="c-41781003">[1 more]</label></div><br/><div class="children"><div class="content">What I meant is that by changing lambda each attention head is able to put its outputs in a subspace that is different than that of the other heads. This means that the outputs of different heads do not mingle with each other, and it&#x27;s easier for the following layer to pick them apart. So I was thinking at increased expressiveness because the attention output can in principle cover a larger volume.<p>Maybe expressiveness is not the right term, or not the main consequence. I could imagine that having different subspaces like that also introduces a degree of robustness to out-of-distribution inputs, as this would make it harder for the outputs of one attention head to shift towards the in-distribution outputs of another head, and thus for the following layer to confuse them.</div><br/></div></div></div></div><div id="41777740" class="c"><input type="checkbox" id="c-41777740" checked=""/><div class="controls bullet"><span class="by">freeqaz</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777492">parent</a><span>|</span><a href="#41779327">prev</a><span>|</span><a href="#41779695">next</a><span>|</span><label class="collapse" for="c-41777740">[-]</label><label class="expand" for="c-41777740">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;m able to follow most of what you&#x27;re saying. It&#x27;s unclear to me what &quot;convex hull&quot; means though.<p>Also, where is each softmax happening here? For each attention head?</div><br/><div id="41777837" class="c"><input type="checkbox" id="c-41777837" checked=""/><div class="controls bullet"><span class="by">Majromax</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777740">parent</a><span>|</span><a href="#41777813">next</a><span>|</span><label class="collapse" for="c-41777837">[-]</label><label class="expand" for="c-41777837">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s unclear to me what &quot;convex hull&quot; means though.<p>The convex hull (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Convex_hull" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Convex_hull</a>) of a set is the smallest convex shape that includes that set.  Geometrically, it&#x27;s what you&#x27;d get if you &quot;shrink wrapped&quot; the thing you&#x27;re looking at: edges still protrude, but any indentations get smoothed over.<p>In this context, the grandparent comment is pointing out that with a traditional transformer block, the resulting computed value for a token can never &quot;stick out&quot; past some weighted average of the values of attended-to tokens, but this differential attention formalism allows that result.</div><br/><div id="41782797" class="c"><input type="checkbox" id="c-41782797" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777837">parent</a><span>|</span><a href="#41777813">next</a><span>|</span><label class="collapse" for="c-41782797">[-]</label><label class="expand" for="c-41782797">[1 more]</label></div><br/><div class="children"><div class="content">The softmax value y is a linear combination of the vectors you&#x27;re attending over:
y = a1<i>v1 + a2</i>v2 + ... + an*vn
where a_i &gt;= 0 and sum(a_i) = 1.<p>Then y is a convex combination of the v_i, and sits in the convex hull of the v_i.</div><br/></div></div></div></div><div id="41777813" class="c"><input type="checkbox" id="c-41777813" checked=""/><div class="controls bullet"><span class="by">blackbear_</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777740">parent</a><span>|</span><a href="#41777837">prev</a><span>|</span><a href="#41777849">next</a><span>|</span><label class="collapse" for="c-41777813">[-]</label><label class="expand" for="c-41777813">[1 more]</label></div><br/><div class="children"><div class="content">The convex hull of a set of points is the region &quot;between&quot; those points. So the convex hull of three points (that do not lie on the same line) is a triangle with those three points as vertices. If you add a fourth point inside the triangle, the convex hull remains the same, but if you add it outside then the convex hull becomes the four-sided region with those points as vertices.<p>In the context of standard transformer attention, each output lies in the convex hull (&quot;somewhere between&quot;) the input values. With the modification of this paper, the input values can be scaled a little so that the output of different heads can be in different &quot;regions&quot; and thus do not interfere with each other (so yes to your third question, the two softmaxes are performed separately for each head).</div><br/></div></div><div id="41777849" class="c"><input type="checkbox" id="c-41777849" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777740">parent</a><span>|</span><a href="#41777813">prev</a><span>|</span><a href="#41779880">next</a><span>|</span><label class="collapse" for="c-41777849">[-]</label><label class="expand" for="c-41777849">[3 more]</label></div><br/><div class="children"><div class="content">O_i = softmax(...) * V_i and softmax is between 0 and 1, so O_i = alpha * V_i for some alpha between 0 and 1 so that makes it convex, and it makes the O_i just a shrunken version of V_i. Whereas if you have the diff of softmaxes, you get O_i = (alpha - beta) * V_i, which can range from -V_i to +V_i, so its output could rescale &#x2F;or&#x2F; flip V_i. And yes this is happening in every head in parallel, then they get summed.</div><br/><div id="41780972" class="c"><input type="checkbox" id="c-41780972" checked=""/><div class="controls bullet"><span class="by">kridsdale3</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777849">parent</a><span>|</span><a href="#41779880">next</a><span>|</span><label class="collapse" for="c-41780972">[-]</label><label class="expand" for="c-41780972">[2 more]</label></div><br/><div class="children"><div class="content">By simply inputting your comment in to 4o, with no other context about the paper, I was able to get a pretty good analysis of the dual-head concept&#x27;s implications.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;67058973-ba94-8008-bed7-c7f9d08dc5fc" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;67058973-ba94-8008-bed7-c7f9d08dc5...</a></div><br/><div id="41782171" class="c"><input type="checkbox" id="c-41782171" checked=""/><div class="controls bullet"><span class="by">spwa4</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41780972">parent</a><span>|</span><a href="#41779880">next</a><span>|</span><label class="collapse" for="c-41782171">[-]</label><label class="expand" for="c-41782171">[1 more]</label></div><br/><div class="children"><div class="content">Uh, this is extracting a LOT from very little data. I don&#x27;t understand where it&#x27;s coming from but it&#x27;s explanation just keeps going into more and more detail ... that doesn&#x27;t seem to follow from the data it&#x27;s got.<p>I just don&#x27;t see how you could answer these questions without trying it out. And chatgtp DEFINITELY isn&#x27;t doing that.<p>Plus the obvious question I&#x27;d pose is not in there. What&#x27;s the difference in performance between this trick and just &quot;softmax() - 0.5 * 2&quot; ? That seems very relevant.</div><br/></div></div></div></div></div></div><div id="41779880" class="c"><input type="checkbox" id="c-41779880" checked=""/><div class="controls bullet"><span class="by">robertsdionne</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777740">parent</a><span>|</span><a href="#41777849">prev</a><span>|</span><a href="#41779695">next</a><span>|</span><label class="collapse" for="c-41779880">[-]</label><label class="expand" for="c-41779880">[1 more]</label></div><br/><div class="children"><div class="content">It means one of these things: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Simplex#Standard_simplex" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Simplex#Standard_simplex</a></div><br/></div></div></div></div><div id="41779695" class="c"><input type="checkbox" id="c-41779695" checked=""/><div class="controls bullet"><span class="by">x1000</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777492">parent</a><span>|</span><a href="#41777740">prev</a><span>|</span><a href="#41779617">next</a><span>|</span><label class="collapse" for="c-41779695">[-]</label><label class="expand" for="c-41779695">[2 more]</label></div><br/><div class="children"><div class="content">Could you help explain how we would achieve an attention score of exactly 0, in practice? Here’s my take:<p>If we’re subtracting one attention matrix from another, we’d end up with attention scores between -1 and 1, with a probability of effectively 0 for any single entry to exactly equal 0.<p>What’s more, the learnable parameter \lambda allows for negative values. This would allow the model to learn to actually add the attention scores, making a score of exactly 0 impossible.</div><br/><div id="41780336" class="c"><input type="checkbox" id="c-41780336" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41779695">parent</a><span>|</span><a href="#41779617">next</a><span>|</span><label class="collapse" for="c-41780336">[-]</label><label class="expand" for="c-41780336">[1 more]</label></div><br/><div class="children"><div class="content">Your comment brings up two interesting variants that could be interesting if your goal is to increase the sparsity of the attention:<p>- Rectify the difference of the softmaxes. (min(0, s(A1) - lambda s(A2)))<p>- Apply the Heaviside function to the second softmax. (softmax(A1) - lambda H(s(A1) - lambda s(A2))<p>The second one being a bit more drastic and maybe harder to train.</div><br/></div></div></div></div><div id="41779617" class="c"><input type="checkbox" id="c-41779617" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777492">parent</a><span>|</span><a href="#41779695">prev</a><span>|</span><a href="#41778090">next</a><span>|</span><label class="collapse" for="c-41779617">[-]</label><label class="expand" for="c-41779617">[3 more]</label></div><br/><div class="children"><div class="content">It is a neat approach, but one that comes with a tradeoff, IIUC: doubling the key heads.<p>I wonder if a different approach without that issue exists. For instance, using max(0, exp(x)-1) instead of exp(x) in the softmax attention formula. That way when the query is orthogonal to the key (or worse), it does not contribute.</div><br/><div id="41782523" class="c"><input type="checkbox" id="c-41782523" checked=""/><div class="controls bullet"><span class="by">smallnamespace</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41779617">parent</a><span>|</span><a href="#41778090">next</a><span>|</span><label class="collapse" for="c-41782523">[-]</label><label class="expand" for="c-41782523">[2 more]</label></div><br/><div class="children"><div class="content">&gt; using max(0, exp(x)-1) instead of exp(x)<p>Won&#x27;t this cause the gradient to vanish on the left half, causing problems with training?</div><br/><div id="41785150" class="c"><input type="checkbox" id="c-41785150" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41782523">parent</a><span>|</span><a href="#41778090">next</a><span>|</span><label class="collapse" for="c-41785150">[-]</label><label class="expand" for="c-41785150">[1 more]</label></div><br/><div class="children"><div class="content">That is a concern that is shared with ReLU. But since the weights are shared across the context&#x2F;minibatch, perhaps that would not be an issue, similar to ReLU.</div><br/></div></div></div></div></div></div><div id="41777971" class="c"><input type="checkbox" id="c-41777971" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777492">parent</a><span>|</span><a href="#41778090">prev</a><span>|</span><a href="#41779803">next</a><span>|</span><label class="collapse" for="c-41777971">[-]</label><label class="expand" for="c-41777971">[7 more]</label></div><br/><div class="children"><div class="content">&gt; predict a weight of exactly zero for some of the values<p>Wouldn’t this be pretty unlikely, though?</div><br/><div id="41778315" class="c"><input type="checkbox" id="c-41778315" checked=""/><div class="controls bullet"><span class="by">schopra909</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777971">parent</a><span>|</span><a href="#41779803">next</a><span>|</span><label class="collapse" for="c-41778315">[-]</label><label class="expand" for="c-41778315">[6 more]</label></div><br/><div class="children"><div class="content">Quite the opposite — if you have a long sequence only a smattering of the words will influence the meaning of the current word. Everything else is “noise”.<p>Attention is really good at finding this smattering of words (ie assign most weight there). But it struggles to put exactly 0 on the other words.</div><br/><div id="41779598" class="c"><input type="checkbox" id="c-41779598" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41778315">parent</a><span>|</span><a href="#41779079">next</a><span>|</span><label class="collapse" for="c-41779598">[-]</label><label class="expand" for="c-41779598">[1 more]</label></div><br/><div class="children"><div class="content">I mean wouldn’t it be unlikely that<p>SoftmaxA[n] - SoftmaxB[n] is exactly 0?<p>Even if 2 attention layers learn two different things, I would imagine the corresponding weights in each layer wouldn’t exactly cancel each other out.</div><br/></div></div><div id="41779079" class="c"><input type="checkbox" id="c-41779079" checked=""/><div class="controls bullet"><span class="by">absoflutely</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41778315">parent</a><span>|</span><a href="#41779598">prev</a><span>|</span><a href="#41779803">next</a><span>|</span><label class="collapse" for="c-41779079">[-]</label><label class="expand" for="c-41779079">[4 more]</label></div><br/><div class="children"><div class="content">why say lot word when few word do</div><br/><div id="41779574" class="c"><input type="checkbox" id="c-41779574" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41779079">parent</a><span>|</span><a href="#41779803">next</a><span>|</span><label class="collapse" for="c-41779574">[-]</label><label class="expand" for="c-41779574">[3 more]</label></div><br/><div class="children"><div class="content">Few word no do tho</div><br/><div id="41781012" class="c"><input type="checkbox" id="c-41781012" checked=""/><div class="controls bullet"><span class="by">kridsdale3</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41779574">parent</a><span>|</span><a href="#41780356">next</a><span>|</span><label class="collapse" for="c-41781012">[-]</label><label class="expand" for="c-41781012">[1 more]</label></div><br/><div class="children"><div class="content">U+1FAE5</div><br/></div></div><div id="41780356" class="c"><input type="checkbox" id="c-41780356" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41779574">parent</a><span>|</span><a href="#41781012">prev</a><span>|</span><a href="#41779803">next</a><span>|</span><label class="collapse" for="c-41780356">[-]</label><label class="expand" for="c-41780356">[1 more]</label></div><br/><div class="children"><div class="content">Phew!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41777981" class="c"><input type="checkbox" id="c-41777981" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41777492">prev</a><span>|</span><a href="#41778514">next</a><span>|</span><label class="collapse" for="c-41777981">[-]</label><label class="expand" for="c-41777981">[4 more]</label></div><br/><div class="children"><div class="content">Noise cancelling headphones are probably the wrong analogy here.<p>The better example is the differential signalling used in professional audio and many digital signaling protocols like Ethernet, HDMI and USB.<p>Instead of using one wire, referencing to ground, they send the signal as the difference between both wires. Both wires end up carrying the same signal with inverted polarity. Because both wires are running next to each-other any external noice will be applied to both equally.<p>The voltage will change, but the difference in voltage between both wires is untouched. And when you subtract the two voltages at the receiver end, any noise simply gets subtracted out.</div><br/><div id="41778331" class="c"><input type="checkbox" id="c-41778331" checked=""/><div class="controls bullet"><span class="by">seamossfet</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777981">parent</a><span>|</span><a href="#41784563">next</a><span>|</span><label class="collapse" for="c-41778331">[-]</label><label class="expand" for="c-41778331">[1 more]</label></div><br/><div class="children"><div class="content">I think when they bring up differential amplifiers they&#x27;re referring more to the DSP technique of how headphone noise cancelling works but the actual electrical properties of how a differential amplifier does that muddies the message a bit.<p>It sort of feels closer to heterodyning and &quot;demodulating&quot; the signal encoded in the softmax. Those tiny little errors we&#x27;re trying to denoise with this technique are almost closer to carrier waves (when encoded to softmax) than noise imo. This wouldn&#x27;t get rid of noise in the training data or noise in the dimensionality of the key &#x2F; value space. It&#x27;s really only removing noise introduced by the process itself.</div><br/></div></div><div id="41784563" class="c"><input type="checkbox" id="c-41784563" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777981">parent</a><span>|</span><a href="#41778331">prev</a><span>|</span><a href="#41779857">next</a><span>|</span><label class="collapse" for="c-41784563">[-]</label><label class="expand" for="c-41784563">[1 more]</label></div><br/><div class="children"><div class="content">This is a cool idea!</div><br/></div></div></div></div><div id="41778514" class="c"><input type="checkbox" id="c-41778514" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41777981">prev</a><span>|</span><a href="#41780581">next</a><span>|</span><label class="collapse" for="c-41778514">[-]</label><label class="expand" for="c-41778514">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t look for an analogy, this just adds a new mathematical capability. It enables &quot;negative attention&quot;, the network can say &quot;I want to subtract the contribution of this token&quot; in the attention calculation. Previously it could only reduce how much it adds.<p>The simple way of doing this would be to just remove the softmax or use a sigmoid instead, but in practice a softmax works better it seems.</div><br/></div></div><div id="41780581" class="c"><input type="checkbox" id="c-41780581" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41778514">prev</a><span>|</span><a href="#41777095">next</a><span>|</span><label class="collapse" for="c-41780581">[-]</label><label class="expand" for="c-41780581">[1 more]</label></div><br/><div class="children"><div class="content">My hypothesis for why this works that it mitigates the downsides of rope<p>to eli5:<p>rope is the modern strategy used to give information to the model about how far a query and a key are apart when doing attention. It&#x27;s the best strategy we have now, but has a major downside, where it makes some connections between tokens that are far apart much stronger than you would like them to be. Xpos (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.10554" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.10554</a>) is another paper by microsoft tackling issues with rope and you can see figure 1 on page 4 to get a visual interpretation of the sinusoidal attention strength (you would like it to be smooth).<p>I think a big reason differential transformers is working so well, especially on long sequence stuff, because when both q1 and q2 don&#x27;t match a token, the rope relative strength will still have the same value and the noise will cancel out. Leaving intended matches, but at the cost of somewhat dampening the original value rope brought.<p>Just a hypothesis though. It would be easy to test by running this experiment against a baseline where both use alibi attention (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2108.12409" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2108.12409</a>) which has a different set of tradeoffs this wouldn&#x27;t mitigate, but still a really interesting result.</div><br/></div></div><div id="41777095" class="c"><input type="checkbox" id="c-41777095" checked=""/><div class="controls bullet"><span class="by">_hl_</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41780581">prev</a><span>|</span><a href="#41778030">next</a><span>|</span><label class="collapse" for="c-41777095">[-]</label><label class="expand" for="c-41777095">[1 more]</label></div><br/><div class="children"><div class="content">Some of the &quot;prior art&quot; here is ladder networks and to some handwavy extent residual nets, both of which can be interpreted as training the model on reducing the error to its previous predictions as opposed to predicting the final result directly. I think some intuition for why it works has to do with changing the gradient descent landscape to be a bit friendlier towards learning in small baby steps, as you are now explicitly designing the network around the idea that it will start off making lots of errors in its predictions and then get better over time.</div><br/></div></div><div id="41778030" class="c"><input type="checkbox" id="c-41778030" checked=""/><div class="controls bullet"><span class="by">seamossfet</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41777095">prev</a><span>|</span><a href="#41777464">next</a><span>|</span><label class="collapse" for="c-41778030">[-]</label><label class="expand" for="c-41778030">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like they&#x27;re just splitting the query &#x2F; key space down the middle. We don&#x27;t know which dimensions are encoded in each matrix, but they&#x27;re assuming the &quot;noise&quot; introduced in one query &#x2F; key space is equivalent to noise introduced in the other space.<p>If that is the case, then the &quot;signal&quot; in this case would be the softmax that encodes the dimensions captured by the query &#x2F; key space. Since the noise ideally is the same in both softmax encodings, subtracting them should &quot;cancel out&quot; the noise.</div><br/></div></div><div id="41777464" class="c"><input type="checkbox" id="c-41777464" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41778030">prev</a><span>|</span><a href="#41780220">next</a><span>|</span><label class="collapse" for="c-41777464">[-]</label><label class="expand" for="c-41777464">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand either. It seems the general idea is that they calculate attention twice, which due to random initialization might be expected to give two slightly different results. I&#x27;d have thought that what these two attention maps would have in common would be the signal, and where they would differ would be noise, so rather than subtracting them (resulting in all noise?!) what you really want is to add (so the common signal gets reinforced) and normalize.</div><br/><div id="41777545" class="c"><input type="checkbox" id="c-41777545" checked=""/><div class="controls bullet"><span class="by">Carlseymanh</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777464">parent</a><span>|</span><a href="#41778568">next</a><span>|</span><label class="collapse" for="c-41777545">[-]</label><label class="expand" for="c-41777545">[1 more]</label></div><br/><div class="children"><div class="content">I think there might be some communalities with system engineering, where you subtract the output from the input in order to get a control signal that steers the plant to the target values. I too fail to see how that would be supposed to work in practice.</div><br/></div></div><div id="41778568" class="c"><input type="checkbox" id="c-41778568" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#41776904">root</a><span>|</span><a href="#41777464">parent</a><span>|</span><a href="#41777545">prev</a><span>|</span><a href="#41780220">next</a><span>|</span><label class="collapse" for="c-41778568">[-]</label><label class="expand" for="c-41778568">[1 more]</label></div><br/><div class="children"><div class="content">The values between the groups are also going to diverge during training due to the structure of the DiffAttn equation.<p>The analogy I can think of is when you&#x27;re paying attention to a variety of things and you actively avoid concentrating on something because it will distract you. You don&#x27;t give it zero attention, you give it negative attention.</div><br/></div></div></div></div><div id="41780220" class="c"><input type="checkbox" id="c-41780220" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41777464">prev</a><span>|</span><a href="#41776928">next</a><span>|</span><label class="collapse" for="c-41780220">[-]</label><label class="expand" for="c-41780220">[1 more]</label></div><br/><div class="children"><div class="content">I think common mode filtering in balanced audio cables is a much better analogy than noise canceling headphones (and where this paper gets its name from I assume), you don&#x27;t know what the noise is ahead of time, but if you take two samples with one positive and one negative, noise displaces both absolutely, which you can take advantage of to denoise the signal (find the differential mode).<p>For example, if you are trying to send a +1V signal on one wire, and a -1V signal on the other and a +0.5V noise exists, one wire will have +1.5V and the other will have -0.5V,<p>Take the difference and divide by 2:<p>(+1.5V - -0.5V) &#x2F; 2 = +1V
or, if your setup is different
(-0.5V - +1.5V) &#x2F; 2 = -1V</div><br/></div></div><div id="41776928" class="c"><input type="checkbox" id="c-41776928" checked=""/><div class="controls bullet"><span class="by">watsonmusic</span><span>|</span><a href="#41776904">parent</a><span>|</span><a href="#41780220">prev</a><span>|</span><a href="#41779329">next</a><span>|</span><label class="collapse" for="c-41776928">[-]</label><label class="expand" for="c-41776928">[1 more]</label></div><br/><div class="children"><div class="content">the model is supposed to learn this</div><br/></div></div></div></div><div id="41779329" class="c"><input type="checkbox" id="c-41779329" checked=""/><div class="controls bullet"><span class="by">islewis</span><span>|</span><a href="#41776904">prev</a><span>|</span><a href="#41777920">next</a><span>|</span><label class="collapse" for="c-41779329">[-]</label><label class="expand" for="c-41779329">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Differential attention takes the difference between two softmax attention functions to eliminate attention noise<p>If I understand correctly, this architecture trades twice as much attention memory in exchange for either a higher quality model, or less parameters at a similar quality.<p>&gt; According to the fitted curves, 6.8B-size
DIFF Transformer achieves a validation loss comparable to 11B-size Transformer, requiring only 62.2% of parameters<p>This raises a few questions for me:<p>- Would having only 60% of the parameters negate the double space for attention, leaving a similar memory profile as a traditional transformer?<p>- Does that tradeoff change noticeably between training and inference?</div><br/><div id="41779508" class="c"><input type="checkbox" id="c-41779508" checked=""/><div class="controls bullet"><span class="by">_hl_</span><span>|</span><a href="#41779329">parent</a><span>|</span><a href="#41780050">next</a><span>|</span><label class="collapse" for="c-41779508">[-]</label><label class="expand" for="c-41779508">[1 more]</label></div><br/><div class="children"><div class="content">My understanding was that the extra parameters required for the second attention mechanism are <i>included</i> in those 6.8B parameters (i.e. those are the total parameters of the model, not some made-up metric of would-be parameter count in a standard transformer). This makes the result doubly impressive!<p>Here&#x27;s the bit from the paper:<p>&gt; We set the number of heads h = dmodel&#x2F;2d, where d is equal to the head dimension
of Transformer. So we can align the parameter counts and computational complexity.<p>In other words, they make up for it by having only half as many attention heads per layer.</div><br/></div></div><div id="41780050" class="c"><input type="checkbox" id="c-41780050" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#41779329">parent</a><span>|</span><a href="#41779508">prev</a><span>|</span><a href="#41779423">next</a><span>|</span><label class="collapse" for="c-41780050">[-]</label><label class="expand" for="c-41780050">[1 more]</label></div><br/><div class="children"><div class="content">I think they mitigated the extra memory&#x2F;compute from this by using half the number of overall heads and doubling V and O. Without actually checking the math I think it should be equivalent in flops, not counting the extra (cheap) multiply by const and subtract.</div><br/></div></div><div id="41779423" class="c"><input type="checkbox" id="c-41779423" checked=""/><div class="controls bullet"><span class="by">entropicdrifter</span><span>|</span><a href="#41779329">parent</a><span>|</span><a href="#41780050">prev</a><span>|</span><a href="#41780064">next</a><span>|</span><label class="collapse" for="c-41779423">[-]</label><label class="expand" for="c-41779423">[1 more]</label></div><br/><div class="children"><div class="content">I think it <i>would</i> negate the RAM savings, but it would also reduce the amount of storage needed at rest and possibly reduce initial start up times depending on storage speed and model size. So, possibly good for low-end models on consumer devices?</div><br/></div></div><div id="41780064" class="c"><input type="checkbox" id="c-41780064" checked=""/><div class="controls bullet"><span class="by">Kubuxu</span><span>|</span><a href="#41779329">parent</a><span>|</span><a href="#41779423">prev</a><span>|</span><a href="#41777920">next</a><span>|</span><label class="collapse" for="c-41780064">[-]</label><label class="expand" for="c-41780064">[1 more]</label></div><br/><div class="children"><div class="content">It would double the size of the KV cache, which can be significant (multi-GB) at larger context sizes.</div><br/></div></div></div></div><div id="41777920" class="c"><input type="checkbox" id="c-41777920" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#41779329">prev</a><span>|</span><a href="#41778413">next</a><span>|</span><label class="collapse" for="c-41777920">[-]</label><label class="expand" for="c-41777920">[7 more]</label></div><br/><div class="children"><div class="content">The key bit I didn&#x27;t understand at first was what happens if the two groups of attention learn the same thing; because their attention masks are subtracted from one another if they both output similar values the attention across the board will drop to zero and this will lead to high loss. So the only way to reduce loss is if they learn to attend to different things. One of the simplest strategies they could learn (and this paper claims that they do) is for one group to focus on relevant context and the other to focus on irrelevant context. Thus one group learns the noise and the other the signal (it&#x27;s not this cut and dry but is a useful simplification for understanding IMO).</div><br/><div id="41778252" class="c"><input type="checkbox" id="c-41778252" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41777920">parent</a><span>|</span><a href="#41780196">next</a><span>|</span><label class="collapse" for="c-41778252">[-]</label><label class="expand" for="c-41778252">[3 more]</label></div><br/><div class="children"><div class="content">An interesting aspect is that they don&#x27;t do a plain subtraction, but rather subtract a portion of the second softmax.<p>This makes sense, if one considers that the two copies are identical then the softmax outputs would be identical and the difference is zero everywhere. However, by subtracting a scaled copy, the normalization of the difference seems to really boost the signal value(s) over the &quot;noise&quot;, making the signal stand out compared to pre-normalization.</div><br/><div id="41780175" class="c"><input type="checkbox" id="c-41780175" checked=""/><div class="controls bullet"><span class="by">testdfkjahdfh</span><span>|</span><a href="#41777920">root</a><span>|</span><a href="#41778252">parent</a><span>|</span><a href="#41780196">next</a><span>|</span><label class="collapse" for="c-41780175">[-]</label><label class="expand" for="c-41780175">[2 more]</label></div><br/><div class="children"><div class="content">if two attentions A, B are identical, would (A - lambda * B) be just (1-lambda) * A, how does it &quot;boost the signal value(s) over the &quot;noise&quot;&quot;?</div><br/><div id="41784462" class="c"><input type="checkbox" id="c-41784462" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41777920">root</a><span>|</span><a href="#41780175">parent</a><span>|</span><a href="#41780196">next</a><span>|</span><label class="collapse" for="c-41784462">[-]</label><label class="expand" for="c-41784462">[1 more]</label></div><br/><div class="children"><div class="content">How embarrassing, I had one of those &quot;autocorrect moments&quot;. I somehow put the lambda inside the softmax when thinking and trying it without realizing. So what I was playing with in a spreadsheet (so not so obvious as plain code) was<p><pre><code>    softmax(A) - softmax(lambda * A)
</code></pre>
And as so happens, normalizing the output of that that with my test vectors seems to really boost the output the largest component if A and B are equal.</div><br/></div></div></div></div></div></div><div id="41780196" class="c"><input type="checkbox" id="c-41780196" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#41777920">parent</a><span>|</span><a href="#41778252">prev</a><span>|</span><a href="#41777939">next</a><span>|</span><label class="collapse" for="c-41780196">[-]</label><label class="expand" for="c-41780196">[1 more]</label></div><br/><div class="children"><div class="content">&gt; what happens if the two groups of attention learn the same thing<p>I wonder if there&#x27;s a metaphor here for our own experience and utility in &quot;surprise&quot;.<p>Like if one attention head is surprised by what another learns, up-weight it. But if they both find the same, assume it&#x27;s not very surprising and down-weight it.<p>Admittedly, &quot;surprise&quot; is something that has a big section of my knowledgebase[1][2][3] (both as a subjective feeling and adaptive function of our minds, one of the most complex adaptive system we know of)<p>[1] <a href="https:&#x2F;&#x2F;plus.maths.org&#x2F;content&#x2F;information-surprise" rel="nofollow">https:&#x2F;&#x2F;plus.maths.org&#x2F;content&#x2F;information-surprise</a><p>[2] <a href="https:&#x2F;&#x2F;blakeelias.name&#x2F;papers&#x2F;Multi-Agent-Cooperation-Intrinsic-Motivation.pdf" rel="nofollow">https:&#x2F;&#x2F;blakeelias.name&#x2F;papers&#x2F;Multi-Agent-Cooperation-Intri...</a><p>[3] <a href="https:&#x2F;&#x2F;complexity.simplecast.com&#x2F;episodes&#x2F;81&#x2F;transcript" rel="nofollow">https:&#x2F;&#x2F;complexity.simplecast.com&#x2F;episodes&#x2F;81&#x2F;transcript</a></div><br/></div></div><div id="41777939" class="c"><input type="checkbox" id="c-41777939" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41777920">parent</a><span>|</span><a href="#41780196">prev</a><span>|</span><a href="#41778284">next</a><span>|</span><label class="collapse" for="c-41777939">[-]</label><label class="expand" for="c-41777939">[1 more]</label></div><br/><div class="children"><div class="content">There’s probably a small chance that they could both learn the same thing, but it’s probably not likely enough to be a major issue.</div><br/></div></div><div id="41778284" class="c"><input type="checkbox" id="c-41778284" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#41777920">parent</a><span>|</span><a href="#41777939">prev</a><span>|</span><a href="#41778413">next</a><span>|</span><label class="collapse" for="c-41778284">[-]</label><label class="expand" for="c-41778284">[1 more]</label></div><br/><div class="children"><div class="content">Maybe the loss function could penalize them learning the same thing?</div><br/></div></div></div></div><div id="41778413" class="c"><input type="checkbox" id="c-41778413" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41777920">prev</a><span>|</span><a href="#41779975">next</a><span>|</span><label class="collapse" for="c-41778413">[-]</label><label class="expand" for="c-41778413">[5 more]</label></div><br/><div class="children"><div class="content"><i>We empirically find that the setting λᵢₙᵢₜ = 0.8 − 0.6 × exp(−0.3 · (l − 1)) works well in practice</i><p>I wonder about the story behind that formula...</div><br/><div id="41779303" class="c"><input type="checkbox" id="c-41779303" checked=""/><div class="controls bullet"><span class="by">Kubuxu</span><span>|</span><a href="#41778413">parent</a><span>|</span><a href="#41781032">next</a><span>|</span><label class="collapse" for="c-41779303">[-]</label><label class="expand" for="c-41779303">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, 0.8 works well, but let&#x27;s try setting lower layers to lower initial value. Let&#x27;s say 0.2. Ok, I need a formula that will go between 0.2 and 0.8, slowly approaching 0.8. Starts fiddling with numbers for 20min, I guess this can work.</div><br/><div id="41783017" class="c"><input type="checkbox" id="c-41783017" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41778413">root</a><span>|</span><a href="#41779303">parent</a><span>|</span><a href="#41781032">next</a><span>|</span><label class="collapse" for="c-41783017">[-]</label><label class="expand" for="c-41783017">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but in research show some comparisons</div><br/></div></div></div></div><div id="41781032" class="c"><input type="checkbox" id="c-41781032" checked=""/><div class="controls bullet"><span class="by">kridsdale3</span><span>|</span><a href="#41778413">parent</a><span>|</span><a href="#41779303">prev</a><span>|</span><a href="#41781407">next</a><span>|</span><label class="collapse" for="c-41781032">[-]</label><label class="expand" for="c-41781032">[1 more]</label></div><br/><div class="children"><div class="content">A whole lot of things are tuned optimally by rotating an analog dial until things look &#x2F; sound right.</div><br/></div></div><div id="41781407" class="c"><input type="checkbox" id="c-41781407" checked=""/><div class="controls bullet"><span class="by">stellalo</span><span>|</span><a href="#41778413">parent</a><span>|</span><a href="#41781032">prev</a><span>|</span><a href="#41779975">next</a><span>|</span><label class="collapse" for="c-41781407">[-]</label><label class="expand" for="c-41781407">[1 more]</label></div><br/><div class="children"><div class="content">Looks like this makes (at least initially in training) the “negative” attention term smaller in the early layers (smaller l) compared to later layers (larger l). Which I guess makes sense: you probably want to attend a little bit to everything before concluding that it’s really a few spots you should look at.<p>(Although it seems the author do not discuss this choice anywhere in the paper?)</div><br/></div></div></div></div><div id="41779975" class="c"><input type="checkbox" id="c-41779975" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#41778413">prev</a><span>|</span><a href="#41777298">next</a><span>|</span><label class="collapse" for="c-41779975">[-]</label><label class="expand" for="c-41779975">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how much of the value here is from canceling out the positional noise rope produces. I would love to see a table comparing an alibi version of this to an alibi baseline in addition to the rope models here.<p>Crazy gains though congrats to the researchers</div><br/></div></div><div id="41777298" class="c"><input type="checkbox" id="c-41777298" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#41779975">prev</a><span>|</span><a href="#41777898">next</a><span>|</span><label class="collapse" for="c-41777298">[-]</label><label class="expand" for="c-41777298">[14 more]</label></div><br/><div class="children"><div class="content">I wonder what is lost here. Surely there&#x27;s a trade-off...<p>I&#x27;m wondering if there&#x27;s any effect of &quot;creativity&quot;, or ability to interpolate between concepts. Hallucination and creativity feel very related to me. I understand hallucinating as simply being misaligned with the space humans feel appropriate to interpolate between</div><br/><div id="41778047" class="c"><input type="checkbox" id="c-41778047" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41777298">parent</a><span>|</span><a href="#41777640">next</a><span>|</span><label class="collapse" for="c-41778047">[-]</label><label class="expand" for="c-41778047">[9 more]</label></div><br/><div class="children"><div class="content">&gt; Hallucination and creativity feel very related to me.<p>Why? I see them as just sampling errors.<p>Sure a mistake can spark inspiration sometimes, but creativity is much more than mistakes.<p>&gt; I understand hallucinating as simply being misaligned with the space humans feel appropriate to interpolate between<p>These language models are next-token predictors. The way the next token is predicted is by sampling a probability space outputted by the model.<p>That sampling process can be non deterministic.<p>Hallucinations are when that sampling results in tokens that come together to create a false or otherwise unintended statement.<p>You can just as well think of everything a model outputs as a hallucination, but we train the model to output a space what we want them to hallucinate is more likely. Otherwise it just outputs meaningless noise.<p>“Hallucinate” is really an awful word for what it’s trying to describe.</div><br/><div id="41785466" class="c"><input type="checkbox" id="c-41785466" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41778047">parent</a><span>|</span><a href="#41779686">next</a><span>|</span><label class="collapse" for="c-41785466">[-]</label><label class="expand" for="c-41785466">[1 more]</label></div><br/><div class="children"><div class="content">Often see this argument but it doesn&#x27;t hold water for me.  What we call hallucination is usually when the model says something confidently wrong. Yes the sampling procedure is nondeterministic but this is unrelated to hallucinations. The model can generate a distribution to sample with very little weight on the &quot;wrong&quot; output and then this should be ignored by procedures like top-k sampling. The fact that this doesn&#x27;t easily solve the problem shows that hallucination is a deeper problem in the model itself and not just a byproduct of sampling.</div><br/></div></div><div id="41779686" class="c"><input type="checkbox" id="c-41779686" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41778047">parent</a><span>|</span><a href="#41785466">prev</a><span>|</span><a href="#41778326">next</a><span>|</span><label class="collapse" for="c-41779686">[-]</label><label class="expand" for="c-41779686">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You can just as well think of everything a model outputs as a hallucination<p>Exactly. Don&#x27;t forget that an important factor in the success of GPT3 was RLHF, which is essentially training the model to produce &quot;hallucinations&quot; that are more acceptable on average to human trainers.</div><br/></div></div><div id="41778326" class="c"><input type="checkbox" id="c-41778326" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41778047">parent</a><span>|</span><a href="#41779686">prev</a><span>|</span><a href="#41779354">next</a><span>|</span><label class="collapse" for="c-41778326">[-]</label><label class="expand" for="c-41778326">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Sure a mistake can spark inspiration sometimes, but creativity is much more than mistakes.<p>It looks like creativity has many steps but being able to come with novel, unprompted stuff is important, as long as you are able to discard the bullshit earlier.<p>&quot;Hallucination&quot; is only a problem if later layers (or additional networks) can&#x27;t detect and remove it</div><br/><div id="41778746" class="c"><input type="checkbox" id="c-41778746" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41778326">parent</a><span>|</span><a href="#41779354">next</a><span>|</span><label class="collapse" for="c-41778746">[-]</label><label class="expand" for="c-41778746">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Hallucination&quot; is only a problem if later layers (or additional networks) can&#x27;t detect and remove it<p>Yeah I mean sure. Anything is only a problem if it goes undetected.
The issue is that if you rely on statistical model, you’ll always have hallucinations, so you can’t filter statistical output with another statistical model if you need real guarantees.<p>Many products don’t need those guarantees though.</div><br/></div></div></div></div><div id="41779354" class="c"><input type="checkbox" id="c-41779354" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41778047">parent</a><span>|</span><a href="#41778326">prev</a><span>|</span><a href="#41779267">next</a><span>|</span><label class="collapse" for="c-41779354">[-]</label><label class="expand" for="c-41779354">[3 more]</label></div><br/><div class="children"><div class="content">LLM’s are too unpredictable for many practical uses so I’d guess better predictability is better. Hopefully the change the paper proposes will help!<p>But here’s a case for the other side: sure, most mistakes are just errors, but evolution happens via “mistakes.” Also, LLM’s often deliberately add add randomness at inference time.</div><br/><div id="41779684" class="c"><input type="checkbox" id="c-41779684" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41779354">parent</a><span>|</span><a href="#41779267">next</a><span>|</span><label class="collapse" for="c-41779684">[-]</label><label class="expand" for="c-41779684">[2 more]</label></div><br/><div class="children"><div class="content">&gt; evolution happens via “mistakes.”<p>That’s a nice slogan, but it’s a gross oversimplification.<p>In the natural world, you can say that mistakes in DNA replication leads to evolution, but that’s discounting the entire process of natural selection.<p>Same with creativity. 
Look at Picasso. His was a technically brilliant realistic painter at 15, but his work later in life evolved to be more abstract and weird.
I don’t think that was the result of mistakes, but rather intentionally breaking patterns he learned in his youth.</div><br/><div id="41779836" class="c"><input type="checkbox" id="c-41779836" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41779684">parent</a><span>|</span><a href="#41779267">next</a><span>|</span><label class="collapse" for="c-41779836">[-]</label><label class="expand" for="c-41779836">[1 more]</label></div><br/><div class="children"><div class="content">To oversimplify, evolution is a generate-and-test process and the evaluation step is critical. Something needs to decide which variations are better. Often, with generative AI, it’s people who judge the results. Still, generating interesting examples (the brainstorming phase) plays <i>some</i> role in that.<p>I don’t know a whole lot about Picasso’s art, but I imagine the way he evaluated his own work played an important role, in being able to see that sometimes creative accidents are interesting.</div><br/></div></div></div></div></div></div><div id="41779267" class="c"><input type="checkbox" id="c-41779267" checked=""/><div class="controls bullet"><span class="by">thomastjeffery</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41778047">parent</a><span>|</span><a href="#41779354">prev</a><span>|</span><a href="#41777640">next</a><span>|</span><label class="collapse" for="c-41779267">[-]</label><label class="expand" for="c-41779267">[1 more]</label></div><br/><div class="children"><div class="content">Hallucinate is an awful word <i>because of</i> what it is trying to describe.<p>Hallucination describes the same feature you just called &quot;non deterministic sampling&quot;, but exclusively the cases that we don&#x27;t like. It would be really convenient if we could actually draw that line, but <i>we can&#x27;t</i>. If non-determinism is a core feature, then that feature will be present in every case; including the ones we find desirable, and the ones we find undesirable.</div><br/></div></div></div></div><div id="41777640" class="c"><input type="checkbox" id="c-41777640" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41777298">parent</a><span>|</span><a href="#41778047">prev</a><span>|</span><a href="#41777437">next</a><span>|</span><label class="collapse" for="c-41777640">[-]</label><label class="expand" for="c-41777640">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Surely there&#x27;s a trade-off...<p>For one, speed and memory. They have twice as many Q and K weights in the attention blocks, leading to a ~10% reduction in throughput on their H100 (table 7 in appendix A).</div><br/><div id="41778717" class="c"><input type="checkbox" id="c-41778717" checked=""/><div class="controls bullet"><span class="by">lennxa</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41777640">parent</a><span>|</span><a href="#41778916">next</a><span>|</span><label class="collapse" for="c-41778717">[-]</label><label class="expand" for="c-41778717">[1 more]</label></div><br/><div class="children"><div class="content">they mention similar performance to vanilla transformer with significantly reduced param count though</div><br/></div></div><div id="41778916" class="c"><input type="checkbox" id="c-41778916" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#41777298">root</a><span>|</span><a href="#41777640">parent</a><span>|</span><a href="#41778717">prev</a><span>|</span><a href="#41777437">next</a><span>|</span><label class="collapse" for="c-41778916">[-]</label><label class="expand" for="c-41778916">[1 more]</label></div><br/><div class="children"><div class="content">I mean it doesn’t necessarily needs 2x QK to match that performance, in terms of accuracy, of a regular transformer right?</div><br/></div></div></div></div><div id="41777437" class="c"><input type="checkbox" id="c-41777437" checked=""/><div class="controls bullet"><span class="by">watsonmusic</span><span>|</span><a href="#41777298">parent</a><span>|</span><a href="#41777640">prev</a><span>|</span><a href="#41777898">next</a><span>|</span><label class="collapse" for="c-41777437">[-]</label><label class="expand" for="c-41777437">[1 more]</label></div><br/><div class="children"><div class="content">not all hallucinations are creativity
Imaginate that for a RAG application, the model is supposed to follow the given documents</div><br/></div></div></div></div><div id="41777898" class="c"><input type="checkbox" id="c-41777898" checked=""/><div class="controls bullet"><span class="by">vsroy</span><span>|</span><a href="#41777298">prev</a><span>|</span><a href="#41779579">next</a><span>|</span><label class="collapse" for="c-41777898">[-]</label><label class="expand" for="c-41777898">[3 more]</label></div><br/><div class="children"><div class="content">Is the thing that&#x27;s going on here that softmax can&#x27;t push a value to 0, but by subtracting 2 softmax maps we can output 0s?</div><br/><div id="41781421" class="c"><input type="checkbox" id="c-41781421" checked=""/><div class="controls bullet"><span class="by">vsroy</span><span>|</span><a href="#41777898">parent</a><span>|</span><a href="#41778150">next</a><span>|</span><label class="collapse" for="c-41781421">[-]</label><label class="expand" for="c-41781421">[1 more]</label></div><br/><div class="children"><div class="content">Follow-up question is: Isn&#x27;t it extremely unlikely to output 0?</div><br/></div></div><div id="41778150" class="c"><input type="checkbox" id="c-41778150" checked=""/><div class="controls bullet"><span class="by">pkoird</span><span>|</span><a href="#41777898">parent</a><span>|</span><a href="#41781421">prev</a><span>|</span><a href="#41779579">next</a><span>|</span><label class="collapse" for="c-41778150">[-]</label><label class="expand" for="c-41778150">[1 more]</label></div><br/><div class="children"><div class="content">Or negatives</div><br/></div></div></div></div><div id="41779579" class="c"><input type="checkbox" id="c-41779579" checked=""/><div class="controls bullet"><span class="by">machinelearning</span><span>|</span><a href="#41777898">prev</a><span>|</span><a href="#41779046">next</a><span>|</span><label class="collapse" for="c-41779579">[-]</label><label class="expand" for="c-41779579">[2 more]</label></div><br/><div class="children"><div class="content">This is a good problem to solve but the approach is wrong imo.<p>It has to be done in a hierarchical way to know what you attended to + full context.<p>If the differential vector is being computed with the same input as the attention vector how do you know how to modify the attention vector correctly</div><br/><div id="41780027" class="c"><input type="checkbox" id="c-41780027" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41779579">parent</a><span>|</span><a href="#41779046">next</a><span>|</span><label class="collapse" for="c-41780027">[-]</label><label class="expand" for="c-41780027">[1 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t everything just get tweaked in whatever direction the back-propagation derivative says and proportionally to that &quot;slope&quot;? In other words, simply by having back-propagation system in effect there&#x27;s never any question about which way to adjust the weights, right?</div><br/></div></div></div></div><div id="41779046" class="c"><input type="checkbox" id="c-41779046" checked=""/><div class="controls bullet"><span class="by">miven</span><span>|</span><a href="#41779579">prev</a><span>|</span><a href="#41777542">next</a><span>|</span><label class="collapse" for="c-41779046">[-]</label><label class="expand" for="c-41779046">[1 more]</label></div><br/><div class="children"><div class="content">Is there an intuitive reason why this ends up working this well compared to, say, applying some kind of thresholding to attention activations that are below average for a given head to filter that same attention noise out?</div><br/></div></div><div id="41777542" class="c"><input type="checkbox" id="c-41777542" checked=""/><div class="controls bullet"><span class="by">pxdm</span><span>|</span><a href="#41779046">prev</a><span>|</span><a href="#41777899">next</a><span>|</span><label class="collapse" for="c-41777542">[-]</label><label class="expand" for="c-41777542">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the comparison with conventional attention using a more aggressive (lower temperature) softmax? I can imagine that for the multi-needle retrieval test this may also give a performance boost, although at some cost other more creative tasks.</div><br/><div id="41781277" class="c"><input type="checkbox" id="c-41781277" checked=""/><div class="controls bullet"><span class="by">mota7</span><span>|</span><a href="#41777542">parent</a><span>|</span><a href="#41777899">next</a><span>|</span><label class="collapse" for="c-41781277">[-]</label><label class="expand" for="c-41781277">[1 more]</label></div><br/><div class="children"><div class="content">I had the same thought: Just eye-balling the graphs, the result of the subtraction looks very close to just reducing the temperature.<p>They&#x27;re effectively doing softmax with a fixed temperature, but it&#x27;s unclear that this work is going to do better than just learning a per-head temperature parameter.<p>c.f. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2010.04245" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2010.04245</a>  which shows an improvement by learning per-head temperature.<p>The other way to think about this is that it looks like a hacked-up kinda-sorta gated attention. If that&#x27;s the case, then doing softmax(alpha<i>q_1</i>k_1^T - log_sigmoid(beta<i>q_2</i>k_2^T)) might be better? (where alpha,beta are learned temperatures).</div><br/></div></div></div></div><div id="41777899" class="c"><input type="checkbox" id="c-41777899" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#41777542">prev</a><span>|</span><a href="#41777857">next</a><span>|</span><label class="collapse" for="c-41777899">[-]</label><label class="expand" for="c-41777899">[1 more]</label></div><br/><div class="children"><div class="content">Was just going to mention that it seems that it should be possible to make a Flash Attention version of this algorithm and was pleasantly surprised to see they already included an implementation of one :)</div><br/></div></div><div id="41777857" class="c"><input type="checkbox" id="c-41777857" checked=""/><div class="controls bullet"><span class="by">nmacias</span><span>|</span><a href="#41777899">prev</a><span>|</span><a href="#41779630">next</a><span>|</span><label class="collapse" for="c-41777857">[-]</label><label class="expand" for="c-41777857">[1 more]</label></div><br/><div class="children"><div class="content">AdderaLLM was <i>right there</i></div><br/></div></div><div id="41779630" class="c"><input type="checkbox" id="c-41779630" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41777857">prev</a><span>|</span><a href="#41778057">next</a><span>|</span><label class="collapse" for="c-41779630">[-]</label><label class="expand" for="c-41779630">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get it. Arbitrary linear combinations are already accommodated via feed forward. What am I missing?</div><br/><div id="41785363" class="c"><input type="checkbox" id="c-41785363" checked=""/><div class="controls bullet"><span class="by">thatsadude</span><span>|</span><a href="#41779630">parent</a><span>|</span><a href="#41781616">next</a><span>|</span><label class="collapse" for="c-41785363">[-]</label><label class="expand" for="c-41785363">[1 more]</label></div><br/><div class="children"><div class="content">True, that&#x27;s a yellow flag for me.</div><br/></div></div><div id="41781616" class="c"><input type="checkbox" id="c-41781616" checked=""/><div class="controls bullet"><span class="by">michalsustr</span><span>|</span><a href="#41779630">parent</a><span>|</span><a href="#41785363">prev</a><span>|</span><a href="#41778057">next</a><span>|</span><label class="collapse" for="c-41781616">[-]</label><label class="expand" for="c-41781616">[1 more]</label></div><br/><div class="children"><div class="content">My hunch is that this effectively creates a differentiable minimax “search” “tree” that can be backpropagated through. Not a tree — a dag really — and not search, but learning. :)</div><br/></div></div></div></div><div id="41778057" class="c"><input type="checkbox" id="c-41778057" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#41779630">prev</a><span>|</span><a href="#41778676">next</a><span>|</span><label class="collapse" for="c-41778057">[-]</label><label class="expand" for="c-41778057">[1 more]</label></div><br/><div class="children"><div class="content">Anyone remember siamese networks?</div><br/></div></div><div id="41778676" class="c"><input type="checkbox" id="c-41778676" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41778057">prev</a><span>|</span><a href="#41784615">next</a><span>|</span><label class="collapse" for="c-41778676">[-]</label><label class="expand" for="c-41778676">[3 more]</label></div><br/><div class="children"><div class="content">Hmmm, this could be expressed as 2 consecutive attentions in a residual branch:<p>Simplified differential T. looks like: (softmax(Q₁K₁) − λ softmax(Q₂K₂)) V<p>You can factor this into:<p><pre><code>    x = softmax(Q₁K₁)V
    x += -λ softmax(Q₂K₂)V
</code></pre>
which is like 2 subsequent regular attentions added that are sharing V</div><br/><div id="41782952" class="c"><input type="checkbox" id="c-41782952" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#41778676">parent</a><span>|</span><a href="#41779130">next</a><span>|</span><label class="collapse" for="c-41782952">[-]</label><label class="expand" for="c-41782952">[1 more]</label></div><br/><div class="children"><div class="content">Now I&#x27;m wondering, isn&#x27;t there usually a `num_heads x value_dim -&gt; model_dim` projection that goes after a MHA? The W in `softmax(QK)VW`? That one can play the role of this subtraction in a vanilla transformer, no?
So I wonder what kind of advantage does splitting things up like this bring.</div><br/></div></div><div id="41779130" class="c"><input type="checkbox" id="c-41779130" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#41778676">parent</a><span>|</span><a href="#41782952">prev</a><span>|</span><a href="#41784615">next</a><span>|</span><label class="collapse" for="c-41779130">[-]</label><label class="expand" for="c-41779130">[1 more]</label></div><br/><div class="children"><div class="content">You could also extrapolate this into more than two terms by squinting your eyes and saying that λ ∈ {1, -1} is close enough to λi ∈R^d ∣ ∥λi ∥=1. No idea if it would result in better performance, but that&#x27;s research babyyyy!</div><br/></div></div></div></div><div id="41784615" class="c"><input type="checkbox" id="c-41784615" checked=""/><div class="controls bullet"><span class="by">badsandwitch</span><span>|</span><a href="#41778676">prev</a><span>|</span><a href="#41776910">next</a><span>|</span><label class="collapse" for="c-41784615">[-]</label><label class="expand" for="c-41784615">[1 more]</label></div><br/><div class="children"><div class="content">What is purpose of the lambda parameter? Why isn&#x27;t it a constant of 1?</div><br/></div></div><div id="41776910" class="c"><input type="checkbox" id="c-41776910" checked=""/><div class="controls bullet"><span class="by">watsonmusic</span><span>|</span><a href="#41784615">prev</a><span>|</span><a href="#41777296">next</a><span>|</span><label class="collapse" for="c-41776910">[-]</label><label class="expand" for="c-41776910">[1 more]</label></div><br/><div class="children"><div class="content">The modification is simple and beautiful. And the improvements are quite significant.</div><br/></div></div><div id="41777296" class="c"><input type="checkbox" id="c-41777296" checked=""/><div class="controls bullet"><span class="by">digdugdirk</span><span>|</span><a href="#41776910">prev</a><span>|</span><a href="#41777923">next</a><span>|</span><label class="collapse" for="c-41777296">[-]</label><label class="expand" for="c-41777296">[5 more]</label></div><br/><div class="children"><div class="content">Is there any way to replicate this with existing models, or are we going to need to wait for models to be trained in this style?<p>I&#x27;m imagining a smaller model examining the output tokens of a larger model and metaphorically slapping it on the wrist with a ruler if the output tokens start drifting off topic. Not quite the same, but an entertaining thought nonetheless.</div><br/><div id="41777738" class="c"><input type="checkbox" id="c-41777738" checked=""/><div class="controls bullet"><span class="by">bionhoward</span><span>|</span><a href="#41777296">parent</a><span>|</span><a href="#41777313">next</a><span>|</span><label class="collapse" for="c-41777738">[-]</label><label class="expand" for="c-41777738">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I believe this is possible, you could clone weights of one or more existing models and fine tune them in groups with different random seeds for noise&#x2F;drop to produce reasonable outputs under a differential transformer decoding scheme whereby tokens with disagreement receive more attention (surprisal analysis)</div><br/></div></div><div id="41777313" class="c"><input type="checkbox" id="c-41777313" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#41777296">parent</a><span>|</span><a href="#41777738">prev</a><span>|</span><a href="#41777923">next</a><span>|</span><label class="collapse" for="c-41777313">[-]</label><label class="expand" for="c-41777313">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a different attention mechanism with a different map setup, so fundamentally a different type of model</div><br/><div id="41777473" class="c"><input type="checkbox" id="c-41777473" checked=""/><div class="controls bullet"><span class="by">om8</span><span>|</span><a href="#41777296">root</a><span>|</span><a href="#41777313">parent</a><span>|</span><a href="#41777923">next</a><span>|</span><label class="collapse" for="c-41777473">[-]</label><label class="expand" for="c-41777473">[2 more]</label></div><br/><div class="children"><div class="content">Looks like it is a drop in replacement for attention, but models will need to be retrained for this one, yes.</div><br/><div id="41778115" class="c"><input type="checkbox" id="c-41778115" checked=""/><div class="controls bullet"><span class="by">aDyslecticCrow</span><span>|</span><a href="#41777296">root</a><span>|</span><a href="#41777473">parent</a><span>|</span><a href="#41777923">next</a><span>|</span><label class="collapse" for="c-41778115">[-]</label><label class="expand" for="c-41778115">[1 more]</label></div><br/><div class="children"><div class="content">It may not need to be entirely retrained. The value spans and input are the same, and no extra weights are needed. You may be able to tune an existing model with this attention mechanism and get some of the benefits.<p>But overall... it&#x27;s mainly a training change, so training is needed to make a difference.</div><br/></div></div></div></div></div></div></div></div><div id="41777923" class="c"><input type="checkbox" id="c-41777923" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41777296">prev</a><span>|</span><a href="#41778345">next</a><span>|</span><label class="collapse" for="c-41777923">[-]</label><label class="expand" for="c-41777923">[4 more]</label></div><br/><div class="children"><div class="content">&gt; By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization<p>I’m very interested in this claim. I was under the impression that hallucination is unavoidable in these kinds of models. IIRC proof for that was trending on HN a couple weeks ago.</div><br/><div id="41779570" class="c"><input type="checkbox" id="c-41779570" checked=""/><div class="controls bullet"><span class="by">pshc</span><span>|</span><a href="#41777923">parent</a><span>|</span><a href="#41778016">next</a><span>|</span><label class="collapse" for="c-41779570">[-]</label><label class="expand" for="c-41779570">[1 more]</label></div><br/><div class="children"><div class="content">More broadly I think hallucination is inevitable in pure text models. We need model architectures incorporating a stream of real-world ground truth such as a live video feed or embodiment.</div><br/></div></div><div id="41778016" class="c"><input type="checkbox" id="c-41778016" checked=""/><div class="controls bullet"><span class="by">ErikBjare</span><span>|</span><a href="#41777923">parent</a><span>|</span><a href="#41779570">prev</a><span>|</span><a href="#41777977">next</a><span>|</span><label class="collapse" for="c-41778016">[-]</label><label class="expand" for="c-41778016">[1 more]</label></div><br/><div class="children"><div class="content">Mitigate, not completely fix.</div><br/></div></div><div id="41777977" class="c"><input type="checkbox" id="c-41777977" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41777923">parent</a><span>|</span><a href="#41778016">prev</a><span>|</span><a href="#41778345">next</a><span>|</span><label class="collapse" for="c-41777977">[-]</label><label class="expand" for="c-41777977">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not possible to get rid of it entirely, but if you can get the model to bullshit only 0.1% of the time instead of 5% of the time it&#x27;s a massive improvement.<p>Most of it should be happening when there&#x27;s no data to draw conclusions from. E.g. STT models make up words in silence, vision models find things in lens cap noise, LLMs make up explanations when they have no data to pull from.<p>The real solution would be more along the lines of training models to specifically ignore these cases, or in the case of LLMs to just know when to say &quot;I don&#x27;t know&quot;.</div><br/></div></div></div></div><div id="41778345" class="c"><input type="checkbox" id="c-41778345" checked=""/><div class="controls bullet"><span class="by">lucidrains</span><span>|</span><a href="#41777923">prev</a><span>|</span><a href="#41783620">next</a><span>|</span><label class="collapse" for="c-41778345">[-]</label><label class="expand" for="c-41778345">[2 more]</label></div><br/><div class="children"><div class="content">does this not mean we should explore usage of talking heads (Shazeer et al) a bit more? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2003.02436" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2003.02436</a></div><br/></div></div><div id="41783620" class="c"><input type="checkbox" id="c-41783620" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#41778345">prev</a><span>|</span><a href="#41778291">next</a><span>|</span><label class="collapse" for="c-41783620">[-]</label><label class="expand" for="c-41783620">[1 more]</label></div><br/><div class="children"><div class="content">How is this different than using a sparsity-inducing prior?</div><br/></div></div><div id="41778291" class="c"><input type="checkbox" id="c-41778291" checked=""/><div class="controls bullet"><span class="by">x49asvk</span><span>|</span><a href="#41783620">prev</a><span>|</span><a href="#41776832">next</a><span>|</span><label class="collapse" for="c-41778291">[-]</label><label class="expand" for="c-41778291">[1 more]</label></div><br/><div class="children"><div class="content">This concept is really interesting to me, I am very very new to transformers but would love to learn more about normal transformers and differential too. 
Can anyone suggest any resources?</div><br/></div></div><div id="41776832" class="c"><input type="checkbox" id="c-41776832" checked=""/><div class="controls bullet"><span class="by">pikseladam</span><span>|</span><a href="#41778291">prev</a><span>|</span><a href="#41777963">next</a><span>|</span><label class="collapse" for="c-41776832">[-]</label><label class="expand" for="c-41776832">[33 more]</label></div><br/><div class="children"><div class="content">Did this mean they solved the hallucination problem of transformers?<p>edit: not fully but it gives promising results. quiet an improvement actually.</div><br/><div id="41777894" class="c"><input type="checkbox" id="c-41777894" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#41776832">parent</a><span>|</span><a href="#41776873">next</a><span>|</span><label class="collapse" for="c-41777894">[-]</label><label class="expand" for="c-41777894">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think there&#x27;s any narrow definition of what &quot;hallucination&quot; means. It generally refers to the model giving non-factual answers in contexts that are meant to be factual, but not all causes of this are going to be fixable without very major changes.<p>The fundamental issue is that most of the time LLMs are going to be combining statistics derived from many training samples when generating a single continuation, and there is just no guarantee that this will result in a semantically coherent response. Of course the model&#x27;s depth of parsing and semantic analysis usually means that each generated word is highly plausible, but this isn&#x27;t the same as being factually correct, especially so in these cases where the model is drawing on multiple sources to create a mashup response, which is the normal mode of operation.</div><br/></div></div><div id="41776873" class="c"><input type="checkbox" id="c-41776873" checked=""/><div class="controls bullet"><span class="by">lafreb</span><span>|</span><a href="#41776832">parent</a><span>|</span><a href="#41777894">prev</a><span>|</span><a href="#41776887">next</a><span>|</span><label class="collapse" for="c-41776873">[-]</label><label class="expand" for="c-41776873">[30 more]</label></div><br/><div class="children"><div class="content">The paper says that they&#x27;ve improved hallucination mitigation, but not really &quot;solved&quot; the issue.</div><br/><div id="41776911" class="c"><input type="checkbox" id="c-41776911" checked=""/><div class="controls bullet"><span class="by">Rhapso</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41776873">parent</a><span>|</span><a href="#41776887">next</a><span>|</span><label class="collapse" for="c-41776911">[-]</label><label class="expand" for="c-41776911">[29 more]</label></div><br/><div class="children"><div class="content">&quot;Hallucination&quot; isn&#x27;t really a problem that can be &quot;fixed&quot;. Its just model error.<p>The root problem is simply that the model doesn&#x27;t capture reality, just an approximation. What we are incorrectly calling &quot;hallucination&quot; is just the best the model has to offer.</div><br/><div id="41777113" class="c"><input type="checkbox" id="c-41777113" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41776911">parent</a><span>|</span><a href="#41776974">next</a><span>|</span><label class="collapse" for="c-41777113">[-]</label><label class="expand" for="c-41777113">[7 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not &quot;just&quot; model error<p>during pre-training, there is never an incentive for the model to say &quot;I don&#x27;t know&quot; because it would be penalized. the model is incentivized to make an educated guess<p>large transformer models are <i>really</i> good at approximating their dataset. there is no data on the internet about what LLMs know. and even if there were such data, it would probably become obsolete soon<p>that being said, maybe a big shift in the architecture could solve this. I hope!</div><br/><div id="41777410" class="c"><input type="checkbox" id="c-41777410" checked=""/><div class="controls bullet"><span class="by">happypumpkin</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777113">parent</a><span>|</span><a href="#41777992">next</a><span>|</span><label class="collapse" for="c-41777410">[-]</label><label class="expand" for="c-41777410">[4 more]</label></div><br/><div class="children"><div class="content">&gt; it would probably become obsolete soon<p>Suppose there are many times more posts about something one generation of LLMs can&#x27;t do (arithmetic, tic-tac-toe, whatever), than posts about how the next generation of models <i>can</i> do that task successfully. I think this is probably the case.<p>While I doubt it will happen, it would be somewhat funny if training on that text caused a future model to claim it can&#x27;t do something that it &quot;should&quot; be able to because it internalized that it was an LLM and &quot;LLMs can&#x27;t do X.&quot;</div><br/><div id="41777500" class="c"><input type="checkbox" id="c-41777500" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777410">parent</a><span>|</span><a href="#41777992">next</a><span>|</span><label class="collapse" for="c-41777500">[-]</label><label class="expand" for="c-41777500">[3 more]</label></div><br/><div class="children"><div class="content">also presumes that the LLM knows it is an LLM</div><br/><div id="41778948" class="c"><input type="checkbox" id="c-41778948" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777500">parent</a><span>|</span><a href="#41778783">next</a><span>|</span><label class="collapse" for="c-41778948">[-]</label><label class="expand" for="c-41778948">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re generally fine tuned not to. I&#x27;m not sure how long that will hold though.</div><br/></div></div><div id="41778783" class="c"><input type="checkbox" id="c-41778783" checked=""/><div class="controls bullet"><span class="by">adwn</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777500">parent</a><span>|</span><a href="#41778948">prev</a><span>|</span><a href="#41777992">next</a><span>|</span><label class="collapse" for="c-41778783">[-]</label><label class="expand" for="c-41778783">[1 more]</label></div><br/><div class="children"><div class="content">System prompts sometimes contain the information that &quot;it&quot; is an LLM.<p>Maybe in the future, those prompts will include motivational phrases, like &quot;You can do it!&quot; or &quot;Believe in yourself, then you can achieve anything.&quot;</div><br/></div></div></div></div></div></div><div id="41777992" class="c"><input type="checkbox" id="c-41777992" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777113">parent</a><span>|</span><a href="#41777410">prev</a><span>|</span><a href="#41777942">next</a><span>|</span><label class="collapse" for="c-41777992">[-]</label><label class="expand" for="c-41777992">[1 more]</label></div><br/><div class="children"><div class="content">in another paper which popped up recently they approximated uncertainty with Entropy and inserted &quot;wait!&quot; tokens whenever Entropy was high, simulating chain of thought within the system.</div><br/></div></div><div id="41777942" class="c"><input type="checkbox" id="c-41777942" checked=""/><div class="controls bullet"><span class="by">spywaregorilla</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777113">parent</a><span>|</span><a href="#41777992">prev</a><span>|</span><a href="#41776974">next</a><span>|</span><label class="collapse" for="c-41777942">[-]</label><label class="expand" for="c-41777942">[1 more]</label></div><br/><div class="children"><div class="content">&gt; during pre-training, there is never an incentive for the model to say &quot;I don&#x27;t know&quot; because it would be penalized. the model is incentivized to make an educated guess<p>The guess can be &quot;I don&#x27;t know&quot;. The base LLM would generally only say I don&#x27;t know if it &quot;knew&quot; that it didn&#x27;t know, which is not going to be very common. The tuned LLM would be the level responsible for trying to equate a lack of understanding to saying &quot;I don&#x27;t know&quot;</div><br/></div></div></div></div><div id="41776974" class="c"><input type="checkbox" id="c-41776974" checked=""/><div class="controls bullet"><span class="by">dilap</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41776911">parent</a><span>|</span><a href="#41777113">prev</a><span>|</span><a href="#41777110">next</a><span>|</span><label class="collapse" for="c-41776974">[-]</label><label class="expand" for="c-41776974">[20 more]</label></div><br/><div class="children"><div class="content">it can be fixed in theory if the model knows-what-it-knows, to avoid saying things its uncertain about (this is what (some) humans do to reduce the frequency w which they say untrue things).<p>theres some promising research using this idea, tho i dont have it at hand.</div><br/><div id="41777105" class="c"><input type="checkbox" id="c-41777105" checked=""/><div class="controls bullet"><span class="by">hoosieree</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41776974">parent</a><span>|</span><a href="#41777424">next</a><span>|</span><label class="collapse" for="c-41777105">[-]</label><label class="expand" for="c-41777105">[8 more]</label></div><br/><div class="children"><div class="content">LLMs can&#x27;t hallucinate. They generate the next most likely token in a sequence. Whether that sequence matches any kind of objective truth is orthogonal to how models work.<p>I suppose depending on your point of view, LLMs either <i>can&#x27;t</i> hallucinate, or <i>that&#x27;s all they can do</i>.</div><br/><div id="41777378" class="c"><input type="checkbox" id="c-41777378" checked=""/><div class="controls bullet"><span class="by">ToValueFunfetti</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777105">parent</a><span>|</span><a href="#41777268">next</a><span>|</span><label class="collapse" for="c-41777378">[-]</label><label class="expand" for="c-41777378">[6 more]</label></div><br/><div class="children"><div class="content">&gt;Whether that sequence matches any kind of objective truth is orthogonal to how models work.<p>Empirically, this cannot be true. If it were, it would be statistically shocking how often models coincidentally say true things. The training does not perfectly align the model with truth, but &#x27;orthogonal&#x27; is off by a minimum of 45 degrees.</div><br/><div id="41777442" class="c"><input type="checkbox" id="c-41777442" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777378">parent</a><span>|</span><a href="#41777268">next</a><span>|</span><label class="collapse" for="c-41777442">[-]</label><label class="expand" for="c-41777442">[5 more]</label></div><br/><div class="children"><div class="content">It matches the training data. Whether the training data matches truth (and whether it&#x27;s correctly understood - sarcasm included) is a completely separate thing.<p>&gt; The training does not perfectly align the model with truth, but &#x27;orthogonal&#x27;<p>Nitpicky, but the more dimensions you have, the easier it is for almost everything to be orthogonal. (<a href="https:&#x2F;&#x2F;softwaredoug.com&#x2F;blog&#x2F;2022&#x2F;12&#x2F;26&#x2F;surpries-at-hi-dimensions-orthoginality" rel="nofollow">https:&#x2F;&#x2F;softwaredoug.com&#x2F;blog&#x2F;2022&#x2F;12&#x2F;26&#x2F;surpries-at-hi-dime...</a>) That&#x27;s why averaging embeddings works.</div><br/><div id="41778012" class="c"><input type="checkbox" id="c-41778012" checked=""/><div class="controls bullet"><span class="by">ToValueFunfetti</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777442">parent</a><span>|</span><a href="#41777784">next</a><span>|</span><label class="collapse" for="c-41778012">[-]</label><label class="expand" for="c-41778012">[3 more]</label></div><br/><div class="children"><div class="content">I went to school to learn about the world and the overwhelming majority of that learning was from professors and textbooks. Whether the professors&#x27; beliefs and the textbooks&#x27; contents reflected the true properties of the world was a completely separate thing, entirely outside of my control. But I did come away with a better understanding of the world and few would say that education is orthogonal to that goal.<p>If you add two vectors that don&#x27;t have a truth component (ie. are orthogonal to the truth), the resulting vector should be no closer to the truth. If you start with random weights and perform some operation on them such that the new weights have a higher likelihood of producing true statements, the operation must not have been orthogonal to the truth. Am I wrong there?</div><br/><div id="41778103" class="c"><input type="checkbox" id="c-41778103" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41778012">parent</a><span>|</span><a href="#41777784">next</a><span>|</span><label class="collapse" for="c-41778103">[-]</label><label class="expand" for="c-41778103">[2 more]</label></div><br/><div class="children"><div class="content">&gt; But I did come away with a better understanding of the world and few would say that education is orthogonal to that goal.<p>That&#x27;s due to the reward function &#x2F; environment. But even outside extremes like North Korea, lots of education environments value conformity over independent analysis.</div><br/><div id="41778979" class="c"><input type="checkbox" id="c-41778979" checked=""/><div class="controls bullet"><span class="by">ToValueFunfetti</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41778103">parent</a><span>|</span><a href="#41777784">next</a><span>|</span><label class="collapse" for="c-41778979">[-]</label><label class="expand" for="c-41778979">[1 more]</label></div><br/><div class="children"><div class="content">Certainly an AI trained on North Korean data would emerge with some very suspect beliefs regarding Kim Jong-Un. My point is just that aligning something with training data is aligning it with truth, to the degree that the training data is true and regardless of why it is true. educate(me, truth) can hardly be called orthogonal to the truth, even if the &#x27;educate&#x27; and &#x27;me&#x27; terms do nothing to prevent educate(me, falsehood).</div><br/></div></div></div></div></div></div><div id="41777784" class="c"><input type="checkbox" id="c-41777784" checked=""/><div class="controls bullet"><span class="by">timcobb</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777442">parent</a><span>|</span><a href="#41778012">prev</a><span>|</span><a href="#41777268">next</a><span>|</span><label class="collapse" for="c-41777784">[-]</label><label class="expand" for="c-41777784">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this the same thing that happens when you train a human on truths vs falsehoods?</div><br/></div></div></div></div></div></div><div id="41777268" class="c"><input type="checkbox" id="c-41777268" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777105">parent</a><span>|</span><a href="#41777378">prev</a><span>|</span><a href="#41777424">next</a><span>|</span><label class="collapse" for="c-41777268">[-]</label><label class="expand" for="c-41777268">[1 more]</label></div><br/><div class="children"><div class="content">Whenever someone takes issue with using the word “hallucinate” with LLMs I get the impression they’re trying to convince me that hallucination is good.<p>Why do you care so much about this particular issue? And why can’t hallucination be something we can aim to improve?</div><br/></div></div></div></div><div id="41777424" class="c"><input type="checkbox" id="c-41777424" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41776974">parent</a><span>|</span><a href="#41777105">prev</a><span>|</span><a href="#41777021">next</a><span>|</span><label class="collapse" for="c-41777424">[-]</label><label class="expand" for="c-41777424">[1 more]</label></div><br/><div class="children"><div class="content">This reminds me it&#x27;s easy to train similarity models, hard to train identity&#x2F;equivalence prediction. Two strings can be similar in many ways, like &quot;Address Line 1&quot; and &quot;Address Line 2&quot; or &quot;Position_X&quot; and &quot;Position_Y&quot;, yet distinct in meaning. That one character makes all the difference. On the other hand &quot;Vendor Name&quot; is equivalent with &quot;Seller Company&quot; even though they are pretty different lexically.<p>The dot product, which is at the core of attention, is good for similarity not identity. I think this is why models hallucinate - how can they tell the distinction between &quot;I have trained on this fact&quot; and &quot;Looks like something I trained on&quot;.</div><br/></div></div><div id="41777021" class="c"><input type="checkbox" id="c-41777021" checked=""/><div class="controls bullet"><span class="by">atrus</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41776974">parent</a><span>|</span><a href="#41777424">prev</a><span>|</span><a href="#41777017">next</a><span>|</span><label class="collapse" for="c-41777021">[-]</label><label class="expand" for="c-41777021">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that fixes it, even in theory, since there&#x27;s always <i>some</i> uncertainty.</div><br/></div></div><div id="41777017" class="c"><input type="checkbox" id="c-41777017" checked=""/><div class="controls bullet"><span class="by">AnimalMuppet</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41776974">parent</a><span>|</span><a href="#41777021">prev</a><span>|</span><a href="#41777110">next</a><span>|</span><label class="collapse" for="c-41777017">[-]</label><label class="expand" for="c-41777017">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure there&#x27;s something I don&#x27;t understand, but:<p>Doesn&#x27;t an LLM pick the &quot;most probable next symbol&quot; (or, depending on temperature, <i>one</i> of the most probable next symbols)?  To do that, doesn&#x27;t it have to have some idea of what the probability is?  Couldn&#x27;t it then, if the probability falls below some threshold, say &quot;I don&#x27;t know&quot; instead of giving what it knows is a low-probability answer?</div><br/><div id="41777153" class="c"><input type="checkbox" id="c-41777153" checked=""/><div class="controls bullet"><span class="by">darkPotato</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777017">parent</a><span>|</span><a href="#41777231">next</a><span>|</span><label class="collapse" for="c-41777153">[-]</label><label class="expand" for="c-41777153">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that the hallucination is, out of all the possibilities, the most probable one (ignoring temperature). So the hallucination is the most probable sequence of tokens at that point. The model may be able to predict an &quot;I don&#x27;t have that information&quot; given the right context. But ensuring that in general is an open question.</div><br/></div></div><div id="41777231" class="c"><input type="checkbox" id="c-41777231" checked=""/><div class="controls bullet"><span class="by">dTal</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777017">parent</a><span>|</span><a href="#41777153">prev</a><span>|</span><a href="#41777071">next</a><span>|</span><label class="collapse" for="c-41777231">[-]</label><label class="expand" for="c-41777231">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t really work like that.<p>1) The model outputs a ranked list of all tokens; the probability always sums to 1. Sometimes there is a clear &quot;#1 candidate&quot;, very often there are a number of plausible candidates. This is just how language works - there are multiple ways to phrase things, and you can&#x27;t have the model give up every time there is a choice of synonyms.<p>2) Probability of a token is not the same as probability of a fact. Consider a language model that knows the approximate population of Paris (2 million) but is not confident about the exact figure. Feed such a model the string &quot;The exact population of Paris is&quot; and it will begin with &quot;2&quot; but halfway through the number it will have a more or less arbitrary choice of 10 digits. &quot;2.1I don&#x27;t know&quot; is neither a desirable answer, nor a plausible one from the model&#x27;s perspective.</div><br/></div></div><div id="41777071" class="c"><input type="checkbox" id="c-41777071" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777017">parent</a><span>|</span><a href="#41777231">prev</a><span>|</span><a href="#41777092">next</a><span>|</span><label class="collapse" for="c-41777071">[-]</label><label class="expand" for="c-41777071">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Doesn&#x27;t an LLM pick the &quot;most probable next symbol&quot;<p>Yes, but that very rarely matters. (Almost never when it&#x27;s brought up in discussions)<p>&gt;  Couldn&#x27;t it then, if the probability falls below some threshold, say &quot;I don&#x27;t know&quot; instead of giving what it knows is a low-probability answer?<p>A low probability doesn&#x27;t necessarily mean something&#x27;s incorrect. Responding to your question in French would also have very low probability, even if it&#x27;s correct. There&#x27;s also some nuance around what&#x27;s classified as a hallucination... Maybe something in the training data did suggest that answer as correct.<p>There are ideas similar to this one though. It&#x27;s just a bit more complex than pure probabilities going down. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.19648" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.19648</a></div><br/></div></div><div id="41777092" class="c"><input type="checkbox" id="c-41777092" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777017">parent</a><span>|</span><a href="#41777071">prev</a><span>|</span><a href="#41777284">next</a><span>|</span><label class="collapse" for="c-41777092">[-]</label><label class="expand" for="c-41777092">[2 more]</label></div><br/><div class="children"><div class="content">You need to separate out the LLM, which only produces a set of probabilities, from the system, which includes the LLM and the sampling methodology. Sampling is currently not very intelligent at all.<p>The next bit of confusion is that the &#x27;probability&#x27; isn&#x27;t &#x27;real&#x27;. It&#x27;s not an actual probability but a weight that sums up to one, which is close enough to how probability works that we call it that. However, sometimes there are several good answers and so all the good answers get a lower probability because there are 5 of them. A fixed threshold is not a good idea in this case. Instead, smarter sampling methods are necessary. One possibility is that if we do have seeming confusion, to put a &#x27;confusion marker&#x27; into the text and predict the next output and train models to refine the answer as they go along. Not sure if any work has been done here, but this seems to go along with what you&#x27;re interested in</div><br/><div id="41777244" class="c"><input type="checkbox" id="c-41777244" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777092">parent</a><span>|</span><a href="#41777284">next</a><span>|</span><label class="collapse" for="c-41777244">[-]</label><label class="expand" for="c-41777244">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However, sometimes there are several good answers and so all the good answers get a lower probability because there are 5 of them.<p>That&#x27;s the result after softmax. If you want to act on the raw results, you can still do that.</div><br/></div></div></div></div><div id="41777284" class="c"><input type="checkbox" id="c-41777284" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777017">parent</a><span>|</span><a href="#41777092">prev</a><span>|</span><a href="#41777106">next</a><span>|</span><label class="collapse" for="c-41777284">[-]</label><label class="expand" for="c-41777284">[1 more]</label></div><br/><div class="children"><div class="content">This may work when the next token is a key concept but when it&#x27;s a filler word or a part of one of many sequences of words that can convey the same meaning but in different ways (synonyms but not only at the word also at the sentence levels) then it&#x27;s harder to know whether the probability is low because the word is absolutely unlikely or because it&#x27;s likelihood is spread&#x2F;shared among other truthful statements</div><br/></div></div><div id="41777106" class="c"><input type="checkbox" id="c-41777106" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777017">parent</a><span>|</span><a href="#41777284">prev</a><span>|</span><a href="#41777112">next</a><span>|</span><label class="collapse" for="c-41777106">[-]</label><label class="expand" for="c-41777106">[1 more]</label></div><br/><div class="children"><div class="content">You would need some kind of referential facts that you hold as true, then some introspection method to align sentences to those. if it can’t be done, the output may be “I don’t know”. But even for programming languages (simplest useful languages), it would be hard to do.</div><br/></div></div><div id="41777112" class="c"><input type="checkbox" id="c-41777112" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41777017">parent</a><span>|</span><a href="#41777106">prev</a><span>|</span><a href="#41777110">next</a><span>|</span><label class="collapse" for="c-41777112">[-]</label><label class="expand" for="c-41777112">[1 more]</label></div><br/><div class="children"><div class="content">My guess is the problem is words with high probabilities that happen to be part of a wrong answer.<p>For one thing the probability of a word occurring is just a probability of the word occurring in a certain sample,  it&#x27;s not an indicator of truth.  (e.g. the most problematic concept in philosophy in that just introducing it undermines the truth,  see &quot;9&#x2F;11 truther&quot;)  It&#x27;s also not sufficient to pick a &quot;true&quot; word or always pick a &quot;true&quot; word but rather the truthfulness of a statement needs to be evaluated based on the statement as a whole.<p>A word might have a low probability because it competes with a large number of alternatives that are equally likely which is not a reason to stop generation.</div><br/></div></div></div></div></div></div><div id="41777110" class="c"><input type="checkbox" id="c-41777110" checked=""/><div class="controls bullet"><span class="by">tucnak</span><span>|</span><a href="#41776832">root</a><span>|</span><a href="#41776911">parent</a><span>|</span><a href="#41776974">prev</a><span>|</span><a href="#41776887">next</a><span>|</span><label class="collapse" for="c-41777110">[-]</label><label class="expand" for="c-41777110">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m led to believe this is mostly because &quot;known unknowns&quot; are not well-represented in the training datasets... I think, instead of bothering with refusals and enforcing a particular &quot;voice&quot; with excessive RL, they ought to focus more on identifying &quot;gaps&quot; in the datasets and feeding them back, perhaps they&#x27;re already doing this with synthetic data &#x2F; distillation.</div><br/></div></div></div></div></div></div><div id="41776887" class="c"><input type="checkbox" id="c-41776887" checked=""/><div class="controls bullet"><span class="by">watsonmusic</span><span>|</span><a href="#41776832">parent</a><span>|</span><a href="#41776873">prev</a><span>|</span><a href="#41777963">next</a><span>|</span><label class="collapse" for="c-41776887">[-]</label><label class="expand" for="c-41776887">[1 more]</label></div><br/><div class="children"><div class="content">that would be huge!</div><br/></div></div></div></div><div id="41777963" class="c"><input type="checkbox" id="c-41777963" checked=""/><div class="controls bullet"><span class="by">nowayno583</span><span>|</span><a href="#41776832">prev</a><span>|</span><a href="#41776886">next</a><span>|</span><label class="collapse" for="c-41777963">[-]</label><label class="expand" for="c-41777963">[5 more]</label></div><br/><div class="children"><div class="content">Does anyone understand why they are taking the difference between transformers instead of the sum? It seems to me that in a noise reducing solution we would be more interested in the sum, as random noise would cancel out and signal would be constructive.<p>Of course, even if I&#x27;m right proper training would account to that by inverting signs where appropriate. Still, it seems weird to present it as the difference, especially seeing as they compare this directly to noise cancelling headphones, where we sum both microphones inputs.</div><br/><div id="41778075" class="c"><input type="checkbox" id="c-41778075" checked=""/><div class="controls bullet"><span class="by">aDyslecticCrow</span><span>|</span><a href="#41777963">parent</a><span>|</span><a href="#41778085">next</a><span>|</span><label class="collapse" for="c-41778075">[-]</label><label class="expand" for="c-41778075">[2 more]</label></div><br/><div class="children"><div class="content">The noise isn&#x27;t truly random; it&#x27;s just a matrix of small values that shouldn&#x27;t be taken into account. Subtracting them cancels them out.<p>As pointed out by a different comment, it&#x27;s actually the attention we are interested in that is cancelled out *if they are both equal*. This is what the paper mentions in its abstract;<p>&gt; promoting the emergence of sparse attention patterns<p>In theory, it is quite clever, and their results seem to back it up.</div><br/></div></div><div id="41778085" class="c"><input type="checkbox" id="c-41778085" checked=""/><div class="controls bullet"><span class="by">thegeomaster</span><span>|</span><a href="#41777963">parent</a><span>|</span><a href="#41778075">prev</a><span>|</span><a href="#41776886">next</a><span>|</span><label class="collapse" for="c-41778085">[-]</label><label class="expand" for="c-41778085">[2 more]</label></div><br/><div class="children"><div class="content">I suspect that plus vs minus is arbitrary in this case (as you said, due to being able to learn a simple negation during training), but they are presenting it in this way because it is more intuitive. Indeed, adding two sources that are noisy in the same way just doubles the noise, whereas subtracting cancels it out. It&#x27;s how balanced audio cables work, for example.<p>But with noise cancelling headphones, we don&#x27;t sum anything directly---we emit an inverted sound, and to the human ear, this sounds like a subtraction of the two signals. (Audio from the audio source, and noise from the microphone.)</div><br/><div id="41778329" class="c"><input type="checkbox" id="c-41778329" checked=""/><div class="controls bullet"><span class="by">nowayno583</span><span>|</span><a href="#41777963">root</a><span>|</span><a href="#41778085">parent</a><span>|</span><a href="#41776886">next</a><span>|</span><label class="collapse" for="c-41778329">[-]</label><label class="expand" for="c-41778329">[1 more]</label></div><br/><div class="children"><div class="content">Oh! It&#x27;s been a good while since I&#x27;ve worked in noise cancelling. I didn&#x27;t know current tech was at the point where we could do direct reproduction of the outside noise, instead of just using mic arrays! That&#x27;s very cool, it used to be considered totally sci fi to do it fast enough in a small headset.</div><br/></div></div></div></div></div></div><div id="41776886" class="c"><input type="checkbox" id="c-41776886" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41777963">prev</a><span>|</span><a href="#41776962">next</a><span>|</span><label class="collapse" for="c-41776886">[-]</label><label class="expand" for="c-41776886">[3 more]</label></div><br/><div class="children"><div class="content"><i>The visualization reveals that Transformer tends to allocate only a small proportion of attention scores to the correct answer, while disproportionately focusing on irrelevant context.</i><p><i>[...] Specifically, we partition the query and key vectors into two groups and compute two separate softmax attention maps. Then the result of subtracting these two maps is regarded as attention scores.</i><p><i>[...] The approach is analogous to noise-canceling headphones and differential amplifiers in electrical engineering, where the difference between two signals cancels out common-mode noise.</i><p>Simple change, with seemingly decent improvements across the board.</div><br/></div></div><div id="41776962" class="c"><input type="checkbox" id="c-41776962" checked=""/><div class="controls bullet"><span class="by">campers</span><span>|</span><a href="#41776886">prev</a><span>|</span><a href="#41776992">next</a><span>|</span><label class="collapse" for="c-41776962">[-]</label><label class="expand" for="c-41776962">[2 more]</label></div><br/><div class="children"><div class="content">The tl;dr on high level performance improvements<p>&quot;The scaling curves indicate that Diff Transformer requires only about 65% of model size or training tokens needed by Transformer to achieve comparable language modeling performance.&quot;<p>&quot;Diff Transformer retains high performance even at reduced bit-widths, ranging from 16 bits to 6 bits. In comparison, Transformer’s accuracy significantly drops with 6-bit quantization. The 4-bit Diff Transformer achieves comparable accuracy as the 6-bit Transformer, and outperforms the 4-bit Transformer by about 25% in accuracy.&quot;</div><br/></div></div><div id="41776852" class="c"><input type="checkbox" id="c-41776852" checked=""/><div class="controls bullet"><span class="by">ExxKA</span><span>|</span><a href="#41776992">prev</a><span>|</span><label class="collapse" for="c-41776852">[-]</label><label class="expand" for="c-41776852">[3 more]</label></div><br/><div class="children"><div class="content">Very interesting. Currently working on timeseries with Transformers. Let me know if anyone else out there is also reading it from that context.</div><br/><div id="41777069" class="c"><input type="checkbox" id="c-41777069" checked=""/><div class="controls bullet"><span class="by">d3m0t3p</span><span>|</span><a href="#41776852">parent</a><span>|</span><label class="collapse" for="c-41777069">[-]</label><label class="expand" for="c-41777069">[2 more]</label></div><br/><div class="children"><div class="content">Really cool, I&#x27;m a CS majoring in AI, but I&#x27;m also interested in that domain,  would you have any recommendation to get started ?</div><br/><div id="41780270" class="c"><input type="checkbox" id="c-41780270" checked=""/><div class="controls bullet"><span class="by">ExxKA</span><span>|</span><a href="#41776852">root</a><span>|</span><a href="#41777069">parent</a><span>|</span><label class="collapse" for="c-41780270">[-]</label><label class="expand" for="c-41780270">[1 more]</label></div><br/><div class="children"><div class="content">Get a lot of data, and just dig in :) No better way to learn.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>