<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725958871810" as="style"/><link rel="stylesheet" href="styles.css?v=1725958871810"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://harrison.ai/harrison-rad-1/">Radiology-specific foundation model</a> <span class="domain">(<a href="https://harrison.ai">harrison.ai</a>)</span></div><div class="subtext"><span>pyromaker</span> | <span>42 comments</span></div><br/><div><div id="41497998" class="c"><input type="checkbox" id="c-41497998" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#41497550">next</a><span>|</span><label class="collapse" for="c-41497998">[-]</label><label class="expand" for="c-41497998">[3 more]</label></div><br/><div class="children"><div class="content">I think the only real reason the general public can&#x27;t access this now is greed and a lack of understanding of technology. They will say that it is dangerous or something to let the general public access it because they may attempt to self-diagnose or something.<p>But radiologists are very busy and this could help many people. Put a strong disclaimer in there. Open it up to subscriptions to everyone. Charge $40 per analysis or something. Integrate some kind of directory or referral service for human medical professionals.<p>Anyway, I hope some non-profit organizations will see the capabilities of this model and work together to create an open dataset. That might involve recruiting volunteers to sign up before they have injuries. Or maybe just recruiting different medical providers that get waivers and give discounts on the spot. Won&#x27;t be easy. But will be worth it.</div><br/><div id="41498453" class="c"><input type="checkbox" id="c-41498453" checked=""/><div class="controls bullet"><span class="by">arathis</span><span>|</span><a href="#41497998">parent</a><span>|</span><a href="#41498032">next</a><span>|</span><label class="collapse" for="c-41498453">[-]</label><label class="expand" for="c-41498453">[1 more]</label></div><br/><div class="children"><div class="content">You think the only real reason the public don&#x27;t get to use this tool is because of greed?<p>Like, that&#x27;s the only REAL reason? Not the technological or ethical implications? The dangers in providing people with no real concept of how any of this works the means to evaluate themselves?</div><br/></div></div><div id="41498032" class="c"><input type="checkbox" id="c-41498032" checked=""/><div class="controls bullet"><span class="by">ImHereToVote</span><span>|</span><a href="#41497998">parent</a><span>|</span><a href="#41498453">prev</a><span>|</span><a href="#41497550">next</a><span>|</span><label class="collapse" for="c-41498032">[-]</label><label class="expand" for="c-41498032">[1 more]</label></div><br/><div class="children"><div class="content">Doctors should be like thesis advisors for their patients. If the patients undergo a minimum competency test. If you can&#x27;t pass. You don&#x27;t get a thesis advisor.</div><br/></div></div></div></div><div id="41497550" class="c"><input type="checkbox" id="c-41497550" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#41497998">prev</a><span>|</span><a href="#41497304">next</a><span>|</span><label class="collapse" for="c-41497550">[-]</label><label class="expand" for="c-41497550">[5 more]</label></div><br/><div class="children"><div class="content">I had an MRI on my ankle several years ago. At first glance, the doctor told me there was nothing wrong, even though I had very painful symptoms. While the visit was unproductive, I requested the MRI images on a CD, just because I was curious (I wanted to reconstruct the layers into a 3D model). After receiving the data in the mail weeks later, I was surprised to find a formal diagnosis on the CD. Apparently a better doctor had gotten around to analyzing it (they never followed up). If I hadn&#x27;t requested my records, I never would have gotten a diagnosis. I had a swollen retrocalcaneal bursa. I googled the treatments, and eventually got better.<p>I&#x27;m curious whether this AI model would have been able to detect my issue more competently than the shitty doctor.</div><br/><div id="41497780" class="c"><input type="checkbox" id="c-41497780" checked=""/><div class="controls bullet"><span class="by">rasmus1610</span><span>|</span><a href="#41497550">parent</a><span>|</span><a href="#41497570">next</a><span>|</span><label class="collapse" for="c-41497780">[-]</label><label class="expand" for="c-41497780">[2 more]</label></div><br/><div class="children"><div class="content">To be honest, I heard of several radiology practices that hand the patients a normal report directly after the exam and they look at the actual images only after the patient has left.<p>I guess the reasoning is that they want to provide „good service“ by giving the patient something to work with directly after the exam and the workload is so high that they couldn’t look at the images so fast. And they accept the risk that some people are getting angry because their exam wasn’t normal in the end.<p>But on the scale a typical radiology practice operates today, the few patients who don’t have a normal exam don’t matter (the number of normal exams in an outpatient setting is quite high).<p>I find it highly unethical, but some radiologists are a little bit more ethically relaxed I guess.<p>What I want to say is that it might be more of a structural&#x2F;organisational problem than incompetence by the radiologist in your case.<p>(Disclaimer: I’m a radiologist myself)</div><br/><div id="41498445" class="c"><input type="checkbox" id="c-41498445" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#41497550">root</a><span>|</span><a href="#41497780">parent</a><span>|</span><a href="#41497570">next</a><span>|</span><label class="collapse" for="c-41498445">[-]</label><label class="expand" for="c-41498445">[1 more]</label></div><br/><div class="children"><div class="content">This is one of those comments where I started thinking &quot;oh come on no way, this guy clearly has no idea what he&#x27;s talking about&quot; then read the last part and realization dawned the world is actually a very messy place.</div><br/></div></div></div></div><div id="41497570" class="c"><input type="checkbox" id="c-41497570" checked=""/><div class="controls bullet"><span class="by">lostlogin</span><span>|</span><a href="#41497550">parent</a><span>|</span><a href="#41497780">prev</a><span>|</span><a href="#41497304">next</a><span>|</span><label class="collapse" for="c-41497570">[-]</label><label class="expand" for="c-41497570">[2 more]</label></div><br/><div class="children"><div class="content">How did this happen?<p>Surely your results went to a requesting physician who should have been following up with you? Radiology doctors don’t usually organise follow up care.<p>Or was the inaccurate result from the requesting physician?</div><br/><div id="41497640" class="c"><input type="checkbox" id="c-41497640" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#41497550">root</a><span>|</span><a href="#41497570">parent</a><span>|</span><a href="#41497304">next</a><span>|</span><label class="collapse" for="c-41497640">[-]</label><label class="expand" for="c-41497640">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know, just incompetence and disorganization on their part. Directly after my MRI, they told me the images didn&#x27;t indicate any meaningful information.</div><br/></div></div></div></div></div></div><div id="41497304" class="c"><input type="checkbox" id="c-41497304" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#41497550">prev</a><span>|</span><a href="#41498554">next</a><span>|</span><label class="collapse" for="c-41497304">[-]</label><label class="expand" for="c-41497304">[3 more]</label></div><br/><div class="children"><div class="content">This is impressive. The next step is to see how well it generalizes outside of such tests.<p>&quot;The Fellowship of the Royal College of Radiologists (FRCR) 2B Rapids exam is considered one of the leading and toughest certifications for radiologists. Only 40-59% of human radiologists pass on their first attempt. Radiologists who re-attempt the exam within a year of passing score an average of 50.88 out of 60 (84.8%)<i>.<p>Harrison.rad.1 scored 51.4 out of 60 (85.67%). Other competing models, including OpenAI’s GPT-4o, Microsoft’s LLaVA-Med, Anthropic’s Claude 3.5 Sonnet and Google’s Gemini 1.5 Pro, mostly scored below 30*, which is statistically no better than random guessing.&quot;</i></div><br/><div id="41497382" class="c"><input type="checkbox" id="c-41497382" checked=""/><div class="controls bullet"><span class="by">rafram</span><span>|</span><a href="#41497304">parent</a><span>|</span><a href="#41498554">next</a><span>|</span><label class="collapse" for="c-41497382">[-]</label><label class="expand" for="c-41497382">[2 more]</label></div><br/><div class="children"><div class="content">Impressive, but was it trained on questions from the exam? Were any of those other models?</div><br/><div id="41497437" class="c"><input type="checkbox" id="c-41497437" checked=""/><div class="controls bullet"><span class="by">aengustran</span><span>|</span><a href="#41497304">root</a><span>|</span><a href="#41497382">parent</a><span>|</span><a href="#41498554">next</a><span>|</span><label class="collapse" for="c-41497437">[-]</label><label class="expand" for="c-41497437">[1 more]</label></div><br/><div class="children"><div class="content">harrison.rad.1 was not trained on any of the exam questions. It can&#x27;t be guaranteed however that other models were not trained on them though.</div><br/></div></div></div></div></div></div><div id="41498554" class="c"><input type="checkbox" id="c-41498554" checked=""/><div class="controls bullet"><span class="by">ZahiF</span><span>|</span><a href="#41497304">prev</a><span>|</span><a href="#41498214">next</a><span>|</span><label class="collapse" for="c-41498554">[-]</label><label class="expand" for="c-41498554">[1 more]</label></div><br/><div class="children"><div class="content">Super cool, love to see it.<p>I recently joined [Sonio](<a href="https:&#x2F;&#x2F;sonio.ai&#x2F;platform&#x2F;" rel="nofollow">https:&#x2F;&#x2F;sonio.ai&#x2F;platform&#x2F;</a>), where we work on AI-powered prenatal ultrasound reporting and image management. Arguably, prenatal ultrasounds are some of the more challenging to get right, but we&#x27;ve already deployed our solution in clinics across the US and Europe.<p>Exciting times indeed!</div><br/></div></div><div id="41498214" class="c"><input type="checkbox" id="c-41498214" checked=""/><div class="controls bullet"><span class="by">daedalus_f</span><span>|</span><a href="#41498554">prev</a><span>|</span><a href="#41497019">next</a><span>|</span><label class="collapse" for="c-41498214">[-]</label><label class="expand" for="c-41498214">[1 more]</label></div><br/><div class="children"><div class="content">The FRCR 2b examination consists of three parts, a rapid reporting component (the candidate assess around 35 x-rays in 30 minutes where the candidate is simply expected to mark the film as normal or abnormal, this is a perceptual test and is largely limited to simple fracture vs normal) alongside a viva and long cases component where the candidate reviews more complex examinations and is expected to provide a report, differential diagnosis and management plan.<p>A quick look at the paper in the BMJ shows that the model did not sit the FRCR 2b examination as claimed, but was given a cut down mock up of the rapid reporting part of the examination invented by one of the authors.<p><a href="https:&#x2F;&#x2F;www.bmj.com&#x2F;content&#x2F;bmj&#x2F;379&#x2F;bmj-2022-072826.full.pdf" rel="nofollow">https:&#x2F;&#x2F;www.bmj.com&#x2F;content&#x2F;bmj&#x2F;379&#x2F;bmj-2022-072826.full.pdf</a></div><br/></div></div><div id="41497019" class="c"><input type="checkbox" id="c-41497019" checked=""/><div class="controls bullet"><span class="by">smitec</span><span>|</span><a href="#41498214">prev</a><span>|</span><a href="#41497037">next</a><span>|</span><label class="collapse" for="c-41497019">[-]</label><label class="expand" for="c-41497019">[1 more]</label></div><br/><div class="children"><div class="content">A very exciting release and I hope it stacks up in the field. I ran into their team a few times in a previous role and they were always extremely robust in their clinical validation which is often lacking in the space.<p>I still see somewhat of a product gap in this whole area when selling into clinics but that can likely be solved with time.</div><br/></div></div><div id="41498095" class="c"><input type="checkbox" id="c-41498095" checked=""/><div class="controls bullet"><span class="by">joelthelion</span><span>|</span><a href="#41497037">prev</a><span>|</span><a href="#41497806">next</a><span>|</span><label class="collapse" for="c-41498095">[-]</label><label class="expand" for="c-41498095">[1 more]</label></div><br/><div class="children"><div class="content">Too bad it&#x27;s not available llama-style. We&#x27;d see a lot of progress and new applications if something like that was available.</div><br/></div></div><div id="41497806" class="c"><input type="checkbox" id="c-41497806" checked=""/><div class="controls bullet"><span class="by">nightski</span><span>|</span><a href="#41498095">prev</a><span>|</span><a href="#41497516">next</a><span>|</span><label class="collapse" for="c-41497806">[-]</label><label class="expand" for="c-41497806">[1 more]</label></div><br/><div class="children"><div class="content">Is it really a foundation model if it is for a specific purpose?</div><br/></div></div><div id="41497516" class="c"><input type="checkbox" id="c-41497516" checked=""/><div class="controls bullet"><span class="by">seanvelasco</span><span>|</span><a href="#41497806">prev</a><span>|</span><a href="#41497893">next</a><span>|</span><label class="collapse" for="c-41497516">[-]</label><label class="expand" for="c-41497516">[8 more]</label></div><br/><div class="children"><div class="content">following this, gonna integrate this with a DICOM viewer i&#x27;m developing from the ground up</div><br/><div id="41497584" class="c"><input type="checkbox" id="c-41497584" checked=""/><div class="controls bullet"><span class="by">lostlogin</span><span>|</span><a href="#41497516">parent</a><span>|</span><a href="#41497893">next</a><span>|</span><label class="collapse" for="c-41497584">[-]</label><label class="expand" for="c-41497584">[7 more]</label></div><br/><div class="children"><div class="content">Fixing the RIS would make radiology happier than fixing the viewer.<p>And while you’re at it, the current ‘integrations’ between RIS and PACS are so jarring it sets my teeth on edge.</div><br/><div id="41497797" class="c"><input type="checkbox" id="c-41497797" checked=""/><div class="controls bullet"><span class="by">rasmus1610</span><span>|</span><a href="#41497516">root</a><span>|</span><a href="#41497584">parent</a><span>|</span><a href="#41497893">next</a><span>|</span><label class="collapse" for="c-41497797">[-]</label><label class="expand" for="c-41497797">[6 more]</label></div><br/><div class="children"><div class="content">Yes please. We hope to move away from our RIS and integrate our reporting workflow into our PACS this year</div><br/><div id="41497882" class="c"><input type="checkbox" id="c-41497882" checked=""/><div class="controls bullet"><span class="by">rahkiin</span><span>|</span><a href="#41497516">root</a><span>|</span><a href="#41497797">parent</a><span>|</span><a href="#41497893">next</a><span>|</span><label class="collapse" for="c-41497882">[-]</label><label class="expand" for="c-41497882">[5 more]</label></div><br/><div class="children"><div class="content">Can you help me with those acronyms?</div><br/><div id="41497928" class="c"><input type="checkbox" id="c-41497928" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#41497516">root</a><span>|</span><a href="#41497882">parent</a><span>|</span><a href="#41497945">next</a><span>|</span><label class="collapse" for="c-41497928">[-]</label><label class="expand" for="c-41497928">[2 more]</label></div><br/><div class="children"><div class="content">&#x27;Radiology Information System&#x27; and &#x27;Picture Archive and Communication System&#x27;, I think<p><a href="https:&#x2F;&#x2F;www.adsc.com&#x2F;blog&#x2F;what-are-the-differences-between-pacs-ris-cis-and-dicom" rel="nofollow">https:&#x2F;&#x2F;www.adsc.com&#x2F;blog&#x2F;what-are-the-differences-between-p...</a></div><br/><div id="41497966" class="c"><input type="checkbox" id="c-41497966" checked=""/><div class="controls bullet"><span class="by">tecleandor</span><span>|</span><a href="#41497516">root</a><span>|</span><a href="#41497928">parent</a><span>|</span><a href="#41497945">next</a><span>|</span><label class="collapse" for="c-41497966">[-]</label><label class="expand" for="c-41497966">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s correct!</div><br/></div></div></div></div><div id="41497945" class="c"><input type="checkbox" id="c-41497945" checked=""/><div class="controls bullet"><span class="by">tecleandor</span><span>|</span><a href="#41497516">root</a><span>|</span><a href="#41497882">parent</a><span>|</span><a href="#41497928">prev</a><span>|</span><a href="#41497893">next</a><span>|</span><label class="collapse" for="c-41497945">[-]</label><label class="expand" for="c-41497945">[2 more]</label></div><br/><div class="children"><div class="content">Radiology Information System. The software that manages the radiology workflow in a clinic.<p>Schedules radiology studies, exchanges information with the modalities (the radiology devices) so the studies have proper metadata, exchanges information with the PACS (the radiology image storage), it might be used by radiologists and&#x2F;or transcriptionists to add the reports for the studies...<p>It might overlap a bit with the HIS (hospital information system) that&#x27;s the more general hospital management software.</div><br/><div id="41498302" class="c"><input type="checkbox" id="c-41498302" checked=""/><div class="controls bullet"><span class="by">lostlogin</span><span>|</span><a href="#41497516">root</a><span>|</span><a href="#41497945">parent</a><span>|</span><a href="#41497893">next</a><span>|</span><label class="collapse" for="c-41498302">[-]</label><label class="expand" for="c-41498302">[1 more]</label></div><br/><div class="children"><div class="content">The RIS became a thing before the PACS as digital imaging arrived later in the piece. They should always have been integrated, as many things are clumsy when spread across two systems. Some ‘reports’ are an image (eg vessel mapping and many cardiac reports). Some imaging doesn’t require a written report (theatre screening for implant placement). Some imaging requires multiple reports (a cardiac CT scan which covers lungs often gets a radiology and cardiology report).
Some imaging is done to aid the acquisition of a different type of imaging. All these scenarios are handled in various clumsy ways by the various system and work around workflows are made up by staff on a near daily basis.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41497893" class="c"><input type="checkbox" id="c-41497893" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41497516">prev</a><span>|</span><a href="#41497369">next</a><span>|</span><label class="collapse" for="c-41497893">[-]</label><label class="expand" for="c-41497893">[2 more]</label></div><br/><div class="children"><div class="content">“AI has peaked”<p>“AI is a bubble”<p>We’re still scratching the surface of what’s possible. I’m hugely optimistic about the future, in a way I never was in other hype&#x2F;tech cycles.</div><br/><div id="41498186" class="c"><input type="checkbox" id="c-41498186" checked=""/><div class="controls bullet"><span class="by">Almondsetat</span><span>|</span><a href="#41497893">parent</a><span>|</span><a href="#41497369">next</a><span>|</span><label class="collapse" for="c-41498186">[-]</label><label class="expand" for="c-41498186">[1 more]</label></div><br/><div class="children"><div class="content">&quot;AI&quot; here refers to general intelligence. A highly specific ML model for radiology is not AI, but a new avenue for improvements in the field of computer vision.</div><br/></div></div></div></div><div id="41497369" class="c"><input type="checkbox" id="c-41497369" checked=""/><div class="controls bullet"><span class="by">isaacfrond</span><span>|</span><a href="#41497893">prev</a><span>|</span><a href="#41497169">next</a><span>|</span><label class="collapse" for="c-41497369">[-]</label><label class="expand" for="c-41497369">[3 more]</label></div><br/><div class="children"><div class="content">From the article:
<i>Other competing models, including OpenAI’s GPT-4o, Microsoft’s LLaVA-Med, Anthropic’s Claude 3.5 Sonnet and Google’s Gemini 1.5 Pro, mostly scored below 30*, which is statistically no better than random guessing.</i><p>How is chatgpt the competion? It’s mostly a text model?</div><br/><div id="41497432" class="c"><input type="checkbox" id="c-41497432" checked=""/><div class="controls bullet"><span class="by">aubanel</span><span>|</span><a href="#41497369">parent</a><span>|</span><a href="#41497429">next</a><span>|</span><label class="collapse" for="c-41497432">[-]</label><label class="expand" for="c-41497432">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4o also has vision capabilities: <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;vision" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;vision</a></div><br/></div></div><div id="41497429" class="c"><input type="checkbox" id="c-41497429" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41497369">parent</a><span>|</span><a href="#41497432">prev</a><span>|</span><a href="#41497169">next</a><span>|</span><label class="collapse" for="c-41497429">[-]</label><label class="expand" for="c-41497429">[1 more]</label></div><br/><div class="children"><div class="content">gpt4o is multimodal</div><br/></div></div></div></div><div id="41497169" class="c"><input type="checkbox" id="c-41497169" checked=""/><div class="controls bullet"><span class="by">Improvement</span><span>|</span><a href="#41497369">prev</a><span>|</span><a href="#41497141">next</a><span>|</span><label class="collapse" for="c-41497169">[-]</label><label class="expand" for="c-41497169">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t find any git link, hopefully I will look into it later.<p>From their benchmarks it&#x27;s looking like a great model that beat competition, but I will see the third party tests after they get released to determine the real performance.</div><br/></div></div><div id="41497141" class="c"><input type="checkbox" id="c-41497141" checked=""/><div class="controls bullet"><span class="by">infocollector</span><span>|</span><a href="#41497169">prev</a><span>|</span><a href="#41497121">next</a><span>|</span><label class="collapse" for="c-41497141">[-]</label><label class="expand" for="c-41497141">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see a release? Perhaps its an internal distribution to subscribers&#x2F;people? Does anyone see a download&#x2F;github page for the model?</div><br/><div id="41497541" class="c"><input type="checkbox" id="c-41497541" checked=""/><div class="controls bullet"><span class="by">stevenbuscemi</span><span>|</span><a href="#41497141">parent</a><span>|</span><a href="#41497209">next</a><span>|</span><label class="collapse" for="c-41497541">[-]</label><label class="expand" for="c-41497541">[1 more]</label></div><br/><div class="children"><div class="content">Harrison.ai typically productionize and commercialize their models through child companies (Annalise.ai for radiology, Franklin.ai for pathology).<p>I&#x27;d imagine access to the model itself will remain pretty exclusive, but would love to see them adopt a more open approach.</div><br/></div></div><div id="41497209" class="c"><input type="checkbox" id="c-41497209" checked=""/><div class="controls bullet"><span class="by">blazerunner</span><span>|</span><a href="#41497141">parent</a><span>|</span><a href="#41497541">prev</a><span>|</span><a href="#41497121">next</a><span>|</span><label class="collapse" for="c-41497209">[-]</label><label class="expand" for="c-41497209">[1 more]</label></div><br/><div class="children"><div class="content">I can see a link to join a waitlist for the model, as well there is this:<p>&gt; Filtered for plain radiographs, Harrison.rad.1 achieves 82% accuracy on closed questions, outperforming other generalist and specialist LLM models available to date (Table 1).<p>The code and methodology used to reach this conclusion will be made available at <a href="https:&#x2F;&#x2F;harrison-ai.github.io&#x2F;radbench&#x2F;" rel="nofollow">https:&#x2F;&#x2F;harrison-ai.github.io&#x2F;radbench&#x2F;</a>.</div><br/></div></div></div></div><div id="41497121" class="c"><input type="checkbox" id="c-41497121" checked=""/><div class="controls bullet"><span class="by">newyankee</span><span>|</span><a href="#41497141">prev</a><span>|</span><a href="#41497534">next</a><span>|</span><label class="collapse" for="c-41497121">[-]</label><label class="expand" for="c-41497121">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if there is any open source radiology model that can be used to test and assist real world radiologists</div><br/><div id="41497128" class="c"><input type="checkbox" id="c-41497128" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#41497121">parent</a><span>|</span><a href="#41497291">next</a><span>|</span><label class="collapse" for="c-41497128">[-]</label><label class="expand" for="c-41497128">[1 more]</label></div><br/><div class="children"><div class="content">I recall there being a couple non-commercial ones on physionet trained on the MIMIC CXR dataset. I could be wrong, I&#x27;ll hopefully remember to check.</div><br/></div></div><div id="41497291" class="c"><input type="checkbox" id="c-41497291" checked=""/><div class="controls bullet"><span class="by">amitport</span><span>|</span><a href="#41497121">parent</a><span>|</span><a href="#41497128">prev</a><span>|</span><a href="#41497534">next</a><span>|</span><label class="collapse" for="c-41497291">[-]</label><label class="expand" for="c-41497291">[2 more]</label></div><br/><div class="children"><div class="content">there are a few for specific tasks (e.g., lung cancer), no &quot;foundation&quot; models afaikt.</div><br/><div id="41497588" class="c"><input type="checkbox" id="c-41497588" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#41497121">root</a><span>|</span><a href="#41497291">parent</a><span>|</span><a href="#41497534">next</a><span>|</span><label class="collapse" for="c-41497588">[-]</label><label class="expand" for="c-41497588">[1 more]</label></div><br/><div class="children"><div class="content">There really should be at this point. Annotated radiology datasets, patients numbering into the millions, are the easiest healthcare datasets to obtain. I suspect there are many startups, and know of several long since failed, who trained on these. I&#x27;ve met radiologists who assert most of their job comes down to contextualizing their findings to their colleagues, as well as within the scope of the case itself. That&#x27;s relevant here - it doesn&#x27;t matter how accurate or precise your model is, if it can&#x27;t do that. Radiologists already use &quot;AI&quot; tools that are very good, and radiology is a very welcoming field for new technology. I think the promise of foundation models at the moment would be to ease burden and help prevent burnout. Unfortunately, those models aren&#x27;t &quot;sexy&quot; - they reduce administrative burden, assemble contextual evidence for better retrieval (have interfaces that don&#x27;t suck when integrated with the EMR).</div><br/></div></div></div></div></div></div><div id="41497534" class="c"><input type="checkbox" id="c-41497534" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#41497121">prev</a><span>|</span><a href="#41498389">next</a><span>|</span><label class="collapse" for="c-41497534">[-]</label><label class="expand" for="c-41497534">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad to see that this model uses multiple patient chart data elements beyond just images. Some earlier more naive models attempted to treat it as a pure image classification problem which isn&#x27;t sufficient outside the simplest cases. Human radiologists rely heavily on other factors including patient age, sex, previous diagnoses, patient reported symptoms, etc.</div><br/><div id="41497603" class="c"><input type="checkbox" id="c-41497603" checked=""/><div class="controls bullet"><span class="by">lostlogin</span><span>|</span><a href="#41497534">parent</a><span>|</span><a href="#41498389">next</a><span>|</span><label class="collapse" for="c-41497603">[-]</label><label class="expand" for="c-41497603">[1 more]</label></div><br/><div class="children"><div class="content">&gt; patient reported symptoms<p>You make it sound like the reporting radiologist is given a referral with helpful, legible information on it. That this ever happened doubtful.</div><br/></div></div></div></div></div></div></div></div></div></body></html>