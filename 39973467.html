<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712653274363" as="style"/><link rel="stylesheet" href="styles.css?v=1712653274363"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/karpathy/llm.c">Llm.c – LLM training in simple, pure C/CUDA</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>tosh</span> | <span>144 comments</span></div><br/><div><div id="39977525" class="c"><input type="checkbox" id="c-39977525" checked=""/><div class="controls bullet"><span class="by">zzbn00</span><span>|</span><a href="#39973964">next</a><span>|</span><label class="collapse" for="c-39977525">[-]</label><label class="expand" for="c-39977525">[1 more]</label></div><br/><div class="children"><div class="content">Very nice.<p>In my experience much of the complexity of numerical software is to enable the search for the algorithm that works well with the problem&#x2F;data you have. Once you know the exact algorithm you want, it is possible to make a nice clean minimalistic implementation, but that does not mean such an implementation would  have been easy at the beginning.</div><br/></div></div><div id="39973964" class="c"><input type="checkbox" id="c-39973964" checked=""/><div class="controls bullet"><span class="by">patrick-fitz</span><span>|</span><a href="#39977525">prev</a><span>|</span><a href="#39977466">next</a><span>|</span><label class="collapse" for="c-39973964">[-]</label><label class="expand" for="c-39973964">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;twitter.com&#x2F;karpathy&#x2F;status&#x2F;1777427944971083809" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;karpathy&#x2F;status&#x2F;1777427944971083809</a><p>&gt; And once this is a in a bit more stable state: videos on building this in more detail and from scratch.<p>Looking forward to watching the videos.</div><br/><div id="39973994" class="c"><input type="checkbox" id="c-39973994" checked=""/><div class="controls bullet"><span class="by">0cf8612b2e1e</span><span>|</span><a href="#39973964">parent</a><span>|</span><a href="#39977466">next</a><span>|</span><label class="collapse" for="c-39973994">[-]</label><label class="expand" for="c-39973994">[2 more]</label></div><br/><div class="children"><div class="content">I love his videos. They are dense, but I get a lot out of them.</div><br/><div id="39974044" class="c"><input type="checkbox" id="c-39974044" checked=""/><div class="controls bullet"><span class="by">sghiassy</span><span>|</span><a href="#39973964">root</a><span>|</span><a href="#39973994">parent</a><span>|</span><a href="#39977466">next</a><span>|</span><label class="collapse" for="c-39974044">[-]</label><label class="expand" for="c-39974044">[1 more]</label></div><br/><div class="children"><div class="content">+100 thank you karpathy!</div><br/></div></div></div></div></div></div><div id="39977466" class="c"><input type="checkbox" id="c-39977466" checked=""/><div class="controls bullet"><span class="by">milansuk</span><span>|</span><a href="#39973964">prev</a><span>|</span><a href="#39975812">next</a><span>|</span><label class="collapse" for="c-39977466">[-]</label><label class="expand" for="c-39977466">[1 more]</label></div><br/><div class="children"><div class="content">This is an implementation of a transformer and in README it&#x27;s presented as text-&gt;text. Tokens are just integers going in and out.<p>Is it possible to use it to train other types of LLMs(text-&gt;image, image-&gt;text, speech-&gt;text, etc.)?</div><br/></div></div><div id="39975812" class="c"><input type="checkbox" id="c-39975812" checked=""/><div class="controls bullet"><span class="by">convexstrictly</span><span>|</span><a href="#39977466">prev</a><span>|</span><a href="#39973830">next</a><span>|</span><label class="collapse" for="c-39975812">[-]</label><label class="expand" for="c-39975812">[6 more]</label></div><br/><div class="children"><div class="content">Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use<p><a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;candle">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;candle</a></div><br/><div id="39976576" class="c"><input type="checkbox" id="c-39976576" checked=""/><div class="controls bullet"><span class="by">jeroenvlek</span><span>|</span><a href="#39975812">parent</a><span>|</span><a href="#39976333">next</a><span>|</span><label class="collapse" for="c-39976576">[-]</label><label class="expand" for="c-39976576">[1 more]</label></div><br/><div class="children"><div class="content">Love Candle! I actually ported Karpathy&#x27;s previous GPT tutorial to candle, including training [0]<p>[0] <a href="https:&#x2F;&#x2F;www.perceptivebits.com&#x2F;building-gpt-from-scratch-in-rust-and-candle&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.perceptivebits.com&#x2F;building-gpt-from-scratch-in-...</a></div><br/></div></div><div id="39976333" class="c"><input type="checkbox" id="c-39976333" checked=""/><div class="controls bullet"><span class="by">basbuller</span><span>|</span><a href="#39975812">parent</a><span>|</span><a href="#39976576">prev</a><span>|</span><a href="#39977036">next</a><span>|</span><label class="collapse" for="c-39976333">[-]</label><label class="expand" for="c-39976333">[1 more]</label></div><br/><div class="children"><div class="content">Not barely as minimal as Karpathy his implementation</div><br/></div></div><div id="39977036" class="c"><input type="checkbox" id="c-39977036" checked=""/><div class="controls bullet"><span class="by">0xfedbee</span><span>|</span><a href="#39975812">parent</a><span>|</span><a href="#39976333">prev</a><span>|</span><a href="#39976215">next</a><span>|</span><label class="collapse" for="c-39977036">[-]</label><label class="expand" for="c-39977036">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t call in &quot;minimalist&quot; after seeing Karpathy&#x27;s code.</div><br/></div></div><div id="39976215" class="c"><input type="checkbox" id="c-39976215" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#39975812">parent</a><span>|</span><a href="#39977036">prev</a><span>|</span><a href="#39973830">next</a><span>|</span><label class="collapse" for="c-39976215">[-]</label><label class="expand" for="c-39976215">[2 more]</label></div><br/><div class="children"><div class="content">Candle focuses on inference though.</div><br/><div id="39976485" class="c"><input type="checkbox" id="c-39976485" checked=""/><div class="controls bullet"><span class="by">l-m-z</span><span>|</span><a href="#39975812">root</a><span>|</span><a href="#39976215">parent</a><span>|</span><a href="#39973830">next</a><span>|</span><label class="collapse" for="c-39976485">[-]</label><label class="expand" for="c-39976485">[1 more]</label></div><br/><div class="children"><div class="content">Candle dev here, we also support training&#x2F;backdrop! We certainly focus on optimizing inference performance but hopefully that should improve the training efficiency too.</div><br/></div></div></div></div></div></div><div id="39973830" class="c"><input type="checkbox" id="c-39973830" checked=""/><div class="controls bullet"><span class="by">yinser</span><span>|</span><a href="#39975812">prev</a><span>|</span><a href="#39973945">next</a><span>|</span><label class="collapse" for="c-39973830">[-]</label><label class="expand" for="c-39973830">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen his nano GPT implemented using JAX, now we have C&#x2F;CUDA. I&#x27;d love to see if nano GPT could be doable in Mojo. I took a stab at a Mojo conversion of his Wavenet project (Andrej&#x27;s zero to hero course) and I gotta say... python has so many nice features lol. Stating the obvious I know but what you see done in 6 lines of python takes so much more work in other languages.</div><br/><div id="39974491" class="c"><input type="checkbox" id="c-39974491" checked=""/><div class="controls bullet"><span class="by">cb321</span><span>|</span><a href="#39973830">parent</a><span>|</span><a href="#39976032">next</a><span>|</span><label class="collapse" for="c-39974491">[-]</label><label class="expand" for="c-39974491">[2 more]</label></div><br/><div class="children"><div class="content">For a prior generation of karpathy-splaining this is this Nim port: <a href="https:&#x2F;&#x2F;github.com&#x2F;Vindaar&#x2F;llama2nim">https:&#x2F;&#x2F;github.com&#x2F;Vindaar&#x2F;llama2nim</a> - maybe of interest if you are interested in Mojo.</div><br/><div id="39975308" class="c"><input type="checkbox" id="c-39975308" checked=""/><div class="controls bullet"><span class="by">yinser</span><span>|</span><a href="#39973830">root</a><span>|</span><a href="#39974491">parent</a><span>|</span><a href="#39976032">next</a><span>|</span><label class="collapse" for="c-39975308">[-]</label><label class="expand" for="c-39975308">[1 more]</label></div><br/><div class="children"><div class="content">Thank you!</div><br/></div></div></div></div><div id="39976032" class="c"><input type="checkbox" id="c-39976032" checked=""/><div class="controls bullet"><span class="by">auraham</span><span>|</span><a href="#39973830">parent</a><span>|</span><a href="#39974491">prev</a><span>|</span><a href="#39975722">next</a><span>|</span><label class="collapse" for="c-39976032">[-]</label><label class="expand" for="c-39976032">[1 more]</label></div><br/><div class="children"><div class="content">Where is the GPT implementation in JAX? I only found this [1] in PyTorch and NumPy.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;nanoGPT">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;nanoGPT</a></div><br/></div></div><div id="39975722" class="c"><input type="checkbox" id="c-39975722" checked=""/><div class="controls bullet"><span class="by">pavelstoev</span><span>|</span><a href="#39973830">parent</a><span>|</span><a href="#39976032">prev</a><span>|</span><a href="#39973945">next</a><span>|</span><label class="collapse" for="c-39975722">[-]</label><label class="expand" for="c-39975722">[3 more]</label></div><br/><div class="children"><div class="content">How in Mojo do you support GPU data parallelism and all the benefits it brings ?</div><br/><div id="39976448" class="c"><input type="checkbox" id="c-39976448" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39973830">root</a><span>|</span><a href="#39975722">parent</a><span>|</span><a href="#39973945">next</a><span>|</span><label class="collapse" for="c-39976448">[-]</label><label class="expand" for="c-39976448">[2 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t. Mojo doesn&#x27;t support GPUs at the moment, which says a lot about a language which claims to be AI first.</div><br/><div id="39976506" class="c"><input type="checkbox" id="c-39976506" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#39973830">root</a><span>|</span><a href="#39976448">parent</a><span>|</span><a href="#39973945">next</a><span>|</span><label class="collapse" for="c-39976506">[-]</label><label class="expand" for="c-39976506">[1 more]</label></div><br/><div class="children"><div class="content">They only made Mojo available outside the preview circle about a couple of months ago, and it is yet to run on Windows laptops of researchers.<p>I love the attitude of considering 0.x languages production ready for all imaginable kinds of workloads.</div><br/></div></div></div></div></div></div></div></div><div id="39973945" class="c"><input type="checkbox" id="c-39973945" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#39973830">prev</a><span>|</span><a href="#39975713">next</a><span>|</span><label class="collapse" for="c-39973945">[-]</label><label class="expand" for="c-39973945">[17 more]</label></div><br/><div class="children"><div class="content">&gt; direct CUDA implementation, which will be significantly faster and probably come close to PyTorch.<p>It almost hurts, to read that PyTorch is faster.<p>But then again, with these GPU-RAM-prices, let&#x27;s see how it speeds up the CPU.<p>We really need SO-DIMM slots on the RTX series (or AMD&#x2F;Intel equivalent) so that we can expand the RAM as we need it to. Is there a technical problem to it?</div><br/><div id="39974028" class="c"><input type="checkbox" id="c-39974028" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#39973945">parent</a><span>|</span><a href="#39974148">next</a><span>|</span><label class="collapse" for="c-39974028">[-]</label><label class="expand" for="c-39974028">[9 more]</label></div><br/><div class="children"><div class="content">Memory speed is more or less directly proportional to how close the memory is to the processor, with the fastest memory being literally inside the processor (SRAM cache), followed by memory on the same package as the processor (HBM GPUs, Apple M-series), followed by soldered down discrete memory chips (regular GPUs, games consoles), followed by socketed DIMMs in distant last place. There&#x27;s not really any getting around it, the bandwidth that GPUs crave just isn&#x27;t compatible with modularity.<p>Even CPUs are starting to move their memory closer to the core in the name of performance, as mentioned Apple is already doing it, Intel is making Xeons with on-chip memory now, and they have a version aimed at consumers on their roadmap.</div><br/><div id="39974420" class="c"><input type="checkbox" id="c-39974420" checked=""/><div class="controls bullet"><span class="by">tverbeure</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39974028">parent</a><span>|</span><a href="#39974324">next</a><span>|</span><label class="collapse" for="c-39974420">[-]</label><label class="expand" for="c-39974420">[2 more]</label></div><br/><div class="children"><div class="content">For data rates, as in bandwidth per IO pin, distance is really only a secondary factor. HBM memory, for example, runs at substantially lower data rates than GDDR, yet it sits right next to the GPU die compared to centimeters for the GDDR. And high-speed serial links run at speeds that are an order of magnitude higher than even the internal register files of a CPU.</div><br/></div></div><div id="39974324" class="c"><input type="checkbox" id="c-39974324" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39974028">parent</a><span>|</span><a href="#39974420">prev</a><span>|</span><a href="#39975049">next</a><span>|</span><label class="collapse" for="c-39974324">[-]</label><label class="expand" for="c-39974324">[3 more]</label></div><br/><div class="children"><div class="content">FYI, most discrete GPUs with discrete memory packages soldered to the board near the GPU are running at substantially higher memory frequencies than the on-package DRAM in Apple&#x27;s chips. But running GDDR at those speeds costs a lot of power.</div><br/><div id="39975392" class="c"><input type="checkbox" id="c-39975392" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39974324">parent</a><span>|</span><a href="#39975049">next</a><span>|</span><label class="collapse" for="c-39975392">[-]</label><label class="expand" for="c-39975392">[2 more]</label></div><br/><div class="children"><div class="content">I watched a presentation on this today. The presenter focused on the soldering and proximity as well. Is this really the only difference or is this transistor based memory (like L1, L2, etc.)? I get the proximity factor of course (1ft &#x2F; ns EE rule of thumb). In any case, soldering and proximity don&#x27;t seem like breakthrough innovations (but maybe I am wrong).</div><br/><div id="39975582" class="c"><input type="checkbox" id="c-39975582" checked=""/><div class="controls bullet"><span class="by">MobiusHorizons</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39975392">parent</a><span>|</span><a href="#39975049">next</a><span>|</span><label class="collapse" for="c-39975582">[-]</label><label class="expand" for="c-39975582">[1 more]</label></div><br/><div class="children"><div class="content">Gpu ram is typically gddr6 or gddr6x which is a different standard to the chips used in ddr5 for example. GPUs have terrible latency to ram, but enormous throughput, and I assume the chips are internally optimized for that. Many aspects of a design change when you choose different latency or clockspeed targets translating into different power &#x2F; area calculations.</div><br/></div></div></div></div></div></div><div id="39975049" class="c"><input type="checkbox" id="c-39975049" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39974028">parent</a><span>|</span><a href="#39974324">prev</a><span>|</span><a href="#39974148">next</a><span>|</span><label class="collapse" for="c-39975049">[-]</label><label class="expand" for="c-39975049">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true it&#x27;s got an impact, but I think there&#x27;s still space available for &quot;slightly slower with 2x memory&quot; models. For many local uses, new cards are way past the &quot;fast enough&quot; line, but having 64gb on them would be really beneficial.<p>It&#x27;s love to see some experiments &#x2F; different SKUs in this area, given people are already diy-ing extra memory on NVIDIA. (<a href="https:&#x2F;&#x2F;hackaday.com&#x2F;2021&#x2F;01&#x2F;29&#x2F;add-an-extra-8gb-of-vram-to-your-2070&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hackaday.com&#x2F;2021&#x2F;01&#x2F;29&#x2F;add-an-extra-8gb-of-vram-to-...</a> there were stable experiments later on, but I don&#x27;t have a link now)</div><br/><div id="39975179" class="c"><input type="checkbox" id="c-39975179" checked=""/><div class="controls bullet"><span class="by">schneehertz</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39975049">parent</a><span>|</span><a href="#39974148">next</a><span>|</span><label class="collapse" for="c-39975179">[-]</label><label class="expand" for="c-39975179">[2 more]</label></div><br/><div class="children"><div class="content">Graphics card manufacturers believe that selling high-memory consumer graphics cards will affect the market for commercial computing cards, so they will not do so, that&#x27;s all.</div><br/><div id="39976569" class="c"><input type="checkbox" id="c-39976569" checked=""/><div class="controls bullet"><span class="by">airspresso</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39975179">parent</a><span>|</span><a href="#39974148">next</a><span>|</span><label class="collapse" for="c-39976569">[-]</label><label class="expand" for="c-39976569">[1 more]</label></div><br/><div class="children"><div class="content">Nice room for a new player to disrupt then</div><br/></div></div></div></div></div></div></div></div><div id="39974148" class="c"><input type="checkbox" id="c-39974148" checked=""/><div class="controls bullet"><span class="by">tverbeure</span><span>|</span><a href="#39973945">parent</a><span>|</span><a href="#39974028">prev</a><span>|</span><a href="#39973999">next</a><span>|</span><label class="collapse" for="c-39974148">[-]</label><label class="expand" for="c-39974148">[1 more]</label></div><br/><div class="children"><div class="content">Check out PCB back drilling. It&#x27;s a process where you remove a few hundred microns from the vias that are used to connect GDDR RAMs to the GPUs, to avoid reflections due to the impedance mismatch that&#x27;s caused by the stub.<p>When you have a pulse coded signal traveling at close to 10GHz, everything becomes an antenna. The technical problem is that you can&#x27;t do this with a flimsy connector like the ones used for DIMMs. The reason GDDR can have a bandwidth per pin that is 4 times higher than regular DDR is because they are soldered down on the PCB.</div><br/></div></div><div id="39973999" class="c"><input type="checkbox" id="c-39973999" checked=""/><div class="controls bullet"><span class="by">LatticeAnimal</span><span>|</span><a href="#39973945">parent</a><span>|</span><a href="#39974148">prev</a><span>|</span><a href="#39975846">next</a><span>|</span><label class="collapse" for="c-39973999">[-]</label><label class="expand" for="c-39973999">[3 more]</label></div><br/><div class="children"><div class="content">&gt; We really need SO-DIMM slots on the RTX series (or AMD&#x2F;Intel equivalent) so that we can expand the RAM as we need it to. Is there a technical problem to it?<p>I imagine it would incur a non trivial latency and cost penalty. The memory modules are placed pretty close to the compute die right now. Cooling would also have to change (the memory modules produce a lot of heat).<p>But there is also no reason for any of the GPU manufacturers to do this. A skew with twice as much memory can go for a lot more than the difference in memory cost alone</div><br/><div id="39977461" class="c"><input type="checkbox" id="c-39977461" checked=""/><div class="controls bullet"><span class="by">ItsBob</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39973999">parent</a><span>|</span><a href="#39974346">next</a><span>|</span><label class="collapse" for="c-39977461">[-]</label><label class="expand" for="c-39977461">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree but (I know nothing about this btw...) would it not benefit in terms of, say, a L3 cache kind of thing?<p>Imagine you could stick 2 x 64GB DDR5 DIMMS on the GPU in sockets, would that not be faster to access than the motherboard DIMMS? It won&#x27;t be as fast as on-die memory of course but could it not act like a sort of halfway house?</div><br/></div></div><div id="39974346" class="c"><input type="checkbox" id="c-39974346" checked=""/><div class="controls bullet"><span class="by">SunlitCat</span><span>|</span><a href="#39973945">root</a><span>|</span><a href="#39973999">parent</a><span>|</span><a href="#39977461">prev</a><span>|</span><a href="#39975846">next</a><span>|</span><label class="collapse" for="c-39974346">[-]</label><label class="expand" for="c-39974346">[1 more]</label></div><br/><div class="children"><div class="content">And especially doing &quot;interesting&quot; combinations of gpu and memory.<p>Like lower end gpu with 16 GB of VRAM, but offering just 8 &#x2F; 12 GB of VRAM in the middle class and then again 16 GB in the upper class of gpu selection.</div><br/></div></div></div></div><div id="39975846" class="c"><input type="checkbox" id="c-39975846" checked=""/><div class="controls bullet"><span class="by">hahnchen</span><span>|</span><a href="#39973945">parent</a><span>|</span><a href="#39973999">prev</a><span>|</span><a href="#39976419">next</a><span>|</span><label class="collapse" for="c-39975846">[-]</label><label class="expand" for="c-39975846">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It almost hurts, to read that PyTorch is faster.<p>Why?</div><br/></div></div><div id="39976419" class="c"><input type="checkbox" id="c-39976419" checked=""/><div class="controls bullet"><span class="by">theGeatZhopa</span><span>|</span><a href="#39973945">parent</a><span>|</span><a href="#39975846">prev</a><span>|</span><a href="#39975507">next</a><span>|</span><label class="collapse" for="c-39976419">[-]</label><label class="expand" for="c-39976419">[1 more]</label></div><br/><div class="children"><div class="content">NVIDIA hates that trick.</div><br/></div></div></div></div><div id="39975713" class="c"><input type="checkbox" id="c-39975713" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#39973945">prev</a><span>|</span><a href="#39977207">next</a><span>|</span><label class="collapse" for="c-39975713">[-]</label><label class="expand" for="c-39975713">[7 more]</label></div><br/><div class="children"><div class="content">Kind of amazing that something that can be expressed in ~1000 lines of code has completely turned the world on its head.</div><br/><div id="39976454" class="c"><input type="checkbox" id="c-39976454" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39975713">parent</a><span>|</span><a href="#39976983">next</a><span>|</span><label class="collapse" for="c-39976454">[-]</label><label class="expand" for="c-39976454">[3 more]</label></div><br/><div class="children"><div class="content">Which important concept or algorithm can&#x27;t be expressed in ≤1000 lines? Seems like a pretty common theme among groundbreaking ideas.</div><br/><div id="39976822" class="c"><input type="checkbox" id="c-39976822" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#39975713">root</a><span>|</span><a href="#39976454">parent</a><span>|</span><a href="#39976983">next</a><span>|</span><label class="collapse" for="c-39976822">[-]</label><label class="expand" for="c-39976822">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a good question. Unfortunately I think you&#x27;re asking to compute the Kolmogorov complexity of every interesting concept we have that doesn&#x27;t yet have an implementation less than n=1000 lines, which is equivalent to the halting problem (modulo unbounded memory).<p>If you could exhaustively list all the interesting algorithms (hard but feasible) you could potentially prove a lower bound for each one&#x27;s complexity by writing a shorter than n implementation (hard, probably infeasiblel and show positively that GP&#x27;s prop isn&#x27;t true. On the other hand showing that it was true would require either some very clever proof which can&#x27;t apply to all programs, but somehow only these interesting ones (very likely impossible) or enumerate all C^n programs where C is the number of possible lines (something like 64^80) and show that none of them implements at least one of the interesting algorithms (absurdly impossible).</div><br/></div></div></div></div><div id="39976983" class="c"><input type="checkbox" id="c-39976983" checked=""/><div class="controls bullet"><span class="by">datascienced</span><span>|</span><a href="#39975713">parent</a><span>|</span><a href="#39976454">prev</a><span>|</span><a href="#39976354">next</a><span>|</span><label class="collapse" for="c-39976983">[-]</label><label class="expand" for="c-39976983">[2 more]</label></div><br/><div class="children"><div class="content">Err… and the exobytes of training data</div><br/><div id="39977159" class="c"><input type="checkbox" id="c-39977159" checked=""/><div class="controls bullet"><span class="by">rnewme</span><span>|</span><a href="#39975713">root</a><span>|</span><a href="#39976983">parent</a><span>|</span><a href="#39976354">next</a><span>|</span><label class="collapse" for="c-39977159">[-]</label><label class="expand" for="c-39977159">[1 more]</label></div><br/><div class="children"><div class="content">Ah, not really</div><br/></div></div></div></div><div id="39976354" class="c"><input type="checkbox" id="c-39976354" checked=""/><div class="controls bullet"><span class="by">daniel_reetz</span><span>|</span><a href="#39975713">parent</a><span>|</span><a href="#39976983">prev</a><span>|</span><a href="#39977207">next</a><span>|</span><label class="collapse" for="c-39976354">[-]</label><label class="expand" for="c-39976354">[1 more]</label></div><br/><div class="children"><div class="content">Echoes of DeCSS ;)</div><br/></div></div></div></div><div id="39977207" class="c"><input type="checkbox" id="c-39977207" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#39975713">prev</a><span>|</span><a href="#39974230">next</a><span>|</span><label class="collapse" for="c-39977207">[-]</label><label class="expand" for="c-39977207">[1 more]</label></div><br/><div class="children"><div class="content">Is this able to replace PyTorch, ... in normal practice? No.<p>Does this show that in general the most used ML frameworks are a mess? Yes.</div><br/></div></div><div id="39974230" class="c"><input type="checkbox" id="c-39974230" checked=""/><div class="controls bullet"><span class="by">flockonus</span><span>|</span><a href="#39977207">prev</a><span>|</span><a href="#39975736">next</a><span>|</span><label class="collapse" for="c-39974230">[-]</label><label class="expand" for="c-39974230">[2 more]</label></div><br/><div class="children"><div class="content">Question, apologize if slightly off-topic, it&#x27;s something I&#x27;d like to use this project for: Is there an example of how to train GPT-2 on time series, in particular with covariates?<p>As my understanding of LLM goes at a basic level it&#x27;s predicting the next token from previous tokens, which sounds directionally similar to time series (perhaps letting aside periodicity).</div><br/><div id="39974425" class="c"><input type="checkbox" id="c-39974425" checked=""/><div class="controls bullet"><span class="by">teruakohatu</span><span>|</span><a href="#39974230">parent</a><span>|</span><a href="#39975736">next</a><span>|</span><label class="collapse" for="c-39974425">[-]</label><label class="expand" for="c-39974425">[1 more]</label></div><br/><div class="children"><div class="content">Yes general LLM models can be used for time series forecasting:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;KimMeen&#x2F;Time-LLM">https:&#x2F;&#x2F;github.com&#x2F;KimMeen&#x2F;Time-LLM</a></div><br/></div></div></div></div><div id="39975736" class="c"><input type="checkbox" id="c-39975736" checked=""/><div class="controls bullet"><span class="by">mrbonner</span><span>|</span><a href="#39974230">prev</a><span>|</span><a href="#39975132">next</a><span>|</span><label class="collapse" for="c-39975736">[-]</label><label class="expand" for="c-39975736">[1 more]</label></div><br/><div class="children"><div class="content">Wow, and this is done after a recent trip to Bhutan to clear his head! I follow karpathy on twitter and he posted that 2 weeks without constantly looking and checking his phone kind of turns off the constantly on radio in his head.</div><br/></div></div><div id="39975132" class="c"><input type="checkbox" id="c-39975132" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#39975736">prev</a><span>|</span><a href="#39973729">next</a><span>|</span><label class="collapse" for="c-39975132">[-]</label><label class="expand" for="c-39975132">[1 more]</label></div><br/><div class="children"><div class="content">Another awesome project! Note that as of this moment the CUDA part is aspirational. There is no gpu code in the repo yet.</div><br/></div></div><div id="39973729" class="c"><input type="checkbox" id="c-39973729" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#39975132">prev</a><span>|</span><a href="#39974235">next</a><span>|</span><label class="collapse" for="c-39973729">[-]</label><label class="expand" for="c-39973729">[1 more]</label></div><br/><div class="children"><div class="content">When Lex recently talked to Andre, Andre said that he gets positively obsessed with a problem and says &quot;this must exist&quot;. I imagine this must be one of those outputs.</div><br/></div></div><div id="39974235" class="c"><input type="checkbox" id="c-39974235" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#39973729">prev</a><span>|</span><a href="#39976143">next</a><span>|</span><label class="collapse" for="c-39974235">[-]</label><label class="expand" for="c-39974235">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to think he took the name from my llm.f90 project <a href="https:&#x2F;&#x2F;github.com&#x2F;rbitr&#x2F;llm.f90">https:&#x2F;&#x2F;github.com&#x2F;rbitr&#x2F;llm.f90</a><p>It was originally based off of Karpathy&#x27;s llama2.c but I renamed it when I added support for other architectures.<p>Probable a coincidence :)</div><br/><div id="39977181" class="c"><input type="checkbox" id="c-39977181" checked=""/><div class="controls bullet"><span class="by">matteogrella</span><span>|</span><a href="#39974235">parent</a><span>|</span><a href="#39976143">next</a><span>|</span><label class="collapse" for="c-39977181">[-]</label><label class="expand" for="c-39977181">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m the creator behind <a href="https:&#x2F;&#x2F;github.com&#x2F;nlpodyssey&#x2F;rwkv.f90">https:&#x2F;&#x2F;github.com&#x2F;nlpodyssey&#x2F;rwkv.f90</a>. How about joining forces?</div><br/></div></div></div></div><div id="39976143" class="c"><input type="checkbox" id="c-39976143" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#39974235">prev</a><span>|</span><a href="#39976845">next</a><span>|</span><label class="collapse" for="c-39976143">[-]</label><label class="expand" for="c-39976143">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;robjinman&#x2F;Richard">https:&#x2F;&#x2F;github.com&#x2F;robjinman&#x2F;Richard</a> uses Vulkan, thus is portable across GPU&#x27;s and much faster. It also has more kernels. In simple C++</div><br/></div></div><div id="39976845" class="c"><input type="checkbox" id="c-39976845" checked=""/><div class="controls bullet"><span class="by">classiebit2025</span><span>|</span><a href="#39976143">prev</a><span>|</span><a href="#39973850">next</a><span>|</span><label class="collapse" for="c-39976845">[-]</label><label class="expand" for="c-39976845">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;classiebit.com&#x2F;eventmie-pro" rel="nofollow">https:&#x2F;&#x2F;classiebit.com&#x2F;eventmie-pro</a>
If you looking to host your events but you don&#x27;t have any platform to host, then once visit Eventmie Pro Platform, Which is the best event management platform in 2024.</div><br/></div></div><div id="39973850" class="c"><input type="checkbox" id="c-39973850" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#39976845">prev</a><span>|</span><a href="#39974127">next</a><span>|</span><label class="collapse" for="c-39973850">[-]</label><label class="expand" for="c-39973850">[5 more]</label></div><br/><div class="children"><div class="content">It would be great if someone created a tutorial around this explaining exactly how it works and how to do a test training run. I’m aware it’s not feasible to train a “real” model on personal hardware but it would be nice to have a practical learning experience. I’m not sure if there are good alternatives for that.</div><br/><div id="39974942" class="c"><input type="checkbox" id="c-39974942" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#39973850">parent</a><span>|</span><a href="#39974828">next</a><span>|</span><label class="collapse" for="c-39974942">[-]</label><label class="expand" for="c-39974942">[3 more]</label></div><br/><div class="children"><div class="content">I wrote this, which might be a bit helpful:
<a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c&#x2F;blob&#x2F;master&#x2F;doc&#x2F;layernorm&#x2F;layernorm.md">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c&#x2F;blob&#x2F;master&#x2F;doc&#x2F;layernorm&#x2F;...</a><p>But if you don&#x27;t have the background, I&#x27;d recommend my YouTube videos, see the Zero To Hero playlist:
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxb...</a></div><br/><div id="39977220" class="c"><input type="checkbox" id="c-39977220" checked=""/><div class="controls bullet"><span class="by">MAMAMassakali</span><span>|</span><a href="#39973850">root</a><span>|</span><a href="#39974942">parent</a><span>|</span><a href="#39975009">next</a><span>|</span><label class="collapse" for="c-39977220">[-]</label><label class="expand" for="c-39977220">[1 more]</label></div><br/><div class="children"><div class="content">Thank you so much for the Zero To Hero playlist!</div><br/></div></div><div id="39975009" class="c"><input type="checkbox" id="c-39975009" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#39973850">root</a><span>|</span><a href="#39974942">parent</a><span>|</span><a href="#39977220">prev</a><span>|</span><a href="#39974828">next</a><span>|</span><label class="collapse" for="c-39975009">[-]</label><label class="expand" for="c-39975009">[1 more]</label></div><br/><div class="children"><div class="content">Thank you so much for responding. I will definitely check these out and also pass it on to others who might be interested.</div><br/></div></div></div></div><div id="39974828" class="c"><input type="checkbox" id="c-39974828" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39973850">parent</a><span>|</span><a href="#39974942">prev</a><span>|</span><a href="#39974127">next</a><span>|</span><label class="collapse" for="c-39974828">[-]</label><label class="expand" for="c-39974828">[1 more]</label></div><br/><div class="children"><div class="content">The author has a whole series where he does exactly that. YouTube videos, code examples, documentation, everything. Explains the math, explains how to code it, explains the architecture. Everything.</div><br/></div></div></div></div><div id="39974127" class="c"><input type="checkbox" id="c-39974127" checked=""/><div class="controls bullet"><span class="by">idkwhatimdoin</span><span>|</span><a href="#39973850">prev</a><span>|</span><a href="#39976228">next</a><span>|</span><label class="collapse" for="c-39974127">[-]</label><label class="expand" for="c-39974127">[7 more]</label></div><br/><div class="children"><div class="content">If I was starting from scratch, what resources should I start with to build up an understanding of what this code does and how to read it? It&#x27;s quite dense and my knowledge of LLMs is quite minimal. Are these terse variable names standard in LLM-land?</div><br/><div id="39974820" class="c"><input type="checkbox" id="c-39974820" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39974127">parent</a><span>|</span><a href="#39975205">next</a><span>|</span><label class="collapse" for="c-39974820">[-]</label><label class="expand" for="c-39974820">[4 more]</label></div><br/><div class="children"><div class="content">Terse variables are a C thing.<p>“What resources would I need” -&gt; you’re literally commenting on a teachers content. Karpathy (the author) has a very informative YouTube channel where he goes step by step through everything. He has a ton of repos and tutorials. Dig a little.<p>If all else fails… Google it.</div><br/><div id="39976264" class="c"><input type="checkbox" id="c-39976264" checked=""/><div class="controls bullet"><span class="by">idkwhatimdoin</span><span>|</span><a href="#39974127">root</a><span>|</span><a href="#39974820">parent</a><span>|</span><a href="#39975083">next</a><span>|</span><label class="collapse" for="c-39976264">[-]</label><label class="expand" for="c-39976264">[2 more]</label></div><br/><div class="children"><div class="content">&gt; you’re literally commenting on a teachers content.<p>How am I supposed to know that?<p>&gt; Karpathy (the author) has a very informative YouTube channel where he goes step by step through everything.<p>Or that, without knowing that he&#x27;s a teacher?<p>&gt; Terse variables are a C thing.<p>I didn&#x27;t realize variables had to be so short in C. Glad I write C++ professionally where they&#x27;ve added support for longer variable names.<p>&gt; If all else fails… Google it.<p>There&#x27;s a lot of LLM garbage out there. I got an answer here in a few minutes pointing to Karpathy&#x27;s course which seems very high quality.<p>Be kinder.</div><br/><div id="39976827" class="c"><input type="checkbox" id="c-39976827" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39974127">root</a><span>|</span><a href="#39976264">parent</a><span>|</span><a href="#39975083">next</a><span>|</span><label class="collapse" for="c-39976827">[-]</label><label class="expand" for="c-39976827">[1 more]</label></div><br/><div class="children"><div class="content">&gt; How am I supposed to know that?<p>You’re not supposed to know that. You asked a question, and this is you being told the answer.<p>It’s very convenient that the author of the post is quite literally the world’s most prolific teacher on this topic. Makes it easy to find Karpathy. You shouldn’t be expected to otherwise know that (or else why ask if you knew).<p>&gt; I didn&#x27;t realize variables had to be so short in C. Glad I write C++ professionally where they&#x27;ve added support for longer variable names.<p>This feels like a joke but old C compilers did have variable length limits. This is part of why C historically had shorter variables than other more modern languages.<p>Sorry if it came off rude, the internet is hard to communicate over.<p><a href="https:&#x2F;&#x2F;publications.gbdirect.co.uk&#x2F;c_book&#x2F;chapter2&#x2F;keywords_and_identifiers.html" rel="nofollow">https:&#x2F;&#x2F;publications.gbdirect.co.uk&#x2F;c_book&#x2F;chapter2&#x2F;keywords...</a></div><br/></div></div></div></div><div id="39975083" class="c"><input type="checkbox" id="c-39975083" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39974127">root</a><span>|</span><a href="#39974820">parent</a><span>|</span><a href="#39976264">prev</a><span>|</span><a href="#39975205">next</a><span>|</span><label class="collapse" for="c-39975083">[-]</label><label class="expand" for="c-39975083">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Terse variables are a C thing.<p>They&#x27;re a math &#x2F; toy code thing. Large C projects have long descriptive names just like other languages.</div><br/></div></div></div></div><div id="39975205" class="c"><input type="checkbox" id="c-39975205" checked=""/><div class="controls bullet"><span class="by">satokema</span><span>|</span><a href="#39974127">parent</a><span>|</span><a href="#39974820">prev</a><span>|</span><a href="#39974386">next</a><span>|</span><label class="collapse" for="c-39975205">[-]</label><label class="expand" for="c-39975205">[1 more]</label></div><br/><div class="children"><div class="content">As siblings have said, his video series are quite good. But if you&#x27;re just looking at this repo only, you probably want to look at the python reference implementation. (The C is designed to exactly replicate its functionality.)</div><br/></div></div><div id="39974386" class="c"><input type="checkbox" id="c-39974386" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#39974127">parent</a><span>|</span><a href="#39975205">prev</a><span>|</span><a href="#39976228">next</a><span>|</span><label class="collapse" for="c-39974386">[-]</label><label class="expand" for="c-39974386">[1 more]</label></div><br/><div class="children"><div class="content">Check out his zero to hero series. Which builds this with python and later pytorch, then probably his other mini C based projects.</div><br/></div></div></div></div><div id="39976228" class="c"><input type="checkbox" id="c-39976228" checked=""/><div class="controls bullet"><span class="by">robot</span><span>|</span><a href="#39974127">prev</a><span>|</span><a href="#39975310">next</a><span>|</span><label class="collapse" for="c-39976228">[-]</label><label class="expand" for="c-39976228">[1 more]</label></div><br/><div class="children"><div class="content">very cool, also the coding style looks good.</div><br/></div></div><div id="39975310" class="c"><input type="checkbox" id="c-39975310" checked=""/><div class="controls bullet"><span class="by">richrichie</span><span>|</span><a href="#39976228">prev</a><span>|</span><a href="#39973727">next</a><span>|</span><label class="collapse" for="c-39975310">[-]</label><label class="expand" for="c-39975310">[1 more]</label></div><br/><div class="children"><div class="content">See, C does it very well.
Great stuff. Karpathy has a gift for teaching.</div><br/></div></div><div id="39973727" class="c"><input type="checkbox" id="c-39973727" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#39975310">prev</a><span>|</span><a href="#39973473">next</a><span>|</span><label class="collapse" for="c-39973727">[-]</label><label class="expand" for="c-39973727">[14 more]</label></div><br/><div class="children"><div class="content">OT but question from someone curious..... is Cuda still entrenched as the only option for doing AI or is there growing support for AMD&#x2F;Intel&#x2F;Other ways of doing AI?</div><br/><div id="39974164" class="c"><input type="checkbox" id="c-39974164" checked=""/><div class="controls bullet"><span class="by">adam_arthur</span><span>|</span><a href="#39973727">parent</a><span>|</span><a href="#39975209">next</a><span>|</span><label class="collapse" for="c-39974164">[-]</label><label class="expand" for="c-39974164">[1 more]</label></div><br/><div class="children"><div class="content">You can run inference today on pretty much any card.<p>Download  Ollama on a modern MacBook and can run 13B and even higher (if your RAM allows) at fast speeds. People run smaller models locally on their phones<p>Google has trained their latest models on their own TPUs... not using Nvidia to my knowledge.<p>So, no, there are alternatives. CUDA has the largest mindshare on the training side though.</div><br/></div></div><div id="39975209" class="c"><input type="checkbox" id="c-39975209" checked=""/><div class="controls bullet"><span class="by">towelpluswater</span><span>|</span><a href="#39973727">parent</a><span>|</span><a href="#39974164">prev</a><span>|</span><a href="#39973746">next</a><span>|</span><label class="collapse" for="c-39975209">[-]</label><label class="expand" for="c-39975209">[2 more]</label></div><br/><div class="children"><div class="content">Modular Mojo is the most well funded and full of respectable players for making an alternative possible</div><br/><div id="39975752" class="c"><input type="checkbox" id="c-39975752" checked=""/><div class="controls bullet"><span class="by">pavelstoev</span><span>|</span><a href="#39973727">root</a><span>|</span><a href="#39975209">parent</a><span>|</span><a href="#39973746">next</a><span>|</span><label class="collapse" for="c-39975752">[-]</label><label class="expand" for="c-39975752">[1 more]</label></div><br/><div class="children"><div class="content">Check out Hidet [1]. Not as well funded, but delivers Python based ML acceleration with GPU support (unlike Mojo).<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;hidet-org&#x2F;hidet">https:&#x2F;&#x2F;github.com&#x2F;hidet-org&#x2F;hidet</a></div><br/></div></div></div></div><div id="39973746" class="c"><input type="checkbox" id="c-39973746" checked=""/><div class="controls bullet"><span class="by">BraverHeart</span><span>|</span><a href="#39973727">parent</a><span>|</span><a href="#39975209">prev</a><span>|</span><a href="#39974359">next</a><span>|</span><label class="collapse" for="c-39973746">[-]</label><label class="expand" for="c-39973746">[5 more]</label></div><br/><div class="children"><div class="content">George Hotz is attempting to solve this: <a href="https:&#x2F;&#x2F;github.com&#x2F;tinygrad&#x2F;tinygrad">https:&#x2F;&#x2F;github.com&#x2F;tinygrad&#x2F;tinygrad</a></div><br/><div id="39974102" class="c"><input type="checkbox" id="c-39974102" checked=""/><div class="controls bullet"><span class="by">ZoomerCretin</span><span>|</span><a href="#39973727">root</a><span>|</span><a href="#39973746">parent</a><span>|</span><a href="#39974359">next</a><span>|</span><label class="collapse" for="c-39974102">[-]</label><label class="expand" for="c-39974102">[4 more]</label></div><br/><div class="children"><div class="content">He loudly gave up on AMD after they did not fix a blocker he had for 5+ months and gave him the runaround the entire time when he asked for the code to fix it himself. He is still shipping the AMD tinybox with huge warning labels.</div><br/><div id="39975915" class="c"><input type="checkbox" id="c-39975915" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#39973727">root</a><span>|</span><a href="#39974102">parent</a><span>|</span><a href="#39975465">next</a><span>|</span><label class="collapse" for="c-39975915">[-]</label><label class="expand" for="c-39975915">[1 more]</label></div><br/><div class="children"><div class="content">Randomly stumbled over this[1] post with another fed up open source contributor, due to several serious issues with AMDs GPU drivers and firmware that remain unresolved for years. It also references the geohot decision you mention.<p>Some quotes:<p><i>I find it incredible that these companies that have large support contracts with you and have invested hundreds of thousands of dollars into your products, have been forced to turn to me, a mostly unknown self-employed hacker with very limited resources to try to work around these bugs (design faults?) in your hardware.</i><p><i>In the VFIO space we no longer recommend AMD GPUs at all, in every instance where people ask for which GPU to use for their new build, the advise is to use NVidia.</i><p>[1]: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Amd&#x2F;comments&#x2F;1bsjm5a&#x2F;letter_to_amd_ongoing_amd&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Amd&#x2F;comments&#x2F;1bsjm5a&#x2F;letter_to_amd_...</a></div><br/></div></div><div id="39975465" class="c"><input type="checkbox" id="c-39975465" checked=""/><div class="controls bullet"><span class="by">Art9681</span><span>|</span><a href="#39973727">root</a><span>|</span><a href="#39974102">parent</a><span>|</span><a href="#39975915">prev</a><span>|</span><a href="#39974359">next</a><span>|</span><label class="collapse" for="c-39975465">[-]</label><label class="expand" for="c-39975465">[2 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t they recently announce that everything was open sourced? Would be cool if he took another look at it once all of the souce code is available (if not already).</div><br/><div id="39975798" class="c"><input type="checkbox" id="c-39975798" checked=""/><div class="controls bullet"><span class="by">xjay</span><span>|</span><a href="#39973727">root</a><span>|</span><a href="#39975465">parent</a><span>|</span><a href="#39974359">next</a><span>|</span><label class="collapse" for="c-39975798">[-]</label><label class="expand" for="c-39975798">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They haven&#x27;t open sourced anything. They posted a tweet. [1]<p>[1@2024-04-06] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=j7MRj4N2Cyk&amp;t=429s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=j7MRj4N2Cyk&amp;t=429s</a><p>[Twitch] <a href="https:&#x2F;&#x2F;twitch.tv&#x2F;georgehotz" rel="nofollow">https:&#x2F;&#x2F;twitch.tv&#x2F;georgehotz</a></div><br/></div></div></div></div></div></div></div></div><div id="39974359" class="c"><input type="checkbox" id="c-39974359" checked=""/><div class="controls bullet"><span class="by">taminka</span><span>|</span><a href="#39973727">parent</a><span>|</span><a href="#39973746">prev</a><span>|</span><a href="#39973791">next</a><span>|</span><label class="collapse" for="c-39974359">[-]</label><label class="expand" for="c-39974359">[1 more]</label></div><br/><div class="children"><div class="content">there are obv alternatives from both intel and amd, performant blas&#x2F;dnn packages, but small teams don’t use them bc cuda is easier to use and has more support, and larger teams don’t use them bc they have deals w&#x2F; nvidia or not enough GPUs are available or they’re after the absolute best performance (which is still nvidia) or bc of other stuff like unstable drivers or smth</div><br/></div></div><div id="39973791" class="c"><input type="checkbox" id="c-39973791" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39973727">parent</a><span>|</span><a href="#39974359">prev</a><span>|</span><a href="#39973917">next</a><span>|</span><label class="collapse" for="c-39973791">[-]</label><label class="expand" for="c-39973791">[1 more]</label></div><br/><div class="children"><div class="content">There are some stirrings but don&#x27;t hold your breath</div><br/></div></div><div id="39973917" class="c"><input type="checkbox" id="c-39973917" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39973727">parent</a><span>|</span><a href="#39973791">prev</a><span>|</span><a href="#39973838">next</a><span>|</span><label class="collapse" for="c-39973917">[-]</label><label class="expand" for="c-39973917">[1 more]</label></div><br/><div class="children"><div class="content">There are a few attempts here and there in various stages of progression. But right now, nothing matches Nvidia+CUDA in speed and usability.</div><br/></div></div><div id="39973838" class="c"><input type="checkbox" id="c-39973838" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#39973727">parent</a><span>|</span><a href="#39973917">prev</a><span>|</span><a href="#39976219">next</a><span>|</span><label class="collapse" for="c-39973838">[-]</label><label class="expand" for="c-39973838">[1 more]</label></div><br/><div class="children"><div class="content">See my comment on this here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39973816">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39973816</a></div><br/></div></div></div></div><div id="39973473" class="c"><input type="checkbox" id="c-39973473" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#39973727">prev</a><span>|</span><a href="#39974779">next</a><span>|</span><label class="collapse" for="c-39973473">[-]</label><label class="expand" for="c-39973473">[25 more]</label></div><br/><div class="children"><div class="content">&gt; LLM training in simple, pure C&#x2F;CUDA. There is no need for 245MB of PyTorch or 107MB of cPython</div><br/><div id="39973682" class="c"><input type="checkbox" id="c-39973682" checked=""/><div class="controls bullet"><span class="by">QuadmasterXLII</span><span>|</span><a href="#39973473">parent</a><span>|</span><a href="#39973573">next</a><span>|</span><label class="collapse" for="c-39973682">[-]</label><label class="expand" for="c-39973682">[11 more]</label></div><br/><div class="children"><div class="content">107MB of cPython defeated<p>Go to try for self<p>Step 1 download 2.4GB of CUDA</div><br/><div id="39973711" class="c"><input type="checkbox" id="c-39973711" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973682">parent</a><span>|</span><a href="#39974007">next</a><span>|</span><label class="collapse" for="c-39973711">[-]</label><label class="expand" for="c-39973711">[6 more]</label></div><br/><div class="children"><div class="content">The size of CUDA really is astonishing. Any chance someone might figure out how to slim that down?</div><br/><div id="39973846" class="c"><input type="checkbox" id="c-39973846" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973711">parent</a><span>|</span><a href="#39973921">next</a><span>|</span><label class="collapse" for="c-39973846">[-]</label><label class="expand" for="c-39973846">[1 more]</label></div><br/><div class="children"><div class="content">Taking a peek inside the package it seems to mostly be the libraries - CuFFT alone is about 350MB for example, twice over for the debug and release versions. I&#x27;m guessing those are probably fat binaries pre-compiled for every generation of Nvidia hardware rather than just the PTX bytecode, which would help to speed up fresh builds, at the expense of being huge.</div><br/></div></div><div id="39973921" class="c"><input type="checkbox" id="c-39973921" checked=""/><div class="controls bullet"><span class="by">phdelightful</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973711">parent</a><span>|</span><a href="#39973846">prev</a><span>|</span><a href="#39973828">next</a><span>|</span><label class="collapse" for="c-39973921">[-]</label><label class="expand" for="c-39973921">[1 more]</label></div><br/><div class="children"><div class="content">Here’s a blog that breaks down how large different pieces of CUDA are:<p><a href="https:&#x2F;&#x2F;carlpearson.net&#x2F;post&#x2F;20231023-cuda-releases&#x2F;" rel="nofollow">https:&#x2F;&#x2F;carlpearson.net&#x2F;post&#x2F;20231023-cuda-releases&#x2F;</a></div><br/></div></div><div id="39973828" class="c"><input type="checkbox" id="c-39973828" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973711">parent</a><span>|</span><a href="#39973921">prev</a><span>|</span><a href="#39973906">next</a><span>|</span><label class="collapse" for="c-39973828">[-]</label><label class="expand" for="c-39973828">[1 more]</label></div><br/><div class="children"><div class="content">Talking directly to the kernel &#x2F; driver &#x2F; firmware.<p>As others have said, George Hotz is doing his best in reverse-engineering and skipping layers.</div><br/></div></div><div id="39973906" class="c"><input type="checkbox" id="c-39973906" checked=""/><div class="controls bullet"><span class="by">maille</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973711">parent</a><span>|</span><a href="#39973828">prev</a><span>|</span><a href="#39973726">next</a><span>|</span><label class="collapse" for="c-39973906">[-]</label><label class="expand" for="c-39973906">[1 more]</label></div><br/><div class="children"><div class="content">Raise your voice on their forum: <a href="https:&#x2F;&#x2F;forums.developer.nvidia.com&#x2F;t&#x2F;how-to-overcome-the-huge-increase-of-windows-dlls-minimum-cuda-cudnn-builds-that-support-ampere&#x2F;217580" rel="nofollow">https:&#x2F;&#x2F;forums.developer.nvidia.com&#x2F;t&#x2F;how-to-overcome-the-hu...</a>
Tried my luck 2 years ago but it keeps increasing.</div><br/></div></div><div id="39973726" class="c"><input type="checkbox" id="c-39973726" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973711">parent</a><span>|</span><a href="#39973906">prev</a><span>|</span><a href="#39974007">next</a><span>|</span><label class="collapse" for="c-39973726">[-]</label><label class="expand" for="c-39973726">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia is the only one who could, since they own it.</div><br/></div></div></div></div><div id="39974007" class="c"><input type="checkbox" id="c-39974007" checked=""/><div class="controls bullet"><span class="by">fsloth</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973682">parent</a><span>|</span><a href="#39973711">prev</a><span>|</span><a href="#39974005">next</a><span>|</span><label class="collapse" for="c-39974007">[-]</label><label class="expand" for="c-39974007">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s about the byte size, but the inherent complexity of the implementation. 1000 lines of C code is extremely simple by any standard. Whereas a sundry collection of Python and PyTorch libraries is anything but.</div><br/></div></div><div id="39974005" class="c"><input type="checkbox" id="c-39974005" checked=""/><div class="controls bullet"><span class="by">dwroberts</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973682">parent</a><span>|</span><a href="#39974007">prev</a><span>|</span><a href="#39973960">next</a><span>|</span><label class="collapse" for="c-39974005">[-]</label><label class="expand" for="c-39974005">[1 more]</label></div><br/><div class="children"><div class="content">A bunch of install methods for torch via pip include ~1.5GB of lib&#x2F; because of CUDA. libtorch_cuda.so is like 800MB on its own</div><br/></div></div><div id="39973960" class="c"><input type="checkbox" id="c-39973960" checked=""/><div class="controls bullet"><span class="by">gcr</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973682">parent</a><span>|</span><a href="#39974005">prev</a><span>|</span><a href="#39973702">next</a><span>|</span><label class="collapse" for="c-39973960">[-]</label><label class="expand" for="c-39973960">[1 more]</label></div><br/><div class="children"><div class="content">I mean, being fair, the 2.4GB CUDA SDK is absolutely required for the cPython implementation as well</div><br/></div></div></div></div><div id="39973573" class="c"><input type="checkbox" id="c-39973573" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#39973473">parent</a><span>|</span><a href="#39973682">prev</a><span>|</span><a href="#39973896">next</a><span>|</span><label class="collapse" for="c-39973573">[-]</label><label class="expand" for="c-39973573">[12 more]</label></div><br/><div class="children"><div class="content">Python has been popular for this because it’s convenient to quickly hack on and experiment with, not because it’s the most efficient thing.</div><br/><div id="39973690" class="c"><input type="checkbox" id="c-39973690" checked=""/><div class="controls bullet"><span class="by">im3w1l</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973573">parent</a><span>|</span><a href="#39973896">next</a><span>|</span><label class="collapse" for="c-39973690">[-]</label><label class="expand" for="c-39973690">[11 more]</label></div><br/><div class="children"><div class="content">The overhead really isn&#x27;t that bad is it? Since the the python code is mostly about saying multiply matrix A with matrix B, and then that actual computation is done by optimized low level code.</div><br/><div id="39974356" class="c"><input type="checkbox" id="c-39974356" checked=""/><div class="controls bullet"><span class="by">llm_nerd</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973690">parent</a><span>|</span><a href="#39973814">next</a><span>|</span><label class="collapse" for="c-39974356">[-]</label><label class="expand" for="c-39974356">[1 more]</label></div><br/><div class="children"><div class="content">It depends on how you define overhead. Runtime overhead and memory usage is absolutely marginal, and the tightest, most perfect implementation will have trouble beating it.<p>Instead people are trying to optimize install size of dependencies, which while maybe a fun hacking project...who really cares?</div><br/></div></div><div id="39973814" class="c"><input type="checkbox" id="c-39973814" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973690">parent</a><span>|</span><a href="#39974356">prev</a><span>|</span><a href="#39973782">next</a><span>|</span><label class="collapse" for="c-39973814">[-]</label><label class="expand" for="c-39973814">[7 more]</label></div><br/><div class="children"><div class="content">For that stuff, yeah you&#x27;re correct.<p>What I&#x27;ve seen is issues with the implementation of those libraries in a project.<p>I don&#x27;t remember exactly, but I was playing with someone&#x27;s wrapper for some kind of machine learning snake game and it was taking way longer than it should have on back of the napkin math.<p>The issue was using either a dict or a list in a hot loop and changing it to the other sped it up like 1000x.<p>So it&#x27;s easy to think &quot;yeah this library is optimized&quot; but then you build something on top of it that is not obviously going to slow it down.<p>But, that&#x27;s the Python tradeoff.</div><br/><div id="39974025" class="c"><input type="checkbox" id="c-39974025" checked=""/><div class="controls bullet"><span class="by">hcarvalhoalves</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973814">parent</a><span>|</span><a href="#39973970">next</a><span>|</span><label class="collapse" for="c-39974025">[-]</label><label class="expand" for="c-39974025">[4 more]</label></div><br/><div class="children"><div class="content">&gt; The issue was using either a dict or a list in a hot loop and changing it to the other sped it up like 1000x.<p>The programmer using the wrong data structure is not a problem with the language.</div><br/><div id="39974698" class="c"><input type="checkbox" id="c-39974698" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39974025">parent</a><span>|</span><a href="#39974212">next</a><span>|</span><label class="collapse" for="c-39974698">[-]</label><label class="expand" for="c-39974698">[1 more]</label></div><br/><div class="children"><div class="content">Kinda. I guess my native tongue is C&#x2F;C++ and I wouldn&#x27;t expect such a huge performance difference when using an array vs a linked list or something.<p>It&#x27;s not like I had millions of items in that structure either, it was like 100. I think it contained the batch training data from each round. I tried to find the project but couldn&#x27;t.<p>I was just shocked that there was such a huge difference between primitive data structures. In that situation, I wouldn&#x27;t have guessed it would make a difference.</div><br/></div></div><div id="39974212" class="c"><input type="checkbox" id="c-39974212" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39974025">parent</a><span>|</span><a href="#39974698">prev</a><span>|</span><a href="#39973970">next</a><span>|</span><label class="collapse" for="c-39974212">[-]</label><label class="expand" for="c-39974212">[2 more]</label></div><br/><div class="children"><div class="content">It really is with Python.  There are simply too many containers and container-like concepts.  Lists, arrays, sets, dicts...</div><br/><div id="39974376" class="c"><input type="checkbox" id="c-39974376" checked=""/><div class="controls bullet"><span class="by">0cf8612b2e1e</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39974212">parent</a><span>|</span><a href="#39973970">next</a><span>|</span><label class="collapse" for="c-39974376">[-]</label><label class="expand" for="c-39974376">[1 more]</label></div><br/><div class="children"><div class="content">What modern language doesn’t have those?<p>Go kind of cheats and has maps play double duty as sets.</div><br/></div></div></div></div></div></div><div id="39973970" class="c"><input type="checkbox" id="c-39973970" checked=""/><div class="controls bullet"><span class="by">0cf8612b2e1e</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973814">parent</a><span>|</span><a href="#39974025">prev</a><span>|</span><a href="#39973782">next</a><span>|</span><label class="collapse" for="c-39973970">[-]</label><label class="expand" for="c-39973970">[2 more]</label></div><br/><div class="children"><div class="content">That sounds irrelevant to Python and just a matter of slow code cropping up in libraries until someone runs a profiler.</div><br/><div id="39974463" class="c"><input type="checkbox" id="c-39974463" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973970">parent</a><span>|</span><a href="#39973782">next</a><span>|</span><label class="collapse" for="c-39974463">[-]</label><label class="expand" for="c-39974463">[1 more]</label></div><br/><div class="children"><div class="content">But then again if your program have places where choosing the right Python primitive is important for performance, then using python is affecting performance here since even the best algorithm in Python would be slower than the equivalent C.<p>Most of the time it doesn&#x27;t matter because there&#x27;s nothing hoy on the Python side, but if there is, then Python is going to be slowing your stuff down.</div><br/></div></div></div></div></div></div><div id="39973782" class="c"><input type="checkbox" id="c-39973782" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973690">parent</a><span>|</span><a href="#39973814">prev</a><span>|</span><a href="#39973896">next</a><span>|</span><label class="collapse" for="c-39973782">[-]</label><label class="expand" for="c-39973782">[2 more]</label></div><br/><div class="children"><div class="content">I suspect that this has a high chance of running afoul of Ahmdal’s Law. Even if you can parallelise the bulk of the computation, the serial parts remain single-threaded and start to dominate the total runtime.</div><br/><div id="39974741" class="c"><input type="checkbox" id="c-39974741" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39973473">root</a><span>|</span><a href="#39973782">parent</a><span>|</span><a href="#39973896">next</a><span>|</span><label class="collapse" for="c-39974741">[-]</label><label class="expand" for="c-39974741">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think the serial parts of ML training are Python’s fault, are they? It’s all “operation B depends on the output of operation A”.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39974006" class="c"><input type="checkbox" id="c-39974006" checked=""/><div class="controls bullet"><span class="by">fori1to10</span><span>|</span><a href="#39973674">prev</a><span>|</span><a href="#39973594">next</a><span>|</span><label class="collapse" for="c-39974006">[-]</label><label class="expand" for="c-39974006">[5 more]</label></div><br/><div class="children"><div class="content">It should be rewritten in Rust. (Just joking)</div><br/><div id="39975041" class="c"><input type="checkbox" id="c-39975041" checked=""/><div class="controls bullet"><span class="by">ddggdd</span><span>|</span><a href="#39974006">parent</a><span>|</span><a href="#39975782">next</a><span>|</span><label class="collapse" for="c-39975041">[-]</label><label class="expand" for="c-39975041">[1 more]</label></div><br/><div class="children"><div class="content">I just pasted the code into claude and reading the converted rust now, definitely need extra work</div><br/></div></div><div id="39975782" class="c"><input type="checkbox" id="c-39975782" checked=""/><div class="controls bullet"><span class="by">naruhodo</span><span>|</span><a href="#39974006">parent</a><span>|</span><a href="#39975041">prev</a><span>|</span><a href="#39974671">next</a><span>|</span><label class="collapse" for="c-39975782">[-]</label><label class="expand" for="c-39975782">[1 more]</label></div><br/><div class="children"><div class="content">I think you just cracked AI safety.</div><br/></div></div><div id="39974027" class="c"><input type="checkbox" id="c-39974027" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39974006">parent</a><span>|</span><a href="#39974671">prev</a><span>|</span><a href="#39973594">next</a><span>|</span><label class="collapse" for="c-39974027">[-]</label><label class="expand" for="c-39974027">[1 more]</label></div><br/><div class="children"><div class="content">Sshh! I asked why it was written in C and got flagged.</div><br/></div></div></div></div><div id="39973581" class="c"><input type="checkbox" id="c-39973581" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#39973594">prev</a><span>|</span><label class="collapse" for="c-39973581">[-]</label><label class="expand" for="c-39973581">[10 more]</label></div><br/><div class="children"><div class="content">Very sad, shouldve used an agnostic framework instead of CUDA</div><br/><div id="39973678" class="c"><input type="checkbox" id="c-39973678" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#39973581">parent</a><span>|</span><a href="#39973700">next</a><span>|</span><label class="collapse" for="c-39973678">[-]</label><label class="expand" for="c-39973678">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s only ~1000 LoC, seems like a pretty good case study to port over to other runtimes and show they can stand up to CUDA.</div><br/></div></div><div id="39973700" class="c"><input type="checkbox" id="c-39973700" checked=""/><div class="controls bullet"><span class="by">geph2021</span><span>|</span><a href="#39973581">parent</a><span>|</span><a href="#39973678">prev</a><span>|</span><a href="#39973716">next</a><span>|</span><label class="collapse" for="c-39973700">[-]</label><label class="expand" for="c-39973700">[3 more]</label></div><br/><div class="children"><div class="content">As far as I can tell, its optional dependency is Open MP, not CUDA.  Doesn&#x27;t seem directly dependent on CUDA.</div><br/><div id="39973976" class="c"><input type="checkbox" id="c-39973976" checked=""/><div class="controls bullet"><span class="by">dlazaro</span><span>|</span><a href="#39973581">root</a><span>|</span><a href="#39973700">parent</a><span>|</span><a href="#39973786">next</a><span>|</span><label class="collapse" for="c-39973976">[-]</label><label class="expand" for="c-39973976">[1 more]</label></div><br/><div class="children"><div class="content">The plan is to eventually implement with CUDA:<p>&quot;Currently, I am working on [...] direct CUDA implementation, which will be significantly faster and probably come close to PyTorch.&quot;</div><br/></div></div><div id="39973786" class="c"><input type="checkbox" id="c-39973786" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#39973581">root</a><span>|</span><a href="#39973700">parent</a><span>|</span><a href="#39973976">prev</a><span>|</span><a href="#39973716">next</a><span>|</span><label class="collapse" for="c-39973786">[-]</label><label class="expand" for="c-39973786">[1 more]</label></div><br/><div class="children"><div class="content">Yes, a quick skim of the code only shows openmp dependency. The C&#x2F;CUDA reference might have meant to be C&#x2F;OMP .<p>Although I wonder if it would work well with GCC PTX OMP offloading.</div><br/></div></div></div></div><div id="39973716" class="c"><input type="checkbox" id="c-39973716" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39973581">parent</a><span>|</span><a href="#39973700">prev</a><span>|</span><a href="#39973845">next</a><span>|</span><label class="collapse" for="c-39973716">[-]</label><label class="expand" for="c-39973716">[3 more]</label></div><br/><div class="children"><div class="content">Are there any strong LLMs trained without CUDA?</div><br/><div id="39973816" class="c"><input type="checkbox" id="c-39973816" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#39973581">root</a><span>|</span><a href="#39973716">parent</a><span>|</span><a href="#39974121">next</a><span>|</span><label class="collapse" for="c-39973816">[-]</label><label class="expand" for="c-39973816">[1 more]</label></div><br/><div class="children"><div class="content">Yes, there are several. See this blog post from Databricks describing the landscape of LLMs trained on AMD hardware for example: <a href="https:&#x2F;&#x2F;www.databricks.com&#x2F;blog&#x2F;training-llms-scale-amd-mi250-gpus" rel="nofollow">https:&#x2F;&#x2F;www.databricks.com&#x2F;blog&#x2F;training-llms-scale-amd-mi25...</a><p>The most interesting one IMO is OLMo from AI2, which is truly open. You can read their blog post about it (<a href="https:&#x2F;&#x2F;blog.allenai.org&#x2F;hello-olmo-a-truly-open-llm-43f7e7359222" rel="nofollow">https:&#x2F;&#x2F;blog.allenai.org&#x2F;hello-olmo-a-truly-open-llm-43f7e73...</a>) but basically it is open everything - they released everything you need to reproduce their weights (training data, training code, evaluation code, and weights) with a friendly (Apache) license.</div><br/></div></div><div id="39974121" class="c"><input type="checkbox" id="c-39974121" checked=""/><div class="controls bullet"><span class="by">ZoomerCretin</span><span>|</span><a href="#39973581">root</a><span>|</span><a href="#39973716">parent</a><span>|</span><a href="#39973816">prev</a><span>|</span><a href="#39973845">next</a><span>|</span><label class="collapse" for="c-39974121">[-]</label><label class="expand" for="c-39974121">[1 more]</label></div><br/><div class="children"><div class="content">Gemini and Gemma were trained on Google&#x27;s TPUs.</div><br/></div></div></div></div><div id="39974241" class="c"><input type="checkbox" id="c-39974241" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#39973581">parent</a><span>|</span><a href="#39973845">prev</a><span>|</span><label class="collapse" for="c-39974241">[-]</label><label class="expand" for="c-39974241">[1 more]</label></div><br/><div class="children"><div class="content">Looking forward to your patches!</div><br/></div></div></div></div></div></div></div></div></div></body></html>