<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733562053692" as="style"/><link rel="stylesheet" href="styles.css?v=1733562053692"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2305.17493">The Curse of Recursion: Training on generated data makes models forget (2023)</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>surprisetalk</span> | <span>21 comments</span></div><br/><div><div id="42348235" class="c"><input type="checkbox" id="c-42348235" checked=""/><div class="controls bullet"><span class="by">aucisson_masque</span><span>|</span><a href="#42347978">next</a><span>|</span><label class="collapse" for="c-42348235">[-]</label><label class="expand" for="c-42348235">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.<p>Does it mean that data hungry corporation like Google, Facebook, Amazon, openai with Microsoft backing, that are already all around the internet and our phone tracking us have an incredibly advantage over open source model?<p>Is that why Google is pushing gemini so hard on Android even though it&#x27;s half ass done? they need fresh human data so much to be able to compete and beat the competition ?</div><br/></div></div><div id="42347978" class="c"><input type="checkbox" id="c-42347978" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#42348235">prev</a><span>|</span><a href="#42347901">next</a><span>|</span><label class="collapse" for="c-42347978">[-]</label><label class="expand" for="c-42347978">[2 more]</label></div><br/><div class="children"><div class="content">This paper was first published in May 2023 and discussed on HN the following month:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36319076">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36319076</a><p>Some research since seems to add nuance to its conclusions:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.01413" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.01413</a></div><br/><div id="42348289" class="c"><input type="checkbox" id="c-42348289" checked=""/><div class="controls bullet"><span class="by">CaptainFever</span><span>|</span><a href="#42347978">parent</a><span>|</span><a href="#42347901">next</a><span>|</span><label class="collapse" for="c-42348289">[-]</label><label class="expand" for="c-42348289">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops proposed that such loops would lead to a phenomenon termed model collapse, under which performance progressively degrades with each model-data feedback iteration until fitted models become useless. However, those studies largely assumed that new data replace old data over time, where an arguably more realistic assumption is that data accumulate over time. In this paper, we ask: what effect does accumulating data have on model collapse? We empirically study this question by pretraining sequences of language models on text corpora. We confirm that replacing the original real data by each generation&#x27;s synthetic data does indeed tend towards model collapse, then demonstrate that accumulating the successive generations of synthetic data alongside the original real data avoids model collapse; these results hold across a range of model sizes, architectures, and hyperparameters. We obtain similar results for deep generative models on other types of real data: diffusion models for molecule conformation generation and variational autoencoders for image generation. To understand why accumulating data can avoid model collapse, we use an analytically tractable framework introduced by prior work in which a sequence of linear models are fit to the previous models&#x27; outputs. Previous work used this framework to show that if data are replaced, the test error increases with the number of model-fitting iterations; we extend this argument to prove that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations, meaning model collapse no longer occurs.<p>TL;DR: This paper confirms that Model Collapse can happen if the original data is replaced with synthetic data, but if both are used alongside each other, it no longer happens.</div><br/></div></div></div></div><div id="42347901" class="c"><input type="checkbox" id="c-42347901" checked=""/><div class="controls bullet"><span class="by">goose-</span><span>|</span><a href="#42347978">prev</a><span>|</span><a href="#42347830">next</a><span>|</span><label class="collapse" for="c-42347901">[-]</label><label class="expand" for="c-42347901">[1 more]</label></div><br/><div class="children"><div class="content">My takeaway after scanning the paper -<p>In an ideal setting, a trained model learns exactly the real world probability distribution, and generates data indistinguishable from those sampled from the real world. Training on them would be fine, but pointless, since the model is already a perfect representation of the real world.<p>Practically, however, a model is only a lossy approximation of the real world probability distribution. Repeated self-training would simply compound the loss - amplifying both the probable and the improbable.</div><br/></div></div><div id="42347830" class="c"><input type="checkbox" id="c-42347830" checked=""/><div class="controls bullet"><span class="by">benchmarkist</span><span>|</span><a href="#42347901">prev</a><span>|</span><a href="#42347762">next</a><span>|</span><label class="collapse" for="c-42347830">[-]</label><label class="expand" for="c-42347830">[3 more]</label></div><br/><div class="children"><div class="content">This is intuitively obvious. If I give you some data x and you transform it with a non-reversible function f into f(x) then you are losing information. Repeated applications of the function, f(f(f(...f(x)...))), can only make the end result worse. The current implementations inject some random bits, b ~ N(u, s), but this can be thought of as a convolution operation with the distribution function g of the random data, g*f, that is injected which, after repeated applications, (g*f)((g*f)((g*f)(...(g*f)(x)...))), reduces the information content of the data you started with because the transformation is still not reversible as convolutions can not really change the non-reversible aspect of the original function.<p>I&#x27;m sure there is some calculation using entropy of random variables and channels that fully formalizes this but I don&#x27;t remember the references off the top of my head. The general reference I remember is called the data processing inequality.¹<p>¹ <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Data_processing_inequality?useskin=vector" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Data_processing_inequality?use...</a></div><br/><div id="42348044" class="c"><input type="checkbox" id="c-42348044" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#42347830">parent</a><span>|</span><a href="#42347762">next</a><span>|</span><label class="collapse" for="c-42348044">[-]</label><label class="expand" for="c-42348044">[2 more]</label></div><br/><div class="children"><div class="content">Relevant XKCD:<p><a href="https:&#x2F;&#x2F;xkcd.com&#x2F;1683&#x2F;" rel="nofollow">https:&#x2F;&#x2F;xkcd.com&#x2F;1683&#x2F;</a></div><br/><div id="42348060" class="c"><input type="checkbox" id="c-42348060" checked=""/><div class="controls bullet"><span class="by">benchmarkist</span><span>|</span><a href="#42347830">root</a><span>|</span><a href="#42348044">parent</a><span>|</span><a href="#42347762">next</a><span>|</span><label class="collapse" for="c-42348060">[-]</label><label class="expand" for="c-42348060">[1 more]</label></div><br/><div class="children"><div class="content">Good one but these theorems are useful to have when thinking about information processing systems and whatever promises the hype artists are making about the latest and greatest iteration of neural networks. There is no way to cheat entropy and basic physics so if it sounds too good to be true then it probably is too good to be true.</div><br/></div></div></div></div></div></div><div id="42347762" class="c"><input type="checkbox" id="c-42347762" checked=""/><div class="controls bullet"><span class="by">banku_brougham</span><span>|</span><a href="#42347830">prev</a><span>|</span><a href="#42348268">next</a><span>|</span><label class="collapse" for="c-42347762">[-]</label><label class="expand" for="c-42347762">[1 more]</label></div><br/><div class="children"><div class="content">My intuition is the public, users, nor the industry will take this problem seriously. To me this paper sounds a thunderclap.</div><br/></div></div><div id="42348268" class="c"><input type="checkbox" id="c-42348268" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#42347762">prev</a><span>|</span><a href="#42347845">next</a><span>|</span><label class="collapse" for="c-42348268">[-]</label><label class="expand" for="c-42348268">[1 more]</label></div><br/><div class="children"><div class="content">While I&#x27;m sure the anti-AI people are taking this and running off with hot takes, the conclusion is still much more mundane: we currently do not have the ability to have an LLM learn from another LLM.<p>A suitably powerful AI <i>should</i> be able to do this though, by the example of the fact that humans learn by being taught by other humans (insert nuance of that process here).<p>So it&#x27;s an important result, but not a doomsday result because what it tells us is that LLM output fails to capture or stabilize important information from the training corpus and accurately communicate it to a newly trained LLM. So we know we&#x27;re missing something in how we construct these models, but the ramifications of solving it are also pretty immense: models being able to &quot;teach&quot; new models means the whole cycle of iteration can be sped up considerably.</div><br/></div></div><div id="42347845" class="c"><input type="checkbox" id="c-42347845" checked=""/><div class="controls bullet"><span class="by">mmastrac</span><span>|</span><a href="#42348268">prev</a><span>|</span><a href="#42347752">next</a><span>|</span><label class="collapse" for="c-42347845">[-]</label><label class="expand" for="c-42347845">[1 more]</label></div><br/><div class="children"><div class="content">All work and no play makes jack a dull boy.</div><br/></div></div><div id="42347752" class="c"><input type="checkbox" id="c-42347752" checked=""/><div class="controls bullet"><span class="by">alterom</span><span>|</span><a href="#42347845">prev</a><span>|</span><a href="#42347639">next</a><span>|</span><label class="collapse" for="c-42347752">[-]</label><label class="expand" for="c-42347752">[4 more]</label></div><br/><div class="children"><div class="content">The dignified way to describe the problem at hand is alluding to Brouwer&#x27;s fixed-point theorem[1], with white noise as the fixed point.<p>The more practical way is alluding to The Human Centipede[2].<p>Either way, the feed-back loop doesn&#x27;t result in a good output.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Brouwer_fixed-point_theorem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Brouwer_fixed-point_theorem</a><p>[2] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Human_Centipede_(First_Sequence)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Human_Centipede_(First_Seq...</a></div><br/><div id="42347900" class="c"><input type="checkbox" id="c-42347900" checked=""/><div class="controls bullet"><span class="by">benchmarkist</span><span>|</span><a href="#42347752">parent</a><span>|</span><a href="#42347639">next</a><span>|</span><label class="collapse" for="c-42347900">[-]</label><label class="expand" for="c-42347900">[3 more]</label></div><br/><div class="children"><div class="content">Also the data processing inequality: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Data_processing_inequality?useskin=vector" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Data_processing_inequality?use...</a></div><br/><div id="42348082" class="c"><input type="checkbox" id="c-42348082" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42347752">root</a><span>|</span><a href="#42347900">parent</a><span>|</span><a href="#42347639">next</a><span>|</span><label class="collapse" for="c-42348082">[-]</label><label class="expand" for="c-42348082">[2 more]</label></div><br/><div class="children"><div class="content">And yet I prefer now to early big bang era of the universe, though technically reversible.</div><br/><div id="42348124" class="c"><input type="checkbox" id="c-42348124" checked=""/><div class="controls bullet"><span class="by">benchmarkist</span><span>|</span><a href="#42347752">root</a><span>|</span><a href="#42348082">parent</a><span>|</span><a href="#42347639">next</a><span>|</span><label class="collapse" for="c-42348124">[-]</label><label class="expand" for="c-42348124">[1 more]</label></div><br/><div class="children"><div class="content">The universe is not a Markov chain, in fact, no one knows what it is but locally we do know that entropy increases and the inevitable endpoint in our corner of the universe is complete annihilation. Your preferences are completely irrelevant in the local scheme of things.</div><br/></div></div></div></div></div></div></div></div><div id="42347639" class="c"><input type="checkbox" id="c-42347639" checked=""/><div class="controls bullet"><span class="by">patrickhogan1</span><span>|</span><a href="#42347752">prev</a><span>|</span><label class="collapse" for="c-42347639">[-]</label><label class="expand" for="c-42347639">[6 more]</label></div><br/><div class="children"><div class="content">This argument seems more like the data generated was bad.  There are examples where AI has surpassed humans by using simulated data (AlphaZero - where it played against itself to become the best at Go).<p>It also seems to happen most on small networks. Which makes sense.<p>Additionally, humans create simulated stories like Dune, Lord of the Rings, or Harry Potter, which introduce fictional concepts, yet these stories still result in trainable data.</div><br/><div id="42347728" class="c"><input type="checkbox" id="c-42347728" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42347639">parent</a><span>|</span><a href="#42347753">next</a><span>|</span><label class="collapse" for="c-42347728">[-]</label><label class="expand" for="c-42347728">[1 more]</label></div><br/><div class="children"><div class="content">&gt; humans create simulated stories like Dune, Lord of the Rings, or Harry Potter<p>People <i>really</i> anthropomorphize LLM to a full circle, don&#x27;t they?</div><br/></div></div><div id="42347753" class="c"><input type="checkbox" id="c-42347753" checked=""/><div class="controls bullet"><span class="by">banku_brougham</span><span>|</span><a href="#42347639">parent</a><span>|</span><a href="#42347728">prev</a><span>|</span><a href="#42347922">next</a><span>|</span><label class="collapse" for="c-42347753">[-]</label><label class="expand" for="c-42347753">[1 more]</label></div><br/><div class="children"><div class="content">this is not a serious argument, please forgive me for saying</div><br/></div></div><div id="42347922" class="c"><input type="checkbox" id="c-42347922" checked=""/><div class="controls bullet"><span class="by">dudeinjapan</span><span>|</span><a href="#42347639">parent</a><span>|</span><a href="#42347753">prev</a><span>|</span><a href="#42347670">next</a><span>|</span><label class="collapse" for="c-42347922">[-]</label><label class="expand" for="c-42347922">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for making this comment, because it exposes some logical gaps.<p>Firstly, Go, Chess, and other games have objective rules and win criteria. (There is no “subjective opinion” as to whether Fischer or Spassky won their match.)<p>Language, the output of LLMs, does not have an objective function. Consider the following to sentences:<p>“My lips, two blushing pilgrims, ready stand.”<p>“My red lips are ready to kiss your red lips.”<p>Both are grammatically correct English sentences and both mean basically the same thing, but clearly the former by Shakespeare has a subjective poetic quality which the latter lacks. Even if we make evaluation rules to target (for example, “do not repeat phrases”, “use descriptive adjectives”, etc.) AI still seems to favor certain (for example “delve”) that are valid but not commonly used in human-originated English. There is then a positive feedback loop where these preferences are used to further train models, hence the next generation of models have no way of knowing whether the now frequent usage of “delve” is a human-originated or AI-originated phenomenon.<p>Lastly, regarding works of fiction, the concern is less about the content of stories—though that is also a concern—but more about the quality of language. (Consider above alternate take on Romeo and Juliet, for example.)</div><br/></div></div><div id="42347670" class="c"><input type="checkbox" id="c-42347670" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#42347639">parent</a><span>|</span><a href="#42347922">prev</a><span>|</span><label class="collapse" for="c-42347670">[-]</label><label class="expand" for="c-42347670">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Additionally, humans create simulated stories like Dune, Lord of the Rings, or Harry Potter, which introduce fictional concepts, yet these stories still result in trainable data.<p>No, they don&#x27;t, not in any sense where they are &quot;simulated data&quot;. Dune is simulated data about what life would be like on Arrakis, and if you train a model to make predictions about that question, your model will be worthless trash. (Doesn&#x27;t matter whether you train it on Dune or not.) Dune is <i>real</i> data about how English is used.</div><br/><div id="42347749" class="c"><input type="checkbox" id="c-42347749" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42347639">root</a><span>|</span><a href="#42347670">parent</a><span>|</span><label class="collapse" for="c-42347749">[-]</label><label class="expand" for="c-42347749">[1 more]</label></div><br/><div class="children"><div class="content">It’s also data around science fiction. With broad spectrum data from both dune and contextual data most LLMs know that dune is from a fictional novel.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>