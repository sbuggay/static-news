<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695459660752" as="style"/><link rel="stylesheet" href="styles.css?v=1695459660752"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2309.01933">Provably safe systems: the only path to controllable AGI</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>antonkar</span> | <span>24 comments</span></div><br/><div><div id="37621670" class="c"><input type="checkbox" id="c-37621670" checked=""/><div class="controls bullet"><span class="by">jdblair</span><span>|</span><a href="#37621266">next</a><span>|</span><label class="collapse" for="c-37621670">[-]</label><label class="expand" for="c-37621670">[2 more]</label></div><br/><div class="children"><div class="content">Reading the abstract, the first thing I thought was this question:<p>Is it moral to bring an artificial general intelligence into existence and then hobble the intelligence’s capability? This sounds a lot like creating a new sentience and then dooming it be slave to humanity.<p>I expect there are less extreme interpretations, and lurking just under the surface are deep questions like “what is free will? do we have free will?”</div><br/><div id="37621680" class="c"><input type="checkbox" id="c-37621680" checked=""/><div class="controls bullet"><span class="by">AgentME</span><span>|</span><a href="#37621670">parent</a><span>|</span><a href="#37621266">next</a><span>|</span><label class="collapse" for="c-37621680">[-]</label><label class="expand" for="c-37621680">[1 more]</label></div><br/><div class="children"><div class="content">Are we hobbled by the fact evolution made us into social creatures who value each other? If not, then I don&#x27;t think it&#x27;s wrong for us to make AI like that too.</div><br/></div></div></div></div><div id="37621266" class="c"><input type="checkbox" id="c-37621266" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#37621670">prev</a><span>|</span><a href="#37621543">next</a><span>|</span><label class="collapse" for="c-37621266">[-]</label><label class="expand" for="c-37621266">[6 more]</label></div><br/><div class="children"><div class="content">Hm.<p>Well, if this is correct, we should routinely be proving safety of ordinary systems long before we get to AI. I&#x27;d like to see formally verified routers, firewalls, mailers, and DNS servers, all of which have definable correct behavior, in wide use. That&#x27;s probably possible now.<p>Defining safe behavior for a LLM is a much harder problem. The paper handwaves this.<p>* Mortal AI. Has death date. Does not require proof, just a hardware timer or limited battery life.<p>* Geofenced AI. Only useful for mobile machines. Not helpful against things which can communicate.<p>* Throttled AI. You have to keep putting in crypto tokens to keep it going. OK, whatever.<p>* AI kill switch. Off switch.<p>* Asimov-style laws. Not an inherently bad idea, but way too ambiguous to rigorously formalize. 
Go read Asimov&#x27;s robot books again. Useful metric: what similar set of bright-line constraints could usefully be enforced on corporations?<p>It&#x27;s worth bearing in mind that most of the problems of regulating AIs apply to regulating corporations, which can be thought of as AIs with slow internal data transfer.</div><br/><div id="37621534" class="c"><input type="checkbox" id="c-37621534" checked=""/><div class="controls bullet"><span class="by">nonrandomstring</span><span>|</span><a href="#37621266">parent</a><span>|</span><a href="#37621307">next</a><span>|</span><label class="collapse" for="c-37621534">[-]</label><label class="expand" for="c-37621534">[1 more]</label></div><br/><div class="children"><div class="content">Formal methods and program proving for safety critical systems is
something I had a little experience with back in the day. We used VDM
and Z, to get &quot;full state maps&quot; that then got coded in Ada. Most of
that was medical&#x2F;defence stuff.<p>Probably the field has advanced hugely since my 1990s take on it but
as I knew it:<p>It only worked for quite small programs. You had to build a modular
system out of many provable units. Enumerating all the ways they could
interact becomes impossible beyond a handful of variables.<p>It&#x27;s was time consuming and laborious. We had this awful but
necessary waterfall model with stage after stage of review and
approval and feedback to correct modules.<p>So my idea of formal methods seems at odds with how I understand
current AI as massively multi-valued.<p>It seems all you could do is place formal <i>constraints</i> on a wild
system, like caging a beast. Anything remotely &quot;intelligent&quot; would try
to break out of that... and we&#x27;re back to square one.<p>Can anyone who is versed in modern formal methods say more about how
an AI can be formally <i>designed</i> (rather than grown by training)?  Or
is this, as I suspect, where two incompatible worlds simply collide?</div><br/></div></div><div id="37621307" class="c"><input type="checkbox" id="c-37621307" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#37621266">parent</a><span>|</span><a href="#37621534">prev</a><span>|</span><a href="#37621315">next</a><span>|</span><label class="collapse" for="c-37621307">[-]</label><label class="expand" for="c-37621307">[2 more]</label></div><br/><div class="children"><div class="content">I think what&#x27;s holding back the correct firewalls, mailers, etc is that the whole computing landscape is so unsound, and fixing just one level of the stack doesn&#x27;t give big payoffs. And the market mechanism doesn&#x27;t know how to climb that hill especially since it&#x27;s been baked into people&#x27;s assumptions and mental models.</div><br/><div id="37621517" class="c"><input type="checkbox" id="c-37621517" checked=""/><div class="controls bullet"><span class="by">mpweiher</span><span>|</span><a href="#37621266">root</a><span>|</span><a href="#37621307">parent</a><span>|</span><a href="#37621315">next</a><span>|</span><label class="collapse" for="c-37621517">[-]</label><label class="expand" for="c-37621517">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Beware of bugs in the above code; I have only proved it correct, not tried it.&quot;<p><pre><code>   Donald Knuth
</code></pre>
--<p>&gt; doesn&#x27;t give big payoffs.<p>I think that&#x27;s the key here.  The idea that proving things correct means they are flawless is...fancyful.  And the expense is very high.</div><br/></div></div></div></div><div id="37621315" class="c"><input type="checkbox" id="c-37621315" checked=""/><div class="controls bullet"><span class="by">nanolith</span><span>|</span><a href="#37621266">parent</a><span>|</span><a href="#37621307">prev</a><span>|</span><a href="#37621543">next</a><span>|</span><label class="collapse" for="c-37621315">[-]</label><label class="expand" for="c-37621315">[2 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s probably possible now.<p>It&#x27;s definitely possible now. Formal methods continues to shift from the theoretical, academic, and difficult to the practical. There are model checkers based on SMT solvers that can be used today to enforce function contracts. It is getting easier to extract software for more complicated logic (e.g. data structures and algorithms) using proof assistants.<p>I can&#x27;t really speak for ML and other forms of AI, because I have not attempted to design such a system with formal methods in mind. But, traditional software can be made safer with formal methods today.</div><br/><div id="37621441" class="c"><input type="checkbox" id="c-37621441" checked=""/><div class="controls bullet"><span class="by">antonvs</span><span>|</span><a href="#37621266">root</a><span>|</span><a href="#37621315">parent</a><span>|</span><a href="#37621543">next</a><span>|</span><label class="collapse" for="c-37621441">[-]</label><label class="expand" for="c-37621441">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a big fan of formal methods, but we have to recognize that their use is still very limited, and that&#x27;s a function of the difficulty of using formal methods effectively.<p>The idea that they could be used to prove the kind of properties claimed in the paper, essentially by throwing AI at the problem, is hand-waving of the fluffiest order.</div><br/></div></div></div></div></div></div><div id="37621543" class="c"><input type="checkbox" id="c-37621543" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#37621266">prev</a><span>|</span><a href="#37621391">next</a><span>|</span><label class="collapse" for="c-37621543">[-]</label><label class="expand" for="c-37621543">[1 more]</label></div><br/><div class="children"><div class="content">I attended Max&#x27;s recent lecture at Harvard, where they were recruiting for their AI safety fellowship; a friend of mine captured a good video, 54 minutes:<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;st9J6GefWeY" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;st9J6GefWeY</a><p>I think it&#x27;s a good approach, we should use AI to write proof carrying code. I think we&#x27;ll want AI to do things beyond what we can prove is safe, but maybe we shouldn&#x27;t.<p>I think some commenters here are being too dismissive, Max came across to me as a provacatuer, making proposals that are not sure things, but whose rebuttals will advance the field.</div><br/></div></div><div id="37621391" class="c"><input type="checkbox" id="c-37621391" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#37621543">prev</a><span>|</span><a href="#37621681">next</a><span>|</span><label class="collapse" for="c-37621391">[-]</label><label class="expand" for="c-37621391">[1 more]</label></div><br/><div class="children"><div class="content">Tegmark&#x27;s thinking here is extremely shallow, discards the costs (opportunity costs and risks of stable dystopia) associated with this grandiose global project of dubious feasibility, and indeed I suspect he does not so much believe his own arguments as he generally prefers global technocratic regulation &quot;for good measure&quot;. We don&#x27;t really have good reasons to expect uncontrollable AGI in a way they posit (increasingly less as we advance down this path of ML model scaling), but we are well acquainted with unaccountable human power structures.</div><br/></div></div><div id="37621681" class="c"><input type="checkbox" id="c-37621681" checked=""/><div class="controls bullet"><span class="by">ogogmad</span><span>|</span><a href="#37621391">prev</a><span>|</span><a href="#37620882">next</a><span>|</span><label class="collapse" for="c-37621681">[-]</label><label class="expand" for="c-37621681">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s time to do a postmortem on formal methods.<p>The problem that needed solving: Computer programs make mistakes because they&#x27;re too literal and they don&#x27;t understand a programmer&#x27;s intentions.<p>The problem they tried to solve instead: Computer programs make mistakes because we don&#x27;t write out mathematical formulas detailing how programs should behave, and mathematically prove that our programs obey said formulas.<p>How they tried solving it: Ignored that the problem they tried to solve is not the problem they were given. Spent 50 years not producing nearly any working product, except for final-year-projects done by their Bachelor&#x27;s students. Published endless papers in conferences and journals.</div><br/></div></div><div id="37620882" class="c"><input type="checkbox" id="c-37620882" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#37621681">prev</a><span>|</span><a href="#37621421">next</a><span>|</span><label class="collapse" for="c-37620882">[-]</label><label class="expand" for="c-37620882">[10 more]</label></div><br/><div class="children"><div class="content">It&#x27;s probably the best approach known so far, but for all their talk of &quot;long termism&quot;, I predict many people will dislike the fact that they&#x27;ll almost certainly be dead (and in an unrecoverable state, before anyone gets their hopes up) before the project gets completed after the 40+ years and trillions of dollars it will take even in the optimistic case, i.e. for them personally there&#x27;s no difference between no AGI&#x2F;ASI and aligned AGI&#x2F;ASI, as it will take too long.<p>Arguments about supposed &quot;races with China&quot; will probably be deployed against this idea, but the only thing the leaders of the AI labs will be racing against is their own deaths and each other.</div><br/><div id="37621205" class="c"><input type="checkbox" id="c-37621205" checked=""/><div class="controls bullet"><span class="by">antonvs</span><span>|</span><a href="#37620882">parent</a><span>|</span><a href="#37621220">next</a><span>|</span><label class="collapse" for="c-37621205">[-]</label><label class="expand" for="c-37621205">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s probably the best approach known so far<p>Why do you think that?<p>I love this line from the paper:<p>&gt; &quot;So, if a person or organization wants to be sure that their AGI never lies, never escapes and never invents bioweapons, they need to impose those requirements and never run versions that don’t provably obey them.&quot;<p>Oh is that all.</div><br/><div id="37621359" class="c"><input type="checkbox" id="c-37621359" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#37620882">root</a><span>|</span><a href="#37621205">parent</a><span>|</span><a href="#37621220">next</a><span>|</span><label class="collapse" for="c-37621359">[-]</label><label class="expand" for="c-37621359">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the best option, even assuming no one raced and you got full funding and enthusiasm, it would be plausible to put success at 40%. That&#x27;s after a generation of work and trillions of (2022, probably) dollars, but it&#x27;s sure better than the ~0% chance I&#x27;d give anything else. The coordination problem for getting AI using this method and the coordination problem to prevent anyone getting AI at all is quite similar though, but probably somewhat more preferable.</div><br/><div id="37621415" class="c"><input type="checkbox" id="c-37621415" checked=""/><div class="controls bullet"><span class="by">antonvs</span><span>|</span><a href="#37620882">root</a><span>|</span><a href="#37621359">parent</a><span>|</span><a href="#37621220">next</a><span>|</span><label class="collapse" for="c-37621415">[-]</label><label class="expand" for="c-37621415">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s the best option<p>That&#x27;s not an answer.<p>&gt; it would be plausible to put success at 40%.<p>No, it wouldn&#x27;t. You just made up that number based on nothing.<p>We have a good amount of experience with provability in software, and its limitations are well-known. Tegmark et al. seem to me just to be waving the magic pixie dust of &quot;AI will solve this&quot;, and making unlikely claims, with no real substance.<p>I was asking what about what they&#x27;re saying makes you think that &quot;it&#x27;s the best option&quot;, but it&#x27;s clear I&#x27;m not going to get a useful answer from you.</div><br/></div></div></div></div></div></div><div id="37621220" class="c"><input type="checkbox" id="c-37621220" checked=""/><div class="controls bullet"><span class="by">rsaarelm</span><span>|</span><a href="#37620882">parent</a><span>|</span><a href="#37621205">prev</a><span>|</span><a href="#37621177">next</a><span>|</span><label class="collapse" for="c-37621220">[-]</label><label class="expand" for="c-37621220">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I predict many people will dislike the fact that they&#x27;ll almost certainly be dead (and in an unrecoverable state, before anyone gets their hopes up)<p>Cryonics is a thing already.</div><br/><div id="37621379" class="c"><input type="checkbox" id="c-37621379" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#37620882">root</a><span>|</span><a href="#37621220">parent</a><span>|</span><a href="#37621605">next</a><span>|</span><label class="collapse" for="c-37621379">[-]</label><label class="expand" for="c-37621379">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I don&#x27;t know. Gwern thinks it would work if done near perfectly, but in practice what gets done is freezing dead bodies in an already pretty much unrecoverable state (if not already dead and warm for hours if not days), with no precautions (or only &quot;feel good&quot; non-functioning precautions) against microscopic ice spike formation, and then at some point someone screws something up and you get a temperature excursion and then there&#x27;s nothing left information recovery wise. I don&#x27;t agree with Gwern that current techniques would work even in theory.</div><br/></div></div><div id="37621605" class="c"><input type="checkbox" id="c-37621605" checked=""/><div class="controls bullet"><span class="by">KineticLensman</span><span>|</span><a href="#37620882">root</a><span>|</span><a href="#37621220">parent</a><span>|</span><a href="#37621379">prev</a><span>|</span><a href="#37621422">next</a><span>|</span><label class="collapse" for="c-37621605">[-]</label><label class="expand" for="c-37621605">[1 more]</label></div><br/><div class="children"><div class="content">Cryonics is a backup mechanism where nobody has yet implemented the ‘restore’ function.</div><br/></div></div><div id="37621422" class="c"><input type="checkbox" id="c-37621422" checked=""/><div class="controls bullet"><span class="by">antonvs</span><span>|</span><a href="#37620882">root</a><span>|</span><a href="#37621220">parent</a><span>|</span><a href="#37621605">prev</a><span>|</span><a href="#37621177">next</a><span>|</span><label class="collapse" for="c-37621422">[-]</label><label class="expand" for="c-37621422">[1 more]</label></div><br/><div class="children"><div class="content">There are meat popsicles in freezers, but they&#x27;ll almost certainly never be conscious again.</div><br/></div></div></div></div><div id="37621177" class="c"><input type="checkbox" id="c-37621177" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#37620882">parent</a><span>|</span><a href="#37621220">prev</a><span>|</span><a href="#37621421">next</a><span>|</span><label class="collapse" for="c-37621177">[-]</label><label class="expand" for="c-37621177">[2 more]</label></div><br/><div class="children"><div class="content">&gt;for them personally there&#x27;s no difference between no AGI&#x2F;ASI and aligned AGI&#x2F;ASI<p>Not really true. There are things worse than death and every AI researcher who is width their salt knows this.<p>Human level intelligence already inflicts not only great amounts of pain but drawn out, unusual, and unnatural durations of pain. So we know artificially enhanced suffering is a risk of intelligence.</div><br/><div id="37621203" class="c"><input type="checkbox" id="c-37621203" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#37620882">root</a><span>|</span><a href="#37621177">parent</a><span>|</span><a href="#37621421">next</a><span>|</span><label class="collapse" for="c-37621203">[-]</label><label class="expand" for="c-37621203">[1 more]</label></div><br/><div class="children"><div class="content">I did not say there was no difference between the other pairs, obviously there&#x27;s a difference between aligned AI and no AI, and between unaligned AI and aligned AI, I&#x27;m saying if aligned AI takes so long to create that you&#x27;d be dead first, to you personally it&#x27;s the same as it not existing.</div><br/></div></div></div></div></div></div><div id="37621421" class="c"><input type="checkbox" id="c-37621421" checked=""/><div class="controls bullet"><span class="by">awestroke</span><span>|</span><a href="#37620882">prev</a><span>|</span><a href="#37621563">next</a><span>|</span><label class="collapse" for="c-37621421">[-]</label><label class="expand" for="c-37621421">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Controllable AGI&quot; seems like an oxymoron. If you have created a true AGI, and then brainwash&#x2F;lobotomize it enough to make it 100% controllable&#x2F;safe, then it will no longer be a true AGI</div><br/></div></div><div id="37621563" class="c"><input type="checkbox" id="c-37621563" checked=""/><div class="controls bullet"><span class="by">moomin</span><span>|</span><a href="#37621421">prev</a><span>|</span><label class="collapse" for="c-37621563">[-]</label><label class="expand" for="c-37621563">[1 more]</label></div><br/><div class="children"><div class="content">Anyone feel like the authors haven’t read “The Naked Sun”?</div><br/></div></div></div></div></div></div></div></body></html>