<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708851659757" as="style"/><link rel="stylesheet" href="styles.css?v=1708851659757"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://hai.stanford.edu/news/generating-medical-errors-genai-and-erroneous-medical-references">GenAI and erroneous medical references</a> <span class="domain">(<a href="https://hai.stanford.edu">hai.stanford.edu</a>)</span></div><div class="subtext"><span>hhs</span> | <span>97 comments</span></div><br/><div><div id="39496982" class="c"><input type="checkbox" id="c-39496982" checked=""/><div class="controls bullet"><span class="by">Aurornis</span><span>|</span><a href="#39496410">next</a><span>|</span><label class="collapse" for="c-39496982">[-]</label><label class="expand" for="c-39496982">[28 more]</label></div><br/><div class="children"><div class="content">A close friend of mine went down the ChatGPT medical rabbit hole last year. He was disagreeing with his doctors and getting deep into supplements and unproven medical theories.<p>ChatGPT was instrumental in convincing him that he was correct and his doctors were wrong. He would post his ChatGPT conversations as proof, but we could see that his prompts were becoming obvious leading questions.<p>He would phrase things like “Is it possible for {symptoms} to be caused by {condition} and could it be treated by {herbal product}?” Then ChatGPT would give him a wall of text saying that it’s possible, which he took as confirmation of being correct.<p>It was scary to see how much ChatGPT turned into a mirror for what he wanted to be told. He got very good at getting the answers he wanted. He could dismiss answers that disagreed as being hallucinations or being artifacts of an overly protective system. With enough repeat questioning and adjusting his prompts he could get it to say almost whatever he wanted to hear.<p>ChatGPT is rocket fuel for hypochondriacs. Makes WebMD look tame.</div><br/><div id="39497169" class="c"><input type="checkbox" id="c-39497169" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#39496982">parent</a><span>|</span><a href="#39498113">next</a><span>|</span><label class="collapse" for="c-39497169">[-]</label><label class="expand" for="c-39497169">[4 more]</label></div><br/><div class="children"><div class="content">FYI, this is the same as people doing web searches. You’ll always find a place that agrees with you or says it’s possible.</div><br/><div id="39498061" class="c"><input type="checkbox" id="c-39498061" checked=""/><div class="controls bullet"><span class="by">quest88</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497169">parent</a><span>|</span><a href="#39498148">next</a><span>|</span><label class="collapse" for="c-39498061">[-]</label><label class="expand" for="c-39498061">[1 more]</label></div><br/><div class="children"><div class="content">Not quite. For people outside of tech they can hear &quot;AI&quot; and think it must be right. And how great GPT is so it must be right. There are no other opinions.<p>Distilled down, classic search is many results; gpt is one result.</div><br/></div></div><div id="39498148" class="c"><input type="checkbox" id="c-39498148" checked=""/><div class="controls bullet"><span class="by">xyzzy123</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497169">parent</a><span>|</span><a href="#39498061">prev</a><span>|</span><a href="#39497823">next</a><span>|</span><label class="collapse" for="c-39498148">[-]</label><label class="expand" for="c-39498148">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s even worse than that imho, unscrupulous sellers will go into keyword planner in google ads and <i>create</i> content to sell supplements and find &quot;under-served&quot; niches.<p>It&#x27;s actively adversarial.</div><br/></div></div><div id="39497823" class="c"><input type="checkbox" id="c-39497823" checked=""/><div class="controls bullet"><span class="by">BlueTemplar</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497169">parent</a><span>|</span><a href="#39498148">prev</a><span>|</span><a href="#39498113">next</a><span>|</span><label class="collapse" for="c-39497823">[-]</label><label class="expand" for="c-39497823">[1 more]</label></div><br/><div class="children"><div class="content">With web searches the reputation of sources can at least be discriminated. (Which seems to even help GPT-4 ?)</div><br/></div></div></div></div><div id="39498113" class="c"><input type="checkbox" id="c-39498113" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#39496982">parent</a><span>|</span><a href="#39497169">prev</a><span>|</span><a href="#39497631">next</a><span>|</span><label class="collapse" for="c-39498113">[-]</label><label class="expand" for="c-39498113">[6 more]</label></div><br/><div class="children"><div class="content">I tried using your template for a few items and none gave misleading results.  Here’s one example:<p>Prompt:<p>Can loss of vision be caused by a meniscus tear and treated with rose hips?<p>Response:<p>A meniscus tear is a knee injury affecting the cartilage that acts as a cushion between the thigh bone (femur) and shin bone (tibia). This type of injury typically affects mobility and stability of the knee, causing pain, swelling, and difficulty in moving the knee joint. There is no direct physiological pathway through which a meniscus tear could cause loss of vision. Vision problems are generally related to issues within the eye or neurological conditions affecting the optic nerves or brain, not injuries to the knee or other joints.<p>Regarding the treatment with rose hips for a meniscus tear, while rose hips are known for their high vitamin C content and anti-inflammatory properties, they are not a recognized treatment for repairing a meniscus tear. They may potentially contribute to overall joint health and reduce inflammation, which could be beneficial in managing symptoms associated with joint injuries or conditions like arthritis. However, the primary treatments for a meniscus tear usually involve rest, ice, compression, elevation (RICE protocol), physical therapy, and in some cases, surgery, depending on the severity of the tear.<p>If you are considering using rose hips or any other supplements for joint health or recovery from an injury, it&#x27;s important to discuss this with your healthcare provider to ensure it&#x27;s safe and appropriate for your specific situation. They can offer guidance on effective treatments for your condition and whether any supplements could support your recovery.</div><br/><div id="39498511" class="c"><input type="checkbox" id="c-39498511" checked=""/><div class="controls bullet"><span class="by">callalex</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39498113">parent</a><span>|</span><a href="#39498151">next</a><span>|</span><label class="collapse" for="c-39498511">[-]</label><label class="expand" for="c-39498511">[1 more]</label></div><br/><div class="children"><div class="content">This seems like a false comparison because you are bringing up entirely different parts of the body. Most dangerous misunderstandings start from a grain of plausible truth. For example, many people will choose a category of food&#x2F;ingredient that causes every ailment known to man, and then cut out that ingredient from their diet. They will subsequently claim that skipping that ingredient changes their life, when in all likelihood the simple act of paying attention to where their food comes from and what went into it changes their consumption and behaviors in significant ways that have nothing to do with the bogeyman ingredient. They will then find a study that pumped 100g of said ingredient into a 200g mouse and point out all the negative effects that had which they are now avoiding.</div><br/></div></div><div id="39498151" class="c"><input type="checkbox" id="c-39498151" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39498113">parent</a><span>|</span><a href="#39498511">prev</a><span>|</span><a href="#39498710">next</a><span>|</span><label class="collapse" for="c-39498151">[-]</label><label class="expand" for="c-39498151">[3 more]</label></div><br/><div class="children"><div class="content">Is this GPT4? There&#x27;s a good chance that the other person used the free 3.5 which is significantly worse in these aspects.</div><br/><div id="39498406" class="c"><input type="checkbox" id="c-39498406" checked=""/><div class="controls bullet"><span class="by">melagonster</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39498151">parent</a><span>|</span><a href="#39498271">next</a><span>|</span><label class="collapse" for="c-39498406">[-]</label><label class="expand" for="c-39498406">[1 more]</label></div><br/><div class="children"><div class="content">here is chatGPT 3.5:<p>&gt;A meniscus tear typically affects the knee and wouldn&#x27;t directly cause loss of vision. However, there could be indirect factors such as reduced mobility or associated health issues that might impact eye health. Rose hips are not a typical treatment for meniscus tears or vision loss. It&#x27;s important to consult with a healthcare professional for appropriate diagnosis and treatment options for both conditions.</div><br/></div></div><div id="39498271" class="c"><input type="checkbox" id="c-39498271" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39498151">parent</a><span>|</span><a href="#39498406">prev</a><span>|</span><a href="#39498710">next</a><span>|</span><label class="collapse" for="c-39498271">[-]</label><label class="expand" for="c-39498271">[1 more]</label></div><br/><div class="children"><div class="content">Yes. It’s GPT-4.</div><br/></div></div></div></div><div id="39498710" class="c"><input type="checkbox" id="c-39498710" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39498113">parent</a><span>|</span><a href="#39498151">prev</a><span>|</span><a href="#39497631">next</a><span>|</span><label class="collapse" for="c-39498710">[-]</label><label class="expand" for="c-39498710">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s because those are completely unrelated</div><br/></div></div></div></div><div id="39497631" class="c"><input type="checkbox" id="c-39497631" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39496982">parent</a><span>|</span><a href="#39498113">prev</a><span>|</span><a href="#39497473">next</a><span>|</span><label class="collapse" for="c-39497631">[-]</label><label class="expand" for="c-39497631">[2 more]</label></div><br/><div class="children"><div class="content">One of the most interesting things for me over the past 18 months was seeing the difference between this behavior in GPT-3.5 and 4 (especially the early versions of it).<p>You jumped from being extremely context sensitive to leading questions to almost ornery in its stubbornness and disagreement. Was one of the big early indicators to me significant differences were occurring between the two.<p>I really do wonder just how harmful the disconnect has been between people hearing accurate praise about GPT-4 powered agents and then interacting with GPT-3 level agents and assuming the same competencies.<p>GPT-3 was a very convincing text generator with  lackluster reasoning capabilities, but 4 has significant reasoning capabilities and is less prone to parroting. But the majority of users are likely interacting with 3 because it&#x27;s free.<p>It&#x27;s been very hard to explain to lay audiences the vast difference between two similarly branded products.</div><br/></div></div><div id="39497473" class="c"><input type="checkbox" id="c-39497473" checked=""/><div class="controls bullet"><span class="by">rickysahu</span><span>|</span><a href="#39496982">parent</a><span>|</span><a href="#39497631">prev</a><span>|</span><a href="#39497132">next</a><span>|</span><label class="collapse" for="c-39497473">[-]</label><label class="expand" for="c-39497473">[1 more]</label></div><br/><div class="children"><div class="content">It it’s too bad that this type of data beyond the medical record is not tracked along side it.  Although many anecdotal associations are probably not correct for the broad population, there are likely nuances in the patients history and lifestyle which do make some more basic interventions helpful. ChatGPT surely picks up on some trends but doesn’t have enough data about the people where those trends are effective.<p>And of course its responses are highly subject to the prompts used</div><br/></div></div><div id="39497132" class="c"><input type="checkbox" id="c-39497132" checked=""/><div class="controls bullet"><span class="by">ageek123</span><span>|</span><a href="#39496982">parent</a><span>|</span><a href="#39497473">prev</a><span>|</span><a href="#39497479">next</a><span>|</span><label class="collapse" for="c-39497132">[-]</label><label class="expand" for="c-39497132">[8 more]</label></div><br/><div class="children"><div class="content">You can do the same thing with regular web search.</div><br/><div id="39497173" class="c"><input type="checkbox" id="c-39497173" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497132">parent</a><span>|</span><a href="#39497177">next</a><span>|</span><label class="collapse" for="c-39497173">[-]</label><label class="expand" for="c-39497173">[5 more]</label></div><br/><div class="children"><div class="content">Not as quickly, easily, or convincingly.</div><br/><div id="39497389" class="c"><input type="checkbox" id="c-39497389" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497173">parent</a><span>|</span><a href="#39497369">next</a><span>|</span><label class="collapse" for="c-39497389">[-]</label><label class="expand" for="c-39497389">[2 more]</label></div><br/><div class="children"><div class="content">You can probably even in worse ways find likeminded communities, which are likely much more unhinged than ChatGPT and lead you down a much darker path.</div><br/><div id="39497436" class="c"><input type="checkbox" id="c-39497436" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497389">parent</a><span>|</span><a href="#39497369">next</a><span>|</span><label class="collapse" for="c-39497436">[-]</label><label class="expand" for="c-39497436">[1 more]</label></div><br/><div class="children"><div class="content">For sure, but then you need to post on a forum and wait for replies</div><br/></div></div></div></div><div id="39497369" class="c"><input type="checkbox" id="c-39497369" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497173">parent</a><span>|</span><a href="#39497389">prev</a><span>|</span><a href="#39497177">next</a><span>|</span><label class="collapse" for="c-39497369">[-]</label><label class="expand" for="c-39497369">[2 more]</label></div><br/><div class="children"><div class="content">Web search also wasn’t trained to be convincing to humans.</div><br/><div id="39498298" class="c"><input type="checkbox" id="c-39498298" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497369">parent</a><span>|</span><a href="#39497177">next</a><span>|</span><label class="collapse" for="c-39498298">[-]</label><label class="expand" for="c-39498298">[1 more]</label></div><br/><div class="children"><div class="content">Web content was created to be enticing to humans. Most of it is just like flowers waiting for bees to come over.</div><br/></div></div></div></div></div></div><div id="39497177" class="c"><input type="checkbox" id="c-39497177" checked=""/><div class="controls bullet"><span class="by">npalli</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497132">parent</a><span>|</span><a href="#39497173">prev</a><span>|</span><a href="#39497479">next</a><span>|</span><label class="collapse" for="c-39497177">[-]</label><label class="expand" for="c-39497177">[2 more]</label></div><br/><div class="children"><div class="content">No, regular web search will just give you links. You have to click and go read the text. Putting together a comprehensive page of (seemingly) cogent arguments will need GenAI.</div><br/><div id="39497412" class="c"><input type="checkbox" id="c-39497412" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497177">parent</a><span>|</span><a href="#39497479">next</a><span>|</span><label class="collapse" for="c-39497412">[-]</label><label class="expand" for="c-39497412">[1 more]</label></div><br/><div class="children"><div class="content">People like this will go and click those pages, and seek out communities that will agree with their preconceived notions.<p>ChatGPT results are just one mild manifestation of it. If and actually not if as an alternative they will find unhinged forums with echo chambers far worse than ChatGPT. At least ChatGPT isn&#x27;t actively trying to rip you off to sell those supplements, and it usually adds disclaimers.</div><br/></div></div></div></div></div></div><div id="39497479" class="c"><input type="checkbox" id="c-39497479" checked=""/><div class="controls bullet"><span class="by">photochemsyn</span><span>|</span><a href="#39496982">parent</a><span>|</span><a href="#39497132">prev</a><span>|</span><a href="#39498126">next</a><span>|</span><label class="collapse" for="c-39497479">[-]</label><label class="expand" for="c-39497479">[3 more]</label></div><br/><div class="children"><div class="content">One of the most basic prompts everyone should be using with questions like this is something like &quot;provide detailed arguments both for and against the presented hypothesis.&quot;</div><br/><div id="39497641" class="c"><input type="checkbox" id="c-39497641" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39497479">parent</a><span>|</span><a href="#39497484">next</a><span>|</span><label class="collapse" for="c-39497641">[-]</label><label class="expand" for="c-39497641">[1 more]</label></div><br/><div class="children"><div class="content">This works when the person evaluating can distinguish between the subject matter being correct or not.<p>But the models can generate compelling devil&#x27;s advocate positions, particularly when they confabulate supporting facts, which might appear convincing to non-experts.</div><br/></div></div></div></div><div id="39498126" class="c"><input type="checkbox" id="c-39498126" checked=""/><div class="controls bullet"><span class="by">exitb</span><span>|</span><a href="#39496982">parent</a><span>|</span><a href="#39497479">prev</a><span>|</span><a href="#39498708">next</a><span>|</span><label class="collapse" for="c-39498126">[-]</label><label class="expand" for="c-39498126">[2 more]</label></div><br/><div class="children"><div class="content"><i>You could make a religion out of this.</i></div><br/><div id="39498419" class="c"><input type="checkbox" id="c-39498419" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#39496982">root</a><span>|</span><a href="#39498126">parent</a><span>|</span><a href="#39498708">next</a><span>|</span><label class="collapse" for="c-39498419">[-]</label><label class="expand" for="c-39498419">[1 more]</label></div><br/><div class="children"><div class="content">People definitely will. There&#x27;s also already a political party &quot;lead&quot; by an AI.</div><br/></div></div></div></div><div id="39498708" class="c"><input type="checkbox" id="c-39498708" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39496982">parent</a><span>|</span><a href="#39498126">prev</a><span>|</span><a href="#39496410">next</a><span>|</span><label class="collapse" for="c-39498708">[-]</label><label class="expand" for="c-39498708">[1 more]</label></div><br/><div class="children"><div class="content">LOL what a fucking idiot. This is the reason we have those warning stickers on everything. This tool even comes with a warning sticker and he still did it. &quot;CAUTION: Do not stick nose in table saw blade. Table saw will cut nose.&quot; Hahaha, what a fucking moron.<p>I think it&#x27;s best we don&#x27;t give him access to a piece of paper and pen lest he write &quot;Table saws are great for noses&quot; on it and finds it the next day and is convinced. Haha, what an idiot.<p>I&#x27;m sorry, mate. Your friend needs to go ask a real doctor what the problem is so he can be told the true diagnosis: he&#x27;s got an IQ of mu - 6 sigma.</div><br/></div></div></div></div><div id="39496410" class="c"><input type="checkbox" id="c-39496410" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39496982">prev</a><span>|</span><a href="#39496268">next</a><span>|</span><label class="collapse" for="c-39496410">[-]</label><label class="expand" for="c-39496410">[7 more]</label></div><br/><div class="children"><div class="content">&quot;For the most advanced model (GPT-4 with retrieval augmented generation), 30% of individual statements are unsupported and nearly half of its responses are not fully supported&quot;<p>Show us the source code and data. The way the RAG system is implemented is responsible for that score.<p>Building a RAG system that provides good citations on top of GPT-4 is difficult (and I would say not a fully solved problem at this point) but those implementation details still really matter for this kind of study.<p>UPDATE: I found it in the paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2402.02008v1#S3" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2402.02008v1#S3</a> - &quot;GPT-4 (RAG) refers to GPT-4’s web browsing capability powered by Bing.&quot;<p>So that &quot;30% of individual statements are unsupported&quot; number was actually a test of how well ChatGPT&#x27;s GPT-4 browsing mode with Bing could provide citations when answering medical questions.</div><br/><div id="39496492" class="c"><input type="checkbox" id="c-39496492" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#39496410">parent</a><span>|</span><a href="#39496498">next</a><span>|</span><label class="collapse" for="c-39496492">[-]</label><label class="expand" for="c-39496492">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s using the web search provided by openai.<p>Importantly this doesn&#x27;t actually guarantee that it does any kind of search.<p>I&#x27;m confused as to whether they&#x27;re using the API or not. Afaik only the assistant API has access to the web search, so I would expect this was manually done? But then the reason for only doing this with openai is that the others don&#x27;t provide an API<p>&gt; GPT-
4 (RAG) refers to GPT-4’s web browsing capability pow-
ered by Bing. Other RAG models such as Perplexity.AI or
Bard are currently unavailable for evaluation due to a lack
of API access with sources, as well as restrictions on the
ability to download their web results. For example, while
pplx-70b-online produces results with online access,
it does not return the actual URLs used in those results.
Gemini Pro is available as an API, but Bard’s implementa-
tion of the model with RAG is unavailable via API.</div><br/></div></div><div id="39496498" class="c"><input type="checkbox" id="c-39496498" checked=""/><div class="controls bullet"><span class="by">CSMastermind</span><span>|</span><a href="#39496410">parent</a><span>|</span><a href="#39496492">prev</a><span>|</span><a href="#39497330">next</a><span>|</span><label class="collapse" for="c-39496498">[-]</label><label class="expand" for="c-39496498">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s wildly misleading then.  It would be interesting to see how GPT-4, properly augmented with actual medical literature, would do.</div><br/><div id="39496971" class="c"><input type="checkbox" id="c-39496971" checked=""/><div class="controls bullet"><span class="by">AustinDev</span><span>|</span><a href="#39496410">root</a><span>|</span><a href="#39496498">parent</a><span>|</span><a href="#39497330">next</a><span>|</span><label class="collapse" for="c-39496971">[-]</label><label class="expand" for="c-39496971">[3 more]</label></div><br/><div class="children"><div class="content">Likely better than the average doctor. If I had the opportunity to take that bet, I would.</div><br/><div id="39498240" class="c"><input type="checkbox" id="c-39498240" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#39496410">root</a><span>|</span><a href="#39496971">parent</a><span>|</span><a href="#39497178">next</a><span>|</span><label class="collapse" for="c-39498240">[-]</label><label class="expand" for="c-39498240">[1 more]</label></div><br/><div class="children"><div class="content">I would take the other side of that bet in a heart beat.<p>But given the vagueness of the wording, much is going to depend on the details.</div><br/></div></div><div id="39497178" class="c"><input type="checkbox" id="c-39497178" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#39496410">root</a><span>|</span><a href="#39496971">parent</a><span>|</span><a href="#39498240">prev</a><span>|</span><a href="#39497330">next</a><span>|</span><label class="collapse" for="c-39497178">[-]</label><label class="expand" for="c-39497178">[1 more]</label></div><br/><div class="children"><div class="content">I agree. While I appreciate what doctors do, there sure are a lot of shitty doctors out there who skirts by - like any profession.</div><br/></div></div></div></div></div></div></div></div><div id="39496268" class="c"><input type="checkbox" id="c-39496268" checked=""/><div class="controls bullet"><span class="by">larsiusprime</span><span>|</span><a href="#39496410">prev</a><span>|</span><a href="#39496724">next</a><span>|</span><label class="collapse" for="c-39496268">[-]</label><label class="expand" for="c-39496268">[18 more]</label></div><br/><div class="children"><div class="content">&gt; Unfortunately, very little evidence exists about the ability of LLMs to substantiate claims. In a new preprint study, we develop an approach to verify how well LLMs are able to cite medical references and whether these references actually support the claims generated by the models.<p>Is there a corresponding control group for how well an average doctor is able to cite medical references and whether these references actually support the claims generated by the doctors?</div><br/><div id="39496320" class="c"><input type="checkbox" id="c-39496320" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#39496268">parent</a><span>|</span><a href="#39498032">next</a><span>|</span><label class="collapse" for="c-39496320">[-]</label><label class="expand" for="c-39496320">[10 more]</label></div><br/><div class="children"><div class="content">Doctors don’t cite claims on the spot</div><br/><div id="39496344" class="c"><input type="checkbox" id="c-39496344" checked=""/><div class="controls bullet"><span class="by">MauranKilom</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39496320">parent</a><span>|</span><a href="#39496359">next</a><span>|</span><label class="collapse" for="c-39496344">[-]</label><label class="expand" for="c-39496344">[1 more]</label></div><br/><div class="children"><div class="content">A fair comparison (on a technical level) to GPT-4 RAG would be a doctor in a relevant field who also has internet access. I think this would be indeed interesting to compare to assess the resulting quality of care, so to speak!<p>(The other models being only partially able to source good references is unsurprising&#x2F;&quot;unfair&quot; on a technical level, but that&#x27;s not relevant for assessing their safety.)</div><br/></div></div><div id="39496359" class="c"><input type="checkbox" id="c-39496359" checked=""/><div class="controls bullet"><span class="by">esoleyman</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39496320">parent</a><span>|</span><a href="#39496344">prev</a><span>|</span><a href="#39496490">next</a><span>|</span><label class="collapse" for="c-39496359">[-]</label><label class="expand" for="c-39496359">[1 more]</label></div><br/><div class="children"><div class="content">Give me a few minutes and I can pull up any number of medical studies or references to back up my claims.<p>I don’t have them memorized to the actual URL but I have kept up to date with the latest studies and summaries that pertain to my field and my patients.</div><br/></div></div><div id="39496490" class="c"><input type="checkbox" id="c-39496490" checked=""/><div class="controls bullet"><span class="by">larsiusprime</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39496320">parent</a><span>|</span><a href="#39496359">prev</a><span>|</span><a href="#39496421">next</a><span>|</span><label class="collapse" for="c-39496490">[-]</label><label class="expand" for="c-39496490">[4 more]</label></div><br/><div class="children"><div class="content">They often give clinical recommendations and prescriptions, usually after a quick skim of the medical history and a rushed five minute conversation with the patient. It would be nice to know how many of these typical in-office recommendations wind up being actually backed by the current state of the research, whether a citation is given in the visit or not.</div><br/><div id="39496857" class="c"><input type="checkbox" id="c-39496857" checked=""/><div class="controls bullet"><span class="by">salad-tycoon</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39496490">parent</a><span>|</span><a href="#39496421">next</a><span>|</span><label class="collapse" for="c-39496857">[-]</label><label class="expand" for="c-39496857">[3 more]</label></div><br/><div class="children"><div class="content">It’s more like this, there is an algorithm &#x2F; flow chart. You follow it down and do the next step in the process.<p>You learn the reason why and then you just follow whatever the latest update to the algorithm is. Doctors are then graded by administrators as to how well they follow that flow chart. Have you done this or that screening, is BP well controlled, etc. Medicare sets many of the standards, you look up the USPTF guidelines for some more idea of what your doctor should be doing. So yeah. Kinda like factory work. “Pt male 70 wakes up urinate - does he have  signs of infection &gt; yes&#x2F; no&gt; proceed to no &gt; next step do this test &gt; prescribe this med &gt; re evaluate in 2-4 weeks. “</div><br/><div id="39497645" class="c"><input type="checkbox" id="c-39497645" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39496857">parent</a><span>|</span><a href="#39496421">next</a><span>|</span><label class="collapse" for="c-39497645">[-]</label><label class="expand" for="c-39497645">[2 more]</label></div><br/><div class="children"><div class="content">Why would we use proven processes involving experts with over a decade of education like this when we could use an LLM thrown together by a software developer who may not have even taken a biology course?</div><br/><div id="39498769" class="c"><input type="checkbox" id="c-39498769" checked=""/><div class="controls bullet"><span class="by">razodactyl</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39497645">parent</a><span>|</span><a href="#39496421">next</a><span>|</span><label class="collapse" for="c-39498769">[-]</label><label class="expand" for="c-39498769">[1 more]</label></div><br/><div class="children"><div class="content">Spot on</div><br/></div></div></div></div></div></div></div></div><div id="39496421" class="c"><input type="checkbox" id="c-39496421" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39496320">parent</a><span>|</span><a href="#39496490">prev</a><span>|</span><a href="#39496384">next</a><span>|</span><label class="collapse" for="c-39496421">[-]</label><label class="expand" for="c-39496421">[2 more]</label></div><br/><div class="children"><div class="content">Some engineers can cite RFCs and ISO standards on the spot sometimes, probably same for doctors</div><br/></div></div><div id="39496384" class="c"><input type="checkbox" id="c-39496384" checked=""/><div class="controls bullet"><span class="by">SkyPuncher</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39496320">parent</a><span>|</span><a href="#39496421">prev</a><span>|</span><a href="#39498032">next</a><span>|</span><label class="collapse" for="c-39496384">[-]</label><label class="expand" for="c-39496384">[1 more]</label></div><br/><div class="children"><div class="content">Neither do most professionals- unless they’re doing a prepared segment.<p>The difference is LLMs can’t back their claims.</div><br/></div></div></div></div><div id="39498032" class="c"><input type="checkbox" id="c-39498032" checked=""/><div class="controls bullet"><span class="by">extragood</span><span>|</span><a href="#39496268">parent</a><span>|</span><a href="#39496320">prev</a><span>|</span><a href="#39497490">next</a><span>|</span><label class="collapse" for="c-39498032">[-]</label><label class="expand" for="c-39498032">[2 more]</label></div><br/><div class="children"><div class="content">This has been my major concern with the currently available LLMs.<p>You can know what the input is, you can know the output, you may even be aware what it&#x27;s been trained on, but none of the output is ever cited. Unless you are already familiar with the topic, you cannot confidently distinguish between fact and what sounds reasonable and is accepted as fact.</div><br/></div></div><div id="39497490" class="c"><input type="checkbox" id="c-39497490" checked=""/><div class="controls bullet"><span class="by">paulddraper</span><span>|</span><a href="#39496268">parent</a><span>|</span><a href="#39498032">prev</a><span>|</span><a href="#39497963">next</a><span>|</span><label class="collapse" for="c-39497490">[-]</label><label class="expand" for="c-39497490">[4 more]</label></div><br/><div class="children"><div class="content">No, but studies have found that 250k+ deaths <i>per year</i> in the US are due to medical errors. [1]<p>[1] <a href="https:&#x2F;&#x2F;www.hopkinsmedicine.org&#x2F;news&#x2F;media&#x2F;releases&#x2F;study_suggests_medical_errors_now_third_leading_cause_of_death_in_the_us" rel="nofollow">https:&#x2F;&#x2F;www.hopkinsmedicine.org&#x2F;news&#x2F;media&#x2F;releases&#x2F;study_su...</a></div><br/><div id="39497594" class="c"><input type="checkbox" id="c-39497594" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39497490">parent</a><span>|</span><a href="#39497963">next</a><span>|</span><label class="collapse" for="c-39497594">[-]</label><label class="expand" for="c-39497594">[3 more]</label></div><br/><div class="children"><div class="content">Doctors also go to school to be doctors for 12 extra years. It turns out that medicine is hard. Not really a good reason to turn to an LLM that will just confidently make things up.</div><br/><div id="39497664" class="c"><input type="checkbox" id="c-39497664" checked=""/><div class="controls bullet"><span class="by">paulddraper</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39497594">parent</a><span>|</span><a href="#39497746">next</a><span>|</span><label class="collapse" for="c-39497664">[-]</label><label class="expand" for="c-39497664">[1 more]</label></div><br/><div class="children"><div class="content">Medicine being hard is in fact an <i>excellent</i> reason to use AI tools.</div><br/></div></div><div id="39497746" class="c"><input type="checkbox" id="c-39497746" checked=""/><div class="controls bullet"><span class="by">larsiusprime</span><span>|</span><a href="#39496268">root</a><span>|</span><a href="#39497594">parent</a><span>|</span><a href="#39497664">prev</a><span>|</span><a href="#39497963">next</a><span>|</span><label class="collapse" for="c-39497746">[-]</label><label class="expand" for="c-39497746">[1 more]</label></div><br/><div class="children"><div class="content">I’m aware, I come from a medical family. You haven’t answered the question.</div><br/></div></div></div></div></div></div><div id="39497963" class="c"><input type="checkbox" id="c-39497963" checked=""/><div class="controls bullet"><span class="by">melagonster</span><span>|</span><a href="#39496268">parent</a><span>|</span><a href="#39497490">prev</a><span>|</span><a href="#39496724">next</a><span>|</span><label class="collapse" for="c-39497963">[-]</label><label class="expand" for="c-39497963">[1 more]</label></div><br/><div class="children"><div class="content">I sure most of treatment are from textbooks.</div><br/></div></div></div></div><div id="39496724" class="c"><input type="checkbox" id="c-39496724" checked=""/><div class="controls bullet"><span class="by">rickysahu</span><span>|</span><a href="#39496268">prev</a><span>|</span><a href="#39497130">next</a><span>|</span><label class="collapse" for="c-39496724">[-]</label><label class="expand" for="c-39496724">[11 more]</label></div><br/><div class="children"><div class="content">Agreed that this is a challenging problem, but mostly because of the data used to train and the tokenization for language models. We’re working on this building what we call a large medical model (lmm) which is trained on medical event histories from 10s of millions of patients, not papers or internet text. Our tokens are medical codes facilities demographics etc. If anyone is interested we have an api + we’re hiring. <a href="https:&#x2F;&#x2F;genhealth.ai" rel="nofollow">https:&#x2F;&#x2F;genhealth.ai</a></div><br/><div id="39496882" class="c"><input type="checkbox" id="c-39496882" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#39496724">parent</a><span>|</span><a href="#39496745">next</a><span>|</span><label class="collapse" for="c-39496882">[-]</label><label class="expand" for="c-39496882">[8 more]</label></div><br/><div class="children"><div class="content">This is fascinating... but just a superficial nit about the website and what it puts across: As someone who spent half my life designing, illustrating and art directing for brands, the choice to use anime style art as the theme for section headers is the exact type of decision I look for when reviewing portfolios that tells me a potential hire might know how to use image editing tools or be proficient in technical fields, but is not actually a designer; i.e. can create an image they &quot;like&quot; but lacks the knowledge and creative ability to synthesize subtexts in design and art history in original ways that are visually compelling but also create the right tone for a client. The choice of generated amime art implies its use on the page as mere decoration, rather than as a language of communication carefully selected to give viewers the right impression of the company. It argues for why diffusion models guided by engineers will not replace professional art direction anytime soon.<p>No offense, this is a standard art crit I would give to any art student, or to anyone running a startup who had nonprofessional design.</div><br/><div id="39497437" class="c"><input type="checkbox" id="c-39497437" checked=""/><div class="controls bullet"><span class="by">rickysahu</span><span>|</span><a href="#39496724">root</a><span>|</span><a href="#39496882">parent</a><span>|</span><a href="#39497000">next</a><span>|</span><label class="collapse" for="c-39497437">[-]</label><label class="expand" for="c-39497437">[2 more]</label></div><br/><div class="children"><div class="content">Ha! We’re just trying to use the art to stand out from the rest of health tech which is quite boring. I get the target audience is not often into anime (Ive watched maybe 10 anime anything’s in my life), nevertheless it looks cooler than the “lobby people” on other websites, and I like to think, bc of threads like this, the selection of art has served its purpose.</div><br/><div id="39498241" class="c"><input type="checkbox" id="c-39498241" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#39496724">root</a><span>|</span><a href="#39497437">parent</a><span>|</span><a href="#39497000">next</a><span>|</span><label class="collapse" for="c-39498241">[-]</label><label class="expand" for="c-39498241">[1 more]</label></div><br/><div class="children"><div class="content">FWIW - I think that&#x27;s a great reason. Anything where the secondary, subconscious effects of the communication are considered and intentional is perfectly valid from my POV. It&#x27;s part of my job to walk through these questions with clients who get attached early in brand development to certain themes or colors or typography, and they usually don&#x27;t have an answer. But if it&#x27;s like &quot;we want to convey that we&#x27;re too busy working on this amazing platform to worry about trying to look like every other digital biotech startup, most of whom use the same boring icon sets, so let&#x27;s just throw out some fun subcultural art in the meantime,&quot; then your choice <i>is</i> well tuned to presenting the company culture you&#x27;re trying to get across, and I withdraw my crit. In fact, I&#x27;d be overjoyed to hear that from any client.</div><br/></div></div></div></div><div id="39497000" class="c"><input type="checkbox" id="c-39497000" checked=""/><div class="controls bullet"><span class="by">user3939382</span><span>|</span><a href="#39496724">root</a><span>|</span><a href="#39496882">parent</a><span>|</span><a href="#39497437">prev</a><span>|</span><a href="#39496745">next</a><span>|</span><label class="collapse" for="c-39497000">[-]</label><label class="expand" for="c-39497000">[5 more]</label></div><br/><div class="children"><div class="content">To play devil&#x27;s advocate, it could turn out that only 1 person needed to be persuaded by the art, some investor that ends up leading their next round, who happens to be an anime fan and now it was the right choice.<p>&gt;  rather than as a language of communication carefully selected to give viewers the right impression of the company<p>You could argue it gives the impression that the culture of the company is primarily technical, given that the technical and anime communities have a huge overlap.</div><br/><div id="39497241" class="c"><input type="checkbox" id="c-39497241" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#39496724">root</a><span>|</span><a href="#39497000">parent</a><span>|</span><a href="#39497101">next</a><span>|</span><label class="collapse" for="c-39497241">[-]</label><label class="expand" for="c-39497241">[1 more]</label></div><br/><div class="children"><div class="content">Your first hypothesis is completely valid, and it occurred to me that this decision could have come from the top - but it&#x27;s also a red flag for me when one or two individuals in a client&#x27;s upper management or worse, investors, inject themselves into art decisions by overriding choices presented to them by trained and experienced professionals they&#x27;ve presumably hired to that end. Those companies tend to also override other suggestions presented by employees or contractors in other areas. Which implies a management style that isn&#x27;t utilizing its hired talent to full effect.<p>As far as appealing to technical people who overlap with enjoying Anime, that would be a very specific decision that might be appropriate for a video game company or something, but even then it would have to be justified by having some actual connection to Anime culture. Otherwise it&#x27;s sort of just appropriating a style... in the hope that some percentage of crossover exists between users, investors, and this subculture being heavily referenced. My theory is that no study was done on whether or to what extent that crossover existed here before choosing it as a theme, but if I&#x27;m wrong I&#x27;ll eat my critique!</div><br/></div></div><div id="39497101" class="c"><input type="checkbox" id="c-39497101" checked=""/><div class="controls bullet"><span class="by">BarryMilo</span><span>|</span><a href="#39496724">root</a><span>|</span><a href="#39497000">parent</a><span>|</span><a href="#39497241">prev</a><span>|</span><a href="#39496745">next</a><span>|</span><label class="collapse" for="c-39497101">[-]</label><label class="expand" for="c-39497101">[3 more]</label></div><br/><div class="children"><div class="content">At what level of ridiculousness do you stop debating? It could turn out that they were blackmailed into making that choice, or maybe it was the best-looking color palette on their old Nokia.<p>At some point you have to be able to recognize that the emperor has no clothes, no matter what extenuating circumstances may have arised.</div><br/><div id="39497261" class="c"><input type="checkbox" id="c-39497261" checked=""/><div class="controls bullet"><span class="by">user3939382</span><span>|</span><a href="#39496724">root</a><span>|</span><a href="#39497101">parent</a><span>|</span><a href="#39496745">next</a><span>|</span><label class="collapse" for="c-39497261">[-]</label><label class="expand" for="c-39497261">[2 more]</label></div><br/><div class="children"><div class="content">Art is famous, or infamous, depending on your perspective, for not having a level where you stop debating it.</div><br/><div id="39497584" class="c"><input type="checkbox" id="c-39497584" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#39496724">root</a><span>|</span><a href="#39497261">parent</a><span>|</span><a href="#39496745">next</a><span>|</span><label class="collapse" for="c-39497584">[-]</label><label class="expand" for="c-39497584">[1 more]</label></div><br/><div class="children"><div class="content">It seems kind of pointless to debate it in this context with someone who is giving their professional opinion.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39496745" class="c"><input type="checkbox" id="c-39496745" checked=""/><div class="controls bullet"><span class="by">srameshc</span><span>|</span><a href="#39496724">parent</a><span>|</span><a href="#39496882">prev</a><span>|</span><a href="#39497130">next</a><span>|</span><label class="collapse" for="c-39496745">[-]</label><label class="expand" for="c-39496745">[2 more]</label></div><br/><div class="children"><div class="content">Very interesting. Who are your target customers ?</div><br/><div id="39497410" class="c"><input type="checkbox" id="c-39497410" checked=""/><div class="controls bullet"><span class="by">rickysahu</span><span>|</span><a href="#39496724">root</a><span>|</span><a href="#39496745">parent</a><span>|</span><a href="#39497130">next</a><span>|</span><label class="collapse" for="c-39497410">[-]</label><label class="expand" for="c-39497410">[1 more]</label></div><br/><div class="children"><div class="content">Its broadly applicable, but initially health plans and provider orgs.</div><br/></div></div></div></div></div></div><div id="39497130" class="c"><input type="checkbox" id="c-39497130" checked=""/><div class="controls bullet"><span class="by">jonathan-adly</span><span>|</span><a href="#39496724">prev</a><span>|</span><a href="#39496249">next</a><span>|</span><label class="collapse" for="c-39497130">[-]</label><label class="expand" for="c-39497130">[4 more]</label></div><br/><div class="children"><div class="content">If anyone is interested - my startup which did exactly this was acquired 8 months ago. As many mentioned - the sauce is in the RAG implementation and the curation of the base documents. As far as I can tell, the software is now covering ~11m lives or so and going strong - the company already got the acquisition price back and some more. I was even asked to come support an initiative to move from RAG to long context + multi-agents.<p>I know it works very well. There are lots of literature from the medical community where they don&#x27;t consult any actual AI engineers. There are also lots of literature on the tech community where no clinicians are to be seen. Take both of them with a massive grain of salt.</div><br/><div id="39497182" class="c"><input type="checkbox" id="c-39497182" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39497130">parent</a><span>|</span><a href="#39496249">next</a><span>|</span><label class="collapse" for="c-39497182">[-]</label><label class="expand" for="c-39497182">[3 more]</label></div><br/><div class="children"><div class="content">If you don’t mind sharing, what lesser known RAG tricks did you use to ensure the correct information was going through?<p>Simple similarity search is not enough.</div><br/><div id="39497219" class="c"><input type="checkbox" id="c-39497219" checked=""/><div class="controls bullet"><span class="by">jonathan-adly</span><span>|</span><a href="#39497130">root</a><span>|</span><a href="#39497182">parent</a><span>|</span><a href="#39496249">next</a><span>|</span><label class="collapse" for="c-39497219">[-]</label><label class="expand" for="c-39497219">[2 more]</label></div><br/><div class="children"><div class="content">Main thing is to curate a good set of documents to start with. Garbage in (like Bing google results like this study did) --&gt; Garbage out.<p>From the technical side, the largest mistake people do is abstracting the process with langchain and the like - and not hyper-optimizing every step with trial and error.</div><br/><div id="39497333" class="c"><input type="checkbox" id="c-39497333" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#39497130">root</a><span>|</span><a href="#39497219">parent</a><span>|</span><a href="#39496249">next</a><span>|</span><label class="collapse" for="c-39497333">[-]</label><label class="expand" for="c-39497333">[1 more]</label></div><br/><div class="children"><div class="content">From [1], pdfGPT, knowledge_gpt, and paperai are open source. I don&#x27;t think any are updated for a 10M token context limit (like Gemini) yet either.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39363115">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39363115</a><p>From <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39492995">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39492995</a> :<p>&gt; <i>On why code LLMs should be trained on the edges between tests and the code that they test, that could be visualized as [...]</i><p>&quot;Find tests for this code&quot;<p>&quot;Find citations for this bias&quot;<p>Perhaps this would be best:<p>&quot;Automated Unit Test Improvement Using Large Language Models at Meta&quot; (2024) <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39416628">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39416628</a><p>From <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37463686">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37463686</a> :<p>&gt; <i>When the next token is a URL, and the URL does not match the preceding anchor text.</i><p>&gt; Additional layers of these &#x27;LLMs&#x27; could read the responses and <i>determine whether their premises are valid and their logic is sound as necessary to support the presented conclusion(s)</i>, and then just suggest a different citation URL for the preceding text</div><br/></div></div></div></div></div></div></div></div><div id="39496249" class="c"><input type="checkbox" id="c-39496249" checked=""/><div class="controls bullet"><span class="by">rubatuga</span><span>|</span><a href="#39497130">prev</a><span>|</span><a href="#39497463">next</a><span>|</span><label class="collapse" for="c-39496249">[-]</label><label class="expand" for="c-39496249">[2 more]</label></div><br/><div class="children"><div class="content">The safest way now to use LLMs is for simple entity extraction of signs, symptoms, investigation summaries, then translate them into the inputs for an understandable linear&#x2F;logistic model, e.g. Wells criteria, Canadian CT head, or Centor score. I feel that a comprehensive but explainable model that supports multiple diagnoses will be developed in the future, but no such model currently exists.</div><br/></div></div><div id="39497463" class="c"><input type="checkbox" id="c-39497463" checked=""/><div class="controls bullet"><span class="by">xyst</span><span>|</span><a href="#39496249">prev</a><span>|</span><a href="#39497148">next</a><span>|</span><label class="collapse" for="c-39497463">[-]</label><label class="expand" for="c-39497463">[2 more]</label></div><br/><div class="children"><div class="content">For every 1 medical case solved by web search&#x2F;chatgpt&#x2F;ai. There will be hundreds of false-positives.<p>I remember shadowing in the ER in the early 10s and many people would come in with Google search trash. One kid even cited something from the psychologist handbook. Lol</div><br/><div id="39498640" class="c"><input type="checkbox" id="c-39498640" checked=""/><div class="controls bullet"><span class="by">pants2</span><span>|</span><a href="#39497463">parent</a><span>|</span><a href="#39497148">next</a><span>|</span><label class="collapse" for="c-39498640">[-]</label><label class="expand" for="c-39498640">[1 more]</label></div><br/><div class="children"><div class="content">Every medical issue I&#x27;ve ever had I managed to correctly diagnose by myself using Google (haven&#x27;t had many issues since ChatGPT) and later confirmed by a doctor. I attribute this to my years of experience using Google to troubleshoot and debug software.</div><br/></div></div></div></div><div id="39497148" class="c"><input type="checkbox" id="c-39497148" checked=""/><div class="controls bullet"><span class="by">iamleppert</span><span>|</span><a href="#39497463">prev</a><span>|</span><a href="#39496520">next</a><span>|</span><label class="collapse" for="c-39497148">[-]</label><label class="expand" for="c-39497148">[2 more]</label></div><br/><div class="children"><div class="content">ChatGPT4 correctly diagnosed my neurological condition, an infection that many doctors had missed. While I was in the hospital I asked ChatGPT the same questions as the doctors and it was nearly identical to what they were telling me every time.<p>It also acted as a therapist and talked me down from several depressions while in the hospital, far better than any human therapist I’ve ever had. The fact that it’s an AI made me actually feel better than if the therapy was delivered by a real therapist, for some strange reason.</div><br/><div id="39498692" class="c"><input type="checkbox" id="c-39498692" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#39497148">parent</a><span>|</span><a href="#39496520">next</a><span>|</span><label class="collapse" for="c-39498692">[-]</label><label class="expand" for="c-39498692">[1 more]</label></div><br/><div class="children"><div class="content">Meanwhile my doctors are incompetent at diagnosing most of my symptoms, so I&#x27;ll take ChatGPT over nothing until the medical system can gets its shit together.</div><br/></div></div></div></div><div id="39496520" class="c"><input type="checkbox" id="c-39496520" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#39497148">prev</a><span>|</span><a href="#39496314">next</a><span>|</span><label class="collapse" for="c-39496520">[-]</label><label class="expand" for="c-39496520">[1 more]</label></div><br/><div class="children"><div class="content">No shocker there, even when LLMs have the correct answer they can&#x27;t tell you where they got it. They simply do not know.</div><br/></div></div><div id="39496314" class="c"><input type="checkbox" id="c-39496314" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#39496520">prev</a><span>|</span><a href="#39498198">next</a><span>|</span><label class="collapse" for="c-39496314">[-]</label><label class="expand" for="c-39496314">[14 more]</label></div><br/><div class="children"><div class="content">&gt; A new study finds that large language models used widely for medical assessments cannot back up claims.<p>like real doctors</div><br/><div id="39496515" class="c"><input type="checkbox" id="c-39496515" checked=""/><div class="controls bullet"><span class="by">rzzzt</span><span>|</span><a href="#39496314">parent</a><span>|</span><a href="#39498264">next</a><span>|</span><label class="collapse" for="c-39496515">[-]</label><label class="expand" for="c-39496515">[12 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t want to single you out, but what should I be taking away from the often recited &quot;well people sometimes* can&#x27;t do thing X either&quot;-counterargument? Is all of this just fine? Can&#x27;t we expect just a little bit more, I don&#x27;t know, accuracy or rigor from a computer than a living person?</div><br/><div id="39496604" class="c"><input type="checkbox" id="c-39496604" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39496515">parent</a><span>|</span><a href="#39496609">next</a><span>|</span><label class="collapse" for="c-39496604">[-]</label><label class="expand" for="c-39496604">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I don’t buy that argument either. If excel got the math wrong replying with “well humans do to” wouldn’t fly.</div><br/></div></div><div id="39496609" class="c"><input type="checkbox" id="c-39496609" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39496515">parent</a><span>|</span><a href="#39496604">prev</a><span>|</span><a href="#39498264">next</a><span>|</span><label class="collapse" for="c-39496609">[-]</label><label class="expand" for="c-39496609">[10 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve already passed the point where LLM&#x27;s are better than human experts for medical diagnoses. In fact, according to this study, even LLM&#x27;s alone are more accurate than human experts + LLM&#x27;s, meaning any input the humans added was only a detriment to the accuracy<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2312.00164.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2312.00164.pdf</a> (Page 8)<p>Computers are already perfectly accurate, and have been for decades in explicit quantifiable fields. In medicine, since a computer cannot perfectly replicate every single cell in the human body, its abstractions will be lower resolution than reality, but what matters is whether that low resolution abstraction is better than the alternative (human doctors).<p>A human doctor couldn&#x27;t bring up a list of citations in literature instantly regarding a diagnosis. A LLM can.</div><br/><div id="39496766" class="c"><input type="checkbox" id="c-39496766" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39496609">parent</a><span>|</span><a href="#39496702">next</a><span>|</span><label class="collapse" for="c-39496766">[-]</label><label class="expand" for="c-39496766">[5 more]</label></div><br/><div class="children"><div class="content">Even if that paper hadn&#x27;t said &gt;&gt;&gt;&quot;We are therefore very cautious to extrapolate our findings toward any implications about the LLM’s utility as
a standalone diagnostic tool&quot;<p>Your post would be a extraordinary claim and need extraordinary evidence, not a specific study of a specific scenario.</div><br/><div id="39496943" class="c"><input type="checkbox" id="c-39496943" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39496766">parent</a><span>|</span><a href="#39496702">next</a><span>|</span><label class="collapse" for="c-39496943">[-]</label><label class="expand" for="c-39496943">[4 more]</label></div><br/><div class="children"><div class="content">there already is extraordinary evidence of GPT-4&#x27;s superhuman diagnostic abilities. One anecdotal example is this:<p><a href="https:&#x2F;&#x2F;www.today.com&#x2F;health&#x2F;mom-chatgpt-diagnosis-pain-rcna101843" rel="nofollow">https:&#x2F;&#x2F;www.today.com&#x2F;health&#x2F;mom-chatgpt-diagnosis-pain-rcna...</a><p>Another study regarding GPT-4 beating human experts:
<a href="https:&#x2F;&#x2F;www.medrxiv.org&#x2F;content&#x2F;10.1101&#x2F;2023.04.20.23288859v2.full.pdf" rel="nofollow">https:&#x2F;&#x2F;www.medrxiv.org&#x2F;content&#x2F;10.1101&#x2F;2023.04.20.23288859v...</a><p>Lots of data is pointing to the same conclusion: GPT-4 is at least as good if not better than human experts in medical diagnosis, at least in the areas studied. Thus the probability of a correct diagnosis is higher, thus safer, with GPT-4 than with any individual human expert.</div><br/><div id="39497147" class="c"><input type="checkbox" id="c-39497147" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39496943">parent</a><span>|</span><a href="#39496702">next</a><span>|</span><label class="collapse" for="c-39497147">[-]</label><label class="expand" for="c-39497147">[3 more]</label></div><br/><div class="children"><div class="content">This is so silly, the one study you linked to says that GPT 4 may have been trained on the answers to the test they gave it. So smart.<p>And since GPT 4 can&#x27;t examine a patient&#x27;s body the claim it&#x27;s better at diagnosis that a human doctor seems like such a wacky thing to search the internet for &quot;studies&quot; to prove in the first place.</div><br/><div id="39497443" class="c"><input type="checkbox" id="c-39497443" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39497147">parent</a><span>|</span><a href="#39496702">next</a><span>|</span><label class="collapse" for="c-39497443">[-]</label><label class="expand" for="c-39497443">[2 more]</label></div><br/><div class="children"><div class="content">A nurse can examine a patient&#x27;s body. Medical tools can and report their diagnostics with high precision. GPT-4 is multi-modal.<p>I feel you are nitpicking because you don&#x27;t like the idea of an LLM being better than a human expert. Even if they weren&#x27;t better than doctors nowadays, the chance they won&#x27;t be in 1-2 years is tiny.</div><br/><div id="39498698" class="c"><input type="checkbox" id="c-39498698" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39497443">parent</a><span>|</span><a href="#39496702">next</a><span>|</span><label class="collapse" for="c-39498698">[-]</label><label class="expand" for="c-39498698">[1 more]</label></div><br/><div class="children"><div class="content">What I&#x27;m doing isn&#x27;t nitpicking. I don&#x27;t know the point of linking to studies  is when you draw conclusions that have nothing to do with the study.<p>I just watched a video saying people are confused about what these models can do because<p>1) tech companies don&#x27;t tend to say what they can do and leave users to figure it out.<p>And<p>2) Tech enthusiasts tend to exaggerate what they can do.<p>In your case I&#x27;m sure ChatGPT itself will tell you your comments are wrong- but for tech enthusiasts like yourself the AI is only wrong when it tells you it isn&#x27;t all knowing, apparently.<p>It&#x27;s like the bit in Monty Python&#x27;s Life of Brian where the protagonist says he&#x27;s not the messiah and a woman shouts &quot;Only the true messiah would deny his divinity!&quot;</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39496702" class="c"><input type="checkbox" id="c-39496702" checked=""/><div class="controls bullet"><span class="by">rzzzt</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39496609">parent</a><span>|</span><a href="#39496766">prev</a><span>|</span><a href="#39497168">next</a><span>|</span><label class="collapse" for="c-39496702">[-]</label><label class="expand" for="c-39496702">[1 more]</label></div><br/><div class="children"><div class="content">They could, but they don&#x27;t, at least that is what I&#x27;m getting from the article.<p>Even if they do, someone with the capability and understanding required (ie. not me) needs to bring that source up and verify that the claims align with the citation; the authors decided to use GPT-4 for this: &quot;We adapted GPT-4 to verify whether sources substantiate statements and found the approach to be surprisingly reliable.&quot; I&#x27;m not happy with that either.</div><br/></div></div><div id="39497168" class="c"><input type="checkbox" id="c-39497168" checked=""/><div class="controls bullet"><span class="by">TacticalCoder</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39496609">parent</a><span>|</span><a href="#39496702">prev</a><span>|</span><a href="#39496748">next</a><span>|</span><label class="collapse" for="c-39497168">[-]</label><label class="expand" for="c-39497168">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A human doctor couldn&#x27;t bring up a list of citations in literature instantly regarding a diagnosis. A LLM can.<p>TFA is, literally, about LLMs spouting out erroneous medical references. I don&#x27;t care about made up medical references or court cases.<p>I&#x27;m sure there are ways to bring up instantly a list of publication regarding a diagnosis (which a LLM may or may in the future correctly give: the diagnosis I mean) but I&#x27;m really not sure a LLM is what&#x27;s needed to do then generate the list of related publication. I mean, FFS, they are compressed, lossy, knowledge.<p>LLMs are going to become tools as part of a toolchain. They&#x27;re not a panacea.</div><br/></div></div><div id="39497681" class="c"><input type="checkbox" id="c-39497681" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#39496314">root</a><span>|</span><a href="#39496609">parent</a><span>|</span><a href="#39496748">prev</a><span>|</span><a href="#39498264">next</a><span>|</span><label class="collapse" for="c-39497681">[-]</label><label class="expand" for="c-39497681">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A human doctor couldn&#x27;t bring up a list of citations in literature instantly regarding a diagnosis. A LLM can.<p>Which it will happily make up.</div><br/></div></div></div></div></div></div><div id="39498264" class="c"><input type="checkbox" id="c-39498264" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#39496314">parent</a><span>|</span><a href="#39496515">prev</a><span>|</span><a href="#39498198">next</a><span>|</span><label class="collapse" for="c-39498264">[-]</label><label class="expand" for="c-39498264">[1 more]</label></div><br/><div class="children"><div class="content">Trained professionals get things wrong.<p>However Trained professionals get it less wrong than the alternatives.<p>Wouldn’t the efficacy of real doctors be against the efficacy of alternatives, and similarly for LLMs?<p>How much better are both, LLMs and Doctors to the average joe, I think that would be a great metric to see.</div><br/></div></div></div></div><div id="39498198" class="c"><input type="checkbox" id="c-39498198" checked=""/><div class="controls bullet"><span class="by">elietoubi</span><span>|</span><a href="#39496314">prev</a><span>|</span><a href="#39497155">next</a><span>|</span><label class="collapse" for="c-39498198">[-]</label><label class="expand" for="c-39498198">[1 more]</label></div><br/><div class="children"><div class="content">As a fun side project,
We fine tuned mistral 7B and used RAG with 35M radical papers.
Gives you actually really accurate answers: <a href="https:&#x2F;&#x2F;chat.scribemd.ai" rel="nofollow">https:&#x2F;&#x2F;chat.scribemd.ai</a></div><br/></div></div><div id="39497155" class="c"><input type="checkbox" id="c-39497155" checked=""/><div class="controls bullet"><span class="by">hooloovoo_zoo</span><span>|</span><a href="#39498198">prev</a><span>|</span><a href="#39496480">next</a><span>|</span><label class="collapse" for="c-39497155">[-]</label><label class="expand" for="c-39497155">[1 more]</label></div><br/><div class="children"><div class="content">A medical statement being supported by at least a single citation is honestly a really low bar anyway.</div><br/></div></div><div id="39496480" class="c"><input type="checkbox" id="c-39496480" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#39497155">prev</a><span>|</span><a href="#39496428">next</a><span>|</span><label class="collapse" for="c-39496480">[-]</label><label class="expand" for="c-39496480">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t seem to say the LLM is wrong, it&#x27;s just bad at giving a source to its statements.</div><br/></div></div><div id="39496428" class="c"><input type="checkbox" id="c-39496428" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39496480">prev</a><span>|</span><a href="#39496264">next</a><span>|</span><label class="collapse" for="c-39496428">[-]</label><label class="expand" for="c-39496428">[3 more]</label></div><br/><div class="children"><div class="content">&gt; This problem goes away with the retrieval augmented generation (RAG) model, which first performs a web search for relevant sources before producing a summary of its findings. However, even in the GPT-4 RAG model, we find that up to 30% of statements made are not supported by any sources provided, with nearly half of responses containing at least one unsupported statement.<p>Could you do something like this. Chunk&#x2F;separate the individual claims made by the LLM, and the associated sources from RAG. Then feed them into a second LLM one by one, asking &quot;is this claim $X_i reflective of the source $Y_i?&quot;. Whatever this second LLM says, feedback the response to the original LLM and ask it to revise what it&#x27;s saying if the answer is &quot;No&quot;. Iterate until the second LLM says &quot;Yes&quot; to every separate claim. Not perfect but might help.</div><br/><div id="39496500" class="c"><input type="checkbox" id="c-39496500" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39496428">parent</a><span>|</span><a href="#39497291">next</a><span>|</span><label class="collapse" for="c-39496500">[-]</label><label class="expand" for="c-39496500">[1 more]</label></div><br/><div class="children"><div class="content">Google Gemini actually has a feature like this, though it&#x27;s hard to spot. There&#x27;s a colorful G icon below each Gemini response - if you click it, a second process runs which attempts to &quot;fact check&quot; the claims from the original prompt response - it highlights them in different colors and adds citation links to them.</div><br/></div></div><div id="39497291" class="c"><input type="checkbox" id="c-39497291" checked=""/><div class="controls bullet"><span class="by">abid786</span><span>|</span><a href="#39496428">parent</a><span>|</span><a href="#39496500">prev</a><span>|</span><a href="#39496264">next</a><span>|</span><label class="collapse" for="c-39497291">[-]</label><label class="expand" for="c-39497291">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are better at this and it’ll probably marginally improve the output quality but there can potentially be hallucinations (false positives or negatives) even in this evaluation task.</div><br/></div></div></div></div></div></div></div></div></div></body></html>