<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1687856459805" as="style"/><link rel="stylesheet" href="styles.css?v=1687856459805"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.deeplearning.ai/the-batch/the-paella-model-for-fast-image-generation-explained/">Like diffusion but faster: The Paella model for fast image generation</a> <span class="domain">(<a href="https://www.deeplearning.ai">www.deeplearning.ai</a>)</span></div><div class="subtext"><span>webmaven</span> | <span>37 comments</span></div><br/><div><div id="36486624" class="c"><input type="checkbox" id="c-36486624" checked=""/><div class="controls bullet"><span class="by">nodja</span><span>|</span><a href="#36485184">next</a><span>|</span><label class="collapse" for="c-36486624">[-]</label><label class="expand" for="c-36486624">[7 more]</label></div><br/><div class="children"><div class="content">Note that Paella is a bit old in image model terms (Nov 2022) and modern stable diffusion tools have access to optimized workflows.<p>My 3060 can generate a 256x256 8 step image in 0.5 seconds, no A100 needed. A 3090 is double the performance of a 3060 at 512x512, and an A100 is 50% faster than a 3090...<p>If you have access to an high end consumer GPU (4090) you can generate 512x512 images in less than a second, it&#x27;s reached the point that you can increase the batch size and have it show 2-4 images per prompt without adversely affecting your workflow.<p>Too bad SD1.5 is too small* and we&#x27;ll require models with more parameters if we want a true general purpose image model. If SD1.5 was the end-game, we&#x27;d have truly instant high res image generation in just a couple more generations of GPUs, think generating images in real time as you type the prompt, or have sliders that affect the strength of certain tokens and see the effects in real time, etc. Tho I heard that SDXL is actually faster for higher resolutions (&gt;1024x1024) due to removing attention on the first layer, making it scale better with resolution even tho SDXL has 4x the parameter size.<p>* Current SD1.5 models that can generate consistent high quality images have been fine-tuned and merged so many times that a lot of general knowledge has been lost, e.g. they can be great at generating landscapes, but lacking in generating humans, or they can be very good at a certain style like comics but can do comic style only and lose the ability to generate more dynamic face variations, etc.</div><br/><div id="36487631" class="c"><input type="checkbox" id="c-36487631" checked=""/><div class="controls bullet"><span class="by">PheonixPharts</span><span>|</span><a href="#36486624">parent</a><span>|</span><a href="#36488505">next</a><span>|</span><label class="collapse" for="c-36487631">[-]</label><label class="expand" for="c-36487631">[1 more]</label></div><br/><div class="children"><div class="content">Personally I think the SD1.5 trade off, loss of general knowledge for surprisingly high quality images on consumer hardware, is worth it.<p>It&#x27;s fairly impressive to me what the community has made possible with SD1.5. Sure on a vanilla task something like Dall-E 2 generally performs better, but with some tweaking you can easily beat out Dall-E on a home gaming PC.<p>The fact that you can fine-tune SD1.5 on a 4090 is incredible to me.<p>Given how much powerful AI is locked behind fees and private APIs it&#x27;s refreshing to see so much cool stuff coming out of the OSS world again. Best of all is it&#x27;s not being driven exclusively by people with an ML background, but moreso curious amateurs. It really brings me back to a time when playing around with software&#x2F;the web felt exciting.</div><br/></div></div><div id="36488505" class="c"><input type="checkbox" id="c-36488505" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36486624">parent</a><span>|</span><a href="#36487631">prev</a><span>|</span><a href="#36487362">next</a><span>|</span><label class="collapse" for="c-36488505">[-]</label><label class="expand" for="c-36488505">[3 more]</label></div><br/><div class="children"><div class="content">All diffusion models are quite inefficent due to running in PyTorch eager mode (with torch.compile being kinda janky in practice on 2.0&#x2F;2.1).<p>I would be more interested to see Paella vs SD running on a ML compiler framework, like TVM or AITemplate. Maybe one or the other is more amenable to optimization.</div><br/><div id="36490500" class="c"><input type="checkbox" id="c-36490500" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#36486624">root</a><span>|</span><a href="#36488505">parent</a><span>|</span><a href="#36488992">next</a><span>|</span><label class="collapse" for="c-36490500">[-]</label><label class="expand" for="c-36490500">[1 more]</label></div><br/><div class="children"><div class="content">A framework could make the programmer <i>think</i> they are in eager mode, while actually making graphs behind the scenes.<p>The trick is to simply not do any calculations till the last possible moment - ie. the time the program tries to convert the finished image to a jpeg.    Only at that point do you compile the graph and run the actual computation on the GPU.<p>You then also cache the graph, so that the compilation step can be avoided if the program tries to do the same computation again with different data.</div><br/></div></div><div id="36488992" class="c"><input type="checkbox" id="c-36488992" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36486624">root</a><span>|</span><a href="#36488505">parent</a><span>|</span><a href="#36490500">prev</a><span>|</span><a href="#36487362">next</a><span>|</span><label class="collapse" for="c-36488992">[-]</label><label class="expand" for="c-36488992">[1 more]</label></div><br/><div class="children"><div class="content">(And for reference, AITemplate roughly doubles SD 1.5&#x27;s speed. Not sure if thats good or if Paella would have even more room for auto optimization).</div><br/></div></div></div></div><div id="36487362" class="c"><input type="checkbox" id="c-36487362" checked=""/><div class="controls bullet"><span class="by">dome271</span><span>|</span><a href="#36486624">parent</a><span>|</span><a href="#36488505">prev</a><span>|</span><a href="#36488557">next</a><span>|</span><label class="collapse" for="c-36487362">[-]</label><label class="expand" for="c-36487362">[1 more]</label></div><br/><div class="children"><div class="content">Fully correct, also the v2 of the paper introduced a model that is bigger and slower, however generates better images. So the 500ms was only for the first model we introduced in v1. I also want to mention our new work as it is very much related to this whole topic of &quot;speeding up models&quot; -&gt; either training or sampling: Würstchen: <a href="https:&#x2F;&#x2F;github.com&#x2F;dome272&#x2F;wuerstchen&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;dome272&#x2F;wuerstchen&#x2F;</a>
With a current version we are training at the moment we can sample (using torch.compile) 4 1024x1024 images in 4 seconds. Also the training of this kind of model is very fast due to spatially encoding images much much more -&gt; 3x512x512 images -&gt; 16x12x12 latents =&gt; 42x spatial compression, whereas StableDiffusion has an 8x compression (3x512x512 -&gt; 4x64x64).</div><br/></div></div><div id="36488557" class="c"><input type="checkbox" id="c-36488557" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#36486624">parent</a><span>|</span><a href="#36487362">prev</a><span>|</span><a href="#36485184">next</a><span>|</span><label class="collapse" for="c-36488557">[-]</label><label class="expand" for="c-36488557">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Current SD1.5 models that can generate consistent high quality images have been fine-tuned and merged so many times that a lot of general knowledge has been lost,<p>You say that as if it was a bad thing, but it&#x27;s actually good: GPU memory being a limiting factor means that general knowledge is mostly overhead, it is much better to have 50 specialized model (that you can all store on disk for cheap) that each takes 5 time less GPU memory than a big general model that you&#x27;ll constantly under-use but still have to load entierly in the GPU memory. And it&#x27;s even more true for LLMs.</div><br/></div></div></div></div><div id="36485184" class="c"><input type="checkbox" id="c-36485184" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#36486624">prev</a><span>|</span><a href="#36484700">next</a><span>|</span><label class="collapse" for="c-36485184">[-]</label><label class="expand" for="c-36485184">[4 more]</label></div><br/><div class="children"><div class="content">Half a second. Can&#x27;t even type a good prompt in that fast.<p>One of my dad&#x27;s preferred anecdotes about how much computers sped up in his career, was the number of digits of pi that the company mainframe could compute.<p>He was born in &#x27;39.<p>And now I can generate images from descriptions faster than I can give those descriptions.<p>At this rate, websites will be replaced with image generators and LLMs, and the loading speed won&#x27;t change.</div><br/><div id="36485288" class="c"><input type="checkbox" id="c-36485288" checked=""/><div class="controls bullet"><span class="by">interroboink</span><span>|</span><a href="#36485184">parent</a><span>|</span><a href="#36488048">next</a><span>|</span><label class="collapse" for="c-36485288">[-]</label><label class="expand" for="c-36485288">[1 more]</label></div><br/><div class="children"><div class="content">Just to be clear, it&#x27;s half a second for 256x256, where Stable Diffusion takes 3.2 seconds. Still a great speed-up, but not producing the big hi-res images people might be thinking of.</div><br/></div></div><div id="36488048" class="c"><input type="checkbox" id="c-36488048" checked=""/><div class="controls bullet"><span class="by">Turskarama</span><span>|</span><a href="#36485184">parent</a><span>|</span><a href="#36485288">prev</a><span>|</span><a href="#36489033">next</a><span>|</span><label class="collapse" for="c-36488048">[-]</label><label class="expand" for="c-36488048">[1 more]</label></div><br/><div class="children"><div class="content">The load speed for modern websites is mostly due to all the trackers rather than the actual UI so I don&#x27;t think load speed would actually be affected at all.</div><br/></div></div><div id="36489033" class="c"><input type="checkbox" id="c-36489033" checked=""/><div class="controls bullet"><span class="by">IIAOPSW</span><span>|</span><a href="#36485184">parent</a><span>|</span><a href="#36488048">prev</a><span>|</span><a href="#36484700">next</a><span>|</span><label class="collapse" for="c-36489033">[-]</label><label class="expand" for="c-36489033">[1 more]</label></div><br/><div class="children"><div class="content">At this rate, we will be replaced.</div><br/></div></div></div></div><div id="36484700" class="c"><input type="checkbox" id="c-36484700" checked=""/><div class="controls bullet"><span class="by">gcanyon</span><span>|</span><a href="#36485184">prev</a><span>|</span><a href="#36484718">next</a><span>|</span><label class="collapse" for="c-36484700">[-]</label><label class="expand" for="c-36484700">[3 more]</label></div><br/><div class="children"><div class="content">Way back when, it was pretty easy to recognize diagrams created with MacDraw. There was a particular visual style to the primitives it included that flowed through to the final product. This was of course easier to notice because there were so few alternatives at the time.<p>Given that Paella uses tokens instead of the source image, I wonder if the results will have a (human- or machine-) detectable &quot;style&quot; to them.</div><br/><div id="36484821" class="c"><input type="checkbox" id="c-36484821" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#36484700">parent</a><span>|</span><a href="#36484718">next</a><span>|</span><label class="collapse" for="c-36484821">[-]</label><label class="expand" for="c-36484821">[2 more]</label></div><br/><div class="children"><div class="content">The usual answer to &quot;all AI art looks the same&quot; is <a href="https:&#x2F;&#x2F;i.redd.it&#x2F;jvwyyqn7776a1.jpg" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.redd.it&#x2F;jvwyyqn7776a1.jpg</a></div><br/><div id="36485227" class="c"><input type="checkbox" id="c-36485227" checked=""/><div class="controls bullet"><span class="by">omnicognate</span><span>|</span><a href="#36484700">root</a><span>|</span><a href="#36484821">parent</a><span>|</span><a href="#36484718">next</a><span>|</span><label class="collapse" for="c-36485227">[-]</label><label class="expand" for="c-36485227">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps if you ask an 11 year old.</div><br/></div></div></div></div></div></div><div id="36484718" class="c"><input type="checkbox" id="c-36484718" checked=""/><div class="controls bullet"><span class="by">RobotToaster</span><span>|</span><a href="#36484700">prev</a><span>|</span><a href="#36484984">next</a><span>|</span><label class="collapse" for="c-36484718">[-]</label><label class="expand" for="c-36484718">[1 more]</label></div><br/><div class="children"><div class="content">Github for those looking for the code <a href="https:&#x2F;&#x2F;github.com&#x2F;dome272&#x2F;Paella">https:&#x2F;&#x2F;github.com&#x2F;dome272&#x2F;Paella</a></div><br/></div></div><div id="36484984" class="c"><input type="checkbox" id="c-36484984" checked=""/><div class="controls bullet"><span class="by">FloatArtifact</span><span>|</span><a href="#36484718">prev</a><span>|</span><a href="#36485161">next</a><span>|</span><label class="collapse" for="c-36484984">[-]</label><label class="expand" for="c-36484984">[8 more]</label></div><br/><div class="children"><div class="content">A question for curiosity. Why can&#x27;t it train on 256x256 pixels yet generate any size image? So if it was trained on multiple sizes of images, could you also generate of a larger size without upscaling?</div><br/><div id="36487316" class="c"><input type="checkbox" id="c-36487316" checked=""/><div class="controls bullet"><span class="by">dome271</span><span>|</span><a href="#36484984">parent</a><span>|</span><a href="#36485856">next</a><span>|</span><label class="collapse" for="c-36487316">[-]</label><label class="expand" for="c-36487316">[3 more]</label></div><br/><div class="children"><div class="content">Yea you can. It&#x27;s the same as with any other CNN based model that is not forced to  have a specific shape (like transformers if they use specific positional embeddings). You can also look at the blog post where different resolution images are generated <a href="https:&#x2F;&#x2F;laion.ai&#x2F;blog&#x2F;paella&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;laion.ai&#x2F;blog&#x2F;paella&#x2F;</a></div><br/><div id="36487408" class="c"><input type="checkbox" id="c-36487408" checked=""/><div class="controls bullet"><span class="by">dome271</span><span>|</span><a href="#36484984">root</a><span>|</span><a href="#36487316">parent</a><span>|</span><a href="#36487670">next</a><span>|</span><label class="collapse" for="c-36487408">[-]</label><label class="expand" for="c-36487408">[1 more]</label></div><br/><div class="children"><div class="content">I also explain more in the video I made about Paella (<a href="https:&#x2F;&#x2F;youtu.be&#x2F;zdE1I6kYKYc" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;zdE1I6kYKYc</a>). Maybe that clarifies things more.</div><br/></div></div><div id="36487670" class="c"><input type="checkbox" id="c-36487670" checked=""/><div class="controls bullet"><span class="by">dplavery92</span><span>|</span><a href="#36484984">root</a><span>|</span><a href="#36487316">parent</a><span>|</span><a href="#36487408">prev</a><span>|</span><a href="#36485856">next</a><span>|</span><label class="collapse" for="c-36487670">[-]</label><label class="expand" for="c-36487670">[1 more]</label></div><br/><div class="children"><div class="content">Transformers are not forced to use a specific input (or output) shape; the original ViT paper demonstrates interpolating positional embeddings to inference with arbitrary image shapes.</div><br/></div></div></div></div><div id="36485856" class="c"><input type="checkbox" id="c-36485856" checked=""/><div class="controls bullet"><span class="by">wincy</span><span>|</span><a href="#36484984">parent</a><span>|</span><a href="#36487316">prev</a><span>|</span><a href="#36486882">next</a><span>|</span><label class="collapse" for="c-36485856">[-]</label><label class="expand" for="c-36485856">[1 more]</label></div><br/><div class="children"><div class="content">As someone who doesn’t know the “why” but uses Stable diffusion a lot and has an intuitive feel for the “what” of what happens, it’s like trying to use a low res pattern for your wallpaper on Windows. It’ll either just repeat over and over so you’ll end up with weird multi headed people with heads on top of their heads, or you just upscale which hallucinates details in a totally different way that doesn’t really add new interesting details.<p>With automatic1111 you can get around this by upscaling then inpainting the spots you want more detail and specifying a specific  prompt for that particular area.</div><br/></div></div><div id="36486882" class="c"><input type="checkbox" id="c-36486882" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#36484984">parent</a><span>|</span><a href="#36485856">prev</a><span>|</span><a href="#36486784">next</a><span>|</span><label class="collapse" for="c-36486882">[-]</label><label class="expand" for="c-36486882">[1 more]</label></div><br/><div class="children"><div class="content">Well, it&#x27;s complicated.<p>The model can&#x27;t just work on arbitrary image sizes because the model was trained with a fixed number of input and output neurons. For example, 512x512 is Stable Diffusion&#x27;s &quot;native size.&quot; However, there are tricks to work around this.<p>Diffusion models work by predicting image noise, which is then subtracted from the image iteratively until you get a result that matches the prompt. Stable Diffusion specifically has the following architectural features:<p>- A Variational Autoencoder (VAE) layer that encodes the 512x512 input into a 128x128 latent space[0]<p>- Three cross-attention blocks that take the encoded text prompt and input latent-space image, and output a downscaled image to the next layer<p>- A simpler downscaling block that just has a linear and convolutional layer<p>- <i>Skip connections</i> between the last four downscaling blocks and corresponding <i>upscaling</i> blocks that do the opposite, in the opposite order (e.g. simple upscale, then three cross-attention blocks).<p>- The aforementioned opposite blocks (upscale + cross-attn upscale)<p>- VAE decoder that goes from latent space back to a 512x512 output<p>At the end of this process you get what the combined model thinks is noise in the image according to the prompt you gave it. You then subtract the noise and repeat for a certain number of iterations until done. So obviously, if you wanted a smaller image, you could crop the input and output at each iteration so that the model can only draw in the &#x27;center&#x27;.<p>Larger images are a bit trickier, you have to feed the image through in halves and then merge the noise predictions together before subtracting. This of course has limitations: since the model is looking at only half the image, there&#x27;s nothing to steer the overall process, so it will draw things that look locally coherent but make no sense globally[1].<p>I <i>suspect</i> - as in, I&#x27;m totally guessing here - that we might be able to fix that by also running the diffusion process on a downscaled version of the image and then scaling the noise prediction back up to average with the other outputs. As far as I&#x27;m aware no SD frontends do this. But if that worked you could build up a resolution pyramid of models at different sizes taking fragments of the image and working together to denoise the image. If you were training from scratch you could even add scale and position information to the condition vector so the model can learn what image features should exist at what sizes.<p>[0] Think of this like if every pixel of the latent-space image was, instead of RGB, four different channels worth of information about the distribution of pixels in the color-space image. This compresses the image so that the U-Net part of the model can be architecturally simpler - in fact, lots of machine learning research is finding new ways to compress data into a smaller amount of input neurons.<p>[1] Moreso than diffusion models normally do</div><br/></div></div><div id="36486784" class="c"><input type="checkbox" id="c-36486784" checked=""/><div class="controls bullet"><span class="by">dplavery92</span><span>|</span><a href="#36484984">parent</a><span>|</span><a href="#36486882">prev</a><span>|</span><a href="#36485161">next</a><span>|</span><label class="collapse" for="c-36486784">[-]</label><label class="expand" for="c-36486784">[2 more]</label></div><br/><div class="children"><div class="content">Presumably a transformer model or similar that uses positional encodings for the tokens could do that, but the U-Net decoder here uses a fixed-shape output and learns relationships between tokens (and sizes of image features) based on the positions of those tokens in a fixed-size vector.  You could still apply this process convolutionally and slide the entire network around to generate an image that is an arbitrary multiple of the token size, but image content in one area of the image will only be &quot;aware&quot; of image content at a fixed-size neighborhood (e.g. 256x256).</div><br/><div id="36488687" class="c"><input type="checkbox" id="c-36488687" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#36484984">root</a><span>|</span><a href="#36486784">parent</a><span>|</span><a href="#36485161">next</a><span>|</span><label class="collapse" for="c-36488687">[-]</label><label class="expand" for="c-36488687">[1 more]</label></div><br/><div class="children"><div class="content">This is not how it works, there is no sliding window, the model has no restrictions on the W and H dimensions, only on the C dim, so the UNet can actually be used directly on images with different aspect ratio and resolution; the attention layers pay attention to the entire image.</div><br/></div></div></div></div></div></div><div id="36485668" class="c"><input type="checkbox" id="c-36485668" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#36485161">prev</a><span>|</span><label class="collapse" for="c-36485668">[-]</label><label class="expand" for="c-36485668">[4 more]</label></div><br/><div class="children"><div class="content">Their main claim of “faster” unfortunately is false.<p>&gt; Running on an Nvidia A100 GPU, Paella took 0.5 seconds to produce a 256x256-pixel image in eight steps, while Stable Diffusion took 3.2 seconds<p>Using the latest methods (torch 2.0 compile, improved schedulers) stable diffusion only takes about 1 second to generate a 512x512 image on an a100 gpu. A 256x256 image 1&#x2F;4 the size presumably takes less than half that time.<p>So the corrected title is “Like diffusion but slightly slower and lower quality.”</div><br/><div id="36487232" class="c"><input type="checkbox" id="c-36487232" checked=""/><div class="controls bullet"><span class="by">dome271</span><span>|</span><a href="#36485668">parent</a><span>|</span><a href="#36486061">next</a><span>|</span><label class="collapse" for="c-36487232">[-]</label><label class="expand" for="c-36487232">[2 more]</label></div><br/><div class="children"><div class="content">Hey, (one of the authors here). First of all the blog post is talking about the v1 of the paper, which was extremely fast, but not comparable to SD in any way. The v2 in arxiv is slower and does not achieve 0.5 seconds, but performs much better and closer to SD. So no doubt on this. But I just want to mention that you should not compare apples with oranges. Torch.compile also makes Paella much faster and using an optimized sampling pipeline it would always be faster than SD at 256x256 if you keep the conditions the same. Of course you could talk about distilling SD and then you can achieve maybe 1 step predictions etc. But you could probably do the same to Paella. I think it&#x27;s important to stick with the main improvement from the paper that naive sampling can be done with much less steps when sticking to the original method, while being simple in its theory and implementation. But hey, way to go and improve on Paella in the future maybe :D</div><br/><div id="36488149" class="c"><input type="checkbox" id="c-36488149" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#36485668">root</a><span>|</span><a href="#36487232">parent</a><span>|</span><a href="#36486061">next</a><span>|</span><label class="collapse" for="c-36488149">[-]</label><label class="expand" for="c-36488149">[1 more]</label></div><br/><div class="children"><div class="content">Hi! I really appreciate folks like you conducting and publishing real research. There have been a ton of companies recently which have been very rosily promoting their new models. My criticism was only to push back on overly optimistic marketing, and regret that some of it was directed at you. If you have a link to the v2 paper, would love to take a look!</div><br/></div></div></div></div><div id="36486061" class="c"><input type="checkbox" id="c-36486061" checked=""/><div class="controls bullet"><span class="by">jeron</span><span>|</span><a href="#36485668">parent</a><span>|</span><a href="#36487232">prev</a><span>|</span><label class="collapse" for="c-36486061">[-]</label><label class="expand" for="c-36486061">[1 more]</label></div><br/><div class="children"><div class="content">to add, there&#x27;s a finetuned version of Stable Diffusion 1.5 that can output 5 fps for 256x256 (0.2 seconds per image)[0]. So over 2x faster than Paella at 256x256<p>[0]:<a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;comments&#x2F;z3m97e&#x2F;minisd_a_256x256_finetune_of_stable_diffusion_15&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;comments&#x2F;z3m97e&#x2F;min...</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>