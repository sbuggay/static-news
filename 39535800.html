<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709197271225" as="style"/><link rel="stylesheet" href="styles.css?v=1709197271225"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.17764">The Era of 1-bit LLMs: ternary parameters for cost-effective computing</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>fgfm</span> | <span>292 comments</span></div><br/><div><div id="39537919" class="c"><input type="checkbox" id="c-39537919" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39536673">next</a><span>|</span><label class="collapse" for="c-39537919">[-]</label><label class="expand" for="c-39537919">[98 more]</label></div><br/><div class="children"><div class="content">There are two findings I find <i>shocking</i> in this work:<p>* In existing LLMs, we can replace all parameter floating-point values representing real numbers with ternary values representing (-1, 0, 1).<p>* In matrix multiplications (e.g., weights by vectors), we can replace elementwise products in each dot product (a₁b₁ + a₂b₂ ...) with elementwise <i>additions</i> (a₁+b₁ + a₂+b₂ ...), in which signs depend on each value. See the paper for exact details.<p>On existing hardware, the gains in compute and memory efficiency are significant, without performance degradation (as tested by the authors).<p>If the proposed methods are implemented in hardware, we will see <i>even greater gains</i> in compute and memory efficiency.<p>Wow.</div><br/><div id="39544500" class="c"><input type="checkbox" id="c-39544500" checked=""/><div class="controls bullet"><span class="by">paul_mk1</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39541965">next</a><span>|</span><label class="collapse" for="c-39544500">[-]</label><label class="expand" for="c-39544500">[11 more]</label></div><br/><div class="children"><div class="content">Fun to see ternary weights making a comeback. This was hot back in 2016 with BinaryConnect and TrueNorth chip from IBM research (disclosure, I was one of the lead chip architects there).<p>Authors seemed to have missed the history. They should at least cite Binary Connect or Straight Through Estimators (not my work).<p>Helpful hint to authors: you can get down to 0.68 bits &#x2F; weight using a similar technique, good chance this will work for LLMs too.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1606.01981" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1606.01981</a><p>This was a passion project of mine in my last few months at IBM research :).<p>I am convinced there is a deep connection to understanding why backprop is unreasonably effective, and the result that you can train low precision DNNs; for those note familiar, the technique is to compute the loss wrt to the low precision parameters (eg project to ternary) but apply the gradient to high precision copy of parameters (known as the straight through estimator). This is a biased estimator and there is no theoretical underpinning for why this should work, but in practice it works well.<p>My best guess is that it is encouraging the network to choose good underlying subnetworks to solve the problem, similar to Lottery Ticket Hypothesis. With ternary weights it is just about who connects to who (ie a graph), and not about the individual weight values anymore.</div><br/><div id="39546069" class="c"><input type="checkbox" id="c-39546069" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544500">parent</a><span>|</span><a href="#39546491">next</a><span>|</span><label class="collapse" for="c-39546069">[-]</label><label class="expand" for="c-39546069">[2 more]</label></div><br/><div class="children"><div class="content">They train using Straight Through Estimator but is cited in the previous BitNet paper. What happen to the TrueNorth Chip? I think investing in specialized hardware for AI is a good bet.</div><br/><div id="39546409" class="c"><input type="checkbox" id="c-39546409" checked=""/><div class="controls bullet"><span class="by">paul_mk1</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39546069">parent</a><span>|</span><a href="#39546491">next</a><span>|</span><label class="collapse" for="c-39546409">[-]</label><label class="expand" for="c-39546409">[1 more]</label></div><br/><div class="children"><div class="content">Nice to know there is a trail to relevant citations. I missed the BitNet paper and need to catch up.<p>Btw TrueNorth project evolved into &quot;NorthPole&quot; chip by the same group, and was recently in the press. From afar NorthPole looks like an interesting design point and leverages on-chip memory (SRAM)--so it&#x27;s targeting speed and efficiency at the expense of memory density (so perhaps like Groq in some respects). Tbh I haven&#x27;t followed the field closely after leaving the group.</div><br/></div></div></div></div><div id="39546491" class="c"><input type="checkbox" id="c-39546491" checked=""/><div class="controls bullet"><span class="by">mjcohen</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544500">parent</a><span>|</span><a href="#39546069">prev</a><span>|</span><a href="#39545698">next</a><span>|</span><label class="collapse" for="c-39546491">[-]</label><label class="expand" for="c-39546491">[3 more]</label></div><br/><div class="children"><div class="content">IIRC, Hamming&#x27;s book &quot;Digital Filters&quot; (1989) has a section on FFTs with only the sign of the coefficient being used. It performed surprisingly well.</div><br/><div id="39547168" class="c"><input type="checkbox" id="c-39547168" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39546491">parent</a><span>|</span><a href="#39546845">next</a><span>|</span><label class="collapse" for="c-39547168">[-]</label><label class="expand" for="c-39547168">[1 more]</label></div><br/><div class="children"><div class="content">What is the sign of a complex number? Do you mean the phase?</div><br/></div></div><div id="39546845" class="c"><input type="checkbox" id="c-39546845" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39546491">parent</a><span>|</span><a href="#39547168">prev</a><span>|</span><a href="#39545698">next</a><span>|</span><label class="collapse" for="c-39546845">[-]</label><label class="expand" for="c-39546845">[1 more]</label></div><br/><div class="children"><div class="content">You mean Fast Hadamard Transform?</div><br/></div></div></div></div><div id="39545698" class="c"><input type="checkbox" id="c-39545698" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544500">parent</a><span>|</span><a href="#39546491">prev</a><span>|</span><a href="#39545417">next</a><span>|</span><label class="collapse" for="c-39545698">[-]</label><label class="expand" for="c-39545698">[1 more]</label></div><br/><div class="children"><div class="content">You can probably apply the same techniques &#x27;Deep neural networks are robust to weight binarization and other non-linear distortions&#x27; used to get to 0.68 bits &#x2F; weight to get your ternary weights below one bit; so you can claim they are still one-bit networks.</div><br/></div></div><div id="39545417" class="c"><input type="checkbox" id="c-39545417" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544500">parent</a><span>|</span><a href="#39545698">prev</a><span>|</span><a href="#39546049">next</a><span>|</span><label class="collapse" for="c-39545417">[-]</label><label class="expand" for="c-39545417">[2 more]</label></div><br/><div class="children"><div class="content">That’s really interesting to see the breadcrumb trail goes back that far.<p>So what are the most important insights in this paper compared to what was previously done?<p>I assume there’s more context to the story and it’s not just that no one thought to apply the concepts to LLM’s until now?</div><br/><div id="39546444" class="c"><input type="checkbox" id="c-39546444" checked=""/><div class="controls bullet"><span class="by">paul_mk1</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39545417">parent</a><span>|</span><a href="#39546049">next</a><span>|</span><label class="collapse" for="c-39546444">[-]</label><label class="expand" for="c-39546444">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think there is anything conceptually new in this work, other than it is applied to LLMs.<p>But in fairness, getting these techniques to work at scale is no small feat. In my experience quantization aware training at these low bit depths was always finicky and required a very careful hand. I&#x27;d be interested to know if it has become easier to do, now that there are so many more parameters in LLMs.<p>In any case full kudos to the authors and I&#x27;m glad to see people continuing this work.</div><br/></div></div></div></div><div id="39546049" class="c"><input type="checkbox" id="c-39546049" checked=""/><div class="controls bullet"><span class="by">antimatter15</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544500">parent</a><span>|</span><a href="#39545417">prev</a><span>|</span><a href="#39545209">next</a><span>|</span><label class="collapse" for="c-39546049">[-]</label><label class="expand" for="c-39546049">[1 more]</label></div><br/><div class="children"><div class="content">They cite straight through estimators in the previous work with many of the same authors on (actual binary) BitNet</div><br/></div></div><div id="39545209" class="c"><input type="checkbox" id="c-39545209" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544500">parent</a><span>|</span><a href="#39546049">prev</a><span>|</span><a href="#39541965">next</a><span>|</span><label class="collapse" for="c-39545209">[-]</label><label class="expand" for="c-39545209">[1 more]</label></div><br/><div class="children"><div class="content">As aside, I&#x27;m curious: what was it like to work at IBM research, especially as a legacy industrial research org?</div><br/></div></div></div></div><div id="39541965" class="c"><input type="checkbox" id="c-39541965" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39544500">prev</a><span>|</span><a href="#39538626">next</a><span>|</span><label class="collapse" for="c-39541965">[-]</label><label class="expand" for="c-39541965">[21 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be VERRY cautious about being excited here.<p>My priors are like this:<p>1. Initial training of a neural network moves all weights around a large amount at first.<p>2. Later training of the network adjusts them a small amount.<p>3. An undertrained network will therefore look a lot like figuring out &quot;positive, negative, or 0?&quot; for each node during early training.<p>If all these things are true, then<p>1. Early training of an fp16 network and a bitnet with 0 added will be <i>roughly</i> similar in results<p>2. Later training will yield different &#x2F; worse results, as the network gets into the &#x27;fine tuning&#x27; part of the training.<p>I think the paper&#x27;s stats back these priors up -- they say &quot;this works on (3B+) large networks, but not small ones.&quot; They then imply there&#x27;s something about the structure of a large network that allows a bitnet to do well. It seems more likely to me it works on large networks because they have not put the compute into 3B+ networks to get past the &#x27;gross tuning&#x27; phase.<p>The networks they have compute to put in to get them &#x27;fully&#x27; trained -- those networks don&#x27;t show the results.<p>Also, a quick reminder that Perplexity 12 is really terrible. You would not want to use such a network. Hopefully I&#x27;m wrong and we can get something for free here! But, I&#x27;m cautious - to - skeptical.</div><br/><div id="39543193" class="c"><input type="checkbox" id="c-39543193" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541965">parent</a><span>|</span><a href="#39542748">next</a><span>|</span><label class="collapse" for="c-39543193">[-]</label><label class="expand" for="c-39543193">[5 more]</label></div><br/><div class="children"><div class="content">Update - I&#x27;m still cautious about this paper, but I had the table numbers inverted in my head while thinking about it. The paper shows better perplexity results than competing models at larger parameter sizes, so I was wrong.</div><br/><div id="39544923" class="c"><input type="checkbox" id="c-39544923" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39543193">parent</a><span>|</span><a href="#39542748">next</a><span>|</span><label class="collapse" for="c-39544923">[-]</label><label class="expand" for="c-39544923">[4 more]</label></div><br/><div class="children"><div class="content">I was pretty unhappy and suspicious for the same reason. Not reporting perplexity for a 70B network while reporting its efficiency means that someone did something and the result wasn&#x27;t good enough to put in the paper.</div><br/><div id="39544978" class="c"><input type="checkbox" id="c-39544978" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544923">parent</a><span>|</span><a href="#39542748">next</a><span>|</span><label class="collapse" for="c-39544978">[-]</label><label class="expand" for="c-39544978">[3 more]</label></div><br/><div class="children"><div class="content">According to the author, the 70B model is not fully trained.</div><br/><div id="39545478" class="c"><input type="checkbox" id="c-39545478" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544978">parent</a><span>|</span><a href="#39542748">next</a><span>|</span><label class="collapse" for="c-39545478">[-]</label><label class="expand" for="c-39545478">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Is not fully trained&quot; can also mean &quot;we did not figure out how to reach an acceptable loss&quot; or &quot;training was unstable,&quot; both of which are common for ML systems.</div><br/><div id="39545822" class="c"><input type="checkbox" id="c-39545822" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39545478">parent</a><span>|</span><a href="#39542748">next</a><span>|</span><label class="collapse" for="c-39545822">[-]</label><label class="expand" for="c-39545822">[1 more]</label></div><br/><div class="children"><div class="content">It probably means that the model is not fully trained, because it is very expensive to train a 70B model, not even Mamba or RWKV have a model that comes close to that size, the leeriness is just kinda silly honestly.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39542748" class="c"><input type="checkbox" id="c-39542748" checked=""/><div class="controls bullet"><span class="by">svantana</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541965">parent</a><span>|</span><a href="#39543193">prev</a><span>|</span><a href="#39544828">next</a><span>|</span><label class="collapse" for="c-39542748">[-]</label><label class="expand" for="c-39542748">[2 more]</label></div><br/><div class="children"><div class="content">Wait, are we reading the same paper? What I&#x27;m seeing is comparable accuracy to unquantized models for &lt;4B params, and nothing reported for larger models except resource consumption.</div><br/><div id="39543186" class="c"><input type="checkbox" id="c-39543186" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39542748">parent</a><span>|</span><a href="#39544828">next</a><span>|</span><label class="collapse" for="c-39543186">[-]</label><label class="expand" for="c-39543186">[1 more]</label></div><br/><div class="children"><div class="content">Nope, you&#x27;re right, I got the table inverted in my head. I&#x27;m updating my top comment.</div><br/></div></div></div></div><div id="39544828" class="c"><input type="checkbox" id="c-39544828" checked=""/><div class="controls bullet"><span class="by">gradascent</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541965">parent</a><span>|</span><a href="#39542748">prev</a><span>|</span><a href="#39543146">next</a><span>|</span><label class="collapse" for="c-39544828">[-]</label><label class="expand" for="c-39544828">[1 more]</label></div><br/><div class="children"><div class="content">Then perhaps a method emerges out of this to make training faster (but not inference) - do early training on highly quantized (even ternary) weights, and then swap out the weights for fp16 or something and fine-tune? Might save $$$ in training large models.</div><br/></div></div><div id="39543146" class="c"><input type="checkbox" id="c-39543146" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541965">parent</a><span>|</span><a href="#39544828">prev</a><span>|</span><a href="#39542071">next</a><span>|</span><label class="collapse" for="c-39543146">[-]</label><label class="expand" for="c-39543146">[3 more]</label></div><br/><div class="children"><div class="content">Thank you. Your key point -- that so far all models with the proposed methods may have been only &quot;grossly trained&quot; -- is compelling. If I understand the authors correctly, they trained the compared models on only 100B tokens, all drawn from RedPajama, to make the comparisons apples-to-apples. That seems sensible to me, and makes replication easier, but I agree we need more to see extensive testing, after more extensive pretraining, on models of larger sizes.</div><br/><div id="39543267" class="c"><input type="checkbox" id="c-39543267" checked=""/><div class="controls bullet"><span class="by">gliptic</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39543146">parent</a><span>|</span><a href="#39542071">next</a><span>|</span><label class="collapse" for="c-39543267">[-]</label><label class="expand" for="c-39543267">[2 more]</label></div><br/><div class="children"><div class="content">They also trained 3B with 2 trillion tokens.<p>&gt; The number of training tokens is a crucial factor for LLMs. To test the scalability of BitNet b1.58 in terms of tokens, we trained a BitNet b1.58 model with 2T tokens following the data recipe of StableLM-3B [ TBMR], which is the state-of-the-art open-source 3B model.<p>&gt; [..]<p>&gt; Our findings shows that BitNet b1.58 achieves a superior performance on all end tasks, indicating that 1.58-bit LLMs also have strong generalization capabilities.</div><br/><div id="39544001" class="c"><input type="checkbox" id="c-39544001" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39543267">parent</a><span>|</span><a href="#39542071">next</a><span>|</span><label class="collapse" for="c-39544001">[-]</label><label class="expand" for="c-39544001">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right. Thank you for pointing that out!</div><br/></div></div></div></div></div></div><div id="39542071" class="c"><input type="checkbox" id="c-39542071" checked=""/><div class="controls bullet"><span class="by">mise_en_place</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541965">parent</a><span>|</span><a href="#39543146">prev</a><span>|</span><a href="#39542849">next</a><span>|</span><label class="collapse" for="c-39542071">[-]</label><label class="expand" for="c-39542071">[8 more]</label></div><br/><div class="children"><div class="content">Intuitively I&#x27;ve always been a bit skeptical of quantization. Wouldn&#x27;t there be a tiny loss in precision by doing this type of quantization? I could imagine the error function increasing by utilizing these types of techniques.</div><br/><div id="39542702" class="c"><input type="checkbox" id="c-39542702" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39542071">parent</a><span>|</span><a href="#39544554">next</a><span>|</span><label class="collapse" for="c-39542702">[-]</label><label class="expand" for="c-39542702">[1 more]</label></div><br/><div class="children"><div class="content">It does increase the “error” (meaning it is less likely to predict the next word when compared against a dataset) but the losses are lower than your intuition would guide you to believe.</div><br/></div></div><div id="39544554" class="c"><input type="checkbox" id="c-39544554" checked=""/><div class="controls bullet"><span class="by">thesz</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39542071">parent</a><span>|</span><a href="#39542702">prev</a><span>|</span><a href="#39542741">next</a><span>|</span><label class="collapse" for="c-39544554">[-]</label><label class="expand" for="c-39544554">[4 more]</label></div><br/><div class="children"><div class="content">John Carmack pointed out (and I learned it here at HN) that what training really needs is the *sign&quot; of each individual gradient parameter. I.e., you can quantize gradient to -1, 0 and 1 and still have neural network learn much of the dataset.</div><br/><div id="39545880" class="c"><input type="checkbox" id="c-39545880" checked=""/><div class="controls bullet"><span class="by">farhanhubble</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544554">parent</a><span>|</span><a href="#39545286">next</a><span>|</span><label class="collapse" for="c-39545880">[-]</label><label class="expand" for="c-39545880">[2 more]</label></div><br/><div class="children"><div class="content">Wow! Is there a link to read up more on this?</div><br/><div id="39547018" class="c"><input type="checkbox" id="c-39547018" checked=""/><div class="controls bullet"><span class="by">thesz</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39545880">parent</a><span>|</span><a href="#39545286">next</a><span>|</span><label class="collapse" for="c-39547018">[-]</label><label class="expand" for="c-39547018">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; It is interesting that things still train even when various parts are pretty wrong — as long as the sign is right most of the time, progress is often made.
</code></pre>
<a href="https:&#x2F;&#x2F;forums.fast.ai&#x2F;t&#x2F;how-to-do-reproducible-models-and-unit-testing&#x2F;14719" rel="nofollow">https:&#x2F;&#x2F;forums.fast.ai&#x2F;t&#x2F;how-to-do-reproducible-models-and-u...</a></div><br/></div></div></div></div><div id="39545286" class="c"><input type="checkbox" id="c-39545286" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544554">parent</a><span>|</span><a href="#39545880">prev</a><span>|</span><a href="#39542741">next</a><span>|</span><label class="collapse" for="c-39545286">[-]</label><label class="expand" for="c-39545286">[1 more]</label></div><br/><div class="children"><div class="content">Why isn&#x27;t John Carmack working for OpenAI? Hell, why did he waste years at Meta to work on a VR headset and NOT AI? He even announced he wants to focus on AGI but he missed out on literally all the action.</div><br/></div></div></div></div><div id="39542741" class="c"><input type="checkbox" id="c-39542741" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39542071">parent</a><span>|</span><a href="#39544554">prev</a><span>|</span><a href="#39543885">next</a><span>|</span><label class="collapse" for="c-39542741">[-]</label><label class="expand" for="c-39542741">[1 more]</label></div><br/><div class="children"><div class="content">Quantization does reduce quality of the outputs. But the point is that you save enough memory doing so that you can cram a larger model into the same hardware, and this more than compensates for lost precision.</div><br/></div></div><div id="39543885" class="c"><input type="checkbox" id="c-39543885" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39542071">parent</a><span>|</span><a href="#39542741">prev</a><span>|</span><a href="#39542849">next</a><span>|</span><label class="collapse" for="c-39543885">[-]</label><label class="expand" for="c-39543885">[1 more]</label></div><br/><div class="children"><div class="content">Yes each weight will not be able to &quot;learn&quot; as much if it has less bits of precision. But the idea is that you can use more weights, and the big question is whether these low-precision weights can make the model more accurate, as a whole.</div><br/></div></div></div></div><div id="39542849" class="c"><input type="checkbox" id="c-39542849" checked=""/><div class="controls bullet"><span class="by">gliptic</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541965">parent</a><span>|</span><a href="#39542071">prev</a><span>|</span><a href="#39538626">next</a><span>|</span><label class="collapse" for="c-39542849">[-]</label><label class="expand" for="c-39542849">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Also, a quick reminder that Perplexity 12 is really terrible.<p>The 3B model had a perplexity of 9.91, less than LLaMa 1 in fp16.</div><br/></div></div></div></div><div id="39538626" class="c"><input type="checkbox" id="c-39538626" checked=""/><div class="controls bullet"><span class="by">nutanc</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39541965">prev</a><span>|</span><a href="#39538470">next</a><span>|</span><label class="collapse" for="c-39538626">[-]</label><label class="expand" for="c-39538626">[13 more]</label></div><br/><div class="children"><div class="content">We have been experimenting with the paper(<a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;372834606_ON_NON-ITERATIVE_NEURAL_ALGORITHMS_BASED_ON_THE_SEPARATION_OF_HIGH_DIMENSIONED_SAMPLE_POINTS_BY_HYPERPLANES" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;372834606_ON_NON-IT...</a>).<p>There is a mathematical proof that binary representation is enough to capture the latent space. And in fact we don&#x27;t even need to do &quot;training&quot; to get that representation.<p>The practical application we tried out for this algorithm was to create an alternate space for mpnet embeddings of Wikipedia paragraphs. Using Bit embedding we are able to represent 36 million passages of Wikipedia in 2GB.(<a href="https:&#x2F;&#x2F;gpt3experiments.substack.com&#x2F;p&#x2F;building-a-vector-database-in-2gb" rel="nofollow">https:&#x2F;&#x2F;gpt3experiments.substack.com&#x2F;p&#x2F;building-a-vector-dat...</a>)</div><br/><div id="39541873" class="c"><input type="checkbox" id="c-39541873" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538626">parent</a><span>|</span><a href="#39540342">next</a><span>|</span><label class="collapse" for="c-39541873">[-]</label><label class="expand" for="c-39541873">[1 more]</label></div><br/><div class="children"><div class="content">Wow, this works better than I would&#x27;ve thought.<p>&gt; Who moderates Hacker News?<p>First result:<p>&gt; Hacker News<p>&gt; At the end of March 2014, Graham stepped away from his leadership role at Y Combinator, leaving Hacker News administration in the hands of other staff members. The site is currently moderated by Daniel Gackle who posts under the username &quot;dang&quot;.</div><br/></div></div><div id="39540342" class="c"><input type="checkbox" id="c-39540342" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538626">parent</a><span>|</span><a href="#39541873">prev</a><span>|</span><a href="#39543632">next</a><span>|</span><label class="collapse" for="c-39540342">[-]</label><label class="expand" for="c-39540342">[4 more]</label></div><br/><div class="children"><div class="content">You&#x27;re talking about mapping floating-point vector representations, i.e., embeddings, computed by a pretrained LLM to binary vector representations, right? And you&#x27;re talking about doing this by first having <i>someone else</i>&#x27;s pretrained LLM compute the embeddings, right? Sorry, but that seems only minimally, tangentially related to the topic of running LLMs in ternary space. I don&#x27;t see how your comment is relevant to the discussion here.</div><br/><div id="39540671" class="c"><input type="checkbox" id="c-39540671" checked=""/><div class="controls bullet"><span class="by">nutanc</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39540342">parent</a><span>|</span><a href="#39543632">next</a><span>|</span><label class="collapse" for="c-39540671">[-]</label><label class="expand" for="c-39540671">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, sorry, needed a much bigger canvas than a comment to explain. Let me try again. The example I took was to show mapping from one space to another space and it may have just come across as not learning anything. Yes. You are right it was someone else&#x27;s pretrained LLM. But this new space learnt the latent representations of the original embedding space. Now, instead of the original embedding space it could also have been some image representation or some audio representation. Even neural networks take input in X space and learn a representation in Y space. The paper shows that any layer of a neural network can in fact be replaced with a set of planes and we can represent a space using those planes and that those planes can be created in a non iterative way.
Not sure if I am being clear, but have written a small blog post to show for MNIST how an NN creates the planes(<a href="https:&#x2F;&#x2F;gpt3experiments.substack.com&#x2F;p&#x2F;understanding-neural-networks-and" rel="nofollow">https:&#x2F;&#x2F;gpt3experiments.substack.com&#x2F;p&#x2F;understanding-neural-...</a>). Will write more on how once these planes are written, how we can use a bit representation instead of floating point values to get similar accuracy in prediction and next how we can draw those planes without the iterative training process.</div><br/><div id="39544842" class="c"><input type="checkbox" id="c-39544842" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39540671">parent</a><span>|</span><a href="#39543632">next</a><span>|</span><label class="collapse" for="c-39544842">[-]</label><label class="expand" for="c-39544842">[2 more]</label></div><br/><div class="children"><div class="content">&gt; how we can draw those planes without the iterative training process.<p>Sounds interesting, but this is the part I would need more explanation on.<p>Just started reading your linked blog, I see it goes into some details there.</div><br/><div id="39546202" class="c"><input type="checkbox" id="c-39546202" checked=""/><div class="controls bullet"><span class="by">nutanc</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544842">parent</a><span>|</span><a href="#39543632">next</a><span>|</span><label class="collapse" for="c-39546202">[-]</label><label class="expand" for="c-39546202">[1 more]</label></div><br/><div class="children"><div class="content">Will add a lot more details next week. Have been postponing it for a long time.</div><br/></div></div></div></div></div></div></div></div><div id="39543632" class="c"><input type="checkbox" id="c-39543632" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538626">parent</a><span>|</span><a href="#39540342">prev</a><span>|</span><a href="#39541987">next</a><span>|</span><label class="collapse" for="c-39543632">[-]</label><label class="expand" for="c-39543632">[2 more]</label></div><br/><div class="children"><div class="content">I find this extremely interesting. Do you share the source code of the process? any more references?</div><br/><div id="39546223" class="c"><input type="checkbox" id="c-39546223" checked=""/><div class="controls bullet"><span class="by">nutanc</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39543632">parent</a><span>|</span><a href="#39541987">next</a><span>|</span><label class="collapse" for="c-39546223">[-]</label><label class="expand" for="c-39546223">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately the source code is currently not open sourced. Some more details at (<a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;370980395_A_NEURAL_INVERSE_TECHNIQUE_FOR_PATTERN_CLASSIFICATION_AND_MEMORY_RECALL" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;370980395_A_NEURAL_...</a>), the source code is built on top of this.<p>The approach is used to solve other problems and papers have been published under <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;profile&#x2F;K-Eswaran" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;profile&#x2F;K-Eswaran</a><p>We are currently trying a build a full fledged LLM using just this approach(no LLM training etc) and also an ASR. We should have something to share in a couple of months.</div><br/></div></div></div></div><div id="39541987" class="c"><input type="checkbox" id="c-39541987" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538626">parent</a><span>|</span><a href="#39543632">prev</a><span>|</span><a href="#39538470">next</a><span>|</span><label class="collapse" for="c-39541987">[-]</label><label class="expand" for="c-39541987">[5 more]</label></div><br/><div class="children"><div class="content">How is this not lossy compression?</div><br/><div id="39542215" class="c"><input type="checkbox" id="c-39542215" checked=""/><div class="controls bullet"><span class="by">sandyarmstrong</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541987">parent</a><span>|</span><a href="#39542458">next</a><span>|</span><label class="collapse" for="c-39542215">[-]</label><label class="expand" for="c-39542215">[2 more]</label></div><br/><div class="children"><div class="content">LLMs and vector embeddings are always lossy compression, yes?</div><br/><div id="39545751" class="c"><input type="checkbox" id="c-39545751" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39542215">parent</a><span>|</span><a href="#39542458">next</a><span>|</span><label class="collapse" for="c-39545751">[-]</label><label class="expand" for="c-39545751">[1 more]</label></div><br/><div class="children"><div class="content">Almost always.  Though you can use them in a lossless compression system, too, with a few tricks.</div><br/></div></div></div></div><div id="39542458" class="c"><input type="checkbox" id="c-39542458" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541987">parent</a><span>|</span><a href="#39542215">prev</a><span>|</span><a href="#39542191">next</a><span>|</span><label class="collapse" for="c-39542458">[-]</label><label class="expand" for="c-39542458">[1 more]</label></div><br/><div class="children"><div class="content">kind of related: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@heinrichpeters&#x2F;commentary-gzip-knn-beats-deep-neural-networks-in-text-classification-f395c71283a6" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@heinrichpeters&#x2F;commentary-gzip-knn-beats...</a></div><br/></div></div><div id="39542191" class="c"><input type="checkbox" id="c-39542191" checked=""/><div class="controls bullet"><span class="by">rf15</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39541987">parent</a><span>|</span><a href="#39542458">prev</a><span>|</span><a href="#39538470">next</a><span>|</span><label class="collapse" for="c-39542191">[-]</label><label class="expand" for="c-39542191">[1 more]</label></div><br/><div class="children"><div class="content">It kind of is!</div><br/></div></div></div></div></div></div><div id="39538470" class="c"><input type="checkbox" id="c-39538470" checked=""/><div class="controls bullet"><span class="by">creshal</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39538626">prev</a><span>|</span><a href="#39544181">next</a><span>|</span><label class="collapse" for="c-39538470">[-]</label><label class="expand" for="c-39538470">[20 more]</label></div><br/><div class="children"><div class="content">&gt; * In existing LLMs, we can replace all parameter floating-point values representing real numbers with ternary values representing (-1, 0, 1).<p>Why is this so shocking? Quantization has been widely explored, driving that to its extreme (and blowing up parameter count to make up for it) just seems like a natural extension of that.<p>Easier said than done, of course, and very impressive that they pulled it off.<p>&gt;  In matrix multiplications (e.g., weights by vectors), we can replace elementwise products in each dot product (a₁b₁ + a₂b₂ ...) with elementwise additions (a₁+b₁ + a₂+b₂ ...), in which signs depend on each value<p>I feel like this follows naturally from having only ternary values, multiplication doesn&#x27;t really bring much to the table here. It&#x27;s a bit surprising that it&#x27;s performing so well on existing hardware, usually multiplication hardware sees more optimization, especially for GPGPU hardware.</div><br/><div id="39538760" class="c"><input type="checkbox" id="c-39538760" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538470">parent</a><span>|</span><a href="#39538866">next</a><span>|</span><label class="collapse" for="c-39538760">[-]</label><label class="expand" for="c-39538760">[10 more]</label></div><br/><div class="children"><div class="content"><i>&gt; Why is this so shocking? Quantization has been widely explored, driving that to its extreme (and blowing up parameter count to make up for it) just seems like a natural extension of that.</i><p>I find it shocking that we don&#x27;t even need lower floating-point precision. <i>We don&#x27;t need precision at all</i>. We only need three symbols to represent every value.<p><i>&gt; I feel like this follows naturally from having only ternary values, multiplication doesn&#x27;t really bring much to the table here. It&#x27;s a bit surprising that it&#x27;s performing so well on existing hardware, usually multiplication hardware sees more optimization, especially for GPGPU hardware.</i><p>I find it shocking. Consider that associative addition over ternary digits, or trits, represented by three symbols (a,b,c) has only three possible input pairs, (a,b), (a,c), or (b,c) (within each pair, order doesn&#x27;t matter), and only three possible outputs, a, b, or c. Matrix multiplications could be executed via crazy-cheap tritwise operations in hardware. Maybe ternary hardware[a] will become a thing in AI?<p>---<p>[a] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ternary_computer" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ternary_computer</a></div><br/><div id="39539180" class="c"><input type="checkbox" id="c-39539180" checked=""/><div class="controls bullet"><span class="by">jerf</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538760">parent</a><span>|</span><a href="#39545695">next</a><span>|</span><label class="collapse" for="c-39539180">[-]</label><label class="expand" for="c-39539180">[4 more]</label></div><br/><div class="children"><div class="content">An integer is just a concatenation of bits. Floating point appears more complicated but from an information theory perspective it is also just a concatenation of bits. If, for the sake of argument, one replaced a 64-bit int with 64 individual bits, that&#x27;s really the same amount of information and a structure could hypothetically then either choose to recreate the original 64-bit int, or use the 64-bits more efficiently by choosing from the much larger set of possibilities of ways to use such resources.<p>Trits are helpful for neural nets, though, since they really love signs and they need a 0.<p>So from the perspective that it&#x27;s all just bits in the end the only thing that is interesting is how useful it is to arrange those bits into trits for this particular algorithm, and that the algorithm seems to be able to use things more effectively that way than with raw bits.<p>This may seem an absolutely bizarre zigzag, but I am reminded of Busy Beavers, because of the way they take very the very small primitives of a Turing Machine, break it down to the smallest pieces, then combine them in ways that almost immediately cease to be humanly comprehensible. Completely different selection mechanism for what appears, but it turns out Turing Machine states can do a lot &quot;more&quot; than you might think simply by looking at human-designed TMs. We humans have very stereotypical design methodologies and they have their advantages, but sometimes just letting algorithms rip can result in much better things than we could ever hope to design with the same resources.</div><br/><div id="39540589" class="c"><input type="checkbox" id="c-39540589" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39539180">parent</a><span>|</span><a href="#39545763">next</a><span>|</span><label class="collapse" for="c-39540589">[-]</label><label class="expand" for="c-39540589">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; So from the perspective that it&#x27;s all just bits in the end the only thing that is interesting is how useful it is to arrange those bits into trits for this particular algorithm, and that the algorithm seems to be able to use things more effectively that way than with raw bits.</i><p>Thank you. I find many other things interesting here, including the potential implications for hardware, but otherwise, yes, I agree with you, <i>that</i> is interesting.</div><br/></div></div><div id="39545763" class="c"><input type="checkbox" id="c-39545763" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39539180">parent</a><span>|</span><a href="#39540589">prev</a><span>|</span><a href="#39542941">next</a><span>|</span><label class="collapse" for="c-39545763">[-]</label><label class="expand" for="c-39545763">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We humans have very stereotypical design methodologies and they have their advantages, but sometimes just letting algorithms rip can result in much better things than we could ever hope to design with the same resources.<p>Yes.  Though here the interesting point is not so much that these structures exist, but that &#x27;stupid&#x27; back-propagation is smart enough to find them.<p>You can&#x27;t find busy beavers like that.</div><br/></div></div><div id="39542941" class="c"><input type="checkbox" id="c-39542941" checked=""/><div class="controls bullet"><span class="by">SkyBelow</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39539180">parent</a><span>|</span><a href="#39545763">prev</a><span>|</span><a href="#39545695">next</a><span>|</span><label class="collapse" for="c-39542941">[-]</label><label class="expand" for="c-39542941">[1 more]</label></div><br/><div class="children"><div class="content">This sort of breakdown also reminds me of the explanation of why busy beavers grow faster than anything humans can ever define.  Anything a human can define is a finite number of steps that can be represented by some turing machine of size M.  A turning machine of size N &gt; M can then use M as a subset of it, growing faster than than the turing machine of size M.  Either it is the busy beaver for size N, or it grows slower than the busy beaver for size N.  Either way, the busy beaver for size N grows faster than whatever the human defined that was captured by the turning machine of size M.  This explanation was what helped me understand why busy beavers is faster growing than any operator that can be formally defined (obviously you can define an operator that references busy beaver itself, but busy beaver can be considered to not be formally defined, and thus any operator defined used it isn&#x27;t formally defined either).<p>The bit about floating point numbers just being a collection of bits interpreted in a certain way helps make sense why a bigger model doesn&#x27;t need floating points at all.</div><br/></div></div></div></div><div id="39545695" class="c"><input type="checkbox" id="c-39545695" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538760">parent</a><span>|</span><a href="#39539180">prev</a><span>|</span><a href="#39539273">next</a><span>|</span><label class="collapse" for="c-39545695">[-]</label><label class="expand" for="c-39545695">[1 more]</label></div><br/><div class="children"><div class="content">If you find three symbols per weight shocking, this paper should completely blow your mind: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.03764" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.03764</a><p>I admit it did shock me when it came out.</div><br/></div></div><div id="39539273" class="c"><input type="checkbox" id="c-39539273" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538760">parent</a><span>|</span><a href="#39545695">prev</a><span>|</span><a href="#39539354">next</a><span>|</span><label class="collapse" for="c-39539273">[-]</label><label class="expand" for="c-39539273">[3 more]</label></div><br/><div class="children"><div class="content">The matrices (weights) are ternary.<p>The vectors are not.</div><br/><div id="39539382" class="c"><input type="checkbox" id="c-39539382" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39539273">parent</a><span>|</span><a href="#39539354">next</a><span>|</span><label class="collapse" for="c-39539382">[-]</label><label class="expand" for="c-39539382">[2 more]</label></div><br/><div class="children"><div class="content">The <i>activations</i> are in (-1, 1), so they&#x27;re also representable by (-1, 0, 1).</div><br/><div id="39547353" class="c"><input type="checkbox" id="c-39547353" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39539382">parent</a><span>|</span><a href="#39539354">next</a><span>|</span><label class="collapse" for="c-39547353">[-]</label><label class="expand" for="c-39547353">[1 more]</label></div><br/><div class="children"><div class="content">This is wrong. The paper described that their activation is in int8 during inference.<p>That being said, before-LLM-era deep learning already had low bit quantization down to 1w2f [0] working back in 2016 [1]. So it&#x27;s certainly possible it would work for LLM too.<p>[0] 1-bit weights, 2-bit activations; though practically people deployed 2w4f instead.
[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1606.06160" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1606.06160</a></div><br/></div></div></div></div></div></div></div></div><div id="39538866" class="c"><input type="checkbox" id="c-39538866" checked=""/><div class="controls bullet"><span class="by">satellite2</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538470">parent</a><span>|</span><a href="#39538760">prev</a><span>|</span><a href="#39546931">next</a><span>|</span><label class="collapse" for="c-39538866">[-]</label><label class="expand" for="c-39538866">[3 more]</label></div><br/><div class="children"><div class="content">Because it&#x27;s no longer a linear optimization or curve fitting problem. It becomes a voting or combinatorial problem. Which at least in my mind are two completely different areas of research.</div><br/><div id="39539469" class="c"><input type="checkbox" id="c-39539469" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538866">parent</a><span>|</span><a href="#39546931">next</a><span>|</span><label class="collapse" for="c-39539469">[-]</label><label class="expand" for="c-39539469">[2 more]</label></div><br/><div class="children"><div class="content">With enough parameters, it probably starts looking continuous again. Like how in physics everything is quantised at the smallest scale but if you put enough atoms together it all smooths out and behaves &quot;classically&quot;.</div><br/><div id="39543573" class="c"><input type="checkbox" id="c-39543573" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39539469">parent</a><span>|</span><a href="#39546931">next</a><span>|</span><label class="collapse" for="c-39543573">[-]</label><label class="expand" for="c-39543573">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but we can simulate classical physics using mathematical shortcuts. Simulating every little atom would take a lot more work.</div><br/></div></div></div></div></div></div><div id="39546931" class="c"><input type="checkbox" id="c-39546931" checked=""/><div class="controls bullet"><span class="by">gemeral</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538470">parent</a><span>|</span><a href="#39538866">prev</a><span>|</span><a href="#39542090">next</a><span>|</span><label class="collapse" for="c-39546931">[-]</label><label class="expand" for="c-39546931">[1 more]</label></div><br/><div class="children"><div class="content">&gt; and blowing up parameter count to make up for it<p>based on (an admittedly rapid and indulgent reading of the paper), it seems like they&#x27;re not increasing the parameter size. Do you mind pointing out where the blowup is occurring?</div><br/></div></div><div id="39542090" class="c"><input type="checkbox" id="c-39542090" checked=""/><div class="controls bullet"><span class="by">ncruces</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538470">parent</a><span>|</span><a href="#39546931">prev</a><span>|</span><a href="#39544181">next</a><span>|</span><label class="collapse" for="c-39542090">[-]</label><label class="expand" for="c-39542090">[5 more]</label></div><br/><div class="children"><div class="content">Well I guess it&#x27;s the “blowing up parameter count to make up for it” that confuses me, but maybe it&#x27;s just ignorance.<p>Like what would be the expected factor of this blow up to make up the difference between ternary and whatever 16 bits encoding they were using?<p>I mean intuitively I&#x27;d expect to need ~10× the symbols to encode the same information? Are they using an order of magnitude more parameters, or is that not how it works?</div><br/><div id="39542768" class="c"><input type="checkbox" id="c-39542768" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39542090">parent</a><span>|</span><a href="#39544181">next</a><span>|</span><label class="collapse" for="c-39542768">[-]</label><label class="expand" for="c-39542768">[4 more]</label></div><br/><div class="children"><div class="content">With existing common quantization techniques, a 70b model quantized to 3-bit still drastically outperforms an unquantized 35b model.</div><br/><div id="39545672" class="c"><input type="checkbox" id="c-39545672" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39542768">parent</a><span>|</span><a href="#39544181">next</a><span>|</span><label class="collapse" for="c-39545672">[-]</label><label class="expand" for="c-39545672">[3 more]</label></div><br/><div class="children"><div class="content">Are you sure? I was under impression that 3b quantization still results in a significant degradation. Which quantization method are you talking about?</div><br/><div id="39546183" class="c"><input type="checkbox" id="c-39546183" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39545672">parent</a><span>|</span><a href="#39544181">next</a><span>|</span><label class="collapse" for="c-39546183">[-]</label><label class="expand" for="c-39546183">[2 more]</label></div><br/><div class="children"><div class="content">It does result in a significant degradation relative to unquantized model <i>of the same size</i>, but even with simple llama.cpp K-quantization, it&#x27;s still worth it all the way down to 2-bit. The chart in this llama.cpp PR speaks for itself:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684#issue-1739619305">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684#issue-17396...</a></div><br/><div id="39546506" class="c"><input type="checkbox" id="c-39546506" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39546183">parent</a><span>|</span><a href="#39544181">next</a><span>|</span><label class="collapse" for="c-39546506">[-]</label><label class="expand" for="c-39546506">[1 more]</label></div><br/><div class="children"><div class="content">Oh wow, you’re right. Though it seems that they are using very small weight group sizes: either 16 or 32 (fp16 scaling factor per group). In this paper it seems there’s no weights grouping, so it’s a bit apples to oranges.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39544181" class="c"><input type="checkbox" id="c-39544181" checked=""/><div class="controls bullet"><span class="by">Noe2097</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39538470">prev</a><span>|</span><a href="#39547082">next</a><span>|</span><label class="collapse" for="c-39544181">[-]</label><label class="expand" for="c-39544181">[5 more]</label></div><br/><div class="children"><div class="content">There is another _shocking_ realization in this work: there are 11 types of people: those who know what binary means, those who don&#x27;t, and those who say they do but actually don&#x27;t.<p>&quot;The era of 1-bit LLMs&quot;<p>Representing { -1, 0, 1 } can&#x27;t be done with 1-bit, I&#x27;m sorry -- and sad, please let&#x27;s all get back to something vaguely sound and rigorous.</div><br/><div id="39544223" class="c"><input type="checkbox" id="c-39544223" checked=""/><div class="controls bullet"><span class="by">npunt</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544181">parent</a><span>|</span><a href="#39545745">next</a><span>|</span><label class="collapse" for="c-39544223">[-]</label><label class="expand" for="c-39544223">[2 more]</label></div><br/><div class="children"><div class="content">Ternary supporters are always bitter about this<p>(I&#x27;ll let myself out)</div><br/></div></div><div id="39545745" class="c"><input type="checkbox" id="c-39545745" checked=""/><div class="controls bullet"><span class="by">esrauch</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544181">parent</a><span>|</span><a href="#39544223">prev</a><span>|</span><a href="#39547082">next</a><span>|</span><label class="collapse" for="c-39545745">[-]</label><label class="expand" for="c-39545745">[2 more]</label></div><br/><div class="children"><div class="content">One trit but that&#x27;s not a word anyone knows.</div><br/><div id="39546827" class="c"><input type="checkbox" id="c-39546827" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39545745">parent</a><span>|</span><a href="#39547082">next</a><span>|</span><label class="collapse" for="c-39546827">[-]</label><label class="expand" for="c-39546827">[1 more]</label></div><br/><div class="children"><div class="content">That used to be true yesterday…</div><br/></div></div></div></div></div></div><div id="39547082" class="c"><input type="checkbox" id="c-39547082" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39544181">prev</a><span>|</span><a href="#39540439">next</a><span>|</span><label class="collapse" for="c-39547082">[-]</label><label class="expand" for="c-39547082">[1 more]</label></div><br/><div class="children"><div class="content">&gt; * In matrix multiplications (e.g., weights by vectors), we can replace elementwise products in each dot product (a₁b₁ + a₂b₂ ...) with elementwise additions (a₁+b₁ + a₂+b₂ ...), in which signs depend on each value. See the paper for exact details.<p>Aren’t you over complicating it a bit here? A dot product between a vector of activations (a₁, a₂, …) and a vector of ternary weights (b₁, b₂, …) can of course be computed as the sum of all activations for which the weight is 1, minus the sum of all activations for which the weight is -1.<p>It can’t however be computed as (a₁+b₁ + a₂+b₂ ...). You must have gotten that wrong.</div><br/></div></div><div id="39540439" class="c"><input type="checkbox" id="c-39540439" checked=""/><div class="controls bullet"><span class="by">abeppu</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39547082">prev</a><span>|</span><a href="#39540074">next</a><span>|</span><label class="collapse" for="c-39540439">[-]</label><label class="expand" for="c-39540439">[2 more]</label></div><br/><div class="children"><div class="content">&gt; On existing hardware, the gains in compute and memory efficiency are significant, without performance degradation (as tested by the authors).<p>Did they actually show absence of performance degradation?<p>I think it&#x27;s conspicuous that Table 1 and Table 2 in the paper, which show perplexity and accuracy results respectively, are only for small model sizes, whereas Figure 2, Figure 3 (latency, memory, energy consumption) and Table 3 (throughput) all show larger model sizes. So it seems like they had every opportunity to show the perplexity&#x2F;accuracy comparisons at the larger model sizes, but did not include them.</div><br/><div id="39540490" class="c"><input type="checkbox" id="c-39540490" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39540439">parent</a><span>|</span><a href="#39540074">next</a><span>|</span><label class="collapse" for="c-39540490">[-]</label><label class="expand" for="c-39540490">[1 more]</label></div><br/><div class="children"><div class="content">Others have already made the same point in this thread. See my response here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39539508">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39539508</a></div><br/></div></div></div></div><div id="39540074" class="c"><input type="checkbox" id="c-39540074" checked=""/><div class="controls bullet"><span class="by">jandrese</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39540439">prev</a><span>|</span><a href="#39543010">next</a><span>|</span><label class="collapse" for="c-39540074">[-]</label><label class="expand" for="c-39540074">[3 more]</label></div><br/><div class="children"><div class="content">It seems like the AI space is slowly coming back around to the old Thinking Machines CM-1 architecture.  It&#x27;s not too often in computing where you see ideas a full 40 years ahead of their time make it into production.</div><br/><div id="39546046" class="c"><input type="checkbox" id="c-39546046" checked=""/><div class="controls bullet"><span class="by">theendisney</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39540074">parent</a><span>|</span><a href="#39544600">next</a><span>|</span><label class="collapse" for="c-39546046">[-]</label><label class="expand" for="c-39546046">[1 more]</label></div><br/><div class="children"><div class="content">Memristors any moment now</div><br/></div></div><div id="39544600" class="c"><input type="checkbox" id="c-39544600" checked=""/><div class="controls bullet"><span class="by">giantrobot</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39540074">parent</a><span>|</span><a href="#39546046">prev</a><span>|</span><a href="#39543010">next</a><span>|</span><label class="collapse" for="c-39544600">[-]</label><label class="expand" for="c-39544600">[1 more]</label></div><br/><div class="children"><div class="content">IIUC the main issue with the CM-1 architecture was feeding the processor cluster with data. That required a heftier front end system than was practical&#x2F;affordable at the time. With modern CPUs and memory subsystems the GPUs can be saturated pretty easily. So going back to huge clusters of super narrow cores won&#x27;t starve them for work.</div><br/></div></div></div></div><div id="39543010" class="c"><input type="checkbox" id="c-39543010" checked=""/><div class="controls bullet"><span class="by">flockonus</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39540074">prev</a><span>|</span><a href="#39543920">next</a><span>|</span><label class="collapse" for="c-39543010">[-]</label><label class="expand" for="c-39543010">[2 more]</label></div><br/><div class="children"><div class="content">Considering how much faster additions are processed, and how a particular silicon chip could be optimized for this very specific case; all parts added together perhaps could show &gt;100x speed up vs current systems.<p>I must concur, &quot;wow&quot;.</div><br/><div id="39545969" class="c"><input type="checkbox" id="c-39545969" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39543010">parent</a><span>|</span><a href="#39543920">next</a><span>|</span><label class="collapse" for="c-39545969">[-]</label><label class="expand" for="c-39545969">[1 more]</label></div><br/><div class="children"><div class="content">For hardware, 2-argument ternary additions and multiplications should be very close in terms of the tiny circuit required for either.<p>If you are doing ternary calculations on 32&#x2F;16-bit hardware, then the additions would be simpler.</div><br/></div></div></div></div><div id="39543920" class="c"><input type="checkbox" id="c-39543920" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39543010">prev</a><span>|</span><a href="#39544359">next</a><span>|</span><label class="collapse" for="c-39543920">[-]</label><label class="expand" for="c-39543920">[1 more]</label></div><br/><div class="children"><div class="content">Ternary networks have been used since 2015. There are hundreds of papers. They all require full QAT (training from scratch). Not sure why you’re shocked.</div><br/></div></div><div id="39544359" class="c"><input type="checkbox" id="c-39544359" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39543920">prev</a><span>|</span><a href="#39538936">next</a><span>|</span><label class="collapse" for="c-39544359">[-]</label><label class="expand" for="c-39544359">[3 more]</label></div><br/><div class="children"><div class="content">This will be big for FPGAs - adders are extremely cheap compared to multipliers and other DSP blocks.</div><br/><div id="39545773" class="c"><input type="checkbox" id="c-39545773" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39544359">parent</a><span>|</span><a href="#39544487">next</a><span>|</span><label class="collapse" for="c-39545773">[-]</label><label class="expand" for="c-39545773">[1 more]</label></div><br/><div class="children"><div class="content">Multipliers for eg 8 bit or 4 bit floating point values should also be pretty cheap?  (I assume multipliers have a cost that grows quadratically with the number of bits?)</div><br/></div></div></div></div><div id="39538936" class="c"><input type="checkbox" id="c-39538936" checked=""/><div class="controls bullet"><span class="by">rhaps0dy</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39544359">prev</a><span>|</span><a href="#39540069">next</a><span>|</span><label class="collapse" for="c-39538936">[-]</label><label class="expand" for="c-39538936">[2 more]</label></div><br/><div class="children"><div class="content">I think you need more evidence than this paper (which is very short and light on actual numbers) to be this shocked.<p>For example, most of the plots in the paper are actually of throughput, memory, etc. all performance characteristics that are better on the ternary version. Which, of course.<p>The only thing that contains perplexities are Table 1 and 2. There, they compare &quot;BitNet b1.58 to our reproduced FP16 LLaMA LLM in various sizes&quot; on the RedPajama data set. The first thing to note is the perplexities are very high: they&#x27;re all at least ~9.9, which compared for example with quantized Llama on wikitext-2 which is 6.15 (<a href="https:&#x2F;&#x2F;www.xzh.me&#x2F;2023&#x2F;09&#x2F;a-perplexity-benchmark-of-llamacpp.html" rel="nofollow">https:&#x2F;&#x2F;www.xzh.me&#x2F;2023&#x2F;09&#x2F;a-perplexity-benchmark-of-llamacp...</a>). Maybe RedPajama is a lot harder than wikitext-2, but that&#x27;s a big gap.<p>I think probably their benchmark (their &quot;reproduced FP16 LLaMA LLM&quot;) is just not very good. They didn&#x27;t invest much in training their baseline and so they handily beat it.</div><br/><div id="39539032" class="c"><input type="checkbox" id="c-39539032" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39538936">parent</a><span>|</span><a href="#39540069">next</a><span>|</span><label class="collapse" for="c-39539032">[-]</label><label class="expand" for="c-39539032">[1 more]</label></div><br/><div class="children"><div class="content">Thank you. I think the paper as it is provides enough evidence to support the claims. If I understand the authors correctly, they trained the compared models on only 100B tokens, all drawn from RedPajama, to make the comparisons apples-to-apples. That&#x27;s sensible. It allows for easier replication of the results. Otherwise, I agree with you that more extensive testing, after more extensive pretraining, is still necessary.</div><br/></div></div></div></div><div id="39540069" class="c"><input type="checkbox" id="c-39540069" checked=""/><div class="controls bullet"><span class="by">beagle3</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39538936">prev</a><span>|</span><a href="#39539472">next</a><span>|</span><label class="collapse" for="c-39540069">[-]</label><label class="expand" for="c-39540069">[1 more]</label></div><br/><div class="children"><div class="content">I haven’t been keeping tabs, but this seems very much like RIP &#x2F; Achilioptas version of the Johnson Lindenstrauss lemma.<p>Perhaps the rest of the JL lemma promise applies as well - compressing the number of parameters by a few orders of magnitude as well.</div><br/></div></div><div id="39539472" class="c"><input type="checkbox" id="c-39539472" checked=""/><div class="controls bullet"><span class="by">lr1970</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39540069">prev</a><span>|</span><a href="#39543846">next</a><span>|</span><label class="collapse" for="c-39539472">[-]</label><label class="expand" for="c-39539472">[4 more]</label></div><br/><div class="children"><div class="content">Authors reported perplexity only for small up to 3B weights models. On the other hand, they reported throughput for 70B model, but not its performance (perplexity, end-to-end tasks). Very unfortunate omission. Overall, the paper is rather poorly written.</div><br/><div id="39539508" class="c"><input type="checkbox" id="c-39539508" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39539472">parent</a><span>|</span><a href="#39543846">next</a><span>|</span><label class="collapse" for="c-39539508">[-]</label><label class="expand" for="c-39539508">[3 more]</label></div><br/><div class="children"><div class="content">If I understand the authors correctly, they trained the compared models on only 100B tokens, all drawn from RedPajama, to make the comparisons apples-to-apples. That&#x27;s sensible. It allows for easier replication of the results. Otherwise, I agree with you that more extensive testing, after more extensive pretraining, at larger model sizes, is still necessary.</div><br/><div id="39540797" class="c"><input type="checkbox" id="c-39540797" checked=""/><div class="controls bullet"><span class="by">lr1970</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39539508">parent</a><span>|</span><a href="#39543846">next</a><span>|</span><label class="collapse" for="c-39540797">[-]</label><label class="expand" for="c-39540797">[2 more]</label></div><br/><div class="children"><div class="content">towards the end of the paper they mentioned training on 2T tokens.</div><br/><div id="39543421" class="c"><input type="checkbox" id="c-39543421" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39540797">parent</a><span>|</span><a href="#39543846">next</a><span>|</span><label class="collapse" for="c-39543421">[-]</label><label class="expand" for="c-39543421">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right. Thank you for pointing that out.</div><br/></div></div></div></div></div></div></div></div><div id="39543846" class="c"><input type="checkbox" id="c-39543846" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39539472">prev</a><span>|</span><a href="#39545149">next</a><span>|</span><label class="collapse" for="c-39543846">[-]</label><label class="expand" for="c-39543846">[1 more]</label></div><br/><div class="children"><div class="content">I am not startled at all.  Dense vector representations are pretty silly, they can’t really be the road to knowledge representation.</div><br/></div></div><div id="39545149" class="c"><input type="checkbox" id="c-39545149" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39543846">prev</a><span>|</span><a href="#39543933">next</a><span>|</span><label class="collapse" for="c-39545149">[-]</label><label class="expand" for="c-39545149">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In existing LLMs, we can replace all parameter floating-point values representing real numbers with ternary values representing (-1, 0, 1).<p>does that mean we can do integer instead of floating point math for some parts of the training? that seems like a really big win</div><br/></div></div><div id="39543933" class="c"><input type="checkbox" id="c-39543933" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39545149">prev</a><span>|</span><a href="#39544228">next</a><span>|</span><label class="collapse" for="c-39543933">[-]</label><label class="expand" for="c-39543933">[1 more]</label></div><br/><div class="children"><div class="content">Conversely, this also implies our current model sizes can still embed a <i>ton</i> more “understanding”</div><br/></div></div><div id="39544228" class="c"><input type="checkbox" id="c-39544228" checked=""/><div class="controls bullet"><span class="by">AaronFriel</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39543933">prev</a><span>|</span><a href="#39543768">next</a><span>|</span><label class="collapse" for="c-39544228">[-]</label><label class="expand" for="c-39544228">[1 more]</label></div><br/><div class="children"><div class="content">In undergrad, some of us math majors would joke that there&#x27;s really only three quantities: 0, 1, infinity.<p>So, do we need the -1, and&#x2F;or would a 2.32 bit (5 state, or 6 with +&#x2F;-0) LLM perform better than a 1.58 bit LLM?</div><br/></div></div><div id="39543768" class="c"><input type="checkbox" id="c-39543768" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39544228">prev</a><span>|</span><a href="#39545656">next</a><span>|</span><label class="collapse" for="c-39543768">[-]</label><label class="expand" for="c-39543768">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also curious about the potential speed gains in automatic differentiation, as there are way less branches to &#x27;go up&#x27;. Or am I wrong here?</div><br/><div id="39543817" class="c"><input type="checkbox" id="c-39543817" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39543768">parent</a><span>|</span><a href="#39545656">next</a><span>|</span><label class="collapse" for="c-39543817">[-]</label><label class="expand" for="c-39543817">[1 more]</label></div><br/><div class="children"><div class="content">They actually use a relu to represent the model weights. But I&#x27;m not convinced that this can&#x27;t be avoided. We do gradient boosted decision tree training without this trick.</div><br/></div></div></div></div><div id="39545656" class="c"><input type="checkbox" id="c-39545656" checked=""/><div class="controls bullet"><span class="by">verytrivial</span><span>|</span><a href="#39537919">parent</a><span>|</span><a href="#39543768">prev</a><span>|</span><a href="#39536673">next</a><span>|</span><label class="collapse" for="c-39545656">[-]</label><label class="expand" for="c-39545656">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If the proposed methods are implemented in hardware<p>.. And the paper is _true_ of course, indeed, this sort of compounding quantum leap in efficiency due to representational change starts to get towards the Black Mirror &#x2F; SciFi foundational mythology level of acceleration. Wild (if true!)</div><br/><div id="39545775" class="c"><input type="checkbox" id="c-39545775" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537919">root</a><span>|</span><a href="#39545656">parent</a><span>|</span><a href="#39536673">next</a><span>|</span><label class="collapse" for="c-39545775">[-]</label><label class="expand" for="c-39545775">[1 more]</label></div><br/><div class="children"><div class="content">Slight tangent: in physics a quantum leap is the smallest possible change.</div><br/></div></div></div></div></div></div><div id="39536673" class="c"><input type="checkbox" id="c-39536673" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#39537919">prev</a><span>|</span><a href="#39537210">next</a><span>|</span><label class="collapse" for="c-39536673">[-]</label><label class="expand" for="c-39536673">[16 more]</label></div><br/><div class="children"><div class="content">&gt; BitNet b1.58 can match the performance of the full precision baseline starting from a 3B size. ... This demonstrates that BitNet b1.58 is a Pareto improvement over the state-of-the-art LLM models.<p>&gt; BitNet b1.58 is enabling a new scaling law with respect to model performance and inference cost. As a reference, we can have the following equivalence between different model sizes in 1.58-bit and 16-bit based on the results in Figure 2 and 3.<p>&gt; • 13B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consumption, than 3B FP16 LLM.<p>&gt; • 30B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consumption, than 7B FP16 LLM.<p>&gt; • 70B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consumption, than 13B FP16 LLM.<p>This paper seems to represent a monumental breakthrough in LLM efficiency, as the efficiency gains come with zero (or negative) performance penalty.<p>Does it seem at all likely that existing models could be converted?</div><br/><div id="39537394" class="c"><input type="checkbox" id="c-39537394" checked=""/><div class="controls bullet"><span class="by">btbuildem</span><span>|</span><a href="#39536673">parent</a><span>|</span><a href="#39536724">next</a><span>|</span><label class="collapse" for="c-39537394">[-]</label><label class="expand" for="c-39537394">[5 more]</label></div><br/><div class="children"><div class="content">Discussion on HF [1] implies that no, conversion is not helpful. It would take training the model from scratch.<p>1: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2402.17764" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2402.17764</a></div><br/><div id="39539049" class="c"><input type="checkbox" id="c-39539049" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39537394">parent</a><span>|</span><a href="#39536724">next</a><span>|</span><label class="collapse" for="c-39539049">[-]</label><label class="expand" for="c-39539049">[4 more]</label></div><br/><div class="children"><div class="content">It’s a pity if realizing these gains absolutely requires full pre-training from scratch. I imagine more than a few people will at least try to find a way to repurpose the knowledge contained in existing models.</div><br/><div id="39543671" class="c"><input type="checkbox" id="c-39543671" checked=""/><div class="controls bullet"><span class="by">cooljoseph</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39539049">parent</a><span>|</span><a href="#39536724">next</a><span>|</span><label class="collapse" for="c-39543671">[-]</label><label class="expand" for="c-39543671">[3 more]</label></div><br/><div class="children"><div class="content">You can also have another model &quot;mentor&quot; a new model you are teaching to speed up training. You don&#x27;t have to start from scratch with zero knowledge. This is done a lot in what are called distillations.</div><br/><div id="39545782" class="c"><input type="checkbox" id="c-39545782" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39543671">parent</a><span>|</span><a href="#39545477">next</a><span>|</span><label class="collapse" for="c-39545782">[-]</label><label class="expand" for="c-39545782">[1 more]</label></div><br/><div class="children"><div class="content">You can also re-use a lot of the infrastructure.  Eg you can re-use your training data.</div><br/></div></div><div id="39545477" class="c"><input type="checkbox" id="c-39545477" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39543671">parent</a><span>|</span><a href="#39545782">prev</a><span>|</span><a href="#39536724">next</a><span>|</span><label class="collapse" for="c-39545477">[-]</label><label class="expand" for="c-39545477">[1 more]</label></div><br/><div class="children"><div class="content">This came out a little bit ago, my open question is if this approach can be used to port weights between architectures like this.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.13144" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.13144</a></div><br/></div></div></div></div></div></div></div></div><div id="39536724" class="c"><input type="checkbox" id="c-39536724" checked=""/><div class="controls bullet"><span class="by">accurrent</span><span>|</span><a href="#39536673">parent</a><span>|</span><a href="#39537394">prev</a><span>|</span><a href="#39537369">next</a><span>|</span><label class="collapse" for="c-39536724">[-]</label><label class="expand" for="c-39536724">[7 more]</label></div><br/><div class="children"><div class="content">They seem to be using LLAMA. Might be worth trying out. Their conversion formula seems stupidly simple.</div><br/><div id="39536832" class="c"><input type="checkbox" id="c-39536832" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39536724">parent</a><span>|</span><a href="#39537333">next</a><span>|</span><label class="collapse" for="c-39536832">[-]</label><label class="expand" for="c-39536832">[5 more]</label></div><br/><div class="children"><div class="content">However they trained their models from scratch, which is also why they only have meaningful numbers for 700M, 1.3B, 3B and 3.9B models. Apparently they are following BitNet&#x27;s approach of replacing linear layers with quantized layers during training? If it was trivial to convert existing models without performance loss I would have expected them to include a benchmark of that somewhere in the paper to generate even more impact.</div><br/><div id="39536855" class="c"><input type="checkbox" id="c-39536855" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39536832">parent</a><span>|</span><a href="#39537333">next</a><span>|</span><label class="collapse" for="c-39536855">[-]</label><label class="expand" for="c-39536855">[4 more]</label></div><br/><div class="children"><div class="content">They present numbers for 7B to 70B models as well.</div><br/><div id="39536941" class="c"><input type="checkbox" id="c-39536941" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39536855">parent</a><span>|</span><a href="#39536925">next</a><span>|</span><label class="collapse" for="c-39536941">[-]</label><label class="expand" for="c-39536941">[1 more]</label></div><br/><div class="children"><div class="content">Those numbers are for cost only, not performance. It’s not clear they actually <i>trained</i> a 70B vs. just using randomly initialized parameters.</div><br/></div></div><div id="39536925" class="c"><input type="checkbox" id="c-39536925" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39536855">parent</a><span>|</span><a href="#39536941">prev</a><span>|</span><a href="#39537333">next</a><span>|</span><label class="collapse" for="c-39536925">[-]</label><label class="expand" for="c-39536925">[2 more]</label></div><br/><div class="children"><div class="content">They do not have perplexity numbers for the larger models (see Table 2), only speed and memory benchmarks.</div><br/><div id="39537386" class="c"><input type="checkbox" id="c-39537386" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39536925">parent</a><span>|</span><a href="#39537333">next</a><span>|</span><label class="collapse" for="c-39537386">[-]</label><label class="expand" for="c-39537386">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re both right, I skimmed the paper, saw large model numbers but didn&#x27;t notice it was for speed. On the HF page they say those models are being trained.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2402.17764" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2402.17764</a><p>&quot;We haven&#x27;t finished the training of the models beyond 3B as it requires much much more resources. However, we&#x27;re optimistic about the results because we have verified that BitNet follows a similar performance-parameter scaling law as the full-precision LLMs. We&#x27;ll update the results on larger models once they&#x27;re ready.&quot;</div><br/></div></div></div></div></div></div></div></div><div id="39537333" class="c"><input type="checkbox" id="c-39537333" checked=""/><div class="controls bullet"><span class="by">FrustratedMonky</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39536724">parent</a><span>|</span><a href="#39536832">prev</a><span>|</span><a href="#39537369">next</a><span>|</span><label class="collapse" for="c-39537333">[-]</label><label class="expand" for="c-39537333">[1 more]</label></div><br/><div class="children"><div class="content">Yes. I wonder then how long before someone that does have a lot of compute power like OpenAI&#x2F;MS, or others, can rapidly pivot and try this out on some even larger models.<p>Doesn&#x27;t this mean that current big players can rapidly expand by huge multiples in size.?</div><br/></div></div></div></div><div id="39537369" class="c"><input type="checkbox" id="c-39537369" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#39536673">parent</a><span>|</span><a href="#39536724">prev</a><span>|</span><a href="#39537210">next</a><span>|</span><label class="collapse" for="c-39537369">[-]</label><label class="expand" for="c-39537369">[3 more]</label></div><br/><div class="children"><div class="content">I wonder if 1bit quantization is the <i>main</i> reason why pplx.ai is faster than any other RAG or chatbot. For instance, Gemini in comparison is a turtle, though it is better at explanations, while pplx is concise.</div><br/><div id="39546682" class="c"><input type="checkbox" id="c-39546682" checked=""/><div class="controls bullet"><span class="by">vitorgrs</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39537369">parent</a><span>|</span><a href="#39545470">next</a><span>|</span><label class="collapse" for="c-39546682">[-]</label><label class="expand" for="c-39546682">[1 more]</label></div><br/><div class="children"><div class="content">Nop. The model on Perplexity is a finetuned GPT 3.5 (the free one).  
And the paid versons, well, you can choose between GPT4 (not turbo), Gemini pro, Claude, etc.<p>You can choose their model (&quot;Experimental&quot;), but is not faster than the other models.<p>All of these, proprietary models are fast on Perplexity. I do guess they are using some insane cache system, better API  infrastructure...</div><br/></div></div><div id="39545470" class="c"><input type="checkbox" id="c-39545470" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39536673">root</a><span>|</span><a href="#39537369">parent</a><span>|</span><a href="#39546682">prev</a><span>|</span><a href="#39537210">next</a><span>|</span><label class="collapse" for="c-39545470">[-]</label><label class="expand" for="c-39545470">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely not, 1 bit isn&#x27;t even  real yet. perplexity does a ton of precaching, TL;Dr every novel query is an opportunity to cache: each web page response, the response turned into embeddings, and the LLM response. That&#x27;s also why I hate it, it&#x27;s just a rushed version of RAG with roughly the same privacy guarantees any incumbent would have given you in last 15 years (read: none, and gleefully will exploit yours while saying &quot;whoops!&quot;)</div><br/></div></div></div></div></div></div><div id="39537210" class="c"><input type="checkbox" id="c-39537210" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#39536673">prev</a><span>|</span><a href="#39538484">next</a><span>|</span><label class="collapse" for="c-39537210">[-]</label><label class="expand" for="c-39537210">[36 more]</label></div><br/><div class="children"><div class="content">I have often mused that, in some ways, it seems like the transistor is really being wasted in AI applications. We use binary states in normal computing to reduce entropy. In AI this is less of a concern, so why not use more of the available voltage range? Basically, re-think the role of the transistor and re-design from the ground up - maybe NAND gates are not the ideal fundamental building block here?</div><br/><div id="39537308" class="c"><input type="checkbox" id="c-39537308" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39545817">next</a><span>|</span><label class="collapse" for="c-39537308">[-]</label><label class="expand" for="c-39537308">[10 more]</label></div><br/><div class="children"><div class="content">People are working on that [1]. In some sense, it&#x27;s a step back to analog computing. Add&#x2F;multiply is possible to do directly in memory with voltages, but it&#x27;s less versatile (and stable) than digital computing. So you can&#x27;t do <i>all</i> calculations in a neural network that way, meaning some digital components will always be necessary. But I&#x27;m pretty sure analog will make a comeback for AI chips sooner or later.<p>[1] <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06337-5" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06337-5</a></div><br/><div id="39538199" class="c"><input type="checkbox" id="c-39538199" checked=""/><div class="controls bullet"><span class="by">zcw100</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39537308">parent</a><span>|</span><a href="#39541383">next</a><span>|</span><label class="collapse" for="c-39538199">[-]</label><label class="expand" for="c-39538199">[3 more]</label></div><br/><div class="children"><div class="content">Reminds me of my father saying something about how vacuum tubes are great integrators.</div><br/><div id="39539490" class="c"><input type="checkbox" id="c-39539490" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39538199">parent</a><span>|</span><a href="#39541383">next</a><span>|</span><label class="collapse" for="c-39539490">[-]</label><label class="expand" for="c-39539490">[2 more]</label></div><br/><div class="children"><div class="content">Chips are too.  Opamps can add, multiply, subtract, divide, integrate and differentiate depending on how they&#x27;re plugged in.</div><br/><div id="39542062" class="c"><input type="checkbox" id="c-39542062" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39539490">parent</a><span>|</span><a href="#39541383">next</a><span>|</span><label class="collapse" for="c-39542062">[-]</label><label class="expand" for="c-39542062">[1 more]</label></div><br/><div class="children"><div class="content">Hence the name &#x27;operational&#x27; amplifier</div><br/></div></div></div></div></div></div><div id="39541383" class="c"><input type="checkbox" id="c-39541383" checked=""/><div class="controls bullet"><span class="by">trebligdivad</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39537308">parent</a><span>|</span><a href="#39538199">prev</a><span>|</span><a href="#39540290">next</a><span>|</span><label class="collapse" for="c-39541383">[-]</label><label class="expand" for="c-39541383">[5 more]</label></div><br/><div class="children"><div class="content">Trinary however is an interesting middle; people have built trinary hardware long ago; it feels like you could make natively trinary hardware for something like this; it might even be quite a win.</div><br/><div id="39542784" class="c"><input type="checkbox" id="c-39542784" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39541383">parent</a><span>|</span><a href="#39541814">next</a><span>|</span><label class="collapse" for="c-39542784">[-]</label><label class="expand" for="c-39542784">[3 more]</label></div><br/><div class="children"><div class="content">People haven&#x27;t built <i>reliable</i> ternary electronics, though. Soviets tried with Setun, but they eventually had to resort to emulating each trit with two hardware bits (and wasting one state out of the possible four).</div><br/><div id="39545833" class="c"><input type="checkbox" id="c-39545833" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39542784">parent</a><span>|</span><a href="#39541814">next</a><span>|</span><label class="collapse" for="c-39545833">[-]</label><label class="expand" for="c-39545833">[2 more]</label></div><br/><div class="children"><div class="content">If you are are using two bits anyway, you might as well represent (-2, -1, 0, 1) instead of ternary?</div><br/><div id="39546144" class="c"><input type="checkbox" id="c-39546144" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39545833">parent</a><span>|</span><a href="#39541814">next</a><span>|</span><label class="collapse" for="c-39546144">[-]</label><label class="expand" for="c-39546144">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but then you lose the symmetry that makes trits so convenient for many things.</div><br/></div></div></div></div></div></div><div id="39541814" class="c"><input type="checkbox" id="c-39541814" checked=""/><div class="controls bullet"><span class="by">thsksbd</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39541383">parent</a><span>|</span><a href="#39542784">prev</a><span>|</span><a href="#39540290">next</a><span>|</span><label class="collapse" for="c-39541814">[-]</label><label class="expand" for="c-39541814">[1 more]</label></div><br/><div class="children"><div class="content">Can you make a &quot;CMOS&quot; three voltage level circuit though? One where the only current flow is when the state changes?<p>Im not in this field but that&#x27;s a question that&#x27;s been bugging me for a while. Off you can&#x27;t do this wouldn&#x27;t energy consumption balloon?</div><br/></div></div></div></div><div id="39540290" class="c"><input type="checkbox" id="c-39540290" checked=""/><div class="controls bullet"><span class="by">irrelative</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39537308">parent</a><span>|</span><a href="#39541383">prev</a><span>|</span><a href="#39545817">next</a><span>|</span><label class="collapse" for="c-39540290">[-]</label><label class="expand" for="c-39540290">[1 more]</label></div><br/><div class="children"><div class="content">Hadn&#x27;t thought about it this way before, but given that LLMs are auto regressive (use their own data for next data), they&#x27;re sensitive to error drift in ways that are rather similar to analog computers.</div><br/></div></div></div></div><div id="39545817" class="c"><input type="checkbox" id="c-39545817" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39537308">prev</a><span>|</span><a href="#39537481">next</a><span>|</span><label class="collapse" for="c-39545817">[-]</label><label class="expand" for="c-39545817">[2 more]</label></div><br/><div class="children"><div class="content">Analog computing for neural networks is always very tempting.<p>&gt; We use binary states in normal computing to reduce entropy. In AI this is less of a concern, so why not use more of the available voltage range?<p>Transistors that are fully closed or fully open use basically no energy: they either have approximately zero current or approximately zero resistance.<p>Transistors that are partially open dissipate a lot of energy; because they have some current flowing at some resistance.  They get hot.<p>In addition, modern transistors are so small and so fast that the number of electrons (or holes..) flowing through them in clock cycle is perhaps in the range of a few dozen to a hundred.  So that gives you at most 7 bits (~log_2(128)) of precision to work with in an analog setting.  In practice, quite a bit less because there&#x27;s a lot of thermal noise.  Say perhaps 4 bits.<p>Going from 1 bit per transistor to 4 bits (of analog precision) is not worth the drastically higher energy consumption nor the deviation from the mainstream of semi-conductor technological advances.</div><br/><div id="39547314" class="c"><input type="checkbox" id="c-39547314" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39545817">parent</a><span>|</span><a href="#39537481">next</a><span>|</span><label class="collapse" for="c-39547314">[-]</label><label class="expand" for="c-39547314">[1 more]</label></div><br/><div class="children"><div class="content">As someone who knows almost nothing about electronics I assume you’d want a transistor which can open in two ways: with positive and negative voltage. I’ve seen TNAND built out of normal transistors, not sure if such exotic ones would help even if they were physically possible.</div><br/></div></div></div></div><div id="39537481" class="c"><input type="checkbox" id="c-39537481" checked=""/><div class="controls bullet"><span class="by">gryn</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39545817">prev</a><span>|</span><a href="#39537356">next</a><span>|</span><label class="collapse" for="c-39537481">[-]</label><label class="expand" for="c-39537481">[4 more]</label></div><br/><div class="children"><div class="content">the reason why digital&#x2F;numeric processing won is the power loss in the analog world.
when design an analog circuit the next processing stage you add at the end has impact on the ones before it.<p>this then require a higher skill from the engineers&#x2F;consumers.<p>if you want to avoid that you need to add op-amps with a gain of 1 at the boundary of each one, this also that care of the power loss at each stage.<p>the other part is that there&#x27;s a limit of to the amount of useful information&#x2F;computation you can do with analog processing too once you take into account  voltage noise. when you do a comparison there are stages where analog win but also place where where digital wins.<p>I&#x27;ll edit later this with a link to some papers that discuss these topics if I manage to find them in my mess.</div><br/><div id="39541487" class="c"><input type="checkbox" id="c-39541487" checked=""/><div class="controls bullet"><span class="by">dazed_confused</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39537481">parent</a><span>|</span><a href="#39545827">next</a><span>|</span><label class="collapse" for="c-39541487">[-]</label><label class="expand" for="c-39541487">[1 more]</label></div><br/><div class="children"><div class="content">Good explanation. When I was working at a semiconductor manufacturer, our thresholds were like 0 - 0.2V to 0.8 - 1.0V. Additionally, if you look at QLC SSDs, their longevity is hugely degraded. Analog computing is non-trivial, to say the least.</div><br/></div></div><div id="39545827" class="c"><input type="checkbox" id="c-39545827" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39537481">parent</a><span>|</span><a href="#39541487">prev</a><span>|</span><a href="#39541627">next</a><span>|</span><label class="collapse" for="c-39545827">[-]</label><label class="expand" for="c-39545827">[1 more]</label></div><br/><div class="children"><div class="content">You also have literal power losses, as in waste heat, to deal with.<p>See <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39545817">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39545817</a></div><br/></div></div><div id="39541627" class="c"><input type="checkbox" id="c-39541627" checked=""/><div class="controls bullet"><span class="by">im3w1l</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39537481">parent</a><span>|</span><a href="#39545827">prev</a><span>|</span><a href="#39537356">next</a><span>|</span><label class="collapse" for="c-39541627">[-]</label><label class="expand" for="c-39541627">[1 more]</label></div><br/><div class="children"><div class="content">For the specific case of neural networks they seem to be very resistant to noise. That&#x27;s why quantization works in the first place.</div><br/></div></div></div></div><div id="39537356" class="c"><input type="checkbox" id="c-39537356" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39537481">prev</a><span>|</span><a href="#39543378">next</a><span>|</span><label class="collapse" for="c-39537356">[-]</label><label class="expand" for="c-39537356">[2 more]</label></div><br/><div class="children"><div class="content">The Veritasium Youtube channel did a video about this about a year ago: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=GVsUOuSjvcg" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=GVsUOuSjvcg</a><p>They visit Texas company Mythic AI to discuss how they use flash memory for machine learning.  There&#x27;s a California company named Syntiant doing something similar.</div><br/><div id="39545807" class="c"><input type="checkbox" id="c-39545807" checked=""/><div class="controls bullet"><span class="by">dwightboyyy</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39537356">parent</a><span>|</span><a href="#39543378">next</a><span>|</span><label class="collapse" for="c-39545807">[-]</label><label class="expand" for="c-39545807">[1 more]</label></div><br/><div class="children"><div class="content">I was thinking of this exact video, crazy to think that the principle is gaining momentum</div><br/></div></div></div></div><div id="39543378" class="c"><input type="checkbox" id="c-39543378" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39537356">prev</a><span>|</span><a href="#39538445">next</a><span>|</span><label class="collapse" for="c-39543378">[-]</label><label class="expand" for="c-39543378">[3 more]</label></div><br/><div class="children"><div class="content">&gt;maybe NAND gates are not the ideal fundamental building block here?<p>It&#x27;s my long held opinion that LUTs (Look Up Tables) are the basis of computation for the future. I&#x27;ve been pondering this for a long time since George Gilder told us that wasting transistors was the winning strategy. What could be more wasteful than just making a huge grid of LUTs that all interconnect, with NO routing hardware?<p>As time goes by, the idea seems to have more and more merit. Imagine a grid of 4x4 bit look up tables, each connected to its neighbors, and clocked in 2 phases, to prevent race conditions. You eliminate the high speed long lines across chips that cause so much grief (except the clock signals, and bits to load the tables, which don&#x27;t happen often).<p>What you lose in performance (in terms of latency), you make up for with the homogenous architecture that is easy to think about, can route around bad cells, and be compiled to almost instantly, thanks to the lack of special cases. You also don&#x27;t  ever have to worry about latency, it&#x27;s constant.</div><br/><div id="39543659" class="c"><input type="checkbox" id="c-39543659" checked=""/><div class="controls bullet"><span class="by">phdelightful</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39543378">parent</a><span>|</span><a href="#39538445">next</a><span>|</span><label class="collapse" for="c-39543659">[-]</label><label class="expand" for="c-39543659">[2 more]</label></div><br/><div class="children"><div class="content">It’s been a long time since I worked on FPGAs, but it sounds like FPGAs! What do you see as the main differences?</div><br/><div id="39543839" class="c"><input type="checkbox" id="c-39543839" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39543659">parent</a><span>|</span><a href="#39538445">next</a><span>|</span><label class="collapse" for="c-39543839">[-]</label><label class="expand" for="c-39543839">[1 more]</label></div><br/><div class="children"><div class="content">No routing, no fast lines that cut across the chip, which cut way down on latency, but make FPGAs harder to build, and especially hard to compile to once you want to use them.<p>All that routing hardware, and the special function units featured in many FPGAs are something you have to optimize the usage of, and route to. You end up with using solvers, simulated annealing, etc... instead of a straight compile to binary expressions, and mapping to the grid.<p>Latency minimization is the key to getting a design to run fast in an FPGA. In a BitGrid, you know the clock speed, you know the latency by just counting the steps in the graph. BitGrid performance is determined by how many answers&#x2F;second you can get from a given chip. If you had a 1 Ghz rack of BitGrid chips that could run GPT-4, with a latency of 1 mSec per token, you&#x27;d think that was horrible, but you could run a million such streams in parallel.</div><br/></div></div></div></div></div></div><div id="39538445" class="c"><input type="checkbox" id="c-39538445" checked=""/><div class="controls bullet"><span class="by">StableAlkyne</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39543378">prev</a><span>|</span><a href="#39538729">next</a><span>|</span><label class="collapse" for="c-39538445">[-]</label><label class="expand" for="c-39538445">[3 more]</label></div><br/><div class="children"><div class="content">It would be something of a full circle I feel went back to dedicated circuits for NNs - that&#x27;s how they began life when Rosenblatt built his Perceptron.<p>I remember reading a review on the history in grad school (can&#x27;t remember the paper) where the author stated that one of the initial interests in NNs by the military was their distributed nature. Even back then, people realized you could remove a neuron or break a connection and they would still work (and even today, dropout is a way of regularizing the network). The thinking was that being able to build a computer or automated device that could be damaged (radiation flipping bits, an impact destroying part of the circuit, etc) and still work would be an advantage given the perceived inevitably of nuclear war.<p>Compared to a normal von Neumann machine which is very fault intolerant - remove the CPU and no processing, no memory=no useful calculation, etc. One reason people may have avoided further attempts at physical neural networks is it&#x27;s intrinsically more complex than von Neumann, since now your processing and memory is intertwined (the NN is the processor and the program and the memory at the same time).</div><br/><div id="39538910" class="c"><input type="checkbox" id="c-39538910" checked=""/><div class="controls bullet"><span class="by">kurisufag</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39538445">parent</a><span>|</span><a href="#39538729">next</a><span>|</span><label class="collapse" for="c-39538910">[-]</label><label class="expand" for="c-39538910">[2 more]</label></div><br/><div class="children"><div class="content">&gt;von Braun machine<p>von neumann? though it is funny to imagine von braun inventing computer architecture as a side hustle to inventing rocket science.</div><br/><div id="39539041" class="c"><input type="checkbox" id="c-39539041" checked=""/><div class="controls bullet"><span class="by">StableAlkyne</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39538910">parent</a><span>|</span><a href="#39538729">next</a><span>|</span><label class="collapse" for="c-39539041">[-]</label><label class="expand" for="c-39539041">[1 more]</label></div><br/><div class="children"><div class="content">Oh fuck, thanks for catching that!</div><br/></div></div></div></div></div></div><div id="39538729" class="c"><input type="checkbox" id="c-39538729" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39538445">prev</a><span>|</span><a href="#39544849">next</a><span>|</span><label class="collapse" for="c-39538729">[-]</label><label class="expand" for="c-39538729">[2 more]</label></div><br/><div class="children"><div class="content">Bits are copyable without data loss. Analog properties of individual transistors are less so.</div><br/><div id="39545846" class="c"><input type="checkbox" id="c-39545846" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39538729">parent</a><span>|</span><a href="#39544849">next</a><span>|</span><label class="collapse" for="c-39545846">[-]</label><label class="expand" for="c-39545846">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but the whole point of the link submitted to HN here is that in some applications, like machine learning, precision doesn&#x27;t matter too much.<p>(However, analog computing is still a bad fit for machine learning, because it requires a lot more power.)</div><br/></div></div></div></div><div id="39544849" class="c"><input type="checkbox" id="c-39544849" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39538729">prev</a><span>|</span><a href="#39537346">next</a><span>|</span><label class="collapse" for="c-39544849">[-]</label><label class="expand" for="c-39544849">[1 more]</label></div><br/><div class="children"><div class="content">It sure looks like this might pair well with ternary optical computing advances:<p><a href="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;9720446" rel="nofollow">https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;9720446</a></div><br/></div></div><div id="39537346" class="c"><input type="checkbox" id="c-39537346" checked=""/><div class="controls bullet"><span class="by">wakawaka28</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39544849">prev</a><span>|</span><a href="#39537843">next</a><span>|</span><label class="collapse" for="c-39537346">[-]</label><label class="expand" for="c-39537346">[2 more]</label></div><br/><div class="children"><div class="content">I have heard of people trying to build analog AI devices but that seems like years ago, and no news has come out about it in recent times. Maybe it is harder than it seems. I bet it is expensive to regulate voltage so precisely and it&#x27;s not a flexible enough scheme to be support training neural networks like we have now, which are highly reconfigurable. I&#x27;ve also heard of people trying to use analog computing for more mundane things. But no devices have hit the market after so many years so I&#x27;m assuming it is a super hard problem, maybe even intractible.</div><br/><div id="39537551" class="c"><input type="checkbox" id="c-39537551" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39537346">parent</a><span>|</span><a href="#39537843">next</a><span>|</span><label class="collapse" for="c-39537551">[-]</label><label class="expand" for="c-39537551">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps another variation on the idea is to allow a higher error rate. For example, if a 0.01% error rate was acceptable in AI, perhaps the voltage range between states could be lowered (which has a quadratic relationship to power consumption) and clock speed could increase.</div><br/></div></div></div></div><div id="39537843" class="c"><input type="checkbox" id="c-39537843" checked=""/><div class="controls bullet"><span class="by">barrenko</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39537346">prev</a><span>|</span><a href="#39542077">next</a><span>|</span><label class="collapse" for="c-39537843">[-]</label><label class="expand" for="c-39537843">[1 more]</label></div><br/><div class="children"><div class="content">Hmm, maybe some (signaling) inspiration from biology other than neural signaling.</div><br/></div></div><div id="39542077" class="c"><input type="checkbox" id="c-39542077" checked=""/><div class="controls bullet"><span class="by">drexlspivey</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39537843">prev</a><span>|</span><a href="#39537693">next</a><span>|</span><label class="collapse" for="c-39542077">[-]</label><label class="expand" for="c-39542077">[1 more]</label></div><br/><div class="children"><div class="content">Next Up: Quantum AI</div><br/></div></div><div id="39537693" class="c"><input type="checkbox" id="c-39537693" checked=""/><div class="controls bullet"><span class="by">BlueTemplar</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39542077">prev</a><span>|</span><a href="#39542027">next</a><span>|</span><label class="collapse" for="c-39537693">[-]</label><label class="expand" for="c-39537693">[1 more]</label></div><br/><div class="children"><div class="content">I have heard that the first commercial neural network chip (by Intel, in the 90s) was analog ?</div><br/></div></div><div id="39542027" class="c"><input type="checkbox" id="c-39542027" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39537693">prev</a><span>|</span><a href="#39537284">next</a><span>|</span><label class="collapse" for="c-39542027">[-]</label><label class="expand" for="c-39542027">[2 more]</label></div><br/><div class="children"><div class="content">let&#x27;s use  cells</div><br/><div id="39542643" class="c"><input type="checkbox" id="c-39542643" checked=""/><div class="controls bullet"><span class="by">Razengan</span><span>|</span><a href="#39537210">root</a><span>|</span><a href="#39542027">parent</a><span>|</span><a href="#39537284">next</a><span>|</span><label class="collapse" for="c-39542643">[-]</label><label class="expand" for="c-39542643">[1 more]</label></div><br/><div class="children"><div class="content">We already do.</div><br/></div></div></div></div><div id="39537284" class="c"><input type="checkbox" id="c-39537284" checked=""/><div class="controls bullet"><span class="by">adrianN</span><span>|</span><a href="#39537210">parent</a><span>|</span><a href="#39542027">prev</a><span>|</span><a href="#39538484">next</a><span>|</span><label class="collapse" for="c-39537284">[-]</label><label class="expand" for="c-39537284">[1 more]</label></div><br/><div class="children"><div class="content">You could call them connection machine and perhaps have an llm trained on Feynman help with the design.</div><br/></div></div></div></div><div id="39538484" class="c"><input type="checkbox" id="c-39538484" checked=""/><div class="controls bullet"><span class="by">w-m</span><span>|</span><a href="#39537210">prev</a><span>|</span><a href="#39536971">next</a><span>|</span><label class="collapse" for="c-39538484">[-]</label><label class="expand" for="c-39538484">[13 more]</label></div><br/><div class="children"><div class="content">I was reading <i>Exposing Floating Point</i> today (as Airfoil is on the HN front page and I was perusing the archive of the author). It&#x27;s a blog explaining the inner workings of floating point representations. About zero values it says [0]:<p>&gt; Yes, the floating point standard specifies both +0.0 and −0.0. This concept is actually useful because it tells us from which “direction” the 0 was approached as a result of storing value too small to be represented in a float. For instance -10e-30f &#x2F; 10e30f won’t fit in a float, however, it will produce the value of -0.0.<p>The authors of the LLM paper use the values {-1, 0, -1}. Connecting the two ideas, I&#x27;m now wondering whether having a 2-bit {-1, -0, 0, 1} representation might have any benefit over the proposed 1.58 bits. Could the additional -0 carry some pseudo-gradient information, (&quot;the 0 leaning towards the negative side&quot;)?<p>Also, I&#x27;ve seen 2-bit quantizations being proposed in other LLM quantization papers. What values are they using?<p>[0] <a href="https:&#x2F;&#x2F;ciechanow.ski&#x2F;exposing-floating-point&#x2F;#zero" rel="nofollow">https:&#x2F;&#x2F;ciechanow.ski&#x2F;exposing-floating-point&#x2F;#zero</a></div><br/><div id="39538658" class="c"><input type="checkbox" id="c-39538658" checked=""/><div class="controls bullet"><span class="by">creshal</span><span>|</span><a href="#39538484">parent</a><span>|</span><a href="#39538642">next</a><span>|</span><label class="collapse" for="c-39538658">[-]</label><label class="expand" for="c-39538658">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Could the additional -0 carry some pseudo-gradient information, (&quot;the 0 leaning towards the negative side&quot;)?<p>Probably, but is it worth the cost? One of the goals behind BitNet and this paper is to find a way to implement LLMs as efficiently in hardware as possible, and foregoing floating point semantics is a big part of it. I&#x27;m not sure if there&#x27;s a way to encode -0 that doesn&#x27;t throw out half the performance gains.</div><br/><div id="39542060" class="c"><input type="checkbox" id="c-39542060" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39538484">root</a><span>|</span><a href="#39538658">parent</a><span>|</span><a href="#39538642">next</a><span>|</span><label class="collapse" for="c-39542060">[-]</label><label class="expand" for="c-39542060">[4 more]</label></div><br/><div class="children"><div class="content">But if I understand it correctly, they already need to use 2 bits, one for the sign and another one for the value, so there is already one wasted state, which could be used for -0.</div><br/><div id="39544110" class="c"><input type="checkbox" id="c-39544110" checked=""/><div class="controls bullet"><span class="by">pennomi</span><span>|</span><a href="#39538484">root</a><span>|</span><a href="#39542060">parent</a><span>|</span><a href="#39538642">next</a><span>|</span><label class="collapse" for="c-39544110">[-]</label><label class="expand" for="c-39544110">[3 more]</label></div><br/><div class="children"><div class="content">You can pack two trits into three bits, however. So one byte could hold 5 values instead of 4.</div><br/><div id="39544556" class="c"><input type="checkbox" id="c-39544556" checked=""/><div class="controls bullet"><span class="by">threatripper</span><span>|</span><a href="#39538484">root</a><span>|</span><a href="#39544110">parent</a><span>|</span><a href="#39544340">next</a><span>|</span><label class="collapse" for="c-39544556">[-]</label><label class="expand" for="c-39544556">[1 more]</label></div><br/><div class="children"><div class="content">How exactly would you do that? 3 states need 1.58 bits which is a tad more than 1.5. Two 3-states have 3²=9 states while three bits only give you 2³=8 states.</div><br/></div></div><div id="39544340" class="c"><input type="checkbox" id="c-39544340" checked=""/><div class="controls bullet"><span class="by">para_parolu</span><span>|</span><a href="#39538484">root</a><span>|</span><a href="#39544110">parent</a><span>|</span><a href="#39544556">prev</a><span>|</span><a href="#39538642">next</a><span>|</span><label class="collapse" for="c-39544340">[-]</label><label class="expand" for="c-39544340">[1 more]</label></div><br/><div class="children"><div class="content">Can processor perform addition on them effectively?</div><br/></div></div></div></div></div></div></div></div><div id="39538642" class="c"><input type="checkbox" id="c-39538642" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#39538484">parent</a><span>|</span><a href="#39538658">prev</a><span>|</span><a href="#39545771">next</a><span>|</span><label class="collapse" for="c-39538642">[-]</label><label class="expand" for="c-39538642">[4 more]</label></div><br/><div class="children"><div class="content">Interesting, how do you use -0 in the add, then? Is -0+1-1 a 0 or a -0?<p>&gt; Could the additional -0 carry some pseudo-gradient information<p>It looks like training was done on fp32 or bf16. Low-bit quantization is approximated with STE during training. I&#x27;d expect training itself cause each point to &quot;polarize&quot; towards 1 or -1.<p>&gt; 2-bit quantizations being proposed<p>Symmetric (i.e. without 0) exponential values were pretty popular IIRC.</div><br/><div id="39538942" class="c"><input type="checkbox" id="c-39538942" checked=""/><div class="controls bullet"><span class="by">w-m</span><span>|</span><a href="#39538484">root</a><span>|</span><a href="#39538642">parent</a><span>|</span><a href="#39545771">next</a><span>|</span><label class="collapse" for="c-39538942">[-]</label><label class="expand" for="c-39538942">[3 more]</label></div><br/><div class="children"><div class="content">&gt; how do you use -0 in the add<p>In my mind the two zero values would represent a tiny epsilon around 0, let&#x27;s say -0.01 and +0.01. Looking at them like this, it would mean<p><pre><code>  +0 +0 -0 = +0
  +0 -0 -0 = -0
  +1 * +0 = +0
  -1 * +0 = -0
</code></pre>
Performing addition with the same sign count in each group would be problematic. How to decide on the sign of +0-0 or +1-1, other than flipping a coin?</div><br/><div id="39544611" class="c"><input type="checkbox" id="c-39544611" checked=""/><div class="controls bullet"><span class="by">npunt</span><span>|</span><a href="#39538484">root</a><span>|</span><a href="#39538942">parent</a><span>|</span><a href="#39542469">next</a><span>|</span><label class="collapse" for="c-39544611">[-]</label><label class="expand" for="c-39544611">[1 more]</label></div><br/><div class="children"><div class="content">maybe they could be stored together in two words until they&#x27;re operated on and lose their pairing?</div><br/></div></div></div></div></div></div><div id="39545771" class="c"><input type="checkbox" id="c-39545771" checked=""/><div class="controls bullet"><span class="by">fabiospampinato</span><span>|</span><a href="#39538484">parent</a><span>|</span><a href="#39538642">prev</a><span>|</span><a href="#39545849">next</a><span>|</span><label class="collapse" for="c-39545771">[-]</label><label class="expand" for="c-39545771">[2 more]</label></div><br/><div class="children"><div class="content">I would guess that having 2 zeros is not that useful for NNs, but in general with 2 bits we could encode 4 states, so are there 4 possible states that would be useful to encode? Sure, but would this be better than encoding 3 states? That&#x27;s the entire question imo. I would guess that 3 states are probably better, because negative&#x2F;neutral&#x2F;positive seems the minimal signal that we need these weights to provide.</div><br/><div id="39545859" class="c"><input type="checkbox" id="c-39545859" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39538484">root</a><span>|</span><a href="#39545771">parent</a><span>|</span><a href="#39545849">next</a><span>|</span><label class="collapse" for="c-39545859">[-]</label><label class="expand" for="c-39545859">[1 more]</label></div><br/><div class="children"><div class="content">You could use a negative-two base, and encode {-2, -1, 0, 1}.  See <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Negative_base" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Negative_base</a><p>Or you could use the regular positive-two base and encode {-2, -1, 0, 1} the normal way with two&#x27;s complement.</div><br/></div></div></div></div><div id="39545849" class="c"><input type="checkbox" id="c-39545849" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39538484">parent</a><span>|</span><a href="#39545771">prev</a><span>|</span><a href="#39536971">next</a><span>|</span><label class="collapse" for="c-39545849">[-]</label><label class="expand" for="c-39545849">[1 more]</label></div><br/><div class="children"><div class="content">You might also use a basis of negative-two and use two bits to represent {-2, -1, 0, 1}.<p>Negative bases are fun.  See <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Negative_base" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Negative_base</a></div><br/></div></div></div></div><div id="39536971" class="c"><input type="checkbox" id="c-39536971" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#39538484">prev</a><span>|</span><a href="#39541716">next</a><span>|</span><label class="collapse" for="c-39536971">[-]</label><label class="expand" for="c-39536971">[43 more]</label></div><br/><div class="children"><div class="content">After reading the results I skipped back to the comment section to ask if this was real because it looks a little too good to be true, but figured I should check authors and it&#x27;s Microsoft research and UCAS so yeah, real. This is going to change a lot of things, obviously the edge computing applications they point out, but also this is going to bottom out the cost of providing high-performance LLMs in the cloud. I don&#x27;t know what that means for the economics long term, naively way less costs maybe means new entrants without an entire cloud available can compete easier? I do wonder if something like this has already been found and implemented by either OpenAI or Google.</div><br/><div id="39537113" class="c"><input type="checkbox" id="c-39537113" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#39536971">parent</a><span>|</span><a href="#39537166">next</a><span>|</span><label class="collapse" for="c-39537113">[-]</label><label class="expand" for="c-39537113">[33 more]</label></div><br/><div class="children"><div class="content">After playing with OpenAI&#x27;s GPT4 API, I&#x27;m quite convinced that LLMs would be in everything and everywhere today if inference cost is as low as loading a website and context size is 100x higher.<p>In other words, only inference cost is holding it back from completely changing everything.<p>So if we have a shortcut to getting something like GPT4 to run locally on a small device, watch out.</div><br/><div id="39537330" class="c"><input type="checkbox" id="c-39537330" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537113">parent</a><span>|</span><a href="#39537358">next</a><span>|</span><label class="collapse" for="c-39537330">[-]</label><label class="expand" for="c-39537330">[19 more]</label></div><br/><div class="children"><div class="content">LLMs will give normal people a firmer standing in technological society. That&#x27;s a good thing. But will it change everything? Not a chance. Even if LLMs did change everything, that probably would not be a good thing. Dijkstra says Muslim algebra died when it returned to the rhetoric style, and the modern civilized world could only emerge —for better or for worse— when Western Europe could free itself from the fetters of medieval scholasticism —a vain attempt at verbal precision!—thanks to the carefully, or at least consciously designed formal symbolisms that we owe to people like Vieta, Descartes, Leibniz, and (later) Boole. So don&#x27;t be so proud of these graphics cards you&#x27;ve made, because the ability to understand the human tongue is insignificant compared to the power of math.</div><br/><div id="39538330" class="c"><input type="checkbox" id="c-39538330" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537330">parent</a><span>|</span><a href="#39543701">next</a><span>|</span><label class="collapse" for="c-39538330">[-]</label><label class="expand" for="c-39538330">[1 more]</label></div><br/><div class="children"><div class="content">I agree with your basic thesis here, retrospection will view LLMs as a transitional architecture.<p>However, this paper is evidence that the field is figuring out how to built what&#x27;s actually needed, which is a good thing.</div><br/></div></div><div id="39543701" class="c"><input type="checkbox" id="c-39543701" checked=""/><div class="controls bullet"><span class="by">ordu</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537330">parent</a><span>|</span><a href="#39538330">prev</a><span>|</span><a href="#39537918">next</a><span>|</span><label class="collapse" for="c-39543701">[-]</label><label class="expand" for="c-39543701">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; the modern civilized world could only emerge —for better or for worse— when Western Europe could free itself from the fetters of medieval scholasticism</i><p>I can propose an alternate view of things. Not that I&#x27;m going to argue that it is the only true statement in the world, but I think it is necessary for a thought to progress to have an alternative hypothesis.<p>So the proposition is: formal symbolisms can deal only with those problems that where already solved in imprecise human&#x27;s languages.<p>To invent calculus and orbital mechanics you need first to talk for a several centuries (or thousands of years?) about what is position and velocity, you need to talk your way upto acceleration, and then you need to find a way to measure them and to define in a strict geometric terms. Ah, and infinity, it was a very counter-intuitive idea, Xenon invented some of his paradoxes specifically to point at counter-intuitiveness. When Newton came all these talks and debates did the most of work for him.<p><i>&gt; the ability to understand the human tongue is insignificant compared to the power of math.</i><p>But the fun is: you cannot know if someone understands math if they do not understand human language too. You cannot teach math to those who cannot speak human language.<p>Math is a cream on top with a limited applicability. What math can say about love? I do not like to sound like Dumbledor, but really behind all we do there is an emotions motivating us. Math cannot deal with emotions, because it was built that way <i>and</i> because non-math talks about emotions hadn&#x27;t bring a good model for emotions, which math could express in a formalized language.<p><i>&gt; Dijkstra says </i><p>I wonder when he said it? Before AI concluded that expert-systems based on logic were acknowledged to be a failure or after that?</div><br/><div id="39545870" class="c"><input type="checkbox" id="c-39545870" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39543701">parent</a><span>|</span><a href="#39537918">next</a><span>|</span><label class="collapse" for="c-39545870">[-]</label><label class="expand" for="c-39545870">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So the proposition is: formal symbolisms can deal only with those problems that where already solved in imprecise human&#x27;s languages.<p>&gt; To invent calculus and orbital mechanics you need first to talk for a several centuries (or thousands of years?) about what is position and velocity, you need to talk your way upto acceleration, and then you need to find a way to measure them and to define in a strict geometric terms. Ah, and infinity, it was a very counter-intuitive idea, Xenon invented some of his paradoxes specifically to point at counter-intuitiveness. When Newton came all these talks and debates did the most of work for him.<p>For the sake of argument, let&#x27;s grant your story about what you need to invent calculus.<p>But once you invented calculus, you can then use it to solve all kinds of problems that you would never in a thousand years be able to handle with mere talk.</div><br/></div></div></div></div><div id="39537918" class="c"><input type="checkbox" id="c-39537918" checked=""/><div class="controls bullet"><span class="by">rafaelero</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537330">parent</a><span>|</span><a href="#39543701">prev</a><span>|</span><a href="#39537358">next</a><span>|</span><label class="collapse" for="c-39537918">[-]</label><label class="expand" for="c-39537918">[15 more]</label></div><br/><div class="children"><div class="content">LLM&#x27;s can do math as well.</div><br/><div id="39542709" class="c"><input type="checkbox" id="c-39542709" checked=""/><div class="controls bullet"><span class="by">cooper_ganglia</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537918">parent</a><span>|</span><a href="#39538268">next</a><span>|</span><label class="collapse" for="c-39542709">[-]</label><label class="expand" for="c-39542709">[1 more]</label></div><br/><div class="children"><div class="content">This comment reminded me of that scene in Indiana Jones where the guy is spinning the sword around about to attack Indy, and then Indy just pulls out his pistol and shoots him.</div><br/></div></div><div id="39538268" class="c"><input type="checkbox" id="c-39538268" checked=""/><div class="controls bullet"><span class="by">dns_snek</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537918">parent</a><span>|</span><a href="#39542709">prev</a><span>|</span><a href="#39541181">next</a><span>|</span><label class="collapse" for="c-39538268">[-]</label><label class="expand" for="c-39538268">[9 more]</label></div><br/><div class="children"><div class="content">Last time I checked, GPT-4 couldn&#x27;t reliably add 2 numbers, never mind anything more complex.</div><br/><div id="39538463" class="c"><input type="checkbox" id="c-39538463" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39538268">parent</a><span>|</span><a href="#39543723">next</a><span>|</span><label class="collapse" for="c-39538463">[-]</label><label class="expand" for="c-39538463">[7 more]</label></div><br/><div class="children"><div class="content">Last I checked (and confirmed by repeating it just now) GPT-4 did just fine at adding 2 numbers up, because it knows better now than to do that manually and will express it as Python. It does <i>worse</i> if you try to force it to do it step by step like a child and don&#x27;t reinforce adherence to the rules every step, because just like humans it gets &quot;sloppy&quot; when you try to get it to repeat the same steps over and over.<p>If you want to measure its ability to do mindlessly repetitive tasks without diverging from instructions, you should compare it to humans doing the same, not expect it to act like a calculator.<p>If you want to measure its ability to <i>solve problems</i> that involve many such steps that are simple to express but tedious to carry out, ask it to write and evaluate code to do it instead.</div><br/><div id="39538559" class="c"><input type="checkbox" id="c-39538559" checked=""/><div class="controls bullet"><span class="by">dns_snek</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39538463">parent</a><span>|</span><a href="#39539470">next</a><span>|</span><label class="collapse" for="c-39538559">[-]</label><label class="expand" for="c-39538559">[4 more]</label></div><br/><div class="children"><div class="content">The claim was that &quot;LLMs can do math&quot;. Below they linked a model from Google that might be capable of that, but as a general rule (and with OpenAI&#x27;s models specifically) LLMs can&#x27;t &quot;do math&quot; by any reasonable definition.</div><br/><div id="39538991" class="c"><input type="checkbox" id="c-39538991" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39538559">parent</a><span>|</span><a href="#39542841">next</a><span>|</span><label class="collapse" for="c-39538991">[-]</label><label class="expand" for="c-39538991">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had it do plenty of math. Some it does badly at, some it does fine. Generally it&#x27;s not &quot;disciplined&quot; enough to do things that requires lots of rote repetitive tasks, but neither are most humans, and that has improved drastically as they&#x27;ve adjusted it to instead do what most humans do and use tools. Would it be nice if it <i>also</i> got more willing to &quot;stick to it&quot; when given rote tasks? Sure.<p>But whether or not it can &quot;do maths&quot; to your definition depends very much on what you want it to do, and how you define &quot;do maths&quot;. To me it&#x27;s irrelevant if it&#x27;s doing the low-level calculations as long as it knows how to express them as code. If I wanted a calculator I&#x27;d use a calculator. And I don&#x27;t consider a calculator able to &quot;do math&quot; just because it can precisely add numbers.<p>Meanwhile I&#x27;ve had lengthy discussions with GPT about subjects like orbital mechanics and calculating atmospheric effects where it correctly used maths that I had to double-check not because I didn&#x27;t trust GPT (though I <i>also</i> want&#x27;t to verify for that reason) but because I didn&#x27;t know the maths (not that it was anything particularly advanced, but I lost interest in maths during my CS degree and picked the minimum amount of maths I could get away with).<p>By <i>my</i> definition it can &quot;do maths&quot; just fine. I guess you don&#x27;t consider my view of that &quot;reasonable&quot;. I can live with that, as meanwhile, it will keep doing maths for me when I need it.<p>Of course this was also a case of moving the goalposts to set up a strawman - in the comment of yours I replied to, you claimed it couldn&#x27;t reliably <i>add two numbers</i>.</div><br/><div id="39544281" class="c"><input type="checkbox" id="c-39544281" checked=""/><div class="controls bullet"><span class="by">dns_snek</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39538991">parent</a><span>|</span><a href="#39542841">next</a><span>|</span><label class="collapse" for="c-39544281">[-]</label><label class="expand" for="c-39544281">[1 more]</label></div><br/><div class="children"><div class="content">It often fails at basic 3-4 digit arithmetic. If you&#x27;re stretching that definition far enough to claim that GPT4 can &quot;do math&quot; then I should be able to call myself a commercial pilot because I can land a plane in a sim 20% of the time.<p>I&#x27;m not moving goalposts, the original claim was that LLMs can &quot;do math&quot;. Primary school arithmetic is math.<p>GPT-4 can&#x27;t do math and that&#x27;s <i>okay</i>, I don&#x27;t understand why so many of you are so touchy and defensive about this. It&#x27;s a limitation that exists, nothing more, nothing less.</div><br/></div></div></div></div><div id="39542841" class="c"><input type="checkbox" id="c-39542841" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39538559">parent</a><span>|</span><a href="#39538991">prev</a><span>|</span><a href="#39539470">next</a><span>|</span><label class="collapse" for="c-39542841">[-]</label><label class="expand" for="c-39542841">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 is a tiny subset of &quot;LLMs&quot;.<p>If you train a model to do math (and optimize representation for that), it&#x27;ll do math. GPT-4 just isn&#x27;t, and, generally speaking, they aren&#x27;t, because it&#x27;s much more efficient to train them to &quot;use a calculator&quot;. Same as with humans.</div><br/></div></div></div></div><div id="39539470" class="c"><input type="checkbox" id="c-39539470" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39538463">parent</a><span>|</span><a href="#39538559">prev</a><span>|</span><a href="#39543723">next</a><span>|</span><label class="collapse" for="c-39539470">[-]</label><label class="expand" for="c-39539470">[2 more]</label></div><br/><div class="children"><div class="content">You do realize that arithmetic is a very simple symbolic manipulation task? All you have to do is keep track of the carry. I haven&#x27;t seen an LLM that couldn&#x27;t get digit by digit addition done, but they always mess up the carry.</div><br/><div id="39542025" class="c"><input type="checkbox" id="c-39542025" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39539470">parent</a><span>|</span><a href="#39543723">next</a><span>|</span><label class="collapse" for="c-39542025">[-]</label><label class="expand" for="c-39542025">[1 more]</label></div><br/><div class="children"><div class="content">Just like humans. Try to get <i>regular people</i> do e.g. add 15-16 digit numbers (where is typically where I&#x27;d see GPT4 start to get &quot;sloppy&quot; unless you prompt it the way you would a child who&#x27;s learning and is still prone to get annoyed and wonder why the hell you make them to it manually), and see how many start making mistakes.<p>I find it really comical that this is what people complain about GPT over - there&#x27;s zero benefit to get LLMs to get good at this over other tasks. To the extent we get it &quot;for free&quot; as a benefit of other learning, sure, but when we make kids practice this <i>over and over again</i> to drill doing it without getting sloppy, it has traditionally been out of some belief that it&#x27;s important, but a computer will always have a &quot;calculator&quot; that is far more efficient than the LLM at its disposal and it&#x27;s idiocy to care about whether it does that part well the tedious and hard way or knows how to describe the problem to a more efficient tool<p>I also find it comical that people use tasks where LLMs behaviour is if anything mot human-like, in its tendency to lose focus and start taking shortcuts (before GPT4 started writing Python instead, it&#x27;d for a while try <i>really</i> hard to not give you a step by step breakdown and instead clearly take shortcuts even you prompted it heavily to reason through it step by step), when presented with stupidly repetitive tasks as examples of how they&#x27;re not good enough.</div><br/></div></div></div></div></div></div><div id="39543723" class="c"><input type="checkbox" id="c-39543723" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39538268">parent</a><span>|</span><a href="#39538463">prev</a><span>|</span><a href="#39541181">next</a><span>|</span><label class="collapse" for="c-39543723">[-]</label><label class="expand" for="c-39543723">[1 more]</label></div><br/><div class="children"><div class="content">GPT-x can&#x27;t add, or subtract, or do anything else of the type... it can APPEAR to do so, because that&#x27;s what it was built to do.... act like the text it&#x27;s seen previously and predict what the next text would be.<p>If you include a large amount of properly solved math in its training text, it gets MUCH better at that kind of math.<p>It has a very deep set of intelligences that are alien to us, that allow it to predict and ACT LIKE us, when it comes to generating the next word. You&#x27;re only seeing the output of those intelligences through a very lossy channel.<p>As a side note, there are structures in human language that apparently encode much more information that you might think at first glance. The fact that Word2Vec had such mathematical properties, despite it&#x27;s relative simplicity, astound me to this day. Throwing a bunch of sine&#x2F;cosine values on top of that to represent position in a sentence to enable LLMs is also amazing in that it works.</div><br/></div></div></div></div><div id="39541181" class="c"><input type="checkbox" id="c-39541181" checked=""/><div class="controls bullet"><span class="by">lovasoa</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537918">parent</a><span>|</span><a href="#39538268">prev</a><span>|</span><a href="#39538007">next</a><span>|</span><label class="collapse" for="c-39541181">[-]</label><label class="expand" for="c-39541181">[1 more]</label></div><br/><div class="children"><div class="content">- Hey ChatGTP ! What it 69*<i>94 ?<p>- The result of 69*</i>94 is 6466.</div><br/></div></div><div id="39538007" class="c"><input type="checkbox" id="c-39538007" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537918">parent</a><span>|</span><a href="#39541181">prev</a><span>|</span><a href="#39538285">next</a><span>|</span><label class="collapse" for="c-39538007">[-]</label><label class="expand" for="c-39538007">[2 more]</label></div><br/><div class="children"><div class="content">What makes you think that? Which LLMs?</div><br/><div id="39538255" class="c"><input type="checkbox" id="c-39538255" checked=""/><div class="controls bullet"><span class="by">rafaelero</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39538007">parent</a><span>|</span><a href="#39538285">next</a><span>|</span><label class="collapse" for="c-39538255">[-]</label><label class="expand" for="c-39538255">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;alphageometry-an-olympiad-level-ai-system-for-geometry&#x2F;" rel="nofollow">https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;alphageometry-an-olymp...</a></div><br/></div></div></div></div><div id="39538285" class="c"><input type="checkbox" id="c-39538285" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537918">parent</a><span>|</span><a href="#39538007">prev</a><span>|</span><a href="#39537358">next</a><span>|</span><label class="collapse" for="c-39538285">[-]</label><label class="expand" for="c-39538285">[1 more]</label></div><br/><div class="children"><div class="content">most open models do it poorly though. ChatGPT is better at it.</div><br/></div></div></div></div></div></div><div id="39537358" class="c"><input type="checkbox" id="c-39537358" checked=""/><div class="controls bullet"><span class="by">declaredapple</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537113">parent</a><span>|</span><a href="#39537330">prev</a><span>|</span><a href="#39537620">next</a><span>|</span><label class="collapse" for="c-39537358">[-]</label><label class="expand" for="c-39537358">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll agree with you, and add that inference speed is a big factor too.<p>SDXL-ligtning&#x2F;cascade can generate images in 200ms which is fast enough to fit in a web request, and paradoxically makes it even cheaper to generate.<p>And using groq at 500 t&#x2F;s is wild compared to any of the other platforms.</div><br/><div id="39540297" class="c"><input type="checkbox" id="c-39540297" checked=""/><div class="controls bullet"><span class="by">pennomi</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537358">parent</a><span>|</span><a href="#39537620">next</a><span>|</span><label class="collapse" for="c-39540297">[-]</label><label class="expand" for="c-39540297">[2 more]</label></div><br/><div class="children"><div class="content">500 t&#x2F;s is uncomfortably fast to me. Generating high quality answers at speeds faster than I can read is the point at which I feel like LLMs are magic.<p>I’m glad people are doing it though, and I’ll happily adapt to accessing inference at that speed.</div><br/><div id="39542030" class="c"><input type="checkbox" id="c-39542030" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39540297">parent</a><span>|</span><a href="#39537620">next</a><span>|</span><label class="collapse" for="c-39542030">[-]</label><label class="expand" for="c-39542030">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s important for new applications to emerge where this happens on lots of data. You can&#x27;t run LLMs at scale on tasks like Google might (every webpage) when the cost of each document is so high to process. Interactive chatbots are just the tip.</div><br/></div></div></div></div></div></div><div id="39537620" class="c"><input type="checkbox" id="c-39537620" checked=""/><div class="controls bullet"><span class="by">gitfan86</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537113">parent</a><span>|</span><a href="#39537358">prev</a><span>|</span><a href="#39537167">next</a><span>|</span><label class="collapse" for="c-39537620">[-]</label><label class="expand" for="c-39537620">[1 more]</label></div><br/><div class="children"><div class="content">That is the plan. Even if these independent software improvements don&#x27;t create 10x improvements NVDA and others are making huge improvements.</div><br/></div></div><div id="39537167" class="c"><input type="checkbox" id="c-39537167" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537113">parent</a><span>|</span><a href="#39537620">prev</a><span>|</span><a href="#39537166">next</a><span>|</span><label class="collapse" for="c-39537167">[-]</label><label class="expand" for="c-39537167">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s coming in October with the new Apple chip</div><br/><div id="39537255" class="c"><input type="checkbox" id="c-39537255" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537167">parent</a><span>|</span><a href="#39537166">next</a><span>|</span><label class="collapse" for="c-39537255">[-]</label><label class="expand" for="c-39537255">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be very surprised if Apple can put something on the level of GPT4 on a handheld. Remember, GPT4 is estimated to be around 1.7 trillion parameters. That&#x27;s 3.4TB at 16 bit and it would still be ~340GB at 1.58bits. The best we can hope for is a low-ish level few billion parameter model. Which would still be cool on a phone, but as of today these models are nowhere near GPT4.</div><br/><div id="39537424" class="c"><input type="checkbox" id="c-39537424" checked=""/><div class="controls bullet"><span class="by">jairuhme</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537255">parent</a><span>|</span><a href="#39537600">next</a><span>|</span><label class="collapse" for="c-39537424">[-]</label><label class="expand" for="c-39537424">[2 more]</label></div><br/><div class="children"><div class="content">They won&#x27;t have something at that size because as you pointed out, it is still huge.  But depending on how they are used, smaller parameter models may be better for specific on-phone tasks that start to make the size of the model not a problem.  GPT4 is so large because it is very general purpose with the goal seeming to be to answer anything.  You could have a smaller model focused solely on Siri or something that wouldn&#x27;t require the parameter size of GPT4</div><br/><div id="39543365" class="c"><input type="checkbox" id="c-39543365" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537424">parent</a><span>|</span><a href="#39537600">next</a><span>|</span><label class="collapse" for="c-39543365">[-]</label><label class="expand" for="c-39543365">[1 more]</label></div><br/><div class="children"><div class="content">The thing a about GPT4 that matters so much is not just raw knowledge retention, but complex, abstract reasoning and even knowing what it doesn&#x27;t know. We haven&#x27;t seen that yet in smaller models and it&#x27;s unclear if it is even possible. The best we could hope for right now is a better natural language interface than Siri for calling OS functions.</div><br/></div></div></div></div><div id="39537600" class="c"><input type="checkbox" id="c-39537600" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537255">parent</a><span>|</span><a href="#39537424">prev</a><span>|</span><a href="#39537166">next</a><span>|</span><label class="collapse" for="c-39537600">[-]</label><label class="expand" for="c-39537600">[5 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need &quot;GPT4&quot; though. Mixtral 8x7B is robust and can be run in 36 Gb, 24 Gb if you&#x27;re willing to compromise. A 1.5 bit quantization should bring it down to 16. That&#x27;s still a lot compared to the iPhone 15&#x27;s 6, but it&#x27;s close enough to imagine it happening soon. With some kind of streaming-from-flash architecture you might be in the realm already.</div><br/><div id="39539123" class="c"><input type="checkbox" id="c-39539123" checked=""/><div class="controls bullet"><span class="by">creshal</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537600">parent</a><span>|</span><a href="#39537166">next</a><span>|</span><label class="collapse" for="c-39539123">[-]</label><label class="expand" for="c-39539123">[4 more]</label></div><br/><div class="children"><div class="content">&gt; With some kind of streaming-from-flash architecture you might be in the realm already.<p>I thought mmap&#x27;ing models to only keep the currently needed pieces in RAM was something that was figured out ~6 months ago? Performance wasn&#x27;t terribly great iirc, but with how much faster 1.58B is, it should still be okay-ish.</div><br/><div id="39542002" class="c"><input type="checkbox" id="c-39542002" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39539123">parent</a><span>|</span><a href="#39543531">next</a><span>|</span><label class="collapse" for="c-39542002">[-]</label><label class="expand" for="c-39542002">[1 more]</label></div><br/><div class="children"><div class="content">There is a more detailed paper from Apple on this. Basically, you can do a little bit better than only keeping current weights in RAM with mmap.<p>For LLM, you are mostly dealing with b = W @ a where a and b are vectors, only W is the matrix. If a is sparse (i.e. have a few 0s), you don&#x27;t need all the columns from W to do the matrix-vector multiplication. A cleverly arranged W can make sure during inference, only related columns loaded from flash. Further more, if you can apply &quot;One Weird Trick&quot; paper to this matrix-vector multiplication, you can shard W by rows, i.e. `b[i:i+n] = W[i:i+n,:] @ a[i:i+n] for i in range(N, N &#x2F; b)` such that while the previous b[i:i+n] is still computing, you have visibility on which columns of the next matrix to be loaded already.</div><br/></div></div><div id="39543531" class="c"><input type="checkbox" id="c-39543531" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39539123">parent</a><span>|</span><a href="#39542002">prev</a><span>|</span><a href="#39539442">next</a><span>|</span><label class="collapse" for="c-39543531">[-]</label><label class="expand" for="c-39543531">[1 more]</label></div><br/><div class="children"><div class="content">You need all of the model in RAM to perform the matmult that gets you the next token from it. There&#x27;s no shortcut.</div><br/></div></div><div id="39539442" class="c"><input type="checkbox" id="c-39539442" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39539123">parent</a><span>|</span><a href="#39543531">prev</a><span>|</span><a href="#39537166">next</a><span>|</span><label class="collapse" for="c-39539442">[-]</label><label class="expand" for="c-39539442">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what use that is, other than to maintain the KV cache across requests.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39537166" class="c"><input type="checkbox" id="c-39537166" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39536971">parent</a><span>|</span><a href="#39537113">prev</a><span>|</span><a href="#39537099">next</a><span>|</span><label class="collapse" for="c-39537166">[-]</label><label class="expand" for="c-39537166">[2 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t be surprised if this causes hardware startups to pop up that build accelerator cards tuned for this architecture. It seems stupidly simple to do inference in hardware, and with most of the training being quantized as well you might even be able to provide speedups (and energy savings) for training with reasonable investment and on cheaper processor nodes than what Nvidia is using.<p>Sure, Nvidia might eat their lunch in a couple of years, but bitcoin ASICs prove that you can have a niche producing specialized processors, and VCs would probably jump at the thought of disrupting Nvidia&#x27;s high margin business.</div><br/><div id="39541381" class="c"><input type="checkbox" id="c-39541381" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537166">parent</a><span>|</span><a href="#39537099">next</a><span>|</span><label class="collapse" for="c-39541381">[-]</label><label class="expand" for="c-39541381">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s like a million startups promising analog &#x2F; bit-level computation, inference-only, cheap computation.<p>There&#x27;s rain.ai, d-matrix, etc.</div><br/></div></div></div></div><div id="39537099" class="c"><input type="checkbox" id="c-39537099" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#39536971">parent</a><span>|</span><a href="#39537166">prev</a><span>|</span><a href="#39537532">next</a><span>|</span><label class="collapse" for="c-39537099">[-]</label><label class="expand" for="c-39537099">[4 more]</label></div><br/><div class="children"><div class="content">It also means the largest models can be scaled up significantly with the same inference budget.</div><br/><div id="39537169" class="c"><input type="checkbox" id="c-39537169" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537099">parent</a><span>|</span><a href="#39537532">next</a><span>|</span><label class="collapse" for="c-39537169">[-]</label><label class="expand" for="c-39537169">[3 more]</label></div><br/><div class="children"><div class="content">Depends. The only paper they cite for training: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.11453.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.11453.pdf</a> doesn&#x27;t improve training costs much and most models are already training constrained. Not everyone has $200m to throw at training another model from scratch.</div><br/><div id="39543826" class="c"><input type="checkbox" id="c-39543826" checked=""/><div class="controls bullet"><span class="by">arunk47</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537169">parent</a><span>|</span><a href="#39537532">next</a><span>|</span><label class="collapse" for="c-39543826">[-]</label><label class="expand" for="c-39543826">[2 more]</label></div><br/><div class="children"><div class="content">Is there any scope for indie builders?</div><br/><div id="39545247" class="c"><input type="checkbox" id="c-39545247" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39543826">parent</a><span>|</span><a href="#39537532">next</a><span>|</span><label class="collapse" for="c-39545247">[-]</label><label class="expand" for="c-39545247">[1 more]</label></div><br/><div class="children"><div class="content">Not really. These are slightly better for memory during pre-training and fine turning but not enough to make a 4090 usable even for a 7b model.</div><br/></div></div></div></div></div></div></div></div><div id="39537532" class="c"><input type="checkbox" id="c-39537532" checked=""/><div class="controls bullet"><span class="by">btbuildem</span><span>|</span><a href="#39536971">parent</a><span>|</span><a href="#39537099">prev</a><span>|</span><a href="#39541716">next</a><span>|</span><label class="collapse" for="c-39537532">[-]</label><label class="expand" for="c-39537532">[3 more]</label></div><br/><div class="children"><div class="content">If this dethrones Nvidia, it would be a wonderful side effect</div><br/><div id="39537930" class="c"><input type="checkbox" id="c-39537930" checked=""/><div class="controls bullet"><span class="by">rafaelero</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537532">parent</a><span>|</span><a href="#39541716">next</a><span>|</span><label class="collapse" for="c-39537930">[-]</label><label class="expand" for="c-39537930">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more likely that Nvidia will offer support to INT2 in the next generation and keep their dominance.</div><br/><div id="39544211" class="c"><input type="checkbox" id="c-39544211" checked=""/><div class="controls bullet"><span class="by">Klipper3</span><span>|</span><a href="#39536971">root</a><span>|</span><a href="#39537930">parent</a><span>|</span><a href="#39541716">next</a><span>|</span><label class="collapse" for="c-39544211">[-]</label><label class="expand" for="c-39544211">[1 more]</label></div><br/><div class="children"><div class="content">INT2 ternary is equivalent to INT1 + binary mask. Nvidia supprted INT1 matrix multiply in RTX20 and RTX30 generations, nobody used it, so they removed INT1 support from RTX40 generation.</div><br/></div></div></div></div></div></div></div></div><div id="39541716" class="c"><input type="checkbox" id="c-39541716" checked=""/><div class="controls bullet"><span class="by">gojomo</span><span>|</span><a href="#39536971">prev</a><span>|</span><a href="#39543834">next</a><span>|</span><label class="collapse" for="c-39541716">[-]</label><label class="expand" for="c-39541716">[17 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a &#x27;bit&#x27; (&quot;Binary digIT&quot;). It&#x27;s closer to a &#x27;trit&#x27; (&quot;TeRnary-digIT&quot;). Specifically, ternary digits spanning {-1, 0, 1} (rather than the usual {0, 1, 2} in a base-3 numbering system) are &#x27;balanced ternary&#x27;.<p>A great intro to the theoretical reasons ternary might have some promise in computing is this 2001 article from &#x27;American Scientist&#x27;, &quot;Third Base&quot;, which quotes Knuth calling balanced-ternary &quot;perhaps the prettiest numbering system of all&quot; and also discusses an abortive Soviet effort in the direction of ternary computing:<p><a href="http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20011205185830&#x2F;http:&#x2F;&#x2F;americanscientist.org&#x2F;Issues&#x2F;Comsci01&#x2F;Compsci2001-11.html" rel="nofollow">http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20011205185830&#x2F;http:&#x2F;&#x2F;americansci...</a><p>In an aside, the article hints that <i>e</i>-nary digits (base 2.718…) if somehow made practical&#x2F;meaningful, might actually be better than ternary (or perhaps even optimal?).<p>So maybe this paper&#x27;s observation that ~&quot;1.58 bits&quot; (ln2(3) binary-digits) is a sweet-spot could be further refined into some method for representing the state of a e-nary-modeled algorithm in ln2(e) binary-digits (~&quot;1.44 bits&quot;) per underlying e-it.<p>(As it may be of renewed interest, I&#x27;ve also put this 2001 &quot;American Scientist&quot;  base-3 intro as a new HN submission for discussion: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39541756">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39541756</a>)</div><br/><div id="39542206" class="c"><input type="checkbox" id="c-39542206" checked=""/><div class="controls bullet"><span class="by">nighthawk454</span><span>|</span><a href="#39541716">parent</a><span>|</span><a href="#39542048">next</a><span>|</span><label class="collapse" for="c-39542206">[-]</label><label class="expand" for="c-39542206">[4 more]</label></div><br/><div class="children"><div class="content">Base e is the optimal base for number representation, so that’s probably why. Followed by base 3, then base 2.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Radix_economy" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Radix_economy</a></div><br/><div id="39542954" class="c"><input type="checkbox" id="c-39542954" checked=""/><div class="controls bullet"><span class="by">o11c</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39542206">parent</a><span>|</span><a href="#39545886">next</a><span>|</span><label class="collapse" for="c-39542954">[-]</label><label class="expand" for="c-39542954">[2 more]</label></div><br/><div class="children"><div class="content">FSVO &quot;optimal&quot;. In practice, both physical reality and algorithm design strongly favors base 2.</div><br/><div id="39543007" class="c"><input type="checkbox" id="c-39543007" checked=""/><div class="controls bullet"><span class="by">nighthawk454</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39542954">parent</a><span>|</span><a href="#39545886">next</a><span>|</span><label class="collapse" for="c-39543007">[-]</label><label class="expand" for="c-39543007">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, specifically, the definition of optimal provided - radix economy. There are plenty of other considerations one could make in other contexts. Practically, a transcendental base seems... rather impractical. And base 2 is not so much &#x27;more optimal&#x27; than base 3 to warrant the electrical complexity probably, for example.</div><br/></div></div></div></div><div id="39545886" class="c"><input type="checkbox" id="c-39545886" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39542206">parent</a><span>|</span><a href="#39542954">prev</a><span>|</span><a href="#39542048">next</a><span>|</span><label class="collapse" for="c-39545886">[-]</label><label class="expand" for="c-39545886">[1 more]</label></div><br/><div class="children"><div class="content">Negative bases are probably even better, because you can represent negative numbers without worrying about extra sign handling.</div><br/></div></div></div></div><div id="39542048" class="c"><input type="checkbox" id="c-39542048" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39541716">parent</a><span>|</span><a href="#39542206">prev</a><span>|</span><a href="#39542823">next</a><span>|</span><label class="collapse" for="c-39542048">[-]</label><label class="expand" for="c-39542048">[1 more]</label></div><br/><div class="children"><div class="content">It is obviously pretty common to represent matrices with lots of zeros in a sparse format, like csr or something. I wonder if they could get away with  1-bit representation using a sparse matrix. Of course, it would be a little different from a typical sparse matrix because there’s no problem normally having a zero-value in a structurally non-zero location.</div><br/></div></div><div id="39542823" class="c"><input type="checkbox" id="c-39542823" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#39541716">parent</a><span>|</span><a href="#39542048">prev</a><span>|</span><a href="#39542101">next</a><span>|</span><label class="collapse" for="c-39542823">[-]</label><label class="expand" for="c-39542823">[1 more]</label></div><br/><div class="children"><div class="content">Note that they&#x27;re not claiming that their LLM is 1-bit - they&#x27;re saying that there is a 1-bit era of LLMs. What they do say is that their approach is <i>a variant</i> of a 1-bit LLM variant, namely a ternary LLM (they explicitly state that in the abstract).</div><br/></div></div><div id="39542101" class="c"><input type="checkbox" id="c-39542101" checked=""/><div class="controls bullet"><span class="by">no_identd</span><span>|</span><a href="#39541716">parent</a><span>|</span><a href="#39542823">prev</a><span>|</span><a href="#39542399">next</a><span>|</span><label class="collapse" for="c-39542101">[-]</label><label class="expand" for="c-39542101">[1 more]</label></div><br/><div class="children"><div class="content">See also:<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nat_(unit)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nat_(unit)</a> (make sure to read the footnotes, too)<p>Edit:
See also also, on the radix economy of balanced ternary (called &quot;tristate&quot;) vs base 3:
<a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20090312094241&#x2F;http:&#x2F;&#x2F;abhijit.info&#x2F;tristate&#x2F;tristate.html#Base3" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20090312094241&#x2F;http:&#x2F;&#x2F;abhijit.in...</a> + a wild Marvin Minsky appears: <a href="https:&#x2F;&#x2F;archive.fo&#x2F;gL2Bv" rel="nofollow">https:&#x2F;&#x2F;archive.fo&#x2F;gL2Bv</a><p>That page also brings up the whole &quot;but division&quot; problem with balanced ternary, however, I personally suspect that <a href="http:&#x2F;&#x2F;degiorgi.math.hr&#x2F;aaa_sem&#x2F;Div_Krishna&#x2F;887-889.pdf" rel="nofollow">http:&#x2F;&#x2F;degiorgi.math.hr&#x2F;aaa_sem&#x2F;Div_Krishna&#x2F;887-889.pdf</a> (&quot;A Division Algorithm for Signed-Digit Arithmetic&quot; by Chin Tung, from 1968 !) might offer an overlooked path to a solution to that problem<p>And see also also², this quote from TAOCP:<p>&quot;Cauchy pointed out that negative digits make it unneccesary for a person to memorize the multiplication table past 5x5.&quot;<p>The—INCREDIBLY ANNOYING TO LOCATE—source for which is &quot;105. Calculs numériques. sur les moyens d&#x27;éviter les erreurs dans les calculs numériques.&quot; on Pdf page 445&#x2F;document page 431 here:<p><a href="https:&#x2F;&#x2F;www.e-rara.ch&#x2F;download&#x2F;pdf&#x2F;5702285?name=Tome%2520V%46" rel="nofollow">https:&#x2F;&#x2F;www.e-rara.ch&#x2F;download&#x2F;pdf&#x2F;5702285?name=Tome%2520V%4...</a><p>See also also³:
<a href="https:&#x2F;&#x2F;pdfs.semanticscholar.org&#x2F;5f77&#x2F;b1cf105024b41b6824ba91ab1cb6e19b0692.pdf" rel="nofollow">https:&#x2F;&#x2F;pdfs.semanticscholar.org&#x2F;5f77&#x2F;b1cf105024b41b6824ba91...</a> (Vince, Andrew - Radix Representation and Rep-Tiling)<p>( +a vaguely related paper here on quantum mechanics &amp; radix economy, BUT it makes the mistake of using an overly specific formula applicable only to unsigned-digit representations thus drawing the wrong conclusions: <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;profile&#x2F;Vladimir_Garcia-Morales&#x2F;publication&#x2F;259578204_Quantum_Mechanics_and_the_Principle_of_Least_Radix_Economy&#x2F;links&#x2F;5688127e08ae1e63f1f73000&#x2F;Quantum-Mechanics-and-the-Principle-of-Least-Radix-Economy.pdf" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;profile&#x2F;Vladimir_Garcia-Morales...</a> )</div><br/></div></div><div id="39542399" class="c"><input type="checkbox" id="c-39542399" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39541716">parent</a><span>|</span><a href="#39542101">prev</a><span>|</span><a href="#39542434">next</a><span>|</span><label class="collapse" for="c-39542399">[-]</label><label class="expand" for="c-39542399">[4 more]</label></div><br/><div class="children"><div class="content">How useful are -0 and 0?  You could splurge on two bits per value which gives you { -1, -0, 0, 1 }</div><br/><div id="39545187" class="c"><input type="checkbox" id="c-39545187" checked=""/><div class="controls bullet"><span class="by">schiffern</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39542399">parent</a><span>|</span><a href="#39542818">next</a><span>|</span><label class="collapse" for="c-39545187">[-]</label><label class="expand" for="c-39545187">[2 more]</label></div><br/><div class="children"><div class="content">Rather than (indistinguishable?) 0 and -0, why not add back some magnitude  in the positive direction?<p><pre><code>  { -1, 0, 1, 2 }
</code></pre>
is most obvious, but it&#x27;s not clear whether it&#x27;s better or worse than<p><pre><code>  { -1, 0, 1&#x2F;2, 1 }
</code></pre>
Maybe theoretically (if not architecturally) it would best to &quot;split the difference&quot; between the two and choose<p><pre><code>  { -1, 0, 1&#x2F;phi, phi }
</code></pre>
or perhaps the more implementable<p><pre><code>  { -1, 0, 1, 3 }


</code></pre>
EDIT: Of course you can also go the <i>other</i> way, with<p><pre><code>  { -1, 1 }</code></pre></div><br/><div id="39545219" class="c"><input type="checkbox" id="c-39545219" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39545187">parent</a><span>|</span><a href="#39542818">next</a><span>|</span><label class="collapse" for="c-39545219">[-]</label><label class="expand" for="c-39545219">[1 more]</label></div><br/><div class="children"><div class="content">-0 is not indistinguishable from 0 in floating point math.  Most ops return +0 and -0 can behave differently.  I don&#x27;t know of any examples where -0 is important for machine learning, though.</div><br/></div></div></div></div><div id="39542818" class="c"><input type="checkbox" id="c-39542818" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39542399">parent</a><span>|</span><a href="#39545187">prev</a><span>|</span><a href="#39542434">next</a><span>|</span><label class="collapse" for="c-39542818">[-]</label><label class="expand" for="c-39542818">[1 more]</label></div><br/><div class="children"><div class="content">3^5=243. so use a byte to represent a vector of 5 ternary values, leaving some possible signaling values.</div><br/></div></div></div></div><div id="39542434" class="c"><input type="checkbox" id="c-39542434" checked=""/><div class="controls bullet"><span class="by">Razengan</span><span>|</span><a href="#39541716">parent</a><span>|</span><a href="#39542399">prev</a><span>|</span><a href="#39543834">next</a><span>|</span><label class="collapse" for="c-39542434">[-]</label><label class="expand" for="c-39542434">[5 more]</label></div><br/><div class="children"><div class="content">Why not a tit?</div><br/><div id="39542480" class="c"><input type="checkbox" id="c-39542480" checked=""/><div class="controls bullet"><span class="by">jdiff</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39542434">parent</a><span>|</span><a href="#39543349">next</a><span>|</span><label class="collapse" for="c-39542480">[-]</label><label class="expand" for="c-39542480">[1 more]</label></div><br/><div class="children"><div class="content">Because bi- is two, tri- is three. Ti- is meaningless, and not good enough of a joke to make up for it.</div><br/></div></div><div id="39543349" class="c"><input type="checkbox" id="c-39543349" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39542434">parent</a><span>|</span><a href="#39542480">prev</a><span>|</span><a href="#39543834">next</a><span>|</span><label class="collapse" for="c-39543349">[-]</label><label class="expand" for="c-39543349">[3 more]</label></div><br/><div class="children"><div class="content">They renamed the biggest ML conference (NIPS) over the same joke, so don&#x27;t count on it.</div><br/><div id="39546023" class="c"><input type="checkbox" id="c-39546023" checked=""/><div class="controls bullet"><span class="by">Razengan</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39543349">parent</a><span>|</span><a href="#39543834">next</a><span>|</span><label class="collapse" for="c-39546023">[-]</label><label class="expand" for="c-39546023">[2 more]</label></div><br/><div class="children"><div class="content">Why? Just because it&#x27;s spelled identical to a human body part?<p>This kind of shit is one of the most bizarre things about human society (or the prude cultures of it at least), to consider the most natural things so taboo and a &quot;joke&quot; to mention.</div><br/><div id="39547223" class="c"><input type="checkbox" id="c-39547223" checked=""/><div class="controls bullet"><span class="by">kaelwd</span><span>|</span><a href="#39541716">root</a><span>|</span><a href="#39546023">parent</a><span>|</span><a href="#39543834">next</a><span>|</span><label class="collapse" for="c-39547223">[-]</label><label class="expand" for="c-39547223">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;youtu.be&#x2F;VS11bRa8NnQ?t=122" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;VS11bRa8NnQ?t=122</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="39543834" class="c"><input type="checkbox" id="c-39543834" checked=""/><div class="controls bullet"><span class="by">esha_manideep</span><span>|</span><a href="#39541716">prev</a><span>|</span><a href="#39537325">next</a><span>|</span><label class="collapse" for="c-39543834">[-]</label><label class="expand" for="c-39543834">[1 more]</label></div><br/><div class="children"><div class="content">These models will are compatible with llama.cpp out of the box, we (GigaML - <a href="https:&#x2F;&#x2F;gigaml.com">https:&#x2F;&#x2F;gigaml.com</a>) are planning to train a small model (3-4B, 1-bit, opensource) with the latest stack-v2 dataset released today. Let me know if anyone is interested in collaborating with us.</div><br/></div></div><div id="39537325" class="c"><input type="checkbox" id="c-39537325" checked=""/><div class="controls bullet"><span class="by">ulnarkressty</span><span>|</span><a href="#39543834">prev</a><span>|</span><a href="#39536659">next</a><span>|</span><label class="collapse" for="c-39537325">[-]</label><label class="expand" for="c-39537325">[1 more]</label></div><br/><div class="children"><div class="content">Take this with a grain of salt until someone reproduces it. Improvements such as these require extraordinary evidence. Not to mention extreme quantization has been tried before.</div><br/></div></div><div id="39536659" class="c"><input type="checkbox" id="c-39536659" checked=""/><div class="controls bullet"><span class="by">tuananh</span><span>|</span><a href="#39537325">prev</a><span>|</span><a href="#39541673">next</a><span>|</span><label class="collapse" for="c-39536659">[-]</label><label class="expand" for="c-39536659">[3 more]</label></div><br/><div class="children"><div class="content">Major breakthrough in LLM scene. Achieve performance and perplexity equivalent to full FP16 models of same parameter size.<p>And you can fit 120B model with a single card 24GB VRAM. This is mind blowing.</div><br/><div id="39536889" class="c"><input type="checkbox" id="c-39536889" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#39536659">parent</a><span>|</span><a href="#39541673">next</a><span>|</span><label class="collapse" for="c-39536889">[-]</label><label class="expand" for="c-39536889">[2 more]</label></div><br/><div class="children"><div class="content">I mean, it expands the hardware selection, but until there&#x27;s models and leader boards etc, can&#x27;t really say it&#x27;s a break through.</div><br/><div id="39545517" class="c"><input type="checkbox" id="c-39545517" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39536659">root</a><span>|</span><a href="#39536889">parent</a><span>|</span><a href="#39541673">next</a><span>|</span><label class="collapse" for="c-39545517">[-]</label><label class="expand" for="c-39545517">[1 more]</label></div><br/><div class="children"><div class="content">I would assume a GPU isn’t specifically optimized for ternary computation and specialized accelerators would whip the pants off a GPU</div><br/></div></div></div></div></div></div><div id="39541673" class="c"><input type="checkbox" id="c-39541673" checked=""/><div class="controls bullet"><span class="by">fgfm</span><span>|</span><a href="#39536659">prev</a><span>|</span><a href="#39542445">next</a><span>|</span><label class="collapse" for="c-39541673">[-]</label><label class="expand" for="c-39541673">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny how discoveries in NLP &amp; computer vision complement each other. The replacement of multiplication by additions made me think about the AdderNet paper (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.13200" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.13200</a>), which concluded as you had to suffer almost no performance drop.<p>Perhaps the accumulators in current hardware cannot leverage this to its full potential, but combined with such a strict quantization, this would open LLM to the wider ML community much earlier than expected (when consumer hardware allows you to train near SOTA LLMs from scratch on your machine).</div><br/></div></div><div id="39542445" class="c"><input type="checkbox" id="c-39542445" checked=""/><div class="controls bullet"><span class="by">oxxoxoxooo</span><span>|</span><a href="#39541673">prev</a><span>|</span><a href="#39537646">next</a><span>|</span><label class="collapse" for="c-39542445">[-]</label><label class="expand" for="c-39542445">[2 more]</label></div><br/><div class="children"><div class="content">Prior art:<p>Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1602.02830" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1602.02830</a><p>Ternary Neural Networks for Resource-Efficient AI Applications<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1609.00222" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1609.00222</a></div><br/><div id="39543596" class="c"><input type="checkbox" id="c-39543596" checked=""/><div class="controls bullet"><span class="by">kandu</span><span>|</span><a href="#39542445">parent</a><span>|</span><a href="#39537646">next</a><span>|</span><label class="collapse" for="c-39543596">[-]</label><label class="expand" for="c-39543596">[1 more]</label></div><br/><div class="children"><div class="content">Also: training neural networks by turning connections on and off, or by just flipping the sign of the weights: 
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.16627" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.16627</a></div><br/></div></div></div></div><div id="39537646" class="c"><input type="checkbox" id="c-39537646" checked=""/><div class="controls bullet"><span class="by">alexey-salmin</span><span>|</span><a href="#39542445">prev</a><span>|</span><a href="#39536825">next</a><span>|</span><label class="collapse" for="c-39537646">[-]</label><label class="expand" for="c-39537646">[1 more]</label></div><br/><div class="children"><div class="content">Also from Microsoft in 2021: Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance [1]<p>[1] <a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;blog&#x2F;make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;blog&#x2F;make-every-fea...</a></div><br/></div></div><div id="39536825" class="c"><input type="checkbox" id="c-39536825" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#39537646">prev</a><span>|</span><a href="#39543439">next</a><span>|</span><label class="collapse" for="c-39536825">[-]</label><label class="expand" for="c-39536825">[5 more]</label></div><br/><div class="children"><div class="content">Too bad there seem to be no pretrained models to download. This is not a quantization method to apply on existing models, so having the pretrained weights is needed if one wants to test it.</div><br/><div id="39537315" class="c"><input type="checkbox" id="c-39537315" checked=""/><div class="controls bullet"><span class="by">bArray</span><span>|</span><a href="#39536825">parent</a><span>|</span><a href="#39543439">next</a><span>|</span><label class="collapse" for="c-39537315">[-]</label><label class="expand" for="c-39537315">[4 more]</label></div><br/><div class="children"><div class="content">+1 On this, the real proof would have been testing both models side-by-side.<p>It seems that it may be published on GitHub [1] according to HuggingFace [2].<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;unilm&#x2F;tree&#x2F;master&#x2F;bitnet">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;unilm&#x2F;tree&#x2F;master&#x2F;bitnet</a><p>[2] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2402.17764" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2402.17764</a></div><br/><div id="39542072" class="c"><input type="checkbox" id="c-39542072" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39536825">root</a><span>|</span><a href="#39537315">parent</a><span>|</span><a href="#39537421">next</a><span>|</span><label class="collapse" for="c-39542072">[-]</label><label class="expand" for="c-39542072">[1 more]</label></div><br/><div class="children"><div class="content">From [2]:<p>&gt; We would definitely be happy to open-source the models for future research. Please stay tuned!</div><br/></div></div><div id="39537421" class="c"><input type="checkbox" id="c-39537421" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#39536825">root</a><span>|</span><a href="#39537315">parent</a><span>|</span><a href="#39542072">prev</a><span>|</span><a href="#39543773">next</a><span>|</span><label class="collapse" for="c-39537421">[-]</label><label class="expand" for="c-39537421">[1 more]</label></div><br/><div class="children"><div class="content">Nothing there yet, but it&#x27;s good to know they want to publish just did not get around to yet.</div><br/></div></div><div id="39543773" class="c"><input type="checkbox" id="c-39543773" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#39536825">root</a><span>|</span><a href="#39537315">parent</a><span>|</span><a href="#39537421">prev</a><span>|</span><a href="#39543439">next</a><span>|</span><label class="collapse" for="c-39543773">[-]</label><label class="expand" for="c-39543773">[1 more]</label></div><br/><div class="children"><div class="content">link #2 appears to be broken.</div><br/></div></div></div></div></div></div><div id="39543439" class="c"><input type="checkbox" id="c-39543439" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#39536825">prev</a><span>|</span><a href="#39537262">next</a><span>|</span><label class="collapse" for="c-39543439">[-]</label><label class="expand" for="c-39543439">[5 more]</label></div><br/><div class="children"><div class="content">This really just sounds absurd. How can ternary possibly encode enough information?<p>Anyone willing to explain it like I’m a Django developer who watched half a karpathy video?</div><br/><div id="39546815" class="c"><input type="checkbox" id="c-39546815" checked=""/><div class="controls bullet"><span class="by">barbarr</span><span>|</span><a href="#39543439">parent</a><span>|</span><a href="#39545414">next</a><span>|</span><label class="collapse" for="c-39546815">[-]</label><label class="expand" for="c-39546815">[1 more]</label></div><br/><div class="children"><div class="content">The activations are still 8-bit, so a lot of complexity and nonlinearity is still expressible. Only the weights are 1.58-bit.</div><br/></div></div><div id="39545414" class="c"><input type="checkbox" id="c-39545414" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#39543439">parent</a><span>|</span><a href="#39546815">prev</a><span>|</span><a href="#39543560">next</a><span>|</span><label class="collapse" for="c-39545414">[-]</label><label class="expand" for="c-39545414">[2 more]</label></div><br/><div class="children"><div class="content">Because by making the model larger you don&#x27;t need 64bit precision floats you only need 64 discrete bits.</div><br/><div id="39547176" class="c"><input type="checkbox" id="c-39547176" checked=""/><div class="controls bullet"><span class="by">gemeral</span><span>|</span><a href="#39543439">root</a><span>|</span><a href="#39545414">parent</a><span>|</span><a href="#39543560">next</a><span>|</span><label class="collapse" for="c-39547176">[-]</label><label class="expand" for="c-39547176">[1 more]</label></div><br/><div class="children"><div class="content">Do you mind pointing out where they make the model larger? The paper seems to suggest they are maintaining the same model sizes.<p>&gt; Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption</div><br/></div></div></div></div></div></div><div id="39537262" class="c"><input type="checkbox" id="c-39537262" checked=""/><div class="controls bullet"><span class="by">Klipper3</span><span>|</span><a href="#39543439">prev</a><span>|</span><a href="#39540944">next</a><span>|</span><label class="collapse" for="c-39537262">[-]</label><label class="expand" for="c-39537262">[3 more]</label></div><br/><div class="children"><div class="content">The theoretical capacity of a binary network is 69% of the capacity of a full-weight network, so it makes sense that LLM would converge to 1-bit networks in the long term.<p>It&#x27;s nice to finally see practical networks reach the theoretical limits found in the statistical mechanics of Ising models. A good pointer to efficient 1-bit training, from the statistical mechanics point of view, is here:<p><a href="https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;full&#x2F;10.1073&#x2F;pnas.0700324104" rel="nofollow">https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;full&#x2F;10.1073&#x2F;pnas.0700324104</a></div><br/><div id="39544061" class="c"><input type="checkbox" id="c-39544061" checked=""/><div class="controls bullet"><span class="by">arunk47</span><span>|</span><a href="#39537262">parent</a><span>|</span><a href="#39540944">next</a><span>|</span><label class="collapse" for="c-39544061">[-]</label><label class="expand" for="c-39544061">[2 more]</label></div><br/><div class="children"><div class="content">What is stopping us right now from doing this one bit networks ?</div><br/><div id="39547452" class="c"><input type="checkbox" id="c-39547452" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39537262">root</a><span>|</span><a href="#39544061">parent</a><span>|</span><a href="#39540944">next</a><span>|</span><label class="collapse" for="c-39547452">[-]</label><label class="expand" for="c-39547452">[1 more]</label></div><br/><div class="children"><div class="content">I think no code was released yet</div><br/></div></div></div></div></div></div><div id="39540944" class="c"><input type="checkbox" id="c-39540944" checked=""/><div class="controls bullet"><span class="by">rapatel0</span><span>|</span><a href="#39537262">prev</a><span>|</span><a href="#39537241">next</a><span>|</span><label class="collapse" for="c-39540944">[-]</label><label class="expand" for="c-39540944">[1 more]</label></div><br/><div class="children"><div class="content">The mathematics of the BNNs are sound. The shannon entropy of a word is really small (I vaguely remember ~2 bits). Also all neural networks are ridiculously over provisioned.<p>I worked on 7 years ago trying to efficiently binarize CNNs from existing models. It the difficult was getting training running without the losses going to high. I think that vision models will be much more difficult to binarize, but you might not need to with clip if the vision encoder stays in regular math {fp16,int8}</div><br/></div></div><div id="39537241" class="c"><input type="checkbox" id="c-39537241" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39540944">prev</a><span>|</span><a href="#39536845">next</a><span>|</span><label class="collapse" for="c-39537241">[-]</label><label class="expand" for="c-39537241">[9 more]</label></div><br/><div class="children"><div class="content">Powers of 3 don&#x27;t pack well into binary memory...<p>A 1 bit multiplier in silicon is a single logic gate, but a ternary decoder to decode a packed tri-state &#x27;weight&#x27; is bigger.<p>I therefore suspect that this method will be extended to make all weights simple 1 or 0 (ie. Binary).   Perhaps that will be done by having half the weights have 1 or 0 values, while the other half are -1 or 0.</div><br/><div id="39537436" class="c"><input type="checkbox" id="c-39537436" checked=""/><div class="controls bullet"><span class="by">tromp</span><span>|</span><a href="#39537241">parent</a><span>|</span><a href="#39537371">next</a><span>|</span><label class="collapse" for="c-39537436">[-]</label><label class="expand" for="c-39537436">[3 more]</label></div><br/><div class="children"><div class="content">5 trits fit into 1 byte pretty well, since 3^5 = 243 is just under 2^8 = 256.<p>That should be called an 8&#x2F;5 = 1.6 bit model though, while the paper names it 1.58 bit, closer to log_2(3) ~ 1.5849625</div><br/><div id="39538208" class="c"><input type="checkbox" id="c-39538208" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39537241">root</a><span>|</span><a href="#39537436">parent</a><span>|</span><a href="#39537689">next</a><span>|</span><label class="collapse" for="c-39538208">[-]</label><label class="expand" for="c-39538208">[1 more]</label></div><br/><div class="children"><div class="content">But the decoder for that will be 25+ gates, which is huge compared to the handful of gates to use the resulting weights.</div><br/></div></div><div id="39537689" class="c"><input type="checkbox" id="c-39537689" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#39537241">root</a><span>|</span><a href="#39537436">parent</a><span>|</span><a href="#39538208">prev</a><span>|</span><a href="#39537371">next</a><span>|</span><label class="collapse" for="c-39537689">[-]</label><label class="expand" for="c-39537689">[1 more]</label></div><br/><div class="children"><div class="content">Would be nice to have hardware instructions that work on 5 tris natively.</div><br/></div></div></div></div><div id="39537371" class="c"><input type="checkbox" id="c-39537371" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39537241">parent</a><span>|</span><a href="#39537436">prev</a><span>|</span><a href="#39545683">next</a><span>|</span><label class="collapse" for="c-39537371">[-]</label><label class="expand" for="c-39537371">[2 more]</label></div><br/><div class="children"><div class="content">You can build dedicated silicon with ternary gates: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@rxseger&#x2F;exploring-ternary-logic-tnand-and-tand-gates-a1ed9f7e6dab" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@rxseger&#x2F;exploring-ternary-logic-tnand-an...</a><p>Not sure if it&#x27;s more efficient than just binary digital circuits in highly integrated chip, though.</div><br/><div id="39538371" class="c"><input type="checkbox" id="c-39538371" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#39537241">root</a><span>|</span><a href="#39537371">parent</a><span>|</span><a href="#39545683">next</a><span>|</span><label class="collapse" for="c-39538371">[-]</label><label class="expand" for="c-39538371">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s optimal if your program is naturally ternary, which this one is. Using three signals, rather than ternary gates, is less effective, because you need much more precision to detect two different voltage levels rather than just up and down.</div><br/></div></div></div></div><div id="39545683" class="c"><input type="checkbox" id="c-39545683" checked=""/><div class="controls bullet"><span class="by">fasa99</span><span>|</span><a href="#39537241">parent</a><span>|</span><a href="#39537371">prev</a><span>|</span><a href="#39543988">next</a><span>|</span><label class="collapse" for="c-39545683">[-]</label><label class="expand" for="c-39545683">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s the right chain of thought.  You could either have 0&#x2F;1 and then have additional nodes with negative activation functions, or -1&#x2F;1<p>-1&#x2F;1 is appealing to me (0 = -1) because bit hackery could be used instead of the multiplication function, presumably on integral or fixed-point representations.  The goal would be to eliminate any &quot;if&#x2F;then&quot; like &quot;if 0 do this if 1 do that&quot; to avoid the need for branch prediction - there are bit-hackery ways to bypass this.  That would lend itself well to all existing processors, ASICs, FPGAs, GPUs, etc.</div><br/></div></div><div id="39543988" class="c"><input type="checkbox" id="c-39543988" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#39537241">parent</a><span>|</span><a href="#39545683">prev</a><span>|</span><a href="#39541035">next</a><span>|</span><label class="collapse" for="c-39543988">[-]</label><label class="expand" for="c-39543988">[1 more]</label></div><br/><div class="children"><div class="content">can&#x27;t you have 2 bits ? first bit for the sign second bit for the 1 0 
you can represent -1 +1 +0 -0</div><br/></div></div></div></div><div id="39536845" class="c"><input type="checkbox" id="c-39536845" checked=""/><div class="controls bullet"><span class="by">dindobre</span><span>|</span><a href="#39537241">prev</a><span>|</span><a href="#39546271">next</a><span>|</span><label class="collapse" for="c-39536845">[-]</label><label class="expand" for="c-39536845">[4 more]</label></div><br/><div class="children"><div class="content">Refreshing paper in terms of machine learning papers, simple explanation, easy to replicate, no alchemy-tier interpretations. Can&#x27;t wait to see this paper replicated or disproved when it comes to real-life production tasks.</div><br/><div id="39536961" class="c"><input type="checkbox" id="c-39536961" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39536845">parent</a><span>|</span><a href="#39536875">next</a><span>|</span><label class="collapse" for="c-39536961">[-]</label><label class="expand" for="c-39536961">[1 more]</label></div><br/><div class="children"><div class="content">The most glaring omission is that they only compared to fp16 models, not to quantized models. And of course the benchmarks might be misleading compared to the real experience.<p>But if you wanted to make LLM-specific hardware (or x64 instructions tuned for LLMs) this model architecture makes that extremely cheap. Multiplication requires a lot of transistors, this architecture requires only two-bit adders. You could make SIMD instructions that do thousands of these in parallel, for fairly little silicon cost.</div><br/></div></div><div id="39536875" class="c"><input type="checkbox" id="c-39536875" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#39536845">parent</a><span>|</span><a href="#39536961">prev</a><span>|</span><a href="#39546271">next</a><span>|</span><label class="collapse" for="c-39536875">[-]</label><label class="expand" for="c-39536875">[2 more]</label></div><br/><div class="children"><div class="content">The presentation is simplified because it implies knowledge of its predeccesor, BitNet <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.11453" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.11453</a></div><br/><div id="39536902" class="c"><input type="checkbox" id="c-39536902" checked=""/><div class="controls bullet"><span class="by">dindobre</span><span>|</span><a href="#39536845">root</a><span>|</span><a href="#39536875">parent</a><span>|</span><a href="#39546271">next</a><span>|</span><label class="collapse" for="c-39536902">[-]</label><label class="expand" for="c-39536902">[1 more]</label></div><br/><div class="children"><div class="content">Makes sense!</div><br/></div></div></div></div></div></div><div id="39546271" class="c"><input type="checkbox" id="c-39546271" checked=""/><div class="controls bullet"><span class="by">smaddox</span><span>|</span><a href="#39536845">prev</a><span>|</span><a href="#39537552">next</a><span>|</span><label class="collapse" for="c-39546271">[-]</label><label class="expand" for="c-39546271">[1 more]</label></div><br/><div class="children"><div class="content">Damn. Well, I guess I better hurry up and write and publish a paper on the Ternary Neural Network research that I&#x27;ve been doing (part-time) for the last several months, before it all gets scooped.</div><br/></div></div><div id="39537552" class="c"><input type="checkbox" id="c-39537552" checked=""/><div class="controls bullet"><span class="by">stormfather</span><span>|</span><a href="#39546271">prev</a><span>|</span><a href="#39546900">next</a><span>|</span><label class="collapse" for="c-39537552">[-]</label><label class="expand" for="c-39537552">[4 more]</label></div><br/><div class="children"><div class="content">How does backprop work here? I can&#x27;t imagine flipping bits of everything upstream of an error is effective.</div><br/><div id="39538702" class="c"><input type="checkbox" id="c-39538702" checked=""/><div class="controls bullet"><span class="by">spyder</span><span>|</span><a href="#39537552">parent</a><span>|</span><a href="#39537619">next</a><span>|</span><label class="collapse" for="c-39538702">[-]</label><label class="expand" for="c-39538702">[1 more]</label></div><br/><div class="children"><div class="content">From the BitNet paper:<p><i>&quot;Straight-through estimator. To train our 1-bit model, we employ the straight-through estimator
(STE)[BLC13] to approximate the gradient during backpropagation. This method bypasses the nondifferentiable functions, such as the Sign (Eq. 2) and Clip (Eq. 5) functions, during the backward pass.
STE allows gradients to flow through the network without being affected by these non-differentiable
functions, making it possible to train our quantized model.&quot;</i><p>also the author&#x27;s (@shumingma) answer in the comments:
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2402.17764#65df17ed4d436404cdc7b34a" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2402.17764#65df17ed4d436404cdc...</a></div><br/></div></div><div id="39537619" class="c"><input type="checkbox" id="c-39537619" checked=""/><div class="controls bullet"><span class="by">joelthelion</span><span>|</span><a href="#39537552">parent</a><span>|</span><a href="#39538702">prev</a><span>|</span><a href="#39546900">next</a><span>|</span><label class="collapse" for="c-39537619">[-]</label><label class="expand" for="c-39537619">[2 more]</label></div><br/><div class="children"><div class="content">(haven&#x27;t read the paper). Maybe you can flip bits with a probability distribution that depends on the gradient?</div><br/><div id="39537686" class="c"><input type="checkbox" id="c-39537686" checked=""/><div class="controls bullet"><span class="by">stormfather</span><span>|</span><a href="#39537552">root</a><span>|</span><a href="#39537619">parent</a><span>|</span><a href="#39546900">next</a><span>|</span><label class="collapse" for="c-39537686">[-]</label><label class="expand" for="c-39537686">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an interesting idea! Would love to try that on MNIST one day.</div><br/></div></div></div></div></div></div><div id="39546900" class="c"><input type="checkbox" id="c-39546900" checked=""/><div class="controls bullet"><span class="by">lavp</span><span>|</span><a href="#39537552">prev</a><span>|</span><a href="#39546441">next</a><span>|</span><label class="collapse" for="c-39546900">[-]</label><label class="expand" for="c-39546900">[1 more]</label></div><br/><div class="children"><div class="content">What does “perform slightly better than Llama” mean exactly? A model like this needs to be trained from scratch right?</div><br/></div></div><div id="39546441" class="c"><input type="checkbox" id="c-39546441" checked=""/><div class="controls bullet"><span class="by">elijahbenizzy</span><span>|</span><a href="#39546900">prev</a><span>|</span><a href="#39538370">next</a><span>|</span><label class="collapse" for="c-39546441">[-]</label><label class="expand" for="c-39546441">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an interesting mental model I&#x27;ve been toying with. At what point do LLMs just become circuit-shaped NNs with stochastic gradient descent backing them?<p>E.G. are we just determining the best program by rearranging 1s and 0s?</div><br/></div></div><div id="39538370" class="c"><input type="checkbox" id="c-39538370" checked=""/><div class="controls bullet"><span class="by">nutate</span><span>|</span><a href="#39546441">prev</a><span>|</span><a href="#39537680">next</a><span>|</span><label class="collapse" for="c-39538370">[-]</label><label class="expand" for="c-39538370">[1 more]</label></div><br/><div class="children"><div class="content">Triggered by the use of 1-bit to describe a trit.</div><br/></div></div><div id="39537680" class="c"><input type="checkbox" id="c-39537680" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#39538370">prev</a><span>|</span><a href="#39546036">next</a><span>|</span><label class="collapse" for="c-39537680">[-]</label><label class="expand" for="c-39537680">[1 more]</label></div><br/><div class="children"><div class="content">Interesting return to ternary. Effectively, each weight says only whether it&#x27;s correlated (+1), uncorrelated (0), or anti-correlated (-1) with the input, and the structure of the network is the actual computation over that information.</div><br/></div></div><div id="39546036" class="c"><input type="checkbox" id="c-39546036" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#39537680">prev</a><span>|</span><a href="#39537168">next</a><span>|</span><label class="collapse" for="c-39546036">[-]</label><label class="expand" for="c-39546036">[1 more]</label></div><br/><div class="children"><div class="content">Well, that&#x27;s 2 bits, but still...<p>LLMs have gone from 32-bit floating point numbers down to 16 and 8 bit values. Now 2 bits. It&#x27;s a hint as to how evolution did it. The basic component is simple and has very wide tolerances. There are just a lot of them. That&#x27;s something biology can evolve.</div><br/></div></div><div id="39537168" class="c"><input type="checkbox" id="c-39537168" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#39546036">prev</a><span>|</span><a href="#39544478">next</a><span>|</span><label class="collapse" for="c-39537168">[-]</label><label class="expand" for="c-39537168">[1 more]</label></div><br/><div class="children"><div class="content">1-bit LLMs remind me of a random forum post I read about SACD and limitations of the 1-bit DSD audio format. <a href="https:&#x2F;&#x2F;www.audiosciencereview.com&#x2F;forum&#x2F;index.php?threads&#x2F;dac-types-and-their-sonic-signature.7959&#x2F;page-10#post-198394" rel="nofollow">https:&#x2F;&#x2F;www.audiosciencereview.com&#x2F;forum&#x2F;index.php?threads&#x2F;d...</a> Accumulating approximate values in one bit leads to being &quot;constantly overloaded&quot;, with any error correction overwriting all of your real signal from the next step. I think this trinary system might leave enough room to avoid this problem.</div><br/></div></div><div id="39544478" class="c"><input type="checkbox" id="c-39544478" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#39537168">prev</a><span>|</span><a href="#39545954">next</a><span>|</span><label class="collapse" for="c-39544478">[-]</label><label class="expand" for="c-39544478">[3 more]</label></div><br/><div class="children"><div class="content">Is it really so surprising that something like this works given how human brain neurons work? My admittedly basic understanding is that these operate through an all-or-nothing principle for their action potentials (firing): they either fire or they don&#x27;t, based on whether the input signals reach a certain threshold. So the output is already sort of binary in biological neurons. The inputs are more like continuous values, since they are the sum of many different neurons sending signals into each neuron, but in this paper the activations are 8-bit, not binary&#x2F;ternary. Can any neuroscientists here comment?</div><br/><div id="39545562" class="c"><input type="checkbox" id="c-39545562" checked=""/><div class="controls bullet"><span class="by">fasa99</span><span>|</span><a href="#39544478">parent</a><span>|</span><a href="#39544544">next</a><span>|</span><label class="collapse" for="c-39545562">[-]</label><label class="expand" for="c-39545562">[1 more]</label></div><br/><div class="children"><div class="content">Well I think it&#x27;s an interesting idea, and to add to that, the &quot;-1&quot; values would correspond to an inhibitory neuron!<p>What neurons can do though is integrate over time, so your output can be one spike, or 3 spikes very quick, same for your input, and maybe 10 quick spikes in a row is a more powerful signal than a lone spike.  We know this intuitively, though, via vision, we don&#x27;t see in mac-classic style black&#x2F;white images, we see shades of brightness and color, indicating that at least our optic nerve is sending what amounts to an analog signal (even if encoded as binary spikes - is the spike timing not analog?)<p>This is not to mention all the biochemical signaling that happens, and the multitude of local neurotransmitters and global physiological&#x2F;hormonal factors at play.  And all that weird stuff like glial cells and astrocytes is there in the mix too.</div><br/></div></div><div id="39544544" class="c"><input type="checkbox" id="c-39544544" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#39544478">parent</a><span>|</span><a href="#39545562">prev</a><span>|</span><a href="#39545954">next</a><span>|</span><label class="collapse" for="c-39544544">[-]</label><label class="expand" for="c-39544544">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t really how neurons work.<p>First of all, they operate independent of a synchronized clock, and they can also accumulate signals instead of executing on a input. Neuromorphic chips are closer to how the brain works, but they&#x27;re still super early. I believe Intel has the best one with the Loihi 2.<p>(Not a neuroscientist but my wife is and that&#x27;s what I understand from our chats)</div><br/></div></div></div></div><div id="39545954" class="c"><input type="checkbox" id="c-39545954" checked=""/><div class="controls bullet"><span class="by">ryeguy_24</span><span>|</span><a href="#39544478">prev</a><span>|</span><a href="#39541182">next</a><span>|</span><label class="collapse" for="c-39545954">[-]</label><label class="expand" for="c-39545954">[2 more]</label></div><br/><div class="children"><div class="content">How does gradient descent work with these discrete ternary parameters?  If you compute the partial differential for a parameter, how do you determine what to nudge the parameter when updating on back propagation?  Do you only update if the &quot;nudging amount&quot; meets a threshold?</div><br/><div id="39546741" class="c"><input type="checkbox" id="c-39546741" checked=""/><div class="controls bullet"><span class="by">edflsafoiewq</span><span>|</span><a href="#39545954">parent</a><span>|</span><a href="#39541182">next</a><span>|</span><label class="collapse" for="c-39546741">[-]</label><label class="expand" for="c-39546741">[1 more]</label></div><br/><div class="children"><div class="content">&gt; While the weights and the activations are quantized to low precision, the gradients and the optimizer states are stored in high precision to ensure training stability and
accuracy. Following the previous work [ LSL+21 ], we maintain a latent weight in a high-precision format for the learnable parameters to accumulate the parameter updates. The latent weights are binarized on the fly during the forward pass and never used for the inference process.</div><br/></div></div></div></div><div id="39541182" class="c"><input type="checkbox" id="c-39541182" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39545954">prev</a><span>|</span><a href="#39537020">next</a><span>|</span><label class="collapse" for="c-39541182">[-]</label><label class="expand" for="c-39541182">[3 more]</label></div><br/><div class="children"><div class="content">How is it a 1 bit LLM if 2 bits are required for each weight (and one of the 4 possible states is wasted to be able to represent 0)</div><br/><div id="39543554" class="c"><input type="checkbox" id="c-39543554" checked=""/><div class="controls bullet"><span class="by">ricardobeat</span><span>|</span><a href="#39541182">parent</a><span>|</span><a href="#39537020">next</a><span>|</span><label class="collapse" for="c-39543554">[-]</label><label class="expand" for="c-39543554">[2 more]</label></div><br/><div class="children"><div class="content">As someone else pointed out here, you can store 5 ternary values in 1 byte, 3^5 == 243.</div><br/><div id="39544026" class="c"><input type="checkbox" id="c-39544026" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39541182">root</a><span>|</span><a href="#39543554">parent</a><span>|</span><a href="#39537020">next</a><span>|</span><label class="collapse" for="c-39544026">[-]</label><label class="expand" for="c-39544026">[1 more]</label></div><br/><div class="children"><div class="content">That’s still not 1 bit, and that would basically destroy whatever perf advantage you might hope to get if you want to keep the model in memory in that format rather than unpack it on load.</div><br/></div></div></div></div></div></div><div id="39537020" class="c"><input type="checkbox" id="c-39537020" checked=""/><div class="controls bullet"><span class="by">raghavtoshniwal</span><span>|</span><a href="#39541182">prev</a><span>|</span><label class="collapse" for="c-39537020">[-]</label><label class="expand" for="c-39537020">[13 more]</label></div><br/><div class="children"><div class="content">Sooo, short Nvidia?</div><br/><div id="39537054" class="c"><input type="checkbox" id="c-39537054" checked=""/><div class="controls bullet"><span class="by">MadDemon</span><span>|</span><a href="#39537020">parent</a><span>|</span><a href="#39537657">next</a><span>|</span><label class="collapse" for="c-39537054">[-]</label><label class="expand" for="c-39537054">[3 more]</label></div><br/><div class="children"><div class="content">Depends if this results in more efficient models or simply larger, more capable models.</div><br/><div id="39537310" class="c"><input type="checkbox" id="c-39537310" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537054">parent</a><span>|</span><a href="#39537657">next</a><span>|</span><label class="collapse" for="c-39537310">[-]</label><label class="expand" for="c-39537310">[2 more]</label></div><br/><div class="children"><div class="content">In both cases this is a prime opportunity for anyone to disrupt Nvidia. They are in this market position in large part because both video games and neural networks do a lot of highly parallel floating point math, especially matrix multiplication. This model architecture doesn&#x27;t do any of that.<p>Of course it should be fairly simple for Nvidia to add special silicon and instructions for two-bit addition to a future generation of their cards. But it&#x27;ll take a while because they already have a roadmap and preexisting commitments. And any competitor doesn&#x27;t have to copy everything Nvidia does to make floating point numbers go fast, they can just focus on making two-bit data handling and addition go fast.</div><br/><div id="39545016" class="c"><input type="checkbox" id="c-39545016" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537310">parent</a><span>|</span><a href="#39537657">next</a><span>|</span><label class="collapse" for="c-39545016">[-]</label><label class="expand" for="c-39545016">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but with their current market cap, the more likely result is they acquire one of the several competitors poised to take advantage of this and throw massive resources behind them.</div><br/></div></div></div></div></div></div><div id="39537657" class="c"><input type="checkbox" id="c-39537657" checked=""/><div class="controls bullet"><span class="by">etiam</span><span>|</span><a href="#39537020">parent</a><span>|</span><a href="#39537054">prev</a><span>|</span><a href="#39537085">next</a><span>|</span><label class="collapse" for="c-39537657">[-]</label><label class="expand" for="c-39537657">[1 more]</label></div><br/><div class="children"><div class="content">Hardly for this reason, but it does look suspiciously high doesn&#x27;t it.</div><br/></div></div><div id="39537085" class="c"><input type="checkbox" id="c-39537085" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#39537020">parent</a><span>|</span><a href="#39537657">prev</a><span>|</span><label class="collapse" for="c-39537085">[-]</label><label class="expand" for="c-39537085">[8 more]</label></div><br/><div class="children"><div class="content">These still run on GPUs</div><br/><div id="39537278" class="c"><input type="checkbox" id="c-39537278" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537085">parent</a><span>|</span><a href="#39537151">next</a><span>|</span><label class="collapse" for="c-39537278">[-]</label><label class="expand" for="c-39537278">[4 more]</label></div><br/><div class="children"><div class="content">GPU&#x27;s aren&#x27;t yet awfully efficient at 1 bit math.<p>I could imagine FPGA designs might be competitive.<p>And dedicated ASIC&#x27;s would almost certainly beat both by a decent margin.</div><br/><div id="39542918" class="c"><input type="checkbox" id="c-39542918" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537278">parent</a><span>|</span><a href="#39537456">next</a><span>|</span><label class="collapse" for="c-39542918">[-]</label><label class="expand" for="c-39542918">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it would be difficult to make them efficient.<p>The main reason why we run this stuff on GPUs is their memory bandwidth, anyway.</div><br/></div></div><div id="39537456" class="c"><input type="checkbox" id="c-39537456" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537278">parent</a><span>|</span><a href="#39542918">prev</a><span>|</span><a href="#39537151">next</a><span>|</span><label class="collapse" for="c-39537456">[-]</label><label class="expand" for="c-39537456">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m very unconvinced that ASICs are better suited for this than for FP16&#x2F;FP8 models that are being used today.</div><br/><div id="39538815" class="c"><input type="checkbox" id="c-39538815" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537456">parent</a><span>|</span><a href="#39537151">next</a><span>|</span><label class="collapse" for="c-39538815">[-]</label><label class="expand" for="c-39538815">[1 more]</label></div><br/><div class="children"><div class="content">BF16 is a pretty big unit in an ASIC - You need at least 9 * 5 gates to calculate the exponent of the result, a 10 bit barrel shifter (10*10 + 10*ceil(log2(10)) gates), and a 10 bit multiplier (approximately 10 * 10 * 9 gates)<p>Total = 1085 gates.   The reality is probably far more, because you&#x27;re going to want to use carry-look-ahead and pipelining.<p>Whereas 1 bit multiplies and add&#x27;s of say a 16 bit accumulator use...    16 gates!   (and probably half since you can probably use scheduling tricks to skip past the zero&#x27;s, at the expense of variable latency...)<p>So when 1 bit math uses only 1&#x2F;100th of the silicon area of 16 bit math, and according to this paper gets the same results, the future is clearly silicon that can do 1 bit math.</div><br/></div></div></div></div></div></div><div id="39537151" class="c"><input type="checkbox" id="c-39537151" checked=""/><div class="controls bullet"><span class="by">leroman</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537085">parent</a><span>|</span><a href="#39537278">prev</a><span>|</span><label class="collapse" for="c-39537151">[-]</label><label class="expand" for="c-39537151">[3 more]</label></div><br/><div class="children"><div class="content">- we have llama.cpp (could be enough or at least as mentioned in the paper a co-processor to accelerate the calc can be added, less need for large RAM &#x2F; high end hardware)<p>- as most work is inference, might not need for as many GPUs<p>- consumer cards (24G) could possibly run the big models</div><br/><div id="39537450" class="c"><input type="checkbox" id="c-39537450" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537151">parent</a><span>|</span><label class="collapse" for="c-39537450">[-]</label><label class="expand" for="c-39537450">[2 more]</label></div><br/><div class="children"><div class="content">If consumer cards can run the big models, then datacenter cards will be able to efficiently run the really big models.</div><br/><div id="39537781" class="c"><input type="checkbox" id="c-39537781" checked=""/><div class="controls bullet"><span class="by">leroman</span><span>|</span><a href="#39537020">root</a><span>|</span><a href="#39537450">parent</a><span>|</span><label class="collapse" for="c-39537781">[-]</label><label class="expand" for="c-39537781">[1 more]</label></div><br/><div class="children"><div class="content">Some tasks we are using LLMs for are performing very close to GPT-4 levels using 7B models, so really depends on what value you are looking to get.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>