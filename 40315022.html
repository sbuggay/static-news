<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715331711272" as="style"/><link rel="stylesheet" href="styles.css?v=1715331711272"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2405.00738">Energy-Efficient Llama 2 Inference on FPGAs via High Level Synthesis</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>PaulHoule</span> | <span>17 comments</span></div><br/><div><div id="40316496" class="c"><input type="checkbox" id="c-40316496" checked=""/><div class="controls bullet"><span class="by">kernelsanderz</span><span>|</span><a href="#40316098">next</a><span>|</span><label class="collapse" for="c-40316496">[-]</label><label class="expand" for="c-40316496">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s really cool, is that this was built on Karpathy&#x27;s llama2.c repo <a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llama2.c">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llama2.c</a><p>A great example of the unexpected things that happen when put great code into the commons.<p>I bet Andrej never expected anything like this when he released it.</div><br/></div></div><div id="40316098" class="c"><input type="checkbox" id="c-40316098" checked=""/><div class="controls bullet"><span class="by">jesprenj</span><span>|</span><a href="#40316496">prev</a><span>|</span><a href="#40316320">next</a><span>|</span><label class="collapse" for="c-40316098">[-]</label><label class="expand" for="c-40316098">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Although the GPU performs inference faster than the FPGA,
one of the primary bottlenecks of deep learning inference is
memory bandwidth and the availability of on-chip memory
(Balasubramanian et al., 2021). A RTX 3090 has 24GB
VRAM running at 1219 MHz with a base core clock of 1395
MHz (TechPowerUp, 2024). In comparison, a VU9P FPGA
has 345.9 MB of combined on-chip BRAM and URAM,
running at a much slower clock speed of around 200-300
MHz depending on the module; however, with much lower
clock speeds, the FPGA is able to achieve better efficiency
on power and energy consumption, as shown below.<p>So as far as I can understand, the biggest &quot;bottleneck&quot;&#x2F;limiting factor with using FPGAs for LLMs is the available memory -- with current large models exceeding 40 GiB in parameter size, GPUs and TPUs with DRAM look like the only way to go forward for the months to come ... Thoughts?</div><br/><div id="40316265" class="c"><input type="checkbox" id="c-40316265" checked=""/><div class="controls bullet"><span class="by">dailykoder</span><span>|</span><a href="#40316098">parent</a><span>|</span><a href="#40316330">next</a><span>|</span><label class="collapse" for="c-40316265">[-]</label><label class="expand" for="c-40316265">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t be surprised if AMD or Intel come up with an FPGA especially for this application. Atleast AMD advertises a lot with their AI FPGA stuff, so they&#x27;ll probably build one with either a lot more BRAM or the ability to attach some very fast RAM? But going from a few Megabytes of RAM to Gigabytes sounds very expensive. DRAM is just too slow, I guess</div><br/></div></div><div id="40316330" class="c"><input type="checkbox" id="c-40316330" checked=""/><div class="controls bullet"><span class="by">bnprks</span><span>|</span><a href="#40316098">parent</a><span>|</span><a href="#40316265">prev</a><span>|</span><a href="#40316320">next</a><span>|</span><label class="collapse" for="c-40316330">[-]</label><label class="expand" for="c-40316330">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I think DRAM is almost certainly the future, just in terms of being able to afford the memory capacity to fit large models. Even Cerebras using a full wafer only gets up to 44 GB of SRAM on a chip (at a cost over $2M).<p>An interesting twist is that this DRAM might not need to be a central pool where bandwidth must be shared globally -- e.g. the Tensortorrent strategy seems to be aiming for using smaller chips that each have their own memory. Splitting up memory should yield very high aggregate bandwidth even with slower DRAM, which is great as long as they can figure out the cross-chip data flow to avoid networking bottlenecks</div><br/></div></div></div></div><div id="40316320" class="c"><input type="checkbox" id="c-40316320" checked=""/><div class="controls bullet"><span class="by">cherioo</span><span>|</span><a href="#40316098">prev</a><span>|</span><a href="#40315961">next</a><span>|</span><label class="collapse" for="c-40316320">[-]</label><label class="expand" for="c-40316320">[2 more]</label></div><br/><div class="children"><div class="content">While FPGA may prove more efficient than 3090, a primarily gaming card, I can’t see how it should be more efficient than dedicated training&#x2F;inference card, as the latter are more effectively ASIC, not to mention memory and bandwidth limitations.<p>Is there something I am missing making FPGA potentially more viable, besides not feeding into NVIDIA’s greed?</div><br/><div id="40316508" class="c"><input type="checkbox" id="c-40316508" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#40316320">parent</a><span>|</span><a href="#40315961">next</a><span>|</span><label class="collapse" for="c-40316508">[-]</label><label class="expand" for="c-40316508">[1 more]</label></div><br/><div class="children"><div class="content">A dedicated training&#x2F;inference card is still more general than an LLama 2 inference card.
It&#x27;s obvious that you will get better efficiency the more you tailor your silicon to the task, with diminishing gains but still.</div><br/></div></div></div></div><div id="40315961" class="c"><input type="checkbox" id="c-40315961" checked=""/><div class="controls bullet"><span class="by">bnprks</span><span>|</span><a href="#40316320">prev</a><span>|</span><a href="#40315825">next</a><span>|</span><label class="collapse" for="c-40315961">[-]</label><label class="expand" for="c-40315961">[1 more]</label></div><br/><div class="children"><div class="content">Seems like the claims of the abstract for speed and energy-efficiency relative to an RTX 3090 are when the GPU is using a batch size of 1. I wonder if someone with more experience can comment on how much throughput gain is possible on a GPU by increasing batch size without severely harming latency (and what the power consumption change might be).<p>And from a hardware cost perspective the AWS f1.2xlarge instances they used are $1.65&#x2F;hr on-demand, vs say $1.29&#x2F;hr for an A100 from Lambda Labs. A very interesting line of thinking to use FPGAs, but I&#x27;m not sure if this is really describing a viable competitor to GPUs even for inference-only scenarios.</div><br/></div></div><div id="40315825" class="c"><input type="checkbox" id="c-40315825" checked=""/><div class="controls bullet"><span class="by">emsal</span><span>|</span><a href="#40315961">prev</a><span>|</span><a href="#40316593">next</a><span>|</span><label class="collapse" for="c-40315825">[-]</label><label class="expand" for="c-40315825">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps this is the obvious comment, but I really hope that something that employs technology like this can get off the ground and become a services company. The ideas about democratizing the AI inference hardware space and making it energy efficient really resonate with me.</div><br/><div id="40316381" class="c"><input type="checkbox" id="c-40316381" checked=""/><div class="controls bullet"><span class="by">om8</span><span>|</span><a href="#40315825">parent</a><span>|</span><a href="#40316593">next</a><span>|</span><label class="collapse" for="c-40316381">[-]</label><label class="expand" for="c-40316381">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t you think that LLM inference is already very democratic? Not saying that there is no room for improvements there -- there is still a lot to do in the space of speculative decoding, quantization and other stuff. I&#x27;m saying that every 16 year old with a decent enough personal computer can run fully locally latest open weights model like Llama3-8b that beats almost everything we had a year ago.<p>The part of this ecosystem that is as non-democratized as it can be is training. It&#x27;s currently impossible to train decent enough model with resources that are available to one person.</div><br/></div></div></div></div><div id="40316593" class="c"><input type="checkbox" id="c-40316593" checked=""/><div class="controls bullet"><span class="by">hongspike</span><span>|</span><a href="#40315825">prev</a><span>|</span><a href="#40315893">next</a><span>|</span><label class="collapse" for="c-40316593">[-]</label><label class="expand" for="c-40316593">[1 more]</label></div><br/><div class="children"><div class="content">Trying to understand what cases we would want to use FPGAs rather than GPUs.<p>Memory bandwidth for FPGAs seems worse, so for serving models don&#x27;t GPUs still win out?</div><br/></div></div><div id="40315893" class="c"><input type="checkbox" id="c-40315893" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#40316593">prev</a><span>|</span><a href="#40316056">next</a><span>|</span><label class="collapse" for="c-40315893">[-]</label><label class="expand" for="c-40315893">[3 more]</label></div><br/><div class="children"><div class="content">am i missing something? since when does vitis connect to vivado and do the p&amp;r too?<p>anyway if you&#x27;re tempted by this, i strongly advise you to ponder this:<p>&gt; Run the Hardware build, should take around ~12 hours.</div><br/><div id="40316000" class="c"><input type="checkbox" id="c-40316000" checked=""/><div class="controls bullet"><span class="by">dailykoder</span><span>|</span><a href="#40315893">parent</a><span>|</span><a href="#40316056">next</a><span>|</span><label class="collapse" for="c-40316000">[-]</label><label class="expand" for="c-40316000">[2 more]</label></div><br/><div class="children"><div class="content">Welcome to the world of FPGA synthesis :-)<p>&gt; am i missing something? since when does vitis connect to vivado and do the p&amp;r too?<p>I haven&#x27;t done much HLS, but isn&#x27;t that the normal case? Translating the HLS into HDL and then do the pnr with vivado?</div><br/><div id="40316044" class="c"><input type="checkbox" id="c-40316044" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#40315893">root</a><span>|</span><a href="#40316000">parent</a><span>|</span><a href="#40316056">next</a><span>|</span><label class="collapse" for="c-40316044">[-]</label><label class="expand" for="c-40316044">[1 more]</label></div><br/><div class="children"><div class="content">i&#x27;ve done plenty of FPGA and HLS as well<p>&gt; Translating the HLS into HDL and then do the pnr with vivado?<p>As far as I know, vitis does not give you tcl scripts (or whatever) for vivado - you have to do that yourself.</div><br/></div></div></div></div></div></div><div id="40316056" class="c"><input type="checkbox" id="c-40316056" checked=""/><div class="controls bullet"><span class="by">sandGorgon</span><span>|</span><a href="#40315893">prev</a><span>|</span><label class="collapse" for="c-40316056">[-]</label><label class="expand" for="c-40316056">[3 more]</label></div><br/><div class="children"><div class="content">if i want to play around with this at home on a fpga devkit...which fpga kit should i use ?<p>the Xilinx Virtex UltraScale+ VU9P FPGA prototyping boards seem to be 9000 USD. Anything in the 1000$ range ?</div><br/><div id="40316106" class="c"><input type="checkbox" id="c-40316106" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#40316056">parent</a><span>|</span><a href="#40316118">next</a><span>|</span><label class="collapse" for="c-40316106">[-]</label><label class="expand" for="c-40316106">[1 more]</label></div><br/><div class="children"><div class="content">ebay has a bunch of Alveo U30 cards for ~500. 500k luts, 3000 dsp slices, ~1M registers, even some DDR.<p>fair warning: one does not &quot;play around&quot; with an FPGA. they are the antithesis of user friendly.</div><br/></div></div><div id="40316118" class="c"><input type="checkbox" id="c-40316118" checked=""/><div class="controls bullet"><span class="by">BonusPlay</span><span>|</span><a href="#40316056">parent</a><span>|</span><a href="#40316106">prev</a><span>|</span><label class="collapse" for="c-40316118">[-]</label><label class="expand" for="c-40316118">[1 more]</label></div><br/><div class="children"><div class="content">Note, that paper provides environment for AWS FPGAs, which you can rent on per-hour basis.<p>As for cheaper FPGAs, the paper notes that the bottleneck is the size of on-chip memory. So I doubt it will be easy to find cheaper model to reduce costs.<p>Another hidden fee would be Vivado and Vitis (tooling) licenses, which you need for most upper-end FPGAs.</div><br/></div></div></div></div></div></div></div></div></div></body></html>