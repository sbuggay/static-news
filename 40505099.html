<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716973278411" as="style"/><link rel="stylesheet" href="styles.css?v=1716973278411"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee">Llama 3-V: Matching GPT4-V with a 100x smaller model and 500 dollars</a> <span class="domain">(<a href="https://aksh-garg.medium.com">aksh-garg.medium.com</a>)</span></div><div class="subtext"><span>minimaxir</span> | <span>55 comments</span></div><br/><div><div id="40507894" class="c"><input type="checkbox" id="c-40507894" checked=""/><div class="controls bullet"><span class="by">dcreater</span><span>|</span><a href="#40505978">next</a><span>|</span><label class="collapse" for="c-40507894">[-]</label><label class="expand" for="c-40507894">[3 more]</label></div><br/><div class="children"><div class="content">If I had a nickel for every outrageous &quot;matches&#x2F;beats GPT-x&quot; claim, I&#x27;d have more money than the capital these projects raise from VC.<p>This absolutely is not the first Llama3 vision model. They even quote it&#x27;s performance compared to Llava. Hard to take anything they say seriously with such obviously false claims</div><br/><div id="40508834" class="c"><input type="checkbox" id="c-40508834" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#40507894">parent</a><span>|</span><a href="#40509120">next</a><span>|</span><label class="collapse" for="c-40508834">[-]</label><label class="expand" for="c-40508834">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This absolutely is not the first Llama3 vision model. They even quote it&#x27;s performance compared to Llava.<p>Although this is true, there have been earlier Llama3 based vision releases, none of the latest Llava releases are Llama3 based.</div><br/></div></div><div id="40509120" class="c"><input type="checkbox" id="c-40509120" checked=""/><div class="controls bullet"><span class="by">vixen99</span><span>|</span><a href="#40507894">parent</a><span>|</span><a href="#40508834">prev</a><span>|</span><a href="#40505978">next</a><span>|</span><label class="collapse" for="c-40509120">[-]</label><label class="expand" for="c-40509120">[1 more]</label></div><br/><div class="children"><div class="content">All models surely write &#x27;its performance&#x27;.</div><br/></div></div></div></div><div id="40505978" class="c"><input type="checkbox" id="c-40505978" checked=""/><div class="controls bullet"><span class="by">lanceflt</span><span>|</span><a href="#40507894">prev</a><span>|</span><a href="#40506565">next</a><span>|</span><label class="collapse" for="c-40505978">[-]</label><label class="expand" for="c-40505978">[6 more]</label></div><br/><div class="children"><div class="content">- Llava is not the SOTA open VLM, InternVL-1.5 is
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;opencompass&#x2F;open_vlm_leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;opencompass&#x2F;open_vlm_leaderboa...</a><p>You need to compare the evals to strong open VLMs including this and CogVLM<p>- This is not &quot;first-ever multimodal model built on top of Llama3&quot;, there&#x27;s already a Llava on Llama3-8b
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;lmms-lab" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;lmms-lab</a></div><br/><div id="40506424" class="c"><input type="checkbox" id="c-40506424" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#40505978">parent</a><span>|</span><a href="#40507809">next</a><span>|</span><label class="collapse" for="c-40506424">[-]</label><label class="expand" for="c-40506424">[2 more]</label></div><br/><div class="children"><div class="content">Very curious how it performs on OCR tasks compared to InternVL. To be competitive at reading text you need tiling support, and InternVL does tiles exceptionally well.</div><br/><div id="40509330" class="c"><input type="checkbox" id="c-40509330" checked=""/><div class="controls bullet"><span class="by">hovering_nox</span><span>|</span><a href="#40505978">root</a><span>|</span><a href="#40506424">parent</a><span>|</span><a href="#40507809">next</a><span>|</span><label class="collapse" for="c-40509330">[-]</label><label class="expand" for="c-40509330">[1 more]</label></div><br/><div class="children"><div class="content">I think CogVLM2 is even better than Intern at OCR (my usecase is extracting information from an invoice)</div><br/></div></div></div></div><div id="40507809" class="c"><input type="checkbox" id="c-40507809" checked=""/><div class="controls bullet"><span class="by">abrichr</span><span>|</span><a href="#40505978">parent</a><span>|</span><a href="#40506424">prev</a><span>|</span><a href="#40506102">next</a><span>|</span><label class="collapse" for="c-40507809">[-]</label><label class="expand" for="c-40507809">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the link! Our initial testing suggests MiniCPM outperforms InternVL for GUI understanding: <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;issues&#x2F;637#issuecomment-2136403745">https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;issues&#x2F;637#issuecom...</a><p>(InternVL appears to hallucinate more.)</div><br/></div></div><div id="40506102" class="c"><input type="checkbox" id="c-40506102" checked=""/><div class="controls bullet"><span class="by">gigel82</span><span>|</span><a href="#40505978">parent</a><span>|</span><a href="#40507809">prev</a><span>|</span><a href="#40506847">next</a><span>|</span><label class="collapse" for="c-40506102">[-]</label><label class="expand" for="c-40506102">[1 more]</label></div><br/><div class="children"><div class="content">Like InternVL, no llama.cpp support severely limits its applications. Close to GPT4v performance level and runnable locally on any machine (no need for a GPU) would be huge for the accessibility community.</div><br/></div></div><div id="40506847" class="c"><input type="checkbox" id="c-40506847" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#40505978">parent</a><span>|</span><a href="#40506102">prev</a><span>|</span><a href="#40506565">next</a><span>|</span><label class="collapse" for="c-40506847">[-]</label><label class="expand" for="c-40506847">[1 more]</label></div><br/><div class="children"><div class="content">I’m going to be saying First Ever AI something for the next 15 years for clout and capital, not going to be listening to anybody’s complicated ten step funnel if they’re not doing the obvious</div><br/></div></div></div></div><div id="40506565" class="c"><input type="checkbox" id="c-40506565" checked=""/><div class="controls bullet"><span class="by">arnaudsm</span><span>|</span><a href="#40505978">prev</a><span>|</span><a href="#40506193">next</a><span>|</span><label class="collapse" for="c-40506565">[-]</label><label class="expand" for="c-40506565">[16 more]</label></div><br/><div class="children"><div class="content">Tangential question : did anyone ever use GPT4-V in production in visual tasks? It&#x27;s never consistent enough for me to be useful</div><br/><div id="40507766" class="c"><input type="checkbox" id="c-40507766" checked=""/><div class="controls bullet"><span class="by">abrichr</span><span>|</span><a href="#40506565">parent</a><span>|</span><a href="#40509166">next</a><span>|</span><label class="collapse" for="c-40507766">[-]</label><label class="expand" for="c-40507766">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very reliable for GUI segment understanding; see e.g. <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;pull&#x2F;610">https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;pull&#x2F;610</a> (scroll down to `gpt-4-vision-preview`).</div><br/></div></div><div id="40509166" class="c"><input type="checkbox" id="c-40509166" checked=""/><div class="controls bullet"><span class="by">alexcnwy</span><span>|</span><a href="#40506565">parent</a><span>|</span><a href="#40507766">prev</a><span>|</span><a href="#40507189">next</a><span>|</span><label class="collapse" for="c-40509166">[-]</label><label class="expand" for="c-40509166">[3 more]</label></div><br/><div class="children"><div class="content">the coolest use case i&#x27;ve seen is this <a href="https:&#x2F;&#x2F;github.com&#x2F;ddupont808&#x2F;GPT-4V-Act">https:&#x2F;&#x2F;github.com&#x2F;ddupont808&#x2F;GPT-4V-Act</a></div><br/><div id="40509319" class="c"><input type="checkbox" id="c-40509319" checked=""/><div class="controls bullet"><span class="by">nucleative</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40509166">parent</a><span>|</span><a href="#40507189">next</a><span>|</span><label class="collapse" for="c-40509319">[-]</label><label class="expand" for="c-40509319">[2 more]</label></div><br/><div class="children"><div class="content">I feel like this is the beginning of the end for all captchas</div><br/><div id="40509525" class="c"><input type="checkbox" id="c-40509525" checked=""/><div class="controls bullet"><span class="by">udev4096</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40509319">parent</a><span>|</span><a href="#40507189">next</a><span>|</span><label class="collapse" for="c-40509525">[-]</label><label class="expand" for="c-40509525">[1 more]</label></div><br/><div class="children"><div class="content">Image based or any kind of visual captchas will never be extremely effective. I think we will see more of PoW captchas in the upcoming years (just like cloudflare&#x27;s turnstile captcha)</div><br/></div></div></div></div></div></div><div id="40507189" class="c"><input type="checkbox" id="c-40507189" checked=""/><div class="controls bullet"><span class="by">serjester</span><span>|</span><a href="#40506565">parent</a><span>|</span><a href="#40509166">prev</a><span>|</span><a href="#40507044">next</a><span>|</span><label class="collapse" for="c-40507189">[-]</label><label class="expand" for="c-40507189">[8 more]</label></div><br/><div class="children"><div class="content">Don’t use it for anything OCR related that needs perfect accuracy. Stuff where some errors are ok, we’ve had great success. Depending on your budget, you can also run it multiple times to catch errors.</div><br/><div id="40507483" class="c"><input type="checkbox" id="c-40507483" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40507189">parent</a><span>|</span><a href="#40507321">next</a><span>|</span><label class="collapse" for="c-40507483">[-]</label><label class="expand" for="c-40507483">[1 more]</label></div><br/><div class="children"><div class="content">&gt; you can also run it multiple times to catch errors.<p>Does this require a slight offset and&#x2F;or rotation to the image, or just literal rerun, with seed seed&#x2F;whatever giving a different result?</div><br/></div></div><div id="40507321" class="c"><input type="checkbox" id="c-40507321" checked=""/><div class="controls bullet"><span class="by">toomuchtodo</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40507189">parent</a><span>|</span><a href="#40507483">prev</a><span>|</span><a href="#40507044">next</a><span>|</span><label class="collapse" for="c-40507321">[-]</label><label class="expand" for="c-40507321">[6 more]</label></div><br/><div class="children"><div class="content">How does it compare to Tesseract?<p>Edit: Thank you!</div><br/><div id="40507438" class="c"><input type="checkbox" id="c-40507438" checked=""/><div class="controls bullet"><span class="by">elanning</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40507321">parent</a><span>|</span><a href="#40507044">next</a><span>|</span><label class="collapse" for="c-40507438">[-]</label><label class="expand" for="c-40507438">[5 more]</label></div><br/><div class="children"><div class="content">I’ve done a lot of OCR work and tesseract is nearly a decade out of date at this point. It is not a serious technology for anything requiring good accuracy or minor complexity. From what I’ve seen, GPT-4V completely smokes tesseract, but then again, most modern OCR systems do. If you want fast and pretty powerful OCR, check out paddle. If you want slower but higher accuracy, check out transformer based models such as TrOCR.</div><br/><div id="40508179" class="c"><input type="checkbox" id="c-40508179" checked=""/><div class="controls bullet"><span class="by">nh2</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40507438">parent</a><span>|</span><a href="#40508283">next</a><span>|</span><label class="collapse" for="c-40508179">[-]</label><label class="expand" for="c-40508179">[1 more]</label></div><br/><div class="children"><div class="content">See this for a comparison of PaddleOCR, TrOCR, and various cloud ones (note: on documents of typed and handwritten text):<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32077375">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32077375</a></div><br/></div></div><div id="40508283" class="c"><input type="checkbox" id="c-40508283" checked=""/><div class="controls bullet"><span class="by">Zuiii</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40507438">parent</a><span>|</span><a href="#40508179">prev</a><span>|</span><a href="#40507044">next</a><span>|</span><label class="collapse" for="c-40508283">[-]</label><label class="expand" for="c-40508283">[3 more]</label></div><br/><div class="children"><div class="content">Tesseract&#x27;s true value is being one apt-get command away (i.e. opensource). Does Debian host more modern OCR systems in their repos?</div><br/><div id="40508687" class="c"><input type="checkbox" id="c-40508687" checked=""/><div class="controls bullet"><span class="by">nunez</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40508283">parent</a><span>|</span><a href="#40508382">next</a><span>|</span><label class="collapse" for="c-40508687">[-]</label><label class="expand" for="c-40508687">[1 more]</label></div><br/><div class="children"><div class="content">Tesseract the tool is one apt-get away but the trained models are not, and I&#x27;ve found that they are a starting point, not a final destination. You still have to do more training on top of them for anything that isn&#x27;t black text on a crisp white background.</div><br/></div></div><div id="40508382" class="c"><input type="checkbox" id="c-40508382" checked=""/><div class="controls bullet"><span class="by">elanning</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40508283">parent</a><span>|</span><a href="#40508687">prev</a><span>|</span><a href="#40507044">next</a><span>|</span><label class="collapse" for="c-40508382">[-]</label><label class="expand" for="c-40508382">[1 more]</label></div><br/><div class="children"><div class="content">Big mistake on my part; I should clarify I fine-tuned both PaddleOCR and TrOCR on large amounts of data specific to my domain. I cannot speak on the best out of the box “ready to go” solutions (besides cloud ones, which were quite good with the right pre and post processing).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40507044" class="c"><input type="checkbox" id="c-40507044" checked=""/><div class="controls bullet"><span class="by">zacmps</span><span>|</span><a href="#40506565">parent</a><span>|</span><a href="#40507189">prev</a><span>|</span><a href="#40506193">next</a><span>|</span><label class="collapse" for="c-40507044">[-]</label><label class="expand" for="c-40507044">[3 more]</label></div><br/><div class="children"><div class="content">Nope, I tried it for graph and diagram understanding and it wasn&#x27;t good enough. Planning to repeat the evaluation with 4o when I have time.</div><br/><div id="40507129" class="c"><input type="checkbox" id="c-40507129" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40507044">parent</a><span>|</span><a href="#40506193">next</a><span>|</span><label class="collapse" for="c-40507129">[-]</label><label class="expand" for="c-40507129">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using 4o to convert visual diagrams into mermaid, and it&#x27;s been almost perfectly accurate in my experience.</div><br/><div id="40508988" class="c"><input type="checkbox" id="c-40508988" checked=""/><div class="controls bullet"><span class="by">cuu508</span><span>|</span><a href="#40506565">root</a><span>|</span><a href="#40507129">parent</a><span>|</span><a href="#40506193">next</a><span>|</span><label class="collapse" for="c-40508988">[-]</label><label class="expand" for="c-40508988">[1 more]</label></div><br/><div class="children"><div class="content">This is the out of the box thinking I love about HN. What do you do with the mermaid?</div><br/></div></div></div></div></div></div></div></div><div id="40506193" class="c"><input type="checkbox" id="c-40506193" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#40506565">prev</a><span>|</span><a href="#40506152">next</a><span>|</span><label class="collapse" for="c-40506193">[-]</label><label class="expand" for="c-40506193">[6 more]</label></div><br/><div class="children"><div class="content">Is there a reason Phi Vision is omitted?</div><br/><div id="40506312" class="c"><input type="checkbox" id="c-40506312" checked=""/><div class="controls bullet"><span class="by">cadence-</span><span>|</span><a href="#40506193">parent</a><span>|</span><a href="#40506152">next</a><span>|</span><label class="collapse" for="c-40506312">[-]</label><label class="expand" for="c-40506312">[5 more]</label></div><br/><div class="children"><div class="content">Is there any place that currently hosts phi3 Vision and provides API access to it? I cannot run it on my local machine, unfortunately.</div><br/><div id="40506620" class="c"><input type="checkbox" id="c-40506620" checked=""/><div class="controls bullet"><span class="by">ai_what</span><span>|</span><a href="#40506193">root</a><span>|</span><a href="#40506312">parent</a><span>|</span><a href="#40506762">next</a><span>|</span><label class="collapse" for="c-40506620">[-]</label><label class="expand" for="c-40506620">[3 more]</label></div><br/><div class="children"><div class="content">Nvidia has a 1000 free credits API for phi3 vision.<p>You only need an email address.<p><a href="https:&#x2F;&#x2F;build.nvidia.com&#x2F;microsoft&#x2F;phi-3-vision-128k-instruct" rel="nofollow">https:&#x2F;&#x2F;build.nvidia.com&#x2F;microsoft&#x2F;phi-3-vision-128k-instruc...</a></div><br/><div id="40507029" class="c"><input type="checkbox" id="c-40507029" checked=""/><div class="controls bullet"><span class="by">trog</span><span>|</span><a href="#40506193">root</a><span>|</span><a href="#40506620">parent</a><span>|</span><a href="#40506733">next</a><span>|</span><label class="collapse" for="c-40507029">[-]</label><label class="expand" for="c-40507029">[1 more]</label></div><br/><div class="children"><div class="content">Was also looking for something like this - I can&#x27;t find pricing listed anywhere for their API usage, only the free 1,000 credits - or am I completely misunderstanding how this works?</div><br/></div></div><div id="40506733" class="c"><input type="checkbox" id="c-40506733" checked=""/><div class="controls bullet"><span class="by">cadence-</span><span>|</span><a href="#40506193">root</a><span>|</span><a href="#40506620">parent</a><span>|</span><a href="#40507029">prev</a><span>|</span><a href="#40506762">next</a><span>|</span><label class="collapse" for="c-40506733">[-]</label><label class="expand" for="c-40506733">[1 more]</label></div><br/><div class="children"><div class="content">Beautiful. Thank you.</div><br/></div></div></div></div><div id="40506762" class="c"><input type="checkbox" id="c-40506762" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#40506193">root</a><span>|</span><a href="#40506312">parent</a><span>|</span><a href="#40506620">prev</a><span>|</span><a href="#40506152">next</a><span>|</span><label class="collapse" for="c-40506762">[-]</label><label class="expand" for="c-40506762">[1 more]</label></div><br/><div class="children"><div class="content">Azure has a playground, I haven&#x27;t tried to use it with an API though. <a href="https:&#x2F;&#x2F;ai.azure.com&#x2F;explore&#x2F;models&#x2F;Phi-3-vision-128k-instruct&#x2F;version&#x2F;1&#x2F;registry&#x2F;azureml" rel="nofollow">https:&#x2F;&#x2F;ai.azure.com&#x2F;explore&#x2F;models&#x2F;Phi-3-vision-128k-instru...</a></div><br/></div></div></div></div></div></div><div id="40506152" class="c"><input type="checkbox" id="c-40506152" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40506193">prev</a><span>|</span><a href="#40507734">next</a><span>|</span><label class="collapse" for="c-40506152">[-]</label><label class="expand" for="c-40506152">[14 more]</label></div><br/><div class="children"><div class="content">This &quot;matching gpt-4&quot; catchy phrase has lost its meaning to me. Everytime an article like this pops up, I see marketing buzz and unrealistic results in practice.</div><br/><div id="40506187" class="c"><input type="checkbox" id="c-40506187" checked=""/><div class="controls bullet"><span class="by">mpalmer</span><span>|</span><a href="#40506152">parent</a><span>|</span><a href="#40506183">next</a><span>|</span><label class="collapse" for="c-40506187">[-]</label><label class="expand" for="c-40506187">[1 more]</label></div><br/><div class="children"><div class="content">For me it&#x27;s become a signal the person making the claim is unserious.</div><br/></div></div><div id="40506183" class="c"><input type="checkbox" id="c-40506183" checked=""/><div class="controls bullet"><span class="by">Mo3</span><span>|</span><a href="#40506152">parent</a><span>|</span><a href="#40506187">prev</a><span>|</span><a href="#40507441">next</a><span>|</span><label class="collapse" for="c-40506183">[-]</label><label class="expand" for="c-40506183">[10 more]</label></div><br/><div class="children"><div class="content">Of course, it&#x27;s nothing else. Who could possibly believe that OpenAI and others would dump billions into development and training and aren&#x27;t smart enough to figure out they could also do it with $500.</div><br/><div id="40506433" class="c"><input type="checkbox" id="c-40506433" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#40506152">root</a><span>|</span><a href="#40506183">parent</a><span>|</span><a href="#40506405">next</a><span>|</span><label class="collapse" for="c-40506433">[-]</label><label class="expand" for="c-40506433">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s llama 3 training cost + their cost. Meta &quot;kindly&quot; covered the first $700M.<p>&gt; We add a vision encoder to Llama3 8B</div><br/><div id="40507284" class="c"><input type="checkbox" id="c-40507284" checked=""/><div class="controls bullet"><span class="by">lanceflt</span><span>|</span><a href="#40506152">root</a><span>|</span><a href="#40506433">parent</a><span>|</span><a href="#40506405">next</a><span>|</span><label class="collapse" for="c-40507284">[-]</label><label class="expand" for="c-40507284">[1 more]</label></div><br/><div class="children"><div class="content">They didn&#x27;t train the vision encoder either, it&#x27;s unchanged SigLIP by Google.</div><br/></div></div></div></div><div id="40506405" class="c"><input type="checkbox" id="c-40506405" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40506152">root</a><span>|</span><a href="#40506183">parent</a><span>|</span><a href="#40506433">prev</a><span>|</span><a href="#40506505">next</a><span>|</span><label class="collapse" for="c-40506405">[-]</label><label class="expand" for="c-40506405">[1 more]</label></div><br/><div class="children"><div class="content">it would have been a lot cheaper for oai if they had access to llama3 in 2018</div><br/></div></div><div id="40506510" class="c"><input type="checkbox" id="c-40506510" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#40506152">root</a><span>|</span><a href="#40506183">parent</a><span>|</span><a href="#40506505">prev</a><span>|</span><a href="#40506408">next</a><span>|</span><label class="collapse" for="c-40506510">[-]</label><label class="expand" for="c-40506510">[1 more]</label></div><br/><div class="children"><div class="content">While that may be true, the opposite has also happened to hundreds of companies in other areas:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39136472">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39136472</a><p>Many companies also optimize for tools, like Python, that have boost productivity more than price&#x2F;performance ratio. OpenAI had billions of other people&#x27;s money. They might just keep using tools which worked before.<p>Lastly, there are tons of papers published on techniques that claim to reduce cost. Most of them aren&#x27;t good. Their benchmarks aren&#x27;t good. Even reviewing most of them is more time than a lot of AI researchers have. Those that make it to established communities usually have gotchas that come with the benefits. So, they could also simply miss a needle in a large haystack.<p>I think you&#x27;re right that they&#x27;d be using whatever really worked with no loss in model performance. It&#x27;s just that they might not for a number of reasons. The rational choice is for others to keep experimenting with those things in case they get a competitive advantage.</div><br/></div></div><div id="40506408" class="c"><input type="checkbox" id="c-40506408" checked=""/><div class="controls bullet"><span class="by">KorematsuFredt</span><span>|</span><a href="#40506152">root</a><span>|</span><a href="#40506183">parent</a><span>|</span><a href="#40506510">prev</a><span>|</span><a href="#40506416">next</a><span>|</span><label class="collapse" for="c-40506408">[-]</label><label class="expand" for="c-40506408">[3 more]</label></div><br/><div class="children"><div class="content">You have clearly not read the article. $500 is the cost of fine tuning.</div><br/><div id="40507417" class="c"><input type="checkbox" id="c-40507417" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#40506152">root</a><span>|</span><a href="#40506408">parent</a><span>|</span><a href="#40506416">next</a><span>|</span><label class="collapse" for="c-40507417">[-]</label><label class="expand" for="c-40507417">[2 more]</label></div><br/><div class="children"><div class="content">Fair enough. Is it now safe to say that OpenAI could have done with a 8B model + $500 of fine tuning instead of running a (much) larger model on their GPU cluster?</div><br/><div id="40507508" class="c"><input type="checkbox" id="c-40507508" checked=""/><div class="controls bullet"><span class="by">wrycoder</span><span>|</span><a href="#40506152">root</a><span>|</span><a href="#40507417">parent</a><span>|</span><a href="#40506416">next</a><span>|</span><label class="collapse" for="c-40507508">[-]</label><label class="expand" for="c-40507508">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they did</div><br/></div></div></div></div></div></div><div id="40506416" class="c"><input type="checkbox" id="c-40506416" checked=""/><div class="controls bullet"><span class="by">bilbo0s</span><span>|</span><a href="#40506152">root</a><span>|</span><a href="#40506183">parent</a><span>|</span><a href="#40506408">prev</a><span>|</span><a href="#40507441">next</a><span>|</span><label class="collapse" for="c-40506416">[-]</label><label class="expand" for="c-40506416">[1 more]</label></div><br/><div class="children"><div class="content"><i>Who could possibly believe that OpenAI and others would dump billions into development and training and aren&#x27;t smart enough to figure out they could also do it with $500.</i><p>People upvoting the post??<p>Not really sure? But PT Barnum said there&#x27;s always a lot of them out there.<p>Pretty sure they mean fine tuning though?<p>But even that is total tripe.<p>These guys are snake oil salesmen. (Or Sylvester McMonkey McBean is behind it.)</div><br/></div></div></div></div><div id="40507441" class="c"><input type="checkbox" id="c-40507441" checked=""/><div class="controls bullet"><span class="by">nbk_2000</span><span>|</span><a href="#40506152">parent</a><span>|</span><a href="#40506183">prev</a><span>|</span><a href="#40507101">next</a><span>|</span><label class="collapse" for="c-40507441">[-]</label><label class="expand" for="c-40507441">[1 more]</label></div><br/><div class="children"><div class="content">Starting to sound like the &quot;iPhone Killer&quot; we&#x27;ve all heard about... for the past 15+ years</div><br/></div></div><div id="40507101" class="c"><input type="checkbox" id="c-40507101" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#40506152">parent</a><span>|</span><a href="#40507441">prev</a><span>|</span><a href="#40507734">next</a><span>|</span><label class="collapse" for="c-40507101">[-]</label><label class="expand" for="c-40507101">[1 more]</label></div><br/><div class="children"><div class="content">Sadly the front page is often riddled with posts like these.</div><br/></div></div></div></div><div id="40507734" class="c"><input type="checkbox" id="c-40507734" checked=""/><div class="controls bullet"><span class="by">vikrantrathore</span><span>|</span><a href="#40506152">prev</a><span>|</span><a href="#40506728">next</a><span>|</span><label class="collapse" for="c-40507734">[-]</label><label class="expand" for="c-40507734">[1 more]</label></div><br/><div class="children"><div class="content">How does it compare with MiniCPM-Llama3-V 2.5 [0]? Based on what I see it seems much better than Llama 3-V on the benchmarks. Also it can directly be tried on Huggingface Spaces to check the performance [1]. It has the dataset, code and fine-tuning details with screenshots of it running on Xiaomi 14 pro. It has strong OCR performance and supports 30+ languages.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenBMB&#x2F;MiniCPM-V">https:&#x2F;&#x2F;github.com&#x2F;OpenBMB&#x2F;MiniCPM-V</a><p>[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;openbmb&#x2F;MiniCPM-Llama3-V-2_5" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;openbmb&#x2F;MiniCPM-Llama3-V-2_5</a></div><br/></div></div><div id="40506728" class="c"><input type="checkbox" id="c-40506728" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#40507734">prev</a><span>|</span><a href="#40506218">next</a><span>|</span><label class="collapse" for="c-40506728">[-]</label><label class="expand" for="c-40506728">[1 more]</label></div><br/><div class="children"><div class="content">Oh wow. I was expecting it to be the 70B one as base given those stats</div><br/></div></div><div id="40506218" class="c"><input type="checkbox" id="c-40506218" checked=""/><div class="controls bullet"><span class="by">yeldarb</span><span>|</span><a href="#40506728">prev</a><span>|</span><a href="#40508002">next</a><span>|</span><label class="collapse" for="c-40506218">[-]</label><label class="expand" for="c-40506218">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t see a license listed in the repo; presumably needs to be the same as Meta&#x27;s Llama 3 license?</div><br/></div></div><div id="40508008" class="c"><input type="checkbox" id="c-40508008" checked=""/><div class="controls bullet"><span class="by">2Gkashmiri</span><span>|</span><a href="#40508002">prev</a><span>|</span><a href="#40505879">next</a><span>|</span><label class="collapse" for="c-40508008">[-]</label><label class="expand" for="c-40508008">[1 more]</label></div><br/><div class="children"><div class="content">Is there a local small llm that can OCR images or haabdwritten invoices ?<p>Traditional OCR do not handle multiple invoice formats or handwritten ones.<p>I would like to train one locally with as many invoices it wants</div><br/></div></div><div id="40505879" class="c"><input type="checkbox" id="c-40505879" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#40508008">prev</a><span>|</span><label class="collapse" for="c-40505879">[-]</label><label class="expand" for="c-40505879">[4 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t CogAgent be in this comparison?</div><br/><div id="40506052" class="c"><input type="checkbox" id="c-40506052" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#40505879">parent</a><span>|</span><label class="collapse" for="c-40506052">[-]</label><label class="expand" for="c-40506052">[3 more]</label></div><br/><div class="children"><div class="content">CogVLM should be, not sure how CogAgent plays into this. This isn&#x27;t an agent.</div><br/><div id="40506302" class="c"><input type="checkbox" id="c-40506302" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#40505879">root</a><span>|</span><a href="#40506052">parent</a><span>|</span><label class="collapse" for="c-40506302">[-]</label><label class="expand" for="c-40506302">[2 more]</label></div><br/><div class="children"><div class="content">You would use CogAgent in VQA mode. Why would someone downvote suggesting to test one of the most powerful multimodal LLMs? Because it doesn&#x27;t have &quot;V&quot; in its name? CogAgent is improved on many tasks compared to CogVLM.</div><br/><div id="40507972" class="c"><input type="checkbox" id="c-40507972" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#40505879">root</a><span>|</span><a href="#40506302">parent</a><span>|</span><label class="collapse" for="c-40507972">[-]</label><label class="expand" for="c-40507972">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t downvote, only replied.<p>CogAgent is also CogVLM modified to handle documents and larger images. CogVLM is better for VQA.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>