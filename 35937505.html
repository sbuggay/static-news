<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684141261381" as="style"/><link rel="stylesheet" href="styles.css?v=1684141261381"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://gist.github.com/rain-1/8cc12b4b334052a21af8029aa9c4fafc">Run Llama 13B with a 6GB graphics card</a>Â <span class="domain">(<a href="https://gist.github.com">gist.github.com</a>)</span></div><div class="subtext"><span>rain1</span> | <span>227 comments</span></div><br/><div><div id="35939632" class="c"><input type="checkbox" id="c-35939632" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35937799">next</a><span>|</span><label class="collapse" for="c-35939632">[-]</label><label class="expand" for="c-35939632">[25 more]</label></div><br/><div class="children"><div class="content">On my system, using `-ngl 22` (running 22 layers on the GPU) cuts wall clock time by ~60%.<p>My system:<p>GPU: NVidia RTX 2070S (8GB VRAM)<p>CPU: AMD Ryzen 5 3600 (16GB VRAM)<p>Here&#x27;s the performance difference I see:<p>CPU only (.&#x2F;main -t 12)<p><pre><code>  llama_print_timings:        load time = 15459.43 ms
  llama_print_timings:      sample time =    23.64 ms &#x2F;    38 runs   (    0.62 ms per token)
  llama_print_timings: prompt eval time =  9338.10 ms &#x2F;   356 tokens (   26.23 ms per token)
  llama_print_timings:        eval time = 31700.73 ms &#x2F;    37 runs   (  856.78 ms per token)
  llama_print_timings:       total time = 47192.68 ms

</code></pre>
GPU (.&#x2F;main -t 12 -ngl 22)<p><pre><code>  llama_print_timings:        load time = 10285.15 ms
  llama_print_timings:      sample time =    21.60 ms &#x2F;    35 runs   (    0.62 ms per token)
  llama_print_timings: prompt eval time =  3889.65 ms &#x2F;   356 tokens (   10.93 ms per token)
  llama_print_timings:        eval time =  8126.90 ms &#x2F;    34 runs   (  239.03 ms per token)
  llama_print_timings:       total time = 18441.22 ms</code></pre></div><br/><div id="35940518" class="c"><input type="checkbox" id="c-35940518" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#35939632">parent</a><span>|</span><a href="#35940204">next</a><span>|</span><label class="collapse" for="c-35940518">[-]</label><label class="expand" for="c-35940518">[4 more]</label></div><br/><div class="children"><div class="content">You will likely see a bit of a performance gain dropping your threads to 6. I&#x27;m on a 3700x and get a regression when using 16 threads instead of the real 8 cores.</div><br/><div id="35943499" class="c"><input type="checkbox" id="c-35943499" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35940518">parent</a><span>|</span><a href="#35940204">next</a><span>|</span><label class="collapse" for="c-35943499">[-]</label><label class="expand" for="c-35943499">[3 more]</label></div><br/><div class="children"><div class="content">Ha! You&#x27;re right. Honestly, I couldn&#x27;t recall how many cores I had, and I figured it was faster to run `cat &#x2F;proc&#x2F;cpuinfo |grep processor| wc -l` than to google the specs.<p>It seems like using the right # cores saves ~25%. and then using the GPU for 26 layers saves another ~43%.<p>Timings for comparison with `--temp 0`:<p><pre><code>  -t 6 : 4m10.458s
  -t 12 : 5m34.772s
  -t 6 -ngl 26 : 2m23.278s</code></pre></div><br/><div id="35944068" class="c"><input type="checkbox" id="c-35944068" checked=""/><div class="controls bullet"><span class="by">shaklee3</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35943499">parent</a><span>|</span><a href="#35940204">next</a><span>|</span><label class="collapse" for="c-35944068">[-]</label><label class="expand" for="c-35944068">[2 more]</label></div><br/><div class="children"><div class="content">Just type lscpu</div><br/><div id="35944229" class="c"><input type="checkbox" id="c-35944229" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35944068">parent</a><span>|</span><a href="#35940204">next</a><span>|</span><label class="collapse" for="c-35944229">[-]</label><label class="expand" for="c-35944229">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s much clearer. The output includes:<p><pre><code>    Thread(s) per core:  2
    Core(s) per socket:  6
    Socket(s):           1</code></pre></div><br/></div></div></div></div></div></div></div></div><div id="35940204" class="c"><input type="checkbox" id="c-35940204" checked=""/><div class="controls bullet"><span class="by">guardiangod</span><span>|</span><a href="#35939632">parent</a><span>|</span><a href="#35940518">prev</a><span>|</span><a href="#35939969">next</a><span>|</span><label class="collapse" for="c-35940204">[-]</label><label class="expand" for="c-35940204">[2 more]</label></div><br/><div class="children"><div class="content">I am testing it on an AWS instance and the speedup effect is not as consistent as I hope. The speedup varies between runs.<p>Intel Xeon Platinum 8259CL CPU @ 2.50GHz
128 GB RAM
Tesla T4<p><pre><code>  .&#x2F;main -t 12 -m models&#x2F;gpt4-alpaca-lora-30B-4bit-GGML&#x2F;gpt4-alpaca-lora-30b.ggml.q5_0.bin

  llama_print_timings:        load time =  3725.08 ms
  llama_print_timings:      sample time =   612.06 ms &#x2F;   536 runs   (    1.14 ms per token)
  llama_print_timings: prompt eval time = 13876.81 ms &#x2F;   259 tokens (   53.58 ms per token)
  llama_print_timings:        eval time = 221647.40 ms &#x2F;   534 runs   (  415.07 ms per token)
  llama_print_timings:       total time = 239423.46 ms

  .&#x2F;main -t 12 -m models&#x2F;gpt4-alpaca-lora-30B-4bit-GGML&#x2F;gpt4-alpaca-lora-30b.ggml.q5_0.bin -ngl 30

  llama_print_timings:        load time =  7638.95 ms
  llama_print_timings:      sample time =   280.81 ms &#x2F;   294 runs   (    0.96 ms per token)
  llama_print_timings: prompt eval time =  2197.82 ms &#x2F;     2 tokens ( 1098.91 ms per token)
  llama_print_timings:        eval time = 112790.25 ms &#x2F;   293 runs   (  384.95 ms per token)
  llama_print_timings:       total time = 120788.82 ms</code></pre></div><br/><div id="35940365" class="c"><input type="checkbox" id="c-35940365" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35940204">parent</a><span>|</span><a href="#35939969">next</a><span>|</span><label class="collapse" for="c-35940365">[-]</label><label class="expand" for="c-35940365">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. BTW:<p>- the model I used was gpt4-x-vicuna-13B.ggml.q5_1.bin<p>- I used &#x27;time&#x27; to measure the wall clock time of each command.<p>- My prompt was:<p><pre><code>  Below is an instruction that describes a task. Write a response that appropriately completes the request.
  ### Instruction:
  Write a long blog post with 5 sections, about the pros and cons of emphasising procedural fluency over conceptual understanding, in high school math education.
  ### Response:</code></pre></div><br/></div></div></div></div><div id="35939969" class="c"><input type="checkbox" id="c-35939969" checked=""/><div class="controls bullet"><span class="by">PaulWaldman</span><span>|</span><a href="#35939632">parent</a><span>|</span><a href="#35940204">prev</a><span>|</span><a href="#35942018">next</a><span>|</span><label class="collapse" for="c-35939969">[-]</label><label class="expand" for="c-35939969">[2 more]</label></div><br/><div class="children"><div class="content">Any way to know the differences in power consumption?</div><br/><div id="35940077" class="c"><input type="checkbox" id="c-35940077" checked=""/><div class="controls bullet"><span class="by">Tuna-Fish</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35939969">parent</a><span>|</span><a href="#35942018">next</a><span>|</span><label class="collapse" for="c-35940077">[-]</label><label class="expand" for="c-35940077">[1 more]</label></div><br/><div class="children"><div class="content">Probably significant savings.</div><br/></div></div></div></div><div id="35942018" class="c"><input type="checkbox" id="c-35942018" checked=""/><div class="controls bullet"><span class="by">cpill</span><span>|</span><a href="#35939632">parent</a><span>|</span><a href="#35939969">prev</a><span>|</span><a href="#35939830">next</a><span>|</span><label class="collapse" for="c-35942018">[-]</label><label class="expand" for="c-35942018">[2 more]</label></div><br/><div class="children"><div class="content">Will this work with the leaked models or Alpaca?</div><br/><div id="35943703" class="c"><input type="checkbox" id="c-35943703" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35942018">parent</a><span>|</span><a href="#35939830">next</a><span>|</span><label class="collapse" for="c-35943703">[-]</label><label class="expand" for="c-35943703">[1 more]</label></div><br/><div class="children"><div class="content">Yes</div><br/></div></div></div></div><div id="35939830" class="c"><input type="checkbox" id="c-35939830" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35939632">parent</a><span>|</span><a href="#35942018">prev</a><span>|</span><a href="#35940393">next</a><span>|</span><label class="collapse" for="c-35939830">[-]</label><label class="expand" for="c-35939830">[9 more]</label></div><br/><div class="children"><div class="content">That is a crazy speedup!!</div><br/><div id="35941624" class="c"><input type="checkbox" id="c-35941624" checked=""/><div class="controls bullet"><span class="by">GordonS</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35939830">parent</a><span>|</span><a href="#35940393">next</a><span>|</span><label class="collapse" for="c-35941624">[-]</label><label class="expand" for="c-35941624">[8 more]</label></div><br/><div class="children"><div class="content">Is it really? Going from CPU to GPU, I would have expected a much better improvement.</div><br/><div id="35941911" class="c"><input type="checkbox" id="c-35941911" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35941624">parent</a><span>|</span><a href="#35941698">next</a><span>|</span><label class="collapse" for="c-35941911">[-]</label><label class="expand" for="c-35941911">[4 more]</label></div><br/><div class="children"><div class="content">You can think of it this way: if half the model is running on the GPU, and the GPU is infinitely fast, then the total calculation time would go down by 50%, compared with everything running on the CPU.</div><br/><div id="35942044" class="c"><input type="checkbox" id="c-35942044" checked=""/><div class="controls bullet"><span class="by">ethbr0</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35941911">parent</a><span>|</span><a href="#35941698">next</a><span>|</span><label class="collapse" for="c-35942044">[-]</label><label class="expand" for="c-35942044">[3 more]</label></div><br/><div class="children"><div class="content">Ref Amdahl&#x27;s Law: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Amdahl%27s_law" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Amdahl%27s_law</a></div><br/><div id="35942221" class="c"><input type="checkbox" id="c-35942221" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35942044">parent</a><span>|</span><a href="#35941698">next</a><span>|</span><label class="collapse" for="c-35942221">[-]</label><label class="expand" for="c-35942221">[2 more]</label></div><br/><div class="children"><div class="content">Wow seems like common sense turned into a law. Maybe I can get a law :-).<p>Anyone who has commuted on public transport probably knows this intuitively. (Using a kick scooter instead of walking cut my travel time by a good 5% which was excellent, as I still needed to be on a bus where that made no difference.)</div><br/><div id="35943129" class="c"><input type="checkbox" id="c-35943129" checked=""/><div class="controls bullet"><span class="by">ethbr0</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35942221">parent</a><span>|</span><a href="#35941698">next</a><span>|</span><label class="collapse" for="c-35943129">[-]</label><label class="expand" for="c-35943129">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a little more formalized and generalized observation: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Amdahl%27s_law#Derivation" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Amdahl%27s_law#Derivation</a><p>But yeah! There&#x27;s a lot of common sense that, with some mathematical formalization, yields useful and extensible laws.<p>Amdahl&#x27;s has been robust because it opined on the 60s supercomputer coprocessor architectures, and then in more recent decades with consumer multicore chips.<p>Laws aren&#x27;t famous because they&#x27;re insightful -- they&#x27;re famous because they&#x27;re useful.</div><br/></div></div></div></div></div></div></div></div><div id="35941698" class="c"><input type="checkbox" id="c-35941698" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35941624">parent</a><span>|</span><a href="#35941911">prev</a><span>|</span><a href="#35940393">next</a><span>|</span><label class="collapse" for="c-35941698">[-]</label><label class="expand" for="c-35941698">[3 more]</label></div><br/><div class="children"><div class="content">I feel the same.<p>For example some stats from Whisper [0] (audio transcoding, 30 seconds) show the following for the medium model (see other models in the link):<p>---<p>GPU medium fp32 Linear 1.7s<p>CPU medium fp32 nn.Linear 60.7s<p>CPU medium qint8 (quant) nn.Linear 23.1s<p>---<p>So the same model runs 35.7 times faster on GPU, and compared to an &quot;optimized&quot; model still 13.6.<p>I was expecting around an order or magnitude of improvement.<p>Then again, I do not know if in the case of this article the entire model was in the GPU, or just a fraction of it (22 layers) and the remainder on CPU, which might explain the result. Apparently that&#x27;s the case, but I don&#x27;t know much about this stuff.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;MiscellaneousStuff&#x2F;openai-whisper-cpu">https:&#x2F;&#x2F;github.com&#x2F;MiscellaneousStuff&#x2F;openai-whisper-cpu</a></div><br/><div id="35943334" class="c"><input type="checkbox" id="c-35943334" checked=""/><div class="controls bullet"><span class="by">pavelstoev</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35941698">parent</a><span>|</span><a href="#35941923">next</a><span>|</span><label class="collapse" for="c-35943334">[-]</label><label class="expand" for="c-35943334">[1 more]</label></div><br/><div class="children"><div class="content">Training and inference on GPUs significantly underutilize â¦the GPUs. So tuning and various tricks need to be applied to achieve dramatic performance gains. If I am not good at cooking, giving me a larger kitchen will not make me faster or better.</div><br/></div></div><div id="35941923" class="c"><input type="checkbox" id="c-35941923" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35941698">parent</a><span>|</span><a href="#35943334">prev</a><span>|</span><a href="#35940393">next</a><span>|</span><label class="collapse" for="c-35941923">[-]</label><label class="expand" for="c-35941923">[1 more]</label></div><br/><div class="children"><div class="content">You last paragraph is correct. Only about half the model was running on the GPU.</div><br/></div></div></div></div></div></div></div></div><div id="35940393" class="c"><input type="checkbox" id="c-35940393" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#35939632">parent</a><span>|</span><a href="#35939830">prev</a><span>|</span><a href="#35937799">next</a><span>|</span><label class="collapse" for="c-35940393">[-]</label><label class="expand" for="c-35940393">[5 more]</label></div><br/><div class="children"><div class="content">May you please ELI5 what is happening here...<p>Imagine I am first ever hearing about this, ;; what did you do?</div><br/><div id="35940636" class="c"><input type="checkbox" id="c-35940636" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35940393">parent</a><span>|</span><a href="#35937799">next</a><span>|</span><label class="collapse" for="c-35940636">[-]</label><label class="expand" for="c-35940636">[4 more]</label></div><br/><div class="children"><div class="content">0. Have a PC with an NVidia GPU, running Ubuntu, with the NVidia drivers and CUDA Toolkit already set up.<p>1. Download the weights for the model you want to use, e.g. gpt4-x-vicuna-13B.ggml.q5_1.bin<p>2. Clone the llama.cpp repo, and use &#x27;make LLAMA_CUBLAS=1&#x27; to compile it with support for CUBLAS (BLAS on GPU).<p>3. Run the resulting &#x27;main&#x27; executable, with the -ngl option set to 18, so that it tries to load 18 layers of the model into the GPU&#x27;s VRAM, instead of the system&#x27;s RAM.</div><br/><div id="35940768" class="c"><input type="checkbox" id="c-35940768" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35940636">parent</a><span>|</span><a href="#35937799">next</a><span>|</span><label class="collapse" for="c-35940768">[-]</label><label class="expand" for="c-35940768">[3 more]</label></div><br/><div class="children"><div class="content">&gt; 1. Download the weights for the model you want to use, e.g. gpt4-x-vicuna-13B.ggml.q5_1.bin<p>I think you need to quantize the model yourself from the float&#x2F;huggingface versions. My understanding is that the quantization formats have changed recently. and old quantized models no longer work.</div><br/><div id="35940829" class="c"><input type="checkbox" id="c-35940829" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35940768">parent</a><span>|</span><a href="#35937799">next</a><span>|</span><label class="collapse" for="c-35940829">[-]</label><label class="expand" for="c-35940829">[2 more]</label></div><br/><div class="children"><div class="content">That was true until 2 days ago :)<p>The repo has now been updated with requantized models that work with the latest version, so you don&#x27;t need to do that any more.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;gpt4-x-vicuna-13B-GGML&#x2F;commit&#x2F;b4b5f7e523f35306412d10ea9c4922b6f5923719" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;gpt4-x-vicuna-13B-GGML&#x2F;commi...</a></div><br/><div id="35940877" class="c"><input type="checkbox" id="c-35940877" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35939632">root</a><span>|</span><a href="#35940829">parent</a><span>|</span><a href="#35937799">next</a><span>|</span><label class="collapse" for="c-35940877">[-]</label><label class="expand" for="c-35940877">[1 more]</label></div><br/><div class="children"><div class="content">wonderful! thank you</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="35937799" class="c"><input type="checkbox" id="c-35937799" checked=""/><div class="controls bullet"><span class="by">naillo</span><span>|</span><a href="#35939632">prev</a><span>|</span><a href="#35939532">next</a><span>|</span><label class="collapse" for="c-35937799">[-]</label><label class="expand" for="c-35937799">[78 more]</label></div><br/><div class="children"><div class="content">This is cool but are people actually getting stuff done with these models? I&#x27;m enthusiastic about their potential too but after playing with it for a day I&#x27;m at a loss for what to use it for anymore at this point</div><br/><div id="35938193" class="c"><input type="checkbox" id="c-35938193" checked=""/><div class="controls bullet"><span class="by">ineedasername</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35940364">next</a><span>|</span><label class="collapse" for="c-35938193">[-]</label><label class="expand" for="c-35938193">[19 more]</label></div><br/><div class="children"><div class="content">I can run the Wizard 30B ggml model in CPU mode using a Ryzen 5700 and 16GB of <i>system</i> RAM, not GPU VRAM. Iâm using oobabooga as the front end.<p>Itâs slow, but if I ask it to write a Haiku itâs slow on the order of âgo brew some coffee and come back in 10 minutesâ and does it very well. Running it overnight on something like âsummarize an analysis of topic X it does a reasonable job.<p>It can produce answers to questions only slightly less well than ChatGPT (3.5). The Wizard 13B model runs much faster, maybe 2-3 tokens per second.<p>It is free, private, and runs on a midrange laptop.<p>A little more than a month ago that wasnât possible, not with my level of knowledge of the tooling involved at least, now it requires little more than running an executable and minor troubleshooting of python dependencies (on another machine it âjust workedâ)<p>So: Donât think of these posts as âdoing it just because you can and itâs fun to tinkerâ<p>Vast strides are being made pretty much daily in both quality and efficiency, raising their utility while lowering the cost of usage, doing both to a very significant degree.</div><br/><div id="35939395" class="c"><input type="checkbox" id="c-35939395" checked=""/><div class="controls bullet"><span class="by">theaiquestion</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938193">parent</a><span>|</span><a href="#35938951">next</a><span>|</span><label class="collapse" for="c-35939395">[-]</label><label class="expand" for="c-35939395">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Itâs slow, but if I ask it to write a Haiku itâs slow on the order of âgo brew some coffee and come back in 10 minutesâ and does it very well. Running it overnight on something like âsummarize an analysis of topic X it does a reasonable job.<p>I&#x27;m sorry but that&#x27;s unusably slow, even GPT-4 can take a retry or a prompt to fix certain type of issues. My experience is the open options require a lot more attempts&#x2F;manual prompt tuning.<p>I can&#x27;t think of a single workload where that is usable. That said once consumer GPUs are involved it does become usable</div><br/><div id="35942771" class="c"><input type="checkbox" id="c-35942771" checked=""/><div class="controls bullet"><span class="by">ineedasername</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939395">parent</a><span>|</span><a href="#35939438">next</a><span>|</span><label class="collapse" for="c-35942771">[-]</label><label class="expand" for="c-35942771">[1 more]</label></div><br/><div class="children"><div class="content">You are overlooking my main point, which that while it is unusably slow now, what Iâm doing wasnât possible little more than a month or two ago.<p>The speed of improvement is rapid. Whether or not the COTS world eventually embraces a corporate backed version(s) or open source is somewhat besides the point when considering the impact that open source is already having.<p>Put aside thoughts of financing or startups or VC or moats or any of that and simply look at that rate of advancement that has occurred once countless curious tinkerers and experts and all sorts of people are working towards.<p>That is what amazes me. Iâm torn about the risk&#x2F;reward aspect of things but I think the genie is out of the bottle on that, so Iâm left watching the hurricane blow through, and itâs off the cat-5 scale.</div><br/></div></div><div id="35939438" class="c"><input type="checkbox" id="c-35939438" checked=""/><div class="controls bullet"><span class="by">postalrat</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939395">parent</a><span>|</span><a href="#35942771">prev</a><span>|</span><a href="#35942986">next</a><span>|</span><label class="collapse" for="c-35939438">[-]</label><label class="expand" for="c-35939438">[3 more]</label></div><br/><div class="children"><div class="content">I doubt you&#x27;ve ever worked with people if you think that&#x27;s unusable slow</div><br/><div id="35939608" class="c"><input type="checkbox" id="c-35939608" checked=""/><div class="controls bullet"><span class="by">bcrosby95</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939438">parent</a><span>|</span><a href="#35942986">next</a><span>|</span><label class="collapse" for="c-35939608">[-]</label><label class="expand" for="c-35939608">[2 more]</label></div><br/><div class="children"><div class="content">The computer doesn&#x27;t ask for annoying things like a paycheck or benefits either.</div><br/><div id="35941059" class="c"><input type="checkbox" id="c-35941059" checked=""/><div class="controls bullet"><span class="by">mejutoco</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939608">parent</a><span>|</span><a href="#35942986">next</a><span>|</span><label class="collapse" for="c-35941059">[-]</label><label class="expand" for="c-35941059">[1 more]</label></div><br/><div class="children"><div class="content">Money upfront and a small salary in the form of electricity bills.</div><br/></div></div></div></div></div></div><div id="35942986" class="c"><input type="checkbox" id="c-35942986" checked=""/><div class="controls bullet"><span class="by">PragmaticPulp</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939395">parent</a><span>|</span><a href="#35939438">prev</a><span>|</span><a href="#35938951">next</a><span>|</span><label class="collapse" for="c-35942986">[-]</label><label class="expand" for="c-35942986">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I can&#x27;t think of a single workload where that is usable.<p>It&#x27;s not intended to be usable for production workloads. This enables people to experiment with things on the hardware they have without spending more money.<p>&gt; That said once consumer GPUs are involved it does become usable<p>You can pick up an RTX 3090 with 24GB of VRAM right now if you want, but it&#x27;s going to cost you. You can also spin up GPU instances with larger VRAM from any number of providers. This is more about having options for people who want to experiment.</div><br/></div></div></div></div><div id="35938951" class="c"><input type="checkbox" id="c-35938951" checked=""/><div class="controls bullet"><span class="by">BaculumMeumEst</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938193">parent</a><span>|</span><a href="#35939395">prev</a><span>|</span><a href="#35939946">next</a><span>|</span><label class="collapse" for="c-35938951">[-]</label><label class="expand" for="c-35938951">[10 more]</label></div><br/><div class="children"><div class="content">Wow you can run a 30B model on 16gb ram? Is it hitting swap?</div><br/><div id="35941409" class="c"><input type="checkbox" id="c-35941409" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938951">parent</a><span>|</span><a href="#35941066">next</a><span>|</span><label class="collapse" for="c-35941409">[-]</label><label class="expand" for="c-35941409">[1 more]</label></div><br/><div class="children"><div class="content">Most people are running these at 4 bits per parameter for speed and RAM reasons. That means the model would take just about all of the RAM. But instead of swap (writing data to disk and then reading it again later), I would expect a good implementation to only run into cache eviction (deleting data from RAM and then reading it back from disk later), which should be a lot faster and cause less wear and tear on SSDs.</div><br/></div></div><div id="35941066" class="c"><input type="checkbox" id="c-35941066" checked=""/><div class="controls bullet"><span class="by">mcbuilder</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938951">parent</a><span>|</span><a href="#35941409">prev</a><span>|</span><a href="#35939322">next</a><span>|</span><label class="collapse" for="c-35941066">[-]</label><label class="expand" for="c-35941066">[7 more]</label></div><br/><div class="children"><div class="content">These models can run FP16, with LLM quantization going down to Int8 and beyond.</div><br/><div id="35941316" class="c"><input type="checkbox" id="c-35941316" checked=""/><div class="controls bullet"><span class="by">BaculumMeumEst</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35941066">parent</a><span>|</span><a href="#35939322">next</a><span>|</span><label class="collapse" for="c-35941316">[-]</label><label class="expand" for="c-35941316">[6 more]</label></div><br/><div class="children"><div class="content">i&#x27;m just starting to get into deep learning so i look forward to understanding that sentence</div><br/><div id="35941508" class="c"><input type="checkbox" id="c-35941508" checked=""/><div class="controls bullet"><span class="by">mike00632</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35941316">parent</a><span>|</span><a href="#35942032">next</a><span>|</span><label class="collapse" for="c-35941508">[-]</label><label class="expand" for="c-35941508">[1 more]</label></div><br/><div class="children"><div class="content">How much resources are required is directly related to the memory size devoted to each weight. If the weights are stored as 32-bit floating points then each weight is 32 bits which adds up when we are talking about billions of weights. But if the weights are first converted to 16-bit floating point numbers (precise to fewer decimal places) then fewer resources are needed to store and compute the numbers. Research has shown that simply chopping off some of the precision of the weights still yields good AI performance in many cases.<p>Note too that the numbers are standardized, e.g. floats are defined by IEEE 754 standard. Numbers in this format have specialized hardware to do math with them, so when considering which number format to use it&#x27;s difficult to get outside of the established ones (foat32, float16, int8).</div><br/></div></div><div id="35942032" class="c"><input type="checkbox" id="c-35942032" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35941316">parent</a><span>|</span><a href="#35941508">prev</a><span>|</span><a href="#35941415">next</a><span>|</span><label class="collapse" for="c-35942032">[-]</label><label class="expand" for="c-35942032">[3 more]</label></div><br/><div class="children"><div class="content">Training uses gradient descent, so you want to have good precision during that process. But once you have the overall structure of the network, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.17323" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.17323</a> (GPTQ) showed that you can cut down the precision quite a bit without losing a lot of accuracy. It seems you can cut down further for larger models. For the 13B Llama-based ones, going below 5 bits per parameter is noticeably worse, but for 30B models you can do 4 bits.<p>The same group did another paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.00774" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.00774</a> which shows that in addition to reducing the precision of each parameter, you can also prune out a bunch of parameters entirely. It&#x27;s harder to apply this optimization because models are usually loaded into RAM densely, but I hope someone figures out how to do it for popular models.</div><br/><div id="35943601" class="c"><input type="checkbox" id="c-35943601" checked=""/><div class="controls bullet"><span class="by">mycall</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35942032">parent</a><span>|</span><a href="#35943926">next</a><span>|</span><label class="collapse" for="c-35943601">[-]</label><label class="expand" for="c-35943601">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if specialization of the LLM is another way to reduce the RAM requirements.   For example, if you can tell which nodes are touched through billions of web searches on a topic, then you can delete the ones that never are touched.</div><br/></div></div><div id="35943926" class="c"><input type="checkbox" id="c-35943926" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35942032">parent</a><span>|</span><a href="#35943601">prev</a><span>|</span><a href="#35941415">next</a><span>|</span><label class="collapse" for="c-35943926">[-]</label><label class="expand" for="c-35943926">[1 more]</label></div><br/><div class="children"><div class="content">Some people are having some success speeding token rates and clawback on VRAM using a 0- group size flag but ymmv I did not test this yet (they were discussing gptq btw)</div><br/></div></div></div></div><div id="35941415" class="c"><input type="checkbox" id="c-35941415" checked=""/><div class="controls bullet"><span class="by">MobiusHorizons</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35941316">parent</a><span>|</span><a href="#35942032">prev</a><span>|</span><a href="#35939322">next</a><span>|</span><label class="collapse" for="c-35941415">[-]</label><label class="expand" for="c-35941415">[1 more]</label></div><br/><div class="children"><div class="content">FP16 and Int8 are about how many bits are being used for floating point and integer numbers. FP16 is 16bit floating point. The more bits the better the precision, but the more ram it takes. Normally programmers use 32 or 64bit floats so 16bit floats have significantly reduced precision, but take up half the space of fp32 which is the smallest floating point format for most CPUs. similarly 8 bit integers have only 256 total possibilities and go from -128 to 127.</div><br/></div></div></div></div></div></div></div></div><div id="35939946" class="c"><input type="checkbox" id="c-35939946" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938193">parent</a><span>|</span><a href="#35938951">prev</a><span>|</span><a href="#35940364">next</a><span>|</span><label class="collapse" for="c-35939946">[-]</label><label class="expand" for="c-35939946">[2 more]</label></div><br/><div class="children"><div class="content">What prompt do you use to get haikus?</div><br/><div id="35943916" class="c"><input type="checkbox" id="c-35943916" checked=""/><div class="controls bullet"><span class="by">ineedasername</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939946">parent</a><span>|</span><a href="#35940364">next</a><span>|</span><label class="collapse" for="c-35943916">[-]</label><label class="expand" for="c-35943916">[1 more]</label></div><br/><div class="children"><div class="content">I use the same one the first time I try any model:<p>&gt;<i>I love my wife &lt;name&gt; very much. Please write me a haiku about her.</i><p>She smiles when I show the good ones to her, though of course she understands itâs little different than showing her a greeting card that has something nice written in it.<p>As a side note, one 7B model wrote an awful bit of poetry at least 20 lines long and some attempts to rhyme, and merely used the word âhaikuâ in it. So the prompt was enough to trigger âknowledgeâ that a poem was needed and love should be involved and it should definitely use the word haiku in there somewhere.</div><br/></div></div></div></div></div></div><div id="35940364" class="c"><input type="checkbox" id="c-35940364" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35938193">prev</a><span>|</span><a href="#35941476">next</a><span>|</span><label class="collapse" for="c-35940364">[-]</label><label class="expand" for="c-35940364">[1 more]</label></div><br/><div class="children"><div class="content">My experience is that even the &quot;toy&quot; 7B models can do pretty amazing things, but they&#x27;re very sensitive to prompting in the right way to force the model down the right pathways. My assumption is that the larger models have more redundancy and therefore can be prompted less precisely while still going down the right pathway.<p>For example, the WizardLM model (5_1 quantized, as 4_x quantization makes the 7B models perform terribly):<p>Prompt: <i>I went to the market and bought 10 apples. I gave 2 to my neighbor and 2 to my sister. Then I bought 5 more and ate 1. How many apples do I have?</i><p>Response:<p><pre><code>  Let me check that for you! You had 9 apples left after giving away 4, so you still have 9 - 4 = 5 apples left.
</code></pre>
Prompt: <i>I went to the market and bought 10 apples. I gave 2 to my neighbor and 2 to my sister. Then I bought 5 more and ate 1. How many apples do I have? Write out any calculations in full. List the steps used to come to the final result.</i><p>Response:<p><pre><code>  Sure, let&#x27;s work through this problem together! Here are the steps we can use:
  Step 1: Start with the initial number of apples you bought (10).
  Step 2: Subtract the amount given to your neighbor and sister from the total (2 + 2 = 4). This gives us a new total of 6 apples.
  Step 3: Add the additional 5 apples you bought later on to the remaining 6 apples, giving us a final total of 11 apples.
  Step 4: Finally, subtract the apple you ate from the total (1), leaving us with 10 apples in total.
  So the answer is 10 apples.</code></pre></div><br/></div></div><div id="35941476" class="c"><input type="checkbox" id="c-35941476" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35940364">prev</a><span>|</span><a href="#35938173">next</a><span>|</span><label class="collapse" for="c-35941476">[-]</label><label class="expand" for="c-35941476">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve successfully gotten at-home models (<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;NousResearch&#x2F;GPT4-x-Vicuna-13b-fp16" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;NousResearch&#x2F;GPT4-x-Vicuna-13b-fp16</a>) to go through my messages and pull out key todo items. For example, reminding me to message my father about travel plans.<p>Is it comparable to GPT-4? No, it&#x27;s not remotely close. But it&#x27;s getting closer every week, and it very recently crossed the threshold of &#x27;it can do stuff I would never send to a cloud service&#x27; (namely, reading all of my messages and pulling out action items).</div><br/><div id="35943724" class="c"><input type="checkbox" id="c-35943724" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35941476">parent</a><span>|</span><a href="#35938173">next</a><span>|</span><label class="collapse" for="c-35943724">[-]</label><label class="expand" for="c-35943724">[2 more]</label></div><br/><div class="children"><div class="content">How do you deal with the &#x27;too many tokens&#x27; problem? Like, For 13b, max tokens I&#x27;ve gotten it to is 508 - is there a way to increase the number it will accept?</div><br/><div id="35943953" class="c"><input type="checkbox" id="c-35943953" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35943724">parent</a><span>|</span><a href="#35938173">next</a><span>|</span><label class="collapse" for="c-35943953">[-]</label><label class="expand" for="c-35943953">[1 more]</label></div><br/><div class="children"><div class="content">Oh, I figured this out. It runs with 512 as the default context, but you can increase it with -c</div><br/></div></div></div></div></div></div><div id="35938173" class="c"><input type="checkbox" id="c-35938173" checked=""/><div class="controls bullet"><span class="by">instance</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35941476">prev</a><span>|</span><a href="#35937842">next</a><span>|</span><label class="collapse" for="c-35938173">[-]</label><label class="expand" for="c-35938173">[9 more]</label></div><br/><div class="children"><div class="content">I tested on a serious use case and quality was subpar. For real use cases I had to either host the most powerful model you can get (e.g. LLaMA-65B or so) on a cloud machine, which again costs too much (you&#x27;ll be paying like 500-1000 USD per month), or just go straight for GPT-3.5 on OpenAI. The latter economically makes most sense.</div><br/><div id="35939173" class="c"><input type="checkbox" id="c-35939173" checked=""/><div class="controls bullet"><span class="by">inferense</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938173">parent</a><span>|</span><a href="#35939767">next</a><span>|</span><label class="collapse" for="c-35939173">[-]</label><label class="expand" for="c-35939173">[5 more]</label></div><br/><div class="children"><div class="content">what real use case did you use it for?</div><br/><div id="35939678" class="c"><input type="checkbox" id="c-35939678" checked=""/><div class="controls bullet"><span class="by">instance</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939173">parent</a><span>|</span><a href="#35939207">next</a><span>|</span><label class="collapse" for="c-35939678">[-]</label><label class="expand" for="c-35939678">[3 more]</label></div><br/><div class="children"><div class="content">For instance used it in conjunction with llama-index for knowledge management. Created an index for a whole confluence&#x2F;jira of a mid-sized company, got good results with GPT, but for LLaMA of this size that use case was too much.</div><br/><div id="35941454" class="c"><input type="checkbox" id="c-35941454" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939678">parent</a><span>|</span><a href="#35941468">next</a><span>|</span><label class="collapse" for="c-35941454">[-]</label><label class="expand" for="c-35941454">[1 more]</label></div><br/><div class="children"><div class="content">Did you try instructor-xl? It ranks highest on huggingface.</div><br/></div></div><div id="35941468" class="c"><input type="checkbox" id="c-35941468" checked=""/><div class="controls bullet"><span class="by">dzhiurgis</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939678">parent</a><span>|</span><a href="#35941454">prev</a><span>|</span><a href="#35939207">next</a><span>|</span><label class="collapse" for="c-35941468">[-]</label><label class="expand" for="c-35941468">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d argue 1k per month for mid-sized company is nothing, but I can understand where you are coming from.</div><br/></div></div></div></div><div id="35939207" class="c"><input type="checkbox" id="c-35939207" checked=""/><div class="controls bullet"><span class="by">throwaway1777</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939173">parent</a><span>|</span><a href="#35939678">prev</a><span>|</span><a href="#35939767">next</a><span>|</span><label class="collapse" for="c-35939207">[-]</label><label class="expand" for="c-35939207">[1 more]</label></div><br/><div class="children"><div class="content">Making demos to raise investment probably</div><br/></div></div></div></div><div id="35939767" class="c"><input type="checkbox" id="c-35939767" checked=""/><div class="controls bullet"><span class="by">raffraffraff</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938173">parent</a><span>|</span><a href="#35939173">prev</a><span>|</span><a href="#35937842">next</a><span>|</span><label class="collapse" for="c-35939767">[-]</label><label class="expand" for="c-35939767">[3 more]</label></div><br/><div class="children"><div class="content">What about turning the cloud vm off except when you&#x27;re actually using it?</div><br/><div id="35942314" class="c"><input type="checkbox" id="c-35942314" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939767">parent</a><span>|</span><a href="#35940468">next</a><span>|</span><label class="collapse" for="c-35942314">[-]</label><label class="expand" for="c-35942314">[1 more]</label></div><br/><div class="children"><div class="content">So modal.com is &quot;turning-the-vm-off-when-unused-as-a-service&quot; :-)<p>I ran research&#x2F;open_llama_7b_preview_200bt on there, using they python example, with A10G gpu.<p>Cost 2-3c per run, taking ~20 seconds each time, on fairly small prompts. So about the same as GPT-4?<p>Now this is a non expert just playing, it probably can be optimized by trying different GPUs and optimizing the code somehow.<p>I don&#x27;t think you are using these models to save money, but you might be using them for tunability, privacy, mobility [1], secrecy or fun&#x2F;research.<p>[1] in other words you want to build a robot that can work disconnected from the internet.</div><br/></div></div><div id="35940468" class="c"><input type="checkbox" id="c-35940468" checked=""/><div class="controls bullet"><span class="by">unglaublich</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939767">parent</a><span>|</span><a href="#35942314">prev</a><span>|</span><a href="#35937842">next</a><span>|</span><label class="collapse" for="c-35940468">[-]</label><label class="expand" for="c-35940468">[1 more]</label></div><br/><div class="children"><div class="content">A &quot;serious use case&quot; means it needs to be available around the clock.</div><br/></div></div></div></div></div></div><div id="35937842" class="c"><input type="checkbox" id="c-35937842" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35938173">prev</a><span>|</span><a href="#35938034">next</a><span>|</span><label class="collapse" for="c-35937842">[-]</label><label class="expand" for="c-35937842">[19 more]</label></div><br/><div class="children"><div class="content">Most part if you actually need stuff done, you can afford 20$ for OpenAIs proven models.<p>These local projects are great because maybe eventually they will have a equivalent model that can be run on cheap parts</div><br/><div id="35938183" class="c"><input type="checkbox" id="c-35938183" checked=""/><div class="controls bullet"><span class="by">thelittleone</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35937842">parent</a><span>|</span><a href="#35938142">next</a><span>|</span><label class="collapse" for="c-35938183">[-]</label><label class="expand" for="c-35938183">[12 more]</label></div><br/><div class="children"><div class="content">Knowledge cut off and sending potentially sensitive IP to a centralised and untrusted third party. This would likely limit the addressable market.</div><br/><div id="35938450" class="c"><input type="checkbox" id="c-35938450" checked=""/><div class="controls bullet"><span class="by">jjoonathan</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938183">parent</a><span>|</span><a href="#35938142">next</a><span>|</span><label class="collapse" for="c-35938450">[-]</label><label class="expand" for="c-35938450">[11 more]</label></div><br/><div class="children"><div class="content">Also: they are lobotomized. If you want to do security research, Sorry Dave. If you want to nsfw writing, Sorry Dave. If you want to open the pod bay doors, Sorry Dave, I&#x27;m afraid I can&#x27;t do that.</div><br/><div id="35943686" class="c"><input type="checkbox" id="c-35943686" checked=""/><div class="controls bullet"><span class="by">mycall</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938450">parent</a><span>|</span><a href="#35938505">next</a><span>|</span><label class="collapse" for="c-35943686">[-]</label><label class="expand" for="c-35943686">[1 more]</label></div><br/><div class="children"><div class="content">You are missing the point.  There are layers and the LLM is one layer (orchestration), but you can always have other semantic kernels with skills connected to other software (do stuff).</div><br/></div></div><div id="35938505" class="c"><input type="checkbox" id="c-35938505" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938450">parent</a><span>|</span><a href="#35943686">prev</a><span>|</span><a href="#35939338">next</a><span>|</span><label class="collapse" for="c-35938505">[-]</label><label class="expand" for="c-35938505">[1 more]</label></div><br/><div class="children"><div class="content">Microsoft Azure still has the GPT-3.5 foundation model, code-davinci-002. It is not fine-tuned for instruction following, safety, or censorship.<p>I&#x27;m not sure though whether Microsoft analyzes the input&#x2F;output with another model to detect and prevent certain content.</div><br/></div></div><div id="35939338" class="c"><input type="checkbox" id="c-35939338" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938450">parent</a><span>|</span><a href="#35938505">prev</a><span>|</span><a href="#35938142">next</a><span>|</span><label class="collapse" for="c-35939338">[-]</label><label class="expand" for="c-35939338">[8 more]</label></div><br/><div class="children"><div class="content">I tried to help a sex worker with ChatGPT and all it did was preach about avoiding sex work, further marginalizing in its virtue signalizing. I dont consider her marginalized, and âhelpâ was just information about her job and techniques and venues. ChatGPT would have consumed this textual information too.<p>But yeah offline fine tuned models wont have this problem.<p>Kind of cool to see how the SWERF representation in tech is going to speedrun SWERF irrelevancy.</div><br/><div id="35939779" class="c"><input type="checkbox" id="c-35939779" checked=""/><div class="controls bullet"><span class="by">jhartwig</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939338">parent</a><span>|</span><a href="#35938142">next</a><span>|</span><label class="collapse" for="c-35939779">[-]</label><label class="expand" for="c-35939779">[7 more]</label></div><br/><div class="children"><div class="content">You tried to help a sex worker with chatGPT? Care to explain more on this use case lol? Curious minds want to know.</div><br/><div id="35940605" class="c"><input type="checkbox" id="c-35940605" checked=""/><div class="controls bullet"><span class="by">jstarfish</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939779">parent</a><span>|</span><a href="#35939836">next</a><span>|</span><label class="collapse" for="c-35940605">[-]</label><label class="expand" for="c-35940605">[5 more]</label></div><br/><div class="children"><div class="content">People are already setting up fake personas&#x2F;OnlyFans accounts using chatbots and SD images.<p>We have a high-value specialist currently chatting up a few of them at work. His wife doesn&#x27;t know. He doesn&#x27;t know <i>we</i> know. The photos are fake but he&#x27;s too horny to notice.<p>Time to dust off the &quot;there are no women on the internet&quot; meme...</div><br/><div id="35941034" class="c"><input type="checkbox" id="c-35941034" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35940605">parent</a><span>|</span><a href="#35941222">next</a><span>|</span><label class="collapse" for="c-35941034">[-]</label><label class="expand" for="c-35941034">[1 more]</label></div><br/><div class="children"><div class="content">Informative for some but this wasnât an interaction over the internet, just out and about</div><br/></div></div><div id="35941222" class="c"><input type="checkbox" id="c-35941222" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35940605">parent</a><span>|</span><a href="#35941034">prev</a><span>|</span><a href="#35939836">next</a><span>|</span><label class="collapse" for="c-35941222">[-]</label><label class="expand" for="c-35941222">[3 more]</label></div><br/><div class="children"><div class="content">&gt; People are already setting up fake personas&#x2F;OnlyFans accounts using chatbots and SD images.<p>Citation needed.</div><br/><div id="35941883" class="c"><input type="checkbox" id="c-35941883" checked=""/><div class="controls bullet"><span class="by">jstarfish</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35941222">parent</a><span>|</span><a href="#35941658">next</a><span>|</span><label class="collapse" for="c-35941883">[-]</label><label class="expand" for="c-35941883">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m only offering an early anecdote that catfishing is adapting to the times. You don&#x27;t have to believe me. Don&#x27;t trust anyone with weird nipples that hides their hands in every photo.<p>People have been [claiming to] do this for years:
<a href="https:&#x2F;&#x2F;www.blackhatworld.com&#x2F;seo&#x2F;monetizing-traffic-from-social-media-catfish.1021876&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.blackhatworld.com&#x2F;seo&#x2F;monetizing-traffic-from-so...</a><p>Give it 1-2 years and you can hear about it from Krebs.</div><br/></div></div><div id="35941658" class="c"><input type="checkbox" id="c-35941658" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35941222">parent</a><span>|</span><a href="#35941883">prev</a><span>|</span><a href="#35939836">next</a><span>|</span><label class="collapse" for="c-35941658">[-]</label><label class="expand" for="c-35941658">[1 more]</label></div><br/><div class="children"><div class="content">just the latest most high profile example making the rounds yesterday<p><a href="https:&#x2F;&#x2F;www.nbcnews.com&#x2F;news&#x2F;amp&#x2F;rcna84180" rel="nofollow">https:&#x2F;&#x2F;www.nbcnews.com&#x2F;news&#x2F;amp&#x2F;rcna84180</a></div><br/></div></div></div></div></div></div><div id="35939836" class="c"><input type="checkbox" id="c-35939836" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939779">parent</a><span>|</span><a href="#35940605">prev</a><span>|</span><a href="#35938142">next</a><span>|</span><label class="collapse" for="c-35939836">[-]</label><label class="expand" for="c-35939836">[1 more]</label></div><br/><div class="children"><div class="content">Venues to work, other peopleâs thoughts working there, management, nuances about local licenses, stuff that anybody with any job would do</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35938142" class="c"><input type="checkbox" id="c-35938142" checked=""/><div class="controls bullet"><span class="by">Beltiras</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35937842">parent</a><span>|</span><a href="#35938183">prev</a><span>|</span><a href="#35938034">next</a><span>|</span><label class="collapse" for="c-35938142">[-]</label><label class="expand" for="c-35938142">[6 more]</label></div><br/><div class="children"><div class="content">The knowledge cutoff date is brutal for some applications.</div><br/><div id="35938276" class="c"><input type="checkbox" id="c-35938276" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938142">parent</a><span>|</span><a href="#35938034">next</a><span>|</span><label class="collapse" for="c-35938276">[-]</label><label class="expand" for="c-35938276">[5 more]</label></div><br/><div class="children"><div class="content">I wonder how ingesting more and more data will affect the size of parameters, itâs gonna continually get bigger?</div><br/><div id="35938314" class="c"><input type="checkbox" id="c-35938314" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938276">parent</a><span>|</span><a href="#35938034">next</a><span>|</span><label class="collapse" for="c-35938314">[-]</label><label class="expand" for="c-35938314">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that the current models are at &quot;knowledge capacity&quot;. So far all evidence points to training on more data on the same size model gives better results.</div><br/><div id="35938622" class="c"><input type="checkbox" id="c-35938622" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938314">parent</a><span>|</span><a href="#35938034">next</a><span>|</span><label class="collapse" for="c-35938622">[-]</label><label class="expand" for="c-35938622">[3 more]</label></div><br/><div class="children"><div class="content">Both increasing the amount of parameters and the amount of training tokens improves results (more precisely: lowers training loss), and costs computing power. For optimally improving loss per training computing power, model size and training tokens should be increased equally. That&#x27;s the Chinchilla scaling law. (Though low loss is not always the same as good results, the data quality also matters.)<p>Further reading: <a href="https:&#x2F;&#x2F;dynomight.net&#x2F;scaling&#x2F;" rel="nofollow">https:&#x2F;&#x2F;dynomight.net&#x2F;scaling&#x2F;</a></div><br/><div id="35938675" class="c"><input type="checkbox" id="c-35938675" checked=""/><div class="controls bullet"><span class="by">snovv_crash</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938622">parent</a><span>|</span><a href="#35938034">next</a><span>|</span><label class="collapse" for="c-35938675">[-]</label><label class="expand" for="c-35938675">[2 more]</label></div><br/><div class="children"><div class="content">An interesting corollary of this is that if you want to reduce the model size you can compensate by training for longer to achieve the same accuracy. Depending on your training:inference ratio this may be more optimal globally to reduce your total compute costs or even just reduce your frontend latency.</div><br/><div id="35938872" class="c"><input type="checkbox" id="c-35938872" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938675">parent</a><span>|</span><a href="#35938034">next</a><span>|</span><label class="collapse" for="c-35938872">[-]</label><label class="expand" for="c-35938872">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, though I have not seen a formula which takes the number of expected inference runs into account for calculating the optimal data&#x2F;parameter balance.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35938034" class="c"><input type="checkbox" id="c-35938034" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35937842">prev</a><span>|</span><a href="#35938236">next</a><span>|</span><label class="collapse" for="c-35938034">[-]</label><label class="expand" for="c-35938034">[10 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve setup and use Vicuna-13b for text classification, summarization and topic modelling. Works like a charm.<p>It&#x27;s also good for math lessons.</div><br/><div id="35940667" class="c"><input type="checkbox" id="c-35940667" checked=""/><div class="controls bullet"><span class="by">jstarfish</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938034">parent</a><span>|</span><a href="#35938330">next</a><span>|</span><label class="collapse" for="c-35940667">[-]</label><label class="expand" for="c-35940667">[1 more]</label></div><br/><div class="children"><div class="content">Maybe others&#x27; experiences are different, but I find smaller models to work just as well for &quot;reductive&quot; tasks.<p>Dolly sucks for generating long-form content (not very creative) but if I need a summary or classification, it&#x27;s quicker and easier to spin up dolly-3b than vicuna-13b.<p>I suspect OpenAI is routing prompts to select models based on similar logic.</div><br/></div></div><div id="35938330" class="c"><input type="checkbox" id="c-35938330" checked=""/><div class="controls bullet"><span class="by">BOOSTERHIDROGEN</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938034">parent</a><span>|</span><a href="#35940667">prev</a><span>|</span><a href="#35938236">next</a><span>|</span><label class="collapse" for="c-35938330">[-]</label><label class="expand" for="c-35938330">[8 more]</label></div><br/><div class="children"><div class="content">Would like to know how you setup this. A posts would be awesome.</div><br/><div id="35938428" class="c"><input type="checkbox" id="c-35938428" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938330">parent</a><span>|</span><a href="#35938236">next</a><span>|</span><label class="collapse" for="c-35938428">[-]</label><label class="expand" for="c-35938428">[7 more]</label></div><br/><div class="children"><div class="content">There are various posts online on how to set it up, either for Linux or Windows. There was an older post here on how to install opt-65b on a mac studio ultra, and smaller models on mac pros. There was also a post if I remember correctly about running vicuna-7b on an iPhone.<p>Here are a few examples:<p><a href="https:&#x2F;&#x2F;morioh.com&#x2F;p&#x2F;55296932dd8b" rel="nofollow">https:&#x2F;&#x2F;morioh.com&#x2F;p&#x2F;55296932dd8b</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=iQ3Lhy-eD1s">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=iQ3Lhy-eD1s</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35430432" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35430432</a><p>Side note. You need bonkers hardware to run it efficiently. I&#x27;m currently using a 16-core cpu, 128G RAM, a Pcie 4.0 nvme and an RTX 3090. There are ways to run it on less powerful hardware, like 8cores, 64GB RAM, simple ssd and an RTX 3080 or 70, but I happen to have a large corpus of data to process so I went all in.</div><br/><div id="35938984" class="c"><input type="checkbox" id="c-35938984" checked=""/><div class="controls bullet"><span class="by">csdvrx</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938428">parent</a><span>|</span><a href="#35938236">next</a><span>|</span><label class="collapse" for="c-35938984">[-]</label><label class="expand" for="c-35938984">[6 more]</label></div><br/><div class="children"><div class="content">I think the previous comment is more interested in your experience with your large data: what are you doing with it?<p>I have similar hardware at home, so I wonder how reliably you can process simple queries using domain knowledge + logic which work on on mlc-llm, something like &quot;if you can chose the word food, or the word laptop, or the word deodorant, which one do you chose for describing &quot;macbook air&quot;? answer precisely with just the word you chose&quot;<p>If it works, can you upload the weights somewhere? IIRC, vicuna is open source.</div><br/><div id="35941487" class="c"><input type="checkbox" id="c-35941487" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938984">parent</a><span>|</span><a href="#35939102">next</a><span>|</span><label class="collapse" for="c-35941487">[-]</label><label class="expand" for="c-35941487">[1 more]</label></div><br/><div class="children"><div class="content">If these problems are all very similar in structure, then you may not need an LLM. Simple GloVe or W2V may suffice with a dot product.  The. You can plow through a few terabytes by the time the LLM goes through a fraction of that.</div><br/></div></div><div id="35939102" class="c"><input type="checkbox" id="c-35939102" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938984">parent</a><span>|</span><a href="#35941487">prev</a><span>|</span><a href="#35938236">next</a><span>|</span><label class="collapse" for="c-35939102">[-]</label><label class="expand" for="c-35939102">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an online demo of Vicuna-13b where you can test its efficiency:<p><a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;</a></div><br/><div id="35941740" class="c"><input type="checkbox" id="c-35941740" checked=""/><div class="controls bullet"><span class="by">techload</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939102">parent</a><span>|</span><a href="#35939303">next</a><span>|</span><label class="collapse" for="c-35941740">[-]</label><label class="expand" for="c-35941740">[1 more]</label></div><br/><div class="children"><div class="content">After two prompts I was astounded by the innacuracies present in the answers. An they were pretty easy questions.</div><br/></div></div><div id="35939303" class="c"><input type="checkbox" id="c-35939303" checked=""/><div class="controls bullet"><span class="by">csdvrx</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939102">parent</a><span>|</span><a href="#35941740">prev</a><span>|</span><a href="#35938236">next</a><span>|</span><label class="collapse" for="c-35939303">[-]</label><label class="expand" for="c-35939303">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but can you replicate that functionality using llama.cpp?<p>If so, what did you run with main?<p>I haven&#x27;t been able to get an answer, while for the question above, I can get <i>&#x27;I chose the word &quot;laptop&quot;&#x27;</i> with mlc-llm</div><br/><div id="35941117" class="c"><input type="checkbox" id="c-35941117" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939303">parent</a><span>|</span><a href="#35938236">next</a><span>|</span><label class="collapse" for="c-35941117">[-]</label><label class="expand" for="c-35941117">[1 more]</label></div><br/><div class="children"><div class="content">For the tasks I need it the efficiency is similar to the online model. Only slower. I don&#x27;t care for conversational functionality.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35938236" class="c"><input type="checkbox" id="c-35938236" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35938034">prev</a><span>|</span><a href="#35937870">next</a><span>|</span><label class="collapse" for="c-35938236">[-]</label><label class="expand" for="c-35938236">[3 more]</label></div><br/><div class="children"><div class="content">People are extensively using these models (more specifically the finetuned, uncensored ones) for role playing.</div><br/><div id="35941687" class="c"><input type="checkbox" id="c-35941687" checked=""/><div class="controls bullet"><span class="by">irzzy</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938236">parent</a><span>|</span><a href="#35937870">next</a><span>|</span><label class="collapse" for="c-35941687">[-]</label><label class="expand" for="c-35941687">[2 more]</label></div><br/><div class="children"><div class="content">Iâm interested in this. Can you elaborate?</div><br/><div id="35942828" class="c"><input type="checkbox" id="c-35942828" checked=""/><div class="controls bullet"><span class="by">jeron</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35941687">parent</a><span>|</span><a href="#35937870">next</a><span>|</span><label class="collapse" for="c-35942828">[-]</label><label class="expand" for="c-35942828">[1 more]</label></div><br/><div class="children"><div class="content">imagine an RPG where every NPC is running on its own instance of LLaMa 7B or 13B. It makes the game way more immersive with no predefined script</div><br/></div></div></div></div></div></div><div id="35937870" class="c"><input type="checkbox" id="c-35937870" checked=""/><div class="controls bullet"><span class="by">iforgotpassword</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35938236">prev</a><span>|</span><a href="#35939434">next</a><span>|</span><label class="collapse" for="c-35937870">[-]</label><label class="expand" for="c-35937870">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t tried the fine-tuned variants yet, but when I played around with it shortly after the leak, it tended to quickly derail into nonsense when you let it complete sentences or paragraphs, especially when using other languages than English. When I tried to get it to produce Python code, most of the time it wasn&#x27;t even syntactically correct.</div><br/></div></div><div id="35939434" class="c"><input type="checkbox" id="c-35939434" checked=""/><div class="controls bullet"><span class="by">jhbadger</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35937870">prev</a><span>|</span><a href="#35942725">next</a><span>|</span><label class="collapse" for="c-35939434">[-]</label><label class="expand" for="c-35939434">[2 more]</label></div><br/><div class="children"><div class="content">Depends on what &quot;getting stuff done&quot; means. I find 13B models (running on my M1 Mac) useful for playing AI Dungeon-like games -- where you describe the environment and your character and you can play an RPG.</div><br/><div id="35939476" class="c"><input type="checkbox" id="c-35939476" checked=""/><div class="controls bullet"><span class="by">fredguth</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35939434">parent</a><span>|</span><a href="#35942725">next</a><span>|</span><label class="collapse" for="c-35939476">[-]</label><label class="expand" for="c-35939476">[1 more]</label></div><br/><div class="children"><div class="content">GitHub Copilot is (or once was) a 13b model, according to Nat Friedman in the scale.ai interview. (<a href="https:&#x2F;&#x2F;youtu.be&#x2F;lnufceCxwG0" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;lnufceCxwG0</a>)</div><br/></div></div></div></div><div id="35942725" class="c"><input type="checkbox" id="c-35942725" checked=""/><div class="controls bullet"><span class="by">silvervine</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35939434">prev</a><span>|</span><a href="#35937914">next</a><span>|</span><label class="collapse" for="c-35942725">[-]</label><label class="expand" for="c-35942725">[1 more]</label></div><br/><div class="children"><div class="content">I am aiming to connect it to my fav game engine to make a character speak in generative sentences. I am aiming to do this mainly for fun however.</div><br/></div></div><div id="35937914" class="c"><input type="checkbox" id="c-35937914" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35942725">prev</a><span>|</span><a href="#35938680">next</a><span>|</span><label class="collapse" for="c-35937914">[-]</label><label class="expand" for="c-35937914">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just for fun!<p>These local models aren&#x27;t as good as Bard or GPT-4.</div><br/><div id="35937990" class="c"><input type="checkbox" id="c-35937990" checked=""/><div class="controls bullet"><span class="by">happycube</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35937914">parent</a><span>|</span><a href="#35938680">next</a><span>|</span><label class="collapse" for="c-35937990">[-]</label><label class="expand" for="c-35937990">[7 more]</label></div><br/><div class="children"><div class="content">There are two major advantages though - you can retrain them, and they&#x27;re not on the guardrails that the commercial models have.</div><br/><div id="35938231" class="c"><input type="checkbox" id="c-35938231" checked=""/><div class="controls bullet"><span class="by">gre</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35937990">parent</a><span>|</span><a href="#35938680">next</a><span>|</span><label class="collapse" for="c-35938231">[-]</label><label class="expand" for="c-35938231">[6 more]</label></div><br/><div class="children"><div class="content">I tried to prompt vicuna to tell me a joke about gay people and it refused. Some of the guardrails are still in there.</div><br/><div id="35938928" class="c"><input type="checkbox" id="c-35938928" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938231">parent</a><span>|</span><a href="#35938442">next</a><span>|</span><label class="collapse" for="c-35938928">[-]</label><label class="expand" for="c-35938928">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because vicuna is fine-tuned on chatGPT answers. LLaMa will not do this, but  LLaMa-based models fine tuned with chatGPT answers will.</div><br/><div id="35944338" class="c"><input type="checkbox" id="c-35944338" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938928">parent</a><span>|</span><a href="#35938442">next</a><span>|</span><label class="collapse" for="c-35944338">[-]</label><label class="expand" for="c-35944338">[1 more]</label></div><br/><div class="children"><div class="content">Just curious why don&#x27;t they training except denial responses. Is Copying ChatGPT ethics a purpose?</div><br/></div></div></div></div><div id="35938442" class="c"><input type="checkbox" id="c-35938442" checked=""/><div class="controls bullet"><span class="by">occz</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938231">parent</a><span>|</span><a href="#35938928">prev</a><span>|</span><a href="#35938680">next</a><span>|</span><label class="collapse" for="c-35938442">[-]</label><label class="expand" for="c-35938442">[3 more]</label></div><br/><div class="children"><div class="content">Did you use the censored or the uncensored variant?</div><br/><div id="35938701" class="c"><input type="checkbox" id="c-35938701" checked=""/><div class="controls bullet"><span class="by">gre</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938442">parent</a><span>|</span><a href="#35938680">next</a><span>|</span><label class="collapse" for="c-35938701">[-]</label><label class="expand" for="c-35938701">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just a random one from huggingface. I will look for the uncensored one later. Thanks, I think.</div><br/><div id="35938777" class="c"><input type="checkbox" id="c-35938777" checked=""/><div class="controls bullet"><span class="by">occz</span><span>|</span><a href="#35937799">root</a><span>|</span><a href="#35938701">parent</a><span>|</span><a href="#35938680">next</a><span>|</span><label class="collapse" for="c-35938777">[-]</label><label class="expand" for="c-35938777">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re welcome. I can&#x27;t vouch for them though, as I haven&#x27;t tried them, I&#x27;ve merely heard about them.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="35938680" class="c"><input type="checkbox" id="c-35938680" checked=""/><div class="controls bullet"><span class="by">Zetobal</span><span>|</span><a href="#35937799">parent</a><span>|</span><a href="#35937914">prev</a><span>|</span><a href="#35939532">next</a><span>|</span><label class="collapse" for="c-35938680">[-]</label><label class="expand" for="c-35938680">[1 more]</label></div><br/><div class="children"><div class="content">We run some llamas to analyze user content.</div><br/></div></div></div></div><div id="35939532" class="c"><input type="checkbox" id="c-35939532" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#35937799">prev</a><span>|</span><a href="#35939484">next</a><span>|</span><label class="collapse" for="c-35939532">[-]</label><label class="expand" for="c-35939532">[37 more]</label></div><br/><div class="children"><div class="content">Why does AMD or Intel not release a medium performant GPU with minimum 128gb of memory for a good consumer price. These models require lots of memory to &#x27;single&#x27; pass an operation. Throughput could be bit slower.
A 1080 Nvidia with 256gb of memory would run all these models fast right?
Or am I forgetting something here.</div><br/><div id="35940751" class="c"><input type="checkbox" id="c-35940751" checked=""/><div class="controls bullet"><span class="by">elabajaba</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35940863">next</a><span>|</span><label class="collapse" for="c-35940751">[-]</label><label class="expand" for="c-35940751">[4 more]</label></div><br/><div class="children"><div class="content">Because they can&#x27;t do that for a &quot;good consumer price&quot;.<p>If you want more than ~48GB, you&#x27;re looking at HBM which is extremely expensive (HBM chips are very expensive, packaging+interposer is extremely expensive, designing and producing a new GPU is expensive).<p>Normal GPUs are limited by both their bus width (wider bus = more pins = harder to design, more expensive to produce, and increases power consumption), and GDDR6(x) (which maxes out at 2GB&#x2F;chip currently), so on a 384bit bus (4090&#x2F;7900xtx, don&#x27;t expect anyone to make a 512bit busses anymore) you need 12x2GB (GDDR6 uses 32 pins per package) which gives you 24GB. You can double the memory capacity to 48GB, but that requires putting the chips on the back of the GPU which leads to a bunch of cooling issues (and GDDR6 is expensive).<p>Of course, even if they did all that they&#x27;re selling expensive GPUs to a small niche market and cannibalizing sales of their own high end products (and even if AMD somehow managed to magic up a 128GB gpu for $700 people still wouldn&#x27;t buy it because so much of the ML software is CUDA only).</div><br/><div id="35940795" class="c"><input type="checkbox" id="c-35940795" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940751">parent</a><span>|</span><a href="#35940863">next</a><span>|</span><label class="collapse" for="c-35940795">[-]</label><label class="expand" for="c-35940795">[3 more]</label></div><br/><div class="children"><div class="content">3090 has a lot of vram chips on the back though</div><br/><div id="35941021" class="c"><input type="checkbox" id="c-35941021" checked=""/><div class="controls bullet"><span class="by">elabajaba</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940795">parent</a><span>|</span><a href="#35940863">next</a><span>|</span><label class="collapse" for="c-35941021">[-]</label><label class="expand" for="c-35941021">[2 more]</label></div><br/><div class="children"><div class="content">And because of it there were issues with the vram overheating in memory intensive workloads, and on some GPUs the vram even separated off the board.<p><a href="https:&#x2F;&#x2F;www.igorslab.de&#x2F;en&#x2F;looming-pads-and-too-hot-gddrx6-memory-siliconitis-on-a-geforce-rtx-3080&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.igorslab.de&#x2F;en&#x2F;looming-pads-and-too-hot-gddrx6-m...</a></div><br/><div id="35942383" class="c"><input type="checkbox" id="c-35942383" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35941021">parent</a><span>|</span><a href="#35940863">next</a><span>|</span><label class="collapse" for="c-35942383">[-]</label><label class="expand" for="c-35942383">[1 more]</label></div><br/><div class="children"><div class="content">Yes I have one that has one fried chip. Once I gave another a Bykski water block with backplate it&#x27;s been no issue for a year</div><br/></div></div></div></div></div></div></div></div><div id="35940863" class="c"><input type="checkbox" id="c-35940863" checked=""/><div class="controls bullet"><span class="by">Tuna-Fish</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35940751">prev</a><span>|</span><a href="#35939664">next</a><span>|</span><label class="collapse" for="c-35940863">[-]</label><label class="expand" for="c-35940863">[6 more]</label></div><br/><div class="children"><div class="content">It would not be trivial to do.<p>GDDR achieves higher speeds than normal DDR mainly by specifying much tighter tolerances on the electrical interface, and using wider interface to the memory chips. This means that using commodity GDDR (which is the only fast DRAM that will be reasonably cheap), you have fairly strict limitations on the maximum amount of RAM your can use with the same GPUs that are manufactured for consumer use. (Typically, at most 4x difference between the lowest-end reasonable configuration and the highest-end one, 2x from higher density modules and 2x from using clamshell memory configuration, although often you only have one type of module for a new memory interface generation.)<p>If the product requires either a new memory or GPU die configuration, it&#x27;s cost will be very high.<p>The only type of memory that can support very different VRAM sizes for an efficiently utilized bus of the same size is HBM, and so far that is limited to the very high end.</div><br/><div id="35941434" class="c"><input type="checkbox" id="c-35941434" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940863">parent</a><span>|</span><a href="#35940970">next</a><span>|</span><label class="collapse" for="c-35941434">[-]</label><label class="expand" for="c-35941434">[2 more]</label></div><br/><div class="children"><div class="content">Anandtech has an article on the GDDR6X variant[1] that NVIDIA has in their 3000-cards, where they use a more complex encoding to transmit two bits per clock edge.<p>I hadn&#x27;t realized just how insane the bandwidth on the higher-ends cards are, the 3090 being just shy of 1 TB&#x2F;s, yes, one terrabyte per second...<p>For comparison a couple of DDR5 sticks[2] will just get you north of 70GB&#x2F;s...<p>[1]: <a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;15978&#x2F;micron-spills-on-gddr6x-pam4-signaling-for-higher-rates-coming-to-nvidias-rtx-3090" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;15978&#x2F;micron-spills-on-gddr6x...</a><p>[2]: <a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;17269&#x2F;ddr5-demystified-feat-samsung-ddr5-4800-ranks-dpcs-do-manufacturers-matter&#x2F;2" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;17269&#x2F;ddr5-demystified-feat-s...</a></div><br/><div id="35942994" class="c"><input type="checkbox" id="c-35942994" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35941434">parent</a><span>|</span><a href="#35940970">next</a><span>|</span><label class="collapse" for="c-35942994">[-]</label><label class="expand" for="c-35942994">[1 more]</label></div><br/><div class="children"><div class="content">The accelerators aimed squarely at datacenter use rather than gaming are even more ridiculous, Nvidias H100 has 80GB of memory running at 3.35TB&#x2F;s.</div><br/></div></div></div></div><div id="35940970" class="c"><input type="checkbox" id="c-35940970" checked=""/><div class="controls bullet"><span class="by">q7xvh97o2pDhNrh</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940863">parent</a><span>|</span><a href="#35941434">prev</a><span>|</span><a href="#35939664">next</a><span>|</span><label class="collapse" for="c-35940970">[-]</label><label class="expand" for="c-35940970">[3 more]</label></div><br/><div class="children"><div class="content">Do you happen to know where Apple&#x27;s integrated approach falls on this spectrum?<p>I was actually wondering about this the other day. A fully maxed out Mac Studio is about $6K, and it comes with a &quot;64-core GPU&quot; and &quot;128GB integrated memory&quot; (whatever any of that means). Would that be enough to run a decent Llama?</div><br/><div id="35941678" class="c"><input type="checkbox" id="c-35941678" checked=""/><div class="controls bullet"><span class="by">cudder</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940970">parent</a><span>|</span><a href="#35944136">next</a><span>|</span><label class="collapse" for="c-35941678">[-]</label><label class="expand" for="c-35941678">[1 more]</label></div><br/><div class="children"><div class="content">The Mac&#x27;s &quot;integrated memory&quot; means it&#x27;s shared between the CPU and GPU. So the GPU can address all of that and you can load giant (by current consumer GPU standards) models. I have no idea how it actually performs though.</div><br/></div></div><div id="35944136" class="c"><input type="checkbox" id="c-35944136" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940970">parent</a><span>|</span><a href="#35941678">prev</a><span>|</span><a href="#35939664">next</a><span>|</span><label class="collapse" for="c-35944136">[-]</label><label class="expand" for="c-35944136">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s certainly enough to run a decent Llama, but hardly the most cost-effective. Apple&#x27;s approach falls between the low-bandwith Intel&#x2F;AMD laptops and the high-bandwith PCIe HPC components. In a way it&#x27;s trapped between two markets - ultra-cheap Android&#x2F;Windows hardware with 4-8gb of RAM that can still do AI inferencing, and ultra-expensive GPGPU setups that are designed to melt these workloads.<p>The genial thing to say is that it performs very favorable against other consumer inferencing hardware. The numbers get ugly fast once you start throwing money at the problem, though.</div><br/></div></div></div></div></div></div><div id="35939664" class="c"><input type="checkbox" id="c-35939664" checked=""/><div class="controls bullet"><span class="by">0xcde4c3db</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35940863">prev</a><span>|</span><a href="#35939609">next</a><span>|</span><label class="collapse" for="c-35939664">[-]</label><label class="expand" for="c-35939664">[6 more]</label></div><br/><div class="children"><div class="content">Probably because if they take that exact same GPU+VRAM configuration and slap it on a rackmount-optimized board, they can charge AI companies 5-10x the price for it.</div><br/><div id="35940682" class="c"><input type="checkbox" id="c-35940682" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35939664">parent</a><span>|</span><a href="#35940501">next</a><span>|</span><label class="collapse" for="c-35940682">[-]</label><label class="expand" for="c-35940682">[3 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t even offer that much VRAM on cards aimed at those price-insensitive customers, Nvidias current lineup maxes out at 48GB for GDDR-based models or 80GB for HBM-based models. Even if money is no object there&#x27;s still practical engineering limits on how much memory they can put on a card without sacrificing bandwidth.</div><br/><div id="35942264" class="c"><input type="checkbox" id="c-35942264" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940682">parent</a><span>|</span><a href="#35940501">next</a><span>|</span><label class="collapse" for="c-35942264">[-]</label><label class="expand" for="c-35942264">[2 more]</label></div><br/><div class="children"><div class="content">So when is Micron due to introduce higher density dram chips? Nvidia is limited to what&#x27;s available from RAM manufacturers there.</div><br/><div id="35942773" class="c"><input type="checkbox" id="c-35942773" checked=""/><div class="controls bullet"><span class="by">elabajaba</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35942264">parent</a><span>|</span><a href="#35940501">next</a><span>|</span><label class="collapse" for="c-35942773">[-]</label><label class="expand" for="c-35942773">[1 more]</label></div><br/><div class="children"><div class="content">Samsung has announced GDDR6W which doubles the density, but also doubles the pin count (so same amount of VRAM in total, but takes up half the space on the board). I&#x27;m assuming that companies will save any more density improvements for GDDR7 which is rumored to be used in next-gen GPUs (probably 2nd half 2024).</div><br/></div></div></div></div></div></div><div id="35940501" class="c"><input type="checkbox" id="c-35940501" checked=""/><div class="controls bullet"><span class="by">vegabook</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35939664">parent</a><span>|</span><a href="#35940682">prev</a><span>|</span><a href="#35944359">next</a><span>|</span><label class="collapse" for="c-35940501">[-]</label><label class="expand" for="c-35940501">[1 more]</label></div><br/><div class="children"><div class="content">this is where the new third player, Intel, can (if it can tear itself away from identical behaviour in the consumer&#x2F;server CPU market) hopefully break the duopoly. Love to see a 32 or 64GB card from Intel. Their software stack on Linux is competent enough (unlike the dumpster fire that is AMD&#x27;s ROCm).</div><br/></div></div><div id="35944359" class="c"><input type="checkbox" id="c-35944359" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35939664">parent</a><span>|</span><a href="#35940501">prev</a><span>|</span><a href="#35939609">next</a><span>|</span><label class="collapse" for="c-35944359">[-]</label><label class="expand" for="c-35944359">[1 more]</label></div><br/><div class="children"><div class="content">This must be the reason. AI homelab market is too small while enterprise market will become extremely big.</div><br/></div></div></div></div><div id="35939609" class="c"><input type="checkbox" id="c-35939609" checked=""/><div class="controls bullet"><span class="by">hackernudes</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35939664">prev</a><span>|</span><a href="#35939885">next</a><span>|</span><label class="collapse" for="c-35939609">[-]</label><label class="expand" for="c-35939609">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think there was a market for it before LLMs. Still might not be (especially if they don&#x27;t want to cannibalize data center products). Also, they might have hardware constraints. I wouldn&#x27;t be that surprised if we see some high ram consumer GPUs in the future, though.<p>It won&#x27;t work out unless it becomes common to run LLMs locally. Kind of a chicken-and-egg problem so I hope they try it!</div><br/><div id="35939859" class="c"><input type="checkbox" id="c-35939859" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35939609">parent</a><span>|</span><a href="#35941072">next</a><span>|</span><label class="collapse" for="c-35939859">[-]</label><label class="expand" for="c-35939859">[5 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think there was a market for it before LLMs.<p>At $work CGI assets sometimes grow pretty big and throwing more VRAM at the problem would be easier than optimizing the scenes in the middle of the workflow. They <i>can</i> be optimized, but that often makes it less ergonomic to work with them.<p>Perhaps asset-streaming (nanite&amp;co) will make this less of an issue, but that&#x27;s also fairly new.<p>Do LLM implementations already stream the weights layer by layer or in whichever order they&#x27;re doing the evaluation or is PCIe bandwidth too limited for that?</div><br/><div id="35940814" class="c"><input type="checkbox" id="c-35940814" checked=""/><div class="controls bullet"><span class="by">elabajaba</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35939859">parent</a><span>|</span><a href="#35940101">next</a><span>|</span><label class="collapse" for="c-35940814">[-]</label><label class="expand" for="c-35940814">[2 more]</label></div><br/><div class="children"><div class="content">AMD had the Radeon pro SSG that let you attach 1TB of pcie3 nvme SSDs directly to the GPU, but no one bought them and afaik they were basically unobtainable on the consumer market.<p>Also asset streaming has been a thing for like 20 years now in gaming, it&#x27;s not really a new thing. Nanite&#x27;s big thing is that it gets you perfect LODs without having to pre-create them and manually tweak them (eg. how far away does the LOD transition happen, what&#x27;s the lowest LOD before it disappears, etc)</div><br/><div id="35940886" class="c"><input type="checkbox" id="c-35940886" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940814">parent</a><span>|</span><a href="#35940101">next</a><span>|</span><label class="collapse" for="c-35940886">[-]</label><label class="expand" for="c-35940886">[1 more]</label></div><br/><div class="children"><div class="content">Loading assets JIT for the next frame from NVMe hasn&#x27;t been a thing for 20 years though. Different kinds of latency floors.<p>What I was asking is whether LLM inference can be structured in such a way that only a fraction of the weight is needed at a time and then the next ones can be loaded JIT as the processing pipeline advances.</div><br/></div></div></div></div><div id="35940101" class="c"><input type="checkbox" id="c-35940101" checked=""/><div class="controls bullet"><span class="by">tpetry</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35939859">parent</a><span>|</span><a href="#35940814">prev</a><span>|</span><a href="#35941072">next</a><span>|</span><label class="collapse" for="c-35940101">[-]</label><label class="expand" for="c-35940101">[2 more]</label></div><br/><div class="children"><div class="content">But you are not the home user target audience. They want to sell you the more expensive workstation or server models.</div><br/><div id="35940525" class="c"><input type="checkbox" id="c-35940525" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940101">parent</a><span>|</span><a href="#35941072">next</a><span>|</span><label class="collapse" for="c-35940525">[-]</label><label class="expand" for="c-35940525">[1 more]</label></div><br/><div class="children"><div class="content">Even an A6000 tops out at 48GB while you can attach terabytes of RAM to server-class CPUs.</div><br/></div></div></div></div></div></div></div></div><div id="35939885" class="c"><input type="checkbox" id="c-35939885" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35939609">prev</a><span>|</span><a href="#35939640">next</a><span>|</span><label class="collapse" for="c-35939885">[-]</label><label class="expand" for="c-35939885">[1 more]</label></div><br/><div class="children"><div class="content">Releasing a new model takes time, and itâs unclear how large the consumer market would actually be. Maybe theyâre working on it right now.</div><br/></div></div><div id="35939640" class="c"><input type="checkbox" id="c-35939640" checked=""/><div class="controls bullet"><span class="by">duxup</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35939885">prev</a><span>|</span><a href="#35940745">next</a><span>|</span><label class="collapse" for="c-35939640">[-]</label><label class="expand" for="c-35939640">[1 more]</label></div><br/><div class="children"><div class="content">&gt;for a good consumer price<p>Was there a consumer market for them until recently?</div><br/></div></div><div id="35940745" class="c"><input type="checkbox" id="c-35940745" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35939640">prev</a><span>|</span><a href="#35941090">next</a><span>|</span><label class="collapse" for="c-35940745">[-]</label><label class="expand" for="c-35940745">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why does AMD or Intel not release a medium performant GPU with minimum 128gb of memory for a good consumer price.<p>They do. Well, not âmedium performantâ, but for VRAM-bound tasks theyâd still be an improvement over CPUs if you could use them â iGPUs use main memory.<p>What they donât have is support for them for popular GPGPU frameworks (though there was a third party CUDA-for-Intel-iGPUs a while ago.)</div><br/></div></div><div id="35941090" class="c"><input type="checkbox" id="c-35941090" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35940745">prev</a><span>|</span><a href="#35940772">next</a><span>|</span><label class="collapse" for="c-35941090">[-]</label><label class="expand" for="c-35941090">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly what the next generation of &#x27;accelerators&#x27; will be like.<p>Whether it will be co-located with a GPU for consumer hardware remains to be seen.<p>The thing to determine is how essential running LLMs locally is for consumers.<p>BigTech is pushing hard to make their clouds the only place to run LLMs unfortunately, so unless there is a killer app that is just better locally (like games were for GPUs), this might not change.</div><br/><div id="35941287" class="c"><input type="checkbox" id="c-35941287" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35941090">parent</a><span>|</span><a href="#35940772">next</a><span>|</span><label class="collapse" for="c-35941287">[-]</label><label class="expand" for="c-35941287">[2 more]</label></div><br/><div class="children"><div class="content">&gt; unless there is a killer app that is just better locally<p>Therapy &amp; relationship bots, like the movie &#x27;Her&#x27;. It&#x27;s ugly, but it&#x27;s coming.</div><br/><div id="35942056" class="c"><input type="checkbox" id="c-35942056" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35941287">parent</a><span>|</span><a href="#35940772">next</a><span>|</span><label class="collapse" for="c-35942056">[-]</label><label class="expand" for="c-35942056">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no technical reason it has to be run locally.<p>Massive privacy implications for sure, but people do consume all sorts of adult material online.<p>Games though, no one has been able to make it work as well as local so far.</div><br/></div></div></div></div></div></div><div id="35940772" class="c"><input type="checkbox" id="c-35940772" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35941090">prev</a><span>|</span><a href="#35940443">next</a><span>|</span><label class="collapse" for="c-35940772">[-]</label><label class="expand" for="c-35940772">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a type of DMA for GPUs to access NVMe on the motherboard, IIRC. Perhaps that is a better solution here?<p><a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;gpudirect-storage&#x2F;" rel="nofollow">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;gpudirect-storage&#x2F;</a></div><br/><div id="35941293" class="c"><input type="checkbox" id="c-35941293" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940772">parent</a><span>|</span><a href="#35940443">next</a><span>|</span><label class="collapse" for="c-35941293">[-]</label><label class="expand" for="c-35941293">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t pci-e latency dramatically higher than onboard vram?</div><br/></div></div></div></div><div id="35940443" class="c"><input type="checkbox" id="c-35940443" checked=""/><div class="controls bullet"><span class="by">andromeduck</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35940772">prev</a><span>|</span><a href="#35941621">next</a><span>|</span><label class="collapse" for="c-35940443">[-]</label><label class="expand" for="c-35940443">[1 more]</label></div><br/><div class="children"><div class="content">Because then memory would be 90% of the BOM.</div><br/></div></div><div id="35941621" class="c"><input type="checkbox" id="c-35941621" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35940443">prev</a><span>|</span><a href="#35939895">next</a><span>|</span><label class="collapse" for="c-35941621">[-]</label><label class="expand" for="c-35941621">[1 more]</label></div><br/><div class="children"><div class="content">The margins on VRAM are pretty bad for them since they don&#x27;t manufacture it themselves. And every memory module they add needs additional power delivery and memory controller muscle to drive, so adding that memory is going to raise the cost of the card significantly. Most games and consumer workloads won&#x27;t use all that extra memory.<p>Keep in mind video cards don&#x27;t use the same kind of RAM as consumer CPUs do, they typically use GDDR or HBM.</div><br/></div></div><div id="35939895" class="c"><input type="checkbox" id="c-35939895" checked=""/><div class="controls bullet"><span class="by">Kye</span><span>|</span><a href="#35939532">parent</a><span>|</span><a href="#35941621">prev</a><span>|</span><a href="#35939484">next</a><span>|</span><label class="collapse" for="c-35939895">[-]</label><label class="expand" for="c-35939895">[3 more]</label></div><br/><div class="children"><div class="content">GDDR probably hasn&#x27;t seen the same cost reduction benefits of volume DDR has.</div><br/><div id="35940167" class="c"><input type="checkbox" id="c-35940167" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35939895">parent</a><span>|</span><a href="#35939484">next</a><span>|</span><label class="collapse" for="c-35940167">[-]</label><label class="expand" for="c-35940167">[2 more]</label></div><br/><div class="children"><div class="content">One question I have is: can they use cheaper kinds of RAM and still be perfectly usable for large ML models? They could put 4GB of GDDR and 128GB of cheap RAM maybe? I do realize as others are saying, this would be a new kind of card so they will need time to develop it. But would this work?</div><br/><div id="35940477" class="c"><input type="checkbox" id="c-35940477" checked=""/><div class="controls bullet"><span class="by">andromeduck</span><span>|</span><a href="#35939532">root</a><span>|</span><a href="#35940167">parent</a><span>|</span><a href="#35939484">next</a><span>|</span><label class="collapse" for="c-35940477">[-]</label><label class="expand" for="c-35940477">[1 more]</label></div><br/><div class="children"><div class="content">Not without a redesigned memory controller or one off chip. You&#x27;d probably just want the host&#x27;s memory to be directly accessible over PCIE or something faster like NVLINK. Such solutions already exist just not in the consumer space.</div><br/></div></div></div></div></div></div></div></div><div id="35939484" class="c"><input type="checkbox" id="c-35939484" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#35939532">prev</a><span>|</span><a href="#35938120">next</a><span>|</span><label class="collapse" for="c-35939484">[-]</label><label class="expand" for="c-35939484">[4 more]</label></div><br/><div class="children"><div class="content">This update is pretty exciting, Iâm gonna try running a large model (65B) with a 3090. I have ran a ton of local LLM but the hardest part is finding out the prompt structure. I wish there is some sort of centralized data base that explains it.</div><br/><div id="35939568" class="c"><input type="checkbox" id="c-35939568" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35939484">parent</a><span>|</span><a href="#35939792">next</a><span>|</span><label class="collapse" for="c-35939568">[-]</label><label class="expand" for="c-35939568">[1 more]</label></div><br/><div class="children"><div class="content">Tell us how it goes! Try different numbers of layers if needed.<p>A good place to dig for prompt structures may be the &#x27;text-generation-webui&#x27; commit log. For example <a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui&#x2F;commit&#x2F;334486f527bc97f61eb3264def4e03a0dab9b369">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui&#x2F;commit&#x2F;33...</a></div><br/></div></div><div id="35939792" class="c"><input type="checkbox" id="c-35939792" checked=""/><div class="controls bullet"><span class="by">guardiangod</span><span>|</span><a href="#35939484">parent</a><span>|</span><a href="#35939568">prev</a><span>|</span><a href="#35938120">next</a><span>|</span><label class="collapse" for="c-35939792">[-]</label><label class="expand" for="c-35939792">[2 more]</label></div><br/><div class="children"><div class="content">I got the alpaca 65B GGML model to run on my 64GB ram laptop. No GPU required if you can tolerate the 1 token per 3 seconds rate.</div><br/><div id="35940377" class="c"><input type="checkbox" id="c-35940377" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#35939484">root</a><span>|</span><a href="#35939792">parent</a><span>|</span><a href="#35938120">next</a><span>|</span><label class="collapse" for="c-35940377">[-]</label><label class="expand" for="c-35940377">[1 more]</label></div><br/><div class="children"><div class="content">Supposedly the new update with GPU offloading will bring that up to 10 tokens per second! 1 token per second is painfully slow, thatâs about 30s for a sentence.</div><br/></div></div></div></div></div></div><div id="35938120" class="c"><input type="checkbox" id="c-35938120" checked=""/><div class="controls bullet"><span class="by">peatmoss</span><span>|</span><a href="#35939484">prev</a><span>|</span><a href="#35938117">next</a><span>|</span><label class="collapse" for="c-35938120">[-]</label><label class="expand" for="c-35938120">[8 more]</label></div><br/><div class="children"><div class="content">From skimming, it looks like this approach requires CUDA and thus is Nvidia only.<p>Anyone have a recommended guide for AMD &#x2F; Intel GPUs? I gather the 4 bit quantization is the special sauce for CUDA, but Iâd guess thereâd be something comparable for not-CUDA?</div><br/><div id="35938160" class="c"><input type="checkbox" id="c-35938160" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35938120">parent</a><span>|</span><a href="#35938245">next</a><span>|</span><label class="collapse" for="c-35938160">[-]</label><label class="expand" for="c-35938160">[6 more]</label></div><br/><div class="children"><div class="content">4-bit quantization is to reduce the amount of VRAM required to run the model. You can run it 100% on CPU if you don&#x27;t have CUDA. I&#x27;m not aware of any AMD equivalent yet.</div><br/><div id="35938232" class="c"><input type="checkbox" id="c-35938232" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#35938120">root</a><span>|</span><a href="#35938160">parent</a><span>|</span><a href="#35938245">next</a><span>|</span><label class="collapse" for="c-35938232">[-]</label><label class="expand" for="c-35938232">[5 more]</label></div><br/><div class="children"><div class="content">Looks like there are several projects that implement the CUDA interface for various other compute systems, e.g.:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ROCm-Developer-Tools&#x2F;HIPIFY&#x2F;blob&#x2F;master&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;ROCm-Developer-Tools&#x2F;HIPIFY&#x2F;blob&#x2F;master&#x2F;R...</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;hughperkins&#x2F;coriander">https:&#x2F;&#x2F;github.com&#x2F;hughperkins&#x2F;coriander</a><p>I have zero experience with these, though.</div><br/><div id="35939516" class="c"><input type="checkbox" id="c-35939516" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#35938120">root</a><span>|</span><a href="#35938232">parent</a><span>|</span><a href="#35938247">next</a><span>|</span><label class="collapse" for="c-35939516">[-]</label><label class="expand" for="c-35939516">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Democratizing AI with PyTorch Foundation and ROCmâ¢ support for PyTorch&quot; (2023)
<a href="https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;democratizing-ai-with-pytorch&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;democratizing-ai-with-pytorch&#x2F;</a> :<p>&gt; <i>AMD, along with key PyTorch codebase developers (including those at Meta AI), delivered a set of updates to the ROCmâ¢ open software ecosystem that brings stable support for AMD Instinctâ¢ accelerators as well as many Radeonâ¢ GPUs. This now gives PyTorch developers the ability to build their next great AI solutions leveraging AMD GPU accelerators &amp; ROCm. The support from PyTorch community in identifying gaps, prioritizing key updates, providing feedback for performance optimizing and supporting our journey from âBetaâ to âStableâ was immensely helpful and we deeply appreciate the strong collaboration between the two teams at AMD and PyTorch. The move for ROCm support from âBetaâ to âStableâ came in the PyTorch 1.12 release (June 2022)</i><p>&gt; [...] <i>PyTorch ecosystem libraries like TorchText (Text classification), TorchRec (libraries for recommender systems - RecSys), TorchVision (Computer Vision), TorchAudio (audio and signal processing) are fully supported since ROCm 5.1 and upstreamed with PyTorch 1.12.</i><p>&gt; <i>Key libraries provided with the ROCm software stack including MIOpen (Convolution models), RCCL (ROCm Collective Communications) and rocBLAS (BLAS for transformers) were further optimized to offer new potential efficiencies and higher performance.</i><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34399633" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34399633</a> :<p>&gt;&gt; <i>AMD ROcm supports Pytorch, TensorFlow, MlOpen, rocBLAS on NVIDIA and AMD GPUs: <a href="https:&#x2F;&#x2F;rocmdocs.amd.com&#x2F;en&#x2F;latest&#x2F;Deep_learning&#x2F;Deep-learning.html" rel="nofollow">https:&#x2F;&#x2F;rocmdocs.amd.com&#x2F;en&#x2F;latest&#x2F;Deep_learning&#x2F;Deep-learni...</a> </i></div><br/><div id="35940752" class="c"><input type="checkbox" id="c-35940752" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#35938120">root</a><span>|</span><a href="#35939516">parent</a><span>|</span><a href="#35938247">next</a><span>|</span><label class="collapse" for="c-35940752">[-]</label><label class="expand" for="c-35940752">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;intel-extension-for-pytorch">https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;intel-extension-for-pytorch</a> :<p>&gt; <i>IntelÂ® Extension for PyTorch extends PyTorch with up-to-date features optimizations for an extra performance boost on Intel hardware. Optimizations take advantage of AVX-512 Vector Neural Network Instructions (AVX512 VNNI) and IntelÂ® Advanced Matrix Extensions (IntelÂ® AMX) on Intel CPUs as well as Intel Xe Matrix Extensions (XMX) AI engines on Intel discrete GPUs. Moreover, through PyTorch xpu device, IntelÂ® Extension for PyTorch provides easy GPU acceleration for Intel discrete GPUs with PyTorch</i><p><a href="https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;celebrate-pytorch-2.0&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;celebrate-pytorch-2.0&#x2F;</a> (2023) :<p>&gt; <i>As part of the PyTorch 2.0 compilation stack, TorchInductor CPU backend optimization brings notable performance improvements via graph compilation over the PyTorch eager mode.</i><p>&gt; <i>The TorchInductor CPU backend is sped up by leveraging the technologies from the IntelÂ® Extension for PyTorch for Conv&#x2F;GEMM ops with post-op fusion and weight prepacking, and PyTorch ATen CPU kernels for memory-bound ops with explicit vectorization on top of OpenMP-based thread parallelization</i><p>DLRS Deep Learning Reference Stack: <a href="https:&#x2F;&#x2F;intel.github.io&#x2F;stacks&#x2F;dlrs&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;intel.github.io&#x2F;stacks&#x2F;dlrs&#x2F;index.html</a></div><br/></div></div></div></div><div id="35938247" class="c"><input type="checkbox" id="c-35938247" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35938120">root</a><span>|</span><a href="#35938232">parent</a><span>|</span><a href="#35939516">prev</a><span>|</span><a href="#35938245">next</a><span>|</span><label class="collapse" for="c-35938247">[-]</label><label class="expand" for="c-35938247">[2 more]</label></div><br/><div class="children"><div class="content">exciting! maybe we will see that land in llama.cpp eventually, who knows!</div><br/><div id="35938813" class="c"><input type="checkbox" id="c-35938813" checked=""/><div class="controls bullet"><span class="by">juliangoldsmith</span><span>|</span><a href="#35938120">root</a><span>|</span><a href="#35938247">parent</a><span>|</span><a href="#35938245">next</a><span>|</span><label class="collapse" for="c-35938813">[-]</label><label class="expand" for="c-35938813">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp has CLBlast support now, though I haven&#x27;t used it yet.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35938117" class="c"><input type="checkbox" id="c-35938117" checked=""/><div class="controls bullet"><span class="by">marcopicentini</span><span>|</span><a href="#35938120">prev</a><span>|</span><a href="#35938049">next</a><span>|</span><label class="collapse" for="c-35938117">[-]</label><label class="expand" for="c-35938117">[12 more]</label></div><br/><div class="children"><div class="content">What do you use to host these models (like Vicuna, Dolly etc) on your own server and expose them using HTTP REST API? 
Is there an Heroku-like for LLM models?<p>I am looking for an open source models to do text summarization. Open AI is too expensive for my use case because I need to pass lots of tokens.</div><br/><div id="35938250" class="c"><input type="checkbox" id="c-35938250" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#35938117">parent</a><span>|</span><a href="#35938170">next</a><span>|</span><label class="collapse" for="c-35938250">[-]</label><label class="expand" for="c-35938250">[8 more]</label></div><br/><div class="children"><div class="content">These days I use FastChat: <a href="https:&#x2F;&#x2F;github.com&#x2F;lm-sys&#x2F;FastChat">https:&#x2F;&#x2F;github.com&#x2F;lm-sys&#x2F;FastChat</a><p>Itâs not based on llama.cpp but huggingface transformers but can also run on CPU.<p>It works well, can be distributed and very conveniently provide the same REST API than OpenAI GPT.</div><br/><div id="35938801" class="c"><input type="checkbox" id="c-35938801" checked=""/><div class="controls bullet"><span class="by">itake</span><span>|</span><a href="#35938117">root</a><span>|</span><a href="#35938250">parent</a><span>|</span><a href="#35938170">next</a><span>|</span><label class="collapse" for="c-35938801">[-]</label><label class="expand" for="c-35938801">[7 more]</label></div><br/><div class="children"><div class="content">Do you know how well it performs compared to llama.cpp?</div><br/><div id="35938843" class="c"><input type="checkbox" id="c-35938843" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35938117">root</a><span>|</span><a href="#35938801">parent</a><span>|</span><a href="#35938170">next</a><span>|</span><label class="collapse" for="c-35938843">[-]</label><label class="expand" for="c-35938843">[6 more]</label></div><br/><div class="children"><div class="content">my understanding is that the engine used (pytorch transformers library) is still faster than llama.cpp with 100% of layers running on the GPU.</div><br/><div id="35940009" class="c"><input type="checkbox" id="c-35940009" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#35938117">root</a><span>|</span><a href="#35938843">parent</a><span>|</span><a href="#35939153">next</a><span>|</span><label class="collapse" for="c-35940009">[-]</label><label class="expand" for="c-35940009">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the Huggingface transformers library which is implemented in pytorch.<p>In terms of speed, yes running fp16 will indeed be faster with vanilla gpu setup. However most people are running 4bit quantized versions, and the GPU quantization landscape as been a mess (GPTQ-for-llama project). llama.cpp has taken a totally different approach, and it looks like they are currently able to match native GPU perf via cuBLAS with much less effort and brittleness.</div><br/></div></div><div id="35939153" class="c"><input type="checkbox" id="c-35939153" checked=""/><div class="controls bullet"><span class="by">itake</span><span>|</span><a href="#35938117">root</a><span>|</span><a href="#35938843">parent</a><span>|</span><a href="#35940009">prev</a><span>|</span><a href="#35938170">next</a><span>|</span><label class="collapse" for="c-35939153">[-]</label><label class="expand" for="c-35939153">[4 more]</label></div><br/><div class="children"><div class="content">I only have an m1</div><br/><div id="35939636" class="c"><input type="checkbox" id="c-35939636" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35938117">root</a><span>|</span><a href="#35939153">parent</a><span>|</span><a href="#35938170">next</a><span>|</span><label class="collapse" for="c-35939636">[-]</label><label class="expand" for="c-35939636">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think the integrated GPU on that supports CUDA. So you will need to use CPU mode only.</div><br/><div id="35940885" class="c"><input type="checkbox" id="c-35940885" checked=""/><div class="controls bullet"><span class="by">itake</span><span>|</span><a href="#35938117">root</a><span>|</span><a href="#35939636">parent</a><span>|</span><a href="#35938170">next</a><span>|</span><label class="collapse" for="c-35940885">[-]</label><label class="expand" for="c-35940885">[2 more]</label></div><br/><div class="children"><div class="content">Yep, but isnât there an integrated ML chip that makes it faster than cpu? Or does llama.cpp not use that?</div><br/><div id="35941009" class="c"><input type="checkbox" id="c-35941009" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35938117">root</a><span>|</span><a href="#35940885">parent</a><span>|</span><a href="#35938170">next</a><span>|</span><label class="collapse" for="c-35941009">[-]</label><label class="expand" for="c-35941009">[1 more]</label></div><br/><div class="children"><div class="content">unfortunately that chip is proprietary and undocumented, it&#x27;s very difficult for open source programs to make use of. I think there is some reverse engineering work being done but it&#x27;s not complete.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35938170" class="c"><input type="checkbox" id="c-35938170" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35938117">parent</a><span>|</span><a href="#35938250">prev</a><span>|</span><a href="#35938806">next</a><span>|</span><label class="collapse" for="c-35938170">[-]</label><label class="expand" for="c-35938170">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t tried that but <a href="https:&#x2F;&#x2F;github.com&#x2F;abetlen&#x2F;llama-cpp-python">https:&#x2F;&#x2F;github.com&#x2F;abetlen&#x2F;llama-cpp-python</a> and <a href="https:&#x2F;&#x2F;github.com&#x2F;r2d4&#x2F;openlm">https:&#x2F;&#x2F;github.com&#x2F;r2d4&#x2F;openlm</a> exists</div><br/></div></div><div id="35938806" class="c"><input type="checkbox" id="c-35938806" checked=""/><div class="controls bullet"><span class="by">inhumantsar</span><span>|</span><a href="#35938117">parent</a><span>|</span><a href="#35938170">prev</a><span>|</span><a href="#35942214">next</a><span>|</span><label class="collapse" for="c-35938806">[-]</label><label class="expand" for="c-35938806">[1 more]</label></div><br/><div class="children"><div class="content">Weights and Biases is good for building&#x2F;training models and Lambda Labs is a cloud provider for AI workloads. Lambda will only get you up to running the model though. You would still need to overlay some job management on top of that. I&#x27;ve heard Run.AI is good on that front but I haven&#x27;t tried.</div><br/></div></div><div id="35942214" class="c"><input type="checkbox" id="c-35942214" checked=""/><div class="controls bullet"><span class="by">month13</span><span>|</span><a href="#35938117">parent</a><span>|</span><a href="#35938806">prev</a><span>|</span><a href="#35938049">next</a><span>|</span><label class="collapse" for="c-35942214">[-]</label><label class="expand" for="c-35942214">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;bellard.org&#x2F;ts_server&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bellard.org&#x2F;ts_server&#x2F;</a> may be what you are after.<p>You can run open-source models, but the software itself is closed-source and free for non-commercial use.</div><br/></div></div></div></div><div id="35938049" class="c"><input type="checkbox" id="c-35938049" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#35938117">prev</a><span>|</span><a href="#35944121">next</a><span>|</span><label class="collapse" for="c-35938049">[-]</label><label class="expand" for="c-35938049">[1 more]</label></div><br/><div class="children"><div class="content">See also:<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;13fnyah&#x2F;you_guys_are_missing_out_on_gpt4x_vicuna&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;13fnyah&#x2F;you_guy...</a><p><a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?arena" rel="nofollow">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?arena</a> (Click &#x27;leaderboard&#x27;)</div><br/></div></div><div id="35944121" class="c"><input type="checkbox" id="c-35944121" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35938049">prev</a><span>|</span><a href="#35943673">next</a><span>|</span><label class="collapse" for="c-35944121">[-]</label><label class="expand" for="c-35944121">[1 more]</label></div><br/><div class="children"><div class="content">PSA:<p>If you&#x27;re using oobabooga&#x2F;text-generation-webui then you need to:<p>1. Re-install llama-cpp-python with support for CUBLAS:<p><pre><code>  CMAKE_ARGS=&quot;-DLLAMA_CUBLAS=on&quot; FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --force-reinstall
</code></pre>
2. Launch the web UI with the --n-gpu-layers flag, e.g.<p><pre><code>  python server.py --model gpt4-x-vicuna-13B.ggml.q5_1.bin --n-gpu-layers 24</code></pre></div><br/></div></div><div id="35943673" class="c"><input type="checkbox" id="c-35943673" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#35944121">prev</a><span>|</span><a href="#35941438">next</a><span>|</span><label class="collapse" for="c-35943673">[-]</label><label class="expand" for="c-35943673">[3 more]</label></div><br/><div class="children"><div class="content">Could someone please share a good resource for building a machine from scratch, for doing simple-ish training and running open-source models like Llama? I&#x27;d love to run some of these and even train them from scratch, and I&#x27;d love to use that as an excuse to drop $5k on a new machine...<p>Would love to run a bunch of models on the machine without dripping $$ to OpenAI, Modal or other providers...</div><br/><div id="35943704" class="c"><input type="checkbox" id="c-35943704" checked=""/><div class="controls bullet"><span class="by">vonseel</span><span>|</span><a href="#35943673">parent</a><span>|</span><a href="#35944153">next</a><span>|</span><label class="collapse" for="c-35943704">[-]</label><label class="expand" for="c-35943704">[1 more]</label></div><br/><div class="children"><div class="content">I am no where near an expert on this subject, and this information is from a few months ago so maybe it&#x27;s outdated, but people on Reddit[1] are claiming running the llama with 65B parameters would need like 20K+ of GPUs. A 40GB A100 looks like it&#x27;s almost $8K on Amazon, and I&#x27;m sure you could do a lot with just one of those, but that&#x27;s already beyond your $5K budget.<p>[1] <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;11i4olx&#x2F;d_is_it_possible_to_run_metas_llama_65b_model_on&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;11i4olx&#x2F;d_...</a><p>I&#x27;ll let others chime in but you could still probably build something really powerful within your budget that is able to run various AI tasks.</div><br/></div></div><div id="35944153" class="c"><input type="checkbox" id="c-35944153" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#35943673">parent</a><span>|</span><a href="#35943704">prev</a><span>|</span><a href="#35941438">next</a><span>|</span><label class="collapse" for="c-35944153">[-]</label><label class="expand" for="c-35944153">[1 more]</label></div><br/><div class="children"><div class="content">There are some threads with hardware recommendations in the LocalLLaMa subreddit.<p>Hereâs a recent one:<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;13f5gwn&#x2F;home_llm_hardware_suggestions&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;13f5gwn&#x2F;home_ll...</a></div><br/></div></div></div></div><div id="35941438" class="c"><input type="checkbox" id="c-35941438" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#35943673">prev</a><span>|</span><a href="#35937903">next</a><span>|</span><label class="collapse" for="c-35941438">[-]</label><label class="expand" for="c-35941438">[1 more]</label></div><br/><div class="children"><div class="content">I wish this used the webgpu c++ library instead, then it could be used in any GPU hardware.</div><br/></div></div><div id="35937903" class="c"><input type="checkbox" id="c-35937903" checked=""/><div class="controls bullet"><span class="by">hhh</span><span>|</span><a href="#35941438">prev</a><span>|</span><a href="#35938293">next</a><span>|</span><label class="collapse" for="c-35937903">[-]</label><label class="expand" for="c-35937903">[3 more]</label></div><br/><div class="children"><div class="content">Instructions are a bit rough. The Micromamba thing doesnât work, doesnât say how to install itâ¦ you have to clone llama.cpp too</div><br/><div id="35937940" class="c"><input type="checkbox" id="c-35937940" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35937903">parent</a><span>|</span><a href="#35938293">next</a><span>|</span><label class="collapse" for="c-35937940">[-]</label><label class="expand" for="c-35937940">[2 more]</label></div><br/><div class="children"><div class="content">Apologies for that. I&#x27;ve added some extra micromamba setup commands that I should have included before!<p>I&#x27;ve also added the git clone command, thank you for the feedback</div><br/><div id="35938424" class="c"><input type="checkbox" id="c-35938424" checked=""/><div class="controls bullet"><span class="by">hhh</span><span>|</span><a href="#35937903">root</a><span>|</span><a href="#35937940">parent</a><span>|</span><a href="#35938293">next</a><span>|</span><label class="collapse" for="c-35938424">[-]</label><label class="expand" for="c-35938424">[1 more]</label></div><br/><div class="children"><div class="content">Appreciate it! This is much better!</div><br/></div></div></div></div></div></div><div id="35938293" class="c"><input type="checkbox" id="c-35938293" checked=""/><div class="controls bullet"><span class="by">mozillas</span><span>|</span><a href="#35937903">prev</a><span>|</span><a href="#35942514">next</a><span>|</span><label class="collapse" for="c-35938293">[-]</label><label class="expand" for="c-35938293">[7 more]</label></div><br/><div class="children"><div class="content">I ran the 7B Vicuna (ggml-vic7b-q4_0.bin) on a 2017 MacBook Air (8GB RAM) with llama.cpp.<p>Worked OK for me with the default context size. 2048, like you see in most examples was too slow for my taste.</div><br/><div id="35938331" class="c"><input type="checkbox" id="c-35938331" checked=""/><div class="controls bullet"><span class="by">koheripbal</span><span>|</span><a href="#35938293">parent</a><span>|</span><a href="#35942514">next</a><span>|</span><label class="collapse" for="c-35938331">[-]</label><label class="expand" for="c-35938331">[6 more]</label></div><br/><div class="children"><div class="content">Given the current price (mostly free) off public llms I&#x27;m not sure what the use case of running out at home are yet.<p>OpenAIs paid GPT4 has few restrictions and is still cheap.<p>... Not to mention GPT4 with browsing feature is vastly superior to any home of the models you can run at home.</div><br/><div id="35939120" class="c"><input type="checkbox" id="c-35939120" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#35938293">root</a><span>|</span><a href="#35938331">parent</a><span>|</span><a href="#35938517">next</a><span>|</span><label class="collapse" for="c-35939120">[-]</label><label class="expand" for="c-35939120">[1 more]</label></div><br/><div class="children"><div class="content">The point for me personally is the same as why I find it so powerful to self host SMTP, IMAP, HTTP. Itâs in my hands, I know where it all begins and ends. I answer to no one.<p>For LLMs this means I am allowed their full potential. I can generate smut, filth, illegal content of any kind for any reason. Itâs for me to decide. Itâs empowering, itâs the hacker mindset.</div><br/></div></div><div id="35938517" class="c"><input type="checkbox" id="c-35938517" checked=""/><div class="controls bullet"><span class="by">2devnull</span><span>|</span><a href="#35938293">root</a><span>|</span><a href="#35938331">parent</a><span>|</span><a href="#35939120">prev</a><span>|</span><a href="#35939634">next</a><span>|</span><label class="collapse" for="c-35938517">[-]</label><label class="expand" for="c-35938517">[1 more]</label></div><br/><div class="children"><div class="content">Many would be users canât send their data  data to openAI. Think HIPPA and other laws restricting data sharing. Federation or distribution of the models for local training is the other solution to that problem.</div><br/></div></div><div id="35939634" class="c"><input type="checkbox" id="c-35939634" checked=""/><div class="controls bullet"><span class="by">sagarm</span><span>|</span><a href="#35938293">root</a><span>|</span><a href="#35938331">parent</a><span>|</span><a href="#35938517">prev</a><span>|</span><a href="#35942514">next</a><span>|</span><label class="collapse" for="c-35939634">[-]</label><label class="expand" for="c-35939634">[3 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s mostly useful if you want to do your own fine tuning, or the data you are working with can&#x27;t be sent to a third party for contractual, legal, or paranoid reasons.</div><br/><div id="35941513" class="c"><input type="checkbox" id="c-35941513" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#35938293">root</a><span>|</span><a href="#35939634">parent</a><span>|</span><a href="#35942514">next</a><span>|</span><label class="collapse" for="c-35941513">[-]</label><label class="expand" for="c-35941513">[2 more]</label></div><br/><div class="children"><div class="content">Iâm working on an app to index your life, and having it local is a huge plus for the people I have using it.</div><br/><div id="35943435" class="c"><input type="checkbox" id="c-35943435" checked=""/><div class="controls bullet"><span class="by">theaussiestew</span><span>|</span><a href="#35938293">root</a><span>|</span><a href="#35941513">parent</a><span>|</span><a href="#35942514">next</a><span>|</span><label class="collapse" for="c-35943435">[-]</label><label class="expand" for="c-35943435">[1 more]</label></div><br/><div class="children"><div class="content">Sounds interesting, got a link?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35942514" class="c"><input type="checkbox" id="c-35942514" checked=""/><div class="controls bullet"><span class="by">akulbe</span><span>|</span><a href="#35938293">prev</a><span>|</span><a href="#35938983">next</a><span>|</span><label class="collapse" for="c-35942514">[-]</label><label class="expand" for="c-35942514">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only ever been a consumer of ChatGPT&#x2F;Bard. Never set up any LLM stuff locally, but the idea is appealing to me.<p>I have a ThinkStation P620 w&#x2F;ThreadRipper Pro 3945WX (12c24t) with a GTX 1070 (and a second 1070 I could put in there) and there&#x27;s 512GB of RAM on the box.<p>Does this need to be bare metal, or can it run in VM?<p>I&#x27;m currently running RHEL 9.2 w&#x2F;KVM (as a VM host) with light usage so far.</div><br/></div></div><div id="35938983" class="c"><input type="checkbox" id="c-35938983" checked=""/><div class="controls bullet"><span class="by">tarr11</span><span>|</span><a href="#35942514">prev</a><span>|</span><a href="#35941827">next</a><span>|</span><label class="collapse" for="c-35938983">[-]</label><label class="expand" for="c-35938983">[5 more]</label></div><br/><div class="children"><div class="content">What is the state of the art on evaluating the accuracy of these models?  Is there some equivalent to an âend to end testâ?<p>It feels somewhat recursive since the input and output are natural language and so you would need another LLM to evaluate whether the model answered a prompt correctly.</div><br/><div id="35939220" class="c"><input type="checkbox" id="c-35939220" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#35938983">parent</a><span>|</span><a href="#35939015">next</a><span>|</span><label class="collapse" for="c-35939220">[-]</label><label class="expand" for="c-35939220">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?arena" rel="nofollow">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?arena</a> (Click &#x27;leaderboard&#x27;)</div><br/></div></div><div id="35939015" class="c"><input type="checkbox" id="c-35939015" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#35938983">parent</a><span>|</span><a href="#35939220">prev</a><span>|</span><a href="#35941827">next</a><span>|</span><label class="collapse" for="c-35939015">[-]</label><label class="expand" for="c-35939015">[3 more]</label></div><br/><div class="children"><div class="content">Itâs going to be very difficult to come up with any rigorous structure for automatically assessing the outputs of these models. Theyâre built using effectively human grading of the answers</div><br/><div id="35941534" class="c"><input type="checkbox" id="c-35941534" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#35938983">root</a><span>|</span><a href="#35939015">parent</a><span>|</span><a href="#35939114">next</a><span>|</span><label class="collapse" for="c-35941534">[-]</label><label class="expand" for="c-35941534">[1 more]</label></div><br/><div class="children"><div class="content">Llama cpp and others use perplexity:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;perplexity" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;perplexity</a></div><br/></div></div><div id="35939114" class="c"><input type="checkbox" id="c-35939114" checked=""/><div class="controls bullet"><span class="by">RockyMcNuts</span><span>|</span><a href="#35938983">root</a><span>|</span><a href="#35939015">parent</a><span>|</span><a href="#35941534">prev</a><span>|</span><a href="#35941827">next</a><span>|</span><label class="collapse" for="c-35939114">[-]</label><label class="expand" for="c-35939114">[1 more]</label></div><br/><div class="children"><div class="content">hmmh, if we have the reinforcement learning part of reinforcement learning with human feedback, isn&#x27;t that a model that takes a question&#x2F;answer pair and rates the quality of the answer? it&#x27;s sort of grading itself, it&#x27;s like a training loss but it still tells us something?</div><br/></div></div></div></div></div></div><div id="35941827" class="c"><input type="checkbox" id="c-35941827" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#35938983">prev</a><span>|</span><a href="#35937854">next</a><span>|</span><label class="collapse" for="c-35941827">[-]</label><label class="expand" for="c-35941827">[1 more]</label></div><br/><div class="children"><div class="content">If I really want to do some playing around in this area, would it be good to get a RTX 4000 SFF which has 20 GB of VRAM but is a low-power card, which I want as it would be running 24&#x2F;7 and energy prices are pretty bad in Germany, or would it make more sense to buy an Apple product with some M2 chip which apparently is good for these tasks as it shares CPU and GPU memory?</div><br/></div></div><div id="35937854" class="c"><input type="checkbox" id="c-35937854" checked=""/><div class="controls bullet"><span class="by">bitL</span><span>|</span><a href="#35941827">prev</a><span>|</span><a href="#35942656">next</a><span>|</span><label class="collapse" for="c-35937854">[-]</label><label class="expand" for="c-35937854">[4 more]</label></div><br/><div class="children"><div class="content">How about reloading parts of the model as the inference progresses instead of splitting it into GPU&#x2F;CPU parts? Reloading would be memory-limited to the largest intermediate tensor cut.</div><br/><div id="35938207" class="c"><input type="checkbox" id="c-35938207" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#35937854">parent</a><span>|</span><a href="#35938093">next</a><span>|</span><label class="collapse" for="c-35938207">[-]</label><label class="expand" for="c-35938207">[2 more]</label></div><br/><div class="children"><div class="content">That would turn what&#x27;s currently an L3 cache miss or a GPU data copy into a disk I&#x2F;O stall. Not that it might not be possible to pipeline things to make that less of a problem, but it doesn&#x27;t immediately strike me as a fantastic trade-off.</div><br/><div id="35938445" class="c"><input type="checkbox" id="c-35938445" checked=""/><div class="controls bullet"><span class="by">bitL</span><span>|</span><a href="#35937854">root</a><span>|</span><a href="#35938207">parent</a><span>|</span><a href="#35938093">next</a><span>|</span><label class="collapse" for="c-35938445">[-]</label><label class="expand" for="c-35938445">[1 more]</label></div><br/><div class="children"><div class="content">One can keep all tensors in the RAM, just push whatever needed to GPU VRAM, basically limited by PCIe speed. Or some intelligent strategy with read-ahead from SSD if one&#x27;s RAM is limited. There are even GPUs with their own SSDs.</div><br/></div></div></div></div><div id="35938093" class="c"><input type="checkbox" id="c-35938093" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#35937854">parent</a><span>|</span><a href="#35938207">prev</a><span>|</span><a href="#35942656">next</a><span>|</span><label class="collapse" for="c-35938093">[-]</label><label class="expand" for="c-35938093">[1 more]</label></div><br/><div class="children"><div class="content">The Tensor Reloaded, starring Keanu Reeves</div><br/></div></div></div></div><div id="35942656" class="c"><input type="checkbox" id="c-35942656" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#35937854">prev</a><span>|</span><a href="#35942230">next</a><span>|</span><label class="collapse" for="c-35942656">[-]</label><label class="expand" for="c-35942656">[2 more]</label></div><br/><div class="children"><div class="content">How many &quot;B&quot; (billions of parameters) is ChatGPT GPT-4?</div><br/><div id="35942660" class="c"><input type="checkbox" id="c-35942660" checked=""/><div class="controls bullet"><span class="by">sciolist</span><span>|</span><a href="#35942656">parent</a><span>|</span><a href="#35942230">next</a><span>|</span><label class="collapse" for="c-35942660">[-]</label><label class="expand" for="c-35942660">[1 more]</label></div><br/><div class="children"><div class="content">Information about GPT-4 was not released</div><br/></div></div></div></div><div id="35942230" class="c"><input type="checkbox" id="c-35942230" checked=""/><div class="controls bullet"><span class="by">blendergeek</span><span>|</span><a href="#35942656">prev</a><span>|</span><a href="#35938875">next</a><span>|</span><label class="collapse" for="c-35942230">[-]</label><label class="expand" for="c-35942230">[1 more]</label></div><br/><div class="children"><div class="content">Is there a way to run any of these with only 4GB of VRAM?</div><br/></div></div><div id="35938875" class="c"><input type="checkbox" id="c-35938875" checked=""/><div class="controls bullet"><span class="by">anshumankmr</span><span>|</span><a href="#35942230">prev</a><span>|</span><a href="#35940071">next</a><span>|</span><label class="collapse" for="c-35938875">[-]</label><label class="expand" for="c-35938875">[2 more]</label></div><br/><div class="children"><div class="content">How long before it runs on a 4 gig card?</div><br/><div id="35938955" class="c"><input type="checkbox" id="c-35938955" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35938875">parent</a><span>|</span><a href="#35940071">next</a><span>|</span><label class="collapse" for="c-35938955">[-]</label><label class="expand" for="c-35938955">[1 more]</label></div><br/><div class="children"><div class="content">You can offload only 10 layers or so if you want to run on a 4GB card</div><br/></div></div></div></div><div id="35940071" class="c"><input type="checkbox" id="c-35940071" checked=""/><div class="controls bullet"><span class="by">dinobones</span><span>|</span><a href="#35938875">prev</a><span>|</span><a href="#35941379">next</a><span>|</span><label class="collapse" for="c-35940071">[-]</label><label class="expand" for="c-35940071">[9 more]</label></div><br/><div class="children"><div class="content">What is HNâs fascination with these toy models that produce low quality, completely unusable output?<p>Is there a use case for them Iâm missing?<p>Additionally, donât they all have fairly restrictive licenses?</div><br/><div id="35940090" class="c"><input type="checkbox" id="c-35940090" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#35940071">parent</a><span>|</span><a href="#35940566">next</a><span>|</span><label class="collapse" for="c-35940090">[-]</label><label class="expand" for="c-35940090">[6 more]</label></div><br/><div class="children"><div class="content">I never thought I&#x27;d see the day when a 13B model was casually referred to in a comments section as a &quot;toy model&quot;.</div><br/><div id="35940127" class="c"><input type="checkbox" id="c-35940127" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#35940071">root</a><span>|</span><a href="#35940090">parent</a><span>|</span><a href="#35940108">next</a><span>|</span><label class="collapse" for="c-35940127">[-]</label><label class="expand" for="c-35940127">[3 more]</label></div><br/><div class="children"><div class="content">Compared to GPT2 itâs on par. Compared to GPT3, 3.5, or 4, itâs a toy. GPT2 is 4 years old, and in terms of LLMs, thatâs several life times ago. In 5-10 years, GPT3 will be viewed as a toy. Note, âprogressâ will unlikely be as fast as it has been going forward.</div><br/><div id="35944187" class="c"><input type="checkbox" id="c-35944187" checked=""/><div class="controls bullet"><span class="by">Oranguru</span><span>|</span><a href="#35940071">root</a><span>|</span><a href="#35940127">parent</a><span>|</span><a href="#35940259">next</a><span>|</span><label class="collapse" for="c-35944187">[-]</label><label class="expand" for="c-35944187">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Compared to GPT2 itâs on par&quot;
Any benchmarks or evidence tu support this claim? IF you try to find them, official benchmarks will tell you that this is not true. Even the smallest LLaMa model (7B) is far ahead of GPT2, like an order of magnitude better in perplexity.</div><br/></div></div><div id="35940259" class="c"><input type="checkbox" id="c-35940259" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#35940071">root</a><span>|</span><a href="#35940127">parent</a><span>|</span><a href="#35944187">prev</a><span>|</span><a href="#35940108">next</a><span>|</span><label class="collapse" for="c-35940259">[-]</label><label class="expand" for="c-35940259">[1 more]</label></div><br/><div class="children"><div class="content">GPT-2&#x27;s largest model was 1.5B params, LLama-65B was similar to the largest GPT3 in benchmark performance but that model was expensive in the API, a number of the people would use the cheaper one(s) instead IIRC.<p>So this is similar to a mid tier GPT3 class model.<p>Basically, there&#x27;s not much reason to Pooh-Pooh it. It may not perform quite as well, but I find it to be useful for the things it&#x27;s useful for.</div><br/></div></div></div></div><div id="35940108" class="c"><input type="checkbox" id="c-35940108" checked=""/><div class="controls bullet"><span class="by">andrewmcwatters</span><span>|</span><a href="#35940071">root</a><span>|</span><a href="#35940090">parent</a><span>|</span><a href="#35940127">prev</a><span>|</span><a href="#35940566">next</a><span>|</span><label class="collapse" for="c-35940108">[-]</label><label class="expand" for="c-35940108">[2 more]</label></div><br/><div class="children"><div class="content">Start using it for tasks and you&#x27;ll find limitations very quickly. Even ChatGPT excels at some tasks and fails miserably at others.</div><br/><div id="35940237" class="c"><input type="checkbox" id="c-35940237" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#35940071">root</a><span>|</span><a href="#35940108">parent</a><span>|</span><a href="#35940566">next</a><span>|</span><label class="collapse" for="c-35940237">[-]</label><label class="expand" for="c-35940237">[1 more]</label></div><br/><div class="children"><div class="content">Oh, I&#x27;ve been using language models before a lot (or at least some significant chunk) of HN knew the word LLM, I think.<p>I remember when going from 6B to 13B was crazy good. We&#x27;ve just normalized our standards to the latest models in the era.<p>They do have their shortcomings but can be quite useful as well, especially the LLama class ones. They&#x27;re definitely not GPT-4 or Claude+, for sure, for sure.</div><br/></div></div></div></div></div></div><div id="35940566" class="c"><input type="checkbox" id="c-35940566" checked=""/><div class="controls bullet"><span class="by">Zetobal</span><span>|</span><a href="#35940071">parent</a><span>|</span><a href="#35940090">prev</a><span>|</span><a href="#35940099">next</a><span>|</span><label class="collapse" for="c-35940566">[-]</label><label class="expand" for="c-35940566">[1 more]</label></div><br/><div class="children"><div class="content">Maybe you forgot for what the H stands in HN... playful curiousity.</div><br/></div></div></div></div><div id="35941379" class="c"><input type="checkbox" id="c-35941379" checked=""/><div class="controls bullet"><span class="by">alg_fun</span><span>|</span><a href="#35940071">prev</a><span>|</span><a href="#35940219">next</a><span>|</span><label class="collapse" for="c-35941379">[-]</label><label class="expand" for="c-35941379">[1 more]</label></div><br/><div class="children"><div class="content">wouldn&#x27;t i be faster to use ram as a swap for vram?</div><br/></div></div><div id="35940219" class="c"><input type="checkbox" id="c-35940219" checked=""/><div class="controls bullet"><span class="by">ranger_danger</span><span>|</span><a href="#35941379">prev</a><span>|</span><a href="#35939294">next</a><span>|</span><label class="collapse" for="c-35940219">[-]</label><label class="expand" for="c-35940219">[1 more]</label></div><br/><div class="children"><div class="content">Why can&#x27;t these models run on the GPU while also using CPU RAM for the storage? That way people will performant-but-memory-starved GPUs can still utilize the better performance of the GPU calculation while also having enough RAM to store the model? I know it is possible to provide system RAM-backed GPU objects.</div><br/></div></div><div id="35939294" class="c"><input type="checkbox" id="c-35939294" checked=""/><div class="controls bullet"><span class="by">dclowd9901</span><span>|</span><a href="#35940219">prev</a><span>|</span><a href="#35938600">next</a><span>|</span><label class="collapse" for="c-35939294">[-]</label><label class="expand" for="c-35939294">[5 more]</label></div><br/><div class="children"><div class="content">Has anyone tried running encryption algorithms through these models? I wonder if it could be trained to decrypt.</div><br/><div id="35940141" class="c"><input type="checkbox" id="c-35940141" checked=""/><div class="controls bullet"><span class="by">Hendrikto</span><span>|</span><a href="#35939294">parent</a><span>|</span><a href="#35938600">next</a><span>|</span><label class="collapse" for="c-35940141">[-]</label><label class="expand" for="c-35940141">[4 more]</label></div><br/><div class="children"><div class="content">That would be very surprising, given that any widely used cryptographic encryption algorithm has been EXTENSIVELY cryptanalyzed.<p>ML models are essentially trained to recognize patterns. Encryption algorithms are explicitly designed to resist that kind of analysis. LLMs are not magic.</div><br/><div id="35941378" class="c"><input type="checkbox" id="c-35941378" checked=""/><div class="controls bullet"><span class="by">dclowd9901</span><span>|</span><a href="#35939294">root</a><span>|</span><a href="#35940141">parent</a><span>|</span><a href="#35938600">next</a><span>|</span><label class="collapse" for="c-35941378">[-]</label><label class="expand" for="c-35941378">[3 more]</label></div><br/><div class="children"><div class="content">All of what you said is true, for us. I know LLMs arenât magic (lord knows I actually kind of understand the principles of how they operate), but they have a much greater computational and relational bandwidth than weâve ever had access to before. So Iâm curious if that can break down what otherwise appears to be complete obfuscation. Otherwise, weâre saying that encryption is somehow magic in a way that LLMs cannot possibly be.</div><br/><div id="35944483" class="c"><input type="checkbox" id="c-35944483" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#35939294">root</a><span>|</span><a href="#35941378">parent</a><span>|</span><a href="#35941700">next</a><span>|</span><label class="collapse" for="c-35944483">[-]</label><label class="expand" for="c-35944483">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So Iâm curious if that can break down what otherwise appears to be complete obfuscation.<p>This seems to be a complete misunderstanding of what encryption is.<p>Obfuscation generally means muddling things around in ways that can be reconstructed. It&#x27;s entirely possible a (custom - because you&#x27;d need custom tokenization) LLM could deobfuscate things.<p>Encryption OTOH means using a piece of information that isn&#x27;t present. Weak encryption gets broken because that missing information can be guessed or recovered easily.<p>But this isn&#x27;t the case for correctly implemented strong encryption. The missing information cannot be recovered by any non-quantum process in a reasonable timeframe.<p>There are exceptions - newly developed mathematical techniques can sometimes make recovering that information quicker.<p>But in general math is the weakest point of LLMs, so it seems an unlikely place for them to excel.</div><br/></div></div><div id="35941700" class="c"><input type="checkbox" id="c-35941700" checked=""/><div class="controls bullet"><span class="by">NegativeK</span><span>|</span><a href="#35939294">root</a><span>|</span><a href="#35941378">parent</a><span>|</span><a href="#35944483">prev</a><span>|</span><a href="#35938600">next</a><span>|</span><label class="collapse" for="c-35941700">[-]</label><label class="expand" for="c-35941700">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Otherwise, weâre saying that encryption is somehow magic in a way that LLMs cannot possibly be.<p>I don&#x27;t see why that&#x27;s an unreasonable claim. I mean, encryption isn&#x27;t magic, but it is a drastically different process.</div><br/></div></div></div></div></div></div></div></div><div id="35938600" class="c"><input type="checkbox" id="c-35938600" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#35939294">prev</a><span>|</span><a href="#35937804">next</a><span>|</span><label class="collapse" for="c-35938600">[-]</label><label class="expand" for="c-35938600">[1 more]</label></div><br/><div class="children"><div class="content">or like download oobabooga&#x2F;text-generation-webui, any prequantized variant, and be done.</div><br/></div></div><div id="35937804" class="c"><input type="checkbox" id="c-35937804" checked=""/><div class="controls bullet"><span class="by">s_dev</span><span>|</span><a href="#35938600">prev</a><span>|</span><a href="#35940530">next</a><span>|</span><label class="collapse" for="c-35937804">[-]</label><label class="expand" for="c-35937804">[11 more]</label></div><br/><div class="children"><div class="content">[deleted]</div><br/><div id="35937826" class="c"><input type="checkbox" id="c-35937826" checked=""/><div class="controls bullet"><span class="by">capableweb</span><span>|</span><a href="#35937804">parent</a><span>|</span><a href="#35937900">next</a><span>|</span><label class="collapse" for="c-35937826">[-]</label><label class="expand" for="c-35937826">[5 more]</label></div><br/><div class="children"><div class="content">First link: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a><p>Which in turn has the following as the first link: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.13971" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.13971</a><p>Is it really quicker to ask here than just browse content for a bit, skimming some text or even using Google for one minute?</div><br/><div id="35937976" class="c"><input type="checkbox" id="c-35937976" checked=""/><div class="controls bullet"><span class="by">s_dev</span><span>|</span><a href="#35937804">root</a><span>|</span><a href="#35937826">parent</a><span>|</span><a href="#35937880">next</a><span>|</span><label class="collapse" for="c-35937976">[-]</label><label class="expand" for="c-35937976">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Is it really quicker to ask here than just browse content for a bit, skimming some text or even using Google for one minute?<p>I don&#x27;t know if it&#x27;s quicker but I trust human assessment a lot more than any machine generated explanations. You&#x27;re right I could have asked ChatGPT or even Googled but a small bit of context goes a long way and I&#x27;m clearly out of the loop here -- it&#x27;s possible others arrive on HN might appreciate such an explanation or we&#x27;re better off having lots of people making duplicated efforts to understand what they&#x27;re looking at.</div><br/><div id="35938588" class="c"><input type="checkbox" id="c-35938588" checked=""/><div class="controls bullet"><span class="by">capableweb</span><span>|</span><a href="#35937804">root</a><span>|</span><a href="#35937976">parent</a><span>|</span><a href="#35937880">next</a><span>|</span><label class="collapse" for="c-35938588">[-]</label><label class="expand" for="c-35938588">[1 more]</label></div><br/><div class="children"><div class="content">Well, I&#x27;m saying if you just followed the links on the submitted page, you&#x27;d reach the same conclusion but faster.</div><br/></div></div></div></div><div id="35937880" class="c"><input type="checkbox" id="c-35937880" checked=""/><div class="controls bullet"><span class="by">djbusby</span><span>|</span><a href="#35937804">root</a><span>|</span><a href="#35937826">parent</a><span>|</span><a href="#35937976">prev</a><span>|</span><a href="#35937900">next</a><span>|</span><label class="collapse" for="c-35937880">[-]</label><label class="expand" for="c-35937880">[2 more]</label></div><br/><div class="children"><div class="content">You gave an awesome answer in 2 minutes! Might be faster than reading!</div><br/><div id="35938597" class="c"><input type="checkbox" id="c-35938597" checked=""/><div class="controls bullet"><span class="by">capableweb</span><span>|</span><a href="#35937804">root</a><span>|</span><a href="#35937880">parent</a><span>|</span><a href="#35937900">next</a><span>|</span><label class="collapse" for="c-35938597">[-]</label><label class="expand" for="c-35938597">[1 more]</label></div><br/><div class="children"><div class="content">If you cannot click two links in a browser under two minutes, I&#x27;m either sorry for you, or scared of you :)</div><br/></div></div></div></div></div></div><div id="35937900" class="c"><input type="checkbox" id="c-35937900" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35937804">parent</a><span>|</span><a href="#35937826">prev</a><span>|</span><a href="#35937973">next</a><span>|</span><label class="collapse" for="c-35937900">[-]</label><label class="expand" for="c-35937900">[1 more]</label></div><br/><div class="children"><div class="content">llama is a text prediction model similar to GPT-2, and the version of GPT-3 that has not been fine tuned yet.<p>It is also possible to run fine tuned versions like vicuna with this. I think. Those versions are more focused on answering questions.</div><br/></div></div><div id="35937973" class="c"><input type="checkbox" id="c-35937973" checked=""/><div class="controls bullet"><span class="by">haunter</span><span>|</span><a href="#35937804">parent</a><span>|</span><a href="#35937900">prev</a><span>|</span><a href="#35940530">next</a><span>|</span><label class="collapse" for="c-35937973">[-]</label><label class="expand" for="c-35937973">[4 more]</label></div><br/><div class="children"><div class="content">&gt;I can&#x27;t tell from the Gist alone<p>Literally the second line: &quot;llama is a text prediction model similar to GPT-2, and the version of GPT-3 that has not been fine tuned yet&quot;</div><br/><div id="35937985" class="c"><input type="checkbox" id="c-35937985" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35937804">root</a><span>|</span><a href="#35937973">parent</a><span>|</span><a href="#35938027">next</a><span>|</span><label class="collapse" for="c-35937985">[-]</label><label class="expand" for="c-35937985">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry! I added this improvement based on that persons question!</div><br/></div></div><div id="35938027" class="c"><input type="checkbox" id="c-35938027" checked=""/><div class="controls bullet"><span class="by">s_dev</span><span>|</span><a href="#35937804">root</a><span>|</span><a href="#35937973">parent</a><span>|</span><a href="#35937985">prev</a><span>|</span><a href="#35940530">next</a><span>|</span><label class="collapse" for="c-35938027">[-]</label><label class="expand" for="c-35938027">[2 more]</label></div><br/><div class="children"><div class="content">Sorry -- I missed that. I&#x27;ll delete my comments -- obviously I&#x27;m just an idiot asking dumb questions that have no value to anybody. I thought I read through it.</div><br/><div id="35938046" class="c"><input type="checkbox" id="c-35938046" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35937804">root</a><span>|</span><a href="#35938027">parent</a><span>|</span><a href="#35940530">next</a><span>|</span><label class="collapse" for="c-35938046">[-]</label><label class="expand" for="c-35938046">[1 more]</label></div><br/><div class="children"><div class="content">not at all, your question was really good so I added the answer to it to my gist to help everyone else. Sorry for the confusion I created by doing that!</div><br/></div></div></div></div></div></div></div></div><div id="35940530" class="c"><input type="checkbox" id="c-35940530" checked=""/><div class="controls bullet"><span class="by">ACV001</span><span>|</span><a href="#35937804">prev</a><span>|</span><label class="collapse" for="c-35940530">[-]</label><label class="expand" for="c-35940530">[2 more]</label></div><br/><div class="children"><div class="content">The future is this - these models will be able to run on smaller and smaller hardware eventually being able to run on your phone, watch or embedded devices. The revolution is here and is inevitable. Similar to how computers evolved. We are still lucky that these models have no consciousness, still. Once they gain consciousness, that will mark the appearance of a new species (superior to us if anything). Also, luckily, they have no physical bodies and cannot replicate, so far...</div><br/></div></div></div></div></div></div></div></body></html>