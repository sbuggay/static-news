<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1701680458663" as="style"/><link rel="stylesheet" href="styles.css?v=1701680458663"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://bbycroft.net/llm">LLM Visualization</a>Â <span class="domain">(<a href="https://bbycroft.net">bbycroft.net</a>)</span></div><div class="subtext"><span>plibither8</span> | <span>74 comments</span></div><br/><div><div id="38508508" class="c"><input type="checkbox" id="c-38508508" checked=""/><div class="controls bullet"><span class="by">warkanlock</span><span>|</span><a href="#38514914">next</a><span>|</span><label class="collapse" for="c-38508508">[-]</label><label class="expand" for="c-38508508">[7 more]</label></div><br/><div class="children"><div class="content">This is an excellent tool to realize how an LLM actually works from the ground up!<p>For those reading it and going through each step, if by chance you get stuck on why 48 elements are in the first array, please refer to the model.py on minGPT [1]<p>It&#x27;s an architectural decision that it will be great to mention in the article since people without too much context might lose it<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;minGPT&#x2F;blob&#x2F;master&#x2F;mingpt&#x2F;model.py">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;minGPT&#x2F;blob&#x2F;master&#x2F;mingpt&#x2F;model....</a></div><br/><div id="38513983" class="c"><input type="checkbox" id="c-38513983" checked=""/><div class="controls bullet"><span class="by">taliesinb</span><span>|</span><a href="#38508508">parent</a><span>|</span><a href="#38508965">next</a><span>|</span><label class="collapse" for="c-38513983">[-]</label><label class="expand" for="c-38513983">[1 more]</label></div><br/><div class="children"><div class="content">Wow, I love the interactive wizzing around and the animation, very neat! Way more explanations should work like this.<p>I&#x27;ve recently finished an unorthodox kind of visualization &#x2F; explanation of transformers. It&#x27;s sadly not interactive, but it does have some maybe unique strengths.<p>First, it gives array axis semantic <i>names</i>, represented in the diagrams as <i>colors</i> (which this post also uses). So sequence axis is red, key feature dimension is green, multihead axis is orange, etc. This helps you show quite complicated array circuits and get an immediate feeling for what is going on and how different arrays are being combined with each-other. Here&#x27;s a pic of the the full multihead self-attention step for example:<p><a href="https:&#x2F;&#x2F;math.tali.link&#x2F;raster&#x2F;052n01bav6yvz_1smxhkus2qrik_0736_0884_02kdqvrzq963t.jpg" rel="nofollow noreferrer">https:&#x2F;&#x2F;math.tali.link&#x2F;raster&#x2F;052n01bav6yvz_1smxhkus2qrik_07...</a><p>It also uses a kind of generalization tensor network diagrammatic notation -- if anyone remembers Penrose&#x27;s tensor notation, it&#x27;s like that but enriched with colors and some other ideas. Underneath these diagrams are string diagrams in a particular category, though you don&#x27;t need to know (nor do I even explain that!).<p>Here&#x27;s the main blog post introducing the formalism: <a href="https:&#x2F;&#x2F;math.tali.link&#x2F;rainbow-array-algebra" rel="nofollow noreferrer">https:&#x2F;&#x2F;math.tali.link&#x2F;rainbow-array-algebra</a><p>Here&#x27;s the section on perceptrons: <a href="https:&#x2F;&#x2F;math.tali.link&#x2F;rainbow-array-algebra&#x2F;#neural-networks" rel="nofollow noreferrer">https:&#x2F;&#x2F;math.tali.link&#x2F;rainbow-array-algebra&#x2F;#neural-network...</a><p>Here&#x27;s the section on transformers: <a href="https:&#x2F;&#x2F;math.tali.link&#x2F;rainbow-array-algebra&#x2F;#transformers" rel="nofollow noreferrer">https:&#x2F;&#x2F;math.tali.link&#x2F;rainbow-array-algebra&#x2F;#transformers</a></div><br/></div></div><div id="38508965" class="c"><input type="checkbox" id="c-38508965" checked=""/><div class="controls bullet"><span class="by">namocat</span><span>|</span><a href="#38508508">parent</a><span>|</span><a href="#38513983">prev</a><span>|</span><a href="#38512086">next</a><span>|</span><label class="collapse" for="c-38508965">[-]</label><label class="expand" for="c-38508965">[2 more]</label></div><br/><div class="children"><div class="content">Yes, thank you - It was unexplained, so I got stuck on &quot;Why 48?&quot;, thinking I&#x27;d missing something right out of the gate.</div><br/><div id="38509827" class="c"><input type="checkbox" id="c-38509827" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#38508508">root</a><span>|</span><a href="#38508965">parent</a><span>|</span><a href="#38512086">next</a><span>|</span><label class="collapse" for="c-38509827">[-]</label><label class="expand" for="c-38509827">[1 more]</label></div><br/><div class="children"><div class="content">I was thinking 42 ;-)</div><br/></div></div></div></div><div id="38512086" class="c"><input type="checkbox" id="c-38512086" checked=""/><div class="controls bullet"><span class="by">riemannzeta</span><span>|</span><a href="#38508508">parent</a><span>|</span><a href="#38508965">prev</a><span>|</span><a href="#38514094">next</a><span>|</span><label class="collapse" for="c-38512086">[-]</label><label class="expand" for="c-38512086">[2 more]</label></div><br/><div class="children"><div class="content">Are you referring specifically to line 141, which sets the number of embedding elements for gpt-nano to 48? That also seems to correspond to the Channel size C referenced in the explanation text?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;minGPT&#x2F;blob&#x2F;master&#x2F;mingpt&#x2F;model.py#L141C70-L141C70">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;minGPT&#x2F;blob&#x2F;master&#x2F;mingpt&#x2F;model....</a></div><br/><div id="38513981" class="c"><input type="checkbox" id="c-38513981" checked=""/><div class="controls bullet"><span class="by">tomnipotent</span><span>|</span><a href="#38508508">root</a><span>|</span><a href="#38512086">parent</a><span>|</span><a href="#38514094">next</a><span>|</span><label class="collapse" for="c-38513981">[-]</label><label class="expand" for="c-38513981">[1 more]</label></div><br/><div class="children"><div class="content">That matches the name of default model selected in the right pane, &quot;nano-gpt&quot;. I missed the &quot;bigger picture&quot; at first before I noticed the other models in the right pane header.</div><br/></div></div></div></div><div id="38514094" class="c"><input type="checkbox" id="c-38514094" checked=""/><div class="controls bullet"><span class="by">jayveeone</span><span>|</span><a href="#38508508">parent</a><span>|</span><a href="#38512086">prev</a><span>|</span><a href="#38514914">next</a><span>|</span><label class="collapse" for="c-38514094">[-]</label><label class="expand" for="c-38514094">[1 more]</label></div><br/><div class="children"><div class="content">Yes yes it was the 48 elements thing that got me stuck. Definitely not everything from the second the page loaded.</div><br/></div></div></div></div><div id="38514914" class="c"><input type="checkbox" id="c-38514914" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#38508508">prev</a><span>|</span><a href="#38511334">next</a><span>|</span><label class="collapse" for="c-38514914">[-]</label><label class="expand" for="c-38514914">[1 more]</label></div><br/><div class="children"><div class="content">Another visualization I would really love would be a clickable circular set of possible prediction branches, projected onto a Poincare disk (to handle the exponential branching component of it all). Would take forever to calculate except on smaller models, but being able to visualize branch probabilities angularly for the top n values or whatever, and to go forwards and backwards up and down different branches would likely yield some important insights into how they work.<p>Good visualization precludes good discoveries in many branches of science, I think.<p>(see my profile for a longer, potentially more silly description ;) )</div><br/></div></div><div id="38511334" class="c"><input type="checkbox" id="c-38511334" checked=""/><div class="controls bullet"><span class="by">holtkam2</span><span>|</span><a href="#38514914">prev</a><span>|</span><a href="#38513457">next</a><span>|</span><label class="collapse" for="c-38511334">[-]</label><label class="expand" for="c-38511334">[2 more]</label></div><br/><div class="children"><div class="content">The visualization I&#x27;ve been looking for for months. I would have happily paid serious money for this... the fact that it&#x27;s free is such a gift and I don&#x27;t take it for granted.</div><br/><div id="38513539" class="c"><input type="checkbox" id="c-38513539" checked=""/><div class="controls bullet"><span class="by">terminous</span><span>|</span><a href="#38511334">parent</a><span>|</span><a href="#38513457">next</a><span>|</span><label class="collapse" for="c-38513539">[-]</label><label class="expand" for="c-38513539">[1 more]</label></div><br/><div class="children"><div class="content">Same... this is like a textbook, but worth it</div><br/></div></div></div></div><div id="38513457" class="c"><input type="checkbox" id="c-38513457" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#38511334">prev</a><span>|</span><a href="#38509399">next</a><span>|</span><label class="collapse" for="c-38513457">[-]</label><label class="expand" for="c-38513457">[1 more]</label></div><br/><div class="children"><div class="content">I am looking at Brendenâs GitHub repo <a href="https:&#x2F;&#x2F;github.com&#x2F;bbycroft&#x2F;llm-viz">https:&#x2F;&#x2F;github.com&#x2F;bbycroft&#x2F;llm-viz</a><p>Really nice stuff.</div><br/></div></div><div id="38509399" class="c"><input type="checkbox" id="c-38509399" checked=""/><div class="controls bullet"><span class="by">wills_forward</span><span>|</span><a href="#38513457">prev</a><span>|</span><a href="#38507988">next</a><span>|</span><label class="collapse" for="c-38509399">[-]</label><label class="expand" for="c-38509399">[3 more]</label></div><br/><div class="children"><div class="content">My jaw drop to see algorhythmic complexity laid out so clearly in a 3d space like that. I wish I was smart enough to know if it&#x27;s accurate or not.</div><br/><div id="38509458" class="c"><input type="checkbox" id="c-38509458" checked=""/><div class="controls bullet"><span class="by">block_dagger</span><span>|</span><a href="#38509399">parent</a><span>|</span><a href="#38507988">next</a><span>|</span><label class="collapse" for="c-38509458">[-]</label><label class="expand" for="c-38509458">[2 more]</label></div><br/><div class="children"><div class="content">To know, you must perform intellectual work, not merely be smart. I bet you are smart enough.</div><br/><div id="38513575" class="c"><input type="checkbox" id="c-38513575" checked=""/><div class="controls bullet"><span class="by">nocoder</span><span>|</span><a href="#38509399">root</a><span>|</span><a href="#38509458">parent</a><span>|</span><a href="#38507988">next</a><span>|</span><label class="collapse" for="c-38513575">[-]</label><label class="expand" for="c-38513575">[1 more]</label></div><br/><div class="children"><div class="content">What a nice comment!! This has been a big failing of my mental model. I always believed if I was smart enough I should understand things without effort. Still trying to unlearn this....</div><br/></div></div></div></div></div></div><div id="38507988" class="c"><input type="checkbox" id="c-38507988" checked=""/><div class="controls bullet"><span class="by">gryfft</span><span>|</span><a href="#38509399">prev</a><span>|</span><a href="#38508906">next</a><span>|</span><label class="collapse" for="c-38507988">[-]</label><label class="expand" for="c-38507988">[2 more]</label></div><br/><div class="children"><div class="content">Damn, this looks <i>phenomenal.</i> I&#x27;ve been wanting to do a deep dive like this for a while-- the 3D model is a spectacular pedagogic device.</div><br/><div id="38510595" class="c"><input type="checkbox" id="c-38510595" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38507988">parent</a><span>|</span><a href="#38508906">next</a><span>|</span><label class="collapse" for="c-38510595">[-]</label><label class="expand" for="c-38510595">[1 more]</label></div><br/><div class="children"><div class="content">Andrej Karpathy twisting his hands as he explains it is also a great device. Not being sarcastic, when he explains it I understand it for a good minute it two. Then need to rewatch as I forget (but that is just me)!</div><br/></div></div></div></div><div id="38508906" class="c"><input type="checkbox" id="c-38508906" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#38507988">prev</a><span>|</span><a href="#38513916">next</a><span>|</span><label class="collapse" for="c-38508906">[-]</label><label class="expand" for="c-38508906">[1 more]</label></div><br/><div class="children"><div class="content">Could as well be titled &#x27;dissecting magic into matmuls and dot products for dummies&#x27;. Great stuff. Went away even more amazed that LLMs work as well as they do.</div><br/></div></div><div id="38513916" class="c"><input type="checkbox" id="c-38513916" checked=""/><div class="controls bullet"><span class="by">8f2ab37a-ed6c</span><span>|</span><a href="#38508906">prev</a><span>|</span><a href="#38511006">next</a><span>|</span><label class="collapse" for="c-38513916">[-]</label><label class="expand" for="c-38513916">[1 more]</label></div><br/><div class="children"><div class="content">Expecting someone to implement an LLM in Factorio any day now, we&#x27;re half-way there already with this blueprint.</div><br/></div></div><div id="38511006" class="c"><input type="checkbox" id="c-38511006" checked=""/><div class="controls bullet"><span class="by">flockonus</span><span>|</span><a href="#38513916">prev</a><span>|</span><a href="#38514698">next</a><span>|</span><label class="collapse" for="c-38511006">[-]</label><label class="expand" for="c-38511006">[1 more]</label></div><br/><div class="children"><div class="content">Twitter thread by the author sharing some extra context on this work: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;BrendanBycroft&#x2F;status&#x2F;1731042957149827140" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;BrendanBycroft&#x2F;status&#x2F;173104295714982714...</a></div><br/></div></div><div id="38514698" class="c"><input type="checkbox" id="c-38514698" checked=""/><div class="controls bullet"><span class="by">cod1r</span><span>|</span><a href="#38511006">prev</a><span>|</span><a href="#38509501">next</a><span>|</span><label class="collapse" for="c-38514698">[-]</label><label class="expand" for="c-38514698">[1 more]</label></div><br/><div class="children"><div class="content">Really cool stuff. Looks like an entire computer but with software. Definitely need to dig into more AI&#x2F;ML things.</div><br/></div></div><div id="38509501" class="c"><input type="checkbox" id="c-38509501" checked=""/><div class="controls bullet"><span class="by">atgctg</span><span>|</span><a href="#38514698">prev</a><span>|</span><a href="#38514991">next</a><span>|</span><label class="collapse" for="c-38509501">[-]</label><label class="expand" for="c-38509501">[22 more]</label></div><br/><div class="children"><div class="content">A lot of transformer explanations fail to mention what makes self attention so powerful.<p>Unlike traditional neural networks with fixed weights, self-attention layers adaptively weight connections between inputs based on context. This allows transformers to accomplish in a single layer what would take traditional networks multiple layers.</div><br/><div id="38509992" class="c"><input type="checkbox" id="c-38509992" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#38509501">parent</a><span>|</span><a href="#38509888">next</a><span>|</span><label class="collapse" for="c-38509992">[-]</label><label class="expand" for="c-38509992">[6 more]</label></div><br/><div class="children"><div class="content">In case itâs confusing for anyone to see âweightâ as a verb and a noun so close together, there are indeed two different things going on:<p>1. There are the model weights, aka the parameters. These are what get adjusted during training to do the learning part. They always exist.<p>2. There are attention weights. These are part of the transformer architecture and they âweightâ the context of the input. They are ephemeral. Used and discarded. Donât always exist.<p>They are both typically 32-bit floats in case youâre curious but still different concepts.</div><br/><div id="38510587" class="c"><input type="checkbox" id="c-38510587" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509992">parent</a><span>|</span><a href="#38514028">next</a><span>|</span><label class="collapse" for="c-38510587">[-]</label><label class="expand" for="c-38510587">[3 more]</label></div><br/><div class="children"><div class="content">I always thought the verb was &quot;weigh&quot; not &quot;weight&quot;, but apparently the latter is also in the dictionary as a verb.<p>Oh well... it seems like it&#x27;s more confusing than I thought <a href="https:&#x2F;&#x2F;www.merriam-webster.com&#x2F;wordplay&#x2F;when-to-use-weigh-and-weight-as-a-verb" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.merriam-webster.com&#x2F;wordplay&#x2F;when-to-use-weigh-a...</a></div><br/><div id="38510950" class="c"><input type="checkbox" id="c-38510950" checked=""/><div class="controls bullet"><span class="by">bobbylarrybobby</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510587">parent</a><span>|</span><a href="#38514028">next</a><span>|</span><label class="collapse" for="c-38510950">[-]</label><label class="expand" for="c-38510950">[2 more]</label></div><br/><div class="children"><div class="content">âTo weightâ is to <i>assign</i> a weight (e.g., to weight variables differently in a model), whereas âto weighâ is to <i>observe</i> and&#x2F;or <i>record</i> a weight (as a scale does).</div><br/><div id="38511049" class="c"><input type="checkbox" id="c-38511049" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510950">parent</a><span>|</span><a href="#38514028">next</a><span>|</span><label class="collapse" for="c-38511049">[-]</label><label class="expand" for="c-38511049">[1 more]</label></div><br/><div class="children"><div class="content">A few other cases of this sort of thing:<p>affect (n). an emotion or feeling. &quot;She has a positive affect.&quot;<p>effect (n). a result or change due to some event. &quot;The effect of her affect is to make people like her.&quot;<p>affect (v). to change or modify [X], have an effect upon [X]. &quot;The weather affects my affect.&quot;<p>effect (v). to bring about [X] or cause [X] to happen. &quot;Our protests are designed to effect change.&quot;<p>Also:<p>cost (v). to require a payment or loss of [X]. &quot;That apple will cost $5.&quot; Past tense cost: &quot;That apple cost $5.&quot;<p>cost (v). to estimate the price of [X]. &quot;The accounting department will cost the construction project at $5 million.&quot; Past tense costed. &quot;The accounting department costed the construction project at $5 million.&quot;</div><br/></div></div></div></div></div></div><div id="38514028" class="c"><input type="checkbox" id="c-38514028" checked=""/><div class="controls bullet"><span class="by">owlbite</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509992">parent</a><span>|</span><a href="#38510587">prev</a><span>|</span><a href="#38510158">next</a><span>|</span><label class="collapse" for="c-38514028">[-]</label><label class="expand" for="c-38514028">[1 more]</label></div><br/><div class="children"><div class="content">I think in most deployments, they&#x27;re not fp32 by the time you&#x27;re doing inference no them, they&#x27;ve been quantized, possibly down to 4 bits or even fewer.<p>On the training side I wouldn&#x27;t be surprised if they were bf16 rather than fp32.</div><br/></div></div><div id="38510158" class="c"><input type="checkbox" id="c-38510158" checked=""/><div class="controls bullet"><span class="by">kirill5pol</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509992">parent</a><span>|</span><a href="#38514028">prev</a><span>|</span><a href="#38509888">next</a><span>|</span><label class="collapse" for="c-38510158">[-]</label><label class="expand" for="c-38510158">[1 more]</label></div><br/><div class="children"><div class="content">I think a good way of explaining #2 is âweightâ in the sense of a weighted average</div><br/></div></div></div></div><div id="38509888" class="c"><input type="checkbox" id="c-38509888" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#38509501">parent</a><span>|</span><a href="#38509992">prev</a><span>|</span><a href="#38511533">next</a><span>|</span><label class="collapse" for="c-38509888">[-]</label><label class="expand" for="c-38509888">[14 more]</label></div><br/><div class="children"><div class="content">None of this seems obvious just reading the original <i>Attention is all you need</i> paper. Is there a more in-depth explanation of how this adaptive weighting works?</div><br/><div id="38510490" class="c"><input type="checkbox" id="c-38510490" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509888">parent</a><span>|</span><a href="#38510193">next</a><span>|</span><label class="collapse" for="c-38510490">[-]</label><label class="expand" for="c-38510490">[4 more]</label></div><br/><div class="children"><div class="content">The audience of this paper are other researchers who already know the concept of attention, which was very well known already in the field. In such research papers, such things are never explained again, as all the researchers already know this or can read other sources, which are cited, but focus on the actual research questions. In this case, the research question was simply: Can we get away by just using attention and not using the LSTM anymore? Before that, everyone was using both together.<p>I think learning it following it more this historical development can be helpful. E.g. in this case here, learn the concept of attention, specifically cross attention first. And that is this paper: Bahdanau, Cho, Bengio, &quot;Neural Machine Translation by Jointly Learning to Align and Translate&quot;, 2014, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473</a><p>That paper introduces it. But even that is maybe quite dense, and to really grasp it, it helps to reimplement those things.<p>It&#x27;s always dense, because those papers already have space constraints given by the conferences, max 9 pages or so. To get a better detailed overview, you can study the authors code, or other resources. There is a lot now about those topics, whole books, etc.</div><br/><div id="38510860" class="c"><input type="checkbox" id="c-38510860" checked=""/><div class="controls bullet"><span class="by">BOOSTERHIDROGEN</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510490">parent</a><span>|</span><a href="#38510193">next</a><span>|</span><label class="collapse" for="c-38510860">[-]</label><label class="expand" for="c-38510860">[3 more]</label></div><br/><div class="children"><div class="content">What books cover exclusively about this topic ? Thanks</div><br/><div id="38511570" class="c"><input type="checkbox" id="c-38511570" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510860">parent</a><span>|</span><a href="#38511336">next</a><span>|</span><label class="collapse" for="c-38511570">[-]</label><label class="expand" for="c-38511570">[1 more]</label></div><br/><div class="children"><div class="content">This is frequently a topic here on HN. E.g.:<p><a href="https:&#x2F;&#x2F;udlbook.github.io&#x2F;udlbook&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;udlbook.github.io&#x2F;udlbook&#x2F;</a> (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38424939">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38424939</a>)<p><a href="https:&#x2F;&#x2F;fleuret.org&#x2F;francois&#x2F;lbdl.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;fleuret.org&#x2F;francois&#x2F;lbdl.html</a> (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35767789">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35767789</a>)<p><a href="https:&#x2F;&#x2F;www.fast.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.fast.ai&#x2F;</a> (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24237207">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24237207</a>)<p><a href="https:&#x2F;&#x2F;d2l.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;d2l.ai&#x2F;</a> (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38428225">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38428225</a>)<p>Some more:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35543774">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35543774</a><p>There is a lot more. Just google for &quot;deep learning&quot;, and you&#x27;ll find a lot of content. And most of that will cover attention, as it is a really basic concept now.</div><br/></div></div><div id="38511336" class="c"><input type="checkbox" id="c-38511336" checked=""/><div class="controls bullet"><span class="by">CardenB</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510860">parent</a><span>|</span><a href="#38511570">prev</a><span>|</span><a href="#38510193">next</a><span>|</span><label class="collapse" for="c-38511336">[-]</label><label class="expand" for="c-38511336">[1 more]</label></div><br/><div class="children"><div class="content">I doubt any.</div><br/></div></div></div></div></div></div><div id="38510193" class="c"><input type="checkbox" id="c-38510193" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509888">parent</a><span>|</span><a href="#38510490">prev</a><span>|</span><a href="#38510467">next</a><span>|</span><label class="collapse" for="c-38510193">[-]</label><label class="expand" for="c-38510193">[3 more]</label></div><br/><div class="children"><div class="content">Itâs definitely not obvious no matter how smart you are!  The common metaphor used is itâs like a conversation.<p>Imagine you read one comment in some forum, posted in a long conversation thread. It wouldnât be obvious whatâs going on unless you read more of the thread right?<p>A single paper is like a single comment, in a thread that goes on for years and years.<p>For example, why donât papers explain what tokens&#x2F;vectors&#x2F;embedding layers are?  Well, they did already, except that comment in the thread came 2013 with the word2vec paper!<p>You might think wth? To keep up with this some one would have to spend a huge part of their time just reading papers. So yeah thatâs kind of what researchers do.<p>The alternative is to try to find where people have distilled down the important information or summarized it. Thatâs where books&#x2F;blogs&#x2F;youtube etc come in.</div><br/><div id="38510689" class="c"><input type="checkbox" id="c-38510689" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510193">parent</a><span>|</span><a href="#38510467">next</a><span>|</span><label class="collapse" for="c-38510689">[-]</label><label class="expand" for="c-38510689">[2 more]</label></div><br/><div class="children"><div class="content">Is there a way of finding interesting &quot;chains&quot; of such papers, short of scanning the references &#x2F; &quot;cited by&quot; page?<p>(For example, Google Scholar lists 98797 citations for Attention is all you need!)</div><br/><div id="38511464" class="c"><input type="checkbox" id="c-38511464" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510689">parent</a><span>|</span><a href="#38510467">next</a><span>|</span><label class="collapse" for="c-38511464">[-]</label><label class="expand" for="c-38511464">[1 more]</label></div><br/><div class="children"><div class="content">As a prerequisite to the attention paper? One to check out is:<p>A Survey on Contextual Embeddings
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2003.07278" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2003.07278</a><p>Embeddings are sort of what all this stuff is built on so it should help demystify the newer papers (itâs actually newer than the attention paper but a better overview than starting with the older word2vec paper).<p>Then after the attention paper an important one is:<p>Language Models are Few-Shot Learners
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.14165" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.14165</a><p>Iâm intentionally trying to not give a big list because theyâre so time-consuming. Iâm sure youâll quickly branch out based on your interests.</div><br/></div></div></div></div></div></div><div id="38510467" class="c"><input type="checkbox" id="c-38510467" checked=""/><div class="controls bullet"><span class="by">kaimac</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509888">parent</a><span>|</span><a href="#38510193">prev</a><span>|</span><a href="#38511124">next</a><span>|</span><label class="collapse" for="c-38510467">[-]</label><label class="expand" for="c-38510467">[3 more]</label></div><br/><div class="children"><div class="content">I found these notes very useful. They also contain a nice summary of how LLMs&#x2F;transformers work. It doesn&#x27;t help that people can&#x27;t seem to help taking a concept that has been around for decades (kernel smoothing) and giving it a fancy new name (attention).<p><a href="http:&#x2F;&#x2F;bactra.org&#x2F;notebooks&#x2F;nn-attention-and-transformers.html" rel="nofollow noreferrer">http:&#x2F;&#x2F;bactra.org&#x2F;notebooks&#x2F;nn-attention-and-transformers.ht...</a></div><br/><div id="38510710" class="c"><input type="checkbox" id="c-38510710" checked=""/><div class="controls bullet"><span class="by">CyberDildonics</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510467">parent</a><span>|</span><a href="#38511124">next</a><span>|</span><label class="collapse" for="c-38510710">[-]</label><label class="expand" for="c-38510710">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just as bad a &quot;convolutional neural networks&quot; instead of &quot;images being scaled down&quot;</div><br/><div id="38512260" class="c"><input type="checkbox" id="c-38512260" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38510710">parent</a><span>|</span><a href="#38511124">next</a><span>|</span><label class="collapse" for="c-38512260">[-]</label><label class="expand" for="c-38512260">[1 more]</label></div><br/><div class="children"><div class="content">âConvolutionâ is a pretty well established word for taking an operation and applying it sliding-window-style across a signal. Convnets are basically just a bunch of Hough transforms with learned convolution kernels.</div><br/></div></div></div></div></div></div><div id="38511124" class="c"><input type="checkbox" id="c-38511124" checked=""/><div class="controls bullet"><span class="by">gtoubassi</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509888">parent</a><span>|</span><a href="#38510467">prev</a><span>|</span><a href="#38510669">next</a><span>|</span><label class="collapse" for="c-38511124">[-]</label><label class="expand" for="c-38511124">[1 more]</label></div><br/><div class="children"><div class="content">I struggled to get an intuition for this, but on another HN thread earlier this year saw the recommendation for Sebastian Raschka&#x27;s series.   Starting with this video: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mDZil99CtSU" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mDZil99CtSU</a> and maybe the next three or four.  It was really helpful to get a sense of the original 2014 concept of attention which is easier to understand but less powerful (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473</a>), and then how it gets powerful with the more modern notion of attention.  So if you have a reasonable intuition for &quot;regular&quot; ANNs I think this is a great place to start.</div><br/></div></div><div id="38510669" class="c"><input type="checkbox" id="c-38510669" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509888">parent</a><span>|</span><a href="#38511124">prev</a><span>|</span><a href="#38510354">next</a><span>|</span><label class="collapse" for="c-38510669">[-]</label><label class="expand" for="c-38510669">[1 more]</label></div><br/><div class="children"><div class="content">Turns out Attention is all you need isn&#x27;t all you need!<p>(I&#x27;m sorry)</div><br/></div></div><div id="38510354" class="c"><input type="checkbox" id="c-38510354" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#38509501">root</a><span>|</span><a href="#38509888">parent</a><span>|</span><a href="#38510669">prev</a><span>|</span><a href="#38511533">next</a><span>|</span><label class="collapse" for="c-38510354">[-]</label><label class="expand" for="c-38510354">[1 more]</label></div><br/><div class="children"><div class="content">softmax(QK) gives you a probability matrix of shape [seq, seq]. Think of this like an adjacency matrix with edges with flow weights that are probabilities. Hence semantic routing of parts of X reduced with V.<p>where<p>- Q = X @ W_Q [query]<p>- K = X @ W_K [key]<p>- V = X @ V [value]<p>- X [input]<p>hence<p>attn_head_i = (softmax(Q@K&#x2F;normalizing term) @ V)<p>Each head corresponds to a different concurrent routing system<p>The transformer just adds normalization and mlp feature learning parts around that.</div><br/></div></div></div></div><div id="38511533" class="c"><input type="checkbox" id="c-38511533" checked=""/><div class="controls bullet"><span class="by">lchengify</span><span>|</span><a href="#38509501">parent</a><span>|</span><a href="#38509888">prev</a><span>|</span><a href="#38514991">next</a><span>|</span><label class="collapse" for="c-38511533">[-]</label><label class="expand" for="c-38511533">[1 more]</label></div><br/><div class="children"><div class="content">Just to add on, a good way to learn these terms is to look at the history of neural networks rather than looking at transformer architecture in a vacuum<p>This [1] post from 2021 goes over attention mechanisms as applied to RNN &#x2F; LSTM networks. It&#x27;s visual and goes into a bit more detail, and I&#x27;ve personally found RNN &#x2F; LSTM networks easier to understand intuitively.<p>[1] <a href="https:&#x2F;&#x2F;medium.com&#x2F;swlh&#x2F;a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b" rel="nofollow noreferrer">https:&#x2F;&#x2F;medium.com&#x2F;swlh&#x2F;a-simple-overview-of-rnn-lstm-and-at...</a></div><br/></div></div></div></div><div id="38514991" class="c"><input type="checkbox" id="c-38514991" checked=""/><div class="controls bullet"><span class="by">Arctic_fly</span><span>|</span><a href="#38509501">prev</a><span>|</span><a href="#38514603">next</a><span>|</span><label class="collapse" for="c-38514991">[-]</label><label class="expand" for="c-38514991">[1 more]</label></div><br/><div class="children"><div class="content">Curse you for being interesting enough to make me get on my desktop.</div><br/></div></div><div id="38514603" class="c"><input type="checkbox" id="c-38514603" checked=""/><div class="controls bullet"><span class="by">meeby</span><span>|</span><a href="#38514991">prev</a><span>|</span><a href="#38508240">next</a><span>|</span><label class="collapse" for="c-38514603">[-]</label><label class="expand" for="c-38514603">[1 more]</label></div><br/><div class="children"><div class="content">This is easily the best visualization I&#x27;ve seen for a long time. Fantastic work!</div><br/></div></div><div id="38508240" class="c"><input type="checkbox" id="c-38508240" checked=""/><div class="controls bullet"><span class="by">airesQ</span><span>|</span><a href="#38514603">prev</a><span>|</span><a href="#38514496">next</a><span>|</span><label class="collapse" for="c-38508240">[-]</label><label class="expand" for="c-38508240">[1 more]</label></div><br/><div class="children"><div class="content">Incredible work.<p>So much depth; initially I thought it&#x27;s &quot;just&quot; a 3d model. The animations are amazing.</div><br/></div></div><div id="38514496" class="c"><input type="checkbox" id="c-38514496" checked=""/><div class="controls bullet"><span class="by">crotchfire</span><span>|</span><a href="#38508240">prev</a><span>|</span><a href="#38509670">next</a><span>|</span><label class="collapse" for="c-38514496">[-]</label><label class="expand" for="c-38514496">[3 more]</label></div><br/><div class="children"><div class="content">Application error: a client-side exception has occurred (see the browser console for more information).</div><br/><div id="38514635" class="c"><input type="checkbox" id="c-38514635" checked=""/><div class="controls bullet"><span class="by">nandhinianand</span><span>|</span><a href="#38514496">parent</a><span>|</span><a href="#38509670">next</a><span>|</span><label class="collapse" for="c-38514635">[-]</label><label class="expand" for="c-38514635">[2 more]</label></div><br/><div class="children"><div class="content">Seems brave blocks some js scripts .. this works in Firefox</div><br/><div id="38514943" class="c"><input type="checkbox" id="c-38514943" checked=""/><div class="controls bullet"><span class="by">crotchfire</span><span>|</span><a href="#38514496">root</a><span>|</span><a href="#38514635">parent</a><span>|</span><a href="#38509670">next</a><span>|</span><label class="collapse" for="c-38514943">[-]</label><label class="expand" for="c-38514943">[1 more]</label></div><br/><div class="children"><div class="content">Not using brave.<p>I get the same fail in both firefox and (chromium-based) qutebrowser.<p>The web is where useful error messages go to die.</div><br/></div></div></div></div></div></div><div id="38509670" class="c"><input type="checkbox" id="c-38509670" checked=""/><div class="controls bullet"><span class="by">skadamat</span><span>|</span><a href="#38514496">prev</a><span>|</span><a href="#38513205">next</a><span>|</span><label class="collapse" for="c-38509670">[-]</label><label class="expand" for="c-38509670">[2 more]</label></div><br/><div class="children"><div class="content">If folks want a lower dimensional version of this for their own models, I&#x27;m a big fan of the Netron library for model architecture visualization.<p>Wrote about it here: <a href="https:&#x2F;&#x2F;about.xethub.com&#x2F;blog&#x2F;visualizing-ml-models-github-netron" rel="nofollow noreferrer">https:&#x2F;&#x2F;about.xethub.com&#x2F;blog&#x2F;visualizing-ml-models-github-n...</a></div><br/><div id="38510427" class="c"><input type="checkbox" id="c-38510427" checked=""/><div class="controls bullet"><span class="by">thierrydamiba</span><span>|</span><a href="#38509670">parent</a><span>|</span><a href="#38513205">next</a><span>|</span><label class="collapse" for="c-38510427">[-]</label><label class="expand" for="c-38510427">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing.<p>What an exciting time to be learning about LLMs. Everyday I come across a new resource, and everything is free!</div><br/></div></div></div></div><div id="38513205" class="c"><input type="checkbox" id="c-38513205" checked=""/><div class="controls bullet"><span class="by">johnklos</span><span>|</span><a href="#38509670">prev</a><span>|</span><a href="#38513426">next</a><span>|</span><label class="collapse" for="c-38513205">[-]</label><label class="expand" for="c-38513205">[4 more]</label></div><br/><div class="children"><div class="content">Am I the only one getting &quot;Application error: a client-side exception has occurred (see the browser console for more information).&quot; messages?</div><br/><div id="38513231" class="c"><input type="checkbox" id="c-38513231" checked=""/><div class="controls bullet"><span class="by">lopkeny12ko</span><span>|</span><a href="#38513205">parent</a><span>|</span><a href="#38514019">next</a><span>|</span><label class="collapse" for="c-38513231">[-]</label><label class="expand" for="c-38513231">[2 more]</label></div><br/><div class="children"><div class="content">Same here. I blame the popularity of Next.js. More and more of the web is slowly becoming more broken on Firefox on Linux, all with the same tired error: &quot;Application error: a client-side exception has occurred&quot;</div><br/><div id="38513284" class="c"><input type="checkbox" id="c-38513284" checked=""/><div class="controls bullet"><span class="by">HellsMaddy</span><span>|</span><a href="#38513205">root</a><span>|</span><a href="#38513231">parent</a><span>|</span><a href="#38514019">next</a><span>|</span><label class="collapse" for="c-38513284">[-]</label><label class="expand" for="c-38513284">[1 more]</label></div><br/><div class="children"><div class="content">Works fine on Firefox on Linux for me.</div><br/></div></div></div></div><div id="38514019" class="c"><input type="checkbox" id="c-38514019" checked=""/><div class="controls bullet"><span class="by">altilunium</span><span>|</span><a href="#38513205">parent</a><span>|</span><a href="#38513231">prev</a><span>|</span><a href="#38513426">next</a><span>|</span><label class="collapse" for="c-38514019">[-]</label><label class="expand" for="c-38514019">[1 more]</label></div><br/><div class="children"><div class="content">It is possible that your machine does not yet support WebGL2.<p>Check here : <a href="https:&#x2F;&#x2F;get.webgl.org&#x2F;webgl2&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;get.webgl.org&#x2F;webgl2&#x2F;</a></div><br/></div></div></div></div><div id="38513426" class="c"><input type="checkbox" id="c-38513426" checked=""/><div class="controls bullet"><span class="by">abrookewood</span><span>|</span><a href="#38513205">prev</a><span>|</span><a href="#38509286">next</a><span>|</span><label class="collapse" for="c-38513426">[-]</label><label class="expand" for="c-38513426">[1 more]</label></div><br/><div class="children"><div class="content">This does an amazing job of showing the difference in complexity between the different models. Click on GPT-3 and you should be able to see all 4 models side-by-side. GPT-3 is a monster compared to nano-gpt.</div><br/></div></div><div id="38509286" class="c"><input type="checkbox" id="c-38509286" checked=""/><div class="controls bullet"><span class="by">hmate9</span><span>|</span><a href="#38513426">prev</a><span>|</span><a href="#38509686">next</a><span>|</span><label class="collapse" for="c-38509286">[-]</label><label class="expand" for="c-38509286">[1 more]</label></div><br/><div class="children"><div class="content">This is a phenomenal visualisation. I wish I saw this when I was trying to wrap my head around transformers a while ago. This would have made it so much easier.</div><br/></div></div><div id="38509686" class="c"><input type="checkbox" id="c-38509686" checked=""/><div class="controls bullet"><span class="by">arikrak</span><span>|</span><a href="#38509286">prev</a><span>|</span><a href="#38511896">next</a><span>|</span><label class="collapse" for="c-38509686">[-]</label><label class="expand" for="c-38509686">[3 more]</label></div><br/><div class="children"><div class="content">This looks pretty cool! Anyone know of visualizations for simpler neural networks? I&#x27;m aware of tensorflow playground but that&#x27;s just for a toy example, is there anything for visualizing a real example (e.g handwriting recognition)?</div><br/><div id="38510083" class="c"><input type="checkbox" id="c-38510083" checked=""/><div class="controls bullet"><span class="by">Logge</span><span>|</span><a href="#38509686">parent</a><span>|</span><a href="#38510417">next</a><span>|</span><label class="collapse" for="c-38510083">[-]</label><label class="expand" for="c-38510083">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;okdalto.github.io&#x2F;VisualizeMNIST_web&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;okdalto.github.io&#x2F;VisualizeMNIST_web&#x2F;</a></div><br/></div></div><div id="38510417" class="c"><input type="checkbox" id="c-38510417" checked=""/><div class="controls bullet"><span class="by">atonalfreerider</span><span>|</span><a href="#38509686">parent</a><span>|</span><a href="#38510083">prev</a><span>|</span><a href="#38511896">next</a><span>|</span><label class="collapse" for="c-38510417">[-]</label><label class="expand" for="c-38510417">[1 more]</label></div><br/><div class="children"><div class="content">We made a VR visualization back in 2017
<a href="https:&#x2F;&#x2F;youtu.be&#x2F;x6y14yAJ9rY" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;x6y14yAJ9rY</a></div><br/></div></div></div></div><div id="38511896" class="c"><input type="checkbox" id="c-38511896" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#38509686">prev</a><span>|</span><a href="#38509457">next</a><span>|</span><label class="collapse" for="c-38511896">[-]</label><label class="expand" for="c-38511896">[2 more]</label></div><br/><div class="children"><div class="content">What happened to this thread? When I saw it before it had 700+ upvotes.</div><br/><div id="38511905" class="c"><input type="checkbox" id="c-38511905" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#38511896">parent</a><span>|</span><a href="#38509457">next</a><span>|</span><label class="collapse" for="c-38511905">[-]</label><label class="expand" for="c-38511905">[1 more]</label></div><br/><div class="children"><div class="content">Oh: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38511659">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38511659</a></div><br/></div></div></div></div><div id="38509457" class="c"><input type="checkbox" id="c-38509457" checked=""/><div class="controls bullet"><span class="by">drdg</span><span>|</span><a href="#38511896">prev</a><span>|</span><a href="#38511758">next</a><span>|</span><label class="collapse" for="c-38509457">[-]</label><label class="expand" for="c-38509457">[1 more]</label></div><br/><div class="children"><div class="content">Very cool. The explanations of what each part is doing is really insightful.
And I especially like how the scale jumps when you move from e.g. Nano all the way to GPT-3 ....</div><br/></div></div><div id="38511758" class="c"><input type="checkbox" id="c-38511758" checked=""/><div class="controls bullet"><span class="by">thistoowontpass</span><span>|</span><a href="#38509457">prev</a><span>|</span><a href="#38510265">next</a><span>|</span><label class="collapse" for="c-38511758">[-]</label><label class="expand" for="c-38511758">[1 more]</label></div><br/><div class="children"><div class="content">Thank you. I&#x27;d just completed doing this manually (much uglier and less accurate) and so can really appreciate the effort behind this.</div><br/></div></div><div id="38510265" class="c"><input type="checkbox" id="c-38510265" checked=""/><div class="controls bullet"><span class="by">shaburn</span><span>|</span><a href="#38511758">prev</a><span>|</span><a href="#38508406">next</a><span>|</span><label class="collapse" for="c-38510265">[-]</label><label class="expand" for="c-38510265">[1 more]</label></div><br/><div class="children"><div class="content">Visualization never seems to get the credit due in software development. This is amazing.</div><br/></div></div><div id="38508406" class="c"><input type="checkbox" id="c-38508406" checked=""/><div class="controls bullet"><span class="by">thefourthchime</span><span>|</span><a href="#38510265">prev</a><span>|</span><a href="#38511776">next</a><span>|</span><label class="collapse" for="c-38508406">[-]</label><label class="expand" for="c-38508406">[2 more]</label></div><br/><div class="children"><div class="content">First off, this is fabulous work. I went through it for the Nano, but is there a way to do the step-by-step for the other LLMs?</div><br/><div id="38508497" class="c"><input type="checkbox" id="c-38508497" checked=""/><div class="controls bullet"><span class="by">Heidaradar</span><span>|</span><a href="#38508406">parent</a><span>|</span><a href="#38511776">next</a><span>|</span><label class="collapse" for="c-38508497">[-]</label><label class="expand" for="c-38508497">[1 more]</label></div><br/><div class="children"><div class="content">Below the title, there&#x27;s a few others you can choose from (GPT-2 small and XL and GPT-3)</div><br/></div></div></div></div><div id="38511776" class="c"><input type="checkbox" id="c-38511776" checked=""/><div class="controls bullet"><span class="by">nikhil896</span><span>|</span><a href="#38508406">prev</a><span>|</span><a href="#38509188">next</a><span>|</span><label class="collapse" for="c-38511776">[-]</label><label class="expand" for="c-38511776">[1 more]</label></div><br/><div class="children"><div class="content">This is by far the best resource I&#x27;ve seen to understand LLMs. Incredibly well done! Thanks for this awesome tool</div><br/></div></div><div id="38509188" class="c"><input type="checkbox" id="c-38509188" checked=""/><div class="controls bullet"><span class="by">Simon_ORourke</span><span>|</span><a href="#38511776">prev</a><span>|</span><a href="#38512278">next</a><span>|</span><label class="collapse" for="c-38509188">[-]</label><label class="expand" for="c-38509188">[2 more]</label></div><br/><div class="children"><div class="content">This is brilliant work, thanks for sharing.</div><br/><div id="38510263" class="c"><input type="checkbox" id="c-38510263" checked=""/><div class="controls bullet"><span class="by">reexpressionist</span><span>|</span><a href="#38509188">parent</a><span>|</span><a href="#38512278">next</a><span>|</span><label class="collapse" for="c-38510263">[-]</label><label class="expand" for="c-38510263">[1 more]</label></div><br/><div class="children"><div class="content">Ditto. This is the most sophisticated viz of parameters I&#x27;ve seen...and it&#x27;s also an interactive, step-through tutorial!</div><br/></div></div></div></div><div id="38512278" class="c"><input type="checkbox" id="c-38512278" checked=""/><div class="controls bullet"><span class="by">RecycledEle</span><span>|</span><a href="#38509188">prev</a><span>|</span><a href="#38511775">next</a><span>|</span><label class="collapse" for="c-38512278">[-]</label><label class="expand" for="c-38512278">[1 more]</label></div><br/><div class="children"><div class="content">This is excellent!<p>This is why I love Hacker News!</div><br/></div></div><div id="38511775" class="c"><input type="checkbox" id="c-38511775" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#38512278">prev</a><span>|</span><a href="#38508813">next</a><span>|</span><label class="collapse" for="c-38511775">[-]</label><label class="expand" for="c-38511775">[2 more]</label></div><br/><div class="children"><div class="content">The score on this post just went down by a factor of 10 and the time went to &quot;1 hour&quot; ago?!</div><br/><div id="38512067" class="c"><input type="checkbox" id="c-38512067" checked=""/><div class="controls bullet"><span class="by">myself248</span><span>|</span><a href="#38511775">parent</a><span>|</span><a href="#38508813">next</a><span>|</span><label class="collapse" for="c-38512067">[-]</label><label class="expand" for="c-38512067">[1 more]</label></div><br/><div class="children"><div class="content">Another post was merged with it: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38507672">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38507672</a></div><br/></div></div></div></div><div id="38508813" class="c"><input type="checkbox" id="c-38508813" checked=""/><div class="controls bullet"><span class="by">gbertasius</span><span>|</span><a href="#38511775">prev</a><span>|</span><label class="collapse" for="c-38508813">[-]</label><label class="expand" for="c-38508813">[1 more]</label></div><br/><div class="children"><div class="content">This is AMAZING! I&#x27;m about to go into Uni and this will be useful for my ML classes.</div><br/></div></div></div></div></div></div></div></body></html>