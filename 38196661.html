<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699520478082" as="style"/><link rel="stylesheet" href="styles.css?v=1699520478082"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/punica-ai/punica">Punica: Serving multiple LoRA finetuned LLM as one</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>abcdabcd987</span> | <span>21 comments</span></div><br/><div><div id="38201156" class="c"><input type="checkbox" id="c-38201156" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#38201398">next</a><span>|</span><label class="collapse" for="c-38201156">[-]</label><label class="expand" for="c-38201156">[3 more]</label></div><br/><div class="children"><div class="content">I think this is one of the most important possible works for open source LLM&#x27;s, really glad y&#x27;all pushed this forward!<p>That&#x27;s not hyperbole. Why is OpenAI able to charge so little for their API&#x27;s? I have heard rival mega LLM company CEO&#x27;s complain that OpenAI&#x27;s prices would be a loss for their rivals. But I think it&#x27;s still positive margin, and that they can charge low prices for API because they&#x27;ve invested more into managing the infra, sure, but most importantly because they have the best utilization of their existing hardware.<p>If it costs everyone $X&#x2F;gpu&#x2F;hr to serve models, the company that has the most throughput wins on price. In a world without finetunes, the most capable model, the one that can zero- or few-shot the most tasks will have the most usage. Finetuned open models can reach parity with GPT on narrow tasks, but until now, having public providers serve the models was expensive. Your private finetune is only going to be queried by you, not everyone, so it&#x27;s super expensive to serve on a per token level. With hot swappable LoRA adapters, that calculus changes, and the cost per token can go way down. Super, super exciting!</div><br/><div id="38202127" class="c"><input type="checkbox" id="c-38202127" checked=""/><div class="controls bullet"><span class="by">therealpygon</span><span>|</span><a href="#38201156">parent</a><span>|</span><a href="#38201469">next</a><span>|</span><label class="collapse" for="c-38202127">[-]</label><label class="expand" for="c-38202127">[1 more]</label></div><br/><div class="children"><div class="content">Doesn’t OpenAI still operate at significant losses by  using massive infusions of capital from Microsoft and other investors? If you are giving away half your product, it’s not surprising that they would be undercutting competition. Not a new strategy.<p>Underprice to avoid or drive out competition and encourage lock-in, then increase prices when you no longer have competitors or your user base is large enough and reliant enough that your attrition is manageable. Then you sell to a bigger company who grinds it up and integrates into their own products. Same as always. Bonus points if you claim to be open source for the free marketing and&#x2F;or free development&#x2F;testing in the form of user contributions before switching to a proprietary model.<p>Shouldn’t we have a standardized corporate strategy  bingo card by now?</div><br/></div></div><div id="38201469" class="c"><input type="checkbox" id="c-38201469" checked=""/><div class="controls bullet"><span class="by">ghotli</span><span>|</span><a href="#38201156">parent</a><span>|</span><a href="#38202127">prev</a><span>|</span><a href="#38201398">next</a><span>|</span><label class="collapse" for="c-38201469">[-]</label><label class="expand" for="c-38201469">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. I&#x27;m not so sure I really &#x27;got&#x27; that part of finetunes &#x2F; LoRA adapters before reading this comment. Makes me want to make one to take it for a spin, see what comes out the other side.</div><br/></div></div></div></div><div id="38201398" class="c"><input type="checkbox" id="c-38201398" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#38201156">prev</a><span>|</span><a href="#38199152">next</a><span>|</span><label class="collapse" for="c-38201398">[-]</label><label class="expand" for="c-38201398">[3 more]</label></div><br/><div class="children"><div class="content">This is amazing, and will unlock many possibilities. I just recently read the S-LoRA paper, which is related, but it&#x27;s even better to have a working (and extremely efficient!) implementation.<p>How hard would it be to adapt your kernels to work with the new-gen quants like AWQ or EXL2?</div><br/><div id="38201489" class="c"><input type="checkbox" id="c-38201489" checked=""/><div class="controls bullet"><span class="by">abcdabcd987</span><span>|</span><a href="#38201398">parent</a><span>|</span><a href="#38199152">next</a><span>|</span><label class="collapse" for="c-38201489">[-]</label><label class="expand" for="c-38201489">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for your encouragement! We are working on quantization as well. We recently submitted a paper, Atom [1], that uses 4-bit quantization, delivering 7.73x throughput compared to FP16 and 2.53x compared to INT8. Atom is able to maintain a perplexity (i.e., model accuracy) close to FP16, outperforming existing quantization approaches.<p>We are polishing the 4-bit code. It will be added to Punica code base soon. Please stay tuned :)<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.19102" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.19102</a></div><br/><div id="38201792" class="c"><input type="checkbox" id="c-38201792" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#38201398">root</a><span>|</span><a href="#38201489">parent</a><span>|</span><a href="#38199152">next</a><span>|</span><label class="collapse" for="c-38201792">[-]</label><label class="expand" for="c-38201792">[1 more]</label></div><br/><div class="children"><div class="content">Added to my reading list! The world of quantizations is moving so fast even TheBloke might not be able to keep up!<p>So Atom base models would be compatible with Punica?<p>I also wonder, many people already train LoRAs in 8 or even 4 bit (for the base model), would it make sense to match the quantization algo used during training and inference?</div><br/></div></div></div></div></div></div><div id="38199152" class="c"><input type="checkbox" id="c-38199152" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#38201398">prev</a><span>|</span><a href="#38200328">next</a><span>|</span><label class="collapse" for="c-38199152">[-]</label><label class="expand" for="c-38199152">[2 more]</label></div><br/><div class="children"><div class="content">Awesome work! Here&#x27;s a recent paper released yesterday, also focused on efficiently serving many LoRAs simultaneously: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03285" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03285</a><p>Really looking forward to these innovations becoming more widespread -- I expect we&#x27;re very close to a world where training a LoRA on a one-off task like &quot;review every HN post from the last 3 years and flag any of them that contain informed speculation about the architecture of GPT-4&quot; will be easy, cheap and routine.</div><br/><div id="38200834" class="c"><input type="checkbox" id="c-38200834" checked=""/><div class="controls bullet"><span class="by">abcdabcd987</span><span>|</span><a href="#38199152">parent</a><span>|</span><a href="#38200328">next</a><span>|</span><label class="collapse" for="c-38200834">[-]</label><label class="expand" for="c-38200834">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! We are also very excited about combining the fast fine-tuning and efficient serving. In fact, what you just said is very related to one of our very first motivations. In my previous blog post [1], I call this scheme &quot;Just-in-time Fine-tuning&quot;. Our previous measurement is that, for a medium-sized webpage (~10K tokens), it takes around 30 seconds to 2 minutes to finetune a LoRA model. Another good side of this JIT fine-tuning scheme is that, we can turn any model into a long-context model.<p>We&#x27;ll keep doing more research on finetuning. And hopefully, we&#x27;ll see the results soon.<p>[1] <a href="https:&#x2F;&#x2F;le.qun.ch&#x2F;en&#x2F;blog&#x2F;2023&#x2F;09&#x2F;11&#x2F;multi-lora-potentials&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;le.qun.ch&#x2F;en&#x2F;blog&#x2F;2023&#x2F;09&#x2F;11&#x2F;multi-lora-potentials&#x2F;</a></div><br/></div></div></div></div><div id="38200328" class="c"><input type="checkbox" id="c-38200328" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#38199152">prev</a><span>|</span><a href="#38198952">next</a><span>|</span><label class="collapse" for="c-38200328">[-]</label><label class="expand" for="c-38200328">[4 more]</label></div><br/><div class="children"><div class="content">Am I correct in understanding that LoRA is basically a way to cheaply create “delta” LLMs that apply onto the main large one to create a specialization? In other words, this would obviate all the vector DB stuff that people are doing right?</div><br/><div id="38202198" class="c"><input type="checkbox" id="c-38202198" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#38200328">parent</a><span>|</span><a href="#38200566">next</a><span>|</span><label class="collapse" for="c-38202198">[-]</label><label class="expand" for="c-38202198">[1 more]</label></div><br/><div class="children"><div class="content">Best as I can tell lora is useful for steering the models behaviour while injecting 100% new knowledge is still largely via rag - so vector db</div><br/></div></div><div id="38200566" class="c"><input type="checkbox" id="c-38200566" checked=""/><div class="controls bullet"><span class="by">lamroger</span><span>|</span><a href="#38200328">parent</a><span>|</span><a href="#38202198">prev</a><span>|</span><a href="#38198952">next</a><span>|</span><label class="collapse" for="c-38200566">[-]</label><label class="expand" for="c-38200566">[2 more]</label></div><br/><div class="children"><div class="content">The general consensus imo is that fine-tuning is more for tone and style vs accuracy. People use vector DBs to grab relevant data to throw into the prompt and call it Retrieval Augmented Generation.<p>From what this seems to do is host multiple deltas fine-tunings and hot swap as needed. Incredible optimization. It&#x27;s like going from AMIs to ECS or Kubernetes.</div><br/></div></div></div></div><div id="38198952" class="c"><input type="checkbox" id="c-38198952" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#38200328">prev</a><span>|</span><a href="#38197263">next</a><span>|</span><label class="collapse" for="c-38198952">[-]</label><label class="expand" for="c-38198952">[2 more]</label></div><br/><div class="children"><div class="content">Nice!<p>Any thoughts as to how this would come together with serving frameworks like vLLM, lmdeploy, Triton Inference Server, etc?</div><br/><div id="38200895" class="c"><input type="checkbox" id="c-38200895" checked=""/><div class="controls bullet"><span class="by">abcdabcd987</span><span>|</span><a href="#38198952">parent</a><span>|</span><a href="#38197263">next</a><span>|</span><label class="collapse" for="c-38200895">[-]</label><label class="expand" for="c-38200895">[1 more]</label></div><br/><div class="children"><div class="content">Certainly! We&#x27;d like our good designs to be picked up by frameworks and serve all users. Currently, Punica is built on top of PyTorch and HuggingFace Transformers ecosystems. Therefore, vLLM and LMDeploy, which are also in the PyTorch ecosystem, should have a smooth adaption. As for Nvidia Triton and TensorRT-LLM, since our kernels are written in CUDA, I believe it will also work seamlessly.<p>We call for the open source community to help us integrate Punica with all frameworks, thus the whole society can benefit from the efficiency improvement!</div><br/></div></div></div></div><div id="38197263" class="c"><input type="checkbox" id="c-38197263" checked=""/><div class="controls bullet"><span class="by">yyding</span><span>|</span><a href="#38198952">prev</a><span>|</span><a href="#38196838">next</a><span>|</span><label class="collapse" for="c-38197263">[-]</label><label class="expand" for="c-38197263">[2 more]</label></div><br/><div class="children"><div class="content">Good job! 
I observed that you implemented many cuda kernels by yourselves. Just wondering your consideration or trade-off between implementating the kernels via pure CUDA code vs. implementing based on compiler like TVM&#x2F;Triton.</div><br/><div id="38198881" class="c"><input type="checkbox" id="c-38198881" checked=""/><div class="controls bullet"><span class="by">zhye</span><span>|</span><a href="#38197263">parent</a><span>|</span><a href="#38196838">next</a><span>|</span><label class="collapse" for="c-38198881">[-]</label><label class="expand" for="c-38198881">[1 more]</label></div><br/><div class="children"><div class="content">Good question, in general implementing kernels on page tables is tricky in Tensor Compilers because integer set analysis might fail sometimes (but can be fixed with some tweaks). I think using compilers like TVM can help deploy serving systems on different platforms (e.g. AMD GPUs) and I&#x27;m optimistic about this direction (and we have to make Tensor Compilers more user-friendly).</div><br/></div></div></div></div><div id="38196838" class="c"><input type="checkbox" id="c-38196838" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#38197263">prev</a><span>|</span><a href="#38199260">next</a><span>|</span><label class="collapse" for="c-38196838">[-]</label><label class="expand" for="c-38196838">[2 more]</label></div><br/><div class="children"><div class="content">This is great! Have you guys considered integrating with one of the existing systems?</div><br/><div id="38198946" class="c"><input type="checkbox" id="c-38198946" checked=""/><div class="controls bullet"><span class="by">abcdabcd987</span><span>|</span><a href="#38196838">parent</a><span>|</span><a href="#38199260">next</a><span>|</span><label class="collapse" for="c-38198946">[-]</label><label class="expand" for="c-38198946">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the question. Currently Punica is on the ecosystem of PyTorch and HuggingFace Transformers. So PyTorch users can start to use Punica now.<p>Look forward to collaboration with TVM and MLC to reach more users :)</div><br/></div></div></div></div><div id="38199260" class="c"><input type="checkbox" id="c-38199260" checked=""/><div class="controls bullet"><span class="by">ruihangl</span><span>|</span><a href="#38196838">prev</a><span>|</span><label class="collapse" for="c-38199260">[-]</label><label class="expand" for="c-38199260">[2 more]</label></div><br/><div class="children"><div class="content">Great work! I am curious that how much effort it would take to support LoRAs with different ranks?</div><br/><div id="38201569" class="c"><input type="checkbox" id="c-38201569" checked=""/><div class="controls bullet"><span class="by">zhye</span><span>|</span><a href="#38199260">parent</a><span>|</span><label class="collapse" for="c-38201569">[-]</label><label class="expand" for="c-38201569">[1 more]</label></div><br/><div class="children"><div class="content">It will take some effort to implement operators but not too much (cutlass&#x27;s group gemm already support different mnk&#x27;s), however the performance benefit is marginal  compared to padding all LoRA ranks to the same rank because all these kernels are not compute bound.</div><br/></div></div></div></div></div></div></div></div></div></body></html>