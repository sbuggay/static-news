<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1731920460311" as="style"/><link rel="stylesheet" href="styles.css?v=1731920460311"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://fleetwood.dev/posts/you-could-have-designed-SOTA-positional-encoding">You could have designed state of the art positional encoding</a> <span class="domain">(<a href="https://fleetwood.dev">fleetwood.dev</a>)</span></div><div class="subtext"><span>Philpax</span> | <span>16 comments</span></div><br/><div><div id="42170851" class="c"><input type="checkbox" id="c-42170851" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42169136">next</a><span>|</span><label class="collapse" for="c-42170851">[-]</label><label class="expand" for="c-42170851">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone know why 2D rope implementations apply two separate 1D rotations to pairs, instead of applying a 2d rotation to triplets?</div><br/></div></div><div id="42169136" class="c"><input type="checkbox" id="c-42169136" checked=""/><div class="controls bullet"><span class="by">rgovostes</span><span>|</span><a href="#42170851">prev</a><span>|</span><a href="#42169009">next</a><span>|</span><label class="collapse" for="c-42169136">[-]</label><label class="expand" for="c-42169136">[1 more]</label></div><br/><div class="children"><div class="content">Thanks to the author for clarifying something that&#x27;s been a mystery to me for a few years. The positional encoding scheme in the &quot;Attention Is All You Need&quot; paper is only given half a page and the construction appears to come out of nowhere.</div><br/></div></div><div id="42169009" class="c"><input type="checkbox" id="c-42169009" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#42169136">prev</a><span>|</span><a href="#42170747">next</a><span>|</span><label class="collapse" for="c-42169009">[-]</label><label class="expand" for="c-42169009">[1 more]</label></div><br/><div class="children"><div class="content">One of the things I really love about rope is that it allows for a lot of interesting encoding schemes at inference time without model retraining. I’ve had a lot of fun playing with different relative positions. You can elicit a lot of interesting behaviors from the model when you use different rotations for keys vs queries, they don’t always have to match.<p>For example exact position doesn’t matter too much when tokens are spaced out. Let’s say you use token position 100 for your query, you can shift all the keys around position 100, and the further they are back in the context the more freedom you have to play with the value.</div><br/></div></div><div id="42170747" class="c"><input type="checkbox" id="c-42170747" checked=""/><div class="controls bullet"><span class="by">elieb44</span><span>|</span><a href="#42169009">prev</a><span>|</span><a href="#42170648">next</a><span>|</span><label class="collapse" for="c-42170747">[-]</label><label class="expand" for="c-42170747">[1 more]</label></div><br/><div class="children"><div class="content">How about context encoding more generally ? Are there techniques to do that. I.E, during training, I want the string &quot;Dubito ergo cogito, cogito ergo sum, sum ergo Deus est.&quot; to have embedded René Descartes as main author, year 1637 as date of writing and &quot;Discours de la méthode&quot; as global context of writing.<p>So that when trained again another part of the same book, the model can learn they were from same context.</div><br/></div></div><div id="42170648" class="c"><input type="checkbox" id="c-42170648" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#42170747">prev</a><span>|</span><a href="#42169762">next</a><span>|</span><label class="collapse" for="c-42170648">[-]</label><label class="expand" for="c-42170648">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think the first code example should work (it indeed says false here).<p>When given a permuted sequence, the attention output will also be permuted, not identical. The need for positional encodings is due to two tokens resulting in the same value in the final attention matrix regardless of the tokens&#x27; absolute and relative position; that is enough to miss a lot of meaning.</div><br/><div id="42170863" class="c"><input type="checkbox" id="c-42170863" checked=""/><div class="controls bullet"><span class="by">FL33TW00D</span><span>|</span><a href="#42170648">parent</a><span>|</span><a href="#42169762">next</a><span>|</span><label class="collapse" for="c-42170863">[-]</label><label class="expand" for="c-42170863">[1 more]</label></div><br/><div class="children"><div class="content">The first code example says False because of high precision, I&#x27;ve updated the example.</div><br/></div></div></div></div><div id="42169762" class="c"><input type="checkbox" id="c-42169762" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#42170648">prev</a><span>|</span><a href="#42169348">next</a><span>|</span><label class="collapse" for="c-42169762">[-]</label><label class="expand" for="c-42169762">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m effectively a complete layman in this (although I do see some parallels to physical positional encoders, which is interesting) so at first read this entire thing went WAAAAY over my head.  At first glance it seemed to be way overcomplicated just to encode position, so I figured I was missing something.  ChatGPT was super helpful in explaining spiking neural networks to me so I just spent 20 minutes asking ChatGPT to explain this to me and I feel like I actually learned something.<p>Then at the end I asked ChatGPT how this all relates to how it operates and it was interesting to see things like:<p>&gt;Tokens as Subword Units: I use a tokenization method called Byte Pair Encoding (BPE), which breaks text into subword units.<p>I don&#x27;t know if it&#x27;s accurate or not, but it&#x27;s wild seeing it talk about how it works.</div><br/><div id="42170211" class="c"><input type="checkbox" id="c-42170211" checked=""/><div class="controls bullet"><span class="by">gloflo</span><span>|</span><a href="#42169762">parent</a><span>|</span><a href="#42169867">next</a><span>|</span><label class="collapse" for="c-42170211">[-]</label><label class="expand" for="c-42170211">[2 more]</label></div><br/><div class="children"><div class="content">The context includes that &quot;it&quot; is ChatGPT. The fact that ChatGPT uses Byte Pair Encoding is widely published. It is expectable that a LLM can regurgitate this kind of information, nothing wild about that.</div><br/><div id="42170485" class="c"><input type="checkbox" id="c-42170485" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#42169762">root</a><span>|</span><a href="#42170211">parent</a><span>|</span><a href="#42169867">next</a><span>|</span><label class="collapse" for="c-42170485">[-]</label><label class="expand" for="c-42170485">[1 more]</label></div><br/><div class="children"><div class="content">Note if you don&#x27;t have a good system prompt, other LLMs will also tell you they&#x27;re ChatGPT or Claude.</div><br/></div></div></div></div><div id="42169867" class="c"><input type="checkbox" id="c-42169867" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42169762">parent</a><span>|</span><a href="#42170211">prev</a><span>|</span><a href="#42169348">next</a><span>|</span><label class="collapse" for="c-42169867">[-]</label><label class="expand" for="c-42169867">[1 more]</label></div><br/><div class="children"><div class="content">100% accurate</div><br/></div></div></div></div><div id="42169348" class="c"><input type="checkbox" id="c-42169348" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#42169762">prev</a><span>|</span><a href="#42169717">next</a><span>|</span><label class="collapse" for="c-42169348">[-]</label><label class="expand" for="c-42169348">[3 more]</label></div><br/><div class="children"><div class="content">Maybe someone could answer this for me:  it seems like encoding the positional embeddings as augmentations to the &quot;natural&quot; activations instead of as their own inputs (concatenated onto the activations) make things like sliding a window much harder... I guess obviously the drawback is you have a somewhat less textually derived information.<p>I recall a early transformers video where they tried both and it turned out that adding the position onto the existing vectors was no worse so they went with it...  No further discussion about motivations happened in that video.<p>Is it worth revisiting that maybe now that activations have a gobsmackingly large dimension?</div><br/><div id="42170034" class="c"><input type="checkbox" id="c-42170034" checked=""/><div class="controls bullet"><span class="by">stephantul</span><span>|</span><a href="#42169348">parent</a><span>|</span><a href="#42169717">next</a><span>|</span><label class="collapse" for="c-42170034">[-]</label><label class="expand" for="c-42170034">[2 more]</label></div><br/><div class="children"><div class="content">They are not concatenated, but summed. I think concatenation wouldn’t work, as you indicate.<p>I think you mean the line in the original paper where they say compared the learned attention weights with the predefined encoding, and it made no difference.</div><br/><div id="42170614" class="c"><input type="checkbox" id="c-42170614" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#42169348">root</a><span>|</span><a href="#42170034">parent</a><span>|</span><a href="#42169717">next</a><span>|</span><label class="collapse" for="c-42170614">[-]</label><label class="expand" for="c-42170614">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think concatenation wouldn’t work, as you indicate.<p>Why do you say that?</div><br/></div></div></div></div></div></div><div id="42169717" class="c"><input type="checkbox" id="c-42169717" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><a href="#42169348">prev</a><span>|</span><a href="#42170229">next</a><span>|</span><label class="collapse" for="c-42169717">[-]</label><label class="expand" for="c-42169717">[1 more]</label></div><br/><div class="children"><div class="content">The binary coding example would have been much better with Gray codes.</div><br/></div></div><div id="42170229" class="c"><input type="checkbox" id="c-42170229" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42169717">prev</a><span>|</span><label class="collapse" for="c-42170229">[-]</label><label class="expand" for="c-42170229">[1 more]</label></div><br/><div class="children"><div class="content">Similarly, &quot;you&quot; could have designed state of the art LLM sampling: <a href="https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=FBkpCyujtS&amp;referrer=%5BTasks%5D(%2Ftasks)" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=FBkpCyujtS&amp;referrer=%5BTasks...</a></div><br/></div></div></div></div></div></div></div></body></html>