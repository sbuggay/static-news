<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1738054866593" as="style"/><link rel="stylesheet" href="styles.css?v=1738054866593"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://finance.yahoo.com/news/asml-sinks-china-ai-startup-081823609.html">Nvidia’s $589B DeepSeek rout</a> <span class="domain">(<a href="https://finance.yahoo.com">finance.yahoo.com</a>)</span></div><div class="subtext"><span>rcarmo</span> | <span>607 comments</span></div><br/><div><div id="42848969" class="c"><input type="checkbox" id="c-42848969" checked=""/><div class="controls bullet"><span class="by">plaidfuji</span><span>|</span><a href="#42848456">next</a><span>|</span><label class="collapse" for="c-42848969">[-]</label><label class="expand" for="c-42848969">[21 more]</label></div><br/><div class="children"><div class="content">Here’s a take I haven’t seen yet:<p>If training and inference just got 40x more efficient, but OpenAI and co. still have the same compute resources, once they’ve baked in all the DeepSeek improvements, we’re about to find out very quickly whether 40x the compute delivers 40x the performance &#x2F; output quality, or if output quality has ceased to be compute-bound.</div><br/><div id="42849546" class="c"><input type="checkbox" id="c-42849546" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#42848969">parent</a><span>|</span><a href="#42849463">next</a><span>|</span><label class="collapse" for="c-42849546">[-]</label><label class="expand" for="c-42849546">[8 more]</label></div><br/><div class="children"><div class="content">&gt; If training and inference just got 40x more efficient<p>Did training <i>and</i> inference just get 40x more efficient, or just training? They trained a model with impressive outputs on a limited number of GPUs, but DeepSeek is still a big model that requires a lot of resources to run. Moreover, which costs more, training a model once or using it for inference across a hundred million people multiple times a day for a year? It was always the second one, and doing the training cheaper makes it even more so.<p>But this implies that we could use those same resources to train even bigger models, right? Except that you then have the same problem. You have a bigger model, maybe it&#x27;s <i>better</i>, but if you&#x27;ve made inference cost linearly more because of the size and the size is now 40x bigger, you now need that much more compute for inference.</div><br/><div id="42850159" class="c"><input type="checkbox" id="c-42850159" checked=""/><div class="controls bullet"><span class="by">mrtesthah</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849546">parent</a><span>|</span><a href="#42849704">next</a><span>|</span><label class="collapse" for="c-42850159">[-]</label><label class="expand" for="c-42850159">[1 more]</label></div><br/><div class="children"><div class="content">Actually inference got more efficient as well, thanks to the multi-head latent attention algorithm that compresses the key-value cache to drastically reduce memory usage.<p><a href="https:&#x2F;&#x2F;mlnotes.substack.com&#x2F;p&#x2F;the-valleys-going-crazy-how-deepseek" rel="nofollow">https:&#x2F;&#x2F;mlnotes.substack.com&#x2F;p&#x2F;the-valleys-going-crazy-how-d...</a></div><br/></div></div><div id="42849704" class="c"><input type="checkbox" id="c-42849704" checked=""/><div class="controls bullet"><span class="by">cyberax</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849546">parent</a><span>|</span><a href="#42850159">prev</a><span>|</span><a href="#42849463">next</a><span>|</span><label class="collapse" for="c-42849704">[-]</label><label class="expand" for="c-42849704">[6 more]</label></div><br/><div class="children"><div class="content">&gt;  DeepSeek is still a big model that requires a lot of resources to run<p>I can run the largest model at 4 tokens per second on a 64GB card. Smaller models are _faster_ than Phi-4.<p>I&#x27;ve just switched to it for my local inference.</div><br/><div id="42849894" class="c"><input type="checkbox" id="c-42849894" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849704">parent</a><span>|</span><a href="#42849928">next</a><span>|</span><label class="collapse" for="c-42849894">[-]</label><label class="expand" for="c-42849894">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the largest model still like 130GB after heavy quantization[1] and 4 tok&#x2F;s borderline unusable for interactive sessions with those long outputs?<p>[1] <a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;blog&#x2F;deepseekr1-dynamic">https:&#x2F;&#x2F;unsloth.ai&#x2F;blog&#x2F;deepseekr1-dynamic</a></div><br/><div id="42849932" class="c"><input type="checkbox" id="c-42849932" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849894">parent</a><span>|</span><a href="#42849928">next</a><span>|</span><label class="collapse" for="c-42849932">[-]</label><label class="expand" for="c-42849932">[2 more]</label></div><br/><div class="children"><div class="content">OP probably means &quot;the largest distilled model&quot;</div><br/><div id="42850146" class="c"><input type="checkbox" id="c-42850146" checked=""/><div class="controls bullet"><span class="by">menaerus</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849932">parent</a><span>|</span><a href="#42849928">next</a><span>|</span><label class="collapse" for="c-42850146">[-]</label><label class="expand" for="c-42850146">[1 more]</label></div><br/><div class="children"><div class="content">So not an actual DeepSeek-R1 model but a distilled Qwen or Llama model.<p>From DeepSeek-R1 paper:<p>&gt; As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeekR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform nonreasoning models like GPT-4o-0513 across the board.<p>and<p>&gt; DeepSeek-R1-14B surpasses QwQ-32BPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks.<p>and<p>&gt; These [Distilled Model Evaluation] results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.</div><br/></div></div></div></div></div></div><div id="42849928" class="c"><input type="checkbox" id="c-42849928" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849704">parent</a><span>|</span><a href="#42849894">prev</a><span>|</span><a href="#42849929">next</a><span>|</span><label class="collapse" for="c-42849928">[-]</label><label class="expand" for="c-42849928">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a mixture of experts model. Those require a higher ratio of VRAM to compute, so they yield more tokens&#x2F;sec for a given model size and quantization (i.e. a given amount of VRAM), but running the big one unquantized requires 1.3TB of VRAM. And is it any faster than prior art mixture of experts models of the same size?</div><br/></div></div><div id="42849929" class="c"><input type="checkbox" id="c-42849929" checked=""/><div class="controls bullet"><span class="by">manojlds</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849704">parent</a><span>|</span><a href="#42849928">prev</a><span>|</span><a href="#42849463">next</a><span>|</span><label class="collapse" for="c-42849929">[-]</label><label class="expand" for="c-42849929">[1 more]</label></div><br/><div class="children"><div class="content">How are you running it, can you be more specific?</div><br/></div></div></div></div></div></div><div id="42849463" class="c"><input type="checkbox" id="c-42849463" checked=""/><div class="controls bullet"><span class="by">gloflo</span><span>|</span><a href="#42848969">parent</a><span>|</span><a href="#42849546">prev</a><span>|</span><a href="#42849420">next</a><span>|</span><label class="collapse" for="c-42849463">[-]</label><label class="expand" for="c-42849463">[3 more]</label></div><br/><div class="children"><div class="content">Does line go up forever?</div><br/><div id="42850185" class="c"><input type="checkbox" id="c-42850185" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849463">parent</a><span>|</span><a href="#42849420">next</a><span>|</span><label class="collapse" for="c-42850185">[-]</label><label class="expand" for="c-42850185">[2 more]</label></div><br/><div class="children"><div class="content">It goes up at least until LLMs match humans - ie until an LLM can write Windows</div><br/><div id="42850260" class="c"><input type="checkbox" id="c-42850260" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42850185">parent</a><span>|</span><a href="#42849420">next</a><span>|</span><label class="collapse" for="c-42850260">[-]</label><label class="expand" for="c-42850260">[1 more]</label></div><br/><div class="children"><div class="content">Slightly tangential, but I want an LLM which can <i>debug</i> windows.</div><br/></div></div></div></div></div></div><div id="42849420" class="c"><input type="checkbox" id="c-42849420" checked=""/><div class="controls bullet"><span class="by">zeven7</span><span>|</span><a href="#42848969">parent</a><span>|</span><a href="#42849463">prev</a><span>|</span><a href="#42849464">next</a><span>|</span><label class="collapse" for="c-42849420">[-]</label><label class="expand" for="c-42849420">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve missed the stories on this until now. Is it known (and is there an ELI5) how they were able to do it so much more efficiently?</div><br/><div id="42850019" class="c"><input type="checkbox" id="c-42850019" checked=""/><div class="controls bullet"><span class="by">rx_tx</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849420">parent</a><span>|</span><a href="#42849434">next</a><span>|</span><label class="collapse" for="c-42850019">[-]</label><label class="expand" for="c-42850019">[1 more]</label></div><br/><div class="children"><div class="content">This article has good background, context, and explanations [1] They skipped CUDA and instead used PTX which is a lower level instruction set where they were able to implement more performant cross-chip comms to make up for the less-performant H800 chips.<p>[1]: <a href="https:&#x2F;&#x2F;stratechery.com&#x2F;2025&#x2F;deepseek-faq&#x2F;" rel="nofollow">https:&#x2F;&#x2F;stratechery.com&#x2F;2025&#x2F;deepseek-faq&#x2F;</a></div><br/></div></div><div id="42849434" class="c"><input type="checkbox" id="c-42849434" checked=""/><div class="controls bullet"><span class="by">riffraff</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849420">parent</a><span>|</span><a href="#42850019">prev</a><span>|</span><a href="#42849464">next</a><span>|</span><label class="collapse" for="c-42849434">[-]</label><label class="expand" for="c-42849434">[3 more]</label></div><br/><div class="children"><div class="content">IIUC they released a paper, it&#x27;s partially algorithmic improvements partially good old low level optimization.</div><br/><div id="42849944" class="c"><input type="checkbox" id="c-42849944" checked=""/><div class="controls bullet"><span class="by">CursedSilicon</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849434">parent</a><span>|</span><a href="#42849464">next</a><span>|</span><label class="collapse" for="c-42849944">[-]</label><label class="expand" for="c-42849944">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s something to be said about the idea that instead of just dumping <i>oceans</i> of money into buying Nvidia cards they just...optimized what they <i>had</i><p>I&#x27;d say the wider industry could learn a thing or two, but as other commentors have joked. The line must go up</div><br/><div id="42850040" class="c"><input type="checkbox" id="c-42850040" checked=""/><div class="controls bullet"><span class="by">plussed_reader</span><span>|</span><a href="#42848969">root</a><span>|</span><a href="#42849944">parent</a><span>|</span><a href="#42849464">next</a><span>|</span><label class="collapse" for="c-42850040">[-]</label><label class="expand" for="c-42850040">[1 more]</label></div><br/><div class="children"><div class="content">The double edged sword of export controls.</div><br/></div></div></div></div></div></div></div></div><div id="42849464" class="c"><input type="checkbox" id="c-42849464" checked=""/><div class="controls bullet"><span class="by">frontalier</span><span>|</span><a href="#42848969">parent</a><span>|</span><a href="#42849420">prev</a><span>|</span><a href="#42848987">next</a><span>|</span><label class="collapse" for="c-42849464">[-]</label><label class="expand" for="c-42849464">[1 more]</label></div><br/><div class="children"><div class="content">there&#x27;s this and that little desktop computer they announced earlier this month - digits<p>they claim it&#x27;s able to run models with 200B parameters on a single node and 400B when paired with another node</div><br/></div></div><div id="42849001" class="c"><input type="checkbox" id="c-42849001" checked=""/><div class="controls bullet"><span class="by">BenFranklin100</span><span>|</span><a href="#42848969">parent</a><span>|</span><a href="#42848987">prev</a><span>|</span><a href="#42849209">next</a><span>|</span><label class="collapse" for="c-42849001">[-]</label><label class="expand" for="c-42849001">[1 more]</label></div><br/><div class="children"><div class="content">That seems to be the key question.</div><br/></div></div><div id="42849209" class="c"><input type="checkbox" id="c-42849209" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#42848969">parent</a><span>|</span><a href="#42849001">prev</a><span>|</span><a href="#42848456">next</a><span>|</span><label class="collapse" for="c-42849209">[-]</label><label class="expand" for="c-42849209">[1 more]</label></div><br/><div class="children"><div class="content">Yeah this was my first thought as well. If it got so efficient how good all the models will be 2-3 months from now</div><br/></div></div></div></div><div id="42848456" class="c"><input type="checkbox" id="c-42848456" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#42848969">prev</a><span>|</span><a href="#42846687">next</a><span>|</span><label class="collapse" for="c-42848456">[-]</label><label class="expand" for="c-42848456">[80 more]</label></div><br/><div class="children"><div class="content">90% of the comments in this thread make it clear that knowing about technology does not in any way qualify someone to think correctly about markets and equity valuations.</div><br/><div id="42850071" class="c"><input type="checkbox" id="c-42850071" checked=""/><div class="controls bullet"><span class="by">sek</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42848647">next</a><span>|</span><label class="collapse" for="c-42850071">[-]</label><label class="expand" for="c-42850071">[2 more]</label></div><br/><div class="children"><div class="content">The crash is absolutely rational; the cascading effect highlights the missing moat for companies like OpenAI. Without a moat, no investor will provide these companies with the billions that fueled most of the demand. This demand was essential for NVIDIA to squeeze such companies with incredible profit margins.<p>NVIDIA was overvalued before, and this correction is entirely justified. The larger impact of DeepSeek is more challenging to grasp. While companies like Google and Meta could benefit in the long term from this development, they still overpaid for an excessive number of GPUs. The rise in their stock prices was assumed to be driven by the moat they were expected to develop themselves.<p>I was always skeptical of those valuations. LLM inference was highly likely to become commoditized in the future anyway.</div><br/><div id="42850209" class="c"><input type="checkbox" id="c-42850209" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42850071">parent</a><span>|</span><a href="#42848647">next</a><span>|</span><label class="collapse" for="c-42850209">[-]</label><label class="expand" for="c-42850209">[1 more]</label></div><br/><div class="children"><div class="content">It has been clear for a while that one of two things is true.<p>1) AI stuff isn&#x27;t really worth trillions, in which case Nvidia is overvalued.<p>2) AI stuff is really worth trillions, in which case there will be no moat, because you can cross any moat for that amount of money, e.g. you could recreate CUDA from scratch for far less than a trillion dollars and in fact Nvidia didn&#x27;t spend anywhere near that much to create it to begin with. Someone else, or many someones, will spend the money to cross the moat and get their share.<p>So Nvidia is overvalued on the fundamentals. But is it overvalued on the hype cycle? Lots of people riding the bubble because number goes up until it doesn&#x27;t, and you can lose money (opportunity cost) by selling too early just like you can lose money by selling too late.<p>Then events like this make some people skittish that they&#x27;re going to sell too late, and number doesn&#x27;t go up that day.</div><br/></div></div></div></div><div id="42848647" class="c"><input type="checkbox" id="c-42848647" checked=""/><div class="controls bullet"><span class="by">cageface</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42850071">prev</a><span>|</span><a href="#42848879">next</a><span>|</span><label class="collapse" for="c-42848647">[-]</label><label class="expand" for="c-42848647">[12 more]</label></div><br/><div class="children"><div class="content">You could even say that overconfidence in abilities and knowledge outside of core expertise is the critical flaw of today&#x27;s entire tech industry.</div><br/><div id="42849129" class="c"><input type="checkbox" id="c-42849129" checked=""/><div class="controls bullet"><span class="by">ponty_rick</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848647">parent</a><span>|</span><a href="#42848735">next</a><span>|</span><label class="collapse" for="c-42849129">[-]</label><label class="expand" for="c-42849129">[3 more]</label></div><br/><div class="children"><div class="content">With the crowdstrike outage earlier last year it was incredible how many hidden security and kernel &quot;experts&quot; came out crawling from the woodwork, questioning why anything needs to run in the kernel and predicting the company&#x27;s demise.</div><br/><div id="42850108" class="c"><input type="checkbox" id="c-42850108" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849129">parent</a><span>|</span><a href="#42849976">next</a><span>|</span><label class="collapse" for="c-42850108">[-]</label><label class="expand" for="c-42850108">[1 more]</label></div><br/><div class="children"><div class="content">They were correct that there is no need for it to run in the kernel. They were incorrect in thinking this would affect the company&#x27;s future, because of course the sales of their product have nothing to do with its technical merit.</div><br/></div></div><div id="42849976" class="c"><input type="checkbox" id="c-42849976" checked=""/><div class="controls bullet"><span class="by">milkshakes</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849129">parent</a><span>|</span><a href="#42850108">prev</a><span>|</span><a href="#42848735">next</a><span>|</span><label class="collapse" for="c-42849976">[-]</label><label class="expand" for="c-42849976">[1 more]</label></div><br/><div class="children"><div class="content">the experts were correct. in 2024 there are now OS APIs that provide the same observability and control with much less risk involved.</div><br/></div></div></div></div><div id="42848735" class="c"><input type="checkbox" id="c-42848735" checked=""/><div class="controls bullet"><span class="by">sympil</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848647">parent</a><span>|</span><a href="#42849129">prev</a><span>|</span><a href="#42848879">next</a><span>|</span><label class="collapse" for="c-42848735">[-]</label><label class="expand" for="c-42848735">[8 more]</label></div><br/><div class="children"><div class="content">As a great example of this read Paul Graham’s essays that aren’t anout his core expertise.</div><br/><div id="42849796" class="c"><input type="checkbox" id="c-42849796" checked=""/><div class="controls bullet"><span class="by">rhubarbtree</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848735">parent</a><span>|</span><a href="#42849167">next</a><span>|</span><label class="collapse" for="c-42849796">[-]</label><label class="expand" for="c-42849796">[1 more]</label></div><br/><div class="children"><div class="content">I think that’s unfair unless you give specific examples and clear evidence he’s wrong.<p>I disagree with PG on economics and politics, but much of his writing on that is subjective.</div><br/></div></div><div id="42849167" class="c"><input type="checkbox" id="c-42849167" checked=""/><div class="controls bullet"><span class="by">haliskerbas</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848735">parent</a><span>|</span><a href="#42849796">prev</a><span>|</span><a href="#42848879">next</a><span>|</span><label class="collapse" for="c-42849167">[-]</label><label class="expand" for="c-42849167">[6 more]</label></div><br/><div class="children"><div class="content">But every successful SV founder and or VC is not only a tech genius but also a geopolitical and socioeconomic expert! That’s why they make war companies, cozy up to politicians, and talk about how woke is ruining the world. &#x2F;s</div><br/><div id="42849455" class="c"><input type="checkbox" id="c-42849455" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849167">parent</a><span>|</span><a href="#42848879">next</a><span>|</span><label class="collapse" for="c-42849455">[-]</label><label class="expand" for="c-42849455">[5 more]</label></div><br/><div class="children"><div class="content">In fairness, &#x27;geopolitical experts&#x27; may not really exist. There are a range of people who make up interesting stories to a greater or lesser extent but all seem to be serially misinformed. Some things are too complicated to have expertise in.<p>Indeed, while the existence of socioeconomic experts seems more likely we don&#x27;t have any way of reliably identifying them. The people who actually end up making social or economic policy seem to be winging it by picking the policy that most benefits wealthy people and&#x2F;or established asset owners. It is barely possible to blink twice without stumbling over a policy disaster.</div><br/><div id="42850045" class="c"><input type="checkbox" id="c-42850045" checked=""/><div class="controls bullet"><span class="by">rl3</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849455">parent</a><span>|</span><a href="#42849575">next</a><span>|</span><label class="collapse" for="c-42850045">[-]</label><label class="expand" for="c-42850045">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>In fairness, &#x27;geopolitical experts&#x27; may not really exist.</i><p>Except for, I don&#x27;t know, the many thousands of people who work at various government agencies (diplomatic, intelligence) or even private sector policy circles whose job it is to literally be geopolitical experts in a given area.</div><br/></div></div><div id="42849575" class="c"><input type="checkbox" id="c-42849575" checked=""/><div class="controls bullet"><span class="by">benzible</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849455">parent</a><span>|</span><a href="#42850045">prev</a><span>|</span><a href="#42850180">next</a><span>|</span><label class="collapse" for="c-42849575">[-]</label><label class="expand" for="c-42849575">[2 more]</label></div><br/><div class="children"><div class="content">So, you think the system is genuinely trying to identify expertise to achieve equitable outcomes, and just happening to fail at it? Rather than policy being shaped by personal networks and existing power structures that tend to benefit themselves?</div><br/><div id="42849728" class="c"><input type="checkbox" id="c-42849728" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849575">parent</a><span>|</span><a href="#42850180">next</a><span>|</span><label class="collapse" for="c-42849728">[-]</label><label class="expand" for="c-42849728">[1 more]</label></div><br/><div class="children"><div class="content">I think the system has been carefully configured to benefit wealthy people and&#x2F;or established asset owners. But the reason that there is no effective resistance to that is because identifying generalist socioeconomic experts is practically impossible.</div><br/></div></div></div></div><div id="42850180" class="c"><input type="checkbox" id="c-42850180" checked=""/><div class="controls bullet"><span class="by">thiago_fm</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849455">parent</a><span>|</span><a href="#42849575">prev</a><span>|</span><a href="#42848879">next</a><span>|</span><label class="collapse" for="c-42850180">[-]</label><label class="expand" for="c-42850180">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s so wrong in so many levels, also cynical. If the world worked by what you described, we would have been already obliterated ourselves a long time ago, or mass-enslavery would have happened. It didn&#x27;t.<p>Geopolitics can be studied and learned, and is something that diplomats heavily rely upon.<p>Of course, those geopolitical strategies can play in certain ways we don&#x27;t foresee, as on the other side we also have an actor that is free to do what they want.<p>But for instance, if you give Mexico a very good trade agreement as a strong country like the US, it&#x27;s very likely that they will work with you on your special requests.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42848879" class="c"><input type="checkbox" id="c-42848879" checked=""/><div class="controls bullet"><span class="by">wbsun</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42848647">prev</a><span>|</span><a href="#42848752">next</a><span>|</span><label class="collapse" for="c-42848879">[-]</label><label class="expand" for="c-42848879">[5 more]</label></div><br/><div class="children"><div class="content">I don’t understand why this is not obvious to many people: tech and stock trading are totally two different things, why on earth a tech expert is expected to know trading at all? Imagining how ridiculous it would be if a computer science graduate will also automatically get a financial degree from college even though no financial class has been taken.</div><br/><div id="42850116" class="c"><input type="checkbox" id="c-42850116" checked=""/><div class="controls bullet"><span class="by">nly</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848879">parent</a><span>|</span><a href="#42849802">next</a><span>|</span><label class="collapse" for="c-42850116">[-]</label><label class="expand" for="c-42850116">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because software devs are smart and make a lot of money - a natural next step is to try and use their smarts to do something with that money. Hence stocks.</div><br/><div id="42850194" class="c"><input type="checkbox" id="c-42850194" checked=""/><div class="controls bullet"><span class="by">Cumpiler69</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42850116">parent</a><span>|</span><a href="#42849802">next</a><span>|</span><label class="collapse" for="c-42850194">[-]</label><label class="expand" for="c-42850194">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt;It&#x27;s because software devs are smart and make a lot of money</i><p>They just think they&#x27;re smart <i>BECAUSE</i> they make a lot of money. Just because you can center divs for six figures a year at a F500 doesn&#x27;t make you smart at everything.</div><br/><div id="42850253" class="c"><input type="checkbox" id="c-42850253" checked=""/><div class="controls bullet"><span class="by">nly</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42850194">parent</a><span>|</span><a href="#42849802">next</a><span>|</span><label class="collapse" for="c-42850253">[-]</label><label class="expand" for="c-42850253">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve never met a fellow software engineer who &quot;centers divs&quot; for 6 figures.<p>But then I work with engineers using FPGAs to trade in the markets with tick to trade times in double digit nanoseconds and processing streams of market data at ~10 million messages per second (80Gbps)<p>The truth is, a lot of P&amp;L in trading these days is a technical feat of mathematics and engineering and not one of fundamental analysis and punting on business plans.</div><br/></div></div></div></div></div></div><div id="42849802" class="c"><input type="checkbox" id="c-42849802" checked=""/><div class="controls bullet"><span class="by">vishnugupta</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848879">parent</a><span>|</span><a href="#42850116">prev</a><span>|</span><a href="#42848752">next</a><span>|</span><label class="collapse" for="c-42849802">[-]</label><label class="expand" for="c-42849802">[1 more]</label></div><br/><div class="children"><div class="content">I’ve noticed this phenomenon among IT &amp; tech VC crowd. They will launch pod cast, offer expert opinion and what not on just about every topic under the Sun, from cold fusion to COVID vaccine to Ukraine war.<p>You wouldn’t see this in other folks, for example, a successful medical surgeon won’t offer much assertion about NVIDIA.<p>And the general tendency among audience is to assume that expertise can be carried across domains.</div><br/></div></div></div></div><div id="42848752" class="c"><input type="checkbox" id="c-42848752" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42848879">prev</a><span>|</span><a href="#42849163">next</a><span>|</span><label class="collapse" for="c-42848752">[-]</label><label class="expand" for="c-42848752">[42 more]</label></div><br/><div class="children"><div class="content">I think you’re wrong and Wallstreet got Deepseek’s impact wrong.<p>You say DeepSeek should decrease Nvidia demand. Wallstreet agreed today.<p>I say DeepSeek should increase Nvidia’s demand due to Jevon’s Paradox.</div><br/><div id="42850208" class="c"><input type="checkbox" id="c-42850208" checked=""/><div class="controls bullet"><span class="by">sidkshatriya</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42848824">next</a><span>|</span><label class="collapse" for="c-42850208">[-]</label><label class="expand" for="c-42850208">[1 more]</label></div><br/><div class="children"><div class="content">No, nvidia&#x27;s demand and importance might reduce in the long term.<p>We are forgetting that China has a whole hardware ecosystem. Now we learn that building SOTA models does not need SOTA hardware in massive quanties from nvidia. So the crash in the market implicitly could mean that the (hardware) monopoly of American companies is not going to be more than a few years. The hardware moat is not as deep as the West thought.<p>Once China brings scale like it did to batteries, EVs, solar, infrastructure, drones (etc) they will be able to run and train their models on their own hardware. Probably some time away but less time than what Wall Street thought.<p>This is actually more about nvidia than about OpenAI. OpenAI owns the end interface and it will be generally safe (maybe at a smaller valuation). In the long term nvidia is more replaceable than you think it is. Inference is going to dominate the market -- its going to be cerebras, groq, amd, intel, nvidia, google TPUs, chinese TPUs etc.<p>On the training side, there will be less demand for nvidia GPUs as meta, google, microsoft etc. extract efficiencies with the GPUs they already have given the embarrasing success of DeepSeek. Now, China might have been another insatiable market for nvidia but the export controls have ensured that it wont be.</div><br/></div></div><div id="42848824" class="c"><input type="checkbox" id="c-42848824" checked=""/><div class="controls bullet"><span class="by">Aloisius</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42850208">prev</a><span>|</span><a href="#42849568">next</a><span>|</span><label class="collapse" for="c-42848824">[-]</label><label class="expand" for="c-42848824">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Jevon’s Paradox<p>I&#x27;ve now seen this referenced two dozen times today which is well up from the 0 times I&#x27;ve seen it over the past year.<p>Is there some recent article referencing it that everyone is regurgitating?</div><br/><div id="42848886" class="c"><input type="checkbox" id="c-42848886" checked=""/><div class="controls bullet"><span class="by">jml7c5</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848824">parent</a><span>|</span><a href="#42849076">next</a><span>|</span><label class="collapse" for="c-42848886">[-]</label><label class="expand" for="c-42848886">[2 more]</label></div><br/><div class="children"><div class="content">Satya Nadella posted about it on Twitter.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;satyanadella&#x2F;status&#x2F;1883753899255046301" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;satyanadella&#x2F;status&#x2F;1883753899255046301</a></div><br/><div id="42849553" class="c"><input type="checkbox" id="c-42849553" checked=""/><div class="controls bullet"><span class="by">ojbyrne</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848886">parent</a><span>|</span><a href="#42849076">next</a><span>|</span><label class="collapse" for="c-42849553">[-]</label><label class="expand" for="c-42849553">[1 more]</label></div><br/><div class="children"><div class="content">Which seems pretty self-serving, and at the very least, premature.</div><br/></div></div></div></div><div id="42849076" class="c"><input type="checkbox" id="c-42849076" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848824">parent</a><span>|</span><a href="#42848886">prev</a><span>|</span><a href="#42848916">next</a><span>|</span><label class="collapse" for="c-42849076">[-]</label><label class="expand" for="c-42849076">[2 more]</label></div><br/><div class="children"><div class="content">Baader-Meinhof phenomenon, but also because everyone is writing about GPU demand and Jevon&#x27;s paradox is an easy way to express the idea in a trite keyword.</div><br/></div></div><div id="42848899" class="c"><input type="checkbox" id="c-42848899" checked=""/><div class="controls bullet"><span class="by">deadbabe</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848824">parent</a><span>|</span><a href="#42848916">prev</a><span>|</span><a href="#42849252">next</a><span>|</span><label class="collapse" for="c-42848899">[-]</label><label class="expand" for="c-42848899">[4 more]</label></div><br/><div class="children"><div class="content">Most people know Jevon’s Paradox there is just rarely an opportunity to bring it up, similar to Poe’s Law.</div><br/><div id="42849560" class="c"><input type="checkbox" id="c-42849560" checked=""/><div class="controls bullet"><span class="by">SOLAR_FIELDS</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848899">parent</a><span>|</span><a href="#42849451">next</a><span>|</span><label class="collapse" for="c-42849560">[-]</label><label class="expand" for="c-42849560">[2 more]</label></div><br/><div class="children"><div class="content">I never knew there was an actual term for this, but I knew of the concept in my professional work because this situation often plays out when the government widens roads here in the States. Ostensibly the road widening is intended to lower congestion, but instead it often just causes more people to live there and use it, thereby increasing congestion.<p>Probably a decent amount of professions have some variation of this, so it probably is accurate to say most people know OF Jevon’s Paradox because it’s pretty easy to dig up examples of it. But probably much fewer know it’s actual name, or even that it has a name</div><br/><div id="42849669" class="c"><input type="checkbox" id="c-42849669" checked=""/><div class="controls bullet"><span class="by">mrtksn</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849560">parent</a><span>|</span><a href="#42849451">next</a><span>|</span><label class="collapse" for="c-42849669">[-]</label><label class="expand" for="c-42849669">[1 more]</label></div><br/><div class="children"><div class="content">IMHO it happens as long as you can find use cases that were previously unfeasible due cost or availability constraints.<p>At some point the thing no longer brings any benefits because other costs or limitations overtake. for example, even faster broadband is no longer that big of a deal because your experience on most websites is now limited by their servers ability to process your request. However maybe in the future the costs and speeds will be so amazing that all the user devices will become thin clients and no one will care about their devices processing power, therefore one more increase in demand can happen.</div><br/></div></div></div></div><div id="42849451" class="c"><input type="checkbox" id="c-42849451" checked=""/><div class="controls bullet"><span class="by">retrac98</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848899">parent</a><span>|</span><a href="#42849560">prev</a><span>|</span><a href="#42849252">next</a><span>|</span><label class="collapse" for="c-42849451">[-]</label><label class="expand" for="c-42849451">[1 more]</label></div><br/><div class="children"><div class="content">I’d bet most people don’t know what a paradox is.</div><br/></div></div></div></div></div></div><div id="42849568" class="c"><input type="checkbox" id="c-42849568" checked=""/><div class="controls bullet"><span class="by">dumpsterdiver</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42848824">prev</a><span>|</span><a href="#42848804">next</a><span>|</span><label class="collapse" for="c-42849568">[-]</label><label class="expand" for="c-42849568">[3 more]</label></div><br/><div class="children"><div class="content">I’m also baffled by the reaction. Even with the ability to do more with less, the nature of the race still encourages everyone to do more with more.</div><br/><div id="42850186" class="c"><input type="checkbox" id="c-42850186" checked=""/><div class="controls bullet"><span class="by">bgnn</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849568">parent</a><span>|</span><a href="#42849973">next</a><span>|</span><label class="collapse" for="c-42850186">[-]</label><label class="expand" for="c-42850186">[1 more]</label></div><br/><div class="children"><div class="content">Why should they invest in Nvidia now instead of investing companies which can capitalize on the applications of AI.<p>Also, why not invest in AMD or Intel bur Nvidia till now: Because Nvidia had the moat and there was a race to buy as much GPU as possible at the moment. Now momentarily Nvidia sales would go down.<p>For long term investers who are investing in a future, not now, Nvidia was way overpriced. They will start buying when the price is right, but at the moment it&#x27;s still way too high. Nvidia is worth 20-30 billion or so in reality.</div><br/></div></div><div id="42849973" class="c"><input type="checkbox" id="c-42849973" checked=""/><div class="controls bullet"><span class="by">bilekas</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849568">parent</a><span>|</span><a href="#42850186">prev</a><span>|</span><a href="#42848804">next</a><span>|</span><label class="collapse" for="c-42849973">[-]</label><label class="expand" for="c-42849973">[1 more]</label></div><br/><div class="children"><div class="content">I was under the impression too that this would bump the retail customers demand for the 50 series given the extra AI and cuda cores, add to that the relatively low cost of the hardware. But I know nothing of the sentiments around wallstreet.<p>I don&#x27;t feel like upgrading my 4090 that said. Maybe wallstreet believes that the larger company deals that have driven the price up for so long might slow down?<p>Or I&#x27;m completely wrong on the impact of the hardware upgrades.</div><br/></div></div></div></div><div id="42848804" class="c"><input type="checkbox" id="c-42848804" checked=""/><div class="controls bullet"><span class="by">fldskfjdslkfj</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42849568">prev</a><span>|</span><a href="#42849551">next</a><span>|</span><label class="collapse" for="c-42848804">[-]</label><label class="expand" for="c-42848804">[12 more]</label></div><br/><div class="children"><div class="content">The increase in efficiency is usually accompanied with the process of commoditization as stuff get cheaper to develop, which is very bad news for nvidia.<p>If you dont need the super high end chips than Nvidia loses it&#x27;s biggest moat and ability to monopolize the tech, CUDA isn&#x27;t enough.</div><br/><div id="42848977" class="c"><input type="checkbox" id="c-42848977" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848804">parent</a><span>|</span><a href="#42849102">next</a><span>|</span><label class="collapse" for="c-42848977">[-]</label><label class="expand" for="c-42848977">[5 more]</label></div><br/><div class="children"><div class="content">Anthropic doesn’t train on Nvidia as far as I know. Google doesn’t either.<p>I don’t see how DeepSeek changes things.<p>By the way, DeepSeek trains on Nvidia.</div><br/><div id="42849232" class="c"><input type="checkbox" id="c-42849232" checked=""/><div class="controls bullet"><span class="by">rsanek</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848977">parent</a><span>|</span><a href="#42849175">next</a><span>|</span><label class="collapse" for="c-42849232">[-]</label><label class="expand" for="c-42849232">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Anthropic currently uses systems containing chips from Nvidia as well as those with AWS’ Trainium and Inferentia chips to train its models.&quot; <a href="https:&#x2F;&#x2F;www.cio.com&#x2F;article&#x2F;3602879&#x2F;anthropic-caught-up-in-a-potential-turf-war-what-could-it-mean-for-competition-complexity-and-lock-in.html" rel="nofollow">https:&#x2F;&#x2F;www.cio.com&#x2F;article&#x2F;3602879&#x2F;anthropic-caught-up-in-a...</a></div><br/></div></div><div id="42849175" class="c"><input type="checkbox" id="c-42849175" checked=""/><div class="controls bullet"><span class="by">haliskerbas</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848977">parent</a><span>|</span><a href="#42849232">prev</a><span>|</span><a href="#42849102">next</a><span>|</span><label class="collapse" for="c-42849175">[-]</label><label class="expand" for="c-42849175">[3 more]</label></div><br/><div class="children"><div class="content">What do the others train on?</div><br/><div id="42849235" class="c"><input type="checkbox" id="c-42849235" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849175">parent</a><span>|</span><a href="#42849229">next</a><span>|</span><label class="collapse" for="c-42849235">[-]</label><label class="expand" for="c-42849235">[1 more]</label></div><br/><div class="children"><div class="content">Anthropic on Amazon trainium chips</div><br/></div></div><div id="42849229" class="c"><input type="checkbox" id="c-42849229" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849175">parent</a><span>|</span><a href="#42849235">prev</a><span>|</span><a href="#42849102">next</a><span>|</span><label class="collapse" for="c-42849229">[-]</label><label class="expand" for="c-42849229">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia.</div><br/></div></div></div></div></div></div><div id="42849102" class="c"><input type="checkbox" id="c-42849102" checked=""/><div class="controls bullet"><span class="by">giantrobot</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848804">parent</a><span>|</span><a href="#42848977">prev</a><span>|</span><a href="#42849551">next</a><span>|</span><label class="collapse" for="c-42849102">[-]</label><label class="expand" for="c-42849102">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Nvidia loses it&#x27;s biggest moat and ability to monopolize the tech, CUDA isn&#x27;t enough<p>CUDA is plenty for right now. AMD can&#x27;t&#x2F;won&#x27;t get their act together with GPU software and drivers. Intel isn&#x27;t in much better of a position than AMD and has a host of other problems. It&#x27;s also unlikely the &quot;let&#x27;s just glue a thousand ARM cores together&quot; hardware will work as planned and still needs the software layer.<p>CUDA won&#x27;t be an Nvidia moat <i>forever</i> but it&#x27;s a decent moat for the next five years. If a company wants to build GPU compute resources it will be hard to go wrong buying Nvidia kit. At least from a platform point of view.</div><br/><div id="42850214" class="c"><input type="checkbox" id="c-42850214" checked=""/><div class="controls bullet"><span class="by">flakeoil</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849102">parent</a><span>|</span><a href="#42849213">next</a><span>|</span><label class="collapse" for="c-42850214">[-]</label><label class="expand" for="c-42850214">[1 more]</label></div><br/><div class="children"><div class="content">Someone commented somewhere above that deepseek avoided using CUDA. So it means you can achieve very good results without Nvidia&#x27;s CUDA.<p>&quot;They skipped CUDA and instead used PTX which is a lower level instruction set&quot;</div><br/></div></div><div id="42849213" class="c"><input type="checkbox" id="c-42849213" checked=""/><div class="controls bullet"><span class="by">fldskfjdslkfj</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849102">parent</a><span>|</span><a href="#42850214">prev</a><span>|</span><a href="#42850078">next</a><span>|</span><label class="collapse" for="c-42849213">[-]</label><label class="expand" for="c-42849213">[2 more]</label></div><br/><div class="children"><div class="content">CUDA will still be a moat for the near future and nobody is saying that Nvidia will die, but the thing is that Nvidia margins will drop like crazy and so will it&#x27;s valuation. It will go back down to being a &quot;medium tech&quot; company.<p>Basically training got way cheaper, and for inference you don&#x27;t really need nvidia, so even if there&#x27;s an increase for cheaper chips there&#x27;s no way the volume makes up for the loss of margin.</div><br/><div id="42849618" class="c"><input type="checkbox" id="c-42849618" checked=""/><div class="controls bullet"><span class="by">Jlagreen</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849213">parent</a><span>|</span><a href="#42850078">next</a><span>|</span><label class="collapse" for="c-42849618">[-]</label><label class="expand" for="c-42849618">[1 more]</label></div><br/><div class="children"><div class="content">No, Nvidia&#x27;s margins won&#x27;t drop at all and the proof for this is Apple.<p>The units of AI accelerators will explode, the market will explode.<p>At the end of the day, Nvidia will have 20-30% of the unit share in AI HW and 70-80% of the profit share in the AI HW market. Just like Apple makes 3x the money compared to the rest of the smartphone market.<p>Jensen has considered Nvidia a premium vendor for 2 decades and track record of Nvidia&#x27;s margins show this.<p>And while Nvidia remains a high premium AI infrastructure vendor, they will also add lots of great SW frameworks to make even more profit.<p>Omniverse has literally no competition. That digital world simulation combines all of Nvidia&#x27;s expertise (AI HW, Graphics HW, Physics HW, Networking, SW) into one huge product. And it will be a revolution because it&#x27;s the first time we will be able to finally digitalize the analog world. And Nvidia will earn tons of money because Omniverse itself is licensed, it needs OVX systems (visual part) and it needs DGX systems (AI part).<p>Don&#x27;t worry, Nvidia&#x27;s margins will be totally fine. I would even expect them to be higher in 10 years than they are today. Nobody believes that but that&#x27;s Jensen&#x27;s goal.<p>There is a reason why Nvidia has always been the company with the highest P&#x2F;S ratio and anyone who understands why, will see the quality management immediately.</div><br/></div></div></div></div><div id="42850078" class="c"><input type="checkbox" id="c-42850078" checked=""/><div class="controls bullet"><span class="by">nly</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849102">parent</a><span>|</span><a href="#42849213">prev</a><span>|</span><a href="#42849771">next</a><span>|</span><label class="collapse" for="c-42850078">[-]</label><label class="expand" for="c-42850078">[1 more]</label></div><br/><div class="children"><div class="content">Nobody ever got fired for buying Nvidia</div><br/></div></div><div id="42849771" class="c"><input type="checkbox" id="c-42849771" checked=""/><div class="controls bullet"><span class="by">cyberax</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849102">parent</a><span>|</span><a href="#42850078">prev</a><span>|</span><a href="#42849551">next</a><span>|</span><label class="collapse" for="c-42849771">[-]</label><label class="expand" for="c-42849771">[1 more]</label></div><br/><div class="children"><div class="content">Intel seems to be getting its act together. Battlemage is a decent mid-range GPU, and Gaudi 3 seems to be a fairly decent AI attempt.</div><br/></div></div></div></div></div></div><div id="42849551" class="c"><input type="checkbox" id="c-42849551" checked=""/><div class="controls bullet"><span class="by">ojbyrne</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42848804">prev</a><span>|</span><a href="#42849834">next</a><span>|</span><label class="collapse" for="c-42849551">[-]</label><label class="expand" for="c-42849551">[2 more]</label></div><br/><div class="children"><div class="content">A minor quibble: It’s Jevons Paradox - the last name of the economist who formulated it is “Jevons”</div><br/><div id="42850125" class="c"><input type="checkbox" id="c-42850125" checked=""/><div class="controls bullet"><span class="by">hbs18</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849551">parent</a><span>|</span><a href="#42849834">next</a><span>|</span><label class="collapse" for="c-42850125">[-]</label><label class="expand" for="c-42850125">[1 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t it be Jevons&#x27; Paradox?</div><br/></div></div></div></div><div id="42849834" class="c"><input type="checkbox" id="c-42849834" checked=""/><div class="controls bullet"><span class="by">vishnugupta</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42849551">prev</a><span>|</span><a href="#42849157">next</a><span>|</span><label class="collapse" for="c-42849834">[-]</label><label class="expand" for="c-42849834">[3 more]</label></div><br/><div class="children"><div class="content">&gt; DeepSeek should increase Nvidia’s demand due to Jevon’s Paradox.<p>How exactly? From what I’ve read the full model can run on MacBook M1 sort of hardware just fine. And this is their first release, I’d expect it to get more efficient and maybe domain specific models can be run on much lower grade hardware sort of raspberry pi sort.</div><br/><div id="42849876" class="c"><input type="checkbox" id="c-42849876" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849834">parent</a><span>|</span><a href="#42850035">next</a><span>|</span><label class="collapse" for="c-42849876">[-]</label><label class="expand" for="c-42849876">[1 more]</label></div><br/><div class="children"><div class="content">&gt;How exactly? From what I’ve read the full model can run on MacBook M1 sort of hardware just fine.<p>No you can&#x27;t. Unless you run their 1.5b model quantized. Almost useless.<p>Their full model, Deepseek V3 has 671B parameters. Not even remotely close to being able to run well on consumer hardware.</div><br/></div></div><div id="42850035" class="c"><input type="checkbox" id="c-42850035" checked=""/><div class="controls bullet"><span class="by">michaelscott</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849834">parent</a><span>|</span><a href="#42849876">prev</a><span>|</span><a href="#42849157">next</a><span>|</span><label class="collapse" for="c-42850035">[-]</label><label class="expand" for="c-42850035">[1 more]</label></div><br/><div class="children"><div class="content">Only the distilled models can run on everyday machines, not the full model</div><br/></div></div></div></div><div id="42849157" class="c"><input type="checkbox" id="c-42849157" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42849834">prev</a><span>|</span><a href="#42849958">next</a><span>|</span><label class="collapse" for="c-42849157">[-]</label><label class="expand" for="c-42849157">[2 more]</label></div><br/><div class="children"><div class="content">Jevon&#x27;s paradox ony applies under certain conditions. It remains to be seen if it will hold in this case.</div><br/><div id="42849871" class="c"><input type="checkbox" id="c-42849871" checked=""/><div class="controls bullet"><span class="by">dredmorbius</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849157">parent</a><span>|</span><a href="#42849958">next</a><span>|</span><label class="collapse" for="c-42849871">[-]</label><label class="expand" for="c-42849871">[1 more]</label></div><br/><div class="children"><div class="content">What do you understand those conditions to be?</div><br/></div></div></div></div><div id="42849958" class="c"><input type="checkbox" id="c-42849958" checked=""/><div class="controls bullet"><span class="by">manojlds</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42849157">prev</a><span>|</span><a href="#42848954">next</a><span>|</span><label class="collapse" for="c-42849958">[-]</label><label class="expand" for="c-42849958">[1 more]</label></div><br/><div class="children"><div class="content">With all the rate limits on Sonnet 3.5, people only want more of these models. We haven&#x27;t even seen wide adoption yet.</div><br/></div></div><div id="42848954" class="c"><input type="checkbox" id="c-42848954" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42849958">prev</a><span>|</span><a href="#42849043">next</a><span>|</span><label class="collapse" for="c-42848954">[-]</label><label class="expand" for="c-42848954">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I say DeepSeek should increase Nvidia’s demand due to Jevon’s Paradox.<p>If their claims were true, DeepSeek would increase the demand for GPU. It&#x27;s so obvious that I don&#x27;t know why we even need a name to describe this scenario (I guess Jeven&#x27;s Paradox just sounds cool).<p>The only issue is that whether it would make a competitor to Nvidia viable. My bet is no, but the market seems to have betted yes.</div><br/></div></div><div id="42849043" class="c"><input type="checkbox" id="c-42849043" checked=""/><div class="controls bullet"><span class="by">spamizbad</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42848954">prev</a><span>|</span><a href="#42848808">next</a><span>|</span><label class="collapse" for="c-42849043">[-]</label><label class="expand" for="c-42849043">[2 more]</label></div><br/><div class="children"><div class="content">Long term: Likely true.
Short-to-medium term: Nope.</div><br/><div id="42849077" class="c"><input type="checkbox" id="c-42849077" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849043">parent</a><span>|</span><a href="#42848808">next</a><span>|</span><label class="collapse" for="c-42849077">[-]</label><label class="expand" for="c-42849077">[1 more]</label></div><br/><div class="children"><div class="content">Why not short term?</div><br/></div></div></div></div><div id="42848808" class="c"><input type="checkbox" id="c-42848808" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848752">parent</a><span>|</span><a href="#42849043">prev</a><span>|</span><a href="#42849163">next</a><span>|</span><label class="collapse" for="c-42848808">[-]</label><label class="expand" for="c-42848808">[2 more]</label></div><br/><div class="children"><div class="content">Will you post on HN in year describing how much money you earned by buying the stock today?</div><br/><div id="42849938" class="c"><input type="checkbox" id="c-42849938" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848808">parent</a><span>|</span><a href="#42849163">next</a><span>|</span><label class="collapse" for="c-42849938">[-]</label><label class="expand" for="c-42849938">[1 more]</label></div><br/><div class="children"><div class="content">I did buy:-) although the amount bought isn&#x27;t impressive because I have other temporary cash flow constraints for a few months.</div><br/></div></div></div></div></div></div><div id="42849163" class="c"><input type="checkbox" id="c-42849163" checked=""/><div class="controls bullet"><span class="by">snake_doc</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42848752">prev</a><span>|</span><a href="#42849819">next</a><span>|</span><label class="collapse" for="c-42849163">[-]</label><label class="expand" for="c-42849163">[4 more]</label></div><br/><div class="children"><div class="content">The other way is certainly also true. Your short piece is rational, but lacks insight into the inference and training dynamics of ML adoption unconstrained.<p>The rate of ML progress is spectacularly compute constrained today. Every step in today’s scaling program is setup to de-risked the next scale up, because the opportunity cost of compute is so high. If the opportunity cost of compute is not so high, you can skip the 1B to 8B scale ups and grid search data mixes and hyperparameters.<p>The market&#x2F;concentration risk premium drove most of the volatility today. If it was truly value driven, then this should have happened 6 months ago when DeepSeek released V2 that had the vast majority of cost optimizations.<p>Cloud data center CapEx is backstopped by their growth outlook driven by the technology, not by GPU manufacturers. Dollars will shift just as quickly (like how Meta literally teared down a half built data center in 2023 to restart it to meet new designs).</div><br/><div id="42849433" class="c"><input type="checkbox" id="c-42849433" checked=""/><div class="controls bullet"><span class="by">optimiz3</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849163">parent</a><span>|</span><a href="#42849819">next</a><span>|</span><label class="collapse" for="c-42849433">[-]</label><label class="expand" for="c-42849433">[3 more]</label></div><br/><div class="children"><div class="content">Everyone can say things that sound smart.  When it comes to markets the only thing that matters is if your portfolio was green or red.</div><br/><div id="42849786" class="c"><input type="checkbox" id="c-42849786" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849433">parent</a><span>|</span><a href="#42849591">next</a><span>|</span><label class="collapse" for="c-42849786">[-]</label><label class="expand" for="c-42849786">[1 more]</label></div><br/><div class="children"><div class="content">Since when gambling on a RNG output makes you smart?</div><br/></div></div><div id="42849591" class="c"><input type="checkbox" id="c-42849591" checked=""/><div class="controls bullet"><span class="by">Jlagreen</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42849433">parent</a><span>|</span><a href="#42849786">prev</a><span>|</span><a href="#42849819">next</a><span>|</span><label class="collapse" for="c-42849591">[-]</label><label class="expand" for="c-42849591">[1 more]</label></div><br/><div class="children"><div class="content">My entry into Nvidia is 2016, my portfolio has never been red since then.</div><br/></div></div></div></div></div></div><div id="42849819" class="c"><input type="checkbox" id="c-42849819" checked=""/><div class="controls bullet"><span class="by">rhubarbtree</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42849163">prev</a><span>|</span><a href="#42848667">next</a><span>|</span><label class="collapse" for="c-42849819">[-]</label><label class="expand" for="c-42849819">[1 more]</label></div><br/><div class="children"><div class="content">I think that’s correct, but it would be a more useful comment if you gave a few examples and explained the correct thinking.</div><br/></div></div><div id="42848667" class="c"><input type="checkbox" id="c-42848667" checked=""/><div class="controls bullet"><span class="by">flashman</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42849819">prev</a><span>|</span><a href="#42849533">next</a><span>|</span><label class="collapse" for="c-42848667">[-]</label><label class="expand" for="c-42848667">[3 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s entirely possible that one categorically can&#x27;t think correctly about markets and equity valuations since they are vibes-based. Post hoc, sure, but not ahead of time.</div><br/><div id="42848772" class="c"><input type="checkbox" id="c-42848772" checked=""/><div class="controls bullet"><span class="by">Rury</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848667">parent</a><span>|</span><a href="#42849533">next</a><span>|</span><label class="collapse" for="c-42848772">[-]</label><label class="expand" for="c-42848772">[2 more]</label></div><br/><div class="children"><div class="content">Most people don&#x27;t care about the fundamentals of equity valuations is the crux of it. If they can make money via derivatives, who cares about the underlying valuations? I mean just look at GME for one example, it&#x27;s been mostly a squeeze driven play between speculators. And then you have the massive dispersion trade that&#x27;s been happening on the SP500 over the last year+. And when most people invest in index funds, and index funds are weighted mostly by market cap, value investing has been essentially dead for a while now.</div><br/><div id="42848924" class="c"><input type="checkbox" id="c-42848924" checked=""/><div class="controls bullet"><span class="by">flashman</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848772">parent</a><span>|</span><a href="#42849533">next</a><span>|</span><label class="collapse" for="c-42848924">[-]</label><label class="expand" for="c-42848924">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, GME for instance shows you can only think about it correctly in retrospect (or perhaps with perfect knowledge of all players&#x27; intentions).</div><br/></div></div></div></div></div></div><div id="42849533" class="c"><input type="checkbox" id="c-42849533" checked=""/><div class="controls bullet"><span class="by">maybesomaybenot</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42848667">prev</a><span>|</span><a href="#42849044">next</a><span>|</span><label class="collapse" for="c-42849533">[-]</label><label class="expand" for="c-42849533">[1 more]</label></div><br/><div class="children"><div class="content">&quot;knowing about technology&quot; ... filter out the javascript programmers and that leaves about 15% of HN.</div><br/></div></div><div id="42849044" class="c"><input type="checkbox" id="c-42849044" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42849533">prev</a><span>|</span><a href="#42849071">next</a><span>|</span><label class="collapse" for="c-42849044">[-]</label><label class="expand" for="c-42849044">[1 more]</label></div><br/><div class="children"><div class="content">Evergreen.  The same observation applies to any subject you know anything about that is discussed on HN.</div><br/></div></div><div id="42849071" class="c"><input type="checkbox" id="c-42849071" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42849044">prev</a><span>|</span><a href="#42850079">next</a><span>|</span><label class="collapse" for="c-42849071">[-]</label><label class="expand" for="c-42849071">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Briefly stated, the Gell-Mann Amnesia effect is as follows. You open the newspaper to an article on some subject you know well. In Murray&#x27;s case, physics. In mine, show business. You read the article and see the journalist has absolutely no understanding of either the facts or the issues. Often, the article is so wrong it actually presents the story backward—reversing cause and effect. I call these the &quot;wet streets cause rain&quot; stories. Paper&#x27;s full of them.<p>In any case, you read with exasperation or amusement the multiple errors in a story, and then turn the page to national or international affairs, and read as if the rest of the newspaper was somehow more accurate about Palestine than the baloney you just read. You turn the page, and forget what you know.&quot;<p>– Michael Crichton (1942-2008)</div><br/></div></div><div id="42850079" class="c"><input type="checkbox" id="c-42850079" checked=""/><div class="controls bullet"><span class="by">simonebrunozzi</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42849071">prev</a><span>|</span><a href="#42849382">next</a><span>|</span><label class="collapse" for="c-42850079">[-]</label><label class="expand" for="c-42850079">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. Given that you clearly belong to the top 10%, what&#x27;s your take on the market?</div><br/></div></div><div id="42849745" class="c"><input type="checkbox" id="c-42849745" checked=""/><div class="controls bullet"><span class="by">biohcacker84</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42849382">prev</a><span>|</span><a href="#42848756">next</a><span>|</span><label class="collapse" for="c-42849745">[-]</label><label class="expand" for="c-42849745">[1 more]</label></div><br/><div class="children"><div class="content">HN comments have always been bad. I&#x27;ve argued with people here that some day Americans would shop using their phones, just like Europeans had been doing at the time. Most people <i>here</i> thought that was crazy.<p>When years ago PG wrote that self-employed&#x2F;founders developers are more focused, hungrier than developers working for someone else. A an easily observed truism. All of HN got their panties in a bunch.</div><br/></div></div><div id="42848756" class="c"><input type="checkbox" id="c-42848756" checked=""/><div class="controls bullet"><span class="by">boringg</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42849745">prev</a><span>|</span><a href="#42848745">next</a><span>|</span><label class="collapse" for="c-42848756">[-]</label><label class="expand" for="c-42848756">[1 more]</label></div><br/><div class="children"><div class="content">100%<p>Equity valuations for AI hardware future earnings changed dramatically in the last day.  The belief that NVIDIA demand for their product is insatiable for the near future had been dented and the concern that energy is the biggest bottle neck might not be the case.<p>Lots to figure out on this information but the playbook radically changed.</div><br/></div></div><div id="42848745" class="c"><input type="checkbox" id="c-42848745" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#42848456">parent</a><span>|</span><a href="#42848756">prev</a><span>|</span><a href="#42846687">next</a><span>|</span><label class="collapse" for="c-42848745">[-]</label><label class="expand" for="c-42848745">[3 more]</label></div><br/><div class="children"><div class="content">That means the remaining 10% are similarly disillusioned by the impression Apple or AMD could &quot;just write&quot; a CUDA alternative and compete on their merits. You don&#x27;t want either of those people spending their money on datacenter bets.<p>10 years ago people said OpenCL would break CUDA&#x27;s moat, 5 years ago people said ASICs would beat CUDA, and now we&#x27;re arguing that older Nvidia GPUs will make CUDA obsolete. I have spent the past decade reading delusional eulogies for Nvidia, and I still find people adamant they&#x27;re doomed despite being unable to name a real CUDA alternative.</div><br/><div id="42848755" class="c"><input type="checkbox" id="c-42848755" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848745">parent</a><span>|</span><a href="#42846687">next</a><span>|</span><label class="collapse" for="c-42848755">[-]</label><label class="expand" for="c-42848755">[2 more]</label></div><br/><div class="children"><div class="content">Did ASICs beat CUDA out in crypto coin mining? Not the benchmark I really care about, but if things slow down in AI (they probably won’t) ASICs could probably take over some of it.</div><br/><div id="42850139" class="c"><input type="checkbox" id="c-42850139" checked=""/><div class="controls bullet"><span class="by">nly</span><span>|</span><a href="#42848456">root</a><span>|</span><a href="#42848755">parent</a><span>|</span><a href="#42846687">next</a><span>|</span><label class="collapse" for="c-42850139">[-]</label><label class="expand" for="c-42850139">[1 more]</label></div><br/><div class="children"><div class="content">Well in the case of Bitcoin it&#x27;s just one simple hash function. ASICs were inevitable.</div><br/></div></div></div></div></div></div></div></div><div id="42846687" class="c"><input type="checkbox" id="c-42846687" checked=""/><div class="controls bullet"><span class="by">ozten</span><span>|</span><a href="#42848456">prev</a><span>|</span><a href="#42846471">next</a><span>|</span><label class="collapse" for="c-42846687">[-]</label><label class="expand" for="c-42846687">[62 more]</label></div><br/><div class="children"><div class="content">NVIDIA sells shovels to the gold rush. One miner (Liang Wenfeng), who has previously purchased at least 10,000 A100 shovels... has a &quot;side project&quot; where they figured out how to dig really well with a shovel and shared their secrets.<p>The gold rush, wether real or a bubble is still there! NVIDA will still sell every shovel they can manufacture, as soon as it is available in inventory.<p>Fortune 100 companies will still want the biggest toolshed to invent the next paradigm or to be the first to get to AGI.</div><br/><div id="42846769" class="c"><input type="checkbox" id="c-42846769" checked=""/><div class="controls bullet"><span class="by">elihu</span><span>|</span><a href="#42846687">parent</a><span>|</span><a href="#42848407">next</a><span>|</span><label class="collapse" for="c-42846769">[-]</label><label class="expand" for="c-42846769">[35 more]</label></div><br/><div class="children"><div class="content">Jevon&#x27;s paradox would imply that there&#x27;s good reason to think that demand for shovels will increase.  AI doesn&#x27;t seem to be one of those things where society as a whole will say, &quot;we have enough of that; we don&#x27;t need any more&quot;.<p>(Many individual people are already saying that, but they aren&#x27;t the people buying the GPUs for this in the first place.  Steam engines weren&#x27;t universally popular either when they were introduced to society.)</div><br/><div id="42848948" class="c"><input type="checkbox" id="c-42848948" checked=""/><div class="controls bullet"><span class="by">flashman</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846769">parent</a><span>|</span><a href="#42848193">next</a><span>|</span><label class="collapse" for="c-42848948">[-]</label><label class="expand" for="c-42848948">[2 more]</label></div><br/><div class="children"><div class="content">Jevons was talking about coal as an input to commercial processes, for which there were other alternatives that competed on price (e.g. manual&#x2F;animal labour). Whatever the process, it generated a return, it had utility, and it had scale.<p>I argue it doesn&#x27;t apply to generative AI because its outputs are mostly no good, have no utility, or are good but only in limited commercial contexts.<p>In the first case, a machine that produces garbage faster and cheaper doesn&#x27;t mean demand for the garbage will increase. And in the second case, there aren&#x27;t enough buyers for high-quality computer-generated pictures of toilets to meaningfully boost demand for Nvidia&#x27;s products.</div><br/><div id="42849943" class="c"><input type="checkbox" id="c-42849943" checked=""/><div class="controls bullet"><span class="by">hnaccount_rng</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848948">parent</a><span>|</span><a href="#42848193">next</a><span>|</span><label class="collapse" for="c-42849943">[-]</label><label class="expand" for="c-42849943">[1 more]</label></div><br/><div class="children"><div class="content">I recently had a discussion with a higher ranked executive and his take on AI changed my outlook a bit. For him the value of ChatGPT:tm: wasn&#x27;t so much the speed up in any particular task (like presentation generation or so). It&#x27;s a replacement for consultants.<p>Yes, the value of those only exists mostly if your internal team is too stubborn to change its opinion. But that seems to be the norm. And the value (those) consultants add is not that high in the first place! They don&#x27;t have the internal knowledge of _why_ things are fucked up _your particular way_ anyways. That part your team has to contribute anyhow. So the value add shrinks to &quot;throw ideas over the wall and see what sticks&quot;. And LLMs are excellent at that.<p>Yes, that doesn&#x27;t replace a highly technical consultant that does the actual implementation. Yes, that doesn&#x27;t give you a good solution. But it probably gives you 5 starting points for a solution before you even finish googling which consultancy to pick (and then waiting for approval and hoping for a goodish team). And that&#x27;s a story that I can map to reality (not that I like this new bit of information about reality..)<p>If we accept that story about LLM value, then I think NVIDIA is fine. That generated value is far greater than any amount of energy you can burn on inferring prompts and the only effect will be that the compute-for-training to compute-for-inference ratio decreases further</div><br/></div></div></div></div><div id="42848193" class="c"><input type="checkbox" id="c-42848193" checked=""/><div class="controls bullet"><span class="by">puppymaster</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846769">parent</a><span>|</span><a href="#42848948">prev</a><span>|</span><a href="#42847289">next</a><span>|</span><label class="collapse" for="c-42848193">[-]</label><label class="expand" for="c-42848193">[11 more]</label></div><br/><div class="children"><div class="content">I also dont get how this is bearish for NVDA. Before this, small to mid companies would give up on finetuning their own model because openai is just so much better and cheaper. Now deepseek SOTA model gives them much better quality baseline model to train on. Wouldn&#x27;t more people want to RAG on top of deepseek? or some startups accountant would run the numbers and figures we can just inference the shit out of deepseek locally and in the long run we still come out ahead of using oenai api.<p>Either way that means a lot more NVDA hardware being sold. You still need CUDAs as rocm is still not there yet. In fact NVDA needs to churn out more CUDAs than ever.</div><br/><div id="42848978" class="c"><input type="checkbox" id="c-42848978" checked=""/><div class="controls bullet"><span class="by">hattmall</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848193">parent</a><span>|</span><a href="#42848875">next</a><span>|</span><label class="collapse" for="c-42848978">[-]</label><label class="expand" for="c-42848978">[1 more]</label></div><br/><div class="children"><div class="content">Because their biggest buyer(s) just ran into a buzzsaw. NVDA&#x27;s stratospheric valuation is based on a few select customer&#x27;s unrestrained ability to purchase the latest products. That unrestrained spending ability was fueled by the &quot;AI&quot; arms race. If those companies see their ability to be profitable with &quot;AI&quot; as diminished then their ability to continue spending with NVDA is probably going to be diminished as well. Anything considered bad for NVDA&#x27;s top spenders is going to be viewed as bad for NVDA as well.</div><br/></div></div><div id="42848875" class="c"><input type="checkbox" id="c-42848875" checked=""/><div class="controls bullet"><span class="by">wisty</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848193">parent</a><span>|</span><a href="#42848978">prev</a><span>|</span><a href="#42848986">next</a><span>|</span><label class="collapse" for="c-42848875">[-]</label><label class="expand" for="c-42848875">[1 more]</label></div><br/><div class="children"><div class="content">At a guess, Nvidia stock prices are basically fiction at this point (are there lots of short AND long selling - IIRC butterfly spreads?).<p>A good fundamental analysis is probably very hard to get right, and the game is probably just guessing which way everyone else will guess the herd is going to jump.</div><br/></div></div><div id="42848986" class="c"><input type="checkbox" id="c-42848986" checked=""/><div class="controls bullet"><span class="by">nycdatasci</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848193">parent</a><span>|</span><a href="#42848875">prev</a><span>|</span><a href="#42848431">next</a><span>|</span><label class="collapse" for="c-42848986">[-]</label><label class="expand" for="c-42848986">[1 more]</label></div><br/><div class="children"><div class="content">NVDA can&#x27;t really &quot;churn out more CUDAs&quot; because CUDA is a software platform&#x2F;framework, not a physical product.</div><br/></div></div><div id="42848431" class="c"><input type="checkbox" id="c-42848431" checked=""/><div class="controls bullet"><span class="by">Rury</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848193">parent</a><span>|</span><a href="#42848986">prev</a><span>|</span><a href="#42848940">next</a><span>|</span><label class="collapse" for="c-42848431">[-]</label><label class="expand" for="c-42848431">[2 more]</label></div><br/><div class="children"><div class="content">Maybe because the loudest news said it was bearish. Lot&#x27;s of people let the media do their thinking for them.</div><br/><div id="42848574" class="c"><input type="checkbox" id="c-42848574" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848431">parent</a><span>|</span><a href="#42848940">next</a><span>|</span><label class="collapse" for="c-42848574">[-]</label><label class="expand" for="c-42848574">[1 more]</label></div><br/><div class="children"><div class="content">GPUs are now 47 times faster, thanks to all the software improvements, so the prices are about to go up, right? Buckle up!</div><br/></div></div></div></div><div id="42848940" class="c"><input type="checkbox" id="c-42848940" checked=""/><div class="controls bullet"><span class="by">ip26</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848193">parent</a><span>|</span><a href="#42848431">prev</a><span>|</span><a href="#42848386">next</a><span>|</span><label class="collapse" for="c-42848940">[-]</label><label class="expand" for="c-42848940">[4 more]</label></div><br/><div class="children"><div class="content">Not sure if I believe it’s bearish, but the PC made computers cheaper and demand exploded, yet wasn’t very good for IBM.</div><br/><div id="42848965" class="c"><input type="checkbox" id="c-42848965" checked=""/><div class="controls bullet"><span class="by">woah</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848940">parent</a><span>|</span><a href="#42848386">next</a><span>|</span><label class="collapse" for="c-42848965">[-]</label><label class="expand" for="c-42848965">[3 more]</label></div><br/><div class="children"><div class="content">It could have been great for IBM if they had done things differently</div><br/><div id="42849056" class="c"><input type="checkbox" id="c-42849056" checked=""/><div class="controls bullet"><span class="by">dralley</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848965">parent</a><span>|</span><a href="#42848386">next</a><span>|</span><label class="collapse" for="c-42849056">[-]</label><label class="expand" for="c-42849056">[2 more]</label></div><br/><div class="children"><div class="content">IBM probably couldn&#x27;t have done things differently given the antitrust scrutiny they were under.  And that was likely the best outcome for the industry anyway.</div><br/><div id="42849327" class="c"><input type="checkbox" id="c-42849327" checked=""/><div class="controls bullet"><span class="by">protocolture</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42849056">parent</a><span>|</span><a href="#42848386">next</a><span>|</span><label class="collapse" for="c-42849327">[-]</label><label class="expand" for="c-42849327">[1 more]</label></div><br/><div class="children"><div class="content">Agree completely. I would hate to have seen what would have happened if IBM could legally prevent the clean room reimplementation of their bios.</div><br/></div></div></div></div></div></div></div></div><div id="42848386" class="c"><input type="checkbox" id="c-42848386" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848193">parent</a><span>|</span><a href="#42848940">prev</a><span>|</span><a href="#42847289">next</a><span>|</span><label class="collapse" for="c-42848386">[-]</label><label class="expand" for="c-42848386">[1 more]</label></div><br/><div class="children"><div class="content">The bear case is something like “investors are going to call BS on multi-billion training DC investments”. That represents most of their short-run demand.<p>Not sure what is supposed to happen to the inference demand but I guess that could be modeled as more of a long-run thing, as inference is going to be very coin-operated (companies need it to be net profitable now) whereas training is more of a build now profit later game.</div><br/></div></div></div></div><div id="42847289" class="c"><input type="checkbox" id="c-42847289" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846769">parent</a><span>|</span><a href="#42848193">prev</a><span>|</span><a href="#42849086">next</a><span>|</span><label class="collapse" for="c-42847289">[-]</label><label class="expand" for="c-42847289">[2 more]</label></div><br/><div class="children"><div class="content">The other thing is that if this pushes the envelope further on what AI models can do given a certain hardware budget, this might actually change minds. The pushback against generative AI today is that much of it is deployed in ways that are ultimately useless and annoying at best, and that in turn is because the capabilities of those models are vastly oversold (including internally in companies that ship products with them). But if a smarter model can actually e.g. reliably organize my email, that&#x27;s a very different story.</div><br/><div id="42848168" class="c"><input type="checkbox" id="c-42848168" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42847289">parent</a><span>|</span><a href="#42849086">next</a><span>|</span><label class="collapse" for="c-42848168">[-]</label><label class="expand" for="c-42848168">[1 more]</label></div><br/><div class="children"><div class="content">An rag model can already sort your email.<p>Its just that it costs too much to do that for the hoi polloi who think everything digital should be free forever.</div><br/></div></div></div></div><div id="42849086" class="c"><input type="checkbox" id="c-42849086" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846769">parent</a><span>|</span><a href="#42847289">prev</a><span>|</span><a href="#42848372">next</a><span>|</span><label class="collapse" for="c-42849086">[-]</label><label class="expand" for="c-42849086">[2 more]</label></div><br/><div class="children"><div class="content">When you get more marginal product from an input, it&#x27;s expected you buy more of that input.<p>But at some point, if the marginal product gets high enough, the world needs not as many, because money spent on other inputs&#x2F;factors pays off more.<p>This is a classic problem with extrapolation.  Making people more efficient through the use of AI will tend to increase employment... until it doesn&#x27;t and employment goes off a cliff.  Getting more work done per unit of GPU will increase demand for GPUs ... until it doesn&#x27;t, and GPU demand goes off the cliff.<p>It&#x27;s always hard to tell where that cliff is, though.</div><br/><div id="42849241" class="c"><input type="checkbox" id="c-42849241" checked=""/><div class="controls bullet"><span class="by">jjk166</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42849086">parent</a><span>|</span><a href="#42848372">next</a><span>|</span><label class="collapse" for="c-42849241">[-]</label><label class="expand" for="c-42849241">[1 more]</label></div><br/><div class="children"><div class="content">If there&#x27;s one thing I doubt the world will have a glut of anytime soon, it&#x27;s intelligence.</div><br/></div></div></div></div><div id="42848372" class="c"><input type="checkbox" id="c-42848372" checked=""/><div class="controls bullet"><span class="by">fabfoe</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846769">parent</a><span>|</span><a href="#42849086">prev</a><span>|</span><a href="#42848519">next</a><span>|</span><label class="collapse" for="c-42848372">[-]</label><label class="expand" for="c-42848372">[1 more]</label></div><br/><div class="children"><div class="content">Apparently it’s just Jevons, not possessive.</div><br/></div></div><div id="42848519" class="c"><input type="checkbox" id="c-42848519" checked=""/><div class="controls bullet"><span class="by">wisty</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846769">parent</a><span>|</span><a href="#42848372">prev</a><span>|</span><a href="#42847315">next</a><span>|</span><label class="collapse" for="c-42848519">[-]</label><label class="expand" for="c-42848519">[1 more]</label></div><br/><div class="children"><div class="content">Yes, a global market of 5 big LLM (ChatGTP, Llama, Claude, Mistral, Qwen ... any other big ones?) is not exactly good for Nvidia.<p>If every well funded start-up can have a shot, then they buy more GPUs and the big players will need to buy even more to stay noticeably ahead.</div><br/></div></div><div id="42847315" class="c"><input type="checkbox" id="c-42847315" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846769">parent</a><span>|</span><a href="#42848519">prev</a><span>|</span><a href="#42848407">next</a><span>|</span><label class="collapse" for="c-42847315">[-]</label><label class="expand" for="c-42847315">[15 more]</label></div><br/><div class="children"><div class="content">&gt; AI doesn&#x27;t seem to be one of those things where society as a whole will say, &quot;we have enough of that; we don&#x27;t need any more&quot;.<p>Really? Has anyone made a useful, commercially successful product with it yet?</div><br/><div id="42848279" class="c"><input type="checkbox" id="c-42848279" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42847315">parent</a><span>|</span><a href="#42848790">next</a><span>|</span><label class="collapse" for="c-42848279">[-]</label><label class="expand" for="c-42848279">[2 more]</label></div><br/><div class="children"><div class="content">Not even Microsoft Copilot 365 had a successful launch. The same happened with Apple Intelligence.<p>People talk like the end user demand part of the equation is really solved when invoking an Econ 101 magical interpretation of the Law of Supply and Demand or Jevons Padadox.</div><br/><div id="42848933" class="c"><input type="checkbox" id="c-42848933" checked=""/><div class="controls bullet"><span class="by">dmix</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848279">parent</a><span>|</span><a href="#42848790">next</a><span>|</span><label class="collapse" for="c-42848933">[-]</label><label class="expand" for="c-42848933">[1 more]</label></div><br/><div class="children"><div class="content">Apple Intelligence has barely released what they announced is coming<p>Deep integration into iOS won&#x27;t be tacked on in a rush to market addition to OS.</div><br/></div></div></div></div><div id="42848790" class="c"><input type="checkbox" id="c-42848790" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42847315">parent</a><span>|</span><a href="#42848279">prev</a><span>|</span><a href="#42848376">next</a><span>|</span><label class="collapse" for="c-42848790">[-]</label><label class="expand" for="c-42848790">[3 more]</label></div><br/><div class="children"><div class="content">ChatGPT has over $10 million paying subscriber.  No I am not counting the people using the API programmatically</div><br/><div id="42849223" class="c"><input type="checkbox" id="c-42849223" checked=""/><div class="controls bullet"><span class="by">MadnessASAP</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848790">parent</a><span>|</span><a href="#42849364">next</a><span>|</span><label class="collapse" for="c-42849223">[-]</label><label class="expand" for="c-42849223">[1 more]</label></div><br/><div class="children"><div class="content">And they&#x27;re still burning billions with no end in sight.</div><br/></div></div><div id="42849364" class="c"><input type="checkbox" id="c-42849364" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848790">parent</a><span>|</span><a href="#42849223">prev</a><span>|</span><a href="#42848376">next</a><span>|</span><label class="collapse" for="c-42849364">[-]</label><label class="expand" for="c-42849364">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t make them profitable though. They spend billions.</div><br/></div></div></div></div><div id="42848376" class="c"><input type="checkbox" id="c-42848376" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42847315">parent</a><span>|</span><a href="#42848790">prev</a><span>|</span><a href="#42849598">next</a><span>|</span><label class="collapse" for="c-42848376">[-]</label><label class="expand" for="c-42848376">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, lots of them. I never thought I&#x27;d be paying for a search subscription but after a few months of using ChatGPT I expect to be paying for the privilege from now on. Maybe not paying OpenAI, but someone. There isn&#x27;t much of a moat there, so there are going to be many companies basically on-selling GPU time. And even if for some weird reason there is no commercially successful AI-specific product it is causing shockwaves in how work is done, most people I know who are effective have worked it into their workflow somehow.</div><br/></div></div><div id="42849598" class="c"><input type="checkbox" id="c-42849598" checked=""/><div class="controls bullet"><span class="by">johnfn</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42847315">parent</a><span>|</span><a href="#42848376">prev</a><span>|</span><a href="#42848388">next</a><span>|</span><label class="collapse" for="c-42849598">[-]</label><label class="expand" for="c-42849598">[1 more]</label></div><br/><div class="children"><div class="content">Cursor?</div><br/></div></div><div id="42848388" class="c"><input type="checkbox" id="c-42848388" checked=""/><div class="controls bullet"><span class="by">trhway</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42847315">parent</a><span>|</span><a href="#42849598">prev</a><span>|</span><a href="#42848407">next</a><span>|</span><label class="collapse" for="c-42848388">[-]</label><label class="expand" for="c-42848388">[7 more]</label></div><br/><div class="children"><div class="content">&gt;Really? Has anyone made a useful, commercially successful product with it yet?<p>Aren&#x27;t millions or even tens of millions students using ChatGPT for example? To me that sounds like a commercial success (and looks comparable  with the usage of Google Search - a money printing machine for more almost 30 years now - in the first years)<p>And enterprise-wise - heard recently a VP complaining about entering expenses. As we don&#x27;t have secretaries anymore in the civilized world, that means &quot;Agents AI&quot; is going to have a blast.<p>(i&#x27;m long on NVDA and wondering is it enough blood on the streets to buy more :)</div><br/><div id="42848444" class="c"><input type="checkbox" id="c-42848444" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848388">parent</a><span>|</span><a href="#42848490">next</a><span>|</span><label class="collapse" for="c-42848444">[-]</label><label class="expand" for="c-42848444">[5 more]</label></div><br/><div class="children"><div class="content">&gt;To me that sounds like a commercial success<p>It isn&#x27;t because it&#x27;s not making them any money. Having users doesn&#x27;t mean you have a business. If you sell two dollars for one dollar having more users is not a blessing financially. Of course you could slap ads on it, like Google, but unlike Google openAI has no moat and there&#x27;s already ten competitors. Competition eliminates profit and AI is being commoditized faster than pretty much anything else.</div><br/><div id="42848800" class="c"><input type="checkbox" id="c-42848800" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848444">parent</a><span>|</span><a href="#42848491">next</a><span>|</span><label class="collapse" for="c-42848800">[-]</label><label class="expand" for="c-42848800">[3 more]</label></div><br/><div class="children"><div class="content">Do you think the cost won’t ever come down to make $20&#x2F;month sustainable?</div><br/><div id="42848907" class="c"><input type="checkbox" id="c-42848907" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848800">parent</a><span>|</span><a href="#42848491">next</a><span>|</span><label class="collapse" for="c-42848907">[-]</label><label class="expand" for="c-42848907">[2 more]</label></div><br/><div class="children"><div class="content">$20 a month does not warrant NVidias 3 trillion dollar valuation though.</div><br/><div id="42850058" class="c"><input type="checkbox" id="c-42850058" checked=""/><div class="controls bullet"><span class="by">michaelscott</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848907">parent</a><span>|</span><a href="#42848491">next</a><span>|</span><label class="collapse" for="c-42850058">[-]</label><label class="expand" for="c-42850058">[1 more]</label></div><br/><div class="children"><div class="content">NVidia&#x27;s not selling LLM subscriptions, they&#x27;re selling shovels in the goldrush. I don&#x27;t think 3 trillion is a reasonable valuation either, but NVidia&#x27;s applications extend way beyond consumer and they&#x27;ve effectively become the chokepoint for any application of AI</div><br/></div></div></div></div></div></div><div id="42848491" class="c"><input type="checkbox" id="c-42848491" checked=""/><div class="controls bullet"><span class="by">trhway</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848444">parent</a><span>|</span><a href="#42848800">prev</a><span>|</span><a href="#42848490">next</a><span>|</span><label class="collapse" for="c-42848491">[-]</label><label class="expand" for="c-42848491">[1 more]</label></div><br/><div class="children"><div class="content">&gt;unlike Google openAI has no moat<p>what was Google moat?<p>&gt;and there&#x27;s already ten potential competitors. Competition eliminates profit and AI is being commoditized faster than pretty much anything else.<p>we&#x27;re discussing NVDA. Where are its competitors? ChatGPT having 10 competitors only makes things better for NVDA.<p>&gt;Competition eliminates profit<p>Competition weeds out bad&#x2F;ineffective performers which is great. History of our industry is littered with competition taking out bad performers, and our industry is only better for that. Fast commoditization of AI is just great and fits the best patterns like say PC-revolution (and like it i think the AI-revolution wouldn&#x27;t be just one app&#x2F;user-case, it will be a tectonic shift instead).</div><br/></div></div></div></div><div id="42848490" class="c"><input type="checkbox" id="c-42848490" checked=""/><div class="controls bullet"><span class="by">starspangled</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848388">parent</a><span>|</span><a href="#42848444">prev</a><span>|</span><a href="#42848407">next</a><span>|</span><label class="collapse" for="c-42848490">[-]</label><label class="expand" for="c-42848490">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Aren&#x27;t millions or even tens of millions students using ChatGPT for example? To me that sounds like a commercial success<p>I read somewhere that OpenAI brought in $3.7 billion in 2024, and made a loss of $6 billion. So... no I don&#x27;t think that is an example. They want to make a commercially successful product, but ChatGPT doesn&#x27;t seem to be there yet.<p>&gt; And enterprise-wise - heard recently a VP complaining about entering expenses. As we don&#x27;t have secretaries anymore in the civilized world, that means &quot;Agents AI&quot; is going to have a blast.<p>We don&#x27;t have secretaries because the word became unfashionable. They are called PAs or executive assistants or something like that now. They&#x27;re still there, but if anything the need for them has probably been reduced with (non-AI) computers (calendars, contacts, emails, electronic documents, etc.) so I&#x27;m not sure that there is some enormous unmet demand for them.</div><br/></div></div></div></div></div></div></div></div><div id="42848407" class="c"><input type="checkbox" id="c-42848407" checked=""/><div class="controls bullet"><span class="by">addicted</span><span>|</span><a href="#42846687">parent</a><span>|</span><a href="#42846769">prev</a><span>|</span><a href="#42849438">next</a><span>|</span><label class="collapse" for="c-42848407">[-]</label><label class="expand" for="c-42848407">[10 more]</label></div><br/><div class="children"><div class="content">What you are missing is that it turns out the gold isn’t actually gold. It’s bronze.<p>So earliest, the shovelers were willing to spend thousands of dollars for a single shovel because they were expecting to get much more valuable gold out the other end.<p>But now that it’s only bronze, they can’t spend that much money on their tools anymore to make their venture profitable. A lot of shovelers are gonna drop out of the race. And the ones that remain will not be willing to spend as much.<p>The fact that there isn’t that much money to be made in AI anymore means that whatever percentage of money would have gone to NVIDIA from the total money to be made in AI will now shrink dramatically.</div><br/><div id="42848497" class="c"><input type="checkbox" id="c-42848497" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848407">parent</a><span>|</span><a href="#42848481">next</a><span>|</span><label class="collapse" for="c-42848497">[-]</label><label class="expand" for="c-42848497">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>it turns out the gold isn’t actually gold. It’s bronze.</i><p>The gold is still gold. We just thought it was 10,000 feet down, which requires lots of shovels, when it was actually just under the surface.</div><br/><div id="42848563" class="c"><input type="checkbox" id="c-42848563" checked=""/><div class="controls bullet"><span class="by">whattheheckheck</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848497">parent</a><span>|</span><a href="#42848846">next</a><span>|</span><label class="collapse" for="c-42848563">[-]</label><label class="expand" for="c-42848563">[3 more]</label></div><br/><div class="children"><div class="content">Still don&#x27;t know if it&#x27;s even gold</div><br/></div></div><div id="42848846" class="c"><input type="checkbox" id="c-42848846" checked=""/><div class="controls bullet"><span class="by">wisty</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848497">parent</a><span>|</span><a href="#42848563">prev</a><span>|</span><a href="#42848481">next</a><span>|</span><label class="collapse" for="c-42848846">[-]</label><label class="expand" for="c-42848846">[1 more]</label></div><br/><div class="children"><div class="content">To continue the analogy, now everyone with a dream and a credit card can buy a shovel and have a shot.</div><br/></div></div></div></div><div id="42848481" class="c"><input type="checkbox" id="c-42848481" checked=""/><div class="controls bullet"><span class="by">deeviant</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848407">parent</a><span>|</span><a href="#42848497">prev</a><span>|</span><a href="#42849438">next</a><span>|</span><label class="collapse" for="c-42848481">[-]</label><label class="expand" for="c-42848481">[4 more]</label></div><br/><div class="children"><div class="content">Wait, so AI might become 25x cheaper to train and run, and your thesis is... no one will make money on AI now?!?!</div><br/><div id="42848503" class="c"><input type="checkbox" id="c-42848503" checked=""/><div class="controls bullet"><span class="by">h0l0cube</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848481">parent</a><span>|</span><a href="#42849606">next</a><span>|</span><label class="collapse" for="c-42848503">[-]</label><label class="expand" for="c-42848503">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps they mean there&#x27;s less wealth to be extracted from the closed-source training side of the equation, which requires huge capital investment, and promises even bigger returns by gatekeeping the technology.</div><br/></div></div><div id="42849606" class="c"><input type="checkbox" id="c-42849606" checked=""/><div class="controls bullet"><span class="by">Ekaros</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848481">parent</a><span>|</span><a href="#42848503">prev</a><span>|</span><a href="#42849019">next</a><span>|</span><label class="collapse" for="c-42849606">[-]</label><label class="expand" for="c-42849606">[1 more]</label></div><br/><div class="children"><div class="content">Many discussed aspects are disconnected. Cost of training, cost of hardware(and margin there), cost of operation, possible use cases, and then finally demand.<p>Cheaper training still expect there is some use case for those trained models. There might or might not be. It can very well be that cost of training did not really limit the number of usable models.</div><br/></div></div><div id="42849019" class="c"><input type="checkbox" id="c-42849019" checked=""/><div class="controls bullet"><span class="by">hattmall</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848481">parent</a><span>|</span><a href="#42849606">prev</a><span>|</span><a href="#42849438">next</a><span>|</span><label class="collapse" for="c-42849019">[-]</label><label class="expand" for="c-42849019">[1 more]</label></div><br/><div class="children"><div class="content">Sort of, it means there&#x27;s less of a chance for massive market domination and a monopoly.</div><br/></div></div></div></div></div></div><div id="42849438" class="c"><input type="checkbox" id="c-42849438" checked=""/><div class="controls bullet"><span class="by">medion</span><span>|</span><a href="#42846687">parent</a><span>|</span><a href="#42848407">prev</a><span>|</span><a href="#42849068">next</a><span>|</span><label class="collapse" for="c-42849438">[-]</label><label class="expand" for="c-42849438">[2 more]</label></div><br/><div class="children"><div class="content">Can anyone comment on why Wenfeng shared his secret sauce? Other than publicity, there only seems to be downsides for him, as now everyone else with larger compute will just copy and improve?</div><br/><div id="42849998" class="c"><input type="checkbox" id="c-42849998" checked=""/><div class="controls bullet"><span class="by">jes5199</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42849438">parent</a><span>|</span><a href="#42849068">next</a><span>|</span><label class="collapse" for="c-42849998">[-]</label><label class="expand" for="c-42849998">[1 more]</label></div><br/><div class="children"><div class="content">pure prestige play, I think</div><br/></div></div></div></div><div id="42848929" class="c"><input type="checkbox" id="c-42848929" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#42846687">parent</a><span>|</span><a href="#42849068">prev</a><span>|</span><a href="#42849139">next</a><span>|</span><label class="collapse" for="c-42848929">[-]</label><label class="expand" for="c-42848929">[2 more]</label></div><br/><div class="children"><div class="content">AGI would be El Dorado in this analogy?</div><br/><div id="42849882" class="c"><input type="checkbox" id="c-42849882" checked=""/><div class="controls bullet"><span class="by">buryat</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848929">parent</a><span>|</span><a href="#42849139">next</a><span>|</span><label class="collapse" for="c-42849882">[-]</label><label class="expand" for="c-42849882">[1 more]</label></div><br/><div class="children"><div class="content">El Dorado will be AGI with Humanoid robots and actual real pick-axes</div><br/></div></div></div></div><div id="42849139" class="c"><input type="checkbox" id="c-42849139" checked=""/><div class="controls bullet"><span class="by">avs733</span><span>|</span><a href="#42846687">parent</a><span>|</span><a href="#42848929">prev</a><span>|</span><a href="#42846746">next</a><span>|</span><label class="collapse" for="c-42849139">[-]</label><label class="expand" for="c-42849139">[1 more]</label></div><br/><div class="children"><div class="content">The thing with a gold rush is you often end up selling shovels after the gold has run out, but no one knows that until hindsight. There will probably be a couple scares that the gold has run out first to. And again the difference is only visible in hindsight.</div><br/></div></div><div id="42846746" class="c"><input type="checkbox" id="c-42846746" checked=""/><div class="controls bullet"><span class="by">culi</span><span>|</span><a href="#42846687">parent</a><span>|</span><a href="#42849139">prev</a><span>|</span><a href="#42846471">next</a><span>|</span><label class="collapse" for="c-42846746">[-]</label><label class="expand" for="c-42846746">[10 more]</label></div><br/><div class="children"><div class="content">Yeah but NVIDIA&#x27;s amazing digging technique that could only be accomplished with NVIDIA shovels is now irrelevant. Meaning there are more people selling shovels for the gold rush</div><br/><div id="42846904" class="c"><input type="checkbox" id="c-42846904" checked=""/><div class="controls bullet"><span class="by">sho_hn</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846746">parent</a><span>|</span><a href="#42846822">next</a><span>|</span><label class="collapse" for="c-42846904">[-]</label><label class="expand" for="c-42846904">[3 more]</label></div><br/><div class="children"><div class="content">DeepSeek&#x27;s stuff is actually <i>more</i> dependent on nVidia shovels. They implemented a bunch of assembly-level optimizations below the CUDA stack that allowed them to efficiently use the H800s they have, which are memory-bandwidth-gimped vs. the H100s they can&#x27;t easily buy on the open market. That&#x27;s cool, but doesn&#x27;t run on any other GPUs.<p>Cue all of China rushing to Jensen to buy all the H800s they can before the embargo gets tightened, now that their peers have demonstrated that they&#x27;re useful for something.<p>At least briefly, Jensen&#x27;s customer audience increased.</div><br/><div id="42848406" class="c"><input type="checkbox" id="c-42848406" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846904">parent</a><span>|</span><a href="#42846822">next</a><span>|</span><label class="collapse" for="c-42848406">[-]</label><label class="expand" for="c-42848406">[2 more]</label></div><br/><div class="children"><div class="content">I was thinking about that, but don’t those same optimizations work on H100s? and the concepts work on every other chip from Nvidia and every other manufacturer’s chip<p>I still think this is bullish: more people will be buying chips once cheaper and more accessible, and the things the will be training with be 1,000% to 10,000% larger</div><br/><div id="42849318" class="c"><input type="checkbox" id="c-42849318" checked=""/><div class="controls bullet"><span class="by">jjk166</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42848406">parent</a><span>|</span><a href="#42846822">next</a><span>|</span><label class="collapse" for="c-42849318">[-]</label><label class="expand" for="c-42849318">[1 more]</label></div><br/><div class="children"><div class="content">Probably possible is nothing compared to already implemented. How long will it take to apply those concepts to other chips? Will they also be made available to the degree DeepSeek has been? By the time those alternatives are implemented how much further improvement will be made on Nvidia chips? Worst case scenario someone implements and open sources these optimizations for a competitor&#x27;s chip basically immediately in which case the competitive landscape remains unchanged, for all other scenarios this is a first mover advantage for Nvidia.</div><br/></div></div></div></div></div></div><div id="42846822" class="c"><input type="checkbox" id="c-42846822" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846746">parent</a><span>|</span><a href="#42846904">prev</a><span>|</span><a href="#42846814">next</a><span>|</span><label class="collapse" for="c-42846822">[-]</label><label class="expand" for="c-42846822">[5 more]</label></div><br/><div class="children"><div class="content">What about DeepSeek negates NVidia’s advantages over other GPU vendors?</div><br/><div id="42848949" class="c"><input type="checkbox" id="c-42848949" checked=""/><div class="controls bullet"><span class="by">koolhead17</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846822">parent</a><span>|</span><a href="#42849118">next</a><span>|</span><label class="collapse" for="c-42848949">[-]</label><label class="expand" for="c-42848949">[1 more]</label></div><br/><div class="children"><div class="content">What is stopping huawei or other Chinese vendors to make chips on deepseek specification and 1&#x2F;10th NVIDIA cost and mass market it?</div><br/></div></div><div id="42849118" class="c"><input type="checkbox" id="c-42849118" checked=""/><div class="controls bullet"><span class="by">culi</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846822">parent</a><span>|</span><a href="#42848949">prev</a><span>|</span><a href="#42846814">next</a><span>|</span><label class="collapse" for="c-42849118">[-]</label><label class="expand" for="c-42849118">[3 more]</label></div><br/><div class="children"><div class="content">You can train, or at least run, llms on intel and less powerful chips</div><br/><div id="42849675" class="c"><input type="checkbox" id="c-42849675" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42849118">parent</a><span>|</span><a href="#42846814">next</a><span>|</span><label class="collapse" for="c-42849675">[-]</label><label class="expand" for="c-42849675">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You can train, or at least run, llms on intel and less powerful chips<p>The claimed training breakthrough is an optimization targeting NVidia chip, not something that reduces NVidia&#x27;s relative advantage. Even if it is easily generalizable to other vendors hardware, it doesn&#x27;t reduce NVidia&#x27;s advantage over other vendors, it just proportionately scales down the training requirements for a model of a given capacity. Which, maybe, <i>very</i> short term reduces demands from the big existing incumbents, but it also increases the number of players for which investing in GPUs for model training at all is worthwhile, increasing aggregate demand.</div><br/><div id="42849726" class="c"><input type="checkbox" id="c-42849726" checked=""/><div class="controls bullet"><span class="by">culi</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42849675">parent</a><span>|</span><a href="#42846814">next</a><span>|</span><label class="collapse" for="c-42849726">[-]</label><label class="expand" for="c-42849726">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not an optimization targeting Nvidia chips. It&#x27;s an optimization of the technique through and through regardless of chip<p>But your point is well taken and perhaps both mine and GP&#x27;s metaphors break down.<p>Either way, we saw massive spikes in demand for Nvidia when crypto mining became huge followed by a massive drop when we hit the crypto winter. We saw another massive spike when LLMs blew up and this may just be the analogous drop in demand for LLMs</div><br/></div></div></div></div></div></div></div></div><div id="42846814" class="c"><input type="checkbox" id="c-42846814" checked=""/><div class="controls bullet"><span class="by">ozten</span><span>|</span><a href="#42846687">root</a><span>|</span><a href="#42846746">parent</a><span>|</span><a href="#42846822">prev</a><span>|</span><a href="#42846471">next</a><span>|</span><label class="collapse" for="c-42846814">[-]</label><label class="expand" for="c-42846814">[1 more]</label></div><br/><div class="children"><div class="content">CUDA begs to differ.</div><br/></div></div></div></div></div></div><div id="42846471" class="c"><input type="checkbox" id="c-42846471" checked=""/><div class="controls bullet"><span class="by">kd913</span><span>|</span><a href="#42846687">prev</a><span>|</span><a href="#42839939">next</a><span>|</span><label class="collapse" for="c-42846471">[-]</label><label class="expand" for="c-42846471">[11 more]</label></div><br/><div class="children"><div class="content">The biggest discussion I have been on having this is the implications on Deepseek for say the RoI H100. Will a sudden spike in available GPUs and reduction in demand (from efficient GPU usage) dramatically shock the cost per hour to rent a GPU. This I think is the critical value for measuring the investment value for Blackwell now.<p>The price for a H100 per hour has gone from the peak of $8.42 to about $1.80.<p>A H100 consumes 700W, lets say $0.10 per kwh?<p>A H100 costs around $30000.<p>Given deepseek, can the price of this drop further given a much larger supply of available GPUs can now be proven to be unlocked (Mi300x, H200s, H800s etc...).<p>Now that LLMs have effectively become commodity, with a significant price floor, is this new value ahead of what is profitable for the card.<p>Given the new Blackwell is $70000, is there sufficient applications that enable customers to get a RoI on the new card?<p>Am curious about this as I think I am currently ignorant of the types of applications that businesses can use to outweigh the costs. I predict that the cost per hour of the GPU dropping such that it isn&#x27;t such a no-brainer investment compared to previously. Especially if it is now possible to unlock potential from much older platforms running at lower electricity rates.</div><br/><div id="42848196" class="c"><input type="checkbox" id="c-42848196" checked=""/><div class="controls bullet"><span class="by">chatmasta</span><span>|</span><a href="#42846471">parent</a><span>|</span><a href="#42839939">next</a><span>|</span><label class="collapse" for="c-42848196">[-]</label><label class="expand" for="c-42848196">[10 more]</label></div><br/><div class="children"><div class="content">Why is there this implicit assumption that more efficient training&#x2F;inference will reduce GPU demand? It seems more likely - based on historical precedent in the computing industry - that demand will expand to fill the available hardware.<p>We can do more inference and more training on fewer GPUs. That doesn’t mean we need to stop buying GPUs. Unless people think we’re already doing the most training&#x2F;inference we’ll ever need to do…<p>“640KB ought to be enough for anybody.”</div><br/><div id="42848648" class="c"><input type="checkbox" id="c-42848648" checked=""/><div class="controls bullet"><span class="by">diamond559</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848196">parent</a><span>|</span><a href="#42848310">next</a><span>|</span><label class="collapse" for="c-42848648">[-]</label><label class="expand" for="c-42848648">[3 more]</label></div><br/><div class="children"><div class="content">Over the long run maybe, but for the next 2 years the market will struggle to find a use for all this possible extra gpus.  There is no real consumer demand for AI products and lots of backlash whenever implemented eg: that Coca Cola ad.  It&#x27;s going to be a big hit to demand in the short to medium term as the hyperscalers cut back&#x2F;reasses.</div><br/><div id="42850162" class="c"><input type="checkbox" id="c-42850162" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848648">parent</a><span>|</span><a href="#42849268">next</a><span>|</span><label class="collapse" for="c-42850162">[-]</label><label class="expand" for="c-42850162">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no consumer demand for AI?<p>In a thread full of people who have no idea what they&#x27;re talking about either from the ML side or the finance side, this is the worst take here.<p>OpenAI alone reports hundreds of millions of MAU. That&#x27;s before we talk about all of the other players. Before we talk about the immense demand in media like Hollywood and games.<p>Heck there&#x27;s an entire new entertainment industry forming with things like character ai having more than 20M MAU. Midjourney has about the same.<p>Definitely. An industry in its infancy that already has hundreds of millions of MAU across of it shows that there&#x27;s zero demand because of some ad no one has seen.</div><br/></div></div><div id="42849268" class="c"><input type="checkbox" id="c-42849268" checked=""/><div class="controls bullet"><span class="by">financypants</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848648">parent</a><span>|</span><a href="#42850162">prev</a><span>|</span><a href="#42848310">next</a><span>|</span><label class="collapse" for="c-42849268">[-]</label><label class="expand" for="c-42849268">[1 more]</label></div><br/><div class="children"><div class="content">Seems like your reasoning for how the next 2 years will go is a little slanted. And everyone in this thread is neglecting any demand issues stemming from market cycles.</div><br/></div></div></div></div><div id="42848310" class="c"><input type="checkbox" id="c-42848310" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848196">parent</a><span>|</span><a href="#42848648">prev</a><span>|</span><a href="#42848776">next</a><span>|</span><label class="collapse" for="c-42848310">[-]</label><label class="expand" for="c-42848310">[4 more]</label></div><br/><div class="children"><div class="content">Historically most compute went to run games in peoples homes, because companies didn&#x27;t see a need to run that much analytics. I don&#x27;t see why that wouldn&#x27;t happen now as well, there is a limit to how much value you can get out of this, since they aren&#x27;t AGI yet.</div><br/><div id="42848457" class="c"><input type="checkbox" id="c-42848457" checked=""/><div class="controls bullet"><span class="by">chatmasta</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848310">parent</a><span>|</span><a href="#42848776">next</a><span>|</span><label class="collapse" for="c-42848457">[-]</label><label class="expand" for="c-42848457">[3 more]</label></div><br/><div class="children"><div class="content">This just seems like a very bold statement to make in the first two years of LLMs. There are so many workflows where they are either not yet embedded at all, or only involved in a limited capacity. It doesn’t take much imagination to see the areas for growth. And that’s before even considering the growth in adoption. I think it’s a safe bet that LLM usage will proliferate in terms of both number of users, and number of inferences per user. And I wouldn’t be surprised if that growth is exponential on both those dimensions.</div><br/><div id="42848803" class="c"><input type="checkbox" id="c-42848803" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848457">parent</a><span>|</span><a href="#42848776">next</a><span>|</span><label class="collapse" for="c-42848803">[-]</label><label class="expand" for="c-42848803">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This just seems like a very bold statement to make in the first two years of LLMs<p>GPT-3 is 5 years old, this tech has been looking for a problem to solve for a really long time now. Many billions has already been burned trying to find a viable business model for these, and so far nothing has been found that warrants anything even close to multi trillion dollar valuations.<p>Even when the product is free people don&#x27;t use ChatGPT that much, making things cheaper will just reduce the demand for compute then.</div><br/><div id="42850203" class="c"><input type="checkbox" id="c-42850203" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848803">parent</a><span>|</span><a href="#42848776">next</a><span>|</span><label class="collapse" for="c-42850203">[-]</label><label class="expand" for="c-42850203">[1 more]</label></div><br/><div class="children"><div class="content">Everyone uses chatgpt now. You too. Hundreds of time per day.<p>It&#x27;s just not called chatgpt. Instead it is at the top of every Google search you do. Same technology.<p>It has basically replaced search for most people. A massive industry turned over in 5 years by a totally new technology.<p>Funny how the tech took over so completely it blends into the background to the point where you think it doesn&#x27;t exist.</div><br/></div></div></div></div></div></div></div></div><div id="42848776" class="c"><input type="checkbox" id="c-42848776" checked=""/><div class="controls bullet"><span class="by">egillie</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848196">parent</a><span>|</span><a href="#42848310">prev</a><span>|</span><a href="#42848437">next</a><span>|</span><label class="collapse" for="c-42848776">[-]</label><label class="expand" for="c-42848776">[1 more]</label></div><br/><div class="children"><div class="content">Could even argue that the price should go up, since the amount of with one GPU can do and its potential ROI just increased</div><br/></div></div><div id="42848437" class="c"><input type="checkbox" id="c-42848437" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#42846471">root</a><span>|</span><a href="#42848196">parent</a><span>|</span><a href="#42848776">prev</a><span>|</span><a href="#42839939">next</a><span>|</span><label class="collapse" for="c-42848437">[-]</label><label class="expand" for="c-42848437">[1 more]</label></div><br/><div class="children"><div class="content">I think training demand is what you might predict would plummet.<p>Inference demand might increase but you could easily believe that there’s substantial inelasticity currently.</div><br/></div></div></div></div></div></div><div id="42839939" class="c"><input type="checkbox" id="c-42839939" checked=""/><div class="controls bullet"><span class="by">Sol-</span><span>|</span><a href="#42846471">prev</a><span>|</span><a href="#42849432">next</a><span>|</span><label class="collapse" for="c-42839939">[-]</label><label class="expand" for="c-42839939">[15 more]</label></div><br/><div class="children"><div class="content">I find it interesting because the DeepSeek stuff, while very cool, doesn&#x27;t seem invalidate that more compute wouldn&#x27;t translate to even _higher_ capabilities?<p>It&#x27;s amazing what they did with a limited budget, but instead of the takeaway being &quot;we don&#x27;t need that much compute to achieve X&quot;, it could also be, &quot;These new results show that we can achieve even 1000*X with our currently planned compute buildout&quot;<p>But perhaps the idea is more like: &quot;We already have more AI capabilities than we know how to integrate into the economy for the time being&quot; and if that&#x27;s the hypothesis, then the availability of something this cheap would change the equation somewhat and possibly justify investing less money in more compute.</div><br/><div id="42846764" class="c"><input type="checkbox" id="c-42846764" checked=""/><div class="controls bullet"><span class="by">yo-cuddles</span><span>|</span><a href="#42839939">parent</a><span>|</span><a href="#42840128">next</a><span>|</span><label class="collapse" for="c-42846764">[-]</label><label class="expand" for="c-42846764">[7 more]</label></div><br/><div class="children"><div class="content">Probably not. If the price of Nvidia is dropping, it&#x27;s because investors see a world where Nvidia hardware is less valuable, probably because it will be used less.<p>You can&#x27;t do the distill&#x2F;magnify cycle like you do with alphago. LLM models have basically stalled in their base capabilities, pre training is basically over at this point, so the news arms race will be over marginal capability gains and (mostly) making them cheaper and cheaper.<p>But inference time scaling, right?<p>A weak model can pretend to be a stronger model if you let it cook for a long time. But right now it looks like models as strong as what we have aren&#x27;t going to be very useful even if you let them run for a long, long time. Basic logic problems still tank o3 if they&#x27;re not a kind that it&#x27;s seen before.<p>Basically, there doesn&#x27;t seem to be a use case for big data centers that run small models for long periods of time, they are in a danger zone of both not doing anything interesting and taking way too long to do it.<p>The AI war is going to turn into a price war, by my estimations. The models will be around as strong as the ones we have, perhaps with one more crank of quality. Then comes the empty, meaningless battle of just providing that service for as close to free as possible.<p>If Openai&#x27;s agents panned out we might be having another conversation. But they didn&#x27;t, and it wasn&#x27;t even close.<p>This is probably it. There&#x27;s not much left in the AI game</div><br/><div id="42849818" class="c"><input type="checkbox" id="c-42849818" checked=""/><div class="controls bullet"><span class="by">Jlagreen</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42846764">parent</a><span>|</span><a href="#42850206">next</a><span>|</span><label class="collapse" for="c-42849818">[-]</label><label class="expand" for="c-42849818">[1 more]</label></div><br/><div class="children"><div class="content">Your implication is that we have unlimited compute and therefore know that LLMs are stalled.<p>Have you considered that compute might be the reason why LLMs are stalled at the moment?<p>What made LLMs possible in the first place? Right, compute! Transformer Model is 8 years old, technically GPT4 could have been released 5 years ago. What stopped it? Simple, the compute being way too low.<p>Nvidia has improved compute by 1000x in the past 8 years but what if training GPT5 takes 6-12 months for 1 run based on what OpenAI tries to do?<p>What we see right now is that pre-training has reached the limits of Hopper and Big Tech is waiting for Blackwell. Blackwell will easily be 10x faster in cluster training (don&#x27;t look on chip performance only) and since Big Tech intends to build 10x larger GPU clusters then they will have 100x compute systems.<p>Let&#x27;s see then how it turns out.<p>The limit on training is time. If you want to make something new and improve then you should limit training time because nobody will wait 5-6 months for results anymore.<p>It was fine for OpenAI years ago to take months to years for new frontier models. But today the expectations are higher.<p>There is a reason why Blackwell is fully sold out for the year. AI research is totally starved for compute.<p>The best thing for Nvidia is also that while AI research companies compete with each other, they all try to get Nvidia AI HW.</div><br/></div></div><div id="42850206" class="c"><input type="checkbox" id="c-42850206" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42846764">parent</a><span>|</span><a href="#42849818">prev</a><span>|</span><a href="#42849982">next</a><span>|</span><label class="collapse" for="c-42850206">[-]</label><label class="expand" for="c-42850206">[1 more]</label></div><br/><div class="children"><div class="content">This argument ignores scaling laws</div><br/></div></div><div id="42849982" class="c"><input type="checkbox" id="c-42849982" checked=""/><div class="controls bullet"><span class="by">jes5199</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42846764">parent</a><span>|</span><a href="#42850206">prev</a><span>|</span><a href="#42849132">next</a><span>|</span><label class="collapse" for="c-42849982">[-]</label><label class="expand" for="c-42849982">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You can&#x27;t do the distill&#x2F;magnify cycle like you do with alphago<p>are you sure? people are saying that there’s an analogous cycle where you use o1-style reasoning to produce better inputs to the next training round</div><br/></div></div><div id="42849132" class="c"><input type="checkbox" id="c-42849132" checked=""/><div class="controls bullet"><span class="by">YZF</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42846764">parent</a><span>|</span><a href="#42849982">prev</a><span>|</span><a href="#42848885">next</a><span>|</span><label class="collapse" for="c-42849132">[-]</label><label class="expand" for="c-42849132">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t know for example what a larger model can do with the new techniques DeepSeek is using for improving&#x2F;refining it. It&#x27;s possible the new models on their [own] failed to show progress but a combination of techniques will enable that barrier to be crossed.<p>We also don&#x27;t know what the next discovery&#x2F;breakthrough will be like. The reward for getting smarter AI is still huge and so the investment will likely remain huge for some time. If anything DeepSeek is showing us that there is still progress to be made.</div><br/></div></div><div id="42848885" class="c"><input type="checkbox" id="c-42848885" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42846764">parent</a><span>|</span><a href="#42849132">prev</a><span>|</span><a href="#42840128">next</a><span>|</span><label class="collapse" for="c-42848885">[-]</label><label class="expand" for="c-42848885">[2 more]</label></div><br/><div class="children"><div class="content">So you&#x27;re saying we&#x27;re close to AGI? Because the game doesn&#x27;t stop until we get there.</div><br/><div id="42848990" class="c"><input type="checkbox" id="c-42848990" checked=""/><div class="controls bullet"><span class="by">r00fus</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42848885">parent</a><span>|</span><a href="#42840128">next</a><span>|</span><label class="collapse" for="c-42848990">[-]</label><label class="expand" for="c-42848990">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think LLMs lead to AGI.  It&#x27;s a local maxima.</div><br/></div></div></div></div></div></div><div id="42840128" class="c"><input type="checkbox" id="c-42840128" checked=""/><div class="controls bullet"><span class="by">lagrange77</span><span>|</span><a href="#42839939">parent</a><span>|</span><a href="#42846764">prev</a><span>|</span><a href="#42839993">next</a><span>|</span><label class="collapse" for="c-42840128">[-]</label><label class="expand" for="c-42840128">[1 more]</label></div><br/><div class="children"><div class="content">&gt; doesn&#x27;t seem invalidate that more compute wouldn&#x27;t translate to even _higher_ capabilities?<p>That&#x27;s how i understand it.<p>And since their current goal seems to be &#x27;AGI&#x27; and their current plan for achieving it seems to be scaling LLMs (network depth wise and at inference time prompt wise), i don&#x27;t see why it wouldn&#x27;t hold.</div><br/></div></div><div id="42839993" class="c"><input type="checkbox" id="c-42839993" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#42839939">parent</a><span>|</span><a href="#42840128">prev</a><span>|</span><a href="#42849432">next</a><span>|</span><label class="collapse" for="c-42839993">[-]</label><label class="expand" for="c-42839993">[6 more]</label></div><br/><div class="children"><div class="content">The stock market is not the economy, Wall Street is not Main Street. You need to look at this more macroscopically if you want to understand this.<p>Basically: China tech sector just made a big splash, traders who witnessed this think other traders will sell because maybe US tech sector wasn&#x27;t as hot, so they sell as other traders also think that and sell.<p>The fall will come to rest once stocks have fallen enough that traders stop thinking other traders will sell.<p>Investors holding for the long haul will see this fall as stocks going on sale and proceed to buy because they think other investors will buy.<p>Meanwhile in the real world, on Main Street, nothing has really changed.<p>Bogleheads meanwhile are just starting the day with their coffee, no damns given to the machinations of the stock market because it&#x27;s Monday and there&#x27;s work to be done.</div><br/><div id="42840460" class="c"><input type="checkbox" id="c-42840460" checked=""/><div class="controls bullet"><span class="by">qwytw</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42839993">parent</a><span>|</span><a href="#42840248">next</a><span>|</span><label class="collapse" for="c-42840460">[-]</label><label class="expand" for="c-42840460">[1 more]</label></div><br/><div class="children"><div class="content">Is it really related to China&#x27;s tech sector as such, though? If this is true then Openai, Google or even many magnitudes smaller companies etc. can just easily replicate similar methods in their processes and provide models which are just as good or better. However they&#x27;ll need way less Nvidia GPUs and other HW to do that than when training their current models.</div><br/></div></div><div id="42840248" class="c"><input type="checkbox" id="c-42840248" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42839993">parent</a><span>|</span><a href="#42840460">prev</a><span>|</span><a href="#42849432">next</a><span>|</span><label class="collapse" for="c-42840248">[-]</label><label class="expand" for="c-42840248">[4 more]</label></div><br/><div class="children"><div class="content">Not really.<p>The Magnificent Seven are the only thing propping up the whole US economy.<p>If they go down, you go down.</div><br/><div id="42843574" class="c"><input type="checkbox" id="c-42843574" checked=""/><div class="controls bullet"><span class="by">rsanek</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42840248">parent</a><span>|</span><a href="#42842376">next</a><span>|</span><label class="collapse" for="c-42843574">[-]</label><label class="expand" for="c-42843574">[1 more]</label></div><br/><div class="children"><div class="content">s&amp;p500 was still up by normal amounts during 2023 and 2024 if you exclude big tech. definitely they are an outsize portion of the index but that doesn&#x27;t mean the rest of the economy isn&#x27;t growing. <a href="https:&#x2F;&#x2F;www.inc.com&#x2F;phil-rosen&#x2F;stock-market-outlook-sp500-investors-economic-fed-rate-cut-magnificent-seven&#x2F;91103119" rel="nofollow">https:&#x2F;&#x2F;www.inc.com&#x2F;phil-rosen&#x2F;stock-market-outlook-sp500-in...</a></div><br/></div></div><div id="42842376" class="c"><input type="checkbox" id="c-42842376" checked=""/><div class="controls bullet"><span class="by">nullocator</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42840248">parent</a><span>|</span><a href="#42843574">prev</a><span>|</span><a href="#42840907">next</a><span>|</span><label class="collapse" for="c-42842376">[-]</label><label class="expand" for="c-42842376">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this a good reason to break these companies up and mitigate the risk?</div><br/></div></div><div id="42840907" class="c"><input type="checkbox" id="c-42840907" checked=""/><div class="controls bullet"><span class="by">SebFender</span><span>|</span><a href="#42839939">root</a><span>|</span><a href="#42840248">parent</a><span>|</span><a href="#42842376">prev</a><span>|</span><a href="#42849432">next</a><span>|</span><label class="collapse" for="c-42840907">[-]</label><label class="expand" for="c-42840907">[1 more]</label></div><br/><div class="children"><div class="content">Well said.</div><br/></div></div></div></div></div></div></div></div><div id="42849432" class="c"><input type="checkbox" id="c-42849432" checked=""/><div class="controls bullet"><span class="by">Too</span><span>|</span><a href="#42839939">prev</a><span>|</span><a href="#42846202">next</a><span>|</span><label class="collapse" for="c-42849432">[-]</label><label class="expand" for="c-42849432">[1 more]</label></div><br/><div class="children"><div class="content">Jeez chill... it’s just back to where it was 4 months ago and even after the drop it is still up 100% compared to this time last year! And it’s all fake inflated money.<p>This unprecedented growth simply couldn’t continue forever.</div><br/></div></div><div id="42846202" class="c"><input type="checkbox" id="c-42846202" checked=""/><div class="controls bullet"><span class="by">skizm</span><span>|</span><a href="#42849432">prev</a><span>|</span><a href="#42848570">next</a><span>|</span><label class="collapse" for="c-42846202">[-]</label><label class="expand" for="c-42846202">[11 more]</label></div><br/><div class="children"><div class="content">I like the chart Bloomberg has of the top 10 largest single day stock drops in history. 8 out of the 10 are NVDA (Meta and Amazon are the other two).</div><br/><div id="42850120" class="c"><input type="checkbox" id="c-42850120" checked=""/><div class="controls bullet"><span class="by">jandrese</span><span>|</span><a href="#42846202">parent</a><span>|</span><a href="#42846753">next</a><span>|</span><label class="collapse" for="c-42850120">[-]</label><label class="expand" for="c-42850120">[1 more]</label></div><br/><div class="children"><div class="content">I feel like this is a symptom of our broken economic system that has allowed too much cash to be trapped in the markets, forever making mostly imaginary numbers go up while the middle class gets squeezed and the poor continue to suffer.<p>A fundamental feature of a capitalist system you can use money to make more money.  That&#x27;s great for growing wealth.  But you have to be careful, it&#x27;s like a sound system at a concert.  When you install it everybody benefits from being able to hear the band.  But it is easily to cause an earspittig feedback loop if you don&#x27;t keep the singers a safe distance from the speakers. Unfortunately, the only way people have to quantify how good a concert sounds is by loudness, and because the awful screeching of a feedback loop is about the loudest thing possible we&#x27;ve been just holding the microphone at the speaker for close to 50 years and telling ourselves that everybody is enjoying the music.<p>It is the job of the government, because nobody else can do it, to prevent the runaway feedback loop that is a fundamental flaw of capitalism, and our government has been entirely derelict in their duty.  This has caused market distortions that go beyond the stock market.  The housing market is also suffering for example.  There is way too much money at the top looking for anything that can create a return, and when something looks promising it gets inflated to ridiculous levels, far beyond what is helpful for a company trying to expand their business.  There&#x27;s so much money most of it has to be dumb money.</div><br/></div></div><div id="42846753" class="c"><input type="checkbox" id="c-42846753" checked=""/><div class="controls bullet"><span class="by">culi</span><span>|</span><a href="#42846202">parent</a><span>|</span><a href="#42850120">prev</a><span>|</span><a href="#42848784">next</a><span>|</span><label class="collapse" for="c-42846753">[-]</label><label class="expand" for="c-42846753">[7 more]</label></div><br/><div class="children"><div class="content">Why mention it if you&#x27;re not gonna link it?</div><br/><div id="42848525" class="c"><input type="checkbox" id="c-42848525" checked=""/><div class="controls bullet"><span class="by">TeaBrain</span><span>|</span><a href="#42846202">root</a><span>|</span><a href="#42846753">parent</a><span>|</span><a href="#42848505">next</a><span>|</span><label class="collapse" for="c-42848525">[-]</label><label class="expand" for="c-42848525">[2 more]</label></div><br/><div class="children"><div class="content">The chart is within the below linked article.  The below link I&#x27;ve provided is a gift link, so you should be able to access the article through it.<p><a href="https:&#x2F;&#x2F;www.bloomberg.com&#x2F;opinion&#x2F;articles&#x2F;2025-01-27&#x2F;deepseek-s-ai-model-bursts-nvidia-s-stock-bubble-m6fnhvoi?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczODAzNDA4OSwiZXhwIjoxNzM4NjM4ODg5LCJhcnRpY2xlSWQiOiJTUVJSVVdEV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2MzAyNDM3NEJFNUM0NzE1OTY5MEZDMTY4MTE1QzZEQiJ9.J05Z5Guk9mvLTvcChP683im9bpTr2jiOPJF9oc3ljKE" rel="nofollow">https:&#x2F;&#x2F;www.bloomberg.com&#x2F;opinion&#x2F;articles&#x2F;2025-01-27&#x2F;deepse...</a></div><br/><div id="42849702" class="c"><input type="checkbox" id="c-42849702" checked=""/><div class="controls bullet"><span class="by">culi</span><span>|</span><a href="#42846202">root</a><span>|</span><a href="#42848525">parent</a><span>|</span><a href="#42848505">next</a><span>|</span><label class="collapse" for="c-42849702">[-]</label><label class="expand" for="c-42849702">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I have the Bypass Paywalls extension so i forgot bloomberg even had paywalls<p><a href="https:&#x2F;&#x2F;github.com&#x2F;bpc-clone&#x2F;bypass-paywalls-firefox-clean">https:&#x2F;&#x2F;github.com&#x2F;bpc-clone&#x2F;bypass-paywalls-firefox-clean</a></div><br/></div></div></div></div><div id="42848505" class="c"><input type="checkbox" id="c-42848505" checked=""/><div class="controls bullet"><span class="by">Jach</span><span>|</span><a href="#42846202">root</a><span>|</span><a href="#42846753">parent</a><span>|</span><a href="#42848525">prev</a><span>|</span><a href="#42847270">next</a><span>|</span><label class="collapse" for="c-42848505">[-]</label><label class="expand" for="c-42848505">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.is&#x2F;Svzfz" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;Svzfz</a> Paywall schmaywall</div><br/></div></div><div id="42847270" class="c"><input type="checkbox" id="c-42847270" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42846202">root</a><span>|</span><a href="#42846753">parent</a><span>|</span><a href="#42848505">prev</a><span>|</span><a href="#42848784">next</a><span>|</span><label class="collapse" for="c-42847270">[-]</label><label class="expand" for="c-42847270">[3 more]</label></div><br/><div class="children"><div class="content">Because paywall.</div><br/><div id="42849709" class="c"><input type="checkbox" id="c-42849709" checked=""/><div class="controls bullet"><span class="by">culi</span><span>|</span><a href="#42846202">root</a><span>|</span><a href="#42847270">parent</a><span>|</span><a href="#42848398">next</a><span>|</span><label class="collapse" for="c-42849709">[-]</label><label class="expand" for="c-42849709">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;bpc-clone&#x2F;bypass-paywalls-firefox-clean">https:&#x2F;&#x2F;github.com&#x2F;bpc-clone&#x2F;bypass-paywalls-firefox-clean</a></div><br/></div></div><div id="42848398" class="c"><input type="checkbox" id="c-42848398" checked=""/><div class="controls bullet"><span class="by">redundantly</span><span>|</span><a href="#42846202">root</a><span>|</span><a href="#42847270">parent</a><span>|</span><a href="#42849709">prev</a><span>|</span><a href="#42848784">next</a><span>|</span><label class="collapse" for="c-42848398">[-]</label><label class="expand" for="c-42848398">[1 more]</label></div><br/><div class="children"><div class="content">It can still be helpful. Some people are already on the other side of that wall, or someone else usually comes along with an archived, non-paywall version.</div><br/></div></div></div></div></div></div><div id="42848784" class="c"><input type="checkbox" id="c-42848784" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#42846202">parent</a><span>|</span><a href="#42846753">prev</a><span>|</span><a href="#42848570">next</a><span>|</span><label class="collapse" for="c-42848784">[-]</label><label class="expand" for="c-42848784">[2 more]</label></div><br/><div class="children"><div class="content">adjusted for inflation?</div><br/><div id="42849262" class="c"><input type="checkbox" id="c-42849262" checked=""/><div class="controls bullet"><span class="by">MichaelDickens</span><span>|</span><a href="#42846202">root</a><span>|</span><a href="#42848784">parent</a><span>|</span><a href="#42848570">next</a><span>|</span><label class="collapse" for="c-42849262">[-]</label><label class="expand" for="c-42849262">[1 more]</label></div><br/><div class="children"><div class="content">Really it should be adjusted for global (or US) total market cap. Market cap tends to go up faster than inflation, so even if you adjust for inflation, it will still be skewed toward modern companies.</div><br/></div></div></div></div></div></div><div id="42848570" class="c"><input type="checkbox" id="c-42848570" checked=""/><div class="controls bullet"><span class="by">ezoe</span><span>|</span><a href="#42846202">prev</a><span>|</span><a href="#42850149">next</a><span>|</span><label class="collapse" for="c-42848570">[-]</label><label class="expand" for="c-42848570">[12 more]</label></div><br/><div class="children"><div class="content">I really don&#x27;t understand the market thinks Nvidia is losing its value.<p>If DeepSeek reduce the required computational resources, we can pour more computational resources to improve it further. There&#x27;s nothing bad about more resources.</div><br/><div id="42848945" class="c"><input type="checkbox" id="c-42848945" checked=""/><div class="controls bullet"><span class="by">qqtt</span><span>|</span><a href="#42848570">parent</a><span>|</span><a href="#42848668">next</a><span>|</span><label class="collapse" for="c-42848945">[-]</label><label class="expand" for="c-42848945">[4 more]</label></div><br/><div class="children"><div class="content">Well you have to keep in mind that Nvidia has a 3 trillion dollar valuation. That kind of heavy valuation comes with heavy expectations about future growth. Some of those assumptions about future Nvidia growth are their ability to maintain their heavy growth rates, for very far into the future.<p>Training is a huge component of Nvidia&#x27;s projected growth. Inference is actually much more competitive, but training is almost exclusively Nvidia&#x27;s domain. If Deepseek&#x27;s claims are true, that would represent a 10x reduction in cost for training for similar models (6 million for r1 vs 60 million for something like o1).<p>It is absolutely not the case in ML that &quot;there is nothing bad about more resources&quot;. There is something very bad - cost. And another bad thing - depreciation. And finally, another bad thing - the fact that new chips and approaches are coming out all the time, so if you are on older hardware you might be missing out. Training complex models for cheaper  will allow companies to potentially re-allocate away from hardware into software (ie, hiring more engineering to build more models, instead of less engineers and more hardware to build less models).<p>Finally, there is a giant elephant in the room that it is very unclear if throwing more resources at LLM training will net better results. There are diminishing returns in terms of return on investment in training, especially with LLM-style use cases. It is actually very non-obvious right now how pouring more compute specifically at training will result in better LLMs.</div><br/><div id="42849449" class="c"><input type="checkbox" id="c-42849449" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#42848570">root</a><span>|</span><a href="#42848945">parent</a><span>|</span><a href="#42848668">next</a><span>|</span><label class="collapse" for="c-42849449">[-]</label><label class="expand" for="c-42849449">[3 more]</label></div><br/><div class="children"><div class="content">My layman view is that more compute (more reasoning) will not solve harder problems. I&#x27;m using those models every day and when problem hits a certain complexity it will fail, no matter how much it &quot;reasons&quot;</div><br/><div id="42849990" class="c"><input type="checkbox" id="c-42849990" checked=""/><div class="controls bullet"><span class="by">jes5199</span><span>|</span><a href="#42848570">root</a><span>|</span><a href="#42849449">parent</a><span>|</span><a href="#42849572">next</a><span>|</span><label class="collapse" for="c-42849990">[-]</label><label class="expand" for="c-42849990">[1 more]</label></div><br/><div class="children"><div class="content">I had a similar intuition for a long time, but I’ve watched the threshold of “certain complexity” move, and I’m no longer convinced that I know when it’s going to stop</div><br/></div></div><div id="42849572" class="c"><input type="checkbox" id="c-42849572" checked=""/><div class="controls bullet"><span class="by">johnfn</span><span>|</span><a href="#42848570">root</a><span>|</span><a href="#42849449">parent</a><span>|</span><a href="#42849990">prev</a><span>|</span><a href="#42848668">next</a><span>|</span><label class="collapse" for="c-42849572">[-]</label><label class="expand" for="c-42849572">[1 more]</label></div><br/><div class="children"><div class="content">I think this is fairly easily debunked by o1, which is basically just 4o in a thinking for loop, and performs better on difficult tasks. Not a LOT better, mind you, but better enough to be measurable.</div><br/></div></div></div></div></div></div><div id="42848668" class="c"><input type="checkbox" id="c-42848668" checked=""/><div class="controls bullet"><span class="by">skellington</span><span>|</span><a href="#42848570">parent</a><span>|</span><a href="#42848945">prev</a><span>|</span><a href="#42848650">next</a><span>|</span><label class="collapse" for="c-42848668">[-]</label><label class="expand" for="c-42848668">[4 more]</label></div><br/><div class="children"><div class="content">Markets aren&#x27;t rational.<p>NVidia is currently a hype stock which means LOTS of speculation, probably with lots of leverage. So, the people who have made large gains and&#x2F;or are leveraged are highly incentivized to panic sell on any PERCEIVED bad news. It doesn&#x27;t even matter if the bad news will materially impact sales. What matters is how the other gamblers will react to the news and getting in front of them.<p>:)</div><br/><div id="42848760" class="c"><input type="checkbox" id="c-42848760" checked=""/><div class="controls bullet"><span class="by">aunty_helen</span><span>|</span><a href="#42848570">root</a><span>|</span><a href="#42848668">parent</a><span>|</span><a href="#42848650">next</a><span>|</span><label class="collapse" for="c-42848760">[-]</label><label class="expand" for="c-42848760">[3 more]</label></div><br/><div class="children"><div class="content">Another thing, the markets had priced in X demand scaling @ ~145 and suddenly it&#x27;s X&#x2F;30 demand scaling and therefore the price should drop.</div><br/><div id="42849766" class="c"><input type="checkbox" id="c-42849766" checked=""/><div class="controls bullet"><span class="by">Jlagreen</span><span>|</span><a href="#42848570">root</a><span>|</span><a href="#42848760">parent</a><span>|</span><a href="#42849515">next</a><span>|</span><label class="collapse" for="c-42849766">[-]</label><label class="expand" for="c-42849766">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s wrong.<p>DeepSeek is a problem for Big Tech, not for Nvidia.<p>Why?<p>Imagine a small startup can do something better than Gemini or ChatGPT or Claude.<p>So it can be disruptive.<p>What can Big Tech do to avoid disruption? Buying every SINGLE GPU Nvidia produces! They have the money and they can use the GPUs in research.<p>The worst nightmare of any Tech CEO is a startup which disrupts you so you have to either be faster or you kill access to needed infrastructure for the startup. Or even better, the startup has to rent your cloud infrastructure, this way you earn money and you have an eye on what&#x27;s going on.<p>Additionally, Hyperscalers only get 50-60% of Nvidia&#x27;s supply. They all complain of being undersupplied yet they get only 60% and not 99% of Nvidia&#x27;s supply. How come? Because Nvidia has a lot of other customers they like to supply to. That alone tells you how huge the demand is that Nvidia even has to delay Big Tech deliveries.<p>Also the demand for Nvidia didn&#x27;t drop. DeepSeek isn&#x27;t a frontier model. It&#x27;s a distilled model therefore the moment OpenAI, Meta or the others release a new frontier model, DeepSeek will become obsolete and will have to start again to optimize.</div><br/></div></div><div id="42849515" class="c"><input type="checkbox" id="c-42849515" checked=""/><div class="controls bullet"><span class="by">skellington</span><span>|</span><a href="#42848570">root</a><span>|</span><a href="#42848760">parent</a><span>|</span><a href="#42849766">prev</a><span>|</span><a href="#42848650">next</a><span>|</span><label class="collapse" for="c-42849515">[-]</label><label class="expand" for="c-42849515">[1 more]</label></div><br/><div class="children"><div class="content">True, but current price isn&#x27;t based on fundamentals, it&#x27;s based on hype-value.<p>nVidia is going to be a very volatile stock for years to come.<p>I don&#x27;t see deepseek changing nvidia&#x27;s short term growth potential though. Efficiencies in training were always inevitable, but more GPU still equals smarter AI....probably.</div><br/></div></div></div></div></div></div><div id="42848650" class="c"><input type="checkbox" id="c-42848650" checked=""/><div class="controls bullet"><span class="by">tonyhart7</span><span>|</span><a href="#42848570">parent</a><span>|</span><a href="#42848668">prev</a><span>|</span><a href="#42848722">next</a><span>|</span><label class="collapse" for="c-42848650">[-]</label><label class="expand" for="c-42848650">[1 more]</label></div><br/><div class="children"><div class="content">- &quot;I really don&#x27;t understand the market thinks Nvidia is losing its value.&quot;<p>because the less GPU need to train, the less money to be made<p>- &quot;If DeepSeek reduce the required computational resources, we can pour more computational resources to improve it further. There&#x27;s nothing bad about more resources.&quot;<p>thats why you are not hedgefund manager, these guys job is to ensure that the HYPETRAIN for company to buy as many nvidia gpu to sell no matter what, if we can produce comparable model without using B (as it stands billions of dollar), it means there are less billions of dollar to be made and the HYPETRAIN is near the end</div><br/></div></div><div id="42848722" class="c"><input type="checkbox" id="c-42848722" checked=""/><div class="controls bullet"><span class="by">BoredomHeights</span><span>|</span><a href="#42848570">parent</a><span>|</span><a href="#42848650">prev</a><span>|</span><a href="#42848812">next</a><span>|</span><label class="collapse" for="c-42848722">[-]</label><label class="expand" for="c-42848722">[1 more]</label></div><br/><div class="children"><div class="content">The market might be right that Nvidia is overvalued, but if so I think only accidentally and not because of this news. Like you said, at least for now I think it&#x27;s fairly clear that if a company has X resources and finds a way to do the same thing with half, instead of using less they&#x27;ll just try to do twice as much. This could eventually changed but I don&#x27;t think AI is anywhere near that point yet.</div><br/></div></div><div id="42848812" class="c"><input type="checkbox" id="c-42848812" checked=""/><div class="controls bullet"><span class="by">luxuryballs</span><span>|</span><a href="#42848570">parent</a><span>|</span><a href="#42848722">prev</a><span>|</span><a href="#42850149">next</a><span>|</span><label class="collapse" for="c-42848812">[-]</label><label class="expand" for="c-42848812">[1 more]</label></div><br/><div class="children"><div class="content">yeah I think people just got spooked and sold to take some profits they were going to take soon anyways</div><br/></div></div></div></div><div id="42850149" class="c"><input type="checkbox" id="c-42850149" checked=""/><div class="controls bullet"><span class="by">cultofmetatron</span><span>|</span><a href="#42848570">prev</a><span>|</span><a href="#42840381">next</a><span>|</span><label class="collapse" for="c-42850149">[-]</label><label class="expand" for="c-42850149">[2 more]</label></div><br/><div class="children"><div class="content">I dont&#x27; know what the surprise is here. the human brain consumes about 20 watts. literally a rounding error compared to what chatgpt uses. so we already know that there was plenty of room on the table to improve.<p>incidentally, I love these kinds of market crashes. just moved a big chunk of my savings account into stocks last night  :). Buy and hold. dont&#x27; sell during a dip lol</div><br/><div id="42850268" class="c"><input type="checkbox" id="c-42850268" checked=""/><div class="controls bullet"><span class="by">ludvigk</span><span>|</span><a href="#42850149">parent</a><span>|</span><a href="#42840381">next</a><span>|</span><label class="collapse" for="c-42850268">[-]</label><label class="expand" for="c-42850268">[1 more]</label></div><br/><div class="children"><div class="content">Why do you think the human brain and chatgpt are in any way related?</div><br/></div></div></div></div><div id="42840381" class="c"><input type="checkbox" id="c-42840381" checked=""/><div class="controls bullet"><span class="by">plaidfuji</span><span>|</span><a href="#42850149">prev</a><span>|</span><a href="#42849471">next</a><span>|</span><label class="collapse" for="c-42840381">[-]</label><label class="expand" for="c-42840381">[17 more]</label></div><br/><div class="children"><div class="content">The part of this that doesn’t jibe with me is the fact that they also released this incredibly detailed technical report on their architecture and training strategy. The paper is well-written and has a lot of specifics. Exactly the opposite of what you would do if you had truly made an advancement of world-altering magnitude. All this says to me is that the models themselves have very little intrinsic value &#x2F; are highly fungible. The true value lies in the software interfaces to the models, and the ability to make it easy to plug your data into the models.<p>My guess is the consumer market will ultimately be won by 2-3 players that make the best app &#x2F; interface and leverage some kind of network effect, and enterprise market will just be captured by the people who have the enterprise data, I.e. MSFT, AMZN, GOOG. Depending on just how impactful AI can be for consumers, this could upend Apple if a full mobile hardware+OS redesign is able to create a step change in seamlessness of UI. That seems to me to be the biggest unknown now - how will hardware and devices adapt?<p>NVDA will still do quite well because as others have noted, if it’s cheaper to train, the balance will just shift toward deploying more edge devices for inference, which is necessary to realize the value built up in the bubble anyway. Some day the compute will become more fungible but the momentum behind the nvidia ecosystem is way too strong right now.</div><br/><div id="42840505" class="c"><input type="checkbox" id="c-42840505" checked=""/><div class="controls bullet"><span class="by">gitfan86</span><span>|</span><a href="#42840381">parent</a><span>|</span><a href="#42841848">next</a><span>|</span><label class="collapse" for="c-42840505">[-]</label><label class="expand" for="c-42840505">[5 more]</label></div><br/><div class="children"><div class="content">What has changed is the perception that people like OpenAI&#x2F;MSFT would have an edge on the competition because of their huge datacenters full of NVDA hardware. That is no longer true. People now believe that you can build very capable AI applications for far less money. So the perception is that the big guys no longer have an edge.<p>Tesla had already proven that to be wrong. Tesla&#x27;s Hardware 3 is a 6 year old design, and it does amazingly well on less than 300 watts. And that was mostly trained on a 8k cluster.</div><br/><div id="42845572" class="c"><input type="checkbox" id="c-42845572" checked=""/><div class="controls bullet"><span class="by">nejsjsjsbsb</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42840505">parent</a><span>|</span><a href="#42840536">next</a><span>|</span><label class="collapse" for="c-42845572">[-]</label><label class="expand" for="c-42845572">[1 more]</label></div><br/><div class="children"><div class="content">The perception only makes sense if it is &quot;that&#x27;s it, pack up your stall&quot; for AI.<p>I think what really happened is day to day trading noise. Nothing fundamentally changed, but traders believed other people believed it would.</div><br/></div></div><div id="42840536" class="c"><input type="checkbox" id="c-42840536" checked=""/><div class="controls bullet"><span class="by">plaidfuji</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42840505">parent</a><span>|</span><a href="#42845572">prev</a><span>|</span><a href="#42841848">next</a><span>|</span><label class="collapse" for="c-42840536">[-]</label><label class="expand" for="c-42840536">[3 more]</label></div><br/><div class="children"><div class="content">I mean, I think they still do have an edge - ChatGPT is a great app and has strong consumer recognition already, very hard to displace.. and MSFT has a major installed base of enterprise customers who cannot readily switch cloud &#x2F; productivity suite providers. So I guess they still have an edge it’s just nore of a traditional edge.</div><br/><div id="42846068" class="c"><input type="checkbox" id="c-42846068" checked=""/><div class="controls bullet"><span class="by">physicsguy</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42840536">parent</a><span>|</span><a href="#42840729">next</a><span>|</span><label class="collapse" for="c-42846068">[-]</label><label class="expand" for="c-42846068">[1 more]</label></div><br/><div class="children"><div class="content">Microsoft don&#x27;t have to use OpenAI though, they could swap that out underneath for the business applications.</div><br/></div></div><div id="42840729" class="c"><input type="checkbox" id="c-42840729" checked=""/><div class="controls bullet"><span class="by">gitfan86</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42840536">parent</a><span>|</span><a href="#42846068">prev</a><span>|</span><a href="#42841848">next</a><span>|</span><label class="collapse" for="c-42840729">[-]</label><label class="expand" for="c-42840729">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it is still a valid business model and I would expect MSFT to continue to make profits.</div><br/></div></div></div></div></div></div><div id="42841848" class="c"><input type="checkbox" id="c-42841848" checked=""/><div class="controls bullet"><span class="by">polygamous_bat</span><span>|</span><a href="#42840381">parent</a><span>|</span><a href="#42840505">prev</a><span>|</span><a href="#42845549">next</a><span>|</span><label class="collapse" for="c-42841848">[-]</label><label class="expand" for="c-42841848">[5 more]</label></div><br/><div class="children"><div class="content">&gt; The part of this that doesn’t jibe with me is the fact that they also released this incredibly detailed technical report on their architecture and training strategy. The paper is well-written and has a lot of specifics. Exactly the opposite of what you would do if you had truly made an advancement of world-altering magnitude.<p>I disagree completely on this sentiment. This was in fact the trend for a century or more (see inventions ranging from the polio vaccine to &quot;Attention is all you need&quot; by Vaswani et. al.) before &quot;Open&quot;AI became the biggest player on the market due and Sam Altman tried to bag all the gains for himself. Hopefully, we can reverse course on this trend and go back to when world-changing innovations are shared openly so they can actually change the world.</div><br/><div id="42843016" class="c"><input type="checkbox" id="c-42843016" checked=""/><div class="controls bullet"><span class="by">evanjrowley</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42841848">parent</a><span>|</span><a href="#42848483">next</a><span>|</span><label class="collapse" for="c-42843016">[-]</label><label class="expand" for="c-42843016">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. There&#x27;s a strong case for being open about the advancements in AI. Secretive companies like Microsoft, OpenAI, and others are undercut by DeepSeek and any other company on the globe who wants to build on what they&#x27;ve published. Politically there are more reasons why China should not become the global center of AI and less reasons why the US should remain the center of it. Therefore, an approach that enables AI institutions worldwide makes more sense for China at this stage. The EU for example has even less reason now to form a dependency on OpenAI and Nvidia, which works to the advantage of China and Chinese AI companies.</div><br/></div></div><div id="42848483" class="c"><input type="checkbox" id="c-42848483" checked=""/><div class="controls bullet"><span class="by">kibibu</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42841848">parent</a><span>|</span><a href="#42843016">prev</a><span>|</span><a href="#42844570">next</a><span>|</span><label class="collapse" for="c-42848483">[-]</label><label class="expand" for="c-42848483">[1 more]</label></div><br/><div class="children"><div class="content">Even the &quot;Language Models are Unsupervised Multitask Learners&quot; paper was pretty open; I&#x27;d say even more open than the R1 paper.</div><br/></div></div><div id="42844570" class="c"><input type="checkbox" id="c-42844570" checked=""/><div class="controls bullet"><span class="by">plaidfuji</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42841848">parent</a><span>|</span><a href="#42848483">prev</a><span>|</span><a href="#42845549">next</a><span>|</span><label class="collapse" for="c-42844570">[-]</label><label class="expand" for="c-42844570">[2 more]</label></div><br/><div class="children"><div class="content">I’m not arguing for&#x2F;against the altruistic ideal of sharing technological advancements with society, I’m just saying that having a great model architecture is really not a defensible value proposition for a business. Maybe more accurate to say publishing everything in detail indicates that it’s likely not a defensible advancement, not that it isn’t significant.</div><br/><div id="42848345" class="c"><input type="checkbox" id="c-42848345" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42844570">parent</a><span>|</span><a href="#42845549">next</a><span>|</span><label class="collapse" for="c-42848345">[-]</label><label class="expand" for="c-42848345">[1 more]</label></div><br/><div class="children"><div class="content">Here is a great interview. They don’t seem to care that much about money. They are already profitable.<p><a href="https:&#x2F;&#x2F;www.chinatalk.media&#x2F;p&#x2F;deepseek-ceo-interview-with-chinas" rel="nofollow">https:&#x2F;&#x2F;www.chinatalk.media&#x2F;p&#x2F;deepseek-ceo-interview-with-ch...</a><p>&gt; Money has never been the problem for us; bans on shipments of advanced chips are the problem.</div><br/></div></div></div></div></div></div><div id="42845549" class="c"><input type="checkbox" id="c-42845549" checked=""/><div class="controls bullet"><span class="by">nejsjsjsbsb</span><span>|</span><a href="#42840381">parent</a><span>|</span><a href="#42841848">prev</a><span>|</span><a href="#42840705">next</a><span>|</span><label class="collapse" for="c-42845549">[-]</label><label class="expand" for="c-42845549">[3 more]</label></div><br/><div class="children"><div class="content">I always thought AMZN is the winner since I looked into Bedrock. When I saw Claude on there it added a fuck yeah, and now the best models being open just takes it to another level.<p>AMZN: no horse picked, we host anything<p>MSFT: Open AI<p>GOOGLE: Google AI<p>AMZN is in the strongest position.</div><br/><div id="42848701" class="c"><input type="checkbox" id="c-42848701" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42845549">parent</a><span>|</span><a href="#42849000">next</a><span>|</span><label class="collapse" for="c-42848701">[-]</label><label class="expand" for="c-42848701">[1 more]</label></div><br/><div class="children"><div class="content">AWS’s usual most doesn’t really apply here. AWS is Hotel California — if your business and data is in AWS, the cost of moving any data-intensive portion out of AWS is absurd due to egress fees. But LLM inference is not data-transfer intensive at all — a relatively small number of bytes&#x2F;tokens go to the model, it does a <i>lot</i> of compute, and a relatively small number of tokens come back. So a business that’s stuck in AWS can cost-effectively outsource their LLM inference to a competitor without any substantial egress fees.<p>RAG is kind of an exception, but RAG still splits the database part from the inference part, and the inference part is what needs lots of inference-time compute. AWS may still have a strong moat for the compute needed to build an embedding database in the first place.<p>Simple, cheap, low-compute inference on large amounts of data is another exception, but this use will strongly favor the “cheap” part, which means there may not be as much money in it for AWS.  No one is about to do o3-style inference on each of 1M old business records.</div><br/></div></div><div id="42849000" class="c"><input type="checkbox" id="c-42849000" checked=""/><div class="controls bullet"><span class="by">mindwok</span><span>|</span><a href="#42840381">root</a><span>|</span><a href="#42845549">parent</a><span>|</span><a href="#42848701">prev</a><span>|</span><a href="#42840705">next</a><span>|</span><label class="collapse" for="c-42849000">[-]</label><label class="expand" for="c-42849000">[1 more]</label></div><br/><div class="children"><div class="content">You can also use Claude, Mistral, Llama and others on Google Vertex, similar to Bedrock.</div><br/></div></div></div></div><div id="42845965" class="c"><input type="checkbox" id="c-42845965" checked=""/><div class="controls bullet"><span class="by">fspeech</span><span>|</span><a href="#42840381">parent</a><span>|</span><a href="#42840705">prev</a><span>|</span><a href="#42845351">next</a><span>|</span><label class="collapse" for="c-42845965">[-]</label><label class="expand" for="c-42845965">[1 more]</label></div><br/><div class="children"><div class="content">You are not taking into account why people are willing to pay exceedingly high prices for GPUs now and that the underlying reason may have been taken away.</div><br/></div></div><div id="42845351" class="c"><input type="checkbox" id="c-42845351" checked=""/><div class="controls bullet"><span class="by">tokioyoyo</span><span>|</span><a href="#42840381">parent</a><span>|</span><a href="#42845965">prev</a><span>|</span><a href="#42849471">next</a><span>|</span><label class="collapse" for="c-42845351">[-]</label><label class="expand" for="c-42845351">[1 more]</label></div><br/><div class="children"><div class="content">Build trust by releasing your inferior product for free and as open as possible. Get attention, then release your superior product behind paywall. Name recognition is incredibly important within and outside of China.<p>Keep in mind, they’re still competing with Baidu, Tencent and other AI labs.</div><br/></div></div></div></div><div id="42849471" class="c"><input type="checkbox" id="c-42849471" checked=""/><div class="controls bullet"><span class="by">divbzero</span><span>|</span><a href="#42840381">prev</a><span>|</span><a href="#42848438">next</a><span>|</span><label class="collapse" for="c-42849471">[-]</label><label class="expand" for="c-42849471">[6 more]</label></div><br/><div class="children"><div class="content">Greater efficiency of light bulbs has led to more light bulb use, not less. More efficient training of LLMs could just as likely lead to more chip use, not less.</div><br/><div id="42849481" class="c"><input type="checkbox" id="c-42849481" checked=""/><div class="controls bullet"><span class="by">gniv</span><span>|</span><a href="#42849471">parent</a><span>|</span><a href="#42848438">next</a><span>|</span><label class="collapse" for="c-42849481">[-]</label><label class="expand" for="c-42849481">[5 more]</label></div><br/><div class="children"><div class="content">But did it not lead to net less electricity used for lighting?</div><br/><div id="42849596" class="c"><input type="checkbox" id="c-42849596" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#42849471">root</a><span>|</span><a href="#42849481">parent</a><span>|</span><a href="#42849569">next</a><span>|</span><label class="collapse" for="c-42849596">[-]</label><label class="expand" for="c-42849596">[1 more]</label></div><br/><div class="children"><div class="content">Maybe, at some point.<p>From what I can tell there&#x27;s are mostly two options: Either AI is and will be useless or it&#x27;s severely undersupplied. People, even those deeply technical, where AI has the most impact right now, still widely argue about if AI even offers <i>any</i> value. Adoption is far from anything that is plausible, if (not when) it became clear that it does.<p>If you land on &quot;does not&quot;, given the investments so far, commercial entities would obviously be overvalued already and any investment goes to 0 over time.<p>But if we land on &quot;does&quot;, how could Nvidia not be anything other than undervalued right now? No matter what frontier model: I can look at my screen, LLM generated characters visibly appearing in chunks, depending on the model after initially waiting for 10-20 seconds, for even benign queries — because that is the best we can do right now. And that&#x27;s while most people still argue if AI will actually do anything and humanity at large does not really use it, neither personally nor societally.<p>If AI does in fact do something valuable and that something gets better, everyone will want it and there will be demand for lots of chips.</div><br/></div></div><div id="42849569" class="c"><input type="checkbox" id="c-42849569" checked=""/><div class="controls bullet"><span class="by">divbzero</span><span>|</span><a href="#42849471">root</a><span>|</span><a href="#42849481">parent</a><span>|</span><a href="#42849596">prev</a><span>|</span><a href="#42849507">next</a><span>|</span><label class="collapse" for="c-42849569">[-]</label><label class="expand" for="c-42849569">[2 more]</label></div><br/><div class="children"><div class="content">With more efficiency offset by more light bulbs, electricity used for lighting has been roughly flat since 2010: <a href="https:&#x2F;&#x2F;www.iea.org&#x2F;data-and-statistics&#x2F;charts&#x2F;global-electricity-consumption-in-lighting-in-the-net-zero-scenario-2010-2030" rel="nofollow">https:&#x2F;&#x2F;www.iea.org&#x2F;data-and-statistics&#x2F;charts&#x2F;global-electr...</a><p>(For LLMs I wish that efficiency could lead to less electricity used for chips, but I think the best we can hope for is for electricity use to flatten out.)</div><br/><div id="42849967" class="c"><input type="checkbox" id="c-42849967" checked=""/><div class="controls bullet"><span class="by">jes5199</span><span>|</span><a href="#42849471">root</a><span>|</span><a href="#42849569">parent</a><span>|</span><a href="#42849507">next</a><span>|</span><label class="collapse" for="c-42849967">[-]</label><label class="expand" for="c-42849967">[1 more]</label></div><br/><div class="children"><div class="content">we just have to get solar deployed sufficiently so that we no longer think of energy use as a waste of something precious</div><br/></div></div></div></div><div id="42849507" class="c"><input type="checkbox" id="c-42849507" checked=""/><div class="controls bullet"><span class="by">BobbyJo</span><span>|</span><a href="#42849471">root</a><span>|</span><a href="#42849481">parent</a><span>|</span><a href="#42849569">prev</a><span>|</span><a href="#42848438">next</a><span>|</span><label class="collapse" for="c-42849507">[-]</label><label class="expand" for="c-42849507">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d wager there is more light today than before LEDs were invented. I&#x27;d wager a lot in fact.</div><br/></div></div></div></div></div></div><div id="42848438" class="c"><input type="checkbox" id="c-42848438" checked=""/><div class="controls bullet"><span class="by">yalogin</span><span>|</span><a href="#42849471">prev</a><span>|</span><a href="#42846209">next</a><span>|</span><label class="collapse" for="c-42848438">[-]</label><label class="expand" for="c-42848438">[24 more]</label></div><br/><div class="children"><div class="content">Nvidia has gotten lucky repeatedly. The GPUs were great for PC gaming and they were the top dog. The crypto boom was such an unexpected win for them partly because Intel killed off their competition by acquiring it. Then the AI boom is also a direct result of Intel killing off their competition but the acquisition is too far removed to credit it to that event.<p>Unlike the crypto boom though, two factors make me think the AI thing was bound to go away quickly.<p>Unlike crypto there is no mathematical lower bound for computation, and if you see technology&#x27;s history we can tell the models are going to get better&#x2F;smaller&#x2F;faster overtime reducing our reliance on the GPU.<p>Crypto was fringe but AI is fundamental to every software stack and every company. There is way too much money in this to just let Nvidia take it all. One way or another the reliance on it will be reduced</div><br/><div id="42848595" class="c"><input type="checkbox" id="c-42848595" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#42848438">parent</a><span>|</span><a href="#42848540">next</a><span>|</span><label class="collapse" for="c-42848595">[-]</label><label class="expand" for="c-42848595">[18 more]</label></div><br/><div class="children"><div class="content">&gt; the models are going to get better&#x2F;smaller&#x2F;faster overtime reducing our reliance on the GPU<p>Yes, because we&#x27;ve seen that with other software.  I no longer want a GPU for my computer because I play games from the 90s and the CPU has grown powerful enough to suffice... except that&#x27;s not the case at all.  Software grew in complexity and quality with available compute resources and we have no reason to think &quot;AI&quot; will be any different.<p>Are you satisfied with today&#x27;s models and their inaccuracies and hallucinations?    Why do you think we will solve those problems without more HW?</div><br/><div id="42848662" class="c"><input type="checkbox" id="c-42848662" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848595">parent</a><span>|</span><a href="#42849792">next</a><span>|</span><label class="collapse" for="c-42848662">[-]</label><label class="expand" for="c-42848662">[11 more]</label></div><br/><div class="children"><div class="content">because that&#x27;s what history shows us. back in the 90s, MPEG-1&#x2F;2 took dedicated hardware expansion cards to handle the encoding because software was just too damn slow. eventually, CPUs caught up, and dedicated instructions were added to the CPU to make software encoding multiple times faster than real-time. Then, H.264 came along and CPUs were slow for encoding again. Special instructions were added to the CPU again, and software encoding is multiple times faster again. We&#x27;re now at H.265 and 8K video where encoding is slow on CPU. Can you guess what the next step will be?<p>Not all software is written badly where it becomes bloatware. Some people still squeeze everything they can, admittedly, the numbers are becoming smaller. Just like the quote, &quot;why would I spend money to optimize Windows when hardware keeps improving&quot; does seem to be group think now. If only more people gave a shit about their code vs meeting some bonus accomplishment</div><br/><div id="42849378" class="c"><input type="checkbox" id="c-42849378" checked=""/><div class="controls bullet"><span class="by">jjk166</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848662">parent</a><span>|</span><a href="#42849280">next</a><span>|</span><label class="collapse" for="c-42849378">[-]</label><label class="expand" for="c-42849378">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Can you guess what the next step will be?<p>H.266 32K encoding being slow on cpu</div><br/></div></div><div id="42849280" class="c"><input type="checkbox" id="c-42849280" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848662">parent</a><span>|</span><a href="#42849378">prev</a><span>|</span><a href="#42849792">next</a><span>|</span><label class="collapse" for="c-42849280">[-]</label><label class="expand" for="c-42849280">[9 more]</label></div><br/><div class="children"><div class="content">&gt; Can you guess what the next step will be?<p>He fixes the cable?<p>But seriously, video encoding isn&#x27;t AI.  Video encoding is a well understood problem.  We can&#x27;t even make &quot;AI&quot; that doesn&#x27;t hallucinate yet.  We&#x27;re not sure what architectures will be needed for progress in AI.  I get that we&#x27;re all drunk on our analogies in the vacuum of our ignorance but we need to have a bit of humility and awareness of where we&#x27;re at.</div><br/><div id="42849396" class="c"><input type="checkbox" id="c-42849396" checked=""/><div class="controls bullet"><span class="by">asadotzler</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849280">parent</a><span>|</span><a href="#42849349">next</a><span>|</span><label class="collapse" for="c-42849396">[-]</label><label class="expand" for="c-42849396">[2 more]</label></div><br/><div class="children"><div class="content">Including considering that it can&#x27;t be made much better, that the hallucinations are a fundamental trait that cannot be eliminated, that this will all come tumbling down in a year or three. You seem to want to consider every possible positive future if we just work harder or longer at it, while ignoring the most likely outcomes that are nearer term and far from positive.</div><br/><div id="42849627" class="c"><input type="checkbox" id="c-42849627" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849396">parent</a><span>|</span><a href="#42849349">next</a><span>|</span><label class="collapse" for="c-42849627">[-]</label><label class="expand" for="c-42849627">[1 more]</label></div><br/><div class="children"><div class="content">There are already strategies to reduce hallucinations but, guess what?  I&#x27;ll let you fill in the rest.</div><br/></div></div></div></div><div id="42849349" class="c"><input type="checkbox" id="c-42849349" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849280">parent</a><span>|</span><a href="#42849396">prev</a><span>|</span><a href="#42849792">next</a><span>|</span><label class="collapse" for="c-42849349">[-]</label><label class="expand" for="c-42849349">[6 more]</label></div><br/><div class="children"><div class="content">Conversely, can you name one computing thing that used to be hard when it was first created that is still hard in the same way today after generations of software&#x2F;hardware improvements?</div><br/><div id="42849635" class="c"><input type="checkbox" id="c-42849635" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849349">parent</a><span>|</span><a href="#42850212">next</a><span>|</span><label class="collapse" for="c-42849635">[-]</label><label class="expand" for="c-42849635">[1 more]</label></div><br/><div class="children"><div class="content">Simulations and pretty much any large scale modelling task.  Why do you think people build supercomputers?<p>Now that I mentioned it, I think supercomputers and the jobs they run are the perfect analog for AI at this stage.  It&#x27;s a problem that we could throw nearly limitless compute at if it were cost effective to do so.  HPC encompasses a class of problems for which we have to make compromises because we can&#x27;t begin to compute the ideal(sort of like using reduced precision in deep-learning).  HPC scale problems have always been hard and as we add capabilities we will likely just soak them up to perform more accurate or larger computational tasks.<p>To quote Andrej Karpathy
(<a href="https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1883941452738355376" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1883941452738355376</a>): &quot;I will say that Deep Learning has a legendary ravenous appetite for compute, like no other algorithm that has ever been developed in AI. You may not always be utilizing it fully but I would never bet against compute as the upper bound for achievable intelligence in the long run. Not just for an individual final training run, but also for the entire innovation &#x2F; experimentation engine that silently underlies all the algorithmic innovations.&quot;</div><br/></div></div><div id="42850212" class="c"><input type="checkbox" id="c-42850212" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849349">parent</a><span>|</span><a href="#42849635">prev</a><span>|</span><a href="#42849419">next</a><span>|</span><label class="collapse" for="c-42850212">[-]</label><label class="expand" for="c-42850212">[1 more]</label></div><br/><div class="children"><div class="content">3D graphics.</div><br/></div></div><div id="42849419" class="c"><input type="checkbox" id="c-42849419" checked=""/><div class="controls bullet"><span class="by">asadotzler</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849349">parent</a><span>|</span><a href="#42850212">prev</a><span>|</span><a href="#42849601">next</a><span>|</span><label class="collapse" for="c-42849419">[-]</label><label class="expand" for="c-42849419">[2 more]</label></div><br/><div class="children"><div class="content">VR is as dead this time around as it was in the mid-2000s and the mid-1990s and the mid-1980s, each of the times I&#x27;ve used it it was just as awful as before with nausea, eyestrain, headaches, neck and face fatigue, it&#x27;s truly a f**ed space and it&#x27;s failed over and over, this time with Apple and Facebook spending tens of billions on it. VR is a perfect reply to your question here.</div><br/><div id="42849436" class="c"><input type="checkbox" id="c-42849436" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849419">parent</a><span>|</span><a href="#42849601">next</a><span>|</span><label class="collapse" for="c-42849436">[-]</label><label class="expand" for="c-42849436">[1 more]</label></div><br/><div class="children"><div class="content">um, not really. just because a tech does not gain popularity and withers on the vine has nothing to do with code and hardware maturity<p>you&#x27;re rant on VR is just weird and out of place here</div><br/></div></div></div></div><div id="42849601" class="c"><input type="checkbox" id="c-42849601" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849349">parent</a><span>|</span><a href="#42849419">prev</a><span>|</span><a href="#42849792">next</a><span>|</span><label class="collapse" for="c-42849601">[-]</label><label class="expand" for="c-42849601">[1 more]</label></div><br/><div class="children"><div class="content">The Entscheidungsproblem, from the 17th century to today and forever in the future.</div><br/></div></div></div></div></div></div></div></div><div id="42849792" class="c"><input type="checkbox" id="c-42849792" checked=""/><div class="controls bullet"><span class="by">amoss</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848595">parent</a><span>|</span><a href="#42848662">prev</a><span>|</span><a href="#42848653">next</a><span>|</span><label class="collapse" for="c-42849792">[-]</label><label class="expand" for="c-42849792">[1 more]</label></div><br/><div class="children"><div class="content">interesting take.<p>the aaa games industry is struggling (e.g. look at the profit warnings, share price drops and studio closures) specifically because people are doing that en masse.<p>but those 90s games are not old - retro has become a movement within gaming and there is a whole cottage industry of &quot;indie&quot; games building that aesthetic because it is cheap and fun.</div><br/></div></div><div id="42848653" class="c"><input type="checkbox" id="c-42848653" checked=""/><div class="controls bullet"><span class="by">dgemm</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848595">parent</a><span>|</span><a href="#42849792">prev</a><span>|</span><a href="#42848661">next</a><span>|</span><label class="collapse" for="c-42848653">[-]</label><label class="expand" for="c-42848653">[1 more]</label></div><br/><div class="children"><div class="content">I think the point was not that we won&#x27;t still use a lot of hardware, it&#x27;s that it won&#x27;t necessarily always be Nvidia.  Nvidia got lucky when both crypto and AI arrived because it had the best available ready-made thing to do the job, but it&#x27;s not like it&#x27;s the best possible thing.  Crypto eventually got its ASICs that made GPUs uncompetitive after all.</div><br/></div></div><div id="42848661" class="c"><input type="checkbox" id="c-42848661" checked=""/><div class="controls bullet"><span class="by">atrus</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848595">parent</a><span>|</span><a href="#42848653">prev</a><span>|</span><a href="#42848540">next</a><span>|</span><label class="collapse" for="c-42848661">[-]</label><label class="expand" for="c-42848661">[4 more]</label></div><br/><div class="children"><div class="content">Honestly, you&#x27;d be shocked at how much gaming you can get done on the integrated gpus that are just shoved in these days. Sure, you won&#x27;t be playing the most graphically demanding things, but think of platforms like the Switch, or games like Stardew. You can easily go without a dedicated GPU and still have a plethora of games.<p>And as for AI, there&#x27;s probably so much room for improvement on the software side that it will probably be the case that the smarter, more performant AIs will not necessarily have to be on the top of the line hardware.</div><br/><div id="42849260" class="c"><input type="checkbox" id="c-42849260" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848661">parent</a><span>|</span><a href="#42848540">next</a><span>|</span><label class="collapse" for="c-42849260">[-]</label><label class="expand" for="c-42849260">[3 more]</label></div><br/><div class="children"><div class="content">Just look at how much insects get done with just a few neurons to run together...</div><br/><div id="42849384" class="c"><input type="checkbox" id="c-42849384" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849260">parent</a><span>|</span><a href="#42849283">next</a><span>|</span><label class="collapse" for="c-42849384">[-]</label><label class="expand" for="c-42849384">[1 more]</label></div><br/><div class="children"><div class="content">Those neurons aren&#x27;t language models though. They&#x27;re not encoding petabytes of human knowledge.</div><br/></div></div><div id="42849283" class="c"><input type="checkbox" id="c-42849283" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42849260">parent</a><span>|</span><a href="#42849384">prev</a><span>|</span><a href="#42848540">next</a><span>|</span><label class="collapse" for="c-42849283">[-]</label><label class="expand" for="c-42849283">[1 more]</label></div><br/><div class="children"><div class="content">Those neurons aren&#x27;t anything remotely similar to a &quot;neuron&quot; in an LLM, for instance.</div><br/></div></div></div></div></div></div></div></div><div id="42848540" class="c"><input type="checkbox" id="c-42848540" checked=""/><div class="controls bullet"><span class="by">gradys</span><span>|</span><a href="#42848438">parent</a><span>|</span><a href="#42848595">prev</a><span>|</span><a href="#42848672">next</a><span>|</span><label class="collapse" for="c-42848540">[-]</label><label class="expand" for="c-42848540">[1 more]</label></div><br/><div class="children"><div class="content">Is it luck, or is scaling arithmetic genuinely a useful capability to offer the world?</div><br/></div></div><div id="42848672" class="c"><input type="checkbox" id="c-42848672" checked=""/><div class="controls bullet"><span class="by">tw1984</span><span>|</span><a href="#42848438">parent</a><span>|</span><a href="#42848540">prev</a><span>|</span><a href="#42848565">next</a><span>|</span><label class="collapse" for="c-42848672">[-]</label><label class="expand" for="c-42848672">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Unlike crypto there is no mathematical lower bound for computation<p>this is what I feel, but is there any scientific proof on that?</div><br/><div id="42848692" class="c"><input type="checkbox" id="c-42848692" checked=""/><div class="controls bullet"><span class="by">brickfaced</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848672">parent</a><span>|</span><a href="#42848565">next</a><span>|</span><label class="collapse" for="c-42848692">[-]</label><label class="expand" for="c-42848692">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the human brain</div><br/></div></div></div></div><div id="42848565" class="c"><input type="checkbox" id="c-42848565" checked=""/><div class="controls bullet"><span class="by">nailer</span><span>|</span><a href="#42848438">parent</a><span>|</span><a href="#42848672">prev</a><span>|</span><a href="#42846209">next</a><span>|</span><label class="collapse" for="c-42848565">[-]</label><label class="expand" for="c-42848565">[2 more]</label></div><br/><div class="children"><div class="content">Money isn’t fringe, and the target for crypto is all transactions, rather than the existing model where you pay between two and 3.5% to a card company or other middleman.</div><br/><div id="42848676" class="c"><input type="checkbox" id="c-42848676" checked=""/><div class="controls bullet"><span class="by">daveguy</span><span>|</span><a href="#42848438">root</a><span>|</span><a href="#42848565">parent</a><span>|</span><a href="#42846209">next</a><span>|</span><label class="collapse" for="c-42848676">[-]</label><label class="expand" for="c-42848676">[1 more]</label></div><br/><div class="children"><div class="content">Credit card companies averaged over 22,000 transactions per second in 2023 without ever having to raise the fee. How many is crypto even capable of processing? Processing without the fee going up? What fraud protection guarantees are offered to the parties of crypto transactions?</div><br/></div></div></div></div></div></div><div id="42846209" class="c"><input type="checkbox" id="c-42846209" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42848438">prev</a><span>|</span><a href="#42849080">next</a><span>|</span><label class="collapse" for="c-42846209">[-]</label><label class="expand" for="c-42846209">[109 more]</label></div><br/><div class="children"><div class="content">IMO this is less about DeepSeek and more that Nvidia is essentially a bubble&#x2F;meme stock that is divorced from the reality of finance and business. People&#x2F;institutions who bought on nothing but hype are now panic selling. DeepSeek provided the spark, but that&#x27;s all that was needed, just like how a vague rumor is enough to cause bank runs.</div><br/><div id="42846496" class="c"><input type="checkbox" id="c-42846496" checked=""/><div class="controls bullet"><span class="by">KiwiJohnno</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846268">next</a><span>|</span><label class="collapse" for="c-42846496">[-]</label><label class="expand" for="c-42846496">[40 more]</label></div><br/><div class="children"><div class="content">Not quite, I believe this sell off was caused by DeepSeek showing with their new model that the hardware demands of AI are not necessarily as high as everyone has assumed (as required by competing models).<p>I&#x27;ve tried their 7b model, running locally on a 6gb laptop GPU.  Its not fast, but the results I&#x27;ve had have rivaled GPT4.  Its impressive.</div><br/><div id="42846548" class="c"><input type="checkbox" id="c-42846548" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42847332">next</a><span>|</span><label class="collapse" for="c-42846548">[-]</label><label class="expand" for="c-42846548">[25 more]</label></div><br/><div class="children"><div class="content">I believe you that it had to do with the selloff, but I believe that efficiency improvements are good news for NVIDIA: each card just got 20x more useful</div><br/><div id="42846674" class="c"><input type="checkbox" id="c-42846674" checked=""/><div class="controls bullet"><span class="by">segasaturn</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846548">parent</a><span>|</span><a href="#42847277">next</a><span>|</span><label class="collapse" for="c-42846674">[-]</label><label class="expand" for="c-42846674">[12 more]</label></div><br/><div class="children"><div class="content">That still means that that AI firms don&#x27;t have to buy as many of Nvidia&#x27;s chips, which is the whole thing that Nvidia&#x27;s price was predicated on. FB, Google and Microsoft just had their their billions of dollars in Nvidia GPU capex blown out by $5M side-project. Tech firms are probably not going to be as generous shelling out whatever overinflated price Nvidia was asking for as they were a week ago.</div><br/><div id="42849891" class="c"><input type="checkbox" id="c-42849891" checked=""/><div class="controls bullet"><span class="by">Jlagreen</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846674">parent</a><span>|</span><a href="#42847037">next</a><span>|</span><label class="collapse" for="c-42849891">[-]</label><label class="expand" for="c-42849891">[1 more]</label></div><br/><div class="children"><div class="content">The $5M was the cost of the training itself.<p>You can rent 10k H100 for 20 days with that money. Go and knock yourself out because that compute is probably higher than what DeepSeek received for that money.  And that is public cloud pricing for single H100. I&#x27;m sure if you ask for 10k H100 you&#x27;ll get them at half price so easily 40 days of training.<p>DeepSeek has fooled everyone by telling them that they need only so less money and people think that they only need to &quot;buy&quot; $5M worth of GPU but that&#x27;s wrong. The money is the training costs of renting the GPU training hours.<p>Somebody had to install the 10k GPUs and that&#x27;s paying $300M to Nvidia.</div><br/></div></div><div id="42847037" class="c"><input type="checkbox" id="c-42847037" checked=""/><div class="controls bullet"><span class="by">epicureanideal</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846674">parent</a><span>|</span><a href="#42849891">prev</a><span>|</span><a href="#42846851">next</a><span>|</span><label class="collapse" for="c-42847037">[-]</label><label class="expand" for="c-42847037">[2 more]</label></div><br/><div class="children"><div class="content">Although there’s the Jevon’s Paradox possibility that more efficient AI will drive even more demand for AI chips because more uses will be found for them.  But possibly not super high end NVDA chips but instead little Apple iPhone AI cores or smartwatch AI cores, etc.<p>Although not all commodities will work like fossil fuels did in Jevon’s Paradox.  It could be the case that demand for AI doesn’t grow fast enough to keep demand for chips as high as it was, as efficiency improves.</div><br/><div id="42849644" class="c"><input type="checkbox" id="c-42849644" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847037">parent</a><span>|</span><a href="#42846851">next</a><span>|</span><label class="collapse" for="c-42849644">[-]</label><label class="expand" for="c-42849644">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  But possibly not super high end NVDA chips but instead little Apple iPhone AI cores or smartwatch AI cores, etc.<p>We tried that, though. NPUs are in all sorts of hardware, and it is entirely wasted silicon for most users, most of the time. They don&#x27;t do LLM inference, they don&#x27;t generate images, and they don&#x27;t train models. Too weak to work, too specialized to be useful.<p>Nvidia &quot;wins&quot; by comparison because they don&#x27;t specialize their hardware. The GPU is the NPU, and it&#x27;s power scales with the size of GPU you own. The capability of a 0.75w NPU is rendered useless by the scale, capability and efficiency of a cluster of 600w dGPU clusters.</div><br/></div></div></div></div><div id="42846851" class="c"><input type="checkbox" id="c-42846851" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846674">parent</a><span>|</span><a href="#42847037">prev</a><span>|</span><a href="#42846832">next</a><span>|</span><label class="collapse" for="c-42846851">[-]</label><label class="expand" for="c-42846851">[2 more]</label></div><br/><div class="children"><div class="content">Wrong conclusion, IMO. This makes inference more cost effective which means self-hosting suddenly becomes more attractive to a wider share of the market.<p>GPUs will continue to be bought up as fast as fabs can spit them out.</div><br/><div id="42846916" class="c"><input type="checkbox" id="c-42846916" checked=""/><div class="controls bullet"><span class="by">segasaturn</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846851">parent</a><span>|</span><a href="#42846832">next</a><span>|</span><label class="collapse" for="c-42846916">[-]</label><label class="expand" for="c-42846916">[1 more]</label></div><br/><div class="children"><div class="content">The number of people interested in doing self-hosting for AI at the moment is a tiny, tiny percentage of enthusiast computer users, who indeed get to play with self-hosted LLMs on consumer hardware now.. but the promise of these AI companies is that LLMs will be the &quot;next internet&quot;, or even the &quot;next electricity&quot; according to Sam Altman, all of which will run exclusively on Nvidia chips running in mega-datacenters, the promise of which was priced into Nvidia&#x27;s share price as of last Friday. That appears on shaky ground now.</div><br/></div></div></div></div><div id="42846832" class="c"><input type="checkbox" id="c-42846832" checked=""/><div class="controls bullet"><span class="by">flowerlad</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846674">parent</a><span>|</span><a href="#42846851">prev</a><span>|</span><a href="#42846903">next</a><span>|</span><label class="collapse" for="c-42846832">[-]</label><label class="expand" for="c-42846832">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>That still means that that AI firms don&#x27;t have to buy as many of Nvidia&#x27;s chips</i><p>Couldn’t you say that about Blackwell as well? Blackwell is 25x more energy-efficient for generative AI tasks and offer up to 2.5x faster AI training performance overall.</div><br/><div id="42849921" class="c"><input type="checkbox" id="c-42849921" checked=""/><div class="controls bullet"><span class="by">Jlagreen</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846832">parent</a><span>|</span><a href="#42846903">next</a><span>|</span><label class="collapse" for="c-42849921">[-]</label><label class="expand" for="c-42849921">[1 more]</label></div><br/><div class="children"><div class="content">And yet, Blackwell is sold out.<p>What does that tell us?<p>The industry is compute starved and that makes totally sense.<p>The tranformer model on which current LLMs are based on are 8 years old. But why took it so much time to get to the LLMs only 2 years ago?<p>Simple, Nvidia first had to push the compute at scale strongly. Try training GPT4 on Voltas from 2017. Good luck with that!<p>Current LLMs are possible thanks to the compute Nvidia has provided in the past decade. You could technically use 20 year old CPUs for LLMs but you might need to connect a billion of them.</div><br/></div></div></div></div><div id="42846903" class="c"><input type="checkbox" id="c-42846903" checked=""/><div class="controls bullet"><span class="by">uxhacker</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846674">parent</a><span>|</span><a href="#42846832">prev</a><span>|</span><a href="#42846715">next</a><span>|</span><label class="collapse" for="c-42846903">[-]</label><label class="expand" for="c-42846903">[3 more]</label></div><br/><div class="children"><div class="content">It means personal ai on every computer. No privacy concerns, but saying that it is quite weird coming from a Chinese start up :)</div><br/><div id="42849084" class="c"><input type="checkbox" id="c-42849084" checked=""/><div class="controls bullet"><span class="by">windsignaling</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846903">parent</a><span>|</span><a href="#42848611">next</a><span>|</span><label class="collapse" for="c-42849084">[-]</label><label class="expand" for="c-42849084">[1 more]</label></div><br/><div class="children"><div class="content">Always hilarious to see westerners concerned about privacy when it comes to China, yet not concerned at all about their own governments that know far more about you. Do they think some Chinese policeman is going to come to their door? Never heard of Snowden or the five eyes?</div><br/></div></div><div id="42848611" class="c"><input type="checkbox" id="c-42848611" checked=""/><div class="controls bullet"><span class="by">grogenaut</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846903">parent</a><span>|</span><a href="#42849084">prev</a><span>|</span><a href="#42846715">next</a><span>|</span><label class="collapse" for="c-42848611">[-]</label><label class="expand" for="c-42848611">[1 more]</label></div><br/><div class="children"><div class="content">It won&#x27;t last long. Agents are where AI is going to go imho. That means giving the ai software access to the internet, and that means telemetry.</div><br/></div></div></div></div><div id="42846715" class="c"><input type="checkbox" id="c-42846715" checked=""/><div class="controls bullet"><span class="by">chgs</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846674">parent</a><span>|</span><a href="#42846903">prev</a><span>|</span><a href="#42847277">next</a><span>|</span><label class="collapse" for="c-42846715">[-]</label><label class="expand" for="c-42846715">[1 more]</label></div><br/><div class="children"><div class="content">Imagine what you can do with all that Nvidia hardware using the deep mind techniques.</div><br/></div></div></div></div><div id="42847277" class="c"><input type="checkbox" id="c-42847277" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846548">parent</a><span>|</span><a href="#42846674">prev</a><span>|</span><a href="#42847286">next</a><span>|</span><label class="collapse" for="c-42847277">[-]</label><label class="expand" for="c-42847277">[1 more]</label></div><br/><div class="children"><div class="content">They only got more useful if the AI goldrush participants actually strike, well, gold. Otherwise it&#x27;s not useful at all. Afaict it remains to be seen whether any of this AI stuff has actual commercial value. It&#x27;s all just speculation predicated on thoughts and prayers.</div><br/></div></div><div id="42847286" class="c"><input type="checkbox" id="c-42847286" checked=""/><div class="controls bullet"><span class="by">davidcbc</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846548">parent</a><span>|</span><a href="#42847277">prev</a><span>|</span><a href="#42846564">next</a><span>|</span><label class="collapse" for="c-42847286">[-]</label><label class="expand" for="c-42847286">[2 more]</label></div><br/><div class="children"><div class="content">When your business is selling a large number of cards to giant companies you don&#x27;t <i>want</i> them to be 20x more useful because then people will buy fewer of them to do the same amount of work</div><br/><div id="42847359" class="c"><input type="checkbox" id="c-42847359" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847286">parent</a><span>|</span><a href="#42846564">next</a><span>|</span><label class="collapse" for="c-42847359">[-]</label><label class="expand" for="c-42847359">[1 more]</label></div><br/><div class="children"><div class="content">or people do 30x more work and buy 50% more cards</div><br/></div></div></div></div><div id="42846564" class="c"><input type="checkbox" id="c-42846564" checked=""/><div class="controls bullet"><span class="by">amazingamazing</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846548">parent</a><span>|</span><a href="#42847286">prev</a><span>|</span><a href="#42847332">next</a><span>|</span><label class="collapse" for="c-42846564">[-]</label><label class="expand" for="c-42846564">[9 more]</label></div><br/><div class="children"><div class="content">each card is not 20x more useful lol. there&#x27;s no evidence yet that the deepseek architecture would even yield a substantially (20x) more performant model with more compute.<p>if there&#x27;s evidence to the contrary I&#x27;d love to see. in any case I don&#x27;t think a h800 is even 20x better than a h100 anyway, so the 20x increase has to be wrong.</div><br/><div id="42846636" class="c"><input type="checkbox" id="c-42846636" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846564">parent</a><span>|</span><a href="#42846756">next</a><span>|</span><label class="collapse" for="c-42846636">[-]</label><label class="expand" for="c-42846636">[6 more]</label></div><br/><div class="children"><div class="content">We need GPUs for inference, not just training. The Jevons Paradox suggests that reducing the cost per token will increase the overall demand for inference.<p>Also, everything we know about LLMs points to an entirely predictable correlation between training compute and performance.</div><br/><div id="42847083" class="c"><input type="checkbox" id="c-42847083" checked=""/><div class="controls bullet"><span class="by">tshaddox</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846636">parent</a><span>|</span><a href="#42846647">next</a><span>|</span><label class="collapse" for="c-42847083">[-]</label><label class="expand" for="c-42847083">[3 more]</label></div><br/><div class="children"><div class="content">Jevons paradox doesn&#x27;t really suggest anything by itself. Jevons paradox is something that occurs in some instances of increased efficiency, but not all. I suppose the important question here is &quot;What is the price elasticity of demand of inference?&quot;</div><br/><div id="42849220" class="c"><input type="checkbox" id="c-42849220" checked=""/><div class="controls bullet"><span class="by">ckw</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847083">parent</a><span>|</span><a href="#42848312">next</a><span>|</span><label class="collapse" for="c-42849220">[-]</label><label class="expand" for="c-42849220">[1 more]</label></div><br/><div class="children"><div class="content">Personally, in the six months prior to the release of the deepseekv3 api, I&#x27;d made probably 100-200 api calls per month to llm services. In the past week I made 2.8 million api calls to dsv3.</div><br/></div></div><div id="42848312" class="c"><input type="checkbox" id="c-42848312" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847083">parent</a><span>|</span><a href="#42849220">prev</a><span>|</span><a href="#42846647">next</a><span>|</span><label class="collapse" for="c-42848312">[-]</label><label class="expand" for="c-42848312">[1 more]</label></div><br/><div class="children"><div class="content">People act like Jevons Paradox is an universal law thanks to Satya&#x27;s tweet.</div><br/></div></div></div></div><div id="42846647" class="c"><input type="checkbox" id="c-42846647" checked=""/><div class="controls bullet"><span class="by">amazingamazing</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846636">parent</a><span>|</span><a href="#42847083">prev</a><span>|</span><a href="#42846756">next</a><span>|</span><label class="collapse" for="c-42846647">[-]</label><label class="expand" for="c-42846647">[2 more]</label></div><br/><div class="children"><div class="content">the jevons paradox isn&#x27;t about any particular product or company&#x27;s product, so is irrelevant here. the relevant resource here is <i>compute</i>, which is already a commodity. secondly, even if it were about GPUs in particular, there&#x27;s no evidence that nvidia would be able to sustain such high margins if fewer were necessary for equivalent performance. things are currently supply constrained, which gives nvidia price optionality.</div><br/><div id="42846666" class="c"><input type="checkbox" id="c-42846666" checked=""/><div class="controls bullet"><span class="by">Scoundreller</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846647">parent</a><span>|</span><a href="#42846756">next</a><span>|</span><label class="collapse" for="c-42846666">[-]</label><label class="expand" for="c-42846666">[1 more]</label></div><br/><div class="children"><div class="content">Uhhh, isn’t it about coal?</div><br/></div></div></div></div></div></div><div id="42846756" class="c"><input type="checkbox" id="c-42846756" checked=""/><div class="controls bullet"><span class="by">numba888</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846564">parent</a><span>|</span><a href="#42846636">prev</a><span>|</span><a href="#42847332">next</a><span>|</span><label class="collapse" for="c-42846756">[-]</label><label class="expand" for="c-42846756">[2 more]</label></div><br/><div class="children"><div class="content">&gt; there&#x27;s no evidence yet that the deepseek architecture would even yield a substantially more performant model with more compute.<p>It&#x27;s supposed to. There was an info that the longer length of &#x27;thinking&#x27; makes o3 model better than o1. I.e. at least at inference compute power still matters.</div><br/><div id="42846772" class="c"><input type="checkbox" id="c-42846772" checked=""/><div class="controls bullet"><span class="by">amazingamazing</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846756">parent</a><span>|</span><a href="#42847332">next</a><span>|</span><label class="collapse" for="c-42846772">[-]</label><label class="expand" for="c-42846772">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s supposed to. There was an info that the longer length of &#x27;thinking&#x27; makes o3 model better than o1. I.e. at least at inference compute power still matters.<p>compute matters, but performance doesn&#x27;t <i>scale</i> with compute from what I&#x27;ve heard about o3 vs o1.<p>you shouldn&#x27;t take my word for it - go on the leaderboards and look at the top models from now, and then the top models from 2023 and look at the compute involved for both. there&#x27;s obviously a huge increase, but it isn&#x27;t proportional</div><br/></div></div></div></div></div></div></div></div><div id="42847332" class="c"><input type="checkbox" id="c-42847332" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42846548">prev</a><span>|</span><a href="#42847261">next</a><span>|</span><label class="collapse" for="c-42847332">[-]</label><label class="expand" for="c-42847332">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a pretty terrible take.<p>People who can use the 585B model will use the best model they can have. What DeepSeek really did was start an AI &quot;space race&quot; to AGI with China, and this race is running on Nvidia GPUs.<p>Some hobbyists will run the smaller model, but if you could, why not use the bigger &amp; better one?<p>Model distillation has been a thing for over a decade, and LLM distillation has been widespread since 2023 [1].<p>There is nothing new in being able to leverage a bigger model to enrich smaller models. This is what people that don&#x27;t understand the AI space got out of it, but it&#x27;s clearly wrong.<p>OpenAI has smaller models too with o1 mini and o4 mini, and phi-1 has shown that distillation could make a model 10x smaller perform as well as a much bigger model. The issue with these models is that they can&#x27;t generalize as well. Bigger models will always win at first, then you can specialize them.<p>Deepseek also showed that Nvidia GPUs could be more memory-efficient, which catapults Nvidia even further ahead of upcoming processors like Groq or AMD.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.02301" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.02301</a></div><br/></div></div><div id="42847261" class="c"><input type="checkbox" id="c-42847261" checked=""/><div class="controls bullet"><span class="by">ipv6ipv4</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42847332">prev</a><span>|</span><a href="#42847204">next</a><span>|</span><label class="collapse" for="c-42847261">[-]</label><label class="expand" for="c-42847261">[2 more]</label></div><br/><div class="children"><div class="content">To me this rings a lot like “640KB ought to be enough for anybody”<p>Similarly, as fast as processors have gotten, people still complain their applications are slow. Because they do so much more.<p>Generally applicable ML is still in its infancy, and usage is exploding. All those newfound spare cycles will get soaked up fairly quickly.</div><br/><div id="42848537" class="c"><input type="checkbox" id="c-42848537" checked=""/><div class="controls bullet"><span class="by">nickthegreek</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847261">parent</a><span>|</span><a href="#42847204">next</a><span>|</span><label class="collapse" for="c-42848537">[-]</label><label class="expand" for="c-42848537">[1 more]</label></div><br/><div class="children"><div class="content">It’s made a NVIDIA Digits even more attractive to me now.</div><br/></div></div></div></div><div id="42847204" class="c"><input type="checkbox" id="c-42847204" checked=""/><div class="controls bullet"><span class="by">khazhoux</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42847261">prev</a><span>|</span><a href="#42846615">next</a><span>|</span><label class="collapse" for="c-42847204">[-]</label><label class="expand" for="c-42847204">[1 more]</label></div><br/><div class="children"><div class="content">Not quite, I believe this sell off was caused by Shockley showing with their &quot;transistor&quot; that the electricity demands of computers are not necessarily as high as everyone has assumed (as required by vacuum tubes).<p>Electricity demands will plummet when transistors take the place of vacuum tubes.</div><br/></div></div><div id="42846615" class="c"><input type="checkbox" id="c-42846615" checked=""/><div class="controls bullet"><span class="by">siwakotisaurav</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42847204">prev</a><span>|</span><a href="#42846946">next</a><span>|</span><label class="collapse" for="c-42846615">[-]</label><label class="expand" for="c-42846615">[3 more]</label></div><br/><div class="children"><div class="content">None of the models other than the 600b one are R1. They’re just prev gen models like llama or qwen trained on r1 output making them slightly better</div><br/><div id="42847229" class="c"><input type="checkbox" id="c-42847229" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846615">parent</a><span>|</span><a href="#42846748">next</a><span>|</span><label class="collapse" for="c-42847229">[-]</label><label class="expand" for="c-42847229">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Slightly&quot; is an understatement, though. Distillations of R1 are significantly better than the underlying models.</div><br/></div></div><div id="42846748" class="c"><input type="checkbox" id="c-42846748" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846615">parent</a><span>|</span><a href="#42847229">prev</a><span>|</span><a href="#42846946">next</a><span>|</span><label class="collapse" for="c-42846748">[-]</label><label class="expand" for="c-42846748">[1 more]</label></div><br/><div class="children"><div class="content">Yeah but the second comment you see believes they are, and belief is truth when it comes to stock market gambling.</div><br/></div></div></div></div><div id="42846946" class="c"><input type="checkbox" id="c-42846946" checked=""/><div class="controls bullet"><span class="by">donsupreme</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42846615">prev</a><span>|</span><a href="#42847485">next</a><span>|</span><label class="collapse" for="c-42846946">[-]</label><label class="expand" for="c-42846946">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve tried their 7b model<p>Anything other than their 671b model are just distilled models on top of Qwen and Llama using their 671b reasoning data output, right?</div><br/><div id="42847983" class="c"><input type="checkbox" id="c-42847983" checked=""/><div class="controls bullet"><span class="by">KiwiJohnno</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846946">parent</a><span>|</span><a href="#42847485">next</a><span>|</span><label class="collapse" for="c-42847983">[-]</label><label class="expand" for="c-42847983">[1 more]</label></div><br/><div class="children"><div class="content">Correct.  Its the best model I&#x27;ve been able to run locally, by a long shot</div><br/></div></div></div></div><div id="42847485" class="c"><input type="checkbox" id="c-42847485" checked=""/><div class="controls bullet"><span class="by">rsanek</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42846946">prev</a><span>|</span><a href="#42847210">next</a><span>|</span><label class="collapse" for="c-42847485">[-]</label><label class="expand" for="c-42847485">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve run their distilled 70B model and didn&#x27;t come away too impressed -- feels similar to the existing base model it was trained on, which also rivaled GPT4</div><br/></div></div><div id="42847139" class="c"><input type="checkbox" id="c-42847139" checked=""/><div class="controls bullet"><span class="by">subarctic</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42847210">prev</a><span>|</span><a href="#42847374">next</a><span>|</span><label class="collapse" for="c-42847139">[-]</label><label class="expand" for="c-42847139">[1 more]</label></div><br/><div class="children"><div class="content">If that&#x27;s the case, then I have high hopes that the increase in efficiency will result in more demand, not less.<p>If only I could figure out how to buy NV stock quickly before it rebounds</div><br/></div></div><div id="42847374" class="c"><input type="checkbox" id="c-42847374" checked=""/><div class="controls bullet"><span class="by">datavirtue</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42847139">prev</a><span>|</span><a href="#42847084">next</a><span>|</span><label class="collapse" for="c-42847374">[-]</label><label class="expand" for="c-42847374">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, and firing up reactors to train models just lost all its luster. Those standing before the Stargate will be bored with the whole thing by then end of the week.</div><br/></div></div><div id="42847084" class="c"><input type="checkbox" id="c-42847084" checked=""/><div class="controls bullet"><span class="by">2-3-7-43-1807</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846496">parent</a><span>|</span><a href="#42847374">prev</a><span>|</span><a href="#42846268">next</a><span>|</span><label class="collapse" for="c-42847084">[-]</label><label class="expand" for="c-42847084">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s a Milchmädchenrechnung. if it turns out that you can achieve status quo with 1% of the expected effort then that just mean you can achieve approximately 10 times the status quo (assuming O(exp)) with the established budget! and this race is a race to the sky (as opposed to the bottom) ... he who reaches AGI first takes the cake, buddy.</div><br/></div></div></div></div><div id="42846268" class="c"><input type="checkbox" id="c-42846268" checked=""/><div class="controls bullet"><span class="by">tgtweak</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846496">prev</a><span>|</span><a href="#42846283">next</a><span>|</span><label class="collapse" for="c-42846268">[-]</label><label class="expand" for="c-42846268">[14 more]</label></div><br/><div class="children"><div class="content">Hype buyers are also Hype sellers - anything Nvidia was last week is exactly what it is this week - DeepSeek doesn&#x27;t really have any impact on Nvidia sales - Some argument could be made that this can shift compute off of cloud and onto end user devices, but that really seems like a stretch given what I&#x27;ve seen running this locally.</div><br/><div id="42846382" class="c"><input type="checkbox" id="c-42846382" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846268">parent</a><span>|</span><a href="#42846360">next</a><span>|</span><label class="collapse" for="c-42846382">[-]</label><label class="expand" for="c-42846382">[3 more]</label></div><br/><div class="children"><div class="content">The full DeepSeek model is ~700B params or so - <i>way</i> too large for most end users to run locally.  What some folks are running locally is fine-tuned versions of Llama and Qwen, that are not going to be directly comparable in any way.</div><br/><div id="42847339" class="c"><input type="checkbox" id="c-42847339" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846382">parent</a><span>|</span><a href="#42846360">next</a><span>|</span><label class="collapse" for="c-42847339">[-]</label><label class="expand" for="c-42847339">[2 more]</label></div><br/><div class="children"><div class="content">Many people are missing this due to journalists completely missing the point when presenting these facts.<p>Distilled models are nothing new.</div><br/><div id="42849779" class="c"><input type="checkbox" id="c-42849779" checked=""/><div class="controls bullet"><span class="by">mtkd</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847339">parent</a><span>|</span><a href="#42846360">next</a><span>|</span><label class="collapse" for="c-42849779">[-]</label><label class="expand" for="c-42849779">[1 more]</label></div><br/><div class="children"><div class="content">Ollama calling the distilled models deepseek-r1:7b etc. doesn&#x27;t help</div><br/></div></div></div></div></div></div><div id="42846360" class="c"><input type="checkbox" id="c-42846360" checked=""/><div class="controls bullet"><span class="by">Analemma_</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846268">parent</a><span>|</span><a href="#42846382">prev</a><span>|</span><a href="#42846283">next</a><span>|</span><label class="collapse" for="c-42846360">[-]</label><label class="expand" for="c-42846360">[10 more]</label></div><br/><div class="children"><div class="content">I agree hype is a big portion of it, but if DeepSeek really has found a way to train models just as good as frontier ones for a hundredth of the hardware investment, that is a substantial material difference for Nvidia&#x27;s future earnings.</div><br/><div id="42846442" class="c"><input type="checkbox" id="c-42846442" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846360">parent</a><span>|</span><a href="#42846532">next</a><span>|</span><label class="collapse" for="c-42846442">[-]</label><label class="expand" for="c-42846442">[3 more]</label></div><br/><div class="children"><div class="content">&gt; if DeepSeek really has found a way to train models just as good as frontier ones for a hundredth of the hardware investment<p>Frontier models are heavily compute constrained - the leading AI model makers have got <i>way</i> more training data already than they could do anything with.  Any improvement in training compute-efficiency is great news for them, no matter where it comes from.  Especially since the DeepSeek folks have gone into great detail wrt. documenting their approach.</div><br/><div id="42846493" class="c"><input type="checkbox" id="c-42846493" checked=""/><div class="controls bullet"><span class="by">rpcope1</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846442">parent</a><span>|</span><a href="#42846532">next</a><span>|</span><label class="collapse" for="c-42846493">[-]</label><label class="expand" for="c-42846493">[2 more]</label></div><br/><div class="children"><div class="content">&gt; leading AI model makers have got way more training data already than they could do anything with.<p>Citation needed.</div><br/><div id="42847393" class="c"><input type="checkbox" id="c-42847393" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846493">parent</a><span>|</span><a href="#42846532">next</a><span>|</span><label class="collapse" for="c-42847393">[-]</label><label class="expand" for="c-42847393">[1 more]</label></div><br/><div class="children"><div class="content">If you include multimodal data then I think it&#x27;s pretty obvious that training is compute limited.<p>Also current SOTA models are good enough that you can generate endless training data by letting the model operate stuff like a C compiler, python interpreter, Sage computer algebra, etc.</div><br/></div></div></div></div></div></div><div id="42846532" class="c"><input type="checkbox" id="c-42846532" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846360">parent</a><span>|</span><a href="#42846442">prev</a><span>|</span><a href="#42847816">next</a><span>|</span><label class="collapse" for="c-42846532">[-]</label><label class="expand" for="c-42846532">[1 more]</label></div><br/><div class="children"><div class="content">Is it? Training is only done once, inference requires GPUs to scale, especially for a 685B model. And now, there’s an open source o1 equivalent model that companies can run locally, which means that there’s a much bigger market for underutilized on-prem GPUs.</div><br/></div></div><div id="42847816" class="c"><input type="checkbox" id="c-42847816" checked=""/><div class="controls bullet"><span class="by">DennisP</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846360">parent</a><span>|</span><a href="#42846532">prev</a><span>|</span><a href="#42847373">next</a><span>|</span><label class="collapse" for="c-42847816">[-]</label><label class="expand" for="c-42847816">[1 more]</label></div><br/><div class="children"><div class="content">Or Nvidia keeps its earnings and our best frontier models get a hundred times better.</div><br/></div></div><div id="42847373" class="c"><input type="checkbox" id="c-42847373" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846360">parent</a><span>|</span><a href="#42847816">prev</a><span>|</span><a href="#42846508">next</a><span>|</span><label class="collapse" for="c-42847373">[-]</label><label class="expand" for="c-42847373">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how that follows.<p>Making training more effective makes every unit of compute spent on training more valuable.  This should increase demand unless we&#x27;ve reached a point where better models are not valuable.<p>The openness of DeepSeek&#x27;s approach also means that there will be more smaller entities engaging in training rather than a few massive entities that have more ability to set the price they pay.<p>Plus reasoning models substantially increase inference costs, since for each token of output you may have hundreds of tokens of reasoning.<p>Arguments on the point can go both ways, but I think on the balance I would expect any improvements in efficiency increase demand.</div><br/><div id="42848748" class="c"><input type="checkbox" id="c-42848748" checked=""/><div class="controls bullet"><span class="by">diamond559</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847373">parent</a><span>|</span><a href="#42846508">next</a><span>|</span><label class="collapse" for="c-42848748">[-]</label><label class="expand" for="c-42848748">[1 more]</label></div><br/><div class="children"><div class="content">Unless we get actual AGI I don&#x27;t honestly care as a non coder.  The art is slop and predatory, the chatbots are stilted and pointless, anytime a company uses AI there is huge backlash and there are just no commercial products with any real demand.  Make it as cheap as dirt and I still don&#x27;t see what use it is besides for scammers I guess...</div><br/></div></div></div></div><div id="42846508" class="c"><input type="checkbox" id="c-42846508" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846360">parent</a><span>|</span><a href="#42847373">prev</a><span>|</span><a href="#42846984">next</a><span>|</span><label class="collapse" for="c-42846508">[-]</label><label class="expand" for="c-42846508">[1 more]</label></div><br/><div class="children"><div class="content">1. Nobody has replicated their DeepSeek&#x27;s results on their reported budget yet. Scale.ai&#x27;s Alexander Wang says they&#x27;re lying and that they have a huge, clandestine H100 cluster. HuggingFace is assembling an effort to publicly duplicate the paper&#x27;s claims.<p>2. Even if DeepSeek&#x27;s budget claims are true, they trained their model on the outputs of an expensive foundation model built from a massive capital outlay. To truly replicate these results from scratch, it might require an expensive model upstream.</div><br/></div></div><div id="42846984" class="c"><input type="checkbox" id="c-42846984" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846360">parent</a><span>|</span><a href="#42846508">prev</a><span>|</span><a href="#42846283">next</a><span>|</span><label class="collapse" for="c-42846984">[-]</label><label class="expand" for="c-42846984">[1 more]</label></div><br/><div class="children"><div class="content">Not really. The training methodology opens up whole new mechanisms that&#x27;ll make it much easier to train non-language models, which have been very much neglected. Think robot multi-modal models; visual &#x2F; video question answering; audio processing, etc.</div><br/></div></div></div></div></div></div><div id="42846283" class="c"><input type="checkbox" id="c-42846283" checked=""/><div class="controls bullet"><span class="by">dgemm</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846268">prev</a><span>|</span><a href="#42846342">next</a><span>|</span><label class="collapse" for="c-42846283">[-]</label><label class="expand" for="c-42846283">[4 more]</label></div><br/><div class="children"><div class="content">I think less of that and more of real risks - Nvidia legitimately has the earnings right now.  The question is how sustainable that is, when most of it is coming from 5 or so customers that are both motivated and capable of taking back those 90% margins for themselves</div><br/><div id="42846829" class="c"><input type="checkbox" id="c-42846829" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846283">parent</a><span>|</span><a href="#42846701">next</a><span>|</span><label class="collapse" for="c-42846829">[-]</label><label class="expand" for="c-42846829">[2 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t have anything close to the earnings to justify the price they have reached.<p>They are getting a lot of money, but their stock price is in a completely different universe. Not even that $500G deal people announced, if spent exclusively on their products could justify their current price. (Nah, notice that just the <i>change</i> on their valuation is already larger than that deal.)</div><br/><div id="42847518" class="c"><input type="checkbox" id="c-42847518" checked=""/><div class="controls bullet"><span class="by">rsanek</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846829">parent</a><span>|</span><a href="#42846701">next</a><span>|</span><label class="collapse" for="c-42847518">[-]</label><label class="expand" for="c-42847518">[1 more]</label></div><br/><div class="children"><div class="content">Their forward PE is fairly reasonable: Nvidia 27, Apple 31, Amazon 38, Microsoft 33.</div><br/></div></div></div></div><div id="42846701" class="c"><input type="checkbox" id="c-42846701" checked=""/><div class="controls bullet"><span class="by">jhickok</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846283">parent</a><span>|</span><a href="#42846829">prev</a><span>|</span><a href="#42846342">next</a><span>|</span><label class="collapse" for="c-42846701">[-]</label><label class="expand" for="c-42846701">[1 more]</label></div><br/><div class="children"><div class="content">Regarding their earnings at the moment, I know it doesn&#x27;t mean everything, but a ~50 P&#x2F;E is still fairly high, although not insane. I think Ciscos was over 200 during the dotcom bubble. I think your question about the 5 major customers is really interesting, and we will continue to see those companies peck at custom silicon until they can maybe bridge the gap from just running inference to training as well.</div><br/></div></div></div></div><div id="42846342" class="c"><input type="checkbox" id="c-42846342" checked=""/><div class="controls bullet"><span class="by">belevme</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846283">prev</a><span>|</span><a href="#42846435">next</a><span>|</span><label class="collapse" for="c-42846342">[-]</label><label class="expand" for="c-42846342">[25 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s fair to say NVDA is meme stock, having reported 35B revenue last quarter.</div><br/><div id="42846513" class="c"><input type="checkbox" id="c-42846513" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846342">parent</a><span>|</span><a href="#42846450">next</a><span>|</span><label class="collapse" for="c-42846513">[-]</label><label class="expand" for="c-42846513">[18 more]</label></div><br/><div class="children"><div class="content">Nvidia&#x27;s annual revenue in 2024 was $60B. In comparison, Apple made $391B. Microsoft made $245B. Amazon made $575B. Google made $278B. And Nvidia is worth more than all of them. You&#x27;d have to go <i>very</i> far down the list to find a company with a comparable ratio of revenue or income to market cap as Nvidia.</div><br/><div id="42846612" class="c"><input type="checkbox" id="c-42846612" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846513">parent</a><span>|</span><a href="#42846671">next</a><span>|</span><label class="collapse" for="c-42846612">[-]</label><label class="expand" for="c-42846612">[11 more]</label></div><br/><div class="children"><div class="content">Nvidia&#x27;s revenue growth rate was 94% and income growth rate was 109% for the Oct 2024 quarter. This compares to Apple&#x27;s 6% and -35%.<p>Nvidia is growing profits faster than income.<p>Nvidia&#x27;s net profit margin is 55% (vs Apple 15%) and they have an operating income of $21B vs Apple&#x27;s $29.5<p>These are some pretty impressive financial results - those growth rates are the reason people are bullish on it.</div><br/><div id="42846637" class="c"><input type="checkbox" id="c-42846637" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846612">parent</a><span>|</span><a href="#42846878">next</a><span>|</span><label class="collapse" for="c-42846637">[-]</label><label class="expand" for="c-42846637">[2 more]</label></div><br/><div class="children"><div class="content">Yes revenue has grown xx% in the last quarter and year, but the stock is valued as if it will keep growing at that rate for years to come and no one will challenge them. That is the definition of a bubble.<p>How sound is the investment thesis when a bunch of online discussions about a technical paper on a new model can cause a 20% overnight selloff? Does Apple drop 20% when Samsung announces a new phone?</div><br/><div id="42846864" class="c"><input type="checkbox" id="c-42846864" checked=""/><div class="controls bullet"><span class="by">hyperpape</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846637">parent</a><span>|</span><a href="#42846878">next</a><span>|</span><label class="collapse" for="c-42846864">[-]</label><label class="expand" for="c-42846864">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the stock is valued as if it will keep growing at that rate for years to come and no one will challenge them.<p>If it were valued that way, the P&#x2F;E would be over 100.<p>Feel free to say Nvidia is overvalued, but you have to get the financials right.</div><br/></div></div></div></div><div id="42846878" class="c"><input type="checkbox" id="c-42846878" checked=""/><div class="controls bullet"><span class="by">tempeler</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846612">parent</a><span>|</span><a href="#42846637">prev</a><span>|</span><a href="#42848658">next</a><span>|</span><label class="collapse" for="c-42846878">[-]</label><label class="expand" for="c-42846878">[1 more]</label></div><br/><div class="children"><div class="content">People do not understand. If you want to make money in the stock market, find growing companies. Pricing of the growing companies is different from others. Since it is not clear when the growth will end, there is a high probability that there will be extreme things in pricing. Since they are market leadership and can lead the price. Don&#x27;t compare growing companies with others. That&#x27;s a big fallacy. Their price always overshooted. I don&#x27;t have any investments in Nvidia, but reality is that. This is why economists always talk about growth.</div><br/></div></div><div id="42848658" class="c"><input type="checkbox" id="c-42848658" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846612">parent</a><span>|</span><a href="#42846878">prev</a><span>|</span><a href="#42846709">next</a><span>|</span><label class="collapse" for="c-42848658">[-]</label><label class="expand" for="c-42848658">[1 more]</label></div><br/><div class="children"><div class="content">One might argue that very high margins could be a bad sign.  If you assume that Apple is efficient at being Apple, then there is not a whole lot of room for someone else to undercut them at similar cost of goods sold. But there is a <i>lot</i> of room to undercut Nvidia with similar COGS — Nvidia is doing well because it’s difficult to compete for various reasons, not that it’s expensive to compete.</div><br/></div></div><div id="42846709" class="c"><input type="checkbox" id="c-42846709" checked=""/><div class="controls bullet"><span class="by">segasaturn</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846612">parent</a><span>|</span><a href="#42848658">prev</a><span>|</span><a href="#42846662">next</a><span>|</span><label class="collapse" for="c-42846709">[-]</label><label class="expand" for="c-42846709">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the thing. Nvidia&#x27;s future growth has been potentially kneecapped by R1&#x27;s leaps in efficiency.</div><br/><div id="42846867" class="c"><input type="checkbox" id="c-42846867" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846709">parent</a><span>|</span><a href="#42846662">next</a><span>|</span><label class="collapse" for="c-42846867">[-]</label><label class="expand" for="c-42846867">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see it, instead of 100 GPUs running the AIs we have today, we&#x27;ll have 100 GPUs running the AI of the future. NVIDIA wins either way. It won&#x27;t be 50 GPUs running the AI of today.</div><br/><div id="42847003" class="c"><input type="checkbox" id="c-42847003" checked=""/><div class="controls bullet"><span class="by">pertymcpert</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846867">parent</a><span>|</span><a href="#42846662">next</a><span>|</span><label class="collapse" for="c-42847003">[-]</label><label class="expand" for="c-42847003">[3 more]</label></div><br/><div class="children"><div class="content">All other things being equal, less demand means lower profits. Even if demand still outstrips supply, it&#x27;s still less demand expected than a month ago.</div><br/><div id="42847195" class="c"><input type="checkbox" id="c-42847195" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847003">parent</a><span>|</span><a href="#42846662">next</a><span>|</span><label class="collapse" for="c-42847195">[-]</label><label class="expand" for="c-42847195">[2 more]</label></div><br/><div class="children"><div class="content">If the demand is for reaching AGI then we&#x27;ll still need all the GPUs NVIDIA can sell, we&#x27;ll just get there faster thanks to DeepSeek.</div><br/><div id="42847918" class="c"><input type="checkbox" id="c-42847918" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847195">parent</a><span>|</span><a href="#42846662">next</a><span>|</span><label class="collapse" for="c-42847918">[-]</label><label class="expand" for="c-42847918">[1 more]</label></div><br/><div class="children"><div class="content">Bearish argument: &quot;How can we adapt you to run on 8,000,000 NVidia GPUs instead of 80,000,000?&quot;<p>That probably won&#x27;t be the <i>first</i> question we ask AGI if&#x2F;when we ever get there, but it will be near the top of the list.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42846662" class="c"><input type="checkbox" id="c-42846662" checked=""/><div class="controls bullet"><span class="by">amazingamazing</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846612">parent</a><span>|</span><a href="#42846709">prev</a><span>|</span><a href="#42846671">next</a><span>|</span><label class="collapse" for="c-42846662">[-]</label><label class="expand" for="c-42846662">[1 more]</label></div><br/><div class="children"><div class="content">to be fair, there&#x27;s no way these rates will be sustained for a decade.</div><br/></div></div></div></div><div id="42846671" class="c"><input type="checkbox" id="c-42846671" checked=""/><div class="controls bullet"><span class="by">numba888</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846513">parent</a><span>|</span><a href="#42846612">prev</a><span>|</span><a href="#42846877">next</a><span>|</span><label class="collapse" for="c-42846671">[-]</label><label class="expand" for="c-42846671">[5 more]</label></div><br/><div class="children"><div class="content">P&#x2F;E ratio is better indicator. Price&#x2F;Earnings. NVidia: 46, Microsoft: 35, Apple: 34, Amazon: 50.<p>As you see NVidia doesn&#x27;t stand out much, it&#x27;s even lower than Amazon.</div><br/><div id="42846906" class="c"><input type="checkbox" id="c-42846906" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846671">parent</a><span>|</span><a href="#42847116">next</a><span>|</span><label class="collapse" for="c-42846906">[-]</label><label class="expand" for="c-42846906">[2 more]</label></div><br/><div class="children"><div class="content">You are sharing numbers <i>after</i> the drop. The p&#x2F;e was 60 yesterday.</div><br/><div id="42848382" class="c"><input type="checkbox" id="c-42848382" checked=""/><div class="controls bullet"><span class="by">numba888</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846906">parent</a><span>|</span><a href="#42847116">next</a><span>|</span><label class="collapse" for="c-42848382">[-]</label><label class="expand" for="c-42848382">[1 more]</label></div><br/><div class="children"><div class="content">anyway it&#x27;s not dramatic. vs 50 for Amazon. $147 was close to historical max for NVidia. Not fair either. last month in was less than $140 average, just estimate.</div><br/></div></div></div></div><div id="42847116" class="c"><input type="checkbox" id="c-42847116" checked=""/><div class="controls bullet"><span class="by">epicureanideal</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846671">parent</a><span>|</span><a href="#42846906">prev</a><span>|</span><a href="#42846877">next</a><span>|</span><label class="collapse" for="c-42847116">[-]</label><label class="expand" for="c-42847116">[2 more]</label></div><br/><div class="children"><div class="content">All of them are overvalued compared to historical average ratios.<p>And NVDA’s P&#x2F;E benefits from very recent huge spending that may not continue.</div><br/><div id="42847183" class="c"><input type="checkbox" id="c-42847183" checked=""/><div class="controls bullet"><span class="by">onlyrealcuzzo</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42847116">parent</a><span>|</span><a href="#42846877">next</a><span>|</span><label class="collapse" for="c-42847183">[-]</label><label class="expand" for="c-42847183">[1 more]</label></div><br/><div class="children"><div class="content">They have higher growth rates than average.<p>Look at their PEG ratios.</div><br/></div></div></div></div></div></div><div id="42846877" class="c"><input type="checkbox" id="c-42846877" checked=""/><div class="controls bullet"><span class="by">slashdev</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846513">parent</a><span>|</span><a href="#42846671">prev</a><span>|</span><a href="#42846450">next</a><span>|</span><label class="collapse" for="c-42846877">[-]</label><label class="expand" for="c-42846877">[1 more]</label></div><br/><div class="children"><div class="content">Stock market valuations are not about current revenue. That’s just a fundamental disconnect from how the financial markets work.<p>In theory it’s more about forward profits per share, taking into account growth over many years. And Nvidia is growing faster than any company with that much revenue.<p>Obviously the future is hard to predict, which leaves a lot of wiggle room.<p>But I say in theory, because in practice it’s more about global liquidity. It has a lot to do with passive investing being so dominant and money flows.<p>Money printer goes brrr and stonks go up.<p>That is not the only thing that matters, but it seems to be the main thing.<p>If it were really about future profits most of these companies would long since be uninvestable. The valuations are too high to expect a positive ROI.</div><br/></div></div></div></div><div id="42846450" class="c"><input type="checkbox" id="c-42846450" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846342">parent</a><span>|</span><a href="#42846513">prev</a><span>|</span><a href="#42848693">next</a><span>|</span><label class="collapse" for="c-42846450">[-]</label><label class="expand" for="c-42846450">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say it&#x27;s a meme stock and based on meme revenue. Much of the 35B comes from the fact that companies believe Nvidia make the best chips, and that they have to have the best chips or they&#x27;ll be out of the game.<p>DeepSeek supposedly nullifies that last part.</div><br/><div id="42846616" class="c"><input type="checkbox" id="c-42846616" checked=""/><div class="controls bullet"><span class="by">buffington</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846450">parent</a><span>|</span><a href="#42848693">next</a><span>|</span><label class="collapse" for="c-42846616">[-]</label><label class="expand" for="c-42846616">[2 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t DeepSeek train on Nvidia hardware though?<p>I can&#x27;t see how DeepSeek hurts Nvidia, if Nvidia is what enables DeepSeek.</div><br/><div id="42846700" class="c"><input type="checkbox" id="c-42846700" checked=""/><div class="controls bullet"><span class="by">amazingamazing</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846616">parent</a><span>|</span><a href="#42848693">next</a><span>|</span><label class="collapse" for="c-42846700">[-]</label><label class="expand" for="c-42846700">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s not entirely relevant.<p>the simplest way to present the counter argument is:<p>- suppose you could train the best model with a single H100 for an hour. would that hurt or harm nvidia?<p>- suppose you could serve 1000x users with a 1&#x2F;1000 the amount of gpus. would that hurt or harm nvidia?<p>the question is how big you think the market size is, and how fast you get to saturation. once things are saturated efficiency just results in less demand.</div><br/></div></div></div></div></div></div><div id="42848693" class="c"><input type="checkbox" id="c-42848693" checked=""/><div class="controls bullet"><span class="by">edm0nd</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846342">parent</a><span>|</span><a href="#42846450">prev</a><span>|</span><a href="#42846404">next</a><span>|</span><label class="collapse" for="c-42848693">[-]</label><label class="expand" for="c-42848693">[1 more]</label></div><br/><div class="children"><div class="content">Would now be a good time to buy into NVDA if you are long on it?</div><br/></div></div><div id="42846404" class="c"><input type="checkbox" id="c-42846404" checked=""/><div class="controls bullet"><span class="by">master_crab</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846342">parent</a><span>|</span><a href="#42848693">prev</a><span>|</span><a href="#42846435">next</a><span>|</span><label class="collapse" for="c-42846404">[-]</label><label class="expand" for="c-42846404">[2 more]</label></div><br/><div class="children"><div class="content">True but with that revenue number it would mean that before today it was valued at ~100x revenue.  That’s pretty bubbly.</div><br/><div id="42846445" class="c"><input type="checkbox" id="c-42846445" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846404">parent</a><span>|</span><a href="#42846435">next</a><span>|</span><label class="collapse" for="c-42846445">[-]</label><label class="expand" for="c-42846445">[1 more]</label></div><br/><div class="children"><div class="content">Thats 100x quarterly revenue, or 25x annual revenue.</div><br/></div></div></div></div></div></div><div id="42846435" class="c"><input type="checkbox" id="c-42846435" checked=""/><div class="controls bullet"><span class="by">segasaturn</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846342">prev</a><span>|</span><a href="#42846861">next</a><span>|</span><label class="collapse" for="c-42846435">[-]</label><label class="expand" for="c-42846435">[1 more]</label></div><br/><div class="children"><div class="content">Correct, Nvidia has been on this bubble-like tragectory since before the stock was split last year. I would argue that today&#x27;s drop is a precursor to a much larger crash to come.</div><br/></div></div><div id="42846861" class="c"><input type="checkbox" id="c-42846861" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846435">prev</a><span>|</span><a href="#42846587">next</a><span>|</span><label class="collapse" for="c-42846861">[-]</label><label class="expand" for="c-42846861">[1 more]</label></div><br/><div class="children"><div class="content">Nah, this is not about Nvidia being a bubble.   This is about people forgetting that software will keep eating the world and Nvidia is a hardware company no matter how many times people say it&#x27;s a software company and talk about Cuda.  Yes, CUDA is their moat, but they are not a software company.   See my post on reddit from 10 months ago about this happening.<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1c0je6h&#x2F;comment&#x2F;kyx335r&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1c0je6h&#x2F;comment...</a><p>&quot;The biggest threat to NVIDIA is not AMD, Intel or Google&#x27;s TPU. It&#x27;s software. Sofware eats the world!&quot;<p>&quot;That&#x27;s what software is going to do. A new architecture&#x2F;algorithm that allows us current performance with 50% of the hardware, would change everything. What would that mean? If Nvidia had it in the books to sell N hardware, all of a sudden the demand won&#x27;t exist since N compute can be realized with the new software and existing hardware. Hardware that might not have been attractive like AMD, Intel or even older hardware would become attractive. They would have to cut their price so much, the violent exodus from their stocks will be shocking. Lots of people are going to get rich via Nvidia, lots are going to get poor after the fact. It&#x27;s not going to be because of hardware, but software.&quot;<p>A lot of people are saying that I&#x27;m wrong on other hardware like AMD or Intel, but this article by Stratechery agrees, all other hardware vendors are possibly relevant again.  I didn&#x27;t talk about Apple because I was focused on the server side, Apple has already won the consumer side and is so far ahead and waiting for the tech to catch up to it.<p>The biggest threat to Nvidia is still more software optimization.</div><br/></div></div><div id="42846587" class="c"><input type="checkbox" id="c-42846587" checked=""/><div class="controls bullet"><span class="by">httpz</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846861">prev</a><span>|</span><a href="#42846377">next</a><span>|</span><label class="collapse" for="c-42846587">[-]</label><label class="expand" for="c-42846587">[3 more]</label></div><br/><div class="children"><div class="content">Nvida has a P&#x2F;E of 47. While it may be a bit high for a semiconductor company, it&#x27;s definitely not a meme stock figure.</div><br/><div id="42846932" class="c"><input type="checkbox" id="c-42846932" checked=""/><div class="controls bullet"><span class="by">snailmailstare</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846587">parent</a><span>|</span><a href="#42846972">next</a><span>|</span><label class="collapse" for="c-42846932">[-]</label><label class="expand" for="c-42846932">[1 more]</label></div><br/><div class="children"><div class="content">Yes and no, going from 47 to 50 would buy a few of the most popular meme stocks so there simply aren&#x27;t enough people to make it a true meme stock with that market cap.</div><br/></div></div><div id="42846972" class="c"><input type="checkbox" id="c-42846972" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846587">parent</a><span>|</span><a href="#42846932">prev</a><span>|</span><a href="#42846377">next</a><span>|</span><label class="collapse" for="c-42846972">[-]</label><label class="expand" for="c-42846972">[1 more]</label></div><br/><div class="children"><div class="content">The forward PE is half that based on real numbers of future orders reported in company reports.</div><br/></div></div></div></div><div id="42846377" class="c"><input type="checkbox" id="c-42846377" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846587">prev</a><span>|</span><a href="#42846781">next</a><span>|</span><label class="collapse" for="c-42846377">[-]</label><label class="expand" for="c-42846377">[9 more]</label></div><br/><div class="children"><div class="content">This is a cookie cutter comment that appears to have been copy pasted from a thread about Gamestop or something. DeepSeek R1 allegedly being almost 50x more compute efficient isn&#x27;t just a &quot;vague rumor&quot;. You do this community a disservice by commenting before understanding what investors are thinking at the current moment.</div><br/><div id="42846419" class="c"><input type="checkbox" id="c-42846419" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846377">parent</a><span>|</span><a href="#42847579">next</a><span>|</span><label class="collapse" for="c-42846419">[-]</label><label class="expand" for="c-42846419">[7 more]</label></div><br/><div class="children"><div class="content">Has anyone verified DeepSeek&#x27;s claims about R1? They have literally published one single paper and it has been out for a week. Nothing about what they did changed Nvidia&#x27;s fundamentals. In fact there was no additional news over the weekend or today morning. The entire market movement is because of a single statement by DeepSeek&#x27;s CEO from over a week ago. People sold because other people sold. This is exactly how a panic selloff happens.</div><br/><div id="42846459" class="c"><input type="checkbox" id="c-42846459" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846419">parent</a><span>|</span><a href="#42846563">next</a><span>|</span><label class="collapse" for="c-42846459">[-]</label><label class="expand" for="c-42846459">[1 more]</label></div><br/><div class="children"><div class="content">They have not verified the claims but those claims are not a &quot;vague rumor&quot;. Expectations of discounted cash flows, which is primarily what drives large cap stock prices, operates on probability, not strange notions of &quot;we must be absolutely certain that something is true&quot;.<p>A credible lab making a credible claim to massive efficiency improvements is a credible threat to Nvidia&#x27;s future earnings. Hence the stock got sold. It&#x27;s not more complicated than that.</div><br/></div></div><div id="42846563" class="c"><input type="checkbox" id="c-42846563" checked=""/><div class="controls bullet"><span class="by">KiwiJohnno</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846419">parent</a><span>|</span><a href="#42846459">prev</a><span>|</span><a href="#42846664">next</a><span>|</span><label class="collapse" for="c-42846563">[-]</label><label class="expand" for="c-42846563">[1 more]</label></div><br/><div class="children"><div class="content">Not a true verification but I have tried the Deepseek R1 7b model running locally, it runs on my 6gb laptop GPU and the results are impressive.<p>Its obviously constrained by this hardware and this model size as it does some strange things sometimes and it is slow (30 secs to respond) but I&#x27;ve got it to do some impressive things that GPT4 struggles with or fails on.<p>Also of note I asked it about Taiwan and it parroted the official CCP line about Taiwan being part of China, without even the usual delay while it generated the result.</div><br/></div></div><div id="42846664" class="c"><input type="checkbox" id="c-42846664" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846419">parent</a><span>|</span><a href="#42846563">prev</a><span>|</span><a href="#42847579">next</a><span>|</span><label class="collapse" for="c-42846664">[-]</label><label class="expand" for="c-42846664">[4 more]</label></div><br/><div class="children"><div class="content">The weights are public. We can&#x27;t verify their claims about the amount of compute used for training, but we can trivially verify the claims about inference cost and benchmark performance. On both those counts, DeepSeek have been entirely honest.</div><br/><div id="42846806" class="c"><input type="checkbox" id="c-42846806" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846664">parent</a><span>|</span><a href="#42847579">next</a><span>|</span><label class="collapse" for="c-42846806">[-]</label><label class="expand" for="c-42846806">[3 more]</label></div><br/><div class="children"><div class="content">Benchmark performance - better models are actually great for Nvidia&#x27;s bottom line, since the company is relying on the advancement of AI as a whole.<p>Inference cost - DeepSeek is charging less than OpenAI to use its public API, but that isn&#x27;t an indicator of anything since it doesn&#x27;t reflect the actual cost of operation. It&#x27;s pretty much a guarantee that both companies are losing money. Looking at DeepSeek&#x27;s published models the inference cost is in the same ballpark as Llama and the rest.<p>Which leaves training, and that&#x27;s what all the speculation is about. The CEO said that the model cost $5.5M and that&#x27;s what the entire world is clinging on. We have literally no other info and no way to verify it (for now, until efforts to replicate it start to show results).</div><br/><div id="42846974" class="c"><input type="checkbox" id="c-42846974" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846806">parent</a><span>|</span><a href="#42847579">next</a><span>|</span><label class="collapse" for="c-42846974">[-]</label><label class="expand" for="c-42846974">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt;Inference cost - DeepSeek is charging less than OpenAI to use its public API, but that isn&#x27;t an indicator of anything since it doesn&#x27;t reflect the actual cost of operation.</i><p>Again, the weights are public. You can run the full-fat version of R1 on your own hardware, or a cloud provider of your choice. The inference costs match what DeepSeek are claiming, for reasons that are entirely obvious based on the architecture. Either the incumbents are secretly making enormous margins on inference, or they&#x27;re vastly less efficient; in the first case they&#x27;re in trouble, in the second case they&#x27;re in <i>real</i> trouble.</div><br/><div id="42847050" class="c"><input type="checkbox" id="c-42847050" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846974">parent</a><span>|</span><a href="#42847579">next</a><span>|</span><label class="collapse" for="c-42847050">[-]</label><label class="expand" for="c-42847050">[1 more]</label></div><br/><div class="children"><div class="content">R1&#x27;s inference costs are in the same ballpark as Llama 3 and every other similar model in its class. People are just reading and repeating &quot;it is cheap!!&quot; ad nauseam without any actual data to back it up.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42847579" class="c"><input type="checkbox" id="c-42847579" checked=""/><div class="controls bullet"><span class="by">spott</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846377">parent</a><span>|</span><a href="#42846419">prev</a><span>|</span><a href="#42846781">next</a><span>|</span><label class="collapse" for="c-42847579">[-]</label><label class="expand" for="c-42847579">[1 more]</label></div><br/><div class="children"><div class="content">I wonder where they got 50… llama405 cost like 60M, which puts deepseek at closer to 10x…</div><br/></div></div></div></div><div id="42846781" class="c"><input type="checkbox" id="c-42846781" checked=""/><div class="controls bullet"><span class="by">digitcatphd</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846377">prev</a><span>|</span><a href="#42846378">next</a><span>|</span><label class="collapse" for="c-42846781">[-]</label><label class="expand" for="c-42846781">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. Everything is massive kindle for a wildfire and it comes to show how tiny of a spark it takes. Yet - people keep adding more kindle.</div><br/></div></div><div id="42846378" class="c"><input type="checkbox" id="c-42846378" checked=""/><div class="controls bullet"><span class="by">codingwagie</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846781">prev</a><span>|</span><a href="#42846241">next</a><span>|</span><label class="collapse" for="c-42846378">[-]</label><label class="expand" for="c-42846378">[1 more]</label></div><br/><div class="children"><div class="content">No the reality of AI models fundamentally changed</div><br/></div></div><div id="42846241" class="c"><input type="checkbox" id="c-42846241" checked=""/><div class="controls bullet"><span class="by">onlyrealcuzzo</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846378">prev</a><span>|</span><a href="#42846811">next</a><span>|</span><label class="collapse" for="c-42846241">[-]</label><label class="expand" for="c-42846241">[4 more]</label></div><br/><div class="children"><div class="content">Hard to argue Nvidia is a meme stock and that Tesla is not a bigger meme stock.<p>If meme stocks were imploding, why is Tesla fine?<p>This is about DeepSeek.</div><br/><div id="42846260" class="c"><input type="checkbox" id="c-42846260" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846241">parent</a><span>|</span><a href="#42846350">next</a><span>|</span><label class="collapse" for="c-42846260">[-]</label><label class="expand" for="c-42846260">[1 more]</label></div><br/><div class="children"><div class="content">What does any of this have to do with Tesla? Even if Tesla is a bigger bubble, not all bubbles have to pop at the same time.</div><br/></div></div><div id="42846350" class="c"><input type="checkbox" id="c-42846350" checked=""/><div class="controls bullet"><span class="by">pokstad</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846241">parent</a><span>|</span><a href="#42846260">prev</a><span>|</span><a href="#42846265">next</a><span>|</span><label class="collapse" for="c-42846350">[-]</label><label class="expand" for="c-42846350">[1 more]</label></div><br/><div class="children"><div class="content">The market can stay irrational longer than you can stay solvent.</div><br/></div></div><div id="42846265" class="c"><input type="checkbox" id="c-42846265" checked=""/><div class="controls bullet"><span class="by">coliveira</span><span>|</span><a href="#42846209">root</a><span>|</span><a href="#42846241">parent</a><span>|</span><a href="#42846350">prev</a><span>|</span><a href="#42846811">next</a><span>|</span><label class="collapse" for="c-42846265">[-]</label><label class="expand" for="c-42846265">[1 more]</label></div><br/><div class="children"><div class="content">Tesla is playing the political game with Trump. They&#x27;re riding that wave. Musk always find some new reason for people to believe the stock.</div><br/></div></div></div></div><div id="42846811" class="c"><input type="checkbox" id="c-42846811" checked=""/><div class="controls bullet"><span class="by">827a</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846241">prev</a><span>|</span><a href="#42846966">next</a><span>|</span><label class="collapse" for="c-42846811">[-]</label><label class="expand" for="c-42846811">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry, but this is just so, so wrong. Nvidia is an insane company. You can make the argument that the entire sector is frothy&#x2F;bubbly; I&#x27;m more likely to believe that. But, here&#x27;s some select financials about NVDA:<p>NVDA Net income, Quarter ending in ~Oct2024: $19B. AMD? $771M. INTC? -$16.6B. QCOM? $3B. AAPL? $14B.<p>Revenue growth, YoY? +93%. AMD? +17%. INTC? -6%. QCOM? +18%. AAPL? +6%.<p>Margin? 55%. AMD? 11%. INTC? -125%. QCOM? 28%. AAPL? 15%.<p>P&#x2F;E Ratio? 46. AMD? 103. INTC? N&#x2F;A, unprofitable. QCOM? 19. AAPL? 34. NFLX? 54. GME? 151.<p>Their P&#x2F;E Ratio doesn&#x27;t even classify them as all that overvalued. Think about that. Price to earnings, they are cheaper than Netflix, Gamestop, they&#x27;re about the same level as WALMART, you know, that Retailer everyone hates that has practically no AI play, yeah their P&#x2F;E is 40.<p>Nvidia is an <i>insane</i> company. Insane. We&#x27;ve had three of the largest country-economies on the planet announce public&#x2F;private funding to the tune of 12 figures, maybe totaling 13 figures when its all said and done, and NVDA is the ONLY company on the PLANET that sells what they want to buy. There is no second player. Oh yeah, Google will rent you some TPUs, haha yeah sure bud. China wants to build AI data centers, and their top tech firms are going to the black market smuggling GPUs across the ocean like bricks of cocaine rather than rely on domestic manufacturers, because not even other AMERICAN manufacturers can catch up.<p>Sure, a 10x drop in cost of intelligence is initially perceived as a hit to the company. But, here&#x27;s the funny thing about, let&#x27;s say, CPUs: The Intel Northwood Pentium 4 was released in 2001; with its 130nm process architecture, it sipped a cool 61 watts of power. With today&#x27;s 3nm process architecture, we&#x27;ve built (drumroll please) the Intel Core Ultra 5 255, which consumes 65 watts of power. Sad trombone? Of course not; its a billion times more performant. We <i>could</i> have directed improvements in process architecture toward reducing power draw (and certainly, we did, for some kinds of chips). But, the VAST, VAST, VAST majority of allocation of these process improvements was in performance.<p>The story here is not &quot;intelligence is 10x cheaper, so we&#x27;ll need 10x fewer GPUs&quot;. The story is: &quot;Intelligence is 10x cheaper, people are going to want 10x more intelligence.&quot;</div><br/></div></div><div id="42846966" class="c"><input type="checkbox" id="c-42846966" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846811">prev</a><span>|</span><a href="#42846766">next</a><span>|</span><label class="collapse" for="c-42846966">[-]</label><label class="expand" for="c-42846966">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia&#x27;s forward PE before this is 27, based off of real orders. Unless orders are being canceled, the stock price should be significantly higher</div><br/></div></div><div id="42846766" class="c"><input type="checkbox" id="c-42846766" checked=""/><div class="controls bullet"><span class="by">behringer</span><span>|</span><a href="#42846209">parent</a><span>|</span><a href="#42846966">prev</a><span>|</span><a href="#42846763">next</a><span>|</span><label class="collapse" for="c-42846766">[-]</label><label class="expand" for="c-42846766">[1 more]</label></div><br/><div class="children"><div class="content">Whenever the internet tells you to buy, it&#x27;s a huge warning that a pump and dump is occurring.</div><br/></div></div></div></div><div id="42849080" class="c"><input type="checkbox" id="c-42849080" checked=""/><div class="controls bullet"><span class="by">DAGdug</span><span>|</span><a href="#42846209">prev</a><span>|</span><a href="#42846428">next</a><span>|</span><label class="collapse" for="c-42849080">[-]</label><label class="expand" for="c-42849080">[2 more]</label></div><br/><div class="children"><div class="content">All the references to Jevon’s paradox fail to account for three things:
1. There’s no good forecasting model to account for how aggregate demand moves as a function of efficiency gains in this space
2. Aggregate demand isn’t the same as Nvidia’s share of market, which could drop if alternative paradigms for training or inferencing gain traction 
3. Forecasting time horizons matter for discounted cash flow&#x2F;valuation calculations, which nobody has a good basis for 
IMO, there’s just a lot of uncertainty, and it’s fair for the market to discount the optimistic trajectory aggressively based on net new info.</div><br/><div id="42849110" class="c"><input type="checkbox" id="c-42849110" checked=""/><div class="controls bullet"><span class="by">ponty_rick</span><span>|</span><a href="#42849080">parent</a><span>|</span><a href="#42846428">next</a><span>|</span><label class="collapse" for="c-42849110">[-]</label><label class="expand" for="c-42849110">[1 more]</label></div><br/><div class="children"><div class="content">Summarizing in Kevin Malone style - Nvidia good now. Later? No know.</div><br/></div></div></div></div><div id="42846428" class="c"><input type="checkbox" id="c-42846428" checked=""/><div class="controls bullet"><span class="by">Kon-Peki</span><span>|</span><a href="#42849080">prev</a><span>|</span><a href="#42846592">next</a><span>|</span><label class="collapse" for="c-42846428">[-]</label><label class="expand" for="c-42846428">[1 more]</label></div><br/><div class="children"><div class="content">If you look at total volume of shares traded, this would be somewhere in the range of 200th highest.<p>If you look at the total monetary value of those shares traded, this would be in the top 5, all of which have happened in the past 5 years.  #1 is probably Tesla on Dec 18 2020 (right before it joined the S&amp;P500).  It lost ~6% that day.<p>Don’t get me wrong, this is definitely a big day.  Just not “lose your mind” big.  It’s clear that most shareholders just sat things out.</div><br/></div></div><div id="42846592" class="c"><input type="checkbox" id="c-42846592" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42846428">prev</a><span>|</span><a href="#42847235">next</a><span>|</span><label class="collapse" for="c-42846592">[-]</label><label class="expand" for="c-42846592">[7 more]</label></div><br/><div class="children"><div class="content">So, what are investors thinking to warrant this? If it is &#x27;DeepSeek means you don&#x27;t need the compute&#x27; that is definitely wrong. Making a more efficient x almost always leads to more of x being sold&#x2F;used, not less. In the long term does anyone believe we won&#x27;t keep needing more compute and not less?<p>I think the market believes that high end compute is not needed anymore so the stuff in datacenters suddenly just became 10x over-provisioned and it will take a while to fill up that capacity. Additionally, things like the mac and AMD unified memory architectures and consumer GPUs are all now suddenly able to run SOTA models locally. So a triple whammy. The competition just caught up, demand is about to drop in the short term for any datacenter compute and the market for exotic, high margin, GPUs might have just evaporated. At least that is what I think the market is thinking. I personally believe this is a short term correction since the long term demand is still there and we will keep wanting more big compute for a long time.</div><br/><div id="42846648" class="c"><input type="checkbox" id="c-42846648" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#42846592">parent</a><span>|</span><a href="#42846598">next</a><span>|</span><label class="collapse" for="c-42846648">[-]</label><label class="expand" for="c-42846648">[5 more]</label></div><br/><div class="children"><div class="content">But the SOTA models basically all suck today. If people don’t think they suck, definitely in 1 year they’ll look back and consider those older models unusably bad</div><br/><div id="42847128" class="c"><input type="checkbox" id="c-42847128" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#42846592">root</a><span>|</span><a href="#42846648">parent</a><span>|</span><a href="#42846598">next</a><span>|</span><label class="collapse" for="c-42847128">[-]</label><label class="expand" for="c-42847128">[4 more]</label></div><br/><div class="children"><div class="content">I recently went to the LLM chat arena and tried my &quot;test input&quot; against the latest frontier models that GPT 3 failed on. This test snippet simply repeats the same four-letter word in a paragraph many times using all of its various possible meanings simultaneously. The request to the AI is to put the meaning of each usage of the word next to it in brackets.<p>None of the frontier models can do this perfectly. They <i>all</i> screw up to various degrees in various interesting ways. A schoolkid could do this flawlessly.<p>This is not some contrived test with bizarre picture puzzles as seen in ARC-AGI or testing obscure knowledge about bleeding-edge scientific research. It&#x27;s simple English comprehension using a word my toddler knows already!<p>It does reveal the fundamental flaw in all transformer-based models: They&#x27;re just shifting vectors around with matrices, and are unable to deal with many categories of inputs that cause overlaps or bring too many of the tokens too close to each other in some internal representation. They get muddled up and confused, resulting in errors in the output.<p>I see similar effects when using LLMs for programming: They get confused when there are many usages of the same identifier or keyword, but with some subtle difference such as being inside a comment, string, or in a local context where the meaning is different.<p>I suspect this will be eventually fixed, but I haven&#x27;t seen any fundamental improvement in three years.</div><br/><div id="42847397" class="c"><input type="checkbox" id="c-42847397" checked=""/><div class="controls bullet"><span class="by">quesera</span><span>|</span><a href="#42846592">root</a><span>|</span><a href="#42847128">parent</a><span>|</span><a href="#42846598">next</a><span>|</span><label class="collapse" for="c-42847397">[-]</label><label class="expand" for="c-42847397">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>This test snippet simply repeats the same four-letter word in a paragraph many times using all of various possible meanings simultaneously</i><p>This sounds like fun. How does it do with an arbitrary quantity of &quot;buffalo&quot;s?</div><br/><div id="42847628" class="c"><input type="checkbox" id="c-42847628" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#42846592">root</a><span>|</span><a href="#42847397">parent</a><span>|</span><a href="#42846598">next</a><span>|</span><label class="collapse" for="c-42847628">[-]</label><label class="expand" for="c-42847628">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a known thing that would be in its training set.<p>I just made up my own thing that no AI model would have seen anywhere before.<p>It&#x27;s pretty easy to create your own, just pick a word that is highly overloaded. It helps if it is also used as proper names, business names, place names, etc...</div><br/><div id="42849026" class="c"><input type="checkbox" id="c-42849026" checked=""/><div class="controls bullet"><span class="by">boilerupnc</span><span>|</span><a href="#42846592">root</a><span>|</span><a href="#42847628">parent</a><span>|</span><a href="#42846598">next</a><span>|</span><label class="collapse" for="c-42849026">[-]</label><label class="expand" for="c-42849026">[1 more]</label></div><br/><div class="children"><div class="content">List of double meaning words as food for thought: <a href="https:&#x2F;&#x2F;word-lists.com&#x2F;word-lists&#x2F;100-english-words-with-multiple-meanings&#x2F;" rel="nofollow">https:&#x2F;&#x2F;word-lists.com&#x2F;word-lists&#x2F;100-english-words-with-mul...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="42846598" class="c"><input type="checkbox" id="c-42846598" checked=""/><div class="controls bullet"><span class="by">amazingamazing</span><span>|</span><a href="#42846592">parent</a><span>|</span><a href="#42846648">prev</a><span>|</span><a href="#42847235">next</a><span>|</span><label class="collapse" for="c-42846598">[-]</label><label class="expand" for="c-42846598">[1 more]</label></div><br/><div class="children"><div class="content">selling more does not necessarily mean you make more money. more efficiency could lead to less margins even if volume is higher.<p>moreover, even things are incredibly efficient, the bar to sufficiently good AI <i>in practice</i> (e.g. applications), might be met with commodity compute, pretty much locking nvidia out, who generally sells high margin high performance chips to whales.</div><br/></div></div></div></div><div id="42847235" class="c"><input type="checkbox" id="c-42847235" checked=""/><div class="controls bullet"><span class="by">t_mann</span><span>|</span><a href="#42846592">prev</a><span>|</span><a href="#42843563">next</a><span>|</span><label class="collapse" for="c-42847235">[-]</label><label class="expand" for="c-42847235">[1 more]</label></div><br/><div class="children"><div class="content">Curious thought: could those large price movements have something to do with the fact that DeepSeek is financed by a hedge fund (rather than the more typical VC)? It is unclear how DS will make money from its current strategy of sharing much of the secret sauce that went into training as well as releasing the results under permissive licenses. But if the play was &quot;short major tech stocks and then release surprising results in a way that maximally undermines their current growth story&quot;, then it could make a lot more sense.</div><br/></div></div><div id="42843563" class="c"><input type="checkbox" id="c-42843563" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42847235">prev</a><span>|</span><a href="#42840288">next</a><span>|</span><label class="collapse" for="c-42843563">[-]</label><label class="expand" for="c-42843563">[17 more]</label></div><br/><div class="children"><div class="content">ASML plunge indicates a hysterical&#x2F;irrational component to the response, right? They aren’t going anywhere. If it turns out training is easier than expected, they make the devices that make the devices that do inference too…<p>If the field is going to produce anything useful, cheap training gets us there faster.</div><br/><div id="42844051" class="c"><input type="checkbox" id="c-42844051" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#42843563">parent</a><span>|</span><a href="#42843734">next</a><span>|</span><label class="collapse" for="c-42844051">[-]</label><label class="expand" for="c-42844051">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ASML plunge indicates a hysterical&#x2F;irrational component to the response, right<p>But don&#x27;t forget about the hysterical&#x2F;irrational component that also causes prices to go up when investors are all worried about FOMO. Of course, sure, ASML isn&#x27;t going anywhere, but their stock price isn&#x27;t based on them &quot;sticking around&quot;, it&#x27;s based on the idea that growing usage of AI will require exponentially more computing power over time, and DeepSeek kinda put a pin to that balloon.</div><br/></div></div><div id="42843734" class="c"><input type="checkbox" id="c-42843734" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#42843563">parent</a><span>|</span><a href="#42844051">prev</a><span>|</span><a href="#42846023">next</a><span>|</span><label class="collapse" for="c-42843734">[-]</label><label class="expand" for="c-42843734">[3 more]</label></div><br/><div class="children"><div class="content">Not making the news in western media yet is PRC claims to have started mass producing their indigenous 28nm litho (70% of global wafer use) this month... the estimated cost is 1&#x2F;30th of ASML machines. Extrapolate and they&#x27;re on trend of produce 14nm machines at comparable fraction cost in next few years.</div><br/><div id="42844625" class="c"><input type="checkbox" id="c-42844625" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42843734">parent</a><span>|</span><a href="#42846023">next</a><span>|</span><label class="collapse" for="c-42844625">[-]</label><label class="expand" for="c-42844625">[2 more]</label></div><br/><div class="children"><div class="content">It seems hard to extrapolate there—the 30nm-10nm range is where Intel really started to start having trouble, right?<p>Anyway, this seems like a bigger problem for companies whose business model is actually selling those chips. It couldn’t be the case that much of ASML’s valuation is based on people continuing to use their old 28nm machines, right?</div><br/><div id="42846642" class="c"><input type="checkbox" id="c-42846642" checked=""/><div class="controls bullet"><span class="by">bgnn</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42844625">parent</a><span>|</span><a href="#42846023">next</a><span>|</span><label class="collapse" for="c-42846642">[-]</label><label class="expand" for="c-42846642">[1 more]</label></div><br/><div class="children"><div class="content">Not that hard for China since they have hired a lot of TSMC top emgineers, like the head of TSMC finfet program. TSMC seems to treat them like dirt while they are treated like superstars in China.<p>Main deficiency they have is in the litho machines.</div><br/></div></div></div></div></div></div><div id="42846023" class="c"><input type="checkbox" id="c-42846023" checked=""/><div class="controls bullet"><span class="by">fspeech</span><span>|</span><a href="#42843563">parent</a><span>|</span><a href="#42843734">prev</a><span>|</span><a href="#42843758">next</a><span>|</span><label class="collapse" for="c-42846023">[-]</label><label class="expand" for="c-42846023">[1 more]</label></div><br/><div class="children"><div class="content">You need to also look at valuation and how much growth justifies that valuation.</div><br/></div></div><div id="42843758" class="c"><input type="checkbox" id="c-42843758" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#42843563">parent</a><span>|</span><a href="#42846023">prev</a><span>|</span><a href="#42843806">next</a><span>|</span><label class="collapse" for="c-42843758">[-]</label><label class="expand" for="c-42843758">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing the sentiment is not about training, but about the R&amp;D capability of China. If Chinese can figure out how to build a good enough model faster and cheaper, they may be able to come up with an ASML competitor as well.</div><br/><div id="42843847" class="c"><input type="checkbox" id="c-42843847" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42843758">parent</a><span>|</span><a href="#42843806">next</a><span>|</span><label class="collapse" for="c-42843847">[-]</label><label class="expand" for="c-42843847">[8 more]</label></div><br/><div class="children"><div class="content">PRC just announced mass producing 28nm litho that cost 1&#x2F;30 ASML hardware. Easy to extrapolate where this goes especially mature nodes like 28nm still accounts fo over 70% of global wafer use.</div><br/><div id="42843914" class="c"><input type="checkbox" id="c-42843914" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42843847">parent</a><span>|</span><a href="#42843806">next</a><span>|</span><label class="collapse" for="c-42843914">[-]</label><label class="expand" for="c-42843914">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m increasingly believing that the West has turned their dream of free trade for comparative advantage into a massive deindustrialization. The end result is unfolding in front of everyone and the sentiment I see, even on HN, is we can&#x27;t outcompete China any more. This is sad. Really sad. And this fits exactly what Liu Cixin said in Three Body Problem: Weakness and ignorance are not barriers to survival, but arrogance is.<p>We thought we won, and we thought we could &quot;control&quot; what other markets do, and we thought we could focus on only the &quot;high-value add&quot;. Now where are we?</div><br/><div id="42844057" class="c"><input type="checkbox" id="c-42844057" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42843914">parent</a><span>|</span><a href="#42846717">next</a><span>|</span><label class="collapse" for="c-42844057">[-]</label><label class="expand" for="c-42844057">[1 more]</label></div><br/><div class="children"><div class="content">IMO not so much as &quot;where are we&quot; as &quot;where are they&quot;, west was always going to have reckon with competing with PRC who is on trend to add more STEM than OECD combined or US adds new people. And eventually this will apply to India as well. Arrogance doesn&#x27;t help, but at some point reality of high value regressing to mean because magnitude more smart brains is involuting the margins out of everything. PRC competitors likely also getting creamed by deepseek as well. The running joke in China is when China does something advanced, that thing is no longer considered advanced since China very good at driving costs to nothing and commodizing advanced into common which ironically hurts PRC from getting into the true high value game.</div><br/></div></div><div id="42844429" class="c"><input type="checkbox" id="c-42844429" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42843914">parent</a><span>|</span><a href="#42846717">prev</a><span>|</span><a href="#42843806">next</a><span>|</span><label class="collapse" for="c-42844429">[-]</label><label class="expand" for="c-42844429">[4 more]</label></div><br/><div class="children"><div class="content">This is the kind of overly dramatic thinking that leads to stock market plunges.<p>China is an enormous country. It has over 4x the population of the USA. Unless you assume Chinese people are fundamentally different, it <i>should</i> be producing 4x the output in every field vs America. Yet the impact and legacy of communism is dire: China clearly isn&#x27;t even close to 4x the productivity of the USA. How many companies on the leading edge of AI does the USA have? Meta, OpenAI, Anthropic, Google, NVIDIA, Cerebras, X.ai to pick just a handful of thousands.<p>Meanwhile Europe has produced one, Mistral (or two if you count DeepMind), and China has produced one. DeepSeek meanwhile, despite being impressive, has been doing the usual thing Chinese firms focus on of rapidly driving down the cost of tech already proven out by companies elsewhere. They have a long history of doing this and it&#x27;s something they take cultural pride in, but at the same time, Chinese tech executives do worry about their relative lack of leading edge innovation. The head of DeepSeek has given interviews where he talks about that specifically and their desire to change attitudes and ideas about what Chinese firms can do, because there&#x27;s a widespread cultural belief there that the Americans go from 0-1 and the Chinese can go from 1-10.<p>It&#x27;s also worth remembering that prices in China are artificial. It&#x27;s a somewhat planned economy still. Sectors of the economy with military relevance are heavily subsidized and they play games with their exchange rates, indeed perhaps in an attempt to forcibly deindustrialize the west. Just because something is made cheaper there doesn&#x27;t necessarily mean they&#x27;re doing it better. It can also be that they&#x27;re just subsidized all the way to do that, and the average Chinese citizen is the loser (because they can&#x27;t afford to buy things that would otherwise be affordable to them).</div><br/><div id="42848502" class="c"><input type="checkbox" id="c-42848502" checked=""/><div class="controls bullet"><span class="by">kibibu</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42844429">parent</a><span>|</span><a href="#42846707">next</a><span>|</span><label class="collapse" for="c-42848502">[-]</label><label class="expand" for="c-42848502">[2 more]</label></div><br/><div class="children"><div class="content">&gt; China clearly isn&#x27;t even close to 4x the productivity of the USA<p>What makes you think innovation&#x2F;productivity&#x2F;performance scales linearly with population?<p>China has roughly 500x the population of Jamaica; should they have 500x as many sprinting gold medals?<p>&gt; has been doing the usual thing Chinese firms focus on of rapidly driving down the cost of tech already proven out by companies elsewhere<p>R1-Zero is actually new and interesting approach to building reasoning capability, that R1 is built on. Worth reading the paper.</div><br/><div id="42849656" class="c"><input type="checkbox" id="c-42849656" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42848502">parent</a><span>|</span><a href="#42846707">next</a><span>|</span><label class="collapse" for="c-42849656">[-]</label><label class="expand" for="c-42849656">[1 more]</label></div><br/><div class="children"><div class="content">Is R1-Zero more than optimized textbook learning&#x2F;distillation? I&#x27;ll check out the paper.<p>I covered the Jamaica disparity by &quot;unless you think there&#x27;s something fundamentally different about the Chinese&quot;. In the case of people from some parts of the world being faster runners there is something different about them genetically, that translates directly into superior athletic performance. Is that the case for Americans vs Chinese? I don&#x27;t know but haven&#x27;t seen much evidence of it. The gaps are probably more due to culture and government i.e. artificial and quickly fixable, if they want to.</div><br/></div></div></div></div><div id="42846707" class="c"><input type="checkbox" id="c-42846707" checked=""/><div class="controls bullet"><span class="by">bgnn</span><span>|</span><a href="#42843563">root</a><span>|</span><a href="#42844429">parent</a><span>|</span><a href="#42848502">prev</a><span>|</span><a href="#42843806">next</a><span>|</span><label class="collapse" for="c-42846707">[-]</label><label class="expand" for="c-42846707">[1 more]</label></div><br/><div class="children"><div class="content">This is missing the historical context completely. One cannot expect China to be 4x productive ehole its socio-economic development level is like 50s-60s of USA. Their population is still mostly peasants. This applies even more to India.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42843806" class="c"><input type="checkbox" id="c-42843806" checked=""/><div class="controls bullet"><span class="by">apexalpha</span><span>|</span><a href="#42843563">parent</a><span>|</span><a href="#42843758">prev</a><span>|</span><a href="#42843789">next</a><span>|</span><label class="collapse" for="c-42843806">[-]</label><label class="expand" for="c-42843806">[1 more]</label></div><br/><div class="children"><div class="content">I think everyone was just a bit overpriced because of the AI hype and it&#x27;s just letting out some air.</div><br/></div></div><div id="42843789" class="c"><input type="checkbox" id="c-42843789" checked=""/><div class="controls bullet"><span class="by">fldskfjdslkfj</span><span>|</span><a href="#42843563">parent</a><span>|</span><a href="#42843806">prev</a><span>|</span><a href="#42840288">next</a><span>|</span><label class="collapse" for="c-42843789">[-]</label><label class="expand" for="c-42843789">[1 more]</label></div><br/><div class="children"><div class="content">Not really, if demand for high end high margin chips goes down considerably than margins and demand will go down across the board.</div><br/></div></div></div></div><div id="42840288" class="c"><input type="checkbox" id="c-42840288" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#42843563">prev</a><span>|</span><a href="#42840017">next</a><span>|</span><label class="collapse" for="c-42840288">[-]</label><label class="expand" for="c-42840288">[6 more]</label></div><br/><div class="children"><div class="content">Traders are saying not doing multitoken prediction, not using Sharpe ratio adjusted rewards, using reward models, and not compressing KV cache tokens by &gt;90%, were supposed to be worth hundreds of billions of dollars of future expected revenue flow, at least according to other traders.<p>I say to the traders: you should have just stuck to reading arxiv, TPOT, and jhana twitter for the past 2 years, rather than listening to other traders, if you were trying to understand the utter spread of low hanging fruit that just hasn’t been picked up yet!</div><br/><div id="42840516" class="c"><input type="checkbox" id="c-42840516" checked=""/><div class="controls bullet"><span class="by">igleria</span><span>|</span><a href="#42840288">parent</a><span>|</span><a href="#42849317">next</a><span>|</span><label class="collapse" for="c-42840516">[-]</label><label class="expand" for="c-42840516">[1 more]</label></div><br/><div class="children"><div class="content">&gt; you should have just stuck to reading arxiv, TPOT, and jhana twitter for the past 2 years<p>Traders expect to be able to trade based on regurgitated headlines made by tech influencers who don&#x27;t know ANYTHING about NOTHING.</div><br/></div></div><div id="42849317" class="c"><input type="checkbox" id="c-42849317" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42840288">parent</a><span>|</span><a href="#42840516">prev</a><span>|</span><a href="#42845639">next</a><span>|</span><label class="collapse" for="c-42849317">[-]</label><label class="expand" for="c-42849317">[1 more]</label></div><br/><div class="children"><div class="content">The low hanging fruit thing is 100% correct. Anyone reading papers saw it everywhere, on every dimension. And it&#x27;s not to say the authors didn&#x27;t see it either, they did - they just had to get something out now. I&#x27;d guess folks in semi conductors saw the same things for ages.</div><br/></div></div><div id="42845639" class="c"><input type="checkbox" id="c-42845639" checked=""/><div class="controls bullet"><span class="by">nejsjsjsbsb</span><span>|</span><a href="#42840288">parent</a><span>|</span><a href="#42849317">prev</a><span>|</span><a href="#42841893">next</a><span>|</span><label class="collapse" for="c-42845639">[-]</label><label class="expand" for="c-42845639">[1 more]</label></div><br/><div class="children"><div class="content">You need to be very solvent to stay rational.</div><br/></div></div><div id="42841893" class="c"><input type="checkbox" id="c-42841893" checked=""/><div class="controls bullet"><span class="by">djtango</span><span>|</span><a href="#42840288">parent</a><span>|</span><a href="#42845639">prev</a><span>|</span><a href="#42840017">next</a><span>|</span><label class="collapse" for="c-42841893">[-]</label><label class="expand" for="c-42841893">[2 more]</label></div><br/><div class="children"><div class="content">Who is jhana?</div><br/><div id="42845453" class="c"><input type="checkbox" id="c-42845453" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#42840288">root</a><span>|</span><a href="#42841893">parent</a><span>|</span><a href="#42840017">next</a><span>|</span><label class="collapse" for="c-42845453">[-]</label><label class="expand" for="c-42845453">[1 more]</label></div><br/><div class="children"><div class="content">It’s a part of twitter that is nominally about advanced forms of meditation but is also a surprisingly overrepresented by ai researchers lol</div><br/></div></div></div></div></div></div><div id="42840017" class="c"><input type="checkbox" id="c-42840017" checked=""/><div class="controls bullet"><span class="by">junon</span><span>|</span><a href="#42840288">prev</a><span>|</span><a href="#42839705">next</a><span>|</span><label class="collapse" for="c-42840017">[-]</label><label class="expand" for="c-42840017">[13 more]</label></div><br/><div class="children"><div class="content">Not sure what the fuss is. I tried Deepseek earlier today for the first time and it was even worse than o1 when it came to reasoning skills and following my requests for how I wanted to engage with it.<p>o1 at least gives it to me straight. When I ask it to engage in more back and forth before assuming what I&#x27;m after, it tends to follow through. Deepseek seemed immediately eager to (very slowly) feed me a bunch of made up information thinking that&#x27;s what I wanted.<p>I feel as though a lot of people get hung up on these sort of &quot;micro benchmarks&quot; whereas trying to get <i>practical work</i> done is severely under tested. I&#x27;m not a fan of openai at all but I don&#x27;t have the spare compute to run anything locally so o1 suffices for now.<p>Still don&#x27;t see how this is anything but a win for Nvidia though.</div><br/><div id="42840132" class="c"><input type="checkbox" id="c-42840132" checked=""/><div class="controls bullet"><span class="by">LittleTimothy</span><span>|</span><a href="#42840017">parent</a><span>|</span><a href="#42846259">next</a><span>|</span><label class="collapse" for="c-42840132">[-]</label><label class="expand" for="c-42840132">[3 more]</label></div><br/><div class="children"><div class="content">The excitement isn&#x27;t the capabilities of the model, it&#x27;s how efficiently it was created. One of the major lessons in AI in the last couple of years was that scale mattered - you would want to throw more and more compute at a problem and that has turned into incredible share prices for Nvidia and incredible investments in data centre and energy generation. If it turns out that actually we didn&#x27;t need quite such incredible scale to get these results and actually we were just missing some really quite basic efficiency optimizations then the entire investment cycle into Nvidia, data centres and energy generation is going to whipsaw in an incredible way.</div><br/><div id="42845992" class="c"><input type="checkbox" id="c-42845992" checked=""/><div class="controls bullet"><span class="by">Balgair</span><span>|</span><a href="#42840017">root</a><span>|</span><a href="#42840132">parent</a><span>|</span><a href="#42840159">next</a><span>|</span><label class="collapse" for="c-42845992">[-]</label><label class="expand" for="c-42845992">[1 more]</label></div><br/><div class="children"><div class="content">Essentially, Deepseek is showing that there is a lot of room for improvement with AIs. To paraphrase Orwell, AIs are a lot more like Alarm Clocks and a lot less like Manhattan Projects.</div><br/></div></div><div id="42840159" class="c"><input type="checkbox" id="c-42840159" checked=""/><div class="controls bullet"><span class="by">nejsjsjsbsb</span><span>|</span><a href="#42840017">root</a><span>|</span><a href="#42840132">parent</a><span>|</span><a href="#42845992">prev</a><span>|</span><a href="#42846259">next</a><span>|</span><label class="collapse" for="c-42840159">[-]</label><label class="expand" for="c-42840159">[1 more]</label></div><br/><div class="children"><div class="content">Which doesn&#x27;t make sense. If anything it has the chance to make AI finally profitable and financially sustainable. Maybe that is the fear!</div><br/></div></div></div></div><div id="42846259" class="c"><input type="checkbox" id="c-42846259" checked=""/><div class="controls bullet"><span class="by">JofArnold</span><span>|</span><a href="#42840017">parent</a><span>|</span><a href="#42840132">prev</a><span>|</span><a href="#42840136">next</a><span>|</span><label class="collapse" for="c-42846259">[-]</label><label class="expand" for="c-42846259">[1 more]</label></div><br/><div class="children"><div class="content">R1 is the first model I&#x27;ve used that one-shotted a full JavaScript tetris with all the edge-case keyboard handling and scoring. It also one-shotted an AI snake game. With the right prompts I&#x27;ve found it consistently better than o1 and Claude 3.5 Sonnet.</div><br/></div></div><div id="42840136" class="c"><input type="checkbox" id="c-42840136" checked=""/><div class="controls bullet"><span class="by">lm28469</span><span>|</span><a href="#42840017">parent</a><span>|</span><a href="#42846259">prev</a><span>|</span><a href="#42840642">next</a><span>|</span><label class="collapse" for="c-42840136">[-]</label><label class="expand" for="c-42840136">[1 more]</label></div><br/><div class="children"><div class="content">It can be a win for nvidia but bad for nvidia stock if the win is smaller than the initially expected win.<p>The business is doing good, the wannabe traders not so much</div><br/></div></div><div id="42840642" class="c"><input type="checkbox" id="c-42840642" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#42840017">parent</a><span>|</span><a href="#42840136">prev</a><span>|</span><a href="#42839705">next</a><span>|</span><label class="collapse" for="c-42840642">[-]</label><label class="expand" for="c-42840642">[7 more]</label></div><br/><div class="children"><div class="content">Deepseek shows you details of the reasoning so you can trust its answers more and you can correct it when it makes a bad turn.</div><br/><div id="42841218" class="c"><input type="checkbox" id="c-42841218" checked=""/><div class="controls bullet"><span class="by">junon</span><span>|</span><a href="#42840017">root</a><span>|</span><a href="#42840642">parent</a><span>|</span><a href="#42839705">next</a><span>|</span><label class="collapse" for="c-42841218">[-]</label><label class="expand" for="c-42841218">[6 more]</label></div><br/><div class="children"><div class="content">o1 also does this.</div><br/><div id="42841865" class="c"><input type="checkbox" id="c-42841865" checked=""/><div class="controls bullet"><span class="by">polygamous_bat</span><span>|</span><a href="#42840017">root</a><span>|</span><a href="#42841218">parent</a><span>|</span><a href="#42839705">next</a><span>|</span><label class="collapse" for="c-42841865">[-]</label><label class="expand" for="c-42841865">[5 more]</label></div><br/><div class="children"><div class="content">o1 does not show the reasoning trace at this point. You may be confusing the final answer for the &lt;think&gt;&lt;&#x2F;think&gt; reasoning trace in the middle, it&#x27;s shown pretty clearly on r1.</div><br/><div id="42845945" class="c"><input type="checkbox" id="c-42845945" checked=""/><div class="controls bullet"><span class="by">junon</span><span>|</span><a href="#42840017">root</a><span>|</span><a href="#42841865">parent</a><span>|</span><a href="#42843955">next</a><span>|</span><label class="collapse" for="c-42845945">[-]</label><label class="expand" for="c-42845945">[3 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t really referring much to the UI as I was the fact that it does it to begin with. The thinking in deepseek trails off into its own nonsense before it answers, whereas I feel openai&#x27;s is way more structured.</div><br/><div id="42846208" class="c"><input type="checkbox" id="c-42846208" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42840017">root</a><span>|</span><a href="#42845945">parent</a><span>|</span><a href="#42847702">next</a><span>|</span><label class="collapse" for="c-42846208">[-]</label><label class="expand" for="c-42846208">[1 more]</label></div><br/><div class="children"><div class="content">o1 does not output the full CoT tokens, they are not comparable.</div><br/></div></div><div id="42847702" class="c"><input type="checkbox" id="c-42847702" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42840017">root</a><span>|</span><a href="#42845945">parent</a><span>|</span><a href="#42846208">prev</a><span>|</span><a href="#42843955">next</a><span>|</span><label class="collapse" for="c-42847702">[-]</label><label class="expand" for="c-42847702">[1 more]</label></div><br/><div class="children"><div class="content">All you get out of o1 is<p><pre><code>    Reassessing directives

    Considering alternatives

    Exploring secondary and tertiary aspects

    Revising initial thoughts

    Confirming factual assertions

    Performing math

    Wasting electricity
</code></pre>
... and other useless (and generally meaningless) placeholder updates.  Nothing like what the &lt;think&gt; output from DeepSeek&#x27;s model demonstrates.<p>As Karpathy (among others) has noted, the &lt;think&gt; output shows signs of genuine emergent behavior.  Presumably the same thing is going on behind the scenes in the OpenAI omni reasoning models, but we have no way of knowing, because they consider revealing the CoT output to be &quot;unsafe.&quot;</div><br/></div></div></div></div><div id="42843955" class="c"><input type="checkbox" id="c-42843955" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#42840017">root</a><span>|</span><a href="#42841865">parent</a><span>|</span><a href="#42845945">prev</a><span>|</span><a href="#42839705">next</a><span>|</span><label class="collapse" for="c-42843955">[-]</label><label class="expand" for="c-42843955">[1 more]</label></div><br/><div class="children"><div class="content">Is this different data or different annotation</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42839705" class="c"><input type="checkbox" id="c-42839705" checked=""/><div class="controls bullet"><span class="by">nabla9</span><span>|</span><a href="#42840017">prev</a><span>|</span><a href="#42846162">next</a><span>|</span><label class="collapse" for="c-42839705">[-]</label><label class="expand" for="c-42839705">[95 more]</label></div><br/><div class="children"><div class="content">Nvidia -13% in Frankfurt stock market just now.<p>Valuations of private unicorns like OpenAi and Anthropic  must be in free fall. DeepSeek spends $6 million in old H800 hardware to develop open source model that overtakes ChatGPT.
AI gets better, but profit margins sink with strong competition.<p>Chinese AI startup DeepSeek overtakes ChatGPT on Apple App Store
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42839656">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42839656</a><p>Edit: Nvidia now -15% in Frankfurt.</div><br/><div id="42839791" class="c"><input type="checkbox" id="c-42839791" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42840339">next</a><span>|</span><label class="collapse" for="c-42839791">[-]</label><label class="expand" for="c-42839791">[37 more]</label></div><br/><div class="children"><div class="content">&gt; DeepSeek spends $6 million in old H800 hardware to develop open source model that overtakes ChatGPT.<p>DeepSeek <i>claims</i> that&#x27;s what they spent. They&#x27;re under a trade embargo, and if they had access to any more than that it would have been obtained illegally.<p>They might be telling the truth, but let&#x27;s wait until someone else replicates it before we fully accept it.</div><br/><div id="42840180" class="c"><input type="checkbox" id="c-42840180" checked=""/><div class="controls bullet"><span class="by">nabla9</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839791">parent</a><span>|</span><a href="#42840550">next</a><span>|</span><label class="collapse" for="c-42840180">[-]</label><label class="expand" for="c-42840180">[8 more]</label></div><br/><div class="children"><div class="content">Huggingface is currently replicating it.<p>Replications of small models indicate that they don&#x27;t lie  any significant amount. The architecture is cheap to train.<p>Berkeley Researchers Replicate DeepSeek R1&#x27;s Core Tech for Just $30: A Small Model RL Revolution <a href="https:&#x2F;&#x2F;xyzlabs.substack.com&#x2F;p&#x2F;berkeley-researchers-replicate-deepseek" rel="nofollow">https:&#x2F;&#x2F;xyzlabs.substack.com&#x2F;p&#x2F;berkeley-researchers-replicat...</a></div><br/><div id="42840555" class="c"><input type="checkbox" id="c-42840555" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840180">parent</a><span>|</span><a href="#42842101">next</a><span>|</span><label class="collapse" for="c-42840555">[-]</label><label class="expand" for="c-42840555">[2 more]</label></div><br/><div class="children"><div class="content">Whoah, that&#x27;s incredible!<p>I remember a year ago I was hoping that in a decade from now it would be great to run GPT4-class models on my own hardware. The reality seems to be far more exciting.</div><br/><div id="42847135" class="c"><input type="checkbox" id="c-42847135" checked=""/><div class="controls bullet"><span class="by">moritzwarhier</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840555">parent</a><span>|</span><a href="#42842101">next</a><span>|</span><label class="collapse" for="c-42847135">[-]</label><label class="expand" for="c-42847135">[1 more]</label></div><br/><div class="children"><div class="content">I first sneered at the idea of LLM generated LLM training sets, but is this what might be driving the big efficiency leap?<p>Asking as someone who honestly only superficially followed the developments since the end of 2023 or so</div><br/></div></div></div></div><div id="42842101" class="c"><input type="checkbox" id="c-42842101" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840180">parent</a><span>|</span><a href="#42840555">prev</a><span>|</span><a href="#42840550">next</a><span>|</span><label class="collapse" for="c-42842101">[-]</label><label class="expand" for="c-42842101">[5 more]</label></div><br/><div class="children"><div class="content">You call R1 a small model? It&#x27;s a 671-billion parameter model.</div><br/><div id="42842972" class="c"><input type="checkbox" id="c-42842972" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842101">parent</a><span>|</span><a href="#42840550">next</a><span>|</span><label class="collapse" for="c-42842972">[-]</label><label class="expand" for="c-42842972">[4 more]</label></div><br/><div class="children"><div class="content">There are multiple variations of the model starting from 1.5B parameters.</div><br/><div id="42843146" class="c"><input type="checkbox" id="c-42843146" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842972">parent</a><span>|</span><a href="#42843471">next</a><span>|</span><label class="collapse" for="c-42843146">[-]</label><label class="expand" for="c-42843146">[1 more]</label></div><br/><div class="children"><div class="content">Those are distillations of the model.</div><br/></div></div><div id="42843471" class="c"><input type="checkbox" id="c-42843471" checked=""/><div class="controls bullet"><span class="by">rsanek</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842972">parent</a><span>|</span><a href="#42843146">prev</a><span>|</span><a href="#42840550">next</a><span>|</span><label class="collapse" for="c-42843471">[-]</label><label class="expand" for="c-42843471">[2 more]</label></div><br/><div class="children"><div class="content">have you used those? in my experience even the 70B distillation is far worse than what you can expect from o1 &#x2F; the R1 available on the web</div><br/><div id="42844458" class="c"><input type="checkbox" id="c-42844458" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42843471">parent</a><span>|</span><a href="#42840550">next</a><span>|</span><label class="collapse" for="c-42844458">[-]</label><label class="expand" for="c-42844458">[1 more]</label></div><br/><div class="children"><div class="content">No, I haven&#x27;t. I&#x27;ve used Perplexity&#x27;s R1 but I don&#x27;t know how many parameters it has. It&#x27;s quite good, although too slow.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42840550" class="c"><input type="checkbox" id="c-42840550" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839791">parent</a><span>|</span><a href="#42840180">prev</a><span>|</span><a href="#42840457">next</a><span>|</span><label class="collapse" for="c-42840550">[-]</label><label class="expand" for="c-42840550">[12 more]</label></div><br/><div class="children"><div class="content">All of the western AI companies trained on illegally obtained data, they barely even bother to deny it. This is an industry where lies are normalised. (Not to contradict your point about this specific number)</div><br/><div id="42842456" class="c"><input type="checkbox" id="c-42842456" checked=""/><div class="controls bullet"><span class="by">pas</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840550">parent</a><span>|</span><a href="#42840457">next</a><span>|</span><label class="collapse" for="c-42842456">[-]</label><label class="expand" for="c-42842456">[11 more]</label></div><br/><div class="children"><div class="content">It&#x27;s legally a grey area. It might even be fair use. Facts themselves are not protected by copyright. If there&#x27;s no unauthorized reproduction&#x2F;copying then it&#x27;s not a copyright issue. (Maybe it&#x27;s a violation of terms of services of course.)</div><br/><div id="42843672" class="c"><input type="checkbox" id="c-42843672" checked=""/><div class="controls bullet"><span class="by">tivert</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842456">parent</a><span>|</span><a href="#42846042">next</a><span>|</span><label class="collapse" for="c-42843672">[-]</label><label class="expand" for="c-42843672">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Facts themselves are not protected by copyright.<p>But don&#x27;t LLMs encode language, not facts?<p>&gt; If there&#x27;s no unauthorized reproduction&#x2F;copying then it&#x27;s not a copyright issue.<p>I&#x27;m pretty sure copyright holders have gotten the models to regurgitate their copyright works verbatim, or nearly so.</div><br/><div id="42845163" class="c"><input type="checkbox" id="c-42845163" checked=""/><div class="controls bullet"><span class="by">MRtecno98</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42843672">parent</a><span>|</span><a href="#42845996">next</a><span>|</span><label class="collapse" for="c-42845163">[-]</label><label class="expand" for="c-42845163">[4 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t know what LLMs encode because we don&#x27;t know what the model weights represent.<p>On the second point it depends how the models were made to reporduce text verbatim. If i copy-paste someone&#x27;s article in MS word i technically made word reproduce the text verbatim., obviously that&#x27;s not Word&#x27;s fault. If i asked an LLM explicitly to list the entire Bee Movie script it would probably do it, which means it was trained on it, but that&#x27;s through a direct and clear request to copy the original verbatim.</div><br/><div id="42847821" class="c"><input type="checkbox" id="c-42847821" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42845163">parent</a><span>|</span><a href="#42849039">next</a><span>|</span><label class="collapse" for="c-42847821">[-]</label><label class="expand" for="c-42847821">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If i copy-paste someone&#x27;s article in MS word i technically made word reproduce the text verbatim., obviously that&#x27;s not Word&#x27;s fault. If i asked an LLM explicitly to list the entire Bee Movie script it would probably do it, which means it was trained on it, but that&#x27;s through a direct and clear request to copy the original verbatim.<p>But that clearly means that the LLM already has the Bee Movie script inside it (somehow), which would be a copyright violation. If MS word came with an &quot;open movie script&quot; button that let you pick a movie and get the script for it, that would clearly be a copyright violation. Of course if the user inputs something then that&#x27;s different - that&#x27;s not the software shipping whatever it is.</div><br/></div></div><div id="42849039" class="c"><input type="checkbox" id="c-42849039" checked=""/><div class="controls bullet"><span class="by">tivert</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42845163">parent</a><span>|</span><a href="#42847821">prev</a><span>|</span><a href="#42846065">next</a><span>|</span><label class="collapse" for="c-42849039">[-]</label><label class="expand" for="c-42849039">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If i asked an LLM explicitly to list the entire Bee Movie script it would probably do it, which means it was trained on it, but that&#x27;s through a direct and clear request to copy the original verbatim.<p>Huh? The &quot;request&quot; part doesn&#x27;t matter. What you describe is exactly like if someone ships me a hard drive with a file containing &quot;the entire Bee Movie script&quot; that they were not authorized to copy: it&#x27;s copyright infringement before <i>and</i> after I request the disk to read out the blocks with the file.</div><br/></div></div><div id="42846065" class="c"><input type="checkbox" id="c-42846065" checked=""/><div class="controls bullet"><span class="by">cdblades</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42845163">parent</a><span>|</span><a href="#42849039">prev</a><span>|</span><a href="#42845996">next</a><span>|</span><label class="collapse" for="c-42846065">[-]</label><label class="expand" for="c-42846065">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a fair comparison.  The user in the word example already had access to the infringing content to copy it, and then paste it into word.<p>But it has to have that copy, verbatim, to produce it, as you acknowledge.<p>If dropbox was hosting and serving IP from paramount, paramount would be able to submit a DCMA request to get that data removed.<p>Not only can you not submit a DMCA request to chatGPT, they can&#x27;t actually obey one.</div><br/></div></div></div></div><div id="42845996" class="c"><input type="checkbox" id="c-42845996" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42843672">parent</a><span>|</span><a href="#42845163">prev</a><span>|</span><a href="#42846042">next</a><span>|</span><label class="collapse" for="c-42845996">[-]</label><label class="expand" for="c-42845996">[1 more]</label></div><br/><div class="children"><div class="content">I mean, it is IP law, this stuff was all invented to help big corps support their business models. So, it is impossible to predict what any of it means until we see who is willing to pay more to get their desired laws enforced. We’ll have to wait for more precedent to be purchased before us little people can figure out what the laws are.</div><br/></div></div></div></div><div id="42846042" class="c"><input type="checkbox" id="c-42846042" checked=""/><div class="controls bullet"><span class="by">hackingonempty</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842456">parent</a><span>|</span><a href="#42843672">prev</a><span>|</span><a href="#42840457">next</a><span>|</span><label class="collapse" for="c-42846042">[-]</label><label class="expand" for="c-42846042">[4 more]</label></div><br/><div class="children"><div class="content">Copies are made in the formation of the training corpus and in the memory of the computers during training so there&#x27;s definitely a copyright issue.  Could be fair use though.</div><br/><div id="42846324" class="c"><input type="checkbox" id="c-42846324" checked=""/><div class="controls bullet"><span class="by">icedchai</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42846042">parent</a><span>|</span><a href="#42840457">next</a><span>|</span><label class="collapse" for="c-42846324">[-]</label><label class="expand" for="c-42846324">[3 more]</label></div><br/><div class="children"><div class="content">Is there also a copyright issue with search engines?</div><br/><div id="42846689" class="c"><input type="checkbox" id="c-42846689" checked=""/><div class="controls bullet"><span class="by">hackingonempty</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42846324">parent</a><span>|</span><a href="#42846501">next</a><span>|</span><label class="collapse" for="c-42846689">[-]</label><label class="expand" for="c-42846689">[1 more]</label></div><br/><div class="children"><div class="content">No, the DMCA amended the law to give search engines (and automated caches and user generated content sites) safe harbor from infringement if they follow the takedown protocol.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42840457" class="c"><input type="checkbox" id="c-42840457" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839791">parent</a><span>|</span><a href="#42840550">prev</a><span>|</span><a href="#42840026">next</a><span>|</span><label class="collapse" for="c-42840457">[-]</label><label class="expand" for="c-42840457">[10 more]</label></div><br/><div class="children"><div class="content">&gt;been obtained illegally.<p>PRC companies breaking US export control laws is legal (for PRC companies). Maybe they&#x27;re trying to avoid US entity listing, lot&#x27;s of PRC companies keep mum about growing capabilites to do so. But the mere fact Deepseek is publicizing means they&#x27;re unlikely to care about the political heat that is coming and the ramifications. If anything, getting on US entity list probably locks in their employees with Deepseek on resume into PRC.</div><br/><div id="42841116" class="c"><input type="checkbox" id="c-42841116" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840457">parent</a><span>|</span><a href="#42843179">next</a><span>|</span><label class="collapse" for="c-42841116">[-]</label><label class="expand" for="c-42841116">[8 more]</label></div><br/><div class="children"><div class="content">&gt; PRC companies breaking US export control laws is legal<p>So long as they don&#x27;t plan to do any business with the US or any of their allies I guess.</div><br/><div id="42842161" class="c"><input type="checkbox" id="c-42842161" checked=""/><div class="controls bullet"><span class="by">surgical_fire</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42841116">parent</a><span>|</span><a href="#42841877">next</a><span>|</span><label class="collapse" for="c-42842161">[-]</label><label class="expand" for="c-42842161">[3 more]</label></div><br/><div class="children"><div class="content">Which allies? The ones the current US president is threatening in all sorts of manner?<p>I actually hope he doubles down. I would love for EU to rely less on the US. It would also reduce the reach of the silly embargoes that benefit no one but the US.</div><br/><div id="42847592" class="c"><input type="checkbox" id="c-42847592" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842161">parent</a><span>|</span><a href="#42841877">next</a><span>|</span><label class="collapse" for="c-42847592">[-]</label><label class="expand" for="c-42847592">[2 more]</label></div><br/><div class="children"><div class="content">Destabilizing world trade and international relations isn&#x27;t something that anyone not named Trump or Putin should be hoping for.</div><br/><div id="42847973" class="c"><input type="checkbox" id="c-42847973" checked=""/><div class="controls bullet"><span class="by">surgical_fire</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42847592">parent</a><span>|</span><a href="#42841877">next</a><span>|</span><label class="collapse" for="c-42847973">[-]</label><label class="expand" for="c-42847973">[1 more]</label></div><br/><div class="children"><div class="content">Depends on how you think this would all play out.</div><br/></div></div></div></div></div></div><div id="42841877" class="c"><input type="checkbox" id="c-42841877" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42841116">parent</a><span>|</span><a href="#42842161">prev</a><span>|</span><a href="#42843397">next</a><span>|</span><label class="collapse" for="c-42841877">[-]</label><label class="expand" for="c-42841877">[2 more]</label></div><br/><div class="children"><div class="content">Hard to think they plan to, PRC strategic companies that gets competitive gets entity listed anyway. And CEO seems mission driven for AGI - if US going to limit hardware inevitably then nothing to do but go gloves off, and try to dunk on competition. At this point US can take deep seek off appstores but what&#x27;s the point except to look petty. Eitherway, more technical ppl have pointed out some of the R1 optimizations _only_ make sense if Deepseek was constrained to older hardware, i.e. engineer at PTX level to circumvent H800 limitations to perfrom more like H100s.<p>Throwing this model out also gives US allies soverign AI a launchpad... reducing US dependency is step 1 to not being US allies.</div><br/><div id="42846742" class="c"><input type="checkbox" id="c-42846742" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42841877">parent</a><span>|</span><a href="#42843397">next</a><span>|</span><label class="collapse" for="c-42846742">[-]</label><label class="expand" for="c-42846742">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Hard to think they plan to<p>They already are. You can make a paid account and use their API from most countries around the world. This is what doing business looks like.</div><br/></div></div></div></div><div id="42843397" class="c"><input type="checkbox" id="c-42843397" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42841116">parent</a><span>|</span><a href="#42841877">prev</a><span>|</span><a href="#42841459">next</a><span>|</span><label class="collapse" for="c-42843397">[-]</label><label class="expand" for="c-42843397">[1 more]</label></div><br/><div class="children"><div class="content">If they sell software and build devices in China and then people from the US or our allies have to break our laws to import it, it seems like an us problem.</div><br/></div></div><div id="42841459" class="c"><input type="checkbox" id="c-42841459" checked=""/><div class="controls bullet"><span class="by">sundaeofshock</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42841116">parent</a><span>|</span><a href="#42843397">prev</a><span>|</span><a href="#42843179">next</a><span>|</span><label class="collapse" for="c-42841459">[-]</label><label class="expand" for="c-42841459">[1 more]</label></div><br/><div class="children"><div class="content">This may not be that much of a moat, as Trump seems committed to turning US current allies into former allies.</div><br/></div></div></div></div><div id="42843179" class="c"><input type="checkbox" id="c-42843179" checked=""/><div class="controls bullet"><span class="by">mytailorisrich</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840457">parent</a><span>|</span><a href="#42841116">prev</a><span>|</span><a href="#42840026">next</a><span>|</span><label class="collapse" for="c-42843179">[-]</label><label class="expand" for="c-42843179">[1 more]</label></div><br/><div class="children"><div class="content">Depending on how the law is written this may be legal even under US law.<p>For instance if the law bans US companies from exporting&#x2F;selling some chips to Chinese companies and that&#x27;s it then it is unclear to me whether a Chinese company would do anything illegal under US law by buying such chips as it would be for the American seller to refuse.<p>Anyway, usually this sort of things takes place through intermediaries in third countries so it is difficult to track but obviously it would be stupid to brag about it if that happened.</div><br/></div></div></div></div><div id="42840026" class="c"><input type="checkbox" id="c-42840026" checked=""/><div class="controls bullet"><span class="by">sekai</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839791">parent</a><span>|</span><a href="#42840457">prev</a><span>|</span><a href="#42839976">next</a><span>|</span><label class="collapse" for="c-42840026">[-]</label><label class="expand" for="c-42840026">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They might be telling the truth, but let&#x27;s wait until someone else replicates it before we fully accept it.<p>Truth and CCP controlled companies are an oxymoron.</div><br/></div></div><div id="42839976" class="c"><input type="checkbox" id="c-42839976" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839791">parent</a><span>|</span><a href="#42840026">prev</a><span>|</span><a href="#42839973">next</a><span>|</span><label class="collapse" for="c-42839976">[-]</label><label class="expand" for="c-42839976">[1 more]</label></div><br/><div class="children"><div class="content">There are already some (limited) reproductions that suggest they&#x27;re not completely lying (ie that there are indeed perf benefits).</div><br/></div></div><div id="42839973" class="c"><input type="checkbox" id="c-42839973" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839791">parent</a><span>|</span><a href="#42839976">prev</a><span>|</span><a href="#42840339">next</a><span>|</span><label class="collapse" for="c-42839973">[-]</label><label class="expand" for="c-42839973">[4 more]</label></div><br/><div class="children"><div class="content">They have a lot of H100: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;NVDA_Stock&#x2F;comments&#x2F;1iadc0s&#x2F;evidence_that_h100_nvidia_gpus_are_in_china_dated&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;NVDA_Stock&#x2F;comments&#x2F;1iadc0s&#x2F;evidenc...</a></div><br/><div id="42840104" class="c"><input type="checkbox" id="c-42840104" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839973">parent</a><span>|</span><a href="#42840339">next</a><span>|</span><label class="collapse" for="c-42840104">[-]</label><label class="expand" for="c-42840104">[3 more]</label></div><br/><div class="children"><div class="content">four GPUs are very convincing indeed! :D</div><br/><div id="42840364" class="c"><input type="checkbox" id="c-42840364" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840104">parent</a><span>|</span><a href="#42840169">next</a><span>|</span><label class="collapse" for="c-42840364">[-]</label><label class="expand" for="c-42840364">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s 8 (not 4), on a NVIDIA platform board to start with.<p>You can&#x27;t buy them as &quot;GPU&quot;s and integrate them to your system. NVIDIA sells your the platform (GPUs + platform board which includes switches and all the support infra), and you integrate that behemoth of a board to your server, as a single unit.<p>So that open server and the wrapped ones at the back are more telling than it looks.</div><br/></div></div><div id="42840169" class="c"><input type="checkbox" id="c-42840169" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840104">parent</a><span>|</span><a href="#42840364">prev</a><span>|</span><a href="#42840339">next</a><span>|</span><label class="collapse" for="c-42840169">[-]</label><label class="expand" for="c-42840169">[1 more]</label></div><br/><div class="children"><div class="content">You missed the black unwrapped boxes in the background....</div><br/></div></div></div></div></div></div></div></div><div id="42840339" class="c"><input type="checkbox" id="c-42840339" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42839791">prev</a><span>|</span><a href="#42839933">next</a><span>|</span><label class="collapse" for="c-42840339">[-]</label><label class="expand" for="c-42840339">[22 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a very strange result.<p>I believe that NVIDIA is overvalued, but if DeepSeek really is as great as has been said, then it&#x27;ll be even greater when scaled up to OpenAI sizes, and when you get more out you have more reason to pay, so this should if it pans out lead to more demand for GPUs-- basically Jevon&#x27;s paradox.</div><br/><div id="42840576" class="c"><input type="checkbox" id="c-42840576" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840339">parent</a><span>|</span><a href="#42841660">next</a><span>|</span><label class="collapse" for="c-42840576">[-]</label><label class="expand" for="c-42840576">[1 more]</label></div><br/><div class="children"><div class="content">If the top-tier premium GPUs aren&#x27;t the difference-maker they were thought to be then that will hurt NVIDIA&#x27;s margins, even if they make some of it up on volume.</div><br/></div></div><div id="42841660" class="c"><input type="checkbox" id="c-42841660" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840339">parent</a><span>|</span><a href="#42840576">prev</a><span>|</span><a href="#42840560">next</a><span>|</span><label class="collapse" for="c-42841660">[-]</label><label class="expand" for="c-42841660">[7 more]</label></div><br/><div class="children"><div class="content">You need to be prepared for the reality that naive scaling no longer works for LLMs anymore.<p>Simple question: where is GPT-5?</div><br/><div id="42842529" class="c"><input type="checkbox" id="c-42842529" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42841660">parent</a><span>|</span><a href="#42841925">next</a><span>|</span><label class="collapse" for="c-42842529">[-]</label><label class="expand" for="c-42842529">[3 more]</label></div><br/><div class="children"><div class="content">It is a possibility, but my understanding of what OpenAI has said is that GPT-5 is delayed because of the apparent promise of RL trained things like o1, etc. and that they&#x27;ve simply decided to train those instead of training a bigger base model training on better data, and I think this is plausible.</div><br/><div id="42845333" class="c"><input type="checkbox" id="c-42845333" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842529">parent</a><span>|</span><a href="#42841925">next</a><span>|</span><label class="collapse" for="c-42845333">[-]</label><label class="expand" for="c-42845333">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI has an incentive to make people believe that the scaling laws are still alive, to justify their enormous capex if nothing else.<p>I wouldn&#x27;t give what they say to much credence, and will only believe the results I see.</div><br/><div id="42846333" class="c"><input type="checkbox" id="c-42846333" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42845333">parent</a><span>|</span><a href="#42841925">next</a><span>|</span><label class="collapse" for="c-42846333">[-]</label><label class="expand" for="c-42846333">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I think I agree that it seems unlikely that the spending they&#x27;re doing can be recouped.<p>But it can still make sense for a state, even if it doesn&#x27;t make sense for investors though.</div><br/></div></div></div></div></div></div><div id="42841925" class="c"><input type="checkbox" id="c-42841925" checked=""/><div class="controls bullet"><span class="by">Jlagreen</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42841660">parent</a><span>|</span><a href="#42842529">prev</a><span>|</span><a href="#42844427">next</a><span>|</span><label class="collapse" for="c-42841925">[-]</label><label class="expand" for="c-42841925">[1 more]</label></div><br/><div class="children"><div class="content">If we expect that the demand for GPT-5 in AI compute is 100x of that of GPT-4 then if GPT-4 was trained in months on 10k of H100 then you would need years with 100k of H100 or maybe again months with 100k of GB200.<p>See, there is your answer. The issue is the compute of GPUs is way to low yet for GPT-5 if they continue parameter scaling as they used to do.<p>GPT3 took months on 10k A100s. 10k H100 would have done it in a fraction of a time. Blackwell could train GPT4 in 10 days with same amount of GPUs as Hopper which took months.<p>Don&#x27;t forget GPT3 is just 2.5 years old. Training is obviously waiting for the next step up in large clusters of training speed increasement. Don&#x27;t be fooled, the 2x Blackwell vs. Hopper is only chip vs. chip. 10k of Blackwell including all networking speedup is easily 10x or more faster than the same amount of Hopper. So building a 1 million Blackwell cluster means 100x more training compute compared to a 100k Hopper cluster.<p>Nobody starts a model training if it takes years to finish... too much risk in that.<p>Transfer model was introduced in 2017 and ChatGPT came out 2022. Why? Because they would have needed millions of Volta GPUs instead of thousands of Ampere GPUs to train it.</div><br/></div></div><div id="42844427" class="c"><input type="checkbox" id="c-42844427" checked=""/><div class="controls bullet"><span class="by">r00fus</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42841660">parent</a><span>|</span><a href="#42841925">prev</a><span>|</span><a href="#42840560">next</a><span>|</span><label class="collapse" for="c-42844427">[-]</label><label class="expand" for="c-42844427">[2 more]</label></div><br/><div class="children"><div class="content">There is a theory that Deepseek gives based on it&#x27;s distillation process that hints towards, that o1 is really a distillation of a bigger GPT (GPT5?).<p>Some consider this to be spurious&#x2F;conspiracy.</div><br/><div id="42846407" class="c"><input type="checkbox" id="c-42846407" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42844427">parent</a><span>|</span><a href="#42840560">next</a><span>|</span><label class="collapse" for="c-42846407">[-]</label><label class="expand" for="c-42846407">[1 more]</label></div><br/><div class="children"><div class="content">There is a big model from NVIDIA that I assume is for this purpose, i.e. Megatron 530b, so it doesn&#x27;t sound too unreasonable.<p>Edit: I assumed that the model was distillation, that is apparently not true.</div><br/></div></div></div></div></div></div><div id="42840560" class="c"><input type="checkbox" id="c-42840560" checked=""/><div class="controls bullet"><span class="by">franktankbank</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840339">parent</a><span>|</span><a href="#42841660">prev</a><span>|</span><a href="#42844928">next</a><span>|</span><label class="collapse" for="c-42840560">[-]</label><label class="expand" for="c-42840560">[3 more]</label></div><br/><div class="children"><div class="content">The architecture doesn&#x27;t keep yielding better results, Jevon&#x27;s paradox doesn&#x27;t apply.</div><br/><div id="42840574" class="c"><input type="checkbox" id="c-42840574" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840560">parent</a><span>|</span><a href="#42840853">next</a><span>|</span><label class="collapse" for="c-42840574">[-]</label><label class="expand" for="c-42840574">[1 more]</label></div><br/><div class="children"><div class="content">But surely it can be scaled up, or is this compression thing something making the approach good only for small models (I haven&#x27;t read the Deepseek papers (can&#x27;t allocate time to it))?</div><br/></div></div><div id="42840853" class="c"><input type="checkbox" id="c-42840853" checked=""/><div class="controls bullet"><span class="by">joak</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840560">parent</a><span>|</span><a href="#42840574">prev</a><span>|</span><a href="#42844928">next</a><span>|</span><label class="collapse" for="c-42840853">[-]</label><label class="expand" for="c-42840853">[1 more]</label></div><br/><div class="children"><div class="content">Anyhow, if you can deliver more with less, this is huge good news for AI industry.<p>After some readjustment we can expect AI companies to start using the new method to deliver more. Science fiction might happen sooner than expected.<p>Buy the dip.</div><br/></div></div></div></div><div id="42844928" class="c"><input type="checkbox" id="c-42844928" checked=""/><div class="controls bullet"><span class="by">r00fus</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840339">parent</a><span>|</span><a href="#42840560">prev</a><span>|</span><a href="#42840624">next</a><span>|</span><label class="collapse" for="c-42844928">[-]</label><label class="expand" for="c-42844928">[1 more]</label></div><br/><div class="children"><div class="content">For Oracle (another Stargate recipient) it was reversion to the mean.  For Nvidia, it&#x27;s a big loss - I imagine they might have predicated their revenue based on the continued need for compute - and now that&#x27;s in question.</div><br/></div></div><div id="42840624" class="c"><input type="checkbox" id="c-42840624" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840339">parent</a><span>|</span><a href="#42844928">prev</a><span>|</span><a href="#42839933">next</a><span>|</span><label class="collapse" for="c-42840624">[-]</label><label class="expand" for="c-42840624">[9 more]</label></div><br/><div class="children"><div class="content">The limit is high quality data, not compute.</div><br/><div id="42841208" class="c"><input type="checkbox" id="c-42841208" checked=""/><div class="controls bullet"><span class="by">vicentwu</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840624">parent</a><span>|</span><a href="#42842027">next</a><span>|</span><label class="collapse" for="c-42841208">[-]</label><label class="expand" for="c-42841208">[1 more]</label></div><br/><div class="children"><div class="content">RL doesn&#x27;t need that much static data, it needs a lot of &quot;good&quot; tasks&#x2F;challenges and computation.</div><br/></div></div><div id="42842027" class="c"><input type="checkbox" id="c-42842027" checked=""/><div class="controls bullet"><span class="by">grajaganDev</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840624">parent</a><span>|</span><a href="#42841208">prev</a><span>|</span><a href="#42840755">next</a><span>|</span><label class="collapse" for="c-42842027">[-]</label><label class="expand" for="c-42842027">[6 more]</label></div><br/><div class="children"><div class="content">Right and LLMs will not be able to generate their own high quality training data.<p>There are no perpetual motion machines.</div><br/><div id="42843576" class="c"><input type="checkbox" id="c-42843576" checked=""/><div class="controls bullet"><span class="by">throw310822</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842027">parent</a><span>|</span><a href="#42844274">next</a><span>|</span><label class="collapse" for="c-42843576">[-]</label><label class="expand" for="c-42843576">[4 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs will not be able to generate their own high quality training data.<p>Humans certainly did. We did not inherit our physics and poetry books from some aliens.</div><br/><div id="42843745" class="c"><input type="checkbox" id="c-42843745" checked=""/><div class="controls bullet"><span class="by">grajaganDev</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42843576">parent</a><span>|</span><a href="#42844886">next</a><span>|</span><label class="collapse" for="c-42843745">[-]</label><label class="expand" for="c-42843745">[1 more]</label></div><br/><div class="children"><div class="content">Humans and LLMs are different things.<p>LLMs can not reason - many people seen to believe that they can.</div><br/></div></div><div id="42844886" class="c"><input type="checkbox" id="c-42844886" checked=""/><div class="controls bullet"><span class="by">gatlin</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42843576">parent</a><span>|</span><a href="#42843745">prev</a><span>|</span><a href="#42845196">next</a><span>|</span><label class="collapse" for="c-42844886">[-]</label><label class="expand" for="c-42844886">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t prove that we did but I don&#x27;t know that we &#x2F;didn&#x27;t&#x2F;.</div><br/></div></div><div id="42845196" class="c"><input type="checkbox" id="c-42845196" checked=""/><div class="controls bullet"><span class="by">MRtecno98</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42843576">parent</a><span>|</span><a href="#42844886">prev</a><span>|</span><a href="#42844274">next</a><span>|</span><label class="collapse" for="c-42845196">[-]</label><label class="expand" for="c-42845196">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are not humans, nowhere near.</div><br/></div></div></div></div><div id="42844274" class="c"><input type="checkbox" id="c-42844274" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42842027">parent</a><span>|</span><a href="#42843576">prev</a><span>|</span><a href="#42840755">next</a><span>|</span><label class="collapse" for="c-42844274">[-]</label><label class="expand" for="c-42844274">[1 more]</label></div><br/><div class="children"><div class="content">They already do. All the current leading edge models are heavily trained on synthetic data. It&#x27;s called textbook learning.</div><br/></div></div></div></div><div id="42840755" class="c"><input type="checkbox" id="c-42840755" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42840624">parent</a><span>|</span><a href="#42842027">prev</a><span>|</span><a href="#42839933">next</a><span>|</span><label class="collapse" for="c-42840755">[-]</label><label class="expand" for="c-42840755">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The limit is high quality data<p>If, as some companies claim, these models truly possess emergent reasoning, their ability to handle imperfect data should serve as a proof of that capability.</div><br/></div></div></div></div></div></div><div id="42839933" class="c"><input type="checkbox" id="c-42839933" checked=""/><div class="controls bullet"><span class="by">chad1n</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42840339">prev</a><span>|</span><a href="#42839837">next</a><span>|</span><label class="collapse" for="c-42839933">[-]</label><label class="expand" for="c-42839933">[1 more]</label></div><br/><div class="children"><div class="content">This is not exactly right, they said they spent $6M on training V3, there aren&#x27;t numbers out there related to the training of R1, I can feel it will be cheaper than o1, but it&#x27;s hard to tell how much cheaper. I can guess that overall deepseek spent way less than openai to release the model, because I have the feeling that the R&amp;D part was cheaper too, but we don&#x27;t have the numbers yet. Anyway, we can assume that deepseek and Alibaba will try to get the most out of their current GPUs however.</div><br/></div></div><div id="42839837" class="c"><input type="checkbox" id="c-42839837" checked=""/><div class="controls bullet"><span class="by">negamax</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42839933">prev</a><span>|</span><a href="#42839823">next</a><span>|</span><label class="collapse" for="c-42839837">[-]</label><label class="expand" for="c-42839837">[3 more]</label></div><br/><div class="children"><div class="content">The bigger correction will be in tech stocks that are overly exposed to datacenter investments to accommodate for ever rising AI demands. MSFT, AMZN, META they are all exposed</div><br/><div id="42839861" class="c"><input type="checkbox" id="c-42839861" checked=""/><div class="controls bullet"><span class="by">myth_drannon</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839837">parent</a><span>|</span><a href="#42839823">next</a><span>|</span><label class="collapse" for="c-42839861">[-]</label><label class="expand" for="c-42839861">[2 more]</label></div><br/><div class="children"><div class="content">MSFT is down 7%</div><br/><div id="42842931" class="c"><input type="checkbox" id="c-42842931" checked=""/><div class="controls bullet"><span class="by">voidfunc</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839861">parent</a><span>|</span><a href="#42839823">next</a><span>|</span><label class="collapse" for="c-42842931">[-]</label><label class="expand" for="c-42842931">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kind of silly. It&#x27;s not like MSFT and the other hyper-scalers dont need the capacity build out for other reasons too. This should be an easy pivot if DeepSeek turns out to be as good as promised.</div><br/></div></div></div></div></div></div><div id="42839823" class="c"><input type="checkbox" id="c-42839823" checked=""/><div class="controls bullet"><span class="by">nolok</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42839837">prev</a><span>|</span><a href="#42839895">next</a><span>|</span><label class="collapse" for="c-42839823">[-]</label><label class="expand" for="c-42839823">[2 more]</label></div><br/><div class="children"><div class="content">They were massively overhyped though, it feels more like a correction (and a partial one at that) than a fall.</div><br/><div id="42840583" class="c"><input type="checkbox" id="c-42840583" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839823">parent</a><span>|</span><a href="#42839895">next</a><span>|</span><label class="collapse" for="c-42840583">[-]</label><label class="expand" for="c-42840583">[1 more]</label></div><br/><div class="children"><div class="content">Of course they are overhyped but in spite of this Altman is always asking for more money. And we know financially they are just burning money. So when someone finally brings a cheap but good model for the masses, this is where money should go. (This will also help all small AI startups.)</div><br/></div></div></div></div><div id="42839895" class="c"><input type="checkbox" id="c-42839895" checked=""/><div class="controls bullet"><span class="by">scrlk</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42839823">prev</a><span>|</span><a href="#42840395">next</a><span>|</span><label class="collapse" for="c-42839895">[-]</label><label class="expand" for="c-42839895">[1 more]</label></div><br/><div class="children"><div class="content">Not a good day for those who decided to hold 3x Nvidia ETPs - down 40% earlier.</div><br/></div></div><div id="42840395" class="c"><input type="checkbox" id="c-42840395" checked=""/><div class="controls bullet"><span class="by">qwytw</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42839895">prev</a><span>|</span><a href="#42841702">next</a><span>|</span><label class="collapse" for="c-42840395">[-]</label><label class="expand" for="c-42840395">[1 more]</label></div><br/><div class="children"><div class="content">&gt; overtakes ChatGPT<p>That&#x27;s arguable, though. I mean it&#x27;s much cheaper and reasonably competitive which is almost the same but IMHO DeepSeek seems to get stuck in random loops and hallucinates more frequently than o1.</div><br/></div></div><div id="42841702" class="c"><input type="checkbox" id="c-42841702" checked=""/><div class="controls bullet"><span class="by">mgh2</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42840395">prev</a><span>|</span><a href="#42839970">next</a><span>|</span><label class="collapse" for="c-42841702">[-]</label><label class="expand" for="c-42841702">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s see how the open vs close ecosystem wins in AI</div><br/></div></div><div id="42839970" class="c"><input type="checkbox" id="c-42839970" checked=""/><div class="controls bullet"><span class="by">kvgr</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42841702">prev</a><span>|</span><a href="#42839779">next</a><span>|</span><label class="collapse" for="c-42839970">[-]</label><label class="expand" for="c-42839970">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if its as hot as it looks. Valuation of those private AIs is stupid.
This looks like Chinese propaganda. Say we dont need that hardware, but it looks like they need it.
<a href="https:&#x2F;&#x2F;wccftech.com&#x2F;chinese-ai-lab-deepseek-has-50000-nvidia-h100-ai-gpus-says-ai-ceo&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wccftech.com&#x2F;chinese-ai-lab-deepseek-has-50000-nvidi...</a></div><br/></div></div><div id="42839779" class="c"><input type="checkbox" id="c-42839779" checked=""/><div class="controls bullet"><span class="by">gostsamo</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42839970">prev</a><span>|</span><a href="#42840744">next</a><span>|</span><label class="collapse" for="c-42839779">[-]</label><label class="expand" for="c-42839779">[11 more]</label></div><br/><div class="children"><div class="content">Consider that the chinese might be misrepresenting their costs. A newsletter was implying that they might do it to undermine the sanctions justifications.<p>Agree that the AI bubble should pop though and the earlier, the better.</div><br/><div id="42839808" class="c"><input type="checkbox" id="c-42839808" checked=""/><div class="controls bullet"><span class="by">nabla9</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839779">parent</a><span>|</span><a href="#42840744">next</a><span>|</span><label class="collapse" for="c-42839808">[-]</label><label class="expand" for="c-42839808">[10 more]</label></div><br/><div class="children"><div class="content">Their model is open and they published paper describing it <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2412.19437" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2412.19437</a> The  can&#x27;t be far off, or it would be noticed.<p>Even if they are heavily government subsidized for energy and hardware, I don see how the cost of training in the US would be more than double.</div><br/><div id="42839931" class="c"><input type="checkbox" id="c-42839931" checked=""/><div class="controls bullet"><span class="by">bythreads</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839808">parent</a><span>|</span><a href="#42839832">next</a><span>|</span><label class="collapse" for="c-42839931">[-]</label><label class="expand" for="c-42839931">[2 more]</label></div><br/><div class="children"><div class="content">Doesnt that say they based it on llama?, sooooo not really a bottoms up training - since the cost of llama is 100% surely not part of their quote.</div><br/><div id="42840192" class="c"><input type="checkbox" id="c-42840192" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839931">parent</a><span>|</span><a href="#42839832">next</a><span>|</span><label class="collapse" for="c-42840192">[-]</label><label class="expand" for="c-42840192">[1 more]</label></div><br/><div class="children"><div class="content">I did a quick search for &quot;llama&quot; and didn&#x27;t find anywhere they outright state they just fine-tuned some llama weights.<p>Is it possible that they based their model architecture on the llama model architecture? Rather than just fine-tuned already training llama weights? In that case, they&#x27;d still have to do &quot;bottoms up&quot; training.</div><br/></div></div></div></div><div id="42839832" class="c"><input type="checkbox" id="c-42839832" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839808">parent</a><span>|</span><a href="#42839931">prev</a><span>|</span><a href="#42839824">next</a><span>|</span><label class="collapse" for="c-42839832">[-]</label><label class="expand" for="c-42839832">[3 more]</label></div><br/><div class="children"><div class="content">People on the internet can lie. Especially when such a lie could cause the nasdaq to dip multiple percent points.<p>Not saying they are lying, but there incentives.</div><br/><div id="42840131" class="c"><input type="checkbox" id="c-42840131" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839832">parent</a><span>|</span><a href="#42839867">next</a><span>|</span><label class="collapse" for="c-42840131">[-]</label><label class="expand" for="c-42840131">[1 more]</label></div><br/><div class="children"><div class="content">Much easier to identify the incentives of the people who just lost a lot of money who were betting on the idea that it was their money that was going to make artificial intelligence intelligent.<p>Everyone’s already begun trying this recipe in-house.  Either it works with much less compute, or it doesn’t.<p>For instance, HKUST just did an experiment where small weak base models trained with DeepSeek’s method beat stronger small base models being trained with much more costly RL methods. Already this seems like it is enough to upend the low end models niche market, things like haiku and 4o-mini.<p>Be really skeptical why the people who should be making tons of money by realizing actually it was all a mirage and that they can now get the real stuff for even cheaper, would spend so much effort shouting about this, in order to undercut their own profitability..</div><br/></div></div><div id="42839867" class="c"><input type="checkbox" id="c-42839867" checked=""/><div class="controls bullet"><span class="by">42lux</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839832">parent</a><span>|</span><a href="#42840131">prev</a><span>|</span><a href="#42839824">next</a><span>|</span><label class="collapse" for="c-42839867">[-]</label><label class="expand" for="c-42839867">[1 more]</label></div><br/><div class="children"><div class="content">Huggingface is reproducing it live on their blog…</div><br/></div></div></div></div><div id="42839824" class="c"><input type="checkbox" id="c-42839824" checked=""/><div class="controls bullet"><span class="by">imdsm</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839808">parent</a><span>|</span><a href="#42839832">prev</a><span>|</span><a href="#42839834">next</a><span>|</span><label class="collapse" for="c-42839824">[-]</label><label class="expand" for="c-42839824">[2 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s wait for reproduction first</div><br/><div id="42845034" class="c"><input type="checkbox" id="c-42845034" checked=""/><div class="controls bullet"><span class="by">jredwards</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839824">parent</a><span>|</span><a href="#42839834">next</a><span>|</span><label class="collapse" for="c-42845034">[-]</label><label class="expand" for="c-42845034">[1 more]</label></div><br/><div class="children"><div class="content">Posted elsewhere: <a href="https:&#x2F;&#x2F;xyzlabs.substack.com&#x2F;p&#x2F;berkeley-researchers-replicate-deepseek" rel="nofollow">https:&#x2F;&#x2F;xyzlabs.substack.com&#x2F;p&#x2F;berkeley-researchers-replicat...</a></div><br/></div></div></div></div><div id="42839834" class="c"><input type="checkbox" id="c-42839834" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839808">parent</a><span>|</span><a href="#42839824">prev</a><span>|</span><a href="#42840744">next</a><span>|</span><label class="collapse" for="c-42839834">[-]</label><label class="expand" for="c-42839834">[2 more]</label></div><br/><div class="children"><div class="content">They express their cost in terms of GPU hours, then convert that to USD based on market GPU rental rates, so it&#x27;s not affected by subsidies. It&#x27;s possible however they lied about GPU hours, but if that was the case an expert should be able to show they lied by working out how many flops are needed to train based on the amount of tokens they say they used vs the flops of the GPUs they say they used.</div><br/><div id="42840397" class="c"><input type="checkbox" id="c-42840397" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#42839705">root</a><span>|</span><a href="#42839834">parent</a><span>|</span><a href="#42840744">next</a><span>|</span><label class="collapse" for="c-42840397">[-]</label><label class="expand" for="c-42840397">[1 more]</label></div><br/><div class="children"><div class="content">Total training FLOPs can be deduced from model architecture (which they can&#x27;t hide since they released weights) and how many tokens they trained on. With total training FLOPs and GPU hours you can calculate MFU. And the MFU of their deepseek-v3 train is around 40%, which sounds right. Both Google and Meta reported higher MFU. So the GPU hours should be correct. The only thing they could have lied is on how many tokens they trained the model on. DeepSeek reported 14T which is also similar to what Meta did so nothing crazy here.<p>tl;dr all numbers check up and the winnings come from the model architecture innovations they made.</div><br/></div></div></div></div></div></div></div></div><div id="42840744" class="c"><input type="checkbox" id="c-42840744" checked=""/><div class="controls bullet"><span class="by">SebFender</span><span>|</span><a href="#42839705">parent</a><span>|</span><a href="#42839779">prev</a><span>|</span><a href="#42839945">next</a><span>|</span><label class="collapse" for="c-42840744">[-]</label><label class="expand" for="c-42840744">[1 more]</label></div><br/><div class="children"><div class="content">Taking out the popcorn early today.</div><br/></div></div></div></div><div id="42846162" class="c"><input type="checkbox" id="c-42846162" checked=""/><div class="controls bullet"><span class="by">h1h1hh1h1h1</span><span>|</span><a href="#42839705">prev</a><span>|</span><a href="#42840529">next</a><span>|</span><label class="collapse" for="c-42846162">[-]</label><label class="expand" for="c-42846162">[38 more]</label></div><br/><div class="children"><div class="content">So the Chinese graciously gift a paper and model which describes methods that radically increase the efficiency of hardware which will allow US AI firms to create much better models due to having significantly more AI hardware and people are bearish on US AI now?</div><br/><div id="42846210" class="c"><input type="checkbox" id="c-42846210" checked=""/><div class="controls bullet"><span class="by">bcrosby95</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846560">next</a><span>|</span><label class="collapse" for="c-42846210">[-]</label><label class="expand" for="c-42846210">[10 more]</label></div><br/><div class="children"><div class="content">If people are bullish on Nvidia because the hot new thing requires tons of Nvidia hardware and someone releases a paper showing you need 1&#x2F;45th of Nvidia&#x27;s hardware to get the same results, of course there&#x27;s going to be pullback.<p>Whether its justified or not is outside my wheelhouse.  There&#x27;s too many &quot;it depends&quot; involved that, best case, only people working in the field can answer, worst case, no one can answer right now.</div><br/><div id="42846390" class="c"><input type="checkbox" id="c-42846390" checked=""/><div class="controls bullet"><span class="by">pokstad</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846210">parent</a><span>|</span><a href="#42846437">next</a><span>|</span><label class="collapse" for="c-42846390">[-]</label><label class="expand" for="c-42846390">[8 more]</label></div><br/><div class="children"><div class="content">Or you could argue you can now do 45x greater things with the same hardware. You can take an optimistic stance on this.</div><br/><div id="42846484" class="c"><input type="checkbox" id="c-42846484" checked=""/><div class="controls bullet"><span class="by">cortesoft</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846390">parent</a><span>|</span><a href="#42846468">next</a><span>|</span><label class="collapse" for="c-42846484">[-]</label><label class="expand" for="c-42846484">[6 more]</label></div><br/><div class="children"><div class="content">For the overall economy, sure... for Nvidia, no<p>A huge increase in fuel efficiency is great for the economy, horrible for fuel companies</div><br/><div id="42848982" class="c"><input type="checkbox" id="c-42848982" checked=""/><div class="controls bullet"><span class="by">asdff</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846484">parent</a><span>|</span><a href="#42846897">next</a><span>|</span><label class="collapse" for="c-42848982">[-]</label><label class="expand" for="c-42848982">[1 more]</label></div><br/><div class="children"><div class="content">Because most people&#x27;s trips are the commute and they haven&#x27;t been given more time and money to go and road trip more. That isn&#x27;t analogous to computing though. People do the same things broadly they&#x27;ve always had with computing, but we&#x27;ve figured out how to create a system where your computer today running microsoft word is 100x as powerful as your computer in 1995 also running microsoft word and you feel the need to upgrade your hardware every couple of years so you can continue running microsoft word. It is the perfect model for exponentially dumping raw compute power into the void to perpetuate value creation. It will not stop in our lifetimes I expect. In 25 years our computers will be 100x more powerful still and we will still have MS word.</div><br/></div></div><div id="42846897" class="c"><input type="checkbox" id="c-42846897" checked=""/><div class="controls bullet"><span class="by">mullingitover</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846484">parent</a><span>|</span><a href="#42848982">prev</a><span>|</span><a href="#42846808">next</a><span>|</span><label class="collapse" for="c-42846897">[-]</label><label class="expand" for="c-42846897">[2 more]</label></div><br/><div class="children"><div class="content">Nvidia isn&#x27;t the fuel company. They&#x27;re the auto manufacturer.</div><br/><div id="42847282" class="c"><input type="checkbox" id="c-42847282" checked=""/><div class="controls bullet"><span class="by">axus</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846897">parent</a><span>|</span><a href="#42846808">next</a><span>|</span><label class="collapse" for="c-42847282">[-]</label><label class="expand" for="c-42847282">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they are the gas station</div><br/></div></div></div></div><div id="42846808" class="c"><input type="checkbox" id="c-42846808" checked=""/><div class="controls bullet"><span class="by">devit</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846484">parent</a><span>|</span><a href="#42846897">prev</a><span>|</span><a href="#42846876">next</a><span>|</span><label class="collapse" for="c-42846808">[-]</label><label class="expand" for="c-42846808">[1 more]</label></div><br/><div class="children"><div class="content">That depends on how whether the demand increase multiplier due to the lower cost per result is lower or higher that the efficiency increase multiplier. It can be either in general.</div><br/></div></div><div id="42846876" class="c"><input type="checkbox" id="c-42846876" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846484">parent</a><span>|</span><a href="#42846808">prev</a><span>|</span><a href="#42846468">next</a><span>|</span><label class="collapse" for="c-42846876">[-]</label><label class="expand" for="c-42846876">[1 more]</label></div><br/><div class="children"><div class="content">Most of the time, a large increase in fuel efficiency is great for fuel companies, and a huge increase means a temporary bump before an even greater future.</div><br/></div></div></div></div><div id="42846468" class="c"><input type="checkbox" id="c-42846468" checked=""/><div class="controls bullet"><span class="by">c0redump</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846390">parent</a><span>|</span><a href="#42846484">prev</a><span>|</span><a href="#42846437">next</a><span>|</span><label class="collapse" for="c-42846468">[-]</label><label class="expand" for="c-42846468">[1 more]</label></div><br/><div class="children"><div class="content">Except it’s not clear at all that this is actually the case. It’s entirely conjecture on your part.</div><br/></div></div></div></div></div></div><div id="42846560" class="c"><input type="checkbox" id="c-42846560" checked=""/><div class="controls bullet"><span class="by">linkregister</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846210">prev</a><span>|</span><a href="#42846205">next</a><span>|</span><label class="collapse" for="c-42846560">[-]</label><label class="expand" for="c-42846560">[5 more]</label></div><br/><div class="children"><div class="content">&gt; the Chinese<p>Daya Guo, Dejian Yang, Haowei Zhang, et.al., quant researchers at High Flyer, a hedge fund based in China, open-sourced their work on a chain-of-thought reasoning model, based on Qwen and LLama (open source LLMs).<p>It would be somewhat bizarre to describe Meta&#x27;s open sourcing of LLama as &quot;the Americans gifting a model&quot;, despite Meta having a corporate headquarters in the United States.</div><br/><div id="42846840" class="c"><input type="checkbox" id="c-42846840" checked=""/><div class="controls bullet"><span class="by">culi</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846560">parent</a><span>|</span><a href="#42846205">next</a><span>|</span><label class="collapse" for="c-42846840">[-]</label><label class="expand" for="c-42846840">[4 more]</label></div><br/><div class="children"><div class="content">Thank you. The amount of casual sinophobia allowed on hackernews has been a real turn off. I find myself avoiding threads like these in anticipation of these comments</div><br/><div id="42850022" class="c"><input type="checkbox" id="c-42850022" checked=""/><div class="controls bullet"><span class="by">beefnugs</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846840">parent</a><span>|</span><a href="#42850056">next</a><span>|</span><label class="collapse" for="c-42850022">[-]</label><label class="expand" for="c-42850022">[2 more]</label></div><br/><div class="children"><div class="content">Nah, its about the &quot;party&quot; not the people or culture. They will never shake the stigma now that they fist their way into controlling any company, creating artificial market manipulation, restricting technology, restricting information,  violence and threats against their own people and everyone else.</div><br/><div id="42850094" class="c"><input type="checkbox" id="c-42850094" checked=""/><div class="controls bullet"><span class="by">rightbyte</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42850022">parent</a><span>|</span><a href="#42850056">next</a><span>|</span><label class="collapse" for="c-42850094">[-]</label><label class="expand" for="c-42850094">[1 more]</label></div><br/><div class="children"><div class="content">A lot of people seem to lose their minds if they hear what could be interpreted as agitation versus an ethnical out group.<p>I think it might be a bad idea to use an nationality or ethnicity to mean the government.</div><br/></div></div></div></div><div id="42850056" class="c"><input type="checkbox" id="c-42850056" checked=""/><div class="controls bullet"><span class="by">rightbyte</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846840">parent</a><span>|</span><a href="#42850022">prev</a><span>|</span><a href="#42846205">next</a><span>|</span><label class="collapse" for="c-42850056">[-]</label><label class="expand" for="c-42850056">[1 more]</label></div><br/><div class="children"><div class="content">Jingoism seems like something that was always lurking beneath the surface waiting to emerge.</div><br/></div></div></div></div></div></div><div id="42846205" class="c"><input type="checkbox" id="c-42846205" checked=""/><div class="controls bullet"><span class="by">magic_hamster</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846560">prev</a><span>|</span><a href="#42846335">next</a><span>|</span><label class="collapse" for="c-42846205">[-]</label><label class="expand" for="c-42846205">[6 more]</label></div><br/><div class="children"><div class="content">I think the idea that SOTA models can run on limited hardware makes people think that Nvidia sales will take a hit.<p>But if you think about it for two more seconds you realize that if SOTA was trained on mid level hardware, top of the line hardware could still put you ahead, and DeepSeek is also open source so it won&#x27;t take long to see what this architecture could do on high end cards.</div><br/><div id="42846380" class="c"><input type="checkbox" id="c-42846380" checked=""/><div class="controls bullet"><span class="by">amazingamazing</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846205">parent</a><span>|</span><a href="#42848512">next</a><span>|</span><label class="collapse" for="c-42846380">[-]</label><label class="expand" for="c-42846380">[4 more]</label></div><br/><div class="children"><div class="content">there&#x27;s no reason to believe that performance will continue to scale with compute, though. <i>that&#x27;s</i> why there&#x27;s a rout. more simply, if you assume maximum performance with the current LLM&#x2F;transformer architecture is say, twice as good as what humanity is capable of now, then that would mean that you&#x27;re approaching 50%+ performance with orders of magnitude less compute. there&#x27;s just no way you could justify the amount of money being spent on nvidia cards if that&#x27;s true, hence the selloff.</div><br/><div id="42846485" class="c"><input type="checkbox" id="c-42846485" checked=""/><div class="controls bullet"><span class="by">futureshock</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846380">parent</a><span>|</span><a href="#42848512">next</a><span>|</span><label class="collapse" for="c-42846485">[-]</label><label class="expand" for="c-42846485">[3 more]</label></div><br/><div class="children"><div class="content">Wait no, there is actually PLENTY of evidence that performance continues to scale with more compute. The entire point of the o3 announcement and benchmark results of throwing a million bucks of test time compute at ARC-AGI is that the ceiling is really really high. We have 3 verified scaling laws of pre-training corpus size, parameter count, and test time compute. More efficiency is fantastic progress, but we will always be able to get more intelligence by spending more. Scale is all you need. DeepSeek did not disprove that.</div><br/><div id="42846499" class="c"><input type="checkbox" id="c-42846499" checked=""/><div class="controls bullet"><span class="by">amazingamazing</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846485">parent</a><span>|</span><a href="#42848512">next</a><span>|</span><label class="collapse" for="c-42846499">[-]</label><label class="expand" for="c-42846499">[2 more]</label></div><br/><div class="children"><div class="content">there&#x27;s evidence that performance increases with compute, but not that it <i>scales</i> with compute, e.g. linearly or exponentially. the SOTA models already are seeing diminishing returns w.r.t parameter size, training time and generally just engineering effort. it&#x27;s a fact that doubling, say, parameter size does not double benchmark performance.<p>would love to see evidence to the contrary. my assertion comes from seeing claude, gemini and o1.<p>if anything I feel performance is more of a function of the quality of data than anything else.</div><br/><div id="42847356" class="c"><input type="checkbox" id="c-42847356" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846499">parent</a><span>|</span><a href="#42848512">next</a><span>|</span><label class="collapse" for="c-42847356">[-]</label><label class="expand" for="c-42847356">[1 more]</label></div><br/><div class="children"><div class="content">The biggest increase in model performance recently came from training them to do chain-of-thought properly - that is <i>why</i> DeepSeek is as good as it is. This requires a lot more tokens for the model to reason, though. Which means that it needs a lot more compute to do its thing even if it doesn&#x27;t have a massive increase in parameter size.</div><br/></div></div></div></div></div></div></div></div><div id="42848512" class="c"><input type="checkbox" id="c-42848512" checked=""/><div class="controls bullet"><span class="by">kibibu</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846205">parent</a><span>|</span><a href="#42846380">prev</a><span>|</span><a href="#42846335">next</a><span>|</span><label class="collapse" for="c-42848512">[-]</label><label class="expand" for="c-42848512">[1 more]</label></div><br/><div class="children"><div class="content">&gt; DeepSeek is also open source so it won&#x27;t take long to see what this architecture could do on high end cards<p>As far as I can see, the training code isn&#x27;t open source. It&#x27;s open weights.</div><br/></div></div></div></div><div id="42846335" class="c"><input type="checkbox" id="c-42846335" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846205">prev</a><span>|</span><a href="#42846217">next</a><span>|</span><label class="collapse" for="c-42846335">[-]</label><label class="expand" for="c-42846335">[1 more]</label></div><br/><div class="children"><div class="content">You can be bullish about US AI but at the same time not believe that the industry is worth $10T+ right now.</div><br/></div></div><div id="42846217" class="c"><input type="checkbox" id="c-42846217" checked=""/><div class="controls bullet"><span class="by">DiscourseFan</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846335">prev</a><span>|</span><a href="#42846250">next</a><span>|</span><label class="collapse" for="c-42846217">[-]</label><label class="expand" for="c-42846217">[7 more]</label></div><br/><div class="children"><div class="content">No, because what this implies is that the Chinese have better labor power in the tech-sector than the US, considering how much more efficient this technology is. Which means that even if US companies adopt these practices, the best workers will still be in China, communicating largely in Chinese, building relationships with other Chinese-speaking people purchasing chinese speaking labor. These relationships are already present. It would be difficult for OpenAI to catch up.</div><br/><div id="42846388" class="c"><input type="checkbox" id="c-42846388" checked=""/><div class="controls bullet"><span class="by">brokencode</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846217">parent</a><span>|</span><a href="#42847010">next</a><span>|</span><label class="collapse" for="c-42846388">[-]</label><label class="expand" for="c-42846388">[5 more]</label></div><br/><div class="children"><div class="content">What a stretch. One Chinese model makes a breakthrough in efficiency and suddenly China has all the best people in the world?<p>What about all the people who invented LLMs and all the necessary hardware here in the US? What about all the models that leapfrog each other in the US every few months?<p>One breakthrough implies that they had a great idea and implemented it well. It doesn’t imply anything more than that.</div><br/><div id="42846488" class="c"><input type="checkbox" id="c-42846488" checked=""/><div class="controls bullet"><span class="by">jonatron</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846388">parent</a><span>|</span><a href="#42847340">next</a><span>|</span><label class="collapse" for="c-42846488">[-]</label><label class="expand" for="c-42846488">[2 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t say about how good they are, but over 400,000 CS graduates in China [1] per year sounds like a lot.
<a href="https:&#x2F;&#x2F;www.ctol.digital&#x2F;news&#x2F;chinas-it-boom-slows-computer-science-popularity-plunges-amid-shrinking-job-market&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ctol.digital&#x2F;news&#x2F;chinas-it-boom-slows-computer-...</a></div><br/><div id="42847075" class="c"><input type="checkbox" id="c-42847075" checked=""/><div class="controls bullet"><span class="by">jessekv</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846488">parent</a><span>|</span><a href="#42847340">next</a><span>|</span><label class="collapse" for="c-42847075">[-]</label><label class="expand" for="c-42847075">[1 more]</label></div><br/><div class="children"><div class="content">The ones I work with are very good :)</div><br/></div></div></div></div><div id="42847340" class="c"><input type="checkbox" id="c-42847340" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846388">parent</a><span>|</span><a href="#42846488">prev</a><span>|</span><a href="#42846583">next</a><span>|</span><label class="collapse" for="c-42847340">[-]</label><label class="expand" for="c-42847340">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not just one model, though. DeepSeek is the hot story today but Qwen&#x27;s QwQ also punched above its weight.</div><br/></div></div><div id="42846583" class="c"><input type="checkbox" id="c-42846583" checked=""/><div class="controls bullet"><span class="by">tokioyoyo</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846388">parent</a><span>|</span><a href="#42847340">prev</a><span>|</span><a href="#42847010">next</a><span>|</span><label class="collapse" for="c-42846583">[-]</label><label class="expand" for="c-42846583">[1 more]</label></div><br/><div class="children"><div class="content">Chinese tech companies are also investing into AI. DeepSeek team isn&#x27;t the only one (and probably the least funded one?) within mainland. This is mostly a challenge to the &quot;American AI is yeas ahead&quot; illusion, and a show that maybe investing only in American companies isn&#x27;t the smartest method, as others might beat them in their own game.</div><br/></div></div></div></div><div id="42847010" class="c"><input type="checkbox" id="c-42847010" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846217">parent</a><span>|</span><a href="#42846388">prev</a><span>|</span><a href="#42846250">next</a><span>|</span><label class="collapse" for="c-42847010">[-]</label><label class="expand" for="c-42847010">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>the best workers will still be in China</i><p>This is quite an assumption.<p>But <i>the majority</i> of the AI R&amp;D may be in China, with a high barrier for participation for outsiders, leading to an increasing gap. Whether this is so is not obvious.</div><br/></div></div></div></div><div id="42846250" class="c"><input type="checkbox" id="c-42846250" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846217">prev</a><span>|</span><a href="#42847166">next</a><span>|</span><label class="collapse" for="c-42846250">[-]</label><label class="expand" for="c-42846250">[1 more]</label></div><br/><div class="children"><div class="content">Not the AI proper, but the need for <i>additional</i> AI hardware down the line. Especially the super-expensive, high-margin, huge AI hardware that DeepSeek seems not to require.<p>Similarly, microcomputers led to an explosion of computer market, but definitely limited the market for mainframe behemoths.</div><br/></div></div><div id="42847166" class="c"><input type="checkbox" id="c-42847166" checked=""/><div class="controls bullet"><span class="by">tester756</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846250">prev</a><span>|</span><a href="#42846301">next</a><span>|</span><label class="collapse" for="c-42847166">[-]</label><label class="expand" for="c-42847166">[1 more]</label></div><br/><div class="children"><div class="content">US thought they were unparalleled and years ahead due to $$$ injection and HW,<p>but got caught by 200 people Chinese team that preferred clever approach instead of &quot;let&#x27;s put as much compute as we can on it&quot;</div><br/></div></div><div id="42846301" class="c"><input type="checkbox" id="c-42846301" checked=""/><div class="controls bullet"><span class="by">paulddraper</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42847166">prev</a><span>|</span><a href="#42846203">next</a><span>|</span><label class="collapse" for="c-42846301">[-]</label><label class="expand" for="c-42846301">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;investing&#x2F;comments&#x2F;1ib5vf9&#x2F;deepseek_uses_nvidias_h800_chips_so_why_are&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;investing&#x2F;comments&#x2F;1ib5vf9&#x2F;deepseek...</a><p>They did it by using H800 chips, not H100 or B200 or anything crazy.<p>This means NVIDIA may not be the only game in town.<p>E.g. Chinese manufacturers.</div><br/></div></div><div id="42846203" class="c"><input type="checkbox" id="c-42846203" checked=""/><div class="controls bullet"><span class="by">baal80spam</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846301">prev</a><span>|</span><a href="#42846245">next</a><span>|</span><label class="collapse" for="c-42846203">[-]</label><label class="expand" for="c-42846203">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t look for logic in the market, I suppose.</div><br/></div></div><div id="42846245" class="c"><input type="checkbox" id="c-42846245" checked=""/><div class="controls bullet"><span class="by">bilbo0s</span><span>|</span><a href="#42846162">parent</a><span>|</span><a href="#42846203">prev</a><span>|</span><a href="#42840529">next</a><span>|</span><label class="collapse" for="c-42846245">[-]</label><label class="expand" for="c-42846245">[4 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s probably more accurate to say that people are now a bit more bullish on what the Chinese will be able to accomplish even in the face of trade restrictions. Now whether or not it makes sense to be bearish on US AI is a totally different issue.<p>Personally I think being bearish on US AI makes zero sense. I&#x27;m almost positive there will be restrictions on using Chinese models forthcoming in the near to medium term. I&#x27;m not saying those restrictions will make sense. I&#x27;m just saying they will steer people in the US market towards US offerings.</div><br/><div id="42846940" class="c"><input type="checkbox" id="c-42846940" checked=""/><div class="controls bullet"><span class="by">mullingitover</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846245">parent</a><span>|</span><a href="#42846308">next</a><span>|</span><label class="collapse" for="c-42846940">[-]</label><label class="expand" for="c-42846940">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m almost positive there will be restrictions on using Chinese models forthcoming in the near to medium term.<p>If the models are open source, there are constitutional issues that would prevent restricting them unless we&#x27;re going down the ridiculous path of classifying integers representing algorithms as munitions, like we tried with crypto.</div><br/></div></div><div id="42846308" class="c"><input type="checkbox" id="c-42846308" checked=""/><div class="controls bullet"><span class="by">paulddraper</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846245">parent</a><span>|</span><a href="#42846940">prev</a><span>|</span><a href="#42840529">next</a><span>|</span><label class="collapse" for="c-42846308">[-]</label><label class="expand" for="c-42846308">[2 more]</label></div><br/><div class="children"><div class="content">US AI is only somewhat related though.<p>The subject is NVIDIA.</div><br/><div id="42846403" class="c"><input type="checkbox" id="c-42846403" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#42846162">root</a><span>|</span><a href="#42846308">parent</a><span>|</span><a href="#42840529">next</a><span>|</span><label class="collapse" for="c-42846403">[-]</label><label class="expand" for="c-42846403">[1 more]</label></div><br/><div class="children"><div class="content">I think the market perception of NVidia’s value is currently heavily driven by the expected demand for datacenter chips following anticipated trendlines of the big US AI firms; I think DeepSeek disrupted that (I think when the implications of greater value per unit of compute applied to AI are realized, it will end up being seen as beneficial to the GPU market in general and, barring a big challenge appearing in the very near future, NVidia specifically, but I think that&#x27;s a slower process.)</div><br/></div></div></div></div></div></div></div></div><div id="42840529" class="c"><input type="checkbox" id="c-42840529" checked=""/><div class="controls bullet"><span class="by">reacharavindh</span><span>|</span><a href="#42846162">prev</a><span>|</span><a href="#42849038">next</a><span>|</span><label class="collapse" for="c-42840529">[-]</label><label class="expand" for="c-42840529">[2 more]</label></div><br/><div class="children"><div class="content">What I’d like to know is..
If a good model can be trained with much fewer GPUs using a breakthrough technique, can the breakthrough technique be used by OpenAI, MSFT et al who has loads of GPUs to train a model that is orders of magnitude better than their state of the art today?<p>We’ve been getting the impression that the limiting factor was the number of GPUs right? If so, this reduces that bottleneck and frees them up to do even better right?</div><br/><div id="42840608" class="c"><input type="checkbox" id="c-42840608" checked=""/><div class="controls bullet"><span class="by">ReptileMan</span><span>|</span><a href="#42840529">parent</a><span>|</span><a href="#42849038">next</a><span>|</span><label class="collapse" for="c-42840608">[-]</label><label class="expand" for="c-42840608">[1 more]</label></div><br/><div class="children"><div class="content">From my understanding the limiting factor is the quantity and quality of data available for training.</div><br/></div></div></div></div><div id="42849038" class="c"><input type="checkbox" id="c-42849038" checked=""/><div class="controls bullet"><span class="by">thw09j9m</span><span>|</span><a href="#42840529">prev</a><span>|</span><a href="#42846918">next</a><span>|</span><label class="collapse" for="c-42849038">[-]</label><label class="expand" for="c-42849038">[4 more]</label></div><br/><div class="children"><div class="content">I think the big picture that many people are missing here is the motivation that all these AI&#x2F;tech companies have for buying up so many GPU&#x27;s in the first place: achieving AGI&#x2F;ASI.<p>And while some still try to portray a dedication&#x2F;duty to AI Alignment, I think most have either secretly or more publicly moved away from it in the race to become the first to achieve it.<p>And I think, given that inference time compute is so much cheaper than pre-training, the first to achieve AGI might have enough compute on hand from having been to first to build it that they would not need to purchase many more GPU&#x27;s from Nvidia. So at some point, Nvidia&#x27;s revenues are going to decline precipitously.<p>So the question is: how far away are we from AGI? Seems like most experts estimate 3-10 years. Did that timeline just shrink by 50x (or at least by some multiple) from these new optimizations from DeepSeek?<p>I think Nvidia&#x27;s revenue is going to continue to grow at its current pace (or faster) until AGI is achieved. Just sell your positions right before that happens.<p>(Not an expert. Not even an amateur)</div><br/><div id="42849092" class="c"><input type="checkbox" id="c-42849092" checked=""/><div class="controls bullet"><span class="by">YZF</span><span>|</span><a href="#42849038">parent</a><span>|</span><a href="#42849049">next</a><span>|</span><label class="collapse" for="c-42849092">[-]</label><label class="expand" for="c-42849092">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t feel like DeepSeek has a big enough breakthrough here. This is just one of many optimizations we&#x27;re going to see over the next years. How close this brings us to &quot;AGI&quot; is a complete unknown.<p>The large investments were mainly for training larger foundation models, or at the very least hedging for that. It hasn&#x27;t been that clear over the last 1+ years that simply increasing the number of parameters continues to lead to the same improvements we&#x27;ve seen before.<p>Markets do not necessarily have any prediction power here. People were spooked by DeepSeek getting ahead of the competition and by the costs they report. There is still a lot of work and some of it may still require brute force and more resources (this seems to be true for training the foundation models still).</div><br/></div></div><div id="42849049" class="c"><input type="checkbox" id="c-42849049" checked=""/><div class="controls bullet"><span class="by">klodolph</span><span>|</span><a href="#42849038">parent</a><span>|</span><a href="#42849092">prev</a><span>|</span><a href="#42849443">next</a><span>|</span><label class="collapse" for="c-42849049">[-]</label><label class="expand" for="c-42849049">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Seems like most experts estimate 3-10 years.<p>That’s spitting distance from “we don’t know”.<p>I think most of these valuations are made by people without the expertise to predict what the next one year of AI looks like, never mind 3-10 years.</div><br/></div></div><div id="42849443" class="c"><input type="checkbox" id="c-42849443" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#42849038">parent</a><span>|</span><a href="#42849049">prev</a><span>|</span><a href="#42846918">next</a><span>|</span><label class="collapse" for="c-42849443">[-]</label><label class="expand" for="c-42849443">[1 more]</label></div><br/><div class="children"><div class="content">Not that I believe it&#x27;s likely to happen, but it seems incredibly fucking dangerous for there to be an ASI race with one winner. To the extent these companies believe it&#x27;s possible, what are they hoping will be the outcome for humanity?<p>That they get to be the trillionaires with an untouchable moat? Wouldn&#x27;t this be like creating a Kwisitz Hadarach thinking you can control it, to borrow a Dune reference?</div><br/></div></div></div></div><div id="42846918" class="c"><input type="checkbox" id="c-42846918" checked=""/><div class="controls bullet"><span class="by">TheAlchemist</span><span>|</span><a href="#42849038">prev</a><span>|</span><a href="#42845138">next</a><span>|</span><label class="collapse" for="c-42846918">[-]</label><label class="expand" for="c-42846918">[18 more]</label></div><br/><div class="children"><div class="content">Nvidia valuation = $3000B<p>Nvidia profits during AI madness last year = $65B (last 4 quarters)<p>Nvidia profits during normal year = $5B<p>This stock could drop 90% from here, and still be expensive. The numbers are absolutely crazy and make no sense at all.<p>Stargate project is aiming to invest $500B over 4 years. Those $500B are a pipe dream, but let&#x27;s suppose for a second that all of that $500B will be Nvidia profits and that we will have another Stargate project in 4 years, again resulting in a direct profits of $500B for Nvidia.<p>And you know what ? In that scenario Nvidia would STILL be overvalued by historical standards !<p>EDIT: Changed $30B from the fiscal year, to $65B for last 4 quarters.</div><br/><div id="42847303" class="c"><input type="checkbox" id="c-42847303" checked=""/><div class="controls bullet"><span class="by">gitfan86</span><span>|</span><a href="#42846918">parent</a><span>|</span><a href="#42847009">next</a><span>|</span><label class="collapse" for="c-42847303">[-]</label><label class="expand" for="c-42847303">[2 more]</label></div><br/><div class="children"><div class="content">Your numbers are wrong. The last quarter profit was $19B and the projected profit is 21B next quarter. That is 84B&#x2F;year profit with zero growth.<p>If META,STARGATE,xAI,etc.. all increased spending rapidly you could get to 200B profit rate in 2026.<p>3T &#x2F; 200B = 15<p>That means they could return a 6.6 dividend which is higher than 10 year bonds and is in no way overvalued by historial standards.<p>All that to say, I sold all my NVDA last month because I don&#x27;t think Wall St. is buying that everyone is going to actually invest that much.</div><br/><div id="42847643" class="c"><input type="checkbox" id="c-42847643" checked=""/><div class="controls bullet"><span class="by">TheAlchemist</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847303">parent</a><span>|</span><a href="#42847009">next</a><span>|</span><label class="collapse" for="c-42847643">[-]</label><label class="expand" for="c-42847643">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right about the profit - I took last fiscal year. Still, it doesn&#x27;t change anything in what I wrote.<p>What you just wrote is &quot;IF the biggest companies on earth, and the US government decide to spend all of their money on a single chip maker, then you could get to 200B profit rate in 2026&quot;. I won&#x27;t disagree with that.</div><br/></div></div></div></div><div id="42847009" class="c"><input type="checkbox" id="c-42847009" checked=""/><div class="controls bullet"><span class="by">epicureanideal</span><span>|</span><a href="#42846918">parent</a><span>|</span><a href="#42847303">prev</a><span>|</span><a href="#42847335">next</a><span>|</span><label class="collapse" for="c-42847009">[-]</label><label class="expand" for="c-42847009">[1 more]</label></div><br/><div class="children"><div class="content">Exactly.<p>Also, what he’s saying is if 500&#x2F;4 =125B were NVDA yearly profits (and of course really that would just be revenue,  not profit), it’d still mean NVDA should be more like 1875B market cap at a more reasonable 15x price to earnings ratio.  If I understand the previous poster correctly.</div><br/></div></div><div id="42847335" class="c"><input type="checkbox" id="c-42847335" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42846918">parent</a><span>|</span><a href="#42847009">prev</a><span>|</span><a href="#42847073">next</a><span>|</span><label class="collapse" for="c-42847335">[-]</label><label class="expand" for="c-42847335">[2 more]</label></div><br/><div class="children"><div class="content">&gt; And you know what ? In that scenario Nvidia would STILL be overvalued by historical standards !<p>It would makes it&#x27;s PE ratio about 24 (3000&#x2F;(500&#x2F;4)).<p>AAPL&#x27;s PE is about 36 at this moment.<p>So no, not &quot;overvalued by historical standards&quot;.</div><br/><div id="42847396" class="c"><input type="checkbox" id="c-42847396" checked=""/><div class="controls bullet"><span class="by">rsanek</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847335">parent</a><span>|</span><a href="#42847073">next</a><span>|</span><label class="collapse" for="c-42847396">[-]</label><label class="expand" for="c-42847396">[1 more]</label></div><br/><div class="children"><div class="content">If you take a longer time frame, these PE ratios are indeed pretty high. Even 24 is much higher than the ~15 historical median <a href="https:&#x2F;&#x2F;www.multpl.com&#x2F;s-p-500-pe-ratio" rel="nofollow">https:&#x2F;&#x2F;www.multpl.com&#x2F;s-p-500-pe-ratio</a></div><br/></div></div></div></div><div id="42847073" class="c"><input type="checkbox" id="c-42847073" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#42846918">parent</a><span>|</span><a href="#42847335">prev</a><span>|</span><a href="#42847108">next</a><span>|</span><label class="collapse" for="c-42847073">[-]</label><label class="expand" for="c-42847073">[5 more]</label></div><br/><div class="children"><div class="content">but you forgot the 80B from MSFT and 60B from META in 2025 alone? I assume quite a chunk of that investment will be going to NVDA.<p>Stargate wise, it&#x27;s a joke, they have no money for their lip service.</div><br/><div id="42847100" class="c"><input type="checkbox" id="c-42847100" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847073">parent</a><span>|</span><a href="#42847108">next</a><span>|</span><label class="collapse" for="c-42847100">[-]</label><label class="expand" for="c-42847100">[4 more]</label></div><br/><div class="children"><div class="content">Most of it will go towards energy. These companies are building literal power plants to power their data centers.</div><br/><div id="42847123" class="c"><input type="checkbox" id="c-42847123" checked=""/><div class="controls bullet"><span class="by">SteveNuts</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847100">parent</a><span>|</span><a href="#42847108">next</a><span>|</span><label class="collapse" for="c-42847123">[-]</label><label class="expand" for="c-42847123">[3 more]</label></div><br/><div class="children"><div class="content">&gt;These companies are building literal power plants to power their data centers.<p>Have any companies publicly announced doing that, let alone actually started the process of building?</div><br/><div id="42847199" class="c"><input type="checkbox" id="c-42847199" checked=""/><div class="controls bullet"><span class="by">iamdeedubs</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847123">parent</a><span>|</span><a href="#42847108">next</a><span>|</span><label class="collapse" for="c-42847199">[-]</label><label class="expand" for="c-42847199">[2 more]</label></div><br/><div class="children"><div class="content">One example of restarting a previously shuttered nuclear power plant to power a data center.<p><a href="https:&#x2F;&#x2F;www.technologyreview.com&#x2F;2024&#x2F;09&#x2F;26&#x2F;1104516&#x2F;three-mile-island-microsoft&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.technologyreview.com&#x2F;2024&#x2F;09&#x2F;26&#x2F;1104516&#x2F;three-mi...</a><p>Although the fine print is that it will be dumping power into the grid  to be pulled out by various DCs vs powering them directly</div><br/><div id="42847234" class="c"><input type="checkbox" id="c-42847234" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847199">parent</a><span>|</span><a href="#42847108">next</a><span>|</span><label class="collapse" for="c-42847234">[-]</label><label class="expand" for="c-42847234">[1 more]</label></div><br/><div class="children"><div class="content">There is an ongoing debate about these companies drawing direct power from private plants vs going through the grid, but I can&#x27;t see why big tech won&#x27;t win in the end, especially in today&#x27;s environment of deregulation.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42847108" class="c"><input type="checkbox" id="c-42847108" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#42846918">parent</a><span>|</span><a href="#42847073">prev</a><span>|</span><a href="#42847033">next</a><span>|</span><label class="collapse" for="c-42847108">[-]</label><label class="expand" for="c-42847108">[2 more]</label></div><br/><div class="children"><div class="content">Agreed, it&#x27;s very strange to me that this is being framed in the media as &quot;CHINESE DEEPSEEK DESTROYS US STOCK MARKET&quot;. It&#x27;s a good model, but not so wildly good that it suddenly destroys Nvidia somehow. The stock was (and remains) incredibly overvalued.<p>This smells a lot to me like someone with deep pockets is looking to get a bailout by framing this as some kind of Chinese threat.<p>Any time the entire media suddenly agrees on a very strange framing of a story you  should immediately be suspicious.</div><br/><div id="42847120" class="c"><input type="checkbox" id="c-42847120" checked=""/><div class="controls bullet"><span class="by">dmix</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847108">parent</a><span>|</span><a href="#42847033">next</a><span>|</span><label class="collapse" for="c-42847120">[-]</label><label class="expand" for="c-42847120">[1 more]</label></div><br/><div class="children"><div class="content">The vibes market</div><br/></div></div></div></div><div id="42847033" class="c"><input type="checkbox" id="c-42847033" checked=""/><div class="controls bullet"><span class="by">outside1234</span><span>|</span><a href="#42846918">parent</a><span>|</span><a href="#42847108">prev</a><span>|</span><a href="#42847132">next</a><span>|</span><label class="collapse" for="c-42847033">[-]</label><label class="expand" for="c-42847033">[2 more]</label></div><br/><div class="children"><div class="content">The Stargate project does not actual exist.  It is just something they completely made up by lumping together data center investments from across the industry and rounded up to the nearest $500B</div><br/><div id="42847121" class="c"><input type="checkbox" id="c-42847121" checked=""/><div class="controls bullet"><span class="by">xbgaf</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847033">parent</a><span>|</span><a href="#42847132">next</a><span>|</span><label class="collapse" for="c-42847121">[-]</label><label class="expand" for="c-42847121">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, perhaps the plan was to pump certain stocks after they saw the unprecedented post election rally of Tesla and Bitcoin.<p>And these people complain about the Pelosi index, which is peanuts in comparison.</div><br/></div></div></div></div><div id="42847132" class="c"><input type="checkbox" id="c-42847132" checked=""/><div class="controls bullet"><span class="by">breadwinner</span><span>|</span><a href="#42846918">parent</a><span>|</span><a href="#42847033">prev</a><span>|</span><a href="#42845138">next</a><span>|</span><label class="collapse" for="c-42847132">[-]</label><label class="expand" for="c-42847132">[3 more]</label></div><br/><div class="children"><div class="content">Are you suggesting &quot;AI madness&quot; is a one time event that will not continue? AI is dramatically improving productivity and not just for developers. The &quot;madness&quot; will continue until every job is fully automated. That requires a lot of chips.</div><br/><div id="42847268" class="c"><input type="checkbox" id="c-42847268" checked=""/><div class="controls bullet"><span class="by">TheAlchemist</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847132">parent</a><span>|</span><a href="#42845138">next</a><span>|</span><label class="collapse" for="c-42847268">[-]</label><label class="expand" for="c-42847268">[2 more]</label></div><br/><div class="children"><div class="content">&quot;AI&quot;, once we get there, is indeed the future and will continue to play an increasing role everywhere.<p>But what happened to Nvidia profits last year is a one time event and will get back to normal sooner or later.</div><br/><div id="42847381" class="c"><input type="checkbox" id="c-42847381" checked=""/><div class="controls bullet"><span class="by">breadwinner</span><span>|</span><a href="#42846918">root</a><span>|</span><a href="#42847268">parent</a><span>|</span><a href="#42845138">next</a><span>|</span><label class="collapse" for="c-42847381">[-]</label><label class="expand" for="c-42847381">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>last year is a one time event</i><p>It is repeating this year as well, given all the announcements. So maybe it is a two time event? I think it could repeat every year as long as Nvidia continues to innovate and keep their lead.<p>DeepSeek is providing an efficiency boost, and that doesn&#x27;t kill Nvidia. In fact, Nvidia themselves delivered an efficiency boost with Blackwell chips. DeepSeek is a one-time efficiency boost, but Nvidia will continue to boost efficiency every year, based on Moore&#x27;s law.</div><br/></div></div></div></div></div></div></div></div><div id="42845138" class="c"><input type="checkbox" id="c-42845138" checked=""/><div class="controls bullet"><span class="by">scoofy</span><span>|</span><a href="#42846918">prev</a><span>|</span><a href="#42846650">next</a><span>|</span><label class="collapse" for="c-42845138">[-]</label><label class="expand" for="c-42845138">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been into investing for my entire adult life, and base my strategy mostly on John Neff&#x27;s work on total return.<p>I have missed out on a lot of investments in the QE period, because many of them seem like &quot;if this mid-level company becomes the biggest company in the world, you&#x27;ll make a reasonable return,&quot; which has always seemed insane to me, but we&#x27;ve seen it happen again and again. I realize that we are probably in a place where insider trading is much more prevalent that we expect, and that the <i>point</i> of an IPO has been turned on it&#x27;s head, but these type of potential blowups of high PE stocks is something I&#x27;ve never really come to terms with.</div><br/></div></div><div id="42846650" class="c"><input type="checkbox" id="c-42846650" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#42845138">prev</a><span>|</span><a href="#42849495">next</a><span>|</span><label class="collapse" for="c-42846650">[-]</label><label class="expand" for="c-42846650">[2 more]</label></div><br/><div class="children"><div class="content">I found the Stratechery and Nadella&#x27;s takes to be interesting.  Also the fact that news orgs say things like:<p>&quot;The situation is particularly remarkable since, as a Chinese company, DeepSeek lacks access to Nvidia’s state-of-the-art chips used to train AI models powering chatbots like ChatGPT.&quot;<p>Whereas they don&#x27;t mention the fact that Deepseek still used Nvidia chips.  news orgs are implying they didn&#x27;t.<p>Stratechery points out that the reduced memory&#x2F;communications bandwidth of the gimped H800 chips they have access to drove the MOE&#x2F;MLA architecture developments to make their model possible on the less powerful chips.<p>Nadella (on X) points out that by Jevons paradox [1], AI usage (and NVidia chip usage) will increase because deepseeks has reduced costs.<p>One other point Statechery made is that DeepSeek likely distilled the output of leading models for V3 and R1.  They have shown that they can replicate the leaders  cheaply and quickly, but they can&#x27;t produce a leading model without copying (yet).<p>[1] <a href="https:&#x2F;&#x2F;x.com&#x2F;satyanadella&#x2F;status&#x2F;1883753899255046301" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;satyanadella&#x2F;status&#x2F;1883753899255046301</a></div><br/><div id="42848859" class="c"><input type="checkbox" id="c-42848859" checked=""/><div class="controls bullet"><span class="by">flashman</span><span>|</span><a href="#42846650">parent</a><span>|</span><a href="#42849495">next</a><span>|</span><label class="collapse" for="c-42848859">[-]</label><label class="expand" for="c-42848859">[1 more]</label></div><br/><div class="children"><div class="content">what people are neglecting about Jevons is that fuel efficiency led to increased use of coal <i>because you could use coal to displace more expensive fuels</i><p>i wonder what is more expensive that cheap large language models can displace? is the problem with selling unreliable B.S. really its price?</div><br/></div></div></div></div><div id="42849495" class="c"><input type="checkbox" id="c-42849495" checked=""/><div class="controls bullet"><span class="by">shahzaibmushtaq</span><span>|</span><a href="#42846650">prev</a><span>|</span><a href="#42839752">next</a><span>|</span><label class="collapse" for="c-42849495">[-]</label><label class="expand" for="c-42849495">[1 more]</label></div><br/><div class="children"><div class="content">DeepSeek direct competitors[0] are OpenAI&#x27;s models, Meta&#x27;s Llama, Anthropic&#x27;s Claude etc.<p>NVIDIA isn&#x27;t even DeepSeek indirect competitor. And no investor&#x2F;expert in their right mind would compare a software company to a hardware company.<p>[0] <a href="https:&#x2F;&#x2F;api-docs.deepseek.com&#x2F;news&#x2F;news250120" rel="nofollow">https:&#x2F;&#x2F;api-docs.deepseek.com&#x2F;news&#x2F;news250120</a></div><br/></div></div><div id="42839752" class="c"><input type="checkbox" id="c-42839752" checked=""/><div class="controls bullet"><span class="by">fire_lake</span><span>|</span><a href="#42849495">prev</a><span>|</span><a href="#42839875">next</a><span>|</span><label class="collapse" for="c-42839752">[-]</label><label class="expand" for="c-42839752">[8 more]</label></div><br/><div class="children"><div class="content">Fascinating.<p>I think Meta is a big winner from this - they still control the content and now mining it for value has been proven even cheaper by DeepSeek.</div><br/><div id="42849913" class="c"><input type="checkbox" id="c-42849913" checked=""/><div class="controls bullet"><span class="by">buryat</span><span>|</span><a href="#42839752">parent</a><span>|</span><a href="#42839952">next</a><span>|</span><label class="collapse" for="c-42849913">[-]</label><label class="expand" for="c-42849913">[1 more]</label></div><br/><div class="children"><div class="content">What kind of quality of content can Meta theoretically mine with all the vast data that they collect? They have a good advertising and a content algorithms but does it approximate general intelligence? And the content you see on Facebook, Instagram, Whatsapp, that&#x27;s just a lot of average content that might actually be great for AGI.</div><br/></div></div><div id="42839952" class="c"><input type="checkbox" id="c-42839952" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42839752">parent</a><span>|</span><a href="#42849913">prev</a><span>|</span><a href="#42839927">next</a><span>|</span><label class="collapse" for="c-42839952">[-]</label><label class="expand" for="c-42839952">[3 more]</label></div><br/><div class="children"><div class="content">Yours is the correct take, it isn&#x27;t about who did it, it was that it was done at all.<p>This shows how effective open source and open development can be.<p>Best way to get a correct answer is ... <a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=ws3qq1xl8hj4yx1izd5oeda0">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=ws3qq1xl8hj4yx1izd5oeda0</a></div><br/><div id="42840040" class="c"><input type="checkbox" id="c-42840040" checked=""/><div class="controls bullet"><span class="by">p2detar</span><span>|</span><a href="#42839752">root</a><span>|</span><a href="#42839952">parent</a><span>|</span><a href="#42839927">next</a><span>|</span><label class="collapse" for="c-42840040">[-]</label><label class="expand" for="c-42840040">[2 more]</label></div><br/><div class="children"><div class="content">Not sure I understand how exactly open source plays a key role here in terms of project development.<p>Looking at <a href="https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai">https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai</a>, those repos have a bunch of of contributors but unless I&#x27;m wrong I don&#x27;t see any significant contributions. What am I missing?</div><br/><div id="42840563" class="c"><input type="checkbox" id="c-42840563" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42839752">root</a><span>|</span><a href="#42840040">parent</a><span>|</span><a href="#42839927">next</a><span>|</span><label class="collapse" for="c-42840563">[-]</label><label class="expand" for="c-42840563">[1 more]</label></div><br/><div class="children"><div class="content">One could argue that by Meta and other companies releasing open weights and detailed got us to where we are now with R1. Even if it wasn&#x27;t your race car that crossed the line first, everyone can now get a copy.</div><br/></div></div></div></div></div></div><div id="42839927" class="c"><input type="checkbox" id="c-42839927" checked=""/><div class="controls bullet"><span class="by">ppsreejith</span><span>|</span><a href="#42839752">parent</a><span>|</span><a href="#42839952">prev</a><span>|</span><a href="#42845133">next</a><span>|</span><label class="collapse" for="c-42839927">[-]</label><label class="expand" for="c-42839927">[1 more]</label></div><br/><div class="children"><div class="content">Agree. Apple should be as well. The only con I can think of would be their (Meta&#x27;s) data center investments but seems this will make them more efficient?</div><br/></div></div><div id="42845133" class="c"><input type="checkbox" id="c-42845133" checked=""/><div class="controls bullet"><span class="by">IAmGraydon</span><span>|</span><a href="#42839752">parent</a><span>|</span><a href="#42839927">prev</a><span>|</span><a href="#42839943">next</a><span>|</span><label class="collapse" for="c-42845133">[-]</label><label class="expand" for="c-42845133">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not so sure about that. Deepseek puts their LLM (Llama) even further behind. It&#x27;s basically at the back of the pack, signaling to the market that they don&#x27;t have the top minds in the industry on board. Second, I&#x27;m not sure how a massive trove of misinformation is of much use or how it&#x27;s of more use to them than it is to others. Can you elaborate on that?</div><br/></div></div><div id="42839943" class="c"><input type="checkbox" id="c-42839943" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#42839752">parent</a><span>|</span><a href="#42845133">prev</a><span>|</span><a href="#42839875">next</a><span>|</span><label class="collapse" for="c-42839943">[-]</label><label class="expand" for="c-42839943">[1 more]</label></div><br/><div class="children"><div class="content">Exactly this. This does not affect Meta as many people believe.<p>The startups that are in trouble are the ones that have been screaming for &quot;AGI&quot; and buying up GPUs and close-sourcing their models.<p>Open source was always going to win the race to the bottom. [0]<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41671582">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41671582</a></div><br/></div></div></div></div><div id="42839875" class="c"><input type="checkbox" id="c-42839875" checked=""/><div class="controls bullet"><span class="by">fancyfredbot</span><span>|</span><a href="#42839752">prev</a><span>|</span><label class="collapse" for="c-42839875">[-]</label><label class="expand" for="c-42839875">[19 more]</label></div><br/><div class="children"><div class="content">The people writing market commentary are simply making it up. The news about DeepSeek is not new and doesn&#x27;t reduce the value of ASML. People are selling now because they are scared because the number went down.</div><br/><div id="42840166" class="c"><input type="checkbox" id="c-42840166" checked=""/><div class="controls bullet"><span class="by">FergusArgyll</span><span>|</span><a href="#42839875">parent</a><span>|</span><a href="#42840049">next</a><span>|</span><label class="collapse" for="c-42840166">[-]</label><label class="expand" for="c-42840166">[1 more]</label></div><br/><div class="children"><div class="content">Yes! and this applies to all market commentary. Market goes down .7% and you have talking heads saying &quot;fear of tariffs&quot; &quot;middle east tensions&quot; &quot;Hurricane season&quot; whatever, next day market goes up .6% &quot;talk of tax cuts&quot; &quot;Jobs numbers&quot; whatever. There&#x27;s no way anyone knows why &quot;the market&quot; behaves the way it does. The free market is the OG &quot;Decentralized&quot; project, it&#x27;s 1 billion different decisions being made in a day each with their personal reasons. Yes, sometimes it&#x27;s fairly obvious that something caused it (plane blows up, stock goes down) but that still doesn&#x27;t explain the entirety of it</div><br/></div></div><div id="42840049" class="c"><input type="checkbox" id="c-42840049" checked=""/><div class="controls bullet"><span class="by">sekai</span><span>|</span><a href="#42839875">parent</a><span>|</span><a href="#42840166">prev</a><span>|</span><a href="#42840570">next</a><span>|</span><label class="collapse" for="c-42840049">[-]</label><label class="expand" for="c-42840049">[8 more]</label></div><br/><div class="children"><div class="content">Andrej Karpathy was tweeting about DeepSeek a month (!) ago.<p>&quot;DeepSeek (Chinese AI co) making it look easy today with an open weights release of a frontier-grade LLM trained on a joke of a budget (2048 GPUs for 2 months, $6M).&quot;<p><a href="https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1872362712958906460" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1872362712958906460</a></div><br/><div id="42848419" class="c"><input type="checkbox" id="c-42848419" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42840049">parent</a><span>|</span><a href="#42840263">next</a><span>|</span><label class="collapse" for="c-42848419">[-]</label><label class="expand" for="c-42848419">[1 more]</label></div><br/><div class="children"><div class="content">Yes exactly. The actual impetus this time was the article I posted here and how it got echoed and amplified by massive X accounts like Chamath and Naval.</div><br/></div></div><div id="42840263" class="c"><input type="checkbox" id="c-42840263" checked=""/><div class="controls bullet"><span class="by">igleria</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42840049">parent</a><span>|</span><a href="#42848419">prev</a><span>|</span><a href="#42840570">next</a><span>|</span><label class="collapse" for="c-42840263">[-]</label><label class="expand" for="c-42840263">[6 more]</label></div><br/><div class="children"><div class="content">this same forum ignored deepseek one month ago, save for a few... open minded people.</div><br/><div id="42843592" class="c"><input type="checkbox" id="c-42843592" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42840263">parent</a><span>|</span><a href="#42840504">next</a><span>|</span><label class="collapse" for="c-42843592">[-]</label><label class="expand" for="c-42843592">[3 more]</label></div><br/><div class="children"><div class="content">Deepseek: The quiet giant leading China’s AI race (27 days ago, 465 comments): <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42557586">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42557586</a><p>Are we talking about the same forum? HN commenters have been raving about DeepSeek v3 for at least a month.</div><br/><div id="42844291" class="c"><input type="checkbox" id="c-42844291" checked=""/><div class="controls bullet"><span class="by">igleria</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42843592">parent</a><span>|</span><a href="#42840504">next</a><span>|</span><label class="collapse" for="c-42844291">[-]</label><label class="expand" for="c-42844291">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42514633">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42514633</a> the current base version release thread and a mere 40 comments. C&#x27;mon, it&#x27;s not a sin to be biased.</div><br/><div id="42844586" class="c"><input type="checkbox" id="c-42844586" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42844291">parent</a><span>|</span><a href="#42840504">next</a><span>|</span><label class="collapse" for="c-42844586">[-]</label><label class="expand" for="c-42844586">[1 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re experiencing isn&#x27;t sin but blatant confirmation bias.</div><br/></div></div></div></div></div></div><div id="42840504" class="c"><input type="checkbox" id="c-42840504" checked=""/><div class="controls bullet"><span class="by">Cumpiler69</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42840263">parent</a><span>|</span><a href="#42843592">prev</a><span>|</span><a href="#42840570">next</a><span>|</span><label class="collapse" for="c-42840504">[-]</label><label class="expand" for="c-42840504">[2 more]</label></div><br/><div class="children"><div class="content"><i>&quot;That forum&quot;</i>, just like the mobs in ancient Rome, doesn&#x27;t operate on logic, it operates on emotional feelings and groupthink.</div><br/><div id="42840970" class="c"><input type="checkbox" id="c-42840970" checked=""/><div class="controls bullet"><span class="by">igleria</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42840504">parent</a><span>|</span><a href="#42840570">next</a><span>|</span><label class="collapse" for="c-42840970">[-]</label><label class="expand" for="c-42840970">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, although like you say, that is a property of forums rather than a unique trait of HN. I do love HN most of the time.</div><br/></div></div></div></div></div></div></div></div><div id="42840570" class="c"><input type="checkbox" id="c-42840570" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#42839875">parent</a><span>|</span><a href="#42840049">prev</a><span>|</span><a href="#42845495">next</a><span>|</span><label class="collapse" for="c-42840570">[-]</label><label class="expand" for="c-42840570">[5 more]</label></div><br/><div class="children"><div class="content">ASML paper value is determined by equipment sales from projected compute supply&#x2F;demand. CHIPS building redundant global fabs = glut from more excess capacity = less future sales. Stargate = excess demand from everyone spending 100s of billions of compute = need even more fabs = more future sales. Then DeepSeek = suddenly no need for that much future compute... if number of future compute demand relative to short term fab overcapacity is going down, then it&#x27;s reasonable to sell. Relatively predictable Semiconductor market cycles due to cost of capex and time to build fabs &#x2F; increase new wafers output to match future demand is a thing.</div><br/><div id="42841047" class="c"><input type="checkbox" id="c-42841047" checked=""/><div class="controls bullet"><span class="by">fancyfredbot</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42840570">parent</a><span>|</span><a href="#42845495">next</a><span>|</span><label class="collapse" for="c-42841047">[-]</label><label class="expand" for="c-42841047">[4 more]</label></div><br/><div class="children"><div class="content">This would be an excellent explanation if DeepSeek had announced its model over the last weekend rather than weeks ago, and if R1 wasn&#x27;t a COT reasoning model which needs a lot more inference time compute than other SOTA models like llama.</div><br/><div id="42841154" class="c"><input type="checkbox" id="c-42841154" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42841047">parent</a><span>|</span><a href="#42845495">next</a><span>|</span><label class="collapse" for="c-42841154">[-]</label><label class="expand" for="c-42841154">[3 more]</label></div><br/><div class="children"><div class="content">Information lag, especially with respect to PRC developments and technical developments. Taking 1-2 week for info to be shared and passed down info chain not unusual. IRC COT typically increase inference 1-5x depending on task complexity, i.e. instead of scaling down compute demand by 50x, it&#x27;s 10x, which is still substantial. Could investors be panicking? Sure, but there&#x27;s rational basis for doing so.</div><br/><div id="42844489" class="c"><input type="checkbox" id="c-42844489" checked=""/><div class="controls bullet"><span class="by">fancyfredbot</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42841154">parent</a><span>|</span><a href="#42845495">next</a><span>|</span><label class="collapse" for="c-42844489">[-]</label><label class="expand" for="c-42844489">[2 more]</label></div><br/><div class="children"><div class="content">DeepSeek V3 is a 671B parameter MOE model? I am not sure why it&#x27;s 50x cheaper at inference time than other models. We don&#x27;t know what the cost of running o1 is, but I doubt it has 50x as many params as R1. Most the advantages of MOE are reduced when using reasonable branch sizes so that wouldn&#x27;t make R1 cheaper in practice either. I think people might be seeing a lower markup from DeepSeek and confusing it with cheaper inference?</div><br/><div id="42845540" class="c"><input type="checkbox" id="c-42845540" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42844489">parent</a><span>|</span><a href="#42845495">next</a><span>|</span><label class="collapse" for="c-42845540">[-]</label><label class="expand" for="c-42845540">[1 more]</label></div><br/><div class="children"><div class="content">If DeepSeek is side&#x2F;pet project for PRC quants whose fine with subsisting on low markup then that&#x27;s market price competitors have to calibrate return on investment and future capex. DeepSeek also appear open and performative enough to drive cheaper inference on commodity hardware with very different margins for variety of use cases, including existing hardware. At least short term there&#x27;s going to be % of LLM use cases that has to compete on China prices or previously considered depreciated hardware. IMO snowballing interests undoubtly getting investors to pay attention and deep dive to related developments, i.e. Bank of China&#x27;s 1T AI fund, and DeepSeek CEO just met PRC premiere a few days ago. AFAIK DeepSeek hasn&#x27;t ever gotten this much domestic political attention before, they&#x27;re potentially going to be elevated as domestic champion and it&#x27;s likely to open a lot of more doors, i.e. significantly more compute. Hard to tell how that will effect western models business models short term.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42845495" class="c"><input type="checkbox" id="c-42845495" checked=""/><div class="controls bullet"><span class="by">IAmGraydon</span><span>|</span><a href="#42839875">parent</a><span>|</span><a href="#42840570">prev</a><span>|</span><a href="#42844853">next</a><span>|</span><label class="collapse" for="c-42845495">[-]</label><label class="expand" for="c-42845495">[2 more]</label></div><br/><div class="children"><div class="content">&gt;The people writing market commentary are simply making it up.<p>Everyone except for you, right?</div><br/><div id="42847379" class="c"><input type="checkbox" id="c-42847379" checked=""/><div class="controls bullet"><span class="by">fancyfredbot</span><span>|</span><a href="#42839875">root</a><span>|</span><a href="#42845495">parent</a><span>|</span><a href="#42844853">next</a><span>|</span><label class="collapse" for="c-42847379">[-]</label><label class="expand" for="c-42847379">[1 more]</label></div><br/><div class="children"><div class="content">Oh no, I am making it up too.</div><br/></div></div></div></div><div id="42844853" class="c"><input type="checkbox" id="c-42844853" checked=""/><div class="controls bullet"><span class="by">tmaly</span><span>|</span><a href="#42839875">parent</a><span>|</span><a href="#42845495">prev</a><span>|</span><a href="#42839934">next</a><span>|</span><label class="collapse" for="c-42844853">[-]</label><label class="expand" for="c-42844853">[1 more]</label></div><br/><div class="children"><div class="content">If you look at Intel, it is a cyclical business.<p>One could argue by extension that ASML is also cyclical.</div><br/></div></div><div id="42839934" class="c"><input type="checkbox" id="c-42839934" checked=""/><div class="controls bullet"><span class="by">knoopx</span><span>|</span><a href="#42839875">parent</a><span>|</span><a href="#42844853">prev</a><span>|</span><label class="collapse" for="c-42839934">[-]</label><label class="expand" for="c-42839934">[1 more]</label></div><br/><div class="children"><div class="content">this, absolutely irrelevant to deepseek</div><br/></div></div></div></div></div></div></div></div></div></body></html>