<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1694854877741" as="style"/><link rel="stylesheet" href="styles.css?v=1694854877741"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/junruxiong/IncarnaMind">Show HN: IncarnaMind-Chat with your multiple docs using LLMs</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>joeyxiong</span> | <span>25 comments</span></div><br/><div><div id="37528780" class="c"><input type="checkbox" id="c-37528780" checked=""/><div class="controls bullet"><span class="by">all2</span><span>|</span><a href="#37529074">next</a><span>|</span><label class="collapse" for="c-37528780">[-]</label><label class="expand" for="c-37528780">[2 more]</label></div><br/><div class="children"><div class="content">A team where I work recently rolled out a doc-answer LLM and context was an issue we ran into. Retrieved doc chunks didn&#x27;t have nearly enough context to answer some of the broader questions well.<p>Another issue I&#x27;ve run into with doc-answer LLMs is that they don&#x27;t handle synonyms well. If I don&#x27;t know the terminology for the tool, say llama-index [0], I can&#x27;t ask around the concept to see if something <i>like</i> what I&#x27;m describing exists.<p>A part of me thinks a lang-chain with the LLM in it might be useful.<p>Something like<p>1. User makes vague query &quot;hey, llama-index, how do I create a moving chunk answer thing with llama-index?&quot;<p>2. Initial context comes back to the LLM, and the LLM determines there is not straight forward answer to the question.<p>2a. The LLM might ask followup questions &quot;when you say X, what do you mean?&quot; to clarify terms it doesn&#x27;t have ready answers for.<p>2b. The LLM says &quot;hm, let me think about that. I&#x27;ll email you when I have a good answer.&quot;<p>2c. The LLM reads the docs and relevant materials and attempts to solve the problem.<p>3. Email the user with a potential answer to the question.<p>4. Stashes the solution text in the docs if the user OKs the plan. Updates an embedding table to include words&#x2F;terms used that the docs didn&#x27;t contain.<p>This last step is the most important. Some kind of method to capture common questions and answers, synonyms, etc. would ensure that the model has access to (potentially) increasingly robust information.</div><br/><div id="37530018" class="c"><input type="checkbox" id="c-37530018" checked=""/><div class="controls bullet"><span class="by">sergiotapia</span><span>|</span><a href="#37528780">parent</a><span>|</span><a href="#37529074">next</a><span>|</span><label class="collapse" for="c-37530018">[-]</label><label class="expand" for="c-37530018">[1 more]</label></div><br/><div class="children"><div class="content">You can have a pre-qualification step to qualify the answer into several highly specific categories. These categories have highly tailored context that allow much better answers.<p>Of course, you can only generate these categories once you see what kind of questions your users ask, but this means your product can continuously improve.</div><br/></div></div></div></div><div id="37529074" class="c"><input type="checkbox" id="c-37529074" checked=""/><div class="controls bullet"><span class="by">SamBam</span><span>|</span><a href="#37528780">prev</a><span>|</span><a href="#37528527">next</a><span>|</span><label class="collapse" for="c-37529074">[-]</label><label class="expand" for="c-37529074">[7 more]</label></div><br/><div class="children"><div class="content">This looks awesome, and really useful.<p>A few weeks ago I asked in Hacker News &quot;I&#x27;m in the middle of a graduate degree and am reading lots of papers, how could I get ChatGPT to use my whole library as context when answering questions?&quot;<p>And I was told, basically, &quot;It&#x27;s really easy! Just First you just extract all of the text from the PDFs into arxiv, parse to separate content from style, then store that in a a DuckDB database, with zstd compression, then just use some encoder model to process all of these texts into Qdrant database. Then use Vicuna or Guanaco 30b GPTQ, with langcgain, and.....&quot;<p>I was like, ok... guess I won&#x27;t be asking ChatGPT where I can find which paper talked about which thing after all.</div><br/><div id="37529462" class="c"><input type="checkbox" id="c-37529462" checked=""/><div class="controls bullet"><span class="by">skeptrune</span><span>|</span><a href="#37529074">parent</a><span>|</span><a href="#37530776">next</a><span>|</span><label class="collapse" for="c-37529462">[-]</label><label class="expand" for="c-37529462">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why you need the &quot;ask chatGPT&quot; piece. Why not just semantic search on the documents?<p>What is the value add of generative output?</div><br/><div id="37529483" class="c"><input type="checkbox" id="c-37529483" checked=""/><div class="controls bullet"><span class="by">all2</span><span>|</span><a href="#37529074">root</a><span>|</span><a href="#37529462">parent</a><span>|</span><a href="#37530776">next</a><span>|</span><label class="collapse" for="c-37529483">[-]</label><label class="expand" for="c-37529483">[3 more]</label></div><br/><div class="children"><div class="content">I think the value is &quot;Hey, I remember a paper talking X topic with Y sentiment, it also mentioned data from &lt;vague source&gt;. Which paper was that?&quot;<p>If you&#x27;re dealing with 100s of papers, then having a front end that can deal with vague queries would be a huge benefit.</div><br/><div id="37529494" class="c"><input type="checkbox" id="c-37529494" checked=""/><div class="controls bullet"><span class="by">skeptrune</span><span>|</span><a href="#37529074">root</a><span>|</span><a href="#37529483">parent</a><span>|</span><a href="#37530776">next</a><span>|</span><label class="collapse" for="c-37529494">[-]</label><label class="expand" for="c-37529494">[2 more]</label></div><br/><div class="children"><div class="content">You could just write &quot;X topic with Y sentiment similar to foo&#x2F;&lt;vague-source&gt;&quot; into a search bar.<p>Then, plain old vector distance on your data would find the chunks relevant. No need for generative AI.<p>citation to prove this works: chat.arguflow.ai &#x2F; search.arguflow.ai</div><br/><div id="37530783" class="c"><input type="checkbox" id="c-37530783" checked=""/><div class="controls bullet"><span class="by">SamBam</span><span>|</span><a href="#37529074">root</a><span>|</span><a href="#37529494">parent</a><span>|</span><a href="#37530776">next</a><span>|</span><label class="collapse" for="c-37530783">[-]</label><label class="expand" for="c-37530783">[1 more]</label></div><br/><div class="children"><div class="content">The simple answer is because I don&#x27;t know how to do a semantic search on a bunch of documents, nor do &quot;plain old vector distances.&quot; It&#x27;s not my field.<p>The longer answer is that I <i>think</i> that it will also be useful to have the background knowledge (however hallucinogenic) that ChatGPT has. I&#x27;d like to be able to have a conversation specifically grounded in the papers, and if I ask about a topic that isn&#x27;t specifically mentioned by those words in the article, I&#x27;d like it to be able to say &quot;none of the articles talk about X, but they do mention Y, which is related in this way.&quot; I&#x27;m not sure if this is expecting too much of it.</div><br/></div></div></div></div></div></div></div></div><div id="37530776" class="c"><input type="checkbox" id="c-37530776" checked=""/><div class="controls bullet"><span class="by">ajhai</span><span>|</span><a href="#37529074">parent</a><span>|</span><a href="#37529462">prev</a><span>|</span><a href="#37529869">next</a><span>|</span><label class="collapse" for="c-37530776">[-]</label><label class="expand" for="c-37530776">[1 more]</label></div><br/><div class="children"><div class="content">We built <a href="https:&#x2F;&#x2F;github.com&#x2F;trypromptly&#x2F;LLMStack">https:&#x2F;&#x2F;github.com&#x2F;trypromptly&#x2F;LLMStack</a> to serve exactly this persona. A low-code platform to quickly build RAG pipelines and other LLM applications.</div><br/></div></div><div id="37529869" class="c"><input type="checkbox" id="c-37529869" checked=""/><div class="controls bullet"><span class="by">jarvist</span><span>|</span><a href="#37529074">parent</a><span>|</span><a href="#37530776">prev</a><span>|</span><a href="#37528527">next</a><span>|</span><label class="collapse" for="c-37529869">[-]</label><label class="expand" for="c-37529869">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;whitead&#x2F;paper-qa">https:&#x2F;&#x2F;github.com&#x2F;whitead&#x2F;paper-qa</a><p>&gt;This is a minimal package for doing question and answering from PDFs or text files (which can be raw HTML). It strives to give very good answers, with no hallucinations, by grounding responses with in-text citations.</div><br/></div></div></div></div><div id="37528527" class="c"><input type="checkbox" id="c-37528527" checked=""/><div class="controls bullet"><span class="by">pstorm</span><span>|</span><a href="#37529074">prev</a><span>|</span><a href="#37529307">next</a><span>|</span><label class="collapse" for="c-37528527">[-]</label><label class="expand" for="c-37528527">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m impressed by your chunking and retrieval strategies. I think this aspect is often overly simplistic.<p>One aspect I don&#x27;t quite understand is why you filter by the sliding window chunks vs just using the medium chunks? If I understand it correctly, you find the large chunks that contain the matched small chunks from the first retrieval. Then in the third retrieval, you are getting the medium chunks that comprise the large chunks? What extra value does that provide?</div><br/><div id="37529221" class="c"><input type="checkbox" id="c-37529221" checked=""/><div class="controls bullet"><span class="by">joeyxiong</span><span>|</span><a href="#37528527">parent</a><span>|</span><a href="#37529307">next</a><span>|</span><label class="collapse" for="c-37529221">[-]</label><label class="expand" for="c-37529221">[2 more]</label></div><br/><div class="children"><div class="content">Thank you for your comment. The sliding window approach allows me to dynamically identify relevant &quot;large chunks,&quot; which can be thought of as sections in a document. Often, your questions may pertain to multiple such sections. Using only medium chunks for retrieval could result in sparse or fragmented information.<p>The third retrieval focuses on &quot;medium chunks&quot; within these identified large chunks. This ensures that only the most relevant information is passed to the Language Model, enhancing both time efficiency and focus. For example, if you&#x27;re asking for a paper summary, I can zero in on medium chunks within the Abstract, Introduction, and Conclusion sections, eliminating noise from other irrelevant sections. Additionally, this strategy helps manage token limitations, like GPT-3.5&#x27;s 4000-token cap, by selectively retrieving information</div><br/><div id="37529329" class="c"><input type="checkbox" id="c-37529329" checked=""/><div class="controls bullet"><span class="by">pstorm</span><span>|</span><a href="#37528527">root</a><span>|</span><a href="#37529221">parent</a><span>|</span><a href="#37529307">next</a><span>|</span><label class="collapse" for="c-37529329">[-]</label><label class="expand" for="c-37529329">[1 more]</label></div><br/><div class="children"><div class="content">Ah I see! So, the large&#x2F;sliding window chunks act as a pre-filter for the medium chunks. That makes a lot of sense. I appreciate the response</div><br/></div></div></div></div></div></div><div id="37529307" class="c"><input type="checkbox" id="c-37529307" checked=""/><div class="controls bullet"><span class="by">SamBam</span><span>|</span><a href="#37528527">prev</a><span>|</span><a href="#37529046">next</a><span>|</span><label class="collapse" for="c-37529307">[-]</label><label class="expand" for="c-37529307">[3 more]</label></div><br/><div class="children"><div class="content">Testing it out. I&#x27;m getting an error after I added my pdfs to the data directory and then ran<p><pre><code>    % python docs2db.py    
      Processing files:   6%
        Traceback (most recent call last):
          File &quot;[...]&#x2F;IncarnaMind&#x2F;docs2db.py&quot;, line 179, in process_metadata
          file_name = doc[0].metadata[&quot;source&quot;].split(&quot;&#x2F;&quot;)[-1].split(&quot;.&quot;)[0]
        IndexError: list index out of range</code></pre></div><br/><div id="37529740" class="c"><input type="checkbox" id="c-37529740" checked=""/><div class="controls bullet"><span class="by">joeyxiong</span><span>|</span><a href="#37529307">parent</a><span>|</span><a href="#37529046">next</a><span>|</span><label class="collapse" for="c-37529740">[-]</label><label class="expand" for="c-37529740">[2 more]</label></div><br/><div class="children"><div class="content">Hi, I&#x27;ve pushed the new commit to the main branch. Could you please test it out?
If it still has this error, you can check if your doc has relevant metadata.<p>````
for d in doc:
    print(&quot;metadata:&quot;, d.metadata)
```<p>before file_name = doc[0].metadata[&quot;source&quot;].split(&quot;&#x2F;&quot;)[-1].split(&quot;.&quot;)[0]</div><br/><div id="37530610" class="c"><input type="checkbox" id="c-37530610" checked=""/><div class="controls bullet"><span class="by">SamBam</span><span>|</span><a href="#37529307">root</a><span>|</span><a href="#37529740">parent</a><span>|</span><a href="#37529046">next</a><span>|</span><label class="collapse" for="c-37530610">[-]</label><label class="expand" for="c-37530610">[1 more]</label></div><br/><div class="children"><div class="content">One PDF is causing the issue. When I remove it I get another issue, `No such file or directory: &#x27;database_store&#x2F;file_names.pkl&#x27;`.<p>Opened up an issue in GitHub so as not to pollute this thread.</div><br/></div></div></div></div></div></div><div id="37529046" class="c"><input type="checkbox" id="c-37529046" checked=""/><div class="controls bullet"><span class="by">dilap</span><span>|</span><a href="#37529307">prev</a><span>|</span><a href="#37529475">next</a><span>|</span><label class="collapse" for="c-37529046">[-]</label><label class="expand" for="c-37529046">[1 more]</label></div><br/><div class="children"><div class="content">I feel like an LLM trained on Slack could be something like the perfect replacement for trying to maintain docs.</div><br/></div></div><div id="37529475" class="c"><input type="checkbox" id="c-37529475" checked=""/><div class="controls bullet"><span class="by">skeptrune</span><span>|</span><a href="#37529046">prev</a><span>|</span><a href="#37528868">next</a><span>|</span><label class="collapse" for="c-37529475">[-]</label><label class="expand" for="c-37529475">[1 more]</label></div><br/><div class="children"><div class="content">Can we talk about how dynamic chunking works by any chance? That is the most interesting piece imo.<p>We have a similar thing (w&#x2F; UIs for search&#x2F;chat) at <a href="https:&#x2F;&#x2F;github.com&#x2F;arguflow&#x2F;arguflow">https:&#x2F;&#x2F;github.com&#x2F;arguflow&#x2F;arguflow</a> .<p>- nick@arguflow.gg</div><br/></div></div><div id="37528868" class="c"><input type="checkbox" id="c-37528868" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#37529475">prev</a><span>|</span><a href="#37528832">next</a><span>|</span><label class="collapse" for="c-37528868">[-]</label><label class="expand" for="c-37528868">[4 more]</label></div><br/><div class="children"><div class="content">Those diagrams are nice! What did you use to make them? The sliding window mechanic is interesting but I&#x27;m not seeing how the first, second and third retrievers relate. Only the final medium chunks are used, but how are those arrived at?</div><br/><div id="37529404" class="c"><input type="checkbox" id="c-37529404" checked=""/><div class="controls bullet"><span class="by">joeyxiong</span><span>|</span><a href="#37528868">parent</a><span>|</span><a href="#37528832">next</a><span>|</span><label class="collapse" for="c-37529404">[-]</label><label class="expand" for="c-37529404">[3 more]</label></div><br/><div class="children"><div class="content">Hi, I created the diagrams using Figma.<p>The retrieval process consists of three stages. The first stage retrieves small chunks from multiple documents to create a document filter using their metadat. This filter is then applied in the second stage to extract relevant large chunks, essentially sections of documents, which further refines our search parameters. Finally, using both the document and large chunk filters, the third stage retrieves the most pertinent medium-sized chunks of information to be passed to the Language Model, ensuring a focused and relevant response to your query.</div><br/><div id="37530457" class="c"><input type="checkbox" id="c-37530457" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#37528868">root</a><span>|</span><a href="#37529404">parent</a><span>|</span><a href="#37528832">next</a><span>|</span><label class="collapse" for="c-37530457">[-]</label><label class="expand" for="c-37530457">[2 more]</label></div><br/><div class="children"><div class="content">So to rephrase: stage 1 - it finds the top k most relevant small chunks, stage 2 - it searches the source documents of those small chunks for most relevant large chunks, stage 3 - it searches medium chunks in the source documents of relevant small chunks that are contained in the large chunks found in stage 2?</div><br/><div id="37530602" class="c"><input type="checkbox" id="c-37530602" checked=""/><div class="controls bullet"><span class="by">joeyxiong</span><span>|</span><a href="#37528868">root</a><span>|</span><a href="#37530457">parent</a><span>|</span><a href="#37528832">next</a><span>|</span><label class="collapse" for="c-37530602">[-]</label><label class="expand" for="c-37530602">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the sliding window chunking mechanism appears in stage 2.</div><br/></div></div></div></div></div></div></div></div><div id="37528832" class="c"><input type="checkbox" id="c-37528832" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#37528868">prev</a><span>|</span><label class="collapse" for="c-37528832">[-]</label><label class="expand" for="c-37528832">[3 more]</label></div><br/><div class="children"><div class="content">Only supports private &#x2F; closed LLMs like OpenAI and Claud. People need to design for local LLM first, then for-profit providers.</div><br/><div id="37529809" class="c"><input type="checkbox" id="c-37529809" checked=""/><div class="controls bullet"><span class="by">joeyxiong</span><span>|</span><a href="#37528832">parent</a><span>|</span><label class="collapse" for="c-37529809">[-]</label><label class="expand" for="c-37529809">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, This can definatly be used for local models, but the problem is that most personal computers cannot host large LLMs and the cost is not cheaper than closed LLMs. But for organisations, local LLMs are a better choice.</div><br/><div id="37530986" class="c"><input type="checkbox" id="c-37530986" checked=""/><div class="controls bullet"><span class="by">bytefactory</span><span>|</span><a href="#37528832">root</a><span>|</span><a href="#37529809">parent</a><span>|</span><label class="collapse" for="c-37530986">[-]</label><label class="expand" for="c-37530986">[1 more]</label></div><br/><div class="children"><div class="content">I think local LLMs are great for tinkerers, and with quantization can run on most modern PCs. I am not comfortable sending over my personal data over to OpenAI&#x2F;Anthropic, so I&#x27;ve been playing around with <a href="https:&#x2F;&#x2F;github.com&#x2F;PromtEngineer&#x2F;localGPT&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;PromtEngineer&#x2F;localGPT&#x2F;</a>, GPT4All, etc. which keep the data all local.<p>Sliding window chunking, RAG, etc. seem more sophisticated than the other document LLM tools, so I would love to try this out if you ever add the ability to run LLMs locally!</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>