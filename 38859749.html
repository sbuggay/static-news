<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704358854790" as="style"/><link rel="stylesheet" href="styles.css?v=1704358854790"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/DLYuanGod/TinyGPT-V">TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>T-A</span> | <span>22 comments</span></div><br/><div><div id="38859789" class="c"><input type="checkbox" id="c-38859789" checked=""/><div class="controls bullet"><span class="by">T-A</span><span>|</span><a href="#38864473">next</a><span>|</span><label class="collapse" for="c-38859789">[-]</label><label class="expand" for="c-38859789">[1 more]</label></div><br/><div class="children"><div class="content">From the paper&#x27;s abstract [1]:<p><i>It stands out by requiring merely a 24G GPU for training and an 8G GPU or CPU for inference. Built upon Phi-2, TinyGPT-V couples an effective language backbone with pre-trained vision modules from BLIP-2 or CLIP. TinyGPT-V&#x27;s 2.8B parameters can undergo a unique quantisation process, suitable for local deployment and inference tasks on 8G various devices.</i><p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.16862" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.16862</a></div><br/></div></div><div id="38862107" class="c"><input type="checkbox" id="c-38862107" checked=""/><div class="controls bullet"><span class="by">infotainment</span><span>|</span><a href="#38864473">prev</a><span>|</span><a href="#38860785">next</a><span>|</span><label class="collapse" for="c-38862107">[-]</label><label class="expand" for="c-38862107">[3 more]</label></div><br/><div class="children"><div class="content">While the non-commercial Phi2 license continues to be a letdown, I&#x27;m excited to see additional development in the space of these ultracompact LLMs. On-device AI excites me far more than relying on yet another cloud-based API.<p>On my M1 Macbook Air, current LLMs can run surprisingly quickly locally already.</div><br/><div id="38862876" class="c"><input type="checkbox" id="c-38862876" checked=""/><div class="controls bullet"><span class="by">rablackburn</span><span>|</span><a href="#38862107">parent</a><span>|</span><a href="#38860785">next</a><span>|</span><label class="collapse" for="c-38862876">[-]</label><label class="expand" for="c-38862876">[2 more]</label></div><br/><div class="children"><div class="content">&gt; On-device AI excites me far more than relying on yet another cloud-based API.<p>Agreed. The huge leap-forward in capability already boggles my mind, the fact that I can run it _on my desktop cpu_ and have an interactive, natural-language oracle of the internet in a mere ~11GB file.<p>Everyone seems to be chasing the network-accessible API approach because lock-in is easy and if you&#x27;re at the bleeding-edge (training) you&#x27;ve got the compute for running it anyway.<p>But now with accessible, local models my bet is on bored young hackers coming up with the best use cases and killer apps, not Microsoft.</div><br/><div id="38863362" class="c"><input type="checkbox" id="c-38863362" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38862107">root</a><span>|</span><a href="#38862876">parent</a><span>|</span><a href="#38860785">next</a><span>|</span><label class="collapse" for="c-38863362">[-]</label><label class="expand" for="c-38863362">[1 more]</label></div><br/><div class="children"><div class="content">Which local models and CPU are you using?</div><br/></div></div></div></div></div></div><div id="38860785" class="c"><input type="checkbox" id="c-38860785" checked=""/><div class="controls bullet"><span class="by">justinl33</span><span>|</span><a href="#38862107">prev</a><span>|</span><a href="#38860424">next</a><span>|</span><label class="collapse" for="c-38860785">[-]</label><label class="expand" for="c-38860785">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>You need to execute the above code 17 times to complete the first stage of training.</i><p>Am I missing something here? Did the authors forget about for loops? What happens if you only do it 16 times?</div><br/><div id="38861023" class="c"><input type="checkbox" id="c-38861023" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#38860785">parent</a><span>|</span><a href="#38860424">next</a><span>|</span><label class="collapse" for="c-38861023">[-]</label><label class="expand" for="c-38861023">[3 more]</label></div><br/><div class="children"><div class="content">Ever feed a Gremlin after midnight?  Same thing.</div><br/><div id="38863778" class="c"><input type="checkbox" id="c-38863778" checked=""/><div class="controls bullet"><span class="by">RonnieOwnsLexus</span><span>|</span><a href="#38860785">root</a><span>|</span><a href="#38861023">parent</a><span>|</span><a href="#38860424">next</a><span>|</span><label class="collapse" for="c-38863778">[-]</label><label class="expand" for="c-38863778">[2 more]</label></div><br/><div class="children"><div class="content">please explain this reference for non US&#x2F;western hemisphere folks.</div><br/><div id="38864302" class="c"><input type="checkbox" id="c-38864302" checked=""/><div class="controls bullet"><span class="by">jl6</span><span>|</span><a href="#38860785">root</a><span>|</span><a href="#38863778">parent</a><span>|</span><a href="#38860424">next</a><span>|</span><label class="collapse" for="c-38864302">[-]</label><label class="expand" for="c-38864302">[1 more]</label></div><br/><div class="children"><div class="content">In the 1984 movie Gremlins, the protagonist is warned not to feed the titular creatures (a type of transformer) after midnight, otherwise they promptly lose alignment and begin to hallucinate.</div><br/></div></div></div></div></div></div></div></div><div id="38860424" class="c"><input type="checkbox" id="c-38860424" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#38860785">prev</a><span>|</span><a href="#38863418">next</a><span>|</span><label class="collapse" for="c-38860424">[-]</label><label class="expand" for="c-38860424">[2 more]</label></div><br/><div class="children"><div class="content">Funny I was just looking for something to substitute GPT4V as they are bounding the API usage to few request per day. Sadly this project is built on top of phi-2 that has the non-commercial friendly Microsoft research license.</div><br/><div id="38863820" class="c"><input type="checkbox" id="c-38863820" checked=""/><div class="controls bullet"><span class="by">ashvardanian</span><span>|</span><a href="#38860424">parent</a><span>|</span><a href="#38863418">next</a><span>|</span><label class="collapse" for="c-38863820">[-]</label><label class="expand" for="c-38863820">[1 more]</label></div><br/><div class="children"><div class="content">We have released an even-smaller model - UForm-Gen a couple of weeks ago. It&#x27;s fully open-source, so it may help, but I wouldn&#x27;t expect results anywhere close to the GPT4V, assuming there is over 1000x difference in model size and consumed resources.</div><br/></div></div></div></div><div id="38863418" class="c"><input type="checkbox" id="c-38863418" checked=""/><div class="controls bullet"><span class="by">jn2clark</span><span>|</span><a href="#38860424">prev</a><span>|</span><a href="#38861311">next</a><span>|</span><label class="collapse" for="c-38863418">[-]</label><label class="expand" for="c-38863418">[1 more]</label></div><br/><div class="children"><div class="content">Can anyone comment on an open source multi-modal LLM that can produce structured outputs based on an image? I have not found a good open source one yet (this included), seems to be only closed source that can do this reliably well. Any suggestions are very welcome!</div><br/></div></div><div id="38861311" class="c"><input type="checkbox" id="c-38861311" checked=""/><div class="controls bullet"><span class="by">smpanaro</span><span>|</span><a href="#38863418">prev</a><span>|</span><a href="#38860331">next</a><span>|</span><label class="collapse" for="c-38861311">[-]</label><label class="expand" for="c-38861311">[1 more]</label></div><br/><div class="children"><div class="content">MobileVLM [1] is another recent small multimodal model. They trained their own 1.4B&#x2F;2.7B LLaMa from scratch using RedPajama and Vicuna instead of leveraging Phi-2.<p>The papers only have one common benchmark (GQA, MobileVLM scores better) so hard to say how they compare otherwise.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.16886" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.16886</a></div><br/></div></div><div id="38860331" class="c"><input type="checkbox" id="c-38860331" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38861311">prev</a><span>|</span><a href="#38862170">next</a><span>|</span><label class="collapse" for="c-38860331">[-]</label><label class="expand" for="c-38860331">[1 more]</label></div><br/><div class="children"><div class="content">Their results seem comparable to BLIP-2, shifted over in the diagram.</div><br/></div></div><div id="38862170" class="c"><input type="checkbox" id="c-38862170" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#38860331">prev</a><span>|</span><a href="#38860599">next</a><span>|</span><label class="collapse" for="c-38862170">[-]</label><label class="expand" for="c-38862170">[4 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t Phi-2 have testing data contamination, hence why it&#x27;s performing well on these benchmarks?<p>Most professionals in the field that I&#x27;m near would not touch that model with a 10 foot pole. We desperately need better validation&#x2F;data contamination detection methods.</div><br/><div id="38863374" class="c"><input type="checkbox" id="c-38863374" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38862170">parent</a><span>|</span><a href="#38863611">next</a><span>|</span><label class="collapse" for="c-38863374">[-]</label><label class="expand" for="c-38863374">[2 more]</label></div><br/><div class="children"><div class="content">Can you explain why you wouldn&#x27;t want to use it, i.e. what data contamination is?</div><br/><div id="38863421" class="c"><input type="checkbox" id="c-38863421" checked=""/><div class="controls bullet"><span class="by">0xDEADFED5</span><span>|</span><a href="#38862170">root</a><span>|</span><a href="#38863374">parent</a><span>|</span><a href="#38863611">next</a><span>|</span><label class="collapse" for="c-38863421">[-]</label><label class="expand" for="c-38863421">[1 more]</label></div><br/><div class="children"><div class="content">contamination:  training on the questions&#x2F;answers used for LLM benchmarks.<p>the top of the HuggingFace Open LLM leaderboard was pretty meaningless for a bit because many of the top scores were achieved by training on the evaluation data</div><br/></div></div></div></div></div></div><div id="38860599" class="c"><input type="checkbox" id="c-38860599" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#38862170">prev</a><span>|</span><label class="collapse" for="c-38860599">[-]</label><label class="expand" for="c-38860599">[3 more]</label></div><br/><div class="children"><div class="content">I really want to understand this post, but I can&#x27;t, may you please direct me, and those noobs like me - to resources to be able to read this? (help anyone to climb the knowledge ladder) ELI3<p>-<p>EDIT - GPT helped me understand better:<p>--<p>&gt;&gt;&gt;  &quot;<i>This model is special because it can do similar tasks as the big models but requires much less computational power1. Itâs like having a small but powerful engine that can do the work of a big one. This makes it more accessible for more people to use it&quot;</i><p>---<p>&gt;&gt;&gt;  <i>&quot;TinyGPT-V is built on another model called Phi-2 and uses pre-trained vision modules from BLIP-2 or CLIP1. It has 2.8 billion parameters (these are like the modelâs brain cells) and can be further compressed to fit on devices with 8GB memory1. This means you could potentially run this model on your personal computer or even some high-end smartphones1&quot;</i><p>----<p>&gt;&gt;&gt;  <i>&quot;In summary, TinyGPT-V is a step towards making powerful AI models more accessible and efficient, which could lead to their use in a wide range of real-world applications1. The authors have also shared their code and training weights for others to use and learn from1&quot;</i><p>-----<p>This is really interesting if you fan out implications over N time?<p>Here is my thinking:<p>Assume this paper results in a way of &quot;compression-alyzed vision&quot; into a model (a tiny compressed view into a model)<p>Then one, in a few years can imagine &quot;laser views&quot; - that slice through fractals of models to find the result. Resulting in tiny agents that have a heat-seeking-fractal-laser that can navigate giant data based on a method of knowing instantaneously what to <i>exclude</i> (meaning the path is defined by the walls that you already know you do not want to hit, so your steps are always that which helps you forward)<p>--<p>Or am I stating something obvious to all you brainiacs?<p>(no shame, I like thinking out loud)</div><br/><div id="38861052" class="c"><input type="checkbox" id="c-38861052" checked=""/><div class="controls bullet"><span class="by">ricopags</span><span>|</span><a href="#38860599">parent</a><span>|</span><a href="#38861515">next</a><span>|</span><label class="collapse" for="c-38861052">[-]</label><label class="expand" for="c-38861052">[1 more]</label></div><br/><div class="children"><div class="content">I am no brainiac, and it isn&#x27;t super clear from your post what you&#x27;re describing, but here is some info that might help you better convey your question:<p>This is a neural net built by conjoining Phi-2 [the best available small LLM, if you&#x27;ll pardon the contradiction in terms] with pre-trained vision models like BLIP or CLIP. Models are piles of weights[&#x2F;parameters] that are generated by training on datasets.<p>Already work has shown that training a multi-model model from the start results in smaller, more effective model. If you want to know more, check out recent work from CVPR [a vision machine learning conference] from &#x27;23[0] and upcoming work for this year[1]<p>edit to add:<p>The work of MS researcher Chunyuan Li[2] is worth keeping an eye on, particularly recent work like LLaVA-Interactive[3], a multi-model multi-task AI system, might be what you&#x27;re trying to describe with your laser&#x2F;fractal view phrasing.<p>[0]<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;@VLPTutorial" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;@VLPTutorial</a><p>[1]<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;search&#x2F;?query=cvpr+2024&amp;searchtype=all&amp;source=header" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;search&#x2F;?query=cvpr+2024&amp;searchtype=all&amp;sou...</a><p>[2]<a href="https:&#x2F;&#x2F;chunyuan.li&#x2F;" rel="nofollow">https:&#x2F;&#x2F;chunyuan.li&#x2F;</a><p>[3]<a href="https:&#x2F;&#x2F;llava-vl.github.io&#x2F;llava-interactive&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llava-vl.github.io&#x2F;llava-interactive&#x2F;</a></div><br/></div></div><div id="38861515" class="c"><input type="checkbox" id="c-38861515" checked=""/><div class="controls bullet"><span class="by">conacts</span><span>|</span><a href="#38860599">parent</a><span>|</span><a href="#38861052">prev</a><span>|</span><label class="collapse" for="c-38861515">[-]</label><label class="expand" for="c-38861515">[1 more]</label></div><br/><div class="children"><div class="content">thanks for sharing this!</div><br/></div></div></div></div></div></div></div></div></div></body></html>