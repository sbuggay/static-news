<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716541273151" as="style"/><link rel="stylesheet" href="styles.css?v=1716541273151"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://depot.dev/blog/faster-ec2-boot-time">Making EC2 boot time faster</a> <span class="domain">(<a href="https://depot.dev">depot.dev</a>)</span></div><div class="subtext"><span>jacobwg</span> | <span>110 comments</span></div><br/><div><div id="40458254" class="c"><input type="checkbox" id="c-40458254" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#40457663">next</a><span>|</span><label class="collapse" for="c-40458254">[-]</label><label class="expand" for="c-40458254">[8 more]</label></div><br/><div class="children"><div class="content">Boot time is the number one factor in your success with auto-scaling.  The smaller your boot time, the smaller your prediction window needs to be.  Ex. If your boot time is five minutes, you need to predict what your traffic will be in five minutes, but if you can boot in 20 seconds, you only need to predict 20 seconds ahead.  By definition your predictions will be more accurate the smaller the window is.<p>But!  Autoscaling serves two purposes. One is to address load spikes.  The other is to reduce costs with scaling down.  What this solution does is trade off some of the cost savings by prewarming the EBS volumes and then paying for them.<p>This feels like a reasonable tradeoff if you can justify the cost with better auto-scaling.<p>And if you&#x27;re not autoscaling, it&#x27;s still worth the cost if the trade off is having your engineers wait around for instance boots.</div><br/><div id="40463512" class="c"><input type="checkbox" id="c-40463512" checked=""/><div class="controls bullet"><span class="by">fhuici</span><span>|</span><a href="#40458254">parent</a><span>|</span><a href="#40464260">next</a><span>|</span><label class="collapse" for="c-40463512">[-]</label><label class="expand" for="c-40463512">[1 more]</label></div><br/><div class="children"><div class="content">Fully agree, doing reactive autoscaling when the actual boot time is slow is an inherently hard problem. We&#x27;ve done years of research into building specialized VMs (unikernels) and fast controllers to be able to provide infra that allows VMs&#x2F;containers to cold start, and thus autoscale&#x2F;scale to zero in milliseconds (eg, a simple Node app cold starts in ~50 ms). If interested, you can try it out at kraft.cloud, or check out info about the tech in our blogs (<a href="https:&#x2F;&#x2F;unikraft.io&#x2F;blog&#x2F;" rel="nofollow">https:&#x2F;&#x2F;unikraft.io&#x2F;blog&#x2F;</a>) or the corresponding LF OSS project (www.unikraft.org).</div><br/></div></div><div id="40464260" class="c"><input type="checkbox" id="c-40464260" checked=""/><div class="controls bullet"><span class="by">bushbaba</span><span>|</span><a href="#40458254">parent</a><span>|</span><a href="#40463512">prev</a><span>|</span><a href="#40458911">next</a><span>|</span><label class="collapse" for="c-40464260">[-]</label><label class="expand" for="c-40464260">[1 more]</label></div><br/><div class="children"><div class="content">Autoscaling also makes performance insights easier. It keeps the resources per processed request relatively consistent over time. Whereas resizing not automatically can leads to a lot of operational complexity in understanding how your service will react under different loads.<p>This dang age everyone should use autoscaling. relying on ODCR (capacity reservations) for guaranteeing resources exist.</div><br/></div></div><div id="40458911" class="c"><input type="checkbox" id="c-40458911" checked=""/><div class="controls bullet"><span class="by">sfilmeyer</span><span>|</span><a href="#40458254">parent</a><span>|</span><a href="#40464260">prev</a><span>|</span><a href="#40462914">next</a><span>|</span><label class="collapse" for="c-40458911">[-]</label><label class="expand" for="c-40458911">[3 more]</label></div><br/><div class="children"><div class="content">&gt;By definition your predictions will be more accurate the smaller the window is.<p>Small nit, and this doesn&#x27;t detract from your points. I don&#x27;t think this is universally true by definition, even if it is almost always true. You could come up with some rare conditions where your traffic at t+5 minutes is actually easier to predict than at t+20 seconds. Of course, even in that case you&#x27;re better off (ceteris paribus) being able to spin things up in 20 seconds.</div><br/><div id="40458948" class="c"><input type="checkbox" id="c-40458948" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#40458254">root</a><span>|</span><a href="#40458911">parent</a><span>|</span><a href="#40462914">next</a><span>|</span><label class="collapse" for="c-40458948">[-]</label><label class="expand" for="c-40458948">[2 more]</label></div><br/><div class="children"><div class="content">I can come up with a lot of examples where it is easier to predict further out[0], but that also means I can predict them 20 seconds out. :)<p>[0] For example I can tell you exactly when spikes will happen to Netflix&#x27;s servers on Saturday morning (because the kids all get up at the same time).  And I can tell you there will be spikes on the hour during prime time as people shift from linear TV to streaming (or at least they did a lot more 10 years ago!).  I can also tell you when spikes to Alexa will be because I already know what times peoples alarms are set for.</div><br/><div id="40462270" class="c"><input type="checkbox" id="c-40462270" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#40458254">root</a><span>|</span><a href="#40458948">parent</a><span>|</span><a href="#40462914">next</a><span>|</span><label class="collapse" for="c-40462270">[-]</label><label class="expand" for="c-40462270">[1 more]</label></div><br/><div class="children"><div class="content">Hm... I think there are things with one sided uncertainty, that make the ops example possible.<p>For example, things rarely start before they&#x27;re scheduled, so if you know something is supposed to happen at 5 and last an hour, it&#x27;s reasonable to assume it will be happening at 5:10, so if it&#x27;s 4:59 right now, you&#x27;re more certain that it will happen in 11 minutes than in one minute.<p>Of course, as time progresses your uncertainty curve flattens out (at 5:09 you&#x27;re pretty certain it&#x27;ll happen any moment now), but that&#x27;s only in the future.<p>Even then, it&#x27;s quite reasonable to say that there&#x27;s no time leading up to the event where you&#x27;re more confident it will start imminently than you&#x27;re confident it will be occurring further out.</div><br/></div></div></div></div></div></div><div id="40462914" class="c"><input type="checkbox" id="c-40462914" checked=""/><div class="controls bullet"><span class="by">cricketlover</span><span>|</span><a href="#40458254">parent</a><span>|</span><a href="#40458911">prev</a><span>|</span><a href="#40457663">next</a><span>|</span><label class="collapse" for="c-40462914">[-]</label><label class="expand" for="c-40462914">[2 more]</label></div><br/><div class="children"><div class="content">won&#x27;t there be more noise while predicting just 20s in advance? The longer the duration, the less effects we will see of temporary events like network blips etc. no? sorry I&#x27;m new to software engineering and just trying to learn.</div><br/><div id="40462982" class="c"><input type="checkbox" id="c-40462982" checked=""/><div class="controls bullet"><span class="by">cyberpunk</span><span>|</span><a href="#40458254">root</a><span>|</span><a href="#40462914">parent</a><span>|</span><a href="#40457663">next</a><span>|</span><label class="collapse" for="c-40462982">[-]</label><label class="expand" for="c-40462982">[1 more]</label></div><br/><div class="children"><div class="content">There’s no one answer for it, you need to learn your traffic &#x2F; resource usage patterns and tune the scaling to match your situation.<p>No shortcuts really, although a lot of web applications behave “kinda” similar.<p>Start conservatively and tweak from there.</div><br/></div></div></div></div></div></div><div id="40457663" class="c"><input type="checkbox" id="c-40457663" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#40458254">prev</a><span>|</span><a href="#40455766">next</a><span>|</span><label class="collapse" for="c-40457663">[-]</label><label class="expand" for="c-40457663">[8 more]</label></div><br/><div class="children"><div class="content">From a technical perspective, Amazon has actually optimized this but turned that into &quot;serverless functions&quot;: their ultra-optimized image paired with Firecracker achieve ultra-fast boot-up of virtual Linux machines. IIRC from when Firecracker was being introduced, they are booting up in sub-second times.<p>I wonder if Amazon would ever decide to offer booting the same image with the same hypervisor in EC2 as they do for lambdas?</div><br/><div id="40463471" class="c"><input type="checkbox" id="c-40463471" checked=""/><div class="controls bullet"><span class="by">fhuici</span><span>|</span><a href="#40457663">parent</a><span>|</span><a href="#40459258">next</a><span>|</span><label class="collapse" for="c-40463471">[-]</label><label class="expand" for="c-40463471">[1 more]</label></div><br/><div class="children"><div class="content">[Disclaimer: I&#x27;m with KraftCloud] 
For what it&#x27;s worth, Firecracker&#x2F;the VMM is only one part of the boot process. Among others, there&#x27;s also the controller and the VM&#x2F;OS itself that typically slow things down. In other words, it&#x27;s not enough to just switch in Firecracker and expect cold starts to immediately drop to sub-second levels.<p>On kraft.cloud we&#x27;ve fundamentally redesigned the cloud stack to be able to cold start containers&#x2F;VMs in milliseconds (eg, about 20 millis for nginx, about 50 millis for a basic Node app), and also scale them to zero and autoscale them in milliseconds. If interested there&#x27;s more info about the tech in our blog posts <a href="https:&#x2F;&#x2F;unikraft.io&#x2F;blog&#x2F;" rel="nofollow">https:&#x2F;&#x2F;unikraft.io&#x2F;blog&#x2F;</a> .</div><br/></div></div><div id="40459258" class="c"><input type="checkbox" id="c-40459258" checked=""/><div class="controls bullet"><span class="by">20thr</span><span>|</span><a href="#40457663">parent</a><span>|</span><a href="#40463471">prev</a><span>|</span><a href="#40457745">next</a><span>|</span><label class="collapse" for="c-40459258">[-]</label><label class="expand" for="c-40459258">[1 more]</label></div><br/><div class="children"><div class="content">100% -- EC2&#x27;s general purpose nature is not in my opinion the best fit for ephemeral use-cases. You&#x27;ll be constantly fighting the infrastructure as the set of trade-offs and design goals are widely different.<p>This is why CodeSandbox, Namespace, and even fly.io built special-purpose architectures to guarantee extremely start-up time.<p>In the case of Namespace it&#x27;s ~2sec on cold boots with a set of user-supplied containers, with storage allocations.<p>(Disclaimer, I&#x27;m with Namespace -- <a href="https:&#x2F;&#x2F;namespace.so" rel="nofollow">https:&#x2F;&#x2F;namespace.so</a>)</div><br/></div></div><div id="40457745" class="c"><input type="checkbox" id="c-40457745" checked=""/><div class="controls bullet"><span class="by">arianvanp</span><span>|</span><a href="#40457663">parent</a><span>|</span><a href="#40459258">prev</a><span>|</span><a href="#40458038">next</a><span>|</span><label class="collapse" for="c-40457745">[-]</label><label class="expand" for="c-40457745">[1 more]</label></div><br/><div class="children"><div class="content">And AWS now has a product to spin up Lambdas for GitHub Actions CI runners<p><a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;codebuild&#x2F;latest&#x2F;userguide&#x2F;action-runner.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;codebuild&#x2F;latest&#x2F;userguide&#x2F;actio...</a></div><br/></div></div><div id="40458038" class="c"><input type="checkbox" id="c-40458038" checked=""/><div class="controls bullet"><span class="by">cr125rider</span><span>|</span><a href="#40457663">parent</a><span>|</span><a href="#40457745">prev</a><span>|</span><a href="#40455766">next</a><span>|</span><label class="collapse" for="c-40458038">[-]</label><label class="expand" for="c-40458038">[4 more]</label></div><br/><div class="children"><div class="content">Fargate is an alternative that runs on Firecracker as well. It&#x27;s hidden behind ECS and EKS, however.</div><br/><div id="40462491" class="c"><input type="checkbox" id="c-40462491" checked=""/><div class="controls bullet"><span class="by">mochomocha</span><span>|</span><a href="#40457663">root</a><span>|</span><a href="#40458038">parent</a><span>|</span><a href="#40462899">next</a><span>|</span><label class="collapse" for="c-40462491">[-]</label><label class="expand" for="c-40462491">[1 more]</label></div><br/><div class="children"><div class="content">According to [1] Fargate is actually not using Firecracker, but probably something closer to a single container running in a single-tenant ec2 VM. If true, this makes VM boot-time optimizations and warm pooling even more important for such product.<p>[1]: <a href="https:&#x2F;&#x2F;justingarrison.com&#x2F;blog&#x2F;2024-02-08-fargate-is-not-firecracker&#x2F;" rel="nofollow">https:&#x2F;&#x2F;justingarrison.com&#x2F;blog&#x2F;2024-02-08-fargate-is-not-fi...</a></div><br/></div></div><div id="40462899" class="c"><input type="checkbox" id="c-40462899" checked=""/><div class="controls bullet"><span class="by">plopz</span><span>|</span><a href="#40457663">root</a><span>|</span><a href="#40458038">parent</a><span>|</span><a href="#40462491">prev</a><span>|</span><a href="#40462183">next</a><span>|</span><label class="collapse" for="c-40462899">[-]</label><label class="expand" for="c-40462899">[1 more]</label></div><br/><div class="children"><div class="content">Fargate is too slow without the container cache you can get with ec2</div><br/></div></div><div id="40462183" class="c"><input type="checkbox" id="c-40462183" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#40457663">root</a><span>|</span><a href="#40458038">parent</a><span>|</span><a href="#40462899">prev</a><span>|</span><a href="#40455766">next</a><span>|</span><label class="collapse" for="c-40462183">[-]</label><label class="expand" for="c-40462183">[1 more]</label></div><br/><div class="children"><div class="content">And CodeBuild…</div><br/></div></div></div></div></div></div><div id="40455766" class="c"><input type="checkbox" id="c-40455766" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#40457663">prev</a><span>|</span><a href="#40460618">next</a><span>|</span><label class="collapse" for="c-40455766">[-]</label><label class="expand" for="c-40455766">[8 more]</label></div><br/><div class="children"><div class="content">I don’t use EC2 enough to have played with this, but a big part here is the population of the AMI into the per-instance EBS volume.<p>ISTM one could do much better with an immutable&#x2F;atomic setup: set up an immutable read-only EBS volume, and have each instance share that volume and have a per-instance  volume that starts out blank.<p>Actually pulling this off looks like it would be limited by the rules of EBS Multi-Attach.  One could have fun experimenting with an extremely minimal boot AMI that streams a squashfs or similar file from S3 and unpacks it.<p>edit: contemplating a bit, unless you are willing to babysit your deployment and operate under serious constraints, EBS multi-attach looks like the wrong solution. I think the right approach would be build a very very small AMI that sets up a rootfs using s3fs or a similar technology and optionally puts an overlayfs on top. Alternatively, it could set up a block device backed by an S3 file and optionally use it as a base layer of a device-mapper stack.  There’s plenty of room to optimize this.</div><br/><div id="40463926" class="c"><input type="checkbox" id="c-40463926" checked=""/><div class="controls bullet"><span class="by">antihero</span><span>|</span><a href="#40455766">parent</a><span>|</span><a href="#40457590">next</a><span>|</span><label class="collapse" for="c-40463926">[-]</label><label class="expand" for="c-40463926">[1 more]</label></div><br/><div class="children"><div class="content">Could you make a snapshot of the booted instance then and boot other instances from that?</div><br/></div></div><div id="40457590" class="c"><input type="checkbox" id="c-40457590" checked=""/><div class="controls bullet"><span class="by">Szpadel</span><span>|</span><a href="#40455766">parent</a><span>|</span><a href="#40463926">prev</a><span>|</span><a href="#40455981">next</a><span>|</span><label class="collapse" for="c-40457590">[-]</label><label class="expand" for="c-40457590">[1 more]</label></div><br/><div class="children"><div class="content">we used s3fs in production. please don&#x27;t use it, it&#x27;s unreliable, unpredictable failure modes, can bring whole instance down.
if you really need something like that use rclone mount</div><br/></div></div><div id="40455981" class="c"><input type="checkbox" id="c-40455981" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#40455766">parent</a><span>|</span><a href="#40457590">prev</a><span>|</span><a href="#40457364">next</a><span>|</span><label class="collapse" for="c-40455981">[-]</label><label class="expand" for="c-40455981">[1 more]</label></div><br/><div class="children"><div class="content">I believe they addressed this in their post because one cannot (currently?) `aws ec2 run-instances --volume-id vol-cafebabe`, rather one can only tell AWS what volume parameters to use when they <i>create</i> the root device. Your theory may still be sound about using some kind of super bare bones AMI but there will be no such outcome of &quot;hey, friend, use this existing EBS as your root volume, don&#x27;t create a new one&quot;</div><br/></div></div><div id="40457364" class="c"><input type="checkbox" id="c-40457364" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#40455766">parent</a><span>|</span><a href="#40455981">prev</a><span>|</span><a href="#40459075">next</a><span>|</span><label class="collapse" for="c-40457364">[-]</label><label class="expand" for="c-40457364">[2 more]</label></div><br/><div class="children"><div class="content">Isn’t EBS multi-attach only available for the (very expensive) io1 &#x2F; io2 volume types?</div><br/><div id="40457658" class="c"><input type="checkbox" id="c-40457658" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#40455766">root</a><span>|</span><a href="#40457364">parent</a><span>|</span><a href="#40459075">next</a><span>|</span><label class="collapse" for="c-40457658">[-]</label><label class="expand" for="c-40457658">[1 more]</label></div><br/><div class="children"><div class="content">Hmm, it does look like it, although one could carefully use large IO.<p>But the bigger issue might be durability. Most EBS types have rather low quoted durability, and, for a shared volume like this, that’s a problem.  Using S3 instead would be better all around except for the smallish engineering effort and deployment effort needed.<p>Getting a tool like mkosi to generate a boot-from-S3 setup should be straightforward. Converting most any bootable container should also be doable, even automatically. Converting an AMI would involve more heuristics and be more fragile, but it ought to work reliably with most modern Linux distros.</div><br/></div></div></div></div><div id="40459075" class="c"><input type="checkbox" id="c-40459075" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#40455766">parent</a><span>|</span><a href="#40457364">prev</a><span>|</span><a href="#40460618">next</a><span>|</span><label class="collapse" for="c-40459075">[-]</label><label class="expand" for="c-40459075">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s reinventing ebs&#x2F;ami&#x2F;snapshots. They are already doing it i.e. data goes lazily from s3 to ebs&#x2F;ec2.</div><br/><div id="40461012" class="c"><input type="checkbox" id="c-40461012" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#40455766">root</a><span>|</span><a href="#40459075">parent</a><span>|</span><a href="#40460618">next</a><span>|</span><label class="collapse" for="c-40461012">[-]</label><label class="expand" for="c-40461012">[1 more]</label></div><br/><div class="children"><div class="content">It’s not, though. The way to boot a transient OS (just like a transient instance of a container on a machine&#x2F;instance with a container runtime) is to give userspace <i>read-only</i> access to the image. It can be outright read-only or it can be an actual efficient overlay mechanism (qcow, overlayfs, device-mapper snapshot, etc).  EBS, as the article notes, can’t actually do a read-only mount of a snapshot <i>at all</i>, and it’s very inefficient at instantiating a volume from a snapshot.</div><br/></div></div></div></div></div></div><div id="40460618" class="c"><input type="checkbox" id="c-40460618" checked=""/><div class="controls bullet"><span class="by">fduran</span><span>|</span><a href="#40455766">prev</a><span>|</span><a href="#40463925">next</a><span>|</span><label class="collapse" for="c-40460618">[-]</label><label class="expand" for="c-40460618">[4 more]</label></div><br/><div class="children"><div class="content">So I&#x27;ve created ~300k ec2 instances with SadServers and my experience was that starting an ec2 VM from stopped took ~30 seconds and creating one from AMI took ~50 seconds.<p>Recently I decided to actually look at boot times since I store in the db when the servers are requested and when they become ready and it turns out for me it&#x27;s really bi-modal; some take about 15-20s and many take about 80s, see graph <a href="https:&#x2F;&#x2F;x.com&#x2F;sadservers_com&#x2F;status&#x2F;1782081065672118367" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;sadservers_com&#x2F;status&#x2F;1782081065672118367</a><p>Pretty baffled by this (same region, same pretty much everything), any idea why?. Definitively going to try this trick in the article.</div><br/><div id="40461703" class="c"><input type="checkbox" id="c-40461703" checked=""/><div class="controls bullet"><span class="by">paranoidrobot</span><span>|</span><a href="#40460618">parent</a><span>|</span><a href="#40460801">next</a><span>|</span><label class="collapse" for="c-40461703">[-]</label><label class="expand" for="c-40461703">[1 more]</label></div><br/><div class="children"><div class="content">My guess is probably related to AWS Spot capacity.<p>The second and third spikes at 80 and 140 seconds lines up nicely with this kind of behavior.<p>The second spike would be optimised workloads that can respond to spot interruption in under 60 seconds.<p>The third spike would be Spot workloads that are being force-terminated.<p>The reason it&#x27;s falling on those bounds is because of whatever is trying to schedule your workload only re-checks for free capacity once a minute.<p>I used to be able to spin up spot instances and basically never get interruptions. They&#x27;d stay on for weeks&#x2F;months.<p>In my experience, it used to be fairly safe to have Spot instances for most workloads. You&#x27;d almost never get Spot interruptions.  Now, some regions and instance types are difficult to run Spot instances at all.</div><br/></div></div><div id="40460801" class="c"><input type="checkbox" id="c-40460801" checked=""/><div class="controls bullet"><span class="by">fletchowns</span><span>|</span><a href="#40460618">parent</a><span>|</span><a href="#40461703">prev</a><span>|</span><a href="#40463925">next</a><span>|</span><label class="collapse" for="c-40460801">[-]</label><label class="expand" for="c-40460801">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps in one case you are getting a slice of a machine that is already running, versus AWS powering up a machine that was offline and getting a slice of that one?</div><br/><div id="40461080" class="c"><input type="checkbox" id="c-40461080" checked=""/><div class="controls bullet"><span class="by">fduran</span><span>|</span><a href="#40460618">root</a><span>|</span><a href="#40460801">parent</a><span>|</span><a href="#40463925">next</a><span>|</span><label class="collapse" for="c-40461080">[-]</label><label class="expand" for="c-40461080">[1 more]</label></div><br/><div class="children"><div class="content">Yes, some internal (AWS operation) explanation like the one you suggest makes sense.</div><br/></div></div></div></div></div></div><div id="40463925" class="c"><input type="checkbox" id="c-40463925" checked=""/><div class="controls bullet"><span class="by">nathants</span><span>|</span><a href="#40460618">prev</a><span>|</span><a href="#40455968">next</a><span>|</span><label class="collapse" for="c-40463925">[-]</label><label class="expand" for="c-40463925">[1 more]</label></div><br/><div class="children"><div class="content">in the us-west-2-lax-1a local zone, i just booted 100 r5.xlarge spot instances as fortnite like game servers[1]. 1 to be a central server, 99 to be fake players. the server broadcasts x100 write amplified data from every player to every player. the 101st serve is my local pc.<p>the server broadcasts at 200 MB&#x2F;s[2]. the whole setup costs me $3-4 usd&#x2F;hour and by far the slowest part of boot is my game compiling on the central server, whether i store ccache data in s3 or not. i&#x27;ve booted this every day for the last 6 months, to test the game.<p>if your system can&#x27;t handle 30s vm boots, your system should improve.<p>1. <a href="https:&#x2F;&#x2F;r2.nathants.workers.dev&#x2F;ec2_snitch.png" rel="nofollow">https:&#x2F;&#x2F;r2.nathants.workers.dev&#x2F;ec2_snitch.png</a><p>2. <a href="https:&#x2F;&#x2F;r2.nathants.workers.dev&#x2F;ec2_boot.mp4" rel="nofollow">https:&#x2F;&#x2F;r2.nathants.workers.dev&#x2F;ec2_boot.mp4</a></div><br/></div></div><div id="40455968" class="c"><input type="checkbox" id="c-40455968" checked=""/><div class="controls bullet"><span class="by">develatio</span><span>|</span><a href="#40463925">prev</a><span>|</span><a href="#40459105">next</a><span>|</span><label class="collapse" for="c-40455968">[-]</label><label class="expand" for="c-40455968">[26 more]</label></div><br/><div class="children"><div class="content">Maybe AWS should actually take a look into this. I know comparing AWS to other (smaller) cloud providers is not totally fair given the size of AWS, but for example creating &#x2F; booting an instance in Hetzner takes a few seconds.</div><br/><div id="40457438" class="c"><input type="checkbox" id="c-40457438" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#40455968">parent</a><span>|</span><a href="#40463717">next</a><span>|</span><label class="collapse" for="c-40457438">[-]</label><label class="expand" for="c-40457438">[4 more]</label></div><br/><div class="children"><div class="content">It also takes a few seconds on AWS. The guy is comparing setting up a whole new machine from an image, with network and all, to turning on a stopped EC2 instance.<p>The latter takes a few seconds, the former is presumably longer. This is the great relevation of this blog post.</div><br/><div id="40457833" class="c"><input type="checkbox" id="c-40457833" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40457438">parent</a><span>|</span><a href="#40463717">next</a><span>|</span><label class="collapse" for="c-40457833">[-]</label><label class="expand" for="c-40457833">[3 more]</label></div><br/><div class="children"><div class="content">wait, restarting a stopped machine is faster than launching an AMI from scracth is a great revelation?<p>That&#x27;s like saying waking your MacbookPro is faster than booting from powered off state. Of course it is, and that&#x27;s precisely why the option exists.</div><br/><div id="40459121" class="c"><input type="checkbox" id="c-40459121" checked=""/><div class="controls bullet"><span class="by">jpambrun</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40457833">parent</a><span>|</span><a href="#40458905">next</a><span>|</span><label class="collapse" for="c-40459121">[-]</label><label class="expand" for="c-40459121">[1 more]</label></div><br/><div class="children"><div class="content">I think this is unexpected. I expected that once created, my boot volume would have the same performance on the first boot than on the second. It&#x27;s really not obvious that the volume is really empty and lazily loaded from S3. The proposed work around is also a bit silly: read all blocks one by one even tho maybe 1% of the block have something in them on a new VM. This is actually a revelation.</div><br/></div></div><div id="40458905" class="c"><input type="checkbox" id="c-40458905" checked=""/><div class="controls bullet"><span class="by">mdeeks</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40457833">parent</a><span>|</span><a href="#40459121">prev</a><span>|</span><a href="#40463717">next</a><span>|</span><label class="collapse" for="c-40458905">[-]</label><label class="expand" for="c-40458905">[1 more]</label></div><br/><div class="children"><div class="content">If you aren&#x27;t familiar with how EBS works and how volumes are warmed, then yes, this is an interesting blog post. Not everyone is an expert. They become experts by reading things like this and learning.<p>If you didn&#x27;t know about this EBS behavior it would be logical to assume that booting from scratch is roughly equivalent to starting&#x2F;stopping&#x2F;starting again.</div><br/></div></div></div></div></div></div><div id="40463717" class="c"><input type="checkbox" id="c-40463717" checked=""/><div class="controls bullet"><span class="by">flaminHotSpeedo</span><span>|</span><a href="#40455968">parent</a><span>|</span><a href="#40457438">prev</a><span>|</span><a href="#40456657">next</a><span>|</span><label class="collapse" for="c-40463717">[-]</label><label class="expand" for="c-40463717">[1 more]</label></div><br/><div class="children"><div class="content">At least if you&#x27;re using the Ec2 optimized amis, Ec2 instances frequently boot fast enough that they&#x27;ll be executing your username initialization before you get the run instances response.<p>Though there&#x27;s a long tail, so sometimes there can be a gap on the order of a couple second between the sync response and when the bootloader transfers over to the kernel.</div><br/></div></div><div id="40456657" class="c"><input type="checkbox" id="c-40456657" checked=""/><div class="controls bullet"><span class="by">everfrustrated</span><span>|</span><a href="#40455968">parent</a><span>|</span><a href="#40463717">prev</a><span>|</span><a href="#40459092">next</a><span>|</span><label class="collapse" for="c-40456657">[-]</label><label class="expand" for="c-40456657">[8 more]</label></div><br/><div class="children"><div class="content">Hetzner does not offer a network block storage comparable to EBS that can be used as a root (bootable) file system.
AWS local-attached ephemeral disk are also immediately available but cannot be seeded with data (same as Hetzner they are wiped clean ahead of boot).</div><br/><div id="40456780" class="c"><input type="checkbox" id="c-40456780" checked=""/><div class="controls bullet"><span class="by">andersa</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456657">parent</a><span>|</span><a href="#40459092">next</a><span>|</span><label class="collapse" for="c-40456780">[-]</label><label class="expand" for="c-40456780">[7 more]</label></div><br/><div class="children"><div class="content">This is an advantage. EBS is terrible! Literally orders of magnitude slower than modern SSDs.</div><br/><div id="40457499" class="c"><input type="checkbox" id="c-40457499" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456780">parent</a><span>|</span><a href="#40456879">next</a><span>|</span><label class="collapse" for="c-40457499">[-]</label><label class="expand" for="c-40457499">[3 more]</label></div><br/><div class="children"><div class="content">Depends on your definition of slow. Throughput-wise, I think it’s fairly decent — we typically set up 4 EBS volumes in raid0 and get 4GB&#x2F;sec for a really decent price.</div><br/><div id="40457993" class="c"><input type="checkbox" id="c-40457993" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40457499">parent</a><span>|</span><a href="#40456879">next</a><span>|</span><label class="collapse" for="c-40457993">[-]</label><label class="expand" for="c-40457993">[2 more]</label></div><br/><div class="children"><div class="content">Sequential throughput <i>can</i> be fine. Random access is always going to be orders of magnitude slower than a direct-attach disk.<p>Remember why we switched from spinning hard drives to SSDs? Well EBS is like going back to a spinning drive.</div><br/><div id="40461967" class="c"><input type="checkbox" id="c-40461967" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40457993">parent</a><span>|</span><a href="#40456879">next</a><span>|</span><label class="collapse" for="c-40461967">[-]</label><label class="expand" for="c-40461967">[1 more]</label></div><br/><div class="children"><div class="content">I think AWS is aware of this. Which is why they have instances with attached SSD’s</div><br/></div></div></div></div></div></div><div id="40456879" class="c"><input type="checkbox" id="c-40456879" checked=""/><div class="controls bullet"><span class="by">tekla</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456780">parent</a><span>|</span><a href="#40457499">prev</a><span>|</span><a href="#40459092">next</a><span>|</span><label class="collapse" for="c-40456879">[-]</label><label class="expand" for="c-40456879">[3 more]</label></div><br/><div class="children"><div class="content">EBS is great for workloads that dont require SSDs, which most don&#x27;t.<p>If it does, you can do provisioned which will get you alot more or go NVME.</div><br/><div id="40458010" class="c"><input type="checkbox" id="c-40458010" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456879">parent</a><span>|</span><a href="#40459092">next</a><span>|</span><label class="collapse" for="c-40458010">[-]</label><label class="expand" for="c-40458010">[2 more]</label></div><br/><div class="children"><div class="content">Even provisioned won&#x27;t get you the access times of a direct-attached SSD. Speed of light and all that - EBS is using the network under the hood, it&#x27;s not a direct connection to the host.</div><br/><div id="40458151" class="c"><input type="checkbox" id="c-40458151" checked=""/><div class="controls bullet"><span class="by">tekla</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40458010">parent</a><span>|</span><a href="#40459092">next</a><span>|</span><label class="collapse" for="c-40458151">[-]</label><label class="expand" for="c-40458151">[1 more]</label></div><br/><div class="children"><div class="content">Yes I know, and? Thats why I mentioned NVME</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40459092" class="c"><input type="checkbox" id="c-40459092" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#40455968">parent</a><span>|</span><a href="#40456657">prev</a><span>|</span><a href="#40456459">next</a><span>|</span><label class="collapse" for="c-40459092">[-]</label><label class="expand" for="c-40459092">[1 more]</label></div><br/><div class="children"><div class="content">It depends on instance type and OS and can be real short on ec2.</div><br/></div></div><div id="40456459" class="c"><input type="checkbox" id="c-40456459" checked=""/><div class="controls bullet"><span class="by">tekla</span><span>|</span><a href="#40455968">parent</a><span>|</span><a href="#40459092">prev</a><span>|</span><a href="#40456108">next</a><span>|</span><label class="collapse" for="c-40456459">[-]</label><label class="expand" for="c-40456459">[1 more]</label></div><br/><div class="children"><div class="content">They have and I know this because I&#x27;ve hammered them on this because we demand thousands of instances to autoscale very aggressively in 1-3 minutes. Very few people give a shit about initialization times. They care more about instance ready times which is constrained by the OS that is running.</div><br/></div></div><div id="40456108" class="c"><input type="checkbox" id="c-40456108" checked=""/><div class="controls bullet"><span class="by">matt-p</span><span>|</span><a href="#40455968">parent</a><span>|</span><a href="#40456459">prev</a><span>|</span><a href="#40459105">next</a><span>|</span><label class="collapse" for="c-40456108">[-]</label><label class="expand" for="c-40456108">[10 more]</label></div><br/><div class="children"><div class="content">What&#x27;s size got to do with boot time? Serious question.</div><br/><div id="40456419" class="c"><input type="checkbox" id="c-40456419" checked=""/><div class="controls bullet"><span class="by">develatio</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456108">parent</a><span>|</span><a href="#40456166">next</a><span>|</span><label class="collapse" for="c-40456419">[-]</label><label class="expand" for="c-40456419">[3 more]</label></div><br/><div class="children"><div class="content">By &quot;the size&quot; I meant to say &quot;the size of the infrastructure&quot;, meaning that AWS has to manage orders of magnitude more instances than Hetzner. This might as well contribute to &quot;things&quot; being slower.</div><br/><div id="40461310" class="c"><input type="checkbox" id="c-40461310" checked=""/><div class="controls bullet"><span class="by">matt-p</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456419">parent</a><span>|</span><a href="#40456631">next</a><span>|</span><label class="collapse" for="c-40461310">[-]</label><label class="expand" for="c-40461310">[1 more]</label></div><br/><div class="children"><div class="content">I understand that, apologies for not making that clear.<p>I guess I&#x27;m just struggling to see how having more VMs means it takes longer to provision one? A database query to find the space and get an allocation should be milliseconds in either case.</div><br/></div></div><div id="40456631" class="c"><input type="checkbox" id="c-40456631" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456419">parent</a><span>|</span><a href="#40461310">prev</a><span>|</span><a href="#40456166">next</a><span>|</span><label class="collapse" for="c-40456631">[-]</label><label class="expand" for="c-40456631">[1 more]</label></div><br/><div class="children"><div class="content">Arguably it can also make things faster.    A small provider might need to migrate other instances around to make space for your new instance, whereas a big provider almost certainly can satisfy your request from existing free capacity, and it should therefore be a matter of milliseconds to identify the physical machine your new VM will run on.</div><br/></div></div></div></div><div id="40456166" class="c"><input type="checkbox" id="c-40456166" checked=""/><div class="controls bullet"><span class="by">RationPhantoms</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456108">parent</a><span>|</span><a href="#40456419">prev</a><span>|</span><a href="#40456492">next</a><span>|</span><label class="collapse" for="c-40456166">[-]</label><label class="expand" for="c-40456166">[2 more]</label></div><br/><div class="children"><div class="content">More employed eyes on an issue or ability to compensate the best-in-class engineers to take a look.</div><br/><div id="40461274" class="c"><input type="checkbox" id="c-40461274" checked=""/><div class="controls bullet"><span class="by">matt-p</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456166">parent</a><span>|</span><a href="#40456492">next</a><span>|</span><label class="collapse" for="c-40461274">[-]</label><label class="expand" for="c-40461274">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, so a large provider should theoretically have faster boot times if anything, no? More chance of having a customer who cares deeply, more wasted CPU cycles (and therefore incentive to optimise) as it&#x27;s multiplied by 10 million rather than 10 thousand instance starts. More likely to have larger quantities of engineers available, more monitoring&#x2F;data&#x2F;telematics.</div><br/></div></div></div></div><div id="40456492" class="c"><input type="checkbox" id="c-40456492" checked=""/><div class="controls bullet"><span class="by">playingalong</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456108">parent</a><span>|</span><a href="#40456166">prev</a><span>|</span><a href="#40456268">next</a><span>|</span><label class="collapse" for="c-40456492">[-]</label><label class="expand" for="c-40456492">[2 more]</label></div><br/><div class="children"><div class="content">Likely they mean that following Conway&#x27;s law in AWS there are more abstraction layers involved.</div><br/><div id="40461333" class="c"><input type="checkbox" id="c-40461333" checked=""/><div class="controls bullet"><span class="by">matt-p</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456492">parent</a><span>|</span><a href="#40456268">next</a><span>|</span><label class="collapse" for="c-40461333">[-]</label><label class="expand" for="c-40461333">[1 more]</label></div><br/><div class="children"><div class="content">Most likely true, but things are going deeply wrong if fresh Debian VM boot times are a function of the organisational structure of your hosting provider.</div><br/></div></div></div></div><div id="40456268" class="c"><input type="checkbox" id="c-40456268" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456108">parent</a><span>|</span><a href="#40456492">prev</a><span>|</span><a href="#40459105">next</a><span>|</span><label class="collapse" for="c-40456268">[-]</label><label class="expand" for="c-40456268">[2 more]</label></div><br/><div class="children"><div class="content">Smaller companies are faster and more nimble than larger corporations.</div><br/><div id="40461295" class="c"><input type="checkbox" id="c-40461295" checked=""/><div class="controls bullet"><span class="by">matt-p</span><span>|</span><a href="#40455968">root</a><span>|</span><a href="#40456268">parent</a><span>|</span><a href="#40459105">next</a><span>|</span><label class="collapse" for="c-40461295">[-]</label><label class="expand" for="c-40461295">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but why specifically does that mean boot times of a VM are faster? One has to be &#x27;nimble&#x27; to improve that?</div><br/></div></div></div></div></div></div></div></div><div id="40459105" class="c"><input type="checkbox" id="c-40459105" checked=""/><div class="controls bullet"><span class="by">mnutt</span><span>|</span><a href="#40455968">prev</a><span>|</span><a href="#40457920">next</a><span>|</span><label class="collapse" for="c-40459105">[-]</label><label class="expand" for="c-40459105">[3 more]</label></div><br/><div class="children"><div class="content">They talk about the limitations of the EC2 autoscaler and mention calling LaunchInstances themselves, but are there any autoscaler service projects for EC2 ASGs out there? The AWS-provided one is slow (as they mention), annoyingly opaque, and has all kinds of limitations like not being able to use Warm Pools with multiple instance types etc.</div><br/><div id="40461403" class="c"><input type="checkbox" id="c-40461403" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#40459105">parent</a><span>|</span><a href="#40457920">next</a><span>|</span><label class="collapse" for="c-40461403">[-]</label><label class="expand" for="c-40461403">[2 more]</label></div><br/><div class="children"><div class="content">I am a little confused by your mention of &quot;EC2 autoscaler&quot; and then &quot;EC2 ASG&quot; autoscaler, but if I&#x27;m hearing you correctly and you&#x27;d want &quot;self managed ASGs,&quot; then you may have some success adapting Keda &lt;<a href="https:&#x2F;&#x2F;github.com&#x2F;kedacore&#x2F;keda#readme">https:&#x2F;&#x2F;github.com&#x2F;kedacore&#x2F;keda#readme</a>&gt; (or your-favorite-event-driven-gizmo) to monitor the metrics that interest you and driving ec2.LaunchInstances on the other side, since as very best I can tell that&#x27;s what ASGs are doing just using <i>their</i> serverless-event-something-or-other versus <i>your</i> serverless-event-something-or-other. I would suspect you could even continue to use the existing ec2.LaunchTemplate as the &quot;stamp out copies of these&quot; system, since there doesn&#x27;t <i>appear</i> to be anything especially ASG-y about them, just that is the only(?) consumer thus far<p>After having typed all that out, I recalled that Open Stack exists and thus they may get you <i>ever further</i> toward your goal since they are trying to be on-prem AWS: <a href="https:&#x2F;&#x2F;docs.openstack.org&#x2F;auto-scaling-sig&#x2F;latest&#x2F;theory-of-auto-scaling.html#components-of-auto-scaling" rel="nofollow">https:&#x2F;&#x2F;docs.openstack.org&#x2F;auto-scaling-sig&#x2F;latest&#x2F;theory-of...</a></div><br/><div id="40463731" class="c"><input type="checkbox" id="c-40463731" checked=""/><div class="controls bullet"><span class="by">flaminHotSpeedo</span><span>|</span><a href="#40459105">root</a><span>|</span><a href="#40461403">parent</a><span>|</span><a href="#40457920">next</a><span>|</span><label class="collapse" for="c-40463731">[-]</label><label class="expand" for="c-40463731">[1 more]</label></div><br/><div class="children"><div class="content">Yeah that&#x27;s basically what asg does, you can see the createFleet requests in cloudtrail</div><br/></div></div></div></div></div></div><div id="40457920" class="c"><input type="checkbox" id="c-40457920" checked=""/><div class="controls bullet"><span class="by">crohr</span><span>|</span><a href="#40459105">prev</a><span>|</span><a href="#40463036">next</a><span>|</span><label class="collapse" for="c-40457920">[-]</label><label class="expand" for="c-40457920">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  while we can boot the Actions runner within 5 seconds of a job starting, it can take GitHub 10+ seconds to actually deliver that job to the runner<p>This. I went the same route with regards to boot time optimisations for [1] (cleaning up the AMI, cloud-init, etc.), and can boot a VM from cold in 15s (I can&#x27;t rely on prewarming pools of machines -- even stopped -- since RunsOn doesn&#x27;t share machines with multiple clients and this would not make sense economically).<p>But the time taken by the official runner binary to load and then get assigned a job by GitHub always takes around 8s, which is more than half of the VM boot time :( At some point it would be great if GitHub could give us a leaner runner binary with less legacy stuff, and tailored for ephemeral runners (that, or reverse-engineer the protocol).<p>[1] <a href="https:&#x2F;&#x2F;runs-on.com" rel="nofollow">https:&#x2F;&#x2F;runs-on.com</a></div><br/></div></div><div id="40463036" class="c"><input type="checkbox" id="c-40463036" checked=""/><div class="controls bullet"><span class="by">albert_e</span><span>|</span><a href="#40457920">prev</a><span>|</span><a href="#40456982">next</a><span>|</span><label class="collapse" for="c-40463036">[-]</label><label class="expand" for="c-40463036">[2 more]</label></div><br/><div class="children"><div class="content">AWS will (should) make this an optional feature.<p>Often the technology is the easier part.<p>The difficult part is how to name the feature intuitively, adding to an ocean of jargon and documentation, and making the configuration knobs intuitive both in UI and CLI&#x2F;SDK.<p>Amazon Simple Compute Service :) ?</div><br/><div id="40463669" class="c"><input type="checkbox" id="c-40463669" checked=""/><div class="controls bullet"><span class="by">kylegalbraith</span><span>|</span><a href="#40463036">parent</a><span>|</span><a href="#40456982">next</a><span>|</span><label class="collapse" for="c-40463669">[-]</label><label class="expand" for="c-40463669">[1 more]</label></div><br/><div class="children"><div class="content">Other founder of Depot here. AWS is pretty close to this idea with their Warm Pools [0]. But for our use case, they&#x27;re just too slow to react to changes. We observed 60s+ to notice a change and actually start the machine. That doesn&#x27;t work when we need to launch the machine as quickly as possible in reaction to a pending GHA job.<p>That said, I think this is a problem they could likely solve with that functionality, and we&#x27;d love to use it.<p>[0] <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;autoscaling&#x2F;ec2&#x2F;userguide&#x2F;ec2-auto-scaling-warm-pools.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;autoscaling&#x2F;ec2&#x2F;userguide&#x2F;ec2-au...</a></div><br/></div></div></div></div><div id="40456982" class="c"><input type="checkbox" id="c-40456982" checked=""/><div class="controls bullet"><span class="by">bingemaker</span><span>|</span><a href="#40463036">prev</a><span>|</span><a href="#40460576">next</a><span>|</span><label class="collapse" for="c-40456982">[-]</label><label class="expand" for="c-40456982">[1 more]</label></div><br/><div class="children"><div class="content">Curious, how do you measure the time taken for those 4 steps listed in &quot;What takes so long?&quot; section?</div><br/></div></div><div id="40457285" class="c"><input type="checkbox" id="c-40457285" checked=""/><div class="controls bullet"><span class="by">waiwai933</span><span>|</span><a href="#40460576">prev</a><span>|</span><a href="#40461848">next</a><span>|</span><label class="collapse" for="c-40457285">[-]</label><label class="expand" for="c-40457285">[2 more]</label></div><br/><div class="children"><div class="content">I believe this is similar to EC2 Fast Launch which is available for Windows AMIs, but I don&#x27;t know exactly how that works under the hood.<p><a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;WindowsGuide&#x2F;win-ami-config-fast-launch.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;WindowsGuide&#x2F;win-a...</a></div><br/><div id="40463637" class="c"><input type="checkbox" id="c-40463637" checked=""/><div class="controls bullet"><span class="by">mcbain</span><span>|</span><a href="#40457285">parent</a><span>|</span><a href="#40461848">next</a><span>|</span><label class="collapse" for="c-40463637">[-]</label><label class="expand" for="c-40463637">[1 more]</label></div><br/><div class="children"><div class="content">It does launch an instance and take a snapshot but what&#x27;s happening is the sysprep and OOBE stuff that can take 10 mins or so (you can find it in the console and startup logs). That&#x27;s a lot more overheard than just hydrating an EBS volume.<p><a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;WindowsGuide&#x2F;win-ami-config-fast-launch.html#win-fast-launch-key-terms" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;WindowsGuide&#x2F;win-a...</a></div><br/></div></div></div></div><div id="40461848" class="c"><input type="checkbox" id="c-40461848" checked=""/><div class="controls bullet"><span class="by">orf</span><span>|</span><a href="#40457285">prev</a><span>|</span><a href="#40456929">next</a><span>|</span><label class="collapse" for="c-40461848">[-]</label><label class="expand" for="c-40461848">[1 more]</label></div><br/><div class="children"><div class="content">It seems that you want to make your root volume as small as possible, and use it to only attach a pre-warmed pool of EBS volumes at launch time that contain the actual config&#x2F;data you need?<p>You can launch a stripped down distribution with what, a 200mb disk? Then attach the “useful” EBS volume, and “do stuff” with that - launch a container, or whatever.</div><br/></div></div><div id="40456929" class="c"><input type="checkbox" id="c-40456929" checked=""/><div class="controls bullet"><span class="by">cmckn</span><span>|</span><a href="#40461848">prev</a><span>|</span><a href="#40456793">next</a><span>|</span><label class="collapse" for="c-40456929">[-]</label><label class="expand" for="c-40456929">[2 more]</label></div><br/><div class="children"><div class="content">You can enable fast restore on the EBS snapshot that backs your AMI: <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;ebs&#x2F;latest&#x2F;userguide&#x2F;ebs-fast-snapshot-restore.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;ebs&#x2F;latest&#x2F;userguide&#x2F;ebs-fast-sn...</a><p>It’s not cheap, but it speeds things up.</div><br/><div id="40457465" class="c"><input type="checkbox" id="c-40457465" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#40456929">parent</a><span>|</span><a href="#40456793">next</a><span>|</span><label class="collapse" for="c-40457465">[-]</label><label class="expand" for="c-40457465">[1 more]</label></div><br/><div class="children"><div class="content">$540&#x2F;month per EBS volume per AZ. And it’s still fairly limited, at a maximum of 8 credits, it wouldn’t nearly cover the use case described in the article (launching 50 instances quickly).</div><br/></div></div></div></div><div id="40456793" class="c"><input type="checkbox" id="c-40456793" checked=""/><div class="controls bullet"><span class="by">everfrustrated</span><span>|</span><a href="#40456929">prev</a><span>|</span><a href="#40458065">next</a><span>|</span><label class="collapse" for="c-40456793">[-]</label><label class="expand" for="c-40456793">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s too bad that EBS doesn&#x27;t natively support Copy-On-Write.<p>Snapshots are persisted into S3 (transparently to the user) but it means each new EBS volume spawned doesn&#x27;t start at full IOPS allocation.<p>I presume this is due to EBS volumes being specific-AZ so to be able to launch an AMI-seeded EBS volume in any AZ it needs to go via S3 (multi-AZ)</div><br/><div id="40456939" class="c"><input type="checkbox" id="c-40456939" checked=""/><div class="controls bullet"><span class="by">Twirrim</span><span>|</span><a href="#40456793">parent</a><span>|</span><a href="#40458065">next</a><span>|</span><label class="collapse" for="c-40456939">[-]</label><label class="expand" for="c-40456939">[1 more]</label></div><br/><div class="children"><div class="content">EBS volumes are &quot;expensive&quot; compared to S3, due to the limitations of what you can do with live block volumes + replicas, vs S3.  It takes more disk space to have an image be a provisioned volume ready to be used for copy-on-write, vs having it as something backed up in S3.  So the incentives aren&#x27;t there vs just trying to make the volume creation process as smooth and fast as possible.<p>I&#x27;d guess it&#x27;s likely that EBS is using a tiered caching system, where they&#x27;ll keep live volumes around for Copy-on-write cloning for the more popular images&#x2F;snapshots, with slightly less popular images maybe stored in an EBS cache of some form, before it goes all the way back to S3.  You&#x27;re just not likely to end up getting a live volume level of caching until you hit a certain threshold of launches.</div><br/></div></div></div></div><div id="40458065" class="c"><input type="checkbox" id="c-40458065" checked=""/><div class="controls bullet"><span class="by">suryao</span><span>|</span><a href="#40456793">prev</a><span>|</span><a href="#40462286">next</a><span>|</span><label class="collapse" for="c-40458065">[-]</label><label class="expand" for="c-40458065">[2 more]</label></div><br/><div class="children"><div class="content">This is very cool optimization.<p>I make a similar product offering fast Github actions runners[1] and we&#x27;ve been down this rabbit hole of boot time optimization.<p>Eventually, we realized that the best solution is to actually build scale. There are two factors in your favor then:
1) Spikes are less pronounced and the workloads are a lot more predictable.
2) The predictability means that you have a decent estimate of the workload to expect at any given time, within reason for maintaining an efficient warm pool.<p>This enables us to simplify the stack and not have high-maintenance optimizations while delivering great user experience.<p>We have some pretty heavy use customers that enable us to do this.<p>[1] <a href="https:&#x2F;&#x2F;www.warpbuild.com">https:&#x2F;&#x2F;www.warpbuild.com</a></div><br/><div id="40461369" class="c"><input type="checkbox" id="c-40461369" checked=""/><div class="controls bullet"><span class="by">matt-p</span><span>|</span><a href="#40458065">parent</a><span>|</span><a href="#40462286">next</a><span>|</span><label class="collapse" for="c-40461369">[-]</label><label class="expand" for="c-40461369">[1 more]</label></div><br/><div class="children"><div class="content">This is almost always the answer, adding an instance should be a fairly rare event.</div><br/></div></div></div></div><div id="40462286" class="c"><input type="checkbox" id="c-40462286" checked=""/><div class="controls bullet"><span class="by">elchief</span><span>|</span><a href="#40458065">prev</a><span>|</span><a href="#40461616">next</a><span>|</span><label class="collapse" for="c-40462286">[-]</label><label class="expand" for="c-40462286">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve noticed that Amazon linux 2023 boots faster than Ubuntu too</div><br/></div></div><div id="40461616" class="c"><input type="checkbox" id="c-40461616" checked=""/><div class="controls bullet"><span class="by">broknbottle</span><span>|</span><a href="#40462286">prev</a><span>|</span><a href="#40455817">next</a><span>|</span><label class="collapse" for="c-40461616">[-]</label><label class="expand" for="c-40461616">[1 more]</label></div><br/><div class="children"><div class="content">minimizing the image definitely helps.<p><a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;apn&#x2F;how-to-build-sparse-ebs-volumes-for-fun-and-easy-snapshotting&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;apn&#x2F;how-to-build-sparse-ebs-vol...</a><p><a href="https:&#x2F;&#x2F;netflixtechblog.medium.com&#x2F;datastore-flash-upgrades-187f1e4ef859" rel="nofollow">https:&#x2F;&#x2F;netflixtechblog.medium.com&#x2F;datastore-flash-upgrades-...</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;Netflix-Skunkworks&#x2F;s3-flash-bootloader">https:&#x2F;&#x2F;github.com&#x2F;Netflix-Skunkworks&#x2F;s3-flash-bootloader</a></div><br/></div></div><div id="40455817" class="c"><input type="checkbox" id="c-40455817" checked=""/><div class="controls bullet"><span class="by">maccard</span><span>|</span><a href="#40461616">prev</a><span>|</span><a href="#40461083">next</a><span>|</span><label class="collapse" for="c-40455817">[-]</label><label class="expand" for="c-40455817">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t use GHA as some of our code is stored in Perforce, but we&#x27;ve faced the same challenges with EC2 instance startup times on our self managed runners on a different provider.<p>We would happily pay someone like depot for &quot;here&#x27;s the AMI I want to run &amp; autoscale, can you please do it faster than AWS?&quot;<p>We hit this problem with containers too - we&#x27;d _love_ to just run all our CI on something like fargate and have it automatically scale and respond to our demand, but the response times and rate limting are just _so slow_ that it means instead we just end up starting&#x2F;stopping instances with a lambda which feels so 2014.</div><br/><div id="40456289" class="c"><input type="checkbox" id="c-40456289" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#40455817">parent</a><span>|</span><a href="#40457681">next</a><span>|</span><label class="collapse" for="c-40456289">[-]</label><label class="expand" for="c-40456289">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We would happily pay someone like depot for &quot;here&#x27;s the AMI I want to run &amp; autoscale, can you please do it faster than AWS?&quot;<p>Change that to &quot;here&#x27;s the ISO&#x2F;IMG I want to run &amp; autoscale, can you please do it faster than AWS?&quot; and you&#x27;ll have tons of options. Most platforms using Firecracker would most likely be faster, maybe try to use that as a search vector.</div><br/><div id="40459126" class="c"><input type="checkbox" id="c-40459126" checked=""/><div class="controls bullet"><span class="by">maccard</span><span>|</span><a href="#40455817">root</a><span>|</span><a href="#40456289">parent</a><span>|</span><a href="#40457681">next</a><span>|</span><label class="collapse" for="c-40459126">[-]</label><label class="expand" for="c-40459126">[1 more]</label></div><br/><div class="children"><div class="content">Can you maybe share some examples? We&#x27;re fine to use other image formats, but a lot of the value of AWS is that the services interact, IAM works nicely together, etc.<p>Fly.io comes up often [0] on HN, but there&#x27;s an overwhelming amount of &quot;it&#x27;s a nice idea, but it just doesn&#x27;t work&quot; feedback on it.<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39363499">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39363499</a></div><br/></div></div></div></div><div id="40457681" class="c"><input type="checkbox" id="c-40457681" checked=""/><div class="controls bullet"><span class="by">Szpadel</span><span>|</span><a href="#40455817">parent</a><span>|</span><a href="#40456289">prev</a><span>|</span><a href="#40456726">next</a><span>|</span><label class="collapse" for="c-40457681">[-]</label><label class="expand" for="c-40457681">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not fully investigated fargate limitations but I think it would be possible to use any k8s native CI on eks + fargate, maybe even use kubevirt for VM creation? from my exploration of fargate with eks, aws provisioned capacity in around 1s region</div><br/><div id="40459111" class="c"><input type="checkbox" id="c-40459111" checked=""/><div class="controls bullet"><span class="by">maccard</span><span>|</span><a href="#40455817">root</a><span>|</span><a href="#40457681">parent</a><span>|</span><a href="#40456726">next</a><span>|</span><label class="collapse" for="c-40459111">[-]</label><label class="expand" for="c-40459111">[1 more]</label></div><br/><div class="children"><div class="content">&gt; AWS offers something very similar to this approach called warm pools for EC2 Auto Scaling. This allows you to define a certain number of EC2 instances inside an autoscaling group that are booted once, perform initialization, then shut down, and the autoscaling group will pull from this pool of compute first when scaling up.<p>&gt; While this sounds like it would serve our needs, autoscaling groups are very slow to react to incoming requests to scale up. From experimentation, it appears that autoscaling groups may have a slow poll loop that checks if new instances are needed, so the delay between requesting a scale up and the instance starting can exceed 60 seconds. For us, this negates the benefit of the warm pool.<p>I pulled this from the article, but it&#x27;s the same problem. Technically yes, eks + fargate works. In practice the response times from &quot;thing added to queue&quot; to &quot;node is responding&quot; is minutes with that setup.</div><br/></div></div></div></div><div id="40456726" class="c"><input type="checkbox" id="c-40456726" checked=""/><div class="controls bullet"><span class="by">everfrustrated</span><span>|</span><a href="#40455817">parent</a><span>|</span><a href="#40457681">prev</a><span>|</span><a href="#40461083">next</a><span>|</span><label class="collapse" for="c-40456726">[-]</label><label class="expand" for="c-40456726">[2 more]</label></div><br/><div class="children"><div class="content">Out of curiosity what CI system are you using with Perforce?</div><br/><div id="40459090" class="c"><input type="checkbox" id="c-40459090" checked=""/><div class="controls bullet"><span class="by">maccard</span><span>|</span><a href="#40455817">root</a><span>|</span><a href="#40456726">parent</a><span>|</span><a href="#40461083">next</a><span>|</span><label class="collapse" for="c-40459090">[-]</label><label class="expand" for="c-40459090">[1 more]</label></div><br/><div class="children"><div class="content">We use buildkite with a customised verison of <a href="https:&#x2F;&#x2F;github.com&#x2F;improbable-eng&#x2F;perforce-buildkite-plugin&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;improbable-eng&#x2F;perforce-buildkite-plugin&#x2F;</a><p>Our game code is in P4, but our backend services are on GH. Having a single CI system means we get easy interop e.g. game updates can trigger backend pipelines and vice versa.<p>In the past I&#x27;ve used TeamCity, Jenkins, and ElectricCommander(!)</div><br/></div></div></div></div></div></div><div id="40461083" class="c"><input type="checkbox" id="c-40461083" checked=""/><div class="controls bullet"><span class="by">uavoperator</span><span>|</span><a href="#40455817">prev</a><span>|</span><a href="#40458078">next</a><span>|</span><label class="collapse" for="c-40461083">[-]</label><label class="expand" for="c-40461083">[3 more]</label></div><br/><div class="children"><div class="content">This is really only tangentially related to the article, but<p>&gt;If AWS responds that there is no current capacity for m7a instances, the instance is updated to a backup type (like m7i) and started again<p>Any ideas why m7i would be chosen as the backup type rather than the other way around? m7a seems to be more expensive than m7i, so maybe there&#x27;s some performance advantage or something else I&#x27;m missing that makes AMD CPU containing instances preferable to Intel ones?</div><br/><div id="40463891" class="c"><input type="checkbox" id="c-40463891" checked=""/><div class="controls bullet"><span class="by">tenplusfive</span><span>|</span><a href="#40461083">parent</a><span>|</span><a href="#40463797">next</a><span>|</span><label class="collapse" for="c-40463891">[-]</label><label class="expand" for="c-40463891">[1 more]</label></div><br/><div class="children"><div class="content">At least with other instance types (m5,m6,t3) it was the case that the AMD processors were cheaper. As it turns out, this does not seem to be a general rule.<p>It seems like performance wise the AMD processors are (in certain workloads) quite a bit faster than their Intel equivalent: <a href="https:&#x2F;&#x2F;www.phoronix.com&#x2F;review&#x2F;aws-m7a-ec2-benchmarks&#x2F;2" rel="nofollow">https:&#x2F;&#x2F;www.phoronix.com&#x2F;review&#x2F;aws-m7a-ec2-benchmarks&#x2F;2</a> (in later pages it seems to be a little bit more mixed)</div><br/></div></div><div id="40463797" class="c"><input type="checkbox" id="c-40463797" checked=""/><div class="controls bullet"><span class="by">crohr</span><span>|</span><a href="#40461083">parent</a><span>|</span><a href="#40463891">prev</a><span>|</span><a href="#40458078">next</a><span>|</span><label class="collapse" for="c-40463797">[-]</label><label class="expand" for="c-40463797">[1 more]</label></div><br/><div class="children"><div class="content">m7i CPU is in the same ballpark figure than m7a (<a href="https:&#x2F;&#x2F;runs-on.com&#x2F;benchmarks&#x2F;aws-ec2-instances&#x2F;" rel="nofollow">https:&#x2F;&#x2F;runs-on.com&#x2F;benchmarks&#x2F;aws-ec2-instances&#x2F;</a>). When you look at the interruption percentage for m7a I think m7i (not m7i-flex if you don&#x27;t want burstable instances) is probably the better choice. But I suppose it depends on availability in their specific zones.</div><br/></div></div></div></div><div id="40458078" class="c"><input type="checkbox" id="c-40458078" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#40461083">prev</a><span>|</span><a href="#40458687">next</a><span>|</span><label class="collapse" for="c-40458078">[-]</label><label class="expand" for="c-40458078">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get why they&#x27;re using EBS here to begin with. EBS trades off cost and performance for durability. It&#x27;s slow because it&#x27;s a network-attached volume that&#x27;s most likely also replicated under the hood. You use this for data that you need high durability for.<p>It looks like their use-case fetches all the data it needs from the network (in the form of the GH Actions runner getting the job from GitHub, and then pulling down Docker containers, etc).<p>What they need is a minimal Linux install (Arch Linux would be good for this) in a squashfs&#x2F;etc and the only thing in EBS should be an HTTP-aware boot loader like IPXE or a kernel+initrd capable of pulling down the squashfs from S3 and run it from memory. Local &quot;scratchspace&quot; storage for the build jobs can be provided by the ephemeral NVME drives which are also direct-attach and much faster than EBS.</div><br/><div id="40458193" class="c"><input type="checkbox" id="c-40458193" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#40458078">parent</a><span>|</span><a href="#40458687">next</a><span>|</span><label class="collapse" for="c-40458193">[-]</label><label class="expand" for="c-40458193">[5 more]</label></div><br/><div class="children"><div class="content">By using EBS they don&#x27;t have to wait for disk to fill from network on second+ boot.</div><br/><div id="40458227" class="c"><input type="checkbox" id="c-40458227" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#40458078">root</a><span>|</span><a href="#40458193">parent</a><span>|</span><a href="#40458687">next</a><span>|</span><label class="collapse" for="c-40458227">[-]</label><label class="expand" for="c-40458227">[4 more]</label></div><br/><div class="children"><div class="content">Ah so they are keeping the machines around? Do they need to do that - does the GH runner actually persist anything worth keeping in between runs?</div><br/><div id="40458290" class="c"><input type="checkbox" id="c-40458290" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#40458078">root</a><span>|</span><a href="#40458227">parent</a><span>|</span><a href="#40458687">next</a><span>|</span><label class="collapse" for="c-40458290">[-]</label><label class="expand" for="c-40458290">[3 more]</label></div><br/><div class="children"><div class="content">They keep the instances in a &quot;stopped&quot; state, which means keeping the EBS volume around (and paying for it) but not paying for the instance (which could be another machine when turn it back on, which is why you can&#x27;t load it into scratch space and then stop it).<p>What&#x27;s on the EBS is their docker image, so they don&#x27;t have to load it back up again.</div><br/><div id="40458427" class="c"><input type="checkbox" id="c-40458427" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#40458078">root</a><span>|</span><a href="#40458290">parent</a><span>|</span><a href="#40458687">next</a><span>|</span><label class="collapse" for="c-40458427">[-]</label><label class="expand" for="c-40458427">[2 more]</label></div><br/><div class="children"><div class="content">Makes sense. I still think it would be cheaper to just reload it from S3 (straight into memory, not using EBS at all) on every boot. The entire OS shouldn&#x27;t be more than a gigabyte which is quite fast to download as a bulk transfer straight into RAM.</div><br/><div id="40458471" class="c"><input type="checkbox" id="c-40458471" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#40458078">root</a><span>|</span><a href="#40458427">parent</a><span>|</span><a href="#40458687">next</a><span>|</span><label class="collapse" for="c-40458471">[-]</label><label class="expand" for="c-40458471">[1 more]</label></div><br/><div class="children"><div class="content">Yes it would be cheaper, but the whole point of this article is trading off cost for faster boot times.  They address your points in the article, how it&#x27;s faster to boot off a warm EBS instead of loading from scratch.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40458687" class="c"><input type="checkbox" id="c-40458687" checked=""/><div class="controls bullet"><span class="by">paulddraper</span><span>|</span><a href="#40458078">prev</a><span>|</span><a href="#40455843">next</a><span>|</span><label class="collapse" for="c-40458687">[-]</label><label class="expand" for="c-40458687">[3 more]</label></div><br/><div class="children"><div class="content">&gt; From a billing perspective, AWS does not charge for the EC2 instance itself when stopped, as there&#x27;s no physical hardware being reserved; a stopped instance is just the configuration that will be used when the instance is started next. Note that you do pay for the root EBS volume though, as it&#x27;s still consuming storage.<p>Shutdown standbys absolutely the way to do it.<p>Does AWS offer anything for this, because it&#x27;s very tedious to set this up.</div><br/><div id="40458720" class="c"><input type="checkbox" id="c-40458720" checked=""/><div class="controls bullet"><span class="by">tekla</span><span>|</span><a href="#40458687">parent</a><span>|</span><a href="#40455843">next</a><span>|</span><label class="collapse" for="c-40458720">[-]</label><label class="expand" for="c-40458720">[2 more]</label></div><br/><div class="children"><div class="content">Warm pools</div><br/><div id="40458971" class="c"><input type="checkbox" id="c-40458971" checked=""/><div class="controls bullet"><span class="by">paulddraper</span><span>|</span><a href="#40458687">root</a><span>|</span><a href="#40458720">parent</a><span>|</span><a href="#40455843">next</a><span>|</span><label class="collapse" for="c-40458971">[-]</label><label class="expand" for="c-40458971">[1 more]</label></div><br/><div class="children"><div class="content">yep, that&#x27;s it, thank you kind person</div><br/></div></div></div></div></div></div><div id="40455843" class="c"><input type="checkbox" id="c-40455843" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40458687">prev</a><span>|</span><label class="collapse" for="c-40455843">[-]</label><label class="expand" for="c-40455843">[16 more]</label></div><br/><div class="children"><div class="content">There&#x27;s something to say about building a tower of abstractions and then trying to tear it back down. We used to just run a compiler on a machine. Startup time: 0.001 seconds. Then we&#x27;d run a Docker container on a machine. Startup time: 0.01 sections. Fine, if you need that abstraction. Now apparently we&#x27;re booting full VMs to run compilers - startup time: 5 seconds. But that&#x27;s not enough, because we&#x27;re also allocating a bunch of resources in a distributed network - startup time: 40 seconds.<p>Do we actually need all this stuff, or does it suffice to get one really powerful server (price less than $40k) and run Docker on it?</div><br/><div id="40460082" class="c"><input type="checkbox" id="c-40460082" checked=""/><div class="controls bullet"><span class="by">iudqnolq</span><span>|</span><a href="#40455843">parent</a><span>|</span><a href="#40457604">next</a><span>|</span><label class="collapse" for="c-40460082">[-]</label><label class="expand" for="c-40460082">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t solve the same problem.<p>GitHub actions in the standard setup needs to run untrusted code and so you essentially need a VM.<p>You can lock it down at the cost of sacrificing features and usability, but that&#x27;s a tradeoff.</div><br/></div></div><div id="40457604" class="c"><input type="checkbox" id="c-40457604" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#40455843">parent</a><span>|</span><a href="#40460082">prev</a><span>|</span><a href="#40457689">next</a><span>|</span><label class="collapse" for="c-40457604">[-]</label><label class="expand" for="c-40457604">[3 more]</label></div><br/><div class="children"><div class="content">A really powerful server should not cost you anywhere near $40k unless you&#x27;re renting bare metal in AWS or something like that.<p>Getting rid of the overhead is possible but hard, unless you&#x27;re willing to sacrifice things people really want.<p>1. Docker. Adds a few hundred msec of startup time to containers, configuration complexity, daemons, disk caches to manage, repositories .... a lot of stuff. In rigorously controlled corp environments it&#x27;s not needed. You can just have a base OS distro that&#x27;s managed centrally and tell people to target it. If they&#x27;re building on e.g. the JVM then Docker isn&#x27;t adding much. I don&#x27;t use it on my own companies CI cluster for example, it&#x27;s just raw TeamCity agents on raw machines.<p>2. VMs. Clouds need them because they don&#x27;t trust the Linux kernel to isolate customers from each other, and they want to buy the biggest machines possible and then subdivide them. That&#x27;s how their business model works. You can solve this a few ways. One is something like Firecracker where they make a super bare bones VM. Another would be to make a super-hardened version of Linux, so hardened people trust it to provide inter-tenant isolation. Another way would be a clean room kernel designed for security from day one (e.g. written in Rust, Java or C#?)<p>3. Drives on a distributed network. Honestly not sure why this is needed. For CI runners entirely ephemeral VMs running off read only root drive images should be fine. They could swap to local NVMe storage. I think the big clouds don&#x27;t always like to offer this because they have a lot of machines with no local storage whatsoever, as that increases the density and allows storage aggregation&#x2F;binpacking, which lowers their costs.<p>Basically a big driver of overheads is that people want to be in the big clouds because it avoids the need to do long term planning or commit capital spend to CI, but the cloud is so popular that providers want to pack everyone in as tightly as possible which requires strong isolation and the need to avoid arbitrary boundaries caused by physical hardware shapes.</div><br/><div id="40462362" class="c"><input type="checkbox" id="c-40462362" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40457604">parent</a><span>|</span><a href="#40462476">prev</a><span>|</span><a href="#40457689">next</a><span>|</span><label class="collapse" for="c-40462362">[-]</label><label class="expand" for="c-40462362">[1 more]</label></div><br/><div class="children"><div class="content">$40k to <i>buy</i> the server, not to rent per month.<p>If you know who&#x27;s using your build server, you probably don&#x27;t need isolation stronger than Docker, because they can to to jail for hacking it.</div><br/></div></div></div></div><div id="40457689" class="c"><input type="checkbox" id="c-40457689" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#40455843">parent</a><span>|</span><a href="#40457604">prev</a><span>|</span><a href="#40455913">next</a><span>|</span><label class="collapse" for="c-40457689">[-]</label><label class="expand" for="c-40457689">[1 more]</label></div><br/><div class="children"><div class="content">How do you get Docker container startup time of 0.01s with any real-life workload (yes, I know they are just processes, so you could build a simple &quot;hello world&quot; thing, but I&#x27;d be surprised if even that runs this fast)?<p>Do you have an example image and network config that would demonstrate that?<p>(I&#x27;d love to understand the performance limits of Docker containers, but never played with them deeply enough since they are usually in &gt;1s space which is too slow for me to care)</div><br/></div></div><div id="40455913" class="c"><input type="checkbox" id="c-40455913" checked=""/><div class="controls bullet"><span class="by">cjk2</span><span>|</span><a href="#40455843">parent</a><span>|</span><a href="#40457689">prev</a><span>|</span><label class="collapse" for="c-40455913">[-]</label><label class="expand" for="c-40455913">[10 more]</label></div><br/><div class="children"><div class="content">I&#x27;m mostly just running the (Go) compiler on my laptop which is considerably faster than on docker and considerably cheaper than the server...<p>I mean an ass end M3 macbook has the same compile time as an i9-14900k. God knows what an equivalent Xeon&#x2F;Epyc costs...</div><br/><div id="40456032" class="c"><input type="checkbox" id="c-40456032" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40455913">parent</a><span>|</span><a href="#40458774">next</a><span>|</span><label class="collapse" for="c-40456032">[-]</label><label class="expand" for="c-40456032">[7 more]</label></div><br/><div class="children"><div class="content">Maybe your container isn&#x27;t set up right - Docker contains run directly on the host, just partitioned off from accessing stuff outside of themselves with the equivalent of chroot. Or it could be a Mac-specific thing. Docker only works that way on Linux, and has to emulate Linux on other platforms.</div><br/><div id="40456456" class="c"><input type="checkbox" id="c-40456456" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40456032">parent</a><span>|</span><a href="#40456626">next</a><span>|</span><label class="collapse" for="c-40456456">[-]</label><label class="expand" for="c-40456456">[1 more]</label></div><br/><div class="children"><div class="content">Right, they said they&#x27;re on a macbook so unless they&#x27;re going out of their way to run Linux bare-metal it has to use a VM. And AIUI there are extra footguns in that situation, especially that mapping volumes from the host is slower because instead of just telling the kernel to make the directory visible you have to actually share from the host to the VM.<p>See also: <a href="https:&#x2F;&#x2F;reece.tech&#x2F;posts&#x2F;osx-docker-performance&#x2F;" rel="nofollow">https:&#x2F;&#x2F;reece.tech&#x2F;posts&#x2F;osx-docker-performance&#x2F;</a><p>See also: <a href="https:&#x2F;&#x2F;docs.docker.com&#x2F;desktop&#x2F;settings&#x2F;mac&#x2F;" rel="nofollow">https:&#x2F;&#x2F;docs.docker.com&#x2F;desktop&#x2F;settings&#x2F;mac&#x2F;</a><p>&gt; Shared folders are designed to allow application code to be edited on the host while being executed in containers. For non-code items such as cache directories or databases, the performance will be much better if they are stored in the Linux VM, using a data volume (named volume) or data container.</div><br/></div></div><div id="40456626" class="c"><input type="checkbox" id="c-40456626" checked=""/><div class="controls bullet"><span class="by">cjk2</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40456032">parent</a><span>|</span><a href="#40456456">prev</a><span>|</span><a href="#40458774">next</a><span>|</span><label class="collapse" for="c-40456626">[-]</label><label class="expand" for="c-40456626">[5 more]</label></div><br/><div class="children"><div class="content">Why would I use docker? You don&#x27;t have to use it. I&#x27;m just generating static binaries.<p>Does anyone understand how to do stuff without containers these days?</div><br/><div id="40458339" class="c"><input type="checkbox" id="c-40458339" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40456626">parent</a><span>|</span><a href="#40457669">next</a><span>|</span><label class="collapse" for="c-40458339">[-]</label><label class="expand" for="c-40458339">[3 more]</label></div><br/><div class="children"><div class="content">Because you just said:<p>&gt; which is considerably faster than on docker<p>And we are curious why it is like so because we not only understand how to do stuff without containers, we <i>also</i> understand how containers work and your claim sounds off.</div><br/><div id="40459066" class="c"><input type="checkbox" id="c-40459066" checked=""/><div class="controls bullet"><span class="by">cjk2</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40458339">parent</a><span>|</span><a href="#40457669">next</a><span>|</span><label class="collapse" for="c-40459066">[-]</label><label class="expand" for="c-40459066">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand what you are saying.<p>I&#x27;m saying it is slower on docker due to container startup, pulling images, overheads, working out what incantations to run, filesystem access, network weirdness, things talking to other things, configuration required, pull limits, API tokens, all sorts.<p>Versus &quot;go run&quot;</div><br/><div id="40462375" class="c"><input type="checkbox" id="c-40462375" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40459066">parent</a><span>|</span><a href="#40457669">next</a><span>|</span><label class="collapse" for="c-40462375">[-]</label><label class="expand" for="c-40462375">[1 more]</label></div><br/><div class="children"><div class="content">But usually it&#x27;s not &quot;considerably&quot;. Obviously setting up the container environment takes time but it should be well under a second per build.</div><br/></div></div></div></div></div></div><div id="40457669" class="c"><input type="checkbox" id="c-40457669" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40456626">parent</a><span>|</span><a href="#40458339">prev</a><span>|</span><a href="#40458774">next</a><span>|</span><label class="collapse" for="c-40457669">[-]</label><label class="expand" for="c-40457669">[1 more]</label></div><br/><div class="children"><div class="content">I’m using VMs these day because of conflicts and inconsistencies between tooling. But the VM is dedicated to one project and I set it up just like a real machine (GUI, browser, and stuff). No file sharing. It’s been a blast.</div><br/></div></div></div></div></div></div><div id="40458774" class="c"><input type="checkbox" id="c-40458774" checked=""/><div class="controls bullet"><span class="by">benwaffle</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40455913">parent</a><span>|</span><a href="#40456032">prev</a><span>|</span><label class="collapse" for="c-40458774">[-]</label><label class="expand" for="c-40458774">[2 more]</label></div><br/><div class="children"><div class="content">reminds me of <a href="https:&#x2F;&#x2F;world.hey.com&#x2F;dhh&#x2F;we-re-moving-continuous-integration-back-to-developer-machines-3ac6c611" rel="nofollow">https:&#x2F;&#x2F;world.hey.com&#x2F;dhh&#x2F;we-re-moving-continuous-integratio...</a></div><br/><div id="40459005" class="c"><input type="checkbox" id="c-40459005" checked=""/><div class="controls bullet"><span class="by">cjk2</span><span>|</span><a href="#40455843">root</a><span>|</span><a href="#40458774">parent</a><span>|</span><label class="collapse" for="c-40459005">[-]</label><label class="expand" for="c-40459005">[1 more]</label></div><br/><div class="children"><div class="content">Yep.<p>And you usually get lumbered with some shitty thing like github actions which consumes one mortal full time to keep it working, goes down twice a month (yesterday wasn&#x27;t it this week?), takes bloody forever to build anything and is impossible to debug.<p>Edit: and MORE YAML HELL!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>