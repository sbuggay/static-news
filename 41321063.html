<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724403664680" as="style"/><link rel="stylesheet" href="styles.css?v=1724403664680"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html">Continuous reinvention: A brief history of block storage at AWS</a> <span class="domain">(<a href="https://www.allthingsdistributed.com">www.allthingsdistributed.com</a>)</span></div><div class="subtext"><span>riv991</span> | <span>78 comments</span></div><br/><div><div id="41321964" class="c"><input type="checkbox" id="c-41321964" checked=""/><div class="controls bullet"><span class="by">mjb</span><span>|</span><a href="#41323232">next</a><span>|</span><label class="collapse" for="c-41321964">[-]</label><label class="expand" for="c-41321964">[5 more]</label></div><br/><div class="children"><div class="content">Super cool to see this here. If you&#x27;re at all interested in big systems, you should read this.<p>&gt; Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior.<p>The effect of this can be huge! Given a reasonably sequential workload, modern magnetic drives can do &gt;100MB&#x2F;s of reads or writes. Given an entirely random 4kB workload, they can be limited to as little as 400kB&#x2F;s of reads or writes. Queuing and scheduling can help avoid the truly bad end of this, but real-world performance still varies by over 100x depending on workload. That&#x27;s really hard for a multi-tenant system to deal with (especially with reads, where you can&#x27;t do the &quot;just write it somewhere else&quot; trick).<p>&gt; To know what to fix, we had to know what was broken, and then prioritize those fixes based on effort and rewards.<p>This was the biggest thing I learned from Marc in my career (so far). He&#x27;d spend time working on visualizations of latency (like the histogram time series in this post) which were much richer than any of the telemetry we had, then tell a story using those visualizations, and completely change the team&#x27;s perspective on the work that needed to be done. Each peak in the histogram came with it&#x27;s own story, and own work to optimize. Really diving into performance data - and looking at that data in multiple ways - unlocks efficiencies  and opportunities that are invisible without that work and investment.<p>&gt; Armed with this knowledge, and a lot of human effort, over the course of a few months in 2013, EBS was able to put a single SSD into each and every one of those thousands of servers.<p>This retrofit project is one of my favorite AWS stories.<p>&gt; The thing that made this possible is that we designed our system from the start with non-disruptive maintenance events in mind. We could retarget EBS volumes to new storage servers, and update software or rebuild the empty servers as needed.<p>This is a great reminder that building distributed systems isn&#x27;t just for scale. Here, we see how building the system in a way that can seamlessly tolerate the failure of a server, and move data around without loss, makes large scale operations (everything from day-to-day software upgrades to a massive hardware retrofit project) possible that just wouldn&#x27;t be possible in a &quot;simpler&quot; architecture. A &quot;simpler&quot; architecture would make these operations much harder, to the point of being impossible, making the end-to-end problem we&#x27;re trying to solve for the customer harder.</div><br/><div id="41322465" class="c"><input type="checkbox" id="c-41322465" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41321964">parent</a><span>|</span><a href="#41323232">next</a><span>|</span><label class="collapse" for="c-41322465">[-]</label><label class="expand" for="c-41322465">[4 more]</label></div><br/><div class="children"><div class="content">It;s funny you mentioned Marc worked on latency viz and used it to tell a story.
Dick Lyon at Google did the same thing for Google&#x27;s storage servers <a href="https:&#x2F;&#x2F;www.pdl.cmu.edu&#x2F;SDI&#x2F;2015&#x2F;slides&#x2F;DatacenterComputers.pdf" rel="nofollow">https:&#x2F;&#x2F;www.pdl.cmu.edu&#x2F;SDI&#x2F;2015&#x2F;slides&#x2F;DatacenterComputers....</a> (starting at Slide 62) identifying various queues and resource contention as major bottlenecks for block storage.</div><br/><div id="41322588" class="c"><input type="checkbox" id="c-41322588" checked=""/><div class="controls bullet"><span class="by">msolson</span><span>|</span><a href="#41321964">root</a><span>|</span><a href="#41322465">parent</a><span>|</span><a href="#41326447">next</a><span>|</span><label class="collapse" for="c-41322588">[-]</label><label class="expand" for="c-41322588">[2 more]</label></div><br/><div class="children"><div class="content">A picture can be worth way more than a thousand words, but sometimes you have to iterate through a thousand pictures to find the one that tells the right story, or helps you ask the right question!</div><br/></div></div><div id="41326447" class="c"><input type="checkbox" id="c-41326447" checked=""/><div class="controls bullet"><span class="by">yetanotherdood</span><span>|</span><a href="#41321964">root</a><span>|</span><a href="#41322465">parent</a><span>|</span><a href="#41322588">prev</a><span>|</span><a href="#41323232">next</a><span>|</span><label class="collapse" for="c-41326447">[-]</label><label class="expand" for="c-41326447">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes diskless borg :)</div><br/></div></div></div></div></div></div><div id="41323232" class="c"><input type="checkbox" id="c-41323232" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41321964">prev</a><span>|</span><a href="#41322518">next</a><span>|</span><label class="collapse" for="c-41323232">[-]</label><label class="expand" for="c-41323232">[2 more]</label></div><br/><div class="children"><div class="content">Ah, this brings back memories.  Reddit was one of the very first users of EBS back in 2008.  I thought I was <i>so</i> clever when I figured out that I could get more IOPS if I build a software raid out of five EBS volumes.<p>At the time each volume had very inconsistent performance, so I would launch seven or eight, and then run some each write and read loads.  I&#x27;d take the five best performers and then put them into a Linux software raid.<p>In the good case, I got the desired effect -- I did in fact get more IOPS then 5x a single node.  But in the bad case, oh boy was it bad.<p>What I didn&#x27;t realize was that if you&#x27;re using a software raid, if one node is slow, the entire raid moves at the speed of the slowest volume.  So this would manifest as a database going bad.  It took a while to figure out it was the RAID that was the problem.  And even then, removing the bad node was hard -- the software raid really didn&#x27;t want to let go of the bad volume until it could finish writing out to it, which of course was super slow.<p>And then I would put in a new EBS volume and have to rebuild the array, which of course it was also bad at because it would be bottlenecked on the IOPS for the new volume.<p>We moved off of those software raids after a while.  We almost never used EBS at Netflix, in part because I would tell everyone who would listen about my folly at reddit, and because they had already standardized on using only local disk before I ever got there.<p>And an amusing side note, when AWS had that massive EBS outage, I still worked at reddit and I was actually watching Netflix while I was waiting for the EBS to come back so I could fix all the databases.  When I interviewed at Netflix one of the questions I asked them was &quot;how were you still up during the EBS outage?&quot;, and they said, &quot;Oh, we just don&#x27;t use EBS&quot;.</div><br/></div></div><div id="41322518" class="c"><input type="checkbox" id="c-41322518" checked=""/><div class="controls bullet"><span class="by">mgdev</span><span>|</span><a href="#41323232">prev</a><span>|</span><a href="#41322149">next</a><span>|</span><label class="collapse" for="c-41322518">[-]</label><label class="expand" for="c-41322518">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s cool to read this.<p>One interesting tidbit is that during the period this author writes about, AWS had a roughly 4-day outage (impacted at least EC2, EBS, and RDS, iirc), caused by EBS, that really shook folks&#x27; confidence in AWS.<p>It resulted in a reorg and much deeper investment in EBS as a standalone service.<p>It also happened around the time Apple was becoming a customer, and AWS in general was going through hockey-stick growth thanks to startup adoption (Netflix, Zynga, Dropbox, etc).<p>It&#x27;s fun to read about these technical and operational bits, but technical innovation in production is messy, and happens against a backdrop of Real Business Needs.<p>I wish more of THOSE stories were told as well.</div><br/><div id="41325358" class="c"><input type="checkbox" id="c-41325358" checked=""/><div class="controls bullet"><span class="by">BikiniPrince</span><span>|</span><a href="#41322518">parent</a><span>|</span><a href="#41322149">next</a><span>|</span><label class="collapse" for="c-41325358">[-]</label><label class="expand" for="c-41325358">[1 more]</label></div><br/><div class="children"><div class="content">It was a good year after that incident. We focused on stability and driving down issues. We turned around a lot of development idea too. However, the wheel turns and we were back on feature development. I’ll always remember that year as having the fewest escalations during my entire time there.</div><br/></div></div></div></div><div id="41322149" class="c"><input type="checkbox" id="c-41322149" checked=""/><div class="controls bullet"><span class="by">simonebrunozzi</span><span>|</span><a href="#41322518">prev</a><span>|</span><a href="#41325415">next</a><span>|</span><label class="collapse" for="c-41322149">[-]</label><label class="expand" for="c-41322149">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re curious, this is a talk I gave back in 2009 [0] about Amazon S3 internals. It was created from internal assets by the S3 team, and a lot in there influenced how EBS was developed.<p>[0]: <a href="https:&#x2F;&#x2F;vimeo.com&#x2F;7330740" rel="nofollow">https:&#x2F;&#x2F;vimeo.com&#x2F;7330740</a></div><br/></div></div><div id="41325415" class="c"><input type="checkbox" id="c-41325415" checked=""/><div class="controls bullet"><span class="by">abrookewood</span><span>|</span><a href="#41322149">prev</a><span>|</span><a href="#41322449">next</a><span>|</span><label class="collapse" for="c-41325415">[-]</label><label class="expand" for="c-41325415">[4 more]</label></div><br/><div class="children"><div class="content">This is the bit I found curious: &quot;adding a small amount of random latency to requests to storage servers counter-intuitively reduced the average latency and the outliers due to the smoothing effect it has on the network&quot;.<p>Can anyone explain why?</div><br/><div id="41325459" class="c"><input type="checkbox" id="c-41325459" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41325415">parent</a><span>|</span><a href="#41322449">next</a><span>|</span><label class="collapse" for="c-41325459">[-]</label><label class="expand" for="c-41325459">[3 more]</label></div><br/><div class="children"><div class="content">Synchronized network traffic can cause incast or other buffer overflows.</div><br/><div id="41326094" class="c"><input type="checkbox" id="c-41326094" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#41325415">root</a><span>|</span><a href="#41325459">parent</a><span>|</span><a href="#41322449">next</a><span>|</span><label class="collapse" for="c-41326094">[-]</label><label class="expand" for="c-41326094">[2 more]</label></div><br/><div class="children"><div class="content">Yeah jitter is generally used to mitigate “thundering herd” type problems because it reduces the peak load by spreading it out over time.</div><br/><div id="41326296" class="c"><input type="checkbox" id="c-41326296" checked=""/><div class="controls bullet"><span class="by">abrookewood</span><span>|</span><a href="#41325415">root</a><span>|</span><a href="#41326094">parent</a><span>|</span><a href="#41322449">next</a><span>|</span><label class="collapse" for="c-41326296">[-]</label><label class="expand" for="c-41326296">[1 more]</label></div><br/><div class="children"><div class="content">Thanks to both of you - makes sense</div><br/></div></div></div></div></div></div></div></div><div id="41322449" class="c"><input type="checkbox" id="c-41322449" checked=""/><div class="controls bullet"><span class="by">lysace</span><span>|</span><a href="#41325415">prev</a><span>|</span><a href="#41323287">next</a><span>|</span><label class="collapse" for="c-41322449">[-]</label><label class="expand" for="c-41322449">[2 more]</label></div><br/><div class="children"><div class="content">I liked the part about them manually retrofitting an SSD in every EBS unit in 2013. That looks a lot like a Samsung SATA SSD:<p><a href="https:&#x2F;&#x2F;www.allthingsdistributed.com&#x2F;images&#x2F;mo-manual-ssd.png" rel="nofollow">https:&#x2F;&#x2F;www.allthingsdistributed.com&#x2F;images&#x2F;mo-manual-ssd.pn...</a><p>I think we got SSDs installed in blades from Dell well before that, but I may be misremembering.<p>I&#x2F;O performance was a big thing in like 2010&#x2F;2011&#x2F;2012. We went from spinning HDs to Flash memory.<p>I remember experimenting with these raw Flash-based devices, no error&#x2F;wear level handling at all. Insanity, but we were all desperate for that insane I&#x2F;O performance bump from spinning rust to silicon.</div><br/><div id="41325393" class="c"><input type="checkbox" id="c-41325393" checked=""/><div class="controls bullet"><span class="by">BikiniPrince</span><span>|</span><a href="#41322449">parent</a><span>|</span><a href="#41323287">next</a><span>|</span><label class="collapse" for="c-41325393">[-]</label><label class="expand" for="c-41325393">[1 more]</label></div><br/><div class="children"><div class="content">It was only a handful of frankenracks. It was challenging and not very performant, but it let everyone get a jump on the research. Disk speed was increasing so fast in six months the first SKU was out of date. I’m glad I didn’t have to make the argument directly to assets when we retired those racks years earlier than planned. The rack positions were so much more valuable with the new denser and faster models.</div><br/></div></div></div></div><div id="41323287" class="c"><input type="checkbox" id="c-41323287" checked=""/><div class="controls bullet"><span class="by">0xbadcafebee</span><span>|</span><a href="#41322449">prev</a><span>|</span><a href="#41325664">next</a><span>|</span><label class="collapse" for="c-41323287">[-]</label><label class="expand" for="c-41323287">[6 more]</label></div><br/><div class="children"><div class="content">At the very start of my career, I got to work for a large-scale (technically&#x2F;logistically, not in staff) internet company doing all the systems stuff. The number of lessons I learned in such a short time was crazy. Since leaving them, I learned that most people can go almost their whole careers without running into all those issues, and so don&#x27;t learn those lessons.<p>That&#x27;s one of the reasons why I think we should have a professional license. By requiring an apprenticeship under a master engineer, somebody can pick up incredibly valuable knowledge and skills (that you only learn by experience) in a very short time frame, and then be released out into the world to be much more effective throughout their career. And as someone who also interviews candidates, some proof of their experience and a reference from their mentor would be invaluable.</div><br/><div id="41323665" class="c"><input type="checkbox" id="c-41323665" checked=""/><div class="controls bullet"><span class="by">ponector</span><span>|</span><a href="#41323287">parent</a><span>|</span><a href="#41325664">next</a><span>|</span><label class="collapse" for="c-41323665">[-]</label><label class="expand" for="c-41323665">[5 more]</label></div><br/><div class="children"><div class="content">Imagine you got your license and then tasked to make a crud service with some simple UI because that is what is needed for the client and they cannot use unlicensed developers.</div><br/><div id="41325054" class="c"><input type="checkbox" id="c-41325054" checked=""/><div class="controls bullet"><span class="by">0xbadcafebee</span><span>|</span><a href="#41323287">root</a><span>|</span><a href="#41323665">parent</a><span>|</span><a href="#41324103">next</a><span>|</span><label class="collapse" for="c-41325054">[-]</label><label class="expand" for="c-41325054">[1 more]</label></div><br/><div class="children"><div class="content">That wouldn&#x27;t happen. Professional licenses vary by trade, state, cost of project, size of project, impact of work, etc, etc. If it&#x27;s trivial, you don&#x27;t need anything. If it is critical, you need a bunch. If it&#x27;s in between, it depends. The world isn&#x27;t black and white.</div><br/></div></div><div id="41324103" class="c"><input type="checkbox" id="c-41324103" checked=""/><div class="controls bullet"><span class="by">lispisok</span><span>|</span><a href="#41323287">root</a><span>|</span><a href="#41323665">parent</a><span>|</span><a href="#41325054">prev</a><span>|</span><a href="#41325664">next</a><span>|</span><label class="collapse" for="c-41324103">[-]</label><label class="expand" for="c-41324103">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a common misunderstanding that a professional license would be required to perform any kind of work which is not true of the professional engineering license.</div><br/><div id="41326663" class="c"><input type="checkbox" id="c-41326663" checked=""/><div class="controls bullet"><span class="by">ponector</span><span>|</span><a href="#41323287">root</a><span>|</span><a href="#41324103">parent</a><span>|</span><a href="#41324705">next</a><span>|</span><label class="collapse" for="c-41326663">[-]</label><label class="expand" for="c-41326663">[1 more]</label></div><br/><div class="children"><div class="content">Imagine some military institution or government body needs a piece of custom but trivial software. I doubt they will be able to hire contractor with unlicensed developers to do the job.</div><br/></div></div><div id="41324705" class="c"><input type="checkbox" id="c-41324705" checked=""/><div class="controls bullet"><span class="by">ipaddr</span><span>|</span><a href="#41323287">root</a><span>|</span><a href="#41324103">parent</a><span>|</span><a href="#41326663">prev</a><span>|</span><a href="#41325664">next</a><span>|</span><label class="collapse" for="c-41324705">[-]</label><label class="expand" for="c-41324705">[1 more]</label></div><br/><div class="children"><div class="content">It becomes a minimum standard for hr regardless of intentions.  The certification of the late nineties craze showed this.</div><br/></div></div></div></div></div></div></div></div><div id="41325664" class="c"><input type="checkbox" id="c-41325664" checked=""/><div class="controls bullet"><span class="by">rnts08</span><span>|</span><a href="#41323287">prev</a><span>|</span><a href="#41322255">next</a><span>|</span><label class="collapse" for="c-41325664">[-]</label><label class="expand" for="c-41325664">[1 more]</label></div><br/><div class="children"><div class="content">This gives me fond memories of building storage-as-a-service infrastructure back before we had useful opensource stuff, moving away from sun san, fibrechannel and solaris we landed on glusterfs on supermicro storage servers, running linux and nfs. We peaked almost 2Pb before I moved on in 2007.<p>Secondly it reminds me of the time when it simply made sense to ninja-break and rebuild mdraids with ssds in-place of the spinning drives WHILE the servers were running (sata kind of supported hotswapping the drives). Going from spinning to ssd gave us a 14x increase in IOPS in the most important system of the platform.</div><br/></div></div><div id="41322255" class="c"><input type="checkbox" id="c-41322255" checked=""/><div class="controls bullet"><span class="by">tanelpoder</span><span>|</span><a href="#41325664">prev</a><span>|</span><a href="#41322831">next</a><span>|</span><label class="collapse" for="c-41322255">[-]</label><label class="expand" for="c-41322255">[8 more]</label></div><br/><div class="children"><div class="content">The first diagram in that article is incorrect&#x2F;quite outdated. Modern computers have most PCIe lanes going directly into the CPU (IO Hub or &quot;Uncore&quot; area of the processor), not via a separate PCH like in the old days. That&#x27;s an important development for both I&#x2F;O throughput and latency.<p>Otherwise, great article, illustrating that it&#x27;s queues all the way down!</div><br/><div id="41322357" class="c"><input type="checkbox" id="c-41322357" checked=""/><div class="controls bullet"><span class="by">msolson</span><span>|</span><a href="#41322255">parent</a><span>|</span><a href="#41322831">next</a><span>|</span><label class="collapse" for="c-41322357">[-]</label><label class="expand" for="c-41322357">[7 more]</label></div><br/><div class="children"><div class="content">Thanks for the comment, and you&#x27;re right, modern computers do have a much better architecture! As I was laying out the story I was thinking about what it looked like when we started. I&#x27;ll clarify that in the image caption that it&#x27;s from that era.</div><br/><div id="41322457" class="c"><input type="checkbox" id="c-41322457" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#41322255">root</a><span>|</span><a href="#41322357">parent</a><span>|</span><a href="#41322500">next</a><span>|</span><label class="collapse" for="c-41322457">[-]</label><label class="expand" for="c-41322457">[5 more]</label></div><br/><div class="children"><div class="content">We may be going full circle with consumer systems being so light for lanes under PCI-e gen5!<p>There&#x27;s usually enough for a GPU, SSD or two... and that&#x27;s about it. I don&#x27;t like having to spend so much for fast IO, dangit.<p>Can sometimes find boards that do switching to appease :&#x2F;</div><br/><div id="41325605" class="c"><input type="checkbox" id="c-41325605" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#41322255">root</a><span>|</span><a href="#41322457">parent</a><span>|</span><a href="#41322564">next</a><span>|</span><label class="collapse" for="c-41325605">[-]</label><label class="expand" for="c-41325605">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say the lack of flexible boards isn&#x27;t really about lane count.<p>A B650(E) board has enough lanes to give you a fast GPU slot, four CPU x4 slots running at 8-16GB&#x2F;s each, and four chipset x1 slots running at 2GB&#x2F;s each.</div><br/></div></div><div id="41322564" class="c"><input type="checkbox" id="c-41322564" checked=""/><div class="controls bullet"><span class="by">trueismywork</span><span>|</span><a href="#41322255">root</a><span>|</span><a href="#41322457">parent</a><span>|</span><a href="#41325605">prev</a><span>|</span><a href="#41322500">next</a><span>|</span><label class="collapse" for="c-41322564">[-]</label><label class="expand" for="c-41322564">[3 more]</label></div><br/><div class="children"><div class="content">Put your gpu on chipset 8 lane and use a pcie to nvme hub to get upto 7 nvmes</div><br/><div id="41323015" class="c"><input type="checkbox" id="c-41323015" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#41322255">root</a><span>|</span><a href="#41322564">parent</a><span>|</span><a href="#41322500">next</a><span>|</span><label class="collapse" for="c-41323015">[-]</label><label class="expand" for="c-41323015">[2 more]</label></div><br/><div class="children"><div class="content">I hadn&#x27;t even gotten into NICs and such yet, though! There was a time when I didn&#x27;t have to play this game of Tetris</div><br/><div id="41325235" class="c"><input type="checkbox" id="c-41325235" checked=""/><div class="controls bullet"><span class="by">lmz</span><span>|</span><a href="#41322255">root</a><span>|</span><a href="#41323015">parent</a><span>|</span><a href="#41322500">next</a><span>|</span><label class="collapse" for="c-41325235">[-]</label><label class="expand" for="c-41325235">[1 more]</label></div><br/><div class="children"><div class="content">That time was probably when SLI&#x2F;crossfire was still around so high-end consumer boards had a reason to include plenty of slots for 3 GPUs.</div><br/></div></div></div></div></div></div></div></div><div id="41322500" class="c"><input type="checkbox" id="c-41322500" checked=""/><div class="controls bullet"><span class="by">tanelpoder</span><span>|</span><a href="#41322255">root</a><span>|</span><a href="#41322357">parent</a><span>|</span><a href="#41322457">prev</a><span>|</span><a href="#41322831">next</a><span>|</span><label class="collapse" for="c-41322500">[-]</label><label class="expand" for="c-41322500">[1 more]</label></div><br/><div class="children"><div class="content">Cool, yep this is just a minor detail and doesn&#x27;t change what the article itself conveys.</div><br/></div></div></div></div></div></div><div id="41322831" class="c"><input type="checkbox" id="c-41322831" checked=""/><div class="controls bullet"><span class="by">pbw</span><span>|</span><a href="#41322255">prev</a><span>|</span><a href="#41323831">next</a><span>|</span><label class="collapse" for="c-41322831">[-]</label><label class="expand" for="c-41322831">[8 more]</label></div><br/><div class="children"><div class="content">Early on, the cloud&#x27;s entire point was to use &quot;commodity hardware,&quot; but now we have hyper-specialized hardware for individual services. AWS has Graviton, Inferentia and Tranium chips. Google has TPUs and Titan security cards, Azure uses FPGA&#x27;s and Sphere for security. This trend will continue.</div><br/><div id="41323366" class="c"><input type="checkbox" id="c-41323366" checked=""/><div class="controls bullet"><span class="by">0xbadcafebee</span><span>|</span><a href="#41322831">parent</a><span>|</span><a href="#41325584">next</a><span>|</span><label class="collapse" for="c-41323366">[-]</label><label class="expand" for="c-41323366">[1 more]</label></div><br/><div class="children"><div class="content">The cloud&#x27;s point was to be a computing utility, not to use commodity hardware. They may have been using commodity hardware but it was just a means to an end. That was also the era that &quot;commodity hardware&quot; was a literal game changer for all forms of large-scale computing for businesses, as before then you&#x27;d be beholden to an overpriced, over-valued proprietary computing vendor with crap support and no right to fix or modify it yourself. But anyway, the businesses you&#x27;re referencing are literally the largest tech companies in the world; custom hardware is pretty normal at that size.</div><br/></div></div><div id="41325584" class="c"><input type="checkbox" id="c-41325584" checked=""/><div class="controls bullet"><span class="by">BikiniPrince</span><span>|</span><a href="#41322831">parent</a><span>|</span><a href="#41323366">prev</a><span>|</span><a href="#41327228">next</a><span>|</span><label class="collapse" for="c-41325584">[-]</label><label class="expand" for="c-41325584">[1 more]</label></div><br/><div class="children"><div class="content">It was more of an opportunity initially. Bezos saw a chance to purchase extremely discounted systems. He had the foresight to move redundancy from a single host to clusters and then eventually data center level redundancy. This decision reshaped how services were implemented and scaled. Specialized devices returned because they offer a better value or in some cases enabled products that were never possible before. At the end of the day a lot of decisions are based around cost. Can you do it with a toaster or an oven cheaper? Can we cut all of our food into pieces and cook it in a Beowulf cluster of toasters? I think you get the idea.</div><br/></div></div><div id="41327228" class="c"><input type="checkbox" id="c-41327228" checked=""/><div class="controls bullet"><span class="by">re-thc</span><span>|</span><a href="#41322831">parent</a><span>|</span><a href="#41325584">prev</a><span>|</span><a href="#41323062">next</a><span>|</span><label class="collapse" for="c-41327228">[-]</label><label class="expand" for="c-41327228">[1 more]</label></div><br/><div class="children"><div class="content">&gt; hyper-specialized hardware for individual services
&gt; AWS has Graviton<p>This is commodity hardware.<p>It pretty much works off ARM spec. Besides AWS owning it and offering it cheaper it&#x27;s not in any way faster &#x2F; better than Intel &#x2F; AMD &#x2F; something else.<p>We&#x27;ve had custom motherboards, server cases, etc long ago even before clouds.<p>If Apple silicon happens in the cloud then maybe...</div><br/></div></div><div id="41323062" class="c"><input type="checkbox" id="c-41323062" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#41322831">parent</a><span>|</span><a href="#41327228">prev</a><span>|</span><a href="#41323831">next</a><span>|</span><label class="collapse" for="c-41323062">[-]</label><label class="expand" for="c-41323062">[4 more]</label></div><br/><div class="children"><div class="content">You must be talking about <i>very</i> early on because it would only have taken a short time spent on practical cloud building to begin realizing that much or even most of what is in &quot;commodity hardware&quot; is there to serve uses cases that cloud providers don&#x27;t have. Why do servers have redundant power supplies? What is the BMC good for? Who cares about these LEDs? Why would anyone use SAS? Is it very important that rack posts are 19 inches between centers, or was that a totally arbitrary decision made by AT&amp;T 90 years ago? What&#x27;s the point of 12V or 5V intermediate power rails? Is there a benefit from AC power to the host?</div><br/><div id="41327245" class="c"><input type="checkbox" id="c-41327245" checked=""/><div class="controls bullet"><span class="by">re-thc</span><span>|</span><a href="#41322831">root</a><span>|</span><a href="#41323062">parent</a><span>|</span><a href="#41323349">next</a><span>|</span><label class="collapse" for="c-41327245">[-]</label><label class="expand" for="c-41327245">[1 more]</label></div><br/><div class="children"><div class="content">&gt; realizing that much or even most of what is in &quot;commodity hardware&quot; is there to serve uses cases that cloud providers don&#x27;t have.<p>Why wouldn&#x27;t they have?<p>E.g.<p>&gt; Why do servers have redundant power supplies?<p>So if you lose power you don&#x27;t lose the whole server? It&#x27;s even more important for cloud providers that have huge servers with high density. You connect the different power supplies each to an independent power feed so 1 can go down. Would you rather have 2x the capacity instead?</div><br/></div></div><div id="41323349" class="c"><input type="checkbox" id="c-41323349" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41322831">root</a><span>|</span><a href="#41323062">parent</a><span>|</span><a href="#41327245">prev</a><span>|</span><a href="#41323409">next</a><span>|</span><label class="collapse" for="c-41323349">[-]</label><label class="expand" for="c-41323349">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not wrong but I would make a distinction between removing features (which requires little or no R&amp;D and saves money immediately) and designing custom ASICs (which requires large upfront R&amp;D and pay off only over time and at large scale).</div><br/></div></div></div></div></div></div><div id="41323831" class="c"><input type="checkbox" id="c-41323831" checked=""/><div class="controls bullet"><span class="by">apitman</span><span>|</span><a href="#41322831">prev</a><span>|</span><a href="#41323769">next</a><span>|</span><label class="collapse" for="c-41323831">[-]</label><label class="expand" for="c-41323831">[9 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the best way to provide a new EC2 instance with a fast ~256GB dataset directory? We&#x27;re currently using EBS volumes but it&#x27;s a pain to do updates to the data because we have to create a separate copy of the volume for each instance. EFS was too slow. Instance storage SSDs are ephemeral. Haven&#x27;t tried FSx Lustre yet.</div><br/><div id="41323891" class="c"><input type="checkbox" id="c-41323891" checked=""/><div class="controls bullet"><span class="by">MaBu</span><span>|</span><a href="#41323831">parent</a><span>|</span><a href="#41324677">next</a><span>|</span><label class="collapse" for="c-41323891">[-]</label><label class="expand" for="c-41323891">[6 more]</label></div><br/><div class="children"><div class="content">EFS supports 30 GiB&#x2F;s throughput now. <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;about-aws&#x2F;whats-new&#x2F;2024&#x2F;08&#x2F;amazon-efs-30-gibs-read-throughput&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;about-aws&#x2F;whats-new&#x2F;2024&#x2F;08&#x2F;amazon-ef...</a><p>Otherwise instance drive and sync over S3.</div><br/><div id="41324017" class="c"><input type="checkbox" id="c-41324017" checked=""/><div class="controls bullet"><span class="by">ayewo</span><span>|</span><a href="#41323831">root</a><span>|</span><a href="#41323891">parent</a><span>|</span><a href="#41324408">next</a><span>|</span><label class="collapse" for="c-41324017">[-]</label><label class="expand" for="c-41324017">[3 more]</label></div><br/><div class="children"><div class="content">Instance storage can be incredibly fast for certain workloads but it&#x27;s a shame AWS doesn&#x27;t offer instance storage on Windows EC2 instances.<p>Instance storage seems to only be available for (large) Linux EC2 instances.</div><br/><div id="41324350" class="c"><input type="checkbox" id="c-41324350" checked=""/><div class="controls bullet"><span class="by">msolson</span><span>|</span><a href="#41323831">root</a><span>|</span><a href="#41324017">parent</a><span>|</span><a href="#41324408">next</a><span>|</span><label class="collapse" for="c-41324350">[-]</label><label class="expand" for="c-41324350">[2 more]</label></div><br/><div class="children"><div class="content">Instance storage can be a good option depending on your workload, but definitely has limitations. There&#x27;s huge value in separating the lifecycle of storage from compute, and EBS provides higher durability than instance storage as well.<p>There are no operating system limitations that I&#x27;m aware of, however. I was just able to launch a Windows m6idn.2xlarge to verify.</div><br/><div id="41326831" class="c"><input type="checkbox" id="c-41326831" checked=""/><div class="controls bullet"><span class="by">ayewo</span><span>|</span><a href="#41323831">root</a><span>|</span><a href="#41324350">parent</a><span>|</span><a href="#41324408">next</a><span>|</span><label class="collapse" for="c-41326831">[-]</label><label class="expand" for="c-41326831">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for checking. I realize now that I wasn’t clear in my original comment.<p>My use case was to bring up a Windows instance using instance storage as the root device instead using of EBS which is the default root device.<p>I wanted to run some benchmarks directly on drive C:\ — backed by an NVMe SSD-based instance store — because of an app that will only install to drive C:\, but it seems there’s no way to do this.<p>The EC2 docs definitely gave me the impression that instance storage is not supported on Windows as a root volume.<p>Here’s one such note from the docs: <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;RootDeviceStorage.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;RootDevi...</a><p><i>”Windows instances do not support instance-store backed root volumes.”</i><p>Really nice that you are engaging with the comments here on HN as the article’s author.<p>(For others who may not be aware, msolson = Marc Olson)</div><br/></div></div></div></div></div></div><div id="41324408" class="c"><input type="checkbox" id="c-41324408" checked=""/><div class="controls bullet"><span class="by">apitman</span><span>|</span><a href="#41323831">root</a><span>|</span><a href="#41323891">parent</a><span>|</span><a href="#41324017">prev</a><span>|</span><a href="#41324677">next</a><span>|</span><label class="collapse" for="c-41324408">[-]</label><label class="expand" for="c-41324408">[2 more]</label></div><br/><div class="children"><div class="content">Impressive, but I don&#x27;t think we every determined conclusively whether our EFS problems were caused by throughput or latency.<p>Also, throughput is going to be limited by your instance type, right? Though that might also be the case for EBS. I can&#x27;t remember. Part of the problem is AWS performance is so confusing.</div><br/><div id="41324927" class="c"><input type="checkbox" id="c-41324927" checked=""/><div class="controls bullet"><span class="by">flybarrel</span><span>|</span><a href="#41323831">root</a><span>|</span><a href="#41324408">parent</a><span>|</span><a href="#41324677">next</a><span>|</span><label class="collapse" for="c-41324927">[-]</label><label class="expand" for="c-41324927">[1 more]</label></div><br/><div class="children"><div class="content">Yes throughput for EBS is also relevant to the instance type. It is all published here: <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;ebs-optimized.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;ebs-opti...</a><p>EBS connects to EC2 via a separate pipeline, different from the EC2 instance Networking bandwidth. This is true for all Nitro instances. 
EFS &#x2F; FSx connects to EC2 via Networking bandwidth. So you should refer to that if you are looking for the bandwidth information.</div><br/></div></div></div></div></div></div><div id="41324677" class="c"><input type="checkbox" id="c-41324677" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41323831">parent</a><span>|</span><a href="#41323891">prev</a><span>|</span><a href="#41326047">next</a><span>|</span><label class="collapse" for="c-41324677">[-]</label><label class="expand" for="c-41324677">[1 more]</label></div><br/><div class="children"><div class="content">EFS has tunable performance knobs.  You can turn them way, way up to get an insanely fast file server, but it will cost $$$</div><br/></div></div><div id="41326047" class="c"><input type="checkbox" id="c-41326047" checked=""/><div class="controls bullet"><span class="by">lustre-fan</span><span>|</span><a href="#41323831">parent</a><span>|</span><a href="#41324677">prev</a><span>|</span><a href="#41323769">next</a><span>|</span><label class="collapse" for="c-41326047">[-]</label><label class="expand" for="c-41326047">[1 more]</label></div><br/><div class="children"><div class="content">I think FSx&#x2F;Lustre would be the simplest assuming you&#x27;re using Linux instances. You can definitely scale to 256GB&#x2F;s or more. Plus, you can automatically hydrate the dataset from s3. Lustre is common for HPC and ML&#x2F;AI, so if you doing anything along those lines it&#x27;s a good fit.</div><br/></div></div></div></div><div id="41323769" class="c"><input type="checkbox" id="c-41323769" checked=""/><div class="controls bullet"><span class="by">herodoturtle</span><span>|</span><a href="#41323831">prev</a><span>|</span><a href="#41323034">next</a><span>|</span><label class="collapse" for="c-41323769">[-]</label><label class="expand" for="c-41323769">[1 more]</label></div><br/><div class="children"><div class="content">Loved this:<p>&gt; While the much celebrated ideal of a “full stack engineer” is valuable, in deep and complex systems it’s often even more valuable to create cohorts of experts who can collaborate and get really creative across the entire stack and all their individual areas of depth.</div><br/></div></div><div id="41323034" class="c"><input type="checkbox" id="c-41323034" checked=""/><div class="controls bullet"><span class="by">mannyv</span><span>|</span><a href="#41323769">prev</a><span>|</span><a href="#41322639">next</a><span>|</span><label class="collapse" for="c-41323034">[-]</label><label class="expand" for="c-41323034">[2 more]</label></div><br/><div class="children"><div class="content">The most surprising thing ia that the author had no previous experience in the domain. It&#x27;s almost impossible to get hired at AWS now without domain expertise, AFAIK.</div><br/><div id="41324388" class="c"><input type="checkbox" id="c-41324388" checked=""/><div class="controls bullet"><span class="by">msolson</span><span>|</span><a href="#41323034">parent</a><span>|</span><a href="#41322639">next</a><span>|</span><label class="collapse" for="c-41324388">[-]</label><label class="expand" for="c-41324388">[1 more]</label></div><br/><div class="children"><div class="content">At least in the organizations I&#x27;m a part of this isn&#x27;t true. We do look for both specialists and generalists, and focus on experience and how it could apply.<p>It&#x27;s difficult to innovate by just repeating what&#x27;s been done before. But everything you learn along the way helps shape that innovation.</div><br/></div></div></div></div><div id="41322639" class="c"><input type="checkbox" id="c-41322639" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#41323034">prev</a><span>|</span><a href="#41323870">next</a><span>|</span><label class="collapse" for="c-41322639">[-]</label><label class="expand" for="c-41322639">[1 more]</label></div><br/><div class="children"><div class="content">Great article.<p><i>&quot;EBS is capable of delivering more IOPS to a single instance today than it could deliver to an entire Availability Zone (AZ) in the early years on top of HDDs.&quot;</i><p>Dang!</div><br/></div></div><div id="41323870" class="c"><input type="checkbox" id="c-41323870" checked=""/><div class="controls bullet"><span class="by">Silasdev</span><span>|</span><a href="#41322639">prev</a><span>|</span><a href="#41322117">next</a><span>|</span><label class="collapse" for="c-41323870">[-]</label><label class="expand" for="c-41323870">[1 more]</label></div><br/><div class="children"><div class="content">Great read, although a shame that it didn&#x27;t go any further than adding the write cache SSD solution, which must have been many years ago. I was hoping for a little more recent info on the EBS architecture.</div><br/></div></div><div id="41322117" class="c"><input type="checkbox" id="c-41322117" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#41323870">prev</a><span>|</span><a href="#41322004">next</a><span>|</span><label class="collapse" for="c-41322117">[-]</label><label class="expand" for="c-41322117">[20 more]</label></div><br/><div class="children"><div class="content">I think the most fascinating thing is watching them relearn every lesson the storage industry already knew about a decade earlier.  Feels like most of this could have been solved by either hiring storage industry experts or just acquiring one of the major vendors.</div><br/><div id="41322541" class="c"><input type="checkbox" id="c-41322541" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41322117">parent</a><span>|</span><a href="#41322147">next</a><span>|</span><label class="collapse" for="c-41322541">[-]</label><label class="expand" for="c-41322541">[3 more]</label></div><br/><div class="children"><div class="content">There are substantial differences in developing hyperscale storage from the systems that were built previously.  But note that many of the architects of these systems <i>were</i> previously storage industry experts, and acquiring an existing vendor would not have been an asset to AWS since these new systems had a wide range of issues that the vendors never had to solve.<p>Your comment elsewhere about NetApp solving all known problems with WAFL.  Hahahaha.  Have you tried deleting a 5TB file in a filesystem at 95% capacity with snapshots enabled?</div><br/><div id="41325457" class="c"><input type="checkbox" id="c-41325457" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322541">parent</a><span>|</span><a href="#41323094">next</a><span>|</span><label class="collapse" for="c-41325457">[-]</label><label class="expand" for="c-41325457">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Your comment elsewhere about NetApp solving all known problems with WAFL. Hahahaha.<p>I guess it&#x27;s a good thing I didn&#x27;t say NetApp solved &quot;all known problems with WAFL&quot; - but critical reading would&#x27;ve required you to respond to the content of the post, not provide an inaccurate summary to make a point that wasn&#x27;t there.<p>What I <i>DID</i> say is that NetApp solved the issue of spewing random small writes all over the disk, resulting in horrendous performance on subsequent reads from spinning disk.sace reclamation takes a while.  What&#x27;s your point, assuming you had one?  Because deleting a 5TB file on a filesystem 95% full with lots of snapshots is a daily workflow for... nobody?  And if it is: there are countless ways to avoid that situation, but I assume you knew that too?</div><br/></div></div><div id="41323094" class="c"><input type="checkbox" id="c-41323094" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322541">parent</a><span>|</span><a href="#41325457">prev</a><span>|</span><a href="#41322147">next</a><span>|</span><label class="collapse" for="c-41323094">[-]</label><label class="expand" for="c-41323094">[1 more]</label></div><br/><div class="children"><div class="content">Or tried to create the 2147483649th inode on a filer and watched it try to repair itself fruitlessly for a month?</div><br/></div></div></div></div><div id="41322147" class="c"><input type="checkbox" id="c-41322147" checked=""/><div class="controls bullet"><span class="by">jeeyoungk</span><span>|</span><a href="#41322117">parent</a><span>|</span><a href="#41322541">prev</a><span>|</span><a href="#41322323">next</a><span>|</span><label class="collapse" for="c-41322147">[-]</label><label class="expand" for="c-41322147">[5 more]</label></div><br/><div class="children"><div class="content">What is there to learn from an &quot;storage industry expert&quot; or major vendors? network attached block level storage at AWS&#x27;s scale hasn&#x27;t been done before.</div><br/><div id="41322387" class="c"><input type="checkbox" id="c-41322387" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322147">parent</a><span>|</span><a href="#41322268">next</a><span>|</span><label class="collapse" for="c-41322387">[-]</label><label class="expand" for="c-41322387">[1 more]</label></div><br/><div class="children"><div class="content">Some of it, like random IOPS, spindle bias etc...was well known.<p>Well among implementers, vendors were mostly locked into the vertical scaling model.<p>I ran a SGI cluster running CXFS in 2000 as an example, and by the time EBS launched, I was spending most of my SAN architect time trying to get away from central storage.<p>There were absolutely new problems and amazing solutions by the EBS team, but there was information.<p>Queue theory was required for any meaningful SAN deployment as an example and RPM&#x2F;3600 had always been a metric for HD performance under random.<p>Not that everyone used them, but I had to.</div><br/></div></div><div id="41322268" class="c"><input type="checkbox" id="c-41322268" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322147">parent</a><span>|</span><a href="#41322387">prev</a><span>|</span><a href="#41322323">next</a><span>|</span><label class="collapse" for="c-41322268">[-]</label><label class="expand" for="c-41322268">[3 more]</label></div><br/><div class="children"><div class="content">&gt;What is there to learn from an &quot;storage industry expert&quot; or major vendors?<p>I mean, literally every problem they outlined.<p>&gt;Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior. Early on, we knew that we needed to spread customers across many disks to achieve reasonable performance. This had a benefit, it dropped the peak outlier latency for the hottest workloads, but unfortunately it spread the inconsistent behavior out so that it impacted many customers.<p>Right - which we all knew about in the 90s, and NetApp more or less solved with WAFL.<p>&gt;We made a small change to our software that staged new writes onto that SSD, allowing us to return completion back to your application, and then flushed the writes to the slower hard disk asynchronously.<p>So a write cache, which again every major vendor had from the beginning of time.  NetApp used NVRam cards, EMC used dedicated UPSs to give their memory time to de-stage.<p>Etc. etc.<p>&gt;network attached block level storage at AWS&#x27;s scale hasn&#x27;t been done before.<p>This is just patently false.  It&#x27;s not like EBS is one giant repository of storage.  The &quot;scale&quot; they push individual instances to isn&#x27;t anything unique.  The fact they&#x27;re deploying more pods in totality than any individual enterprise isn&#x27;t really relevant beyond the fact they&#x27;re getting even greater volume discounts from their suppliers.  At some point whether I&#x27;m managing 100 of the same thing or 1,000 - if I&#x27;ve built proper automation my only additional overhead is replacing failed hardware.<p>Downvote away, watching HN think that re-inventing the wheel instead of asking someone who has been there already what the landmines are seems to be a common theme.</div><br/><div id="41322436" class="c"><input type="checkbox" id="c-41322436" checked=""/><div class="controls bullet"><span class="by">tanelpoder</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322268">parent</a><span>|</span><a href="#41322427">next</a><span>|</span><label class="collapse" for="c-41322436">[-]</label><label class="expand" for="c-41322436">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing that at cloud scale, more innovation &amp; scalability is needed for the control plane (not to mention the network itself).<p>Regarding a durable&#x2F;asynchronously destaged write cache, I think EMC Symmetrix already had such a feature in the end of &#x27;80s or 1990 (can&#x27;t find the source anymore).</div><br/></div></div><div id="41322427" class="c"><input type="checkbox" id="c-41322427" checked=""/><div class="controls bullet"><span class="by">abadpoli</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322268">parent</a><span>|</span><a href="#41322436">prev</a><span>|</span><a href="#41322323">next</a><span>|</span><label class="collapse" for="c-41322427">[-]</label><label class="expand" for="c-41322427">[1 more]</label></div><br/><div class="children"><div class="content">&gt; whether I&#x27;m managing 100 of the same thing or 1,000 - if I&#x27;ve built proper automation my only additional overhead is replacing failed hardware<p>Hahahah surely this is a joke, right?<p>If it’s so easy and you already had solved all these problems, why didn’t someone already build it? Why didn’t <i>you</i> build EBS, since you apparently have all the answers?</div><br/></div></div></div></div></div></div><div id="41322323" class="c"><input type="checkbox" id="c-41322323" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41322117">parent</a><span>|</span><a href="#41322147">prev</a><span>|</span><a href="#41325474">next</a><span>|</span><label class="collapse" for="c-41322323">[-]</label><label class="expand" for="c-41322323">[9 more]</label></div><br/><div class="children"><div class="content">For the right kind of people, it&#x27;s tremendously satisfying for themselves to rethink and independently rediscover the best solutions. If you hire an industry expert that already knows everything, asking them to design the same things and write the same code again is not satisfying at all.</div><br/><div id="41322846" class="c"><input type="checkbox" id="c-41322846" checked=""/><div class="controls bullet"><span class="by">hobs</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322323">parent</a><span>|</span><a href="#41322339">prev</a><span>|</span><a href="#41322540">next</a><span>|</span><label class="collapse" for="c-41322846">[-]</label><label class="expand" for="c-41322846">[6 more]</label></div><br/><div class="children"><div class="content">Ok, but we&#x27;re running a business here not catering to the satisfaction of random programmers. The NIH part of this stuff is a bit weird.</div><br/><div id="41323482" class="c"><input type="checkbox" id="c-41323482" checked=""/><div class="controls bullet"><span class="by">shermantanktop</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322846">parent</a><span>|</span><a href="#41323563">next</a><span>|</span><label class="collapse" for="c-41323482">[-]</label><label class="expand" for="c-41323482">[4 more]</label></div><br/><div class="children"><div class="content">Turns out that if you don&#x27;t cater to the satisfaction of programmers, you often kill the golden goose, and then you end up on the sidelines saying &quot;oh, you shouldn&#x27;t have done it that way&quot; while someone else is succeeding.<p>People often talk about standing on the shoulders of giants.  To extend that metaphor, the only ones standing there are the ones who climbed the giant.  If you hire a college grad and stick them on that giant&#x27;s shoulder, it&#x27;s not the same.</div><br/><div id="41324319" class="c"><input type="checkbox" id="c-41324319" checked=""/><div class="controls bullet"><span class="by">hobs</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41323482">parent</a><span>|</span><a href="#41323563">next</a><span>|</span><label class="collapse" for="c-41324319">[-]</label><label class="expand" for="c-41324319">[3 more]</label></div><br/><div class="children"><div class="content">Sure... but as an eng manager my job isn&#x27;t to cater to your creative outputs in subordination to good products.<p>Craft comes before art.</div><br/><div id="41325056" class="c"><input type="checkbox" id="c-41325056" checked=""/><div class="controls bullet"><span class="by">d110af5ccf</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41324319">parent</a><span>|</span><a href="#41324440">next</a><span>|</span><label class="collapse" for="c-41325056">[-]</label><label class="expand" for="c-41325056">[1 more]</label></div><br/><div class="children"><div class="content">Presumably your job is to get results. If catering to someone in some way is able to achieve that then it&#x27;s a perfectly legitimate approach to consider.</div><br/></div></div><div id="41324440" class="c"><input type="checkbox" id="c-41324440" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41324319">parent</a><span>|</span><a href="#41325056">prev</a><span>|</span><a href="#41323563">next</a><span>|</span><label class="collapse" for="c-41324440">[-]</label><label class="expand" for="c-41324440">[1 more]</label></div><br/><div class="children"><div class="content">Sure but that assumes the company itself has sufficient attraction that can overpower the lack of job satisfaction. Maybe you pay more. Maybe your brand is more attractive thanks to PR. Maybe you offer more perks. Maybe the product itself is more appealing. Maybe it&#x27;s something else. You have to have some other advantage to retain people who aren&#x27;t satisfied in their jobs.</div><br/></div></div></div></div></div></div><div id="41323563" class="c"><input type="checkbox" id="c-41323563" checked=""/><div class="controls bullet"><span class="by">01HNNWZ0MV43FF</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322846">parent</a><span>|</span><a href="#41323482">prev</a><span>|</span><a href="#41322540">next</a><span>|</span><label class="collapse" for="c-41323563">[-]</label><label class="expand" for="c-41323563">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d burn out quicker and have to charge more for my salary if the work was too boring</div><br/></div></div></div></div><div id="41322540" class="c"><input type="checkbox" id="c-41322540" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41322117">root</a><span>|</span><a href="#41322323">parent</a><span>|</span><a href="#41322846">prev</a><span>|</span><a href="#41325474">next</a><span>|</span><label class="collapse" for="c-41322540">[-]</label><label class="expand" for="c-41322540">[1 more]</label></div><br/><div class="children"><div class="content">also, you can often discover new and better solutions if you don&#x27;t know they&#x27;re impossible.</div><br/></div></div></div></div><div id="41325474" class="c"><input type="checkbox" id="c-41325474" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#41322117">parent</a><span>|</span><a href="#41322323">prev</a><span>|</span><a href="#41322129">next</a><span>|</span><label class="collapse" for="c-41325474">[-]</label><label class="expand" for="c-41325474">[1 more]</label></div><br/><div class="children"><div class="content">The answer to why your strategy most likely would have been a failure lies outside of technology domain</div><br/></div></div></div></div></div></div></div></div></div></body></html>