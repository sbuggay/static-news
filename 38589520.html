<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702198857130" as="style"/><link rel="stylesheet" href="styles.css?v=1702198857130"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Ask HN: What&#x27;s the best hardware to run small/medium models locally?</a> </div><div class="subtext"><span>triyambakam</span> | <span>41 comments</span></div><br/><div><div id="38590217" class="c"><input type="checkbox" id="c-38590217" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#38589752">next</a><span>|</span><label class="collapse" for="c-38590217">[-]</label><label class="expand" for="c-38590217">[1 more]</label></div><br/><div class="children"><div class="content">1. The GPU market is a mess! <a href="https:&#x2F;&#x2F;www.tweaktown.com&#x2F;news&#x2F;94394&#x2F;amds-top-end-rdna-3-sales-blow-away-nvidia-rivals-is-this-why-new-super-gpus-are-coming&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tweaktown.com&#x2F;news&#x2F;94394&#x2F;amds-top-end-rdna-3-sal...</a> Insiders who watch the prices and talk to VAR&#x27;s all say that the channels seem stuffed and that prices are holding back sales.<p>2. AMD: They may change the land scape in coming months. And it looks like the US gov restrictions on GPU&#x27;s are going to impact price in the server market in 2024.<p>3. The stacks are evolving quickly. What you buy for today may be supersede by something tomorrow that means you should have spent more or could have spent less.<p>If you want to play: Ram, is what matters most. GPU ram and system ram (in that order). Get the best GPU you can (ram wise) under clock it and then add system memory if you can. Once you have a test bed that works for you, renting&#x2F;cloud is a way to scale and play with bigger toys till you have a better sense of what you want and&#x2F;or need.</div><br/></div></div><div id="38589752" class="c"><input type="checkbox" id="c-38589752" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#38590217">prev</a><span>|</span><a href="#38589688">next</a><span>|</span><label class="collapse" for="c-38589752">[-]</label><label class="expand" for="c-38589752">[3 more]</label></div><br/><div class="children"><div class="content">I think there are a couple of basic questions need answered before we can find a good solution:<p>1) What are you trying to do?<p>2) What&#x27;s your budget?<p>Generically saying, &quot;run inference&quot; is like... you can do that on your current thinkpad, if you want a small enough model. If you want to run 7B or 13B or 34B models for document or sentiment analysis, or whatever, then you can move to the budget question.<p>When I was faced with this question, I bought the cheapest 4060 Ti with 16GB I could find. It does &quot;okay&quot;. Here&#x27;s an example run:<p><pre><code>  Llama.generate: prefix-match hit
  
  llama_print_timings:        load time =     627.53 ms
  llama_print_timings:      sample time =     415.30 ms &#x2F;   200 runs   (    2.08 ms per token,   481.58 tokens per second)
  llama_print_timings: prompt eval time =     162.12 ms &#x2F;    62 tokens (    2.61 ms per token,   382.44 tokens per second)
  llama_print_timings:        eval time =    8587.32 ms &#x2F;   199 runs   (   43.15 ms per token,    23.17 tokens per second)
  llama_print_timings:       total time =    9498.89 ms
  Output generated in 9.79 seconds (20.43 tokens&#x2F;s, 200 tokens, context 63, seed 1836128893)

</code></pre>
I&#x27;m using the text-generation-webui to provide the OpenAI API interface. It&#x27;s pretty easy to hit:<p><pre><code>  import os
  import openai
  url = &quot;http:&#x2F;&#x2F;localhost:7860&#x2F;v1&quot;
  openai_api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)
  client = openai.OpenAI(base_url=url, api_key=openai_api_key)
  result = client.chat.completions.create(
      model=&quot;wizardlm_wizardcoder-python-13b-v1.0&quot;,
      messages = [
          {&quot;role&quot;:&quot;system&quot;, &quot;content&quot;:&quot;You are a helpful AI agent. You are honest and truthful&quot;},
          {&quot;role&quot;:&quot;user&quot;, &quot;content&quot;: &quot;What is the best approach when writing recursive functions?&quot;},
      ]
  print(result)

</code></pre>
But again, it just depends on what you want to do.</div><br/><div id="38589857" class="c"><input type="checkbox" id="c-38589857" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38589752">parent</a><span>|</span><a href="#38590169">next</a><span>|</span><label class="collapse" for="c-38589857">[-]</label><label class="expand" for="c-38589857">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What are you trying to do?<p>This is by far the most important question. Because frankly, I can run LLaMA on my raspberry pi. It&#x27;s slow as hell and not suited for any real time task but there are definitely operations where this would be an appropriate cost effective solution (preferably with actually a smaller distilled model).<p>There is no one size-fits all solution. The general advice is going to be a general mid tier graphics card but I assume that&#x27;s information OP already has or could have found just as easily by typing this question into Google or any LLM. So if you (OP) want better advice, we got to have more information. The more detailed, the better (if this is a commercial application, then the answer is A100 because geforce cards are not allowed to be used for commercial environments, but no one&#x27;s really going to stop you either). Ask vague question, get vague answers. But we will ask refining questions to help you ask better questions too :)</div><br/></div></div><div id="38590169" class="c"><input type="checkbox" id="c-38590169" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#38589752">parent</a><span>|</span><a href="#38589857">prev</a><span>|</span><a href="#38589688">next</a><span>|</span><label class="collapse" for="c-38590169">[-]</label><label class="expand" for="c-38590169">[1 more]</label></div><br/><div class="children"><div class="content">And a third question - what kind of machine are you looking for?<p>If you want a laptop, that&#x27;s going to be a very different machine to a desktop.</div><br/></div></div></div></div><div id="38589688" class="c"><input type="checkbox" id="c-38589688" checked=""/><div class="controls bullet"><span class="by">MandieD</span><span>|</span><a href="#38589752">prev</a><span>|</span><a href="#38589664">next</a><span>|</span><label class="collapse" for="c-38589688">[-]</label><label class="expand" for="c-38589688">[1 more]</label></div><br/><div class="children"><div class="content">A data point for you: 7B models at 5-bit quantization run quite comfortably under llama.cpp on the AMD Radeon RX 6700 XT, which has 12GB VRAM and was part of a lot of gaming PC builds around 2021-22.<p>I can’t give this as a recommendation - there are far more tools available for Nvidia GPUs, but larger VRAM is available on AMD GPUs at lower prices from what I can see.</div><br/></div></div><div id="38589664" class="c"><input type="checkbox" id="c-38589664" checked=""/><div class="controls bullet"><span class="by">CJefferson</span><span>|</span><a href="#38589688">prev</a><span>|</span><a href="#38589760">next</a><span>|</span><label class="collapse" for="c-38589664">[-]</label><label class="expand" for="c-38589664">[1 more]</label></div><br/><div class="children"><div class="content">In my experience, an nvidia card with the most memory you can get — that’s more important than speed, as models are tending to get bigger, and streaming models really hits speed.<p>I don’t have any Mac experience.</div><br/></div></div><div id="38589760" class="c"><input type="checkbox" id="c-38589760" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#38589664">prev</a><span>|</span><a href="#38589904">next</a><span>|</span><label class="collapse" for="c-38589760">[-]</label><label class="expand" for="c-38589760">[1 more]</label></div><br/><div class="children"><div class="content">Somewhat related; how to run an uncensored model locally? I run llamafile (llamafile-server-0.1-llava-v1.5-7b-q4 and mistral-7b-instruct-v0.1-Q4_K_M-server) ones on my macbook m1 and they run file (fast enough for playing), but they both seem neutered quite a bit. It&#x27;s hard to get them off the rails and mistral (the above one) actually barfs really quickly just repeating the same letter (fffffff usually) where it should&#x27;ve said fuck. Now i&#x27;m not looking for something that writes porn or whatnot, but the online models are so pc, it&#x27;s getting on my nerves.</div><br/></div></div><div id="38589904" class="c"><input type="checkbox" id="c-38589904" checked=""/><div class="controls bullet"><span class="by">rkagerer</span><span>|</span><a href="#38589760">prev</a><span>|</span><a href="#38589851">next</a><span>|</span><label class="collapse" for="c-38589904">[-]</label><label class="expand" for="c-38589904">[2 more]</label></div><br/><div class="children"><div class="content">Any links to setting up a ChatGPT-like experience that is entirely local - ie. no connectivity to the web&#x2F;cloud?</div><br/><div id="38590216" class="c"><input type="checkbox" id="c-38590216" checked=""/><div class="controls bullet"><span class="by">plasticchris</span><span>|</span><a href="#38589904">parent</a><span>|</span><a href="#38589851">next</a><span>|</span><label class="collapse" for="c-38590216">[-]</label><label class="expand" for="c-38590216">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a></div><br/></div></div></div></div><div id="38589851" class="c"><input type="checkbox" id="c-38589851" checked=""/><div class="controls bullet"><span class="by">gchadwick</span><span>|</span><a href="#38589904">prev</a><span>|</span><a href="#38589783">next</a><span>|</span><label class="collapse" for="c-38589851">[-]</label><label class="expand" for="c-38589851">[2 more]</label></div><br/><div class="children"><div class="content">Have you considered running on a cloud machine instead? You can rent machines on <a href="https:&#x2F;&#x2F;vast.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;vast.ai&#x2F;</a> for under $1 an hour that should work for small&#x2F;medium models (I&#x27;ve mostly been playing with stable diffusion so I don&#x27;t know what you&#x27;d need for an LLM off hand).<p>Good GPUs and Apple hardware is pricey. Get a bit of automation setup with some cloud storage (e.g backblaze B2) and you can have a machine ready to run your personally fined tuned model rapidly with a CLI command or two.<p>There will be a break even point of course. Though a major advantage of renting is you can move easily as the tech does. You don&#x27;t want to sink large amounts of money into a GPU only to find the next new hot open model needs more memory than you&#x27;ve got.</div><br/><div id="38589889" class="c"><input type="checkbox" id="c-38589889" checked=""/><div class="controls bullet"><span class="by">firtoz</span><span>|</span><a href="#38589851">parent</a><span>|</span><a href="#38589783">next</a><span>|</span><label class="collapse" for="c-38589889">[-]</label><label class="expand" for="c-38589889">[1 more]</label></div><br/><div class="children"><div class="content">I will link a few that I haven&#x27;t used yet but seem promising:<p>- <a href="https:&#x2F;&#x2F;octoai.cloud&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;octoai.cloud&#x2F;</a><p>- <a href="https:&#x2F;&#x2F;www.fal.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.fal.ai&#x2F;</a><p>- <a href="https:&#x2F;&#x2F;vast.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;vast.ai&#x2F;</a> (linked by gchadwick above)<p>- <a href="https:&#x2F;&#x2F;www.runpod.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.runpod.io&#x2F;</a><p>- <a href="https:&#x2F;&#x2F;www.cerebrium.ai&#x2F;">https:&#x2F;&#x2F;www.cerebrium.ai&#x2F;</a></div><br/></div></div></div></div><div id="38589783" class="c"><input type="checkbox" id="c-38589783" checked=""/><div class="controls bullet"><span class="by">oaththrowaway</span><span>|</span><a href="#38589851">prev</a><span>|</span><a href="#38590175">next</a><span>|</span><label class="collapse" for="c-38589783">[-]</label><label class="expand" for="c-38589783">[1 more]</label></div><br/><div class="children"><div class="content">4060Ti w&#x2F; 16GB VRAM or 3090 w&#x2F; 24 GB VRAM<p>Of course with those you&#x27;ll also have to spend some money on motherboard, ram, SSD, PSU, CPU, ect.<p>I think the best bang for the buck is probably a Mac studio with as much ram as you can afford.<p>I bought an RTX A2000 (12GB VRAM), and it&#x27;s fine for 7B models and some 13B models with 4 bit quantization, but I kind of regret not getting something with more VRAM.</div><br/></div></div><div id="38590175" class="c"><input type="checkbox" id="c-38590175" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#38589783">prev</a><span>|</span><a href="#38589828">next</a><span>|</span><label class="collapse" for="c-38590175">[-]</label><label class="expand" for="c-38590175">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m happy with my used 3090.</div><br/></div></div><div id="38589828" class="c"><input type="checkbox" id="c-38589828" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38590175">prev</a><span>|</span><a href="#38589697">next</a><span>|</span><label class="collapse" for="c-38589828">[-]</label><label class="expand" for="c-38589828">[3 more]</label></div><br/><div class="children"><div class="content">A gaming desktop PC with Nvidia 3060 12GB or better. Upgrade the GPU first if you can afford it, prioritizing VRAM capacity and bandwidth. Nvidia GPU performance will blow any CPU including M3 out of the water and the software ecosystem pretty much assumes you are using Nvidia. Laptop GPUs are not equivalent to the desktop ones with the same number so don&#x27;t be fooled. 8x 3090 (purchased used) is a popular configuration for people who have money and want to run the biggest models, but splitting models between GPUs requires extra work.<p>Personally I have 1x 4090 because I like gaming too, but it isn&#x27;t really a big improvement over 3090 for ML unless you have a specific use for FP8, because VRAM capacity and bandwidth are very similar.</div><br/></div></div><div id="38589697" class="c"><input type="checkbox" id="c-38589697" checked=""/><div class="controls bullet"><span class="by">RandomWorker</span><span>|</span><a href="#38589828">prev</a><span>|</span><a href="#38589748">next</a><span>|</span><label class="collapse" for="c-38589697">[-]</label><label class="expand" for="c-38589697">[2 more]</label></div><br/><div class="children"><div class="content">I’m running mistral 7B on a M1 Mac 8GB just barely. It’s ask a question get a coffee type of thing. No idea how this works, as 32 bit floats require 4 bytes and with 7B it would need to be swapping with the SSD.<p>If I had the cash I would go for 24GB M2&#x2F;3 pro. That would allow me to comfortably load the 7B model in to ram.</div><br/><div id="38589727" class="c"><input type="checkbox" id="c-38589727" checked=""/><div class="controls bullet"><span class="by">puchatek</span><span>|</span><a href="#38589697">parent</a><span>|</span><a href="#38589748">next</a><span>|</span><label class="collapse" for="c-38589727">[-]</label><label class="expand" for="c-38589727">[1 more]</label></div><br/><div class="children"><div class="content">Can I ask what you&#x27;re using it for?</div><br/></div></div></div></div><div id="38589748" class="c"><input type="checkbox" id="c-38589748" checked=""/><div class="controls bullet"><span class="by">pogue</span><span>|</span><a href="#38589697">prev</a><span>|</span><a href="#38589848">next</a><span>|</span><label class="collapse" for="c-38589748">[-]</label><label class="expand" for="c-38589748">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t want to derail the OP&#x27;s question, but would the same kind of system to run an LLM on also be suitable for an image generator like Stable Diffusion or does it work through different methods?</div><br/><div id="38589776" class="c"><input type="checkbox" id="c-38589776" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#38589748">parent</a><span>|</span><a href="#38589848">next</a><span>|</span><label class="collapse" for="c-38589776">[-]</label><label class="expand" for="c-38589776">[1 more]</label></div><br/><div class="children"><div class="content">I think so. A big nvidia gpu will run both.</div><br/></div></div></div></div><div id="38589848" class="c"><input type="checkbox" id="c-38589848" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38589748">prev</a><span>|</span><a href="#38589686">next</a><span>|</span><label class="collapse" for="c-38589848">[-]</label><label class="expand" for="c-38589848">[3 more]</label></div><br/><div class="children"><div class="content">A used RTX3090 of eBay is the most interesting budget option by far.<p>If you have twice the cash, go for a new RTX4090 for rougly twice the performance.<p>If you need more than 24GB vram, you want to get comfortable with sharding across a few of the 3090&#x27;s, or spend <i>a</i> <i>lott</i> more on a 48, 80, 100 GB card.<p>If you feel adventurous, you can go a non nvidia route, but expect a lott of friction and elbow grease at least for now.</div><br/><div id="38589958" class="c"><input type="checkbox" id="c-38589958" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38589848">parent</a><span>|</span><a href="#38589686">next</a><span>|</span><label class="collapse" for="c-38589958">[-]</label><label class="expand" for="c-38589958">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, the 3090 is a meme in local AI communities. Additionally, the support is amazing because its essentially the same architecture as an A100.<p>The 3060 is popular too, being a 3090 cut in half.</div><br/><div id="38590204" class="c"><input type="checkbox" id="c-38590204" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#38589848">root</a><span>|</span><a href="#38589958">parent</a><span>|</span><a href="#38589686">next</a><span>|</span><label class="collapse" for="c-38590204">[-]</label><label class="expand" for="c-38590204">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Yeah, the 3090 is a meme in local AI communities.<p>Calling the 3090 a &quot;meme&quot; makes it sound like the 3090 is a joke. Do you mean that the 3090 is &quot;well-known&quot; in local AI communities?</div><br/></div></div></div></div></div></div><div id="38589686" class="c"><input type="checkbox" id="c-38589686" checked=""/><div class="controls bullet"><span class="by">steve_adams_86</span><span>|</span><a href="#38589848">prev</a><span>|</span><a href="#38589856">next</a><span>|</span><label class="collapse" for="c-38589686">[-]</label><label class="expand" for="c-38589686">[1 more]</label></div><br/><div class="children"><div class="content">Somewhat related. I’ve got an M2 Max Mac Studio with 32GB of ram. Is there anything interesting I can do with it in terms of ML? What’s the scene like on moderately powered equipment like this?</div><br/></div></div><div id="38589856" class="c"><input type="checkbox" id="c-38589856" checked=""/><div class="controls bullet"><span class="by">rickette</span><span>|</span><a href="#38589686">prev</a><span>|</span><a href="#38589777">next</a><span>|</span><label class="collapse" for="c-38589856">[-]</label><label class="expand" for="c-38589856">[1 more]</label></div><br/><div class="children"><div class="content">Llama models run fine on the M2&#x2F;M3 Macbooks thanks to llama.cpp&#x2F;GGML.</div><br/></div></div><div id="38589777" class="c"><input type="checkbox" id="c-38589777" checked=""/><div class="controls bullet"><span class="by">_vojam</span><span>|</span><a href="#38589856">prev</a><span>|</span><a href="#38589666">next</a><span>|</span><label class="collapse" for="c-38589777">[-]</label><label class="expand" for="c-38589777">[1 more]</label></div><br/><div class="children"><div class="content">You can try edgeimpulse.com they support a lot of “small” hardware for running different models.</div><br/></div></div><div id="38589666" class="c"><input type="checkbox" id="c-38589666" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#38589777">prev</a><span>|</span><a href="#38589867">next</a><span>|</span><label class="collapse" for="c-38589666">[-]</label><label class="expand" for="c-38589666">[1 more]</label></div><br/><div class="children"><div class="content">MacBook, thanks to Apple&#x27;s new MLX framework.</div><br/></div></div><div id="38589867" class="c"><input type="checkbox" id="c-38589867" checked=""/><div class="controls bullet"><span class="by">rootusrootus</span><span>|</span><a href="#38589666">prev</a><span>|</span><a href="#38589701">next</a><span>|</span><label class="collapse" for="c-38589867">[-]</label><label class="expand" for="c-38589867">[1 more]</label></div><br/><div class="children"><div class="content">I started to put together a second machine to be good at inference and then decided to just make my daily driver capable enough.  Ended up upgrading my laptop to an MBP w&#x2F;M2 MAX and 96GB.  It runs even bigger models fairly well.</div><br/></div></div><div id="38589701" class="c"><input type="checkbox" id="c-38589701" checked=""/><div class="controls bullet"><span class="by">f0000</span><span>|</span><a href="#38589867">prev</a><span>|</span><a href="#38589663">next</a><span>|</span><label class="collapse" for="c-38589701">[-]</label><label class="expand" for="c-38589701">[4 more]</label></div><br/><div class="children"><div class="content">Just run them on AWS</div><br/><div id="38589861" class="c"><input type="checkbox" id="c-38589861" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38589701">parent</a><span>|</span><a href="#38589751">next</a><span>|</span><label class="collapse" for="c-38589861">[-]</label><label class="expand" for="c-38589861">[1 more]</label></div><br/><div class="children"><div class="content">How would AWS run the user&#x27;s models locally (as per the question)?</div><br/></div></div><div id="38589751" class="c"><input type="checkbox" id="c-38589751" checked=""/><div class="controls bullet"><span class="by">pogue</span><span>|</span><a href="#38589701">parent</a><span>|</span><a href="#38589861">prev</a><span>|</span><a href="#38589663">next</a><span>|</span><label class="collapse" for="c-38589751">[-]</label><label class="expand" for="c-38589751">[2 more]</label></div><br/><div class="children"><div class="content">Can you expand more on how that&#x27;s done or point me in the direction to a guide?</div><br/><div id="38589860" class="c"><input type="checkbox" id="c-38589860" checked=""/><div class="controls bullet"><span class="by">dr_kiszonka</span><span>|</span><a href="#38589701">root</a><span>|</span><a href="#38589751">parent</a><span>|</span><a href="#38589663">next</a><span>|</span><label class="collapse" for="c-38589860">[-]</label><label class="expand" for="c-38589860">[1 more]</label></div><br/><div class="children"><div class="content">Possibly the easiest way would be to run them in a notebook on Amazon SageMaker.<p><a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;sagemaker&#x2F;notebooks&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;aws.amazon.com&#x2F;sagemaker&#x2F;notebooks&#x2F;</a></div><br/></div></div></div></div></div></div><div id="38589747" class="c"><input type="checkbox" id="c-38589747" checked=""/><div class="controls bullet"><span class="by">jocaal</span><span>|</span><a href="#38589663">prev</a><span>|</span><label class="collapse" for="c-38589747">[-]</label><label class="expand" for="c-38589747">[7 more]</label></div><br/><div class="children"><div class="content">Nvidia GPU&#x27;s are really your only choice. There is no framework as mature as CUDA and nvidia has been making the fastest hardware for decades. They know their stuff when it comes to architecture, so its unlikely that the hot new thing will actually be able to compete.</div><br/><div id="38589768" class="c"><input type="checkbox" id="c-38589768" checked=""/><div class="controls bullet"><span class="by">sam_lowry_</span><span>|</span><a href="#38589747">parent</a><span>|</span><a href="#38589759">next</a><span>|</span><label class="collapse" for="c-38589768">[-]</label><label class="expand" for="c-38589768">[5 more]</label></div><br/><div class="children"><div class="content">Unless you use linux, where the quality of Nvidia support continues to decline.</div><br/><div id="38589811" class="c"><input type="checkbox" id="c-38589811" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38589747">root</a><span>|</span><a href="#38589768">parent</a><span>|</span><a href="#38589793">next</a><span>|</span><label class="collapse" for="c-38589811">[-]</label><label class="expand" for="c-38589811">[2 more]</label></div><br/><div class="children"><div class="content">Have there been issues with CUDA on Linux?<p>Having their cards run in servers is a big part of their business model, and Linux owns that market, so I’m surprised if their support is getting worse.<p>Things like poor Wayland support—sure. But then, why would somebody use this matrix-multiplication accelerator to draw graphics, right?</div><br/><div id="38589932" class="c"><input type="checkbox" id="c-38589932" checked=""/><div class="controls bullet"><span class="by">webnrrd2k</span><span>|</span><a href="#38589747">root</a><span>|</span><a href="#38589811">parent</a><span>|</span><a href="#38589793">next</a><span>|</span><label class="collapse" for="c-38589932">[-]</label><label class="expand" for="c-38589932">[1 more]</label></div><br/><div class="children"><div class="content">On my home system I&#x27;ve been running 2x RTX 2070&#x27;s on Fedora and have had serious enough problems. It&#x27;s been fairly stable for a while, but the last week or so I keep having the screen go black and not come back. I&#x27;m going to try Debian as it&#x27;s supposed to have better support for nvidia cards. I&#x27;ve been using Fedora or Redhat for a long time, and I&#x27;d rather not switch, but these driver issues make the system unusable.</div><br/></div></div></div></div><div id="38589793" class="c"><input type="checkbox" id="c-38589793" checked=""/><div class="controls bullet"><span class="by">coolspot</span><span>|</span><a href="#38589747">root</a><span>|</span><a href="#38589768">parent</a><span>|</span><a href="#38589811">prev</a><span>|</span><a href="#38590024">next</a><span>|</span><label class="collapse" for="c-38589793">[-]</label><label class="expand" for="c-38589793">[1 more]</label></div><br/><div class="children"><div class="content">I am running 3090 on Linux Mint and 2060 on Rocky (RHEL 9) without any issues. Both CUDA and regular desktop use.</div><br/></div></div><div id="38590024" class="c"><input type="checkbox" id="c-38590024" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38589747">root</a><span>|</span><a href="#38589768">parent</a><span>|</span><a href="#38589793">prev</a><span>|</span><a href="#38589759">next</a><span>|</span><label class="collapse" for="c-38590024">[-]</label><label class="expand" for="c-38590024">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  where the quality of Nvidia support continues to decline.<p>what are you talking about?</div><br/></div></div></div></div><div id="38589759" class="c"><input type="checkbox" id="c-38589759" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38589747">parent</a><span>|</span><a href="#38589768">prev</a><span>|</span><label class="collapse" for="c-38589759">[-]</label><label class="expand" for="c-38589759">[1 more]</label></div><br/><div class="children"><div class="content">Which GPUs should I consider?</div><br/></div></div></div></div></div></div></div></div></div></body></html>