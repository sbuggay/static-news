<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696410064615" as="style"/><link rel="stylesheet" href="styles.css?v=1696410064615"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2309.17453">StreamingLLM: Efficient streaming technique enable infinite sequence lengths</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>TheJCDenton</span> | <span>10 comments</span></div><br/><div><div id="37756896" class="c"><input type="checkbox" id="c-37756896" checked=""/><div class="controls bullet"><span class="by">TheJCDenton</span><span>|</span><a href="#37760958">next</a><span>|</span><label class="collapse" for="c-37756896">[-]</label><label class="expand" for="c-37756896">[1 more]</label></div><br/><div class="children"><div class="content">The demo [3] seems very promising.<p>&quot;Their method cleverly exploits the LLMs&#x27; tendency to use initial tokens as &quot;attention sinks&quot; to anchor the distribution of attention scores. By caching initial tokens alongside recent ones, StreamingLLM restored perplexity and achieved up to 22x faster decoding than prior techniques.&quot; [1]<p>&quot;We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.&quot; [2]<p>&quot;we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment.&quot; [2]<p>&quot;StreamingLLM achieves an impressive speedup, reaching up to 22.2× per token. Despite its reduced latency, StreamingLLM sustains a memory footprint consistent with the re-computation baseline.&quot; [2]<p>[1] <a href="https:&#x2F;&#x2F;notes.aimodels.fyi&#x2F;llm-infinite-context-window-streamingllm&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;notes.aimodels.fyi&#x2F;llm-infinite-context-window-strea...</a><p>[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.17453.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.17453.pdf</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;streaming-llm">https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;streaming-llm</a></div><br/></div></div><div id="37760958" class="c"><input type="checkbox" id="c-37760958" checked=""/><div class="controls bullet"><span class="by">kirill5pol</span><span>|</span><a href="#37756896">prev</a><span>|</span><a href="#37758771">next</a><span>|</span><label class="collapse" for="c-37760958">[-]</label><label class="expand" for="c-37760958">[1 more]</label></div><br/><div class="children"><div class="content">In figure 3, it shows that Falcon and Pythia are much less susceptible to the lack of an attention sink than Llama or MPT… seems they work almost as well by just doing naïve windowed attention</div><br/></div></div><div id="37758771" class="c"><input type="checkbox" id="c-37758771" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#37760958">prev</a><span>|</span><a href="#37756925">next</a><span>|</span><label class="collapse" for="c-37758771">[-]</label><label class="expand" for="c-37758771">[6 more]</label></div><br/><div class="children"><div class="content">Yeah &quot;by conveniently dropping token out of attention we can have infinite tokens&quot; not exactly a breakthrough</div><br/><div id="37760039" class="c"><input type="checkbox" id="c-37760039" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#37758771">parent</a><span>|</span><a href="#37758898">next</a><span>|</span><label class="collapse" for="c-37760039">[-]</label><label class="expand" for="c-37760039">[3 more]</label></div><br/><div class="children"><div class="content">The nuance is that their technique allows you to slide the context window along without introducing a jagged discontinuity in generation. At least that’s my feel for it.</div><br/><div id="37760072" class="c"><input type="checkbox" id="c-37760072" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#37758771">root</a><span>|</span><a href="#37760039">parent</a><span>|</span><a href="#37758898">next</a><span>|</span><label class="collapse" for="c-37760072">[-]</label><label class="expand" for="c-37760072">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but is that really why people want the context window to be &quot;infinite&quot;? In my experience, the desire for bigger context is the ability to say, dump an 1000 page book into an LLM and ask questions about any part of it, not just the last chapter.</div><br/><div id="37760829" class="c"><input type="checkbox" id="c-37760829" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#37758771">root</a><span>|</span><a href="#37760072">parent</a><span>|</span><a href="#37758898">next</a><span>|</span><label class="collapse" for="c-37760829">[-]</label><label class="expand" for="c-37760829">[1 more]</label></div><br/><div class="children"><div class="content">I think the title is misleading. This is an MIT PR piece.</div><br/></div></div></div></div></div></div><div id="37758898" class="c"><input type="checkbox" id="c-37758898" checked=""/><div class="controls bullet"><span class="by">ogogmad</span><span>|</span><a href="#37758771">parent</a><span>|</span><a href="#37760039">prev</a><span>|</span><a href="#37756925">next</a><span>|</span><label class="collapse" for="c-37758898">[-]</label><label class="expand" for="c-37758898">[2 more]</label></div><br/><div class="children"><div class="content">Why do you think it&#x27;s only that? I&#x27;ve seen a lot of dismissive comments on AI articles that were shown to misunderstand or underappreciate the article.</div><br/><div id="37758965" class="c"><input type="checkbox" id="c-37758965" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#37758771">root</a><span>|</span><a href="#37758898">parent</a><span>|</span><a href="#37756925">next</a><span>|</span><label class="collapse" for="c-37758965">[-]</label><label class="expand" for="c-37758965">[1 more]</label></div><br/><div class="children"><div class="content">Why do you claim I &quot;think it&#x27;s only that&quot;?<p>The other part of the paper handling the sliding KV cache compares favourably with prefix caching, sure, but then we moved away prefix caching for serving since a while now, with paged attention (which should really have been called paged KV cache but oh well) offering a lot of interesting improvement in that area including supporting extremely well parallel decoding.<p>And I do not care enough to compare the streaming cache with the paged attention cache directly, first because it&#x27;s work they should have done and not I, second because dropping token silently is something that confuses and frustrated users significantly enough that it puts me down from wanting to investigate further.</div><br/></div></div></div></div></div></div><div id="37756925" class="c"><input type="checkbox" id="c-37756925" checked=""/><div class="controls bullet"><span class="by">firebaze</span><span>|</span><a href="#37758771">prev</a><span>|</span><label class="collapse" for="c-37756925">[-]</label><label class="expand" for="c-37756925">[1 more]</label></div><br/><div class="children"><div class="content">See also <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.17453" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.17453</a>, or <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37740932">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37740932</a></div><br/></div></div></div></div></div></div></div></body></html>