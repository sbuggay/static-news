<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719046861062" as="style"/><link rel="stylesheet" href="styles.css?v=1719046861062"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://zachartrand.github.io/SoME-3-Living/">How does a computer/calculator compute logarithms?</a> <span class="domain">(<a href="https://zachartrand.github.io">zachartrand.github.io</a>)</span></div><div class="subtext"><span>ykonstant</span> | <span>59 comments</span></div><br/><div><div id="40750946" class="c"><input type="checkbox" id="c-40750946" checked=""/><div class="controls bullet"><span class="by">pavpanchekha</span><span>|</span><a href="#40751094">next</a><span>|</span><label class="collapse" for="c-40750946">[-]</label><label class="expand" for="c-40750946">[11 more]</label></div><br/><div class="children"><div class="content">This is a well done post, but please note that computers do not in fact use Taylor series for evaluating polynomials. They do use polynomials! (Or, sometimes, rational functions, but I think the recent trend has been toward polynomials only.) They also use transformations like the log(1+x) - log(1-x) one described in this article. (That one is used in fdlibm, a very influential math library, though more modern libraries seem to not use it.)<p>But the polynomials themselves are usually generated through some other technique, especially the Remez algorithm (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Remez_algorithm" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Remez_algorithm</a>) and modern improvements to it like Sollya&#x27;s (<a href="https:&#x2F;&#x2F;sollya.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;sollya.org&#x2F;</a>) LLL-based floating-point polynomial fitting. It&#x27;s still an active area of research; the RLibM project (<a href="https:&#x2F;&#x2F;people.cs.rutgers.edu&#x2F;~sn349&#x2F;rlibm&#x2F;" rel="nofollow">https:&#x2F;&#x2F;people.cs.rutgers.edu&#x2F;~sn349&#x2F;rlibm&#x2F;</a>) from Rutgers has introduced a totally new way to fit low-precision polynomials (say, up to 32 bits) using massive linear programming problems (roughly, 4 constraints on 10 variables).<p>Source: am researcher in this field.</div><br/><div id="40752121" class="c"><input type="checkbox" id="c-40752121" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#40750946">parent</a><span>|</span><a href="#40753320">next</a><span>|</span><label class="collapse" for="c-40752121">[-]</label><label class="expand" for="c-40752121">[2 more]</label></div><br/><div class="children"><div class="content">Hi Pavel.  For people who don&#x27;t know, this is the person whose lab produced Herbie (<a href="https:&#x2F;&#x2F;herbie.uwplse.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;herbie.uwplse.org&#x2F;</a>).<p>Tacking on to this, I have seen rational approximants in &quot;fast approximate&quot; math libraries, while the ones targeting the highest numerical accuracy are often still using polynomials.  Setting up a solver for 0.5 ULP when you have a rational function is definitely a lot harder.  In my own work, it also empirically seems that the division creates some problems for precision of the last bit, so your polynomials are longer than you might expect.<p>One interesting approach for bit-accurate rational approximants is to have a rational approximant get most of the way there while fitting the error of your rational approximant to a polynomial, but I don&#x27;t think there&#x27;s a good way to figure out where the computationally-efficient split of the problem is (ie how big to make the rational side and the polynomial side).</div><br/><div id="40753096" class="c"><input type="checkbox" id="c-40753096" checked=""/><div class="controls bullet"><span class="by">pavpanchekha</span><span>|</span><a href="#40750946">root</a><span>|</span><a href="#40752121">parent</a><span>|</span><a href="#40753320">next</a><span>|</span><label class="collapse" for="c-40753096">[-]</label><label class="expand" for="c-40753096">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, you&#x27;re right on all of that. I think there have been recent breakthroughs on quadratic (I think at ARITH 23? Yep, just looked, it&#x27;s the best paper: <a href="https:&#x2F;&#x2F;arith2023.arithsymposium.org&#x2F;program.html" rel="nofollow">https:&#x2F;&#x2F;arith2023.arithsymposium.org&#x2F;program.html</a>) so hopefully this will become more accessible with time. Though OTOH RLibM-style techniques can get you <i>very</i> short polynomials, to the point that it&#x27;s hard to imagine beating them by much given the high cost of division operations (like 3x a multiplication).</div><br/></div></div></div></div><div id="40753320" class="c"><input type="checkbox" id="c-40753320" checked=""/><div class="controls bullet"><span class="by">Sesse__</span><span>|</span><a href="#40750946">parent</a><span>|</span><a href="#40752121">prev</a><span>|</span><a href="#40752888">next</a><span>|</span><label class="collapse" for="c-40753320">[-]</label><label class="expand" for="c-40753320">[4 more]</label></div><br/><div class="children"><div class="content">They used to, though! The AMD K5 paper is a classic, and it describes how they used Taylor series because they could compute them on-the-fly (they had limited ROM space). <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;3612479_The_K5_transcendental_functions" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;3612479_The_K5_tran...</a></div><br/><div id="40753683" class="c"><input type="checkbox" id="c-40753683" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#40750946">root</a><span>|</span><a href="#40753320">parent</a><span>|</span><a href="#40752888">next</a><span>|</span><label class="collapse" for="c-40753683">[-]</label><label class="expand" for="c-40753683">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Used to&quot; is sort of an overstatement, IMO.  The use of Taylor series here is remarkable because they need to generate coefficients on the fly.  Chebyshev and Remez have been around for quite a long time, since the early 1930&#x27;s, and people doing numerics in a serious fashion have generally used them for polynomial approximation since the birth of computers (except in unique circumstances like the paper you cited).<p>The new engineering that has come recently with Sollya and similar solvers is explicit design around floating point operations.  Chebyshev and Remez use the real numbers, but Sollya uses floats in its minimax function, and the coefficients you get are actually somewhat different due to rounding.  Fast LP solvers have also enabled this approach more generally.</div><br/><div id="40753720" class="c"><input type="checkbox" id="c-40753720" checked=""/><div class="controls bullet"><span class="by">Sesse__</span><span>|</span><a href="#40750946">root</a><span>|</span><a href="#40753683">parent</a><span>|</span><a href="#40752888">next</a><span>|</span><label class="collapse" for="c-40753720">[-]</label><label class="expand" for="c-40753720">[2 more]</label></div><br/><div class="children"><div class="content">It should be said that Sollya doesn&#x27;t _really_ use floats. It restricts its coefficients to rationals that are (mostly) representable by floats, but the minimax is still run in full precision. Which means you can often beat it by brute force or similar.</div><br/><div id="40753831" class="c"><input type="checkbox" id="c-40753831" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#40750946">root</a><span>|</span><a href="#40753720">parent</a><span>|</span><a href="#40752888">next</a><span>|</span><label class="collapse" for="c-40753831">[-]</label><label class="expand" for="c-40753831">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that is a good clarification - it uses wider precision internally, but it takes floating point restrictions and operations into account.  If you otherwise used Remez, you would have to just quantize the coefficients blindly and then tweak (probably manually) if something was off.<p>Shamelessly plugging, you can sort of see the old-school process here (using integers and mixed-precision fixed point is harder with Sollya): <a href="https:&#x2F;&#x2F;specbranch.com&#x2F;posts&#x2F;faster-div8&#x2F;" rel="nofollow">https:&#x2F;&#x2F;specbranch.com&#x2F;posts&#x2F;faster-div8&#x2F;</a><p>The results of a follow-up quantization process that was programmatic using an LP solver to narrow the operands went into a paper I wrote for ARITH last year: <a href="https:&#x2F;&#x2F;arith2023.arithsymposium.org&#x2F;papers&#x2F;Newton-Raphson%20Integer%20Division%20for%20Area-Constrained%20Microcontrollers.pdf" rel="nofollow">https:&#x2F;&#x2F;arith2023.arithsymposium.org&#x2F;papers&#x2F;Newton-Raphson%2...</a><p>The coefficients and constants in both cases are substantially higher than what you get from Remez, to allow for truncation.  When you have quantized operands, they are quite a bit higher.  The same goes for approximations generated by Sollya - the optimal coefficients are relatively far from what Remez would tell you to do because of the error you get from rounding.</div><br/></div></div></div></div></div></div></div></div><div id="40752888" class="c"><input type="checkbox" id="c-40752888" checked=""/><div class="controls bullet"><span class="by">tech_ken</span><span>|</span><a href="#40750946">parent</a><span>|</span><a href="#40753320">prev</a><span>|</span><a href="#40754858">next</a><span>|</span><label class="collapse" for="c-40752888">[-]</label><label class="expand" for="c-40752888">[1 more]</label></div><br/><div class="children"><div class="content">&gt; computers do not in fact use Taylor series for evaluating polynomials.<p>I mean I get what you&#x27;re saying, but the smartass in me observes that since any polynomial is it&#x27;s own Taylor expansion...</div><br/></div></div><div id="40754858" class="c"><input type="checkbox" id="c-40754858" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#40750946">parent</a><span>|</span><a href="#40752888">prev</a><span>|</span><a href="#40751094">next</a><span>|</span><label class="collapse" for="c-40754858">[-]</label><label class="expand" for="c-40754858">[3 more]</label></div><br/><div class="children"><div class="content">Do you think we&#x27;ll ever get to a point where there is tooling to generate implementations for user specified functions?  E.g.  say I want log2(x!)  (Picked that example because the size of the intermediate value blocks computing it naively, and because it&#x27;s a function I&#x27;ve had to implement before).</div><br/><div id="40756405" class="c"><input type="checkbox" id="c-40756405" checked=""/><div class="controls bullet"><span class="by">thayne</span><span>|</span><a href="#40750946">root</a><span>|</span><a href="#40754858">parent</a><span>|</span><a href="#40751094">next</a><span>|</span><label class="collapse" for="c-40756405">[-]</label><label class="expand" for="c-40756405">[2 more]</label></div><br/><div class="children"><div class="content">Technically, that isn&#x27;t a continuous function, so  I don&#x27;t think the Remez algorithm and similar would work directly.<p>But if you replace the factorial with the gamma function, you get a continuous function that is equivalent for positive integers. And you might be able to get something from that. But there might be something more efficient that can take advantage of the fact that the range of your function is positive integers.<p>If you just need something that doesn&#x27;t overflow, you can take advantage of the properties of logarithms to compute it as the sum of log2(n) for all n from 1 to x.</div><br/><div id="40756904" class="c"><input type="checkbox" id="c-40756904" checked=""/><div class="controls bullet"><span class="by">thayne</span><span>|</span><a href="#40750946">root</a><span>|</span><a href="#40756405">parent</a><span>|</span><a href="#40751094">next</a><span>|</span><label class="collapse" for="c-40756904">[-]</label><label class="expand" for="c-40756904">[1 more]</label></div><br/><div class="children"><div class="content">And if you want an integer result, log2 can be quickly computed using the number of leading zeros.</div><br/></div></div></div></div></div></div></div></div><div id="40751094" class="c"><input type="checkbox" id="c-40751094" checked=""/><div class="controls bullet"><span class="by">tasty_freeze</span><span>|</span><a href="#40750946">prev</a><span>|</span><a href="#40753539">next</a><span>|</span><label class="collapse" for="c-40751094">[-]</label><label class="expand" for="c-40751094">[4 more]</label></div><br/><div class="children"><div class="content">Wang Labs made an amazing (for its time) calculator, the LOCI-1, quickly replaced by the much more capable LOCI-2. They were from 1964&#x2F;1965 and were built with core memory and discrete transistors, 1200 of them, no ICs.<p>It operated with 10 digits of precision, and its native hardware could add and subtract. Rather than doing multiplication and division and square root directly, instead the calculator was able to compute logs and antilogs via successive addition and subtraction. Multiplying two numbers might return 199.9999999 instead of 200.0, but since this was built for engineers who were mathematically literate, it was acceptable.<p>There were other calculators that could be programmed to compute logs and antilogs, but they were slow. On the LOCI machines, the results were instant (on a human timescale).<p>Description of the LOCI-2:<p><a href="https:&#x2F;&#x2F;www.oldcalculatormuseum.com&#x2F;wangloci.html" rel="nofollow">https:&#x2F;&#x2F;www.oldcalculatormuseum.com&#x2F;wangloci.html</a><p>Just found this description of the log algorithm -- it used only six constants in its iterative add&#x2F;subtract&#x2F;shift algorithm to compute logs and antilogs:<p><a href="https:&#x2F;&#x2F;osgalleries.org&#x2F;journal&#x2F;pdf_files&#x2F;20.2&#x2F;V20.2P51.pdf" rel="nofollow">https:&#x2F;&#x2F;osgalleries.org&#x2F;journal&#x2F;pdf_files&#x2F;20.2&#x2F;V20.2P51.pdf</a></div><br/><div id="40751151" class="c"><input type="checkbox" id="c-40751151" checked=""/><div class="controls bullet"><span class="by">martin293</span><span>|</span><a href="#40751094">parent</a><span>|</span><a href="#40753539">next</a><span>|</span><label class="collapse" for="c-40751151">[-]</label><label class="expand" for="c-40751151">[3 more]</label></div><br/><div class="children"><div class="content">&gt; antilog<p>So exp? e to the power of x?</div><br/><div id="40751983" class="c"><input type="checkbox" id="c-40751983" checked=""/><div class="controls bullet"><span class="by">anamexis</span><span>|</span><a href="#40751094">root</a><span>|</span><a href="#40751151">parent</a><span>|</span><a href="#40753539">next</a><span>|</span><label class="collapse" for="c-40751983">[-]</label><label class="expand" for="c-40751983">[2 more]</label></div><br/><div class="children"><div class="content">Yes. I think antilog name used to be more common in engineering, and commonly referred to 10^x rather than e^x. Some calculators even had a log^-1 button.</div><br/><div id="40753008" class="c"><input type="checkbox" id="c-40753008" checked=""/><div class="controls bullet"><span class="by">eesmith</span><span>|</span><a href="#40751094">root</a><span>|</span><a href="#40751983">parent</a><span>|</span><a href="#40753539">next</a><span>|</span><label class="collapse" for="c-40753008">[-]</label><label class="expand" for="c-40753008">[1 more]</label></div><br/><div class="children"><div class="content">Neat Google Ngram for it at <a href="https:&#x2F;&#x2F;books.google.com&#x2F;ngrams&#x2F;graph?content=antilog&amp;year_start=1800&amp;year_end=2019&amp;corpus=en-2019&amp;smoothing=3" rel="nofollow">https:&#x2F;&#x2F;books.google.com&#x2F;ngrams&#x2F;graph?content=antilog&amp;year_s...</a> showing a peak for &quot;antilog&quot; in 1976, decreasing now to prewar usage of 25% that of peak.</div><br/></div></div></div></div></div></div></div></div><div id="40753539" class="c"><input type="checkbox" id="c-40753539" checked=""/><div class="controls bullet"><span class="by">xg15</span><span>|</span><a href="#40751094">prev</a><span>|</span><a href="#40754888">next</a><span>|</span><label class="collapse" for="c-40753539">[-]</label><label class="expand" for="c-40753539">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>While these equations of polynomials contain a finite number of terms, we can have polynomials with an infinite number of terms. These are called series, and one of the simplest types of series is called a geometric series.</i><p>Slightly OT, but what made infinite sums a lot more sane to me was the understanding that the sums are in fact <i>not</i> infinite - it&#x27;s just syntactic sugar for a perfectly ordinary limit over a series of finite sums, each with one more term than the last.<p>E.g.,<p><pre><code>  sum(1 &#x2F; 2^n) for n from 1 to +infinity 
</code></pre>
&quot;really&quot; means:<p><pre><code>  lim(sum(1 &#x2F; 2^n) for n from 1 to m) for m -&gt; +infinity 
</code></pre>
So no infinite loops of additions are actually involved.</div><br/><div id="40755135" class="c"><input type="checkbox" id="c-40755135" checked=""/><div class="controls bullet"><span class="by">arvindh-manian</span><span>|</span><a href="#40753539">parent</a><span>|</span><a href="#40754888">next</a><span>|</span><label class="collapse" for="c-40755135">[-]</label><label class="expand" for="c-40755135">[2 more]</label></div><br/><div class="children"><div class="content">Yes, great point. In fact, you don&#x27;t even have to have exactly one more term than the last (assuming the limit converges).</div><br/><div id="40756801" class="c"><input type="checkbox" id="c-40756801" checked=""/><div class="controls bullet"><span class="by">xg15</span><span>|</span><a href="#40753539">root</a><span>|</span><a href="#40755135">parent</a><span>|</span><a href="#40754888">next</a><span>|</span><label class="collapse" for="c-40756801">[-]</label><label class="expand" for="c-40756801">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes, that&#x27;s true of course.</div><br/></div></div></div></div></div></div><div id="40754888" class="c"><input type="checkbox" id="c-40754888" checked=""/><div class="controls bullet"><span class="by">bluenose69</span><span>|</span><a href="#40753539">prev</a><span>|</span><a href="#40750940">next</a><span>|</span><label class="collapse" for="c-40754888">[-]</label><label class="expand" for="c-40754888">[1 more]</label></div><br/><div class="children"><div class="content">For anyone who remembers the Sinclair Scientific calculator, the site <a href="http:&#x2F;&#x2F;files.righto.com&#x2F;calculator&#x2F;sinclair_scientific_simulator.html" rel="nofollow">http:&#x2F;&#x2F;files.righto.com&#x2F;calculator&#x2F;sinclair_scientific_simul...</a> has a good explanation of its way of calculating.  It is unlike other machines, overcoming serious electronic limitations with some simple but clever approximate methods.<p>This little machine was my first calculator, a present from my Dad. Man, I loved that thing. It was so small that I could easily hold it, and strike the keys, with one hand.  That single-hand aspect proved helpful in my undergraduate physics and chemistry labs.  The RPN methodology also made it easy to work through complicated formulae without getting lost in parentheses.<p>Over time, the keys stopped working well.  And then stopped working at all.  I kept it for a year or two, but then got rid of it, having moved on to another calculator that was more powerful but much less fun.  I don&#x27;t even remember the brand.<p>Looking back four decades, I wish I had kept the Sinclair Scientific, for a memento. But I did not appreciate that a dead little machine like that, in its thin plastic case, would bring back so many fond memories.  There&#x27;s a life lesson in that.</div><br/></div></div><div id="40750940" class="c"><input type="checkbox" id="c-40750940" checked=""/><div class="controls bullet"><span class="by">owlbite</span><span>|</span><a href="#40754888">prev</a><span>|</span><a href="#40752095">next</a><span>|</span><label class="collapse" for="c-40750940">[-]</label><label class="expand" for="c-40750940">[1 more]</label></div><br/><div class="children"><div class="content">If I remember correctly, real libm implementations rarely use an analytically derived approximation, they just fit a polynomial rational approximation to minimize the error on the required range and precision.</div><br/></div></div><div id="40752095" class="c"><input type="checkbox" id="c-40752095" checked=""/><div class="controls bullet"><span class="by">r00tanon</span><span>|</span><a href="#40750940">prev</a><span>|</span><a href="#40753067">next</a><span>|</span><label class="collapse" for="c-40752095">[-]</label><label class="expand" for="c-40752095">[1 more]</label></div><br/><div class="children"><div class="content">Good article. I, too, am fascinated by calculators and how they work, especially the HP scientific RPN calculators. So much so, that I ended up developing this:<p><a href="https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.limpidfox.hp67_2&amp;hl=en_US&amp;pli=1">https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.limpidfox....</a></div><br/></div></div><div id="40753067" class="c"><input type="checkbox" id="c-40753067" checked=""/><div class="controls bullet"><span class="by">agumonkey</span><span>|</span><a href="#40752095">prev</a><span>|</span><a href="#40751504">next</a><span>|</span><label class="collapse" for="c-40753067">[-]</label><label class="expand" for="c-40753067">[1 more]</label></div><br/><div class="children"><div class="content">Is there a recursive computation for logarithms just like subtraction or division ?</div><br/></div></div><div id="40751504" class="c"><input type="checkbox" id="c-40751504" checked=""/><div class="controls bullet"><span class="by">Thoreandan</span><span>|</span><a href="#40753067">prev</a><span>|</span><a href="#40750351">next</a><span>|</span><label class="collapse" for="c-40751504">[-]</label><label class="expand" for="c-40751504">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CORDIC" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CORDIC</a> used a lot?</div><br/></div></div><div id="40750351" class="c"><input type="checkbox" id="c-40750351" checked=""/><div class="controls bullet"><span class="by">082349872349872</span><span>|</span><a href="#40751504">prev</a><span>|</span><a href="#40750523">next</a><span>|</span><label class="collapse" for="c-40750351">[-]</label><label class="expand" for="c-40750351">[1 more]</label></div><br/><div class="children"><div class="content">see also <a href="https:&#x2F;&#x2F;www.feynmanlectures.caltech.edu&#x2F;I_22.html#footnote_source_1" rel="nofollow">https:&#x2F;&#x2F;www.feynmanlectures.caltech.edu&#x2F;I_22.html#footnote_s...</a></div><br/></div></div><div id="40750523" class="c"><input type="checkbox" id="c-40750523" checked=""/><div class="controls bullet"><span class="by">myth2018</span><span>|</span><a href="#40750351">prev</a><span>|</span><a href="#40750311">next</a><span>|</span><label class="collapse" for="c-40750523">[-]</label><label class="expand" for="c-40750523">[15 more]</label></div><br/><div class="children"><div class="content">Btw: I&#x27;m looking for efficient ways to represent numbers of arbitrary precision, and performing mathematical operations on the them. It&#x27;s for a toy financial calculator I&#x27;m writing in my spare time. My knowledge is really lacking in this field and I&#x27;m not even sure about what I should be asking&#x2F;searching for. Could someone suggest me some sources? Preferably online, but books would also be very welcome.</div><br/><div id="40750898" class="c"><input type="checkbox" id="c-40750898" checked=""/><div class="controls bullet"><span class="by">pavpanchekha</span><span>|</span><a href="#40750523">parent</a><span>|</span><a href="#40751940">next</a><span>|</span><label class="collapse" for="c-40750898">[-]</label><label class="expand" for="c-40750898">[8 more]</label></div><br/><div class="children"><div class="content">The typical library folks use is called MPFR: <a href="https:&#x2F;&#x2F;mpfr.org" rel="nofollow">https:&#x2F;&#x2F;mpfr.org</a><p>It&#x27;s fast, high quality, and very complete.</div><br/><div id="40750958" class="c"><input type="checkbox" id="c-40750958" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40750523">root</a><span>|</span><a href="#40750898">parent</a><span>|</span><a href="#40750983">next</a><span>|</span><label class="collapse" for="c-40750958">[-]</label><label class="expand" for="c-40750958">[5 more]</label></div><br/><div class="children"><div class="content">It might be also important to note that mpfr is really for scientific purposes that need to deal with such large precision - there are significant memory and performance costs. For financial accounting applications, a fixed precision floating point library (aka fixed point arithmetic) is probably a better choice. Also mpfr is c&#x2F;c++ but the author didn’t really specify their language and there may be other better options although bindings likely exist.</div><br/><div id="40752384" class="c"><input type="checkbox" id="c-40752384" checked=""/><div class="controls bullet"><span class="by">denton-scratch</span><span>|</span><a href="#40750523">root</a><span>|</span><a href="#40750958">parent</a><span>|</span><a href="#40750983">next</a><span>|</span><label class="collapse" for="c-40752384">[-]</label><label class="expand" for="c-40752384">[4 more]</label></div><br/><div class="children"><div class="content">I once wrote some accounting software for a tiny team in an investment bank. The bosses decided that we should use our brand-new platform, which was cheap to develop for, because most (normal) things could be done without code.<p>All went well for a couple of months; then the customer reported that the system failed for currency conversions to and from Yen. There wasn&#x27;t enough precision. We had to write an arbitrary-precision maths library, resulting in the project overrunning budget by a factor of three.</div><br/><div id="40752556" class="c"><input type="checkbox" id="c-40752556" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40750523">root</a><span>|</span><a href="#40752384">parent</a><span>|</span><a href="#40750983">next</a><span>|</span><label class="collapse" for="c-40752556">[-]</label><label class="expand" for="c-40752556">[3 more]</label></div><br/><div class="children"><div class="content">There’s a world of difference between an arbitrary precision library like mpfr suited for financial applications, a fixed precision “Decimal” type that’s better suited for financial applications, and trying to roll your own thing.<p>I don’t believe anywhere that I wrote that you should roll your own. All I said is that for financial stuff “Decimal”-like APIs are going to be easier to configure, manage and be better suited for financial applications.</div><br/><div id="40752700" class="c"><input type="checkbox" id="c-40752700" checked=""/><div class="controls bullet"><span class="by">denton-scratch</span><span>|</span><a href="#40750523">root</a><span>|</span><a href="#40752556">parent</a><span>|</span><a href="#40750983">next</a><span>|</span><label class="collapse" for="c-40752700">[-]</label><label class="expand" for="c-40752700">[2 more]</label></div><br/><div class="children"><div class="content">Sure. This was the early eighties; we didn&#x27;t really have &quot;libraries&quot;, and even if we had, there was no way to plug them into this proprietary development system.<p>&gt; library like mpfr suited for financial applications, a fixed precision “Decimal” type that’s better suited for financial applications<p>I don&#x27;t know mpfr, but my guess is you meant to say that mpfr is better-suited for scientific applications?</div><br/><div id="40754152" class="c"><input type="checkbox" id="c-40754152" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40750523">root</a><span>|</span><a href="#40752700">parent</a><span>|</span><a href="#40750983">next</a><span>|</span><label class="collapse" for="c-40754152">[-]</label><label class="expand" for="c-40754152">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, arbitrary precision is different from fixed point precision that financial institutions typically need. MPFR is arbitrary precision so if you’re doing stuff involving our solar system and beyond, you probably want to use mpfr and be very careful in setting up the equations you’re using. Financial transactions have different requirements though and the arbitrary precision is unnecessarily slow, uses unnecessary amounts of RAM, and I believe will probably difficult to configure correctly. That being said, I’ve never really used mpfr. More just relaying what I understand to be the lay of the land.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40750983" class="c"><input type="checkbox" id="c-40750983" checked=""/><div class="controls bullet"><span class="by">myth2018</span><span>|</span><a href="#40750523">root</a><span>|</span><a href="#40750898">parent</a><span>|</span><a href="#40750958">prev</a><span>|</span><a href="#40751940">next</a><span>|</span><label class="collapse" for="c-40750983">[-]</label><label class="expand" for="c-40750983">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the suggestion.<p>And do you know where I could read about the underlying theory? I&#x27;d like to implement the most I could from scratch, for learning purposes.</div><br/><div id="40752184" class="c"><input type="checkbox" id="c-40752184" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#40750523">root</a><span>|</span><a href="#40750983">parent</a><span>|</span><a href="#40751940">next</a><span>|</span><label class="collapse" for="c-40752184">[-]</label><label class="expand" for="c-40752184">[1 more]</label></div><br/><div class="children"><div class="content">The underlying theory is primary school math if you stay in a decimal fixed point format.<p>All you have to do is choose the number of decimals you want. Say, half the number of digits of the biggest number that fits your desired size. Half these digits will be for the integral part, the other half for the decimal part. For instance, if you want to store signed fixed point on 32 bits, the highest number in base 10 is 2 billion. That&#x27;s 9 zeros for you. Say you pick 4 for the decimal part, just multiply your numbers by 10000.<p>So 1 is 10000. You can use all the regular operators you want on these numbers, just remember to x10000 when storing them, and &#x2F;10000 when extracting from them. You&#x27;re really just &quot;computing on tenths of thousands&quot; instead of &quot;computing on ones&quot;.<p>This is obviously wasteful to do that in a decimal base, but it&#x27;s also trivial, so unless you cannot afford to lose that many digits, stick with it.<p>When confortable, consider moving to a binary base so that you don&#x27;t waste as much digits.<p>Note that it is not _stricly_ mandatory to use fixed point numbers when doing financial computations. Floating point can work just fine, it all depends on what kind of number and computation you have to deal with.<p>Typically, the need for fixed point arises when<p>1) You work with really small and large numbers together, these are the most susceptible to be misrepresented as floating points. The best example is if you need to deal with exchange rates to convert between currencies that have widely different bases.<p>2) You need to chain a lot of operations, which could compound imprécision.<p>3) You need to store exact numbers of arbitrary precision (e.g. market data)</div><br/></div></div></div></div></div></div><div id="40751940" class="c"><input type="checkbox" id="c-40751940" checked=""/><div class="controls bullet"><span class="by">constantcrying</span><span>|</span><a href="#40750523">parent</a><span>|</span><a href="#40750898">prev</a><span>|</span><a href="#40752463">next</a><span>|</span><label class="collapse" for="c-40751940">[-]</label><label class="expand" for="c-40751940">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Btw: I&#x27;m looking for efficient ways to represent numbers of arbitrary precision, and performing mathematical operations on the them<p>The easiest way is to look at how fixed length numbers are implemented and extend the ranges of values to arbitrary length.<p>For financial operations it seems likely that you want fixed point arithmetic, which essentially just means you calculate in integers of cents. So you just need to implement arbitrary length integers, which isn&#x27;t particularly hard I would say. The algorithms you need are the ones you already know from school. Long division and multiplication. (There are better ones though).</div><br/></div></div><div id="40752463" class="c"><input type="checkbox" id="c-40752463" checked=""/><div class="controls bullet"><span class="by">denton-scratch</span><span>|</span><a href="#40750523">parent</a><span>|</span><a href="#40751940">prev</a><span>|</span><a href="#40751179">next</a><span>|</span><label class="collapse" for="c-40752463">[-]</label><label class="expand" for="c-40752463">[1 more]</label></div><br/><div class="children"><div class="content">&gt; efficient ways to represent numbers of arbitrary precision<p>I used to write COBOL for Burroughs machines, which had hardware BCD arithmetic. It wasn&#x27;t &quot;arbitrary precision&quot; in the sense that you could input an arbitrary number and get an exact result; you had to allocate storage for each intermediate result, so your program had to be designed to handle some specific amount of precision.<p>I&#x27;m sure some Intel microprocessors had BCD hardware; maybe they still do.</div><br/></div></div><div id="40751179" class="c"><input type="checkbox" id="c-40751179" checked=""/><div class="controls bullet"><span class="by">physicsguy</span><span>|</span><a href="#40750523">parent</a><span>|</span><a href="#40752463">prev</a><span>|</span><a href="#40752179">next</a><span>|</span><label class="collapse" for="c-40751179">[-]</label><label class="expand" for="c-40751179">[1 more]</label></div><br/><div class="children"><div class="content">For finances, numbers are typically stored as integers, since floating point problems can crop up quite easily if you don&#x27;t. Some languages can handle arbitrary precision integers out of the box - this is true in Python for e.g., in others then you have to use special libraries to do so. Even when you can represent the numbers, you still have to take care since doing things like calculating interest might lead to floating point numbers which need to be handled.</div><br/></div></div><div id="40752179" class="c"><input type="checkbox" id="c-40752179" checked=""/><div class="controls bullet"><span class="by">tzs</span><span>|</span><a href="#40750523">parent</a><span>|</span><a href="#40751179">prev</a><span>|</span><a href="#40750632">next</a><span>|</span><label class="collapse" for="c-40752179">[-]</label><label class="expand" for="c-40752179">[1 more]</label></div><br/><div class="children"><div class="content">For implementing it yourself from scratch for learning it is a good idea to first step back and ask how much precision do you need and how fast does it need to be. Also are we talking about just integers, or do you also want fixed point and&#x2F;or floating point?<p>If you don&#x27;t need really huge numbers and you don&#x27;t need it to be very fast and just need the add, subtract, multiply, and divide (with remainder) on integers there&#x27;s a decent chance you can write something by just implementing the same algorithms you would use to do it by hand on paper. You<p>To make is easy to debug you could represent an arbitrary integer as an array of base 10 digits, least significant digit first. E.g., the number 123456 would be represented as [6, 5, 4, 3, 2, 1].<p>Addition and subtraction are pretty straightforward in that representation. Multiplication is more complicated by just follow what you do when you hand multiply and you&#x27;ll get it. Division is harder but for a project you are doing for learning purposes it is doable.<p>Once you&#x27;ve got those operations you can improve them by changing to a higher base. For example you might switch to base 1000. Then 123456 would be [456, 123]. Assuming the base is small enough that each digit fits in one machine word addition and subtraction are O(n) and multiplication and division as described above are O(n^2) where n is the number of digits in your operands (let&#x27;s assume equal sized operands).<p>Switching from base 10 to base 1000 cuts the number of digits in your numbers by a factor of 3, speeding up addition and subtraction by a factor of 3 and multiplication and division by a factor of 9.<p>It usually is easy to go up to base whatever fits in half a machine word. For example on a 32-bit processor that would be base 65536. Digits then range from 0 through 65535 and and adding or multiplying a pair of digits will not overflow.<p>You can also go up to the full machine word for your base but that can be more work. So for a 32-bit machine that would be base 4294967295. You then have to deal with overflow which adds a small amount of extra logic for addition&#x2F;subtraction. For multiplication you need a 32-bit x 32-bit =&gt; 64-bit operation. Many 32-bit processors do have that but it might not be exposed in whatever language you are using.<p>The above is all stuff you can probably do without any more theory than you can remember from elementary school. Taking it farther you probably want to do some reading. You can get a decent speedup on multiplication by using the Karatsuba algorithm, which speeds up multiplication from O(n^2) to O(n^1.58). Karatsuba is pretty easy and Wikipedia explains it adequately.<p>An extensive treatment of all of this is in Knuth volume 2. He covers everything from the &quot;do it like you do by hand&quot; up to whatever was state of the art at the time the book was written. 
I wouldn&#x27;t necessarily say you should go out and buy Knuth volume 2, but if you can borrow it from a library or someone you know take a look.</div><br/></div></div><div id="40750632" class="c"><input type="checkbox" id="c-40750632" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40750523">parent</a><span>|</span><a href="#40752179">prev</a><span>|</span><a href="#40750621">next</a><span>|</span><label class="collapse" for="c-40750632">[-]</label><label class="expand" for="c-40750632">[1 more]</label></div><br/><div class="children"><div class="content">You can check how python does it, and also ask chatgpt - I found it to be very helpful in cases like these.</div><br/></div></div></div></div><div id="40750311" class="c"><input type="checkbox" id="c-40750311" checked=""/><div class="controls bullet"><span class="by">constantcrying</span><span>|</span><a href="#40750523">prev</a><span>|</span><a href="#40750455">next</a><span>|</span><label class="collapse" for="c-40750311">[-]</label><label class="expand" for="c-40750311">[8 more]</label></div><br/><div class="children"><div class="content">&gt;for if our calculators and computers calculated logarithms inaccurately, as well as exponentials, trig functions, and square roots, to name but a few, a lot of scientific and engineering work would be broken and end in catastrophe.<p>I think I have some news for the writer of the article.</div><br/><div id="40751465" class="c"><input type="checkbox" id="c-40751465" checked=""/><div class="controls bullet"><span class="by">kps</span><span>|</span><a href="#40750311">parent</a><span>|</span><a href="#40750383">next</a><span>|</span><label class="collapse" for="c-40751465">[-]</label><label class="expand" for="c-40751465">[1 more]</label></div><br/><div class="children"><div class="content">“It would be better to never make a dime of profit than to have a product out there with a problem.”<p>(Today, the generator attached to David Packard powers half of California.)</div><br/></div></div><div id="40750383" class="c"><input type="checkbox" id="c-40750383" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40750311">parent</a><span>|</span><a href="#40751465">prev</a><span>|</span><a href="#40750619">next</a><span>|</span><label class="collapse" for="c-40750383">[-]</label><label class="expand" for="c-40750383">[5 more]</label></div><br/><div class="children"><div class="content">Wait until they learn about floating point.</div><br/><div id="40750442" class="c"><input type="checkbox" id="c-40750442" checked=""/><div class="controls bullet"><span class="by">stephencanon</span><span>|</span><a href="#40750311">root</a><span>|</span><a href="#40750383">parent</a><span>|</span><a href="#40750619">next</a><span>|</span><label class="collapse" for="c-40750442">[-]</label><label class="expand" for="c-40750442">[4 more]</label></div><br/><div class="children"><div class="content">What are they going to learn? That’s it’s by far the best fixed-size approximation to real number arithmetic we have, sufficient for essentially all of our scientific and engineering needs?</div><br/><div id="40751368" class="c"><input type="checkbox" id="c-40751368" checked=""/><div class="controls bullet"><span class="by">constantcrying</span><span>|</span><a href="#40750311">root</a><span>|</span><a href="#40750442">parent</a><span>|</span><a href="#40750937">next</a><span>|</span><label class="collapse" for="c-40751368">[-]</label><label class="expand" for="c-40751368">[1 more]</label></div><br/><div class="children"><div class="content">Besides what you said (which I mostly agree with), that floating point calculations can at times be extremely counter intuitive, weird and totally inaccurate.<p>While a single floating point operation gives you the best possible floating point result, the error for <i>two</i> consecutive floating point operations is unbounded.<p>They need to be treated carefully and people need to be aware of their shortcomings. You would be surprised how often I have seen even people here getting hung up over the fact that floating point numbers aren&#x27;t behaving identically to real numbers.</div><br/></div></div><div id="40750937" class="c"><input type="checkbox" id="c-40750937" checked=""/><div class="controls bullet"><span class="by">account42</span><span>|</span><a href="#40750311">root</a><span>|</span><a href="#40750442">parent</a><span>|</span><a href="#40751368">prev</a><span>|</span><a href="#40750661">next</a><span>|</span><label class="collapse" for="c-40750937">[-]</label><label class="expand" for="c-40750937">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That’s it’s by far the best fixed-size approximation to real number arithmetic we have (*)<p>* for some use cases (**)<p>** conditions apply</div><br/></div></div><div id="40750661" class="c"><input type="checkbox" id="c-40750661" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40750311">root</a><span>|</span><a href="#40750442">parent</a><span>|</span><a href="#40750937">prev</a><span>|</span><a href="#40750619">next</a><span>|</span><label class="collapse" for="c-40750661">[-]</label><label class="expand" for="c-40750661">[1 more]</label></div><br/><div class="children"><div class="content">I know that, but there are assumptions you cannot make about equivalence etc. So if he&#x27;s commenting on inaccuracies of numbers, he may find that surprising.<p>Tough crowd, sheesh.</div><br/></div></div></div></div></div></div></div></div><div id="40750455" class="c"><input type="checkbox" id="c-40750455" checked=""/><div class="controls bullet"><span class="by">rhelz</span><span>|</span><a href="#40750311">prev</a><span>|</span><label class="collapse" for="c-40750455">[-]</label><label class="expand" for="c-40750455">[11 more]</label></div><br/><div class="children"><div class="content">Its polynomials all the way down.</div><br/><div id="40750664" class="c"><input type="checkbox" id="c-40750664" checked=""/><div class="controls bullet"><span class="by">082349872349872</span><span>|</span><a href="#40750455">parent</a><span>|</span><a href="#40750665">next</a><span>|</span><label class="collapse" for="c-40750664">[-]</label><label class="expand" for="c-40750664">[6 more]</label></div><br/><div class="children"><div class="content">The VAX POLY instruction is generally held by s&#x2F;w folk to be the poster child for CISC over-complication.<p>Evidently they were mostly correct, as it&#x27;s disappeared, but one thing that is missing from many criticisms: POLY as part of the ISA should have enabled the h&#x2F;w folk to play games with representations of intermediate results (higher precision than the visible register set, or maybe even carry save?) that would not have been possible with RISC algorithmic implementations.<p>[I once worked with people who provided additional crunch for your POLY-heavy problems via extra VAXBI boards. Their bugaboo was that their approach (like GPU offload today?) often succeeded in turning what had been compute-bound problems into i&#x2F;o-bound ones]</div><br/><div id="40750790" class="c"><input type="checkbox" id="c-40750790" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#40750455">root</a><span>|</span><a href="#40750664">parent</a><span>|</span><a href="#40751259">next</a><span>|</span><label class="collapse" for="c-40750790">[-]</label><label class="expand" for="c-40750790">[3 more]</label></div><br/><div class="children"><div class="content">it turns out that fma is the right framework here. it gets you the extra accuracy you want, and can be really fast.</div><br/><div id="40751104" class="c"><input type="checkbox" id="c-40751104" checked=""/><div class="controls bullet"><span class="by">rhelz</span><span>|</span><a href="#40750455">root</a><span>|</span><a href="#40750790">parent</a><span>|</span><a href="#40751259">next</a><span>|</span><label class="collapse" for="c-40751104">[-]</label><label class="expand" for="c-40751104">[2 more]</label></div><br/><div class="children"><div class="content">What is fms?</div><br/><div id="40751225" class="c"><input type="checkbox" id="c-40751225" checked=""/><div class="controls bullet"><span class="by">masfuerte</span><span>|</span><a href="#40750455">root</a><span>|</span><a href="#40751104">parent</a><span>|</span><a href="#40751259">next</a><span>|</span><label class="collapse" for="c-40751225">[-]</label><label class="expand" for="c-40751225">[1 more]</label></div><br/><div class="children"><div class="content">Fused multiply add.  A single operation that does<p><pre><code>    a += b * c
</code></pre>
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multiply%E2%80%93accumulate_operation" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multiply%E2%80%93accumulate_op...</a></div><br/></div></div></div></div></div></div><div id="40751259" class="c"><input type="checkbox" id="c-40751259" checked=""/><div class="controls bullet"><span class="by">rhelz</span><span>|</span><a href="#40750455">root</a><span>|</span><a href="#40750664">parent</a><span>|</span><a href="#40750790">prev</a><span>|</span><a href="#40751098">next</a><span>|</span><label class="collapse" for="c-40751259">[-]</label><label class="expand" for="c-40751259">[1 more]</label></div><br/><div class="children"><div class="content">To your point about i&#x2F;o bottlenecks....these days, there is an i&#x2F;o bottleneck between main memory and the registers--and even between the registers and the ALUs.<p>This changes the tradeoffs. And its why we use chips sporting data parallelism and pipelined floating point units.  Both techniques are, in the final analysis, backing off of pure RISC and making more complex instructions.</div><br/></div></div><div id="40751098" class="c"><input type="checkbox" id="c-40751098" checked=""/><div class="controls bullet"><span class="by">rhelz</span><span>|</span><a href="#40750455">root</a><span>|</span><a href="#40750664">parent</a><span>|</span><a href="#40751259">prev</a><span>|</span><a href="#40750665">next</a><span>|</span><label class="collapse" for="c-40751098">[-]</label><label class="expand" for="c-40751098">[1 more]</label></div><br/><div class="children"><div class="content">Damn good point.  Sure, there’s no point in microcoding something if it is just doing the same steps as a hand-coded version could do.<p>But that doesn’t mean we just, say, stop implementing floating point operations!!   We just make the hardware do it better than hand-coding can.</div><br/></div></div></div></div><div id="40750665" class="c"><input type="checkbox" id="c-40750665" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#40750455">parent</a><span>|</span><a href="#40750664">prev</a><span>|</span><a href="#40751224">next</a><span>|</span><label class="collapse" for="c-40750665">[-]</label><label class="expand" for="c-40750665">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think Taylor series (or, for that matter, Fourier series) will ever stop seeming magical to me.<p>It seems to defy all common sense that things could <i>really</i> add up and cancel each other out that perfectly.<p>I understand all the intuition behind them. But they still feel like magic.</div><br/><div id="40751585" class="c"><input type="checkbox" id="c-40751585" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#40750455">root</a><span>|</span><a href="#40750665">parent</a><span>|</span><a href="#40751181">next</a><span>|</span><label class="collapse" for="c-40751585">[-]</label><label class="expand" for="c-40751585">[1 more]</label></div><br/><div class="children"><div class="content">For polynomials, it&#x27;s kind of trivial that taylor series work.  It&#x27;s sort of magical in the case of sine and cosine and the exponential function, but that&#x27;s more a case, i think, of the &quot;magical&quot; properties of those functions than the taylor series.  In the general case, taylor series only approximate a function around a point and it&#x27;s not even guaranteed to do it very well or very far from that point.</div><br/></div></div><div id="40751181" class="c"><input type="checkbox" id="c-40751181" checked=""/><div class="controls bullet"><span class="by">rhelz</span><span>|</span><a href="#40750455">root</a><span>|</span><a href="#40750665">parent</a><span>|</span><a href="#40751585">prev</a><span>|</span><a href="#40751224">next</a><span>|</span><label class="collapse" for="c-40751181">[-]</label><label class="expand" for="c-40751181">[1 more]</label></div><br/><div class="children"><div class="content">You wonder why we don’t have statues of him in college towns.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>