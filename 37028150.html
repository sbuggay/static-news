<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691398881124" as="style"/><link rel="stylesheet" href="styles.css?v=1691398881124"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://agi-sphere.com/llama-2/">What&#x27;s new in Llama 2 and how to run it locally</a> <span class="domain">(<a href="https://agi-sphere.com">agi-sphere.com</a>)</span></div><div class="subtext"><span>andrewon</span> | <span>17 comments</span></div><br/><div><div id="37030910" class="c"><input type="checkbox" id="c-37030910" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37031271">next</a><span>|</span><label class="collapse" for="c-37030910">[-]</label><label class="expand" for="c-37030910">[5 more]</label></div><br/><div class="children"><div class="content">In my tests LLaMa2-13B is useable for information extraction tasks and LLaMA2-70B is almost as good as GPT-4 (for IE). These models are the real thing. We can fine-tune LLaMAs, unlike OpenAI&#x27;s models. Now we can have privacy, control and lower prices. We can introduce guidance, KV caching and other tricks to improve the models.<p>The enthusiasm around it reminds me of JavaScript framework wars of 10 years ago - tons of people innovating and debating approaches, lots of projects popping up, so much energy!</div><br/><div id="37031102" class="c"><input type="checkbox" id="c-37031102" checked=""/><div class="controls bullet"><span class="by">asabla</span><span>|</span><a href="#37030910">parent</a><span>|</span><a href="#37031116">next</a><span>|</span><label class="collapse" for="c-37031102">[-]</label><label class="expand" for="c-37031102">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The enthusiasm around it reminds me of Javascript wars 10 years ago... so much energy!<p>I kind of have the same feeling as well. With all this energy it&#x27;s really hard to keep up with all new ideas, implementations, frameworks and services.<p>Really excited for what this will bring us the next coming years</div><br/></div></div><div id="37031116" class="c"><input type="checkbox" id="c-37031116" checked=""/><div class="controls bullet"><span class="by">pavlov</span><span>|</span><a href="#37030910">parent</a><span>|</span><a href="#37031102">prev</a><span>|</span><a href="#37031139">next</a><span>|</span><label class="collapse" for="c-37031116">[-]</label><label class="expand" for="c-37031116">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; “The enthusiasm around it reminds me of JavaScript framework wars of 10 years ago”</i><p>Hmm. If LLMs turned out like JS frameworks, that would mean that in ten years people will be saying:<p>“Maybe we don’t really need all this expensive ceremony, honestly this could be done with vanilla if&#x2F;else heuristics…?”</div><br/><div id="37031986" class="c"><input type="checkbox" id="c-37031986" checked=""/><div class="controls bullet"><span class="by">spacebanana7</span><span>|</span><a href="#37030910">root</a><span>|</span><a href="#37031116">parent</a><span>|</span><a href="#37031139">next</a><span>|</span><label class="collapse" for="c-37031986">[-]</label><label class="expand" for="c-37031986">[1 more]</label></div><br/><div class="children"><div class="content">I can imagine a bloated world where 500B param models are used for tasks where 7B param modes perform adequately.<p>At that time, there could be complaints on hacker news about messaging apps with autocomplete models that take up gigabytes.</div><br/></div></div></div></div><div id="37031139" class="c"><input type="checkbox" id="c-37031139" checked=""/><div class="controls bullet"><span class="by">worldsavior</span><span>|</span><a href="#37030910">parent</a><span>|</span><a href="#37031116">prev</a><span>|</span><a href="#37031271">next</a><span>|</span><label class="collapse" for="c-37031139">[-]</label><label class="expand" for="c-37031139">[1 more]</label></div><br/><div class="children"><div class="content">It looks like it will always be a war like Android VS iOS only now it&#x27;s with AI models.</div><br/></div></div></div></div><div id="37031271" class="c"><input type="checkbox" id="c-37031271" checked=""/><div class="controls bullet"><span class="by">jurmous</span><span>|</span><a href="#37030910">prev</a><span>|</span><a href="#37030782">next</a><span>|</span><label class="collapse" for="c-37031271">[-]</label><label class="expand" for="c-37031271">[2 more]</label></div><br/><div class="children"><div class="content">Did anybody try the Llama 2 model with languages other than English? The paper notes that it works best with English and the amount of training data for other languages is only a fraction. Which likely would make it unusable for me..<p>See table 10 (page 22) of the whitepaper for the numbers:
<a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;research&#x2F;publications&#x2F;llama-2-open-foundation-and-fine-tuned-chat-models&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;ai.meta.com&#x2F;research&#x2F;publications&#x2F;llama-2-open-found...</a><p>Are there other downloadable models which can be used in a multilingual environment that people here are aware of?</div><br/><div id="37031780" class="c"><input type="checkbox" id="c-37031780" checked=""/><div class="controls bullet"><span class="by">Gijs4g</span><span>|</span><a href="#37031271">parent</a><span>|</span><a href="#37030782">next</a><span>|</span><label class="collapse" for="c-37031780">[-]</label><label class="expand" for="c-37031780">[1 more]</label></div><br/><div class="children"><div class="content">At Mirage Studio we have successfully finetuned Llama 2 7B on a Dutch dataset to get it to output Dutch in a coherent way: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Mirage-Studio&#x2F;llama-gaan-2-7b-chat-hf-dutch" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Mirage-Studio&#x2F;llama-gaan-2-7b-chat-hf...</a></div><br/></div></div></div></div><div id="37030782" class="c"><input type="checkbox" id="c-37030782" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#37031271">prev</a><span>|</span><a href="#37031700">next</a><span>|</span><label class="collapse" for="c-37030782">[-]</label><label class="expand" for="c-37030782">[3 more]</label></div><br/><div class="children"><div class="content">If you want to try Llama 2 on a Mac and have Homebrew (or Python&#x2F;pip) you may find my LLM CLI tool interesting: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Aug&#x2F;1&#x2F;llama-2-mac&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Aug&#x2F;1&#x2F;llama-2-mac&#x2F;</a></div><br/><div id="37030858" class="c"><input type="checkbox" id="c-37030858" checked=""/><div class="controls bullet"><span class="by">petulla</span><span>|</span><a href="#37030782">parent</a><span>|</span><a href="#37031700">next</a><span>|</span><label class="collapse" for="c-37030858">[-]</label><label class="expand" for="c-37030858">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the inference time without gpu?</div><br/><div id="37030975" class="c"><input type="checkbox" id="c-37030975" checked=""/><div class="controls bullet"><span class="by">lm2s</span><span>|</span><a href="#37030782">root</a><span>|</span><a href="#37030858">parent</a><span>|</span><a href="#37031700">next</a><span>|</span><label class="collapse" for="c-37030975">[-]</label><label class="expand" for="c-37030975">[1 more]</label></div><br/><div class="children"><div class="content">It might the time mentioned at the bottom of the page since the author isn&#x27;t sure that the GPU is being used:<p>&gt;How to speed this up—right now my Llama prompts often take 20+ seconds to complete.</div><br/></div></div></div></div></div></div><div id="37031700" class="c"><input type="checkbox" id="c-37031700" checked=""/><div class="controls bullet"><span class="by">growt</span><span>|</span><a href="#37030782">prev</a><span>|</span><a href="#37031453">next</a><span>|</span><label class="collapse" for="c-37031700">[-]</label><label class="expand" for="c-37031700">[2 more]</label></div><br/><div class="children"><div class="content">Is there an overview somewhere how much RAM is needed for which model? Is it possible at all to run 4bit 70B on CPU and RAM?</div><br/><div id="37031958" class="c"><input type="checkbox" id="c-37031958" checked=""/><div class="controls bullet"><span class="by">cfn</span><span>|</span><a href="#37031700">parent</a><span>|</span><a href="#37031453">next</a><span>|</span><label class="collapse" for="c-37031958">[-]</label><label class="expand" for="c-37031958">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I run the 4bit, 70B on a threadripper 32 core using llama.cpp. It uses around 37Gb of RAM and I get 4-5 tokens per second (slow but usable). Core usage is very uneven with many cores at 0% so maybe there&#x27;s some more performance to be had in the future. Sometimes it gets stuck for a few seconds and then recovers.<p>It gives very detailed answers to coding questions and tasks just like GPT4 does (though I did not do a proper comparison).<p>The 13b uses 13Gb with 27 tokens per second the 7b uses 0.5Gb and I get 39 tokens per second on this machine.Both produce interesting results even for CUDA code generation, for example.</div><br/></div></div></div></div><div id="37031453" class="c"><input type="checkbox" id="c-37031453" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#37031700">prev</a><span>|</span><a href="#37031548">next</a><span>|</span><label class="collapse" for="c-37031453">[-]</label><label class="expand" for="c-37031453">[1 more]</label></div><br/><div class="children"><div class="content">Just set things up locally last night. If you&#x27;re a developer, llama.cpp was a pleasure to build and run. I wanted to run the weights from Meta and couldn&#x27;t figure out text generation web ui. It seemed that one was optimized for grabbing something off HuggingFace.<p>Running on a 3090. The 13b chat model quantized to fp8 is giving about 42 tok&#x2F;s.</div><br/></div></div><div id="37031548" class="c"><input type="checkbox" id="c-37031548" checked=""/><div class="controls bullet"><span class="by">gorenb</span><span>|</span><a href="#37031453">prev</a><span>|</span><a href="#37030721">next</a><span>|</span><label class="collapse" for="c-37031548">[-]</label><label class="expand" for="c-37031548">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only used the 13b model and I&#x27;d say it was as good as GPT-3 (not GPT-4). It&#x27;s amazing, and I only have a laptop to run it locally on so 13b is as good as I can do.</div><br/></div></div><div id="37030721" class="c"><input type="checkbox" id="c-37030721" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37031548">prev</a><span>|</span><a href="#37031378">next</a><span>|</span><label class="collapse" for="c-37030721">[-]</label><label class="expand" for="c-37030721">[1 more]</label></div><br/><div class="children"><div class="content">I am partial to Koboldcpp over text gen UI for a number of reasons.<p>...But I am also a bit out of the loop. For instance, I have not kept up with the CFG&#x2F;negative prompt or grammar implementations in the UIs.</div><br/></div></div><div id="37031378" class="c"><input type="checkbox" id="c-37031378" checked=""/><div class="controls bullet"><span class="by">ktaube</span><span>|</span><a href="#37030721">prev</a><span>|</span><label class="collapse" for="c-37031378">[-]</label><label class="expand" for="c-37031378">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the cheapest way to run e.g. LLaMa2-13B and have it served as an API?<p>I&#x27;ve tried  Inference Endpoints and Replicate, but both would cost more than just using the OpenAI offering.</div><br/></div></div></div></div></div></div></div></body></html>