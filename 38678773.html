<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702890066737" as="style"/><link rel="stylesheet" href="styles.css?v=1702890066737"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://graphthinking.blogspot.com/2023/12/small-offline-large-language-model.html">Small offline large language model – TinyChatEngine from MIT</a> <span class="domain">(<a href="https://graphthinking.blogspot.com">graphthinking.blogspot.com</a>)</span></div><div class="subtext"><span>physicsgraph</span> | <span>7 comments</span></div><br/><div><div id="38680244" class="c"><input type="checkbox" id="c-38680244" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#38679762">next</a><span>|</span><label class="collapse" for="c-38680244">[-]</label><label class="expand" for="c-38680244">[1 more]</label></div><br/><div class="children"><div class="content">Use llama.cpp for quantized model inference. It is simpler (no Docker nor Python required), faster (works well on CPUs), and supports many models.<p>Also there are better models than the one suggested. Mistral for 7B parameters. Yi if you want to go larger and happen to have 32Gb of memory. Mixtral MoE is the best but requires too much memory right now for most users.</div><br/></div></div><div id="38679762" class="c"><input type="checkbox" id="c-38679762" checked=""/><div class="controls bullet"><span class="by">upon_drumhead</span><span>|</span><a href="#38680244">prev</a><span>|</span><a href="#38680179">next</a><span>|</span><label class="collapse" for="c-38679762">[-]</label><label class="expand" for="c-38679762">[4 more]</label></div><br/><div class="children"><div class="content">I’m a tad confused<p>&gt; TinyChatEngine provides an off-line open-source large language model (LLM) that has been reduced in size.<p>But then they download the models from huggingface. I don’t understand how these are smaller? Or do they modify them locally?</div><br/><div id="38679780" class="c"><input type="checkbox" id="c-38679780" checked=""/><div class="controls bullet"><span class="by">lrem</span><span>|</span><a href="#38679762">parent</a><span>|</span><a href="#38680179">next</a><span>|</span><label class="collapse" for="c-38679780">[-]</label><label class="expand" for="c-38679780">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;TinyChatEngine">https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;TinyChatEngine</a><p>Turns out the original source is actually somewhat informative. Including telling you how much hardware do you need. This blog post looks like your typical note you leave for yourself to annotate a bit of your shell history.</div><br/><div id="38680117" class="c"><input type="checkbox" id="c-38680117" checked=""/><div class="controls bullet"><span class="by">pmontra</span><span>|</span><a href="#38679762">root</a><span>|</span><a href="#38679780">parent</a><span>|</span><a href="#38680179">next</a><span>|</span><label class="collapse" for="c-38680117">[-]</label><label class="expand" for="c-38680117">[2 more]</label></div><br/><div class="children"><div class="content">I wish that all these repos were more clear about the hardware requirements. Seeing that it runs on a 8 GB Raspberry, probably with abysmal performance, I&#x27;d say that it will run on my 32 GB Intel laptop on the CPU. Will it run on its Nvidia card? I remember that the rule of thumb was one GB of GPU RAM per G parameters, so I&#x27;d say that it won&#x27;t run. However this has 4 bit quantization so it could have lower requirements.<p>Of course the main problem is that I don&#x27;t know enough about the subject to reason on it on my own.</div><br/><div id="38680256" class="c"><input type="checkbox" id="c-38680256" checked=""/><div class="controls bullet"><span class="by">dkjaudyeqooe</span><span>|</span><a href="#38679762">root</a><span>|</span><a href="#38680117">parent</a><span>|</span><a href="#38680179">next</a><span>|</span><label class="collapse" for="c-38680256">[-]</label><label class="expand" for="c-38680256">[1 more]</label></div><br/><div class="children"><div class="content">Roughly speaking I believe it&#x27;s the number of parameters times the size of the parameters. So in the 4 bit case it&#x27;s half a gigabyte per billion parameters.<p>From a performance point of view (quantized) integer parameters are going to run better on CPUs than floating point parameters.</div><br/></div></div></div></div></div></div></div></div><div id="38680179" class="c"><input type="checkbox" id="c-38680179" checked=""/><div class="controls bullet"><span class="by">aravindgp</span><span>|</span><a href="#38679762">prev</a><span>|</span><label class="collapse" for="c-38680179">[-]</label><label class="expand" for="c-38680179">[1 more]</label></div><br/><div class="children"><div class="content">I have used them and I can say it&#x27;s pretty decent overall. I personally plan to use tinyengineon iot devices  which is  for even smaller iot microcontroller devices.</div><br/></div></div></div></div></div></div></div></body></html>