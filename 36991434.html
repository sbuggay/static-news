<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691139660200" as="style"/><link rel="stylesheet" href="styles.css?v=1691139660200"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://guochengqian.github.io/project/magic123/">Magic123: One Image to High-Quality 3D Object Generation</a> <span class="domain">(<a href="https://guochengqian.github.io">guochengqian.github.io</a>)</span></div><div class="subtext"><span>GaggiX</span> | <span>23 comments</span></div><br/><div><div id="36994031" class="c"><input type="checkbox" id="c-36994031" checked=""/><div class="controls bullet"><span class="by">Geee</span><span>|</span><a href="#36996680">next</a><span>|</span><label class="collapse" for="c-36994031">[-]</label><label class="expand" for="c-36994031">[6 more]</label></div><br/><div class="children"><div class="content">Why are researchers focusing on single-image reconstruction? It seems like a party trick which isn&#x27;t very useful, and pretty much impossible to reconstruct the original object accurately. It would be much more useful if many images from different angles could be used. Somewhat like NERF, but also predict missing views with 2D diffusion. Adding more images would get the model closer to ground truth.</div><br/><div id="36996720" class="c"><input type="checkbox" id="c-36996720" checked=""/><div class="controls bullet"><span class="by">fxtentacle</span><span>|</span><a href="#36994031">parent</a><span>|</span><a href="#36995314">next</a><span>|</span><label class="collapse" for="c-36996720">[-]</label><label class="expand" for="c-36996720">[2 more]</label></div><br/><div class="children"><div class="content">They likely focus on the 1 image case because there it&#x27;s the easiest to show visual progress over the competition. If you have 50+ images, the tech from 10 years ago is already good enough to get Hollywood-quality 3D scans.<p>From what I understand, they use the text keywords detected from the image as guidance and they also apply a loss between the current diffusion state and the source image. In effect, this is stable diffusion for 3D shapes but with clever conditioning. That means this algorithm will also work just fine if you have 2+ input images.</div><br/><div id="36997127" class="c"><input type="checkbox" id="c-36997127" checked=""/><div class="controls bullet"><span class="by">timlod</span><span>|</span><a href="#36994031">root</a><span>|</span><a href="#36996720">parent</a><span>|</span><a href="#36995314">next</a><span>|</span><label class="collapse" for="c-36997127">[-]</label><label class="expand" for="c-36997127">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any source for scene reconstruction being hollywood-quality given 50+ images 10 years ago (assuming the images don&#x27;t come from a controlled environment)?
I can find for example this from 2016: <a href="https:&#x2F;&#x2F;substance3d.adobe.com&#x2F;magazine&#x2F;go-scan-the-world-photogrammetry-with-a-smartphone&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;substance3d.adobe.com&#x2F;magazine&#x2F;go-scan-the-world-pho...</a>
However, there&#x27;s still a lot of manual work involved, and you don&#x27;t easily get near-perfect PBR textures (which would be what I&#x27;d consider hollywood-quality).
I&#x27;d say the devil is in the details - if you want the best quality, you&#x27;d still need to control a lot of the environment.<p>The more assumptions you can bake into the parameters of some model, the more degrees of freedom you get in the actual measurement process (e.g. reducing the amount of actual data necessary).</div><br/></div></div></div></div><div id="36995314" class="c"><input type="checkbox" id="c-36995314" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#36994031">parent</a><span>|</span><a href="#36996720">prev</a><span>|</span><a href="#36996221">next</a><span>|</span><label class="collapse" for="c-36995314">[-]</label><label class="expand" for="c-36995314">[1 more]</label></div><br/><div class="children"><div class="content">Because there’s people working on all sorts of different problems and solving a problem in one area can apply better to some problems than others. Not to mention that solution approaches can often cross pollinate.<p>Research is additive not a zero sum thing.</div><br/></div></div><div id="36996221" class="c"><input type="checkbox" id="c-36996221" checked=""/><div class="controls bullet"><span class="by">zdkl</span><span>|</span><a href="#36994031">parent</a><span>|</span><a href="#36995314">prev</a><span>|</span><a href="#36995498">next</a><span>|</span><label class="collapse" for="c-36996221">[-]</label><label class="expand" for="c-36996221">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Photogrammetry" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Photogrammetry</a><p><a href="https:&#x2F;&#x2F;colmap.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;colmap.github.io&#x2F;</a>
Or, more modern:
<a href="https:&#x2F;&#x2F;alicevision.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;alicevision.org&#x2F;</a></div><br/></div></div><div id="36995498" class="c"><input type="checkbox" id="c-36995498" checked=""/><div class="controls bullet"><span class="by">inconceivable</span><span>|</span><a href="#36994031">parent</a><span>|</span><a href="#36996221">prev</a><span>|</span><a href="#36996680">next</a><span>|</span><label class="collapse" for="c-36995498">[-]</label><label class="expand" for="c-36995498">[1 more]</label></div><br/><div class="children"><div class="content">yeah, it&#x27;s totally useless.... unless of course, you have only one image of the thing you&#x27;re trying to model. and getting a second image will take millions of dollars or another trip around the earth from orbit.<p>lmao are you for real?</div><br/></div></div></div></div><div id="36996680" class="c"><input type="checkbox" id="c-36996680" checked=""/><div class="controls bullet"><span class="by">going_ham</span><span>|</span><a href="#36994031">prev</a><span>|</span><a href="#36993365">next</a><span>|</span><label class="collapse" for="c-36996680">[-]</label><label class="expand" for="c-36996680">[2 more]</label></div><br/><div class="children"><div class="content">NeRFs are cool technology that has its place and use. It is going to help with scene reconstruction and so on. And that is the reason why these CV researchers are flocking into using it. Despite knowing the limitations, they are giving their best to improve this technology. What I believe is NeRF is insufficient to be meaningful on its own. If you look into the architecture from this problem (the paper in the post), it clearly shows that they have some refinement phase going later on. A single RGB 2D to 3D model is such an ill-posed, we have to consider a lot of priors before diving into it.<p>There needs to be more foundational work in this field that can outperform or even improve the NeRF-based techniques. And the current herd mentality of researchers should be changed into exploring the alternatives. There is a reason why expensive automobile companies still rely on the physical modelling of their design. It&#x27;s hard to simulate the physical conditions only through CAD modelling. Sure NeRFs are cool and they can make impressive results. That doesn&#x27;t necessarily mean it is the means to an end. Look where rasterization brought us! NeRF is like rasterization. It is going to be used. But highly quality graphics was possible through GI and ray tracing! NeRF needs something equivalent that is physically grounded.</div><br/><div id="36997094" class="c"><input type="checkbox" id="c-36997094" checked=""/><div class="controls bullet"><span class="by">milkey_mouse</span><span>|</span><a href="#36996680">parent</a><span>|</span><a href="#36993365">next</a><span>|</span><label class="collapse" for="c-36997094">[-]</label><label class="expand" for="c-36997094">[1 more]</label></div><br/><div class="children"><div class="content">&gt; NeRF needs something equivalent that is physically grounded.<p>Have you seen Plenoxels? <a href="https:&#x2F;&#x2F;alexyu.net&#x2F;plenoxels&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;alexyu.net&#x2F;plenoxels&#x2F;</a></div><br/></div></div></div></div><div id="36993365" class="c"><input type="checkbox" id="c-36993365" checked=""/><div class="controls bullet"><span class="by">kallistisoft</span><span>|</span><a href="#36996680">prev</a><span>|</span><a href="#36993426">next</a><span>|</span><label class="collapse" for="c-36993365">[-]</label><label class="expand" for="c-36993365">[1 more]</label></div><br/><div class="children"><div class="content">Very impressive work, but this reads more as data compression and retrieval algorithm than a generative model. For all of the examples they&#x27;re inputting a novel 2D view of a known object and reconstructing the 3D mesh from the 2D&#x2F;3D training set of the same object.<p>I&#x27;d like to see how it performs on a novel object that is <i>similar</i> to one in the training set, but not included in the training data.</div><br/></div></div><div id="36993426" class="c"><input type="checkbox" id="c-36993426" checked=""/><div class="controls bullet"><span class="by">q_andrew</span><span>|</span><a href="#36993365">prev</a><span>|</span><a href="#36992304">next</a><span>|</span><label class="collapse" for="c-36993426">[-]</label><label class="expand" for="c-36993426">[3 more]</label></div><br/><div class="children"><div class="content">Certainly a step above what a normal person could do in a 3D modeling program. The interesting thing about this type of stuff is that it would be harder for a professional to use those meshes as a base model; the topology doesn&#x27;t lend itself to their process. For example -- the banana peels are puffy and two-dimensional. It would have to be completely restructured to be a convincingly peeled banana. So either the generated model has to be a finished product, or they are cost-effectively useless.  CAD files are notoriously bad, and those are nice and mathematically easy to break down.</div><br/><div id="36993727" class="c"><input type="checkbox" id="c-36993727" checked=""/><div class="controls bullet"><span class="by">badsectoracula</span><span>|</span><a href="#36993426">parent</a><span>|</span><a href="#36993475">next</a><span>|</span><label class="collapse" for="c-36993727">[-]</label><label class="expand" for="c-36993727">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So either the generated model has to be a finished product, or they are cost-effectively useless.<p>Not really. Depending on the use case and adequate tools it can be much faster to the alternative of making these manually as meshes and textures.<p>Using the banana as an example, if this can be converted to a volumetric model (voxels) the puffied side can be shaved off using sculpting tools and the model be converted to a mesh much faster than making it from scratch. While the end result wouldn&#x27;t be good for looking at it up front, it can be perfectly viable for background props in a game, especially something that is viewed from a bird&#x27;s eye view or a drawn out third person perspective (though even up front it&#x27;ll look better than what you see in some games[0] - and that is AAA).<p>In fact there have been several games using photogrammetry already to construct 3D models out of taking photos of real places from various angles and converting them to point clouds and then to meshes - which after that they need to be cleaned up by artists. This all takes time, is costly and needs specialized hardware and software and yet developers do it. The linked paper is about a method that significantly lowers those barriers while giving decent results even if they still need to be edited.<p>[0] <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;0OeNkZu.jpeg" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.imgur.com&#x2F;0OeNkZu.jpeg</a></div><br/></div></div><div id="36993475" class="c"><input type="checkbox" id="c-36993475" checked=""/><div class="controls bullet"><span class="by">spookie</span><span>|</span><a href="#36993426">parent</a><span>|</span><a href="#36993727">prev</a><span>|</span><a href="#36992304">next</a><span>|</span><label class="collapse" for="c-36993475">[-]</label><label class="expand" for="c-36993475">[1 more]</label></div><br/><div class="children"><div class="content">Quite true. It&#x27;s great to have a 3D representation of what you want, however.<p>Gotta say, tools like these or NeRF will only revolutionise 3D modelling if they ever get topology right. That&#x27;s the hard part.</div><br/></div></div></div></div><div id="36992304" class="c"><input type="checkbox" id="c-36992304" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#36993426">prev</a><span>|</span><a href="#36996924">next</a><span>|</span><label class="collapse" for="c-36992304">[-]</label><label class="expand" for="c-36992304">[1 more]</label></div><br/><div class="children"><div class="content">Amazing work! It was be interesting to create a &#x27;spatial viewer&#x27; that could navigate around the object, rather than just watching the object rotate in front of you. If theres a background scene, then the object of interest could be extracted and three-dimensionalized, while keeping the background scene flat.  Then the viewer could move around the object while maintaining (at least a flat version of) the background.</div><br/></div></div><div id="36996924" class="c"><input type="checkbox" id="c-36996924" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#36992304">prev</a><span>|</span><a href="#36993663">next</a><span>|</span><label class="collapse" for="c-36996924">[-]</label><label class="expand" for="c-36996924">[1 more]</label></div><br/><div class="children"><div class="content">It would be nice if you could refine this by adding more images.</div><br/></div></div><div id="36993663" class="c"><input type="checkbox" id="c-36993663" checked=""/><div class="controls bullet"><span class="by">BudaDude</span><span>|</span><a href="#36996924">prev</a><span>|</span><a href="#36994209">next</a><span>|</span><label class="collapse" for="c-36993663">[-]</label><label class="expand" for="c-36993663">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty cool how quickly we&#x27;re able to turn a 2D image into a 3D one these days. Although it&#x27;s not perfect yet, you can see that it doesn&#x27;t quite capture the depth of the horse and dragon sculpture near the edges. But this technology will definitely improve over time.<p>Now, here&#x27;s a thought to chew on: once we&#x27;ve mastered 2D to 3D, what&#x27;s stopping us from exploring 3D to 4D conversion? That would take this to an entirely new level.</div><br/><div id="36993825" class="c"><input type="checkbox" id="c-36993825" checked=""/><div class="controls bullet"><span class="by">inportb</span><span>|</span><a href="#36993663">parent</a><span>|</span><a href="#36994209">next</a><span>|</span><label class="collapse" for="c-36993825">[-]</label><label class="expand" for="c-36993825">[1 more]</label></div><br/><div class="children"><div class="content">This project seems to perform some rudimentary 3D to 4D conversion by rotating the object as a function of time. A first step, perhaps, on the path to inferring entire timelines.</div><br/></div></div></div></div><div id="36994209" class="c"><input type="checkbox" id="c-36994209" checked=""/><div class="controls bullet"><span class="by">lukevp</span><span>|</span><a href="#36993663">prev</a><span>|</span><a href="#36994524">next</a><span>|</span><label class="collapse" for="c-36994209">[-]</label><label class="expand" for="c-36994209">[2 more]</label></div><br/><div class="children"><div class="content">The banana has 2 bananas, and the bird has 2 beaks! Why is that? Does it not know about real life objects and what the image might be representing? Would it be possible to combine this with some other kind of model so it would be able to handle things like that?</div><br/><div id="36997120" class="c"><input type="checkbox" id="c-36997120" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#36994209">parent</a><span>|</span><a href="#36994524">next</a><span>|</span><label class="collapse" for="c-36997120">[-]</label><label class="expand" for="c-36997120">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.digital-science.com&#x2F;tldr&#x2F;article&#x2F;the-lone-banana-problem-or-the-new-programming-speaking-ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.digital-science.com&#x2F;tldr&#x2F;article&#x2F;the-lone-banana...</a></div><br/></div></div></div></div><div id="36994524" class="c"><input type="checkbox" id="c-36994524" checked=""/><div class="controls bullet"><span class="by">pdar4123</span><span>|</span><a href="#36994209">prev</a><span>|</span><a href="#36992796">next</a><span>|</span><label class="collapse" for="c-36994524">[-]</label><label class="expand" for="c-36994524">[2 more]</label></div><br/><div class="children"><div class="content">Good lord the horror - devil horse is going to haunt me</div><br/><div id="36995657" class="c"><input type="checkbox" id="c-36995657" checked=""/><div class="controls bullet"><span class="by">TheCleric</span><span>|</span><a href="#36994524">parent</a><span>|</span><a href="#36992796">next</a><span>|</span><label class="collapse" for="c-36995657">[-]</label><label class="expand" for="c-36995657">[1 more]</label></div><br/><div class="children"><div class="content">5 legged devil horse.</div><br/></div></div></div></div><div id="36992796" class="c"><input type="checkbox" id="c-36992796" checked=""/><div class="controls bullet"><span class="by">reidjs</span><span>|</span><a href="#36994524">prev</a><span>|</span><label class="collapse" for="c-36992796">[-]</label><label class="expand" for="c-36992796">[2 more]</label></div><br/><div class="children"><div class="content">Can I use this?</div><br/><div id="36993141" class="c"><input type="checkbox" id="c-36993141" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#36992796">parent</a><span>|</span><label class="collapse" for="c-36993141">[-]</label><label class="expand" for="c-36993141">[1 more]</label></div><br/><div class="children"><div class="content">The code is here, so it seems so: <a href="https:&#x2F;&#x2F;github.com&#x2F;guochengqian&#x2F;Magic123">https:&#x2F;&#x2F;github.com&#x2F;guochengqian&#x2F;Magic123</a><p>It looks like if you use the full pipeline, it takes 2.5 hours for textual inversion, 40 minutes for coarse estimation, and 20 minutes for fine estimation. For one image, on a 32G V100.</div><br/></div></div></div></div></div></div></div></div></div></body></html>