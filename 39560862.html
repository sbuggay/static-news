<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709370077097" as="style"/><link rel="stylesheet" href="styles.css?v=1709370077097"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cgad.ski/blog/where-is-noethers-principle-in-machine-learning.html">Where is Noether&#x27;s principle in machine learning?</a> <span class="domain">(<a href="https://cgad.ski">cgad.ski</a>)</span></div><div class="subtext"><span>cgadski</span> | <span>64 comments</span></div><br/><div><div id="39563371" class="c"><input type="checkbox" id="c-39563371" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#39561877">next</a><span>|</span><label class="collapse" for="c-39563371">[-]</label><label class="expand" for="c-39563371">[3 more]</label></div><br/><div class="children"><div class="content">A related paper I just found and am digesting: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2012.04728" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2012.04728</a><p>Softmax gives rise to translation symmetry, batch normalization to scale symmetry, homogeneous activations to rescale symmetry. Each of those induce their own learning invariants through training.</div><br/><div id="39563901" class="c"><input type="checkbox" id="c-39563901" checked=""/><div class="controls bullet"><span class="by">cgadski</span><span>|</span><a href="#39563371">parent</a><span>|</span><a href="#39561877">next</a><span>|</span><label class="collapse" for="c-39563901">[-]</label><label class="expand" for="c-39563901">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s also a neat result! I&#x27;d just like to highlight that the conservation laws proved in that paper are functions of the parameters that hold over the course of gradient descent, whereas my post is talking about functions of the activations that are conserved from one layer to the next within an optimized network.<p>By the way, maybe I&#x27;m being too much of a math snob, but I&#x27;d argue Kunin&#x27;s result is only superficially similar to Noether&#x27;s theorem. (In the paper they call it a &quot;striking similarity&quot;!) Geometrically, what they&#x27;re saying is that, if a loss function is invariant under a non-zero vector field, then the trajectory of gradient descent will be tangent to the codimension-1 distribution of vectors perpendicular to the vector field. If that distribution is integrable (in the sense of the Frobenius theorem), then any of its integrals is conserved under gradient descent. That&#x27;s a very different geometric picture from Noether&#x27;s theorem. For example, Noether&#x27;s theorem gives a direct mapping from invariances to conserved quantities, whereas they need a special integrability condition to hold. But yes, it is a nice result, certainly worth keeping in mind when thinking about your gradient flows. :)<p>By the way, you might be interested in [1], which also studies gradient descent from the point of view of mechanics and seems to really use Noether-like results.<p>[1] Tanaka, Hidenori, and Daniel Kunin. “Noether’s Learning Dynamics: Role of Symmetry Breaking in Neural Networks.” In Advances in Neural Information Processing Systems, 34:25646–60. Curran Associates, Inc., 2021. <a href="https:&#x2F;&#x2F;papers.nips.cc&#x2F;paper&#x2F;2021&#x2F;hash&#x2F;d76d8deea9c19cc9aaf2237d2bf2f785-Abstract.html" rel="nofollow">https:&#x2F;&#x2F;papers.nips.cc&#x2F;paper&#x2F;2021&#x2F;hash&#x2F;d76d8deea9c19cc9aaf22...</a>.</div><br/><div id="39569979" class="c"><input type="checkbox" id="c-39569979" checked=""/><div class="controls bullet"><span class="by">jonathanyc</span><span>|</span><a href="#39563371">root</a><span>|</span><a href="#39563901">parent</a><span>|</span><a href="#39561877">next</a><span>|</span><label class="collapse" for="c-39569979">[-]</label><label class="expand" for="c-39569979">[1 more]</label></div><br/><div class="children"><div class="content">Not GP, but thanks for your detailed comment and the paper reference.</div><br/></div></div></div></div></div></div><div id="39561877" class="c"><input type="checkbox" id="c-39561877" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39563371">prev</a><span>|</span><a href="#39562606">next</a><span>|</span><label class="collapse" for="c-39561877">[-]</label><label class="expand" for="c-39561877">[27 more]</label></div><br/><div class="children"><div class="content">This is one of those links where just seeing the title sets you off, thinking about the implications.<p>I&#x27;m going to have to spend more time digesting the article, but one thing that jumps out at me, and maybe it&#x27;s answered in the article and I don&#x27;t understand it, is the role of time.  Generally in physics, you&#x27;re talking about a quantity being conserved over time, and I&#x27;m not sure what plays the role of time when you&#x27;re talking about conserved quantities in machine learning -- is it conserved over training iterations or over inference layers, or what?<p>edit: now that i&#x27;ve read it again, I just saw that they described in the second paragraph.<p>I&#x27;m now wondering if in something like Sora that can do a kind of physical modeling, if there&#x27;s some conserved quantity in the neural network that is _directly analagous_ to conserved quantities in physics -- if there is, for example, something that represents momentum, that operates exactly as momentum as it progresses through the layers.</div><br/><div id="39562498" class="c"><input type="checkbox" id="c-39562498" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#39561877">parent</a><span>|</span><a href="#39563722">next</a><span>|</span><label class="collapse" for="c-39562498">[-]</label><label class="expand" for="c-39562498">[12 more]</label></div><br/><div class="children"><div class="content">In physics, the conserved quantity isn&#x27;t always time.  Invariance over time translation is specifically conservation of energy.  Invariance over spatial translation is conservation of momentum, invariance over spatial rotation is conservation of conservation of angular momentum, invariance of electromagnetic field is conservation of current, and invariance of wave function phase is conservation of charge.<p>I think the analogue in machine learning is <i>conservation over changes in the training data</i>.  After all, the point of machine learning is to find general models that describe the training data given, and minimize the loss function.  Assuming that a useful model can be trained, the whole <i>point</i> is that it generalizes to new, unseen instances with minimal losses, i.e. the model remains invariant under shifts in the instances seen.<p>The more interesting part to me is what this says about philosophy of physics.  Noether&#x27;s Theorem can be restated as &quot;The laws of physics are invariant under X transformation&quot;, where X is the gauge symmetry associated with the conservation law.  But <i>maybe this is simply a consequence of how we do physics</i>.  After all, the point of science is to produce generalized laws from empirical observations.  It&#x27;s trivially easy to find a real-world situation where conservation of energy does <i>not</i> hold (any system with friction, which is basically all of them), but the math gets very messy if you try to actually model the real data, so we rely on approximations that are close enough most of the time.  And if many people take empirical measurements at many different points in space, and time, and orientations, you get generalized laws that hold regardless of where&#x2F;when&#x2F;who takes the measurement.<p>Machine learning could be viewed as doing science on empirically measurable social quantities.  It won&#x27;t always be accurate, as individual machine-learning fails show.  But it&#x27;s accurate <i>enough</i> that it can provide useful models for civilization-scale quantities.</div><br/><div id="39563731" class="c"><input type="checkbox" id="c-39563731" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562498">parent</a><span>|</span><a href="#39563324">next</a><span>|</span><label class="collapse" for="c-39563731">[-]</label><label class="expand" for="c-39563731">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In physics, the conserved quantity isn&#x27;t always time. Invariance over time translation is specifically conservation of energy.<p>That&#x27;s not what i meant.<p>When you talk about &quot;conservation of angular momentum&quot;, the symmetry is invariance over rotation, but the angular momentum is conserved _over time_.</div><br/></div></div><div id="39563324" class="c"><input type="checkbox" id="c-39563324" checked=""/><div class="controls bullet"><span class="by">jedbrown</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562498">parent</a><span>|</span><a href="#39563731">prev</a><span>|</span><a href="#39563276">next</a><span>|</span><label class="collapse" for="c-39563324">[-]</label><label class="expand" for="c-39563324">[4 more]</label></div><br/><div class="children"><div class="content">&gt;  It&#x27;s trivially easy to find a real-world situation where conservation of energy does not hold (any system with friction, which is basically all of them)<p>Conservation of energy absolutely still holds, but entropy is not conserved so the process is irreversible. If your model doesn&#x27;t include heat, then discrete energy won&#x27;t be conserved in a process that produces heat, but that&#x27;s your modeling choice, not a statement about physics. It is common to model such processes using a dissipation potential.</div><br/><div id="39563653" class="c"><input type="checkbox" id="c-39563653" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39563324">parent</a><span>|</span><a href="#39563276">next</a><span>|</span><label class="collapse" for="c-39563653">[-]</label><label class="expand" for="c-39563653">[3 more]</label></div><br/><div class="children"><div class="content">Right, but I&#x27;m saying that it&#x27;s <i>all</i> modeling choices, all the way down.  Extend the model to include thermal energy and most of the time it holds again - but then it falls down if you also have static electricity that generates a visible spark (say, a wool sweater on a slide) or magnetic drag (say, regenerative braking on a car).  Then you can include models for <i>those</i> too, but you&#x27;re introducing new concepts with each, and the math gets much hairier.  We <i>call</i> the unified model where we abstract away all the different forms of energy &quot;conservation of energy&quot;, but there are a good many practical systems where making tangible predictions using conservation of energy gives wrong answers.<p>Basically this is a restatement of Box&#x27;s Aphorism (&quot;All models are wrong, but some are useful&quot;) or the ideas in Thomas Kuhn&#x27;s &quot;The Structure of Scientific Revolutions&quot;.  The goal of science is to from concrete observations to abstract principles which ideally will accurately predict the value of future concrete observations.  In many cases, you can do this.  But not all.  There is always messy data that doesn&#x27;t fit into neat, simple, general laws.  Usually the messy data is just ignored, because it can&#x27;t be predicted and is assumed to average out or generally be irrelevant in the end.  But sometimes the messy outliers bite you, or someone comes up with a new way to handle them elegantly, and then you get a paradigm shift.<p>And this has implications for understanding what machine learning is or why it&#x27;s important.  Few people would think that a model linking background color to likeliness to click on ads is a fundamental physical quality, but Google had one 15+ years ago, and it was pretty accurate, and made them a bunch of money.  Or similarly, most people wouldn&#x27;t think of a model of the English language as being a fundamental physical quality, but that&#x27;s exactly what an LLM is, and they&#x27;re pretty useful too.</div><br/><div id="39565521" class="c"><input type="checkbox" id="c-39565521" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39563653">parent</a><span>|</span><a href="#39563276">next</a><span>|</span><label class="collapse" for="c-39565521">[-]</label><label class="expand" for="c-39565521">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been a long time since I have cracked a physics book, but your mention of interesting &quot;fundamental physical quantities&quot; triggered the recollection of there being a conservation of information result in quantum mechanics where you can come up with an action whose equations of motion are Schrödinger&#x27;s equation and the conserved quantity is a probability current. So I wonder to what extent (if any) it might make sense to try to approach these things in terms of the <i>really fundamental</i> quantity of information itself?</div><br/><div id="39568140" class="c"><input type="checkbox" id="c-39568140" checked=""/><div class="controls bullet"><span class="by">jerf</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39565521">parent</a><span>|</span><a href="#39563276">next</a><span>|</span><label class="collapse" for="c-39568140">[-]</label><label class="expand" for="c-39568140">[1 more]</label></div><br/><div class="children"><div class="content">Approaching physics from a pure information flow is definitely a current research topic. I suspect we see less popsci treatment of it because almost nobody understands information at all, then trying to apply it to physics that also almost nobody understands is probably at least three or four bridges too far for a popsci treatment, but it&#x27;s a current and active topic.</div><br/></div></div></div></div></div></div></div></div><div id="39563276" class="c"><input type="checkbox" id="c-39563276" checked=""/><div class="controls bullet"><span class="by">Aardwolf</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562498">parent</a><span>|</span><a href="#39563324">prev</a><span>|</span><a href="#39563580">next</a><span>|</span><label class="collapse" for="c-39563276">[-]</label><label class="expand" for="c-39563276">[3 more]</label></div><br/><div class="children"><div class="content">Is there any way to deduce which invariance gives which conservation? I mean for example: how can you tell that time invariance is the one paired with conservation of energy? Why is e.g. time invariance not paired with momentum, current, or anything else, but specifically energy?<p>I know that I can remember momentum is paired with translation simply because there&#x27;s both the angular momentum and the non-angular momentum one and in space you have translation and rotation, so for time energy is the only one that&#x27;s left over, but I&#x27;m not looking for a trick to remember it, I&#x27;m looking for the fundamental reason, as well as how to tell what will be paired with some invariance when looking at some other new invariance</div><br/><div id="39563517" class="c"><input type="checkbox" id="c-39563517" checked=""/><div class="controls bullet"><span class="by">chunky1994</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39563276">parent</a><span>|</span><a href="#39567985">next</a><span>|</span><label class="collapse" for="c-39563517">[-]</label><label class="expand" for="c-39563517">[1 more]</label></div><br/><div class="children"><div class="content">The conserved quantity is derived from Noether&#x27;s theorem itself. One thing that is a bit hairy is that Noether&#x27;s theorem only applies to a continuous, smooth (physical -&gt; there is some wiggle room here) space.<p>When deriving the conservation of energy from Noether&#x27;s theorem you basically say that your Lagrangian (which is just a set of equations that describes a physical system) is invariant over time. When you do that you automatically get that energy is conserved. Each invariant produces a conserved quantity as explained in parent comment when you apple a specific transformation that is supposed to not change the system (i.e remain invariant).<p>Now in doing this you&#x27;re also invoking the principle of least action (by using Lagrangians to describe the state of a physical system) but that is a separate topic.</div><br/></div></div><div id="39567985" class="c"><input type="checkbox" id="c-39567985" checked=""/><div class="controls bullet"><span class="by">Cleonis</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39563276">parent</a><span>|</span><a href="#39563517">prev</a><span>|</span><a href="#39563580">next</a><span>|</span><label class="collapse" for="c-39567985">[-]</label><label class="expand" for="c-39567985">[1 more]</label></div><br/><div class="children"><div class="content">In retrospect: the earliest recognition of a conserved quantity was Kepler&#x27;s law of areas. Isaac Newton later showed that Kepler&#x27;s law of areas is a specific instance of a property that obtains for any central force, not just the (inverse square) law of gravity.<p>About symmetry under change of orientation: for a given (spherically symmetric) source of gravitational interaction the amount of gravitational force is the same in any orientation.<p>For orbital motion the motion is in a plane, so for the case of orbital motion the relevant symmetry is cilindrical symmetry with respect to the plane of the orbit.<p>The very first derivation that is presented in Newton&#x27;s Principia is a derivation that shows that for any central force we have: in equal intervals of time equal amounts of area are swept out.<p>(The swept out area is proportional to the angular momentum of the orbiting object. That is, the area law anticipated the principle of conservation of angular momentum)<p>A discussion of Newton&#x27;s derivation, illustrated with diagrams, is available on my website:
<a href="http:&#x2F;&#x2F;cleonis.nl&#x2F;physics&#x2F;phys256&#x2F;angular_momentum.php" rel="nofollow">http:&#x2F;&#x2F;cleonis.nl&#x2F;physics&#x2F;phys256&#x2F;angular_momentum.php</a><p>The thrust of the derivation is that if the force that the motion is subject to is a central force (cilindrical symmetry) then angular momentum is conserved.<p>So:
In retrospect we see that Newton&#x27;s demonstration of the area law is an instance of symmetry-and-conserved-quantity-relation being used. Symmetry of a force under change of orientation has as corresponding conserved quantity of the resulting (orbiting) motion: conservation of angular momentum.<p>About conservation laws:<p>The law of conservation of angular momentum and the law of conservation of momentum are about quantities that are associated with specific spatial characteristics, and the conserved quantity is conserved over <i>time</i>.<p>I&#x27;m actually not sure about the reason(s) for classification of conservation of energy. My own view: we have that kinetic energy is not associated with any form of keeping track of orientation; the velocity vector is squared, and that squaring operation discards directional information. More generally, Energy is not associated with any spatial characteristic. Arguably Energy conservation is categorized as associated with symmetry under time translation because of <i>absence</i> of association with any spatial characteristic.</div><br/></div></div></div></div><div id="39563580" class="c"><input type="checkbox" id="c-39563580" checked=""/><div class="controls bullet"><span class="by">chunky1994</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562498">parent</a><span>|</span><a href="#39563276">prev</a><span>|</span><a href="#39562698">next</a><span>|</span><label class="collapse" for="c-39563580">[-]</label><label class="expand" for="c-39563580">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a bit skeptical to give up conservation of energy in a system with friction. Isn&#x27;t it more accurate to say that if we were to calculate every specific interaction we&#x27;d still end up having conservation of energy. Now whether or not we&#x27;re dealing with a closed system etc becomes important but if we were to able to truly model the entire physical system with friction, we&#x27;d still adhere to our conservation laws.<p>So they are not approximations, but are just terribly difficult calculations, no?<p>Maybe I&#x27;m misunderstanding your point, but this should be true regardless of our philosophy of physics correct?</div><br/><div id="39565141" class="c"><input type="checkbox" id="c-39565141" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39563580">parent</a><span>|</span><a href="#39562698">next</a><span>|</span><label class="collapse" for="c-39565141">[-]</label><label class="expand" for="c-39565141">[1 more]</label></div><br/><div class="children"><div class="content">It is an analogy stating that dissipative systems do not have a Lagrangian, Noether&#x27;s work applies to Lagrangian systems<p>Conservation laws in particular are measurable properties of an isolated physical system do not change as the system evolves over time.<p>It is important to remember that Physics is about finding useful models that make useful predictions about a system.  So it is important to not confuse the map for the territory.<p>Gibbs free energy and Helmholtz free energy are not conserved.<p>As thermodynamics, entropy, and entropy are difficult topics due to didactic half-truths, here is a paper that shows that the nbody problem becomes invariant  and may be undecidable due to what is a similar issue (in a contrived fashion)<p><a href="http:&#x2F;&#x2F;philsci-archive.pitt.edu&#x2F;13175&#x2F;" rel="nofollow">http:&#x2F;&#x2F;philsci-archive.pitt.edu&#x2F;13175&#x2F;</a><p>While Noether&#x27;s principle often allows you to see things that can often be simplified in an equation, often it allows you to not just simplify &#x27;terribly difficult calculations&#x27; but to actually find computationally possible calculations.</div><br/></div></div></div></div><div id="39562698" class="c"><input type="checkbox" id="c-39562698" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562498">parent</a><span>|</span><a href="#39563580">prev</a><span>|</span><a href="#39563722">next</a><span>|</span><label class="collapse" for="c-39562698">[-]</label><label class="expand" for="c-39562698">[1 more]</label></div><br/><div class="children"><div class="content">A nice way to formulate (most) data augmentations is: a family of functions A = {a} such that our optimized neural network f obeys f(x) ~= f(a(x)).<p>So in this case, we&#x27;re explicitly defining the set of desired invariances.</div><br/></div></div></div></div><div id="39563722" class="c"><input type="checkbox" id="c-39563722" checked=""/><div class="controls bullet"><span class="by">nurple</span><span>|</span><a href="#39561877">parent</a><span>|</span><a href="#39562498">prev</a><span>|</span><a href="#39562563">next</a><span>|</span><label class="collapse" for="c-39563722">[-]</label><label class="expand" for="c-39563722">[2 more]</label></div><br/><div class="children"><div class="content">I think the most profound insight I&#x27;ve come across while studying this particular topic is the insight that information theory ended up being the answer to conserving the 2nd law with respect to Maxwell&#x27;s demon thought experiment. Not to put too fine a point, but essentially the knowledge organized in the mind of the demon, about the particles in its system, was calculated to offset the creation of the energy gradient.<p>I found the thinking of William Sidis to be particularly thought provoking perspective on Noether&#x27;s benchmark work, in his paper The Animate and the Inanimate he posits--at a high level--that life is a &quot;reversal of the second law of thermodynamics&quot;; not that the 2nd law is a physical symmetry, but a mental one in an existence where energy reversibly flows between positive and negative states.<p>Indeed, when considering machine learning, I think it&#x27;s quite interesting to consider how the organizing of information&#x2F;knowledge done during training in some real way mirrors the energy-creating information interred in the mind of Maxwell&#x27;s demon.<p>When taking into account the possible transitive benefits of knowledge organized via machine learning, and its attendant oracle through application, it&#x27;s easy to see a world where this results in a net entropy loss, the creation of a previously non-existent energy gradient.<p>In my mind this has interesting implications for Fermi&#x27;s paradox as it seems to imply the inevitibility of the organization of information. Taken further into my own personal dogma, I think it&#x27;s inevitable that we create--what we would consider--a sentient being as I believe this is the cycle of our own origin in the larger evolutionary timeline.</div><br/><div id="39564999" class="c"><input type="checkbox" id="c-39564999" checked=""/><div class="controls bullet"><span class="by">Jerrrry</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39563722">parent</a><span>|</span><a href="#39562563">next</a><span>|</span><label class="collapse" for="c-39564999">[-]</label><label class="expand" for="c-39564999">[1 more]</label></div><br/><div class="children"><div class="content">&gt;at a high level--that life is a &quot;reversal of the second law of thermodynamics&quot;;<p>Life temporarily displaces entropy, locally.<p>Life wins battles, chaos wins the war.<p>&gt;Indeed, when considering machine learning, I think it&#x27;s quite interesting to consider how the organizing of information&#x2F;knowledge done during training in some real way mirrors the energy-creating information interred in the mind of Maxwell&#x27;s demon.<p>This is our human bias favoring the common myth of ever-expanding complexity is an &quot;inevitable&quot; result of the passage of time; refer to Stephen Jay Gould&#x27;s  &quot;Full House: The Spread of Excellence from Plato to Darwin&quot;[0] for the only palatable refute modern evolutionists can offer.<p>&gt;When taking into account the possible transitive benefits of knowledge organized via machine learning, and its attendant oracle through application, it&#x27;s easy to see a world where this results in a net entropy loss, the creation of a previously non-existent energy gradient.<p>Because it is. Randomness combined with a sieve, like a generator and a discriminator, like the primordial protein soup and our own existence as a selector, like chaos and order themselves,  MAY - but DOES NOT have to - lead to 
 temporary, localized areas of complexity, that we call &#x27;life&#x27;.<p>This &quot;energy gradient&quot; you speak of is literally gravity pulling baryonic matter foward thru space time. All work requires a temperature gradient - Hawking&#x27;s musings on the second law of thermodynamics and your own intuition can reason why.<p>&gt;In my mind this has interesting implications for Fermi&#x27;s paradox as it seems to imply the inevitibility of the organization of information. Taken further into my own personal dogma, I think it&#x27;s inevitable that we create--what we would consider--a sentient being as I believe this is the cycle of our own origin in the larger evolutionary timeline.<p>Over cosmological time spans, it is a near-mathematical certainty, that we are to either reach the universe&#x27;s Omega point[1] on &quot;our&quot; own accord,  perish to our own, by our own creation, or by our own son&#x27;s, hands.<p>[0]: <a href="https:&#x2F;&#x2F;www.amazon.com&#x2F;Full-House-Spread-Excellence-Darwin&#x2F;dp&#x2F;0674061616" rel="nofollow">https:&#x2F;&#x2F;www.amazon.com&#x2F;Full-House-Spread-Excellence-Darwin&#x2F;d...</a><p>[1]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=eOxHRFN4rs0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=eOxHRFN4rs0</a></div><br/></div></div></div></div><div id="39562563" class="c"><input type="checkbox" id="c-39562563" checked=""/><div class="controls bullet"><span class="by">shiandow</span><span>|</span><a href="#39561877">parent</a><span>|</span><a href="#39563722">prev</a><span>|</span><a href="#39561971">next</a><span>|</span><label class="collapse" for="c-39562563">[-]</label><label class="expand" for="c-39562563">[3 more]</label></div><br/><div class="children"><div class="content">A convolutional neural network ought to have translational symmetry, which should lead to a generalized version of momentum. If I understood the article correctly the conserved quantity would be &lt;gx, dx&gt;, where dx is the finite difference gradient of x.<p>This gives a vector with dimensions equal to however many directions you can translate a layer in and which is conserved over all (convolutional) layers.</div><br/><div id="39562869" class="c"><input type="checkbox" id="c-39562869" checked=""/><div class="controls bullet"><span class="by">cgadski</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562563">parent</a><span>|</span><a href="#39561971">next</a><span>|</span><label class="collapse" for="c-39562869">[-]</label><label class="expand" for="c-39562869">[2 more]</label></div><br/><div class="children"><div class="content">Exactly right! In fact, because that symmetry does not include an action on the parameters of the layer, your conserved quantity &lt;gx, dx&gt; should hold whether or not the network is stationary for a loss. This means that it&#x27;ll be stationary on every single data point. (In an image classification model, these values are just telling you whether or not the loss would be improved if the input image were translated.)</div><br/><div id="39566231" class="c"><input type="checkbox" id="c-39566231" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562869">parent</a><span>|</span><a href="#39561971">next</a><span>|</span><label class="collapse" for="c-39566231">[-]</label><label class="expand" for="c-39566231">[1 more]</label></div><br/><div class="children"><div class="content">Everything in the paper is talking about global symmetries, is there also the possibility of gauge symmetries?</div><br/></div></div></div></div></div></div><div id="39561971" class="c"><input type="checkbox" id="c-39561971" checked=""/><div class="controls bullet"><span class="by">Raro</span><span>|</span><a href="#39561877">parent</a><span>|</span><a href="#39562563">prev</a><span>|</span><a href="#39562027">next</a><span>|</span><label class="collapse" for="c-39561971">[-]</label><label class="expand" for="c-39561971">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, I&#x27;ve been thinking about similar concepts in a different context. Fascinating.<p>Regarding the role of time, the idea of a purely conserved quantity is that it is conserved under the conditions of the system (that&#x27;s why the article frequently references Newton&#x27;s First Law), so they&#x27;re generally held &quot;for all time that  these symmetries exist in the system&quot;.<p>Specifically on time: the invariant for systems that exhibit continuous time symmetries (i.e. you move a little bit forward or backward in time and the system looks exactly the same)  is energy.</div><br/><div id="39562306" class="c"><input type="checkbox" id="c-39562306" checked=""/><div class="controls bullet"><span class="by">dustingetz</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39561971">parent</a><span>|</span><a href="#39562027">next</a><span>|</span><label class="collapse" for="c-39562306">[-]</label><label class="expand" for="c-39562306">[4 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s my ELI5 attempt of the time&#x2F;energy relation:<p>imagine a spring at rest (not moving)<p>strike the spring, it&#x27;s now oscillating<p>the system now contains energy like a battery<p>what is energy? it&#x27;s stored work potential<p>the battery is storing the energy, which can then be taken out at some future time<p>the spring is transporting the energy through time<p>in fact how do we measure time? with clocks. What&#x27;s a clock? It&#x27;s an oscillator. The energized spring <i>is</i> the clock. When system energy is zero, what is time even? There&#x27;s no baseline against which to measure change when nothing is changing</div><br/><div id="39562476" class="c"><input type="checkbox" id="c-39562476" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562306">parent</a><span>|</span><a href="#39562677">next</a><span>|</span><label class="collapse" for="c-39562476">[-]</label><label class="expand" for="c-39562476">[2 more]</label></div><br/><div class="children"><div class="content">Symmetry exists abstractly,  apart from time.<p>There are many machine learning problems which should have symmetries:  a picture of a cow rotated 135 degrees is still a picture of a cow,  the meaning of spoken words shouldn&#x27;t change with the audio level, etc.  If they were doing machine learning on tracks from the LHC the system ought to take account of relativistic momentum and energy.<p>Can a model learn a symmetry?  Or should a symmetry just be built into the model from the beginning?</div><br/><div id="39562733" class="c"><input type="checkbox" id="c-39562733" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#39561877">root</a><span>|</span><a href="#39562476">parent</a><span>|</span><a href="#39562677">next</a><span>|</span><label class="collapse" for="c-39562733">[-]</label><label class="expand" for="c-39562733">[1 more]</label></div><br/><div class="children"><div class="content">Equivariant machine learning is a thing that people have tried... Tends to be expensive and slow, though, and imposes invariances that our model (a universal function approximator, recall) should just learn anyway: If you don&#x27;t have enough pictures of upside down cows, just train a normal model with augmentations.</div><br/></div></div></div></div></div></div></div></div><div id="39562027" class="c"><input type="checkbox" id="c-39562027" checked=""/><div class="controls bullet"><span class="by">Raro</span><span>|</span><a href="#39561877">parent</a><span>|</span><a href="#39561971">prev</a><span>|</span><a href="#39567314">next</a><span>|</span><label class="collapse" for="c-39562027">[-]</label><label class="expand" for="c-39562027">[1 more]</label></div><br/><div class="children"><div class="content">Ha,  my previous comment was before your new edit mentioning Sora. There is a good reason why the accompanying research report to the Sora demo isn&#x27;t titled &quot;Awesome Generative Video,&quot; but references world models. The interesting feature is how many apparently (approximations to) physical properties emerge (object permanence, linear motion, partially elastic collisions, as well as many of the elements of grammar of film), and which do not (notably material properties of solid and fluids, creation of objects from nothing, etc.)</div><br/></div></div><div id="39567314" class="c"><input type="checkbox" id="c-39567314" checked=""/><div class="controls bullet"><span class="by">jungturk</span><span>|</span><a href="#39561877">parent</a><span>|</span><a href="#39562027">prev</a><span>|</span><a href="#39563268">next</a><span>|</span><label class="collapse" for="c-39567314">[-]</label><label class="expand" for="c-39567314">[1 more]</label></div><br/><div class="children"><div class="content">&gt; now wondering...if there&#x27;s some conserved quantity in the neural network that is _directly analagous_ to conserved quantities in physics<p>Isn&#x27;t the model attempting to conserve information during training?  And isn&#x27;t information a physical quantity?</div><br/></div></div><div id="39562479" class="c"><input type="checkbox" id="c-39562479" checked=""/><div class="controls bullet"><span class="by">Communitivity</span><span>|</span><a href="#39561877">parent</a><span>|</span><a href="#39563268">prev</a><span>|</span><a href="#39562606">next</a><span>|</span><label class="collapse" for="c-39562479">[-]</label><label class="expand" for="c-39562479">[1 more]</label></div><br/><div class="children"><div class="content">&quot;I&#x27;m now wondering if in something like Sora that can do a kind of physical modeling, if there&#x27;s some conserved quantity in the neural network that is _directly analogous_ to conserved quantities in physics&quot;<p>My first thought on reading that was that if there was it would be interesting to see if there was some way it tied into the concept of us living in a simulation, i.e. we&#x27;re all living in a complex ML network simulation.</div><br/></div></div></div></div><div id="39562606" class="c"><input type="checkbox" id="c-39562606" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#39561877">prev</a><span>|</span><a href="#39562911">next</a><span>|</span><label class="collapse" for="c-39562606">[-]</label><label class="expand" for="c-39562606">[2 more]</label></div><br/><div class="children"><div class="content">People have mentioned the discrete - continuous tradeoff. One way to bridge that gap would be to use <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1806.07366" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1806.07366</a> - they draw an equivalence between vanilla (FC layer) neural nets of constant width with differential equations, and then use a differential equation solver to &quot;train&quot; a &quot;neural net&quot; (from what I remember - it&#x27;s been years since that paper...).<p>Another approach might be to take an information theoretic view with the infinite-width finite-entropy nets.</div><br/><div id="39562863" class="c"><input type="checkbox" id="c-39562863" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#39562606">parent</a><span>|</span><a href="#39562911">next</a><span>|</span><label class="collapse" for="c-39562863">[-]</label><label class="expand" for="c-39562863">[1 more]</label></div><br/><div class="children"><div class="content">Another angle to look at would be the S4 models, which admit both a continuous time and recurrent discrete representation.</div><br/></div></div></div></div><div id="39562911" class="c"><input type="checkbox" id="c-39562911" checked=""/><div class="controls bullet"><span class="by">pmayrgundter</span><span>|</span><a href="#39562606">prev</a><span>|</span><a href="#39563075">next</a><span>|</span><label class="collapse" for="c-39562911">[-]</label><label class="expand" for="c-39562911">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if an energy and work metric could be derived for gradient descent.  This might be useful for a more rigorous approach to hyperparameter development, and maybe for characterizing the data being learned. We say that some datasets are harder to learn, or measure difficulty by the overall compute needed to hit a quality benchmark.  Something more essential would be a step forward.<p>Like in ANN backprop, the gradient descent algorithm can use a momentum to overcome getting stuck in local minima.  This was heuristically physical when I learned it.. perhaps it&#x27;s been developed since.  Maybe only allowing a &quot;real&quot; energy to the momentum would then align it with an ability to do work calculation.  Might also help with ensemble&#x2F;monte carlo methods, to maintain an energy account across the ensemble.</div><br/></div></div><div id="39563075" class="c"><input type="checkbox" id="c-39563075" checked=""/><div class="controls bullet"><span class="by">irchans</span><span>|</span><a href="#39562911">prev</a><span>|</span><a href="#39564615">next</a><span>|</span><label class="collapse" for="c-39563075">[-]</label><label class="expand" for="c-39563075">[6 more]</label></div><br/><div class="children"><div class="content">I liked the article and I hope that I can understand it more with some study.<p>I think the following sentence in the article is wrong
&quot;Applying Noether&#x27;s theorem gives us three conserved quantities—one for each degree of freedom in our group of transformations—which turn out to be horizontal, vertical, and angular momentum.”<p>I think the correct statement is 
&quot;Applying Noether&#x27;s theorem gives us three conserved quantities—one for each degree of freedom in our group of transformations—which turn out to be translation, rotation, and time shifting.”<p>I think translation leads to conservation of momentum, rotation leads to conservation of angular momentum, and time shifting leads to conservation of energy (potential+kinetic).  It&#x27;s been a few decades since I saw the proof, so I might be wrong.</div><br/><div id="39564120" class="c"><input type="checkbox" id="c-39564120" checked=""/><div class="controls bullet"><span class="by">cgadski</span><span>|</span><a href="#39563075">parent</a><span>|</span><a href="#39563468">next</a><span>|</span><label class="collapse" for="c-39564120">[-]</label><label class="expand" for="c-39564120">[1 more]</label></div><br/><div class="children"><div class="content">Hi, thanks!<p>In that sentence I was only talking about the translations and rotations of the plane as a group of invariances for the action of the two-body problem. This group is generated by one-parameter subgroups producing vertical translation, horizontal translation, and rotation about a particular point. Those are the &quot;three degrees of freedom&quot; I was counting.<p>You&#x27;re right about the correspondence from symmetries to conservation laws in general.</div><br/></div></div><div id="39563468" class="c"><input type="checkbox" id="c-39563468" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#39563075">parent</a><span>|</span><a href="#39564120">prev</a><span>|</span><a href="#39563724">next</a><span>|</span><label class="collapse" for="c-39563468">[-]</label><label class="expand" for="c-39563468">[1 more]</label></div><br/><div class="children"><div class="content">I think your last paragraph is correct, but the statement in the article is referring to the specific 2D 2-body example given, and its original phrasing is also correct.  Translation, rotation, and time-shifting are <i>transformations</i> (matrices), not <i>quantities</i>.  Horizontal, vertical, and angular (2D) momentum are scalars.  The article is saying that if you take the action potential given in the example, there exist scalar quantities (which we call horizontal momentum, vertical momentum, and angular momentum) that remain constant regardless of any horizontal, vertical, or rotational transformation of the coordinate system used to measure the 2-body problem.</div><br/></div></div><div id="39563724" class="c"><input type="checkbox" id="c-39563724" checked=""/><div class="controls bullet"><span class="by">kurthr</span><span>|</span><a href="#39563075">parent</a><span>|</span><a href="#39563468">prev</a><span>|</span><a href="#39563300">next</a><span>|</span><label class="collapse" for="c-39563724">[-]</label><label class="expand" for="c-39563724">[1 more]</label></div><br/><div class="children"><div class="content">The application of Noether&#x27;s theorem in this case refers only to the energy integral shown (KE = ME - GPE for 2D Kinetic Mechanical and Gravitational Potential Energies) over time. It&#x27;s really only for that particular 2 body 2 dimensional problem.<p>More generically in 3 dimensions a transformation with 3 translational 2 rotational and 1 time independence would provide conservation of 3 momenta 2 angular momenta and 1 energy.</div><br/></div></div><div id="39563300" class="c"><input type="checkbox" id="c-39563300" checked=""/><div class="controls bullet"><span class="by">chunky1994</span><span>|</span><a href="#39563075">parent</a><span>|</span><a href="#39563724">prev</a><span>|</span><a href="#39566243">next</a><span>|</span><label class="collapse" for="c-39563300">[-]</label><label class="expand" for="c-39563300">[1 more]</label></div><br/><div class="children"><div class="content">Right, the rephrasing of the sentence is a tad more accurate. Your three entities are [invariant -&gt; conserved quantity]: (translation -&gt; momentum), (rotation -&gt; angular momentum) and (time -&gt; energy).</div><br/></div></div><div id="39566243" class="c"><input type="checkbox" id="c-39566243" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39563075">parent</a><span>|</span><a href="#39563300">prev</a><span>|</span><a href="#39564615">next</a><span>|</span><label class="collapse" for="c-39566243">[-]</label><label class="expand" for="c-39566243">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll be walking tall the day I can leisurely read articles like this! I wish I had studied this stuff; now time is short.</div><br/></div></div></div></div><div id="39564615" class="c"><input type="checkbox" id="c-39564615" checked=""/><div class="controls bullet"><span class="by">waveBidder</span><span>|</span><a href="#39563075">prev</a><span>|</span><a href="#39569318">next</a><span>|</span><label class="collapse" for="c-39564615">[-]</label><label class="expand" for="c-39564615">[2 more]</label></div><br/><div class="children"><div class="content">so I think this is a great connection that deserves more thought. as well as an absolutely gorgeous write-up.<p>The main problem I see with it is that most of the time you <i>don&#x27;t</i> want the optimum for your objective function, as that frequently results in overfitting. this leads to things like early stopping being typical.</div><br/><div id="39569858" class="c"><input type="checkbox" id="c-39569858" checked=""/><div class="controls bullet"><span class="by">cgadski</span><span>|</span><a href="#39564615">parent</a><span>|</span><a href="#39569318">next</a><span>|</span><label class="collapse" for="c-39569858">[-]</label><label class="expand" for="c-39569858">[1 more]</label></div><br/><div class="children"><div class="content">Thanks so much!<p>And yes, that&#x27;s quite true. When parameter gradients don&#x27;t quite vanish, then the equation<p>&lt;g_x, d x &#x2F; d eps&gt; = &lt;g_y, d y &#x2F; d eps&gt;<p>becomes<p>&lt;g_x, d x &#x2F; d eps&gt; = &lt;g_y, d y &#x2F; d eps&gt; - &lt;g_theta, d theta &#x2F; d eps&gt;<p>where g_theta is the gradient with respect to theta.<p>In defense of my hypothesis that interesting approximate conservation laws exist in practice, I&#x27;d argue that maybe parameter gradients at early stopping are small enough that the last term is pretty small compared to the first two.<p>On the other hand, stepping back, the condition that our network parameters are approximately stationary for a loss function feels pretty... shallow. My impression of deep learning is that an optimized model _cannot_ be understood as just &quot;some solution to an optimization problem,&quot; but is more like a sample from a Boltzmann distribution which happens to concentrate a lot of its probability mass around _certain_ minimizers of an energy. So, if we can prove something that is true for neural networks simply because they&#x27;re &quot;near stationary points&quot;, we probably aren&#x27;t saying anything very fundamental about deep learning.</div><br/></div></div></div></div><div id="39569318" class="c"><input type="checkbox" id="c-39569318" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#39564615">prev</a><span>|</span><a href="#39562309">next</a><span>|</span><label class="collapse" for="c-39569318">[-]</label><label class="expand" for="c-39569318">[1 more]</label></div><br/><div class="children"><div class="content">Completely irrelevant but I love the way the color theme on this blog feels like a chalk board</div><br/></div></div><div id="39562309" class="c"><input type="checkbox" id="c-39562309" checked=""/><div class="controls bullet"><span class="by">r34</span><span>|</span><a href="#39569318">prev</a><span>|</span><a href="#39564977">next</a><span>|</span><label class="collapse" for="c-39562309">[-]</label><label class="expand" for="c-39562309">[7 more]</label></div><br/><div class="children"><div class="content">As a complete amateur I was wondering if it could be possible to use that property of light (&quot;to always choose the most optimal route&quot;) to solve the traveling salesman problem (and the whole class of those problems as a consequence). Maybe not with an algorithmic approach, but rather some smart implementation of the machine itself.</div><br/><div id="39562391" class="c"><input type="checkbox" id="c-39562391" checked=""/><div class="controls bullet"><span class="by">shiandow</span><span>|</span><a href="#39562309">parent</a><span>|</span><a href="#39562643">next</a><span>|</span><label class="collapse" for="c-39562391">[-]</label><label class="expand" for="c-39562391">[1 more]</label></div><br/><div class="children"><div class="content">If somehow you can ensure that light can only reach a point by travelling through all other points then yes.<p>It&#x27;s basically the same way you could use light to solve a maze, just flood the exit with light and walk in the direction which is brightest. Works better for mirror mazes.</div><br/></div></div><div id="39562643" class="c"><input type="checkbox" id="c-39562643" checked=""/><div class="controls bullet"><span class="by">pvg</span><span>|</span><a href="#39562309">parent</a><span>|</span><a href="#39562391">prev</a><span>|</span><a href="#39563350">next</a><span>|</span><label class="collapse" for="c-39562643">[-]</label><label class="expand" for="c-39562643">[2 more]</label></div><br/><div class="children"><div class="content">Google up &#x27;soap film steiner tree&#x27; for a fun, well-known variant of this.</div><br/><div id="39568179" class="c"><input type="checkbox" id="c-39568179" checked=""/><div class="controls bullet"><span class="by">jerf</span><span>|</span><a href="#39562309">root</a><span>|</span><a href="#39562643">parent</a><span>|</span><a href="#39563350">next</a><span>|</span><label class="collapse" for="c-39568179">[-]</label><label class="expand" for="c-39568179">[1 more]</label></div><br/><div class="children"><div class="content">Then follow it up with <a href="https:&#x2F;&#x2F;www.scottaaronson.com&#x2F;papers&#x2F;npcomplete.pdf" rel="nofollow">https:&#x2F;&#x2F;www.scottaaronson.com&#x2F;papers&#x2F;npcomplete.pdf</a> . While reality can &quot;solve&quot; these problems to some extent it turns out that people overestimate reality&#x27;s ability to solve it <i>optimally</i>.</div><br/></div></div></div></div><div id="39563350" class="c"><input type="checkbox" id="c-39563350" checked=""/><div class="controls bullet"><span class="by">richk449</span><span>|</span><a href="#39562309">parent</a><span>|</span><a href="#39562643">prev</a><span>|</span><a href="#39562341">next</a><span>|</span><label class="collapse" for="c-39563350">[-]</label><label class="expand" for="c-39563350">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like some of the trendy analogy computing approaches, like this one for example:<p><a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;uploads&#x2F;prod&#x2F;2023&#x2F;05&#x2F;Analog-Iterative-Machine-AIM-using-light-to-solve-quadratic-optimization-problems-with-mixed-variables.pdf" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;uploads&#x2F;prod&#x2F;2023&#x2F;0...</a></div><br/></div></div><div id="39562341" class="c"><input type="checkbox" id="c-39562341" checked=""/><div class="controls bullet"><span class="by">nkozyra</span><span>|</span><a href="#39562309">parent</a><span>|</span><a href="#39563350">prev</a><span>|</span><a href="#39564977">next</a><span>|</span><label class="collapse" for="c-39562341">[-]</label><label class="expand" for="c-39562341">[2 more]</label></div><br/><div class="children"><div class="content">This sounds a bit like LIDAR implementations, I assume you mean something similar at a smaller scale, where physical obstacles provide a &quot;path&quot; representation of a problem space?</div><br/><div id="39562401" class="c"><input type="checkbox" id="c-39562401" checked=""/><div class="controls bullet"><span class="by">r34</span><span>|</span><a href="#39562309">root</a><span>|</span><a href="#39562341">parent</a><span>|</span><a href="#39564977">next</a><span>|</span><label class="collapse" for="c-39562401">[-]</label><label class="expand" for="c-39562401">[1 more]</label></div><br/><div class="children"><div class="content">Yup, something like that came to my mind first: create a physical representation (like a map) of the graph you want to solve and use physics to determine the shortest path. Once you have it you could easily compute the winning path&#x27;s length etc.</div><br/></div></div></div></div></div></div><div id="39564977" class="c"><input type="checkbox" id="c-39564977" checked=""/><div class="controls bullet"><span class="by">raptortech</span><span>|</span><a href="#39562309">prev</a><span>|</span><a href="#39562000">next</a><span>|</span><label class="collapse" for="c-39564977">[-]</label><label class="expand" for="c-39564977">[1 more]</label></div><br/><div class="children"><div class="content">See also &quot;Noether Networks: Meta-Learning Useful Conserved Quantities&quot; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2112.03321" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2112.03321</a> from 2021.<p>Abstract: Progress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether&#x27;s theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.</div><br/></div></div><div id="39562000" class="c"><input type="checkbox" id="c-39562000" checked=""/><div class="controls bullet"><span class="by">platz</span><span>|</span><a href="#39564977">prev</a><span>|</span><a href="#39564414">next</a><span>|</span><label class="collapse" for="c-39562000">[-]</label><label class="expand" for="c-39562000">[2 more]</label></div><br/><div class="children"><div class="content">how do you direct what the network learns if it all comes from supervised learning training sets?<p>How do you insert rules that aren&#x27;t learned into what weights are learned?</div><br/><div id="39563236" class="c"><input type="checkbox" id="c-39563236" checked=""/><div class="controls bullet"><span class="by">nrub</span><span>|</span><a href="#39562000">parent</a><span>|</span><a href="#39564414">next</a><span>|</span><label class="collapse" for="c-39563236">[-]</label><label class="expand" for="c-39563236">[1 more]</label></div><br/><div class="children"><div class="content">There are promising methods developing for Physic&#x27;s informed neural networks. Mathematical models can be integrated into the architecture of neural networks such that the parameters of the designed mathematical models can be learned. Examples include learning the frequency of a swinging pendulum from video, amongst more advanced ideas.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Physics-informed_neural_networks" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Physics-informed_neural_networ...</a>
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=JoFW2uSd3Uo" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=JoFW2uSd3Uo</a></div><br/></div></div></div></div><div id="39563274" class="c"><input type="checkbox" id="c-39563274" checked=""/><div class="controls bullet"><span class="by">iskander</span><span>|</span><a href="#39564414">prev</a><span>|</span><a href="#39562001">next</a><span>|</span><label class="collapse" for="c-39563274">[-]</label><label class="expand" for="c-39563274">[4 more]</label></div><br/><div class="children"><div class="content">I love the simple but elegant formatting of this blog.<p>cgadski: what did you use to make it?</div><br/><div id="39569897" class="c"><input type="checkbox" id="c-39569897" checked=""/><div class="controls bullet"><span class="by">cgadski</span><span>|</span><a href="#39563274">parent</a><span>|</span><a href="#39566393">next</a><span>|</span><label class="collapse" for="c-39569897">[-]</label><label class="expand" for="c-39569897">[1 more]</label></div><br/><div class="children"><div class="content">Thank you!<p>In the beginning, I used kognise&#x27;s water.css [1], so most of the smart decisions (background&#x2F;text color, margins, line spacing I think) probably come from there. Since then it&#x27;s been some amount of little adjustments. The font is by Jean François Porchez, called Le Monde Livre Classic [2].<p>I draft in Obsidian [3] and build the site with a couple python scripts and KaTeX.<p>[1] <a href="https:&#x2F;&#x2F;watercss.kognise.dev&#x2F;" rel="nofollow">https:&#x2F;&#x2F;watercss.kognise.dev&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;typofonderie.com&#x2F;fr&#x2F;fonts&#x2F;le-monde-livre-classic" rel="nofollow">https:&#x2F;&#x2F;typofonderie.com&#x2F;fr&#x2F;fonts&#x2F;le-monde-livre-classic</a><p>[3] <a href="https:&#x2F;&#x2F;obsidian.md&#x2F;" rel="nofollow">https:&#x2F;&#x2F;obsidian.md&#x2F;</a></div><br/></div></div><div id="39566393" class="c"><input type="checkbox" id="c-39566393" checked=""/><div class="controls bullet"><span class="by">iskander</span><span>|</span><a href="#39563274">parent</a><span>|</span><a href="#39569897">prev</a><span>|</span><a href="#39567931">next</a><span>|</span><label class="collapse" for="c-39566393">[-]</label><label class="expand" for="c-39566393">[1 more]</label></div><br/><div class="children"><div class="content">Only clue in the source:<p>&lt;!-- this blog is proudly generated by, like, GNU make --&gt;</div><br/></div></div></div></div><div id="39562001" class="c"><input type="checkbox" id="c-39562001" checked=""/><div class="controls bullet"><span class="by">brodolage</span><span>|</span><a href="#39563274">prev</a><span>|</span><a href="#39562654">next</a><span>|</span><label class="collapse" for="c-39562001">[-]</label><label class="expand" for="c-39562001">[5 more]</label></div><br/><div class="children"><div class="content">How does he create those animations? I&#x27;d like to make them as well for myself.</div><br/><div id="39562236" class="c"><input type="checkbox" id="c-39562236" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#39562001">parent</a><span>|</span><a href="#39567072">next</a><span>|</span><label class="collapse" for="c-39562236">[-]</label><label class="expand" for="c-39562236">[3 more]</label></div><br/><div class="children"><div class="content">They seem to be built with some love by the author.  Apparently they have written it in Haxe, judging from the comment in the page source.</div><br/><div id="39570006" class="c"><input type="checkbox" id="c-39570006" checked=""/><div class="controls bullet"><span class="by">cgadski</span><span>|</span><a href="#39562001">root</a><span>|</span><a href="#39562236">parent</a><span>|</span><a href="#39564545">next</a><span>|</span><label class="collapse" for="c-39570006">[-]</label><label class="expand" for="c-39570006">[1 more]</label></div><br/><div class="children"><div class="content">Haha yeah, took some love. I have a scrappy little &quot;framework&quot; that I&#x27;ve been adjusting since I started making interactive posts last year. Writing my interactive widgets feels a bit like doing a game jam now: just copy a template and start compiling+reloading the page, seeing what I can get onto the screen. I&#x27;ve just been using the canvas2d API.<p>Besides figuring out a good way of dealing with reference frames, the only trick I&#x27;d pass on is to use CSS variables to change colors and sizes (line widths, arrow dimensions, etc.) interactively. It definitely helps to tighten the feedback loop on those decisions.</div><br/></div></div><div id="39564545" class="c"><input type="checkbox" id="c-39564545" checked=""/><div class="controls bullet"><span class="by">brodolage</span><span>|</span><a href="#39562001">root</a><span>|</span><a href="#39562236">parent</a><span>|</span><a href="#39570006">prev</a><span>|</span><a href="#39567072">next</a><span>|</span><label class="collapse" for="c-39564545">[-]</label><label class="expand" for="c-39564545">[1 more]</label></div><br/><div class="children"><div class="content">Oh that&#x27;s way out of my league unfortunately. I wonder if there&#x27;s a library or something that does something like this.</div><br/></div></div></div></div><div id="39567072" class="c"><input type="checkbox" id="c-39567072" checked=""/><div class="controls bullet"><span class="by">aeonik</span><span>|</span><a href="#39562001">parent</a><span>|</span><a href="#39562236">prev</a><span>|</span><a href="#39562654">next</a><span>|</span><label class="collapse" for="c-39567072">[-]</label><label class="expand" for="c-39567072">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to know too.<p>I&#x27;ve been using Emmy from the Clojurescript ecosystem, which works pretty good, but has a few quirks.<p><a href="https:&#x2F;&#x2F;emmy-viewers.mentat.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;emmy-viewers.mentat.org&#x2F;</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>