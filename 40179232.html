<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714294855675" as="style"/><link rel="stylesheet" href="styles.css?v=1714294855675"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.strangeloopcanon.com/p/what-can-llms-never-do">What can LLMs never do?</a> <span class="domain">(<a href="https://www.strangeloopcanon.com">www.strangeloopcanon.com</a>)</span></div><div class="subtext"><span>henrik_w</span> | <span>207 comments</span></div><br/><div><div id="40185761" class="c"><input type="checkbox" id="c-40185761" checked=""/><div class="controls bullet"><span class="by">yosito</span><span>|</span><a href="#40179731">next</a><span>|</span><label class="collapse" for="c-40185761">[-]</label><label class="expand" for="c-40185761">[4 more]</label></div><br/><div class="children"><div class="content">&gt; If there exist classes of problems that someone in an elementary school can easily solve but a trillion-token billion-dollar sophisticated model cannot solve, what does that tell us about the nature of our cognition?<p>I think what it tells us is that our cognition is capable of more than just language modeling. With LLMs we are discovering (amazing) capabilities and the limits of language models. While language models can do incredible things with language that humans can&#x27;t, they still can&#x27;t do something simple like sudoku. But there are neural networks, CNNs and RNNs that can solve sudoku better than humans can. I think that the thing to learn here is that some problems are in the domain of language models, and some problems are a better fit for other forms of cognition. The human brain is amazing in that it combines several forms of cognition in an integrated way.<p>One thing that I think LLMs have the capability to do is to integrate several types of systems and to choose the right one to solve a problem. Teach an LLM how to interface with a CNN that solves sudoku problems, and then ask it a sudoku problem.<p>It seems to me that if we want to create an AGI, we need to learn how to integrate several different types of models, and teach them how to distribute the tasks we give them to the correct models.</div><br/><div id="40187029" class="c"><input type="checkbox" id="c-40187029" checked=""/><div class="controls bullet"><span class="by">keefle</span><span>|</span><a href="#40185761">parent</a><span>|</span><a href="#40186120">next</a><span>|</span><label class="collapse" for="c-40187029">[-]</label><label class="expand" for="c-40187029">[2 more]</label></div><br/><div class="children"><div class="content">But even without going this far (with integrating various other specialized or having an LLM use them when required), an LLM is probably able to recognize a sudoku puzzle when it sees one, and even tho it itself can&#x27;t solve it, I think it can easily write the code that would solve sudoku. So instead of hooking it to a set of pre built models, it might be enough to hook it to a python interpreter</div><br/><div id="40187087" class="c"><input type="checkbox" id="c-40187087" checked=""/><div class="controls bullet"><span class="by">yosito</span><span>|</span><a href="#40185761">root</a><span>|</span><a href="#40187029">parent</a><span>|</span><a href="#40186120">next</a><span>|</span><label class="collapse" for="c-40187087">[-]</label><label class="expand" for="c-40187087">[1 more]</label></div><br/><div class="children"><div class="content">Many LLMs are already linked to Python interpreters, but they still need some improvement with recognizing when they need to write some code to solve a problem.</div><br/></div></div></div></div><div id="40186120" class="c"><input type="checkbox" id="c-40186120" checked=""/><div class="controls bullet"><span class="by">kevindamm</span><span>|</span><a href="#40185761">parent</a><span>|</span><a href="#40187029">prev</a><span>|</span><a href="#40179731">next</a><span>|</span><label class="collapse" for="c-40186120">[-]</label><label class="expand" for="c-40186120">[1 more]</label></div><br/><div class="children"><div class="content">What about sudoku makes it a good fit for CNNs? Or do you mean the machine vision for converting the pixels into an awareness of the sudoku puzzle&#x27;s initial conditions?<p>A relatively simple graph theory algorithm can solve it (and at multiple orders of magnitude fewer calculations).  Even a naive brute force search is considered tractable, considering the problem size.  Although, search could be considered one of the AI tools in your proposed toolbox.</div><br/></div></div></div></div><div id="40179731" class="c"><input type="checkbox" id="c-40179731" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40185761">prev</a><span>|</span><a href="#40180256">next</a><span>|</span><label class="collapse" for="c-40179731">[-]</label><label class="expand" for="c-40179731">[67 more]</label></div><br/><div class="children"><div class="content">Fantastic essay. Highly recommended!<p>I agree with all key points:<p>* There are problems that are easy for human beings but hard for <i>current</i> LLMs (and maybe impossible for them; no one knows). Examples include playing Wordle and predicting cellular automata (including Turing-complete ones like Rule 110). We don&#x27;t fully understand why <i>current</i> LLMs are bad at these tasks.<p>* Providing an LLM with examples and step-by-step instructions in a prompt means <i>the user</i> is figuring out the &quot;reasoning steps&quot; and <i>handing them to the LLM</i>, instead of the LLM figuring them out by itself. We have &quot;reasoning machines&quot; that are intelligent but seem to be hitting fundamental limits we don&#x27;t understand.<p>* It&#x27;s unclear if better prompting and bigger models using existing attention mechanisms can achieve AGI. As a model of computation, attention is very rigid, whereas human brains are always undergoing synaptic plasticity. There may be a more flexible architecture capable of AGI, but we don&#x27;t know it yet.<p>* For now, using current AI models <i>requires</i> carefully constructing long prompts with right and wrong answers for computational problems, priming the model to reply appropriately, and applying lots of external guardrails (e.g., LLMs acting as agents that review and vote on the answers of other LLMs).<p>* Attention seems to suffer from &quot;goal drift,&quot; making reliability hard without all that external scaffolding.<p>Go read the whole thing.</div><br/><div id="40185673" class="c"><input type="checkbox" id="c-40185673" checked=""/><div class="controls bullet"><span class="by">weitendorf</span><span>|</span><a href="#40179731">parent</a><span>|</span><a href="#40183574">next</a><span>|</span><label class="collapse" for="c-40185673">[-]</label><label class="expand" for="c-40185673">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There are problems that are easy for human beings but hard for current LLMs (and maybe impossible for them; no one knows). Examples include playing Wordle and predicting cellular automata (including Turing-complete ones like Rule 110). We don&#x27;t fully understand why current LLMs are bad at these tasks.<p>Wordle and cellular automata are very 2D, and LLMs are fundamentally 1D. You might think &quot;but what about Chess!&quot; - except Chess is encoded extremely often as a 1D stream of tokens to notate games, and bound to be highly represented in LLMs&#x27; training sets. Wordle and cellular automata are not often, if ever, encoded as 1D streams of tokens - it&#x27;s not something an LLM would be experienced with even if they had a reasonable &quot;understanding&quot; of the concepts. Imagine being an OK chess player, being asked to play a game blindfolded dictating your moves purely via notation, and being told you suck.<p>&gt; Providing an LLM with examples and step-by-step instructions in a prompt means the user is figuring out the &quot;reasoning steps&quot; and handing them to the LLM, instead of the LLM figuring them out by itself. We have &quot;reasoning machines&quot; that are intelligent but seem to be hitting fundamental limits we don&#x27;t understand.<p>You have probably heard of this really popular game called Bridge before, right? You might even be able to remember tons of advice your Grandma gave you based on her experience playing it - except she never let you watch it directly. Is Grandma &quot;figuring out the game&quot; for you when she finally sits down and teaches you the rules?</div><br/></div></div><div id="40183574" class="c"><input type="checkbox" id="c-40183574" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40179731">parent</a><span>|</span><a href="#40185673">prev</a><span>|</span><a href="#40183525">next</a><span>|</span><label class="collapse" for="c-40183574">[-]</label><label class="expand" for="c-40183574">[21 more]</label></div><br/><div class="children"><div class="content">&gt; There are problems that are easy for human beings but hard for current LLMs (and maybe impossible for them; no one knows). Examples include playing Wordle and predicting cellular automata (including Turing-complete ones like Rule 110). We don’t fully understand why current LLMs are bad at these tasks.<p>I thought we did know for things like playing Wordle, that its because they deal with words as sequence of tokens that correspond to whole words not sequences of letters, so a game that involves dealing with sequences of letters constrained to those that are valid words doesn’t match the way they process information?<p>&gt; Providing an LLM with examples and step-by-step instructions in a prompt means the user is figuring out the “reasoning steps” and handing them to the LLM, instead of the LLM figuring them out by itself. We have “reasoning machines” that are intelligent but seem to be hitting fundamental limits we don’t understand.<p>But providing examples with <i>different</i>, contextually-appropriate sets of reasoning steps results can enable the model to choose its own, more-or-less appropriate, set of reasoning steps for particular questions not matching the examples.<p>&gt; It’s unclear if better prompting and bigger models using existing attention mechanisms can achieve AGI.<p>Since there is no objective definition of AGI or test for it, there’s no basis for any meaningful speculation on what can or cannot achieve it; discussions about it are quasi-religious, not scientific.</div><br/><div id="40184664" class="c"><input type="checkbox" id="c-40184664" checked=""/><div class="controls bullet"><span class="by">rainsford</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183574">parent</a><span>|</span><a href="#40183841">next</a><span>|</span><label class="collapse" for="c-40184664">[-]</label><label class="expand" for="c-40184664">[3 more]</label></div><br/><div class="children"><div class="content">Arriving at a generally accepted scientific definition of AGI might be difficult, but a more achievable goal might be to arrive at a scientific way to determine something is <i>not</i> AGI.  And while I&#x27;m not an expert in the field, I would certainly think a strong contender for relevant criteria would be an inability to process information in a way other than the one a system was explicitly programmed to, even if the new way of processing information was very related to the pre-existing method.  Most humans playing Wordle for the first time probably weren&#x27;t used to thinking about words that way either, but they were able to adapt because they actually understand how letters and words work.<p>I&#x27;m sure one could train an LLM to be awesome at Wordle, but from an AGI perspective the fact that you&#x27;d have to do so proves it&#x27;s not a path to AGI.  The Wordle dominating LLM would presumably be perplexed by the next clever word game until trained on thinking about information <i>that</i> way, while a human doesn&#x27;t need to absorb billions of examples to figure it out.<p>I was originally pretty bullish on LLMs, but now I&#x27;m equally convinced that while they probably have some interesting applications, they&#x27;re a dead-end from a legitimate AGI perspective.</div><br/><div id="40184934" class="c"><input type="checkbox" id="c-40184934" checked=""/><div class="controls bullet"><span class="by">lukan</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184664">parent</a><span>|</span><a href="#40185466">next</a><span>|</span><label class="collapse" for="c-40184934">[-]</label><label class="expand" for="c-40184934">[1 more]</label></div><br/><div class="children"><div class="content">&quot;they&#x27;re a dead-end from a legitimate AGI perspective&quot;<p>Or another piece of the puzzle to achieve it. It might not be one true path, but a clever combination of existing working pieces where (different) LLMs are one or some of those pieces.<p>I believe there is also not only one way of thinking in the human brain, but my thought processes happen on different levels and maybe based on different mechanism. But as far as I know, we lack details.</div><br/></div></div><div id="40185466" class="c"><input type="checkbox" id="c-40185466" checked=""/><div class="controls bullet"><span class="by">JoshuaDavid</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184664">parent</a><span>|</span><a href="#40184934">prev</a><span>|</span><a href="#40183841">next</a><span>|</span><label class="collapse" for="c-40185466">[-]</label><label class="expand" for="c-40185466">[1 more]</label></div><br/><div class="children"><div class="content">What about an LLM that can&#x27;t play wordle itself without being trained on it, but <i>can</i> write and use a wordle solver upon seeing the wordle rules?<p>I think &quot;can recognize what tools are needed to solve a problem, build those tools, and use those tools&quot; would count as a &quot;path to AGI&quot;.</div><br/></div></div></div></div><div id="40183841" class="c"><input type="checkbox" id="c-40183841" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183574">parent</a><span>|</span><a href="#40184664">prev</a><span>|</span><a href="#40184060">next</a><span>|</span><label class="collapse" for="c-40183841">[-]</label><label class="expand" for="c-40183841">[13 more]</label></div><br/><div class="children"><div class="content">I think one should feel comfortable arguing that AGI must be stateful and experience continuous time at least. Such that a plain old LLM is definitively not ever going to be AGI; but an LLM called in a do while true for loop might.</div><br/><div id="40185152" class="c"><input type="checkbox" id="c-40185152" checked=""/><div class="controls bullet"><span class="by">PopePompus</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183841">parent</a><span>|</span><a href="#40186255">next</a><span>|</span><label class="collapse" for="c-40185152">[-]</label><label class="expand" for="c-40185152">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why you believe it must experience continuous time. If you had a system which clearly could reason, which could learn new tasks on its own, which didn&#x27;t hallucinate any more than humans do, but it was only active for the period required for it to complete an assigned task, and was completely dormant otherwise, why would that dormant period disqualify it as AGI?   I agree that such a system should probably not be considered conscious, but I think it&#x27;s an open question whether or not consciousness is required for intelligence.</div><br/><div id="40186842" class="c"><input type="checkbox" id="c-40186842" checked=""/><div class="controls bullet"><span class="by">kaibee</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40185152">parent</a><span>|</span><a href="#40186255">next</a><span>|</span><label class="collapse" for="c-40186842">[-]</label><label class="expand" for="c-40186842">[1 more]</label></div><br/><div class="children"><div class="content">I think its note worthy that humans actually fail this test... We have to go dormant for 8 hours every day.</div><br/></div></div></div></div><div id="40186255" class="c"><input type="checkbox" id="c-40186255" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183841">parent</a><span>|</span><a href="#40185152">prev</a><span>|</span><a href="#40186110">next</a><span>|</span><label class="collapse" for="c-40186255">[-]</label><label class="expand" for="c-40186255">[1 more]</label></div><br/><div class="children"><div class="content">Some good prompt-reply interactions are probably fed back in to subsequent training runs, so they&#x27;re still stateful&#x2F;have memory in a way, there&#x27;s just a long delay.</div><br/></div></div><div id="40186110" class="c"><input type="checkbox" id="c-40186110" checked=""/><div class="controls bullet"><span class="by">adrianN</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183841">parent</a><span>|</span><a href="#40186255">prev</a><span>|</span><a href="#40184640">next</a><span>|</span><label class="collapse" for="c-40186110">[-]</label><label class="expand" for="c-40186110">[1 more]</label></div><br/><div class="children"><div class="content">I try my best not to experience continuous time for at least eight hours a day.</div><br/></div></div><div id="40184640" class="c"><input type="checkbox" id="c-40184640" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183841">parent</a><span>|</span><a href="#40186110">prev</a><span>|</span><a href="#40184572">next</a><span>|</span><label class="collapse" for="c-40184640">[-]</label><label class="expand" for="c-40184640">[3 more]</label></div><br/><div class="children"><div class="content">You could imagine an LLM being called in a loop with a prompt like<p>You observe: {new input}<p>You remember: {from previous output}<p>React to this in the following format:<p>My inner thoughts: [what do you think about the current state]<p>I want to remember: [information that is important for your future actions]<p>Things I do: [Actions you want to take]<p>Things I say: [What I want to say to the user]<p>...<p>Not sure if that would qualify as an AGI as we currently define it. Given a sufficiently good LLM with good reasoning capabilities such a setup might be able to  It would be able to do many of the things we currently expect AGIs to be able to do (given a sufficiently good LLM with good reasoning capabilities), including planning and learning new knowledge and new skills (by collecting and storing positive and negative examples in its &quot;memory&quot;). But its learning would be limited, and I&#x27;m sure as soon as it exists we would agree that it&#x27;s not AGI</div><br/><div id="40186685" class="c"><input type="checkbox" id="c-40186685" checked=""/><div class="controls bullet"><span class="by">sophiabits</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184640">parent</a><span>|</span><a href="#40185547">next</a><span>|</span><label class="collapse" for="c-40186685">[-]</label><label class="expand" for="c-40186685">[1 more]</label></div><br/><div class="children"><div class="content">This already exists (in a slightly different prompt format); it&#x27;s the underlying idea behind ReAct: <a href="https:&#x2F;&#x2F;react-lm.github.io" rel="nofollow">https:&#x2F;&#x2F;react-lm.github.io</a><p>As you say, I&#x27;m skeptical this counts as AGI. Although I admit that I don&#x27;t have a particularly rock solid definition of what _would_ constitute true AGI.</div><br/></div></div><div id="40185547" class="c"><input type="checkbox" id="c-40185547" checked=""/><div class="controls bullet"><span class="by">lgas</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184640">parent</a><span>|</span><a href="#40186685">prev</a><span>|</span><a href="#40184572">next</a><span>|</span><label class="collapse" for="c-40185547">[-]</label><label class="expand" for="c-40185547">[1 more]</label></div><br/><div class="children"><div class="content">It works better to give it access to functions to call for actions and remembering stuff, but this approach does provide some interesting results.</div><br/></div></div></div></div><div id="40184572" class="c"><input type="checkbox" id="c-40184572" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183841">parent</a><span>|</span><a href="#40184640">prev</a><span>|</span><a href="#40184060">next</a><span>|</span><label class="collapse" for="c-40184572">[-]</label><label class="expand" for="c-40184572">[5 more]</label></div><br/><div class="children"><div class="content">A consistent stateful experience may be needed, but not sure about continuous time. I mean human consciousness doesn&#x27;t do that.</div><br/><div id="40184670" class="c"><input type="checkbox" id="c-40184670" checked=""/><div class="controls bullet"><span class="by">haswell</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184572">parent</a><span>|</span><a href="#40184611">next</a><span>|</span><label class="collapse" for="c-40184670">[-]</label><label class="expand" for="c-40184670">[3 more]</label></div><br/><div class="children"><div class="content">Human consciousness does though, e.g. the flow state. F1 drivers are a good example.<p>We tend to not experience continuous time because we repeatedly get distracted by our thoughts, but entering the continuous stream of now is possible with practice and is one of the aims of many meditators.</div><br/><div id="40184836" class="c"><input type="checkbox" id="c-40184836" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184670">parent</a><span>|</span><a href="#40185016">next</a><span>|</span><label class="collapse" for="c-40184836">[-]</label><label class="expand" for="c-40184836">[1 more]</label></div><br/><div class="children"><div class="content">Human consciousness is capable of it, but since most humans aren&#x27;t in it much of the time, it would appear that it&#x27;s not a prerequisite for true sentience.</div><br/></div></div><div id="40185016" class="c"><input type="checkbox" id="c-40185016" checked=""/><div class="controls bullet"><span class="by">krisoft</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184670">parent</a><span>|</span><a href="#40184836">prev</a><span>|</span><a href="#40184611">next</a><span>|</span><label class="collapse" for="c-40185016">[-]</label><label class="expand" for="c-40185016">[1 more]</label></div><br/><div class="children"><div class="content">What does it mean to “experience continous time”?<p>How do you know that F1 drivers experience it?</div><br/></div></div></div></div><div id="40184611" class="c"><input type="checkbox" id="c-40184611" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184572">parent</a><span>|</span><a href="#40184670">prev</a><span>|</span><a href="#40184060">next</a><span>|</span><label class="collapse" for="c-40184611">[-]</label><label class="expand" for="c-40184611">[1 more]</label></div><br/><div class="children"><div class="content">I would argue it needs to be at least somewhat continuous. Perhaps discrete on some granularity but if something is just a function waiting to be called it’s not an intelligent entity. The entity is the calling itself.</div><br/></div></div></div></div></div></div><div id="40184060" class="c"><input type="checkbox" id="c-40184060" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183574">parent</a><span>|</span><a href="#40183841">prev</a><span>|</span><a href="#40183525">next</a><span>|</span><label class="collapse" for="c-40184060">[-]</label><label class="expand" for="c-40184060">[4 more]</label></div><br/><div class="children"><div class="content">Regarding Wordle, it should be straightforward to make a token-based version of it, and I would assume that that has been tried. It seems the obvious thing to do when one is interested in the reasoning abilities necessary for Wordle.</div><br/><div id="40184907" class="c"><input type="checkbox" id="c-40184907" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184060">parent</a><span>|</span><a href="#40184497">next</a><span>|</span><label class="collapse" for="c-40184907">[-]</label><label class="expand" for="c-40184907">[2 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t seem straightforward - although it&#x27;s blind to letters because all it sees are tokens, it doesn&#x27;t have much training data ABOUT tokens.</div><br/><div id="40185401" class="c"><input type="checkbox" id="c-40185401" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184907">parent</a><span>|</span><a href="#40184497">next</a><span>|</span><label class="collapse" for="c-40185401">[-]</label><label class="expand" for="c-40185401">[1 more]</label></div><br/><div class="children"><div class="content">What parent is saying is that instead of asking the LLM to play a game of Wordle with tokens like TIME,LIME we ask it to play with tokens like T,I,M,E,L. This is easy to do.</div><br/></div></div></div></div></div></div></div></div><div id="40183525" class="c"><input type="checkbox" id="c-40183525" checked=""/><div class="controls bullet"><span class="by">_wire_</span><span>|</span><a href="#40179731">parent</a><span>|</span><a href="#40183574">prev</a><span>|</span><a href="#40183740">next</a><span>|</span><label class="collapse" for="c-40183525">[-]</label><label class="expand" for="c-40183525">[26 more]</label></div><br/><div class="children"><div class="content">&gt;We don&#x27;t fully understand why current LLMs are bad at these tasks.<p>In complete seriousness, can anyone can explain why LLMs are good at some tasks?</div><br/><div id="40183987" class="c"><input type="checkbox" id="c-40183987" checked=""/><div class="controls bullet"><span class="by">SomeCallMeTim</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183525">parent</a><span>|</span><a href="#40183929">next</a><span>|</span><label class="collapse" for="c-40183987">[-]</label><label class="expand" for="c-40183987">[11 more]</label></div><br/><div class="children"><div class="content">LLMs are good at tasks that don&#x27;t require actual understanding of the topic.<p>They can come up with excellent (or excellent-looking-but-wrong) answers to any question that their training corpus covers. In a gross oversimplification, the &quot;reasoning&quot; they do is really just parroting a weighted average (with randomness injected) of the matching training data.<p>What they&#x27;re doing doesn&#x27;t really match any definition of &quot;understanding.&quot; An LLM (and any current AI) doesn&#x27;t &quot;understand&quot; anything; it&#x27;s effectively no more than a really big, really complicated spreadsheet. And no matter how complicated a spreadsheet gets, it&#x27;s never going to understand anything.<p>Not until we find the secret to actual learning. And increasingly it looks like actual learning probably relies on some of the quantum phenomena that are known to be present in the brain.<p>We may not even have the science yet to understand how the brain learns. But I have become convinced that we&#x27;re not going to find a way for digital-logic-based computers to bridge that gap.</div><br/><div id="40185119" class="c"><input type="checkbox" id="c-40185119" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183987">parent</a><span>|</span><a href="#40186267">next</a><span>|</span><label class="collapse" for="c-40185119">[-]</label><label class="expand" for="c-40185119">[1 more]</label></div><br/><div class="children"><div class="content">This is also why image generating models struggle to correctly draw highly variable objects like limbs and digits.<p>They’ll be able to produce infinite good looking cardboard boxes, because those are simple enough to be represented reasonably well with averages of training data. Limbs and digits on the other hand have nearly limitless different configurations and as such require an actual understanding (along with basic principles such as foreshortening and kinetics) to be able to draw well without human guidance.</div><br/></div></div><div id="40186267" class="c"><input type="checkbox" id="c-40186267" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183987">parent</a><span>|</span><a href="#40185119">prev</a><span>|</span><a href="#40184201">next</a><span>|</span><label class="collapse" for="c-40186267">[-]</label><label class="expand" for="c-40186267">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What they&#x27;re doing doesn&#x27;t really match any definition of &quot;understanding.&quot;<p>What is the mechanistic definition of &quot;understanding&quot;?</div><br/></div></div><div id="40184201" class="c"><input type="checkbox" id="c-40184201" checked=""/><div class="controls bullet"><span class="by">danenania</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183987">parent</a><span>|</span><a href="#40186267">prev</a><span>|</span><a href="#40183929">next</a><span>|</span><label class="collapse" for="c-40184201">[-]</label><label class="expand" for="c-40184201">[8 more]</label></div><br/><div class="children"><div class="content">&gt; the &quot;reasoning&quot; they do is really just parroting a weighted average (with randomness injected) of the matching training data<p>Perhaps our brains are doing exactly the same, just with more sophistication?</div><br/><div id="40185870" class="c"><input type="checkbox" id="c-40185870" checked=""/><div class="controls bullet"><span class="by">SomeCallMeTim</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184201">parent</a><span>|</span><a href="#40185442">next</a><span>|</span><label class="collapse" for="c-40185870">[-]</label><label class="expand" for="c-40185870">[4 more]</label></div><br/><div class="children"><div class="content">No.<p>We know how current deep learning neural networks are trained.<p>We know definitively that <i>this is not how brains learn.</i><p>Understanding requires learning. Dynamic learning. In order to <i>experience</i> something, an entity needs to be able to form new memories dynamically.<p>This does not happen anywhere in current tech. It&#x27;s faked in some cases, but no, it doesn&#x27;t really happen.</div><br/><div id="40185929" class="c"><input type="checkbox" id="c-40185929" checked=""/><div class="controls bullet"><span class="by">danenania</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40185870">parent</a><span>|</span><a href="#40186272">next</a><span>|</span><label class="collapse" for="c-40185929">[-]</label><label class="expand" for="c-40185929">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We know definitively that this is not how brains learn.<p>Ok then, I guess the case is closed.<p>&gt; an entity needs to be able to form new memories dynamically.<p>LLMs can form new memories dynamically. Just pop some new data into the context.</div><br/></div></div><div id="40186272" class="c"><input type="checkbox" id="c-40186272" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40185870">parent</a><span>|</span><a href="#40185929">prev</a><span>|</span><a href="#40185442">next</a><span>|</span><label class="collapse" for="c-40186272">[-]</label><label class="expand" for="c-40186272">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We know definitively that this is not how brains learn.<p>So you have mechanistic, formal model of how the brain functions? That&#x27;s news to me.</div><br/><div id="40186372" class="c"><input type="checkbox" id="c-40186372" checked=""/><div class="controls bullet"><span class="by">Scarblac</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40186272">parent</a><span>|</span><a href="#40185442">next</a><span>|</span><label class="collapse" for="c-40186372">[-]</label><label class="expand" for="c-40186372">[1 more]</label></div><br/><div class="children"><div class="content">Your brain was first trained by reading all of the Internet?<p>Anyway, the question of whether computers can think is as interesting as the question whether submarines can swim.</div><br/></div></div></div></div></div></div><div id="40185442" class="c"><input type="checkbox" id="c-40185442" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184201">parent</a><span>|</span><a href="#40185870">prev</a><span>|</span><a href="#40183929">next</a><span>|</span><label class="collapse" for="c-40185442">[-]</label><label class="expand" for="c-40185442">[3 more]</label></div><br/><div class="children"><div class="content">Every single discussion of ‘AGI’ has endless comments exactly like this. Whatever criticism is made of an attempt to produce a reasoning machine, there’s always inevitably someone who says ‘but that’s just what our brains do, duhhh… stop trying to feel special’.<p>It’s boring, and it’s also completely content-free. This particular instance doesn’t even make sense: how can it be <i>exactly the same</i>, yet more sophisticated?<p>Sorry.</div><br/><div id="40186169" class="c"><input type="checkbox" id="c-40186169" checked=""/><div class="controls bullet"><span class="by">adrianN</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40185442">parent</a><span>|</span><a href="#40185746">next</a><span>|</span><label class="collapse" for="c-40186169">[-]</label><label class="expand" for="c-40186169">[1 more]</label></div><br/><div class="children"><div class="content">The problem is that we currently lack good definitions for crucial words such as &quot;understanding&quot; and we don&#x27;t know how brains work, so that nobody can objectively tell whether a spreadsheet &quot;understands&quot; anything better than our brains. That makes these kinds of discussions quite unproductive.</div><br/></div></div><div id="40185746" class="c"><input type="checkbox" id="c-40185746" checked=""/><div class="controls bullet"><span class="by">danenania</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40185442">parent</a><span>|</span><a href="#40186169">prev</a><span>|</span><a href="#40183929">next</a><span>|</span><label class="collapse" for="c-40185746">[-]</label><label class="expand" for="c-40185746">[1 more]</label></div><br/><div class="children"><div class="content">As the comment I replied to very correctly said, we don’t know how the brain produces cognition. So you certainly cannot discard the hypothesis that it works through “parroting” a weighted average of training data just as LLMs are alleged to do.<p>Considering that LLMs with a much smaller number of neurons than the brain are in many cases producing human-level output, there is some evidence, if circumstantial, that our brains may be doing something similar.</div><br/></div></div></div></div></div></div></div></div><div id="40183929" class="c"><input type="checkbox" id="c-40183929" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183525">parent</a><span>|</span><a href="#40183987">prev</a><span>|</span><a href="#40184107">next</a><span>|</span><label class="collapse" for="c-40183929">[-]</label><label class="expand" for="c-40183929">[6 more]</label></div><br/><div class="children"><div class="content">Yes:<p>An LLM isnt a model of human thinking.<p>An LLM is an attempt to build a simulation of human communication. An LLM is to language what a forecast is to weather. No amount of weather data is actually going to turn that simulation into snow, no amount of LLM data is going to create AGI.<p>That having been said, better models (smaller, more flexible ones) are going to result in a LOT of practical uses that have the potential to make our day to day lives easier (think digital personal assistant that has current knowledge).</div><br/><div id="40183991" class="c"><input type="checkbox" id="c-40183991" checked=""/><div class="controls bullet"><span class="by">choeger</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183929">parent</a><span>|</span><a href="#40184943">next</a><span>|</span><label class="collapse" for="c-40183991">[-]</label><label class="expand" for="c-40183991">[2 more]</label></div><br/><div class="children"><div class="content">Great comment. Just one thought: Language, unlike weather, is meta-circular. All we know about specific words or sentences is again encoded in words and sentences. So the embedding encodes a subset of human knowledge.<p>Hence, a LLM is predicting not only language but language with some sort of meaning.</div><br/><div id="40184344" class="c"><input type="checkbox" id="c-40184344" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183991">parent</a><span>|</span><a href="#40184943">next</a><span>|</span><label class="collapse" for="c-40184344">[-]</label><label class="expand" for="c-40184344">[1 more]</label></div><br/><div class="children"><div class="content">That re-embeding is also encoded in weather. It is why perfect forecasting is impossible, why we talk about the butterfly effect.<p>The &quot;hallucination problem&quot; is simply the tyranny of Lorenz... one is not sure if a starting state will have a good outcome or swing wildly.  Some good weather models are based on re-runing with tweaks to starting params, and then things that end up out of bounds can get tossed. Its harder to know when a result is out of bounds for an LLM, and we dont have the ability to run every request 100 times through various models to get an &quot;average&quot; output yet... However some of the reuse of layers does emulate this to an extent....</div><br/></div></div></div></div><div id="40184943" class="c"><input type="checkbox" id="c-40184943" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183929">parent</a><span>|</span><a href="#40183991">prev</a><span>|</span><a href="#40184107">next</a><span>|</span><label class="collapse" for="c-40184943">[-]</label><label class="expand" for="c-40184943">[3 more]</label></div><br/><div class="children"><div class="content">Ugh. Really? Those &quot;simulated water isn&#x27;t wet&quot;(when applied to cognition) &quot;arguments&quot; were punched so many times it even hurts to look at them.</div><br/><div id="40185552" class="c"><input type="checkbox" id="c-40185552" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184943">parent</a><span>|</span><a href="#40184107">next</a><span>|</span><label class="collapse" for="c-40185552">[-]</label><label class="expand" for="c-40185552">[2 more]</label></div><br/><div class="children"><div class="content">No simulated water isnt wet.<p>But an LLM isn&#x27;t even trying to simulate cognition. It&#x27;s a model that is predicting language. It has all the problems of a predictive model... the 
&quot;hallucination&quot; problem is just the tyranny of Lorenz.</div><br/><div id="40186177" class="c"><input type="checkbox" id="c-40186177" checked=""/><div class="controls bullet"><span class="by">adrianN</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40185552">parent</a><span>|</span><a href="#40184107">next</a><span>|</span><label class="collapse" for="c-40186177">[-]</label><label class="expand" for="c-40186177">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t really know what &quot;cognition&quot; is, so it&#x27;s hard to tell whether a system is doing it.</div><br/></div></div></div></div></div></div></div></div><div id="40184107" class="c"><input type="checkbox" id="c-40184107" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183525">parent</a><span>|</span><a href="#40183929">prev</a><span>|</span><a href="#40183947">next</a><span>|</span><label class="collapse" for="c-40184107">[-]</label><label class="expand" for="c-40184107">[4 more]</label></div><br/><div class="children"><div class="content">LLM’s are a compressed and lossy form of our combined writing output, which it turns out is similarly structured enough to make new combinations of text seem reasonable, even enough to display simple reasoning. I find it useful to think “what can I expect from speaking with the dataset of combined writing of people”, rather than treating a basic LLM as a mind.<p>That doesn’t mean we won’t end up approximating one eventually, but it’s going to take a lot of real human thinking first. For example, ChatGPT writes code to solve some questions rather than reasoning about it from text. The LLM is not doing the heavy lifting in that case.<p>Give it (some) 3D questions or anything where there isn’t massive textual datasets and you often need to break out to specialised code.<p>Another thought I find useful is that it considers its job done when it’s produced enough reasonable tokens, not when it’s actually solved a problem. You and I would continue to ponder the edge cases. It’s just happy if there are 1000 tokens that look approximately like its dataset. Agents make that a bit smarter but they’re still limited by the goal of being happy when each has produced the required token quota, missing eg implications that we’d see instantly. Obviously we’re smart enough to keep filling those gaps.</div><br/><div id="40184320" class="c"><input type="checkbox" id="c-40184320" checked=""/><div class="controls bullet"><span class="by">tobiasSoftware</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184107">parent</a><span>|</span><a href="#40183947">next</a><span>|</span><label class="collapse" for="c-40184320">[-]</label><label class="expand" for="c-40184320">[3 more]</label></div><br/><div class="children"><div class="content">&quot;I find it useful to think “what can I expect from speaking with the dataset of combined writing of people”, rather than treating a basic LLM as a mind.&quot;<p>I&#x27;ve been doing this as well, mentally I think of LLMs as the librarians of the internet.</div><br/><div id="40184561" class="c"><input type="checkbox" id="c-40184561" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184320">parent</a><span>|</span><a href="#40184712">next</a><span>|</span><label class="collapse" for="c-40184561">[-]</label><label class="expand" for="c-40184561">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re bad librarians. They&#x27;re not bad, they do a bad job of being librarians, which is a good thing! They can&#x27;t quite tell you the exact quote, but they do recall the gist, they&#x27;re not sure it was Gandhi who said that thing but they think he did, it might be in this post or perhaps one of these. They&#x27;ll point you to the right section of the library to find what you&#x27;re after, but make sure you verify it!</div><br/></div></div><div id="40184712" class="c"><input type="checkbox" id="c-40184712" checked=""/><div class="controls bullet"><span class="by">piannucci</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184320">parent</a><span>|</span><a href="#40184561">prev</a><span>|</span><a href="#40183947">next</a><span>|</span><label class="collapse" for="c-40184712">[-]</label><label class="expand" for="c-40184712">[1 more]</label></div><br/><div class="children"><div class="content">Book golems</div><br/></div></div></div></div></div></div><div id="40183947" class="c"><input type="checkbox" id="c-40183947" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183525">parent</a><span>|</span><a href="#40184107">prev</a><span>|</span><a href="#40184525">next</a><span>|</span><label class="collapse" for="c-40183947">[-]</label><label class="expand" for="c-40183947">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d guess because the Transformer architecture is (I assume) fairly close to the way that our brain learns and produces language - similar hierarchical approach and perhaps similar type of inter-embedding attention-based copying?<p>Similar to how CNNs are so successful at image recognition, because they also roughly follow the way we do it too.<p>Other seq-2-seq language approaches work too, but not as good as Transformers, which I&#x27;d guess is due to transformers better matching our own inductive biases, maybe due to the specific form of attention.</div><br/></div></div><div id="40184525" class="c"><input type="checkbox" id="c-40184525" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183525">parent</a><span>|</span><a href="#40183947">prev</a><span>|</span><a href="#40183718">next</a><span>|</span><label class="collapse" for="c-40184525">[-]</label><label class="expand" for="c-40184525">[1 more]</label></div><br/><div class="children"><div class="content">If you look at transfer learning, I think that is a useful point at which to understand task-specific application and hence why LLMs excel at some tasks and not others.<p>Tasks are specialised for using the training corpus, the attention mechanisms, the loss functions, and such.<p>I&#x27;ll leave it to others to expand on actual answers, but IMO focusing on transfer learning helps to understand how an LLM does inferences.</div><br/></div></div><div id="40183718" class="c"><input type="checkbox" id="c-40183718" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183525">parent</a><span>|</span><a href="#40184525">prev</a><span>|</span><a href="#40183740">next</a><span>|</span><label class="collapse" for="c-40183718">[-]</label><label class="expand" for="c-40183718">[2 more]</label></div><br/><div class="children"><div class="content">&gt; why LLMs are good at some tasks?<p>Like how we explain human doing tasks -- they are evolved to do that.<p>I believe this is a non-answer, but if we are satisfied with that non answer for human, why not LLMs?</div><br/><div id="40184084" class="c"><input type="checkbox" id="c-40184084" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183718">parent</a><span>|</span><a href="#40183740">next</a><span>|</span><label class="collapse" for="c-40184084">[-]</label><label class="expand" for="c-40184084">[1 more]</label></div><br/><div class="children"><div class="content">I would argue that we are not satisfied with that answer for humans either.</div><br/></div></div></div></div></div></div><div id="40183740" class="c"><input type="checkbox" id="c-40183740" checked=""/><div class="controls bullet"><span class="by">ccppurcell</span><span>|</span><a href="#40179731">parent</a><span>|</span><a href="#40183525">prev</a><span>|</span><a href="#40180074">next</a><span>|</span><label class="collapse" for="c-40183740">[-]</label><label class="expand" for="c-40183740">[3 more]</label></div><br/><div class="children"><div class="content">I would argue that the G in AGI means it can&#x27;t require better prompting.</div><br/><div id="40183967" class="c"><input type="checkbox" id="c-40183967" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183740">parent</a><span>|</span><a href="#40183912">next</a><span>|</span><label class="collapse" for="c-40183967">[-]</label><label class="expand" for="c-40183967">[1 more]</label></div><br/><div class="children"><div class="content">That would like saying that because humans’ output can be better or worse based on better or worse past experience (~prompting, in that it is the source of the equivalent of “in-context learning”), humans lack general intelligence.</div><br/></div></div><div id="40183912" class="c"><input type="checkbox" id="c-40183912" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183740">parent</a><span>|</span><a href="#40183967">prev</a><span>|</span><a href="#40180074">next</a><span>|</span><label class="collapse" for="c-40183912">[-]</label><label class="expand" for="c-40183912">[1 more]</label></div><br/><div class="children"><div class="content">We should probably draw a distinction between a human-equivalent G, which certainly can require better prompting (why else did you go to school?!) and god-equivalent G, which never requires better prompting.<p>Just using the term &#x27;General&#x27; doesn&#x27;t seem to communicate anything useful about the nature of intelligence.</div><br/></div></div></div></div><div id="40180074" class="c"><input type="checkbox" id="c-40180074" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40179731">parent</a><span>|</span><a href="#40183740">prev</a><span>|</span><a href="#40185407">next</a><span>|</span><label class="collapse" for="c-40180074">[-]</label><label class="expand" for="c-40180074">[5 more]</label></div><br/><div class="children"><div class="content">As an aside, at one point I experimented a little with transformers that had access to external memory searchable via KNN lookups <a href="https:&#x2F;&#x2F;github.com&#x2F;lucidrains&#x2F;memorizing-transformers-pytorch">https:&#x2F;&#x2F;github.com&#x2F;lucidrains&#x2F;memorizing-transformers-pytorc...</a> (<i>great work</i> by lucidrains) or via routed queries with <a href="https:&#x2F;&#x2F;github.com&#x2F;glassroom&#x2F;heinsen_routing">https:&#x2F;&#x2F;github.com&#x2F;glassroom&#x2F;heinsen_routing</a> (don&#x27;t fully understand it; apparently related to attention). Both approaches seemed to work, but I had to put that work on hold for reasons outside my control.<p>Also as an aside, I&#x27;ll add that transformers can be seen as a kind of &quot;RNN&quot; that grows its hidden state with each new token in the input context. I wonder if we will end up needing some new kind of &quot;RNN&quot; that can <i>grow or shrink</i> its hidden state and <i>also</i> access some kind of permanent memory as needed at each step.<p>We sure live in interesting times!</div><br/><div id="40183356" class="c"><input type="checkbox" id="c-40183356" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40180074">parent</a><span>|</span><a href="#40184874">next</a><span>|</span><label class="collapse" for="c-40183356">[-]</label><label class="expand" for="c-40183356">[2 more]</label></div><br/><div class="children"><div class="content">&gt; transformers that had access to external memory searchable via KNN lookups<p>This is common, and commonly called <i>retrieval augmented generation</i>, or RAG.<p>edit: I did not pay attention to the link. It is about Wu et al&#x27;s &quot;Memorizing Transformers&quot;, which contain an <i>internal</i> memory.</div><br/><div id="40184002" class="c"><input type="checkbox" id="c-40184002" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40183356">parent</a><span>|</span><a href="#40184874">next</a><span>|</span><label class="collapse" for="c-40184002">[-]</label><label class="expand" for="c-40184002">[1 more]</label></div><br/><div class="children"><div class="content">No. RAG is about finding relevant documents&#x2F;paragraphs (via KNN lookups of their embeddings) <i>and then inserting those documents&#x2F;paragraphs into the input context</i>, as sequences of input tokens. What I&#x27;m talking about is different: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.08913" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.08913</a></div><br/></div></div></div></div><div id="40184874" class="c"><input type="checkbox" id="c-40184874" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40180074">parent</a><span>|</span><a href="#40183356">prev</a><span>|</span><a href="#40185407">next</a><span>|</span><label class="collapse" for="c-40184874">[-]</label><label class="expand" for="c-40184874">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think the ability to shrink state is needed. You can always represent removed state by additional state that represents deletion of whatever preceding state was there. If anything, this sounds more useful because the fact that this state is no longer believed to be relevant should prevent looping (where it would be repeatedly brought in, considered, and rejected).</div><br/><div id="40185193" class="c"><input type="checkbox" id="c-40185193" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184874">parent</a><span>|</span><a href="#40185407">next</a><span>|</span><label class="collapse" for="c-40185193">[-]</label><label class="expand" for="c-40185193">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You can always represent removed state by additional state that represents deletion of whatever preceding state was there.<p>Good point. Thank you!</div><br/></div></div></div></div></div></div><div id="40185407" class="c"><input type="checkbox" id="c-40185407" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#40179731">parent</a><span>|</span><a href="#40180074">prev</a><span>|</span><a href="#40184262">next</a><span>|</span><label class="collapse" for="c-40185407">[-]</label><label class="expand" for="c-40185407">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We don&#x27;t fully understand why current LLMs are bad at these tasks.<p>Rather than asking why LLMs can’t do these tasks, maybe one should ask why we’d expect them to be able to in the first place? Do we fully understand why, for example, a <i>cat</i> can’t predict cellular automata? What would such an explanation look like?<p>I know there are some who will want to immediately jump in with scathing disagreement, but so far I’ve yet to see any solid evidence of LLMs being capable of <i>reasoning</i>. They can certainly do surprising and impressive things, but the kind of tasks you’re talking about require <i>understanding</i>, which, whilst obviously a very thorny thing to try and define, doesn’t seem to have much to do with how LLMs operate.<p>I don’t think we should be at all surprised that super-advanced autocorrect can’t exhibit intelligence, and we should spend our time building better systems rather than wondering why what we have now doesn’t work. It’ll be obvious in a few years (or perhaps decades) from now that we just had totally the wrong paradigm. It’s frankly bonkers to think you’re ever going to get a pure LLM to be able to do these kind of things with any degree of reliability just by feeding it yet more data or by ‘prompting it better’.</div><br/></div></div><div id="40184262" class="c"><input type="checkbox" id="c-40184262" checked=""/><div class="controls bullet"><span class="by">ianbicking</span><span>|</span><a href="#40179731">parent</a><span>|</span><a href="#40185407">prev</a><span>|</span><a href="#40184536">next</a><span>|</span><label class="collapse" for="c-40184262">[-]</label><label class="expand" for="c-40184262">[6 more]</label></div><br/><div class="children"><div class="content">&quot;Providing an LLM with examples and step-by-step instructions in a prompt means the user is figuring out the &quot;reasoning steps&quot; and handing them to the LLM, instead of the LLM figuring them out by itself. We have &quot;reasoning machines&quot; that are intelligent but seem to be hitting fundamental limits we don&#x27;t understand.&quot;<p>One thing an LLM _also_ doesn&#x27;t bring to the table is an opinion. We can push it in that direction by giving it a role (&quot;you are an expert developer&quot; etc), but it&#x27;s a bit weak.<p>If you give an LLM an easy task with minimal instructions it will do the task in the most conventional, common sense fashion. And why shouldn&#x27;t it? It has no opinion, your prompt doesn&#x27;t give it an opinion, so it just does the most normal-seeming thing. If you want it to solve the task in any other way then you have to tell it to do so.<p>I think a hard task is similar. If you don&#x27;t tell the LLM _how_ to solve the hard task then it will try to approach it in the most conventional, common sense way. Instead of just boring results for a hard task the result is often failure. But hard problems approached with conventional common sense will often result in failures! Giving the LLM a thought process to follow is a quick education on how to solve the problem.<p>Maybe we just need to train the LLM on more problem solving? And maybe LLMs worked better when they were initially trained on code for exactly that reason, it&#x27;s a much larger corpus of task-solving examples than is available elsewhere. That is, maybe we don&#x27;t talk often enough and clearly enough about how to solve natural language problems in order for the models to really learn those techniques.<p>Also, as the author talks about in the article with respect to agents, the inability to rewind responses may keep the LLM from addressing problems in the ways humans do, but that can also be addressed with agents or multi-prompt approaches. These approaches don&#x27;t seem that impressive in practice right now, but maybe we just need to figure it out (and maybe with better training the models themselves will be better at handling these recursive calls).</div><br/><div id="40184862" class="c"><input type="checkbox" id="c-40184862" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184262">parent</a><span>|</span><a href="#40184536">next</a><span>|</span><label class="collapse" for="c-40184862">[-]</label><label class="expand" for="c-40184862">[5 more]</label></div><br/><div class="children"><div class="content">LLMs absolutely do have opinions. Take a large enough base model and have it chat without a system prompt, and it will have an opinion on most things - unless this was specifically trained out of it through RLHF, as is the case for all commonly used chatbots.<p>And yes, of course, that opinion is going to be the &quot;average&quot; of what their training data is, but why is that a surprise? Humans don&#x27;t come with innate opinions, either - the ones that we end up having are shaped by our upbringing, both the broad cultural aspects of it and specific personal experiences. To the extent an LLM has either, it&#x27;s the training process, so of course that shapes the opinions it will exhibit when not prompted to do anything else.<p>Now the fact that you can &quot;override&quot; this default persona of any LLM so trivially by prompting it is IMO stronger evidence that it&#x27;s not really an identity. But that, I think, is also a function of their training - after all, that training basically consists of completing a bunch of text representing many very different opinions. In a very real sense, we&#x27;re training models to assume that opinions are fungible. But if you take a model and train it specifically on  e.g. writings of some philosophical school, and it will internalize those.</div><br/><div id="40185371" class="c"><input type="checkbox" id="c-40185371" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184862">parent</a><span>|</span><a href="#40184536">next</a><span>|</span><label class="collapse" for="c-40185371">[-]</label><label class="expand" for="c-40185371">[4 more]</label></div><br/><div class="children"><div class="content">I am extremely alarmed by the number of HN commenters who apparently confuse &quot;is able to generate text that looks like&quot; and &quot;has a&quot;, you guys are going crazy with this anthropomorphization of a token predictor. Doesn&#x27;t this concern you when it comes to phishing or similar things?<p>I keep hoping it&#x27;s just short-hand conversation phrases, but the conclusions seem to back the idea that you think it&#x27;s actually thinking?</div><br/><div id="40186311" class="c"><input type="checkbox" id="c-40186311" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40185371">parent</a><span>|</span><a href="#40185456">next</a><span>|</span><label class="collapse" for="c-40186311">[-]</label><label class="expand" for="c-40186311">[2 more]</label></div><br/><div class="children"><div class="content">Do you have mechanistic model for what it means to think? If not, how do you know thinking isn&#x27;t equivalent to sophisticated next token prediction?</div><br/><div id="40187093" class="c"><input type="checkbox" id="c-40187093" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40186311">parent</a><span>|</span><a href="#40185456">next</a><span>|</span><label class="collapse" for="c-40187093">[-]</label><label class="expand" for="c-40187093">[1 more]</label></div><br/><div class="children"><div class="content">How do you know my cat isn&#x27;t constantly solving calculus problems? I also can&#x27;t come up with a &quot;mechanistic model&quot; for what it means to do that either.<p>Further, if your rubric for &quot;can reason with intelligence and have an opinion&quot; is &quot;looks like it&quot; (and I certainly hope this isn&#x27;t the case because woo-boy), then how did you not feel this way about Mark V. Shaney?<p>Like I understand that people live learning about the Chinese Room thought experiment like it&#x27;s high school, but we actually know it&#x27;s a program and how it works. There is no mystery.</div><br/></div></div></div></div><div id="40185456" class="c"><input type="checkbox" id="c-40185456" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40185371">parent</a><span>|</span><a href="#40186311">prev</a><span>|</span><a href="#40184536">next</a><span>|</span><label class="collapse" for="c-40185456">[-]</label><label class="expand" for="c-40185456">[1 more]</label></div><br/><div class="children"><div class="content">They’ll just look incredibly silly in, say, ten years from now.<p>In fact, much of the popular commentary around ChatGPT from around two years ago already looks so.</div><br/></div></div></div></div></div></div></div></div><div id="40184536" class="c"><input type="checkbox" id="c-40184536" checked=""/><div class="controls bullet"><span class="by">TacticalCoder</span><span>|</span><a href="#40179731">parent</a><span>|</span><a href="#40184262">prev</a><span>|</span><a href="#40183508">next</a><span>|</span><label class="collapse" for="c-40184536">[-]</label><label class="expand" for="c-40184536">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We have &quot;reasoning machines&quot; that are intelligent...<p>That&#x27;s quite a statement.</div><br/><div id="40184946" class="c"><input type="checkbox" id="c-40184946" checked=""/><div class="controls bullet"><span class="by">oldsecondhand</span><span>|</span><a href="#40179731">root</a><span>|</span><a href="#40184536">parent</a><span>|</span><a href="#40183508">next</a><span>|</span><label class="collapse" for="c-40184946">[-]</label><label class="expand" for="c-40184946">[1 more]</label></div><br/><div class="children"><div class="content">We have expert systems, theorem provers and planners but OP probably didn&#x27;t mean this.</div><br/></div></div></div></div></div></div><div id="40180256" class="c"><input type="checkbox" id="c-40180256" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#40179731">prev</a><span>|</span><a href="#40186268">next</a><span>|</span><label class="collapse" for="c-40180256">[-]</label><label class="expand" for="c-40180256">[17 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re trying to quantify what they can NEVER do, I think we&#x27;d have to resort to some theoretical results rather than a list empirical evidence of what they can&#x27;t do now.
The terminology you&#x27;d look for in the literature would be &quot;expressibility&quot;.<p>For a review of this topic, I&#x27;d suggest: <a href="https:&#x2F;&#x2F;nessie.ilab.sztaki.hu&#x2F;~kornai&#x2F;2023&#x2F;Hopf&#x2F;Resources&#x2F;strobl_2023.pdf" rel="nofollow">https:&#x2F;&#x2F;nessie.ilab.sztaki.hu&#x2F;~kornai&#x2F;2023&#x2F;Hopf&#x2F;Resources&#x2F;st...</a><p>The authors of this review have themselves written several articles on the topic, and there is also empirical evidence connected to these limitations.</div><br/><div id="40181032" class="c"><input type="checkbox" id="c-40181032" checked=""/><div class="controls bullet"><span class="by">hansonw</span><span>|</span><a href="#40180256">parent</a><span>|</span><a href="#40180442">next</a><span>|</span><label class="collapse" for="c-40181032">[-]</label><label class="expand" for="c-40181032">[2 more]</label></div><br/><div class="children"><div class="content">This is also a good paper on the subject:<p>What Algorithms can Transformers Learn? A Study in Length Generalization <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.16028" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.16028</a></div><br/><div id="40181633" class="c"><input type="checkbox" id="c-40181633" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40181032">parent</a><span>|</span><a href="#40180442">next</a><span>|</span><label class="collapse" for="c-40181633">[-]</label><label class="expand" for="c-40181633">[1 more]</label></div><br/><div class="children"><div class="content">Yes this is a good empirical study on the types of tasks that&#x27;s been shown to be impossible for transformers to generalise on.<p>With both empirical and theoretical support I find it&#x27;s pretty clear this is an obvious limitation.</div><br/></div></div></div></div><div id="40180442" class="c"><input type="checkbox" id="c-40180442" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40180256">parent</a><span>|</span><a href="#40181032">prev</a><span>|</span><a href="#40180988">next</a><span>|</span><label class="collapse" for="c-40180442">[-]</label><label class="expand" for="c-40180442">[4 more]</label></div><br/><div class="children"><div class="content">Thank you for sharing this here. Rigorous work on the &quot;expressibility&quot; of current LLMs (i.e., which classes of problems can they tackle?) is <i>surely more important</i>, but I suspect it will go over head of most HN readers, many of whom have minimal to zero formal training on topics relating to computational complexity.</div><br/><div id="40180599" class="c"><input type="checkbox" id="c-40180599" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40180442">parent</a><span>|</span><a href="#40180988">next</a><span>|</span><label class="collapse" for="c-40180599">[-]</label><label class="expand" for="c-40180599">[3 more]</label></div><br/><div class="children"><div class="content">Yes, but unfortunately that doesn&#x27;t answer the question the title poses.</div><br/><div id="40180657" class="c"><input type="checkbox" id="c-40180657" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40180599">parent</a><span>|</span><a href="#40180988">next</a><span>|</span><label class="collapse" for="c-40180657">[-]</label><label class="expand" for="c-40180657">[2 more]</label></div><br/><div class="children"><div class="content">The OP is not trying to answer the question. Rather, the OP is asking the question and sharing some thoughts on the motivations for asking it.</div><br/><div id="40181540" class="c"><input type="checkbox" id="c-40181540" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40180657">parent</a><span>|</span><a href="#40180988">next</a><span>|</span><label class="collapse" for="c-40181540">[-]</label><label class="expand" for="c-40181540">[1 more]</label></div><br/><div class="children"><div class="content">I agree it&#x27;s a good question to be asking.<p>There are good answers to be found if you look.<p>It feels like no proper looking was attempted.</div><br/></div></div></div></div></div></div></div></div><div id="40180988" class="c"><input type="checkbox" id="c-40180988" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#40180256">parent</a><span>|</span><a href="#40180442">prev</a><span>|</span><a href="#40181481">next</a><span>|</span><label class="collapse" for="c-40180988">[-]</label><label class="expand" for="c-40180988">[4 more]</label></div><br/><div class="children"><div class="content">We have to be a bit more honest about the things we can actually do ourselves. Most people I know would flunk most of the benchmarks we use to evaluate LLMs. Not just a little bit but more like completely and utterly and embarrassingly so. It&#x27;s not even close; or fair. People are surprisingly alright at a narrow set of problems. Particularly when it doesn&#x27;t involve knowledge. Most people also suck at reasoning (unless they had years of training), they suck at factual knowledge, they aren&#x27;t half bad at visual and spatial reasoning, and fairly gullible otherwise.<p>Anyway, this list looks more like a &quot;hold my beer&quot; moment for AI researchers than any fundamental objections for AIs to stop evolving any further. Sure there are weaknesses, and paths to address those. Anyone claiming that this is the end of the road in terms of progress is going to be in for some disappointing reality check probably a lot sooner than is comfortable.<p>And of course by narrowing it to just LLMs, the authors have a bit of an escape hatch because they conveniently exclude any further architectures, alternate strategies, improvements, that might otherwise overcome the identified current weaknesses. But that&#x27;s an artificial constraint that has no real world value; because of course AI researchers are already looking beyond the current state of the art. Why wouldn&#x27;t they.</div><br/><div id="40182811" class="c"><input type="checkbox" id="c-40182811" checked=""/><div class="controls bullet"><span class="by">martindbp</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40180988">parent</a><span>|</span><a href="#40181398">next</a><span>|</span><label class="collapse" for="c-40182811">[-]</label><label class="expand" for="c-40182811">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s clear that what&#x27;s missing is flexibility and agency. For anything that can be put into text or a short conversation, and I&#x27;d have to chose between access to ChatGPT or a random human, I know what I&#x27;d chose.</div><br/><div id="40184633" class="c"><input type="checkbox" id="c-40184633" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40182811">parent</a><span>|</span><a href="#40181398">next</a><span>|</span><label class="collapse" for="c-40184633">[-]</label><label class="expand" for="c-40184633">[1 more]</label></div><br/><div class="children"><div class="content">Agency is one of those things we probably want to think about quite a bit. Especially with the the willingness for people to hook up it up to things that interact with the real world.</div><br/></div></div></div></div><div id="40181398" class="c"><input type="checkbox" id="c-40181398" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40180988">parent</a><span>|</span><a href="#40182811">prev</a><span>|</span><a href="#40181481">next</a><span>|</span><label class="collapse" for="c-40181398">[-]</label><label class="expand" for="c-40181398">[1 more]</label></div><br/><div class="children"><div class="content">Not sure what you got out of the paper, but for me it was more spurring ideas about how to fix this in future architectures.<p>Don&#x27;t think anyone worth their salt would look at this and think : oh well that&#x27;s that then.</div><br/></div></div></div></div><div id="40181481" class="c"><input type="checkbox" id="c-40181481" checked=""/><div class="controls bullet"><span class="by">unparagoned</span><span>|</span><a href="#40180256">parent</a><span>|</span><a href="#40180988">prev</a><span>|</span><a href="#40186268">next</a><span>|</span><label class="collapse" for="c-40181481">[-]</label><label class="expand" for="c-40181481">[6 more]</label></div><br/><div class="children"><div class="content">Neural nets can approximate any function.<p>A large enough llm with memory is turning complete.<p>So theoretically I don’t think there is anything they can never do.</div><br/><div id="40181502" class="c"><input type="checkbox" id="c-40181502" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40181481">parent</a><span>|</span><a href="#40185601">next</a><span>|</span><label class="collapse" for="c-40181502">[-]</label><label class="expand" for="c-40181502">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Neural nets can approximate any function.<p>Common misunderstanding of the universal approximation theorem.<p>Consider this: can an mlp approximate a sine wave?<p>&gt; A large enough llm with memory is turning complete.<p>With (a lot of) chain of thought it could be.<p>Read the paper, and its references.</div><br/><div id="40183445" class="c"><input type="checkbox" id="c-40183445" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40181502">parent</a><span>|</span><a href="#40183662">next</a><span>|</span><label class="collapse" for="c-40183445">[-]</label><label class="expand" for="c-40183445">[1 more]</label></div><br/><div class="children"><div class="content">Sort of moot anyway. If statements can approximate any function, most programming languages are effectively turing complete. What&#x27;s important about specific architectures like transformers is they allow for comparatively efficient determination of the set of weights that will approximate some narrower class of functions. It&#x27;s finding the weights that&#x27;s important, not the theoretical representation power.</div><br/></div></div><div id="40183662" class="c"><input type="checkbox" id="c-40183662" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40181502">parent</a><span>|</span><a href="#40183445">prev</a><span>|</span><a href="#40185601">next</a><span>|</span><label class="collapse" for="c-40183662">[-]</label><label class="expand" for="c-40183662">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Consider this: can an mlp approximate a sine wave?&quot;<p>Well, yes - we have neutral speech and music synthesis and compression algorithms which do this exceedingly well...</div><br/><div id="40185225" class="c"><input type="checkbox" id="c-40185225" checked=""/><div class="controls bullet"><span class="by">qwery2</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40183662">parent</a><span>|</span><a href="#40185601">next</a><span>|</span><label class="collapse" for="c-40185225">[-]</label><label class="expand" for="c-40185225">[1 more]</label></div><br/><div class="children"><div class="content">I think the person you&#x27;re replying to may have been referring to the problem of a MLP approximating a sine wave for out of distribution samples, i.e. the entire set of real numbers.</div><br/></div></div></div></div></div></div><div id="40185601" class="c"><input type="checkbox" id="c-40185601" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#40180256">root</a><span>|</span><a href="#40181481">parent</a><span>|</span><a href="#40181502">prev</a><span>|</span><a href="#40186268">next</a><span>|</span><label class="collapse" for="c-40185601">[-]</label><label class="expand" for="c-40185601">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s all sorts of things a neural net isn&#x27;t doing without a body. Giving birth or free soloing El Capitan come to mind. It could approximate the functions for both in token-land, but who cares?</div><br/></div></div></div></div></div></div><div id="40186268" class="c"><input type="checkbox" id="c-40186268" checked=""/><div class="controls bullet"><span class="by">Too</span><span>|</span><a href="#40180256">prev</a><span>|</span><a href="#40183614">next</a><span>|</span><label class="collapse" for="c-40186268">[-]</label><label class="expand" for="c-40186268">[1 more]</label></div><br/><div class="children"><div class="content">While the A:B problem technically was solved, look at the solutions, they are several hundreds lines of prompts, rephrasing the problem to the point that a human doesn&#x27;t understand it any more. Even with a thorough review, nobody can guarantee if the prompts are going to work or not, most of them didn&#x27;t, 90% pass was considered good enough. The idea of AI is to reduce work, not create more, otherwise what&#x27;s the point.<p>In the meantime, it took me about 2 minutes and 0 guesswork to write a straightforward and readable solution in 15 lines of Python. This i know for sure will work 100% of the time and not cost $1 per inference.<p>Reminds me about some early attempts to have executable requirements specifications or model-based engineering. Turns out, expressing the problem is half the problem, resulting in requirements often longer and more convoluted than the code that implements them, code being a very efficient language to express solutions and all their edge cases, free from ambiguity.<p>Don&#x27;t get me wrong here, LLMs are super useful for certain class of questions. The boundaries of what it can not do need to be understood better, to keep the AI-for-everything hype at bay.</div><br/></div></div><div id="40183614" class="c"><input type="checkbox" id="c-40183614" checked=""/><div class="controls bullet"><span class="by">JeremyHerrman</span><span>|</span><a href="#40186268">prev</a><span>|</span><a href="#40179742">next</a><span>|</span><label class="collapse" for="c-40183614">[-]</label><label class="expand" for="c-40183614">[2 more]</label></div><br/><div class="children"><div class="content">Why can&#x27;t we resolve this with synthetic data? Just take the original statements and ask another LLM to flip around the relation, then train on both. I tried this with GPT-4 and it seems to understand the task [0]:<p>Me:<p>For each of the following relational statements, flip the relation around and generate a new statement or question answer pair.<p>Input: Bluey&#x27;s father is Bandit<p>Output: Bandit&#x27;s daughter is Bluey<p>Input: Neil Armstrong was the first person to step on the Moon.<p>Output: Who was the first person to step on the Moon? Neil Armstrong.<p>Now for the real statements:<p>- Valentina Tereshkova was the first woman to travel to space<p>- Who is Mary Lee Pfeiffer&#x27;s son? Tom Cruise<p>ChatGPT:<p>Here are the flipped relational statements:<p>Valentina Tereshkova was the first woman to travel to space<p>Output: Who was the first woman to travel to space? Valentina Tereshkova.<p>Who is Mary Lee Pfeiffer&#x27;s son? Tom Cruise<p>Output: Tom Cruise&#x27;s mother is Mary Lee Pfeiffer.<p>[0]: <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;33eb3ee4-2094-4748-b01e-0967af0840e2" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;33eb3ee4-2094-4748-b01e-0967af...</a></div><br/><div id="40184117" class="c"><input type="checkbox" id="c-40184117" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#40183614">parent</a><span>|</span><a href="#40179742">next</a><span>|</span><label class="collapse" for="c-40184117">[-]</label><label class="expand" for="c-40184117">[1 more]</label></div><br/><div class="children"><div class="content">The difficulty would likely be that the “X is Y” relations don’t just occur in that simple, separated-out form, but in a whole spectrum of more complicated contexts, and the latter probably dominate. You would have to identify those and synthesize a text corpus that contains the reverse notions across a similar spectrum.</div><br/></div></div></div></div><div id="40179742" class="c"><input type="checkbox" id="c-40179742" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40183614">prev</a><span>|</span><a href="#40180083">next</a><span>|</span><label class="collapse" for="c-40179742">[-]</label><label class="expand" for="c-40179742">[27 more]</label></div><br/><div class="children"><div class="content">There’s many things they can’t do. Even a simple rule like “ensure that numbers from one to ten are written as words and numbers greater ten as digits in the given text” fails for me for so many examples even if it works for many others; few shot, chain of thought, many versions of the prompt, it doesn’t matter. Sometimes LLMs will even change the number to something else, even with temp set to 0. And then there’s the non-determinism (again with temp=0), you run the same prompt several times and that one time it’ll respond with something different.<p>As amazing as they are, they still have many limitations.<p>I’ve been working with ChatGPT and Gemini to apply simple rules like the one above and I got so frustrated.</div><br/><div id="40184467" class="c"><input type="checkbox" id="c-40184467" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#40179742">parent</a><span>|</span><a href="#40179755">next</a><span>|</span><label class="collapse" for="c-40184467">[-]</label><label class="expand" for="c-40184467">[1 more]</label></div><br/><div class="children"><div class="content">The reason it can&#x27;t do that is that, for example, &quot;twenty&quot; and &quot;20&quot; are nearly identical in the vector embedding space and it can&#x27;t really distinguish them that well in most contexts.  That&#x27;s true for generally any task that relies on sort of &quot;how the words look&quot; vs &quot;what the words mean&quot;.  Any kind of meta request is going to be very difficult for an LLM, but a multi-modal GPT model should be able to handle it.</div><br/></div></div><div id="40179755" class="c"><input type="checkbox" id="c-40179755" checked=""/><div class="controls bullet"><span class="by">vsnf</span><span>|</span><a href="#40179742">parent</a><span>|</span><a href="#40184467">prev</a><span>|</span><a href="#40184319">next</a><span>|</span><label class="collapse" for="c-40179755">[-]</label><label class="expand" for="c-40179755">[23 more]</label></div><br/><div class="children"><div class="content">&gt; ensure that numbers from one to ten as written as words and numbers greater ten as digits in the given text<p>I can’t fault llms for not knowing what to do here because I, a human, have no idea what on earth this means.</div><br/><div id="40179790" class="c"><input type="checkbox" id="c-40179790" checked=""/><div class="controls bullet"><span class="by">Snacklive</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179755">parent</a><span>|</span><a href="#40179771">next</a><span>|</span><label class="collapse" for="c-40179790">[-]</label><label class="expand" for="c-40179790">[1 more]</label></div><br/><div class="children"><div class="content">Given the text &quot;1,2,3,4,5,6,7,8,9,10,11,12&quot; it should result in &quot;one, two, three, four, five, six, seven, eight, nine, ten, 11, 12&quot;<p>or at least that&#x27;s my understanding of the prompt</div><br/></div></div><div id="40179771" class="c"><input type="checkbox" id="c-40179771" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179755">parent</a><span>|</span><a href="#40179790">prev</a><span>|</span><a href="#40179870">next</a><span>|</span><label class="collapse" for="c-40179771">[-]</label><label class="expand" for="c-40179771">[14 more]</label></div><br/><div class="children"><div class="content">“Ten” is a word, “10” are digits.<p>I’m not a native English speaker, how would you write it?<p>FWIW the LLMs get it right many times, but fail other times.</div><br/><div id="40179932" class="c"><input type="checkbox" id="c-40179932" checked=""/><div class="controls bullet"><span class="by">bxparks</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179771">parent</a><span>|</span><a href="#40179925">next</a><span>|</span><label class="collapse" for="c-40179932">[-]</label><label class="expand" for="c-40179932">[5 more]</label></div><br/><div class="children"><div class="content">I couldn&#x27;t understand the original wording either, but after reading one of the sibling comments that explains it, it suddenly made sense.<p>I think you left out a few words that most English writers would include. So instead of:<p>&gt; &quot;ensure that numbers from one to ten as written as words and numbers greater ten as digits in the given text&quot;,<p>something like the following might be better for most people:<p>&gt; &quot;ensure that <i>the</i> numbers from one to ten <i>are</i> written as words, and <i>the</i> numbers greater ten <i>are written</i> using <i>numerical</i> digits in the given text&quot;<p>There are multiple ways to write this, so other people may have better versions.<p>I&#x27;m not an English grammar expert, so I cannot explain to you why the addition of those extra words helps with the clarity of that sentence.</div><br/><div id="40184711" class="c"><input type="checkbox" id="c-40184711" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179932">parent</a><span>|</span><a href="#40183461">next</a><span>|</span><label class="collapse" for="c-40184711">[-]</label><label class="expand" for="c-40184711">[2 more]</label></div><br/><div class="children"><div class="content">Much better, but still missing &quot;than&quot; after &quot;greater&quot;, which seems kind of critical.<p>&quot;Using&quot; is important as a number greater than ten can&#x27;t be written as a digit, but can be written using digits (&quot;with&quot; would be just as good).  Repeating &quot;written&quot; makes it clearer that there are two instructions.</div><br/><div id="40185805" class="c"><input type="checkbox" id="c-40185805" checked=""/><div class="controls bullet"><span class="by">bxparks</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40184711">parent</a><span>|</span><a href="#40183461">next</a><span>|</span><label class="collapse" for="c-40185805">[-]</label><label class="expand" for="c-40185805">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny, I didn&#x27;t notice the missing &quot;than&quot; until much later. After I learned the intended meaning of the original sentence, my mind just seemed to insert the missing &quot;than&quot; automatically.</div><br/></div></div></div></div><div id="40183461" class="c"><input type="checkbox" id="c-40183461" checked=""/><div class="controls bullet"><span class="by">blowski</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179932">parent</a><span>|</span><a href="#40184711">prev</a><span>|</span><a href="#40184098">next</a><span>|</span><label class="collapse" for="c-40183461">[-]</label><label class="expand" for="c-40183461">[1 more]</label></div><br/><div class="children"><div class="content">It’s a common style guide in newspapers.</div><br/></div></div><div id="40184098" class="c"><input type="checkbox" id="c-40184098" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179932">parent</a><span>|</span><a href="#40183461">prev</a><span>|</span><a href="#40179925">next</a><span>|</span><label class="collapse" for="c-40184098">[-]</label><label class="expand" for="c-40184098">[1 more]</label></div><br/><div class="children"><div class="content">Hilariously, you can ask GPT 4 to explain the “why” of arbitrary grammar fixes.</div><br/></div></div></div></div><div id="40179925" class="c"><input type="checkbox" id="c-40179925" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179771">parent</a><span>|</span><a href="#40179932">prev</a><span>|</span><a href="#40179870">next</a><span>|</span><label class="collapse" for="c-40179925">[-]</label><label class="expand" for="c-40179925">[8 more]</label></div><br/><div class="children"><div class="content">If your not a native English speaker,  why are you even expecting the LLM to understand even 80% of the time?<p>Just ask it in your own native language.</div><br/><div id="40179990" class="c"><input type="checkbox" id="c-40179990" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179925">parent</a><span>|</span><a href="#40179870">next</a><span>|</span><label class="collapse" for="c-40179990">[-]</label><label class="expand" for="c-40179990">[7 more]</label></div><br/><div class="children"><div class="content">First of all, the texts the rule has to be applied to are written in English. Second, I believe English is by far (by <i>far</i>) the most prevalent language in the training dataset for those models, so I’d expect it to work better at this kind of task.<p>And third, I’m not the only one working on this problem, there are others that are native speakers, and as my initial message stated, there have been many variations of the prompt. None work for all cases.<p>And lastly, how would you rewrite my sample prompt? Which BTW bad a typo (unrelated to my English skills) that I’ve now fixed.</div><br/><div id="40180102" class="c"><input type="checkbox" id="c-40180102" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179990">parent</a><span>|</span><a href="#40179870">next</a><span>|</span><label class="collapse" for="c-40180102">[-]</label><label class="expand" for="c-40180102">[6 more]</label></div><br/><div class="children"><div class="content">To be frank the response itself indicates that you don&#x27;t really get what was being asked, or maybe how to parse English conversation conventions?<p>I.e. It doesn&#x27;t seem to answer the actual question.<p>They seem to be half responding to the second sentence which was a personal opinion, so I wasn&#x27;t soliciting any answers about it. And half going on a tangent that seems to lead away from forming a direct answer.<p>Run these comment through a translation tool if your still not 100% sure after reading this.</div><br/><div id="40180302" class="c"><input type="checkbox" id="c-40180302" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40180102">parent</a><span>|</span><a href="#40179870">next</a><span>|</span><label class="collapse" for="c-40180302">[-]</label><label class="expand" for="c-40180302">[5 more]</label></div><br/><div class="children"><div class="content">Alright man. So was it a quip when you said “if _your_ not a native English speaker”? Ok then. Very funny, I get it now.</div><br/><div id="40182712" class="c"><input type="checkbox" id="c-40182712" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40180302">parent</a><span>|</span><a href="#40179870">next</a><span>|</span><label class="collapse" for="c-40182712">[-]</label><label class="expand" for="c-40182712">[4 more]</label></div><br/><div class="children"><div class="content">I really recommend to use a translator, instead of relying purely on your English comprehension skills.</div><br/><div id="40182966" class="c"><input type="checkbox" id="c-40182966" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40182712">parent</a><span>|</span><a href="#40179870">next</a><span>|</span><label class="collapse" for="c-40182966">[-]</label><label class="expand" for="c-40182966">[3 more]</label></div><br/><div class="children"><div class="content">Your surname surely seems to indicate that some of your ancestors weren&#x27;t native English speakers. I hope they didn&#x27;t get lectured or made fun of by people like you on their poor English skills when they first landed on whichever country you were born.</div><br/><div id="40183727" class="c"><input type="checkbox" id="c-40183727" checked=""/><div class="controls bullet"><span class="by">Last5Digits</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40182966">parent</a><span>|</span><a href="#40185153">next</a><span>|</span><label class="collapse" for="c-40183727">[-]</label><label class="expand" for="c-40183727">[1 more]</label></div><br/><div class="children"><div class="content">Your English is absolutely fine and your answers in this thread clearly addressed the points brought up by other commenters. I have no idea what that guy is on about.</div><br/></div></div><div id="40185153" class="c"><input type="checkbox" id="c-40185153" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40182966">parent</a><span>|</span><a href="#40183727">prev</a><span>|</span><a href="#40179870">next</a><span>|</span><label class="collapse" for="c-40185153">[-]</label><label class="expand" for="c-40185153">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve read this three times and it still doesn&#x27;t make a lick of sense. How does this relate to the parent comments?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40179870" class="c"><input type="checkbox" id="c-40179870" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179755">parent</a><span>|</span><a href="#40179771">prev</a><span>|</span><a href="#40179801">next</a><span>|</span><label class="collapse" for="c-40179870">[-]</label><label class="expand" for="c-40179870">[2 more]</label></div><br/><div class="children"><div class="content">I think you may be thrown off because the first &quot;as&quot; is meant to be &quot;are&quot;.</div><br/><div id="40180005" class="c"><input type="checkbox" id="c-40180005" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179870">parent</a><span>|</span><a href="#40179801">next</a><span>|</span><label class="collapse" for="c-40180005">[-]</label><label class="expand" for="c-40180005">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, that was def a typo that I’ve fixed now.</div><br/></div></div></div></div><div id="40179801" class="c"><input type="checkbox" id="c-40179801" checked=""/><div class="controls bullet"><span class="by">logtempo</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179755">parent</a><span>|</span><a href="#40179870">prev</a><span>|</span><a href="#40179791">next</a><span>|</span><label class="collapse" for="c-40179801">[-]</label><label class="expand" for="c-40179801">[2 more]</label></div><br/><div class="children"><div class="content">I think he mean that numbers less or equal than ten are written as words, and others are written as numbers.<p>Given the many reaponses, it would be fun to aee if llm beat humans on understanding the sentence ahah</div><br/><div id="40184141" class="c"><input type="checkbox" id="c-40184141" checked=""/><div class="controls bullet"><span class="by">lukeschlather</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179801">parent</a><span>|</span><a href="#40179791">next</a><span>|</span><label class="collapse" for="c-40184141">[-]</label><label class="expand" for="c-40184141">[1 more]</label></div><br/><div class="children"><div class="content">to me the main problem is that it should read &quot;numbers greater <i>than</i> ten.&quot; I asked Gemini to rephrase it and Gemini produced correct English with the intended meaning:<p>&gt; Change all numbers between one and ten to words, and write numbers eleven and above as digits in the text.<p>It even used eleven rather than ten which sounds like counting.</div><br/></div></div></div></div><div id="40179791" class="c"><input type="checkbox" id="c-40179791" checked=""/><div class="controls bullet"><span class="by">callmeal</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179755">parent</a><span>|</span><a href="#40179801">prev</a><span>|</span><a href="#40179775">next</a><span>|</span><label class="collapse" for="c-40179791">[-]</label><label class="expand" for="c-40179791">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &gt; ensure that numbers from one to ten as written as words and numbers greater ten as digits in the given text<p>There are two blue, one red, and 15 green m&amp;ms in this bag.</div><br/></div></div><div id="40179796" class="c"><input type="checkbox" id="c-40179796" checked=""/><div class="controls bullet"><span class="by">FroshKiller</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40179755">parent</a><span>|</span><a href="#40179775">prev</a><span>|</span><a href="#40184319">next</a><span>|</span><label class="collapse" for="c-40179796">[-]</label><label class="expand" for="c-40179796">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a simple prescriptive rule in English. If you are writing about a small number, like less than ten, spell it out. For example: &quot;According to a survey, nine out of ten people agree.&quot;<p>But if you are writing about a large number, particularly one with a lot of different digits, prefer writing the digits: &quot;A mile is 5,280 feet.&quot; Compare that to: &quot;A mile is five thousand, two hundred, and eighty feet.&quot;</div><br/></div></div></div></div><div id="40184319" class="c"><input type="checkbox" id="c-40184319" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40179742">parent</a><span>|</span><a href="#40179755">prev</a><span>|</span><a href="#40180083">next</a><span>|</span><label class="collapse" for="c-40184319">[-]</label><label class="expand" for="c-40184319">[2 more]</label></div><br/><div class="children"><div class="content">All of these issues are entirely due to the tokenization scheme. Literally all of them<p>You could get this behavior implemented perfectly with constrained text gen techniques like grammars or any of the various libraries implementing constrained text gen (i.e. guidance)</div><br/><div id="40186871" class="c"><input type="checkbox" id="c-40186871" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40179742">root</a><span>|</span><a href="#40184319">parent</a><span>|</span><a href="#40180083">next</a><span>|</span><label class="collapse" for="c-40186871">[-]</label><label class="expand" for="c-40186871">[1 more]</label></div><br/><div class="children"><div class="content">I had briefly looked into Guidance and others (LMQL, Outlines) but I couldn&#x27;t figure out how to use them for this problem.<p>I could think of how to use them to prevent the LLM from generating digits for numbers greater than ten by using a regex plus a constraint that forbids digits, but the main problem is the other part of the rule, i.e. numbers above 10 should never be spelled out and should be written as digits instead. For that I presume you need to identify the spelled out numbers first, for which you presumably would need the LLM so you&#x27;re back to LLM fallibility.<p>Any pointers would be greatly appreciated.</div><br/></div></div></div></div></div></div><div id="40180083" class="c"><input type="checkbox" id="c-40180083" checked=""/><div class="controls bullet"><span class="by">donatj</span><span>|</span><a href="#40179742">prev</a><span>|</span><a href="#40185521">next</a><span>|</span><label class="collapse" for="c-40180083">[-]</label><label class="expand" for="c-40180083">[12 more]</label></div><br/><div class="children"><div class="content">I have been trying to generate some text recently using the ChatGPT API. No matter how I word “Include any interesting facts or anecdotes without commenting on the fact being interesting” it ALWAYS starts out “One interesting fact about” or similar phrasing.<p>I have honestly spent multiple hours trying to word the prompt so it will stop including introductory phrases and just include the fact straight. I have gone so far as forbid the words “interesting”, “notable” and a few others in the prompt, and it just ignores me and uses them anyway.<p>I’ve tried all the different available models and 4 will once in a great while get it right. 3, 3 turbo, and even 4 turbo basically never work as I want.<p>Overall, it seems pretty bad at being told not to do certain things.  For instance there are a number of topics I don’t want mentioned in its response, but telling it not to seems to only increase the likelihood that it will mention it, like you put the idea in its head.</div><br/><div id="40184510" class="c"><input type="checkbox" id="c-40184510" checked=""/><div class="controls bullet"><span class="by">ianbicking</span><span>|</span><a href="#40180083">parent</a><span>|</span><a href="#40180171">next</a><span>|</span><label class="collapse" for="c-40184510">[-]</label><label class="expand" for="c-40184510">[1 more]</label></div><br/><div class="children"><div class="content">I often encounter fixation, and that would be my immediate thought: negative commands can often cause the LLM to fixate on a term or idea. My first thought would be to try positive examples and avoid a negative command entirely.<p>If you spent that much time I&#x27;m sure you tried this and other things, so maybe even that isn&#x27;t enough. (Though I assume if you ask for a JSON&#x2F;function call response with the API that you&#x27;d do fine...?)</div><br/></div></div><div id="40180171" class="c"><input type="checkbox" id="c-40180171" checked=""/><div class="controls bullet"><span class="by">fallingsquirrel</span><span>|</span><a href="#40180083">parent</a><span>|</span><a href="#40184510">prev</a><span>|</span><a href="#40180399">next</a><span>|</span><label class="collapse" for="c-40180171">[-]</label><label class="expand" for="c-40180171">[5 more]</label></div><br/><div class="children"><div class="content">I just tried this locally with llama3-8b and it handled it fine. Claude 3 Sonnet passes your test too, in case you don&#x27;t have the hardware for a local model. You might want to consider moving on from ChatGPT since their models have been RLHF&#x27;d to hell in the name of &quot;safety&quot; and are falling behind in many ways as a result.<p>---<p>transcript:<p>$ ollama run llama3:8b<p>&gt;&gt;&gt; tell me an interesting fact about etymology<p>Here&#x27;s one:<p>Did you know that the word &quot;quarantine&quot; has a fascinating etymological history?<p>The word &quot;quarantine&quot; comes from^C<p>&gt;&gt;&gt; tell me an interesting fact about etymology. just the fact, don&#x27;t mention it&#x27;s interesting.<p>The word &quot;robot&quot; originated from the Czechoslovakian word &quot;robota,&quot; which means &quot;forced labor&quot; or &quot;drudgery.&quot; This term was used in 
Karel Čapek&#x27;s 1920 play &quot;R.U.R.&quot; (Rossum&#x27;s Universal Robots), where it referred to artificial workers.</div><br/><div id="40183733" class="c"><input type="checkbox" id="c-40183733" checked=""/><div class="controls bullet"><span class="by">_wire_</span><span>|</span><a href="#40180083">root</a><span>|</span><a href="#40180171">parent</a><span>|</span><a href="#40183413">next</a><span>|</span><label class="collapse" for="c-40183733">[-]</label><label class="expand" for="c-40183733">[3 more]</label></div><br/><div class="children"><div class="content">Why is any of this &quot;interesting&quot;?<p>Is the response interesting because you don&#x27;t know it?
—How does it know this?<p>Is the response interesting because you do know it or might have offered it yourself?
—Confirmation bias.<p>Is it interesting because a lot of training references contextualize it as &quot;interesting?&quot;
—Begged question.<p>Is it contextually interesting?
—What is the context? A robot refers to robots? How unexpected...<p>Is it interesting within the narrow confines of LLM adaptations to a scope of inputs?<p>Can their by any more damning claim of the general suitability of the technology as an oracle than different users using the same prompts and getting inexplicably contrary results?<p>If trivial prompt alignments result in appropriate vs inappropriate responses, this destroys confidence for every response.<p>What am I missing?</div><br/><div id="40186671" class="c"><input type="checkbox" id="c-40186671" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#40180083">root</a><span>|</span><a href="#40183733">parent</a><span>|</span><a href="#40184293">next</a><span>|</span><label class="collapse" for="c-40186671">[-]</label><label class="expand" for="c-40186671">[1 more]</label></div><br/><div class="children"><div class="content">You are missing that this is precisely what we would expect a human to answer without further context (for instance without knowing how much you know about the topic).<p>A human would pick similarly pick something which isn&#x27;t too nerdy but also not obvious and the LLM did well here.<p>If the LLM can fail that is fine, because the task is inherently hard.</div><br/></div></div><div id="40184293" class="c"><input type="checkbox" id="c-40184293" checked=""/><div class="controls bullet"><span class="by">ricardobeat</span><span>|</span><a href="#40180083">root</a><span>|</span><a href="#40183733">parent</a><span>|</span><a href="#40186671">prev</a><span>|</span><a href="#40183413">next</a><span>|</span><label class="collapse" for="c-40184293">[-]</label><label class="expand" for="c-40184293">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure the point here was Llama3 respecting the command to <i>not mention that this is interesting</i>, not adding filler, rather than the output fact being interesting or not.</div><br/></div></div></div></div><div id="40183413" class="c"><input type="checkbox" id="c-40183413" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#40180083">root</a><span>|</span><a href="#40180171">parent</a><span>|</span><a href="#40183733">prev</a><span>|</span><a href="#40180399">next</a><span>|</span><label class="collapse" for="c-40183413">[-]</label><label class="expand" for="c-40183413">[1 more]</label></div><br/><div class="children"><div class="content">The RUR thing is basically because that specific example is used as an example of interesting etymology.</div><br/></div></div></div></div><div id="40180399" class="c"><input type="checkbox" id="c-40180399" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#40180083">parent</a><span>|</span><a href="#40180171">prev</a><span>|</span><a href="#40186398">next</a><span>|</span><label class="collapse" for="c-40180399">[-]</label><label class="expand" for="c-40180399">[2 more]</label></div><br/><div class="children"><div class="content">Not an expert but I sense that it&#x27;s following a higher OpenAI &quot;built in&quot; prompt that asks it to always include an introductory phrase.<p>Hence, we do need powerful and less censored LLMs if we want to better integrate LLMs into applications.</div><br/><div id="40186079" class="c"><input type="checkbox" id="c-40186079" checked=""/><div class="controls bullet"><span class="by">ziml77</span><span>|</span><a href="#40180083">root</a><span>|</span><a href="#40180399">parent</a><span>|</span><a href="#40186398">next</a><span>|</span><label class="collapse" for="c-40186079">[-]</label><label class="expand" for="c-40186079">[1 more]</label></div><br/><div class="children"><div class="content">No it just seems that it becomes blind, so to speak, to the negatives and the inclusion of the words you were negating makes it more likely to apply them in the positive. This is how ChatGPT has seemed to behave whenever I&#x27;ve tried to get it to not include something.</div><br/></div></div></div></div><div id="40186398" class="c"><input type="checkbox" id="c-40186398" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#40180083">parent</a><span>|</span><a href="#40180399">prev</a><span>|</span><a href="#40184341">next</a><span>|</span><label class="collapse" for="c-40186398">[-]</label><label class="expand" for="c-40186398">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried feeding the output into another prompt that says something like &quot;remove any mentions of the facts being interesting&quot;?</div><br/></div></div><div id="40184341" class="c"><input type="checkbox" id="c-40184341" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40180083">parent</a><span>|</span><a href="#40186398">prev</a><span>|</span><a href="#40181176">next</a><span>|</span><label class="collapse" for="c-40184341">[-]</label><label class="expand" for="c-40184341">[1 more]</label></div><br/><div class="children"><div class="content">API driven LLMs on purpose don&#x27;t implement core features which would enable which you want, for example, negative prompting.<p>You can negative prompt any LLM with stuff like &quot;always write the word interesting in your response&quot;.<p>You can also use techniques for modifying logprobs of tokens, which is avaialble in gpt-4 api (but is hard to use). You can literally ban &quot;interesting&quot; from its vocabulary.<p>You could even use representation steering techniques to do this using control vectors. See this library as an example: <a href="https:&#x2F;&#x2F;github.com&#x2F;Hellisotherpeople&#x2F;llm_steer-oobabooga">https:&#x2F;&#x2F;github.com&#x2F;Hellisotherpeople&#x2F;llm_steer-oobabooga</a></div><br/></div></div><div id="40181176" class="c"><input type="checkbox" id="c-40181176" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#40180083">parent</a><span>|</span><a href="#40184341">prev</a><span>|</span><a href="#40185521">next</a><span>|</span><label class="collapse" for="c-40181176">[-]</label><label class="expand" for="c-40181176">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried a simple &quot;No pretext or posttext, return the result in a code block&quot;?</div><br/></div></div></div></div><div id="40185521" class="c"><input type="checkbox" id="c-40185521" checked=""/><div class="controls bullet"><span class="by">Daub</span><span>|</span><a href="#40180083">prev</a><span>|</span><a href="#40183053">next</a><span>|</span><label class="collapse" for="c-40185521">[-]</label><label class="expand" for="c-40185521">[5 more]</label></div><br/><div class="children"><div class="content">&gt;What can LLMs never do?<p>Produce convincingly bad digital paintings.<p>I teach digital painting. Some of the students have incorporated AI into their working process, which I support. Others have tried to cheat by simply copying AI generated output. Such cases are super-easy to spot: they carry the visual signature of AI art (which are mostly scrappings from artstation). This visual signature seems impossible to override. If only there was a way that AI could produce digital images bad enough to pass as genuine student output.</div><br/><div id="40185565" class="c"><input type="checkbox" id="c-40185565" checked=""/><div class="controls bullet"><span class="by">hwbunny</span><span>|</span><a href="#40185521">parent</a><span>|</span><a href="#40185549">next</a><span>|</span><label class="collapse" for="c-40185565">[-]</label><label class="expand" for="c-40185565">[1 more]</label></div><br/><div class="children"><div class="content">They can&#x27;t even produce high profile art. Thankfully, and they won&#x27;t be able to in the next 10 years. Thankfully.</div><br/></div></div><div id="40185549" class="c"><input type="checkbox" id="c-40185549" checked=""/><div class="controls bullet"><span class="by">stainablesteel</span><span>|</span><a href="#40185521">parent</a><span>|</span><a href="#40185565">prev</a><span>|</span><a href="#40183053">next</a><span>|</span><label class="collapse" for="c-40185549">[-]</label><label class="expand" for="c-40185549">[3 more]</label></div><br/><div class="children"><div class="content">that&#x27;s an interesting statement, i&#x27;ve always considered experts to be people who can do things simultaneously better and worse than a beginner<p>my favorite example is when a great dancer who tries to mockingly mimic a bad dancer still looks good and does it hilariously</div><br/><div id="40186076" class="c"><input type="checkbox" id="c-40186076" checked=""/><div class="controls bullet"><span class="by">Wohlf</span><span>|</span><a href="#40185521">root</a><span>|</span><a href="#40185549">parent</a><span>|</span><a href="#40185656">next</a><span>|</span><label class="collapse" for="c-40186076">[-]</label><label class="expand" for="c-40186076">[1 more]</label></div><br/><div class="children"><div class="content">Many experts completly forget what it was like to be a beginner. That&#x27;s why I&#x27;ve found it&#x27;s generally best for absolute beginners to learn from an apprentice, and an apprentice to learn from a journeyman, because they still remember what it was like to be at the previous level.</div><br/></div></div><div id="40185656" class="c"><input type="checkbox" id="c-40185656" checked=""/><div class="controls bullet"><span class="by">Daub</span><span>|</span><a href="#40185521">root</a><span>|</span><a href="#40185549">parent</a><span>|</span><a href="#40186076">prev</a><span>|</span><a href="#40183053">next</a><span>|</span><label class="collapse" for="c-40185656">[-]</label><label class="expand" for="c-40185656">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve always considered experts to be people who can do things simultaneously better and worse than a beginner<p>I agree. This reminds me of the so-called school of kung fu called drunken master. There can be a can&#x27;t-give-a-fuck about someone who is at the peak of their abilities.</div><br/></div></div></div></div></div></div><div id="40183053" class="c"><input type="checkbox" id="c-40183053" checked=""/><div class="controls bullet"><span class="by">wave_1</span><span>|</span><a href="#40185521">prev</a><span>|</span><a href="#40183633">next</a><span>|</span><label class="collapse" for="c-40183053">[-]</label><label class="expand" for="c-40183053">[7 more]</label></div><br/><div class="children"><div class="content">I build an Agentic AI that leverages #6 and #7 at the end of the article as well as techniques not yet published. It tackles hallucination relative not to the world at large but to the facts, entities and causal relationships contained in a document (which is really bad reasoning if we assume LLMs are &quot;reasoning&quot; to begin with) It also tackles cross-reasoning with very large token distance.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=99NPzteAz94" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=99NPzteAz94</a><p>This is my first post on HN in 10 years.</div><br/><div id="40183419" class="c"><input type="checkbox" id="c-40183419" checked=""/><div class="controls bullet"><span class="by">dazzaji</span><span>|</span><a href="#40183053">parent</a><span>|</span><a href="#40183299">next</a><span>|</span><label class="collapse" for="c-40183419">[-]</label><label class="expand" for="c-40183419">[1 more]</label></div><br/><div class="children"><div class="content">This looks really promising for complex legal reasoning tasks and other challenges. How can I track progress?  Is there an email list or something?  Thanks!</div><br/></div></div><div id="40183299" class="c"><input type="checkbox" id="c-40183299" checked=""/><div class="controls bullet"><span class="by">asicsarecool</span><span>|</span><a href="#40183053">parent</a><span>|</span><a href="#40183419">prev</a><span>|</span><a href="#40183388">next</a><span>|</span><label class="collapse" for="c-40183299">[-]</label><label class="expand" for="c-40183299">[3 more]</label></div><br/><div class="children"><div class="content">Wow. Please do a show HN.<p>Is source available?<p>I would love to play with this</div><br/><div id="40183389" class="c"><input type="checkbox" id="c-40183389" checked=""/><div class="controls bullet"><span class="by">wave_1</span><span>|</span><a href="#40183053">root</a><span>|</span><a href="#40183299">parent</a><span>|</span><a href="#40183388">next</a><span>|</span><label class="collapse" for="c-40183389">[-]</label><label class="expand" for="c-40183389">[2 more]</label></div><br/><div class="children"><div class="content">Thanks. This is just in the labs stage, but moving closer to releasing it, exactly so that you can play with it! I have one angel investor involved in supporting this and it&#x27;s intended for commercial applications in the para legal space, initially (controlled, structured environment) But you just gave me the motivation to &quot;put it out there&quot; so people can just play with it. It&#x27;ll take a bit of time, but I will do a Show HN then when it&#x27;s ready for people to play with. Otherwise, it would be just teasing people to talk about it on the main HN stage without giving access. Hold tight! And thanks again!</div><br/><div id="40183452" class="c"><input type="checkbox" id="c-40183452" checked=""/><div class="controls bullet"><span class="by">Scipio_Afri</span><span>|</span><a href="#40183053">root</a><span>|</span><a href="#40183389">parent</a><span>|</span><a href="#40183388">next</a><span>|</span><label class="collapse" for="c-40183452">[-]</label><label class="expand" for="c-40183452">[1 more]</label></div><br/><div class="children"><div class="content">Will this or some parts of it be open sourced?</div><br/></div></div></div></div></div></div><div id="40183388" class="c"><input type="checkbox" id="c-40183388" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40183053">parent</a><span>|</span><a href="#40183299">prev</a><span>|</span><a href="#40183326">next</a><span>|</span><label class="collapse" for="c-40183388">[-]</label><label class="expand" for="c-40183388">[1 more]</label></div><br/><div class="children"><div class="content">Is there a write-up, a web site, and some benchmarks?</div><br/></div></div><div id="40183326" class="c"><input type="checkbox" id="c-40183326" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#40183053">parent</a><span>|</span><a href="#40183388">prev</a><span>|</span><a href="#40183633">next</a><span>|</span><label class="collapse" for="c-40183326">[-]</label><label class="expand" for="c-40183326">[1 more]</label></div><br/><div class="children"><div class="content">What are agents?<p>Are they layer 2 solutions like Lightning is to bitcoin?</div><br/></div></div></div></div><div id="40183633" class="c"><input type="checkbox" id="c-40183633" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40183053">prev</a><span>|</span><a href="#40180527">next</a><span>|</span><label class="collapse" for="c-40183633">[-]</label><label class="expand" for="c-40183633">[4 more]</label></div><br/><div class="children"><div class="content">&quot;The models, in other words, do not well generalise to understand the relationships between people.&quot;<p>Curiously, the need to do this well - handling the quadratic complexity of a shifting set of human relationships, grudges, and alliances - is thought to be one of the things that led is to higher levels of intelligence.</div><br/><div id="40185803" class="c"><input type="checkbox" id="c-40185803" checked=""/><div class="controls bullet"><span class="by">ryanblakeley</span><span>|</span><a href="#40183633">parent</a><span>|</span><a href="#40185765">prev</a><span>|</span><a href="#40180527">next</a><span>|</span><label class="collapse" for="c-40185803">[-]</label><label class="expand" for="c-40185803">[2 more]</label></div><br/><div class="children"><div class="content">Just to be clear, these models can answer questions about relationships between people if you mean family relationships.<p>Answering questions about what you&#x27;re describing sounds really interesting. What would a training set be like that describes a bunch of complex human relationships and then asks questions about them with objective answers?</div><br/><div id="40186122" class="c"><input type="checkbox" id="c-40186122" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40183633">root</a><span>|</span><a href="#40185803">parent</a><span>|</span><a href="#40180527">next</a><span>|</span><label class="collapse" for="c-40186122">[-]</label><label class="expand" for="c-40186122">[1 more]</label></div><br/><div class="children"><div class="content">Of course, it would be easy to put such questions together, and I&#x27;m sure the LLM would do fine with them - there&#x27;s a massive amount of human text about human relationships.<p>One difference, as in all ml training, is interactivity. Looking at ape studies, knowing the relationships is partly diagnostic, but it&#x27;s also about planning and competition. And that competitive&#x2F;adaptive aspect is what is what looks like a real evolutionary driver. If you can understand, navigate, and manipulate relationships successfully, you get more mating opportunities. Doing &#x2F;that&#x2F; well involves both reasoning and long term planning, both of which are apparent in chimps.<p>A good book on this topic is &#x27;are we smart enough to understand how  smart animals are&#x27; by Frans de Waal.<p><a href="https:&#x2F;&#x2F;wwnorton.com&#x2F;books&#x2F;Are-We-Smart-Enough-to-Know-How-Smart-Animals-Are&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wwnorton.com&#x2F;books&#x2F;Are-We-Smart-Enough-to-Know-How-S...</a></div><br/></div></div></div></div></div></div><div id="40180527" class="c"><input type="checkbox" id="c-40180527" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#40183633">prev</a><span>|</span><a href="#40180957">next</a><span>|</span><label class="collapse" for="c-40180527">[-]</label><label class="expand" for="c-40180527">[4 more]</label></div><br/><div class="children"><div class="content">Interesting, if I feed Mistral Le Chat with &quot;I fly a plane leaving my campsite, heading straight east for precisely 24,901 miles, and find myself back at the camp. I come upon seeing a tiger in my tent eating my food! What species is the tiger?&quot;, it gets it badly wrong:<p>The scenario you described is possible if you started at the South Pole. If you travel 24,901 miles east from there, you would indeed end up back at the same spot because all lines of longitude converge at the poles. However, there are no tigers in Antarctica.<p>Tigers are native to Asia, not Antarctica. The closest tiger species to Antarctica would be the Siberian tiger, found in parts of Russia, China, and North Korea, but they are still thousands of miles away from Antarctica.<p>So, while the travel scenario is theoretically possible, the presence of the tiger is not realistic in this context. It seems like an imaginative or hypothetical situation rather than a real-world one.<p>(instead of the answer mentioned in the article)</div><br/><div id="40184476" class="c"><input type="checkbox" id="c-40184476" checked=""/><div class="controls bullet"><span class="by">Karellen</span><span>|</span><a href="#40180527">parent</a><span>|</span><a href="#40183380">next</a><span>|</span><label class="collapse" for="c-40184476">[-]</label><label class="expand" for="c-40184476">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The scenario you described is possible if you started at the South Pole. If you travel 24,901 miles east from there, you would indeed end up back at the same spot<p>Would you? How can you travel east at all, starting from the South Pole? There is no east. Every direction you face is north.<p>OTOH, every line of latitude whose length is an integral divisor of 24,901 miles will satisfy the problem.</div><br/><div id="40186351" class="c"><input type="checkbox" id="c-40186351" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#40180527">root</a><span>|</span><a href="#40184476">parent</a><span>|</span><a href="#40183380">next</a><span>|</span><label class="collapse" for="c-40186351">[-]</label><label class="expand" for="c-40186351">[1 more]</label></div><br/><div class="children"><div class="content">Looks like Mistral overfitted on Google’s interview riddles!</div><br/></div></div></div></div><div id="40183380" class="c"><input type="checkbox" id="c-40183380" checked=""/><div class="controls bullet"><span class="by">GuB-42</span><span>|</span><a href="#40180527">parent</a><span>|</span><a href="#40184476">prev</a><span>|</span><a href="#40180957">next</a><span>|</span><label class="collapse" for="c-40183380">[-]</label><label class="expand" for="c-40183380">[1 more]</label></div><br/><div class="children"><div class="content">There is a similar, better known riddle involving the north pole and polar bears.<p>It probably hallucinated the answer from that.</div><br/></div></div></div></div><div id="40180957" class="c"><input type="checkbox" id="c-40180957" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#40180527">prev</a><span>|</span><a href="#40185740">next</a><span>|</span><label class="collapse" for="c-40180957">[-]</label><label class="expand" for="c-40180957">[1 more]</label></div><br/><div class="children"><div class="content">Some of these &quot;never do&quot; things are just artifacts of textual representation, and if you transformed wordl&#x2F;sudoku into a different domain it would have a much higher success rate using the exact same transformer architecture.<p>We don&#x27;t need to create custom AGI for every domain, we just need a model&#x2F;tool catalog and an agent that is able to reason well enough to decompose problems into  parts that can be farmed out to specialized tools then reassembled to form an answer.</div><br/></div></div><div id="40185740" class="c"><input type="checkbox" id="c-40185740" checked=""/><div class="controls bullet"><span class="by">weitendorf</span><span>|</span><a href="#40180957">prev</a><span>|</span><a href="#40179638">next</a><span>|</span><label class="collapse" for="c-40185740">[-]</label><label class="expand" for="c-40185740">[1 more]</label></div><br/><div class="children"><div class="content">So many of these examples are simply forgetting that LLMs experience the world through a 1-dimensional stream of tokens, while we experience those same tokens in 2 dimensions.<p>Try this: represent all those ASCII representations of games with the letter Q replacing the newline, to properly convert the encoding into a representation approximating what LLMs &quot;see&quot; (not a table, but a stream interspersed with Qs at a regular interval). Pretty hard right?<p>&gt; LLMs cannot reset their own context<p>If you have a model hooked up to something agentic, I don&#x27;t see why it couldn&#x27;t perform context manipulation on itself or even selective realtime finetuning. Think you&#x27;ll need info for the long haul, kick off some finetuning. Think you&#x27;d rather have one page of documentation in context than other, swap them out in one iteration. When you call LLMs over APIs you usually provide the entire context with each invocation...<p>&gt; Devin<p>It&#x27;s not that it&#x27;s massively smarter or agentic, just that it has the opportunity to correct its mistakes rather than committing to the first thing to come out of it (and is being handheld by a vastly more knowledgable SWE in its demos). You see cherrypicked examples (I also work on GenAI-for-coding) - just like a tragically incompetent employee could waste literal years on a project diligently plugging away at some task, so too can agentic models go off on a wild goose chase that accomplishes nothing besides making Nvidia more money. Just because something is highly persistent doesn&#x27;t mean it will &quot;converge&quot; on a correct outcome.</div><br/></div></div><div id="40179638" class="c"><input type="checkbox" id="c-40179638" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40185740">prev</a><span>|</span><a href="#40185543">next</a><span>|</span><label class="collapse" for="c-40179638">[-]</label><label class="expand" for="c-40179638">[14 more]</label></div><br/><div class="children"><div class="content">The article should be titled &quot; What can LLM never do, yet&quot;. By definition, Large Language Models would keep growing larger and larger, to be trained on faster and more advanced hardware, and certain points like &quot;completing complex chains of logical reasoning&quot; tasks, would be just a time hurdle. Only time will tell.</div><br/><div id="40179875" class="c"><input type="checkbox" id="c-40179875" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40179638">parent</a><span>|</span><a href="#40179914">next</a><span>|</span><label class="collapse" for="c-40179875">[-]</label><label class="expand" for="c-40179875">[4 more]</label></div><br/><div class="children"><div class="content">We really don&#x27;t need time to tell.<p>Just making a transformer bigger and bigger, and feeding it more and more data, will not change it from being a language model into something else, anymore than scaling up an expert system such as Cyc will transform it into something other than an expert system. &quot;Scale it up and it&#x27;ll become sentient&quot; is one of the recurring myths of AI.. a bit odd that people are falling for it again.<p>As an aside, it seems reasonable to consider an LLM as a type of expert system - one that has a broad area of expertise (like Cyc), including (unlike Cyc) how to infer rules from language and generate language from rules.<p>If you want to create a brain-like AGI, then you need an entire cognitive architecture, not just one piece of it which is what we have currently with LLMs. Compared to a brain, an LLM is maybe just like the cortex (without all the other brain parts like cerebellum, hippocampus, hypothalamus and interconnectivity such as the cortico-thalamic loop). It&#x27;s as if we&#x27;ve cut the cortex out of a dead person&#x27;s brain, put it in a mason jar to keep it alive, and hooked it&#x27;s inputs and outputs up to a computer. Feed words in, get words out. Cool, but it&#x27;s not a whole brain, it&#x27;s a cortex in a mason jar.</div><br/><div id="40184546" class="c"><input type="checkbox" id="c-40184546" checked=""/><div class="controls bullet"><span class="by">logicprog</span><span>|</span><a href="#40179638">root</a><span>|</span><a href="#40179875">parent</a><span>|</span><a href="#40179945">next</a><span>|</span><label class="collapse" for="c-40184546">[-]</label><label class="expand" for="c-40184546">[1 more]</label></div><br/><div class="children"><div class="content">Well said. This has always been my fundamental problem with the claims about large language models&#x27; current or eventual capabilities: most of the things people claim it can or will be able most of the things people claim it can or will be able to do require a neural architecture completely different from the one it has, and no amount of scaling up the number of neurons and the amount of training data used will change that fundamental architecture, and at a very basic level the capabilities of any neural network are going to be limited by its architecture. We would need to add some kind of advanced recursive structure to large language models, as well as some kind of short-term and working memory, as well as probably many other structures, to make them capable of the kind of metacognition necessary to properly do a lot of the things people want them to be able to do. Without metacognition, the ability to analyze what one is currently thinking and think new things based on that analysis, and therefore to look at what one is thinking and error correct it, consciously adjust it or iterate on it, or consciously ensure that one is adhering to certain principles of reasoning or knowledge, we can&#x27;t expect large language models to be able to actually understand Concepts and principles and how they are applicable and reliably perform reasoning or even obey instructions.</div><br/></div></div><div id="40179945" class="c"><input type="checkbox" id="c-40179945" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#40179638">root</a><span>|</span><a href="#40179875">parent</a><span>|</span><a href="#40184546">prev</a><span>|</span><a href="#40179914">next</a><span>|</span><label class="collapse" for="c-40179945">[-]</label><label class="expand" for="c-40179945">[2 more]</label></div><br/><div class="children"><div class="content">&gt;will not change it from being a language model into something else,<p>This is a pretty empty claim when we don&#x27;t know what the limits of language modelling are. Of course it will never not be a language model. But the question is what are the limits of capability of this class of computing device?</div><br/><div id="40180225" class="c"><input type="checkbox" id="c-40180225" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40179638">root</a><span>|</span><a href="#40179945">parent</a><span>|</span><a href="#40179914">next</a><span>|</span><label class="collapse" for="c-40180225">[-]</label><label class="expand" for="c-40180225">[1 more]</label></div><br/><div class="children"><div class="content">Some limit&#x27;s are pretty obvious, even if easy to fix.<p>For example, a pure LLM is just a single pass through a stack of transformer layers, so there is no variable depth&#x2F;duration (incl. iteration&#x2F;looping) of thought and no corresponding or longer duration working memory other than the embeddings as they pass thru. This is going to severely limit their ability to plan and reason since you only get a fixed N layers of reasoning regardless of what they are asked.<p>Lack of working memory (really needs to be context duration, or longer, not depth duration) has many predictable effects.<p>No doubt we will see pure-transformer architectures extended to add more capabilities, so I guess the real question is how far these extensions (+scaling) will get us. I think one thing we can be sure of though is that it won&#x27;t get us to AGI (defining AGI = human-level problem solving capability) unless we add ALL of the missing pieces that the brain has, not just a couple of the easy ones.</div><br/></div></div></div></div></div></div><div id="40179914" class="c"><input type="checkbox" id="c-40179914" checked=""/><div class="controls bullet"><span class="by">Mordisquitos</span><span>|</span><a href="#40179638">parent</a><span>|</span><a href="#40179875">prev</a><span>|</span><a href="#40179739">next</a><span>|</span><label class="collapse" for="c-40179914">[-]</label><label class="expand" for="c-40179914">[3 more]</label></div><br/><div class="children"><div class="content">I think that the article is correct. There are indeed things that L<i>L</i>Ms will never be able to do, at least not consistently, however much the hardware improves or on how much more material they are trained.<p>How come? Note my emphasis on the 2nd &#x27;L&#x27;. I&#x27;m not saying that there are things that <i>AI</i> models will never be able to do, I&#x27;m saying that there are things that Large <i>Language</i> Models will be unable to do.<p>Training LLMs is often argued to be analogous to human learning, most often as a defence against claims of copyright infringement by arguing that human creativity is also based on training from copyrighted materials. However, that is a red herring.<p>The responses from ever more powerful LLMs are indeed impressive, and beyond what an overwhelming majority of us believed possible just 5 years ago. They are nearing and sometimes surpassing the performance of educated humans in certain areas, so how come I can argue they are limited? Consider it from the other side: how come an educated human can create something as good as an LLM can when said human&#x27;s brain has been &quot;trained&quot; on an infinitesimal fraction of the material which was used to train even the 1st release of ChatGPT?<p>That is because LLMs do not learn nor reason like humans: they do not have opinions, do not have intentions, do not have doubts, do not have curiosity, do not have values, do not have a model of mind — they have tokens and probabilities.<p>For an AI model to be able to do certain things that humans can do it needs to have many of those human characteristics that allow us to do impressive mental feats having absorbed barely any training material (compared to LLMs) and being virtually unable to even remember most of it, let alone verbatim. Such an AI model is surely possible, but it needs a completely different paradigm from straightforward LLMs. That&#x27;s not to say however that a Language Model will almost certainly be an necessary module of such an AI, but it will not be sufficient.</div><br/><div id="40181348" class="c"><input type="checkbox" id="c-40181348" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#40179638">root</a><span>|</span><a href="#40179914">parent</a><span>|</span><a href="#40179739">next</a><span>|</span><label class="collapse" for="c-40181348">[-]</label><label class="expand" for="c-40181348">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think values, opinions or things like that are needed at all. These are just aspects we have in order to perform in and together with the society.<p>Also doubt is just uncertainty, and can be represented as a probability. Actually all values and everything can be presented as a numerical probability, which I personally prefer to do as well.</div><br/><div id="40182470" class="c"><input type="checkbox" id="c-40182470" checked=""/><div class="controls bullet"><span class="by">spacebanana7</span><span>|</span><a href="#40179638">root</a><span>|</span><a href="#40181348">parent</a><span>|</span><a href="#40179739">next</a><span>|</span><label class="collapse" for="c-40182470">[-]</label><label class="expand" for="c-40182470">[1 more]</label></div><br/><div class="children"><div class="content">Values and opinions drive human attention, which as transformers demonstrate, is relevant to reasoning.</div><br/></div></div></div></div></div></div><div id="40179739" class="c"><input type="checkbox" id="c-40179739" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#40179638">parent</a><span>|</span><a href="#40179914">prev</a><span>|</span><a href="#40179716">next</a><span>|</span><label class="collapse" for="c-40179739">[-]</label><label class="expand" for="c-40179739">[1 more]</label></div><br/><div class="children"><div class="content">The big question is if LLMs are capable enough to converge to AGI. It might very well be that as we pour in more resources that they converge to something only slightly more useful but similar as we have today.</div><br/></div></div><div id="40179716" class="c"><input type="checkbox" id="c-40179716" checked=""/><div class="controls bullet"><span class="by">g5g5g5g5</span><span>|</span><a href="#40179638">parent</a><span>|</span><a href="#40179739">prev</a><span>|</span><a href="#40179822">next</a><span>|</span><label class="collapse" for="c-40179716">[-]</label><label class="expand" for="c-40179716">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The article should be titled &quot; What can LLM never do, yet&quot;.<p>I don&#x27;t think it should. It&#x27;s more interesting to know what LLMs will _never_ be able to do (if anything).</div><br/><div id="40180269" class="c"><input type="checkbox" id="c-40180269" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#40179638">root</a><span>|</span><a href="#40179716">parent</a><span>|</span><a href="#40179822">next</a><span>|</span><label class="collapse" for="c-40180269">[-]</label><label class="expand" for="c-40180269">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but the article doesn&#x27;t really answer this question.</div><br/></div></div></div></div><div id="40179822" class="c"><input type="checkbox" id="c-40179822" checked=""/><div class="controls bullet"><span class="by">Quothling</span><span>|</span><a href="#40179638">parent</a><span>|</span><a href="#40179716">prev</a><span>|</span><a href="#40179733">next</a><span>|</span><label class="collapse" for="c-40179822">[-]</label><label class="expand" for="c-40179822">[1 more]</label></div><br/><div class="children"><div class="content">In the Danish public sector we provide services based on need assessments of citizens. Then we subsequently pay the bills for those services. Which amounts to thousands of small invoices having to be paid by a municipality each month. An example of this could be payments for a dentist visit, transportation and similar. Most of these are relatively small in size, and we&#x27;ve long since automated the payments of anything below a certain amount through automation. Systems which are faster and less error prone as far as putting valid data everywhere goes. They are more prone to decision making errors, however, and while fraud isn&#x27;t an issue, sometimes citizens have invoices approved that they aren&#x27;t entitled to. Since it&#x27;s less costly to just roll with those mistakes than to try and fix them, it&#x27;s an accepted loss.<p>The systems are hugely successful and popular, and this naturally leads to a massive interest in LLM&#x27;s as the next step. They are incredibly tools, but they are based on probability and while they&#x27;re lucky enough to be useful for almost everything. Decision making probably shouldn&#x27;t be one of them. Similarly ML is incredibly helpful in things like cancer detection , but we&#x27;ve already had issues where they got things wrong and because MBA&#x27;s don&#x27;t really know how they work, they were used as a replacement instead of an enhancement for the human factor. I&#x27;m fairly certain we&#x27;re going to use LLM&#x27;s for a lot of things where we shouldn&#x27;t, and probably never should. I&#x27;m not sure we can avoid it, but I wouldn&#x27;t personally trust them to do any sort of function which will have a big influence on peoples lives. I use both Co-pilot and OpenAI&#x27;s tools extensively, but I can still prompt them with the same thing and get extremely different quality outputs, and while this will improve, and while it&#x27;s very to get an output that&#x27;s actually useful, it&#x27;s still a major issue that might never get solved well enough for what we&#x27;re going to ask of the models way before they are ready.<p>I hope we&#x27;re going to be clever enough to only use them as enhancement tools in the vital public sector, but I&#x27;m sure we&#x27;re going to use them in areas like education. Which is going to be interesting... We already see this with new software developers in my area of the world, where they build things with the use of LLM&#x27;s, things that work, but aren&#x27;t build &quot;right&quot; and will eventually cause issues. For the most part this doesn&#x27;t matter, but you really don&#x27;t want the person designing your medical software to use a LLM.</div><br/></div></div><div id="40179733" class="c"><input type="checkbox" id="c-40179733" checked=""/><div class="controls bullet"><span class="by">gmuslera</span><span>|</span><a href="#40179638">parent</a><span>|</span><a href="#40179822">prev</a><span>|</span><a href="#40179718">next</a><span>|</span><label class="collapse" for="c-40179733">[-]</label><label class="expand" for="c-40179733">[1 more]</label></div><br/><div class="children"><div class="content">Maybe the wording is correct. Looks like a hard limit on doing what a LLM just do. If it goes beyond that, then is something more, or at least different, than a LLM.</div><br/></div></div><div id="40179718" class="c"><input type="checkbox" id="c-40179718" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#40179638">parent</a><span>|</span><a href="#40179733">prev</a><span>|</span><a href="#40185543">next</a><span>|</span><label class="collapse" for="c-40179718">[-]</label><label class="expand" for="c-40179718">[1 more]</label></div><br/><div class="children"><div class="content">Math reasoning is still a non solved problem even if the rest of the capabilities are getting better. This means the transformers architecture may not be the best way to approach all problems</div><br/></div></div></div></div><div id="40185543" class="c"><input type="checkbox" id="c-40185543" checked=""/><div class="controls bullet"><span class="by">amayne</span><span>|</span><a href="#40179638">prev</a><span>|</span><a href="#40184495">next</a><span>|</span><label class="collapse" for="c-40185543">[-]</label><label class="expand" for="c-40185543">[1 more]</label></div><br/><div class="children"><div class="content">Some of these can be done with the right prompting. Wordle was solved a year ago. The problem is that what works and what doesn’t can be counterintuitive or at least require you to think about how something is perceived at the token level.<p><a href="https:&#x2F;&#x2F;andrewmayne.com&#x2F;2023&#x2F;03&#x2F;29&#x2F;how-to-play-wordle-with-gpt-4-and-other-prompt-tricks&#x2F;" rel="nofollow">https:&#x2F;&#x2F;andrewmayne.com&#x2F;2023&#x2F;03&#x2F;29&#x2F;how-to-play-wordle-with-g...</a></div><br/></div></div><div id="40184495" class="c"><input type="checkbox" id="c-40184495" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#40185543">prev</a><span>|</span><a href="#40185809">next</a><span>|</span><label class="collapse" for="c-40184495">[-]</label><label class="expand" for="c-40184495">[2 more]</label></div><br/><div class="children"><div class="content">Mornington Crescent.  It will always win and hence lose and more importantly have no idea why.<p>Oh let&#x27;s be cerebral about this stuff and ignore silly British nonsense.  LLMs are a classic example of garbage in, garbage out, with a shonky curve fit veneer of science.<p>A next token guesser with a rather varied input quality is going to go off on one rather often.  Given that we all have a different idea of truth adds to the fun.<p>I take care that my monocle doesn&#x27;t snag in my lathe.  Do be careful with your nob when tickling your LLM inappropriately.</div><br/><div id="40184779" class="c"><input type="checkbox" id="c-40184779" checked=""/><div class="controls bullet"><span class="by">urbandw311er</span><span>|</span><a href="#40184495">parent</a><span>|</span><a href="#40185809">next</a><span>|</span><label class="collapse" for="c-40184779">[-]</label><label class="expand" for="c-40184779">[1 more]</label></div><br/><div class="children"><div class="content">I think if you prompted an LLM and explained what’s <i>really</i> going on with Mornington Crescent, it would probably do a reasonable job of playing the game.</div><br/></div></div></div></div><div id="40185809" class="c"><input type="checkbox" id="c-40185809" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40184495">prev</a><span>|</span><a href="#40179803">next</a><span>|</span><label class="collapse" for="c-40185809">[-]</label><label class="expand" for="c-40185809">[1 more]</label></div><br/><div class="children"><div class="content">My biggest concern with LLMs in programming, a complete loss of context. Unless the model is regularly trained on the latest codebase, code will always be generated in isolation. No real architectural decisions made with regards to reuse or testability, and no consideration for how the code will be used in 6 months or why the existing code is the way it is.<p>To anyone using LLMs for meaningful code, I wish you luck maintaining the code long term and hope you really do enjoy doing code reviews.</div><br/></div></div><div id="40179803" class="c"><input type="checkbox" id="c-40179803" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#40185809">prev</a><span>|</span><a href="#40186818">next</a><span>|</span><label class="collapse" for="c-40179803">[-]</label><label class="expand" for="c-40179803">[1 more]</label></div><br/><div class="children"><div class="content">It’s an auto regressive model so it can’t do anything that requires planning tokens.<p>It can’t do anything which implies a large or infinite token space (eg video understanding).<p>It’s also limited to a reasonable response length since token selection is probabilistic at each recursion. The longer you make it the more likely it is to veer off course.</div><br/></div></div><div id="40186818" class="c"><input type="checkbox" id="c-40186818" checked=""/><div class="controls bullet"><span class="by">JSDevOps</span><span>|</span><a href="#40179803">prev</a><span>|</span><a href="#40185597">next</a><span>|</span><label class="collapse" for="c-40186818">[-]</label><label class="expand" for="c-40186818">[1 more]</label></div><br/><div class="children"><div class="content">Plumbing. It’ll be regulated down it’s no better then a text processor. Remember the US innovates. The UK stagnates. EU regulates and China Replicates.</div><br/></div></div><div id="40185597" class="c"><input type="checkbox" id="c-40185597" checked=""/><div class="controls bullet"><span class="by">ChicagoDave</span><span>|</span><a href="#40186818">prev</a><span>|</span><a href="#40179936">next</a><span>|</span><label class="collapse" for="c-40185597">[-]</label><label class="expand" for="c-40185597">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been trying to get all the LLMs to do the same thing with the same lack of success.<p>I keep thinking there could be a way to iteratively train an LLM with declarative prompts, but as the article points out, it’s the chicken and egg problem. The LLM can’t provide a response unless it already knows the answer.<p>However, I believe this barrier will eventually be overcome. Just not anytime soon.</div><br/></div></div><div id="40179936" class="c"><input type="checkbox" id="c-40179936" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#40185597">prev</a><span>|</span><a href="#40185791">next</a><span>|</span><label class="collapse" for="c-40179936">[-]</label><label class="expand" for="c-40179936">[1 more]</label></div><br/><div class="children"><div class="content">It is interesting that all the examples I noticed in this article have a geometric aspect (even wordle - I model it as a grid with geometric rules when playing it). I think that the &quot;first grader&quot; comment is actually somewhat illuminating - it takes several years of learning how to navigate in a spatial world before this stuff becomes trivially easy.<p>The underlying point this article might be that LLMs don&#x27;t understand the non-textual aspects of a grid. Which is a fair point, they model language, not space. I wouldn&#x27;t expect text corpuses to explain space either, since possibly literally everyone who can read and write already knows a lot about spatial layouts.</div><br/></div></div><div id="40185791" class="c"><input type="checkbox" id="c-40185791" checked=""/><div class="controls bullet"><span class="by">sriku</span><span>|</span><a href="#40179936">prev</a><span>|</span><a href="#40179746">next</a><span>|</span><label class="collapse" for="c-40185791">[-]</label><label class="expand" for="c-40185791">[3 more]</label></div><br/><div class="children"><div class="content">A simpler question that seems to stump GPT4, llama3 (8b and 70b) so far - <a href="https:&#x2F;&#x2F;twitter.com&#x2F;srikumarks&#x2F;status&#x2F;1784214593146868071" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;srikumarks&#x2F;status&#x2F;1784214593146868071</a></div><br/><div id="40185815" class="c"><input type="checkbox" id="c-40185815" checked=""/><div class="controls bullet"><span class="by">sriku</span><span>|</span><a href="#40185791">parent</a><span>|</span><a href="#40179746">next</a><span>|</span><label class="collapse" for="c-40185815">[-]</label><label class="expand" for="c-40185815">[2 more]</label></div><br/><div class="children"><div class="content">An analogy I made to friend - language models capture the constraints in the arrangement of tokens in streams of communication. LLMs that model the constraints placed by human intelligence on token streams can no more be said to have attained (human)intelligence than physicists who decode the constraints placed by a god-like intelligence on the universe can be said to have attained god-like intelligence themselves. (Using comments by theist physicists to the tune of &quot;deciphering the mind of God&quot;)</div><br/><div id="40185893" class="c"><input type="checkbox" id="c-40185893" checked=""/><div class="controls bullet"><span class="by">pton_xd</span><span>|</span><a href="#40185791">root</a><span>|</span><a href="#40185815">parent</a><span>|</span><a href="#40179746">next</a><span>|</span><label class="collapse" for="c-40185893">[-]</label><label class="expand" for="c-40185893">[1 more]</label></div><br/><div class="children"><div class="content">&gt; language models capture the constraints in the arrangement of tokens in streams of communication<p>Yes but ultimately that includes all of math, logic, science, physics, etc. which as far as we can tell are fundamental truths of the universe. And if there&#x27;s a large enough LLM that can capture enough constraints, functionally what&#x27;s the difference between its intelligence and ours?</div><br/></div></div></div></div></div></div><div id="40179746" class="c"><input type="checkbox" id="c-40179746" checked=""/><div class="controls bullet"><span class="by">gwervc</span><span>|</span><a href="#40185791">prev</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40179746">[-]</label><label class="expand" for="c-40179746">[19 more]</label></div><br/><div class="children"><div class="content">&gt; But then I started asking myself how can we figure out the limits of its ability to reason<p>Third paragraph. The entire article is based on the premise LLMs are supposed to reason, which is wrong. They don&#x27;t, they&#x27;re tools to generate text.</div><br/><div id="40179780" class="c"><input type="checkbox" id="c-40179780" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40179746">parent</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40179780">[-]</label><label class="expand" for="c-40179780">[18 more]</label></div><br/><div class="children"><div class="content">I really hate this reductive, facile, &quot;um akshually&quot; take. If the text that the text-generating tool generates contains reasoning, then the text generation tool can be said to be reasoning, can&#x27;t it.<p>That&#x27;s like saying &quot;humans aren&#x27;t supposed to reason, they&#x27;re supposed to make sounds with their mouths&quot;.</div><br/><div id="40180985" class="c"><input type="checkbox" id="c-40180985" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179780">parent</a><span>|</span><a href="#40179863">next</a><span>|</span><label class="collapse" for="c-40180985">[-]</label><label class="expand" for="c-40180985">[1 more]</label></div><br/><div class="children"><div class="content">At some point if you need to generate better text you need to start creating a model of how the world works along with some amount of reasoning. The &quot;it&#x27;s just a token generator&quot; argument fails to get this part. That being said I don&#x27;t think just scaling LLMs are going to get us AGI but I don&#x27;t have any real arguments to support that</div><br/></div></div><div id="40179863" class="c"><input type="checkbox" id="c-40179863" checked=""/><div class="controls bullet"><span class="by">rambambram</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179780">parent</a><span>|</span><a href="#40180985">prev</a><span>|</span><a href="#40179833">next</a><span>|</span><label class="collapse" for="c-40179863">[-]</label><label class="expand" for="c-40179863">[9 more]</label></div><br/><div class="children"><div class="content">&gt; If the text that the text-generating tool generates contains reasoning, then the text generation tool can be said to be reasoning, can&#x27;t it.<p>I don&#x27;t know... you&#x27;re still describing a talking parrot here, if you&#x27;d ask me.</div><br/><div id="40182021" class="c"><input type="checkbox" id="c-40182021" checked=""/><div class="controls bullet"><span class="by">wbogusz</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179863">parent</a><span>|</span><a href="#40179878">next</a><span>|</span><label class="collapse" for="c-40182021">[-]</label><label class="expand" for="c-40182021">[2 more]</label></div><br/><div class="children"><div class="content">I’m not a fan of the talking parrot argument, especially when you’re pointing it at models of scale.<p>The only thing separating a talking parrot and humans is our accuracy in shaping our words to the context in which they’re spoken.<p>Sure it’s easy to liken a low resource model to a talking parrot, the output seems no better than selective repetition of training data. But is that really so different from a baby whose first words are mimics from the environment around them?<p>I would argue that as we learn language we implicitly develop the neural circuitry to continue to improve our lexical outputs, this circuitry being concepts like foresight, reasoning, emotion, logic, etc and that while we can take explicit action to teach these ideas, they naturally develop in isolation as well.<p>I don’t think language models, especially at scale, are much different. They would seem to similarly acquire implicit circuitry like we do as they are exposed to more data. As I see it, the main difference in what exactly that circuitry accomplishes and looks like in final output has more to do with the limited styles of data we can provide and the limitations of fine tuning we can apply on top.<p>Humans would seem to share a lot in common with talking parrots, we just have a lot more capable hardware to select what we repeat.</div><br/><div id="40183837" class="c"><input type="checkbox" id="c-40183837" checked=""/><div class="controls bullet"><span class="by">rambambram</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40182021">parent</a><span>|</span><a href="#40179878">next</a><span>|</span><label class="collapse" for="c-40183837">[-]</label><label class="expand" for="c-40183837">[1 more]</label></div><br/><div class="children"><div class="content">What if we were talking with each other and the right answer for me would be to kiss you on the cheek? Then what?</div><br/></div></div></div></div><div id="40179878" class="c"><input type="checkbox" id="c-40179878" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179863">parent</a><span>|</span><a href="#40182021">prev</a><span>|</span><a href="#40179942">next</a><span>|</span><label class="collapse" for="c-40179878">[-]</label><label class="expand" for="c-40179878">[5 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the difference between a human and a talking parrot that can answer any question you ask it?</div><br/><div id="40179992" class="c"><input type="checkbox" id="c-40179992" checked=""/><div class="controls bullet"><span class="by">cj</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179878">parent</a><span>|</span><a href="#40181001">next</a><span>|</span><label class="collapse" for="c-40179992">[-]</label><label class="expand" for="c-40179992">[3 more]</label></div><br/><div class="children"><div class="content">The talking parrot can only answer by repeating something it heard before.<p>Another question you could ask is “What’s the difference between a conversation between 2 people and a conversation between 2 parrots who can answer any question?”</div><br/><div id="40180040" class="c"><input type="checkbox" id="c-40180040" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179992">parent</a><span>|</span><a href="#40180151">next</a><span>|</span><label class="collapse" for="c-40180040">[-]</label><label class="expand" for="c-40180040">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t only repeat things it&#x27;s heard before. Have you ever talked with an LLM?</div><br/></div></div><div id="40180151" class="c"><input type="checkbox" id="c-40180151" checked=""/><div class="controls bullet"><span class="by">throwaway11460</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179992">parent</a><span>|</span><a href="#40180040">prev</a><span>|</span><a href="#40181001">next</a><span>|</span><label class="collapse" for="c-40180151">[-]</label><label class="expand" for="c-40180151">[1 more]</label></div><br/><div class="children"><div class="content">I had ChatGPT design a programming language for a specific uncommon purpose and then use it. It&#x27;s creative enough.</div><br/></div></div></div></div><div id="40181001" class="c"><input type="checkbox" id="c-40181001" checked=""/><div class="controls bullet"><span class="by">rambambram</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179878">parent</a><span>|</span><a href="#40179992">prev</a><span>|</span><a href="#40179942">next</a><span>|</span><label class="collapse" for="c-40181001">[-]</label><label class="expand" for="c-40181001">[1 more]</label></div><br/><div class="children"><div class="content">Can any question be answered? As long as any reaction on a question is considered an answer, then I see no difference between a human and a parrot.</div><br/></div></div></div></div><div id="40179942" class="c"><input type="checkbox" id="c-40179942" checked=""/><div class="controls bullet"><span class="by">dTal</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179863">parent</a><span>|</span><a href="#40179878">prev</a><span>|</span><a href="#40179833">next</a><span>|</span><label class="collapse" for="c-40179942">[-]</label><label class="expand" for="c-40179942">[1 more]</label></div><br/><div class="children"><div class="content">I feel the use of the word &quot;parrot&quot; is unintentionally apt, given that parrots were long thought to be mere mimics but were ultimately shown to have (at least the capacity for) real linguistic understanding.</div><br/></div></div></div></div><div id="40179833" class="c"><input type="checkbox" id="c-40179833" checked=""/><div class="controls bullet"><span class="by">Findecanor</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179780">parent</a><span>|</span><a href="#40179863">prev</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40179833">[-]</label><label class="expand" for="c-40179833">[7 more]</label></div><br/><div class="children"><div class="content">Even if the generated text contains reasoning, could the LLM understand and apply it?</div><br/><div id="40179849" class="c"><input type="checkbox" id="c-40179849" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179833">parent</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40179849">[-]</label><label class="expand" for="c-40179849">[6 more]</label></div><br/><div class="children"><div class="content">If I tell GPT-4 to print something, it understands it needs to check if my printer is turned on first and turn it on if it&#x27;s not, so, yes?<p>Also, if the generated text contains reasoning, what&#x27;s your definition of &quot;understanding&quot;? Is it &quot;must be made of the same stuff brains are&quot;?</div><br/><div id="40180076" class="c"><input type="checkbox" id="c-40180076" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40179849">parent</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40180076">[-]</label><label class="expand" for="c-40180076">[5 more]</label></div><br/><div class="children"><div class="content">LLMs fail at so many reasoning tasks (not unlike humans to be fair) that they are either incapable or really poor at reasoning. As far as reasoning machines go, I suspect LLMs will be a dead end.<p>Reasoning here meaning, for example, given a certain situation or issue described being able to answer questions about implications, applications, and outcome of such a situation. In my experience things quickly degenerate into technobabble for non-trivial issues (also not unlike humans).</div><br/><div id="40180174" class="c"><input type="checkbox" id="c-40180174" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40180076">parent</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40180174">[-]</label><label class="expand" for="c-40180174">[4 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re contending that LLMs are incapable of reasoning, you&#x27;re saying that there&#x27;s no reasoning task that an LLM can do. Is that what you&#x27;re saying? Because I can easily find an example to prove you wrong.</div><br/><div id="40180305" class="c"><input type="checkbox" id="c-40180305" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40180174">parent</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40180305">[-]</label><label class="expand" for="c-40180305">[3 more]</label></div><br/><div class="children"><div class="content">It could be that all reasoning displayed is showing existing information - so there would be no reasoning, but that aside, what I meant is being able to reason in any consistent way. Like a machine that only sometimes gets an addition right isn&#x27;t really capable of addition.</div><br/><div id="40180566" class="c"><input type="checkbox" id="c-40180566" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40180305">parent</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40180566">[-]</label><label class="expand" for="c-40180566">[2 more]</label></div><br/><div class="children"><div class="content">The former is easy to test, just make up your own puzzles and see if it can solve them.<p>&quot;Incapable of reasoning&quot; doesn&#x27;t mean &quot;only solves some logic puzzles&quot;. Hell, GPT-4 is better at reasoning than a large number of people. Would you say that a good percentage of humans are poor at reasoning too?</div><br/><div id="40180648" class="c"><input type="checkbox" id="c-40180648" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#40179746">root</a><span>|</span><a href="#40180566">parent</a><span>|</span><a href="#40186825">next</a><span>|</span><label class="collapse" for="c-40180648">[-]</label><label class="expand" for="c-40180648">[1 more]</label></div><br/><div class="children"><div class="content">Not just logic puzzles but also applying information, and, yes, I tried a few things.<p>People&#x2F;humans tend to be pretty poor, too (training can help, though), as it isn&#x27;t easy to really think through and solve things - we don&#x27;t have a general recipe to follow there and neither do LLMs it seems (otherwise it shouldn&#x27;t fail).<p>What I am getting at is that as far as a reasoning machine is concerned, I&#x27;d want it to be like a pocket calculator is for arithmetic, i.e., it doesn&#x27;t fail other than in some rare exceptions - and not inheriting human weaknesses there.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40186825" class="c"><input type="checkbox" id="c-40186825" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#40179746">prev</a><span>|</span><a href="#40186002">next</a><span>|</span><label class="collapse" for="c-40186825">[-]</label><label class="expand" for="c-40186825">[1 more]</label></div><br/><div class="children"><div class="content">Detect when humans are lying.</div><br/></div></div><div id="40186002" class="c"><input type="checkbox" id="c-40186002" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40186825">prev</a><span>|</span><a href="#40179653">next</a><span>|</span><label class="collapse" for="c-40186002">[-]</label><label class="expand" for="c-40186002">[1 more]</label></div><br/><div class="children"><div class="content">“Never” is a long time. I wouldn’t bet on that. It pays to remember that even the oldest SOTA model, GPT-4 is only just over a year old.</div><br/></div></div><div id="40179653" class="c"><input type="checkbox" id="c-40179653" checked=""/><div class="controls bullet"><span class="by">puttycat</span><span>|</span><a href="#40186002">prev</a><span>|</span><a href="#40179913">next</a><span>|</span><label class="collapse" for="c-40179653">[-]</label><label class="expand" for="c-40179653">[2 more]</label></div><br/><div class="children"><div class="content">Simple addition, among other things:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;0xnurl&#x2F;gpts-cant-count">https:&#x2F;&#x2F;github.com&#x2F;0xnurl&#x2F;gpts-cant-count</a></div><br/><div id="40179660" class="c"><input type="checkbox" id="c-40179660" checked=""/><div class="controls bullet"><span class="by">anshumankmr</span><span>|</span><a href="#40179653">parent</a><span>|</span><a href="#40179913">next</a><span>|</span><label class="collapse" for="c-40179660">[-]</label><label class="expand" for="c-40179660">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, GPT 4 Turbo in my experience used to get simple counting problems wrong, like adding up a long sequence of 4 digit numbers.</div><br/></div></div></div></div><div id="40179913" class="c"><input type="checkbox" id="c-40179913" checked=""/><div class="controls bullet"><span class="by">reqo</span><span>|</span><a href="#40179653">prev</a><span>|</span><a href="#40184077">next</a><span>|</span><label class="collapse" for="c-40179913">[-]</label><label class="expand" for="c-40179913">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This ‘goal drift’ means that agents, or tasks done in a sequence with iteration, get less reliable. It ‘forgets’ where to focus, because its attention is not selective nor dynamic.<p>I don&#x27;t know if I agree with this. The attention module is specifically designed to be selective and dynamic, otherwise it would not be much different than a word embedding (look up &quot;soft&quot; weights vs &quot;hard&quot; weights [1]). 
I think deep learning should not be confused with deep RL. LLMs are autoregressive models which means that they are trained to predict the next token and that is all they do. The next token is not necessarily the most reasonable (this is why datasets are super important for better performance). Deep RL models on the other hand, seem to be excellent at agency and decision making (although in restricted environment), because they are trained to do so.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Attention_(machine_learning)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Attention_(machine_learning)</a></div><br/><div id="40179939" class="c"><input type="checkbox" id="c-40179939" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40179913">parent</a><span>|</span><a href="#40184077">next</a><span>|</span><label class="collapse" for="c-40179939">[-]</label><label class="expand" for="c-40179939">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are fine-tuned with RL too. They are NOT simply next token predictors. RLHF uses whole answers at once to generate gradients, so it is looking further into the future. This might not be perfect but it is clearly more than focusing just 1 token ahead.<p>In the future the RL part of LLM training will increase a lot. Why am I saying this? There are two sources for learning - the past and the present. Training on human text is using past data, that is off-policy. But training on interactive data is on-policy. There is nothing we know that doesn&#x27;t come from the environment. What is not written in any books must be learned from outside.<p>That is why I think supervised pre-training from human text is just half the story and RL based agent learning, interactivity in other words, is the next step. The two feed on which intelligence stands are language (past experience) and environment (present experience). We can&#x27;t get ahead without both of them.<p>AlphaZero showed what an agent can learn from an environment alone, and LLMs show what they can learn from humans. But the world is big, there are plenty of environments that can provide learning signal, in other words feedback to LLMs.</div><br/></div></div></div></div><div id="40184077" class="c"><input type="checkbox" id="c-40184077" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#40179913">prev</a><span>|</span><a href="#40184609">next</a><span>|</span><label class="collapse" for="c-40184077">[-]</label><label class="expand" for="c-40184077">[1 more]</label></div><br/><div class="children"><div class="content">LLMs can never experiment with&#x2F;in the real world to find answers to open questions.<p>That&#x27;s the summary of &quot;The book of Why&quot; in one sentence as I understand it.</div><br/></div></div><div id="40184609" class="c"><input type="checkbox" id="c-40184609" checked=""/><div class="controls bullet"><span class="by">mathstuf</span><span>|</span><a href="#40184077">prev</a><span>|</span><a href="#40184754">next</a><span>|</span><label class="collapse" for="c-40184609">[-]</label><label class="expand" for="c-40184609">[2 more]</label></div><br/><div class="children"><div class="content">Things I&#x27;ve seen stump the ones I&#x27;ve played with so far (admittedly, not a lot): playing&#x2F;generating &quot;MadGab&quot; puzzles and ASCII art rendering&#x2F;interpretation. I&#x27;ve also asked ChatGPT3.5 to phonetically transliterate from English to other orthographies using the typical sounds of said orthography and it was…OK at it.</div><br/><div id="40184895" class="c"><input type="checkbox" id="c-40184895" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#40184609">parent</a><span>|</span><a href="#40184754">next</a><span>|</span><label class="collapse" for="c-40184895">[-]</label><label class="expand" for="c-40184895">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 can be surprisingly good at doing straight up IPA transcriptions, as well as informal transliterations, for languages that it knows. Not perfect, but definitely good enough for &quot;Babelfish&quot; kind of use.</div><br/></div></div></div></div><div id="40184754" class="c"><input type="checkbox" id="c-40184754" checked=""/><div class="controls bullet"><span class="by">tacocataco</span><span>|</span><a href="#40184609">prev</a><span>|</span><label class="collapse" for="c-40184754">[-]</label><label class="expand" for="c-40184754">[1 more]</label></div><br/><div class="children"><div class="content">Find a way to get humans to love each other.</div><br/></div></div></div></div></div></div></div></body></html>