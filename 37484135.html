<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1694595675911" as="style"/><link rel="stylesheet" href="styles.css?v=1694595675911"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Fine-tune your own Llama 2 to replace GPT-3.5/4</a> </div><div class="subtext"><span>kcorbitt</span> | <span>156 comments</span></div><br/><div><div id="37485407" class="c"><input type="checkbox" id="c-37485407" checked=""/><div class="controls bullet"><span class="by">ronyfadel</span><span>|</span><a href="#37484521">next</a><span>|</span><label class="collapse" for="c-37485407">[-]</label><label class="expand" for="c-37485407">[75 more]</label></div><br/><div class="children"><div class="content">For translation jobs, I&#x27;ve experimented with Llama 2 70B (running on Replicate) v&#x2F;s GPT-3.5;<p>For about 1000 input tokens (and resulting 1000 output tokens), to my surprise, GPT-3.5 turbo was <i>100x cheaper</i> than Llama 2.<p>Llama 7B wasn&#x27;t up to the task fyi, producing very poor translations.<p>I believe that OpenAI priced GPT-3.5 aggressively cheap in order to make it a non-brainer to rely on them rather than relying on other vendors (even open source models).<p>I&#x27;m curious to see if others have gotten different results?</div><br/><div id="37485645" class="c"><input type="checkbox" id="c-37485645" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37485786">next</a><span>|</span><label class="collapse" for="c-37485645">[-]</label><label class="expand" for="c-37485645">[3 more]</label></div><br/><div class="children"><div class="content">Yes, if you&#x27;re just using Llama 2 off the shelf (without fine-tuning) I don&#x27;t think there are a lot of workloads where it makes sense as a replacement for GPT-3.5. The one exception being for organizations where data security is non-negotiable and they really need to host on-prem. The calculus changes drastically though when you bring fine-tuning in, which lets a much smaller model outperform a larger one on many classes of task.<p>Also, it&#x27;s worth noting that Replicate started out with a focus on image generation, and their current inference stack for LLMs is extremely inefficient. A significant fraction of the 100x cost difference you mentioned can be made up by using an optimized inference server like vLLM. Replicate knows about this and is working hard on improving their stack, it&#x27;s just really early for all of us. :)</div><br/><div id="37488474" class="c"><input type="checkbox" id="c-37488474" checked=""/><div class="controls bullet"><span class="by">bfirsh</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485645">parent</a><span>|</span><a href="#37485786">next</a><span>|</span><label class="collapse" for="c-37488474">[-]</label><label class="expand" for="c-37488474">[2 more]</label></div><br/><div class="children"><div class="content">Founder of Replicate here. It&#x27;s early indeed.<p>OpenAI aren&#x27;t doing anything magic. We&#x27;re optimizing Llama inference at the moment and it looks like we&#x27;ll be able to roughly match GPT 3.5&#x27;s price for Llama 2 70B.<p>Running a fine-tuned GPT-3.5 is surprisingly expensive. That&#x27;s where using Llama makes a ton of sense. Once we’ve optimized inference, it’ll be much cheaper to run a fine-tuned Llama.</div><br/><div id="37492776" class="c"><input type="checkbox" id="c-37492776" checked=""/><div class="controls bullet"><span class="by">yixu34</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37488474">parent</a><span>|</span><a href="#37485786">next</a><span>|</span><label class="collapse" for="c-37492776">[-]</label><label class="expand" for="c-37492776">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re working on LLM Engine (<a href="https:&#x2F;&#x2F;llm-engine.scale.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm-engine.scale.com</a>) at Scale, which is our open source, self-hostable framework for open source LLM inference and fine-tuning. We have similar findings to Replicate: Llama 2 70B can be comparable to GPT 3.5 price, etc. Would be great to discuss this further!</div><br/></div></div></div></div></div></div><div id="37485786" class="c"><input type="checkbox" id="c-37485786" checked=""/><div class="controls bullet"><span class="by">Arctic_fly</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37485645">prev</a><span>|</span><a href="#37486442">next</a><span>|</span><label class="collapse" for="c-37485786">[-]</label><label class="expand" for="c-37485786">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Llama 7B wasn&#x27;t up to the task fyi, producing very poor translations.<p>From what I&#x27;ve read and personally experimented with, none of the Llama 2 models are well-suited to translation in particular (they were mainly trained on English data). Still, there are a number of tasks that they&#x27;re really good at if fine-tuned correctly, such as classification and data extraction.<p>&gt; I believe that OpenAI priced GPT-3.5 aggressively cheap in order to make it a non-brainer to rely on them rather than relying on other vendors (even open source models).<p>I think you&#x27;re definitely right about that, and in most cases just using GPT 3.5 for one-off tasks makes the most sense. I think when you get into production workflows that scale, that&#x27;s when using a small fine-tuned models starts making more sense. You can drop the system prompt and get data in the format you&#x27;d expect it in, and train on GPT-4&#x27;s output to sometimes get better accuracy than 3.5 would give you right off the bat. And keep in mind, while you can do the same thing with a fine-tuned 3.5 model, it&#x27;s going to cost 8x the base 3.5 price per token.</div><br/><div id="37488904" class="c"><input type="checkbox" id="c-37488904" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485786">parent</a><span>|</span><a href="#37486442">next</a><span>|</span><label class="collapse" for="c-37488904">[-]</label><label class="expand" for="c-37488904">[2 more]</label></div><br/><div class="children"><div class="content">Is that because translation is typically an encoder-decoder task and llama is decoder only or is there something else about it that makes the last difficult for llama?</div><br/><div id="37489681" class="c"><input type="checkbox" id="c-37489681" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37488904">parent</a><span>|</span><a href="#37486442">next</a><span>|</span><label class="collapse" for="c-37489681">[-]</label><label class="expand" for="c-37489681">[1 more]</label></div><br/><div class="children"><div class="content">If you don&#x27;t make it learn other-language texts, it won&#x27;t be able to speak that language.</div><br/></div></div></div></div></div></div><div id="37486442" class="c"><input type="checkbox" id="c-37486442" checked=""/><div class="controls bullet"><span class="by">AnonymousPlanet</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37485786">prev</a><span>|</span><a href="#37489601">next</a><span>|</span><label class="collapse" for="c-37486442">[-]</label><label class="expand" for="c-37486442">[7 more]</label></div><br/><div class="children"><div class="content">Cost isn&#x27;t the only incentive not to use an LLM service that resides in a foreign country. Around here, there are industries for which it&#x27;s pretty much a no-brainer to <i>avoid</i> anything that sends data across the atlantic.</div><br/><div id="37486780" class="c"><input type="checkbox" id="c-37486780" checked=""/><div class="controls bullet"><span class="by">unoti</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486442">parent</a><span>|</span><a href="#37489601">next</a><span>|</span><label class="collapse" for="c-37486780">[-]</label><label class="expand" for="c-37486780">[6 more]</label></div><br/><div class="children"><div class="content">Although it wouldn&#x27;t surprise me if today&#x27;s Azure OpenAI offerings route to certain US-centric regions, I&#x27;d be very surprised if Azure isn&#x27;t working day and night to try to provision OpenAI capacity everywhere they can in the world.<p>(Disclaimer: I work in the cloud organization at Microsoft, and these are totally my own thoughts and opinions and don&#x27;t reflect any kind of inside knowledge I have.  I think I can say that provisioning LLM capacity and GPU&#x27;s is something we basically all have a tremendous amount of passion about.)</div><br/><div id="37490773" class="c"><input type="checkbox" id="c-37490773" checked=""/><div class="controls bullet"><span class="by">deet</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486780">parent</a><span>|</span><a href="#37489824">next</a><span>|</span><label class="collapse" for="c-37490773">[-]</label><label class="expand" for="c-37490773">[1 more]</label></div><br/><div class="children"><div class="content">Azure GPT 4 is already available in: Australia East, Canada East, East US, East US 2, France Central, Japan East, Sweden Central, Switzerland North, UK South (<a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;ai-services&#x2F;openai&#x2F;concepts&#x2F;models#gpt-4" rel="nofollow noreferrer">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;ai-services&#x2F;openai&#x2F;c...</a>)</div><br/></div></div><div id="37489824" class="c"><input type="checkbox" id="c-37489824" checked=""/><div class="controls bullet"><span class="by">AnonymousPlanet</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486780">parent</a><span>|</span><a href="#37490773">prev</a><span>|</span><a href="#37489601">next</a><span>|</span><label class="collapse" for="c-37489824">[-]</label><label class="expand" for="c-37489824">[4 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s say a French company would offer the same service in the US, swearing no data would be ever siphoned out of the US and no French intelligence service would be allowed to review the data. Would you be comfortable with your patient records being stored there or the business secrets of US companies?<p>Do you believe Microsoft can actually make the same promises and keep them? You don&#x27;t have to answer the last question, of course, but please think about it. It doesn&#x27;t matter <i>where</i> the LLM is located but who controls it and who holds the resulting data.</div><br/><div id="37493525" class="c"><input type="checkbox" id="c-37493525" checked=""/><div class="controls bullet"><span class="by">ozgune</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489824">parent</a><span>|</span><a href="#37490874">next</a><span>|</span><label class="collapse" for="c-37493525">[-]</label><label class="expand" for="c-37493525">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this is a promise Microsoft can make. The US Cloud Act states that Microsoft falls under US jurisdiction and it&#x27;s legally bound to share foreign data if asked by US law enforcement.<p>&quot;The CLOUD Act asserts that U.S. data and communication companies must provide stored data for a customer or subscriber on any server they own and operate when requested by warrant, but provides mechanisms for the companies or the courts to reject or challenge these if they believe the request violates the privacy rights of the foreign country the data is stored in.&quot;<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CLOUD_Act" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CLOUD_Act</a></div><br/></div></div><div id="37490874" class="c"><input type="checkbox" id="c-37490874" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489824">parent</a><span>|</span><a href="#37493525">prev</a><span>|</span><a href="#37490594">next</a><span>|</span><label class="collapse" for="c-37490874">[-]</label><label class="expand" for="c-37490874">[1 more]</label></div><br/><div class="children"><div class="content">Worldwide big corps already utilized Microsoft 365 especially SharePoint. That&#x27;s Microsoft&#x27;s advantage.</div><br/></div></div><div id="37490594" class="c"><input type="checkbox" id="c-37490594" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489824">parent</a><span>|</span><a href="#37490874">prev</a><span>|</span><a href="#37489601">next</a><span>|</span><label class="collapse" for="c-37490594">[-]</label><label class="expand" for="c-37490594">[1 more]</label></div><br/><div class="children"><div class="content">I do think large tech companies do pretty well with customer data. As a former Googler I would be comfortable with my Gmail data residing in a foreign datacenter.</div><br/></div></div></div></div></div></div></div></div><div id="37489601" class="c"><input type="checkbox" id="c-37489601" checked=""/><div class="controls bullet"><span class="by">ttt3ts</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37486442">prev</a><span>|</span><a href="#37486556">next</a><span>|</span><label class="collapse" for="c-37489601">[-]</label><label class="expand" for="c-37489601">[7 more]</label></div><br/><div class="children"><div class="content">You can run 70B LLAMA on dual 4090s&#x2F;3090s with quantization. Going with dual 3090s you can get a system that can run LLAMA 2 70B with 12K context for &lt; $2K.<p>I built two such a systems after burning that much in a week on ChatGPT.</div><br/><div id="37492100" class="c"><input type="checkbox" id="c-37492100" checked=""/><div class="controls bullet"><span class="by">coryrc</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489601">parent</a><span>|</span><a href="#37489754">next</a><span>|</span><label class="collapse" for="c-37492100">[-]</label><label class="expand" for="c-37492100">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I built two such a systems after burning that much in a week on ChatGPT.<p>What are you doing!?</div><br/></div></div><div id="37489754" class="c"><input type="checkbox" id="c-37489754" checked=""/><div class="controls bullet"><span class="by">zakki</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489601">parent</a><span>|</span><a href="#37492100">prev</a><span>|</span><a href="#37490613">next</a><span>|</span><label class="collapse" for="c-37489754">[-]</label><label class="expand" for="c-37489754">[3 more]</label></div><br/><div class="children"><div class="content">Would you mind to share all your PC HW (mobo, casing, cooling, etc) for this dual GPU configuration? Thanks.</div><br/><div id="37490173" class="c"><input type="checkbox" id="c-37490173" checked=""/><div class="controls bullet"><span class="by">ttt3ts</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489754">parent</a><span>|</span><a href="#37490613">next</a><span>|</span><label class="collapse" for="c-37490173">[-]</label><label class="expand" for="c-37490173">[2 more]</label></div><br/><div class="children"><div class="content">The one you could build for under 2K is last gen hardware.<p>* Chenbro Rackmount 4U Server Chassis RM42300-F (rack mount case  Remove the air filter on 120mm fan. Put two decent 80mm exhaust at rear).
* Two used air cooled 3090s. About $650 a piece on ebay. Check slot width and make sure everything will fit on your motherboard. Do a burn in when you get them cause used GPUs can be hit or miss.
* 5950x CPU (overkill just had it)
* 128GB DDR4
* Motherboard with x570 chipset and dual pcie x16. These will birificate to x8 pcie 4.0 lanes to each GPU. This is enough bandwidth to push GPUs to max IME
* 1200W+ ATX power supply.
* ebay &quot;u.2 pcie 3.84TB&quot; and adaptor for m.2 NVME slot. (again what I had &amp; it is cheap)<p>If you&#x27;re going to really beat the thing I would power limit the 3090s to 320w (from 350w). Perf change is not really notable and keeps temps better.</div><br/><div id="37490412" class="c"><input type="checkbox" id="c-37490412" checked=""/><div class="controls bullet"><span class="by">zakki</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37490173">parent</a><span>|</span><a href="#37490613">next</a><span>|</span><label class="collapse" for="c-37490412">[-]</label><label class="expand" for="c-37490412">[1 more]</label></div><br/><div class="children"><div class="content">Thank you so much.</div><br/></div></div></div></div></div></div><div id="37490613" class="c"><input type="checkbox" id="c-37490613" checked=""/><div class="controls bullet"><span class="by">apstls</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489601">parent</a><span>|</span><a href="#37489754">prev</a><span>|</span><a href="#37486556">next</a><span>|</span><label class="collapse" for="c-37490613">[-]</label><label class="expand" for="c-37490613">[2 more]</label></div><br/><div class="children"><div class="content">Are there any good resources related to expanding context windows, or even just the mechanics of how they actually work as properties of a model?</div><br/><div id="37490669" class="c"><input type="checkbox" id="c-37490669" checked=""/><div class="controls bullet"><span class="by">ttt3ts</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37490613">parent</a><span>|</span><a href="#37486556">next</a><span>|</span><label class="collapse" for="c-37490669">[-]</label><label class="expand" for="c-37490669">[1 more]</label></div><br/><div class="children"><div class="content">Lots. LLAMA 2 was trained on 4K context windows but can run on arbitrary length just the results become garbage as you go longer.<p>I refer you to <a href="https:&#x2F;&#x2F;blog.gopenai.com&#x2F;how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.gopenai.com&#x2F;how-to-speed-up-llms-and-use-100k-c...</a> for an &quot;easy&quot; to digest summary</div><br/></div></div></div></div></div></div><div id="37486556" class="c"><input type="checkbox" id="c-37486556" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37489601">prev</a><span>|</span><a href="#37485629">next</a><span>|</span><label class="collapse" for="c-37486556">[-]</label><label class="expand" for="c-37486556">[1 more]</label></div><br/><div class="children"><div class="content">TBH, Replicate is not a great way to run 7B beyond experimentation. You want a host with cheap consumer GPUs (like vast.ai) since the 4-bit requirements are so modest.<p>You either need a backend with good batching support (vLLM), or if you don&#x27;t need much throughput, an extremely low end GPU or no GPU at all for exLlama&#x2F;llama.cpp.<p>OpenAI benefits from quantization&#x2F;batching, optimized kernels and very high utilization on their end, so the huge price gap vs a default HF Transformers instance is understandable. But even then, you are probably right about their aggressive pricing.<p>As for quality, you need a llama model finetunes on the target language (many already exist on Huggingface) and possibly custom grammar if your backend supports it.</div><br/></div></div><div id="37485629" class="c"><input type="checkbox" id="c-37485629" checked=""/><div class="controls bullet"><span class="by">halflings</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37486556">prev</a><span>|</span><a href="#37489646">next</a><span>|</span><label class="collapse" for="c-37485629">[-]</label><label class="expand" for="c-37485629">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think translation is a great use case for ChatGPT and LLAMA. These models are overwhelmingly trained on English, and LLAMA2 which should have more data from other languages is still focused on languages w&#x2F; Latin&#x2F;Cyrillic characters (so won&#x27;t work well for Arabic, Hebrew, or CJK languages).<p>You&#x27;re better off using models specialized in translation; General purpose LLMs are more useful when fine-tuning on specific tasks (some form of extraction, summarization, generative tasks, etc.), or for general chatbot-like uses.</div><br/><div id="37487982" class="c"><input type="checkbox" id="c-37487982" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485629">parent</a><span>|</span><a href="#37488396">next</a><span>|</span><label class="collapse" for="c-37487982">[-]</label><label class="expand" for="c-37487982">[4 more]</label></div><br/><div class="children"><div class="content">&gt;You&#x27;re better off using models specialized in translation<p>For a couple dozen languages, GPT-4 is by far the best translator you can get your hand on so basically no.</div><br/><div id="37488376" class="c"><input type="checkbox" id="c-37488376" checked=""/><div class="controls bullet"><span class="by">daniels11</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487982">parent</a><span>|</span><a href="#37488396">next</a><span>|</span><label class="collapse" for="c-37488376">[-]</label><label class="expand" for="c-37488376">[3 more]</label></div><br/><div class="children"><div class="content">I will say that GPT-4 is just incredibly expensive. For my app I only use it for advanced translations&#x2F;corrections, and usually a combination of GPT-3.5+Wiktionary is able to get the more simple stuff done</div><br/><div id="37489050" class="c"><input type="checkbox" id="c-37489050" checked=""/><div class="controls bullet"><span class="by">all2</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37488376">parent</a><span>|</span><a href="#37488396">next</a><span>|</span><label class="collapse" for="c-37489050">[-]</label><label class="expand" for="c-37489050">[2 more]</label></div><br/><div class="children"><div class="content">&gt; GPT-3.5+Wiktionary<p>Can you share more about your app and what you&#x27;re doing?</div><br/><div id="37489346" class="c"><input type="checkbox" id="c-37489346" checked=""/><div class="controls bullet"><span class="by">daniels11</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489050">parent</a><span>|</span><a href="#37488396">next</a><span>|</span><label class="collapse" for="c-37489346">[-]</label><label class="expand" for="c-37489346">[1 more]</label></div><br/><div class="children"><div class="content">Sure! I&#x27;m building a personalized AI language learning tutor using Open AI&#x27;s API and ElevenLabs (for Text to Speech).<p>Right now it&#x27;s basically a chat bot that you can use to practice conversing with. It provides corrections for the things you type. Eventually I&#x27;d like to try adding Whisper as well to allow users to speak out loud.<p>When you hover over a word, you get a translation. Initially I thought using Open AI for every word translation would be too much, but I&#x27;ve been able to get it down to ~36-40 tokens&#x2F;request. (3-4 cents&#x2F;1000 requests). I also began parsing and uploading some of this [Wiktionary data](<a href="https:&#x2F;&#x2F;kaikki.org&#x2F;dictionary&#x2F;rawdata.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;kaikki.org&#x2F;dictionary&#x2F;rawdata.html</a>) and am working on a feature that integrates the GPT-3.5 translation with this Wiktionary data.<p>A lot of these features are still in the works but you can feel free to try it if you like (<a href="https:&#x2F;&#x2F;trytutor.app" rel="nofollow noreferrer">https:&#x2F;&#x2F;trytutor.app</a>).</div><br/></div></div></div></div></div></div></div></div><div id="37488396" class="c"><input type="checkbox" id="c-37488396" checked=""/><div class="controls bullet"><span class="by">achileas</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485629">parent</a><span>|</span><a href="#37487982">prev</a><span>|</span><a href="#37486376">next</a><span>|</span><label class="collapse" for="c-37488396">[-]</label><label class="expand" for="c-37488396">[1 more]</label></div><br/><div class="children"><div class="content">There are plenty of examples in the literature of using LLMs for translation beating the metrics of non-LLM models, even for languages for which there isn&#x27;t a lot of data. Transliterating non-Latin characters helps a lot with accuracy as well.</div><br/></div></div><div id="37486376" class="c"><input type="checkbox" id="c-37486376" checked=""/><div class="controls bullet"><span class="by">daniels11</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485629">parent</a><span>|</span><a href="#37488396">prev</a><span>|</span><a href="#37489646">next</a><span>|</span><label class="collapse" for="c-37486376">[-]</label><label class="expand" for="c-37486376">[1 more]</label></div><br/><div class="children"><div class="content">what models would you use for translation? I am working on a language learning tutor (trytutor.app, very early) and GPT-3.5 turbo has been working fine, for the most part.<p>For foreign language corrections (&quot;correct this German sentence and give a reason for the correction&quot;), GPT-3.5 doesn&#x27;t quite have the horsepower so I use GPT-4</div><br/></div></div></div></div><div id="37489646" class="c"><input type="checkbox" id="c-37489646" checked=""/><div class="controls bullet"><span class="by">nborwankar</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37485629">prev</a><span>|</span><a href="#37487305">next</a><span>|</span><label class="collapse" for="c-37489646">[-]</label><label class="expand" for="c-37489646">[1 more]</label></div><br/><div class="children"><div class="content">Llama and GPT are auto-regressive decoder only architectures which for pure translation jobs are not the optimal architectures. Training seq2seq models or encoder&#x2F;decoder models on datasets of sentence pairs designed for translation will likely allow you to use much smaller models. You will not be wasting parameters on general “language understanding” capability that Llama and GPT have if pure translation is all you need. T5 or Flan-T5 might be good starting points.</div><br/></div></div><div id="37487305" class="c"><input type="checkbox" id="c-37487305" checked=""/><div class="controls bullet"><span class="by">octacat</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37489646">prev</a><span>|</span><a href="#37492162">next</a><span>|</span><label class="collapse" for="c-37487305">[-]</label><label class="expand" for="c-37487305">[1 more]</label></div><br/><div class="children"><div class="content">Google Maps was also cheap. Initially.
So it is aggressively cheap now, but would aggressively change later.</div><br/></div></div><div id="37492162" class="c"><input type="checkbox" id="c-37492162" checked=""/><div class="controls bullet"><span class="by">yessenzhar</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37487305">prev</a><span>|</span><a href="#37486942">next</a><span>|</span><label class="collapse" for="c-37492162">[-]</label><label class="expand" for="c-37492162">[2 more]</label></div><br/><div class="children"><div class="content">We provide per token based Llama 2 70B API at Deep Infra, $1&#x2F;1M tokens, which is 25-50% cheaper than ChatGPT.</div><br/><div id="37492388" class="c"><input type="checkbox" id="c-37492388" checked=""/><div class="controls bullet"><span class="by">tuckerconnelly</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37492162">parent</a><span>|</span><a href="#37486942">next</a><span>|</span><label class="collapse" for="c-37492388">[-]</label><label class="expand" for="c-37492388">[1 more]</label></div><br/><div class="children"><div class="content">Can you provide a larger context length? Looking for a replacement of GPT-3.5 16k model. Might be interested for a higher-scale project.</div><br/></div></div></div></div><div id="37486942" class="c"><input type="checkbox" id="c-37486942" checked=""/><div class="controls bullet"><span class="by">mrybczyn</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37492162">prev</a><span>|</span><a href="#37486602">next</a><span>|</span><label class="collapse" for="c-37486942">[-]</label><label class="expand" for="c-37486942">[16 more]</label></div><br/><div class="children"><div class="content">Yes, openAI is dumping the market with chat-gpt 3.5.  Vulture capital behaviour at its finest, and I&#x27;m sure government regulations will definitely catch on to this in 20 or 30 years...<p>It&#x27;s cheaper than the ELECTRICITY cost of running a llama-70 on your own M1.Max (very energy efficient chip) assuming free hardware.<p>I guess they are also getting a pretty good cache hit rate - there are only so many questions people ask at scale.  But still, it&#x27;s dumping.</div><br/><div id="37487337" class="c"><input type="checkbox" id="c-37487337" checked=""/><div class="controls bullet"><span class="by">sacred_numbers</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486942">parent</a><span>|</span><a href="#37487386">next</a><span>|</span><label class="collapse" for="c-37487337">[-]</label><label class="expand" for="c-37487337">[3 more]</label></div><br/><div class="children"><div class="content">Based on my research, GPT-3.5 is likely significantly smaller than 70B parameters, so it would make sense that it&#x27;s cheaper to run.  My guess is that OpenAI significantly overtrained GPT-3.5 to get as small a model as possible to optimize for inference.  Also, Nvidia chips are way more efficient at inference than M1 Max.  OpenAI also has the advantage of batching API calls which leads to better hardware utilization. I don&#x27;t have definitive proof that they&#x27;re not dumping, but economies of scale and optimization seem like better explanations to me.</div><br/><div id="37487478" class="c"><input type="checkbox" id="c-37487478" checked=""/><div class="controls bullet"><span class="by">hutzlibu</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487337">parent</a><span>|</span><a href="#37488387">next</a><span>|</span><label class="collapse" for="c-37487478">[-]</label><label class="expand" for="c-37487478">[1 more]</label></div><br/><div class="children"><div class="content">I also do not have proof of anything here, but can&#x27;t it be both?<p>They have lots of money now and the market lead. They want to keep the lead and some extra electricity and hardware costs are surely worth it for them, if it keeps the competition from getting traction.</div><br/></div></div><div id="37488387" class="c"><input type="checkbox" id="c-37488387" checked=""/><div class="controls bullet"><span class="by">csjh</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487337">parent</a><span>|</span><a href="#37487478">prev</a><span>|</span><a href="#37487386">next</a><span>|</span><label class="collapse" for="c-37488387">[-]</label><label class="expand" for="c-37488387">[1 more]</label></div><br/><div class="children"><div class="content">What makes you think 3.5 is significantly smaller than 70B?</div><br/></div></div></div></div><div id="37487386" class="c"><input type="checkbox" id="c-37487386" checked=""/><div class="controls bullet"><span class="by">haxton</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486942">parent</a><span>|</span><a href="#37487337">prev</a><span>|</span><a href="#37487175">next</a><span>|</span><label class="collapse" for="c-37487386">[-]</label><label class="expand" for="c-37487386">[9 more]</label></div><br/><div class="children"><div class="content">gpt3.5 turbo is (mostly likely) Curie which is (most likely) 6.7b params. So, yeah, makes perfect sense that it can&#x27;t compete with a 70b model on cost.</div><br/><div id="37488963" class="c"><input type="checkbox" id="c-37488963" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487386">parent</a><span>|</span><a href="#37488379">next</a><span>|</span><label class="collapse" for="c-37488963">[-]</label><label class="expand" for="c-37488963">[1 more]</label></div><br/><div class="children"><div class="content">gpt3.5 turbo is a new model, not Curie. As others have stated, it probably uses Mixture of Experts which lowers inference cost.</div><br/></div></div><div id="37488379" class="c"><input type="checkbox" id="c-37488379" checked=""/><div class="controls bullet"><span class="by">csjh</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487386">parent</a><span>|</span><a href="#37488963">prev</a><span>|</span><a href="#37487507">next</a><span>|</span><label class="collapse" for="c-37488379">[-]</label><label class="expand" for="c-37488379">[1 more]</label></div><br/><div class="children"><div class="content">Is there a source on that? I&#x27;ve never seen anyone think it&#x27;s below even 70B</div><br/></div></div><div id="37487507" class="c"><input type="checkbox" id="c-37487507" checked=""/><div class="controls bullet"><span class="by">ronyfadel</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487386">parent</a><span>|</span><a href="#37488379">prev</a><span>|</span><a href="#37489487">next</a><span>|</span><label class="collapse" for="c-37487507">[-]</label><label class="expand" for="c-37487507">[4 more]</label></div><br/><div class="children"><div class="content">It still does a much better job at translation than llama 2 70b even, at 6.7b params</div><br/><div id="37487790" class="c"><input type="checkbox" id="c-37487790" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487507">parent</a><span>|</span><a href="#37489487">next</a><span>|</span><label class="collapse" for="c-37487790">[-]</label><label class="expand" for="c-37487790">[3 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s MOE that may explain why it&#x27;s faster and better...</div><br/><div id="37488411" class="c"><input type="checkbox" id="c-37488411" checked=""/><div class="controls bullet"><span class="by">yumraj</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487790">parent</a><span>|</span><a href="#37489487">next</a><span>|</span><label class="collapse" for="c-37488411">[-]</label><label class="expand" for="c-37488411">[2 more]</label></div><br/><div class="children"><div class="content">MOE?</div><br/><div id="37488533" class="c"><input type="checkbox" id="c-37488533" checked=""/><div class="controls bullet"><span class="by">sarthaksrinivas</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37488411">parent</a><span>|</span><a href="#37489487">next</a><span>|</span><label class="collapse" for="c-37488533">[-]</label><label class="expand" for="c-37488533">[1 more]</label></div><br/><div class="children"><div class="content">Mixture of Experts Model - <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mixture_of_experts" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mixture_of_experts</a></div><br/></div></div></div></div></div></div></div></div><div id="37489487" class="c"><input type="checkbox" id="c-37489487" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487386">parent</a><span>|</span><a href="#37487507">prev</a><span>|</span><a href="#37487175">next</a><span>|</span><label class="collapse" for="c-37489487">[-]</label><label class="expand" for="c-37489487">[2 more]</label></div><br/><div class="children"><div class="content">I thought it was fairly well established that GPT 3.5 has something like 130B parameters and that GPT 4 is on the order of 600-1,000</div><br/><div id="37493978" class="c"><input type="checkbox" id="c-37493978" checked=""/><div class="controls bullet"><span class="by">avion23</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37489487">parent</a><span>|</span><a href="#37487175">next</a><span>|</span><label class="collapse" for="c-37493978">[-]</label><label class="expand" for="c-37493978">[1 more]</label></div><br/><div class="children"><div class="content">I remember:<p>- gpt-3.5 175b params<p>- gpt-4 1800b params</div><br/></div></div></div></div></div></div><div id="37487175" class="c"><input type="checkbox" id="c-37487175" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486942">parent</a><span>|</span><a href="#37487386">prev</a><span>|</span><a href="#37487245">next</a><span>|</span><label class="collapse" for="c-37487175">[-]</label><label class="expand" for="c-37487175">[2 more]</label></div><br/><div class="children"><div class="content">You think they are caching? Even though one of the parameters is temperature? Can of worms, and should be reflected in the pricing if true, don&#x27;t get me started if they are charging per token for cached responses.<p>I just don&#x27;t see it.</div><br/><div id="37488983" class="c"><input type="checkbox" id="c-37488983" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487175">parent</a><span>|</span><a href="#37487245">next</a><span>|</span><label class="collapse" for="c-37488983">[-]</label><label class="expand" for="c-37488983">[1 more]</label></div><br/><div class="children"><div class="content">You can keep around the KV cache from previous generations which lowers the cost of prompts significantly.</div><br/></div></div></div></div><div id="37487245" class="c"><input type="checkbox" id="c-37487245" checked=""/><div class="controls bullet"><span class="by">read_if_gay_</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486942">parent</a><span>|</span><a href="#37487175">prev</a><span>|</span><a href="#37486602">next</a><span>|</span><label class="collapse" for="c-37487245">[-]</label><label class="expand" for="c-37487245">[1 more]</label></div><br/><div class="children"><div class="content">turbo is likely nowhere near 70b.</div><br/></div></div></div></div><div id="37486602" class="c"><input type="checkbox" id="c-37486602" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37486942">prev</a><span>|</span><a href="#37487891">next</a><span>|</span><label class="collapse" for="c-37486602">[-]</label><label class="expand" for="c-37486602">[8 more]</label></div><br/><div class="children"><div class="content">&gt;For about 1000 input tokens (and resulting 1000 output tokens), to my surprise, GPT-3.5 turbo was 100x cheaper than Llama 2.<p>You&#x27;ll never get actual economics out of switching to open models without running your own hardware. That&#x27;s the whole point. There&#x27;s orders of magnitude difference in price, where a single V100&#x2F;3090 instance can run llama2-70b inference for ~0.50$&#x2F;hr.</div><br/><div id="37486957" class="c"><input type="checkbox" id="c-37486957" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486602">parent</a><span>|</span><a href="#37487891">next</a><span>|</span><label class="collapse" for="c-37486957">[-]</label><label class="expand" for="c-37486957">[7 more]</label></div><br/><div class="children"><div class="content">No, they can&#x27;t run it. llama 70 with 4 bit quantization takes ~50 GB VRAM for decent enough context size. You need A100, or 2-3 V100 or 4 3090 which all costs roughly roughly $3-5&#x2F;h</div><br/><div id="37487046" class="c"><input type="checkbox" id="c-37487046" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486957">parent</a><span>|</span><a href="#37487891">next</a><span>|</span><label class="collapse" for="c-37487046">[-]</label><label class="expand" for="c-37487046">[6 more]</label></div><br/><div class="children"><div class="content">Wrong. I am running 8bit GGML with 24GB VRAM on a single 4090 with 2048 context right now</div><br/><div id="37487069" class="c"><input type="checkbox" id="c-37487069" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487046">parent</a><span>|</span><a href="#37487891">next</a><span>|</span><label class="collapse" for="c-37487069">[-]</label><label class="expand" for="c-37487069">[5 more]</label></div><br/><div class="children"><div class="content">Which model? I am talking about 70b as mentioned clearly. 70b 8b is 70GB just for the model itself. How much token&#x2F;second are you getting with single 4090?</div><br/><div id="37487140" class="c"><input type="checkbox" id="c-37487140" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487069">parent</a><span>|</span><a href="#37487891">next</a><span>|</span><label class="collapse" for="c-37487140">[-]</label><label class="expand" for="c-37487140">[4 more]</label></div><br/><div class="children"><div class="content">Offloading 40% of layers to CPU, about 50t&#x2F;s with 16 threads.</div><br/><div id="37487679" class="c"><input type="checkbox" id="c-37487679" checked=""/><div class="controls bullet"><span class="by">pocketarc</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487140">parent</a><span>|</span><a href="#37491113">next</a><span>|</span><label class="collapse" for="c-37487679">[-]</label><label class="expand" for="c-37487679">[2 more]</label></div><br/><div class="children"><div class="content">That is more than an order of magnitude better than my experience; I get around 2 t&#x2F;s with similar hardware. I had also seen others reporting similar figures to mine so I assumed it was normal. Is there a secret to what you&#x27;re doing?</div><br/><div id="37489606" class="c"><input type="checkbox" id="c-37489606" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487679">parent</a><span>|</span><a href="#37491113">next</a><span>|</span><label class="collapse" for="c-37489606">[-]</label><label class="expand" for="c-37489606">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Is there a secret to what you&#x27;re doing?<p>Core speed and memory bandwidth matter a lot. This is on a Ryzen 7950 with DDR5.</div><br/></div></div></div></div><div id="37491113" class="c"><input type="checkbox" id="c-37491113" checked=""/><div class="controls bullet"><span class="by">jpdus</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487140">parent</a><span>|</span><a href="#37487679">prev</a><span>|</span><a href="#37487891">next</a><span>|</span><label class="collapse" for="c-37491113">[-]</label><label class="expand" for="c-37491113">[1 more]</label></div><br/><div class="children"><div class="content">Care to share your detailed stack and command to reach 50t&#x2F;s? I also have a 7950 with DDR 5 and I don&#x27;t even get 50 t&#x2F;s on my two RTX 4090s....</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37487891" class="c"><input type="checkbox" id="c-37487891" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37486602">prev</a><span>|</span><a href="#37485902">next</a><span>|</span><label class="collapse" for="c-37487891">[-]</label><label class="expand" for="c-37487891">[1 more]</label></div><br/><div class="children"><div class="content">Together AI has new aggressive pricing where 70b are on par with gpt35 and everything smaller is fairly cheaper. The catch is the only 32k context length model as of today is their llama 7b which is fairly limited.</div><br/></div></div><div id="37485902" class="c"><input type="checkbox" id="c-37485902" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37487891">prev</a><span>|</span><a href="#37487653">next</a><span>|</span><label class="collapse" for="c-37485902">[-]</label><label class="expand" for="c-37485902">[13 more]</label></div><br/><div class="children"><div class="content">I thought Llama was opensource&#x2F;free and you could run it yourself?</div><br/><div id="37486800" class="c"><input type="checkbox" id="c-37486800" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485902">parent</a><span>|</span><a href="#37486091">next</a><span>|</span><label class="collapse" for="c-37486800">[-]</label><label class="expand" for="c-37486800">[3 more]</label></div><br/><div class="children"><div class="content">You (currently) need a GPU to run any of the useful models. I haven&#x27;t really seen a business use-case that runs it on the user&#x27;s computer, but given the hardware requirements it wouldn&#x27;t be very feasible to expect.<p>So you&#x27;ll have to figure out how to run&#x2F;scale the model inference. Cloud GPU instances are generally very expensive, and once you start needing to horizontally scale it&#x27;ll get messy fast.<p>At least at the moment it&#x27;s expensive, especially if it&#x27;s either very light usage or very intensive usage - you either need just a few seconds of compute occasionally, or lots of compute all the time requiring scaling.<p>The &quot;lucky&quot; ones in this scenario are small-medium businesses that can use one or a few cards on-site for their traffic. Even then when you take the cost of an A100 + maintaining it, etc. OpenAI&#x27;s offering still looks attractive.<p>I know there&#x27;s a few services that try to provide an api similar to what openai has, and some software to self orchestrate it, I&#x27;m curious how those compare...</div><br/><div id="37488340" class="c"><input type="checkbox" id="c-37488340" checked=""/><div class="controls bullet"><span class="by">hereonout2</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486800">parent</a><span>|</span><a href="#37486091">next</a><span>|</span><label class="collapse" for="c-37488340">[-]</label><label class="expand" for="c-37488340">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>once you start needing to horizontally scale it&#x27;ll get messy fast.</i><p>It gets expensive fast, but not messy, these things scale horizontally really well. All the state is encapsulated in the request, no replication, synchronisation, user data to worry about. I&#x27;d rather have the job of horizontally scaling llama2 than a relational database.</div><br/><div id="37488572" class="c"><input type="checkbox" id="c-37488572" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37488340">parent</a><span>|</span><a href="#37486091">next</a><span>|</span><label class="collapse" for="c-37488572">[-]</label><label class="expand" for="c-37488572">[1 more]</label></div><br/><div class="children"><div class="content">For sure, and yeah it wouldn&#x27;t be terrible you&#x27;re right. You&#x27;d just need the api servers + a load balancer.<p>My thing is that dynamically doing that is still a lot compared to just calling a single endpoint and all of that is handled for you.<p>But for sure this is a very decent horizontal use-case.</div><br/></div></div></div></div></div></div><div id="37486091" class="c"><input type="checkbox" id="c-37486091" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485902">parent</a><span>|</span><a href="#37486800">prev</a><span>|</span><a href="#37489592">next</a><span>|</span><label class="collapse" for="c-37486091">[-]</label><label class="expand" for="c-37486091">[7 more]</label></div><br/><div class="children"><div class="content">You can run the smaller Llama variants on consumer grade hardware, but people typically rent GPUs from the cloud to run the larger variants.  It is possible to run even larger variants on a beefy workstation or gaming rig, but the performance on consumer hardware usually makes this impractical.<p>So the comparison would be the cost of renting a cloud GPU to run Llama vs querying ChatGPT.</div><br/><div id="37486656" class="c"><input type="checkbox" id="c-37486656" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486091">parent</a><span>|</span><a href="#37489592">next</a><span>|</span><label class="collapse" for="c-37486656">[-]</label><label class="expand" for="c-37486656">[6 more]</label></div><br/><div class="children"><div class="content">&gt;So the comparison would be the cost of renting a cloud GPU to run Llama vs querying ChatGPT.<p>Yes, and it doesn&#x27;t even come close. Llama2-70b can run inference at 300+tokens&#x2F;s on a single V100 instance at ~$0.50&#x2F;hr. Anyone who can should be switching away from OpenAI right now.</div><br/><div id="37487543" class="c"><input type="checkbox" id="c-37487543" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37486656">parent</a><span>|</span><a href="#37489592">next</a><span>|</span><label class="collapse" for="c-37487543">[-]</label><label class="expand" for="c-37487543">[5 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the best way to use LLama2-70b without existing infrastructure for orchestrating it?</div><br/><div id="37488998" class="c"><input type="checkbox" id="c-37488998" checked=""/><div class="controls bullet"><span class="by">mjirv</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487543">parent</a><span>|</span><a href="#37490473">next</a><span>|</span><label class="collapse" for="c-37488998">[-]</label><label class="expand" for="c-37488998">[1 more]</label></div><br/><div class="children"><div class="content">I stumbled upon OpenRouter[0] a few days ago. Easiest I’ve seen by far (if you want SaaS, not hosting it yourself).<p>[0] <a href="https:&#x2F;&#x2F;openrouter.ai" rel="nofollow noreferrer">https:&#x2F;&#x2F;openrouter.ai</a></div><br/></div></div><div id="37490473" class="c"><input type="checkbox" id="c-37490473" checked=""/><div class="controls bullet"><span class="by">pdntspa</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487543">parent</a><span>|</span><a href="#37488998">prev</a><span>|</span><a href="#37487668">next</a><span>|</span><label class="collapse" for="c-37490473">[-]</label><label class="expand" for="c-37490473">[1 more]</label></div><br/><div class="children"><div class="content">I bought an old server off ServerMonkey for like $700 with a stupid amount of RAM and CPUs and it runs Llama2-70b fine, if a little slowly. Good for experimenting</div><br/></div></div><div id="37487668" class="c"><input type="checkbox" id="c-37487668" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487543">parent</a><span>|</span><a href="#37490473">prev</a><span>|</span><a href="#37489592">next</a><span>|</span><label class="collapse" for="c-37487668">[-]</label><label class="expand" for="c-37487668">[2 more]</label></div><br/><div class="children"><div class="content">&gt;What&#x27;s the best way to use LLama2-70b without existing infrastructure for orchestrating it?<p>That&#x27;s an exercise left to the reader for now, and is where your value&#x2F;moat lies.</div><br/><div id="37487879" class="c"><input type="checkbox" id="c-37487879" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487668">parent</a><span>|</span><a href="#37489592">next</a><span>|</span><label class="collapse" for="c-37487879">[-]</label><label class="expand" for="c-37487879">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s an exercise left to the reader for now, and is where your value&#x2F;moat lies.<p>Hopefully more on-demand services enter the space. Currently where I am we don&#x27;t have the resources for any type of self orchestration and our use case is so low&#x2F;sporadic that we can&#x27;t simply have a dedicated instance.<p>Last I saw the current services were rather expensive but I should recheck.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37489592" class="c"><input type="checkbox" id="c-37489592" checked=""/><div class="controls bullet"><span class="by">axpy906</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485902">parent</a><span>|</span><a href="#37486091">prev</a><span>|</span><a href="#37486039">next</a><span>|</span><label class="collapse" for="c-37489592">[-]</label><label class="expand" for="c-37489592">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately, Lama2 is not a fully open source license.</div><br/></div></div><div id="37486039" class="c"><input type="checkbox" id="c-37486039" checked=""/><div class="controls bullet"><span class="by">kuchenbecker</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485902">parent</a><span>|</span><a href="#37489592">prev</a><span>|</span><a href="#37487653">next</a><span>|</span><label class="collapse" for="c-37486039">[-]</label><label class="expand" for="c-37486039">[1 more]</label></div><br/><div class="children"><div class="content">Compute costs money.</div><br/></div></div></div></div><div id="37487653" class="c"><input type="checkbox" id="c-37487653" checked=""/><div class="controls bullet"><span class="by">computerex</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37485902">prev</a><span>|</span><a href="#37485893">next</a><span>|</span><label class="collapse" for="c-37487653">[-]</label><label class="expand" for="c-37487653">[1 more]</label></div><br/><div class="children"><div class="content">Replicate has terrible pricing. Have you tried deepinfra?</div><br/></div></div><div id="37485893" class="c"><input type="checkbox" id="c-37485893" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37485407">parent</a><span>|</span><a href="#37487653">prev</a><span>|</span><a href="#37484521">next</a><span>|</span><label class="collapse" for="c-37485893">[-]</label><label class="expand" for="c-37485893">[3 more]</label></div><br/><div class="children"><div class="content">For use cases well within the capabilities of an LLM from last year, fine-tuned LLaMa 2 13B should&#x2F;will blow ChatGPT out of the water: think &quot;rate the sentiment of this text from 0-10&quot;.<p>I believe this because LLaMa-2 13B is more than good enough to handle what I call &quot;quick search&quot;, i.e.<p>```
User: &quot;What&#x27;s the weather in Milwaukee?&quot;<p>System: Here&#x27;s some docs, answer concisely in one sentence.<p>AI: It&#x27;s 73 degrees Farenheit.
```<p>YMMV on cost still, depends on cloud vendor, and my intuition agrees with yours: GPT-3.5 is priced low enough that there isn&#x27;t a case where it makes sense to use another model. It strikes me now that&#x27;s there&#x27;s a good reason for that intuition: OpenAI&#x27;s $&#x2F;GPU hour is likely &lt;= any other vendor&#x27;s and inference time of LLaMa 2 ~= GPT.<p>I do think this will change with local LLMs. They&#x27;ve been way over-hyped for months, but after LLaMa 2, the challenges remaining are more sociological than technical.<p>For months now it&#x27;s been one-off $LATEST_BUZZY_MODEL.c stunts that run on desktop.<p>The vast majority of the _actual_ usage and progress is coming from porn-y stuff, and the investment occurs in one-off stunts.<p>That split of effort, and lack of engineering rigor, is stunting progress overall.<p>Microsoft has LLaMa-2 ONNX available on GitHub[1]. There&#x27;s budding but very small projects in different languages to wrap ONNX. Once there&#x27;s a genuine cross-platform[2] ONNX wrapper that makes running LLaMa-2 easy, there will be a step change. It&#x27;ll be &quot;free&quot;[3] to run your fine-tuned model that does as well as GPT-4.<p>It&#x27;s not clear to me exactly when this will occur. It&#x27;s &quot;difficult&quot; now, but only because the _actual usage_ in the local LLM community doesn&#x27;t have a reason to invest in ONNX, and it&#x27;s extremely intimidating to figure out how exactly to get LLaMa-2 running in ONNX. Microsoft kinda threw it up on GitHub and moved on, the sample code even still needs a PyTorch model. I see at least one very small company on HuggingFace that _may_ have figured out full ONNX.<p>Funnily enough, ONNX is getting a spike in mindshare over the last month in the _Stable Diffusion_ community. There&#x27;s decent cross-pollination between local art and local LLMs, ex. LoRA&#x27;s were first a thing for Stable Diffusion. So I&#x27;m hoping we see this sooner rather than later.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;Llama-2-Onnx">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;Llama-2-Onnx</a><p>[2] Definition of cross-platform matters a ton here, what I mean is &quot;I can import $ONNX_WRAPPER_LIB on iOS &#x2F; Android &#x2F; Mac &#x2F; Windows and call Llama2.reply(String prompt, ...)&quot;<p>[3] Runs on somebody else&#x27;s computer, where &quot;somebody else&quot; is the user, instead of a cloud vendor.</div><br/><div id="37487851" class="c"><input type="checkbox" id="c-37487851" checked=""/><div class="controls bullet"><span class="by">homarp</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37485893">parent</a><span>|</span><a href="#37484521">next</a><span>|</span><label class="collapse" for="c-37487851">[-]</label><label class="expand" for="c-37487851">[2 more]</label></div><br/><div class="children"><div class="content">you already have TVM for the cross platform stuff<p>see <a href="https:&#x2F;&#x2F;tvm.apache.org&#x2F;docs&#x2F;how_to&#x2F;deploy&#x2F;android.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;tvm.apache.org&#x2F;docs&#x2F;how_to&#x2F;deploy&#x2F;android.html</a><p>or <a href="https:&#x2F;&#x2F;octoml.ai&#x2F;blog&#x2F;using-swift-and-apache-tvm-to-develop-ml-apps-for-the-apple-ecosystem&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;octoml.ai&#x2F;blog&#x2F;using-swift-and-apache-tvm-to-develop...</a><p>or <a href="https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm">https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm</a></div><br/><div id="37490324" class="c"><input type="checkbox" id="c-37490324" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37485407">root</a><span>|</span><a href="#37487851">parent</a><span>|</span><a href="#37484521">next</a><span>|</span><label class="collapse" for="c-37490324">[-]</label><label class="expand" for="c-37490324">[1 more]</label></div><br/><div class="children"><div class="content">My deepest thanks, I owe you one. Overlooked this completely. &amp; spent dozens of hours learning way too much to still fall short of understanding how to make it work in ONNX.</div><br/></div></div></div></div></div></div></div></div><div id="37484521" class="c"><input type="checkbox" id="c-37484521" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#37485407">prev</a><span>|</span><a href="#37484631">next</a><span>|</span><label class="collapse" for="c-37484521">[-]</label><label class="expand" for="c-37484521">[9 more]</label></div><br/><div class="children"><div class="content">Looks really well executed, nice! I&#x27;d shared this idea with a few people. GPT and other LLMs don&#x27;t allow you to use their output to train competing models, but the implication is that it&#x27;s fine to use their output to train your own internal alternative models. So you can&#x27;t sell access to the output as an API, but you can use it to replace your GPT API calls.<p>My other thoughts to extend this are that you could make it seamless. To start, it&#x27;ll simply pipe the user&#x27;s requests to OpenAI or their existing model. So it&#x27;d be a drop in replacement. Then, it&#x27;ll every so often offer to the user - &quot;hey we think at this point there&#x27;s enough data that a fine tune might save you approx $x&#x2F;month based on your current calls, click the button to start the fine tune and we&#x27;ll email you once we have the results&quot; - and then the user gets the email &quot;here are the results, based on that we recommend switching, click here to switch to calling your fine-tuned model&quot; - Helicone and the other monitoring platforms could also offer something similar. (Side note I&#x27;m working on an &quot;ai infra handbook&quot; aimed at technical people in software orgs looking to deploy unspecified &quot;AI&quot; features and trying to figure out what to do and what resources they&#x27;ll need - it&#x27;s a 20+ page google doc, if anyone can help me review what I have so far please let me know and I&#x27;ll add you.)<p><i>If</i> it&#x27;s latency&#x2F;error&#x2F;speed competitive, and cheaper, and equivalently accurate, then for anyone doing production scale LLM API usage it&#x27;d make sense to use something like this - either the fine-tune is worse so you keep using the regular API, or the fine tune has parity plus cost and&#x2F;or speed advantage, so you switch. (It wouldn&#x27;t make sense for prototyping scale, because the additional complexity of the switch wouldn&#x27;t be worth it unless it could save you 4&#x2F;5 or more figures a year in API costs I&#x27;d think.)</div><br/><div id="37484927" class="c"><input type="checkbox" id="c-37484927" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37484521">parent</a><span>|</span><a href="#37486300">next</a><span>|</span><label class="collapse" for="c-37484927">[-]</label><label class="expand" for="c-37484927">[1 more]</label></div><br/><div class="children"><div class="content">&gt; My other thoughts to extend this are that you could make it seamless. To start, it&#x27;ll simply pipe the user&#x27;s requests to OpenAI or their existing model. So it&#x27;d be a drop in replacement. Then, it&#x27;ll every so often offer to the user - &quot;hey we think at this point there&#x27;s enough data that a fine tune might save you approx $x&#x2F;month based on your current calls, click the button to start the fine tune and we&#x27;ll email you once we have the results&quot; - and then the user gets the email &quot;here are the results, based on that we recommend switching, click here to switch to calling your fine-tuned model&quot;<p>You just described our short-term roadmap. :) Currently an OpenPipe user has to explicitly kick off a fine-tuning job, but they&#x27;re so cheap to run we&#x27;re planning on letting users opt in to running them proactively once they have enough data so we can provide exactly that experience.</div><br/></div></div><div id="37486300" class="c"><input type="checkbox" id="c-37486300" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#37484521">parent</a><span>|</span><a href="#37484927">prev</a><span>|</span><a href="#37485592">next</a><span>|</span><label class="collapse" for="c-37486300">[-]</label><label class="expand" for="c-37486300">[2 more]</label></div><br/><div class="children"><div class="content">&gt;GPT and other LLMs don&#x27;t allow you to use their output to train competing models<p>ToS is unenforceable and irrelevant to anyone that&#x27;s in this space</div><br/><div id="37490695" class="c"><input type="checkbox" id="c-37490695" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#37484521">root</a><span>|</span><a href="#37486300">parent</a><span>|</span><a href="#37485592">next</a><span>|</span><label class="collapse" for="c-37490695">[-]</label><label class="expand" for="c-37490695">[1 more]</label></div><br/><div class="children"><div class="content">That seems mostly right, particularly for internal models, but I wonder about adding some ringers to prove that copying happened:<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Trap_street" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Trap_street</a><p>Also, it seems sort of like how cryptocurrency folks assumed their transactions were anonymous? It&#x27;s an API, so they could log the calls. (Maybe not the contents.)</div><br/></div></div></div></div><div id="37485592" class="c"><input type="checkbox" id="c-37485592" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#37484521">parent</a><span>|</span><a href="#37486300">prev</a><span>|</span><a href="#37491956">next</a><span>|</span><label class="collapse" for="c-37485592">[-]</label><label class="expand" for="c-37485592">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Side note I&#x27;m working on an &quot;ai infra handbook&quot; aimed at technical people in software orgs looking to deploy unspecified &quot;AI&quot; features and trying to figure out what to do and what resources they&#x27;ll need - it&#x27;s a 20+ page google doc, if anyone can help me review what I have so far please let me know and I&#x27;ll add you.</i><p>Interested in helping out.</div><br/></div></div><div id="37491956" class="c"><input type="checkbox" id="c-37491956" checked=""/><div class="controls bullet"><span class="by">animeshjain</span><span>|</span><a href="#37484521">parent</a><span>|</span><a href="#37485592">prev</a><span>|</span><a href="#37489628">next</a><span>|</span><label class="collapse" for="c-37491956">[-]</label><label class="expand" for="c-37491956">[1 more]</label></div><br/><div class="children"><div class="content">I would be interested in reviewing your handbook too. I am technical, but have not deployed any AI related tooling so far. keen to know if this is targeted to AI noobs as well.</div><br/></div></div><div id="37489628" class="c"><input type="checkbox" id="c-37489628" checked=""/><div class="controls bullet"><span class="by">jlm521</span><span>|</span><a href="#37484521">parent</a><span>|</span><a href="#37491956">prev</a><span>|</span><a href="#37484540">next</a><span>|</span><label class="collapse" for="c-37489628">[-]</label><label class="expand" for="c-37489628">[1 more]</label></div><br/><div class="children"><div class="content">I would be like to help in reviewing your handbook.</div><br/></div></div><div id="37484540" class="c"><input type="checkbox" id="c-37484540" checked=""/><div class="controls bullet"><span class="by">bongobingo1</span><span>|</span><a href="#37484521">parent</a><span>|</span><a href="#37489628">prev</a><span>|</span><a href="#37484631">next</a><span>|</span><label class="collapse" for="c-37484540">[-]</label><label class="expand" for="c-37484540">[2 more]</label></div><br/><div class="children"><div class="content">&gt; GPT and other LLMs don&#x27;t allow you to use their output to train competing models<p>I didn&#x27;t allow them to use my output to train theirs either, <i>so fuck &#x27;em</i>.</div><br/></div></div></div></div><div id="37484631" class="c"><input type="checkbox" id="c-37484631" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#37484521">prev</a><span>|</span><a href="#37493210">next</a><span>|</span><label class="collapse" for="c-37484631">[-]</label><label class="expand" for="c-37484631">[14 more]</label></div><br/><div class="children"><div class="content">&gt; Fine-tuning has one huge advantage though: it is far more effective at guiding a model&#x27;s behavior than prompting, so you can often get away with a much smaller model. That gets you faster responses and lower inference costs. A fine-tuned Llama 7B model is 50x cheaper than GPT-3.5 on a per-token basis, and for many use cases can produce results that are as good or better!<p>These comparisons are reductive to the point of being misleading. Even with all the optimizations in the ecosystem, it&#x27;s not trivial to get a finetuned 7B param model running at an acceptable inference latency. Even if you use a GPU such as an A100 for maximum speed, then you have scalability issues since A100s are scarce. Also, the &quot;50% cheaper&quot; assumes 100% utilization of a GPU which will never happen in production use cases.<p>Quality-wise, a finetuned Llama 2 is not necessairly better than ChatGPT. Finetuning requires a high-quality dataset which is not easy to construct. And in my own experience with finetuning Llama 2, qualitivately it caused more frustration to get outputs on par with just using ChatGPT.<p>The value of the ChatGPT API is more dependable scaling and not having to pay for an infra.</div><br/><div id="37484884" class="c"><input type="checkbox" id="c-37484884" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37484631">parent</a><span>|</span><a href="#37485091">next</a><span>|</span><label class="collapse" for="c-37484884">[-]</label><label class="expand" for="c-37484884">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re finding that when running Llama-2-7B with vLLM (<a href="https:&#x2F;&#x2F;github.com&#x2F;vllm-project&#x2F;vllm">https:&#x2F;&#x2F;github.com&#x2F;vllm-project&#x2F;vllm</a>) on an A40 GPU we&#x27;re getting consistently lower time-to-first-token and lower average token generation time than GPT-3.5, even when processing multiple requests in parallel. A40s are pretty easy to get your hands on these days (much easer than A100s anyway).<p>The 50x cheaper (that&#x27;s 2% of the cost, not 50% of the cost) number does assume 100% GPU utilization, which may or may not be realistic for your use case. If you&#x27;re doing batch processing as part of a data pipeline, which is not an unusual use case, you can run your GPU at 100% utilization and turn it off when the batch finishes.<p>If you&#x27;ve got a highly variable workload then you&#x27;re right, you&#x27;ll have much lower utilization numbers. But if you work with an aggregator that can quickly hot swap LoRA fine-tunes (as a disclaimer, my company OpenPipe works in this space) you can get back a lot of that lost efficiency since we can increase&#x2F;decrease GPU capacity only when our aggregate usage changes, which smooths things out.</div><br/></div></div><div id="37485091" class="c"><input type="checkbox" id="c-37485091" checked=""/><div class="controls bullet"><span class="by">hereonout2</span><span>|</span><a href="#37484631">parent</a><span>|</span><a href="#37484884">prev</a><span>|</span><a href="#37484801">next</a><span>|</span><label class="collapse" for="c-37485091">[-]</label><label class="expand" for="c-37485091">[2 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t this depend a lot on your application though? Not every workload needs low latency and massive horizontal scalability.<p>Take their example of running the llm over the 2 million recipes and saving $23k over GPT 4. That could easily be 2 million documents in some back end system running in a batch. Many people would wait a few days or weeks for a job like that to finish if it offered significant savings.</div><br/><div id="37485145" class="c"><input type="checkbox" id="c-37485145" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#37484631">root</a><span>|</span><a href="#37485091">parent</a><span>|</span><a href="#37484801">next</a><span>|</span><label class="collapse" for="c-37485145">[-]</label><label class="expand" for="c-37485145">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s more of a fair use case.<p>It though also demonstrates why the economics are complicated and there&#x27;s no one-size-fits-all.</div><br/></div></div></div></div><div id="37484801" class="c"><input type="checkbox" id="c-37484801" checked=""/><div class="controls bullet"><span class="by">moonchrome</span><span>|</span><a href="#37484631">parent</a><span>|</span><a href="#37485091">prev</a><span>|</span><a href="#37485159">next</a><span>|</span><label class="collapse" for="c-37484801">[-]</label><label class="expand" for="c-37484801">[9 more]</label></div><br/><div class="children"><div class="content">We are talking about 7B models ? Those can run on consumer GPUs with lower latency than A100s AFAIK (because gaming GPUs are clocked different).<p>Not to mention OpenAI has shit latency and terrible reliability - you should be using Azure models if you care about that - but pricing is also higher.<p>I would say fixed costs and development time is on openai side but I&#x27;ve seen people post great practical comparisons for latency and cost using hostes fine-tuned small models.</div><br/><div id="37484848" class="c"><input type="checkbox" id="c-37484848" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#37484631">root</a><span>|</span><a href="#37484801">parent</a><span>|</span><a href="#37484832">next</a><span>|</span><label class="collapse" for="c-37484848">[-]</label><label class="expand" for="c-37484848">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Running&quot; and &quot;acceptable inference speed and quality&quot; are two different constraints, particularly at scale&#x2F;production.</div><br/><div id="37488459" class="c"><input type="checkbox" id="c-37488459" checked=""/><div class="controls bullet"><span class="by">moonchrome</span><span>|</span><a href="#37484631">root</a><span>|</span><a href="#37484848">parent</a><span>|</span><a href="#37484832">next</a><span>|</span><label class="collapse" for="c-37488459">[-]</label><label class="expand" for="c-37488459">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand what you&#x27;re trying to say ?<p>From what I&#x27;ve read 4090 should blow A100 away if you can fit within 22GB VRAM, which a 7B model should comfortably.<p>And the latency (along with variability and availability) on OpenAI API is terrible because of the load they are getting.</div><br/></div></div></div></div><div id="37485270" class="c"><input type="checkbox" id="c-37485270" checked=""/><div class="controls bullet"><span class="by">7speter</span><span>|</span><a href="#37484631">root</a><span>|</span><a href="#37484801">parent</a><span>|</span><a href="#37484832">prev</a><span>|</span><a href="#37485159">next</a><span>|</span><label class="collapse" for="c-37485270">[-]</label><label class="expand" for="c-37485270">[5 more]</label></div><br/><div class="children"><div class="content">When you say it can run on consumer gpus, do you mean pretty much just the 4090&#x2F;3090 or can it run on lesser cards?</div><br/><div id="37485816" class="c"><input type="checkbox" id="c-37485816" checked=""/><div class="controls bullet"><span class="by">halflings</span><span>|</span><a href="#37484631">root</a><span>|</span><a href="#37485270">parent</a><span>|</span><a href="#37488693">next</a><span>|</span><label class="collapse" for="c-37485816">[-]</label><label class="expand" for="c-37485816">[2 more]</label></div><br/><div class="children"><div class="content">I was able to run the 4bit quantized LLAMA2 7B on a 2070 Super, though latency was so-so.<p>I was surprised by how fast it runs on an M2 MBP + llama.cpp; Way way faster than ChatGPT, and that&#x27;s not even using the Apple neural engine.</div><br/><div id="37487796" class="c"><input type="checkbox" id="c-37487796" checked=""/><div class="controls bullet"><span class="by">hereonout2</span><span>|</span><a href="#37484631">root</a><span>|</span><a href="#37485816">parent</a><span>|</span><a href="#37488693">next</a><span>|</span><label class="collapse" for="c-37487796">[-]</label><label class="expand" for="c-37487796">[1 more]</label></div><br/><div class="children"><div class="content">It runs fantastically well on M2 Mac + llama.cpp, such a variety of factors in the Apple hardware making it possible. The ARM fp16 vector intrinsics, the Macbook&#x27;s AMX co-processor, the unified memory architecture, etc.<p>It&#x27;s more than fast enough for my experiments and the laptop doesn&#x27;t seem to break a sweat.</div><br/></div></div></div></div><div id="37486702" class="c"><input type="checkbox" id="c-37486702" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#37484631">root</a><span>|</span><a href="#37485270">parent</a><span>|</span><a href="#37488693">prev</a><span>|</span><a href="#37485159">next</a><span>|</span><label class="collapse" for="c-37486702">[-]</label><label class="expand" for="c-37486702">[1 more]</label></div><br/><div class="children"><div class="content">Quantized 7B&#x27;s can comfortably run with 8GB vram</div><br/></div></div></div></div></div></div></div></div><div id="37493210" class="c"><input type="checkbox" id="c-37493210" checked=""/><div class="controls bullet"><span class="by">szesiongteo</span><span>|</span><a href="#37484631">prev</a><span>|</span><a href="#37488810">next</a><span>|</span><label class="collapse" for="c-37493210">[-]</label><label class="expand" for="c-37493210">[1 more]</label></div><br/><div class="children"><div class="content">I think the cost calculation here does not reflect the actual scenario where most people face. In real world scenario, we don&#x27;t get inputs queued up to millions and wait for the GPU to inference them continuously at 100% utilization. We need to ensure the user get their response in time, and assume that we get all the inputs spread out evenly within a month, we have to look at the cost of running GPU for a month vs using OpenAI API.</div><br/></div></div><div id="37488810" class="c"><input type="checkbox" id="c-37488810" checked=""/><div class="controls bullet"><span class="by">derekpankaew</span><span>|</span><a href="#37493210">prev</a><span>|</span><a href="#37493174">next</a><span>|</span><label class="collapse" for="c-37488810">[-]</label><label class="expand" for="c-37488810">[2 more]</label></div><br/><div class="children"><div class="content">Can you clarify the 50x cheaper number? Is this for self-hosting, or if you&#x27;re hosting on OpenPipe?<p>The pricing on OpenPipe says it&#x27;s 0.0012 to 0.0016 per 1K tokens for Llama 7b. GPT-3.5 pricing is 0.0015 to 0.002, so not that different.<p>I&#x27;m assuming the 50x cost reductions are primarily from self-hosting?</div><br/><div id="37489086" class="c"><input type="checkbox" id="c-37489086" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37488810">parent</a><span>|</span><a href="#37493174">next</a><span>|</span><label class="collapse" for="c-37489086">[-]</label><label class="expand" for="c-37489086">[1 more]</label></div><br/><div class="children"><div class="content">Yep, the 50x cost reduction is if you self-host a fine-tuned model using the setup demonstrated in in the linked notebooks.</div><br/></div></div></div></div><div id="37493174" class="c"><input type="checkbox" id="c-37493174" checked=""/><div class="controls bullet"><span class="by">imhoguy</span><span>|</span><a href="#37488810">prev</a><span>|</span><a href="#37485972">next</a><span>|</span><label class="collapse" for="c-37493174">[-]</label><label class="expand" for="c-37493174">[1 more]</label></div><br/><div class="children"><div class="content">I have such use case: I have Java project I develop, I also used phind-codellama-36B-q8 with very satisfying results to aid the development.<p>Can I train it further using the project source to let the model &quot;understand&quot; the project context more?</div><br/></div></div><div id="37485972" class="c"><input type="checkbox" id="c-37485972" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#37493174">prev</a><span>|</span><a href="#37485651">next</a><span>|</span><label class="collapse" for="c-37485972">[-]</label><label class="expand" for="c-37485972">[5 more]</label></div><br/><div class="children"><div class="content">This looks awesome!  Tangential question - do you find GPT function calling to work consistently and without error, or do you get errors when using it?  By errors I mostly mean incorrect function signatures&#x2F;types or missing values...but if you see other unpredictable behavior that would help too.</div><br/><div id="37489262" class="c"><input type="checkbox" id="c-37489262" checked=""/><div class="controls bullet"><span class="by">llwj</span><span>|</span><a href="#37485972">parent</a><span>|</span><a href="#37486241">next</a><span>|</span><label class="collapse" for="c-37489262">[-]</label><label class="expand" for="c-37489262">[1 more]</label></div><br/><div class="children"><div class="content">I see wrong responses about 1% of the time, but I love it, considering parsing raw text output without function calling had a much higher error rate.</div><br/></div></div><div id="37486241" class="c"><input type="checkbox" id="c-37486241" checked=""/><div class="controls bullet"><span class="by">Arctic_fly</span><span>|</span><a href="#37485972">parent</a><span>|</span><a href="#37489262">prev</a><span>|</span><a href="#37485651">next</a><span>|</span><label class="collapse" for="c-37486241">[-]</label><label class="expand" for="c-37486241">[3 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t had much trouble with GPT 3.5 or 4 function calls returning in an undesirable format recently. I did get a few bad syntax responses when OpenAI first rolled it out, but not for the past few months.<p>Llama 2 can also pick the function call format up, given sufficient training data that contains function call responses, though you&#x27;ll then have to parse the returned object out of the text-based response.</div><br/><div id="37491877" class="c"><input type="checkbox" id="c-37491877" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37485972">root</a><span>|</span><a href="#37486241">parent</a><span>|</span><a href="#37485651">next</a><span>|</span><label class="collapse" for="c-37491877">[-]</label><label class="expand" for="c-37491877">[2 more]</label></div><br/><div class="children"><div class="content">Has anyone done such fine tuning on llama though? Afaik most projects like llama.cpp use grammars instead.</div><br/><div id="37492342" class="c"><input type="checkbox" id="c-37492342" checked=""/><div class="controls bullet"><span class="by">Arctic_fly</span><span>|</span><a href="#37485972">root</a><span>|</span><a href="#37491877">parent</a><span>|</span><a href="#37485651">next</a><span>|</span><label class="collapse" for="c-37492342">[-]</label><label class="expand" for="c-37492342">[1 more]</label></div><br/><div class="children"><div class="content">Yep! The linked notebook includes an example of exactly that (fine-tuning a 7b model to match the syntax of GPT-4 function call responses): <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenPipe&#x2F;OpenPipe&#x2F;blob&#x2F;main&#x2F;examples&#x2F;classify-recipes&#x2F;3-evaluate.ipynb">https:&#x2F;&#x2F;github.com&#x2F;OpenPipe&#x2F;OpenPipe&#x2F;blob&#x2F;main&#x2F;examples&#x2F;clas...</a></div><br/></div></div></div></div></div></div></div></div><div id="37485651" class="c"><input type="checkbox" id="c-37485651" checked=""/><div class="controls bullet"><span class="by">brianjking</span><span>|</span><a href="#37485972">prev</a><span>|</span><a href="#37491558">next</a><span>|</span><label class="collapse" for="c-37485651">[-]</label><label class="expand" for="c-37485651">[1 more]</label></div><br/><div class="children"><div class="content">Very nice, thanks!<p>Check out what Matt Shumer put together as well: <a href="https:&#x2F;&#x2F;github.com&#x2F;mshumer&#x2F;gpt-llm-trainer">https:&#x2F;&#x2F;github.com&#x2F;mshumer&#x2F;gpt-llm-trainer</a>.<p>I have used his trainer for auto distillation of GPT-4 into GPT3.5 fine tunes, but plan to do the same for Llama as well.<p>Cheers!</div><br/></div></div><div id="37491558" class="c"><input type="checkbox" id="c-37491558" checked=""/><div class="controls bullet"><span class="by">zten</span><span>|</span><a href="#37485651">prev</a><span>|</span><a href="#37491695">next</a><span>|</span><label class="collapse" for="c-37491558">[-]</label><label class="expand" for="c-37491558">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for posting this. I had to go look for your HuggingFace data sets to find the labeled variety you produced with GPT-4, but other than that, everything was easy to follow.</div><br/></div></div><div id="37491695" class="c"><input type="checkbox" id="c-37491695" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#37491558">prev</a><span>|</span><a href="#37486658">next</a><span>|</span><label class="collapse" for="c-37491695">[-]</label><label class="expand" for="c-37491695">[2 more]</label></div><br/><div class="children"><div class="content">Fine-tuned low parameter LLM&#x27;s are superficially good but the cracks are obvious if you test them on anything that isn&#x27;t very strictly tied to the training data. IMO GPT-4 is really the first LLM that&#x27;s broken out of the fake intelligence quality most LLM&#x27;s seem to have, though only by a little.</div><br/><div id="37492788" class="c"><input type="checkbox" id="c-37492788" checked=""/><div class="controls bullet"><span class="by">kytazo</span><span>|</span><a href="#37491695">parent</a><span>|</span><a href="#37486658">next</a><span>|</span><label class="collapse" for="c-37492788">[-]</label><label class="expand" for="c-37492788">[1 more]</label></div><br/><div class="children"><div class="content">If we assume this is true:
<a href="https:&#x2F;&#x2F;iv.nboeck.de&#x2F;watch?v=K5iDUZPx60E&amp;t=2989" rel="nofollow noreferrer">https:&#x2F;&#x2F;iv.nboeck.de&#x2F;watch?v=K5iDUZPx60E&amp;t=2989</a><p>Then there isn&#x27;t anything in particular which makes their model(s) stand out. On the contrary, they seem rather inefficient, which is probably reflected on the inference cost this gargantuan conglomerate takes to run.</div><br/></div></div></div></div><div id="37486658" class="c"><input type="checkbox" id="c-37486658" checked=""/><div class="controls bullet"><span class="by">divbzero</span><span>|</span><a href="#37491695">prev</a><span>|</span><a href="#37486319">next</a><span>|</span><label class="collapse" for="c-37486658">[-]</label><label class="expand" for="c-37486658">[5 more]</label></div><br/><div class="children"><div class="content">Is Llama 2 currently the way to go for fine-tuning your own models? Are there other  open-source LLMs worth considering?</div><br/><div id="37486768" class="c"><input type="checkbox" id="c-37486768" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37486658">parent</a><span>|</span><a href="#37487194">next</a><span>|</span><label class="collapse" for="c-37486768">[-]</label><label class="expand" for="c-37486768">[1 more]</label></div><br/><div class="children"><div class="content">Depends on your use case. If you&#x27;re doing pure classification then there are smaller encoder-only models like DeBERTa that might get you better performance with a much smaller model size (so cheaper inference).<p>But if you need text generation and are ok with a 7B+ parameter model, Llama 2 or one of its derivatives is what I&#x27;d strongly recommend. The community around it is much larger than any of the alternatives so the tooling is better, and it&#x27;s either state of the art or close to it on all evals when compared to other similarly-sized open models.<p>If you&#x27;re comfortable sharing more details of the task you&#x27;re trying to do I might be able to give more specific advice.</div><br/></div></div><div id="37487194" class="c"><input type="checkbox" id="c-37487194" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#37486658">parent</a><span>|</span><a href="#37486768">prev</a><span>|</span><a href="#37489483">next</a><span>|</span><label class="collapse" for="c-37487194">[-]</label><label class="expand" for="c-37487194">[1 more]</label></div><br/><div class="children"><div class="content">The Huggingface Leaderboard is mostly dominated by Llama 2 variants:
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderboard" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderb...</a><p>It depends a lot on what you&#x27;re trying to do.  If have a focused use case of the type of fine-tuning you want, you can probably get away with one of the smaller models.<p>Another thing to look out for is Retrieval Augmented Generation (RAG).  I don&#x27;t see it in wide use yet, but it may turn out to more useful than fine tuning for a lot of situations.</div><br/></div></div><div id="37489483" class="c"><input type="checkbox" id="c-37489483" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#37486658">parent</a><span>|</span><a href="#37487194">prev</a><span>|</span><a href="#37487218">next</a><span>|</span><label class="collapse" for="c-37489483">[-]</label><label class="expand" for="c-37489483">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve found Flan-T5 to be useful for text-to-text (mostly document QA).  Haven&#x27;t done a lot of testing on fine-tuning yet though.</div><br/></div></div><div id="37487218" class="c"><input type="checkbox" id="c-37487218" checked=""/><div class="controls bullet"><span class="by">jw903</span><span>|</span><a href="#37486658">parent</a><span>|</span><a href="#37489483">prev</a><span>|</span><a href="#37486319">next</a><span>|</span><label class="collapse" for="c-37487218">[-]</label><label class="expand" for="c-37487218">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s one of widely fine tuned model for now. Take a look at this colab for fine tuning on your dataset <a href="https:&#x2F;&#x2F;github.com&#x2F;mlabonne&#x2F;llm-course&#x2F;blob&#x2F;main&#x2F;Fine_tune_Llama_2_in_Google_Colab.ipynb">https:&#x2F;&#x2F;github.com&#x2F;mlabonne&#x2F;llm-course&#x2F;blob&#x2F;main&#x2F;Fine_tune_L...</a></div><br/></div></div></div></div><div id="37486319" class="c"><input type="checkbox" id="c-37486319" checked=""/><div class="controls bullet"><span class="by">rrherr</span><span>|</span><a href="#37486658">prev</a><span>|</span><a href="#37492719">next</a><span>|</span><label class="collapse" for="c-37486319">[-]</label><label class="expand" for="c-37486319">[3 more]</label></div><br/><div class="children"><div class="content">&quot;You do this by training an existing model on example input&#x2F;output pairs that demonstrate the task you want your fine-tuned model to learn.&quot;<p>Are fine-tuning datasets required to be input&#x2F;output pairs? Or instead, can the fine-tuning be autoregressive (predict the next token throughout this corpus of unlabeled documents)?</div><br/><div id="37486687" class="c"><input type="checkbox" id="c-37486687" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37486319">parent</a><span>|</span><a href="#37492719">next</a><span>|</span><label class="collapse" for="c-37486687">[-]</label><label class="expand" for="c-37486687">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no rule that your fine-tuning dataset needs to be split into input&#x2F;output pairs -- you can of course fine-tune a model to just continue a sequence.<p>As a practical matter though, most of the fine-tuning frameworks, including Axolotl (which this guide uses) and HuggingFace&#x27;s SFTTrainer (the actual fine-tuning trainer most frameworks use under the hood) assume your data comes in input&#x2F;output pairs, and automatically inserts a separator token to let the model know that the input has finished and it should start generating the output. In general most tasks can be formulated this way, including autocomplete tasks, so I&#x27;d probably recommend going that way unless you have a very strong reason not to.</div><br/><div id="37489316" class="c"><input type="checkbox" id="c-37489316" checked=""/><div class="controls bullet"><span class="by">rrherr</span><span>|</span><a href="#37486319">root</a><span>|</span><a href="#37486687">parent</a><span>|</span><a href="#37492719">next</a><span>|</span><label class="collapse" for="c-37489316">[-]</label><label class="expand" for="c-37489316">[1 more]</label></div><br/><div class="children"><div class="content">“most tasks can be formulated this way, including autocomplete tasks”<p>For autocomplete tasks, with a corpus of unlabeled documents, would you insert a separator token at an arbitrary space in each document, in order to form input&#x2F;output pairs?</div><br/></div></div></div></div></div></div><div id="37492719" class="c"><input type="checkbox" id="c-37492719" checked=""/><div class="controls bullet"><span class="by">davidwritesbugs</span><span>|</span><a href="#37486319">prev</a><span>|</span><a href="#37486841">next</a><span>|</span><label class="collapse" for="c-37492719">[-]</label><label class="expand" for="c-37492719">[1 more]</label></div><br/><div class="children"><div class="content">Genuinely informative reply for those (few) of us on HN who don’t know the details of LLMs, thanks</div><br/></div></div><div id="37486841" class="c"><input type="checkbox" id="c-37486841" checked=""/><div class="controls bullet"><span class="by">ingridpan</span><span>|</span><a href="#37492719">prev</a><span>|</span><a href="#37488234">next</a><span>|</span><label class="collapse" for="c-37486841">[-]</label><label class="expand" for="c-37486841">[1 more]</label></div><br/><div class="children"><div class="content">I found this tutorial helpful for getting started with fine-tuning <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=74NSDMvYZ9Y">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=74NSDMvYZ9Y</a><p>This guy used gradient.ai and he has a Google Collab to try it</div><br/></div></div><div id="37488234" class="c"><input type="checkbox" id="c-37488234" checked=""/><div class="controls bullet"><span class="by">rookie123</span><span>|</span><a href="#37486841">prev</a><span>|</span><a href="#37484359">next</a><span>|</span><label class="collapse" for="c-37488234">[-]</label><label class="expand" for="c-37488234">[2 more]</label></div><br/><div class="children"><div class="content">To all those who are on this panel, which is the most comprehensive way a newbie can learn fine-tuning these models with or without the GPUs?<p>Are there any well directed courses available?</div><br/><div id="37488506" class="c"><input type="checkbox" id="c-37488506" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37488234">parent</a><span>|</span><a href="#37484359">next</a><span>|</span><label class="collapse" for="c-37488506">[-]</label><label class="expand" for="c-37488506">[1 more]</label></div><br/><div class="children"><div class="content">I wrote the notebooks in the post with the intention of them being a gentle introduction to fine-tuning. Would love any feedback on open questions you have as you go through them!</div><br/></div></div></div></div><div id="37484359" class="c"><input type="checkbox" id="c-37484359" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#37488234">prev</a><span>|</span><a href="#37484592">next</a><span>|</span><label class="collapse" for="c-37484359">[-]</label><label class="expand" for="c-37484359">[1 more]</label></div><br/><div class="children"><div class="content">This looks very helpful! I&#x27;m just starting out in the ML&#x2F;LLM space and have an opportunity to work on this at $dayjob, bookmarking as this looks like an excellent resource. Thank you!</div><br/></div></div><div id="37484592" class="c"><input type="checkbox" id="c-37484592" checked=""/><div class="controls bullet"><span class="by">indeyets</span><span>|</span><a href="#37484359">prev</a><span>|</span><a href="#37487844">next</a><span>|</span><label class="collapse" for="c-37484592">[-]</label><label class="expand" for="c-37484592">[4 more]</label></div><br/><div class="children"><div class="content">What are hardware requirements for larger models? What can I fine-tune on Nvidia A100? Will it be possible to work with 70b for example?</div><br/><div id="37484809" class="c"><input type="checkbox" id="c-37484809" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37484592">parent</a><span>|</span><a href="#37487844">next</a><span>|</span><label class="collapse" for="c-37484809">[-]</label><label class="expand" for="c-37484809">[3 more]</label></div><br/><div class="children"><div class="content">Depending on what you&#x27;re trying to accomplish, I&#x27;d highly recommend trying the 7B and 13B models first before jumping to the 70B. They&#x27;re quite capable and I think lots of folks assume they need to jump to a 70B model when really a smaller one would work fine.<p>That said, you should be able to fine-tune a 70B model on an A100 using QLoRA. However, depending on the specifics of your dataset it might actually be cheaper to run on an 8xA100 machine since that way you don&#x27;t have to swap any weights out to the machine&#x27;s non-GPU memory, and you might get enough time savings from that that the more expensive machine pays for itself.</div><br/><div id="37485131" class="c"><input type="checkbox" id="c-37485131" checked=""/><div class="controls bullet"><span class="by">indeyets</span><span>|</span><a href="#37484592">root</a><span>|</span><a href="#37484809">parent</a><span>|</span><a href="#37487844">next</a><span>|</span><label class="collapse" for="c-37485131">[-]</label><label class="expand" for="c-37485131">[2 more]</label></div><br/><div class="children"><div class="content">The plan was to do it in-house. And buying 8xA100 is a bit too much ;)</div><br/><div id="37492780" class="c"><input type="checkbox" id="c-37492780" checked=""/><div class="controls bullet"><span class="by">FrostKiwi</span><span>|</span><a href="#37484592">root</a><span>|</span><a href="#37485131">parent</a><span>|</span><a href="#37487844">next</a><span>|</span><label class="collapse" for="c-37492780">[-]</label><label class="expand" for="c-37492780">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m in the exactly same boat. Targeting to fine tune llama 2 70b on 2xA100, with the hope of having one A100 run an 8bit quantized 70b model 24&#x2F;7.<p>If you have an experiences to share, successes or failures, please do.</div><br/></div></div></div></div></div></div></div></div><div id="37487844" class="c"><input type="checkbox" id="c-37487844" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#37484592">prev</a><span>|</span><a href="#37487440">next</a><span>|</span><label class="collapse" for="c-37487844">[-]</label><label class="expand" for="c-37487844">[1 more]</label></div><br/><div class="children"><div class="content">A 7b model will work for very specific cases, but it will have a hard time drawing parallels between synonims, so you&#x27;ll need to be extremely careful in building your fine tuning samples.</div><br/></div></div><div id="37487440" class="c"><input type="checkbox" id="c-37487440" checked=""/><div class="controls bullet"><span class="by">he11ow</span><span>|</span><a href="#37487844">prev</a><span>|</span><a href="#37486025">next</a><span>|</span><label class="collapse" for="c-37487440">[-]</label><label class="expand" for="c-37487440">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! When it comes to choosing where to work with these models, which compute platform do you recommend (assuming locally doesn&#x27;t really make sense with my resources)?
Colab? 
AWS StudioLab?<p>Which is your go to?</div><br/></div></div><div id="37486025" class="c"><input type="checkbox" id="c-37486025" checked=""/><div class="controls bullet"><span class="by">jxf</span><span>|</span><a href="#37487440">prev</a><span>|</span><a href="#37484872">next</a><span>|</span><label class="collapse" for="c-37486025">[-]</label><label class="expand" for="c-37486025">[3 more]</label></div><br/><div class="children"><div class="content">Q: How did you arrive at the $23k figure for classifying 2M examples using GPT-4?</div><br/><div id="37486207" class="c"><input type="checkbox" id="c-37486207" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37486025">parent</a><span>|</span><a href="#37484872">next</a><span>|</span><label class="collapse" for="c-37486207">[-]</label><label class="expand" for="c-37486207">[2 more]</label></div><br/><div class="children"><div class="content">We ran 5K randomly selected recipes through GPT-4 and extrapolated based on the average cost per query.</div><br/><div id="37487469" class="c"><input type="checkbox" id="c-37487469" checked=""/><div class="controls bullet"><span class="by">jxf</span><span>|</span><a href="#37486025">root</a><span>|</span><a href="#37486207">parent</a><span>|</span><a href="#37484872">next</a><span>|</span><label class="collapse" for="c-37487469">[-]</label><label class="expand" for="c-37487469">[1 more]</label></div><br/><div class="children"><div class="content">Makes sense. Thank you!</div><br/></div></div></div></div></div></div><div id="37484872" class="c"><input type="checkbox" id="c-37484872" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#37486025">prev</a><span>|</span><a href="#37484756">next</a><span>|</span><label class="collapse" for="c-37484872">[-]</label><label class="expand" for="c-37484872">[4 more]</label></div><br/><div class="children"><div class="content">Do you still use few-shot prompting with a fine-tune? Or does it make little difference?</div><br/><div id="37484943" class="c"><input type="checkbox" id="c-37484943" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37484872">parent</a><span>|</span><a href="#37485456">next</a><span>|</span><label class="collapse" for="c-37484943">[-]</label><label class="expand" for="c-37484943">[2 more]</label></div><br/><div class="children"><div class="content">Nope, no need for few-shot prompting in most cases once you&#x27;ve fine-tuned on your dataset, so you can save those tokens and get cheaper&#x2F;faster responses!</div><br/><div id="37485600" class="c"><input type="checkbox" id="c-37485600" checked=""/><div class="controls bullet"><span class="by">selfhoster11</span><span>|</span><a href="#37484872">root</a><span>|</span><a href="#37484943">parent</a><span>|</span><a href="#37485456">next</a><span>|</span><label class="collapse" for="c-37485600">[-]</label><label class="expand" for="c-37485600">[1 more]</label></div><br/><div class="children"><div class="content">Not only that, but in a lot of cases you won&#x27;t have to fine-tune at all if an existing instruct model does a good enough job with unambiguous enough instructions.</div><br/></div></div></div></div><div id="37485456" class="c"><input type="checkbox" id="c-37485456" checked=""/><div class="controls bullet"><span class="by">selfhoster11</span><span>|</span><a href="#37484872">parent</a><span>|</span><a href="#37484943">prev</a><span>|</span><a href="#37484756">next</a><span>|</span><label class="collapse" for="c-37485456">[-]</label><label class="expand" for="c-37485456">[1 more]</label></div><br/><div class="children"><div class="content">In my experience, there is little need to do that. With completely unambiguous instructions that describe the exact output format, you can often get away with no examples whatsoever. Single examples might be helpful, but multi-shot prompting will be definitely unneeded (and may even harm the model&#x27;s output quality).</div><br/></div></div></div></div><div id="37489073" class="c"><input type="checkbox" id="c-37489073" checked=""/><div class="controls bullet"><span class="by">robot</span><span>|</span><a href="#37484756">prev</a><span>|</span><a href="#37486123">next</a><span>|</span><label class="collapse" for="c-37489073">[-]</label><label class="expand" for="c-37489073">[1 more]</label></div><br/><div class="children"><div class="content">for startups I guess this means nail your use case with gpt-4, and when scaling cost becomes an issue consider fine tuning.</div><br/></div></div><div id="37486123" class="c"><input type="checkbox" id="c-37486123" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37489073">prev</a><span>|</span><a href="#37490504">next</a><span>|</span><label class="collapse" for="c-37486123">[-]</label><label class="expand" for="c-37486123">[2 more]</label></div><br/><div class="children"><div class="content">Do you think this would end up facilitating the diffusion of finetuned LLMs ckpt models, just like stable diffusion? What&#x27;s missing is web-UI?</div><br/><div id="37486393" class="c"><input type="checkbox" id="c-37486393" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37486123">parent</a><span>|</span><a href="#37490504">next</a><span>|</span><label class="collapse" for="c-37486393">[-]</label><label class="expand" for="c-37486393">[1 more]</label></div><br/><div class="children"><div class="content">There are already many hundreds of finetunes on huggingface, and many excellent UIs to run them in, like KoboldCPP and Text-gen-ui: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&amp;search=13B" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&amp;search=13B</a><p>There is even a crowdsourced version of the UI like artbot: <a href="https:&#x2F;&#x2F;lite.koboldai.net&#x2F;#" rel="nofollow noreferrer">https:&#x2F;&#x2F;lite.koboldai.net&#x2F;#</a><p>And there are some excellent extant finetuning frameworks, like Aoxotol, that run on consumer GPUs: <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAccess-AI-Collective&#x2F;axolotl">https:&#x2F;&#x2F;github.com&#x2F;OpenAccess-AI-Collective&#x2F;axolotl</a><p>IIRC Text-gen-ui had a QLORA finetuning UI too.<p>What I am saying is that its <i>already</i> like Stable Diffusion, but the community is just somewhat under the radar, and finetuning will never be quite as turnkey as dreambooth&#x2F;sd 1.5 LORA due to the nature of the training data.</div><br/></div></div></div></div><div id="37490504" class="c"><input type="checkbox" id="c-37490504" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#37486123">prev</a><span>|</span><a href="#37485885">next</a><span>|</span><label class="collapse" for="c-37490504">[-]</label><label class="expand" for="c-37490504">[1 more]</label></div><br/><div class="children"><div class="content">What are your thoughts on fine tuning vs low rank adaptations?</div><br/></div></div><div id="37485885" class="c"><input type="checkbox" id="c-37485885" checked=""/><div class="controls bullet"><span class="by">Maschinesky</span><span>|</span><a href="#37490504">prev</a><span>|</span><a href="#37486190">next</a><span>|</span><label class="collapse" for="c-37485885">[-]</label><label class="expand" for="c-37485885">[3 more]</label></div><br/><div class="children"><div class="content">What makes sense to fine-tune and what not?<p>You said 50-1000 examples.<p>Do I fine-tune when having specific q&#x2F;a sets like from real customers and I want to add the right answer to the model?<p>Do I fine-tune facts or should I use some lookup?<p>Does adding some code and API docs for a current version of something I want more support make sense? Like chatgpt knows quarkus 2 but not quarkus 3</div><br/><div id="37486105" class="c"><input type="checkbox" id="c-37486105" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#37485885">parent</a><span>|</span><a href="#37486109">next</a><span>|</span><label class="collapse" for="c-37486105">[-]</label><label class="expand" for="c-37486105">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What makes sense to fine-tune and what not?<p>In general, fine-tuning helps a model figure out how to do the exact task that is being done in the examples it&#x27;s given. So fine-tuning it on 1000 examples of an API being used in the wild is likely to teach it to use that API really effectively, but fine-tuning it on just the API docs probably won&#x27;t.<p>That said, there are a lot of interesting ideas floating around on how to most effectively teach a model purely from instructions like API docs. Powerful models like GPT-4 can figure it out from in-context learning (ie. if you paste in a page of API docs and ask GPT-4 to write something with the API it can usually do a decent job). I suspect the community will figure out techniques either through new training objectives or synthetic training data to do it for smaller fine-tuned models as well.</div><br/></div></div><div id="37486109" class="c"><input type="checkbox" id="c-37486109" checked=""/><div class="controls bullet"><span class="by">Arctic_fly</span><span>|</span><a href="#37485885">parent</a><span>|</span><a href="#37486105">prev</a><span>|</span><a href="#37486190">next</a><span>|</span><label class="collapse" for="c-37486109">[-]</label><label class="expand" for="c-37486109">[1 more]</label></div><br/><div class="children"><div class="content">Generally speaking, fine-tuning a small model makes sense when the task that you want it to carry out is well-defined and doesn&#x27;t vary too much from one prompt to another. Fine-tuning facts into a model doesn&#x27;t seem to scale super well, but general textual style, output format, and evaluation criteria for example can all be instilled through the fine-tuning process. I would use lookup if you need your answers to include a wide array of information that the model you&#x27;re basing off of wasn&#x27;t initially trained on.</div><br/></div></div></div></div><div id="37486190" class="c"><input type="checkbox" id="c-37486190" checked=""/><div class="controls bullet"><span class="by">idosh</span><span>|</span><a href="#37485885">prev</a><span>|</span><a href="#37486007">next</a><span>|</span><label class="collapse" for="c-37486190">[-]</label><label class="expand" for="c-37486190">[4 more]</label></div><br/><div class="children"><div class="content">Can you elaborate on your plans for OpenPipe? Sounds like a very interesting project</div><br/><div id="37486499" class="c"><input type="checkbox" id="c-37486499" checked=""/><div class="controls bullet"><span class="by">Arctic_fly</span><span>|</span><a href="#37486190">parent</a><span>|</span><a href="#37486007">next</a><span>|</span><label class="collapse" for="c-37486499">[-]</label><label class="expand" for="c-37486499">[3 more]</label></div><br/><div class="children"><div class="content">Currently OpenPipe allows you to capture input&#x2F;output from a powerful model and use it to fine-tune a much smaller one, then offers you the option to host through OpenPipe or download it and host it elsewhere. Models hosted on OpenPipe enjoy a few benefits, like data drift detection and automatic reformatting of output to match the original model you trained against (think extraction &quot;function call&quot; responses from a purely textual Llama 2 response) through the sdk.<p>Longer-term, we&#x27;d love to expand the selection of base models to include specialized LLMs that are particularly good at a certain task, e.g. language translation, and let you train off of those as well. Providing a ton of specialized starting models will decrease the amount of training data you need, and increase the number of tasks at which fine-tuned models can excel.</div><br/><div id="37487950" class="c"><input type="checkbox" id="c-37487950" checked=""/><div class="controls bullet"><span class="by">idosh</span><span>|</span><a href="#37486190">root</a><span>|</span><a href="#37486499">parent</a><span>|</span><a href="#37489890">next</a><span>|</span><label class="collapse" for="c-37487950">[-]</label><label class="expand" for="c-37487950">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I need to dive into the project and learn more. Sounds exciting</div><br/></div></div><div id="37489890" class="c"><input type="checkbox" id="c-37489890" checked=""/><div class="controls bullet"><span class="by">throw03172019</span><span>|</span><a href="#37486190">root</a><span>|</span><a href="#37486499">parent</a><span>|</span><a href="#37487950">prev</a><span>|</span><a href="#37486007">next</a><span>|</span><label class="collapse" for="c-37489890">[-]</label><label class="expand" for="c-37489890">[1 more]</label></div><br/><div class="children"><div class="content">Any compliance yet? HIPAA etc</div><br/></div></div></div></div></div></div><div id="37486007" class="c"><input type="checkbox" id="c-37486007" checked=""/><div class="controls bullet"><span class="by">notShabu</span><span>|</span><a href="#37486190">prev</a><span>|</span><a href="#37485897">next</a><span>|</span><label class="collapse" for="c-37486007">[-]</label><label class="expand" for="c-37486007">[2 more]</label></div><br/><div class="children"><div class="content">This post made me think of human hierarchies. Line level ICs are cheap because they are specialized and fine tuned. Leet code is a way to roughly measure degree of fine-tuning even though it doesn&#x27;t accurately measure how well the fine tuning is for the job.<p>As you go up the hierarchy what you want is higher quality answers to more and more abstract and general questions.<p>AGI, God, CEOs, and figures like Paul Graham, Elon Musk etc.. all answer to various degrees the ultimate abstract question of &quot;What is the meaning of <i>gestures wildly at everything</i>&quot;<p>Cost efficiency and commoditization basically increases &quot;how&quot; capacity at the cost of &quot;why&quot; capacity</div><br/><div id="37487251" class="c"><input type="checkbox" id="c-37487251" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#37486007">parent</a><span>|</span><a href="#37485897">next</a><span>|</span><label class="collapse" for="c-37487251">[-]</label><label class="expand" for="c-37487251">[1 more]</label></div><br/><div class="children"><div class="content">&gt; AGI, God, CEOs, and figures like Paul Graham, Elon Musk<p>hacker news pantheon just dropped</div><br/></div></div></div></div><div id="37485897" class="c"><input type="checkbox" id="c-37485897" checked=""/><div class="controls bullet"><span class="by">halyconWays</span><span>|</span><a href="#37486007">prev</a><span>|</span><a href="#37489045">next</a><span>|</span><label class="collapse" for="c-37485897">[-]</label><label class="expand" for="c-37485897">[3 more]</label></div><br/><div class="children"><div class="content">Someone needs to make an LLM purpose-built for creating high-quality datasets for fine-tuning other LLMs.</div><br/><div id="37487210" class="c"><input type="checkbox" id="c-37487210" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#37485897">parent</a><span>|</span><a href="#37492336">prev</a><span>|</span><a href="#37489045">next</a><span>|</span><label class="collapse" for="c-37487210">[-]</label><label class="expand" for="c-37487210">[1 more]</label></div><br/><div class="children"><div class="content">This. The best use of the current llms is to create better Datasets.</div><br/></div></div></div></div><div id="37489045" class="c"><input type="checkbox" id="c-37489045" checked=""/><div class="controls bullet"><span class="by">facu17y</span><span>|</span><a href="#37485897">prev</a><span>|</span><label class="collapse" for="c-37489045">[-]</label><label class="expand" for="c-37489045">[1 more]</label></div><br/><div class="children"><div class="content">&quot;to replace GPT-3.5&#x2F;4&quot;<p>Very inflated statement when it comes to GPT4 since it is a MoE model with 8 separate models each an expert in one area, and you can&#x27;t replace all 8 models with one model trained for $19.<p>I call BS on this claim. Maybe it matches GPT4 in the narrow domain you fine-tune it for, and if that can be done for $19 then for $19*8 you can take OpenAI out of business. That doesn&#x27;t add up.</div><br/></div></div></div></div></div></div></div></body></html>