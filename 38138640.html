<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699088454564" as="style"/><link rel="stylesheet" href="styles.css?v=1699088454564"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.cloudflare.com/post-mortem-on-cloudflare-control-plane-and-analytics-outage/">Post Mortem on Cloudflare Control Plane and Analytics Outage</a> <span class="domain">(<a href="https://blog.cloudflare.com">blog.cloudflare.com</a>)</span></div><div class="subtext"><span>eastdakota</span> | <span>35 comments</span></div><br/><div><div id="38138973" class="c"><input type="checkbox" id="c-38138973" checked=""/><div class="controls bullet"><span class="by">Dunedan</span><span>|</span><a href="#38139228">next</a><span>|</span><label class="collapse" for="c-38138973">[-]</label><label class="expand" for="c-38138973">[4 more]</label></div><br/><div class="children"><div class="content">&gt; While most of our critical control plane systems had been migrated to the high availability cluster, some services, especially for some newer products, had not yet been added to the high availability cluster.<p>&gt; The other two data centers running in the area would take over responsibility for the high availability cluster and keep critical services online. Generally that worked as planned. Unfortunately, we discovered that a subset of services that were supposed to be on the high availability cluster had dependencies on services exclusively running in PDX-04.<p>&gt; A handful of products did not properly get stood up on our disaster recovery sites. These tended to be newer products where we had not fully implemented and tested a disaster recovery procedure.<p>So the root cause for the outage was that they relied on a single data center. I find that pretty embarrassing for a company like Cloudflare, which powers such relevant parts of the internet.</div><br/><div id="38139008" class="c"><input type="checkbox" id="c-38139008" checked=""/><div class="controls bullet"><span class="by">thelastparadise</span><span>|</span><a href="#38138973">parent</a><span>|</span><a href="#38139063">next</a><span>|</span><label class="collapse" for="c-38139008">[-]</label><label class="expand" for="c-38139008">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I find that pretty embarrassing for a company like Cloudflare, which powers such relevant parts of the internet.<p>Absolute lack of faith in cloudflare rn.<p>This is amateur hour stuff.<p>It&#x27;s especially egregious that these are <i>new</i> services that were rolled out without HA.</div><br/><div id="38139123" class="c"><input type="checkbox" id="c-38139123" checked=""/><div class="controls bullet"><span class="by">NicoJuicy</span><span>|</span><a href="#38138973">root</a><span>|</span><a href="#38139008">parent</a><span>|</span><a href="#38139063">next</a><span>|</span><label class="collapse" for="c-38139123">[-]</label><label class="expand" for="c-38139123">[1 more]</label></div><br/><div class="children"><div class="content">?<p>Tbh. As far as I can see, their data plane worked at the edge.<p>Cloudflare released a lot of new products and the ones that affected were: streams, new image upload and logpush.<p>Their control plane was bad though. But since most products worked, that&#x27;s more redundancy than most products.<p>The proposed solution is simple:<p>- GA requires to be in the high availability cluster<p>- test entire DC outages</div><br/></div></div></div></div><div id="38139063" class="c"><input type="checkbox" id="c-38139063" checked=""/><div class="controls bullet"><span class="by">simion314</span><span>|</span><a href="#38138973">parent</a><span>|</span><a href="#38139008">prev</a><span>|</span><a href="#38139228">next</a><span>|</span><label class="collapse" for="c-38139063">[-]</label><label class="expand" for="c-38139063">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I find that pretty embarrassing for a company like Cloudflare,<p>Cloudflare has a shit reputation in my eyes, because their terrible captchas , like they are broken but they ship that for millions to suffer.  Latest problem for me is ChatGPT, I get a captcha at any query on Firefox, changing to Chromium and I get zero.  And the captcha is also broken, I use the audio version, I put the correct answer and they claim is wrong. I try again and they have the exact same question, they always ask about a cat sound , probably the system got broken on an update and they did not notice yet.<p>So IMO do not expect quality from them or OpenAI dev ops.</div><br/></div></div></div></div><div id="38139228" class="c"><input type="checkbox" id="c-38139228" checked=""/><div class="controls bullet"><span class="by">awesomebing1</span><span>|</span><a href="#38138973">prev</a><span>|</span><a href="#38138749">next</a><span>|</span><label class="collapse" for="c-38139228">[-]</label><label class="expand" for="c-38139228">[1 more]</label></div><br/><div class="children"><div class="content">Somewhat amazed at the structure of this article: after first blaming their third-party across for 75% of blog post, their recovery efforts were detailed with considerably less paragraphs. It’s promising to see a path forward mentioned but I can’t help but wonder why this was immediately published instead of acknowledging their failure&#x2F;circumstances right now and later on publishing a complete post-mortem after the dust fully settles (i.e. without speculation).</div><br/></div></div><div id="38138749" class="c"><input type="checkbox" id="c-38138749" checked=""/><div class="controls bullet"><span class="by">austinkhale</span><span>|</span><a href="#38139228">prev</a><span>|</span><a href="#38139106">next</a><span>|</span><label class="collapse" for="c-38138749">[-]</label><label class="expand" for="c-38138749">[5 more]</label></div><br/><div class="children"><div class="content">I love how thorough Cloudflare post mortem’s are. Reading the frank, transparent explanations are like a breath of fresh air compared to the obfuscation of nearly every other company comm’s strategy.<p>We were affected but it’s blog posts like these that make me never want to move away. Everyone makes mistakes. Everyone has bad days. It’s how you react afterwards that makes the difference.</div><br/><div id="38138896" class="c"><input type="checkbox" id="c-38138896" checked=""/><div class="controls bullet"><span class="by">w10-1</span><span>|</span><a href="#38138749">parent</a><span>|</span><a href="#38139106">next</a><span>|</span><label class="collapse" for="c-38138896">[-]</label><label class="expand" for="c-38138896">[4 more]</label></div><br/><div class="children"><div class="content">I agree, but I also think that for security purposes they should leave out extraneous detail.  Also, I know they want to hold their suppliers accountable, but I would hold off pointing fingers.  It doesn&#x27;t really improve behavior, and it makes incentives worse.<p>I really appreciate that they&#x27;re going to fix the process errors here.  But as they suggested, there&#x27;s a tension between moving fast and being sure.  This is typically managed like the weather, buying rain jackets afterwards (not optimal).  I&#x27;d be curious to see how they can make reliability part of the culture without tying development up in process.<p>Perhaps they can model the system in software, then use traffic analytics to validate their models.  If they can lower the cost of reliability experiments by doing virtual experiments, they might be able to catch more before roll-out.</div><br/><div id="38138969" class="c"><input type="checkbox" id="c-38138969" checked=""/><div class="controls bullet"><span class="by">logifail</span><span>|</span><a href="#38138749">root</a><span>|</span><a href="#38138896">parent</a><span>|</span><a href="#38139165">next</a><span>|</span><label class="collapse" for="c-38138969">[-]</label><label class="expand" for="c-38138969">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I also think that for security purposes they should leave out extraneous detail<p>Disagree completely, it&#x27;s the frank detail that makes me trust their story.</div><br/></div></div><div id="38139165" class="c"><input type="checkbox" id="c-38139165" checked=""/><div class="controls bullet"><span class="by">NicoJuicy</span><span>|</span><a href="#38138749">root</a><span>|</span><a href="#38138896">parent</a><span>|</span><a href="#38138969">prev</a><span>|</span><a href="#38138991">next</a><span>|</span><label class="collapse" for="c-38139165">[-]</label><label class="expand" for="c-38139165">[1 more]</label></div><br/><div class="children"><div class="content">&gt; know they want to hold their suppliers accountable<p>They do both. They stated what their problem was and they stated their due diligence in picking a DC<p>&gt; While the PDX-04’s design was certified Tier III before construction and is expected to provide high availability SLAs<p>They said the core issue: innovating fast, which led to not requiring in the high availability cluster.<p>Which is also a fix.<p>From cloudflare &#x27;s POV, part of what made it originally worse, is the lack of communication by the DC.<p>Which is an issue, if you want to inform clients.</div><br/></div></div><div id="38138991" class="c"><input type="checkbox" id="c-38138991" checked=""/><div class="controls bullet"><span class="by">devwastaken</span><span>|</span><a href="#38138749">root</a><span>|</span><a href="#38138896">parent</a><span>|</span><a href="#38139165">prev</a><span>|</span><a href="#38139106">next</a><span>|</span><label class="collapse" for="c-38138991">[-]</label><label class="expand" for="c-38138991">[1 more]</label></div><br/><div class="children"><div class="content">What &quot;security purposes&quot;? Good security isn&#x27;t based on ignorance of a system, it is on the system being good. We create a self fulfilling prophecy when we hide security practices because what happens is then very few will properly implement their security. Openness is necessary for learning.</div><br/></div></div></div></div></div></div><div id="38139106" class="c"><input type="checkbox" id="c-38139106" checked=""/><div class="controls bullet"><span class="by">__turbobrew__</span><span>|</span><a href="#38138749">prev</a><span>|</span><a href="#38138747">next</a><span>|</span><label class="collapse" for="c-38139106">[-]</label><label class="expand" for="c-38139106">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However, we had never tested fully taking the entire PDX-04 facility offline.<p>That is a painful lesson, but unless you are physically powering off the dc or at least disconnecting the network from the outside world you are not testing a real disaster.<p>You can point fingers at the facility operators, but at the end of the day you have to be able to recover from a dc going completely offline and maybe never coming back. Mother Nature may wipe it off the face of the earth.</div><br/></div></div><div id="38138747" class="c"><input type="checkbox" id="c-38138747" checked=""/><div class="controls bullet"><span class="by">tebbers</span><span>|</span><a href="#38139106">prev</a><span>|</span><a href="#38138841">next</a><span>|</span><label class="collapse" for="c-38138747">[-]</label><label class="expand" for="c-38138747">[1 more]</label></div><br/><div class="children"><div class="content">I think overall Cloudflare did a decent job on this. Clearly the DC provider cocked up big time here, but Cloudflare kept running fine for the vast majority of customers globally. No system is perfect and it’s only apocalyptic scenarios like this where the vulnerabilities are exposed - and they will now be fixed. Hope the SRE guys got some rest after all that stress.</div><br/></div></div><div id="38138841" class="c"><input type="checkbox" id="c-38138841" checked=""/><div class="controls bullet"><span class="by">solatic</span><span>|</span><a href="#38138747">prev</a><span>|</span><a href="#38138697">next</a><span>|</span><label class="collapse" for="c-38138841">[-]</label><label class="expand" for="c-38138841">[2 more]</label></div><br/><div class="children"><div class="content">Not criticism, just remarks:<p>&gt; While, over time, our practice is to migrate the backend for these services to our best practices, we did not formally require that before products were declared generally available (GA).<p>I really like the model where a single team in a company, with Product + Dev, can quickly ship, iterate on a new product, and prove market demand without going through layers and layers of internal bureaucracy (Ops&#x2F;Infra, Security, Privacy&#x2F;Legal, Finance approval for production-scale), with the main stipulation being that such work is marked as alpha&#x2F;beta&#x2F;preview, and only going through the layers of internal bureaucracy once it&#x27;s ready to go GA. But most companies really struggle with this, especially with ensuring that customers are never exposed to a&#x2F;b&#x2F;p software by default, requiring opt-in from the customer, allowing the customer to easily opt-out, and ensuring that using a&#x2F;b&#x2F;p software never endangers GA features they depend on. Building that out, if it&#x27;s even on a company&#x27;s internal Platform&#x2F;DevX backlog, is usually super far down as a &quot;wishlist&quot; item. So I&#x27;m super interested to see what Cloudflare can build here and whether that can ever get exposed as part of their public Product portfolio as well.<p>&gt; We need to use the distributed systems products that we make available to all our customers for all our services so they continue to function mostly as normal even if our core facilities are disrupted.<p>Super excited to see this. Cloudflare Workers is still too much of an &quot;edge&quot; platform and not a &quot;main datacenter&quot; platform, at least because D1 is still in beta and even if it wasn&#x27;t, Postgres is far more feature-ful, and that pulls more software into a traditional single-datacenter model. So if Cloudflare can really succeed at this, then it&#x27;ll be a much stronger statement in favor of building out software in an edge-only model.<p>Between the Pages outage and the API outage happening in one week, I was considering selling my NET stock, but reading a postmortem like this reminds me why I invested in NET in the first place. Thanks Matt.</div><br/><div id="38139157" class="c"><input type="checkbox" id="c-38139157" checked=""/><div class="controls bullet"><span class="by">throwaway6920</span><span>|</span><a href="#38138841">parent</a><span>|</span><a href="#38138697">next</a><span>|</span><label class="collapse" for="c-38139157">[-]</label><label class="expand" for="c-38139157">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I really like the model where a single team in a company, with Product + Dev, can quickly ship, iterate on a new product, and prove market demand without going through layers and layers of internal bureaucracy (Ops&#x2F;Infra, Security, Privacy&#x2F;Legal, Finance approval for production-scale), with the main stipulation being that such work is marked as alpha&#x2F;beta&#x2F;preview, and only going through the layers of internal bureaucracy once it&#x27;s ready to go GA.<p>Speaking from personal experience, what you&#x27;re claiming as &#x27;good&#x27;, for CF meant SRE- usually core, but edge also suffered- got stuck with trying to fix a fundamentally broken design that was known faulty- and called faulty repeatedly- but forced through.<p>Nothing about this is desirable or will end well.<p>This reckoning was known and raised by multiple SRE near a decade before this occurred, and there were multiple near misses in the last few years that were ignored.<p>The part that&#x27;s probably funny- and painful- for ex-CF SRE is that the company will do a hard pivot and try to rectify this mess.  It&#x27;s always harder to fix after, rather than building for, and they&#x27;ve ignored this for a long while.</div><br/></div></div></div></div><div id="38138697" class="c"><input type="checkbox" id="c-38138697" checked=""/><div class="controls bullet"><span class="by">matdehaast</span><span>|</span><a href="#38138841">prev</a><span>|</span><a href="#38138830">next</a><span>|</span><label class="collapse" for="c-38138697">[-]</label><label class="expand" for="c-38138697">[1 more]</label></div><br/><div class="children"><div class="content">Some days things just go badly. The only thing you can change is how you respond. Well done to you and the team for getting through this.<p>I for one am and will always be a cloudflare customer</div><br/></div></div><div id="38138830" class="c"><input type="checkbox" id="c-38138830" checked=""/><div class="controls bullet"><span class="by">TekMol</span><span>|</span><a href="#38138697">prev</a><span>|</span><a href="#38138742">next</a><span>|</span><label class="collapse" for="c-38138830">[-]</label><label class="expand" for="c-38138830">[5 more]</label></div><br/><div class="children"><div class="content">Contrary to others here, I find the postmortem a bit lacking.<p>The TLDR is that CF runs in multiple data centers, one went down, and the services that depend on it went down with it.<p>The interesting question would be why those services <i>did</i> depend on a single data center.<p>They are pretty vague about it<p><pre><code>    Cloudflare allows multiple teams to innovate quickly. As such,
    products often take different paths toward their initial alpha.
</code></pre>
If I was the CEO, I would look into the specific decisions of the engineers and why they decided to make services depend on just one data center. That would make an interesting blog post to me.<p>Designing a highly available system and building a company fast leads to interesting tradeoffs. The <i>details</i> would be interesting.</div><br/><div id="38138897" class="c"><input type="checkbox" id="c-38138897" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#38138830">parent</a><span>|</span><a href="#38138868">next</a><span>|</span><label class="collapse" for="c-38138897">[-]</label><label class="expand" for="c-38138897">[2 more]</label></div><br/><div class="children"><div class="content">&gt; why they decided to make services depend on just one data center<p>In my experience, no engineers really <i>decided</i> to make services depend on just one data center. It happened because the dependency was overlooked. Or it happened because the dependency was thought to be a &quot;soft dependency&quot; with graceful degradation in case of unavailability but the graceful degradation path had a bug. Or it happened because the engineers thought it had a dependency on one of multiple data centers, but then the failover process had a bug.<p>Reminds me of that time when a single data center in Paris for GCP brought down the entire Google Cloud Console albeit briefly. Really the same thing.</div><br/><div id="38139185" class="c"><input type="checkbox" id="c-38139185" checked=""/><div class="controls bullet"><span class="by">throwaway6920</span><span>|</span><a href="#38138830">root</a><span>|</span><a href="#38138897">parent</a><span>|</span><a href="#38138868">next</a><span>|</span><label class="collapse" for="c-38139185">[-]</label><label class="expand" for="c-38139185">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In my experience, no engineers really decided to make services depend on just one data center.<p>Partially true in this case; I can&#x27;t speak to modern CF (or won&#x27;t, moreso) but a large amount of internal services were built around SQL db&#x27;s, and weren&#x27;t built with any sense of eventual consistency.  Usage of read replicas was basically unheard of.  Knowing that, and that this was normal, it&#x27;s a cultural issue rather than an &quot;oops&quot; issue.<p>Flipping the whole DC data sources is a sign of what I&#x27;m describing; FAANG would instead be running services in multiple DC&#x27;s rather than relying on primary&#x2F;secondary architecture.</div><br/></div></div></div></div><div id="38138868" class="c"><input type="checkbox" id="c-38138868" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#38138830">parent</a><span>|</span><a href="#38138897">prev</a><span>|</span><a href="#38138878">next</a><span>|</span><label class="collapse" for="c-38138868">[-]</label><label class="expand" for="c-38138868">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this sentence a bit further down more clear?<p>&gt; This is a system design that we began implementing four years ago. While most of our critical control plane systems had been migrated to the high availability cluster, some services, especially for some newer products, had not yet been added to the high availability cluster.<p>and<p>&gt; It [PDX-04] is also the default location for services that have not yet been onboarded onto our high availability cluster.</div><br/></div></div><div id="38138878" class="c"><input type="checkbox" id="c-38138878" checked=""/><div class="controls bullet"><span class="by">nanankcornering</span><span>|</span><a href="#38138830">parent</a><span>|</span><a href="#38138868">prev</a><span>|</span><a href="#38138742">next</a><span>|</span><label class="collapse" for="c-38138878">[-]</label><label class="expand" for="c-38138878">[1 more]</label></div><br/><div class="children"><div class="content">I experienced it myself within the last 24 hours. New D1 &amp; Hyperdrive deployment was not working. It would spew out internal errors &amp; timeouts.<p>Both are non-GA products, and the point is that non-GA are not part of the HA cluster (yet)</div><br/></div></div></div></div><div id="38138742" class="c"><input type="checkbox" id="c-38138742" checked=""/><div class="controls bullet"><span class="by">divbzero</span><span>|</span><a href="#38138830">prev</a><span>|</span><a href="#38139186">next</a><span>|</span><label class="collapse" for="c-38138742">[-]</label><label class="expand" for="c-38138742">[3 more]</label></div><br/><div class="children"><div class="content"><i>Cloudflare&#x27;s control plane and analytics systems run primarily on servers in three data centers around Hillsboro, Oregon. The three data centers are independent of one another, each have multiple utility power feeds, and each have multiple redundant and independent network connections. The facilities were intentionally chosen to be at a distance apart that would minimize the chances that a natural disaster would cause all three to be impacted, while still close enough that they could all run active-active redundant data clusters.</i><p>If the three data centers are all around Hillsboro, Oregon, an earthquake could probably take out all three simultaneously.</div><br/><div id="38138831" class="c"><input type="checkbox" id="c-38138831" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#38138742">parent</a><span>|</span><a href="#38138762">next</a><span>|</span><label class="collapse" for="c-38138831">[-]</label><label class="expand" for="c-38138831">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Hillsboro, Oregon, an earthquake could probably take out all three simultaneously.<p>Is it west of I5?<p>(yes)<p>Oh yeah, they all gone.<p>Cascadia Subduction Zone - <a href="https:&#x2F;&#x2F;pnsn.org&#x2F;outreach&#x2F;earthquakesources&#x2F;csz" rel="nofollow noreferrer">https:&#x2F;&#x2F;pnsn.org&#x2F;outreach&#x2F;earthquakesources&#x2F;csz</a></div><br/></div></div><div id="38138762" class="c"><input type="checkbox" id="c-38138762" checked=""/><div class="controls bullet"><span class="by">nanankcornering</span><span>|</span><a href="#38138742">parent</a><span>|</span><a href="#38138831">prev</a><span>|</span><a href="#38139186">next</a><span>|</span><label class="collapse" for="c-38138762">[-]</label><label class="expand" for="c-38138762">[1 more]</label></div><br/><div class="children"><div class="content">thats why DRC exists right?</div><br/></div></div></div></div><div id="38139186" class="c"><input type="checkbox" id="c-38139186" checked=""/><div class="controls bullet"><span class="by">sidcool</span><span>|</span><a href="#38138742">prev</a><span>|</span><a href="#38138904">next</a><span>|</span><label class="collapse" for="c-38139186">[-]</label><label class="expand" for="c-38139186">[1 more]</label></div><br/><div class="children"><div class="content">They really threw the electricity power provider under the bus there.</div><br/></div></div><div id="38138904" class="c"><input type="checkbox" id="c-38138904" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#38139186">prev</a><span>|</span><a href="#38139104">next</a><span>|</span><label class="collapse" for="c-38138904">[-]</label><label class="expand" for="c-38138904">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  It is not unusual for utilities to ask data centers to drop off the grid when power demands are high and run exclusively on generators.<p>Are the data centers compensated or anything for this? I&#x27;d imagine generator-only might cost more in terms of fuel and wear-and-tear&#x2F;maintinaince&#x2F;inspections.<p>edit:<p>&gt; DSG allows the local utility to run a data center&#x27;s generators to help supply additional power to the grid. In exchange, the power company helps maintain the generators and supplies fuel<p>Interesting.</div><br/></div></div><div id="38139104" class="c"><input type="checkbox" id="c-38139104" checked=""/><div class="controls bullet"><span class="by">pseudocoder_sp</span><span>|</span><a href="#38138904">prev</a><span>|</span><a href="#38138975">next</a><span>|</span><label class="collapse" for="c-38139104">[-]</label><label class="expand" for="c-38139104">[1 more]</label></div><br/><div class="children"><div class="content">Some one should make a webseries about this indecent. It will be a nice story to tell.
Name: Mordern Day Disaster.
Directed by : Mathew prince
Releasing on : 25th December at Netflix
Based on a true story.</div><br/></div></div><div id="38138975" class="c"><input type="checkbox" id="c-38138975" checked=""/><div class="controls bullet"><span class="by">iot_devs</span><span>|</span><a href="#38139104">prev</a><span>|</span><a href="#38138783">next</a><span>|</span><label class="collapse" for="c-38138975">[-]</label><label class="expand" for="c-38138975">[2 more]</label></div><br/><div class="children"><div class="content">Why the very first step was not to fail over Europe?</div><br/><div id="38138984" class="c"><input type="checkbox" id="c-38138984" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#38138975">parent</a><span>|</span><a href="#38138783">next</a><span>|</span><label class="collapse" for="c-38138984">[-]</label><label class="expand" for="c-38138984">[1 more]</label></div><br/><div class="children"><div class="content">They did after two hours. After the first they assumed the generators would be back but then ran into the breaker issue which caused the full day delay.</div><br/></div></div></div></div><div id="38138708" class="c"><input type="checkbox" id="c-38138708" checked=""/><div class="controls bullet"><span class="by">nanankcornering</span><span>|</span><a href="#38138783">prev</a><span>|</span><label class="collapse" for="c-38138708">[-]</label><label class="expand" for="c-38138708">[5 more]</label></div><br/><div class="children"><div class="content">At what time were you notified Matt?</div><br/><div id="38138721" class="c"><input type="checkbox" id="c-38138721" checked=""/><div class="controls bullet"><span class="by">eastdakota</span><span>|</span><a href="#38138708">parent</a><span>|</span><label class="collapse" for="c-38138721">[-]</label><label class="expand" for="c-38138721">[4 more]</label></div><br/><div class="children"><div class="content">Of the incident? Someone on my team called me about 30 minutes after it started. It was challenging for me to stay on top of because it was also the same day as our Q3 earnings call. But team kept me informed throughout the day. I helped where I could. And they handled a very difficult situation very well. That said, lots we can learn from and improve.</div><br/><div id="38138952" class="c"><input type="checkbox" id="c-38138952" checked=""/><div class="controls bullet"><span class="by">chaz6</span><span>|</span><a href="#38138708">root</a><span>|</span><a href="#38138721">parent</a><span>|</span><label class="collapse" for="c-38138952">[-]</label><label class="expand" for="c-38138952">[3 more]</label></div><br/><div class="children"><div class="content">What I find bizarre is that the Cloudflare share price jumped when the outage happend!<p>Having read the post mortem, I do not think it could have been handled any better. I think the decision to extend the outage in order to provide rest was absolutely correct.<p>I always enjoy reading these reports from Cloudflare as they are the best in the business.</div><br/><div id="38138962" class="c"><input type="checkbox" id="c-38138962" checked=""/><div class="controls bullet"><span class="by">eastdakota</span><span>|</span><a href="#38138708">root</a><span>|</span><a href="#38138952">parent</a><span>|</span><a href="#38139094">next</a><span>|</span><label class="collapse" for="c-38138962">[-]</label><label class="expand" for="c-38138962">[1 more]</label></div><br/><div class="children"><div class="content">I was surprised we didn&#x27;t get a single question about it from an analyst or investor, either formally on the Q3 call or on any callbacks we did after. One weird phenomenon we&#x27;ve seen — though not so much in this case because the impact wasn&#x27;t as publicly exposed — is that investors after we&#x27;ve had a really bad outage say: &quot;Oh, wow, I didn&#x27;t fully appreciate how important you were until you took down most of the Internet.&quot; So… ¯\_(ツ)_&#x2F;¯</div><br/></div></div><div id="38139094" class="c"><input type="checkbox" id="c-38139094" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#38138708">root</a><span>|</span><a href="#38138952">parent</a><span>|</span><a href="#38138962">prev</a><span>|</span><label class="collapse" for="c-38139094">[-]</label><label class="expand" for="c-38139094">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a class of investor (and their trade bots presumably) that sees outrage over a service outage as proof the provider is now mission critical, hence able to &quot;extract value&quot; from the market.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>