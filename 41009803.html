<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721466050736" as="style"/><link rel="stylesheet" href="styles.css?v=1721466050736"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising">What happened to BERT and T5?</a> <span class="domain">(<a href="https://www.yitay.net">www.yitay.net</a>)</span></div><div class="subtext"><span>fzliu</span> | <span>54 comments</span></div><br/><div><div id="41012046" class="c"><input type="checkbox" id="c-41012046" checked=""/><div class="controls bullet"><span class="by">hdhshdhshdjd</span><span>|</span><a href="#41011707">next</a><span>|</span><label class="collapse" for="c-41012046">[-]</label><label class="expand" for="c-41012046">[9 more]</label></div><br/><div class="children"><div class="content">Maybe in SOTA ml&#x2F;nlp research, but in the world of building useful tools and products, BERT models are dead simple to tune, work great if you have decent training data, and most importantly are very very fast and very very cheap to run.<p>I have a small Swiss army collection of custom BERT fine tunes that are equal or better than the best LLM and execute document classification tasks in 2.4ms. Find me an LLM that can do anything in 2.4ms.</div><br/><div id="41013051" class="c"><input type="checkbox" id="c-41013051" checked=""/><div class="controls bullet"><span class="by">deepsquirrelnet</span><span>|</span><a href="#41012046">parent</a><span>|</span><a href="#41012422">next</a><span>|</span><label class="collapse" for="c-41013051">[-]</label><label class="expand" for="c-41013051">[1 more]</label></div><br/><div class="children"><div class="content">Latency, throughput and cost are still very important for many applications.<p>Also the output of a purpose-built encoder model is preferable to natural language. Not only is it unambiguous, but scores are often an important part of the result.<p>Last, if you need to get into some advanced methods of training, like pseudolabeling and semi-supervised learning, there’s different options and outlets for utilizing real world datasets.<p>That said, I’m not sure there’s much value in scaling up current encoder models. It seems like there’s already a point of diminishing returns.</div><br/></div></div><div id="41012422" class="c"><input type="checkbox" id="c-41012422" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#41012046">parent</a><span>|</span><a href="#41013051">prev</a><span>|</span><a href="#41013837">next</a><span>|</span><label class="collapse" for="c-41012422">[-]</label><label class="expand" for="c-41012422">[2 more]</label></div><br/><div class="children"><div class="content">Want to share your collection with the class so we can all learn? Seems useful.</div><br/><div id="41012924" class="c"><input type="checkbox" id="c-41012924" checked=""/><div class="controls bullet"><span class="by">hdhshdhshdjd</span><span>|</span><a href="#41012046">root</a><span>|</span><a href="#41012422">parent</a><span>|</span><a href="#41013837">next</a><span>|</span><label class="collapse" for="c-41012924">[-]</label><label class="expand" for="c-41012924">[1 more]</label></div><br/><div class="children"><div class="content">Product in stealth for a little bit longer, so can’t say much. :-)</div><br/></div></div></div></div><div id="41013837" class="c"><input type="checkbox" id="c-41013837" checked=""/><div class="controls bullet"><span class="by">Seattle3503</span><span>|</span><a href="#41012046">parent</a><span>|</span><a href="#41012422">prev</a><span>|</span><a href="#41012385">next</a><span>|</span><label class="collapse" for="c-41013837">[-]</label><label class="expand" for="c-41013837">[2 more]</label></div><br/><div class="children"><div class="content">What technique do you use to get BERT to work on longer documents?</div><br/><div id="41014382" class="c"><input type="checkbox" id="c-41014382" checked=""/><div class="controls bullet"><span class="by">hdhshdhshdjd</span><span>|</span><a href="#41012046">root</a><span>|</span><a href="#41013837">parent</a><span>|</span><a href="#41012385">next</a><span>|</span><label class="collapse" for="c-41014382">[-]</label><label class="expand" for="c-41014382">[1 more]</label></div><br/><div class="children"><div class="content">512 has been sufficient to solve my problems. I had done some initial attempts with BigBird that weren’t going well, but then realized I didn’t really need it.<p>I may revisit at some point.</div><br/></div></div></div></div><div id="41012385" class="c"><input type="checkbox" id="c-41012385" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#41012046">parent</a><span>|</span><a href="#41013837">prev</a><span>|</span><a href="#41012147">next</a><span>|</span><label class="collapse" for="c-41012385">[-]</label><label class="expand" for="c-41012385">[2 more]</label></div><br/><div class="children"><div class="content">What does your swiss army collection do?</div><br/><div id="41012931" class="c"><input type="checkbox" id="c-41012931" checked=""/><div class="controls bullet"><span class="by">hdhshdhshdjd</span><span>|</span><a href="#41012046">root</a><span>|</span><a href="#41012385">parent</a><span>|</span><a href="#41012147">next</a><span>|</span><label class="collapse" for="c-41012931">[-]</label><label class="expand" for="c-41012931">[1 more]</label></div><br/><div class="children"><div class="content">Document classification in highly ambiguous contextual space. Solving some specific large scale classification tasks, so multi million document sets.</div><br/></div></div></div></div><div id="41012147" class="c"><input type="checkbox" id="c-41012147" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#41012046">parent</a><span>|</span><a href="#41012385">prev</a><span>|</span><a href="#41011707">next</a><span>|</span><label class="collapse" for="c-41012147">[-]</label><label class="expand" for="c-41012147">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, pretty much. When you have 2b files you need to troll through good luck using anything but a vector database. Once you do a level or two of pruning of the results then you can feed it into an LLM for final classification.</div><br/></div></div></div></div><div id="41011707" class="c"><input type="checkbox" id="c-41011707" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41012046">prev</a><span>|</span><a href="#41011213">next</a><span>|</span><label class="collapse" for="c-41011707">[-]</label><label class="expand" for="c-41011707">[4 more]</label></div><br/><div class="children"><div class="content">BERT didn’t go anywhere and I have seen fine-tuned BERT backbones everywhere. They are useful for generating embeddings to be used downstream, and small enough to be handled on consumer (pre Ampere) hardware. One of the trends I have seen is scaling BERT down rather than up, since BERT already gave good performance, we want to be able to do it faster and cheaper. That gave rise to RoBERTa, ALBERT and distillBERT.<p>T5 I have worked less with but I would be curious about its head to head performance with decoder-only models these days. My guess is the downsides from before (context window limitations) are less of a factor than they used to be.</div><br/><div id="41012064" class="c"><input type="checkbox" id="c-41012064" checked=""/><div class="controls bullet"><span class="by">hdhshdhshdjd</span><span>|</span><a href="#41011707">parent</a><span>|</span><a href="#41011213">next</a><span>|</span><label class="collapse" for="c-41012064">[-]</label><label class="expand" for="c-41012064">[3 more]</label></div><br/><div class="children"><div class="content">I tried some large scale translation tasks with T5 and results were iffy at best. I’m going to try the same task with the newest Mistral small models and compare. My guess is Mistral will be better.</div><br/><div id="41012151" class="c"><input type="checkbox" id="c-41012151" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#41011707">root</a><span>|</span><a href="#41012064">parent</a><span>|</span><a href="#41011213">next</a><span>|</span><label class="collapse" for="c-41012151">[-]</label><label class="expand" for="c-41012151">[2 more]</label></div><br/><div class="children"><div class="content">T5 is not Bert, translation is not embedding.</div><br/><div id="41012294" class="c"><input type="checkbox" id="c-41012294" checked=""/><div class="controls bullet"><span class="by">hdhshdhshdjd</span><span>|</span><a href="#41011707">root</a><span>|</span><a href="#41012151">parent</a><span>|</span><a href="#41011213">next</a><span>|</span><label class="collapse" for="c-41012294">[-]</label><label class="expand" for="c-41012294">[1 more]</label></div><br/><div class="children"><div class="content">The article mentions T5 and translation is something T5 is supposedly good at - just sharing I was less than impressed.</div><br/></div></div></div></div></div></div></div></div><div id="41011213" class="c"><input type="checkbox" id="c-41011213" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#41011707">prev</a><span>|</span><a href="#41014137">next</a><span>|</span><label class="collapse" for="c-41011213">[-]</label><label class="expand" for="c-41011213">[2 more]</label></div><br/><div class="children"><div class="content">For people like me who gave up trying to follow Arxiv ML papers 3+ years ago, articles like these are gold. I would love a Youtube channel or blog which does retrospectives on &quot;big&quot; papers of the last decade (those that everyone paid attention to at the time) and look at where the ideas are today.</div><br/><div id="41014485" class="c"><input type="checkbox" id="c-41014485" checked=""/><div class="controls bullet"><span class="by">ildon</span><span>|</span><a href="#41011213">parent</a><span>|</span><a href="#41014137">next</a><span>|</span><label class="collapse" for="c-41014485">[-]</label><label class="expand" for="c-41014485">[1 more]</label></div><br/><div class="children"><div class="content">All you need is uninterrupted attention</div><br/></div></div></div></div><div id="41014137" class="c"><input type="checkbox" id="c-41014137" checked=""/><div class="controls bullet"><span class="by">empiko</span><span>|</span><a href="#41011213">prev</a><span>|</span><a href="#41010724">next</a><span>|</span><label class="collapse" for="c-41014137">[-]</label><label class="expand" for="c-41014137">[1 more]</label></div><br/><div class="children"><div class="content">BERT is still the most downloaded LM at huggingface with 46M downloads last month. XLM Roberta has 24M and Distilbert is at 15M. I feel like BERTs are doing okay.</div><br/></div></div><div id="41010724" class="c"><input type="checkbox" id="c-41010724" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#41014137">prev</a><span>|</span><a href="#41010309">next</a><span>|</span><label class="collapse" for="c-41010724">[-]</label><label class="expand" for="c-41010724">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a bit embarrassed to admit, but I still don&#x27;t understand decoder vs encoder vs decoder&#x2F;encoder models.<p>Is the input&#x2F;output of these models any different? Are they all just &quot;text context goes in, scores for all tokens in the vocabulary come out&quot; ? Is the difference only in how they achieve this output?</div><br/><div id="41011224" class="c"><input type="checkbox" id="c-41011224" checked=""/><div class="controls bullet"><span class="by">ambrozk</span><span>|</span><a href="#41010724">parent</a><span>|</span><a href="#41010899">next</a><span>|</span><label class="collapse" for="c-41011224">[-]</label><label class="expand" for="c-41011224">[2 more]</label></div><br/><div class="children"><div class="content">Encoder: Text tokens -&gt; Fixed representation vector<p>Decoder: Fixed representation vector + N decoded text tokens -&gt; N+1th text token<p>Encoder&#x2F;Decoder architecture: You take some tokenized text, run an encoder on it to get a fixed representation vector, and then recursively apply the decoder to your fixed representation vector and the 0...N tokens you&#x27;ve already produced to produce the N+1th token.<p>Decoder-only architecture: You take some tokenized text, and recursively apply a decoder to the 0...N tokens you&#x27;ve already produced to produce the N+1th token (without ever using an encoded representation vector).<p>Basically, an encoder produces this intermediate output which a decoder knows how to combine with some existing output to create more output (imagine, e.g., encoding a sentence in French, and then feeding a decoder the vector representation of that sentence plus the three words you&#x27;ve translated so far, so that it can figure out the next word in the translation). A decoder can be made to require an intermediate context vector, or (this is how it&#x27;s done in decoder-only architectures) it can be made to require only the text produced so far.</div><br/><div id="41011623" class="c"><input type="checkbox" id="c-41011623" checked=""/><div class="controls bullet"><span class="by">opprobium</span><span>|</span><a href="#41010724">root</a><span>|</span><a href="#41011224">parent</a><span>|</span><a href="#41010899">next</a><span>|</span><label class="collapse" for="c-41011623">[-]</label><label class="expand" for="c-41011623">[1 more]</label></div><br/><div class="children"><div class="content">Encoder in the T5 sense doesn&#x27;t produce a fixed vector, it produces one encoded vector for every step of input and all of that is given to the decoder.<p>The only difference between encoder&#x2F;decoder and decoder-only is masking:<p>In an encoder, none of the tokens are masked at any step, and are all visible in both directions to the encoder. Each output of the encoder can attend to any input of the encoder.<p>In the decoder, the tokens are masked causally - each N+1 token can only attend to the previous N tokens.</div><br/></div></div></div></div><div id="41010899" class="c"><input type="checkbox" id="c-41010899" checked=""/><div class="controls bullet"><span class="by">mbowcut2</span><span>|</span><a href="#41010724">parent</a><span>|</span><a href="#41011224">prev</a><span>|</span><a href="#41011339">next</a><span>|</span><label class="collapse" for="c-41010899">[-]</label><label class="expand" for="c-41010899">[1 more]</label></div><br/><div class="children"><div class="content">The biggest difference is when you feed a sequence into a decoder only model, it will only attend to previous tokens when computing hidden states for the current token. So the hidden states for the nth token is only based on tokens &lt;n. This is where you hear the talk about &quot;causal masking&quot;, as the attention matrix is masked to achieve this restriction. Encoder architectures on the other hand allow for each position in the sequence to attend to every other position in the sequence.<p>Encoder architectures have been used for semantic analysis, and feature extraction of sequences, and encoder only for generation (i.e. next token prediction).</div><br/></div></div><div id="41011339" class="c"><input type="checkbox" id="c-41011339" checked=""/><div class="controls bullet"><span class="by">chant4747</span><span>|</span><a href="#41010724">parent</a><span>|</span><a href="#41010899">prev</a><span>|</span><a href="#41011097">next</a><span>|</span><label class="collapse" for="c-41011339">[-]</label><label class="expand" for="c-41011339">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t be embarrassed. This article makes the mistake of _saying_ they&#x27;re going catch the under-informed up to speed but then immediately dives all the way in to the deep end.</div><br/></div></div><div id="41011097" class="c"><input type="checkbox" id="c-41011097" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#41010724">parent</a><span>|</span><a href="#41011339">prev</a><span>|</span><a href="#41010838">next</a><span>|</span><label class="collapse" for="c-41011097">[-]</label><label class="expand" for="c-41011097">[1 more]</label></div><br/><div class="children"><div class="content">You can think of encoder&#x2F;decoder models as specifically addressing the translation problem. They are also known as sequence-to-sequence models.<p>Take the task of translation. A translator needs to keep in mind the original text and the translation so far in order to predict the next translated token. The original text is encoded, and the translation so far is passed into the decoder to generate the next translated token. The next token is appended to the translation and the process repeats autoregressively.<p>Decoder-only models use just the decoder architecture of encoder&#x2F;decoders. They are prompted and generate completions autoregressively.<p>Encoder-only models use just the encoder architecture which you can think of similarly to embedding. A task here is, producing vectors where vector distance is related to the semantic similarity of the input documents. This can be useful for retrieval tasks among other things.<p>You can of course translate using just the decoder, by constructing a &quot;please translate this from A to B, &lt;original text&gt;&quot; prompt and generating tokens just using the decoder. I&#x27;ll leave it to people with more expertise than I do describe the pros and cons of these.</div><br/></div></div><div id="41010838" class="c"><input type="checkbox" id="c-41010838" checked=""/><div class="controls bullet"><span class="by">lalaland1125</span><span>|</span><a href="#41010724">parent</a><span>|</span><a href="#41011097">prev</a><span>|</span><a href="#41011802">next</a><span>|</span><label class="collapse" for="c-41010838">[-]</label><label class="expand" for="c-41010838">[5 more]</label></div><br/><div class="children"><div class="content">The key to understanding the difference is that transformers are attention models where tokens can &quot;attend&quot; to different tokens.<p>Encoder models allow all tokens to attend to every other token. This increases the number of connections and makes it easier for the model to reason, but requires all tokens at once to produce any output. These models generally can&#x27;t generate text.<p>Decoder models only allow tokens to attend to previous tokens in the sequence. This decreases the amount of tokens, but allows the model to be run incrementally, one token at a time. This incremental processing is key to allowing the models to generate text.</div><br/><div id="41012903" class="c"><input type="checkbox" id="c-41012903" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#41010724">root</a><span>|</span><a href="#41010838">parent</a><span>|</span><a href="#41012496">next</a><span>|</span><label class="collapse" for="c-41012903">[-]</label><label class="expand" for="c-41012903">[3 more]</label></div><br/><div class="children"><div class="content">This is wrong.<p>The term for models that look only at previous tokens in the sequence is auto-regressive.<p>Encoder and decoder has nothing to do with this.</div><br/><div id="41014120" class="c"><input type="checkbox" id="c-41014120" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#41010724">root</a><span>|</span><a href="#41012903">parent</a><span>|</span><a href="#41012496">next</a><span>|</span><label class="collapse" for="c-41014120">[-]</label><label class="expand" for="c-41014120">[2 more]</label></div><br/><div class="children"><div class="content">arent a lot of transformers built in a way where attention is only applied to previous tokens in sequence, even though its fully possible to apply it both ways?</div><br/><div id="41014371" class="c"><input type="checkbox" id="c-41014371" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#41010724">root</a><span>|</span><a href="#41014120">parent</a><span>|</span><a href="#41012496">next</a><span>|</span><label class="collapse" for="c-41014371">[-]</label><label class="expand" for="c-41014371">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the autoregressive aspect. The decoder aspect is that the last layer converts representations into output sequences (and the generation happens autoregressively, one at a time). Similarly at the last layer an encoder outputs a representation&#x2F;embedding (while being able to attend to the entire sequence).</div><br/></div></div></div></div></div></div><div id="41012496" class="c"><input type="checkbox" id="c-41012496" checked=""/><div class="controls bullet"><span class="by">bjourne</span><span>|</span><a href="#41010724">root</a><span>|</span><a href="#41010838">parent</a><span>|</span><a href="#41012903">prev</a><span>|</span><a href="#41011802">next</a><span>|</span><label class="collapse" for="c-41012496">[-]</label><label class="expand" for="c-41012496">[1 more]</label></div><br/><div class="children"><div class="content">But this has nothing to do with encoding and decoding.</div><br/></div></div></div></div><div id="41011802" class="c"><input type="checkbox" id="c-41011802" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#41010724">parent</a><span>|</span><a href="#41010838">prev</a><span>|</span><a href="#41010309">next</a><span>|</span><label class="collapse" for="c-41011802">[-]</label><label class="expand" for="c-41011802">[1 more]</label></div><br/><div class="children"><div class="content">If you look at the classical [transformer architecture picture](<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transformer_(deep_learning_architecture)#&#x2F;media&#x2F;File:The-Transformer-model-architecture.png" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transformer_(deep_learning_arc...</a>) there is an &quot;encoder&quot; tower on the left and a &quot;decoder&quot; tower on the right.<p>- Bert is encoder only.<p>- GPT is decoder only.<p>- T5 uses both the encoder and the decoder.</div><br/></div></div></div></div><div id="41010309" class="c"><input type="checkbox" id="c-41010309" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41010724">prev</a><span>|</span><a href="#41010740">next</a><span>|</span><label class="collapse" for="c-41010309">[-]</label><label class="expand" for="c-41010309">[1 more]</label></div><br/><div class="children"><div class="content">What happened is that &quot;transformers go whrrrrrr.&quot; (yes, that&#x27;s the academic term)<p>In the end, LLMs using causal language modeling or masked language modeling learn to best solve their objectives by creating an efficient global model of language patterns: CLM is actually a harder problem to solve since MLM can leak information through surrounding context, and with transformer scaling law research post-BERT&#x2F;GPT it&#x27;s not a surprise CLM won out in the long run.</div><br/></div></div><div id="41010740" class="c"><input type="checkbox" id="c-41010740" checked=""/><div class="controls bullet"><span class="by">k8si</span><span>|</span><a href="#41010309">prev</a><span>|</span><a href="#41012719">next</a><span>|</span><label class="collapse" for="c-41010740">[-]</label><label class="expand" for="c-41010740">[1 more]</label></div><br/><div class="children"><div class="content">I believe many high-quality embedding models are still based on BERT, even recent ones, so I don&#x27;t think it&#x27;s entirely fair to characterize it as &quot;deprecated&quot;.</div><br/></div></div><div id="41012719" class="c"><input type="checkbox" id="c-41012719" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#41010740">prev</a><span>|</span><a href="#41010687">next</a><span>|</span><label class="collapse" for="c-41012719">[-]</label><label class="expand" for="c-41012719">[1 more]</label></div><br/><div class="children"><div class="content">DNABERT-S came out half a year ago: seems like xBERT is still useful in genomics&#x2F;DNA? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.08777" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.08777</a></div><br/></div></div><div id="41010687" class="c"><input type="checkbox" id="c-41010687" checked=""/><div class="controls bullet"><span class="by">lalaland1125</span><span>|</span><a href="#41012719">prev</a><span>|</span><a href="#41010770">next</a><span>|</span><label class="collapse" for="c-41010687">[-]</label><label class="expand" for="c-41010687">[8 more]</label></div><br/><div class="children"><div class="content">I think the big reason why BERT and T5 have fallen out of favor is the lack of zero shot (or few shot) ability.<p>When you have hundreds or thousands of examples, BERT works great. But that is very restricting.</div><br/><div id="41011643" class="c"><input type="checkbox" id="c-41011643" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#41010687">parent</a><span>|</span><a href="#41014481">next</a><span>|</span><label class="collapse" for="c-41011643">[-]</label><label class="expand" for="c-41011643">[4 more]</label></div><br/><div class="children"><div class="content">Yes but you can use an llm to label data and then train a bert model which then costs a small fraction of time and money to run than the original llm.</div><br/><div id="41014157" class="c"><input type="checkbox" id="c-41014157" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#41010687">root</a><span>|</span><a href="#41011643">parent</a><span>|</span><a href="#41012069">next</a><span>|</span><label class="collapse" for="c-41014157">[-]</label><label class="expand" for="c-41014157">[1 more]</label></div><br/><div class="children"><div class="content">Is the encoder style arch better for representing classification tasks at a given compute budget than a causal LM?<p>Is this because the final represention in bert style models more globally focused, rather than being optimized for next token prediction?</div><br/></div></div><div id="41012069" class="c"><input type="checkbox" id="c-41012069" checked=""/><div class="controls bullet"><span class="by">hdhshdhshdjd</span><span>|</span><a href="#41010687">root</a><span>|</span><a href="#41011643">parent</a><span>|</span><a href="#41014157">prev</a><span>|</span><a href="#41014481">next</a><span>|</span><label class="collapse" for="c-41012069">[-]</label><label class="expand" for="c-41012069">[2 more]</label></div><br/><div class="children"><div class="content">Shhh, don’t tell everybody the secret. ;-)</div><br/><div id="41015119" class="c"><input type="checkbox" id="c-41015119" checked=""/><div class="controls bullet"><span class="by">Karrot_Kream</span><span>|</span><a href="#41010687">root</a><span>|</span><a href="#41012069">parent</a><span>|</span><a href="#41014481">next</a><span>|</span><label class="collapse" for="c-41015119">[-]</label><label class="expand" for="c-41015119">[1 more]</label></div><br/><div class="children"><div class="content">Lol isn&#x27;t everyone doing it? That&#x27;s how I bootstraped my BERT fine-tunes.</div><br/></div></div></div></div></div></div><div id="41014481" class="c"><input type="checkbox" id="c-41014481" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41010687">parent</a><span>|</span><a href="#41011643">prev</a><span>|</span><a href="#41012989">next</a><span>|</span><label class="collapse" for="c-41014481">[-]</label><label class="expand" for="c-41014481">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because you need to mess with embeddings or even train new heads on top of a network to use it. LLMs just use tokens-in tokens-out, they don&#x27;t classify with softmax over classes, they softmax over vocabulary tokens. LLMs are more convenient</div><br/></div></div><div id="41012989" class="c"><input type="checkbox" id="c-41012989" checked=""/><div class="controls bullet"><span class="by">deepsquirrelnet</span><span>|</span><a href="#41010687">parent</a><span>|</span><a href="#41014481">prev</a><span>|</span><a href="#41011719">next</a><span>|</span><label class="collapse" for="c-41012989">[-]</label><label class="expand" for="c-41012989">[1 more]</label></div><br/><div class="children"><div class="content">They are there, you just have to look. Tasksource, NuNER, Flan, T0. There’s not a lot, but still at least a few good zero shot models in both architectures.</div><br/></div></div><div id="41011719" class="c"><input type="checkbox" id="c-41011719" checked=""/><div class="controls bullet"><span class="by">byefruit</span><span>|</span><a href="#41010687">parent</a><span>|</span><a href="#41012989">prev</a><span>|</span><a href="#41010770">next</a><span>|</span><label class="collapse" for="c-41011719">[-]</label><label class="expand" for="c-41011719">[1 more]</label></div><br/><div class="children"><div class="content">Yes, no zero shot. Few shot is possible for some use cases with setfit: <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;setfit">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;setfit</a> and the very recent Fastfit: <a href="https:&#x2F;&#x2F;github.com&#x2F;IBM&#x2F;fastfit">https:&#x2F;&#x2F;github.com&#x2F;IBM&#x2F;fastfit</a> ( <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.12365" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.12365</a> )</div><br/></div></div></div></div><div id="41010770" class="c"><input type="checkbox" id="c-41010770" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#41010687">prev</a><span>|</span><a href="#41012244">next</a><span>|</span><label class="collapse" for="c-41010770">[-]</label><label class="expand" for="c-41010770">[5 more]</label></div><br/><div class="children"><div class="content">feels like large language models sucked all the air out of the room because it was a lot easier to scale compute and data, and after roberta, no one was willing to continue exploring.</div><br/><div id="41011183" class="c"><input type="checkbox" id="c-41011183" checked=""/><div class="controls bullet"><span class="by">nshm</span><span>|</span><a href="#41010770">parent</a><span>|</span><a href="#41011751">next</a><span>|</span><label class="collapse" for="c-41011183">[-]</label><label class="expand" for="c-41011183">[3 more]</label></div><br/><div class="children"><div class="content">No, there are mathematical reasons LLMs are better. They are trained with multiobjective loss (coding skills, translation skills, etc) so they understand the world much better than MLM. Original post discuss that but with more words and points than necessary.</div><br/><div id="41014487" class="c"><input type="checkbox" id="c-41014487" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41010770">root</a><span>|</span><a href="#41011183">parent</a><span>|</span><a href="#41011480">next</a><span>|</span><label class="collapse" for="c-41014487">[-]</label><label class="expand" for="c-41014487">[1 more]</label></div><br/><div class="children"><div class="content">GPTs also get gradients from all tokens, BERT only on 15% masked tokens. GPTs are more effective.</div><br/></div></div><div id="41011480" class="c"><input type="checkbox" id="c-41011480" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#41010770">root</a><span>|</span><a href="#41011183">parent</a><span>|</span><a href="#41014487">prev</a><span>|</span><a href="#41011751">next</a><span>|</span><label class="collapse" for="c-41011480">[-]</label><label class="expand" for="c-41011480">[1 more]</label></div><br/><div class="children"><div class="content">Call it a CLM vs MLM, not LLM vs MLM. Soon LMLM&#x27;s will exist, which will be LLMs too...</div><br/></div></div></div></div><div id="41011751" class="c"><input type="checkbox" id="c-41011751" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#41010770">parent</a><span>|</span><a href="#41011183">prev</a><span>|</span><a href="#41012244">next</a><span>|</span><label class="collapse" for="c-41011751">[-]</label><label class="expand" for="c-41011751">[1 more]</label></div><br/><div class="children"><div class="content">T5 is LLM, I think first one of them.</div><br/></div></div></div></div><div id="41012244" class="c"><input type="checkbox" id="c-41012244" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#41010770">prev</a><span>|</span><a href="#41010266">next</a><span>|</span><label class="collapse" for="c-41012244">[-]</label><label class="expand" for="c-41012244">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It is also worth to note that, generally speaking, an Encoder-Decoders of 2N parameters has the same compute cost as a decoder-only model of N parameters which gives it a different FLOP to parameter count ratio.<p>Can someone explain this to me? I&#x27;m not sure how the compute costs are the same between the 2N and N nets.</div><br/><div id="41012322" class="c"><input type="checkbox" id="c-41012322" checked=""/><div class="controls bullet"><span class="by">phillypham</span><span>|</span><a href="#41012244">parent</a><span>|</span><a href="#41010266">next</a><span>|</span><label class="collapse" for="c-41012322">[-]</label><label class="expand" for="c-41012322">[1 more]</label></div><br/><div class="children"><div class="content">You can break your sequence into two parts. One part goes through the encoder and the other goes through the decoder, so each token only goes through one transformer stack.</div><br/></div></div></div></div><div id="41010266" class="c"><input type="checkbox" id="c-41010266" checked=""/><div class="controls bullet"><span class="by">caprock</span><span>|</span><a href="#41012244">prev</a><span>|</span><a href="#41011180">next</a><span>|</span><label class="collapse" for="c-41010266">[-]</label><label class="expand" for="c-41010266">[1 more]</label></div><br/><div class="children"><div class="content">Yi is a good source in this area, and a good follow on Twitter.</div><br/></div></div><div id="41011180" class="c"><input type="checkbox" id="c-41011180" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#41010266">prev</a><span>|</span><a href="#41010279">next</a><span>|</span><label class="collapse" for="c-41011180">[-]</label><label class="expand" for="c-41011180">[1 more]</label></div><br/><div class="children"><div class="content">Wasn’t there a recent paper that demonstrated BERT models are still competitive or beat LLMs in many tasks?</div><br/></div></div><div id="41010279" class="c"><input type="checkbox" id="c-41010279" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41011180">prev</a><span>|</span><label class="collapse" for="c-41010279">[-]</label><label class="expand" for="c-41010279">[5 more]</label></div><br/><div class="children"><div class="content">&gt;If BERT worked so well, why not scale it?<p>I mean, the scaling already happened in 2019 with RoBERTa, my guess is that these models are already good enough at what they need to do (creating meaningful text embeddings), and making them extremely large wasn&#x27;t feasible for deployment.</div><br/><div id="41010470" class="c"><input type="checkbox" id="c-41010470" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#41010279">parent</a><span>|</span><label class="collapse" for="c-41010470">[-]</label><label class="expand" for="c-41010470">[4 more]</label></div><br/><div class="children"><div class="content">For text classification&#x2F;clustering&#x2F;retrieval I am pretty happy with BERT-family models.  It&#x27;s only the last few month that I&#x27;ve seen better models come out that are practical (e.g. not sell all your children to Open AI to afford them)</div><br/><div id="41010579" class="c"><input type="checkbox" id="c-41010579" checked=""/><div class="controls bullet"><span class="by">murkt</span><span>|</span><a href="#41010279">root</a><span>|</span><a href="#41010470">parent</a><span>|</span><a href="#41010711">next</a><span>|</span><label class="collapse" for="c-41010579">[-]</label><label class="expand" for="c-41010579">[2 more]</label></div><br/><div class="children"><div class="content">What would you say are the better models nowadays that are practical?</div><br/><div id="41011009" class="c"><input type="checkbox" id="c-41011009" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#41010279">root</a><span>|</span><a href="#41010579">parent</a><span>|</span><a href="#41010711">next</a><span>|</span><label class="collapse" for="c-41011009">[-]</label><label class="expand" for="c-41011009">[1 more]</label></div><br/><div class="children"><div class="content">For my recommender&#x2F;object sorter I have not been in a hurry to upgrade because I have other things to think about.  This table should give you some idea of the time-space-accuracy trade offs<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard</a><p>In a lot of cases you will see two models with a huge difference in size but a tiny difference in accuracy.  I could fit either the big or small Stella on my 4080.</div><br/></div></div></div></div><div id="41010711" class="c"><input type="checkbox" id="c-41010711" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#41010279">root</a><span>|</span><a href="#41010470">parent</a><span>|</span><a href="#41010579">prev</a><span>|</span><label class="collapse" for="c-41010711">[-]</label><label class="expand" for="c-41010711">[1 more]</label></div><br/><div class="children"><div class="content">Classification is just too damn convenient with LLMs.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>