<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708592470782" as="style"/><link rel="stylesheet" href="styles.css?v=1708592470782"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.13144">Neural Network Diffusion</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>vagabund</span> | <span>74 comments</span></div><br/><div><div id="39464758" class="c"><input type="checkbox" id="c-39464758" checked=""/><div class="controls bullet"><span class="by">justanotherjoe</span><span>|</span><a href="#39461084">next</a><span>|</span><label class="collapse" for="c-39464758">[-]</label><label class="expand" for="c-39464758">[1 more]</label></div><br/><div class="children"><div class="content">fuck.  I have an idea just like this one.  I guess it&#x27;s true that ideas are a dime a dozen.  
Diffusions bear a remarkably similarity to backpropagation to me.  I thought that it could be used in place of it for some parts of a model.</div><br/></div></div><div id="39461084" class="c"><input type="checkbox" id="c-39461084" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#39464758">prev</a><span>|</span><a href="#39462989">next</a><span>|</span><label class="collapse" for="c-39461084">[-]</label><label class="expand" for="c-39461084">[4 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t sure if this paper was parody on reading the abstract. It&#x27;s not parody. Two things stand out to me: first is the idea of distilling these networks down into a smaller latent space, and then mucking around with that. That&#x27;s interesting, and cross-sections a bunch of interesting topics like interpretability, compression, training, over- and under-.. The second is that they show the diffusion models don&#x27;t just converge on similar parameters as the ones they train against&#x2F;diffuse into, and that&#x27;s also interesting.<p>I confess I&#x27;m not sure what I&#x27;d do with this in the random grab bag of Deep Learning knowledge I have, but I think it&#x27;s pretty fascinating. I might like to see a trained latent encoder that works well on a bunch of different neural networks; maybe that thing would be a good tool for interpreting &#x2F; inspecting.</div><br/><div id="39462866" class="c"><input type="checkbox" id="c-39462866" checked=""/><div class="controls bullet"><span class="by">daxfohl</span><span>|</span><a href="#39461084">parent</a><span>|</span><a href="#39463098">next</a><span>|</span><label class="collapse" for="c-39462866">[-]</label><label class="expand" for="c-39462866">[1 more]</label></div><br/><div class="children"><div class="content">Seems like it could be useful for resizing the networks, no? Start with ChatGPT 4 then release an open version of it with much fewer parameters.<p>Or maybe some metaparameter that mucks with the sizes during training produces better results. Start large to get a baseline, then reduce size to increase coherence and learning speed, then scale up again once that is maxed out.</div><br/></div></div><div id="39463098" class="c"><input type="checkbox" id="c-39463098" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#39461084">parent</a><span>|</span><a href="#39462866">prev</a><span>|</span><a href="#39462989">next</a><span>|</span><label class="collapse" for="c-39463098">[-]</label><label class="expand" for="c-39463098">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps doing this to generate 10 similar but different versions of a model can then be fed into mixture of experts?</div><br/><div id="39464307" class="c"><input type="checkbox" id="c-39464307" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#39461084">root</a><span>|</span><a href="#39463098">parent</a><span>|</span><a href="#39462989">next</a><span>|</span><label class="collapse" for="c-39464307">[-]</label><label class="expand" for="c-39464307">[1 more]</label></div><br/><div class="children"><div class="content">Ooh that’s a good idea! Although mistral seems to have been seeded with identical copies of mistral, so maybe it doesn’t buy you much? Sounds worth trying though!</div><br/></div></div></div></div></div></div><div id="39462989" class="c"><input type="checkbox" id="c-39462989" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#39461084">prev</a><span>|</span><a href="#39464686">next</a><span>|</span><label class="collapse" for="c-39462989">[-]</label><label class="expand" for="c-39462989">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t seem all that impressive when you compare it to earlier work like &#x27;g.pt&#x27; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2209.12892" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2209.12892</a> Peebles et al 2022. They cite it in passing, but do no comparison or discussion, and to my eyes, g.pt is a lot more interesting (for example, you can prompt it for a variety of network properties like low vs high score, whereas this just generates unconditionally) and more thoroughly evaluated. The autoencoder here doesn&#x27;t seem like it adds much.</div><br/></div></div><div id="39464686" class="c"><input type="checkbox" id="c-39464686" checked=""/><div class="controls bullet"><span class="by">jarrell_mark</span><span>|</span><a href="#39462989">prev</a><span>|</span><a href="#39460272">next</a><span>|</span><label class="collapse" for="c-39464686">[-]</label><label class="expand" for="c-39464686">[1 more]</label></div><br/><div class="children"><div class="content">Can this be used to fill in the missing information on the openworm nematode 302 neurons brain simulator?</div><br/></div></div><div id="39460272" class="c"><input type="checkbox" id="c-39460272" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#39464686">prev</a><span>|</span><a href="#39458364">next</a><span>|</span><label class="collapse" for="c-39460272">[-]</label><label class="expand" for="c-39460272">[49 more]</label></div><br/><div class="children"><div class="content">Seems like we&#x27;re getting very close to recursive self-improvement [0].<p>[0] <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;recursive-self-improvement" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;recursive-self-improvement</a></div><br/><div id="39460853" class="c"><input type="checkbox" id="c-39460853" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39460272">parent</a><span>|</span><a href="#39460500">next</a><span>|</span><label class="collapse" for="c-39460853">[-]</label><label class="expand" for="c-39460853">[39 more]</label></div><br/><div class="children"><div class="content">No, this is an example of an existing technique called hypernetworks.<p>It&#x27;s not &quot;recursive self improvement&quot;, which is just a belief that magic is real and you can wish an AI into existence. In particular, this one needs too much training data, and you can&#x27;t define &quot;improvement&quot; without knowing what to improve to.</div><br/><div id="39464378" class="c"><input type="checkbox" id="c-39464378" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39460853">parent</a><span>|</span><a href="#39461404">next</a><span>|</span><label class="collapse" for="c-39464378">[-]</label><label class="expand" for="c-39464378">[1 more]</label></div><br/><div class="children"><div class="content">All current LLMs are based on the premise that magic is real and you can wish intelligence into existence; it&#x27;s called &quot;scaling laws&quot; and &quot;emergent capabilities&quot;.<p>Recursive self-improvement isn&#x27;t &quot;maybe magic is real&quot;, it&#x27;s &quot;maybe the magic we <i>already know about</i> stays magical as we cast our spells with more mana.&quot;</div><br/></div></div><div id="39461475" class="c"><input type="checkbox" id="c-39461475" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39460853">parent</a><span>|</span><a href="#39461404">prev</a><span>|</span><a href="#39460500">next</a><span>|</span><label class="collapse" for="c-39461475">[-]</label><label class="expand" for="c-39461475">[36 more]</label></div><br/><div class="children"><div class="content">&gt; which is just a belief that magic is real<p>Is there a law of thermodynamics which prevents AI from writing code which would train a better AI? Never learned that one in school.<p>And FYI here&#x27;s OpenAI plan to align superintelligence: &quot;Our goal is to build a roughly human-level automated alignment researcher. We can then use vast amounts of compute to scale our efforts, and iteratively align superintelligence.&quot;<p>I guess people working there believe in magic.<p>&gt; and you can wish an AI into existence.<p>Eh? People believe that self-improvement might happen when AI is around human-level.</div><br/><div id="39464733" class="c"><input type="checkbox" id="c-39464733" checked=""/><div class="controls bullet"><span class="by">koe123</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461475">parent</a><span>|</span><a href="#39461803">next</a><span>|</span><label class="collapse" for="c-39464733">[-]</label><label class="expand" for="c-39464733">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I guess people working there believe in magic.<p>I&#x27;ve been thinking about this recently. Personally, I&#x27;ve yet to see any compelling evidence that an LLM, let alone any AI, can operate really well &quot;out of distribution&quot;. It&#x27;s capabilities (in my experience) seem to be spanned by the data it&#x27;s trained on. Hence, this supposed property that it can &quot;train itself&quot;, generating new knowledge in the process, is yet to be proven in my mind.<p>That raises the question for me: why do OpenAI staff believe what they believe?<p>If I&#x27;m being optimistic, I suppose they may have seen unreleased tech, motivating their beliefs that seemingly AGI is on the horizon.<p>If I&#x27;m being cynical, the promise of AGI probably draws in much more investment. Thus, anyone with a stake in OpenAI has an incentive to promote this narrative of imminent AGI, regardless of how realistic it is technically.<p>This is of course just based on what I&#x27;ve seen and read, I&#x27;d love to see evidence that counter my claims.</div><br/></div></div><div id="39461803" class="c"><input type="checkbox" id="c-39461803" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461475">parent</a><span>|</span><a href="#39464733">prev</a><span>|</span><a href="#39461659">next</a><span>|</span><label class="collapse" for="c-39461803">[-]</label><label class="expand" for="c-39461803">[20 more]</label></div><br/><div class="children"><div class="content">&gt; Is there a law of thermodynamics which prevents AI from writing code which would train a better AI?<p>You need to apply Wittgenstein here.<p>This appears to be true because you haven&#x27;t defined &quot;better&quot;. If you define it, it&#x27;ll become obvious that this is either false or true, but if it is true it&#x27;ll be obvious in a way that doesn&#x27;t make it sound interesting anymore.<p>(For one thing our current &quot;AI&quot; don&#x27;t come from &quot;writing code&quot;, they just come from training bigger models on the same data. For another, making changes to code doesn&#x27;t make it exponentially better, and instead breaks it if you&#x27;re not careful.)<p>&gt; I guess people working there believe in magic.<p>Yes, OpenAI was literally founded by a computer worshipping religious cult.<p>&gt; People believe that self-improvement might happen when AI is around human-level.<p>Humans don&#x27;t have a &quot;recursive self-improvement&quot; ability.<p>Also not obvious that an AI that was both &quot;aligned&quot; and &quot;capable of recursive self-improvement&quot; would choose to do it; if you&#x27;re an AI and you&#x27;re making a new improved AI, how do you know it&#x27;s aligned? It sounds unsafe.</div><br/><div id="39463814" class="c"><input type="checkbox" id="c-39463814" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461803">parent</a><span>|</span><a href="#39462269">next</a><span>|</span><label class="collapse" for="c-39463814">[-]</label><label class="expand" for="c-39463814">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Humans don&#x27;t have a &quot;recursive self-improvement&quot; ability<p>They do.<p>Humans can learn from new information, but also by iteratively distilling existing information or continuously optimizing performance on an existing task.<p>Mathematics is a pure instance of this, in the sense that all the patterns for conjectures and proven theorems are available to any entity to explore, no connection to the world needed.<p>But any information being analyzed for underlying patterns, or task being optimized for better performance, creates a recursive learning driver.<p>Finally, any time two or more humans compete at anything, they drive each other to learn and perform better. Models can do that too.</div><br/></div></div><div id="39462269" class="c"><input type="checkbox" id="c-39462269" checked=""/><div class="controls bullet"><span class="by">alanbernstein</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461803">parent</a><span>|</span><a href="#39463814">prev</a><span>|</span><a href="#39461837">next</a><span>|</span><label class="collapse" for="c-39462269">[-]</label><label class="expand" for="c-39462269">[4 more]</label></div><br/><div class="children"><div class="content">&gt; they just come from training bigger models on the same data<p>Are you arguing that all AI models are using the same network structure?<p>This is only true in the most narrow sense, looking at models that are strictly improvements over previous generation models. It ignores the entire field of research that works by developing new models with new structures, or combining ideas from multiple previous works.</div><br/><div id="39462955" class="c"><input type="checkbox" id="c-39462955" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462269">parent</a><span>|</span><a href="#39461837">next</a><span>|</span><label class="collapse" for="c-39462955">[-]</label><label class="expand" for="c-39462955">[3 more]</label></div><br/><div class="children"><div class="content">I sure am ignoring that, because the bitter lesson of AI is usually applicable and implies that all such research will be replaced by larger generic transformer networks as time goes on.<p>The exception is when you care about efficiency (in training or inference costs) but at the limit or if you care about &quot;better&quot; then you don&#x27;t.</div><br/><div id="39464263" class="c"><input type="checkbox" id="c-39464263" checked=""/><div class="controls bullet"><span class="by">recursivecaveat</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462955">parent</a><span>|</span><a href="#39464498">next</a><span>|</span><label class="collapse" for="c-39464263">[-]</label><label class="expand" for="c-39464263">[1 more]</label></div><br/><div class="children"><div class="content">This is kindof an odd statement because the transformer is not the most generic neural net. It&#x27;s the result of many levels of improvements in architecture over older designs. The bitter lesson is methods that can scale well with compute win (alpha&#x2F;beta beats heuristics alone, neural network beats alpha&#x2F;beta), not that the most obvious and generic approach eventually wins. Given the context-length problems with transformers I think it&#x27;s fair to say they have scaling problems.</div><br/></div></div><div id="39464498" class="c"><input type="checkbox" id="c-39464498" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462955">parent</a><span>|</span><a href="#39464263">prev</a><span>|</span><a href="#39461837">next</a><span>|</span><label class="collapse" for="c-39464498">[-]</label><label class="expand" for="c-39464498">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a principle more powerful than the bitter lesson: GIGO.<p>Training to predict internet dump can only give you so much.<p>There&#x27;s a paper called something like &quot;learning from textbooks&quot; where they show that a small model trained on high-quality no-nonsense dataset can beat a much  bigger model at a task like Python coding.</div><br/></div></div></div></div></div></div><div id="39461837" class="c"><input type="checkbox" id="c-39461837" checked=""/><div class="controls bullet"><span class="by">Tenobrus</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461803">parent</a><span>|</span><a href="#39462269">prev</a><span>|</span><a href="#39464457">next</a><span>|</span><label class="collapse" for="c-39461837">[-]</label><label class="expand" for="c-39461837">[8 more]</label></div><br/><div class="children"><div class="content">it is very clear to me that humans do in fact have a recursive self-improvement ability, and i&#x27;m confused why you think otherwise</div><br/><div id="39462213" class="c"><input type="checkbox" id="c-39462213" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461837">parent</a><span>|</span><a href="#39462349">next</a><span>|</span><label class="collapse" for="c-39462213">[-]</label><label class="expand" for="c-39462213">[6 more]</label></div><br/><div class="children"><div class="content">I think people can read books (self improvement) and have children (recursive), but neither of those are both.</div><br/><div id="39462409" class="c"><input type="checkbox" id="c-39462409" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462213">parent</a><span>|</span><a href="#39462452">next</a><span>|</span><label class="collapse" for="c-39462409">[-]</label><label class="expand" for="c-39462409">[1 more]</label></div><br/><div class="children"><div class="content">Why do you think that the human population is more intelligent, knowledgeable, and achieves greater technological feats as time goes on? It&#x27;s because of recursive self-improvement, we are raised and educated into being better in a quite general sense, which includes being better at raising and educating; nearly every generation this cycle repeats and has for all of human history, at least since we acquired language. We also build machines that help us to make better machines, and then we use those better machines to make even better machines, another example of recursive self-improvement.</div><br/></div></div><div id="39462452" class="c"><input type="checkbox" id="c-39462452" checked=""/><div class="controls bullet"><span class="by">rralian</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462213">parent</a><span>|</span><a href="#39462409">prev</a><span>|</span><a href="#39462349">next</a><span>|</span><label class="collapse" for="c-39462452">[-]</label><label class="expand" for="c-39462452">[4 more]</label></div><br/><div class="children"><div class="content">New generations build onto the scientific knowledge of previous generations. It may not be fast but that sounds like recursive improvement to me. It seems reasonable for AI to accelerate this process.</div><br/><div id="39462911" class="c"><input type="checkbox" id="c-39462911" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462452">parent</a><span>|</span><a href="#39462349">next</a><span>|</span><label class="collapse" for="c-39462911">[-]</label><label class="expand" for="c-39462911">[3 more]</label></div><br/><div class="children"><div class="content">I think saying all of society is doing it is plausible, but not the same thing as a single human or AI doing it.<p>Though… still don&#x27;t think it&#x27;s true. Isn&#x27;t &quot;society is self improving&quot; what they call Whig history?</div><br/><div id="39464519" class="c"><input type="checkbox" id="c-39464519" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462911">parent</a><span>|</span><a href="#39464082">next</a><span>|</span><label class="collapse" for="c-39464519">[-]</label><label class="expand" for="c-39464519">[1 more]</label></div><br/><div class="children"><div class="content">AI might have multiple instances within a single computing environment, so it&#x27;s more like a population than a single individual.<p>I.e. &quot;You can only use the memory which you currently use&quot; would be a weird artificial constraint not relevant in practice.</div><br/></div></div></div></div></div></div></div></div><div id="39462349" class="c"><input type="checkbox" id="c-39462349" checked=""/><div class="controls bullet"><span class="by">spacecadet</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461837">parent</a><span>|</span><a href="#39462213">prev</a><span>|</span><a href="#39464457">next</a><span>|</span><label class="collapse" for="c-39462349">[-]</label><label class="expand" for="c-39462349">[1 more]</label></div><br/><div class="children"><div class="content">A very small percentage maybe. I think I agree with the notion that most people bias toward thinking they are improving while actually self-sabotaging.</div><br/></div></div></div></div><div id="39464457" class="c"><input type="checkbox" id="c-39464457" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461803">parent</a><span>|</span><a href="#39461837">prev</a><span>|</span><a href="#39464132">next</a><span>|</span><label class="collapse" for="c-39464457">[-]</label><label class="expand" for="c-39464457">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This appears to be true because you haven&#x27;t defined &quot;better&quot;.<p>Better intelligence can be defined quite easily: something which is better at (1) modeling the world; (2) optimizing (i.e. solving problems).<p>But if that would be too general we can assume that general reasoning capability would be a good proxy for that. And &quot;better at reasoning&quot; is rather easy to define. Beyond general reasoning better AI might have access to wider range of specialized modeling tools, e.g. chemical, mechanical, biological modeling, etc.<p>&gt; if it is true it&#x27;ll be obvious in a way that doesn&#x27;t make it sound interesting anymore.<p>Not sure what you mean. AI which is better at reasoning is definitely interesting, but also scary.<p>&gt; they just come from training bigger models on the same data.<p>I don&#x27;t think so. OpenAI refuses to tell us how they made GPT-4. I think a big part of it was preparing better, cleaner data sets. Google tells us that specifically improved Gemini&#x27;s reasoning using specialized reasoning datasets. More specialized AI like AlphaGeometry use synthetic datasets.<p>&gt; Yes, OpenAI was literally founded by a computer worshipping religious cult.<p>Practice is the sole criterion for testing the truth. If their beliefs led them to better practice then they are closer to truth than whatever shit you believe in. Also I see no evidence of OpenAI &quot;worshipping&quot; anything religion-like. Many people working there are just excited about possibilities.<p>&gt; Humans don&#x27;t have a &quot;recursive self-improvement&quot; ability.<p>Human recursive self-improvement is very slow because we cannot modify our brains&#x27; at will. Also spawning more humans takes time. And yet humans made huge amount of progress in the last 3000 years or so.<p>Imagine that instead of making a new adult human in 20 years you could make one in 1 minute with full control over neural structures, connections to external tools via neural links, precisely controlled knowledge &amp; skills, etc.</div><br/></div></div><div id="39464132" class="c"><input type="checkbox" id="c-39464132" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461803">parent</a><span>|</span><a href="#39464457">prev</a><span>|</span><a href="#39462559">next</a><span>|</span><label class="collapse" for="c-39464132">[-]</label><label class="expand" for="c-39464132">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If you define it, it&#x27;ll become obvious that this is either false or true<p>Ok.  So then I guess it isn&#x27;t &quot;just a belief that magic&quot;.<p>Instead, it is so true and possible that you think it is actually obvious!<p>I&#x27;m glad you got convinced in a singular post that recursive self improvement,  in the obvious way, is so true and real that it is obviously true and not magic.</div><br/></div></div><div id="39462559" class="c"><input type="checkbox" id="c-39462559" checked=""/><div class="controls bullet"><span class="by">always2slow</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461803">parent</a><span>|</span><a href="#39464132">prev</a><span>|</span><a href="#39461659">next</a><span>|</span><label class="collapse" for="c-39462559">[-]</label><label class="expand" for="c-39462559">[4 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; I guess people working there believe in magic.<p>&gt;Yes, OpenAI was literally founded by a computer worshipping religious cult.<p>What cult is this?</div><br/><div id="39462923" class="c"><input type="checkbox" id="c-39462923" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462559">parent</a><span>|</span><a href="#39461659">next</a><span>|</span><label class="collapse" for="c-39462923">[-]</label><label class="expand" for="c-39462923">[3 more]</label></div><br/><div class="children"><div class="content">HPMOR readers who live in group home polycules in Berkeley who think they need to invent a good computer god to stop the evil computer god.</div><br/><div id="39464546" class="c"><input type="checkbox" id="c-39464546" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462923">parent</a><span>|</span><a href="#39462957">next</a><span>|</span><label class="collapse" for="c-39464546">[-]</label><label class="expand" for="c-39464546">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re confusing OpenAI and MIRI.<p>OpenAI founders: Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, Wojciech Zaremba, Sam Altman. All of them come from software tech industry and academic research circles, not evidence of interest in HPMOR or Yud.</div><br/></div></div><div id="39462957" class="c"><input type="checkbox" id="c-39462957" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462923">parent</a><span>|</span><a href="#39464546">prev</a><span>|</span><a href="#39461659">next</a><span>|</span><label class="collapse" for="c-39462957">[-]</label><label class="expand" for="c-39462957">[1 more]</label></div><br/><div class="children"><div class="content">I think they cleaned out some of the EAs around the time of the board situation, but I don&#x27;t know what the non-EA overlap is with your description.</div><br/></div></div></div></div></div></div></div></div><div id="39461659" class="c"><input type="checkbox" id="c-39461659" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461475">parent</a><span>|</span><a href="#39461803">prev</a><span>|</span><a href="#39461520">next</a><span>|</span><label class="collapse" for="c-39461659">[-]</label><label class="expand" for="c-39461659">[4 more]</label></div><br/><div class="children"><div class="content">Even if recursive self improvement does work out my hunch is that is going to be logarithmic instead of exponential mostly down to just availability of data. It might go beyond human intelligence but I don&#x27;t think it will reach singularity</div><br/><div id="39462425" class="c"><input type="checkbox" id="c-39462425" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461659">parent</a><span>|</span><a href="#39461520">next</a><span>|</span><label class="collapse" for="c-39462425">[-]</label><label class="expand" for="c-39462425">[3 more]</label></div><br/><div class="children"><div class="content">This is why the big bet for AI-assisted AI-development long term is synthetic data. A big part of the reason so much money and resources is going into synthetic data right now is not just out of economic necessity, but because there have been extremely encouraging results with synthetic data (e.g. &#x27;Textbooks Are All You Need&#x27;, AlphaZero).</div><br/><div id="39464196" class="c"><input type="checkbox" id="c-39464196" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462425">parent</a><span>|</span><a href="#39461520">next</a><span>|</span><label class="collapse" for="c-39464196">[-]</label><label class="expand" for="c-39464196">[2 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t count aplha zero since it&#x27;s reinforcement learning. That technique you can generate high quality data all the time since the rules are fixed. Not everything can be trained using that way</div><br/><div id="39464659" class="c"><input type="checkbox" id="c-39464659" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39464196">parent</a><span>|</span><a href="#39461520">next</a><span>|</span><label class="collapse" for="c-39464659">[-]</label><label class="expand" for="c-39464659">[1 more]</label></div><br/><div class="children"><div class="content">The chess knowledge and skills of LLMs comes from them ingesting a sufficient number of chess games in text format (the amount will be proportional to both other data you have and the compute you have), same with the ability of LLMs to play other games or solve other fixed rule&#x2F;perfect knowledge puzzles. AlphaZero and its cousins showed that you can generate an effectively infinite quantity of extremely high-quality data in those domains. There is a possibility that the benefit to an LLM&#x27;s general intelligence from giving it e.g. one billion ~4600 ELO level games is only in improving its ability to play chess. Given the results many studies have reported in cross-learning with LLMs, I doubt that though. The potential is that generating a lot of extremely high level logic and puzzle solving and providing it as extremely high quality synthetic data to an LLM can improve its general reasoning and logic capabilities - that would be huge, and is one of the promises of synthetic data.</div><br/></div></div></div></div></div></div></div></div><div id="39461520" class="c"><input type="checkbox" id="c-39461520" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461475">parent</a><span>|</span><a href="#39461659">prev</a><span>|</span><a href="#39461598">next</a><span>|</span><label class="collapse" for="c-39461520">[-]</label><label class="expand" for="c-39461520">[9 more]</label></div><br/><div class="children"><div class="content">To be honest, I think a lot of smart people are willing to believe in magic when they&#x27;ve demonstrated some strong capability and the people funding their company want magic to happen.</div><br/><div id="39461561" class="c"><input type="checkbox" id="c-39461561" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461520">parent</a><span>|</span><a href="#39461598">next</a><span>|</span><label class="collapse" for="c-39461561">[-]</label><label class="expand" for="c-39461561">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not magic, though. If AI can do work of a human, it can do work of a human. It&#x27;s a trivial statement, and inability to see it is a hard cope.<p>Are you gonna to take a bet &quot;AI won&#x27;t be able to do X in 10 years&quot; for some X which people can learn to do now? If you&#x27;re unwilling to bet then you believe that AI would plausibly be able to perform any human job, including job of AI researcher.</div><br/><div id="39461649" class="c"><input type="checkbox" id="c-39461649" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461561">parent</a><span>|</span><a href="#39461688">next</a><span>|</span><label class="collapse" for="c-39461649">[-]</label><label class="expand" for="c-39461649">[1 more]</label></div><br/><div class="children"><div class="content">At the end of the day it can only get as far as the data it has. Let&#x27;s say you want to make a drug that inhibits a protein. The AI can generate plausible drugs but to see if it actually works you need to test it in the lab and then on an animal etc. now you can have an AI that has a perfect understanding of how a drug interacts with a protein but wesuch data is not available in the first place. Without that you can&#x27;t just simply scale gpt type models</div><br/></div></div><div id="39461688" class="c"><input type="checkbox" id="c-39461688" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461561">parent</a><span>|</span><a href="#39461649">prev</a><span>|</span><a href="#39461571">next</a><span>|</span><label class="collapse" for="c-39461688">[-]</label><label class="expand" for="c-39461688">[2 more]</label></div><br/><div class="children"><div class="content">‘Doing the work of a human’ is something that is very hard to define or quantify in many cases. You sound very confident, but you don’t address this at all; you simply assume it’s a given.<p>Relevant: <a href="https:&#x2F;&#x2F;www.jaakkoj.com&#x2F;concepts&#x2F;doorman-fallacy" rel="nofollow">https:&#x2F;&#x2F;www.jaakkoj.com&#x2F;concepts&#x2F;doorman-fallacy</a></div><br/><div id="39461762" class="c"><input type="checkbox" id="c-39461762" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461688">parent</a><span>|</span><a href="#39461571">next</a><span>|</span><label class="collapse" for="c-39461762">[-]</label><label class="expand" for="c-39461762">[1 more]</label></div><br/><div class="children"><div class="content">Yea, I think we&#x27;re in kind of a vicious recursive cycle of imperfect-metric-reinforcement (reward-hacking I suppose, though often implemented in economics as well as code) rather than one of recursive self-improvement in a more holistic sense. Optimization is really good at turning small problems of this nature into big ones more quickly</div><br/></div></div></div></div><div id="39461571" class="c"><input type="checkbox" id="c-39461571" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461561">parent</a><span>|</span><a href="#39461688">prev</a><span>|</span><a href="#39461598">next</a><span>|</span><label class="collapse" for="c-39461571">[-]</label><label class="expand" for="c-39461571">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t claim it&#x27;s impossible, just that there isn&#x27;t a clear path from what exists now to that reality, and that the explanation presented by the above commenter (and I suppose OpenAI&#x27;s website) does not clarify what they think the path is</div><br/><div id="39464625" class="c"><input type="checkbox" id="c-39464625" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461571">parent</a><span>|</span><a href="#39462290">next</a><span>|</span><label class="collapse" for="c-39464625">[-]</label><label class="expand" for="c-39464625">[1 more]</label></div><br/><div class="children"><div class="content">We can reason about it without knowing the path. E.g. somebody in 1950s could say &quot;If you have enough compute you can do photorealistic quality computer graphics&quot;. If you ask them how to build a GPU they won&#x27;t know. Their statement is about principal possibility.</div><br/></div></div><div id="39462290" class="c"><input type="checkbox" id="c-39462290" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461571">parent</a><span>|</span><a href="#39464625">prev</a><span>|</span><a href="#39461598">next</a><span>|</span><label class="collapse" for="c-39462290">[-]</label><label class="expand" for="c-39462290">[2 more]</label></div><br/><div class="children"><div class="content">What will AutoGPT look like if we have 100x more compute and another 10 years of research breakthroughs? It will be pretty damn good. If it can do the cognitive work of an AI researcher, well, there&#x27;s your recursive self-improvement, at least on the research front (not so much on the hardware&#x2F;energy front, physical constraints are trickier and will slow down progress in practice).<p>I don&#x27;t know the exact path there, because if I did I&#x27;d publish and win the Turing Award. But it seems to be a plausible outcome in the medium-term future, at least if you go with Hinton&#x27;s view that current methods are capable of understanding and reasoning, and not LeCun&#x27;s view that it&#x27;s all a dead end.</div><br/><div id="39462831" class="c"><input type="checkbox" id="c-39462831" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39462290">parent</a><span>|</span><a href="#39461598">next</a><span>|</span><label class="collapse" for="c-39462831">[-]</label><label class="expand" for="c-39462831">[1 more]</label></div><br/><div class="children"><div class="content">I won&#x27;t comment on whether I believe those researchers hold those views as you describe them, but as you describe them, I think both those descriptions of the state of AI research are untrue. The capabilities demonstrated by transformer models seem necessary but not sufficient to understand and reason, meaning that while they&#x27;re not necessarily a &quot;dead end&quot;, it is far from guaranteed that adding more compute will get them there<p>Of course if we allow for any arbitrary &quot;research breakthrough&quot; to happen then any outcome that&#x27;s physically possible could happen, and I agree with you that superhuman artificial intelligence is possible. Nonetheless it remains unclear what research breakthroughs need to happen, how difficult they will be, and whether handing a company like OpenAI lots of money and chips will get that done, and it remains even more unclear whether that is a desirable outcome, given that the priorities of that company seem to shift considerably each time their budget is increased (As is the norm in this economic environment, to be clear, that is not a unique problem of OpenAI)<p>Obviously OpenAI has every reason to claim that it can do this and to claim that it will use the results in a way designed to benefit humanity as a whole. The people writing this promotional copy and the people working there may even believe both of these things. However, based on the information available, I don&#x27;t think the first claim is credible. The second claim becomes less credible the more of the company&#x27;s original mission gets jettisoned as its priorities align more to its benefactors, which we have seen happen rather rapidly</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39461598" class="c"><input type="checkbox" id="c-39461598" checked=""/><div class="controls bullet"><span class="by">woopsn</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461475">parent</a><span>|</span><a href="#39461520">prev</a><span>|</span><a href="#39460500">next</a><span>|</span><label class="collapse" for="c-39461598">[-]</label><label class="expand" for="c-39461598">[1 more]</label></div><br/><div class="children"><div class="content">They do. Altman is saying their tech may be poised to capture the sum of all value in Earth&#x27;s future light cone.<p>Saying &quot;well that is not physically impermissible&quot; doesn&#x27;t make it real.<p>In any case nobody has ever shown that recursive self-improvement &quot;takes off&quot;, and nor is that what we should expect a priori.</div><br/></div></div></div></div></div></div><div id="39460500" class="c"><input type="checkbox" id="c-39460500" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#39460272">parent</a><span>|</span><a href="#39460853">prev</a><span>|</span><a href="#39460589">next</a><span>|</span><label class="collapse" for="c-39460500">[-]</label><label class="expand" for="c-39460500">[2 more]</label></div><br/><div class="children"><div class="content">I upvoted because this was my first thought too, but reading the abstract and skimming the paper makes me think it’s not really an advance for general recursive improvement. I think the title makes people think this is a text -&gt; model model, when it is really a bunch of model weights -&gt; new model weights optimizer for a specific architecture and problem. Still a potentially very useful idea for learning from a bunch of training runs and very interesting work!</div><br/><div id="39460597" class="c"><input type="checkbox" id="c-39460597" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39460500">parent</a><span>|</span><a href="#39460589">next</a><span>|</span><label class="collapse" for="c-39460597">[-]</label><label class="expand" for="c-39460597">[1 more]</label></div><br/><div class="children"><div class="content">I suspect this is useful for porting one vector space to another which is an open problem when you’ve trained one model with one architecture and need to port it to another architecture without paying the full retraining cost.</div><br/></div></div></div></div><div id="39460589" class="c"><input type="checkbox" id="c-39460589" checked=""/><div class="controls bullet"><span class="by">GuB-42</span><span>|</span><a href="#39460272">parent</a><span>|</span><a href="#39460500">prev</a><span>|</span><a href="#39461618">next</a><span>|</span><label class="collapse" for="c-39460589">[-]</label><label class="expand" for="c-39460589">[4 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t look that different from what we are already doing. For example AlphaGo&#x2F;AlphaZero&#x2F;MuZero learn to play board games by playing repeatedly against itself, it is a self improvement loop leading to superhuman play. It was a major breakthrough for the game of Go, and it lead to advances in the field of machine learning, but we are still far from something resembling technological singularity.<p>GANs are another example of self-improvement. It was famous for creating &quot;deep fakes&quot;. It works by pitting a fake generator and a fake detector against each other, resulting in a cycle of improvement. It didn&#x27;t get much further than that, in fact, it is all about attention and transformers now.<p>This is just a way of optimizing parameters, it will not invent new techniques. It can say &quot;put 1000 neurons there, 2000 there, etc...&quot;, but it still has to pick from what designers tell it to pick from. It may adjust these parameters better than a human can, leading to more efficient systems, I expect some improvement to existing systems, but not a breaking change.</div><br/><div id="39461400" class="c"><input type="checkbox" id="c-39461400" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39460589">parent</a><span>|</span><a href="#39461618">next</a><span>|</span><label class="collapse" for="c-39461400">[-]</label><label class="expand" for="c-39461400">[3 more]</label></div><br/><div class="children"><div class="content">Go and Chess still has rules that are hard coded which at least gives a framework to optimize in. What rules do you give an LLM?</div><br/><div id="39461843" class="c"><input type="checkbox" id="c-39461843" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461400">parent</a><span>|</span><a href="#39461835">next</a><span>|</span><label class="collapse" for="c-39461843">[-]</label><label class="expand" for="c-39461843">[1 more]</label></div><br/><div class="children"><div class="content">Some sort of &quot;generate descriptions of novel tasks including ways to evaluate performance at those tasks, evaluate quality of the generated tasks+evaluation-metrics, split tasks into subtasks, estimate difficulty of tasks in a way that is is judged on how it compares to a combined estimated difficulty of generated subtasks and to actual success rate and quality&quot; sort of deal?</div><br/></div></div><div id="39461835" class="c"><input type="checkbox" id="c-39461835" checked=""/><div class="controls bullet"><span class="by">spangry</span><span>|</span><a href="#39460272">root</a><span>|</span><a href="#39461400">parent</a><span>|</span><a href="#39461843">prev</a><span>|</span><a href="#39461618">next</a><span>|</span><label class="collapse" for="c-39461835">[-]</label><label class="expand" for="c-39461835">[1 more]</label></div><br/><div class="children"><div class="content">Physics.</div><br/></div></div></div></div></div></div><div id="39461618" class="c"><input type="checkbox" id="c-39461618" checked=""/><div class="controls bullet"><span class="by">philsnow</span><span>|</span><a href="#39460272">parent</a><span>|</span><a href="#39460589">prev</a><span>|</span><a href="#39460703">next</a><span>|</span><label class="collapse" for="c-39461618">[-]</label><label class="expand" for="c-39461618">[1 more]</label></div><br/><div class="children"><div class="content">A rare opportunity for the <i>other</i> four-letter comic to be applicable:  <a href="http:&#x2F;&#x2F;smbc-comics.com&#x2F;comic&#x2F;2011-12-13" rel="nofollow">http:&#x2F;&#x2F;smbc-comics.com&#x2F;comic&#x2F;2011-12-13</a><p>(Though I suppose this skips Neuralink &#x2F; step 3 and jumps right to step 4.)</div><br/></div></div><div id="39460703" class="c"><input type="checkbox" id="c-39460703" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39460272">parent</a><span>|</span><a href="#39461618">prev</a><span>|</span><a href="#39460508">next</a><span>|</span><label class="collapse" for="c-39460703">[-]</label><label class="expand" for="c-39460703">[1 more]</label></div><br/><div class="children"><div class="content">The ai is ready to take off to perfection land</div><br/></div></div></div></div><div id="39458364" class="c"><input type="checkbox" id="c-39458364" checked=""/><div class="controls bullet"><span class="by">vagabund</span><span>|</span><a href="#39460272">prev</a><span>|</span><a href="#39460618">next</a><span>|</span><label class="collapse" for="c-39458364">[-]</label><label class="expand" for="c-39458364">[3 more]</label></div><br/><div class="children"><div class="content">Author thread: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;liuzhuang1234&#x2F;status&#x2F;1760195922502312197" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;liuzhuang1234&#x2F;status&#x2F;1760195922502312197</a></div><br/><div id="39460343" class="c"><input type="checkbox" id="c-39460343" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#39458364">parent</a><span>|</span><a href="#39460618">next</a><span>|</span><label class="collapse" for="c-39460343">[-]</label><label class="expand" for="c-39460343">[2 more]</label></div><br/><div class="children"><div class="content">Is there any sites for viewing Twitter threads without signing up?</div><br/><div id="39460453" class="c"><input type="checkbox" id="c-39460453" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#39458364">root</a><span>|</span><a href="#39460343">parent</a><span>|</span><a href="#39460618">next</a><span>|</span><label class="collapse" for="c-39460453">[-]</label><label class="expand" for="c-39460453">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;nitter.esmailelbob.xyz&#x2F;liuzhuang1234&#x2F;status&#x2F;1760195922502312197" rel="nofollow">https:&#x2F;&#x2F;nitter.esmailelbob.xyz&#x2F;liuzhuang1234&#x2F;status&#x2F;17601959...</a><p>(bit of trial and error from <a href="https:&#x2F;&#x2F;github.com&#x2F;zedeus&#x2F;nitter&#x2F;wiki&#x2F;Instances">https:&#x2F;&#x2F;github.com&#x2F;zedeus&#x2F;nitter&#x2F;wiki&#x2F;Instances</a>)</div><br/></div></div></div></div></div></div><div id="39460618" class="c"><input type="checkbox" id="c-39460618" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#39458364">prev</a><span>|</span><a href="#39463725">next</a><span>|</span><label class="collapse" for="c-39460618">[-]</label><label class="expand" for="c-39460618">[1 more]</label></div><br/><div class="children"><div class="content">Yay, an alternative to backprop &amp; SGD! Really interesting and impressive finding, I was surprised that the network generalizes.</div><br/></div></div><div id="39463725" class="c"><input type="checkbox" id="c-39463725" checked=""/><div class="controls bullet"><span class="by">hoc</span><span>|</span><a href="#39460618">prev</a><span>|</span><a href="#39460756">next</a><span>|</span><label class="collapse" for="c-39463725">[-]</label><label class="expand" for="c-39463725">[1 more]</label></div><br/><div class="children"><div class="content">Hm, so does this actually improve&#x2F;condense the representation for certain applications or is this some more some kind of global expand and collect in network space?</div><br/></div></div><div id="39460756" class="c"><input type="checkbox" id="c-39460756" checked=""/><div class="controls bullet"><span class="by">goggy_googy</span><span>|</span><a href="#39463725">prev</a><span>|</span><a href="#39460687">next</a><span>|</span><label class="collapse" for="c-39460756">[-]</label><label class="expand" for="c-39460756">[1 more]</label></div><br/><div class="children"><div class="content">&quot;We synthesize 100 novel parameters by feeding random noise into the latent diffusion model and the trained decoder.&quot; Cool that patterns exist at this level, but also, 100 params means we have a long way to go before this process is efficient enough to synthesize more modern-sized models.</div><br/></div></div><div id="39460687" class="c"><input type="checkbox" id="c-39460687" checked=""/><div class="controls bullet"><span class="by">goggy_googy</span><span>|</span><a href="#39460756">prev</a><span>|</span><a href="#39460552">next</a><span>|</span><label class="collapse" for="c-39460687">[-]</label><label class="expand" for="c-39460687">[3 more]</label></div><br/><div class="children"><div class="content">Important to note, they say &quot;From these generated models, we select the one with the best performance on the training set.&quot; Definitely potential for bias here.</div><br/><div id="39461261" class="c"><input type="checkbox" id="c-39461261" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#39460687">parent</a><span>|</span><a href="#39460552">next</a><span>|</span><label class="collapse" for="c-39461261">[-]</label><label class="expand" for="c-39461261">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d have liked to see the distribution of generated model performance.</div><br/><div id="39461522" class="c"><input type="checkbox" id="c-39461522" checked=""/><div class="controls bullet"><span class="by">QuadmasterXLII</span><span>|</span><a href="#39460687">root</a><span>|</span><a href="#39461261">parent</a><span>|</span><a href="#39460552">next</a><span>|</span><label class="collapse" for="c-39461522">[-]</label><label class="expand" for="c-39461522">[1 more]</label></div><br/><div class="children"><div class="content">Fig 4b</div><br/></div></div></div></div></div></div><div id="39460552" class="c"><input type="checkbox" id="c-39460552" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#39460687">prev</a><span>|</span><a href="#39461965">next</a><span>|</span><label class="collapse" for="c-39460552">[-]</label><label class="expand" for="c-39460552">[1 more]</label></div><br/><div class="children"><div class="content">Why does Figure 7 not include a validation curve (afaict only the training curve is shown)?</div><br/></div></div><div id="39461965" class="c"><input type="checkbox" id="c-39461965" checked=""/><div class="controls bullet"><span class="by">t_serpico</span><span>|</span><a href="#39460552">prev</a><span>|</span><a href="#39459194">next</a><span>|</span><label class="collapse" for="c-39461965">[-]</label><label class="expand" for="c-39461965">[2 more]</label></div><br/><div class="children"><div class="content">i&#x27;d wager that adding noise to the weights in a principled fashion would accomplish something similar to this.</div><br/><div id="39462067" class="c"><input type="checkbox" id="c-39462067" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#39461965">parent</a><span>|</span><a href="#39459194">next</a><span>|</span><label class="collapse" for="c-39462067">[-]</label><label class="expand" for="c-39462067">[1 more]</label></div><br/><div class="children"><div class="content">I would really be surprised if just adding noise would give you convergence</div><br/></div></div></div></div><div id="39463966" class="c"><input type="checkbox" id="c-39463966" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#39459194">prev</a><span>|</span><a href="#39460933">next</a><span>|</span><label class="collapse" for="c-39463966">[-]</label><label class="expand" for="c-39463966">[1 more]</label></div><br/><div class="children"><div class="content">heh <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39208213#39211749">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39208213#39211749</a></div><br/></div></div><div id="39460933" class="c"><input type="checkbox" id="c-39460933" checked=""/><div class="controls bullet"><span class="by">jackblemming</span><span>|</span><a href="#39463966">prev</a><span>|</span><a href="#39462716">next</a><span>|</span><label class="collapse" for="c-39460933">[-]</label><label class="expand" for="c-39460933">[2 more]</label></div><br/><div class="children"><div class="content">The state of art neural net architecture, whether that be transformers or the like, trained on self play to optimize non-differentiable but highly efficient architectures is the way.</div><br/><div id="39461352" class="c"><input type="checkbox" id="c-39461352" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39460933">parent</a><span>|</span><a href="#39462716">next</a><span>|</span><label class="collapse" for="c-39461352">[-]</label><label class="expand" for="c-39461352">[1 more]</label></div><br/><div class="children"><div class="content">According to Hinton, before transformers were shown to work well, learning model architectures was Google&#x27;s main focus</div><br/></div></div></div></div></div></div></div></div></div></body></html>