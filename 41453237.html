<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725526862753" as="style"/><link rel="stylesheet" href="styles.css?v=1725526862753"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md">Yi-Coder: A Small but Mighty LLM for Code</a> <span class="domain">(<a href="https://01-ai.github.io">01-ai.github.io</a>)</span></div><div class="subtext"><span>crbelaus</span> | <span>19 comments</span></div><br/><div><div id="41454272" class="c"><input type="checkbox" id="c-41454272" checked=""/><div class="controls bullet"><span class="by">mythz</span><span>|</span><a href="#41454228">next</a><span>|</span><label class="collapse" for="c-41454272">[-]</label><label class="expand" for="c-41454272">[4 more]</label></div><br/><div class="children"><div class="content">Claude 3.5 Sonnet still holds the LLM crown for code which I&#x27;ll use when wanting to check the output of the best LLM, however my Continue Dev, Aider and Claude Dev plugins are currently configured to use DeepSeek Coder V2 236B (and local ollama DeepSeek Coder V2 for tab completions) as it offers the best value at $0.14M&#x2F;$0.28M which sits just below Claude 3.5 Sonnet on Aider&#x27;s leaderboard [1] whilst being 43x cheaper.<p>[1] <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;leaderboards&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;leaderboards&#x2F;</a></div><br/><div id="41454482" class="c"><input type="checkbox" id="c-41454482" checked=""/><div class="controls bullet"><span class="by">dsp_person</span><span>|</span><a href="#41454272">parent</a><span>|</span><a href="#41454228">next</a><span>|</span><label class="collapse" for="c-41454482">[-]</label><label class="expand" for="c-41454482">[3 more]</label></div><br/><div class="children"><div class="content">DeepSeek sounds really good, but the terms&#x2F;privacy policy look a bit sketch (e.g. grant full license to use&#x2F;reproduce inputs and outputs). Is there anywhere feasible to spin up the 240B model for a similarly cheap price in private?<p>The following quotes from a reddit comment here <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1dkgjqg&#x2F;comment&#x2F;l9i4ujo&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1dkgjqg&#x2F;comment...</a><p>&gt; under International Data Transfers (in the Privacy Policy):
&quot;&quot;&quot; The personal information we collect from you may be stored on a server located outside of the country where you live. We store the information we collect in secure servers located in the People&#x27;s Republic of China . &quot;&quot;&quot;<p>&gt; under How We Share Your Information &gt; Our Corporate Group (in the Privacy Policy):
&quot;&quot;&quot; The Services are supported by certain entities within our corporate group. These entities process Information You Provide, and Automatically Collected Information for us, as necessary to provide certain functions, such as storage, content delivery, security, research and development, analytics, customer and technical support, and content moderation. &quot;&quot;&quot;<p>&gt; under How We Use Your Information (in the Privacy Policy):
&quot;&quot;&quot; Carry out data analysis, research and investigations, and test the Services to ensure its stability and security; &quot;&quot;&quot;<p>&gt; under 4.Intellectual Property (in the Terms):
&quot;&quot;&quot; 4.3 By using our Services, you hereby grant us an unconditional, irrevocable, non-exclusive, royalty-free, sublicensable, transferable, perpetual and worldwide licence, to the extent permitted by local law, to reproduce, use, modify your Inputs and Outputs in connection with the provision of the Services. &quot;&quot;&quot;</div><br/><div id="41454737" class="c"><input type="checkbox" id="c-41454737" checked=""/><div class="controls bullet"><span class="by">mythz</span><span>|</span><a href="#41454272">root</a><span>|</span><a href="#41454482">parent</a><span>|</span><a href="#41454707">next</a><span>|</span><label class="collapse" for="c-41454737">[-]</label><label class="expand" for="c-41454737">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a 236B MoE model with only 21B active parameters that ollama is reporting having 258k downloads [1] (for 16&#x2F;236 combined) whilst Hugging Face says was downloaded 37k times last month [2], which can run at 25 tok&#x2F;s on a single M2 Ultra [3].<p>At $0.14M&#x2F;$0.28M it&#x27;s a no brainier to use their APIs. I understand some people would have privacy concerns and would want to avoid their APIs, although I personally spend all my time contributing to publicly available OSS code bases so I&#x27;m happy for any OSS LLM to use any of our code bases to improve their LLM and hopefully also improving the generated code for anyone using our libraries.<p>Since many LLM orgs are looking to build proprietary moats around their LLMs to maintain their artificially high prices, I&#x27;ll personally make an effort to use the best OSS LLMs available first (i.e. from DeepSeek, Meta, Qwen or Mistral AI) since they&#x27;re bringing down the cost of LLMs and aiming to render the technology a commodity.<p>[1] <a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;deepseek-coder-v2">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;deepseek-coder-v2</a><p>[2] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;deepseek-ai&#x2F;DeepSeek-Coder-V2-Lite-Instruct" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;deepseek-ai&#x2F;DeepSeek-Coder-V2-Lite-In...</a><p>[3] <a href="https:&#x2F;&#x2F;x.com&#x2F;awnihannun&#x2F;status&#x2F;1814045712512090281" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;awnihannun&#x2F;status&#x2F;1814045712512090281</a></div><br/></div></div><div id="41454707" class="c"><input type="checkbox" id="c-41454707" checked=""/><div class="controls bullet"><span class="by">yumraj</span><span>|</span><a href="#41454272">root</a><span>|</span><a href="#41454482">parent</a><span>|</span><a href="#41454737">prev</a><span>|</span><a href="#41454228">next</a><span>|</span><label class="collapse" for="c-41454707">[-]</label><label class="expand" for="c-41454707">[1 more]</label></div><br/><div class="children"><div class="content">There’s no company info on DeepSeek’s website. Looking at the above, and considering that, it seems very sketchy indeed.<p>Maybe OK for trying out stuff, a big no no for real work.</div><br/></div></div></div></div></div></div><div id="41454228" class="c"><input type="checkbox" id="c-41454228" checked=""/><div class="controls bullet"><span class="by">theshrike79</span><span>|</span><a href="#41454272">prev</a><span>|</span><a href="#41453822">next</a><span>|</span><label class="collapse" for="c-41454228">[-]</label><label class="expand" for="c-41454228">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Continue pretrained on 2.4 Trillion high-quality tokens over 52 major programming languages.<p>I&#x27;m still waiting for a model that&#x27;s highly specialised for a single language only - and either a lot smaller than these jack of all trades ones or VERY good at that specific language&#x27;s nuances + libraries.</div><br/><div id="41454307" class="c"><input type="checkbox" id="c-41454307" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#41454228">parent</a><span>|</span><a href="#41454443">next</a><span>|</span><label class="collapse" for="c-41454307">[-]</label><label class="expand" for="c-41454307">[1 more]</label></div><br/><div class="children"><div class="content">An unfortunate fact is, similar to human with infinite time, LLMs usually have better performance on your specific langauge when they are not limited to learn or over-sample one single language. Not unlike the common saying &quot;learning to code in Haskell makes you a better C++ programmer&quot;.<p>Of course, this is far from trivial, you don&#x27;t just add more data and expect it to automatically be better for everything. So is time management for us mere mortals.</div><br/></div></div><div id="41454443" class="c"><input type="checkbox" id="c-41454443" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41454228">parent</a><span>|</span><a href="#41454307">prev</a><span>|</span><a href="#41454298">next</a><span>|</span><label class="collapse" for="c-41454443">[-]</label><label class="expand" for="c-41454443">[1 more]</label></div><br/><div class="children"><div class="content">Unclear how much of their coding knowledge is in the space of syntax&#x2F;semantics of a given language and how much in the latent space that generalizes across languages and logic in general. If I were to guess I&#x27;d say 80% is in the latter for the larger capable models. Even very small models (like in Karpathy&#x27;s famous RNN blog) will get syntax right but that is superficial knowledge.</div><br/></div></div><div id="41454298" class="c"><input type="checkbox" id="c-41454298" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#41454228">parent</a><span>|</span><a href="#41454443">prev</a><span>|</span><a href="#41454294">next</a><span>|</span><label class="collapse" for="c-41454298">[-]</label><label class="expand" for="c-41454298">[1 more]</label></div><br/><div class="children"><div class="content">I’d be interested to know if that trade off ends up better. There’s probably a lot of useful training that transfers well between languages, so I wouldn’t be that surprised if the extra tokens helped across all languages. I would guess a top quality single language model would need to be very well supported, eg Python or JavaScript. Not, say, Clojure.</div><br/></div></div><div id="41454294" class="c"><input type="checkbox" id="c-41454294" checked=""/><div class="controls bullet"><span class="by">wiz21c</span><span>|</span><a href="#41454228">parent</a><span>|</span><a href="#41454298">prev</a><span>|</span><a href="#41454480">next</a><span>|</span><label class="collapse" for="c-41454294">[-]</label><label class="expand" for="c-41454294">[1 more]</label></div><br/><div class="children"><div class="content">If the LLM training makes the LLM generalize things <i>between</i> languages, then it is better to leave it like it is...</div><br/></div></div><div id="41454480" class="c"><input type="checkbox" id="c-41454480" checked=""/><div class="controls bullet"><span class="by">kamphey</span><span>|</span><a href="#41454228">parent</a><span>|</span><a href="#41454294">prev</a><span>|</span><a href="#41453822">next</a><span>|</span><label class="collapse" for="c-41454480">[-]</label><label class="expand" for="c-41454480">[2 more]</label></div><br/><div class="children"><div class="content">I wonder what those 52 languages are.</div><br/><div id="41454524" class="c"><input type="checkbox" id="c-41454524" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#41454228">root</a><span>|</span><a href="#41454480">parent</a><span>|</span><a href="#41453822">next</a><span>|</span><label class="collapse" for="c-41454524">[-]</label><label class="expand" for="c-41454524">[1 more]</label></div><br/><div class="children"><div class="content">According to the repo README:
  &#x27;java&#x27;, &#x27;markdown&#x27;, &#x27;python&#x27;, &#x27;php&#x27;, &#x27;javascript&#x27;, &#x27;c++&#x27;, &#x27;c#&#x27;, &#x27;c&#x27;, &#x27;typescript&#x27;, &#x27;html&#x27;, &#x27;go&#x27;, &#x27;java_server_pages&#x27;, &#x27;dart&#x27;, &#x27;objective-c&#x27;, &#x27;kotlin&#x27;, &#x27;tex&#x27;, &#x27;swift&#x27;, &#x27;ruby&#x27;, &#x27;sql&#x27;, &#x27;rust&#x27;, &#x27;css&#x27;, &#x27;yaml&#x27;, &#x27;matlab&#x27;, &#x27;lua&#x27;, &#x27;json&#x27;, &#x27;shell&#x27;, &#x27;visual_basic&#x27;, &#x27;scala&#x27;, &#x27;rmarkdown&#x27;, &#x27;pascal&#x27;, &#x27;fortran&#x27;, &#x27;haskell&#x27;, &#x27;assembly&#x27;, &#x27;perl&#x27;, &#x27;julia&#x27;, &#x27;cmake&#x27;, &#x27;groovy&#x27;, &#x27;ocaml&#x27;, &#x27;powershell&#x27;, &#x27;elixir&#x27;, &#x27;clojure&#x27;, &#x27;makefile&#x27;, &#x27;coffeescript&#x27;, &#x27;erlang&#x27;, &#x27;lisp&#x27;, &#x27;toml&#x27;, &#x27;batchfile&#x27;, &#x27;cobol&#x27;, &#x27;dockerfile&#x27;, &#x27;r&#x27;, &#x27;prolog&#x27;, &#x27;verilog&#x27;<p><a href="https:&#x2F;&#x2F;github.com&#x2F;01-ai&#x2F;Yi-Coder">https:&#x2F;&#x2F;github.com&#x2F;01-ai&#x2F;Yi-Coder</a></div><br/></div></div></div></div></div></div><div id="41453822" class="c"><input type="checkbox" id="c-41453822" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#41454228">prev</a><span>|</span><a href="#41454470">next</a><span>|</span><label class="collapse" for="c-41453822">[-]</label><label class="expand" for="c-41453822">[3 more]</label></div><br/><div class="children"><div class="content">Weird they&#x27;re comparing it to really old deepseek v1 models, even v2 has been out a long time now.</div><br/><div id="41454723" class="c"><input type="checkbox" id="c-41454723" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#41453822">parent</a><span>|</span><a href="#41454279">next</a><span>|</span><label class="collapse" for="c-41454723">[-]</label><label class="expand" for="c-41454723">[1 more]</label></div><br/><div class="children"><div class="content">My barely-informed guess is that they don&#x27;t have the resources to run it (it&#x27;s a 200b+ model).</div><br/></div></div></div></div><div id="41454470" class="c"><input type="checkbox" id="c-41454470" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#41453822">prev</a><span>|</span><a href="#41454337">next</a><span>|</span><label class="collapse" for="c-41454470">[-]</label><label class="expand" for="c-41454470">[2 more]</label></div><br/><div class="children"><div class="content">Beats deepseek 33. That’s impressive</div><br/><div id="41454588" class="c"><input type="checkbox" id="c-41454588" checked=""/><div class="controls bullet"><span class="by">tuukkah</span><span>|</span><a href="#41454470">parent</a><span>|</span><a href="#41454337">next</a><span>|</span><label class="collapse" for="c-41454588">[-]</label><label class="expand" for="c-41454588">[1 more]</label></div><br/><div class="children"><div class="content">They used DeepSeek-Coder-33B-Instruct in comparisons, while DeepSeek-Coder-v2-Instruct (236B) and -Lite-Instruct (16B) are available since a while: <a href="https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;DeepSeek-Coder-v2">https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;DeepSeek-Coder-v2</a><p>EDIT: Granted, Yi-Coder 9B is still smaller than any of these.</div><br/></div></div></div></div><div id="41454337" class="c"><input type="checkbox" id="c-41454337" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#41454470">prev</a><span>|</span><label class="collapse" for="c-41454337">[-]</label><label class="expand" for="c-41454337">[2 more]</label></div><br/><div class="children"><div class="content">Are coding LLMs trained with the help of interpreters?</div><br/><div id="41454732" class="c"><input type="checkbox" id="c-41454732" checked=""/><div class="controls bullet"><span class="by">willvarfar</span><span>|</span><a href="#41454337">parent</a><span>|</span><label class="collapse" for="c-41454732">[-]</label><label class="expand" for="c-41454732">[1 more]</label></div><br/><div class="children"><div class="content">Google&#x27;s Gemini does.<p>I can&#x27;t find a post that I remember Google published just after all the ChatGPT SQL generation hype happened, but it felt like they were trying to counter that hype by explaining that most complex LLM-generated code snippets won&#x27;t actually run or work, and that they were putting a code-evaluation step after the LLM for Bard.<p>(A bit like why did they never put an old fashioned rules-based grammar checker check stage in google translate results?)<p>Fast forward to today and it seems it&#x27;s a normal step for Gemini etc <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;code-execution?lang=python" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;code-execution?lang=py...</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>