<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707123658250" as="style"/><link rel="stylesheet" href="styles.css?v=1707123658250"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://shyam.blog/posts/beyond-self-attention/">Beyond self-attention: How a small language model predicts the next token</a> <span class="domain">(<a href="https://shyam.blog">shyam.blog</a>)</span></div><div class="subtext"><span>tplrbv</span> | <span>66 comments</span></div><br/><div><div id="39256432" class="c"><input type="checkbox" id="c-39256432" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#39253846">next</a><span>|</span><label class="collapse" for="c-39256432">[-]</label><label class="expand" for="c-39256432">[8 more]</label></div><br/><div class="children"><div class="content">Some of the topics in the parent post should not be a major surprise to anyone who has read <a href="https:&#x2F;&#x2F;people.math.harvard.edu&#x2F;~ctm&#x2F;home&#x2F;text&#x2F;others&#x2F;shannon&#x2F;entropy&#x2F;entropy.pdf" rel="nofollow">https:&#x2F;&#x2F;people.math.harvard.edu&#x2F;~ctm&#x2F;home&#x2F;text&#x2F;others&#x2F;shanno...</a> ! If we do not have read the foundations of the field that we are in, we are doomed to be mystified by unexplained phenomena which arise pretty naturally as consequences of already-distilled work!<p>That said, the experiments seem very thorough, on a first pass&#x2F;initial cursory examination, I appreciate the amount of detail that seemed to go into them.<p>The tradeoff between learning existing theory, and attempting to re-derive it from scratch, I think, is a hard tradeoff, as not having the traditional foundation allows for the discovery of new things, but having it allows for a deeper understanding of certain phenomena. There is a tradeoff either way.<p>I&#x27;ve seen several people here in the comments seemingly shocked that a model that maximizes the log likelihood of a sequence given the data somehow does not magically deviate from that behavior when run in inference. It&#x27;s a density estimation model, do you want it to magically recite Shakespeare from the void?<p>Please! Let&#x27;s stick to the basics, it will help experiments like this make much more sense as there already is a very clear mathematical foundation which clearly explains it (and said emergent phenomena).<p>If you want more specifics, there are several layers, Shannon&#x27;s treatment of ergodic systems is a good start (though there is some minor deviation from that here, but it likely is a &#x27;close enough&#x27; match to what&#x27;s happening here to be properly instructive to the reader about the general dynamics of what is going on, overall.)</div><br/><div id="39258577" class="c"><input type="checkbox" id="c-39258577" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#39256432">parent</a><span>|</span><a href="#39256760">next</a><span>|</span><label class="collapse" for="c-39258577">[-]</label><label class="expand" for="c-39258577">[1 more]</label></div><br/><div class="children"><div class="content">In another adjacent thread, people are talking about the implications of a neural network conforming to the training data with some error margin with regards to copyright.<p>Many textbooks on information theory already call out the content-addressable nature of such networks[1], and they’re even used in applications like compression due to this purpose[2][3], and therefore it’s no surprise that the NYT prompting OpenAI models with a few paragraphs of their articles reproduced them nearly verbatim.<p>[1] <a href="https:&#x2F;&#x2F;www.inference.org.uk&#x2F;itprnn&#x2F;book.pdf" rel="nofollow">https:&#x2F;&#x2F;www.inference.org.uk&#x2F;itprnn&#x2F;book.pdf</a><p>[2] <a href="https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;</a><p>[3] <a href="https:&#x2F;&#x2F;pub.towardsai.net&#x2F;stable-diffusion-based-image-compresssion-6f1f0a399202" rel="nofollow">https:&#x2F;&#x2F;pub.towardsai.net&#x2F;stable-diffusion-based-image-compr...</a></div><br/></div></div><div id="39256760" class="c"><input type="checkbox" id="c-39256760" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#39256432">parent</a><span>|</span><a href="#39258577">prev</a><span>|</span><a href="#39256519">next</a><span>|</span><label class="collapse" for="c-39256760">[-]</label><label class="expand" for="c-39256760">[1 more]</label></div><br/><div class="children"><div class="content">I appreciate what you&#x27;re saying, but convergence (via alternative paths, of various depths) is its own signal. Repeated rediscovery perhaps isn&#x27;t necessarily wastefulness, but affirmation and validation of deep truth for which there are multiple paths of arrival :)</div><br/></div></div><div id="39256519" class="c"><input type="checkbox" id="c-39256519" checked=""/><div class="controls bullet"><span class="by">jackblemming</span><span>|</span><a href="#39256432">parent</a><span>|</span><a href="#39256760">prev</a><span>|</span><a href="#39257525">next</a><span>|</span><label class="collapse" for="c-39256519">[-]</label><label class="expand" for="c-39256519">[2 more]</label></div><br/><div class="children"><div class="content">&gt; the topics in the parent post should not be a major surprise to anyone who has read <a href="https:&#x2F;&#x2F;people.math.harvard.edu&#x2F;~ctm&#x2F;home&#x2F;text&#x2F;others&#x2F;shanno" rel="nofollow">https:&#x2F;&#x2F;people.math.harvard.edu&#x2F;~ctm&#x2F;home&#x2F;text&#x2F;others&#x2F;shanno</a>... !<p>&gt; which clearly explains it (and said emergent phenomena)<p>Very smart information theory people have looked at neural networks through the lens of information theory and published famous papers about it years ago. It couldn&#x27;t explain many things about neural networks, but it was interesting nonetheless.<p>FWIW it&#x27;s not uncommon for smart people to say &quot;this mathematical structure looks like this other idea with [+&#x2F;- some structure]!!&quot; and that it totally explains everything... (kind of with so and so exceptions, well and also this and that and..). Truthfully, we just don&#x27;t know. And I&#x27;ve never seen theorists in this field actually take the theory and produce something novel or make useful predictions with it. It&#x27;s all try stuff and see what works, and then retroactively make up some crud on why it worked, if it did work (otherwise brush it under the rug).<p>There was this one posted recently on transformers being kernel smoothers:
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1908.11775" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1908.11775</a></div><br/><div id="39256648" class="c"><input type="checkbox" id="c-39256648" checked=""/><div class="controls bullet"><span class="by">randomNumber7</span><span>|</span><a href="#39256432">root</a><span>|</span><a href="#39256519">parent</a><span>|</span><a href="#39257525">next</a><span>|</span><label class="collapse" for="c-39256648">[-]</label><label class="expand" for="c-39256648">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s all try stuff and see what works, and then retroactively make up some crud on why it worked<p>People have done this in earlier days too. The theory around control systems was developed after PID controllers had been succesfully used in praxis.</div><br/></div></div></div></div><div id="39257525" class="c"><input type="checkbox" id="c-39257525" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#39256432">parent</a><span>|</span><a href="#39256519">prev</a><span>|</span><a href="#39256500">next</a><span>|</span><label class="collapse" for="c-39257525">[-]</label><label class="expand" for="c-39257525">[2 more]</label></div><br/><div class="children"><div class="content">Ok but why didn’t Shannon get us gpt</div><br/><div id="39258009" class="c"><input type="checkbox" id="c-39258009" checked=""/><div class="controls bullet"><span class="by">david_draco</span><span>|</span><a href="#39256432">root</a><span>|</span><a href="#39257525">parent</a><span>|</span><a href="#39256500">next</a><span>|</span><label class="collapse" for="c-39258009">[-]</label><label class="expand" for="c-39258009">[1 more]</label></div><br/><div class="children"><div class="content">He was busy getting us towards wifi first.</div><br/></div></div></div></div><div id="39256500" class="c"><input type="checkbox" id="c-39256500" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#39256432">parent</a><span>|</span><a href="#39257525">prev</a><span>|</span><a href="#39253846">next</a><span>|</span><label class="collapse" for="c-39256500">[-]</label><label class="expand" for="c-39256500">[1 more]</label></div><br/><div class="children"><div class="content">Kudos for pluggimg shannomçs masterpiece</div><br/></div></div></div></div><div id="39253846" class="c"><input type="checkbox" id="c-39253846" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#39256432">prev</a><span>|</span><a href="#39255312">next</a><span>|</span><label class="collapse" for="c-39253846">[-]</label><label class="expand" for="c-39253846">[11 more]</label></div><br/><div class="children"><div class="content">I had the exact same idea after seeing Google point out that you can[0] get ChatGPT to regurgitate verbatim training data by asking it to repeat the same word over and over again[1]. I&#x27;m glad to see someone else actually bring it to fruition.<p>This, of course, brings two additional questions:<p>1. Is this &quot;AI, hold the AI&quot; approach more energy-efficient than having gradient descent backpropagation compress a bunch of training data into a model that can then be run on specialized AI coprocessors?<p>2. Will this result wind up being evidence in the ongoing lawsuits against OpenAI and Stability AI?<p>[0] Could. OpenAI now blocks generation if you fill the context window with a single word.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.17035" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.17035</a></div><br/><div id="39254966" class="c"><input type="checkbox" id="c-39254966" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#39253846">parent</a><span>|</span><a href="#39258149">next</a><span>|</span><label class="collapse" for="c-39254966">[-]</label><label class="expand" for="c-39254966">[1 more]</label></div><br/><div class="children"><div class="content">This approach cannot possibly be more efficient than running the original model because it relies on running the original model to get the activations to search the text corpus for strings with similar activations to compute the next-token statistics. You don&#x27;t get to skip many steps, and you end up having to do a bunch of extra work.<p>I&#x27;d be surprised if doing this with two completely separate corpora, one for training the model and the other to search for strings with similar activations, wouldn&#x27;t lead to much the same results. Because the hard part is constructing similar activations for strings with similar next-token statistics in the first place.<p>Note that in the per-layer weights [0.01, 0.01, 0.1, 1.5, 6, 0.01] the penultimate layer ist the most important, where the input has already been transformed a lot. So you can&#x27;t expect to use this to replace a transformer with a simple grep over the training data. (My guess as to why the penultimate layer has a much higher weight than the final one is that this is due to induction heads <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2021&#x2F;framework&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2021&#x2F;framework&#x2F;index.html</a> which implement copying repeated strings <i>from the input</i>, with the penultimate layer determining what to look for and the final layer doing the copying.)</div><br/></div></div><div id="39258149" class="c"><input type="checkbox" id="c-39258149" checked=""/><div class="controls bullet"><span class="by">bruce343434</span><span>|</span><a href="#39253846">parent</a><span>|</span><a href="#39254966">prev</a><span>|</span><a href="#39255894">next</a><span>|</span><label class="collapse" for="c-39258149">[-]</label><label class="expand" for="c-39258149">[1 more]</label></div><br/><div class="children"><div class="content">In my experience before they blocked it: it hallucinates something that looks like training data. A GitHub readme that under closer inspection doesn&#x27;t actually exist and is incoherent. Some informational brochure about nothing. A random dialogue.</div><br/></div></div><div id="39255894" class="c"><input type="checkbox" id="c-39255894" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#39253846">parent</a><span>|</span><a href="#39258149">prev</a><span>|</span><a href="#39255509">next</a><span>|</span><label class="collapse" for="c-39255894">[-]</label><label class="expand" for="c-39255894">[3 more]</label></div><br/><div class="children"><div class="content">re: 2... if you copyright a work, then surely you also hold rights to a zip file of that work. So why not also the probability distribution of letters in that work?</div><br/><div id="39256682" class="c"><input type="checkbox" id="c-39256682" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#39253846">root</a><span>|</span><a href="#39255894">parent</a><span>|</span><a href="#39256063">next</a><span>|</span><label class="collapse" for="c-39256682">[-]</label><label class="expand" for="c-39256682">[1 more]</label></div><br/><div class="children"><div class="content">To be precise, you don’t hold rights to a zip file, copyright doesn’t know anything about files. You hold rights to a work, an abstract legal concept. Your rights to the work allow you to control the reproduction of that work, and distributing a zip file is an instance of reproducing the work.<p>Probability distributions don’t contain enough information to reproduce a work (since they don’t preserve order). They are not copyrightable in and of themselves, and distributing a probability distribution of a work doesn’t amount to reproduction.</div><br/></div></div><div id="39256063" class="c"><input type="checkbox" id="c-39256063" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#39253846">root</a><span>|</span><a href="#39255894">parent</a><span>|</span><a href="#39256682">prev</a><span>|</span><a href="#39255509">next</a><span>|</span><label class="collapse" for="c-39256063">[-]</label><label class="expand" for="c-39256063">[1 more]</label></div><br/><div class="children"><div class="content">If the probability distribution is enough to reproduce a copyrighted work to the level of substantial similarity, then yes, a copy has legally been made.<p>However, that&#x27;s not the only question involved in a copyright lawsuit[0].<p>So far most of the evidence of copying has been circumstantial: a regurgitated Quake lighting function here, a Getty Images watermark there, but everything else has looked like wholly original output. We know from how these models are trained that copyrighted work is involved somewhere, but a court could just say it&#x27;s Fair Use to scrape data and train a model on it. <i>However</i>, that defense is way harder to make if we can actually open up a model and show &quot;ok, this is where and how it&#x27;s storing copied training set data&quot;. At a minimum, it takes the &quot;how much was used&quot; Fair Use factor from &quot;a few watermarks&quot; to &quot;your honor, the entire fucking Internet&quot;.<p>[0] As usual we will assume jurisdiction in US court</div><br/></div></div></div></div><div id="39255509" class="c"><input type="checkbox" id="c-39255509" checked=""/><div class="controls bullet"><span class="by">sureglymop</span><span>|</span><a href="#39253846">parent</a><span>|</span><a href="#39255894">prev</a><span>|</span><a href="#39254758">next</a><span>|</span><label class="collapse" for="c-39255509">[-]</label><label class="expand" for="c-39255509">[2 more]</label></div><br/><div class="children"><div class="content">I found it interesting that in the arxiv paper you linked they are talking about an <i>attack</i>, ethics and responsible disclosure.<p>But when it comes to scraping the entirety of the internet to train such models that&#x27;s never referred to as an <i>attack</i>.</div><br/><div id="39255811" class="c"><input type="checkbox" id="c-39255811" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#39253846">root</a><span>|</span><a href="#39255509">parent</a><span>|</span><a href="#39254758">next</a><span>|</span><label class="collapse" for="c-39255811">[-]</label><label class="expand" for="c-39255811">[1 more]</label></div><br/><div class="children"><div class="content">Scraping the whole web isn&#x27;t considered an attack because, well, that&#x27;s just how search engines work. That being said, there are all sorts of norms (e.g. robots.txt) qualifying what kinds of scraping are accepted.<p>As far as I can tell, AI researchers assumed they could just piggyback on top of those norms to get access to large amounts of training data. The problem is that it&#x27;s difficult to call <i>copying</i> an attack unless you go full MAFIAA[0]brain and argue that monopoly rents on creative works are the only functional backstop to the 1st Amendment. Hell, even if you do, the EU and Japan[1] both have a statutory copyright exception explicitly legalizing AI training on other people&#x27;s text. It&#x27;s not even accepted dogma <i>among copyright holders</i> that this is an attack.<p>[0] Music And Film Industry Association of America, a fictional industry association purported to be the merger of the MPAA and RIAA announced on April 1st, 2006: <a href="http:&#x2F;&#x2F;mafiaa.org&#x2F;" rel="nofollow">http:&#x2F;&#x2F;mafiaa.org&#x2F;</a><p>[1] Yes, the same country whose copyright laws infamously <i>have no Fair Use equivalent</i>. In Japan, it is illegal to review or parody a copyrighted work without a license, but it is legal to train an AI on it.</div><br/></div></div></div></div><div id="39254758" class="c"><input type="checkbox" id="c-39254758" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39253846">parent</a><span>|</span><a href="#39255509">prev</a><span>|</span><a href="#39255312">next</a><span>|</span><label class="collapse" for="c-39254758">[-]</label><label class="expand" for="c-39254758">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m confused, you had the exact same idea that LLM output is based on probability of next token, which is based on the training data?<p>If that&#x27;s the case, no, its unlikely this result will end up <i>becoming</i> evidence, that is well known and fundamental.<p>The author&#x27;s contribution to discussion is showing this to a technical audience writing their own GPT, as they note, most &quot;how to implement this?&quot; focus on transformers</div><br/><div id="39255971" class="c"><input type="checkbox" id="c-39255971" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#39253846">root</a><span>|</span><a href="#39254758">parent</a><span>|</span><a href="#39255312">next</a><span>|</span><label class="collapse" for="c-39255971">[-]</label><label class="expand" for="c-39255971">[2 more]</label></div><br/><div class="children"><div class="content">Much of the sales hype and other literature surrounding LLMs specifically obfuscates the role that training data plays in the model. Training data is &quot;learned from&quot;, but that&#x27;s implying the data goes away after the training process ends and you have a model that&#x27;s solely composed of uncopyrightable knowledge about how to write or draw. If the models are actually retaining training data, and we have a way to extract that data, then the models didn&#x27;t learn - legally speaking[0], they copied training set data.<p>The idea I had wasn&#x27;t &quot;LLMs are based on probabilities&quot;, it was &quot;what if you benchmarked an LLM against a traditional search index over the training corpus&quot;. The linked blog post doesn&#x27;t completely rip out the LLMs entirely, just the feed-forward layer, but the result is what I thought would happen: an attention-augmented search index that is producing nearly <i>identical</i> probability distributions to the <i>66%</i> of the model that was removed.<p>[0] Programmers talking about copyright usually get tripped up on this, so I&#x27;ll spell it out: copyright is a matter of data provenance, not bit-exactness. Just because the weights are harder to inspect does not mean no copyright infringement has occurred. Compression does not launder copyright.</div><br/><div id="39256137" class="c"><input type="checkbox" id="c-39256137" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39253846">root</a><span>|</span><a href="#39255971">parent</a><span>|</span><a href="#39255312">next</a><span>|</span><label class="collapse" for="c-39256137">[-]</label><label class="expand" for="c-39256137">[1 more]</label></div><br/><div class="children"><div class="content">Should make sure to establish this up front: People know this, it&#x27;s not controversial. It&#x27;s not only known to a few. It&#x27;s how it works.<p>Also note that this example purposefully minimizes training data down to an absurdity, so it is possible to correlate 1:1 that the next letter&#x27;s probabilities to the input. The key of the rest of this comment,, and the discussions you reference, is the observation that&#x27;s vastly harder once the training data is measured in terabytes, to the point the question becomes interesting.<p>The argument of which you&#x27;re speaking, the people you think are speaking literally are speaking figuratively: they know it reproduces _some_ training data, i.e. 2+2=4 was surely in the training data. Or c.f. NY Times v. OpenAI, where they were able to get it to complete an article given the first ~5 paragraphs of the article.<p>The unsettled question, in US legal parlance, is if LLMs are sufficiently transformative of the training data that it becomes fair use.<p>Eschewing US legal parlance: where, exactly, on the spectrum of &quot;completely original&quot; to &quot;photocopier with perfect recall&quot; LLMs fall, given we know it isn&#x27;t at either of those extremes? What responsibility does that give someone operating an LLM commercially to the entities who originated the training data?</div><br/></div></div></div></div></div></div></div></div><div id="39255312" class="c"><input type="checkbox" id="c-39255312" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#39253846">prev</a><span>|</span><a href="#39255881">next</a><span>|</span><label class="collapse" for="c-39255312">[-]</label><label class="expand" for="c-39255312">[2 more]</label></div><br/><div class="children"><div class="content">&gt;I trained a small (~10 million parameter) transformer following Andrej Karpathy’s excellent tutorial, Let’s build GPT: from scratch, in code, spelled out<p>As soon as I learned about Andrej Karpathy&#x27;s NanoGPT, I trained it on War and Peace (in Russian), and what I found interesting is that it almost grokked Russian grammar despite being just a 3 MB model. Russian language has a complex synthetic-inflectional structure. For example, preposition &quot;na&quot; (&quot;upon&quot;) requires the following noun to be in accusative case, which is manifested as ending -a for animate masculine nouns, but as null ending for inanimate nouns, or as -ia for nouns which end in a &quot;soft consonant&quot;, -u for feminine nouns, etc. etc. Or the verb &quot;to use&quot; requires the following noun to be in instrumental case if it&#x27;s used as a tool.<p>Although it&#x27;s not perfect and had mistakes, I found it interesting that NanoGPT was able to infer certain complex rules in just 3 minutes of training - and I searched in the texts for the exact examples it generated and found nothing verbatim.<p>However, despite understanding grammar more-less, semantically, it was complete nonsense.</div><br/><div id="39256254" class="c"><input type="checkbox" id="c-39256254" checked=""/><div class="controls bullet"><span class="by">lingeringdoubts</span><span>|</span><a href="#39255312">parent</a><span>|</span><a href="#39255881">next</a><span>|</span><label class="collapse" for="c-39256254">[-]</label><label class="expand" for="c-39256254">[1 more]</label></div><br/><div class="children"><div class="content">Not too surprising, since the inflections would be among the most common tokens in the training text.</div><br/></div></div></div></div><div id="39255881" class="c"><input type="checkbox" id="c-39255881" checked=""/><div class="controls bullet"><span class="by">quag</span><span>|</span><a href="#39255312">prev</a><span>|</span><a href="#39255147">next</a><span>|</span><label class="collapse" for="c-39255881">[-]</label><label class="expand" for="c-39255881">[3 more]</label></div><br/><div class="children"><div class="content">Is the author claiming that LLMs are Markov Chain text generators? That is, the probability distribution of the next token generated is the same as the probability of those token sequences in the training data?<p>If so, does it suggest we could “just” build a Markov Chain using the original training data and get similar performance to the LLM?</div><br/><div id="39257891" class="c"><input type="checkbox" id="c-39257891" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#39255881">parent</a><span>|</span><a href="#39255147">next</a><span>|</span><label class="collapse" for="c-39257891">[-]</label><label class="expand" for="c-39257891">[2 more]</label></div><br/><div class="children"><div class="content">From the post:<p>&gt; I implemented imperative code that does what I’m proposing the transformer is doing. It produces outputs very similar to the transformer.<p>This means there is probably a way to bypass transformers and get the same results. Would be interesting if it&#x27;s more efficient. Like given foundation model train something else and run it on much smaller device.</div><br/><div id="39258095" class="c"><input type="checkbox" id="c-39258095" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#39255881">root</a><span>|</span><a href="#39257891">parent</a><span>|</span><a href="#39255147">next</a><span>|</span><label class="collapse" for="c-39258095">[-]</label><label class="expand" for="c-39258095">[1 more]</label></div><br/><div class="children"><div class="content">I explained that it&#x27;s not bypassing transformers and not more efficient in another comment: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39254966">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39254966</a></div><br/></div></div></div></div></div></div><div id="39255147" class="c"><input type="checkbox" id="c-39255147" checked=""/><div class="controls bullet"><span class="by">empiko</span><span>|</span><a href="#39255881">prev</a><span>|</span><a href="#39257488">next</a><span>|</span><label class="collapse" for="c-39255147">[-]</label><label class="expand" for="c-39255147">[2 more]</label></div><br/><div class="children"><div class="content">nice project, but the model that was being studied is really just a toy-model (both in size and training data). as such, this model can indeed be approximated by simpler models (I would suspect even n-gram LMs), but it might not be representative of how the larger LMs work.</div><br/><div id="39255581" class="c"><input type="checkbox" id="c-39255581" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#39255147">parent</a><span>|</span><a href="#39257488">next</a><span>|</span><label class="collapse" for="c-39255581">[-]</label><label class="expand" for="c-39255581">[1 more]</label></div><br/><div class="children"><div class="content">This is probably true - i.e. you could make an even smaller model and then likely come up with an even-simpler explanation for how it worked.</div><br/></div></div></div></div><div id="39257488" class="c"><input type="checkbox" id="c-39257488" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39255147">prev</a><span>|</span><a href="#39254978">next</a><span>|</span><label class="collapse" for="c-39257488">[-]</label><label class="expand" for="c-39257488">[2 more]</label></div><br/><div class="children"><div class="content">Is the behavior that the attention + FF displacements tend point in the same direction known?  I am kind of surprised they are even in the same latent space across layers.  The FF network could be doing arbitrary rotations, right?  I suspect I misunderstand what is going on.</div><br/></div></div><div id="39254978" class="c"><input type="checkbox" id="c-39254978" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#39257488">prev</a><span>|</span><a href="#39257006">next</a><span>|</span><label class="collapse" for="c-39254978">[-]</label><label class="expand" for="c-39254978">[2 more]</label></div><br/><div class="children"><div class="content">This was a good 3D visualization of the same systems and they probably should be read together for maximum effect ....<p>LLM Visualization (<a href="https:&#x2F;&#x2F;bbycroft.net&#x2F;llm" rel="nofollow">https:&#x2F;&#x2F;bbycroft.net&#x2F;llm</a>)
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38505211">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38505211</a></div><br/><div id="39256447" class="c"><input type="checkbox" id="c-39256447" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#39254978">parent</a><span>|</span><a href="#39257006">next</a><span>|</span><label class="collapse" for="c-39256447">[-]</label><label class="expand" for="c-39256447">[1 more]</label></div><br/><div class="children"><div class="content">I appreciate the effort that went into this visualization, however, as someone who has worked with neural networks for 9 years, I found it far more confusing than helpful. I believe it was due to trying to present all items at once instead of deferring to abstract concepts, however, I am not entirely sure of this fact. &lt;3 :&#x27;))))</div><br/></div></div></div></div><div id="39257006" class="c"><input type="checkbox" id="c-39257006" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39254978">prev</a><span>|</span><a href="#39256563">next</a><span>|</span><label class="collapse" for="c-39257006">[-]</label><label class="expand" for="c-39257006">[3 more]</label></div><br/><div class="children"><div class="content">Is the behavior of that the attention + FF displacements tend point in the same direction known?  I am kind of surprised they are even in the same latent space across layers.  The FF network could be doing arbitrary rotations, right?  I suspect I misunderstand what is going on.</div><br/><div id="39258113" class="c"><input type="checkbox" id="c-39258113" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#39257006">parent</a><span>|</span><a href="#39257107">next</a><span>|</span><label class="collapse" for="c-39258113">[-]</label><label class="expand" for="c-39258113">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a 2D representation of very high-dimensional vectors. Something has to be left out and accurately depicting arbitrary rotations in the high-dimensional space is one of those things.</div><br/></div></div><div id="39257107" class="c"><input type="checkbox" id="c-39257107" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#39257006">parent</a><span>|</span><a href="#39258113">prev</a><span>|</span><a href="#39256563">next</a><span>|</span><label class="collapse" for="c-39257107">[-]</label><label class="expand" for="c-39257107">[1 more]</label></div><br/><div class="children"><div class="content">Best to replace attention addition with scaling and see.</div><br/></div></div></div></div><div id="39256563" class="c"><input type="checkbox" id="c-39256563" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#39257006">prev</a><span>|</span><a href="#39255072">next</a><span>|</span><label class="collapse" for="c-39256563">[-]</label><label class="expand" for="c-39256563">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m having a very hard time understanding exactly what the author is claiming to show. I&#x27;ve read the &quot;Interpretation: Why Does the Approximation Work?&quot; section a few times, but it feels like it&#x27;s just a mechanical description of the steps of a transformer. What&#x27;s the core claim?</div><br/></div></div><div id="39255072" class="c"><input type="checkbox" id="c-39255072" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39256563">prev</a><span>|</span><a href="#39254932">next</a><span>|</span><label class="collapse" for="c-39255072">[-]</label><label class="expand" for="c-39255072">[28 more]</label></div><br/><div class="children"><div class="content">This is a weird post. &quot;What the transformer is actually doing&quot;? You can just follow the code and see what it&#x27;s doing. It&#x27;s not doing something more or less than that. It&#x27;s not doing some other thing.</div><br/><div id="39255752" class="c"><input type="checkbox" id="c-39255752" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#39255072">parent</a><span>|</span><a href="#39255196">next</a><span>|</span><label class="collapse" for="c-39255752">[-]</label><label class="expand" for="c-39255752">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll go with an example to demonstrate why that&#x27;s not always enough.  Many people are quite keen to know what this (for example) is <i>actually</i> doing:<p><pre><code>  float InvSqrt(float x){
      float xhalf = 0.5f * x;
      int i = *(int*)&amp;x;
      i = 0x5f3759df - (i &gt;&gt; 1);
      x = *(float*)&amp;i;
      x = x*(1.5f - xhalf*x*x);
      return x;
  }
</code></pre>
From <a href="https:&#x2F;&#x2F;betterexplained.com&#x2F;articles&#x2F;understanding-quakes-fast-inverse-square-root&#x2F;" rel="nofollow">https:&#x2F;&#x2F;betterexplained.com&#x2F;articles&#x2F;understanding-quakes-fa...</a><p>In my case I don&#x27;t have a huge amount of time to chase down every rabbit hole, but I&#x27;d love to accelerate intuition for LLM&#x27;s. Multiple points of intuition or comparison really help. I&#x27;m also not a python expert - what you see and what I see from a line of code will be quite different.</div><br/><div id="39255929" class="c"><input type="checkbox" id="c-39255929" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255752">parent</a><span>|</span><a href="#39255196">next</a><span>|</span><label class="collapse" for="c-39255929">[-]</label><label class="expand" for="c-39255929">[1 more]</label></div><br/><div class="children"><div class="content">The author is attempting to build an explicit mental model of what a bunch of weights are &quot;doing&quot;. It&#x27;s not really the same thing. They are minimizing the loss function.<p>People try to (and often do) generate intuition for architectures that will work given the lay out of data. But, the reason models are so big now is that trying to understand what the model is &quot;doing&quot; in a way humans understand didn&#x27;t work out so well.</div><br/></div></div></div></div><div id="39255196" class="c"><input type="checkbox" id="c-39255196" checked=""/><div class="controls bullet"><span class="by">gjm11</span><span>|</span><a href="#39255072">parent</a><span>|</span><a href="#39255752">prev</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39255196">[-]</label><label class="expand" for="c-39255196">[15 more]</label></div><br/><div class="children"><div class="content">The post is long and complicated and I haven&#x27;t read most of it, so whether it&#x27;s actually any good I shan&#x27;t try to decide. But the above seems like a very weird argument.<p>Sure, the code is doing what it&#x27;s doing. But trying to understand it at that level of abstraction seems ... not at all promising.<p>Consider a question about psychology. Say: &quot;What are people doing when they decide what to buy in a shop?&quot;.<p>If someone writes an article about this, drawing on some (necessarily simplified) model of human thinking and decision-making, and some experimental evidence about how people&#x27;s purchasing decisions change in response to changes in price, different lighting conditions, mood, etc., ... would you say &quot;You can just apply the laws of physics and see what the people are doing. They&#x27;re not doing something more or less than that.&quot;?<p>I mean, it would be <i>true</i>. People, so far as we know, do in fact obey the laws of physics. You could, in principle, predict what someone will buy in a given situation by modelling their body and surroundings at the level of atoms or thereabouts (quantum physics is a thing, of course, but it seems likely that a basically-classical model could be good enough for this purpose). When we make decisions, we are obeying the laws of physics and not doing some other thing.<p>But this answer is completely useless for <i>actually understanding</i> what we do. If you&#x27;re wondering &quot;what would happen if the price were ten cents higher?&quot; you&#x27;ve got no way to answer it other than running the whole simulation again. Maybe running thousands of versions of it since other factors could affect the results. If you&#x27;re wondering &quot;does the lighting make a difference, and what level of lighting in the shop will lead to people spending least or most?&quot; then you&#x27;ve got no way to answer it other than running simulations with many different lighting conditions.<p>Whereas if you have a higher-level, less precise model that says things like &quot;people mostly prefer to spend less&quot; and &quot;people try to predict quality on the basis of price, so sometimes they will spend more if it seems like they&#x27;re getting something better that way&quot; and &quot;people like to feel that they&#x27;re getting a bargain&quot; and so on, you may be able to make predictions without running an impossibly detailed person-simulation zillions of times. You may be able to <i>give general advice</i> to someone with a spending problem who&#x27;d like to spend more wisely, or to a shopkeeper who wants to encourage their customers to spend more.<p>Similarly with language models and similar systems. Sure, you can find out what it does in some very specific situation by just running the code. But what if you have some broader question than that? Then simply knowing what the code does may not help you at all, because what the code does is gazillions of copies of &quot;multiply these numbers together and add them&quot;.<p>Again, I make no claim about whether the particular thing linked here offers much real insight. But it makes zero sense, so far as I can see, to dismiss it on the grounds that all you need to do is read the code.</div><br/><div id="39255282" class="c"><input type="checkbox" id="c-39255282" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255196">parent</a><span>|</span><a href="#39255765">next</a><span>|</span><label class="collapse" for="c-39255282">[-]</label><label class="expand" for="c-39255282">[6 more]</label></div><br/><div class="children"><div class="content">You’re spot on; it’s like saying you can understand the game of chess by simply reading the rules. In a certain very superficial sense, yes. But the universe isn’t so simple. The same reason even a perfect understanding of what goes on at the level of subatomic particles isn’t thought to be enough to say we ‘understand the universe’. A hell of a lot can happen in between the setting out of some basic rules and the end — much higher level — result.</div><br/><div id="39255788" class="c"><input type="checkbox" id="c-39255788" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255282">parent</a><span>|</span><a href="#39255765">next</a><span>|</span><label class="collapse" for="c-39255788">[-]</label><label class="expand" for="c-39255788">[5 more]</label></div><br/><div class="children"><div class="content">And yet...alpha zero.</div><br/><div id="39255808" class="c"><input type="checkbox" id="c-39255808" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255788">parent</a><span>|</span><a href="#39255765">next</a><span>|</span><label class="collapse" for="c-39255808">[-]</label><label class="expand" for="c-39255808">[4 more]</label></div><br/><div class="children"><div class="content">My entire point is that implementation isn’t sufficient for understanding. Alpha Zero is the perfect example of that; you can create an amazing chess playing machine and (potentially) learn nothing at all about how to play chess.<p>…so what’s your point? I’m not getting it from those two words.</div><br/><div id="39255970" class="c"><input type="checkbox" id="c-39255970" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255808">parent</a><span>|</span><a href="#39255765">next</a><span>|</span><label class="collapse" for="c-39255970">[-]</label><label class="expand" for="c-39255970">[3 more]</label></div><br/><div class="children"><div class="content">Understanding how the machine plays or how you should play? They aren&#x27;t the same thing. And that is the point - trying to analogize to some explicit, concrete function you can describe is backwards. These models are gigantic (even the &#x27;small&#x27; ones), they are looking to minimize a loss function by looking in multi thousand dimensional space. It is the very opposite of something that fits in a human brain in any explicit fashion.</div><br/><div id="39256754" class="c"><input type="checkbox" id="c-39256754" checked=""/><div class="controls bullet"><span class="by">gjm11</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255970">parent</a><span>|</span><a href="#39255765">next</a><span>|</span><label class="collapse" for="c-39256754">[-]</label><label class="expand" for="c-39256754">[2 more]</label></div><br/><div class="children"><div class="content">So is what happens in an actual literal human brain.<p>And yet, we spend quite a lot of our time thinking about what human brains do, and sometimes it&#x27;s pretty useful.<p>For a lot of this, we treat the actual brain as a black box and don&#x27;t particularly care about <i>how</i> it does what it does, but knowing something about the internal workings at various levels of abstraction is useful too.<p>Similarly, if for whatever reason you are interested in, or spend some of your time interacting with, transformer-based language models, then you might want some intuition for what they do and how.<p>You&#x27;ll never fit the whole thing in your brain. That&#x27;s why you want simplified abstracted versions of it. Which, AIUI, is one thing that the OP is trying to do. (As I said before, I don&#x27;t know how well it does it; what I&#x27;m objecting to is the idea that trying to do this is a waste of time because the only thing there is to know is that the model does what the code says it does.)</div><br/><div id="39257058" class="c"><input type="checkbox" id="c-39257058" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39256754">parent</a><span>|</span><a href="#39255765">next</a><span>|</span><label class="collapse" for="c-39257058">[-]</label><label class="expand" for="c-39257058">[1 more]</label></div><br/><div class="children"><div class="content">Sure, good abstractions are good. But bad abstractions are worse than none. Think of all the nonsense abstractions about the weather before people understood and could simulate the underlying process. No one in modern weather forecasting suggests there is a way to understand that process at some high level of abstraction. Understand the low level, run the calcs.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39255765" class="c"><input type="checkbox" id="c-39255765" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255196">parent</a><span>|</span><a href="#39255282">prev</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39255765">[-]</label><label class="expand" for="c-39255765">[8 more]</label></div><br/><div class="children"><div class="content">It is very promising. In fact, in industry there are jokes about how getting rid of linguists has helped language modeling.<p>Trying to understand it at some level of abstraction that humans can fit in their head has been a dead end.</div><br/><div id="39256443" class="c"><input type="checkbox" id="c-39256443" checked=""/><div class="controls bullet"><span class="by">knightoffaith</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255765">parent</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39256443">[-]</label><label class="expand" for="c-39256443">[7 more]</label></div><br/><div class="children"><div class="content">Trying to build systems top-down using principles humans can fit in their head has arguably been a dead end. But this doesn&#x27;t mean that we cannot try to understand parts of current AI systems at a higher level of abstraction, right? They may not have been designed top-down with human-understandable principles, but that doesn&#x27;t mean that trained, human-understandable principles couldn&#x27;t have emerged organically from the training process.<p>Evolution optimized the human brain to do things over an unbelievably long period of time. Human brains were not designed top-down with human-understandable principles. But neuroscientists, cognitive scientists, and psychologists have arguably had success with understanding the brain partially at a higher level of abstraction than just neurons, or just saying &quot;evolution optimized these clumps of matter for spreading genes; there&#x27;s nothing more to say&quot;. What do you think is the relevant difference between the human brain and current machine learning models that makes the latter just utterly incomprehensible at any higher level of abstraction, but the former worth pursuing by means of different scientific fields?</div><br/><div id="39256650" class="c"><input type="checkbox" id="c-39256650" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39256443">parent</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39256650">[-]</label><label class="expand" for="c-39256650">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know neuroscience at all, so I don&#x27;t know if that&#x27;s a good analogy. I&#x27;ll make a guess though - if you consider a standard RAG application. That&#x27;s a system which uses at least a couple models. A person might reasonably say &quot;the embeddings in the db are where the system stores memories. The LLM acts as the part of the brain that reasons over whatever is in working memory plus it&#x27;s sort of implicit knowledge.&quot; I&#x27;d argue that&#x27;s reasonable. But systems and models are different things.<p>People use many abstractions in AI&#x2F;ML. Just look at all the functionality you get in PyTorch as an example. But they are abstractions of pieces of a model, or pieces of the training process etc. They aren&#x27;t abstractions of the function the model is trying to learn.</div><br/><div id="39256737" class="c"><input type="checkbox" id="c-39256737" checked=""/><div class="controls bullet"><span class="by">knightoffaith</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39256650">parent</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39256737">[-]</label><label class="expand" for="c-39256737">[5 more]</label></div><br/><div class="children"><div class="content">Right, I&#x27;ve used pytorch before. I&#x27;m just trying to understand why the question of &quot;how does a transformer work?&quot; is only meaningfully answered by describing the mechanisms of self-attention layers at the highest level of abstraction, with any higher level of abstraction being nonsense. More specifically, why we should have a ban on any higher level of abstraction in this scenario when we can answer the question of &quot;how does the human mind work?&quot; at not just the atom level, but also the neuroscientific level or psychological level. Presumably you could say the same thing about this question: The human mind is a bunch of atoms obeying the laws of physics. That&#x27;s what it&#x27;s doing. It&#x27;s not something else.<p>I understand you&#x27;re emphasizing the point that the connectionist paradigm has had a lot more empirical success than the computationalist paradigm - letting AI systems learn organically, bottom-up is more effective than trying to impose human mind-like principles top-down when we design them. But I don&#x27;t understand why this means understanding bottom-up systems at higher level of abstractions is necessarily impossible when we have a clear example of a bottom-up system that we&#x27;ve had some success in understanding at a high level of abstraction, viz. the human mind.</div><br/><div id="39257008" class="c"><input type="checkbox" id="c-39257008" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39256737">parent</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39257008">[-]</label><label class="expand" for="c-39257008">[4 more]</label></div><br/><div class="children"><div class="content">It would be great if they were good, but they seem to be bad, it seems that they must be bad given the dimensionality of the space, and humans latch onto simple explanations even when they are bad.<p>Think about MoE models. Each expert learns to be good at completing certain types of inputs. It sounds like a great explanation for how it works. Except, it doesn&#x27;t seem to actually work that way. The mixtral paper showed that the activated routes seemed to follow basically no pattern. Maybe if they trained it differently it would? Who knows. It certainly isn&#x27;t a good name regardless.<p>Many fields&#x2F;things can be understood at higher and higher levels of abstraction. Computer science is full of <i>good</i> high level abstractions. Humans love it. It doesn&#x27;t work everywhere.</div><br/><div id="39257114" class="c"><input type="checkbox" id="c-39257114" checked=""/><div class="controls bullet"><span class="by">knightoffaith</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39257008">parent</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39257114">[-]</label><label class="expand" for="c-39257114">[3 more]</label></div><br/><div class="children"><div class="content">Right, of course we should validate explanations based on empirical data. We rejected the idea that there was a particular neuron that activated only when you saw your grandmother (the &quot;grandmother neuron&quot;) after experimentation. But just because explanations have been bad, doesn&#x27;t mean that all future explanations must also be bad. Shouldn&#x27;t we evaluate explanations on a case-by-case basis instead of dismissing them as impossible? Aren&#x27;t we better off having evaluated the intuitive explanation for mixtures of experts instead of dismissing them a priori? There&#x27;s a whole field - mechanistic interpretability - where researchers are working on this kind of thing. Do you think that they simply haven&#x27;t realized that the models they&#x27;re working on interpreting are operating in a high-dimensional space?</div><br/><div id="39257359" class="c"><input type="checkbox" id="c-39257359" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39257114">parent</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39257359">[-]</label><label class="expand" for="c-39257359">[2 more]</label></div><br/><div class="children"><div class="content">Mechanistic interpretability studies a bunch of things though. Like, the mixtral paper where they show the routing activations is mechanistic interpretability. That sort of feature visualization stuff is good. I don&#x27;t know what % of the field is spending their time on trying to interpret the models in a way that involves higher level, human can explain, approximating the following code type work though? I&#x27;m certainly not the only one who thinks it&#x27;s a waste of time, I don&#x27;t believe anything I&#x27;ve said in this thread is original in any way.<p>I... don&#x27;t know if the people involved in that specific stuff have really grokked they are working in high dimensional space? A lot of otherwise smart people work in macroeconomics, where for decades they haven&#x27;t really made any progress because it&#x27;s so complex. It seems stupid to suggest a whole field of smart people don&#x27;t realize what they are up against, but sheesh it kinda seems that way doesn&#x27;t it? Maybe I&#x27;ll be eating my words in 10 years.</div><br/><div id="39257518" class="c"><input type="checkbox" id="c-39257518" checked=""/><div class="controls bullet"><span class="by">knightoffaith</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39257359">parent</a><span>|</span><a href="#39255243">next</a><span>|</span><label class="collapse" for="c-39257518">[-]</label><label class="expand" for="c-39257518">[1 more]</label></div><br/><div class="children"><div class="content">They certainly understand they&#x27;re working in a high dimensional space. No question. What they deny is that this necessarily means the goal of interpretability is a futile one.<p>But the main thrust of what I&#x27;m saying is that we shouldn&#x27;t be dismissing explanations a priori - answers to &quot;how does a transformer work?&quot; that go beyond descriptions of self-attention aren&#x27;t necessarily nonsensical. You can think it&#x27;s a waste of time (...frankly, I kind of think it&#x27;s a waste of time too...), but just like any other field, it&#x27;s not really fair to close our eyes and ears and dismiss proposals out of hand. I suppose
&gt; Maybe I&#x27;ll be eating my words in 10 years.
indicates you understand this though.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39255243" class="c"><input type="checkbox" id="c-39255243" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#39255072">parent</a><span>|</span><a href="#39255196">prev</a><span>|</span><a href="#39255167">next</a><span>|</span><label class="collapse" for="c-39255243">[-]</label><label class="expand" for="c-39255243">[3 more]</label></div><br/><div class="children"><div class="content">Understanding how a given CPU (+ the other computer hardware) works, does not suffice to understand what is going on when a particular program is running. For that, you need to either read the program, or an execution trace, or both, or something along these lines, which is specific to the program being run.</div><br/><div id="39255734" class="c"><input type="checkbox" id="c-39255734" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255243">parent</a><span>|</span><a href="#39255167">next</a><span>|</span><label class="collapse" for="c-39255734">[-]</label><label class="expand" for="c-39255734">[2 more]</label></div><br/><div class="children"><div class="content">This is the wrong analogy. The transformer block <i>is a bunch of code</i> and weights. It&#x27;s a set of instructions laying out which numbers to run which operations on. The optimizer changes weights to minimize a loss function during training and then the code implementing a forward pass just runs during inference. That&#x27;s what it is doing. It&#x27;s not doing something else.<p>If the argument is that a model is a function approximator, then it certainly isn&#x27;t approximating some function that performs worse at the task at hand, and it certainly isn&#x27;t approximating a function we can describe in a few hundred words.</div><br/><div id="39258325" class="c"><input type="checkbox" id="c-39258325" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255734">parent</a><span>|</span><a href="#39255167">next</a><span>|</span><label class="collapse" for="c-39258325">[-]</label><label class="expand" for="c-39258325">[1 more]</label></div><br/><div class="children"><div class="content">We have no reason at all to be certain of the latter.</div><br/></div></div></div></div></div></div><div id="39255167" class="c"><input type="checkbox" id="c-39255167" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39255072">parent</a><span>|</span><a href="#39255243">prev</a><span>|</span><a href="#39254932">next</a><span>|</span><label class="collapse" for="c-39255167">[-]</label><label class="expand" for="c-39255167">[7 more]</label></div><br/><div class="children"><div class="content">A walk through with what the data at each point looks like is actually pretty useful.</div><br/><div id="39255724" class="c"><input type="checkbox" id="c-39255724" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255167">parent</a><span>|</span><a href="#39254932">next</a><span>|</span><label class="collapse" for="c-39255724">[-]</label><label class="expand" for="c-39255724">[6 more]</label></div><br/><div class="children"><div class="content">Sure, it is. But trying to explain it as though the weights have some goal is weird. They aren&#x27;t <i>trying</i> to do anything. You have a loss function. The optimizer keeps moving weights around in an attempt to minimize the loss function. It&#x27;s not more or less than that.</div><br/><div id="39256289" class="c"><input type="checkbox" id="c-39256289" checked=""/><div class="controls bullet"><span class="by">MarkusQ</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39255724">parent</a><span>|</span><a href="#39254932">next</a><span>|</span><label class="collapse" for="c-39256289">[-]</label><label class="expand" for="c-39256289">[5 more]</label></div><br/><div class="children"><div class="content">This is just wrong.<p>First of all, you&#x27;re rejecting teleological anthropomorphizing (saying it&#x27;s weird to act as if &quot;the weights have some goal&quot;)  but then in the very next line you talk about the optimizer making &quot;an attempt&quot; to accomplish a goal.  All of which misses the point, since the question is about _explanations_ not goals and intentions.<p>Then you reject out of hand any other level of explanation than the one you favor, saying &quot;it&#x27;s not more or less than that&quot; when in fact it is both more and less than that; you can climb the ladder of abstraction either way to build more or less abstract explanations.  We can dig down and talk about how the optimizer adjusts weights, or how tensor math works and how it&#x27;s used in this case, or about how GPUs work, or gates, or transistors, etc.  Or we could climb up and talk about (as this article does) and talk about what attention heads do, and why they work, when they work, when they don&#x27;t, etc.</div><br/><div id="39256324" class="c"><input type="checkbox" id="c-39256324" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39256289">parent</a><span>|</span><a href="#39254932">next</a><span>|</span><label class="collapse" for="c-39256324">[-]</label><label class="expand" for="c-39256324">[4 more]</label></div><br/><div class="children"><div class="content">The optimizer has a goal. The weights in the model do not. The optimizer isn&#x27;t the model. There is no contradiction if you know how it works.<p>Climbing the layer of abstraction from model weights doesn&#x27;t seem to work in this field. Just saying it&#x27;s so doesn&#x27;t make it so.</div><br/><div id="39256358" class="c"><input type="checkbox" id="c-39256358" checked=""/><div class="controls bullet"><span class="by">MarkusQ</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39256324">parent</a><span>|</span><a href="#39254932">next</a><span>|</span><label class="collapse" for="c-39256358">[-]</label><label class="expand" for="c-39256358">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  Just saying it&#x27;s so doesn&#x27;t make it so.<p>Does that apply to you as well?</div><br/><div id="39256402" class="c"><input type="checkbox" id="c-39256402" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39256358">parent</a><span>|</span><a href="#39254932">next</a><span>|</span><label class="collapse" for="c-39256402">[-]</label><label class="expand" for="c-39256402">[2 more]</label></div><br/><div class="children"><div class="content">Of course. You shouldn&#x27;t take my word for it. You can learn the basics of AI&#x2F;ML from a number of good texts. Simon Prince just released a very approachable text, although it doesn&#x27;t cover much in the way of history to see the move to &quot;more data&#x2F;more compute, less human lead abstraction&quot;. I think Norvig&#x27;s book covers that but I haven&#x27;t read the latest version.</div><br/><div id="39256474" class="c"><input type="checkbox" id="c-39256474" checked=""/><div class="controls bullet"><span class="by">MarkusQ</span><span>|</span><a href="#39255072">root</a><span>|</span><a href="#39256402">parent</a><span>|</span><a href="#39254932">next</a><span>|</span><label class="collapse" for="c-39256474">[-]</label><label class="expand" for="c-39256474">[1 more]</label></div><br/><div class="children"><div class="content">You sure like to make assumptions, don&#x27;t you?  :)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39254932" class="c"><input type="checkbox" id="c-39254932" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#39255072">prev</a><span>|</span><label class="collapse" for="c-39254932">[-]</label><label class="expand" for="c-39254932">[3 more]</label></div><br/><div class="children"><div class="content">A thousand hands on a Ouija board.</div><br/><div id="39257752" class="c"><input type="checkbox" id="c-39257752" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#39254932">parent</a><span>|</span><label class="collapse" for="c-39257752">[-]</label><label class="expand" for="c-39257752">[2 more]</label></div><br/><div class="children"><div class="content">Is that an analogy? If so, it is an extremely interesting one, and I would like to know where does it come from :D</div><br/><div id="39257959" class="c"><input type="checkbox" id="c-39257959" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#39254932">root</a><span>|</span><a href="#39257752">parent</a><span>|</span><label class="collapse" for="c-39257959">[-]</label><label class="expand" for="c-39257959">[1 more]</label></div><br/><div class="children"><div class="content">as you&#x27;d expect:<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ouija" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ouija</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>