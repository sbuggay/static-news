<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1717750850254" as="style"/><link rel="stylesheet" href="styles.css?v=1717750850254"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://openai.com/index/extracting-concepts-from-gpt-4/">Extracting concepts from GPT-4</a> <span class="domain">(<a href="https://openai.com">openai.com</a>)</span></div><div class="subtext"><span>davidbarker</span> | <span>112 comments</span></div><br/><div><div id="40601640" class="c"><input type="checkbox" id="c-40601640" checked=""/><div class="controls bullet"><span class="by">andreyk</span><span>|</span><a href="#40601668">next</a><span>|</span><label class="collapse" for="c-40601640">[-]</label><label class="expand" for="c-40601640">[43 more]</label></div><br/><div class="children"><div class="content">Exciting to see this so soon after Anthropic&#x27;s &quot;Mapping the Mind of a Large Language Model&quot; (under 3 weeks). I find these efforts really exciting; it is still common to hear people say &quot;we have no idea how LLMs &#x2F; Deep Learning works&quot;, but that is really a gross generalization as stuff like this shows.<p>Wonder if this was a bit rushed out in response to Anthropic&#x27;s release (as well as the departure of Jan Leike from OpenAI)... the paper link doesn&#x27;t even go to Arxiv, and the analysis is not nearly as deep. Though who knows, might be unrelated.</div><br/><div id="40601827" class="c"><input type="checkbox" id="c-40601827" checked=""/><div class="controls bullet"><span class="by">thegrim33</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40604949">next</a><span>|</span><label class="collapse" for="c-40601827">[-]</label><label class="expand" for="c-40601827">[27 more]</label></div><br/><div class="children"><div class="content">From the article:<p>&quot;We currently don&#x27;t understand how to make sense of the neural activity within language models.&quot;<p>&quot;Unlike with most human creations, we don’t really understand the inner workings of neural networks.&quot;<p>&quot;The [..] networks are not well understood and cannot be easily decomposed into identifiable parts&quot;<p>&quot;[..] the neural activations inside a language model activate with unpredictable patterns, seemingly representing many concepts simultaneously&quot;<p>&quot;Learning a large number of sparse features is challenging, and past work has not been shown to scale well.&quot;<p>etc., etc., etc.<p>People say we don&#x27;t (currently) know why they output what they output, because .. as the article clearly states, we don&#x27;t.</div><br/><div id="40603136" class="c"><input type="checkbox" id="c-40603136" checked=""/><div class="controls bullet"><span class="by">TrainedMonkey</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40601827">parent</a><span>|</span><a href="#40605992">next</a><span>|</span><label class="collapse" for="c-40603136">[-]</label><label class="expand" for="c-40603136">[5 more]</label></div><br/><div class="children"><div class="content">I read this as &quot;we have not built up tools &#x2F; math to understand neural networks as they are new and exciting&quot; and not as &quot;neural networks are magical and complex and not understandable because we are meddling with something we cannot control&quot;.<p>A good example would be planes - it took a long while to develop mathematical models that could be used to model behavior. Meanwhile practical experimentation developed decent rule of thumb for what worked &#x2F; did not work.<p>So I don&#x27;t think it&#x27;s fair to say that &quot;we don&#x27;t&quot; (know how neural networks work), we don&#x27;t have math &#x2F; models yet that can explain&#x2F;model their behavior...</div><br/><div id="40603407" class="c"><input type="checkbox" id="c-40603407" checked=""/><div class="controls bullet"><span class="by">gradus_ad</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40603136">parent</a><span>|</span><a href="#40606076">next</a><span>|</span><label class="collapse" for="c-40603407">[-]</label><label class="expand" for="c-40603407">[2 more]</label></div><br/><div class="children"><div class="content">Chaotic nonlinear dynamics have been an object of mathematical research for a very long time and we have built up good mathematical tools to work with them, but in spite of that turbulent flow and similar phenomena (brains&#x2F;LLM&#x27;s) remain poorly understood.<p>The problem is that the macro and micro dynamics of complex systems are intimately linked, making for non-stationary non-ergodic behavior that cannot be reduced to a few principles upon which we can build a model or extrapolate a body of knowledge. We simply cannot understand complex systems because they cannot be &quot;reduced&quot;. They are what they are, unique and unprincipled in every moment (hey, like people!).</div><br/></div></div><div id="40606076" class="c"><input type="checkbox" id="c-40606076" checked=""/><div class="controls bullet"><span class="by">freilanzer</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40603136">parent</a><span>|</span><a href="#40603407">prev</a><span>|</span><a href="#40603558">next</a><span>|</span><label class="collapse" for="c-40606076">[-]</label><label class="expand" for="c-40606076">[1 more]</label></div><br/><div class="children"><div class="content">&gt; we don&#x27;t have math &#x2F; models yet that can explain&#x2F;model their behavior...<p>So, what you&#x27;re saying is we don&#x27;t know how they work yet? It&#x27;s not that deep.</div><br/></div></div><div id="40603558" class="c"><input type="checkbox" id="c-40603558" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40603136">parent</a><span>|</span><a href="#40606076">prev</a><span>|</span><a href="#40605992">next</a><span>|</span><label class="collapse" for="c-40603558">[-]</label><label class="expand" for="c-40603558">[1 more]</label></div><br/><div class="children"><div class="content">&quot;We don&#x27;t know how X works&quot; <i>literally means</i> &quot;we don&#x27;t have models yet that can explain X&#x27;s behavior&quot;.<p>TFA is about making a tiny bit of progress towards such models. Perhaps you should read it.</div><br/></div></div></div></div><div id="40605992" class="c"><input type="checkbox" id="c-40605992" checked=""/><div class="controls bullet"><span class="by">realPtolemy</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40601827">parent</a><span>|</span><a href="#40603136">prev</a><span>|</span><a href="#40602256">next</a><span>|</span><label class="collapse" for="c-40605992">[-]</label><label class="expand" for="c-40605992">[1 more]</label></div><br/><div class="children"><div class="content">Could there also be a “legal hedging” reason for why you would release a paper like this?<p>By reaffirming that “we don’t know how this works, nobody does” it’s easier to avoid being charged with copyright infringement from various actors&#x2F;data sources that have sued them.</div><br/></div></div><div id="40602256" class="c"><input type="checkbox" id="c-40602256" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40601827">parent</a><span>|</span><a href="#40605992">prev</a><span>|</span><a href="#40605498">next</a><span>|</span><label class="collapse" for="c-40602256">[-]</label><label class="expand" for="c-40602256">[16 more]</label></div><br/><div class="children"><div class="content">Not holding my breath for that hallucinated cure for cancer then.</div><br/><div id="40602453" class="c"><input type="checkbox" id="c-40602453" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40602256">parent</a><span>|</span><a href="#40605498">next</a><span>|</span><label class="collapse" for="c-40602453">[-]</label><label class="expand" for="c-40602453">[15 more]</label></div><br/><div class="children"><div class="content">LLMs aren&#x27;t the only kind of AI, just one of the two current shiny kinds.<p>If a &quot;cure for cancer&quot; (cancer is not just one disease so, unfortunately, that&#x27;s not even as coherent a request as we&#x27;d all like it to be) is what you&#x27;re hoping for, look instead at the stuff like AlphaFold etc.: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;AlphaFold" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;AlphaFold</a><p>I don&#x27;t know how to tell where real science ends and PR bluster begins in such models, though I can say that the closest I&#x27;ve heard to a word against it is &quot;sure, but we&#x27;ve got other things besides protein folding to solve&quot;, which is a good sign.<p>(I assume AlphaFold is also a mysterious black box, and that tools such as the one under discussion may help us demystify it too).</div><br/></div></div></div></div><div id="40605498" class="c"><input type="checkbox" id="c-40605498" checked=""/><div class="controls bullet"><span class="by">submeta</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40601827">parent</a><span>|</span><a href="#40602256">prev</a><span>|</span><a href="#40604949">next</a><span>|</span><label class="collapse" for="c-40605498">[-]</label><label class="expand" for="c-40605498">[4 more]</label></div><br/><div class="children"><div class="content">Scary actually. Because how can we asses the risks when we don’t know what the system is capabale of doing.</div><br/><div id="40605549" class="c"><input type="checkbox" id="c-40605549" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40605498">parent</a><span>|</span><a href="#40604949">next</a><span>|</span><label class="collapse" for="c-40605549">[-]</label><label class="expand" for="c-40605549">[3 more]</label></div><br/><div class="children"><div class="content">We know exactly what the system is capable of doing. It’s capable of outputting tokens which can then be converted into text.</div><br/><div id="40605753" class="c"><input type="checkbox" id="c-40605753" checked=""/><div class="controls bullet"><span class="by">reducesuffering</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40605549">parent</a><span>|</span><a href="#40604949">next</a><span>|</span><label class="collapse" for="c-40605753">[-]</label><label class="expand" for="c-40605753">[2 more]</label></div><br/><div class="children"><div class="content">And social media manipulation is just registers and bytes, wait no, sand and electrons.</div><br/><div id="40606108" class="c"><input type="checkbox" id="c-40606108" checked=""/><div class="controls bullet"><span class="by">isaacremuant</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40605753">parent</a><span>|</span><a href="#40604949">next</a><span>|</span><label class="collapse" for="c-40606108">[-]</label><label class="expand" for="c-40606108">[1 more]</label></div><br/><div class="children"><div class="content">Just because you can do something with technology doesn&#x27;t mean the problem is technology itself. It&#x27;s like newspapers.  Printing them I technology and allows all kind of things. If you&#x27;re of the authoritarian mindset, you&#x27;ll want to control it all out of some stated fear, but you can do that for everything.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40604949" class="c"><input type="checkbox" id="c-40604949" checked=""/><div class="controls bullet"><span class="by">leogao</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40601827">prev</a><span>|</span><a href="#40605975">next</a><span>|</span><label class="collapse" for="c-40604949">[-]</label><label class="expand" for="c-40604949">[1 more]</label></div><br/><div class="children"><div class="content">We were planning to release the paper around this time independent of the other events you mention.<p>I think it is still predominantly accurate to say that we have no idea how LLMs work. SAEs might eventually change that, but there&#x27;s still a long way to go.</div><br/></div></div><div id="40605975" class="c"><input type="checkbox" id="c-40605975" checked=""/><div class="controls bullet"><span class="by">realPtolemy</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40604949">prev</a><span>|</span><a href="#40605765">next</a><span>|</span><label class="collapse" for="c-40605975">[-]</label><label class="expand" for="c-40605975">[3 more]</label></div><br/><div class="children"><div class="content">Indeed, and the very last section about how they’ve now “open sourced” this research is also a bit vague. They’ve shared their research methodology and findings… But isn’t that obligatory when writing a public paper?</div><br/><div id="40606102" class="c"><input type="checkbox" id="c-40606102" checked=""/><div class="controls bullet"><span class="by">lanceflt</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40605975">parent</a><span>|</span><a href="#40605765">next</a><span>|</span><label class="collapse" for="c-40606102">[-]</label><label class="expand" for="c-40606102">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;sparse_autoencoder">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;sparse_autoencoder</a><p>They actually open sourced it, for GPT-2 which is an open model.</div><br/><div id="40606665" class="c"><input type="checkbox" id="c-40606665" checked=""/><div class="controls bullet"><span class="by">realPtolemy</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40606102">parent</a><span>|</span><a href="#40605765">next</a><span>|</span><label class="collapse" for="c-40606665">[-]</label><label class="expand" for="c-40606665">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, I must have read through the document to hastily.</div><br/></div></div></div></div></div></div><div id="40605765" class="c"><input type="checkbox" id="c-40605765" checked=""/><div class="controls bullet"><span class="by">darby_nine</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40605975">prev</a><span>|</span><a href="#40601774">next</a><span>|</span><label class="collapse" for="c-40605765">[-]</label><label class="expand" for="c-40605765">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Mapping the Mind of a Large Language Model<p>The fact that a paper is implying a LLM has a mind doesn&#x27;t exactly bode well for the people who wrote it, not to mention the continued meaningless babbling about &quot;safety&quot;. It&#x27;d also be nice if they could show their work so we could replicate it. Still, not shabby for an ad!</div><br/></div></div><div id="40601774" class="c"><input type="checkbox" id="c-40601774" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40605765">prev</a><span>|</span><a href="#40602943">next</a><span>|</span><label class="collapse" for="c-40601774">[-]</label><label class="expand" for="c-40601774">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but that is really a gross generalization as stuff like this shows.<p>I think this research actually still reinforces that we still have very little understanding of the internals. The blog post also reiterates that this is early work with many limitations.</div><br/></div></div><div id="40602943" class="c"><input type="checkbox" id="c-40602943" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40601774">prev</a><span>|</span><a href="#40603940">next</a><span>|</span><label class="collapse" for="c-40602943">[-]</label><label class="expand" for="c-40602943">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Wonder if this was a bit rushed out in response to Anthropic&#x27;s release<p>too lazy to dig up source but some twitter sleuth found that the first commit to the project was 6 months ago<p>likely all these guys went to the same metaphorical SF bars, it was in the water</div><br/><div id="40603286" class="c"><input type="checkbox" id="c-40603286" checked=""/><div class="controls bullet"><span class="by">szvsw</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40602943">parent</a><span>|</span><a href="#40604938">next</a><span>|</span><label class="collapse" for="c-40603286">[-]</label><label class="expand" for="c-40603286">[1 more]</label></div><br/><div class="children"><div class="content">&gt; likely all these guys went to the same metaphorical SF bars, it was in the water<p>It also is coming from a long lineage of thought no? For instance, one of the things often thought early in an ML course is the notion that “early layers respond to&#x2F;generate general information&#x2F;patterns, and deeper layers respond to&#x2F;generate more detailed&#x2F;complex patterns&#x2F;information.” That is obviously an overly broad and vague statement but it is a useful intuition and can be backed up by doing some various inspection of eg what maximally activates some convolution filters. So already there is a notion that there is some sort of spatial structure to how semantics are processed and represented in a neural network (even if in a totally different context, as in image processing mentioned above), where “spatial” here is used to refer to different regions of the network.<p>Even more simply, in fact as simple as you can get: with linear regression, the most interpretable model you can get- you have a clear notion that different parameter groups of the model respond to different “concepts” (where a concept is taken to be whatever the variables associated with a given subset of coefficients represent).<p>In some sense, at least in a high-level&#x2F;intuitive reading of the new research coming out of Anthropic and OpenAI, I think the current research is just a natural extension of these ideas, albeit in a much more complicated context and massive scale.<p>Somebody else, please correct me if you think my reading is incorrect!!</div><br/></div></div><div id="40604938" class="c"><input type="checkbox" id="c-40604938" checked=""/><div class="controls bullet"><span class="by">leogao</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40602943">parent</a><span>|</span><a href="#40603286">prev</a><span>|</span><a href="#40603240">next</a><span>|</span><label class="collapse" for="c-40604938">[-]</label><label class="expand" for="c-40604938">[2 more]</label></div><br/><div class="children"><div class="content">This project has been in the works for about a year. The initial commit to the public repo was not really closely related to this project, it was part of the release of the Transformer debugger, and the repo was just reused for this release.</div><br/><div id="40605459" class="c"><input type="checkbox" id="c-40605459" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40604938">parent</a><span>|</span><a href="#40603240">next</a><span>|</span><label class="collapse" for="c-40605459">[-]</label><label class="expand" for="c-40605459">[1 more]</label></div><br/><div class="children"><div class="content">ha thank you Leo; i myself felt uneasy pointing out commit date based evidence and you just proved why.<p>mild followup question: any alpha to be gained from training the same SAEs on two different generations of GPT4, eg GPT4 on march 2023 vs june 2023 vintage, whatever is most architecturally comparable, and diffing them. what would be your priors on what you’d find?</div><br/></div></div></div></div><div id="40603240" class="c"><input type="checkbox" id="c-40603240" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40602943">parent</a><span>|</span><a href="#40604938">prev</a><span>|</span><a href="#40603940">next</a><span>|</span><label class="collapse" for="c-40603240">[-]</label><label class="expand" for="c-40603240">[2 more]</label></div><br/><div class="children"><div class="content">Visualizer was added 18 hours ago:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;sparse_autoencoder&#x2F;commit&#x2F;764586aedaeb469c3adea166859259a437b8353f">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;sparse_autoencoder&#x2F;commit&#x2F;764586ae...</a></div><br/><div id="40605480" class="c"><input type="checkbox" id="c-40605480" checked=""/><div class="controls bullet"><span class="by">pininja</span><span>|</span><a href="#40601640">root</a><span>|</span><a href="#40603240">parent</a><span>|</span><a href="#40603940">next</a><span>|</span><label class="collapse" for="c-40605480">[-]</label><label class="expand" for="c-40605480">[1 more]</label></div><br/><div class="children"><div class="content">It’s hard to believe it was written overnight.. this seems more like a public stable dump of what they’ve been working on without saying when they started. Some clues could come from looking at when all the deps it uses were released. They’re also calling this version 0.1.67, though I’m not sure that means anything either.</div><br/></div></div></div></div></div></div><div id="40603940" class="c"><input type="checkbox" id="c-40603940" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40602943">prev</a><span>|</span><a href="#40601850">next</a><span>|</span><label class="collapse" for="c-40603940">[-]</label><label class="expand" for="c-40603940">[1 more]</label></div><br/><div class="children"><div class="content">But even with current efforts so far, I don&#x27;t think we have an understanding of how&#x2F;why these emergent capabilities are formed. LLMs are still a black box as ever.</div><br/></div></div><div id="40601850" class="c"><input type="checkbox" id="c-40601850" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40603940">prev</a><span>|</span><a href="#40604141">next</a><span>|</span><label class="collapse" for="c-40601850">[-]</label><label class="expand" for="c-40601850">[1 more]</label></div><br/><div class="children"><div class="content">Both Leike and Sutskever are still credited in the post.</div><br/></div></div><div id="40604141" class="c"><input type="checkbox" id="c-40604141" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#40601640">parent</a><span>|</span><a href="#40601850">prev</a><span>|</span><a href="#40601668">next</a><span>|</span><label class="collapse" for="c-40604141">[-]</label><label class="expand" for="c-40604141">[1 more]</label></div><br/><div class="children"><div class="content">The Deep Visualization Toolbox from nearly 10 years ago is solid precedent for understanding deep models, albeit much smaller models than LLMs.  It’s hard to say OpenAI’s “visualization” released today is nearly as effective.  It could be that GPT-4 is much harder to instrument.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;yosinski&#x2F;deep-visualization-toolbox">https:&#x2F;&#x2F;github.com&#x2F;yosinski&#x2F;deep-visualization-toolbox</a></div><br/></div></div></div></div><div id="40601668" class="c"><input type="checkbox" id="c-40601668" checked=""/><div class="controls bullet"><span class="by">svieira</span><span>|</span><a href="#40601640">prev</a><span>|</span><a href="#40601264">next</a><span>|</span><label class="collapse" for="c-40601668">[-]</label><label class="expand" for="c-40601668">[5 more]</label></div><br/><div class="children"><div class="content">When one of the first examples is:<p>&gt; GPT-4 feature: ends of phrases related to price increases<p>and the 2&#x2F;5s of the responses don&#x27;t have any relation to <i>increase</i> at all:<p>&gt; Brent crude, fell 38 cents to $118.29 a barrel on the ICE Futures Exchange in London. The U.S. benchmark, West Texas Intermediate crude, was down 53 cents to $99.34 a barrel on the New York Mercantile Exchange. -- Ronald D. White Graphic: The AAA<p>and<p>&gt; ,115.18. The record reflects that appellant also included several hand-prepared invoices and employee pay slips, including an allegedly un-invoiced laundry ticket dated 29 June 2013 for 53 bags oflaundry weighing 478 pounds, which, at the contract price of $<p>I think I must be mis-understanding something. Why would this example (out of all the potential examples) be picked?</div><br/><div id="40601841" class="c"><input type="checkbox" id="c-40601841" checked=""/><div class="controls bullet"><span class="by">Metus</span><span>|</span><a href="#40601668">parent</a><span>|</span><a href="#40601264">next</a><span>|</span><label class="collapse" for="c-40601841">[-]</label><label class="expand" for="c-40601841">[4 more]</label></div><br/><div class="children"><div class="content">Notice that most of the examples have none of the green highlight counter, which is shown for<p>&gt; small losses. KEEPING SCORE: The Dow Jones industrial average rose 32 points, or 0.2 percent, to 18,156 as of 3:15 p.m. Eastern time. The Standard &amp; Poor’s ... OMAHA, Neb. (AP) — Warren Buffett’s company has bought nearly<p>the other sentences are in contrast to show how specific this neuron is.</div><br/><div id="40601940" class="c"><input type="checkbox" id="c-40601940" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#40601668">root</a><span>|</span><a href="#40601841">parent</a><span>|</span><a href="#40601864">next</a><span>|</span><label class="collapse" for="c-40601940">[-]</label><label class="expand" for="c-40601940">[2 more]</label></div><br/><div class="children"><div class="content">The highlights are better visible in this visualisation: <a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;sparse-autoencoder&#x2F;sae-viewer&#x2F;index.html#&#x2F;model&#x2F;gpt4&#x2F;family&#x2F;v5_latelayer_postmlp&#x2F;feature&#x2F;9260" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;sparse-autoencode...</a><p>There are also many top activations not showing increases, e.g.<p>&gt; 0.06 of a cent to 90.01 cents US.↵↵U.S. indexes were mainly lower as the Dow Jones industrials lost 21.72 points to 16,329.53, the Nasdaq was up 11.71 points at 4,318.9 and the S&amp;P 500<p>(Highlight on the first comma.)</div><br/></div></div><div id="40601864" class="c"><input type="checkbox" id="c-40601864" checked=""/><div class="controls bullet"><span class="by">svieira</span><span>|</span><a href="#40601668">root</a><span>|</span><a href="#40601841">parent</a><span>|</span><a href="#40601940">prev</a><span>|</span><a href="#40601264">next</a><span>|</span><label class="collapse" for="c-40601864">[-]</label><label class="expand" for="c-40601864">[1 more]</label></div><br/><div class="children"><div class="content">Ah, that makes a lot of sense, thank you!</div><br/></div></div></div></div></div></div><div id="40601264" class="c"><input type="checkbox" id="c-40601264" checked=""/><div class="controls bullet"><span class="by">OmarShehata</span><span>|</span><a href="#40601668">prev</a><span>|</span><a href="#40602967">next</a><span>|</span><label class="collapse" for="c-40601264">[-]</label><label class="expand" for="c-40601264">[2 more]</label></div><br/><div class="children"><div class="content">This is super cool, it feels like going in the direction of the &quot;deep&quot;&#x2F;high level type of semantic searching I&#x27;ve been waiting for. I like their examples of basically filtering documents for the &quot;concept&quot; of price increases, or even something as high level as a rhetorical question<p>I wonder how this compares to training&#x2F;fine tuning a model on examples of rhetorical questions and asking it to find it in a given document. This is maybe faster&#x2F;more accurate? Since it involves just looking at neural network activation, vs running it with input and having it generate an answer...?</div><br/><div id="40604395" class="c"><input type="checkbox" id="c-40604395" checked=""/><div class="controls bullet"><span class="by">f0e4c2f7</span><span>|</span><a href="#40601264">parent</a><span>|</span><a href="#40602967">next</a><span>|</span><label class="collapse" for="c-40604395">[-]</label><label class="expand" for="c-40604395">[1 more]</label></div><br/><div class="children"><div class="content">Exa is trying to do this. I&#x27;ve found some sort of interesting stuff this way but it honestly doesn&#x27;t feel quite good enough yet to me.<p><a href="https:&#x2F;&#x2F;exa.ai&#x2F;search?c=all">https:&#x2F;&#x2F;exa.ai&#x2F;search?c=all</a></div><br/></div></div></div></div><div id="40602967" class="c"><input type="checkbox" id="c-40602967" checked=""/><div class="controls bullet"><span class="by">aeonik</span><span>|</span><a href="#40601264">prev</a><span>|</span><a href="#40603314">next</a><span>|</span><label class="collapse" for="c-40602967">[-]</label><label class="expand" for="c-40602967">[1 more]</label></div><br/><div class="children"><div class="content">In their other examples, they have what looks to be a scientific explanation of reproductive anatomy classified as erotic content...<p>Here is the link to the concept [content warning]:  <a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;sparse-autoencoder&#x2F;sae-viewer&#x2F;index.html#&#x2F;model&#x2F;gpt2-small&#x2F;family&#x2F;v5_l8_postmlp&#x2F;num_features&#x2F;131072&#x2F;num_active_features&#x2F;32&#x2F;feature&#x2F;72185" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;sparse-autoencode...</a><p>DocID: 191632</div><br/></div></div><div id="40603314" class="c"><input type="checkbox" id="c-40603314" checked=""/><div class="controls bullet"><span class="by">adamiscool8</span><span>|</span><a href="#40602967">prev</a><span>|</span><a href="#40601600">next</a><span>|</span><label class="collapse" for="c-40603314">[-]</label><label class="expand" for="c-40603314">[2 more]</label></div><br/><div class="children"><div class="content">How does this compare to or improve on applying something like SHAP[0][1] on a model? The idea in the first line that &quot;we currently don&#x27;t understand how to make sense of the neural activity within language models.&quot; is..straight up false?<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;shap&#x2F;shap">https:&#x2F;&#x2F;github.com&#x2F;shap&#x2F;shap</a><p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shapley_value#In_machine_learning" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shapley_value#In_machine_learn...</a></div><br/><div id="40603348" class="c"><input type="checkbox" id="c-40603348" checked=""/><div class="controls bullet"><span class="by">szvsw</span><span>|</span><a href="#40603314">parent</a><span>|</span><a href="#40601600">next</a><span>|</span><label class="collapse" for="c-40603348">[-]</label><label class="expand" for="c-40603348">[1 more]</label></div><br/><div class="children"><div class="content">SHAP is pretty separate IMO. Shapley analysis is really a game theoretical methodology that is model agnostic and is only about determining how individual sections of the input contribute to a given prediction, not about how the model actually works internally to produce an output.<p>As long as you have a callable black box, you can compute Shapley values (or approximations); it does not speak to how or why the model actually works internally.</div><br/></div></div></div></div><div id="40601600" class="c"><input type="checkbox" id="c-40601600" checked=""/><div class="controls bullet"><span class="by">yismail</span><span>|</span><a href="#40603314">prev</a><span>|</span><a href="#40605961">next</a><span>|</span><label class="collapse" for="c-40601600">[-]</label><label class="expand" for="c-40601600">[8 more]</label></div><br/><div class="children"><div class="content">Interesting, reminds me of similar work Anthropic did on Claude 3 Sonnet [0].<p>[0] <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticity&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticit...</a></div><br/><div id="40602614" class="c"><input type="checkbox" id="c-40602614" checked=""/><div class="controls bullet"><span class="by">longdog</span><span>|</span><a href="#40601600">parent</a><span>|</span><a href="#40602563">next</a><span>|</span><label class="collapse" for="c-40602614">[-]</label><label class="expand" for="c-40602614">[2 more]</label></div><br/><div class="children"><div class="content">I feel the webpage strongly hints that sparse autoencoders were invented by OpenAI for this project.<p>Very weird that they don&#x27;t cite this in their webpage and instead bury the source in their paper.</div><br/><div id="40606168" class="c"><input type="checkbox" id="c-40606168" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#40601600">root</a><span>|</span><a href="#40602614">parent</a><span>|</span><a href="#40602563">next</a><span>|</span><label class="collapse" for="c-40606168">[-]</label><label class="expand" for="c-40606168">[1 more]</label></div><br/><div class="children"><div class="content">Nahhh, that&#x27;s the tried-and-true Apple approach to marketing, and OpenAI is well positioned to adopt it for themselves. They act like they invented transformers as much as Apple acts like they invented the smartphone.</div><br/></div></div></div></div><div id="40602563" class="c"><input type="checkbox" id="c-40602563" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#40601600">parent</a><span>|</span><a href="#40602614">prev</a><span>|</span><a href="#40602038">next</a><span>|</span><label class="collapse" for="c-40602563">[-]</label><label class="expand" for="c-40602563">[3 more]</label></div><br/><div class="children"><div class="content">The methods are the same, this is just OpenAI applying Anthropic&#x27;s research to their own model.</div><br/><div id="40606195" class="c"><input type="checkbox" id="c-40606195" checked=""/><div class="controls bullet"><span class="by">colah3</span><span>|</span><a href="#40601600">root</a><span>|</span><a href="#40602563">parent</a><span>|</span><a href="#40604965">next</a><span>|</span><label class="collapse" for="c-40606195">[-]</label><label class="expand" for="c-40606195">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m the research lead of Anthropic&#x27;s interpretability team. I&#x27;ve seen some comments like this one, which I worry downplay the importance of @leogao et al&#x27;s paper due to the similarity of ours. I think these comments are really undervaluing Gao et al&#x27;s work.<p>It&#x27;s not just that this is contemporaneous work (a project like this takes many months at the very least), but also that it introduces a number of novel contributions like TopK activations and new evaluations. It seems very possible that some of these innovations will be very important for this line of work going forward.<p>More generally, I think it&#x27;s really unfortunate when we don&#x27;t value contemporaneous work or replications. Prior to this paper, one could have imagined it being the case that sparse autoencoders worked on Claude due some idiosyncracy, but wouldn&#x27;t work on other frontier models for some reason. This paper can give us increased confidence that they work broadly, and that in itself is something to celebrate. It gives us a more stable foundation to build on.<p>I&#x27;m personally really grateful to all the authors of this paper for their work pushing sparse autoencoders and mechanistic interpretability forward.</div><br/></div></div><div id="40604965" class="c"><input type="checkbox" id="c-40604965" checked=""/><div class="controls bullet"><span class="by">leogao</span><span>|</span><a href="#40601600">root</a><span>|</span><a href="#40602563">parent</a><span>|</span><a href="#40606195">prev</a><span>|</span><a href="#40602038">next</a><span>|</span><label class="collapse" for="c-40604965">[-]</label><label class="expand" for="c-40604965">[1 more]</label></div><br/><div class="children"><div class="content">The paper introduces substantial improvements over the methodology in the Anthropic SAE paper, and the research was done concurrently.</div><br/></div></div></div></div><div id="40602038" class="c"><input type="checkbox" id="c-40602038" checked=""/><div class="controls bullet"><span class="by">ranman</span><span>|</span><a href="#40601600">parent</a><span>|</span><a href="#40602563">prev</a><span>|</span><a href="#40605961">next</a><span>|</span><label class="collapse" for="c-40602038">[-]</label><label class="expand" for="c-40602038">[2 more]</label></div><br/><div class="children"><div class="content">Someone mentioned that this took almost as much compute to train as the original model.</div><br/><div id="40602945" class="c"><input type="checkbox" id="c-40602945" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40601600">root</a><span>|</span><a href="#40602038">parent</a><span>|</span><a href="#40605961">next</a><span>|</span><label class="collapse" for="c-40602945">[-]</label><label class="expand" for="c-40602945">[1 more]</label></div><br/><div class="children"><div class="content">source please!</div><br/></div></div></div></div></div></div><div id="40605961" class="c"><input type="checkbox" id="c-40605961" checked=""/><div class="controls bullet"><span class="by">dazhbog</span><span>|</span><a href="#40601600">prev</a><span>|</span><a href="#40601621">next</a><span>|</span><label class="collapse" for="c-40605961">[-]</label><label class="expand" for="c-40605961">[1 more]</label></div><br/><div class="children"><div class="content">Is this like an fMRI for a neural net? We can see which regions light up depending on various topics..<p>I wonder if an assessment neural net can be plugged in to evaluate the regions that light up automatically.., just like when they had an AI reconstruct what the patient was looking at, from only fMRI scans!</div><br/></div></div><div id="40601621" class="c"><input type="checkbox" id="c-40601621" checked=""/><div class="controls bullet"><span class="by">obiefernandez</span><span>|</span><a href="#40605961">prev</a><span>|</span><a href="#40601975">next</a><span>|</span><label class="collapse" for="c-40601621">[-]</label><label class="expand" for="c-40601621">[10 more]</label></div><br/><div class="children"><div class="content">Can someone ELI5 the significance of this? (okay maybe not 5, but in basic language)</div><br/><div id="40601965" class="c"><input type="checkbox" id="c-40601965" checked=""/><div class="controls bullet"><span class="by">OtherShrezzing</span><span>|</span><a href="#40601621">parent</a><span>|</span><a href="#40602065">next</a><span>|</span><label class="collapse" for="c-40601965">[-]</label><label class="expand" for="c-40601965">[6 more]</label></div><br/><div class="children"><div class="content">LLM based AIs have lots of &quot;features&quot; which are kind of synonymous with &quot;concepts&quot; - these can be anything from `the concept of an apostrophe in the word don&#x27;t`, to `&quot;George Wash&quot; is usually followed by &quot;ington&quot; in the context of early American History`. Inside of the LLMs neural network, these are mapped to some circuitry-in-software-esque paths.<p>We don&#x27;t really have a good way of understanding how these features are generated inside of the LLMs or how their circuitry is activated when outputting them, or why the LLMs are following those circuits. Because of this, we don&#x27;t have any way to debug this component of an LLM - which makes them harder to improve. Similarly, if LLMs&#x2F;AIs ever get advanced enough, we&#x27;ll want to be able to identify if they&#x27;re being wilfully deceptive towards us, which we can&#x27;t currently do. For these reasons, we&#x27;d like to understand what is actually happening in the neural network to produce &amp; output concepts. This domain of research is usually referred to as &quot;interpretability&quot;.<p>OpenAI (and also DeepMind and Anthropic) have found a few ways to inspect the inner circuitry of the LLMs, and reveal a handful of these features. They do this by asking questions of the model, and then inspecting which parts of the LLM&#x27;s inner circuitry &quot;lights up&quot;. They then ablate (turn off) circuitry to see if those features become less frequently used in the AIs response as a verification step.<p>The graphs and highlighted words are visual representations of concepts that they are reasonably certain about - for example, the concept of the word &quot;AND&quot; linking two parts of a sentence together highlights the word &quot;AND&quot;.<p>Neel Nanda is the best source for this info if you&#x27;re interested in interpretability (IMO it&#x27;s the most interesting software problem out there at the moment), but note that his approach is different to OpenAI&#x27;s methodology discussed in the post: <a href="https:&#x2F;&#x2F;www.neelnanda.io&#x2F;mechanistic-interpretability" rel="nofollow">https:&#x2F;&#x2F;www.neelnanda.io&#x2F;mechanistic-interpretability</a></div><br/><div id="40602011" class="c"><input type="checkbox" id="c-40602011" checked=""/><div class="controls bullet"><span class="by">localfirst</span><span>|</span><a href="#40601621">root</a><span>|</span><a href="#40601965">parent</a><span>|</span><a href="#40602065">next</a><span>|</span><label class="collapse" for="c-40602011">[-]</label><label class="expand" for="c-40602011">[5 more]</label></div><br/><div class="children"><div class="content">hallucination solution?</div><br/><div id="40606649" class="c"><input type="checkbox" id="c-40606649" checked=""/><div class="controls bullet"><span class="by">irthomasthomas</span><span>|</span><a href="#40601621">root</a><span>|</span><a href="#40602011">parent</a><span>|</span><a href="#40602043">next</a><span>|</span><label class="collapse" for="c-40606649">[-]</label><label class="expand" for="c-40606649">[1 more]</label></div><br/><div class="children"><div class="content">Aye, but where only openai gets to decide what is hallucination.</div><br/></div></div><div id="40602043" class="c"><input type="checkbox" id="c-40602043" checked=""/><div class="controls bullet"><span class="by">OtherShrezzing</span><span>|</span><a href="#40601621">root</a><span>|</span><a href="#40602011">parent</a><span>|</span><a href="#40606649">prev</a><span>|</span><a href="#40602065">next</a><span>|</span><label class="collapse" for="c-40602043">[-]</label><label class="expand" for="c-40602043">[3 more]</label></div><br/><div class="children"><div class="content">Solving this problem would be a step on the way to debugging (and then resolving, or at least highlighting) hallucinations.</div><br/><div id="40603385" class="c"><input type="checkbox" id="c-40603385" checked=""/><div class="controls bullet"><span class="by">skywhopper</span><span>|</span><a href="#40601621">root</a><span>|</span><a href="#40602043">parent</a><span>|</span><a href="#40602065">next</a><span>|</span><label class="collapse" for="c-40603385">[-]</label><label class="expand" for="c-40603385">[2 more]</label></div><br/><div class="children"><div class="content">I’m skeptical that it could ever be possible to tell the difference between a hallucination and a “fact” in terms of what’s going on inside the model. Because hallucinations aren’t really a bug in the usual sense. Ie, there’s not some logic wrong or something misfiring.<p>Instead, it’s more appropriate to think of LLMs as <i>always</i> hallucinating. And sometimes that comes really close to reality because there’s a lot of reinforcement in the training data. And sometimes we humans infer meaning that isn’t there because that’s how humans work. And sometimes the leaps show clearly as “hallucinations” because the patterns the model is expressing don’t match the patterns that are meaningful to us. (Eg when they hallucinate strongly patterned things like URLs or academic citations, which don’t actually point to anything real. The model picked up the pattern of what such citations look like really well, but it didn’t and can’t make the leap to linking those patterns to reality.)<p>Not to mention that a lot of use cases for LLMs we actually <i>want</i> “hallucination”. Eg when we ask it to do any creative task or make up stories or jokes or songs or pictures. It’s only a hallucination in the wrong context.  But context is the main thing LLMs just don’t have.</div><br/><div id="40605430" class="c"><input type="checkbox" id="c-40605430" checked=""/><div class="controls bullet"><span class="by">cwalv</span><span>|</span><a href="#40601621">root</a><span>|</span><a href="#40603385">parent</a><span>|</span><a href="#40602065">next</a><span>|</span><label class="collapse" for="c-40605430">[-]</label><label class="expand" for="c-40605430">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Instead, it’s more appropriate to think of LLMs as always hallucinating.<p>That matches my mental model as well. To get rid of hallucinations, &quot;I don&#x27;t know&quot; would have to be an acceptable answer, and it would have to output that when &#x27;appropriate&#x27; ... Which, it doesn&#x27;t know (and to be fair, neither do we most of the time, without some way of checking&#x2F;validating(.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40602065" class="c"><input type="checkbox" id="c-40602065" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#40601621">parent</a><span>|</span><a href="#40601965">prev</a><span>|</span><a href="#40602070">next</a><span>|</span><label class="collapse" for="c-40602065">[-]</label><label class="expand" for="c-40602065">[1 more]</label></div><br/><div class="children"><div class="content">High-level concepts stored inside the large models (diffusion models, transformers etc) are normally hard to separate from each other, and the model is more or less a black box. A lot of research is put into obtaining the insight into what model knows. This is another advancement in this direction; it allows for easy separation of the concepts.<p>This can be used to analyze the knowledge inside the model, and potentially modify (add, erase, change the importance) certain concepts without affecting unrelated ones. The precision achievable with the particular technique is always in question though, and some concepts are just too close to separate from each other, so it&#x27;s probably not perfect.</div><br/></div></div><div id="40602070" class="c"><input type="checkbox" id="c-40602070" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40601621">parent</a><span>|</span><a href="#40602065">prev</a><span>|</span><a href="#40601854">next</a><span>|</span><label class="collapse" for="c-40602070">[-]</label><label class="expand" for="c-40602070">[1 more]</label></div><br/><div class="children"><div class="content">In general this is just copying work done by Anthropic, so there&#x27;s nothing fundamentally new here.<p>What they have done here is to identify patterns internal to GPT-4 that correspond to specific identifiable concepts. The work was done my OpenAI&#x27;s mostly dismantled safety team (it has the names of this teams recently departed co-leads Ilya &amp; Jan Leike on it), so this is nominally being done for safety reasons to be able to boost or suppress specific concepts from being activated when the model is running, such as Anthropic&#x27;s demonstration of boosting their models fixation on the Golden Gate bridge:<p><a href="https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;golden-gate-claude" rel="nofollow">https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;golden-gate-claude</a><p>This kind of work would also seem to have potential functional uses as well as safety ones, given that it allow you to control the model in specific ways.</div><br/></div></div><div id="40601854" class="c"><input type="checkbox" id="c-40601854" checked=""/><div class="controls bullet"><span class="by">93po</span><span>|</span><a href="#40601621">parent</a><span>|</span><a href="#40602070">prev</a><span>|</span><a href="#40601975">next</a><span>|</span><label class="collapse" for="c-40601854">[-]</label><label class="expand" for="c-40601854">[1 more]</label></div><br/><div class="children"><div class="content">from chatgpt itself: The article discusses how researchers use sparse autoencoders to identify and interpret key features within complex language models like GPT-4, making their inner workings more understandable. This advancement helps improve AI safety and reliability by breaking down the models&#x27; decision-making processes into simpler, human-interpretable parts.</div><br/></div></div></div></div><div id="40601975" class="c"><input type="checkbox" id="c-40601975" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#40601621">prev</a><span>|</span><a href="#40605903">next</a><span>|</span><label class="collapse" for="c-40601975">[-]</label><label class="expand" for="c-40601975">[2 more]</label></div><br/><div class="children"><div class="content">Does this mean that it could be a good practice to release the auto encoder that was trained on a neural network to explain its outputs? Like all open models in hugging face could have this as a useful accompaniment?</div><br/><div id="40603609" class="c"><input type="checkbox" id="c-40603609" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#40601975">parent</a><span>|</span><a href="#40605903">next</a><span>|</span><label class="collapse" for="c-40603609">[-]</label><label class="expand" for="c-40603609">[1 more]</label></div><br/><div class="children"><div class="content">I imagine such an encoder would be specific to a model.</div><br/></div></div></div></div><div id="40605903" class="c"><input type="checkbox" id="c-40605903" checked=""/><div class="controls bullet"><span class="by">ndricca</span><span>|</span><a href="#40601975">prev</a><span>|</span><a href="#40601917">next</a><span>|</span><label class="collapse" for="c-40605903">[-]</label><label class="expand" for="c-40605903">[1 more]</label></div><br/><div class="children"><div class="content">Can this in some way be related with sparse embeddings (see Splade for example?) and if yes can it be used for hybrid search?</div><br/></div></div><div id="40601917" class="c"><input type="checkbox" id="c-40601917" checked=""/><div class="controls bullet"><span class="by">andy12_</span><span>|</span><a href="#40605903">prev</a><span>|</span><a href="#40605428">next</a><span>|</span><label class="collapse" for="c-40601917">[-]</label><label class="expand" for="c-40601917">[4 more]</label></div><br/><div class="children"><div class="content">a.k.a, the same work as Anthropic, but with less interpretable and interesting features. I guess there won&#x27;t be Golden Gate[0] GPT anytime soon.<p>I mean, you just have to compare the couple of interesting features of the OpenAI feature browser [1] and the features of the Anthropic feature browser [2].<p>[0] <a href="https:&#x2F;&#x2F;twitter.com&#x2F;AnthropicAI&#x2F;status&#x2F;1793741051867615494" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;AnthropicAI&#x2F;status&#x2F;1793741051867615494</a><p>[1] <a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;sparse-autoencoder&#x2F;sae-viewer&#x2F;index.html#&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;sparse-autoencode...</a><p>[2] <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticity&#x2F;features&#x2F;index.html?featureId=1M_22623" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticit...</a></div><br/><div id="40604992" class="c"><input type="checkbox" id="c-40604992" checked=""/><div class="controls bullet"><span class="by">leogao</span><span>|</span><a href="#40601917">parent</a><span>|</span><a href="#40606123">next</a><span>|</span><label class="collapse" for="c-40604992">[-]</label><label class="expand" for="c-40604992">[2 more]</label></div><br/><div class="children"><div class="content">Note that we focus on random positive activations, which are less susceptible to interpretability illusions than top activations (but also look less impressive as a result). We also provide access to random uncherrypicked features, whereas Anthropic does not. We made these choices deliberately to give as accurate an impression of autoencoder feature quality as possible.<p>Also note that GPT-4 is a more powerful model than Sonnet, which makes it harder to train autoencoders with the same quality features.</div><br/></div></div><div id="40606123" class="c"><input type="checkbox" id="c-40606123" checked=""/><div class="controls bullet"><span class="by">justanotherjoe</span><span>|</span><a href="#40601917">parent</a><span>|</span><a href="#40604992">prev</a><span>|</span><a href="#40605428">next</a><span>|</span><label class="collapse" for="c-40606123">[-]</label><label class="expand" for="c-40606123">[1 more]</label></div><br/><div class="children"><div class="content">yeah this one is much less presentable than Anthropic&#x27;s work.  It sure looks bad on them to be compared so poorly like this.</div><br/></div></div></div></div><div id="40605428" class="c"><input type="checkbox" id="c-40605428" checked=""/><div class="controls bullet"><span class="by">kovezd</span><span>|</span><a href="#40601917">prev</a><span>|</span><a href="#40605136">next</a><span>|</span><label class="collapse" for="c-40605428">[-]</label><label class="expand" for="c-40605428">[1 more]</label></div><br/><div class="children"><div class="content">What applications do you think this opens up?<p>One of the first ones I can think of is pairing it up with Browser extensions to increase productivity of knowledge workers. Think of sales people quickly assessing the viability of leads, or simply reducing noise on social media as a B2C app.</div><br/></div></div><div id="40605136" class="c"><input type="checkbox" id="c-40605136" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#40605428">prev</a><span>|</span><a href="#40601840">next</a><span>|</span><label class="collapse" for="c-40605136">[-]</label><label class="expand" for="c-40605136">[2 more]</label></div><br/><div class="children"><div class="content">Is anyone else weirded out that they do not acknowledge Anthropic at all?</div><br/><div id="40605228" class="c"><input type="checkbox" id="c-40605228" checked=""/><div class="controls bullet"><span class="by">leogao</span><span>|</span><a href="#40605136">parent</a><span>|</span><a href="#40601840">next</a><span>|</span><label class="collapse" for="c-40605228">[-]</label><label class="expand" for="c-40605228">[1 more]</label></div><br/><div class="children"><div class="content">The paper cites Anthropic&#x27;s work extensively.</div><br/></div></div></div></div><div id="40601840" class="c"><input type="checkbox" id="c-40601840" checked=""/><div class="controls bullet"><span class="by">Shoop</span><span>|</span><a href="#40605136">prev</a><span>|</span><a href="#40603957">next</a><span>|</span><label class="collapse" for="c-40601840">[-]</label><label class="expand" for="c-40601840">[1 more]</label></div><br/><div class="children"><div class="content">Can anyone summarize the major differences between this and Scaling Monosemanticity?</div><br/></div></div><div id="40603957" class="c"><input type="checkbox" id="c-40603957" checked=""/><div class="controls bullet"><span class="by">josu</span><span>|</span><a href="#40601840">prev</a><span>|</span><a href="#40601420">next</a><span>|</span><label class="collapse" for="c-40603957">[-]</label><label class="expand" for="c-40603957">[2 more]</label></div><br/><div class="children"><div class="content">Totally off topic, but I love that the external visualisations are hosted on windows.net!<p><a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;sparse-autoencoder&#x2F;sae-viewer&#x2F;index.html#&#x2F;model&#x2F;gpt4&#x2F;family&#x2F;v5_latelayer_postmlp&#x2F;feature&#x2F;63541" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;sparse-autoencode...</a></div><br/><div id="40603991" class="c"><input type="checkbox" id="c-40603991" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#40603957">parent</a><span>|</span><a href="#40601420">next</a><span>|</span><label class="collapse" for="c-40603991">[-]</label><label class="expand" for="c-40603991">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s static content hosted in the Azure blob storage.</div><br/></div></div></div></div><div id="40601856" class="c"><input type="checkbox" id="c-40601856" checked=""/><div class="controls bullet"><span class="by">calibas</span><span>|</span><a href="#40601420">prev</a><span>|</span><a href="#40604205">next</a><span>|</span><label class="collapse" for="c-40601856">[-]</label><label class="expand" for="c-40601856">[6 more]</label></div><br/><div class="children"><div class="content">I want to be able to view exactly how my input is translated into tokens, as well as the embeddings for the tokens.</div><br/><div id="40601880" class="c"><input type="checkbox" id="c-40601880" checked=""/><div class="controls bullet"><span class="by">franzb</span><span>|</span><a href="#40601856">parent</a><span>|</span><a href="#40604205">next</a><span>|</span><label class="collapse" for="c-40601880">[-]</label><label class="expand" for="c-40601880">[5 more]</label></div><br/><div class="children"><div class="content">For your first question: <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;tokenizer" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;tokenizer</a></div><br/><div id="40601911" class="c"><input type="checkbox" id="c-40601911" checked=""/><div class="controls bullet"><span class="by">calibas</span><span>|</span><a href="#40601856">root</a><span>|</span><a href="#40601880">parent</a><span>|</span><a href="#40604205">next</a><span>|</span><label class="collapse" for="c-40601911">[-]</label><label class="expand" for="c-40601911">[4 more]</label></div><br/><div class="children"><div class="content">I saw that, but the language makes me think it&#x27;s not quite the same as what&#x27;s really being used?<p>&quot;how a piece of text <i>might</i> be tokenized by a language model&quot;<p>&quot;It&#x27;s important to note that the exact tokenization process varies between models.&quot;</div><br/><div id="40601961" class="c"><input type="checkbox" id="c-40601961" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#40601856">root</a><span>|</span><a href="#40601911">parent</a><span>|</span><a href="#40604205">next</a><span>|</span><label class="collapse" for="c-40601961">[-]</label><label class="expand" for="c-40601961">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why they have buttons to choose which model&#x27;s tokenizer to use.</div><br/><div id="40602521" class="c"><input type="checkbox" id="c-40602521" checked=""/><div class="controls bullet"><span class="by">calibas</span><span>|</span><a href="#40601856">root</a><span>|</span><a href="#40601961">parent</a><span>|</span><a href="#40604205">next</a><span>|</span><label class="collapse" for="c-40602521">[-]</label><label class="expand" for="c-40602521">[2 more]</label></div><br/><div class="children"><div class="content">Yes, thank you, I understand that part.<p>It&#x27;s the <i>might</i> condition in the description that makes me think the results might not be the exact same as what&#x27;s used in the live models.</div><br/><div id="40604299" class="c"><input type="checkbox" id="c-40604299" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#40601856">root</a><span>|</span><a href="#40602521">parent</a><span>|</span><a href="#40604205">next</a><span>|</span><label class="collapse" for="c-40604299">[-]</label><label class="expand" for="c-40604299">[1 more]</label></div><br/><div class="children"><div class="content">The results are the same.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40604205" class="c"><input type="checkbox" id="c-40604205" checked=""/><div class="controls bullet"><span class="by">jackallis</span><span>|</span><a href="#40601856">prev</a><span>|</span><a href="#40602178">next</a><span>|</span><label class="collapse" for="c-40604205">[-]</label><label class="expand" for="c-40604205">[2 more]</label></div><br/><div class="children"><div class="content">&quot;We currently don&#x27;t understand how to make sense of the neural activity within language models&quot; this is why peopl are up-in-arms.</div><br/><div id="40604243" class="c"><input type="checkbox" id="c-40604243" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#40604205">parent</a><span>|</span><a href="#40602178">next</a><span>|</span><label class="collapse" for="c-40604243">[-]</label><label class="expand" for="c-40604243">[1 more]</label></div><br/><div class="children"><div class="content">Up in arms for what reason? That neural networks are perfectly interpretable? That&#x27;s the nature of huge amorphous deep networks. These feature extraction forays are a good step forward though.</div><br/></div></div></div></div><div id="40602178" class="c"><input type="checkbox" id="c-40602178" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#40604205">prev</a><span>|</span><a href="#40605069">next</a><span>|</span><label class="collapse" for="c-40602178">[-]</label><label class="expand" for="c-40602178">[7 more]</label></div><br/><div class="children"><div class="content">This is interesting:<p>&gt; Autoencoder family<p>&gt; Note: Only 65536 features available. Activations shown on The Pile (uncopyrighted) instead of our internal training dataset.<p>So, the Pile is uncopyrighted, but the internal training dataset is copyrighted? Copyrighted by whom?<p>Huh?</div><br/><div id="40602252" class="c"><input type="checkbox" id="c-40602252" checked=""/><div class="controls bullet"><span class="by">Arcsech</span><span>|</span><a href="#40602178">parent</a><span>|</span><a href="#40602914">next</a><span>|</span><label class="collapse" for="c-40602252">[-]</label><label class="expand" for="c-40602252">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Copyrighted by whom?<p>By people who would get angry if they could definitively prove their stuff was in OpenAI&#x27;s training set.</div><br/></div></div><div id="40602914" class="c"><input type="checkbox" id="c-40602914" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40602178">parent</a><span>|</span><a href="#40602252">prev</a><span>|</span><a href="#40602436">next</a><span>|</span><label class="collapse" for="c-40602914">[-]</label><label class="expand" for="c-40602914">[3 more]</label></div><br/><div class="children"><div class="content">Hehe, related to this, <i>someone</i> created a &quot;book4&quot; dataset and put it on torrent websites. I don&#x27;t think it&#x27;s being used in any major LLMs, but the future &quot;piracy&quot; community intersection with AI is going to be exciting.<p>Watching the cyberpunk world that all of my favorite literature predicted slowly come to our world is fun indeed.</div><br/><div id="40602951" class="c"><input type="checkbox" id="c-40602951" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40602178">root</a><span>|</span><a href="#40602914">parent</a><span>|</span><a href="#40602436">next</a><span>|</span><label class="collapse" for="c-40602951">[-]</label><label class="expand" for="c-40602951">[2 more]</label></div><br/><div class="children"><div class="content">i think you mean @sillysaurus&#x27; books3? not books4?</div><br/><div id="40606351" class="c"><input type="checkbox" id="c-40606351" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#40602178">root</a><span>|</span><a href="#40602951">parent</a><span>|</span><a href="#40602436">next</a><span>|</span><label class="collapse" for="c-40606351">[-]</label><label class="expand" for="c-40606351">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40405443">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40405443</a></div><br/></div></div></div></div></div></div><div id="40602436" class="c"><input type="checkbox" id="c-40602436" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40602178">parent</a><span>|</span><a href="#40602914">prev</a><span>|</span><a href="#40605069">next</a><span>|</span><label class="collapse" for="c-40602436">[-]</label><label class="expand" for="c-40602436">[2 more]</label></div><br/><div class="children"><div class="content">Basically everyone. You, and me, and Elon Musk, and EMPRESS, and my uncle who works for Nintendo. They&#x27;re just hoping that AI training legally ignores copyright.</div><br/><div id="40603488" class="c"><input type="checkbox" id="c-40603488" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#40602178">root</a><span>|</span><a href="#40602436">parent</a><span>|</span><a href="#40605069">next</a><span>|</span><label class="collapse" for="c-40603488">[-]</label><label class="expand" for="c-40603488">[1 more]</label></div><br/><div class="children"><div class="content">When you can ask an AI for an entire book with no errors in the output… god that would be a huge token model</div><br/></div></div></div></div></div></div><div id="40605069" class="c"><input type="checkbox" id="c-40605069" checked=""/><div class="controls bullet"><span class="by">paul7986</span><span>|</span><a href="#40602178">prev</a><span>|</span><a href="#40602201">next</a><span>|</span><label class="collapse" for="c-40605069">[-]</label><label class="expand" for="c-40605069">[1 more]</label></div><br/><div class="children"><div class="content">Curious what interesting ways have you used chatGPT ... yesterday i used it to test my pool levels (took a pic of my pool strip i had laying around and told it the brand of pool strip).</div><br/></div></div><div id="40602201" class="c"><input type="checkbox" id="c-40602201" checked=""/><div class="controls bullet"><span class="by">russellbeattie</span><span>|</span><a href="#40605069">prev</a><span>|</span><a href="#40601298">next</a><span>|</span><label class="collapse" for="c-40602201">[-]</label><label class="expand" for="c-40602201">[1 more]</label></div><br/><div class="children"><div class="content">One feature I expect we&#x27;ll get from this sort of research is identifying &quot;hot spots&quot; that are used during inference. Like virtual machines, these could be cached in whole or in part and used to both speed up the response time and reduce computation cycles needed.</div><br/></div></div><div id="40601298" class="c"><input type="checkbox" id="c-40601298" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#40602201">prev</a><span>|</span><a href="#40601275">next</a><span>|</span><label class="collapse" for="c-40601298">[-]</label><label class="expand" for="c-40601298">[6 more]</label></div><br/><div class="children"><div class="content">The worrying part is that first concept in the doc they show&#x2F;found is &quot;human imperfection&quot;. Hope this is just coincidence..</div><br/><div id="40601884" class="c"><input type="checkbox" id="c-40601884" checked=""/><div class="controls bullet"><span class="by">calibas</span><span>|</span><a href="#40601298">parent</a><span>|</span><a href="#40601327">next</a><span>|</span><label class="collapse" for="c-40601884">[-]</label><label class="expand" for="c-40601884">[2 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s done on purpose, it&#x27;s related to a very important point when understanding AI.<p>Humans aren&#x27;t perfect, AI is trained by humans, therefore...</div><br/><div id="40602244" class="c"><input type="checkbox" id="c-40602244" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#40601298">root</a><span>|</span><a href="#40601884">parent</a><span>|</span><a href="#40601327">next</a><span>|</span><label class="collapse" for="c-40602244">[-]</label><label class="expand" for="c-40602244">[1 more]</label></div><br/><div class="children"><div class="content">I think the point of the doc is that &quot;human imperfection&quot; is very prominent concept in trained LLM..</div><br/></div></div></div></div><div id="40601327" class="c"><input type="checkbox" id="c-40601327" checked=""/><div class="controls bullet"><span class="by">jackphilson</span><span>|</span><a href="#40601298">parent</a><span>|</span><a href="#40601884">prev</a><span>|</span><a href="#40601536">next</a><span>|</span><label class="collapse" for="c-40601327">[-]</label><label class="expand" for="c-40601327">[1 more]</label></div><br/><div class="children"><div class="content">I think its safety implications</div><br/></div></div><div id="40601354" class="c"><input type="checkbox" id="c-40601354" checked=""/><div class="controls bullet"><span class="by">thelastparadise</span><span>|</span><a href="#40601298">parent</a><span>|</span><a href="#40601536">prev</a><span>|</span><a href="#40601275">next</a><span>|</span><label class="collapse" for="c-40601354">[-]</label><label class="expand" for="c-40601354">[1 more]</label></div><br/><div class="children"><div class="content">Spooky!</div><br/></div></div></div></div></div></div></div></div></div></body></html>