<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730710881540" as="style"/><link rel="stylesheet" href="styles.css?v=1730710881540"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://si.inc/hertz-dev/">Hertz-dev, the first open-source base model for conversational audio</a> <span class="domain">(<a href="https://si.inc">si.inc</a>)</span></div><div class="subtext"><span>mnk47</span> | <span>21 comments</span></div><br/><div><div id="42039253" class="c"><input type="checkbox" id="c-42039253" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#42039322">next</a><span>|</span><label class="collapse" for="c-42039253">[-]</label><label class="expand" for="c-42039253">[1 more]</label></div><br/><div class="children"><div class="content">This is really cool. FWIW, existing open-source TTS engines are <i>really</i> bad in comparison to what you have here: I know this is voice-to-voice, but I think there&#x27;d be a lot of appetite to get this to also be multimodal and accept text (essentially making it a really good TTS model, in addition to a great voice-to-voice model).<p>I suppose someone could hack their way around the problem by finetuning it to essentially replay Piper (or whatever) output, only with more natural prosody and intonation. And then have the text LLM pipe to Piper, and Piper pipe to Hertz-dev. But it would be pretty useful to have it accept text natively!</div><br/></div></div><div id="42039322" class="c"><input type="checkbox" id="c-42039322" checked=""/><div class="controls bullet"><span class="by">blixt</span><span>|</span><a href="#42039253">prev</a><span>|</span><a href="#42038434">next</a><span>|</span><label class="collapse" for="c-42039322">[-]</label><label class="expand" for="c-42039322">[3 more]</label></div><br/><div class="children"><div class="content">They say Hertz is first of its kind but Moshi is another duplex audio model from earlier this year that seems to perform similarly (and it runs on a MacBook):
<a href="https:&#x2F;&#x2F;github.com&#x2F;kyutai-labs&#x2F;moshi">https:&#x2F;&#x2F;github.com&#x2F;kyutai-labs&#x2F;moshi</a></div><br/><div id="42039684" class="c"><input type="checkbox" id="c-42039684" checked=""/><div class="controls bullet"><span class="by">a2128</span><span>|</span><a href="#42039322">parent</a><span>|</span><a href="#42039359">next</a><span>|</span><label class="collapse" for="c-42039684">[-]</label><label class="expand" for="c-42039684">[1 more]</label></div><br/><div class="children"><div class="content">Moshi never released the base model, only two conversationally finetuned models. They also never released training code except for the codec. Though I don&#x27;t see any training code for Hertz either, just 3 inference notebooks, and model code full of no_grad. No paper either to help me understand how this was trained and what the architecture is like. So I&#x27;m not too sure about researcher-friendliness unless I&#x27;m missing something.</div><br/></div></div><div id="42039359" class="c"><input type="checkbox" id="c-42039359" checked=""/><div class="controls bullet"><span class="by">nicholas-cc</span><span>|</span><a href="#42039322">parent</a><span>|</span><a href="#42039684">prev</a><span>|</span><a href="#42038434">next</a><span>|</span><label class="collapse" for="c-42039359">[-]</label><label class="expand" for="c-42039359">[1 more]</label></div><br/><div class="children"><div class="content">Moshi is a good model to build chat applications on, this is designed to be more of a proper base model with all the quirkiness, naturalness, and researcher-friendliness of base modeling.</div><br/></div></div></div></div><div id="42038434" class="c"><input type="checkbox" id="c-42038434" checked=""/><div class="controls bullet"><span class="by">wwwlouishinofun</span><span>|</span><a href="#42039322">prev</a><span>|</span><a href="#42038204">next</a><span>|</span><label class="collapse" for="c-42038434">[-]</label><label class="expand" for="c-42038434">[6 more]</label></div><br/><div class="children"><div class="content">Tesla’s approach to pure vision-based autonomous driving—temporarily setting aside lidar and other sensors—seems designed to make this technology more accessible and scalable. By focusing on a vision-only model, they can accelerate adoption and gather large datasets for quicker iterations. Once the vision-based system reaches a mature stage, I imagine Tesla might reintegrate additional sensor data, like lidar or radar, to refine their autonomous driving suite, making it even more robust and closer to perfection.<p>Additionally, I’ve been exploring an idea about voice interaction systems. Currently, most voice interactions are processed by converting voice input into text, generating a text-based response, and then turning this text back into audio. But what if we could train the system to respond directly in voice, without involving text at all? If developed to maturity, this model could produce responses that feel more natural and spontaneous, possibly diverging from traditional text-to-speech outputs. Natural speech has unique syntax and rhythm, not to mention dialect and tone variations, which could make a purely voice-trained system fascinating and more human-like.<p>Could you let me know if your current voice interaction model follows the standard speech-to-text-to-speech process, or if there is exploration in voice-to-voice processing?</div><br/><div id="42038567" class="c"><input type="checkbox" id="c-42038567" checked=""/><div class="controls bullet"><span class="by">nicholas-cc</span><span>|</span><a href="#42038434">parent</a><span>|</span><a href="#42038204">next</a><span>|</span><label class="collapse" for="c-42038567">[-]</label><label class="expand" for="c-42038567">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m one of the devs. Our model is fully voice-to-voice, no text was involved in the making of hertz-dev for exactly this reason.</div><br/><div id="42038672" class="c"><input type="checkbox" id="c-42038672" checked=""/><div class="controls bullet"><span class="by">oidar</span><span>|</span><a href="#42038434">root</a><span>|</span><a href="#42038567">parent</a><span>|</span><a href="#42038204">next</a><span>|</span><label class="collapse" for="c-42038672">[-]</label><label class="expand" for="c-42038672">[4 more]</label></div><br/><div class="children"><div class="content">So essentially this is voice input to voice output? Can you change gender&#x2F;age&#x2F;accent? Does it track prosodic information? I&#x27;ve been waiting for something like this.</div><br/><div id="42038734" class="c"><input type="checkbox" id="c-42038734" checked=""/><div class="controls bullet"><span class="by">nicholas-cc</span><span>|</span><a href="#42038434">root</a><span>|</span><a href="#42038672">parent</a><span>|</span><a href="#42038204">next</a><span>|</span><label class="collapse" for="c-42038734">[-]</label><label class="expand" for="c-42038734">[3 more]</label></div><br/><div class="children"><div class="content">Hertz-dev is a base model, meaning it&#x27;s just trained to predict the next token of audio. If your prompt is an old male voice with a British accent, the model will most likely continue speaking in an old male voice with a British accent. Being a base model, hertz-dev is easily finetunable for specific tasks - it would be a simple change to add manual configurations for the gender&#x2F;age&#x2F;accent.</div><br/><div id="42039246" class="c"><input type="checkbox" id="c-42039246" checked=""/><div class="controls bullet"><span class="by">hunter2_</span><span>|</span><a href="#42038434">root</a><span>|</span><a href="#42038734">parent</a><span>|</span><a href="#42039398">next</a><span>|</span><label class="collapse" for="c-42039246">[-]</label><label class="expand" for="c-42039246">[1 more]</label></div><br/><div class="children"><div class="content">I assume this mirroring is due to symmetry being more typical than not among the training data, and if instead trained with contrived diversity (e.g., males only conversing with females) then the output of the base model would follow suit without pulling any levers?<p>It&#x27;s interesting to think about what complete diversity (i.e., no tendencies toward homogeneous conversation partners whatsoever among training data) would yield, given that it&#x27;s trying to deliver whatever is most probable.</div><br/></div></div><div id="42039398" class="c"><input type="checkbox" id="c-42039398" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42038434">root</a><span>|</span><a href="#42038734">parent</a><span>|</span><a href="#42039246">prev</a><span>|</span><a href="#42038204">next</a><span>|</span><label class="collapse" for="c-42039398">[-]</label><label class="expand" for="c-42039398">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m interested to hear more detail about approaches to adding manual controls for speaker characteristics or emotion or other things you might want to vary. What techniques do you have in mind?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42038204" class="c"><input type="checkbox" id="c-42038204" checked=""/><div class="controls bullet"><span class="by">BrandiATMuhkuh</span><span>|</span><a href="#42038434">prev</a><span>|</span><a href="#42039418">next</a><span>|</span><label class="collapse" for="c-42038204">[-]</label><label class="expand" for="c-42038204">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s really cool.
I&#x27;m currently exploring VUI (Voice User Interface) and this might come in handy.<p>I might be a bit biased (did my PhD exploring how VUI can persuade humans), but I think VUI is &quot;the future&quot; of computer interaction. 
If it&#x27;s not the future, than at least it adds a new group of people (kids + elderly people) as potential users.</div><br/></div></div><div id="42039418" class="c"><input type="checkbox" id="c-42039418" checked=""/><div class="controls bullet"><span class="by">xarope</span><span>|</span><a href="#42038204">prev</a><span>|</span><a href="#42037752">next</a><span>|</span><label class="collapse" for="c-42039418">[-]</label><label class="expand" for="c-42039418">[1 more]</label></div><br/><div class="children"><div class="content">the One-channel generation seems to be speaking gibberish english.  I&#x27;m not sure what it is supposed to represent?<p>And is the interactive generation just doing an ELIZA?  i.e. &quot;P: tell us about how AI will be interesting&quot;, &quot;A: Yeah AI will, yeah, be interesting&quot;.</div><br/></div></div><div id="42037752" class="c"><input type="checkbox" id="c-42037752" checked=""/><div class="controls bullet"><span class="by">lordofgibbons</span><span>|</span><a href="#42039418">prev</a><span>|</span><a href="#42036996">next</a><span>|</span><label class="collapse" for="c-42037752">[-]</label><label class="expand" for="c-42037752">[1 more]</label></div><br/><div class="children"><div class="content">Can it effectively be used as a TTS model?</div><br/></div></div><div id="42036996" class="c"><input type="checkbox" id="c-42036996" checked=""/><div class="controls bullet"><span class="by">mnk47</span><span>|</span><a href="#42037752">prev</a><span>|</span><a href="#42038951">next</a><span>|</span><label class="collapse" for="c-42036996">[-]</label><label class="expand" for="c-42036996">[1 more]</label></div><br/><div class="children"><div class="content">Repo: <a href="https:&#x2F;&#x2F;github.com&#x2F;Standard-Intelligence&#x2F;hertz-dev">https:&#x2F;&#x2F;github.com&#x2F;Standard-Intelligence&#x2F;hertz-dev</a></div><br/></div></div><div id="42038951" class="c"><input type="checkbox" id="c-42038951" checked=""/><div class="controls bullet"><span class="by">awinter-py</span><span>|</span><a href="#42036996">prev</a><span>|</span><a href="#42037446">next</a><span>|</span><label class="collapse" for="c-42038951">[-]</label><label class="expand" for="c-42038951">[2 more]</label></div><br/><div class="children"><div class="content">what is up with the first sample? and&#x2F;or am I having a stroke</div><br/><div id="42039428" class="c"><input type="checkbox" id="c-42039428" checked=""/><div class="controls bullet"><span class="by">spuz</span><span>|</span><a href="#42038951">parent</a><span>|</span><a href="#42037446">next</a><span>|</span><label class="collapse" for="c-42039428">[-]</label><label class="expand" for="c-42039428">[1 more]</label></div><br/><div class="children"><div class="content">Pay attention to the given prompt length in the examples. The first 2 seconds of the first example is a real human speaking. Everything after is generated by the model. It produces what almost sounds like real human speech mimicking the voice of the input but it&#x27;s currently at a level of something like GPT-2 in terms of meaningful words.</div><br/></div></div></div></div><div id="42037446" class="c"><input type="checkbox" id="c-42037446" checked=""/><div class="controls bullet"><span class="by">wg0</span><span>|</span><a href="#42038951">prev</a><span>|</span><a href="#42038918">next</a><span>|</span><label class="collapse" for="c-42037446">[-]</label><label class="expand" for="c-42037446">[1 more]</label></div><br/><div class="children"><div class="content">So it is kind of LLM but audio LLM where prompt is audio and generated output is audio too?</div><br/></div></div><div id="42038918" class="c"><input type="checkbox" id="c-42038918" checked=""/><div class="controls bullet"><span class="by">Dawny33</span><span>|</span><a href="#42037446">prev</a><span>|</span><a href="#42039284">next</a><span>|</span><label class="collapse" for="c-42038918">[-]</label><label class="expand" for="c-42038918">[2 more]</label></div><br/><div class="children"><div class="content">Congrats, team.<p>Does Hertz support multi-lingual audio right now?</div><br/><div id="42039309" class="c"><input type="checkbox" id="c-42039309" checked=""/><div class="controls bullet"><span class="by">nicholas-cc</span><span>|</span><a href="#42038918">parent</a><span>|</span><a href="#42039284">next</a><span>|</span><label class="collapse" for="c-42039309">[-]</label><label class="expand" for="c-42039309">[1 more]</label></div><br/><div class="children"><div class="content">Yes</div><br/></div></div></div></div><div id="42039284" class="c"><input type="checkbox" id="c-42039284" checked=""/><div class="controls bullet"><span class="by">blixt</span><span>|</span><a href="#42038918">prev</a><span>|</span><label class="collapse" for="c-42039284">[-]</label><label class="expand" for="c-42039284">[1 more]</label></div><br/><div class="children"><div class="content">Gotta say I was confused for a second but yeah apparently si.inc and ssi.inc are the domains for two different AGI companies and I can only assume it’s intentional?</div><br/></div></div></div></div></div></div></div></body></html>