<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735549258664" as="style"/><link rel="stylesheet" href="styles.css?v=1735549258664"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://mill-build.org/blog/3-selective-testing.html">Faster CI with Selective Testing</a> <span class="domain">(<a href="https://mill-build.org">mill-build.org</a>)</span></div><div class="subtext"><span>lihaoyi</span> | <span>35 comments</span></div><br/><div><div id="42547406" class="c"><input type="checkbox" id="c-42547406" checked=""/><div class="controls bullet"><span class="by">ay</span><span>|</span><a href="#42547317">next</a><span>|</span><label class="collapse" for="c-42547406">[-]</label><label class="expand" for="c-42547406">[1 more]</label></div><br/><div class="children"><div class="content">The better you can describe the interdependencies between the components, the more chances the selective approaches have.<p>However, often if you knew about a given dependency, you might have avoided bugs in the first place!<p>A simple scenario to illustrate what I have in mind: a system with two plugins, A and B. They provide completely independent functionality, and are otherwise entirely unrelated.<p>Plugin A adds a new function which allocates a large amount of memory. All tests for A pass. All tests for B pass. The tests for B when A is loaded fail.<p>Turns out A has done two things:<p>1) the new memory allocation together with the existing memory allocations in B causes an OOM when both are used.<p>2) the new function addition offset the function table in the main program, and the plugin B was relying on a haedcoded function index, which didn’t change, by sheer chance, for years.<p>Those are the the tales based on the real world experience, which made me abandon the idea for the project I am working on (VPP) - the trade offs didn’t seem to be worth it. For some other scenario they may be different though, so thanks for looking into this issue!</div><br/></div></div><div id="42547317" class="c"><input type="checkbox" id="c-42547317" checked=""/><div class="controls bullet"><span class="by">turboponyy</span><span>|</span><a href="#42547406">prev</a><span>|</span><a href="#42546323">next</a><span>|</span><label class="collapse" for="c-42547317">[-]</label><label class="expand" for="c-42547317">[1 more]</label></div><br/><div class="children"><div class="content">Nix solves this as a byproduct (as it does with many things) of its design. You can have your tests be a &quot;build&quot;, where the build succeeds if your tests pass. Any builds can be cached, which means you&#x27;re essentially caching tests you&#x27;ve already run. Since Nix is deterministic, you never have to rerun any tests until anything about them that could change the evaluation changes.</div><br/></div></div><div id="42546323" class="c"><input type="checkbox" id="c-42546323" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#42547317">prev</a><span>|</span><a href="#42547041">next</a><span>|</span><label class="collapse" for="c-42546323">[-]</label><label class="expand" for="c-42546323">[7 more]</label></div><br/><div class="children"><div class="content">Path-based selection of tests, in every single CI system I have ever seen it implemented in, is wrong. TFA thankfully gets the &quot;run the downstream dependent tests&quot; which is the biggest miss, but even then, <i>you can get the wrong answer</i>. Say I have the following: paths a&#x2F;* need to run tests A, and paths b&#x2F;* need to runs tests B. I configure the CI system as such. Commit A&#x27; is pushed, changing paths under a&#x2F;* only, so the CI runs tests A, only. The tests fail. A separate engineer, quickly afterwards (perhaps, lets say, even before the prior commit has finished; this could just be two quick merges) pushes commit B&#x27;, changing paths under b&#x2F;* only. The CI system runs tests B only, and they all pass <i>so the commit is marked green incorrectly.</i> Automated deployment systems, or other engineers, proceed to see passing tests, <i>and use a broken but &quot;green&quot; build</i>.<p>Since ever rarely do I see the downstream-tests requirement correctly done, and almost always the &quot;successive miss&quot; bug, I&#x27;m convinced for these reasons path based is just basically a broken approach. I think a better approach is to a.) compute your inputs to the tests, and cache results, and b.) have the CI system be able to suss out flakes and non-determinism. But I think a.) is actually quite difficult (enough to be one of the hardest problems in CS, remember?) and b.) is not at all well supported by the current CI systems.<p>(I have seen both of these approaches result in bad (known bad, had the tests run!) builds pushed to production. The common complaint is &quot;but CI is <i>slow</i>&quot; followed by a need to do <i>something</i>, but without care towards the correctness of that something. Responsibility for a slow CI is often diffused across the whole company, managers do not want to do the hard task of getting their engineers to fix the tests they&#x27;re responsible for, since that just doesn&#x27;t get a promotion. So CI remains slow, and brittle.)</div><br/><div id="42546517" class="c"><input type="checkbox" id="c-42546517" checked=""/><div class="controls bullet"><span class="by">emidln</span><span>|</span><a href="#42546323">parent</a><span>|</span><a href="#42547041">next</a><span>|</span><label class="collapse" for="c-42546517">[-]</label><label class="expand" for="c-42546517">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible to use bazel to do this. You need to be very explicit (The Bazel Way(tm)), but in exchange, you can ask the graph for everything that is an rdep of a given file. This isn&#x27;t always necessary in bazel (if you avoid weird integration targets, deploy targets, etc) where `bazel test &#x2F;&#x2F;...` generally does this by default anyway. It&#x27;s sometimes necessary to manually express due to incomplete graphs, tests that are not executed every time (for non-determinism, execution cost, etc), and a few other reasons but at least it&#x27;s possible.</div><br/><div id="42546741" class="c"><input type="checkbox" id="c-42546741" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#42546323">root</a><span>|</span><a href="#42546517">parent</a><span>|</span><a href="#42547041">next</a><span>|</span><label class="collapse" for="c-42546741">[-]</label><label class="expand" for="c-42546741">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, bazel is sort of the exception that proves the rule, here. I wish it did not have such an absurd learning curve; I&#x27;ve found it next to impossible to get started with it.</div><br/><div id="42547299" class="c"><input type="checkbox" id="c-42547299" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42546323">root</a><span>|</span><a href="#42546741">parent</a><span>|</span><a href="#42546919">next</a><span>|</span><label class="collapse" for="c-42547299">[-]</label><label class="expand" for="c-42547299">[2 more]</label></div><br/><div class="children"><div class="content">Bazel is weird. I work at Google, and it&#x27;s fundamental to our engineering, and I love it for that, but to me it&#x27;s an internal tool. Nearly all Bazel code I work with is internal (most open source uses of it I see aren&#x27;t doing enough customisation to be worth it in my opinion), it integrates into so many parts of our engineering workflow, and uses so many internal services like our build farm.<p>I&#x27;m not sure I see much point in using it outside of Google. Maybe if you&#x27;re a company within 1&#x2F;10th the size and have a lot of Xooglers? It seems like the sort of thing where most companies don&#x27;t need it and therefore shouldn&#x27;t use it, and those that do need it probably need to build their own that&#x27;s right for them.</div><br/><div id="42547507" class="c"><input type="checkbox" id="c-42547507" checked=""/><div class="controls bullet"><span class="by">emidln</span><span>|</span><a href="#42546323">root</a><span>|</span><a href="#42547299">parent</a><span>|</span><a href="#42546919">next</a><span>|</span><label class="collapse" for="c-42547507">[-]</label><label class="expand" for="c-42547507">[1 more]</label></div><br/><div class="children"><div class="content">It worked well enough for my last company. It does require a team to teach it and to do the heavy lifting on custom rules and tooling. I&#x27;d rather do that than worry about whether my c++ that was exposing me to millions&#x2F;billions of risk might have skipped some tests in our haste for an intraday release.</div><br/></div></div></div></div><div id="42546919" class="c"><input type="checkbox" id="c-42546919" checked=""/><div class="controls bullet"><span class="by">pianoben</span><span>|</span><a href="#42546323">root</a><span>|</span><a href="#42546741">parent</a><span>|</span><a href="#42547299">prev</a><span>|</span><a href="#42547041">next</a><span>|</span><label class="collapse" for="c-42546919">[-]</label><label class="expand" for="c-42546919">[2 more]</label></div><br/><div class="children"><div class="content">Not only is it hard to get started with Bazel, it&#x27;s hard to <i>continue</i> with Bazel even once you do.  Unless you are using Google practices (say, vendoring and compiling all dependencies), you will certainly end up mired in poorly-maintained third-party tools that will cause no end of fun side-quests.<p>My work became a lot more, ahem, <i>linear</i>, once I moved us off of Bazel.</div><br/><div id="42547376" class="c"><input type="checkbox" id="c-42547376" checked=""/><div class="controls bullet"><span class="by">Maxious</span><span>|</span><a href="#42546323">root</a><span>|</span><a href="#42546919">parent</a><span>|</span><a href="#42547041">next</a><span>|</span><label class="collapse" for="c-42547376">[-]</label><label class="expand" for="c-42547376">[1 more]</label></div><br/><div class="children"><div class="content">Eg. 1 in 10 open source projects abandon bazel within 2 years. <a href="https:&#x2F;&#x2F;blogsystem5.substack.com&#x2F;p&#x2F;bazelcon-2024-recap" rel="nofollow">https:&#x2F;&#x2F;blogsystem5.substack.com&#x2F;p&#x2F;bazelcon-2024-recap</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="42547041" class="c"><input type="checkbox" id="c-42547041" checked=""/><div class="controls bullet"><span class="by">motorest</span><span>|</span><a href="#42546323">prev</a><span>|</span><a href="#42546874">next</a><span>|</span><label class="collapse" for="c-42547041">[-]</label><label class="expand" for="c-42547041">[16 more]</label></div><br/><div class="children"><div class="content">The premise of this article sounds an awful lot like a solution desperately searching for a problem.<p>The scenario used to drive this idea is a slippery slope fallacy that tests can take over an hour to run after years. That&#x27;s rather dubious, but still this leaves out the fact that tests can be run in parallel. In fact, that is also a necessary condition of selective testing. So why bother with introducing with yet more complexity?<p>To make matters worse if your project grows so large that your hypothetical tests take over an hour to run, it sounds like the project would already be broken down into modules. That, alone, already allows tests to only run if a specific part of the project run.<p>So it&#x27;s clear that test run time is not a problem that justifies throwing complexity to solve it. Excluding the test runtime argument, is there anything at all that justifies this?</div><br/><div id="42547662" class="c"><input type="checkbox" id="c-42547662" checked=""/><div class="controls bullet"><span class="by">atq2119</span><span>|</span><a href="#42547041">parent</a><span>|</span><a href="#42547463">next</a><span>|</span><label class="collapse" for="c-42547662">[-]</label><label class="expand" for="c-42547662">[1 more]</label></div><br/><div class="children"><div class="content">&gt; To make matters worse if your project grows so large that your hypothetical tests take over an hour to run, it sounds like the project would already be broken down into modules.<p>Story time so that you can revisit your assumptions.<p>Imagine your product is a graphics driver. Graphics APIs have extensive test suites with millions of individual tests. Running them serially typically takes many hours, depending on the target hardware.<p>But over the years you invariably also run across bugs exposed by real applications that the conformance suites don&#x27;t catch. So, you also accumulate additional tests, some of them distilled versions of those triggers, some of them captured frames with known good &quot;golden&quot; output pictures. Those add further to the test runtime.<p>Then, mostly due to performance pressures, there are many options that can affect the precise details of how the driver runs. Many of them are heuristics auto-tuned, but the conformance suite is unlikely to hit all the heuristic cases, so really you should also run all your tests with overrides for the heuristic. Now you have a combinatorial explosion that means the space of tests you really ought to run is at least in the quadrillions.<p>It&#x27;s simply infeasible to run all tests on every PR, so what tends to happen in practice is that a manually curated subset of tests is run on every commit, and then more thorough testing happens on various asynchronous schedules (e.g. on release branches, daily on the development branch).<p>I&#x27;m not convinced that the article is <i>the</i> solution, but it could be part of one. New ideas in this space are certainly welcome.</div><br/></div></div><div id="42547463" class="c"><input type="checkbox" id="c-42547463" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#42547041">parent</a><span>|</span><a href="#42547662">prev</a><span>|</span><a href="#42547119">next</a><span>|</span><label class="collapse" for="c-42547463">[-]</label><label class="expand" for="c-42547463">[1 more]</label></div><br/><div class="children"><div class="content">How does modules solve this problem? If you changed Module A, don&#x27;t you need to also test every component that depends on that module to account for any possible regression?<p>&gt; The scenario used to drive this idea is a slippery slope fallacy that tests can take over an hour to run after years.<p>It doesn&#x27;t really take much for a project to grow to one hour for tests to run. If you have a CI&#x2F;CD pipeline that executes tests on every commit and you have 30-40 commits daily, that&#x27;s 30-40 hours.<p>Still, I&#x27;d rather setup a full 2 machines to run the tests than rather have to deal with selective testing. It might make sense if you have hundreds of commits per day.</div><br/></div></div><div id="42547119" class="c"><input type="checkbox" id="c-42547119" checked=""/><div class="controls bullet"><span class="by">lihaoyi</span><span>|</span><a href="#42547041">parent</a><span>|</span><a href="#42547463">prev</a><span>|</span><a href="#42547080">next</a><span>|</span><label class="collapse" for="c-42547119">[-]</label><label class="expand" for="c-42547119">[8 more]</label></div><br/><div class="children"><div class="content">&gt; So it&#x27;s clear that test run time is not a problem that justifies throwing complexity to solve it.<p>There&#x27;s a lot I can respond to in this post, but I think the bottom line is: if you have not experienced the problem, count your blessings.<p>Lots of places do face these problems, with test suites that take hours or days to run if not parallelized. And while parallelization reduces latency, it does not reduce costs, and test suites taking 10, 20, or 50USD every time you update a pull request are not uncommon either<p>If you never hit these scenarios, just know that many others are not so lucky</div><br/><div id="42547156" class="c"><input type="checkbox" id="c-42547156" checked=""/><div class="controls bullet"><span class="by">motorest</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547119">parent</a><span>|</span><a href="#42547080">next</a><span>|</span><label class="collapse" for="c-42547156">[-]</label><label class="expand" for="c-42547156">[7 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s a lot I can respond to in this post, but I think the bottom line is: if you have not experienced the problem, count your blessings.<p>You don&#x27;t even specify what problem is it. Again, this is a solution searching for a problem, and one you can&#x27;t even describe.<p>&gt; Lots of places do face these problems, with test suites that take hours or days to run if not parallelized.<p>What do you mean &quot;if not parallelized&quot;? Are you running tests sequentially and then complaining about how long they take to run?<p>I won&#x27;t even touch on the red flag which is the apparent lack of modularization.<p>&gt; And while parallelization reduces latency, it does not reduce costs, and test suites taking 10, 20, or 50USD every time you update a pull request are not uncommon either<p>Please explain exactly how you managed to put together a test suite that costs up to 50€ to run.<p>I assure you the list of problems and red flags you state along the way will never even feature selective testing as a factor or as a solution.</div><br/><div id="42547220" class="c"><input type="checkbox" id="c-42547220" checked=""/><div class="controls bullet"><span class="by">TypingOutBugs</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547156">parent</a><span>|</span><a href="#42547475">next</a><span>|</span><label class="collapse" for="c-42547220">[-]</label><label class="expand" for="c-42547220">[4 more]</label></div><br/><div class="children"><div class="content">&gt; What do you mean &quot;if not parallelized&quot;? Are you running tests sequentially and then complaining about how long they take to run?<p>Some CI runners are single core unless you pay more, some tests are very hard to parallelise (integration, E2E), some test code bases are large and unwieldy and would require a lot of cost to improve to allow parallelisation.<p>&gt; Please explain exactly how you managed to put together a test suite that costs up to 50€ to run.<p>If you need to run a hefty windows runner on GitHub it’s $0.54 per minute, so a 90 minute full suite will cost $50.<p>You could be better with different tests running at PR&#x2F;Nightly but if quality is a known issue some orgs will push to run total test coverage each PR (saw this at a large finance company).</div><br/><div id="42547238" class="c"><input type="checkbox" id="c-42547238" checked=""/><div class="controls bullet"><span class="by">lihaoyi</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547220">parent</a><span>|</span><a href="#42547345">next</a><span>|</span><label class="collapse" for="c-42547238">[-]</label><label class="expand" for="c-42547238">[1 more]</label></div><br/><div class="children"><div class="content">Notably Github Actions runners are about 4x more expensive than AWS on demand, and windows is also more expensive than Linux.<p>50USD gets you about 960 linux core-hours on AWS. A lot more than 90 minutes on 1 box on Windows&#x2F;GHA, but not so much that someone running a lot of unit&#x2F;integration&#x2F;e2e tests in a large codebase won&#x27;t be able to use</div><br/></div></div><div id="42547345" class="c"><input type="checkbox" id="c-42547345" checked=""/><div class="controls bullet"><span class="by">motorest</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547220">parent</a><span>|</span><a href="#42547238">prev</a><span>|</span><a href="#42547475">next</a><span>|</span><label class="collapse" for="c-42547345">[-]</label><label class="expand" for="c-42547345">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Some CI runners are single core unless you pay more<p>I don&#x27;t know which hypothetical CI runner you have in mind. Back in reality, most CICD services bill you on time spent running tests, not &quot;core&quot;. Even core count is reflected as runtime multpliers. Moreover, services like GitHub not only offer a baseline of minutes per month, and on top of that provide support for self-hosted runners that cost you nothing.<p>&gt; some tests are very hard to parallelise (integration, E2E)<p>No, that&#x27;s simply false and fundamentally wrong. You can run any test on any scenario independently of any other test from another scenario. You need to go way out of your way to screw up a test suite so badly you have dependencies between tests covering different sets of features.<p>&gt; If you need to run a hefty windows runner on GitHub it’s $0.54 per minute, so a 90 minute full suite will cost $50.<p>This argument just proves you&#x27;re desperately grasping at straws.<p>The only Windows test runner from GitHub that costs that is their largest, most expensive runner: Windows 64-core runner. Do you need a Threadripper to use the app you&#x27;re testing?<p>Even so, that hypothetical cost is only factored in if you run out of minutes from your plan, and you certainly do not order Windows 64-core runners from your free tier plan.<p>Even in your hypothetical cost-conscious scenario, GitHub actions do support self-hosted runners, which cost zero per minute.<p>&gt; You could be better with different tests running at PR&#x2F;Nightly (...)<p>None of the hypothetical scenarios you fabricated pose a concern. Even assuming you need a Windows machine with 64 processor cores to do an E2E test run of your app, the most basic intro to automated testing tutorial and test pyramids mentions how these tests can and often run on deployments to pre-prod and prod stages. This means that anyone following the most basic intro tutorial on the topic will manage to sidestep any of your hypothetical scenarios by gating auto promotions to prod.</div><br/><div id="42547439" class="c"><input type="checkbox" id="c-42547439" checked=""/><div class="controls bullet"><span class="by">TypingOutBugs</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547345">parent</a><span>|</span><a href="#42547475">next</a><span>|</span><label class="collapse" for="c-42547439">[-]</label><label class="expand" for="c-42547439">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t know which hypothetical CI runner you have in mind. Back in reality, most CICD services bill you on time spent running tests, not &quot;core&quot;. Even core count is reflected as runtime multpliers.<p>If you’re charging a runtime multiplier per core then there’s a cost per core. Included minutes on most runners are limited to basic versions with limited cores. Try xdist pytest on a default GitHub runner and get any speed up…<p>&gt; Moreover, services like GitHub not only offer a baseline of minutes per month, and on top of that provide support for self-hosted runners that cost you nothing.<p>Except the cost of the hardware, sysadmin time for setting up and supporting ephemeral runners, monitoring, etc…<p>&gt; No, that&#x27;s simply false and fundamentally wrong. You can run any test on any scenario independently of any other test from another scenario.<p>In end to end system tests if you have 100 hitting at the same time how do you guarantee the underlying state is the same so the tests are idempotent?<p>Also another example, I set up testing pipelines for an OS that ran in an FPGA in a HIL CI test. I had three of these due to operating costs. How could I parallelise tests that required flashing firmware AND have the most pipelines running as possible?</div><br/></div></div></div></div></div></div><div id="42547475" class="c"><input type="checkbox" id="c-42547475" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547156">parent</a><span>|</span><a href="#42547220">prev</a><span>|</span><a href="#42547199">next</a><span>|</span><label class="collapse" for="c-42547475">[-]</label><label class="expand" for="c-42547475">[1 more]</label></div><br/><div class="children"><div class="content">I can see tests running 50$ if you are running them on Github compute which is x10-x15 times more expensive than AWS. Still, even at 2-3$ that&#x27;s still quite expensive. Maybe if you have a really large low-level build. I have a really small one and it consumes roughly 10 minutes on Github.</div><br/></div></div><div id="42547199" class="c"><input type="checkbox" id="c-42547199" checked=""/><div class="controls bullet"><span class="by">mhlakhani</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547156">parent</a><span>|</span><a href="#42547475">prev</a><span>|</span><a href="#42547080">next</a><span>|</span><label class="collapse" for="c-42547199">[-]</label><label class="expand" for="c-42547199">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Please explain exactly how you managed to put together a test suite that costs up to 50€ to run.<p>I&#x27;m not OP but have worked with them: have you considered a repo that might have tents of thousands of committers over decades? It&#x27;s very easy to just have an insane amount of tests.</div><br/></div></div></div></div></div></div><div id="42547080" class="c"><input type="checkbox" id="c-42547080" checked=""/><div class="controls bullet"><span class="by">mhlakhani</span><span>|</span><a href="#42547041">parent</a><span>|</span><a href="#42547119">prev</a><span>|</span><a href="#42546874">next</a><span>|</span><label class="collapse" for="c-42547080">[-]</label><label class="expand" for="c-42547080">[5 more]</label></div><br/><div class="children"><div class="content">In large mono-repos, like this one is presumably targeting, running all tests in the repo for a given PR would take years (maybe even decades&#x2F;centuries) of compute time. You have to do <i>some</i> level of test selection, and there are full time engineers who just work on optimizing this.<p>The test runtime argument is the main one IMO.<p>(source: while I did not work on this at a prior job, I worked closely with the team that did this work).</div><br/><div id="42547101" class="c"><input type="checkbox" id="c-42547101" checked=""/><div class="controls bullet"><span class="by">motorest</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547080">parent</a><span>|</span><a href="#42546874">next</a><span>|</span><label class="collapse" for="c-42547101">[-]</label><label class="expand" for="c-42547101">[4 more]</label></div><br/><div class="children"><div class="content">&gt; In large mono-repos, like this one is presumably targeting, running all tests in the repo for a given PR would take years (maybe even decades&#x2F;centuries) of compute time.<p>No, not really. That&#x27;s a silly example. Your assertion makes as much sense as arguing that your monorepo would take years to build because all code is tracked by the same repo. It doesn&#x27;t it, does it? Why?<p>How you store your code has nothing to do with how you build it.</div><br/><div id="42547146" class="c"><input type="checkbox" id="c-42547146" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547101">parent</a><span>|</span><a href="#42546874">next</a><span>|</span><label class="collapse" for="c-42547146">[-]</label><label class="expand" for="c-42547146">[3 more]</label></div><br/><div class="children"><div class="content">Being a monorepo is typically about more than just how code is stored, it leads to very different practices about dependency management, building, etc. On the monorepo I work on, the fact it is a monorepo is intrinsically linked to the build, testing, and deployment processes in many ways.<p>The ideal is that your build system by-necessity contains the data to be able to selectively test – typically the case if you&#x27;re linking code in some way. You import a library, now your tests get run if that library changes. As the article suggests, this breaks down over service boundaries, but as you suggest, you still hopefully have modules you can link up.<p>The problem is when you have hundreds of services, maintaining those dependencies manually could be hard. When you have thousands it may be nearly impossible. When you have hundreds of thousands it may be impossible. I think applying ML to that problem so that you can incrementally understand the ever changing dependencies across services.<p>I can also assure you that however smart the build system is, there will always be spooky action at a distance between components.</div><br/><div id="42547193" class="c"><input type="checkbox" id="c-42547193" checked=""/><div class="controls bullet"><span class="by">mhlakhani</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547146">parent</a><span>|</span><a href="#42546874">next</a><span>|</span><label class="collapse" for="c-42547193">[-]</label><label class="expand" for="c-42547193">[2 more]</label></div><br/><div class="children"><div class="content">yeah. I am talking about repos with hundreds of thousands (probably millions) of tests. Here&#x27;s just one example of the scale: <a href="https:&#x2F;&#x2F;engineering.fb.com&#x2F;2018&#x2F;11&#x2F;21&#x2F;developer-tools&#x2F;predictive-test-selection&#x2F;" rel="nofollow">https:&#x2F;&#x2F;engineering.fb.com&#x2F;2018&#x2F;11&#x2F;21&#x2F;developer-tools&#x2F;predic...</a><p>if you change a low level library that&#x27;s the equivalent of the C++ standard library and you want to test the changes, you effectively have to rebuild the world. And you don&#x27;t want to.</div><br/><div id="42547252" class="c"><input type="checkbox" id="c-42547252" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42547041">root</a><span>|</span><a href="#42547193">parent</a><span>|</span><a href="#42546874">next</a><span>|</span><label class="collapse" for="c-42547252">[-]</label><label class="expand" for="c-42547252">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, or when &quot;one&quot; build rule connecting a service to another is actually a hundred deeply nested build rules doing a lot more work, and every one of those code paths would need to correctly convey whether a dependency is required at the test level or not.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42546874" class="c"><input type="checkbox" id="c-42546874" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#42547041">prev</a><span>|</span><a href="#42546167">next</a><span>|</span><label class="collapse" for="c-42546874">[-]</label><label class="expand" for="c-42546874">[1 more]</label></div><br/><div class="children"><div class="content">When your CI is too big, do at least a random selection. So you&#x27;ll catch bugs at least as somewhen. With selective testing you just ignore them</div><br/></div></div><div id="42546167" class="c"><input type="checkbox" id="c-42546167" checked=""/><div class="controls bullet"><span class="by">reynaldi</span><span>|</span><a href="#42546874">prev</a><span>|</span><a href="#42544985">next</a><span>|</span><label class="collapse" for="c-42546167">[-]</label><label class="expand" for="c-42546167">[4 more]</label></div><br/><div class="children"><div class="content">Interesting, I just now know about selective testing after reading this post. But, now I wonder when do you do selective testing and when do you just break up the codebase?</div><br/><div id="42546701" class="c"><input type="checkbox" id="c-42546701" checked=""/><div class="controls bullet"><span class="by">lihaoyi</span><span>|</span><a href="#42546167">parent</a><span>|</span><a href="#42547078">next</a><span>|</span><label class="collapse" for="c-42546701">[-]</label><label class="expand" for="c-42546701">[2 more]</label></div><br/><div class="children"><div class="content">Even if you break up a codebase, you need selective testing.<p>Let&#x27;s say you have 100 small repos, and make a change to one. How confident are you in the changed repo&#x27;s test suite that you can guarantee there are no bugs that will affect other related repos? If not, which other repos do you test with your change to get that confidence?</div><br/><div id="42547542" class="c"><input type="checkbox" id="c-42547542" checked=""/><div class="controls bullet"><span class="by">menaerus</span><span>|</span><a href="#42546167">root</a><span>|</span><a href="#42546701">parent</a><span>|</span><a href="#42547078">next</a><span>|</span><label class="collapse" for="c-42547542">[-]</label><label class="expand" for="c-42547542">[1 more]</label></div><br/><div class="children"><div class="content">Splitting the code into 10s or 100s repositories is a cancer. It&#x27;s almost as if you&#x27;re intentionally trying to make your dev life miserable by pretending that you speeded up the dev turnaround times by not running the full test circle.</div><br/></div></div></div></div><div id="42547078" class="c"><input type="checkbox" id="c-42547078" checked=""/><div class="controls bullet"><span class="by">atmosx</span><span>|</span><a href="#42546167">parent</a><span>|</span><a href="#42546701">prev</a><span>|</span><a href="#42544985">next</a><span>|</span><label class="collapse" for="c-42547078">[-]</label><label class="expand" for="c-42547078">[1 more]</label></div><br/><div class="children"><div class="content">You break up the codebase when the communication patterns in the org calls for it e.g. engineers complain about it.<p>It’s not a purely technical problem.</div><br/></div></div></div></div><div id="42544985" class="c"><input type="checkbox" id="c-42544985" checked=""/><div class="controls bullet"><span class="by">brunoarueira</span><span>|</span><a href="#42546167">prev</a><span>|</span><a href="#42545954">next</a><span>|</span><label class="collapse" for="c-42544985">[-]</label><label class="expand" for="c-42544985">[1 more]</label></div><br/><div class="children"><div class="content">On my last job, since the project is based on Ruby on Rails, we implementei this <a href="https:&#x2F;&#x2F;github.com&#x2F;toptal&#x2F;crystalball">https:&#x2F;&#x2F;github.com&#x2F;toptal&#x2F;crystalball</a> and additional modifications based on the gitlab setup. After that, the pull requests test suite runs pretty fast</div><br/></div></div><div id="42545954" class="c"><input type="checkbox" id="c-42545954" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#42544985">prev</a><span>|</span><a href="#42546384">next</a><span>|</span><label class="collapse" for="c-42545954">[-]</label><label class="expand" for="c-42545954">[1 more]</label></div><br/><div class="children"><div class="content">Years ago there was a system that would track code coverage per test and would rerun only the tests that intersected with the lines you changed., as a more specific watch command.<p>But watch itself still has a lot of value, because it starts in the gap between when you save changes and get curious about the state of the tests.</div><br/></div></div><div id="42546384" class="c"><input type="checkbox" id="c-42546384" checked=""/><div class="controls bullet"><span class="by">pluto_modadic</span><span>|</span><a href="#42545954">prev</a><span>|</span><a href="#42544873">next</a><span>|</span><label class="collapse" for="c-42546384">[-]</label><label class="expand" for="c-42546384">[1 more]</label></div><br/><div class="children"><div class="content">I think... an approach that might work... is testing the immediate unit first (fastest, guess by changed files), then dependent tests you didn&#x27;t test yet (e.g., guessing based on imports), then integration tests that use the feature, then full tests (regardless), then fuzzing...<p>order the test by what is likely to fail quickest. you could even have deployed it during the dependent tests (and run the full and fuzz tests overnight), and have it report back that &quot;no, the commit actually didn&#x27;t pass&quot;.<p>or, bonus points, if you just made a new test case as part of a bugfix or regression ticket, it should definitely test that test unit you just modified first. any commit in a tests&#x2F; folder or `test_`.<p>I mean... maybe it should be running on your laptop...</div><br/></div></div><div id="42544873" class="c"><input type="checkbox" id="c-42544873" checked=""/><div class="controls bullet"><span class="by">dshacker</span><span>|</span><a href="#42546384">prev</a><span>|</span><label class="collapse" for="c-42544873">[-]</label><label class="expand" for="c-42544873">[1 more]</label></div><br/><div class="children"><div class="content">Hah, funnily enough this is exactly what I was working on this year :)</div><br/></div></div></div></div></div></div></div></body></html>