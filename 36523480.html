<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688115646420" as="style"/><link rel="stylesheet" href="styles.css?v=1688115646420"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://martinfowler.com/articles/building-boba.html">Building Boba AI: Lessons learnt in building an LLM-powered application</a> <span class="domain">(<a href="https://martinfowler.com">martinfowler.com</a>)</span></div><div class="subtext"><span>nalgeon</span> | <span>55 comments</span></div><br/><div><div id="36527762" class="c"><input type="checkbox" id="c-36527762" checked=""/><div class="controls bullet"><span class="by">mvdtnz</span><span>|</span><a href="#36524113">next</a><span>|</span><label class="collapse" for="c-36527762">[-]</label><label class="expand" for="c-36527762">[19 more]</label></div><br/><div class="children"><div class="content">I am so despondent at the lack of creativity in most of the (many, many) LLM powered projects that are popping up. I have seen hardly a single thing that goes beyond &quot;it&#x27;s a chat bot, but with a special prompt&quot;. Like, is this the best we can expect from this supposedly ground-breaking technology?</div><br/><div id="36528430" class="c"><input type="checkbox" id="c-36528430" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36529741">next</a><span>|</span><label class="collapse" for="c-36528430">[-]</label><label class="expand" for="c-36528430">[2 more]</label></div><br/><div class="children"><div class="content">I agree. I wrote a book on LangChain and LlamaIndex [1] and was initially very enthusiastic about possible applications. However, most of what I now do is just writing simple scripts to interact with my data. I feel like my “lines of code per month” metrics are at an all time low. I wanted a local chat interface that worked with all the books I have written, and various local PDFs that I have collected on the semantic web and other technologies. I ended up with two Python scripts that use a new library embedchain (which uses LangChain) that total 30 lines of code. So easy to do this stuff, that I am not so sure about the idea of using products from the new flood of LLM startups. All of this does require either using the OpenAI APIs, or the Hugging Face APIs, or renting something like a Lambda Labs GPU server and running a 33B model (if you use FastChat, you get an OpenAI compatible API).<p>I would urge companies and people to build their own stuff because there is so much value in learning the tech. For off the shelf LLM tools, it is hard to beat OpenAI’s web app, Microsoft Bing+ChatGPT and Office 365, and Google’s beta Bard integrations with Google Docs, etc.<p>[1] <a href="https:&#x2F;&#x2F;leanpub.com&#x2F;langchain" rel="nofollow noreferrer">https:&#x2F;&#x2F;leanpub.com&#x2F;langchain</a></div><br/><div id="36530917" class="c"><input type="checkbox" id="c-36530917" checked=""/><div class="controls bullet"><span class="by">ekabod</span><span>|</span><a href="#36527762">root</a><span>|</span><a href="#36528430">parent</a><span>|</span><a href="#36529741">next</a><span>|</span><label class="collapse" for="c-36530917">[-]</label><label class="expand" for="c-36530917">[1 more]</label></div><br/><div class="children"><div class="content">&gt;[1] <a href="https:&#x2F;&#x2F;leanpub.com&#x2F;langchain" rel="nofollow noreferrer">https:&#x2F;&#x2F;leanpub.com&#x2F;langchain</a><p>Too short to call it a book.</div><br/></div></div></div></div><div id="36529741" class="c"><input type="checkbox" id="c-36529741" checked=""/><div class="controls bullet"><span class="by">carlossouza</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36528430">prev</a><span>|</span><a href="#36528389">next</a><span>|</span><label class="collapse" for="c-36529741">[-]</label><label class="expand" for="c-36529741">[1 more]</label></div><br/><div class="children"><div class="content">Remember that the most downloaded app in Apple Store’s first year was this infantile thing called iBeer… it actually hit the mark of $20k of sales&#x2F;day<p>This was an extreme bizarre example, but most mobile apps from the first wave  were also pretty dull<p>Give it some time. I think something similar will happen: most LLM apps from this first wave will be forgotten soon, and when people shift the focus from the technology back to the users and their real pain points, the good stuff will surface</div><br/></div></div><div id="36528389" class="c"><input type="checkbox" id="c-36528389" checked=""/><div class="controls bullet"><span class="by">ebiester</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36529741">prev</a><span>|</span><a href="#36530277">next</a><span>|</span><label class="collapse" for="c-36528389">[-]</label><label class="expand" for="c-36528389">[1 more]</label></div><br/><div class="children"><div class="content">Because these are the people who are trying to &quot;front run&quot; with an MVP. They think if they can build something and get feedback, they&#x27;ll be ahead with their fuller vision.<p>I see a lot of companies that are experimenting with it, and there are a few press releases here and there, but there is a next generation that will be more ambitious. You just can&#x27;t build more ambitious in the timeframe yet.</div><br/></div></div><div id="36530277" class="c"><input type="checkbox" id="c-36530277" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36528389">prev</a><span>|</span><a href="#36532096">next</a><span>|</span><label class="collapse" for="c-36530277">[-]</label><label class="expand" for="c-36530277">[2 more]</label></div><br/><div class="children"><div class="content">Thanks to this comment:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36529885">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36529885</a><p>I realized how primitive even pure prompting is. Stable Diffusion is kind of primitive too, but the input&#x2F;prompting methods it has are <i>lightyears</i> ahead.</div><br/><div id="36531675" class="c"><input type="checkbox" id="c-36531675" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36527762">root</a><span>|</span><a href="#36530277">parent</a><span>|</span><a href="#36532096">next</a><span>|</span><label class="collapse" for="c-36531675">[-]</label><label class="expand" for="c-36531675">[1 more]</label></div><br/><div class="children"><div class="content">All techniques for prompting in Stable Diffusion work with regular LLMs. I have complained bitterly about this lack of tooling in NLP and wrote a github gist with sample implementations to prove this.<p>NLP folks don&#x27;t know shit about prompt engineering, which is ironic.<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865ca4bb328eb58faf" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865c...</a><p>I still can&#x27;t emphasize certain tokens in ChatGPT, or mathamatically average them. Not sure why NLP folks don&#x27;t bother implementing these things, even in the oogabooga frontend (which is supposed to be the automatic1111 of LLMs)</div><br/></div></div></div></div><div id="36532096" class="c"><input type="checkbox" id="c-36532096" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36530277">prev</a><span>|</span><a href="#36530441">next</a><span>|</span><label class="collapse" for="c-36532096">[-]</label><label class="expand" for="c-36532096">[1 more]</label></div><br/><div class="children"><div class="content">The fact that this tech gave us so many low hanging fruit is the testament of how powerful it is.</div><br/></div></div><div id="36530441" class="c"><input type="checkbox" id="c-36530441" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36532096">prev</a><span>|</span><a href="#36529859">next</a><span>|</span><label class="collapse" for="c-36530441">[-]</label><label class="expand" for="c-36530441">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s cool stuff going on in medicine <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06160-y" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06160-y</a></div><br/></div></div><div id="36529859" class="c"><input type="checkbox" id="c-36529859" checked=""/><div class="controls bullet"><span class="by">magicseth</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36530441">prev</a><span>|</span><a href="#36527861">next</a><span>|</span><label class="collapse" for="c-36529859">[-]</label><label class="expand" for="c-36529859">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m genuinely curious how you would classify my product, Wonder an AI powered browser for kids: hellowonder.ai<p>Does it cross the threshold?</div><br/><div id="36530358" class="c"><input type="checkbox" id="c-36530358" checked=""/><div class="controls bullet"><span class="by">tuchsen</span><span>|</span><a href="#36527762">root</a><span>|</span><a href="#36529859">parent</a><span>|</span><a href="#36527861">next</a><span>|</span><label class="collapse" for="c-36530358">[-]</label><label class="expand" for="c-36530358">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I&#x27;ll raise my hand here to say I did a file manager that automates file management tasks by using AI to write Bash scripts (Aerome.net). It&#x27;s still super primitive though, if I&#x27;m being honest. I think the problem is that it&#x27;s way harder to write a cross platform file manager, or browser wrapper in your case, then it is to write a chat interface on top of ChatGPT. I suspect in a year or two many good use cases will emerge, as people write more complicated software to take advantage of LLM&#x27;s capabilities.<p>I&#x27;m going to check out you&#x27;re browser thing later tonight, it looks good!</div><br/></div></div></div></div><div id="36527861" class="c"><input type="checkbox" id="c-36527861" checked=""/><div class="controls bullet"><span class="by">pertymcpert</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36529859">prev</a><span>|</span><a href="#36527972">next</a><span>|</span><label class="collapse" for="c-36527861">[-]</label><label class="expand" for="c-36527861">[1 more]</label></div><br/><div class="children"><div class="content">Same. You just know most of the paid apps are going to be abandoned in a few months.</div><br/></div></div><div id="36527972" class="c"><input type="checkbox" id="c-36527972" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36527861">prev</a><span>|</span><a href="#36528557">next</a><span>|</span><label class="collapse" for="c-36527972">[-]</label><label class="expand" for="c-36527972">[4 more]</label></div><br/><div class="children"><div class="content">Most of the stuff it’s actually good at (like NLP tasks) are both super boring and  require a secondary layer of processing to  catch hallucinations. Not as cool of a sales pitch to everyone on the “it’s alive!” hype train.</div><br/><div id="36532073" class="c"><input type="checkbox" id="c-36532073" checked=""/><div class="controls bullet"><span class="by">fhd2</span><span>|</span><a href="#36527762">root</a><span>|</span><a href="#36527972">parent</a><span>|</span><a href="#36528346">next</a><span>|</span><label class="collapse" for="c-36532073">[-]</label><label class="expand" for="c-36532073">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been working on NLP stuff using LLMs for a while, and it&#x27;s not that the problems aren&#x27;t somewhat interesting, but solving them is ridiculously tedious.<p>Most of the time on that project I&#x27;ve just played around with different prompts to make it do what I&#x27;m hoping for, with almost no mental process for understanding problems and solutions, mostly just randomly coming up with experiments and reading the results very carefully, checking how consistent they are.<p>I moved towards finding and isolating the parts LLMs are good (and reliable!) at and using deterministic approaches for everything I possibly can. That part is not too tedious, but all this black box trial and error (with all the waiting and errors)...<p>Luckily the client doesn&#x27;t expect LLMs to be what the hype says and mainly just wants some reasonably useful features so they can say they use AI - wouldn&#x27;t enjoy dealing with someone thinking it&#x27;s super easy and I just need to write a few scripts because they tried this in ChatGPT once.</div><br/></div></div><div id="36528346" class="c"><input type="checkbox" id="c-36528346" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#36527762">root</a><span>|</span><a href="#36527972">parent</a><span>|</span><a href="#36532073">prev</a><span>|</span><a href="#36528557">next</a><span>|</span><label class="collapse" for="c-36528346">[-]</label><label class="expand" for="c-36528346">[2 more]</label></div><br/><div class="children"><div class="content">Any advice for people building the second layer stuff?</div><br/><div id="36530151" class="c"><input type="checkbox" id="c-36530151" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#36527762">root</a><span>|</span><a href="#36528346">parent</a><span>|</span><a href="#36528557">next</a><span>|</span><label class="collapse" for="c-36530151">[-]</label><label class="expand" for="c-36530151">[1 more]</label></div><br/><div class="children"><div class="content">Just basic sanity checking. You can use an LLM call for this or something procedural. Generally, it’s just of the order of like “does the output conform to this structure”, “is the returned text actually in the input” etc</div><br/></div></div></div></div></div></div><div id="36528557" class="c"><input type="checkbox" id="c-36528557" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36527972">prev</a><span>|</span><a href="#36528421">next</a><span>|</span><label class="collapse" for="c-36528557">[-]</label><label class="expand" for="c-36528557">[1 more]</label></div><br/><div class="children"><div class="content">Exactly.<p>All these projects are desperately slapping buzzwords such as ‘LLMs’, ‘AI’, ‘ChatGPT’, etc to pretend that their product is somehow revolutionary despite sitting on someone else’s AI model in the cloud via an API  that they do not own.<p>They are slowing realizing that there is little to no moat with these LLMs let alone any serious use cases other than summarizing &#x2F; rewording text.<p>Everything else requires the triple checking of another human to review the bullshit that the so-called black-box AI outputs.</div><br/></div></div><div id="36530394" class="c"><input type="checkbox" id="c-36530394" checked=""/><div class="controls bullet"><span class="by">jasfi</span><span>|</span><a href="#36527762">parent</a><span>|</span><a href="#36528421">prev</a><span>|</span><a href="#36524113">next</a><span>|</span><label class="collapse" for="c-36530394">[-]</label><label class="expand" for="c-36530394">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m working on something to help you code with LLMs with projects of any size. If you go to <a href="https:&#x2F;&#x2F;inventai.xyz" rel="nofollow noreferrer">https:&#x2F;&#x2F;inventai.xyz</a>, sign-in with your email, you&#x27;ll be notified on release.<p>I started off with something to create AI art, but that didn&#x27;t really take off. I&#x27;m also disappointed with Dall-E 2 lagging behind the others in terms of image quality. So now I&#x27;m focusing on code generation.</div><br/></div></div></div></div><div id="36524113" class="c"><input type="checkbox" id="c-36524113" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#36527762">prev</a><span>|</span><a href="#36530966">next</a><span>|</span><label class="collapse" for="c-36524113">[-]</label><label class="expand" for="c-36524113">[19 more]</label></div><br/><div class="children"><div class="content"><i>&gt; Along the way, we’ve learned some useful lessons on how to build these kinds of applications, which we’ve formulated in terms of patterns.</i><p><pre><code>    * Use a text template to enrich a prompt with context and structure
    * Tell the LLM to respond in a structured data format
    * Stream the response to the UI so users can monitor progress
    * Capture and add relevant context information to subsequent action
    * Allow direct conversation with the LLM within a context.
    * Tell LLM to generate intermediate results while answering
    * Provide affordances for the user to have a back-and-forth interaction with the co-pilot
    * Combine LLM with other information sources to access data beyond the LLM&#x27;s training set</code></pre></div><br/><div id="36527892" class="c"><input type="checkbox" id="c-36527892" checked=""/><div class="controls bullet"><span class="by">manojlds</span><span>|</span><a href="#36524113">parent</a><span>|</span><a href="#36524839">next</a><span>|</span><label class="collapse" for="c-36527892">[-]</label><label class="expand" for="c-36527892">[1 more]</label></div><br/><div class="children"><div class="content">The short courses from dl.ai are better at driving these points - <a href="https:&#x2F;&#x2F;www.deeplearning.ai&#x2F;short-courses&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.deeplearning.ai&#x2F;short-courses&#x2F;</a></div><br/></div></div><div id="36524839" class="c"><input type="checkbox" id="c-36524839" checked=""/><div class="controls bullet"><span class="by">frankgrecojr</span><span>|</span><a href="#36524113">parent</a><span>|</span><a href="#36527892">prev</a><span>|</span><a href="#36530966">next</a><span>|</span><label class="collapse" for="c-36524839">[-]</label><label class="expand" for="c-36524839">[17 more]</label></div><br/><div class="children"><div class="content">&gt; Stream the response to the UI so users can monitor progress<p>This is a game changer to the UX</div><br/><div id="36525683" class="c"><input type="checkbox" id="c-36525683" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36524839">parent</a><span>|</span><a href="#36525819">next</a><span>|</span><label class="collapse" for="c-36525683">[-]</label><label class="expand" for="c-36525683">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a crutch to minimize the user annoyance at having to wait up to a minute for the response. It sure beats the spinner but it&#x27;s still a crutch.</div><br/><div id="36531517" class="c"><input type="checkbox" id="c-36531517" checked=""/><div class="controls bullet"><span class="by">thelittleone</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36525683">parent</a><span>|</span><a href="#36525819">next</a><span>|</span><label class="collapse" for="c-36531517">[-]</label><label class="expand" for="c-36531517">[2 more]</label></div><br/><div class="children"><div class="content">Really? I feel like in most uses the stream is close to, if not slightly faster than my reading. I actually prefer that over an instant full-page response. It helps me keep my place in the text and feels like reduced cognitive load.</div><br/><div id="36532128" class="c"><input type="checkbox" id="c-36532128" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36531517">parent</a><span>|</span><a href="#36525819">next</a><span>|</span><label class="collapse" for="c-36532128">[-]</label><label class="expand" for="c-36532128">[1 more]</label></div><br/><div class="children"><div class="content">Also, the vibrations when LLM “types” in OpenAI iOS app are so satisfying for some reason. It may be a stupid subjective thing, but from my experience in game dev, that&#x27;s exactly the kind of detail that creates the overall user experience feeling.</div><br/></div></div></div></div></div></div><div id="36525819" class="c"><input type="checkbox" id="c-36525819" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36524839">parent</a><span>|</span><a href="#36525683">prev</a><span>|</span><a href="#36527893">next</a><span>|</span><label class="collapse" for="c-36525819">[-]</label><label class="expand" for="c-36525819">[4 more]</label></div><br/><div class="children"><div class="content">Actually, it&#x27;s annoying because as you start reading the first lines, the content keeps scrolling (often with jagged movements). I always have to scroll up immediately after the stream begins to disable this behavior.</div><br/><div id="36526074" class="c"><input type="checkbox" id="c-36526074" checked=""/><div class="controls bullet"><span class="by">tobr</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36525819">parent</a><span>|</span><a href="#36527893">next</a><span>|</span><label class="collapse" for="c-36526074">[-]</label><label class="expand" for="c-36526074">[3 more]</label></div><br/><div class="children"><div class="content">That’s totally fixable, though. ReadyRunner handles it simply by scrolling all the way from the start, leaving space for the message to grow.</div><br/><div id="36527565" class="c"><input type="checkbox" id="c-36527565" checked=""/><div class="controls bullet"><span class="by">trafnar</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36526074">parent</a><span>|</span><a href="#36527893">next</a><span>|</span><label class="collapse" for="c-36527565">[-]</label><label class="expand" for="c-36527565">[2 more]</label></div><br/><div class="children"><div class="content">Hey, that&#x27;s my app! <a href="https:&#x2F;&#x2F;www.readyrunner.ai" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.readyrunner.ai</a></div><br/><div id="36529672" class="c"><input type="checkbox" id="c-36529672" checked=""/><div class="controls bullet"><span class="by">darkteflon</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36527565">parent</a><span>|</span><a href="#36527893">next</a><span>|</span><label class="collapse" for="c-36529672">[-]</label><label class="expand" for="c-36529672">[1 more]</label></div><br/><div class="children"><div class="content">That looks really nice. I do wish you would consider one-time pricing for those bringing their own API key on the “Dev” plan, though :) I’d pay ~$20-30 for a nice desktop app like this but won’t enter into another $48&#x2F;year sub.</div><br/></div></div></div></div></div></div></div></div><div id="36527893" class="c"><input type="checkbox" id="c-36527893" checked=""/><div class="controls bullet"><span class="by">jamifsud</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36524839">parent</a><span>|</span><a href="#36525819">prev</a><span>|</span><a href="#36525822">next</a><span>|</span><label class="collapse" for="c-36527893">[-]</label><label class="expand" for="c-36527893">[7 more]</label></div><br/><div class="children"><div class="content">Anyone know of any good “tolerant” JSON parsers?  I’d love to be able to stream a JSON response down to the client and have it be able to parse the JSON as it goes and handle the formatting errors that we sometimes see.</div><br/><div id="36532173" class="c"><input type="checkbox" id="c-36532173" checked=""/><div class="controls bullet"><span class="by">anentropic</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36527893">parent</a><span>|</span><a href="#36528193">next</a><span>|</span><label class="collapse" for="c-36532173">[-]</label><label class="expand" for="c-36532173">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I’d love to be able to stream a JSON response down to the client and have it be able to parse the JSON as it goes<p>why though?</div><br/></div></div><div id="36528193" class="c"><input type="checkbox" id="c-36528193" checked=""/><div class="controls bullet"><span class="by">icyfox</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36527893">parent</a><span>|</span><a href="#36532173">prev</a><span>|</span><a href="#36530177">next</a><span>|</span><label class="collapse" for="c-36528193">[-]</label><label class="expand" for="c-36528193">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no bulletproof solution to this. JSON5 (<a href="https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;json5" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;json5</a>) gets you slightly more leniency, as does plugging the currently streamed content into another smaller LLM. I also wrote a deterministic parser more tailored towards these partially-complete LM outputs. Not perfect certainly but handles the 99% of cases well: <a href="https:&#x2F;&#x2F;github.com&#x2F;piercefreeman&#x2F;gpt-json">https:&#x2F;&#x2F;github.com&#x2F;piercefreeman&#x2F;gpt-json</a>. In particular the &quot;streaming&quot; functionality here might be of interest to you.</div><br/><div id="36529223" class="c"><input type="checkbox" id="c-36529223" checked=""/><div class="controls bullet"><span class="by">darkteflon</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36528193">parent</a><span>|</span><a href="#36530177">next</a><span>|</span><label class="collapse" for="c-36529223">[-]</label><label class="expand" for="c-36529223">[3 more]</label></div><br/><div class="children"><div class="content">This looks really cool, thanks for open-sourcing this. I’ve been similarly parsing and validating output from OpenAI’s new functions using a schema defined on a custom Pydantic class, but I can see that your code has a lot of niceties coming from proper battle-testing, including elegant error handling, transformations and good docs.<p>I’d like to incorporate this in a production workflow for generating schema-compliant test data for use in few-shot promoting - would you mind saying a few words about your medium term plans for the library? The LangChain API is changing all the time at the moment so we’re trying to figure out where it’s safe to stand. No expectations, of course, just curious.</div><br/><div id="36529346" class="c"><input type="checkbox" id="c-36529346" checked=""/><div class="controls bullet"><span class="by">icyfox</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36529223">parent</a><span>|</span><a href="#36530177">next</a><span>|</span><label class="collapse" for="c-36529346">[-]</label><label class="expand" for="c-36529346">[2 more]</label></div><br/><div class="children"><div class="content">Sure - I&#x27;m using it in a few different internal tools and know others are using it in production. The API should be relatively stable at this point since I intentionally kept the scope pretty limited. The main changes over time will be improved robustness and error correction as issues report different JSON schema breaks that we can fix automatically. Let me know if you see more cases that can be addressed here, would love to collaborate on it.</div><br/><div id="36529820" class="c"><input type="checkbox" id="c-36529820" checked=""/><div class="controls bullet"><span class="by">darkteflon</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36529346">parent</a><span>|</span><a href="#36530177">next</a><span>|</span><label class="collapse" for="c-36529820">[-]</label><label class="expand" for="c-36529820">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Absolutely, will do. I’ll have a play with it today and reach out with a PR any time it makes sense to do so.<p>I noticed some occasional funkiness from GPT-4 around sending back properly formatted dates yesterday but haven’t yet dug into it properly. Might be a good candidate for a transformation.</div><br/></div></div></div></div></div></div></div></div><div id="36530177" class="c"><input type="checkbox" id="c-36530177" checked=""/><div class="controls bullet"><span class="by">n4te</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36527893">parent</a><span>|</span><a href="#36528193">prev</a><span>|</span><a href="#36525822">next</a><span>|</span><label class="collapse" for="c-36530177">[-]</label><label class="expand" for="c-36530177">[1 more]</label></div><br/><div class="children"><div class="content">My JsonReader in libgdx and JsonBeans can parse a more relaxed version of JSON. It uses Ragel.</div><br/></div></div></div></div><div id="36525822" class="c"><input type="checkbox" id="c-36525822" checked=""/><div class="controls bullet"><span class="by">huydotnet</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36524839">parent</a><span>|</span><a href="#36527893">prev</a><span>|</span><a href="#36530966">next</a><span>|</span><label class="collapse" for="c-36525822">[-]</label><label class="expand" for="c-36525822">[2 more]</label></div><br/><div class="children"><div class="content">Still not a reasonable way if you&#x27;re expecting a structured data in the response, like JSON or something that you&#x27;re required to parse before showing to the user.</div><br/><div id="36530839" class="c"><input type="checkbox" id="c-36530839" checked=""/><div class="controls bullet"><span class="by">jelled</span><span>|</span><a href="#36524113">root</a><span>|</span><a href="#36525822">parent</a><span>|</span><a href="#36530966">next</a><span>|</span><label class="collapse" for="c-36530839">[-]</label><label class="expand" for="c-36530839">[1 more]</label></div><br/><div class="children"><div class="content">On one of my apps I ask for a plain text response first and then make a second call to parse the original response to json.</div><br/></div></div></div></div></div></div></div></div><div id="36530966" class="c"><input type="checkbox" id="c-36530966" checked=""/><div class="controls bullet"><span class="by">mercurialsolo</span><span>|</span><a href="#36524113">prev</a><span>|</span><a href="#36531387">next</a><span>|</span><label class="collapse" for="c-36530966">[-]</label><label class="expand" for="c-36530966">[1 more]</label></div><br/><div class="children"><div class="content">Retries, answer verification systems, formatting and parsing. Reasoning and validation of reasoning with natural language is a key challenge.<p>Closed feedback loops to improve the quality of reasoning for LLM&#x27;s requires human-in-the-loop tools.<p>Prompting methods are often a hit and miss as the same prompts can lead to variable quality outputs.</div><br/></div></div><div id="36531387" class="c"><input type="checkbox" id="c-36531387" checked=""/><div class="controls bullet"><span class="by">sandGorgon</span><span>|</span><a href="#36530966">prev</a><span>|</span><a href="#36526971">next</a><span>|</span><label class="collapse" for="c-36531387">[-]</label><label class="expand" for="c-36531387">[1 more]</label></div><br/><div class="children"><div class="content">One of the things that strongly resonate with me is the &quot;text templating&quot; part. I faced the same thing when i was building my AI application. Most SDK&#x2F;frameworks for writing generative AI model this as libraries (or even React UI widgets).<p>I think that is wrong - it is a config management problem. Think Prompts X Chains X LLMs. Your prompts wont work across everything and everything will break on model change. Coding this into ur classes is what everyone does.<p>Instead we pull out the prompts X chains as jsonnet code. Call it trauma &amp; learnings from the K8s&#x2F;Borg world. We have formats that have evolved as a result of millions of lines of code wrangling clusters&#x2F;terraform&#x2F;etc - so we decided to build a SDK over it.<p>that is what we did here - <a href="https:&#x2F;&#x2F;github.com&#x2F;arakoodev&#x2F;EdgeChains&#x2F;releases&#x2F;tag&#x2F;0.2.0">https:&#x2F;&#x2F;github.com&#x2F;arakoodev&#x2F;EdgeChains&#x2F;releases&#x2F;tag&#x2F;0.2.0</a><p>EdgeChains is basically Generative AI prompt engineering modeled as config management. Funnily people organically build this out over 6 months of engineering AI applications. That&#x27;s what Martin did with templating this as text !!</div><br/></div></div><div id="36526971" class="c"><input type="checkbox" id="c-36526971" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#36531387">prev</a><span>|</span><a href="#36527148">next</a><span>|</span><label class="collapse" for="c-36526971">[-]</label><label class="expand" for="c-36526971">[2 more]</label></div><br/><div class="children"><div class="content">LLM latency is a huge no go for most apps except for chat apps.  I’ve try to build apps based on OpenAI and that itself creates a bad experience no matter how much elevator music&#x2F;mirrors&#x2F;spinners you place.    Then you need proper error correction when dealing with structured responses&#x2F;occasional hallucinations</div><br/><div id="36531344" class="c"><input type="checkbox" id="c-36531344" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#36526971">parent</a><span>|</span><a href="#36527148">next</a><span>|</span><label class="collapse" for="c-36531344">[-]</label><label class="expand" for="c-36531344">[1 more]</label></div><br/><div class="children"><div class="content">Latency is acceptable when the value of the result outweighs the time you believe to be excess.<p>If AI can build me a useful marketing plan in 15 mins with multiple agents doing work this seems fine to me. It’s going to take much longer to get a human involved.</div><br/></div></div></div></div><div id="36527148" class="c"><input type="checkbox" id="c-36527148" checked=""/><div class="controls bullet"><span class="by">daviding</span><span>|</span><a href="#36526971">prev</a><span>|</span><a href="#36525919">next</a><span>|</span><label class="collapse" for="c-36527148">[-]</label><label class="expand" for="c-36527148">[5 more]</label></div><br/><div class="children"><div class="content">This is an interesting article, and a bit of a mish mash of UI conventions, application use ideas for GPT and actual patterns for LLMs. I really do miss Martin Fowler&#x27;s actual take on these things, but using his name as some sort of gestalt brain for Thoughtworks works too.<p>It still feels like a bit of a Wild West for patterns in this area as yet, with a lot of people trying lots of things and it might be too soon for defining terms. A useful resource is still things like the OpenAI Cookbook, that is a decent collection of a lot of the things in this article but with a more implementation bent.[1]<p>The area that seems to get a lot of idea duplication currently is in providing either a &#x27;session&#x27; or a longer term context for GPT, be it with embeddings or rolling prompts for these apps. The use of vector search and embedded chunks is something that seems to be missing so far from vendors like OpenAI, and you can&#x27;t help but wonder that they&#x27;ll move it behind their API eventually with a &#x27;session id&#x27; in the end. I think that was mentioned as on their roadmap for this year too. The lack of GPT-4 fine tuning options just seems to push people more to look at the Pinecone, Weaviates etc stores and chaining up their own sequences to achieve some sort of memory.<p>I&#x27;ve implemented features with GPT-4 and functions and so far it&#x27;s feeling useful for &#x27;data model&#x27; like use (where you&#x27;re bringing json into the prompt about a domain noun, e.g. &#x27;Tasks&#x27;) but is pretty hairy when it comes to pure functions - the tuning they&#x27;ve done to get it to pick which function and which parameters to use is still hard going to get right, which means there doesn&#x27;t feel like a lot of trust that it is going to be usable. It&#x27;s like there needs to be a set of patterns or categories for &#x27;business apps&#x27; that are heavily siloed into just a subset of available functions it can work with, making it more task-specific rather than as a general chat agent we see a lot of. The difference in approach between LangChain&#x27;s Chain of Thought pattern and just using OpenAI functions is sort of up in the air as well. Like I said, it still all feels like we&#x27;re in wild west times, at least as an app developer.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-cookbook">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-cookbook</a></div><br/><div id="36527481" class="c"><input type="checkbox" id="c-36527481" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36527148">parent</a><span>|</span><a href="#36525919">next</a><span>|</span><label class="collapse" for="c-36527481">[-]</label><label class="expand" for="c-36527481">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>A useful resource is still things like the OpenAI Cookbook, that is a decent collection of a lot of the things in this article</i><p>By far, the best resource I&#x27;ve found is the <i>Prompt Engineering Guide</i>: <a href="https:&#x2F;&#x2F;www.promptingguide.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.promptingguide.ai&#x2F;</a><p>&gt; <i>you can&#x27;t help but wonder that they&#x27;ll move it behind their API eventually with a &#x27;session id&#x27; in the end</i><p>For in-context learning, I think it is fair to expect <i>100k</i> to <i>500k</i> context windows sooner. OpenAI is already at <i>32k</i>.</div><br/><div id="36527615" class="c"><input type="checkbox" id="c-36527615" checked=""/><div class="controls bullet"><span class="by">daviding</span><span>|</span><a href="#36527148">root</a><span>|</span><a href="#36527481">parent</a><span>|</span><a href="#36531701">next</a><span>|</span><label class="collapse" for="c-36527615">[-]</label><label class="expand" for="c-36527615">[2 more]</label></div><br/><div class="children"><div class="content">&gt; By far, the best resource I&#x27;ve found is the Prompt Engineering Guide: <a href="https:&#x2F;&#x2F;www.promptingguide.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.promptingguide.ai&#x2F;</a><p>Agreed, that is a good resource for sure. For tooling I like <a href="https:&#x2F;&#x2F;promptmetheus.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;promptmetheus.com&#x2F;</a> but any pun name gets bonus points from me.<p>&gt; For in-context learning, I think it is fair to expect 100k to 500k context windows sooner. OpenAI is already at 32k.<p>It has been interesting to see that window increase so quickly. For LLM context the biggest thing is the pay-per-token constraint if you don&#x27;t run your own, so have to wonder if that is what will be around in the future given how this is trending? Just in terms of idempotent calls, throwing everything in context up every time seems like it makes it likely that OpenAI will encroach on the stores side as well and do sessions?</div><br/><div id="36528540" class="c"><input type="checkbox" id="c-36528540" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#36527148">root</a><span>|</span><a href="#36527615">parent</a><span>|</span><a href="#36531701">next</a><span>|</span><label class="collapse" for="c-36528540">[-]</label><label class="expand" for="c-36528540">[1 more]</label></div><br/><div class="children"><div class="content">It is interesting to see the context window size increasing. I think the time complexity on window size is quadratic - ouch!</div><br/></div></div></div></div><div id="36531701" class="c"><input type="checkbox" id="c-36531701" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36527148">root</a><span>|</span><a href="#36527481">parent</a><span>|</span><a href="#36527615">prev</a><span>|</span><a href="#36525919">next</a><span>|</span><label class="collapse" for="c-36531701">[-]</label><label class="expand" for="c-36531701">[1 more]</label></div><br/><div class="children"><div class="content">Can we stop calling it &quot;in-context learning&quot; and call it what it is, zero-shot&#x2F;one-shot&#x2F;few-shot prompting instead?<p>Learning implies that the underlying weights of the LLM changed. They didn&#x27;t.</div><br/></div></div></div></div></div></div><div id="36525919" class="c"><input type="checkbox" id="c-36525919" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#36527148">prev</a><span>|</span><a href="#36525842">next</a><span>|</span><label class="collapse" for="c-36525919">[-]</label><label class="expand" for="c-36525919">[4 more]</label></div><br/><div class="children"><div class="content">I find the whole idea of adding text into text to drive a outcome pretty worrying if I have to rely on the output.<p>If the probability of the model spitting out something bad is 0.01% will my testing find it? Probably not.. but my users certainly will.</div><br/><div id="36526633" class="c"><input type="checkbox" id="c-36526633" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#36525919">parent</a><span>|</span><a href="#36525842">next</a><span>|</span><label class="collapse" for="c-36526633">[-]</label><label class="expand" for="c-36526633">[3 more]</label></div><br/><div class="children"><div class="content">Well, it&#x27;s a tool for ideation, not a strategy emitter. You don&#x27;t rely on the output, you rely on the people who finalize and commit to a strategy.</div><br/><div id="36526683" class="c"><input type="checkbox" id="c-36526683" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#36525919">root</a><span>|</span><a href="#36526633">parent</a><span>|</span><a href="#36528566">next</a><span>|</span><label class="collapse" for="c-36526683">[-]</label><label class="expand" for="c-36526683">[1 more]</label></div><br/><div class="children"><div class="content">Yeah - for an application like this I get it. But no one is getting rich or shifting the dial on scientific progress with this sort of thing.</div><br/></div></div><div id="36528566" class="c"><input type="checkbox" id="c-36528566" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#36525919">root</a><span>|</span><a href="#36526633">parent</a><span>|</span><a href="#36526683">prev</a><span>|</span><a href="#36525842">next</a><span>|</span><label class="collapse" for="c-36528566">[-]</label><label class="expand" for="c-36528566">[1 more]</label></div><br/><div class="children"><div class="content">Ideation is a prime application. At least once a week, I work out an idea for something with a long “conversation” chain staring with “I want to design a system that…” and asking for more detail, suggesting resources, etc. that said, being able to do this is not life changing for me. It just saves some time.</div><br/></div></div></div></div></div></div><div id="36525842" class="c"><input type="checkbox" id="c-36525842" checked=""/><div class="controls bullet"><span class="by">selalipop</span><span>|</span><a href="#36525919">prev</a><span>|</span><label class="collapse" for="c-36525842">[-]</label><label class="expand" for="c-36525842">[3 more]</label></div><br/><div class="children"><div class="content">I worked on something very much in this vein (notionsmith.ai) and feel like I should do a write up after reading this!<p>I think a lot of people are learning these lessons in isolation, I do wish there was a centralized place where people working on UX-focused LLM based apps were exchanging lessons</div><br/><div id="36526853" class="c"><input type="checkbox" id="c-36526853" checked=""/><div class="controls bullet"><span class="by">tinco</span><span>|</span><a href="#36525842">parent</a><span>|</span><a href="#36527560">next</a><span>|</span><label class="collapse" for="c-36526853">[-]</label><label class="expand" for="c-36526853">[1 more]</label></div><br/><div class="children"><div class="content">I think a lot of us are working heads down in isolation because we don&#x27;t have a shareworthy project yet. In a week or two I think my system will be fancy enough to write a blog post about and maybe make open source.<p>HN has been a pretty good source of exchanging knowledge so far, every couple days or so there&#x27;s a write up like this that has some new tidbits or confirmations of ideas. If everyone keeps doing that we&#x27;re doing great in my opinion. Looking forward to seeing your write up on here!</div><br/></div></div><div id="36527560" class="c"><input type="checkbox" id="c-36527560" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36525842">parent</a><span>|</span><a href="#36526853">prev</a><span>|</span><label class="collapse" for="c-36527560">[-]</label><label class="expand" for="c-36527560">[1 more]</label></div><br/><div class="children"><div class="content">Things on the LLM front for utility apps are fairly nascent and by OpenAI&#x27;s own admission, the current limitations are fleeting, as in, as a developer, you will soon not need the workarounds used today.<p>Multi-modal models are going to change things even further.</div><br/></div></div></div></div></div></div></div></div></div></body></html>