<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718442057583" as="style"/><link rel="stylesheet" href="styles.css?v=1718442057583"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://johnthenerd.com/blog/faster-local-llm-assistant/">Making my local LLM voice assistant faster and more scalable with RAG</a> <span class="domain">(<a href="https://johnthenerd.com">johnthenerd.com</a>)</span></div><div class="subtext"><span>JohnTheNerd</span> | <span>11 comments</span></div><br/><div><div id="40687073" class="c"><input type="checkbox" id="c-40687073" checked=""/><div class="controls bullet"><span class="by">pw378</span><span>|</span><a href="#40687418">next</a><span>|</span><label class="collapse" for="c-40687073">[-]</label><label class="expand" for="c-40687073">[3 more]</label></div><br/><div class="children"><div class="content">That lag between query and response ruins it for me.</div><br/><div id="40687095" class="c"><input type="checkbox" id="c-40687095" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40687073">parent</a><span>|</span><a href="#40687116">next</a><span>|</span><label class="collapse" for="c-40687095">[-]</label><label class="expand" for="c-40687095">[1 more]</label></div><br/><div class="children"><div class="content">“Excellent query good sir! &lt;said slowly enough to let the LLM catch up&gt;…”<p>And more seriously, it seems like the LLM could be used to precreate lots of filler prefixes that correspond to the rag’d document that are being sent to the model.<p>While it wouldn’t work if you’re GPU’d bound, multiple prompts could be run in parallel with different pieces of context and then have the model chose the most appropriate response (which could be done in parallel too).</div><br/></div></div><div id="40687116" class="c"><input type="checkbox" id="c-40687116" checked=""/><div class="controls bullet"><span class="by">lettergram</span><span>|</span><a href="#40687073">parent</a><span>|</span><a href="#40687095">prev</a><span>|</span><a href="#40687418">next</a><span>|</span><label class="collapse" for="c-40687116">[-]</label><label class="expand" for="c-40687116">[1 more]</label></div><br/><div class="children"><div class="content">For me, it was the cuts between each call haha</div><br/></div></div></div></div><div id="40687418" class="c"><input type="checkbox" id="c-40687418" checked=""/><div class="controls bullet"><span class="by">geniium</span><span>|</span><a href="#40687073">prev</a><span>|</span><a href="#40687317">next</a><span>|</span><label class="collapse" for="c-40687418">[-]</label><label class="expand" for="c-40687418">[1 more]</label></div><br/><div class="children"><div class="content">I was having a look at the model mentioned, specifcially `casperhansen&#x2F;llama-3-70b-instruct-awq`.<p>When checking this model, I found out [1] it&#x27;s based on llama-2 ?<p>```
Expand  
Llama 3 70B Instruct AWQ Parameters and Internals
LLM Name Llama 3 70B Instruct AWQ
Repository Open on  
Base Model(s)   Llama 2 70B Instruct   quantumaikr&#x2F;llama-2-70B-instruct
Model Size 70b
```<p>I added a question [2] on Hugging Face to learn more about this.<p>Anyone could explain to me what this means? Does it mean that it has been trained on the version 2 and wrongly named version 3? Or is it something that is not well intended?<p>[1] <a href="https:&#x2F;&#x2F;llm.extractum.io&#x2F;model&#x2F;casperhansen%2Fllama-3-70b-instruct-awq,1F94jNKFjgC3UuxjZWwmOW" rel="nofollow">https:&#x2F;&#x2F;llm.extractum.io&#x2F;model&#x2F;casperhansen%2Fllama-3-70b-in...</a><p>[2] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;casperhansen&#x2F;llama-3-70b-instruct-awq&#x2F;discussions" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;casperhansen&#x2F;llama-3-70b-instruct-awq...</a></div><br/></div></div><div id="40687317" class="c"><input type="checkbox" id="c-40687317" checked=""/><div class="controls bullet"><span class="by">jijji</span><span>|</span><a href="#40687418">prev</a><span>|</span><a href="#40687119">next</a><span>|</span><label class="collapse" for="c-40687317">[-]</label><label class="expand" for="c-40687317">[1 more]</label></div><br/><div class="children"><div class="content">I love how The llm responds back to you in a sarcastic, patronising, condescending, uninterested tone...</div><br/></div></div><div id="40687119" class="c"><input type="checkbox" id="c-40687119" checked=""/><div class="controls bullet"><span class="by">elevatedastalt</span><span>|</span><a href="#40687317">prev</a><span>|</span><label class="collapse" for="c-40687119">[-]</label><label class="expand" for="c-40687119">[5 more]</label></div><br/><div class="children"><div class="content">Cringe conversation. Why can&#x27;t AIs just do stuff that you ask them to do without pretending to be human?</div><br/><div id="40687166" class="c"><input type="checkbox" id="c-40687166" checked=""/><div class="controls bullet"><span class="by">zx8080</span><span>|</span><a href="#40687119">parent</a><span>|</span><a href="#40687176">next</a><span>|</span><label class="collapse" for="c-40687166">[-]</label><label class="expand" for="c-40687166">[2 more]</label></div><br/><div class="children"><div class="content">Then how is it different from Excel&#x2F;Word and shell&#x2F;python scripts?</div><br/><div id="40687669" class="c"><input type="checkbox" id="c-40687669" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#40687119">root</a><span>|</span><a href="#40687166">parent</a><span>|</span><a href="#40687176">next</a><span>|</span><label class="collapse" for="c-40687669">[-]</label><label class="expand" for="c-40687669">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s much slower, gets things wrong, and insists on things that ain&#x27;t so.</div><br/></div></div></div></div><div id="40687176" class="c"><input type="checkbox" id="c-40687176" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#40687119">parent</a><span>|</span><a href="#40687166">prev</a><span>|</span><a href="#40688113">next</a><span>|</span><label class="collapse" for="c-40687176">[-]</label><label class="expand" for="c-40687176">[1 more]</label></div><br/><div class="children"><div class="content">Llama3 is very keen to be nice. I kind of wonder if that&#x27;s due to better results on the chatbot arena (probably not, just a conspiracy theory I like). But with enough context available, you can definitely tweak the response in many ways. Give an example or two, tell it to be an emotionally detached HAL, you&#x27;ll get what you want.</div><br/></div></div><div id="40688113" class="c"><input type="checkbox" id="c-40688113" checked=""/><div class="controls bullet"><span class="by">colechristensen</span><span>|</span><a href="#40687119">parent</a><span>|</span><a href="#40687176">prev</a><span>|</span><label class="collapse" for="c-40688113">[-]</label><label class="expand" for="c-40688113">[1 more]</label></div><br/><div class="children"><div class="content">Because they’re trained to.<p>I hate the introduction to the response.  That’s not even trying to be human, i don’t know something more like a deranged patronizing butler.</div><br/></div></div></div></div></div></div></div></div></div></body></html>