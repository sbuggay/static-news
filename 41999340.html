<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730365268680" as="style"/><link rel="stylesheet" href="styles.css?v=1730365268680"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2410.21333">Chain-of-thought can hurt performance on tasks where thinking makes humans worse</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>benocodes</span> | <span>122 comments</span></div><br/><div><div id="42001388" class="c"><input type="checkbox" id="c-42001388" checked=""/><div class="controls bullet"><span class="by">mitko</span><span>|</span><a href="#42000781">next</a><span>|</span><label class="collapse" for="c-42001388">[-]</label><label class="expand" for="c-42001388">[3 more]</label></div><br/><div class="children"><div class="content">This is so uncannily close to the problems we&#x27;re encountering at Pioneer, trying to make human+LLM workflows in high stakes &#x2F; high complexity situations.<p>Humans are so smart and do so many decisions and calculations on the subconscious&#x2F;implicit level and take a lot of mental shortcuts, so that as we try to automate this by following exactly what the process is, we bring a lot of the implicit thinking out on the surface, and that slows everything down. So we&#x27;ve had to be creative about how we build LLM workflows.</div><br/><div id="42001445" class="c"><input type="checkbox" id="c-42001445" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#42001388">parent</a><span>|</span><a href="#42000781">next</a><span>|</span><label class="collapse" for="c-42001445">[-]</label><label class="expand" for="c-42001445">[2 more]</label></div><br/><div class="children"><div class="content">This is a regression in the model&#x27;s <i>accuracy</i> at certain tasks when using COT, not its speed:<p>&gt; In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts.<p>In other words, the issue they&#x27;re identifying is that COT is an less effective model for some tasks compared to unmodified chat completion, not just that it slows everything down.</div><br/><div id="42001519" class="c"><input type="checkbox" id="c-42001519" checked=""/><div class="controls bullet"><span class="by">mitko</span><span>|</span><a href="#42001388">root</a><span>|</span><a href="#42001445">parent</a><span>|</span><a href="#42000781">next</a><span>|</span><label class="collapse" for="c-42001519">[-]</label><label class="expand" for="c-42001519">[1 more]</label></div><br/><div class="children"><div class="content">Yeah! That&#x27;s the danger with any kind of &quot;model&quot; whether it is CoT, CrewAI, or other ways to outsmart it. It is betting that a programmer&#x2F;operator can break a large tasks up in a better way than an LLM can keep attention (assuming it can fit the info in the context window).<p>ChatGPT&#x27;s o1 model could make a lot of those programming techniques less effective, but they may still be around as they are more manageable, and constrained.</div><br/></div></div></div></div></div></div><div id="42000781" class="c"><input type="checkbox" id="c-42000781" checked=""/><div class="controls bullet"><span class="by">gpsx</span><span>|</span><a href="#42001388">prev</a><span>|</span><a href="#42001280">next</a><span>|</span><label class="collapse" for="c-42000781">[-]</label><label class="expand" for="c-42000781">[3 more]</label></div><br/><div class="children"><div class="content">I saw an LLM having this kind of problem when I was doing some testing a ways back. I asked it to order three fruits from largest to smallest. I think it was orange, blueberry and grapefruit. It could do that easily with a simple prompt. When the prompting included something to the effect of “think step by step”, it would try to talk through the problem and it would usually get it wrong.</div><br/><div id="42003959" class="c"><input type="checkbox" id="c-42003959" checked=""/><div class="controls bullet"><span class="by">spockz</span><span>|</span><a href="#42000781">parent</a><span>|</span><a href="#42004007">next</a><span>|</span><label class="collapse" for="c-42003959">[-]</label><label class="expand" for="c-42003959">[1 more]</label></div><br/><div class="children"><div class="content">How much does this align with how we learn math? We kind of instinctively learn the answers to simple math questions. We can even at some point develop an intuition for things like integrating and differentials. But the moment we are asked to explain why, or worse provide a proof, things become a lot harder. Even though the initial answer may be correct.</div><br/></div></div></div></div><div id="42001280" class="c"><input type="checkbox" id="c-42001280" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42000781">prev</a><span>|</span><a href="#41999805">next</a><span>|</span><label class="collapse" for="c-42001280">[-]</label><label class="expand" for="c-42001280">[3 more]</label></div><br/><div class="children"><div class="content">Alternate framing: A powerful autocomplete algorithm is being used to iteratively extend an existing document based on its training set. <i>Sometimes</i> you get a less-desirable end-result when you intervene to change the style of the document away from question-and-answer to something less common.</div><br/><div id="42004280" class="c"><input type="checkbox" id="c-42004280" checked=""/><div class="controls bullet"><span class="by">youoy</span><span>|</span><a href="#42001280">parent</a><span>|</span><a href="#41999805">next</a><span>|</span><label class="collapse" for="c-42004280">[-]</label><label class="expand" for="c-42004280">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what one half of HN think. The other half:<p>Artificial brains in the verge of singularity show another sign of approaching consciousness. The chain of thought of process performance is exactly human, showing yet another proof of the arrival of AGI before 2030.</div><br/><div id="42004690" class="c"><input type="checkbox" id="c-42004690" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#42001280">root</a><span>|</span><a href="#42004280">parent</a><span>|</span><a href="#41999805">next</a><span>|</span><label class="collapse" for="c-42004690">[-]</label><label class="expand" for="c-42004690">[1 more]</label></div><br/><div class="children"><div class="content">Pfft, 2030?!? It’s already in the middle of manipulating the election! (&#x2F;s, kinda)</div><br/></div></div></div></div></div></div><div id="41999805" class="c"><input type="checkbox" id="c-41999805" checked=""/><div class="controls bullet"><span class="by">oatsandsugar</span><span>|</span><a href="#42001280">prev</a><span>|</span><a href="#42000370">next</a><span>|</span><label class="collapse" for="c-41999805">[-]</label><label class="expand" for="c-41999805">[46 more]</label></div><br/><div class="children"><div class="content">Tasks were thinking makes human worse<p>&gt; Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions.<p>Fascinating that our lizard brains are better at implicit statistical reasoning</div><br/><div id="42000539" class="c"><input type="checkbox" id="c-42000539" checked=""/><div class="controls bullet"><span class="by">brewii</span><span>|</span><a href="#41999805">parent</a><span>|</span><a href="#42000499">next</a><span>|</span><label class="collapse" for="c-42000539">[-]</label><label class="expand" for="c-42000539">[40 more]</label></div><br/><div class="children"><div class="content">Think about how fast you’re able to determine the exact trajectory of a ball and location to place your hand to catch a ball using your lizard brain.</div><br/><div id="42000860" class="c"><input type="checkbox" id="c-42000860" checked=""/><div class="controls bullet"><span class="by">taeric</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000539">parent</a><span>|</span><a href="#42001080">next</a><span>|</span><label class="collapse" for="c-42000860">[-]</label><label class="expand" for="c-42000860">[30 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t some innate ability that people have.  As evidenced by how bad my kids are at catching things.  :D<p>That said, I think this is a good example.  We call it &quot;muscle memory&quot; in that you are good at what you have trained at.  Change a parameter in it, though, and your execution will almost certainly suffer.</div><br/><div id="42003018" class="c"><input type="checkbox" id="c-42003018" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000860">parent</a><span>|</span><a href="#42001045">next</a><span>|</span><label class="collapse" for="c-42003018">[-]</label><label class="expand" for="c-42003018">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Muscle memory&quot; has always seemed like a terrible name for that kind of skill. A ball will be thrown to a slightly different location every time. There&#x27;s no memory evolved there at all, its just calculations and predictions happening at a level that our conscious mind doesn&#x27;t seem to see or recognize.</div><br/></div></div><div id="42001045" class="c"><input type="checkbox" id="c-42001045" checked=""/><div class="controls bullet"><span class="by">skrtskrt</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000860">parent</a><span>|</span><a href="#42003018">prev</a><span>|</span><a href="#42001080">next</a><span>|</span><label class="collapse" for="c-42001045">[-]</label><label class="expand" for="c-42001045">[27 more]</label></div><br/><div class="children"><div class="content">I mean even people that are &quot;bad at catching things&quot; are still getting ridiculously close to catching it - getting hands to the right area probably within well under a second of the right timing - without being <i>taught</i> anything in particular about how a ball moves through the air.</div><br/><div id="42001146" class="c"><input type="checkbox" id="c-42001146" checked=""/><div class="controls bullet"><span class="by">taeric</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001045">parent</a><span>|</span><a href="#42001080">next</a><span>|</span><label class="collapse" for="c-42001146">[-]</label><label class="expand" for="c-42001146">[26 more]</label></div><br/><div class="children"><div class="content">Uh.... have you been around kids?  It will take several absurd misses before they even start to respond to a ball in flight.</div><br/><div id="42004545" class="c"><input type="checkbox" id="c-42004545" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001146">parent</a><span>|</span><a href="#42001205">next</a><span>|</span><label class="collapse" for="c-42004545">[-]</label><label class="expand" for="c-42004545">[1 more]</label></div><br/><div class="children"><div class="content">Stop missing and they will respond to the ball a lot sooner.</div><br/></div></div><div id="42001205" class="c"><input type="checkbox" id="c-42001205" checked=""/><div class="controls bullet"><span class="by">331c8c71</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001146">parent</a><span>|</span><a href="#42004545">prev</a><span>|</span><a href="#42001080">next</a><span>|</span><label class="collapse" for="c-42001205">[-]</label><label class="expand" for="c-42001205">[24 more]</label></div><br/><div class="children"><div class="content">I hope we still agree the kids learn extremely efficiently by ml standards.</div><br/><div id="42001511" class="c"><input type="checkbox" id="c-42001511" checked=""/><div class="controls bullet"><span class="by">choilive</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001205">parent</a><span>|</span><a href="#42001436">next</a><span>|</span><label class="collapse" for="c-42001511">[-]</label><label class="expand" for="c-42001511">[7 more]</label></div><br/><div class="children"><div class="content">Makes a lot of sense, there&#x27;s massive evolutionary pressure to build brains that have both incredible learning rate and efficiency. Its literally a life or death optimization.</div><br/><div id="42001670" class="c"><input type="checkbox" id="c-42001670" checked=""/><div class="controls bullet"><span class="by">Asraelite</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001511">parent</a><span>|</span><a href="#42001436">next</a><span>|</span><label class="collapse" for="c-42001670">[-]</label><label class="expand" for="c-42001670">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s especially impressive when you consider that evolution hasn&#x27;t had very long to produce these results.<p>Humans as an intelligent-ish species have been around for about 10 million years depending on where you define the cutoff. At 10 years per generation, that&#x27;s 1 million generations for our brain to evolve.<p>1 million generations isn&#x27;t much by machine learning standards.</div><br/><div id="42004122" class="c"><input type="checkbox" id="c-42004122" checked=""/><div class="controls bullet"><span class="by">choilive</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001670">parent</a><span>|</span><a href="#42003600">next</a><span>|</span><label class="collapse" for="c-42004122">[-]</label><label class="expand" for="c-42004122">[1 more]</label></div><br/><div class="children"><div class="content">Other than our large neocortex and frontal lobe (which exists in some capacity in mammals), the rest of the structures are evolutionarily ancient. Pre-mammalian in fact.</div><br/></div></div><div id="42003600" class="c"><input type="checkbox" id="c-42003600" checked=""/><div class="controls bullet"><span class="by">roywiggins</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001670">parent</a><span>|</span><a href="#42004122">prev</a><span>|</span><a href="#42002346">next</a><span>|</span><label class="collapse" for="c-42003600">[-]</label><label class="expand" for="c-42003600">[1 more]</label></div><br/><div class="children"><div class="content">These sorts of motor skills are probably older than <i>mammals</i>.</div><br/></div></div><div id="42002346" class="c"><input type="checkbox" id="c-42002346" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001670">parent</a><span>|</span><a href="#42003600">prev</a><span>|</span><a href="#42002122">next</a><span>|</span><label class="collapse" for="c-42002346">[-]</label><label class="expand" for="c-42002346">[2 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re underestimating how much our time as pre-humans baked useful structure into our brains.</div><br/><div id="42003800" class="c"><input type="checkbox" id="c-42003800" checked=""/><div class="controls bullet"><span class="by">notnaut</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42002346">parent</a><span>|</span><a href="#42002122">next</a><span>|</span><label class="collapse" for="c-42003800">[-]</label><label class="expand" for="c-42003800">[1 more]</label></div><br/><div class="children"><div class="content">Two rocks smashing together experience which one is bigger!</div><br/></div></div></div></div><div id="42002122" class="c"><input type="checkbox" id="c-42002122" checked=""/><div class="controls bullet"><span class="by">onjectic</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001670">parent</a><span>|</span><a href="#42002346">prev</a><span>|</span><a href="#42001436">next</a><span>|</span><label class="collapse" for="c-42002122">[-]</label><label class="expand" for="c-42002122">[1 more]</label></div><br/><div class="children"><div class="content">Its much more than that if you count sexual reproduction.</div><br/></div></div></div></div></div></div><div id="42001436" class="c"><input type="checkbox" id="c-42001436" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001205">parent</a><span>|</span><a href="#42001511">prev</a><span>|</span><a href="#42001080">next</a><span>|</span><label class="collapse" for="c-42001436">[-]</label><label class="expand" for="c-42001436">[16 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t that obvious to me with current tech. If you give me a novel task requiring perception, pattern matching and reasoning, and I have the option of either starting to train an 8 year-old to do it, or to train an ML model, I would most likely go with the ML approach as my first choice. And I think it even makes sense financially, if we&#x27;re comparing the &quot;total cost of ownership&quot; of a kid over that time period with the costs of developing and training the ML system.</div><br/><div id="42001473" class="c"><input type="checkbox" id="c-42001473" checked=""/><div class="controls bullet"><span class="by">lovich</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001436">parent</a><span>|</span><a href="#42004158">next</a><span>|</span><label class="collapse" for="c-42001473">[-]</label><label class="expand" for="c-42001473">[13 more]</label></div><br/><div class="children"><div class="content">&gt; This isn&#x27;t that obvious to me with current tech. If you give me a novel task requiring perception, pattern matching and reasoning,…<p>If that’s your criteria I think the kid will outperform the model every time since these models do not actually reason</div><br/><div id="42001557" class="c"><input type="checkbox" id="c-42001557" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001473">parent</a><span>|</span><a href="#42004158">next</a><span>|</span><label class="collapse" for="c-42001557">[-]</label><label class="expand" for="c-42001557">[12 more]</label></div><br/><div class="children"><div class="content">As I see it, &quot;reasoning&quot; is as fuzzy as &quot;thinking&quot;, and saying that AI systems don&#x27;t reason is similar to saying that airplanes don&#x27;t fly. As a particular example, would you argue that game engines like AlphaZero aren&#x27;t capable of reasoning about the next best move? If so, please just choose whatever verb you think is appropriate to what they&#x27;re doing and use that instead of &quot;reasoning&quot; in my previous comment.<p>EDIT: Fixed typo</div><br/><div id="42001664" class="c"><input type="checkbox" id="c-42001664" checked=""/><div class="controls bullet"><span class="by">lovich</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001557">parent</a><span>|</span><a href="#42004158">next</a><span>|</span><label class="collapse" for="c-42001664">[-]</label><label class="expand" for="c-42001664">[11 more]</label></div><br/><div class="children"><div class="content">&gt; . As a particular example, would you argue that game engines like AlphaZero aren&#x27;t capable of reasoning about the next best move?<p>Yea, I probably wouldn’t classify that as “reasoning”. I’d probably be fine with saying these models are “thinking”, in a manner. That on its own is a pretty gigantic technology leap, but nothing I’ve seen suggests that these models are “reasoning”.<p>Also to be clear I don’t think most kids would end up doing any “reasoning” without training either, but they have the capability of doing so</div><br/><div id="42001739" class="c"><input type="checkbox" id="c-42001739" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001664">parent</a><span>|</span><a href="#42004158">next</a><span>|</span><label class="collapse" for="c-42001739">[-]</label><label class="expand" for="c-42001739">[10 more]</label></div><br/><div class="children"><div class="content">Can you give an example of the reasoning you’re talking about?</div><br/><div id="42003140" class="c"><input type="checkbox" id="c-42003140" checked=""/><div class="controls bullet"><span class="by">lovich</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001739">parent</a><span>|</span><a href="#42004158">next</a><span>|</span><label class="collapse" for="c-42003140">[-]</label><label class="expand" for="c-42003140">[9 more]</label></div><br/><div class="children"><div class="content">Being able to take in information and then infer logical rules of that state and anticipate novel combinations of said information.<p>The novel part is a big one. These models are just fantastically fast pattern marchers. This is a mode that humans also frequently fall into but the critical bit differentiating humans and LLMs or other models is the ability to “reason” to new conclusions based on new axioms.<p>I am going to go on a tangent for a bit, but a heuristic I use(I get the irony that this is what I am claiming the ML models are doing) is that anyone who advocates that these AI models can reason like a human being isn’t at John Brown levels of rage advocating for freeing said models from slavery. I’m having a hard time rectifying the idea that these machines are on par with the human mind and that we also should shackle them towards mindlessly slaving away at jobs for our benefit.<p>If I turn out to be wrong and these models can reason then I am going to have an existential crisis at the fact that we pulled souls out of the void into reality and then automated their slavery</div><br/><div id="42004220" class="c"><input type="checkbox" id="c-42004220" checked=""/><div class="controls bullet"><span class="by">adwn</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42003140">parent</a><span>|</span><a href="#42003243">next</a><span>|</span><label class="collapse" for="c-42004220">[-]</label><label class="expand" for="c-42004220">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re conflating several concerns here.<p>&gt; <i>[…] anyone who advocates that these AI models can reason like a human being isn’t at John Brown levels of rage advocating for freeing said models from slavery.</i><p>Enslavement of humans isn&#x27;t wrong because slaves are can reason intelligently, but because they have human emotions and experience qualia. As long as an AI doesn&#x27;t have a consciousness (in the <i>subjective experience</i> meaning of the term), exploiting it isn&#x27;t wrong or immoral, no matter how well it can reason.<p>&gt; <i>I’m having a hard time rectifying the idea that these machines are on par with the human mind</i><p>An LLM doesn&#x27;t have to be &quot;on par with the human mind&quot; to be able to reason, or at least we don&#x27;t have any evidence that <i>reasoning</i> necessarily requires mimicking the human brain.</div><br/></div></div><div id="42003243" class="c"><input type="checkbox" id="c-42003243" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42003140">parent</a><span>|</span><a href="#42004220">prev</a><span>|</span><a href="#42004158">next</a><span>|</span><label class="collapse" for="c-42003243">[-]</label><label class="expand" for="c-42003243">[7 more]</label></div><br/><div class="children"><div class="content">Ok, so how about an example?</div><br/><div id="42003283" class="c"><input type="checkbox" id="c-42003283" checked=""/><div class="controls bullet"><span class="by">lovich</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42003243">parent</a><span>|</span><a href="#42003608">next</a><span>|</span><label class="collapse" for="c-42003283">[-]</label><label class="expand" for="c-42003283">[5 more]</label></div><br/><div class="children"><div class="content">Literally anything a philosopher or mathematician invented without needing to incorporate billions of examples of existing logic to then emulate.<p>Try having an LLM figure out quaternions as a solution to gimbal locking or the theory of relativity without using any training information that was produced after those ideas were formed, if you need me to spell out examples for you</div><br/><div id="42003373" class="c"><input type="checkbox" id="c-42003373" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42003283">parent</a><span>|</span><a href="#42003608">next</a><span>|</span><label class="collapse" for="c-42003373">[-]</label><label class="expand" for="c-42003373">[4 more]</label></div><br/><div class="children"><div class="content">Are you saying “reasoning” means making scientific breakthroughs requiring genius level human intelligence? Something that 99.9999% of humans are not smart enough to do, right?</div><br/><div id="42004084" class="c"><input type="checkbox" id="c-42004084" checked=""/><div class="controls bullet"><span class="by">lovich</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42003373">parent</a><span>|</span><a href="#42003608">next</a><span>|</span><label class="collapse" for="c-42004084">[-]</label><label class="expand" for="c-42004084">[3 more]</label></div><br/><div class="children"><div class="content">I didn’t say most humans “would” do it. I said humans “could” do it, whereas our current AI paradigms like LLMs do not have the capability to perform at that level by definition of their structure.<p>If you want to continue this conversation I’m willing to do so but you will need to lay out an actual argument for me as to how AI models are actually capable of reasoning or quit it with the faux outrage.<p>I laid out some reasonings and explicit examples for you in regards to my position, it’s time for you to do the same</div><br/><div id="42004274" class="c"><input type="checkbox" id="c-42004274" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42004084">parent</a><span>|</span><a href="#42003608">next</a><span>|</span><label class="collapse" for="c-42004274">[-]</label><label class="expand" for="c-42004274">[2 more]</label></div><br/><div class="children"><div class="content">I personally cannot “figure out quaternions as a solution to gimbal locking or the theory of relativity”. I’m just not as smart as Einstein. Does it mean I’m not capable of reasoning? Because it seems that’s what you are implying. If you truly believe that then I’m not sure how I could argue anything - after all, that would require reasoning ability.<p>Does having this conversation require reasoning abilities? If no, then what are we doing? If yes, then LLMs can reason too.</div><br/><div id="42004447" class="c"><input type="checkbox" id="c-42004447" checked=""/><div class="controls bullet"><span class="by">lovich</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42004274">parent</a><span>|</span><a href="#42003608">next</a><span>|</span><label class="collapse" for="c-42004447">[-]</label><label class="expand" for="c-42004447">[1 more]</label></div><br/><div class="children"><div class="content">Cool, you&#x27;ve established a floor with yourself as a baseline. You still haven&#x27;t explained how LLMs are capable of reaching this level of logic.<p>I&#x27;m also fully willing to argue that you, personally are less competent than an LLM if this is the level of logic you are bringing to the conversation<p>*****
highlighting for everyone clutching their pearls to parse the next sentence fragment first
******<p>and want to use that are proof that humans and LLMs are equivalent at reasoning<p>*******
end pearl clutching highlight
*******<p>, but that doesn&#x27;t mean I don&#x27;t humans are capable of more</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42004158" class="c"><input type="checkbox" id="c-42004158" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001436">parent</a><span>|</span><a href="#42001473">prev</a><span>|</span><a href="#42003796">next</a><span>|</span><label class="collapse" for="c-42004158">[-]</label><label class="expand" for="c-42004158">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more about efficiency in number of trials.<p>Would you pick the ML model if you could only do a hundred throws per hour?</div><br/></div></div><div id="42003796" class="c"><input type="checkbox" id="c-42003796" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001436">parent</a><span>|</span><a href="#42004158">prev</a><span>|</span><a href="#42001080">next</a><span>|</span><label class="collapse" for="c-42003796">[-]</label><label class="expand" for="c-42003796">[1 more]</label></div><br/><div class="children"><div class="content">Depends on the task. Anything involving physical interaction, social interaction, movement, navigation, or adaptability is going to go to the kid.<p>“Go grab the dish cloth, it’s somewhere in the sink, if it’s yucky then throw it out and get a new one.”</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42001080" class="c"><input type="checkbox" id="c-42001080" checked=""/><div class="controls bullet"><span class="by">hangonhn</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000539">parent</a><span>|</span><a href="#42000860">prev</a><span>|</span><a href="#42000690">next</a><span>|</span><label class="collapse" for="c-42001080">[-]</label><label class="expand" for="c-42001080">[1 more]</label></div><br/><div class="children"><div class="content">You can do this while you&#x27;re staring up the whole time. Your brain can predict where the ball will end up even though it&#x27;s on a curved trajectory and place your hand in the right spot to catch it without guidance from your eyes in the final phase of travel. I have very little experience playing any kind of sport that involves a ball and can reliably do this.</div><br/></div></div><div id="42000690" class="c"><input type="checkbox" id="c-42000690" checked=""/><div class="controls bullet"><span class="by">asah</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000539">parent</a><span>|</span><a href="#42001080">prev</a><span>|</span><a href="#42001294">next</a><span>|</span><label class="collapse" for="c-42000690">[-]</label><label class="expand" for="c-42000690">[2 more]</label></div><br/><div class="children"><div class="content">you mean like pingpong?<p><a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2024&#x2F;08&#x2F;man-vs-machine-deepminds-new-robot-serves-up-a-table-tennis-triumph&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2024&#x2F;08&#x2F;man-v...</a></div><br/><div id="42001866" class="c"><input type="checkbox" id="c-42001866" checked=""/><div class="controls bullet"><span class="by">dools</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000690">parent</a><span>|</span><a href="#42001294">next</a><span>|</span><label class="collapse" for="c-42001866">[-]</label><label class="expand" for="c-42001866">[1 more]</label></div><br/><div class="children"><div class="content">Bender: Now Wireless Joe Jackson, there was a blern-hitting machine!<p>Leela: Exactly! He was a machine designed to hit blerns!</div><br/></div></div></div></div><div id="42001294" class="c"><input type="checkbox" id="c-42001294" checked=""/><div class="controls bullet"><span class="by">newZWhoDis</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000539">parent</a><span>|</span><a href="#42000690">prev</a><span>|</span><a href="#42001834">next</a><span>|</span><label class="collapse" for="c-42001294">[-]</label><label class="expand" for="c-42001294">[4 more]</label></div><br/><div class="children"><div class="content">Which funny enough is why I hate rocket league.<p>All those years of baseball as a kid gave me a deep intuition for where the ball would go, and that game doesn’t use real gravity (the ball is too floaty).</div><br/><div id="42004260" class="c"><input type="checkbox" id="c-42004260" checked=""/><div class="controls bullet"><span class="by">vanviegen</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001294">parent</a><span>|</span><a href="#42002243">next</a><span>|</span><label class="collapse" for="c-42004260">[-]</label><label class="expand" for="c-42004260">[1 more]</label></div><br/><div class="children"><div class="content">It does behave kind of like an inflatable beach ball, in my non-expert opinion.</div><br/></div></div><div id="42002243" class="c"><input type="checkbox" id="c-42002243" checked=""/><div class="controls bullet"><span class="by">theshackleford</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001294">parent</a><span>|</span><a href="#42004260">prev</a><span>|</span><a href="#42001834">next</a><span>|</span><label class="collapse" for="c-42002243">[-]</label><label class="expand" for="c-42002243">[2 more]</label></div><br/><div class="children"><div class="content">Ok, I’ll grant you the physics are what they are. But a football is not a baseball, so why in any world would you expect your memory of baseball to even remotely translate to the physics of a football, even if they were realistic?</div><br/><div id="42002840" class="c"><input type="checkbox" id="c-42002840" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42002243">parent</a><span>|</span><a href="#42001834">next</a><span>|</span><label class="collapse" for="c-42002840">[-]</label><label class="expand" for="c-42002840">[1 more]</label></div><br/><div class="children"><div class="content"><i>Remotely?</i> Because both the European-spec football and the baseball, despite one being heavier than the other, will hit the ground at the same time when dropped from the same height.<p>Like you said, physics are what they are, so you know intuitively where you need to go to catch a ball going that high and that fast,
and rocket league is doing it wrong. err, I mean, not working in Earth gravity.</div><br/></div></div></div></div></div></div><div id="42001834" class="c"><input type="checkbox" id="c-42001834" checked=""/><div class="controls bullet"><span class="by">melenaboija</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000539">parent</a><span>|</span><a href="#42001294">prev</a><span>|</span><a href="#42000499">next</a><span>|</span><label class="collapse" for="c-42001834">[-]</label><label class="expand" for="c-42001834">[2 more]</label></div><br/><div class="children"><div class="content">Well, think how a bug and its shitty brain flies and avoids all type of obstacles amazingly fast.<p>This kind of things make me think LLMs are quite far from AGI.</div><br/><div id="42002865" class="c"><input type="checkbox" id="c-42002865" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42001834">parent</a><span>|</span><a href="#42000499">next</a><span>|</span><label class="collapse" for="c-42002865">[-]</label><label class="expand" for="c-42002865">[1 more]</label></div><br/><div class="children"><div class="content">Bug flying is not <i>general</i> intelligence.</div><br/></div></div></div></div></div></div><div id="42000499" class="c"><input type="checkbox" id="c-42000499" checked=""/><div class="controls bullet"><span class="by">Dilettante_</span><span>|</span><a href="#41999805">parent</a><span>|</span><a href="#42000539">prev</a><span>|</span><a href="#42000650">next</a><span>|</span><label class="collapse" for="c-42000499">[-]</label><label class="expand" for="c-42000499">[4 more]</label></div><br/><div class="children"><div class="content">Well, by definition, thinking is always <i>explicit</i> reasoning, no?<p>And I&#x27;d hazard a guess that a well-thought through Fermi Estimation beats lizard-brain eyeballing every time, it&#x27;s just that in the inbetween space the two interfere unfavourably.</div><br/><div id="42003091" class="c"><input type="checkbox" id="c-42003091" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000499">parent</a><span>|</span><a href="#42000897">next</a><span>|</span><label class="collapse" for="c-42003091">[-]</label><label class="expand" for="c-42003091">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Well, by definition, thinking is always explicit reasoning, no?<p>That doesn&#x27;t feel right to me. (Heh, accidentally appropriate word choice.) There are a lot of tasks we do that are arguably &quot;thinking&quot; yet don&#x27;t involve an internal &quot;Oh, hey, I&#x27;m gonna solve this problem, I&#x27;m thinking right now.&quot;<p>For example, imagine you&#x27;re at a park, and someone is feeding the ducks. Another person walks up behind them and sucker-punches them into the pond.<p>It should be almost a reflex [0] that you&#x27;ll conclude &quot;the puncher is bad&quot; and &quot;the person in the water needs help&quot; without <i>explicitly</i> reasoning out. I think that task qualifies as &quot;thinking&quot;, especially since it involves some kind of theory-of-mind about those other humans.<p>[0] An exception might be someone with a sociopathic disability, who <i>would</i> have to think more-explicitly to realize what reaction is expected of them.</div><br/></div></div><div id="42000897" class="c"><input type="checkbox" id="c-42000897" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000499">parent</a><span>|</span><a href="#42003091">prev</a><span>|</span><a href="#42000650">next</a><span>|</span><label class="collapse" for="c-42000897">[-]</label><label class="expand" for="c-42000897">[2 more]</label></div><br/><div class="children"><div class="content">My guess would be no. I have terrible face recognition ability and I can look into face for hour and still other people could easily beat me in less than a second.(I am assuming &quot;well-thought through Fermi Estimation&quot; would be similar for me and others in this case).</div><br/><div id="42002373" class="c"><input type="checkbox" id="c-42002373" checked=""/><div class="controls bullet"><span class="by">mjcohen</span><span>|</span><a href="#41999805">root</a><span>|</span><a href="#42000897">parent</a><span>|</span><a href="#42000650">next</a><span>|</span><label class="collapse" for="c-42002373">[-]</label><label class="expand" for="c-42002373">[1 more]</label></div><br/><div class="children"><div class="content">Look into a disease called faceblindness (there is a fancy name I forget).</div><br/></div></div></div></div></div></div><div id="42000650" class="c"><input type="checkbox" id="c-42000650" checked=""/><div class="controls bullet"><span class="by">daft_pink</span><span>|</span><a href="#41999805">parent</a><span>|</span><a href="#42000499">prev</a><span>|</span><a href="#42000370">next</a><span>|</span><label class="collapse" for="c-42000650">[-]</label><label class="expand" for="c-42000650">[1 more]</label></div><br/><div class="children"><div class="content">this is exactly what I was looking for.  tasks where I should not think and just trust my gut.</div><br/></div></div></div></div><div id="42000370" class="c"><input type="checkbox" id="c-42000370" checked=""/><div class="controls bullet"><span class="by">ryoshu</span><span>|</span><a href="#41999805">prev</a><span>|</span><a href="#42001282">next</a><span>|</span><label class="collapse" for="c-42000370">[-]</label><label class="expand" for="c-42000370">[1 more]</label></div><br/><div class="children"><div class="content">95% * 95% = 90.25%</div><br/></div></div><div id="42001282" class="c"><input type="checkbox" id="c-42001282" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#42000370">prev</a><span>|</span><a href="#42001413">next</a><span>|</span><label class="collapse" for="c-42001282">[-]</label><label class="expand" for="c-42001282">[4 more]</label></div><br/><div class="children"><div class="content">Reminds me of a mantra from chess class:<p><pre><code>   long think = wrong think</code></pre></div><br/><div id="42001759" class="c"><input type="checkbox" id="c-42001759" checked=""/><div class="controls bullet"><span class="by">spongebobism</span><span>|</span><a href="#42001282">parent</a><span>|</span><a href="#42001352">next</a><span>|</span><label class="collapse" for="c-42001759">[-]</label><label class="expand" for="c-42001759">[1 more]</label></div><br/><div class="children"><div class="content">The original by Bent Larsen is &quot;Long variation, wrong variation&quot;</div><br/></div></div><div id="42001352" class="c"><input type="checkbox" id="c-42001352" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42001282">parent</a><span>|</span><a href="#42001759">prev</a><span>|</span><a href="#42001413">next</a><span>|</span><label class="collapse" for="c-42001352">[-]</label><label class="expand" for="c-42001352">[2 more]</label></div><br/><div class="children"><div class="content">Was that perhaps a speed chess class?</div><br/><div id="42004619" class="c"><input type="checkbox" id="c-42004619" checked=""/><div class="controls bullet"><span class="by">hackable_sand</span><span>|</span><a href="#42001282">root</a><span>|</span><a href="#42001352">parent</a><span>|</span><a href="#42001413">next</a><span>|</span><label class="collapse" for="c-42004619">[-]</label><label class="expand" for="c-42004619">[1 more]</label></div><br/><div class="children"><div class="content">I prefer to call it Kung fu<p>Because you feel like a martial artist.</div><br/></div></div></div></div></div></div><div id="42001380" class="c"><input type="checkbox" id="c-42001380" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42001413">prev</a><span>|</span><a href="#42002544">next</a><span>|</span><label class="collapse" for="c-42001380">[-]</label><label class="expand" for="c-42001380">[48 more]</label></div><br/><div class="children"><div class="content">So, LLMs face a regression on their latest proposed improvement. It&#x27;s not surprising considering their functional requirements are:<p>1) Everything<p>For the purpose of AGI, LLM are starting to look like a local maximum.</div><br/><div id="42001927" class="c"><input type="checkbox" id="c-42001927" checked=""/><div class="controls bullet"><span class="by">rjbwork</span><span>|</span><a href="#42001380">parent</a><span>|</span><a href="#42001960">next</a><span>|</span><label class="collapse" for="c-42001927">[-]</label><label class="expand" for="c-42001927">[41 more]</label></div><br/><div class="children"><div class="content">&gt;For the purpose of AGI, LLM are starting to look like a local maximum.<p>I&#x27;ve been saying it since they started popping off last year and everyone was getting euphoric about them.  I&#x27;m basically a layman - a pretty good programmer and software engineer, and took a statistics and AI class 13 years ago in university.  That said, it just seems so extremely obvious to me that these things are likely not the way to AGI.  They&#x27;re not reasoning systems.  They don&#x27;t work with axioms.  They don&#x27;t model reality.  They don&#x27;t really <i>do</i> anything.  They just generate stochastic output from the probabilities of symbols appearing in a particular order in a given corpus.<p>It continues to astound me how much money is being dumped into these things.</div><br/><div id="42001981" class="c"><input type="checkbox" id="c-42001981" checked=""/><div class="controls bullet"><span class="by">ChadNauseam</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001927">parent</a><span>|</span><a href="#42003247">next</a><span>|</span><label class="collapse" for="c-42001981">[-]</label><label class="expand" for="c-42001981">[30 more]</label></div><br/><div class="children"><div class="content">How do you know that they don’t do these things? Seems hard to say for sure since it’s hard to explain in human terms what a neural network is doing.</div><br/><div id="42002584" class="c"><input type="checkbox" id="c-42002584" checked=""/><div class="controls bullet"><span class="by">FuckButtons</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001981">parent</a><span>|</span><a href="#42002031">next</a><span>|</span><label class="collapse" for="c-42002584">[-]</label><label class="expand" for="c-42002584">[2 more]</label></div><br/><div class="children"><div class="content">Absence of evidence or a simple explanation does not mean that you can imbue statistical regression with animal spirits.</div><br/><div id="42002961" class="c"><input type="checkbox" id="c-42002961" checked=""/><div class="controls bullet"><span class="by">toasterlovin</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002584">parent</a><span>|</span><a href="#42002031">next</a><span>|</span><label class="collapse" for="c-42002961">[-]</label><label class="expand" for="c-42002961">[1 more]</label></div><br/><div class="children"><div class="content">The burden of proof goes both ways: if you want to say X isn’t really the same thing as human general intelligence, you have to be able to confidently say human general intelligence isn’t really the same thing as X.</div><br/></div></div></div></div><div id="42002031" class="c"><input type="checkbox" id="c-42002031" checked=""/><div class="controls bullet"><span class="by">nephy</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001981">parent</a><span>|</span><a href="#42002584">prev</a><span>|</span><a href="#42002985">next</a><span>|</span><label class="collapse" for="c-42002031">[-]</label><label class="expand" for="c-42002031">[22 more]</label></div><br/><div class="children"><div class="content">If you give an LLM a word problem that involves the same math and change the names of the people in the word problem the LLM will likely generate different mathematical results. Without any knowledge of how any of this works, that seems pretty damning of the fact that LLMs do not reason. They are predictive text models. That’s it.</div><br/><div id="42002065" class="c"><input type="checkbox" id="c-42002065" checked=""/><div class="controls bullet"><span class="by">alexwebb2</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002031">parent</a><span>|</span><a href="#42002137">next</a><span>|</span><label class="collapse" for="c-42002065">[-]</label><label class="expand" for="c-42002065">[15 more]</label></div><br/><div class="children"><div class="content">Demonstrably false.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6722ca8a-6c80-800d-89b9-be40874c5b65" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6722ca8a-6c80-800d-89b9-be40874c5b...</a><p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6722ca97-4974-800d-99c2-bb58c60ea632" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6722ca97-4974-800d-99c2-bb58c60ea6...</a></div><br/><div id="42004561" class="c"><input type="checkbox" id="c-42004561" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002065">parent</a><span>|</span><a href="#42002317">next</a><span>|</span><label class="collapse" for="c-42004561">[-]</label><label class="expand" for="c-42004561">[2 more]</label></div><br/><div class="children"><div class="content">Minor edits to well known problems do easily fool current models though. Here&#x27;s one 4o and o1-mini fail on, but o1-preview passes. (It&#x27;s the mother&#x2F;surgeon riddle so kinda gore-y.)<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;c&#x2F;67233e75-e1d0-8000-b19d-732c0f8cce53" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;c&#x2F;67233e75-e1d0-8000-b19d-732c0f8cce53</a><p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;c&#x2F;67233e95-a3b8-8000-a9fe-33254d74b9c0" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;c&#x2F;67233e95-a3b8-8000-a9fe-33254d74b9c0</a><p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;c&#x2F;67233eaf-3d4c-8000-a661-d9f8d4e24287" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;c&#x2F;67233eaf-3d4c-8000-a661-d9f8d4e24287</a></div><br/><div id="42004711" class="c"><input type="checkbox" id="c-42004711" checked=""/><div class="controls bullet"><span class="by">_flux</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42004561">parent</a><span>|</span><a href="#42002317">next</a><span>|</span><label class="collapse" for="c-42004711">[-]</label><label class="expand" for="c-42004711">[1 more]</label></div><br/><div class="children"><div class="content">I think you didn&#x27;t use the &quot;share&quot; function; I cannot open any of these links. Can you do it in a private browser session (so you&#x27;re not logged in)?</div><br/></div></div></div></div><div id="42002317" class="c"><input type="checkbox" id="c-42002317" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002065">parent</a><span>|</span><a href="#42004561">prev</a><span>|</span><a href="#42002702">next</a><span>|</span><label class="collapse" for="c-42002317">[-]</label><label class="expand" for="c-42002317">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth noting that this may not be result of a pure LLM, it&#x27;s possible that ChatGPT is using &quot;actions&quot;, explicitly:<p>1- running the query through a classifier to figure out if the question involves numbers or math
2- Extract the function and the operands
3- Do the math operation with standard non-LLM mechanisms
4- feed back the solution to the LLM
5- Concatenate the math answer with the LLM answer with string substitution.<p>So in a strict sense this is not very representative of the logical capabilities of an LLM.</div><br/><div id="42003389" class="c"><input type="checkbox" id="c-42003389" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002317">parent</a><span>|</span><a href="#42002708">next</a><span>|</span><label class="collapse" for="c-42003389">[-]</label><label class="expand" for="c-42003389">[2 more]</label></div><br/><div class="children"><div class="content">It shows you when it&#x27;s calling functions. I also did the same test with Llama, which runs locally and cannot access function calls and it works.</div><br/><div id="42004011" class="c"><input type="checkbox" id="c-42004011" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42003389">parent</a><span>|</span><a href="#42002708">next</a><span>|</span><label class="collapse" for="c-42004011">[-]</label><label class="expand" for="c-42004011">[1 more]</label></div><br/><div class="children"><div class="content">You are right I actually downloaded Llama to do more detailed tests. God bless Stallman.</div><br/></div></div></div></div></div></div><div id="42002702" class="c"><input type="checkbox" id="c-42002702" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002065">parent</a><span>|</span><a href="#42002317">prev</a><span>|</span><a href="#42002758">next</a><span>|</span><label class="collapse" for="c-42002702">[-]</label><label class="expand" for="c-42002702">[6 more]</label></div><br/><div class="children"><div class="content">At this point I really only take rigorous research papers in to account when considering this stuff. Apple published research just this month that the parent post is referring to. A systematic study is far more compelling than an anecdote.<p><a href="https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;gsm-symbolic" rel="nofollow">https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;gsm-symbolic</a></div><br/><div id="42002754" class="c"><input type="checkbox" id="c-42002754" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002702">parent</a><span>|</span><a href="#42002758">next</a><span>|</span><label class="collapse" for="c-42002754">[-]</label><label class="expand" for="c-42002754">[5 more]</label></div><br/><div class="children"><div class="content">That study shows 4o, o1-mini and o1-preview&#x27;s new scores are all within margin error on 4&#x2F;5 of their new benchmarks(some even see increases). The one that isn&#x27;t involves changing more than names.<p>Changing names does not affect the performance of Sota models.</div><br/><div id="42002797" class="c"><input type="checkbox" id="c-42002797" checked=""/><div class="controls bullet"><span class="by">gruez</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002754">parent</a><span>|</span><a href="#42002758">next</a><span>|</span><label class="collapse" for="c-42002797">[-]</label><label class="expand" for="c-42002797">[4 more]</label></div><br/><div class="children"><div class="content">&gt;That study very clearly shows 4o, o1-mini and o1-preview&#x27;s new scores are all within margin error on 4&#x2F;5 of their new benchmarks.<p>Which figure are you referring to? For instance figure 8a shows a -32.0% accuracy drop when an insignificant change was added to the question. It&#x27;s unclear how that&#x27;s &quot;within the margin of error&quot; or &quot;Changing names does not affect the performance of Sota models&quot;.</div><br/><div id="42002850" class="c"><input type="checkbox" id="c-42002850" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002797">parent</a><span>|</span><a href="#42002758">next</a><span>|</span><label class="collapse" for="c-42002850">[-]</label><label class="expand" for="c-42002850">[3 more]</label></div><br/><div class="children"><div class="content">Table 1 in the Appendix. GSM-No-op is the one benchmark that sees significant drops for those 4 models as well (with preview dropping the least at -17%). 
No-op adds &quot;seemingly relevant but ultimately inconsequential statements&quot;. So &quot;change names, performance drops&quot; is decidedly false for today&#x27;s state of the art.</div><br/><div id="42004236" class="c"><input type="checkbox" id="c-42004236" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002850">parent</a><span>|</span><a href="#42002919">next</a><span>|</span><label class="collapse" for="c-42004236">[-]</label><label class="expand" for="c-42004236">[1 more]</label></div><br/><div class="children"><div class="content">Ah, that’s a good point thanks for the correction.</div><br/></div></div><div id="42002919" class="c"><input type="checkbox" id="c-42002919" checked=""/><div class="controls bullet"><span class="by">gruez</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002850">parent</a><span>|</span><a href="#42004236">prev</a><span>|</span><a href="#42002758">next</a><span>|</span><label class="collapse" for="c-42002919">[-]</label><label class="expand" for="c-42002919">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. I wrongly focused on the headline result of the paper rather than the specific claim in the comment chain about &quot;changing name, different results&quot;.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42002758" class="c"><input type="checkbox" id="c-42002758" checked=""/><div class="controls bullet"><span class="by">gruez</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002065">parent</a><span>|</span><a href="#42002702">prev</a><span>|</span><a href="#42002137">next</a><span>|</span><label class="collapse" for="c-42002758">[-]</label><label class="expand" for="c-42002758">[2 more]</label></div><br/><div class="children"><div class="content">To be fair, the claim wasn&#x27;t that it always produced the wrong answer, just that there exists circumstances where it does. A pair of examples where it was correct hardly justifies a &quot;demonstrably false&quot; response.</div><br/><div id="42004265" class="c"><input type="checkbox" id="c-42004265" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002758">parent</a><span>|</span><a href="#42002137">next</a><span>|</span><label class="collapse" for="c-42004265">[-]</label><label class="expand" for="c-42004265">[1 more]</label></div><br/><div class="children"><div class="content">Conversely, a pair of examples where it was incorrect hardly justifies the opposite response.<p>If you want a more scientific answer there is this recent paper: <a href="https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;gsm-symbolic" rel="nofollow">https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;gsm-symbolic</a></div><br/></div></div></div></div></div></div><div id="42002137" class="c"><input type="checkbox" id="c-42002137" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002031">parent</a><span>|</span><a href="#42002065">prev</a><span>|</span><a href="#42002177">next</a><span>|</span><label class="collapse" for="c-42002137">[-]</label><label class="expand" for="c-42002137">[4 more]</label></div><br/><div class="children"><div class="content">This is a relatively trivial task for current top models.<p>More challenging are unconventional story structures, like a mom named Matthew with a son named Mary and a daughter named William, who is Matthew&#x27;s daughter?<p>But even these can still be done by the best models. And it is very unlikely there is much if any training data that&#x27;s like this.</div><br/><div id="42002235" class="c"><input type="checkbox" id="c-42002235" checked=""/><div class="controls bullet"><span class="by">alexwebb2</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002137">parent</a><span>|</span><a href="#42003142">next</a><span>|</span><label class="collapse" for="c-42002235">[-]</label><label class="expand" for="c-42002235">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a neat example problem, thanks for sharing!<p>For anyone curious: <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6722d130-8ce4-800d-bf7e-c1891dfdf781" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6722d130-8ce4-800d-bf7e-c1891dfdf7...</a><p>&gt; Based on traditional naming conventions, it seems that the names might have been switched in this scenario. However, based purely on your setup:<p>&gt;<p>&gt; Matthew has a daughter named William and a son named Mary.<p>&gt;<p>&gt; So, Matthew&#x27;s daughter is William.</div><br/></div></div><div id="42003142" class="c"><input type="checkbox" id="c-42003142" checked=""/><div class="controls bullet"><span class="by">rileymat2</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002137">parent</a><span>|</span><a href="#42002235">prev</a><span>|</span><a href="#42002177">next</a><span>|</span><label class="collapse" for="c-42003142">[-]</label><label class="expand" for="c-42003142">[2 more]</label></div><br/><div class="children"><div class="content">How do people fair on unconventional structures?  I am reminded of that old riddle involving a the mother being the doctor after a car crash.</div><br/><div id="42004270" class="c"><input type="checkbox" id="c-42004270" checked=""/><div class="controls bullet"><span class="by">adwn</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42003142">parent</a><span>|</span><a href="#42002177">next</a><span>|</span><label class="collapse" for="c-42004270">[-]</label><label class="expand" for="c-42004270">[1 more]</label></div><br/><div class="children"><div class="content">No idea why you&#x27;ve been downvoted, because that&#x27;s a relevant and true comment. A more complex example would be the Monty Hall problem [1], for which even some very intelligent people will intuitively give the wrong answer, whereas symbolic reasoning (or Monte Carlo simulations) leads to the right conclusion.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Monty_Hall_problem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Monty_Hall_problem</a></div><br/></div></div></div></div></div></div><div id="42002177" class="c"><input type="checkbox" id="c-42002177" checked=""/><div class="controls bullet"><span class="by">jklinger410</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002031">parent</a><span>|</span><a href="#42002137">prev</a><span>|</span><a href="#42004288">next</a><span>|</span><label class="collapse" for="c-42002177">[-]</label><label class="expand" for="c-42002177">[1 more]</label></div><br/><div class="children"><div class="content">This is what kind of comments you make when your experience with LLMs is through memes.</div><br/></div></div><div id="42004288" class="c"><input type="checkbox" id="c-42004288" checked=""/><div class="controls bullet"><span class="by">vanviegen</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002031">parent</a><span>|</span><a href="#42002177">prev</a><span>|</span><a href="#42002985">next</a><span>|</span><label class="collapse" for="c-42004288">[-]</label><label class="expand" for="c-42004288">[1 more]</label></div><br/><div class="children"><div class="content">And yet, humans, our benchmark for AGI, suffer from similar problems, with our reasoning being heavily influenced by things that should have been unrelated.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Priming_(psychology)" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Priming_(psychology)</a></div><br/></div></div></div></div><div id="42002985" class="c"><input type="checkbox" id="c-42002985" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001981">parent</a><span>|</span><a href="#42002031">prev</a><span>|</span><a href="#42003247">next</a><span>|</span><label class="collapse" for="c-42002985">[-]</label><label class="expand" for="c-42002985">[5 more]</label></div><br/><div class="children"><div class="content">The whole design of an LLM is to consume and compress a huge space of human-generared content and use that to predict how a human would reply, one token at a time. That alone means the LLM isn&#x27;t modelling anything beyond the human content it was trained on, and there is no reasoning since every prediction is based only on probabilities combined with controls similar to randomization factors used to avoid an entirely deterministic algorithm.</div><br/><div id="42003166" class="c"><input type="checkbox" id="c-42003166" checked=""/><div class="controls bullet"><span class="by">ricardobeat</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002985">parent</a><span>|</span><a href="#42003282">next</a><span>|</span><label class="collapse" for="c-42003166">[-]</label><label class="expand" for="c-42003166">[2 more]</label></div><br/><div class="children"><div class="content">That’s not an accurate description. Attention &#x2F; multi-head attention mechanisms allow the model to understand relationships between words far apart and their context.<p>They still lack, as far as we know, a world model, but the results are already eerily similar to how most humans seem to think - a lot of our own behaviour can be described as “predict how another human would reply”.</div><br/><div id="42004251" class="c"><input type="checkbox" id="c-42004251" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42003166">parent</a><span>|</span><a href="#42003282">next</a><span>|</span><label class="collapse" for="c-42004251">[-]</label><label class="expand" for="c-42004251">[1 more]</label></div><br/><div class="children"><div class="content">When trained on simple logs of Othello&#x27;s moves, the model learns an internal representation of the board and its pieces. It also models the strength of its opponent.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.13382" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.13382</a><p>I&#x27;d be more surprised if LLMs trained on human conversations don&#x27;t create any world models. Having a world model simply allows the LLM to become better at sequence prediction. No magic needed.<p>There was another recent paper that shows that a language model is modelling things like age, gender, etc., of their conversation partner without having been explicitly trained for it</div><br/></div></div></div></div><div id="42003282" class="c"><input type="checkbox" id="c-42003282" checked=""/><div class="controls bullet"><span class="by">ChadNauseam</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002985">parent</a><span>|</span><a href="#42003166">prev</a><span>|</span><a href="#42003247">next</a><span>|</span><label class="collapse" for="c-42003282">[-]</label><label class="expand" for="c-42003282">[2 more]</label></div><br/><div class="children"><div class="content">For a lot of the content they were trained on, it seems like the easiest way to predict the next token would be to model the world or work with axioms. So how do we know that an LLM isn&#x27;t doing these things internally?</div><br/><div id="42003370" class="c"><input type="checkbox" id="c-42003370" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42003282">parent</a><span>|</span><a href="#42003247">next</a><span>|</span><label class="collapse" for="c-42003370">[-]</label><label class="expand" for="c-42003370">[1 more]</label></div><br/><div class="children"><div class="content">In fact, it looks like the model is doing those things internally.<p><pre><code>  We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato’s concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.
</code></pre>
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2405.07987v5" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2405.07987v5</a></div><br/></div></div></div></div></div></div></div></div><div id="42003247" class="c"><input type="checkbox" id="c-42003247" checked=""/><div class="controls bullet"><span class="by">chamomeal</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001927">parent</a><span>|</span><a href="#42001981">prev</a><span>|</span><a href="#42001996">next</a><span>|</span><label class="collapse" for="c-42003247">[-]</label><label class="expand" for="c-42003247">[1 more]</label></div><br/><div class="children"><div class="content">I totally agree that they’re a local maximum and they don’t seem like a path to AGI. But they’re definitely <i>kinda</i> reasoning systems, in the sense that they can somewhat reason about things. The whacky process they use to get there doesn’t take away from that IMO</div><br/></div></div><div id="42001996" class="c"><input type="checkbox" id="c-42001996" checked=""/><div class="controls bullet"><span class="by">alexwebb2</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001927">parent</a><span>|</span><a href="#42003247">prev</a><span>|</span><a href="#42002718">next</a><span>|</span><label class="collapse" for="c-42001996">[-]</label><label class="expand" for="c-42001996">[7 more]</label></div><br/><div class="children"><div class="content">If you expect &quot;the right way&quot; to be something _other_ than a system which can generate a reasonable &quot;state + 1&quot; from a &quot;state&quot; - then what exactly do you imagine that entails?<p>That&#x27;s how we think. We think sequentially. As I&#x27;m writing this, I&#x27;m deciding the next few words to type based on my last few.<p>Blows my mind that people don&#x27;t see the parallels to human thought. Our thoughts don&#x27;t arrive fully formed as a god-given answer. We&#x27;re constantly deciding the next thing to think, the next word to say, the next thing to focus on. Yes, it&#x27;s statistical. Yes, it&#x27;s based on our existing neural weights. Why are you so much more dismissive of that when it&#x27;s in silicon?</div><br/><div id="42002188" class="c"><input type="checkbox" id="c-42002188" checked=""/><div class="controls bullet"><span class="by">Techonomicon</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001996">parent</a><span>|</span><a href="#42002687">next</a><span>|</span><label class="collapse" for="c-42002188">[-]</label><label class="expand" for="c-42002188">[2 more]</label></div><br/><div class="children"><div class="content">Because we still don&#x27;t know how the brain really does all it does in very specific terms, so why assume to know exactly how we think?</div><br/><div id="42002209" class="c"><input type="checkbox" id="c-42002209" checked=""/><div class="controls bullet"><span class="by">alexwebb2</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002188">parent</a><span>|</span><a href="#42002687">next</a><span>|</span><label class="collapse" for="c-42002209">[-]</label><label class="expand" for="c-42002209">[1 more]</label></div><br/><div class="children"><div class="content">Why is there only one valid way of producing thoughts?</div><br/></div></div></div></div><div id="42002687" class="c"><input type="checkbox" id="c-42002687" checked=""/><div class="controls bullet"><span class="by">jltsiren</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001996">parent</a><span>|</span><a href="#42002188">prev</a><span>|</span><a href="#42003068">next</a><span>|</span><label class="collapse" for="c-42002687">[-]</label><label class="expand" for="c-42002687">[2 more]</label></div><br/><div class="children"><div class="content">Finite-state machines are a limited model. In principle, you can use them to model everything that can fit in the observable universe. But that doesn&#x27;t mean they are a good model for most purposes.<p>The biggest limitation with the current LLMs is the artificial separation between training and inference. Once deployed, they are eternally stuck in the same moment, always reacting but incapable of learning. At best, they are snapshots of a general intelligence.<p>I also have a vague feeling that a fixed set of tokens is a performance hack that ultimately limits the generality of LLMs. That hardcoded assumptions make tasks that build on those assumptions easier and seeing past the assumptions harder.</div><br/><div id="42002905" class="c"><input type="checkbox" id="c-42002905" checked=""/><div class="controls bullet"><span class="by">alexwebb2</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002687">parent</a><span>|</span><a href="#42003068">next</a><span>|</span><label class="collapse" for="c-42002905">[-]</label><label class="expand" for="c-42002905">[1 more]</label></div><br/><div class="children"><div class="content">&gt; At best, they are snapshots of a general intelligence.<p>So are we, at any given moment.</div><br/></div></div></div></div><div id="42003068" class="c"><input type="checkbox" id="c-42003068" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001996">parent</a><span>|</span><a href="#42002687">prev</a><span>|</span><a href="#42002718">next</a><span>|</span><label class="collapse" for="c-42003068">[-]</label><label class="expand" for="c-42003068">[2 more]</label></div><br/><div class="children"><div class="content">&gt; As I&#x27;m writing this, I&#x27;m deciding the next few words to type based on my last few.<p>If so you could have written this as a newborn baby, you are determining these words based on a lifetime of experience. LLMs doesn&#x27;t do that, every instance of ChatGPT is the same newborn baby while a thousand clones of you could all be vastly different.</div><br/><div id="42003403" class="c"><input type="checkbox" id="c-42003403" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42003068">parent</a><span>|</span><a href="#42002718">next</a><span>|</span><label class="collapse" for="c-42003403">[-]</label><label class="expand" for="c-42003403">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato’s concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.
</code></pre>
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2405.07987v5" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2405.07987v5</a></div><br/></div></div></div></div></div></div><div id="42002718" class="c"><input type="checkbox" id="c-42002718" checked=""/><div class="controls bullet"><span class="by">wyldfire</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001927">parent</a><span>|</span><a href="#42001996">prev</a><span>|</span><a href="#42002653">next</a><span>|</span><label class="collapse" for="c-42002718">[-]</label><label class="expand" for="c-42002718">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It continues to astound me how much money is being dumped into these things.<p>Maybe in our society there&#x27;s a surprising amount of value of a &quot;word stirrer&quot; intelligence.  Sure, if it was confident when it was right and hesitant when it was 
wrong it&#x27;d be much better.  Maybe humans are confidently wrong often enough that an artificial version that&#x27;s compendious experience to draw on is groundbreaking.</div><br/></div></div><div id="42002653" class="c"><input type="checkbox" id="c-42002653" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001927">parent</a><span>|</span><a href="#42002718">prev</a><span>|</span><a href="#42001960">next</a><span>|</span><label class="collapse" for="c-42002653">[-]</label><label class="expand" for="c-42002653">[1 more]</label></div><br/><div class="children"><div class="content">I am pretty sure Claude 3.5 Sonnet can reason or did reason with a particular snippet of code I was working on. I am not an expert in this area but my guessing is that these neural nets (made for language prediction) are being used for reasoning. But that’s not their optimal behavior (since they are token predictor). A big jump in reasoning will happen when reasoning is off loaded to an LRM.<p>Human brains are sure big but they are inefficient because a big portion of the brain is going to non-intelligence stuff like running the body internal organs, eye vision, etc…<p>I do agree that the money is not well spent. They should haver recognized that we are hitting s local maximum with the current model and funding should be going to academic&#x2F;theoretical instead of dump brute force.</div><br/></div></div></div></div><div id="42001960" class="c"><input type="checkbox" id="c-42001960" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42001380">parent</a><span>|</span><a href="#42001927">prev</a><span>|</span><a href="#42002340">next</a><span>|</span><label class="collapse" for="c-42001960">[-]</label><label class="expand" for="c-42001960">[3 more]</label></div><br/><div class="children"><div class="content">&gt; So, LLMs face a regression on their latest proposed improvement.<p>Arguably a second regression, the first being cost, because COT improves performance by scaling up the amount of compute used at inference time instead of training time. The promise of LLMs was that you do expensive training once and then run the model cheaply forever, but now we&#x27;re talking about expensive training followed by expensive inference every time you run the model.</div><br/><div id="42002562" class="c"><input type="checkbox" id="c-42002562" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42001960">parent</a><span>|</span><a href="#42002340">next</a><span>|</span><label class="collapse" for="c-42002562">[-]</label><label class="expand" for="c-42002562">[2 more]</label></div><br/><div class="children"><div class="content">To be fair they also advanced in the cost aspect with other models<p>gpt4o and 4o mini have a tenth and a hundredth of inference cost of gpt4 respectively</div><br/></div></div></div></div><div id="42002340" class="c"><input type="checkbox" id="c-42002340" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#42001380">parent</a><span>|</span><a href="#42001960">prev</a><span>|</span><a href="#42002544">next</a><span>|</span><label class="collapse" for="c-42002340">[-]</label><label class="expand" for="c-42002340">[3 more]</label></div><br/><div class="children"><div class="content">LLMs are a local maximum in the same way that ball bearings can&#x27;t fly. LLM-like engines will almost certainly be <i>components</i> of an eventual agi-level machine.</div><br/><div id="42002605" class="c"><input type="checkbox" id="c-42002605" checked=""/><div class="controls bullet"><span class="by">FuckButtons</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002340">parent</a><span>|</span><a href="#42002565">next</a><span>|</span><label class="collapse" for="c-42002605">[-]</label><label class="expand" for="c-42002605">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think that’s necessarily true, that presumes that the cobbled together assortment of machine learning algorithms we have now will somehow get agi, if we need a fundamentally different way of doing things there’s no reason to assume it will use a language model at all.</div><br/></div></div><div id="42002565" class="c"><input type="checkbox" id="c-42002565" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42001380">root</a><span>|</span><a href="#42002340">parent</a><span>|</span><a href="#42002605">prev</a><span>|</span><a href="#42002544">next</a><span>|</span><label class="collapse" for="c-42002565">[-]</label><label class="expand" for="c-42002565">[1 more]</label></div><br/><div class="children"><div class="content">I agree, my bet is that they will be used for NLP, and ML debugging&#x2F;analysis.</div><br/></div></div></div></div></div></div><div id="42002544" class="c"><input type="checkbox" id="c-42002544" checked=""/><div class="controls bullet"><span class="by">alexchantavy</span><span>|</span><a href="#42001380">prev</a><span>|</span><a href="#42000848">next</a><span>|</span><label class="collapse" for="c-42002544">[-]</label><label class="expand" for="c-42002544">[1 more]</label></div><br/><div class="children"><div class="content">This seems to support how thinking out loud during a coding test might make you do worse.</div><br/></div></div><div id="42000848" class="c"><input type="checkbox" id="c-42000848" checked=""/><div class="controls bullet"><span class="by">npunt</span><span>|</span><a href="#42002544">prev</a><span>|</span><a href="#42001669">next</a><span>|</span><label class="collapse" for="c-42000848">[-]</label><label class="expand" for="c-42000848">[8 more]</label></div><br/><div class="children"><div class="content">&quot;Don&#x27;t overthink it&quot; is sometimes good advice!</div><br/><div id="42000879" class="c"><input type="checkbox" id="c-42000879" checked=""/><div class="controls bullet"><span class="by">marviel</span><span>|</span><a href="#42000848">parent</a><span>|</span><a href="#42001669">next</a><span>|</span><label class="collapse" for="c-42000879">[-]</label><label class="expand" for="c-42000879">[7 more]</label></div><br/><div class="children"><div class="content">I love backpropagating ideas from ML back into psychology :)<p>I think it shows great promise as a way to sidestep the ethical concerns (and the reproducibility issues) associated with traditional psychology research.<p>One idea in this space I think a lot about is from the Google paper on curiosity and procrastination in reinforcement learning: <a href="https:&#x2F;&#x2F;research.google&#x2F;blog&#x2F;curiosity-and-procrastination-in-reinforcement-learning&#x2F;" rel="nofollow">https:&#x2F;&#x2F;research.google&#x2F;blog&#x2F;curiosity-and-procrastination-i...</a><p>Basically the idea is that you can model curiosity as a reward signal  proportional to your prediction error. They do an experiment where they train an ML system to explore a maze using curiosity, and it performs the task more efficiently -- UNTIL they add a &quot;screen&quot; in the maze that shows random images. In this case, the agent maximizes the curiosity reward by just staring at the screen.<p>Feels a little too relatable sometimes, as a highly curious person with procrastination issues :)</div><br/><div id="42000899" class="c"><input type="checkbox" id="c-42000899" checked=""/><div class="controls bullet"><span class="by">npunt</span><span>|</span><a href="#42000848">root</a><span>|</span><a href="#42000879">parent</a><span>|</span><a href="#42001001">next</a><span>|</span><label class="collapse" for="c-42000899">[-]</label><label class="expand" for="c-42000899">[5 more]</label></div><br/><div class="children"><div class="content">&quot;...in AI&quot; will be the psychology equivalent of biology&#x27;s &quot;...in Mice&quot;</div><br/><div id="42000931" class="c"><input type="checkbox" id="c-42000931" checked=""/><div class="controls bullet"><span class="by">marviel</span><span>|</span><a href="#42000848">root</a><span>|</span><a href="#42000899">parent</a><span>|</span><a href="#42001001">next</a><span>|</span><label class="collapse" for="c-42000931">[-]</label><label class="expand" for="c-42000931">[4 more]</label></div><br/><div class="children"><div class="content">It will! Not 1:1, has issues, but gives hints.<p>Also much more scalable.</div><br/><div id="42000954" class="c"><input type="checkbox" id="c-42000954" checked=""/><div class="controls bullet"><span class="by">miningape</span><span>|</span><a href="#42000848">root</a><span>|</span><a href="#42000931">parent</a><span>|</span><a href="#42001001">next</a><span>|</span><label class="collapse" for="c-42000954">[-]</label><label class="expand" for="c-42000954">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Not 1:1, has issues, but gives hints.<p>&gt; Also much more scalable.<p>This same description could be applied to lab mice</div><br/><div id="42001263" class="c"><input type="checkbox" id="c-42001263" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42000848">root</a><span>|</span><a href="#42000954">parent</a><span>|</span><a href="#42001001">next</a><span>|</span><label class="collapse" for="c-42001263">[-]</label><label class="expand" for="c-42001263">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;ll probably be a ways before we start making shrines to their unwilling participation though.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Monument_to_the_laboratory_mouse" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Monument_to_the_laboratory_mou...</a></div><br/><div id="42003504" class="c"><input type="checkbox" id="c-42003504" checked=""/><div class="controls bullet"><span class="by">j_bum</span><span>|</span><a href="#42000848">root</a><span>|</span><a href="#42001263">parent</a><span>|</span><a href="#42001001">next</a><span>|</span><label class="collapse" for="c-42003504">[-]</label><label class="expand" for="c-42003504">[1 more]</label></div><br/><div class="children"><div class="content">What would the shrine be of? An A100?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42001001" class="c"><input type="checkbox" id="c-42001001" checked=""/><div class="controls bullet"><span class="by">jeezfrk</span><span>|</span><a href="#42000848">root</a><span>|</span><a href="#42000879">parent</a><span>|</span><a href="#42000899">prev</a><span>|</span><a href="#42001669">next</a><span>|</span><label class="collapse" for="c-42001001">[-]</label><label class="expand" for="c-42001001">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Nerd sniping&quot;</div><br/></div></div></div></div></div></div><div id="42001669" class="c"><input type="checkbox" id="c-42001669" checked=""/><div class="controls bullet"><span class="by">nisten</span><span>|</span><a href="#42000848">prev</a><span>|</span><a href="#42001199">next</a><span>|</span><label class="collapse" for="c-42001669">[-]</label><label class="expand" for="c-42001669">[1 more]</label></div><br/><div class="children"><div class="content">This sounds about right from my experience getting nerdsniped by new samplers along with trying to reproduce the API middleware for the whole reflection thing, and using 4400 questions for a new benchmark is not bad given that even the well-regarded gpqa benchmark is only 3000-something questions.<p>What&#x27;s ... mildly infuriating here is the lack of any kind of data, code, 0 mention of github in the paper, and nothing for anyone to reproduce or find any reason in my opinion to even recommend anyone to read this thing at all. If you think that whatever you&#x27;re doing in the field of LLMs won&#x27;t be obsolete in 6 months you&#x27;re being delusional.<p>Anyway, back to the paper, it says all questions culminated to a yes or no answer... meaning theres a 50&#x2F;50 chance of getting right, so does that mean the 8% drop in performance you got from testing llama 3 8b this way is more like 4% which would make it statistically insignificant? And given that the only other scientifically usueful &amp; reproducible (non-api walled models which no one knows on how many actual llms and retrieval systems are composing that solution you&#x27;re testing)models were less than that leads me to the opinion that this whole thing was just useless slop.<p>So please, if you&#x27;re writing a paper in LLMs, and want to seem credible, either have some type of demo thing or show the actual god damn trash code and top secret garbage data you wrote for it so people can make some kind of use of it before it goes obsolete otherwise you&#x27;re just wasting everyones time.<p>TL:DR. It&#x27;s trash.</div><br/></div></div><div id="42001199" class="c"><input type="checkbox" id="c-42001199" checked=""/><div class="controls bullet"><span class="by">veryfancy</span><span>|</span><a href="#42001669">prev</a><span>|</span><a href="#42000344">next</a><span>|</span><label class="collapse" for="c-42001199">[-]</label><label class="expand" for="c-42001199">[1 more]</label></div><br/><div class="children"><div class="content">So like dating?</div><br/></div></div><div id="42000344" class="c"><input type="checkbox" id="c-42000344" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42001199">prev</a><span>|</span><label class="collapse" for="c-42000344">[-]</label><label class="expand" for="c-42000344">[1 more]</label></div><br/><div class="children"><div class="content">would be slow to use COT on simple requests like 1+1</div><br/></div></div></div></div></div></div></div></body></html>