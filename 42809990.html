<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737709274090" as="style"/><link rel="stylesheet" href="styles.css?v=1737709274090"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.vectorchord.ai/supercharge-vector-search-with-colbert-rerank-in-postgresql">Supercharge vector search with ColBERT rerank in PostgreSQL</a> <span class="domain">(<a href="https://blog.vectorchord.ai">blog.vectorchord.ai</a>)</span></div><div class="subtext"><span>gaocegege</span> | <span>9 comments</span></div><br/><div><div id="42810575" class="c"><input type="checkbox" id="c-42810575" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42810404">next</a><span>|</span><label class="collapse" for="c-42810575">[-]</label><label class="expand" for="c-42810575">[7 more]</label></div><br/><div class="children"><div class="content">&gt; However, generating sentence embeddings through pooling token embeddings can potentially sacrifice fine-grained details present at the token level. ColBERT overcomes this by representing text as token-level multi-vectors rather than a single, aggregated vector. This approach, leveraging contextual late interaction at the token level, allows ColBERT to retain more nuanced information and improve search accuracy compared to methods relying solely on sentence embeddings.<p>I don&#x27;t know what it is about ColBERT that affords such opaque descriptions, but this is sadly common. I find the above explanation <i>incredibly</i> difficult to parse.<p>I have my own explanation of ColBERT here but I&#x27;m not particularly happy with that either: <a href="https:&#x2F;&#x2F;til.simonwillison.net&#x2F;llms&#x2F;colbert-ragatouille" rel="nofollow">https:&#x2F;&#x2F;til.simonwillison.net&#x2F;llms&#x2F;colbert-ragatouille</a><p>If anyone wants to try explaining ColBERT without using jargon like &quot;token-level multi-vectors&quot; or &quot;contextual late interaction&quot; I&#x27;d love to see a clear description of it!</div><br/><div id="42811352" class="c"><input type="checkbox" id="c-42811352" checked=""/><div class="controls bullet"><span class="by">nighthawk454</span><span>|</span><a href="#42810575">parent</a><span>|</span><a href="#42811052">next</a><span>|</span><label class="collapse" for="c-42811352">[-]</label><label class="expand" for="c-42811352">[2 more]</label></div><br/><div class="children"><div class="content">Usually, we destructively compress (mean-pooling) both the query and the document, and then compare the two compressed forms.<p>With ColBERT, we compare first - at the full token level - for more detailed comparison. Then reduce the full set of comparisons to a single vector. Naturally this takes more memory and compute to do the more comparisons. The idea is it’s worth it because the more detailed comparisons lead to better results<p>tokens —&gt; reduced vector —&gt; comparison<p>Vs<p>tokens —&gt; comparisons —&gt; reduced vector</div><br/><div id="42811503" class="c"><input type="checkbox" id="c-42811503" checked=""/><div class="controls bullet"><span class="by">kroolik</span><span>|</span><a href="#42810575">root</a><span>|</span><a href="#42811352">parent</a><span>|</span><a href="#42811052">next</a><span>|</span><label class="collapse" for="c-42811503">[-]</label><label class="expand" for="c-42811503">[1 more]</label></div><br/><div class="children"><div class="content">Why do you need the vector if you have already compared the query with the result candidate?</div><br/></div></div></div></div><div id="42811052" class="c"><input type="checkbox" id="c-42811052" checked=""/><div class="controls bullet"><span class="by">tadkar</span><span>|</span><a href="#42810575">parent</a><span>|</span><a href="#42811352">prev</a><span>|</span><a href="#42811326">next</a><span>|</span><label class="collapse" for="c-42811052">[-]</label><label class="expand" for="c-42811052">[1 more]</label></div><br/><div class="children"><div class="content">Here’s my understanding. It is intimidating to write a response to you, because you have an exceptionally clear writing style. I hope the more knowledgable HN crowd will correct any errors in fact or presentation style below.<p>Old school word embedding models (like Word2Vec) come up with embeddings by using masked word predictions. You can embed a whole sentence by taking the average of all the word embeddings in the sentence.<p>There are many scenarios where this average fails to distinguish between multiple meanings of a word. For example “fine weather” and “fine hair” both contain “fine” but mean different things.<p>Transformers are great at producing better embeddings by considering context, and using the words in the rest of the sentence to produce a better representation of each word. BERT is a great model to do this.<p>The problem is that if you want to use BERT by itself to compute relevance you need to perform a lot of compute per query because you have to concatenate the query and the document vector to produce a long sequence that can then be “embedded” by BERT. Figure 2c in the ColBERT paper [1]<p>What ColBERT does is to use the fact that BERT can use context from the entire sentence and its attention heads to produce a more nuanced representation of any token in its input. It does this once for all documents in its index. So for example (assuming “fine” was a token) it would embed the “fine” in the sentence “we’re having fine weather today” to a different vector than the fine in “Sarah has fine blond hair”. In ColBERT the size of the output embeddings are usually much smaller than the typical 1024 you might expect from a Word2Vec.<p>Now, if you have a query, you can do the same and produce token level embeddings for all the tokens in the query.<p>Once you have these two contextualised embeddings, you can check for the presence of the particular meaning of a word in the document using the dot product. For example the query “which children have fine hair” matches the document “Sarah had fine blond hair” because the token “fine” is used in the exact same context in both the query and the document and should be picked up by the MaxSim operation.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2004.12832" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2004.12832</a></div><br/></div></div><div id="42811326" class="c"><input type="checkbox" id="c-42811326" checked=""/><div class="controls bullet"><span class="by">thoughtlede</span><span>|</span><a href="#42810575">parent</a><span>|</span><a href="#42811052">prev</a><span>|</span><a href="#42811235">next</a><span>|</span><label class="collapse" for="c-42811326">[-]</label><label class="expand" for="c-42811326">[1 more]</label></div><br/><div class="children"><div class="content">tadkar did a good job at explaining ColBERT. I understood ColBERT well in the context of where it lies on the spectrum of choices.<p>On one side of the spectrum, you reduce each of the documents as well as the query to a lower-dimensional space (aka embeddings) and perform similarity. This has the advantage that the document embeddings could be precomputed. At query time, you only compute the query embedding and compare its similarity with document embeddings. The problem is that the lower-dimensional embedding acts as a decent, but not great, proxy for the documents as well as for the query. Your query-document similarity is only as good as the semantics that could be captured in those lower-dimensional embeddings.<p>On the other side of the spectrum, you consider the query with each document (as a pair) and see how much the query &quot;attends&quot; to each of the documents. The power of trained attention weights means that you get a much reliable similarity score. The problem is that this approach requires you to run attention-forward-pass as many times as there are documents -- for each query. In other words, this has a performance issue.<p>ColBERT sits in the middle of the spectrum. It &quot;attends&quot; to each of the documents separately and captures the lower-dimensional embedding for each token in each document. This we precompute. Once we have done that, we captured the essence of how tokens within a given document attend to each other, and is captured in the token embeddings.<p>Then, at query time, we do the same for each token in the query. And we see which query-token embedding is greatly similar to which document-token embedding. If we find that there is a document which has more tokens that are found to be greatly similar to the query tokens, then we consider that to the best document match. (The degree of similarity between each query-document token is used to score the ranking - it is called Sum of MaxSim).<p>Obviously, attention based similarity, like in the second approach, is better than reducing to token embeddings and scoring similarity. But ColBERT avoids the performance hit compared to the second approach. ColBERT also avoids the lower fidelity of &quot;reducing the entire document to a lower-dimensional space issue&quot; because it reduces each token in the document separately.<p>By the way, the first approach is what bi-encoders do. The second approach is cross-encoding.</div><br/></div></div><div id="42811235" class="c"><input type="checkbox" id="c-42811235" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42810575">parent</a><span>|</span><a href="#42811326">prev</a><span>|</span><a href="#42810782">next</a><span>|</span><label class="collapse" for="c-42811235">[-]</label><label class="expand" for="c-42811235">[1 more]</label></div><br/><div class="children"><div class="content">Translation: typically in BERT a sentence embedding is an single embedding at the &lt;cls&gt; token. In other words, 768 floats.<p>ColBERT gives you a vector for each token in the sequence. For k tokens, your output shape is (k, 768). At the cost of more storage this lets you compare similarity of the query to your document at the token level.<p>Afaik “token level multi vectors” and “contextual late interaction” are not standard terms. They’re more like smashing concepts together in a random order, some sort of linguistic trapdoor function that only makes sense if you knew what it meant to begin with.<p>I share your animosity towards unclear writing. Researchers think it makes them sound smart but frankly I think the opposite. (Unfortunately for me, the converse is not true.)</div><br/></div></div><div id="42810782" class="c"><input type="checkbox" id="c-42810782" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#42810575">parent</a><span>|</span><a href="#42811235">prev</a><span>|</span><a href="#42810404">next</a><span>|</span><label class="collapse" for="c-42810782">[-]</label><label class="expand" for="c-42810782">[1 more]</label></div><br/><div class="children"><div class="content">FYI you have a broken hot linked image in that post.</div><br/></div></div></div></div><div id="42810404" class="c"><input type="checkbox" id="c-42810404" checked=""/><div class="controls bullet"><span class="by">haki</span><span>|</span><a href="#42810575">prev</a><span>|</span><label class="collapse" for="c-42810404">[-]</label><label class="expand" for="c-42810404">[1 more]</label></div><br/><div class="children"><div class="content">See psycopg Identifier for binding table names<p><a href="https:&#x2F;&#x2F;www.psycopg.org&#x2F;psycopg3&#x2F;docs&#x2F;api&#x2F;sql.html#psycopg.sql.Identifier" rel="nofollow">https:&#x2F;&#x2F;www.psycopg.org&#x2F;psycopg3&#x2F;docs&#x2F;api&#x2F;sql.html#psycopg.s...</a></div><br/></div></div></div></div></div></div></div></body></html>