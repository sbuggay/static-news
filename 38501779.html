<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1701594056206" as="style"/><link rel="stylesheet" href="styles.css?v=1701594056206"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://thenewstack.io/how-meta-patches-linux-at-hyperscale/">How Meta patches Linux at hyperscale</a> <span class="domain">(<a href="https://thenewstack.io">thenewstack.io</a>)</span></div><div class="subtext"><span>elorant</span> | <span>90 comments</span></div><br/><div><div id="38503541" class="c"><input type="checkbox" id="c-38503541" checked=""/><div class="controls bullet"><span class="by">TheAceOfHearts</span><span>|</span><a href="#38502010">next</a><span>|</span><label class="collapse" for="c-38503541">[-]</label><label class="expand" for="c-38503541">[30 more]</label></div><br/><div class="children"><div class="content">I wish they mentioned how long a full deployment takes Meta using this method, that seems like an important detail to omit.<p>&gt; So, if you’d rather not have downtime with your servers, data centers, and clouds, follow Meta’s example and use live patching. You’ll be glad you did.<p>Maybe if you&#x27;re working at Meta&#x27;s scale it makes sense... But I think most well designed services and applications should be able to get by just fine with a full reboot of any single server. I can&#x27;t really fathom the complexity of managing millions of servers though.</div><br/><div id="38504808" class="c"><input type="checkbox" id="c-38504808" checked=""/><div class="controls bullet"><span class="by">cortesoft</span><span>|</span><a href="#38503541">parent</a><span>|</span><a href="#38503826">next</a><span>|</span><label class="collapse" for="c-38504808">[-]</label><label class="expand" for="c-38504808">[12 more]</label></div><br/><div class="children"><div class="content">&gt; Maybe if you&#x27;re working at Meta&#x27;s scale it makes sense... But I think most well designed services and applications should be able to get by just fine with a full reboot of any single server.<p>I feel like this should be the opposite... I don&#x27;t work at Meta scale, but I do work for a CDN with 10s of thousands of servers, and everything we do is based on the idea that some machines will always be going down, because some hardware is going to fail every day just from probability. You have to design everything for failure.<p>Given that, it shouldn&#x27;t be hard to take servers out of production for patching and updates.<p>In other words, a hyperscaler is going to have less incentive to minimize down time than smaller shops.</div><br/><div id="38504835" class="c"><input type="checkbox" id="c-38504835" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38504808">parent</a><span>|</span><a href="#38505191">next</a><span>|</span><label class="collapse" for="c-38504835">[-]</label><label class="expand" for="c-38504835">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t follow. Reboot is downtime. Of course your architecture must allow for downtime if it happens, but it&#x27;s lost money either way, your hardware is not doing any useful work while rebooting. So more computers you have, more money is lost. At small scale that&#x27;s not significant, but at large scale that might become significant so there&#x27;s more incentive to reduce downtime.</div><br/><div id="38505094" class="c"><input type="checkbox" id="c-38505094" checked=""/><div class="controls bullet"><span class="by">jonhohle</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38504835">parent</a><span>|</span><a href="#38505558">next</a><span>|</span><label class="collapse" for="c-38505094">[-]</label><label class="expand" for="c-38505094">[4 more]</label></div><br/><div class="children"><div class="content">A reboot, a software deployment (kernel upgrade), server replacement, etc. are all the same process. That simplifies things dramatically. You can micro-optimize the 30s it takes to reboot a server, or you can simplify a runbook to have one process for any “deployment”. Different scenarios require different things but for most “web scale” things that need to be overprovisioned anyway, I’d take the simpler process.</div><br/><div id="38505164" class="c"><input type="checkbox" id="c-38505164" checked=""/><div class="controls bullet"><span class="by">nickstinemates</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38505094">parent</a><span>|</span><a href="#38505558">next</a><span>|</span><label class="collapse" for="c-38505164">[-]</label><label class="expand" for="c-38505164">[3 more]</label></div><br/><div class="children"><div class="content">These servers don&#x27;t take 30s to reboot. Some servers take many minutes. It&#x27;s a lot.</div><br/><div id="38505408" class="c"><input type="checkbox" id="c-38505408" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38505164">parent</a><span>|</span><a href="#38505558">next</a><span>|</span><label class="collapse" for="c-38505408">[-]</label><label class="expand" for="c-38505408">[2 more]</label></div><br/><div class="children"><div class="content">Worse, some just don&#x27;t come back without manual intervention. Power supplies don&#x27;t last forever and might run fine while the machine is on, but after a reboot... boom, gone.</div><br/><div id="38505553" class="c"><input type="checkbox" id="c-38505553" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38505408">parent</a><span>|</span><a href="#38505558">next</a><span>|</span><label class="collapse" for="c-38505553">[-]</label><label class="expand" for="c-38505553">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d prefer kexec to kpatch, then</div><br/></div></div></div></div></div></div></div></div><div id="38505558" class="c"><input type="checkbox" id="c-38505558" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38504835">parent</a><span>|</span><a href="#38505094">prev</a><span>|</span><a href="#38505095">next</a><span>|</span><label class="collapse" for="c-38505558">[-]</label><label class="expand" for="c-38505558">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you need to overprovision the server.a little bit.<p>But you got a much simpler process.<p>Process ain&#x27;t free either.</div><br/></div></div><div id="38505095" class="c"><input type="checkbox" id="c-38505095" checked=""/><div class="controls bullet"><span class="by">tentacleuno</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38504835">parent</a><span>|</span><a href="#38505558">prev</a><span>|</span><a href="#38505191">next</a><span>|</span><label class="collapse" for="c-38505095">[-]</label><label class="expand" for="c-38505095">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t it more significant at smaller scale?  That is, if you have less computers running to serve requests, the downtime of the singular system will be more pronounced (as opposed to rebooting one machine out of 20 in a rack).</div><br/></div></div></div></div><div id="38505191" class="c"><input type="checkbox" id="c-38505191" checked=""/><div class="controls bullet"><span class="by">MarkSweep</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38504808">parent</a><span>|</span><a href="#38504835">prev</a><span>|</span><a href="#38503826">next</a><span>|</span><label class="collapse" for="c-38505191">[-]</label><label class="expand" for="c-38505191">[4 more]</label></div><br/><div class="children"><div class="content">Edit: ignore the below numbers, I got hours and minutes confused.<p>If you each sever spends 7.5 minutes each month rebooting, that is 1% of all your computers wasted. If you have 10,000 servers, that’s worth 100 severs. If you have 1 million servers, that’s worth 10,000 servers. If each server costs $10,000, that’s $100 million dollars of compute capacity. You can see how that amount of lost computer capacity can start to justify spending engineering time on driving down the amount of time servers are rebooting.</div><br/><div id="38505248" class="c"><input type="checkbox" id="c-38505248" checked=""/><div class="controls bullet"><span class="by">sghiassy</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38505191">parent</a><span>|</span><a href="#38503826">next</a><span>|</span><label class="collapse" for="c-38505248">[-]</label><label class="expand" for="c-38505248">[3 more]</label></div><br/><div class="children"><div class="content">There’s 1,440 minutes per day. In a month with 30 days that’s 43,200 minutes per month.<p>7.5min&#x2F;43,200min = 0.00017361<p>Where are you getting 1% of all your computers wasted per month???</div><br/><div id="38505380" class="c"><input type="checkbox" id="c-38505380" checked=""/><div class="controls bullet"><span class="by">MarkSweep</span><span>|</span><a href="#38503541">root</a><span>|</span><a href="#38505248">parent</a><span>|</span><a href="#38505367">next</a><span>|</span><label class="collapse" for="c-38505380">[-]</label><label class="expand" for="c-38505380">[1 more]</label></div><br/><div class="children"><div class="content">Sorry I got hours and minutes confused and overstated the benefits.<p>So the correct numbers would be 7.5 minutes of downtime divided by 43,200 minutes times 1,000,000 servers. That’s 173 servers wasted. That is probably still enough servers wasted to devote some engineering time to increasing utilization.</div><br/></div></div></div></div></div></div></div></div><div id="38503826" class="c"><input type="checkbox" id="c-38503826" checked=""/><div class="controls bullet"><span class="by">sweettea</span><span>|</span><a href="#38503541">parent</a><span>|</span><a href="#38504808">prev</a><span>|</span><a href="#38504450">next</a><span>|</span><label class="collapse" for="c-38503826">[-]</label><label class="expand" for="c-38503826">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=ILTqn1EYIXQ" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=ILTqn1EYIXQ</a> is the original talk, which says it takes 4 days to deploy a KLP to the whole fleet.</div><br/></div></div><div id="38504450" class="c"><input type="checkbox" id="c-38504450" checked=""/><div class="controls bullet"><span class="by">mattboardman</span><span>|</span><a href="#38503541">parent</a><span>|</span><a href="#38503826">prev</a><span>|</span><a href="#38503719">next</a><span>|</span><label class="collapse" for="c-38504450">[-]</label><label class="expand" for="c-38504450">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s more in-depth information on that subject here:<p><a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;conference&#x2F;osdi23&#x2F;presentation&#x2F;grubic" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.usenix.org&#x2F;conference&#x2F;osdi23&#x2F;presentation&#x2F;grubic</a></div><br/></div></div></div></div><div id="38502010" class="c"><input type="checkbox" id="c-38502010" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#38503541">prev</a><span>|</span><a href="#38503552">next</a><span>|</span><label class="collapse" for="c-38502010">[-]</label><label class="expand" for="c-38502010">[8 more]</label></div><br/><div class="children"><div class="content">Ksplice is the original live patching technology that got bought by Oracle and was later extended to user space programs while I worked there. It&#x27;s a really neat technology that isn&#x27;t made obsolete by the move to cloud, since you still don&#x27;t want to have to restart the whole fleet at scale.</div><br/><div id="38503314" class="c"><input type="checkbox" id="c-38503314" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#38502010">parent</a><span>|</span><a href="#38503552">next</a><span>|</span><label class="collapse" for="c-38503314">[-]</label><label class="expand" for="c-38503314">[7 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the live app patcher called?</div><br/><div id="38503599" class="c"><input type="checkbox" id="c-38503599" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#38502010">root</a><span>|</span><a href="#38503314">parent</a><span>|</span><a href="#38503552">next</a><span>|</span><label class="collapse" for="c-38503599">[-]</label><label class="expand" for="c-38503599">[6 more]</label></div><br/><div class="children"><div class="content">I have objections to the name, but it goes under Ksplice for User Space.<p><a href="https:&#x2F;&#x2F;blogs.oracle.com&#x2F;virtualization&#x2F;post&#x2F;ksplice-zero-downtime-patching-for-user-space-packages" rel="nofollow noreferrer">https:&#x2F;&#x2F;blogs.oracle.com&#x2F;virtualization&#x2F;post&#x2F;ksplice-zero-do...</a></div><br/><div id="38504140" class="c"><input type="checkbox" id="c-38504140" checked=""/><div class="controls bullet"><span class="by">Sarkie</span><span>|</span><a href="#38502010">root</a><span>|</span><a href="#38503599">parent</a><span>|</span><a href="#38503552">next</a><span>|</span><label class="collapse" for="c-38504140">[-]</label><label class="expand" for="c-38504140">[5 more]</label></div><br/><div class="children"><div class="content">How was Oracle to work at?</div><br/><div id="38505394" class="c"><input type="checkbox" id="c-38505394" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#38502010">root</a><span>|</span><a href="#38504140">parent</a><span>|</span><a href="#38504315">next</a><span>|</span><label class="collapse" for="c-38505394">[-]</label><label class="expand" for="c-38505394">[3 more]</label></div><br/><div class="children"><div class="content">Post acquisition, we moved offices, and used their HR department, and certain product decisions were made for us, but we largely did our own thing, for better or worse, so I don&#x27;t know how much I can speak about how it was to work at Oracle. We were under Wim Coekaerts who is a big open source guy. Oracle&#x27;s reputation as a cutthroat legal entity is well deserved, but working at that side of the company it felt unfair because, Oracle&#x27;s open source contributions with VirtualBox, the UEK, and others are lost in the grar over MySQL.<p>I think the biggest &quot;big company&quot; blunder while I was there was a public blog post by someone high up at the company decrying open source as bad&#x2F;wrong while at the same time, Oracle was doing all this other open source stuff.</div><br/><div id="38505564" class="c"><input type="checkbox" id="c-38505564" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#38502010">root</a><span>|</span><a href="#38505394">parent</a><span>|</span><a href="#38505426">next</a><span>|</span><label class="collapse" for="c-38505564">[-]</label><label class="expand" for="c-38505564">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but working at that side of the company it felt unfair because, Oracle&#x27;s open source contributions with VirtualBox, the UEK, and others are lost in the grar over MySQL.<p>They are also remembered for closing OpenSolaris and shaking people down over the VirtualBox extension pack ( <a href="https:&#x2F;&#x2F;www.theregister.com&#x2F;2019&#x2F;10&#x2F;04&#x2F;oracle_virtualbox_merula&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theregister.com&#x2F;2019&#x2F;10&#x2F;04&#x2F;oracle_virtualbox_mer...</a> ).</div><br/></div></div><div id="38505426" class="c"><input type="checkbox" id="c-38505426" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38502010">root</a><span>|</span><a href="#38505394">parent</a><span>|</span><a href="#38505564">prev</a><span>|</span><a href="#38504315">next</a><span>|</span><label class="collapse" for="c-38505426">[-]</label><label class="expand" for="c-38505426">[1 more]</label></div><br/><div class="children"><div class="content">Funny given that they have been selling open source software for ages...<p><a href="https:&#x2F;&#x2F;docs.oracle.com&#x2F;cd&#x2F;B10463_01&#x2F;web.904&#x2F;b10320&#x2F;apjsvsup.htm" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.oracle.com&#x2F;cd&#x2F;B10463_01&#x2F;web.904&#x2F;b10320&#x2F;apjsvsup...</a><p>I believe this was circa 2000, but going on longer than that.</div><br/></div></div></div></div><div id="38504315" class="c"><input type="checkbox" id="c-38504315" checked=""/><div class="controls bullet"><span class="by">grepfru_it</span><span>|</span><a href="#38502010">root</a><span>|</span><a href="#38504140">parent</a><span>|</span><a href="#38505394">prev</a><span>|</span><a href="#38503552">next</a><span>|</span><label class="collapse" for="c-38504315">[-]</label><label class="expand" for="c-38504315">[1 more]</label></div><br/><div class="children"><div class="content">Not OP but I loved oracle in the 90s&#x2F;early 2000s. After the Sun acquisition it became cutthroat and I left. IBM acquiring redhat seems very similar</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38503552" class="c"><input type="checkbox" id="c-38503552" checked=""/><div class="controls bullet"><span class="by">iseletsk</span><span>|</span><a href="#38502010">prev</a><span>|</span><a href="#38503190">next</a><span>|</span><label class="collapse" for="c-38503552">[-]</label><label class="expand" for="c-38503552">[1 more]</label></div><br/><div class="children"><div class="content">There is also KernelCare from tuxcare <a href="https:&#x2F;&#x2F;tuxcare.com&#x2F;enterprise-live-patching-services&#x2F;kernelcare-enterprise&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tuxcare.com&#x2F;enterprise-live-patching-services&#x2F;kernel...</a> for kernel livepatching. It supports most linux distros.</div><br/></div></div><div id="38503190" class="c"><input type="checkbox" id="c-38503190" checked=""/><div class="controls bullet"><span class="by">hellozomo</span><span>|</span><a href="#38503552">prev</a><span>|</span><a href="#38503432">next</a><span>|</span><label class="collapse" for="c-38503190">[-]</label><label class="expand" for="c-38503190">[19 more]</label></div><br/><div class="children"><div class="content">&quot;Draining and un-draining hosts is hard.&quot;<p>I&#x27;d stop right there and fix that, because that&#x27;s a bullshit reason. Cycling hosts in and out of service is <i>easy</i> unless you&#x27;re not doing things properly.<p>The Linux kernel is simply not designed to be live patched and it&#x27;s a total hack to try to do it, it will never work 100% of the time, always be a source of uncertainty, and always be expensive in terms of engineering work. Disaster will always be looming.<p>By contrast, fixing their system for taking hosts in and out of service, so that it&#x27;s extremely robust and reliable would likely pay big dividends in reliability.<p>My guess would be that this approach is papering over organizational dysfunction. One team can patch all the kernels but one team can&#x27;t make all the hosts support proper cycling in and out of service. And no one cares to fix it because there&#x27;s no real incentive to do so. Only cool hacks and new projects are properly rewarded.</div><br/><div id="38504021" class="c"><input type="checkbox" id="c-38504021" checked=""/><div class="controls bullet"><span class="by">mypalmike</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38503524">next</a><span>|</span><label class="collapse" for="c-38504021">[-]</label><label class="expand" for="c-38504021">[5 more]</label></div><br/><div class="children"><div class="content">&gt; fixing their system for taking hosts in and out of service, so that it&#x27;s extremely robust and reliable would likely pay big dividends in reliability.<p>Facebook hosts can be robustly cycled. Of course. They&#x27;ve been doing this stuff for years. They&#x27;ve figured it out. That&#x27;s not the issue.<p>Scaling up brings about new problems. This article specifically mentions the 45 day rolling restart issue. That&#x27;s not an issue when you have 1000s of servers. It&#x27;s one that shows up a couple of orders of magnitude later.<p>So you either solve the problem with a hack like kernel patching or you work to reduce restart times (drain + shutdown + OS restart  + process initialization across every service). Get those restart times down 50% (good luck accomplishing that) and congrats, you&#x27;re down to maybe a 25 day rolling restart, which is still quite a problem.</div><br/><div id="38504479" class="c"><input type="checkbox" id="c-38504479" checked=""/><div class="controls bullet"><span class="by">hellozomo</span><span>|</span><a href="#38503190">root</a><span>|</span><a href="#38504021">parent</a><span>|</span><a href="#38503524">next</a><span>|</span><label class="collapse" for="c-38504479">[-]</label><label class="expand" for="c-38504479">[4 more]</label></div><br/><div class="children"><div class="content">What exactly do you think requires cycling millions of hosts to take 45 days? That&#x27;s a couple hundred hosts every five minutes across a large number of datacenters?<p>I wouldn&#x27;t expect it to be halved by optimization. I&#x27;d expect it to be an order of magnitude faster and take more like 4.5 days.<p>I wouldn&#x27;t be <i>surprised</i> (but I would be impressed) if they tried and got it down to a full cycle requiring one working day. That&#x27;s around 1% of hosts cycling every five minutes.</div><br/><div id="38505143" class="c"><input type="checkbox" id="c-38505143" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#38503190">root</a><span>|</span><a href="#38504479">parent</a><span>|</span><a href="#38505664">next</a><span>|</span><label class="collapse" for="c-38505143">[-]</label><label class="expand" for="c-38505143">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s certainly weird. I&#x27;ve worked at ~million host scale where uptime never exceeded a week by default (with the odd carve out for problem child software supplied by vendors)<p>I&#x27;m betting it&#x27;s moreso that teaching developers to write software that tolerates draining properly (or is even able to communicate draining) is too difficult for them so they work around it.</div><br/><div id="38505744" class="c"><input type="checkbox" id="c-38505744" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#38503190">root</a><span>|</span><a href="#38505143">parent</a><span>|</span><a href="#38505664">next</a><span>|</span><label class="collapse" for="c-38505744">[-]</label><label class="expand" for="c-38505744">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s certainly weird. I&#x27;ve worked at ~million host scale where uptime never exceeded a week by default<p>How many individual teams had software running on your hosts? 
How many those hosts were stateful, and were fragmented across hundreds or thousands of service groups that had their own fault tolerances and unknown (to infra team) warm-up times. Adding complexity (rolling reboots) to already complex systems is almost never a good idea - at some point, there will be an issue caused by hosts rebooted in the wrong order, or too many hosts of a certain type 2-dependency-levels down being simultaneously offline</div><br/></div></div></div></div><div id="38505664" class="c"><input type="checkbox" id="c-38505664" checked=""/><div class="controls bullet"><span class="by">raverbashing</span><span>|</span><a href="#38503190">root</a><span>|</span><a href="#38504479">parent</a><span>|</span><a href="#38505143">prev</a><span>|</span><a href="#38503524">next</a><span>|</span><label class="collapse" for="c-38505664">[-]</label><label class="expand" for="c-38505664">[1 more]</label></div><br/><div class="children"><div class="content">1 Million hosts over 45 days = 15 min per host<p>That&#x27;s a very realistic&#x2F;optimistic number (especially as you do want to wait for all services to be running and marked as healthy)<p>&quot;Oh but you can batch this&quot; sure, but you don&#x27;t want too much of a big batch that will make your service slow or want to risk shooting yourself in the foot - like rebooting your whole control plane then figuring out it doesn&#x27;t work like that<p>(The 45 days is probably an estimate as well, I&#x27;m not sure they actually do that server by server)</div><br/></div></div></div></div></div></div><div id="38503524" class="c"><input type="checkbox" id="c-38503524" checked=""/><div class="controls bullet"><span class="by">WookieRushing</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38504021">prev</a><span>|</span><a href="#38504697">next</a><span>|</span><label class="collapse" for="c-38503524">[-]</label><label class="expand" for="c-38503524">[1 more]</label></div><br/><div class="children"><div class="content">Why do you think its easy?<p>There&#x27;s a lot of systems where you can easily take down some hosts, but taking down more than N% at a time causes issues. If your fleet is large enough then you are limited by the largest set of hosts where you can only take N% down at a time. Now you could say keep the sets of hosts small or N% large. But that can cause other issues as you typically lose efficiency or zonal outage protection.<p>A solution to this could be VM live migration or something similar. This breaks down for storage systems where you can&#x27;t just migrate those disks virtually since they&#x27;re physical disks or places that don&#x27;t use VMs.</div><br/></div></div><div id="38504697" class="c"><input type="checkbox" id="c-38504697" checked=""/><div class="controls bullet"><span class="by">lokar</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38503524">prev</a><span>|</span><a href="#38503376">next</a><span>|</span><label class="collapse" for="c-38504697">[-]</label><label class="expand" for="c-38504697">[2 more]</label></div><br/><div class="children"><div class="content">At google we did pretty much the same thing.  Aimed to be able roll a kernel in 30 days, but various edge cases always made it drag out at the end unless you really spend a lot of human time on it.  So use kaplice for really critical stuff (where the patch was easy, not always the case).<p>A reasonable compromise in the real world.</div><br/><div id="38504777" class="c"><input type="checkbox" id="c-38504777" checked=""/><div class="controls bullet"><span class="by">ungamedplayer</span><span>|</span><a href="#38503190">root</a><span>|</span><a href="#38504697">parent</a><span>|</span><a href="#38503376">next</a><span>|</span><label class="collapse" for="c-38504777">[-]</label><label class="expand" for="c-38504777">[1 more]</label></div><br/><div class="children"><div class="content">At redhat, we just maintain kpatch and hire kernel engineering to do the same.  Turn around is about a week for 40 variants of kernel.</div><br/></div></div></div></div><div id="38503376" class="c"><input type="checkbox" id="c-38503376" checked=""/><div class="controls bullet"><span class="by">tysonfurytoo</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38504697">prev</a><span>|</span><a href="#38505416">next</a><span>|</span><label class="collapse" for="c-38503376">[-]</label><label class="expand" for="c-38503376">[1 more]</label></div><br/><div class="children"><div class="content">It may be easy to cycle hosts in and out, but it can also be time consuming, apparently. In the article it mentions taking 45 days to patch all hosts. The article also points out that this is too long for security updates.<p>Nothing will work 100% of the time. If their patching mechanism is thoroughly tested and battle hardened, I think the risk would be acceptable. Once you do the initial kpatch security upgrade, you could even schedule the machine for serivce so that it&#x27;s not relying on that, limiting your exposure to bugs.</div><br/></div></div><div id="38505416" class="c"><input type="checkbox" id="c-38505416" checked=""/><div class="controls bullet"><span class="by">jb_gericke</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38503376">prev</a><span>|</span><a href="#38503735">next</a><span>|</span><label class="collapse" for="c-38505416">[-]</label><label class="expand" for="c-38505416">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, especially with containerisation and orchestration &#x2F; Kubernetes, I get that perhaps not everything is viable to containerise, but in 2023 this feels archaic and like a lot of (potentially unnecessary) engineering work.</div><br/></div></div><div id="38503735" class="c"><input type="checkbox" id="c-38503735" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38505416">prev</a><span>|</span><a href="#38504475">next</a><span>|</span><label class="collapse" for="c-38503735">[-]</label><label class="expand" for="c-38503735">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a bullshit reason. You can put all the lipstick you want on the pig and put software all around it to make it all sorts of easy, but at the end of the day, having to reboot is a stop-the-(machine&#x27;s)-world situation. Not having to do that is just better. Even if it doesn&#x27;t work 100% of the time, that&#x27;s still better than having to reboot the whole fleet. 45 days to reboot the whole fleet!<p>Throwing FUD and saying disaster is looming because its scary computer magic (out of MIT) was a scare tactic RedHat used to throw around about Oracle&#x2F;Ksplice until they developed their own (Kpatch), then suddenly their sales team had to backtrack and say actually hot patching is good and can be trusted. I&#x27;m not saying it&#x27;s not risky or dangerous,  it&#x27;s operating in kernel space, but that&#x27;s why they pay really smart people to be careful  when doing it, and not digital equivalent of a plumber who can&#x27;t do more than glue libraries together.<p>A better understanding of the underlying technology so it&#x27;s less magic might assuage your fear of it, but thinking Facebook is so dysfunctional that they haven&#x27;t already made it easier to reboot is to misunderstand the problem at hand.</div><br/></div></div><div id="38504475" class="c"><input type="checkbox" id="c-38504475" checked=""/><div class="controls bullet"><span class="by">raincom</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38503735">prev</a><span>|</span><a href="#38503326">next</a><span>|</span><label class="collapse" for="c-38504475">[-]</label><label class="expand" for="c-38504475">[1 more]</label></div><br/><div class="children"><div class="content">Redhat provides kpatches for 6 months, that&#x27;s all. If you are running a year old kernel, no kpatches are provided for that kernel. Definitely, one needs to recycle hosts every six months.</div><br/></div></div><div id="38503676" class="c"><input type="checkbox" id="c-38503676" checked=""/><div class="controls bullet"><span class="by">natbennett</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38503326">prev</a><span>|</span><a href="#38503370">next</a><span>|</span><label class="collapse" for="c-38503676">[-]</label><label class="expand" for="c-38503676">[1 more]</label></div><br/><div class="children"><div class="content">The work that makes cycling hosts in and out easy is itself hard.<p>I agree that it’s the right thing to do but it’s hard.</div><br/></div></div><div id="38503370" class="c"><input type="checkbox" id="c-38503370" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#38503190">parent</a><span>|</span><a href="#38503676">prev</a><span>|</span><a href="#38503775">next</a><span>|</span><label class="collapse" for="c-38503370">[-]</label><label class="expand" for="c-38503370">[2 more]</label></div><br/><div class="children"><div class="content">&gt; My guess would be that this approach is papering over organizational dysfunction.<p>So what? At large enough scale organization problems are harder than technical ones: if you can fix the former with the latter, that&#x27;s still a win.</div><br/><div id="38504496" class="c"><input type="checkbox" id="c-38504496" checked=""/><div class="controls bullet"><span class="by">hellozomo</span><span>|</span><a href="#38503190">root</a><span>|</span><a href="#38503370">parent</a><span>|</span><a href="#38503775">next</a><span>|</span><label class="collapse" for="c-38504496">[-]</label><label class="expand" for="c-38504496">[1 more]</label></div><br/><div class="children"><div class="content">But it&#x27;s not &quot;fixing&quot; the problem, it&#x27;s <i>papering over</i> the problem by piling tech debt on top of tech debt.<p>Framing this like it&#x27;s a <i>good thing</i> is my only objection.</div><br/></div></div></div></div></div></div><div id="38503432" class="c"><input type="checkbox" id="c-38503432" checked=""/><div class="controls bullet"><span class="by">faitswulff</span><span>|</span><a href="#38503190">prev</a><span>|</span><a href="#38502840">next</a><span>|</span><label class="collapse" for="c-38503432">[-]</label><label class="expand" for="c-38503432">[7 more]</label></div><br/><div class="children"><div class="content">&gt; So, if you’d rather not have downtime with your servers, data centers, and clouds, follow Meta’s example and use live patching. You’ll be glad you did.<p>Most orgs don’t need and won’t benefit from emulating Meta for the sake of emulating Meta.</div><br/><div id="38503824" class="c"><input type="checkbox" id="c-38503824" checked=""/><div class="controls bullet"><span class="by">nickysielicki</span><span>|</span><a href="#38503432">parent</a><span>|</span><a href="#38503517">next</a><span>|</span><label class="collapse" for="c-38503824">[-]</label><label class="expand" for="c-38503824">[2 more]</label></div><br/><div class="children"><div class="content">This sort of criticism gets repeated all the time (&quot;Google designs for Google scale, but you&#x27;re not Google, so don&#x27;t use kubernetes!&quot;), and sometimes it&#x27;s fair, but this doesn&#x27;t really make sense to me on this particular article.<p>If the infrastructure exists within your org&#x27;s distribution of choice to do this, it&#x27;s basically all upside. On AL2023, you just do:<p>`sudo dnf install -y kpatch-dnf kpatch-runtime`<p>`sudo dnf kernel-livepatch -y auto`<p>`sudo systemctl enable --now kpatch.service`<p>Super simple, one less thing to worry about.</div><br/><div id="38504067" class="c"><input type="checkbox" id="c-38504067" checked=""/><div class="controls bullet"><span class="by">ies7</span><span>|</span><a href="#38503432">root</a><span>|</span><a href="#38503824">parent</a><span>|</span><a href="#38503517">next</a><span>|</span><label class="collapse" for="c-38504067">[-]</label><label class="expand" for="c-38504067">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Super simple, one less thing to worry about.<p>We got bitten by kpatch a few times before decided to abandon it around 2017.<p>From kpatch github (2023):<p>WARNING: Use with caution! Kernel crashes, spontaneous reboots, and data loss may occur!</div><br/></div></div></div></div><div id="38503517" class="c"><input type="checkbox" id="c-38503517" checked=""/><div class="controls bullet"><span class="by">baby_souffle</span><span>|</span><a href="#38503432">parent</a><span>|</span><a href="#38503824">prev</a><span>|</span><a href="#38504524">next</a><span>|</span><label class="collapse" for="c-38503517">[-]</label><label class="expand" for="c-38503517">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Most orgs don’t need and won’t benefit from emulating Meta for the sake of emulating Meta.<p>SO. Much. This.<p>Worked at more than a few places where the stack had almost as many layers as it had engineers &quot;because this is how we did it at FAANG...&quot;<p>Right, and those places also had a few orders of magnitude more engineers on staff to support it all. We do one hundredth of the things FAANG does and we have less than 50 _total_ people in the company; the simpler the stack, the better.</div><br/></div></div><div id="38504524" class="c"><input type="checkbox" id="c-38504524" checked=""/><div class="controls bullet"><span class="by">imetatroll</span><span>|</span><a href="#38503432">parent</a><span>|</span><a href="#38503517">prev</a><span>|</span><a href="#38503679">next</a><span>|</span><label class="collapse" for="c-38504524">[-]</label><label class="expand" for="c-38504524">[1 more]</label></div><br/><div class="children"><div class="content">The danger in your comment is that people who are primarily &quot;working&quot; because they want to play on someone else&#x27;s dollar with some new, frivolous tech - from the perspective of what their business actually needs - are going to be insulted by this.  Good luck.</div><br/></div></div><div id="38503679" class="c"><input type="checkbox" id="c-38503679" checked=""/><div class="controls bullet"><span class="by">influx</span><span>|</span><a href="#38503432">parent</a><span>|</span><a href="#38504524">prev</a><span>|</span><a href="#38502840">next</a><span>|</span><label class="collapse" for="c-38503679">[-]</label><label class="expand" for="c-38503679">[2 more]</label></div><br/><div class="children"><div class="content">Likewise at Meta there&#x27;s a bunch of cargo-culting from Google.</div><br/><div id="38503725" class="c"><input type="checkbox" id="c-38503725" checked=""/><div class="controls bullet"><span class="by">phyrex</span><span>|</span><a href="#38503432">root</a><span>|</span><a href="#38503679">parent</a><span>|</span><a href="#38502840">next</a><span>|</span><label class="collapse" for="c-38503725">[-]</label><label class="expand" for="c-38503725">[1 more]</label></div><br/><div class="children"><div class="content">Could you expand please?</div><br/></div></div></div></div></div></div><div id="38502864" class="c"><input type="checkbox" id="c-38502864" checked=""/><div class="controls bullet"><span class="by">ravenstine</span><span>|</span><a href="#38502840">prev</a><span>|</span><label class="collapse" for="c-38502864">[-]</label><label class="expand" for="c-38502864">[23 more]</label></div><br/><div class="children"><div class="content">Never heard of this &quot;hyperscale&quot; concept before.  How is this any different from... scaling?</div><br/><div id="38502927" class="c"><input type="checkbox" id="c-38502927" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#38502864">parent</a><span>|</span><a href="#38505684">next</a><span>|</span><label class="collapse" for="c-38502927">[-]</label><label class="expand" for="c-38502927">[1 more]</label></div><br/><div class="children"><div class="content">In general every order of magnitude brings new challenges. Companies running over a million servers have a lot of problems that smaller ones don&#x27;t. Also they just have more room to amortize R&amp;D.</div><br/></div></div><div id="38505684" class="c"><input type="checkbox" id="c-38505684" checked=""/><div class="controls bullet"><span class="by">rthnbgrredf</span><span>|</span><a href="#38502864">parent</a><span>|</span><a href="#38502927">prev</a><span>|</span><a href="#38503569">next</a><span>|</span><label class="collapse" for="c-38505684">[-]</label><label class="expand" for="c-38505684">[1 more]</label></div><br/><div class="children"><div class="content">I dislike the term &#x27;hyperscale&#x27; as it lacks concrete meaning. We need a metric akin to kilobytes or megabytes for storage. Terms like &#x27;kiloscale&#x27; and &#x27;megascale&#x27; could better indicate the scaling range of a service. For instance, scaling a service to a thousand instances and back to zero or rolling out patches to thousand instances could be termed &#x27;kiloscale&#x27;.</div><br/></div></div><div id="38503569" class="c"><input type="checkbox" id="c-38503569" checked=""/><div class="controls bullet"><span class="by">comprev</span><span>|</span><a href="#38502864">parent</a><span>|</span><a href="#38505684">prev</a><span>|</span><a href="#38503040">next</a><span>|</span><label class="collapse" for="c-38503569">[-]</label><label class="expand" for="c-38503569">[1 more]</label></div><br/><div class="children"><div class="content">The &quot;H&quot; in ADHD stands for &quot;hyperactivity&quot;.  This implies _significantly_ more activity than the norm to a point where they really stand out.<p>In sci-fi a spaceship travelling at &quot;hyperspeed&quot; was perceived to be so much faster than anything known to man that it would be difficult to comprehend.<p>The performance and cost of a hypercar compared to the average family car is sometimes difficult to understand too.  The average person would have to work (potentially) hundreds of years to afford a €10M hypercar.<p>&quot;Hyperscale&quot; is so large that even us working in tech have difficulty grasping it because we have nothing tangible to compare it to.  A million servers is bind boggling to me even with the &quot;cattle not pets&quot; mindset</div><br/></div></div><div id="38503040" class="c"><input type="checkbox" id="c-38503040" checked=""/><div class="controls bullet"><span class="by">tester756</span><span>|</span><a href="#38502864">parent</a><span>|</span><a href="#38503569">prev</a><span>|</span><a href="#38502912">next</a><span>|</span><label class="collapse" for="c-38503040">[-]</label><label class="expand" for="c-38503040">[8 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hyperscale_computing" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hyperscale_computing</a></div><br/><div id="38503179" class="c"><input type="checkbox" id="c-38503179" checked=""/><div class="controls bullet"><span class="by">zug_zug</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503040">parent</a><span>|</span><a href="#38502912">next</a><span>|</span><label class="collapse" for="c-38503179">[-]</label><label class="expand" for="c-38503179">[7 more]</label></div><br/><div class="children"><div class="content">&gt; In computing, hyperscale is the ability of an architecture to scale appropriately as increased demand is added to the system.<p>So yeah just scaling. I agree I&#x27;ve never heard the word &quot;hyperscale&quot; before and don&#x27;t think we need that extra intensifier for a well-understood idea.</div><br/><div id="38503544" class="c"><input type="checkbox" id="c-38503544" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503179">parent</a><span>|</span><a href="#38503353">next</a><span>|</span><label class="collapse" for="c-38503544">[-]</label><label class="expand" for="c-38503544">[1 more]</label></div><br/><div class="children"><div class="content">The word hyperscale afaik was coined by Wall St people as a collective noun for FB, Google, MS, et al, in the context of their in-house data center operations.</div><br/></div></div><div id="38503353" class="c"><input type="checkbox" id="c-38503353" checked=""/><div class="controls bullet"><span class="by">tester756</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503179">parent</a><span>|</span><a href="#38503544">prev</a><span>|</span><a href="#38502912">next</a><span>|</span><label class="collapse" for="c-38503353">[-]</label><label class="expand" for="c-38503353">[5 more]</label></div><br/><div class="children"><div class="content">1x, 10x, 1000x are all &quot;scales&quot;, yet problems may be a little bit different at each of them</div><br/><div id="38503410" class="c"><input type="checkbox" id="c-38503410" checked=""/><div class="controls bullet"><span class="by">zug_zug</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503353">parent</a><span>|</span><a href="#38502912">next</a><span>|</span><label class="collapse" for="c-38503410">[-]</label><label class="expand" for="c-38503410">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m just quoting the wikipedia definiton of hyperscale. Nowhere in there does it say anything about 1000x, probably because that is an ill-defined concept.<p>1000x what? Today&#x27;s computers are a 1000x the ones from the 90s, should we call them all hypercomputers? Pretty much any startup can boot 20,000 nodes on aws, are they all hyperstartups hyperscaling?<p>Seems like marketing nonsense.</div><br/><div id="38503472" class="c"><input type="checkbox" id="c-38503472" checked=""/><div class="controls bullet"><span class="by">kreeben</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503410">parent</a><span>|</span><a href="#38502912">next</a><span>|</span><label class="collapse" for="c-38503472">[-]</label><label class="expand" for="c-38503472">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing you&#x27;re mad about the term but not the concept? And that you do agree, that scaling from 1 to 10 is not the same animal as scaling from 10 to 100? So why, then, call both animals &quot;plain and simple scaling&quot;?</div><br/><div id="38503617" class="c"><input type="checkbox" id="c-38503617" checked=""/><div class="controls bullet"><span class="by">zug_zug</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503472">parent</a><span>|</span><a href="#38502912">next</a><span>|</span><label class="collapse" for="c-38503617">[-]</label><label class="expand" for="c-38503617">[2 more]</label></div><br/><div class="children"><div class="content">&gt; And that you do agree, that scaling from 1 to 10 is not the same animal as scaling from 10 to 100? So why, then, call both animals &quot;plain and simple scaling&quot;?<p>So you think we need 4 different words for scaling 1-10, 10-100, 100-1000....?<p>Cut it out, you know it&#x27;s just marketing hype. Not everything needs its own word.</div><br/><div id="38505444" class="c"><input type="checkbox" id="c-38505444" checked=""/><div class="controls bullet"><span class="by">tverbeure</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503617">parent</a><span>|</span><a href="#38502912">next</a><span>|</span><label class="collapse" for="c-38505444">[-]</label><label class="expand" for="c-38505444">[1 more]</label></div><br/><div class="children"><div class="content">It makes sense to have different words for totally different orders of scaling, yes.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38502912" class="c"><input type="checkbox" id="c-38502912" checked=""/><div class="controls bullet"><span class="by">mc32</span><span>|</span><a href="#38502864">parent</a><span>|</span><a href="#38503040">prev</a><span>|</span><a href="#38502951">next</a><span>|</span><label class="collapse" for="c-38502912">[-]</label><label class="expand" for="c-38502912">[4 more]</label></div><br/><div class="children"><div class="content">Supermarkets and super cars aren&#x27;t enough.  We need hypermarkets and hyper cars to be current.  So now we can’t just scale, I guess that’s for trucks, so to be current you need hyperscale.</div><br/><div id="38503919" class="c"><input type="checkbox" id="c-38503919" checked=""/><div class="controls bullet"><span class="by">phyrex</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38502912">parent</a><span>|</span><a href="#38505099">next</a><span>|</span><label class="collapse" for="c-38503919">[-]</label><label class="expand" for="c-38503919">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Hypermarket" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Hypermarket</a> ¯\_(ツ)_&#x2F;¯</div><br/></div></div><div id="38505099" class="c"><input type="checkbox" id="c-38505099" checked=""/><div class="controls bullet"><span class="by">kevindamm</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38502912">parent</a><span>|</span><a href="#38503919">prev</a><span>|</span><a href="#38503247">next</a><span>|</span><label class="collapse" for="c-38505099">[-]</label><label class="expand" for="c-38505099">[1 more]</label></div><br/><div class="children"><div class="content">give it a couple more years, we&#x27;ll have ultrascale</div><br/></div></div><div id="38503247" class="c"><input type="checkbox" id="c-38503247" checked=""/><div class="controls bullet"><span class="by">froh</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38502912">parent</a><span>|</span><a href="#38505099">prev</a><span>|</span><a href="#38502951">next</a><span>|</span><label class="collapse" for="c-38503247">[-]</label><label class="expand" for="c-38503247">[1 more]</label></div><br/><div class="children"><div class="content">Cybertruck and cyberscale?<p>(I&#x27;m joking)</div><br/></div></div></div></div><div id="38502951" class="c"><input type="checkbox" id="c-38502951" checked=""/><div class="controls bullet"><span class="by">revskill</span><span>|</span><a href="#38502864">parent</a><span>|</span><a href="#38502912">prev</a><span>|</span><a href="#38503407">next</a><span>|</span><label class="collapse" for="c-38502951">[-]</label><label class="expand" for="c-38502951">[4 more]</label></div><br/><div class="children"><div class="content">Universe scale i guess.</div><br/><div id="38503260" class="c"><input type="checkbox" id="c-38503260" checked=""/><div class="controls bullet"><span class="by">adaml_623</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38502951">parent</a><span>|</span><a href="#38503407">next</a><span>|</span><label class="collapse" for="c-38503260">[-]</label><label class="expand" for="c-38503260">[3 more]</label></div><br/><div class="children"><div class="content">Size of the entire universe man</div><br/><div id="38503339" class="c"><input type="checkbox" id="c-38503339" checked=""/><div class="controls bullet"><span class="by">HankB99</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503260">parent</a><span>|</span><a href="#38503407">next</a><span>|</span><label class="collapse" for="c-38503339">[-]</label><label class="expand" for="c-38503339">[2 more]</label></div><br/><div class="children"><div class="content">Multiverse.</div><br/><div id="38503444" class="c"><input type="checkbox" id="c-38503444" checked=""/><div class="controls bullet"><span class="by">ratsmack</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503339">parent</a><span>|</span><a href="#38503407">next</a><span>|</span><label class="collapse" for="c-38503444">[-]</label><label class="expand" for="c-38503444">[1 more]</label></div><br/><div class="children"><div class="content">Meta already does that.</div><br/></div></div></div></div></div></div></div></div><div id="38503407" class="c"><input type="checkbox" id="c-38503407" checked=""/><div class="controls bullet"><span class="by">phyzome</span><span>|</span><a href="#38502864">parent</a><span>|</span><a href="#38502951">prev</a><span>|</span><label class="collapse" for="c-38503407">[-]</label><label class="expand" for="c-38503407">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, and this isn&#x27;t a problem that interacts with scale. If you can patch 100 servers with an automated method you can patch 1,000,000 of them.</div><br/><div id="38503567" class="c"><input type="checkbox" id="c-38503567" checked=""/><div class="controls bullet"><span class="by">agrajag</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503407">parent</a><span>|</span><label class="collapse" for="c-38503567">[-]</label><label class="expand" for="c-38503567">[2 more]</label></div><br/><div class="children"><div class="content">This is absolutely a problem that interacts with scale. With 1M servers you’re almost certainly dealing with hundreds of service owners, and some of those are going to need additional features you don’t have to worry about with 100 servers. Some examples are databases with graceful failover, long running AI model training jobs, or distributed databases like etc where you have to be mindful about how many can be down at a time.<p>It’s not 10,000x harder to patch that 10,000x more machines, but it’s not 1x either. Easily 10-20x harder, if not more.</div><br/><div id="38503665" class="c"><input type="checkbox" id="c-38503665" checked=""/><div class="controls bullet"><span class="by">natbennett</span><span>|</span><a href="#38502864">root</a><span>|</span><a href="#38503567">parent</a><span>|</span><label class="collapse" for="c-38503665">[-]</label><label class="expand" for="c-38503665">[1 more]</label></div><br/><div class="children"><div class="content">And at a certain scale patches will come out faster than you can deploy them!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>