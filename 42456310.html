<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734598860864" as="style"/><link rel="stylesheet" href="styles.css?v=1734598860864"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://purplesyringa.moe/blog/the-ram-myth/">The RAM Myth</a> <span class="domain">(<a href="https://purplesyringa.moe">purplesyringa.moe</a>)</span></div><div class="subtext"><span>signa11</span> | <span>52 comments</span></div><br/><div><div id="42457506" class="c"><input type="checkbox" id="c-42457506" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#42457692">next</a><span>|</span><label class="collapse" for="c-42457506">[-]</label><label class="expand" for="c-42457506">[29 more]</label></div><br/><div class="children"><div class="content">In my 3 years old laptop, system memory (dual channel DDR4-3200) delivers about 50 GB &#x2F; second. You have measured 50M elements per second, with 8 bytes per element translates to 400 MB &#x2F; second. If your hardware is similar, your implementation did less than 1% of theoretical bottleneck.<p>When your data is indeed big and the performance actually matters, consider doing something completely different.</div><br/><div id="42457759" class="c"><input type="checkbox" id="c-42457759" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#42457506">parent</a><span>|</span><a href="#42457840">next</a><span>|</span><label class="collapse" for="c-42457759">[-]</label><label class="expand" for="c-42457759">[18 more]</label></div><br/><div class="children"><div class="content">In the real world, your application is running in a container among hundreds or thousands of other containers. The system&#x27;s resources are also probably being managed by a hypervisor. The CPU is shared among N tenants _and_ is overcommitted. It&#x27;s not that much to ask to optimize where you can, when it isn&#x27;t unreasonably difficult.<p>More importantly, this attitude is precisely why software sucks today. &quot;[CPU, Memory, Disk] is cheap, engineering time isn&#x27;t.&quot; Fuck that. Bad engineering time is _incredibly_ expensive. This is an excuse to not spend time learning the ins and outs of your language, and your hardware.<p>It also frustrates me to no end that people are so deeply incurious. This seems to only happen in tech, which is baffling considering how incredibly broad and deep the industry is. Instead, everyone clamors to learn the new Javascript abstraction that lets them get even further away from the reality of what the magic boxes are doing.</div><br/><div id="42457904" class="c"><input type="checkbox" id="c-42457904" checked=""/><div class="controls bullet"><span class="by">kmarc</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42457759">parent</a><span>|</span><a href="#42458863">next</a><span>|</span><label class="collapse" for="c-42457904">[-]</label><label class="expand" for="c-42457904">[3 more]</label></div><br/><div class="children"><div class="content">Agree with the sentiment. However it&#x27;s hard to stay curious, even harder to stay up-to-date.<p>I liked fiddling with storage for a while, got really into it, deepened my knowledge about it. A couple years later I realized everything else (networking, architectures, languages) developed so much, mot of my (non-basic) knowledge was obsolete. Picking up where I left off with all technologies is incredibly hard and caused fatigue.<p>Now I&#x27;m at a point where I have the feeling I don&#x27;t know nothing about anything. It&#x27;s factually not true, but my gut feeling tells this. Would I be younger, this would trigger a lot of anxiety. Thankfully I can janfle this by now.</div><br/><div id="42457993" class="c"><input type="checkbox" id="c-42457993" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42457904">parent</a><span>|</span><a href="#42458863">next</a><span>|</span><label class="collapse" for="c-42457993">[-]</label><label class="expand" for="c-42457993">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s understandable. I&#x27;m into databases (both professionally and personally), so I get to touch a wide variety of things. Following a query the whole way is pretty neat. Typed query --&gt; client --&gt; wire protocol --&gt; network --&gt; server --&gt; DB frontend --&gt; DB planner --&gt; DB storage engine --&gt; OS --&gt; Memory &#x2F; Disk. `blktrace` is super interesting to watch commands being sent to the disk.</div><br/><div id="42459679" class="c"><input type="checkbox" id="c-42459679" checked=""/><div class="controls bullet"><span class="by">buran77</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42457993">parent</a><span>|</span><a href="#42458863">next</a><span>|</span><label class="collapse" for="c-42459679">[-]</label><label class="expand" for="c-42459679">[1 more]</label></div><br/><div class="children"><div class="content">When you are deeply involved in tech both personally and professionally you are probably very passionate about this and makes sense that you&#x27;d only look closely at this field and think &quot;people are so deeply incurious. This seems to only happen in tech&quot;.<p>Tech is also one of the (if not <i>the</i>) most dynamic and fast evolving field a normal person will ever touch. Curiosity in tech can drain every single bit of free time and energy you have and you will hardly keep up with the progress, maybe barely scratch the surface. But people&#x27;s available free time and energy wanes and curiosity is a collateral victim.<p>I&#x27;ve painfully gone through the entire cycle of this, including the bit of resurgence later on when you have a combination of free time but less energy. What I can say is that absolutely <i>does not</i> happen just in tech. If anything tech is flooded with people with more curiosity than almost any other field.</div><br/></div></div></div></div></div></div><div id="42458863" class="c"><input type="checkbox" id="c-42458863" checked=""/><div class="controls bullet"><span class="by">ddtaylor</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42457759">parent</a><span>|</span><a href="#42457904">prev</a><span>|</span><a href="#42458629">next</a><span>|</span><label class="collapse" for="c-42458863">[-]</label><label class="expand" for="c-42458863">[1 more]</label></div><br/><div class="children"><div class="content">I can somewhat echo some of the statements here and provide my own experience that is similar.<p>I spend a decent amount of time writing decent C++ code. My friends in other parts of the industry are writing certainly better C++ code than me because they are doing it in environments that are more constricted in various ways. In either case, I do spend my time catching up a bit and would consider myself a competent C++21 programmer in some ways.<p>My experience and my conversations lead me to understand there is so much left on the table with even the most basic implementations. When I implement it correctly in C++ we get close to some of the theoretical limits for the hardware for some workloads, compared to something that is literally 1% as fast running in NodeJS.<p>Wit that said, for so many situations I cannot justify the time and complexity to use C++ for many projects. At least for the stage most projects are in. In theory this optimization can happen later, but it never really does because the horizontal (or sometimes even vertical) scaling kicks in and we&#x27;re all just willing to throw a few more dollars at the problem instead of re-engineering it. Sure, some of the really big companies like Netflix find a decent reason from time to time to invest the engineering time squeeze out those numbers, but it&#x27;s becoming the exception and not the rule.</div><br/></div></div><div id="42458629" class="c"><input type="checkbox" id="c-42458629" checked=""/><div class="controls bullet"><span class="by">tonyarkles</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42457759">parent</a><span>|</span><a href="#42458863">prev</a><span>|</span><a href="#42457970">next</a><span>|</span><label class="collapse" for="c-42458629">[-]</label><label class="expand" for="c-42458629">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In the real world, your application is running in a container among hundreds or thousands of other containers<p>I mean, that’s an engineering decision too. In my day job we’re capturing, pre-processing, running inference on, and post-processing about 500Mpx&#x2F;s worth of live image data at about 80ms&#x2F;frame end-to-end at the edge. The processor SoM costs about $3000&#x2F;unit and uses about 50W running flat out. The retail cost of our overall product is two orders of magnitude more than what the processor is worth but it incurs zero recurring costs for us.<p>Edit: and it’s got 64GB of Unified RAM that I’ve got all to myself :)</div><br/></div></div><div id="42457970" class="c"><input type="checkbox" id="c-42457970" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42457759">parent</a><span>|</span><a href="#42458629">prev</a><span>|</span><a href="#42457840">next</a><span>|</span><label class="collapse" for="c-42457970">[-]</label><label class="expand" for="c-42457970">[12 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;[CPU, Memory, Disk] is cheap, engineering time isn&#x27;t.&quot; Fuck that.<p>It is.  It&#x27;s absurdly cheap.  I ensure I check the amount of time it would take for me to make a performance improvement against the runtime costs of my functions.  It&#x27;s rarely worth the extra effort.<p>Seriously,  until you get into the millions of records per second level,  you&#x27;re almost never benefited.  You may make your function 2x faster,  at a cost of additional complexity,  but you never run it enough in a year for it to pay itself back.<p>&gt; Bad engineering time is _incredibly_ expensive.<p>Engineering time is expensive.  Period.  It speaks to the need to minimize it.<p>&gt; This is an excuse to not spend time learning the ins and outs of your language, and your hardware.<p>All of which will change in a few years,  which is fine,  if you&#x27;re also committing to keeping _all that code_ up to date right along with it.  Otherwise you end up with an obscure mess that you have to unwind 5 years of context to understand and fix again.<p>Complexity and available mental contexts are forgotten costs.  If your language even has that many &quot;ins and outs&quot; to begin with you may want to reconsider that.</div><br/><div id="42458180" class="c"><input type="checkbox" id="c-42458180" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42457970">parent</a><span>|</span><a href="#42458845">next</a><span>|</span><label class="collapse" for="c-42458180">[-]</label><label class="expand" for="c-42458180">[6 more]</label></div><br/><div class="children"><div class="content">&gt; You may make your function 2x faster, at a cost of additional complexity, but you never run it enough in a year for it to pay itself back.<p>I&#x27;m not talking about increased complexity, I&#x27;m talking about extremely basic things that take zero extra time, like using the correct data structure. For example, in Python:<p><pre><code>    In [8]: a = array(&quot;i&quot;, (x for x in range(1_000_000)))
       ...: l = [x for x in range(1_000_000)]
       ...: d = deque(l)
       ...: for x in (a, l, d):
       ...:     print(f&quot;{sys.getsizeof(x) &#x2F; 2**20} MiB&quot;)
       ...:
    3.902385711669922 MiB
    8.057334899902344 MiB
    7.868537902832031 MiB

</code></pre>
Very similar structures, with very different memory requirements and access speeds. I can count on one hand with no fingers the number of times I&#x27;ve seen an array used.<p>Or knowing that `random.randint` is remarkably slow compared to `random.random()`, which can matter in a hot loop:<p><pre><code>    In [10]: %timeit math.floor(random.random() * 1_000_000)
    31.9 ns ± 0.138 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)

    In [11]: %timeit random.randint(0, 1_000_000)
    163 ns ± 0.653 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)
</code></pre>
&gt; All of which will change in a few years, which is fine, if you&#x27;re also committing to keeping _all that code_ up to date right along with it.<p>With the exception of list comprehension over large ranges slowing down from 3.11 --&gt; now, I don&#x27;t think there&#x27;s been much in Python that&#x27;s become dramatically worse such that you would need to refactor it later (I gather the Javascript community does this ritual every quarter or so). Anything being deprecated has years of warning.</div><br/><div id="42459086" class="c"><input type="checkbox" id="c-42459086" checked=""/><div class="controls bullet"><span class="by">masklinn</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458180">parent</a><span>|</span><a href="#42458754">next</a><span>|</span><label class="collapse" for="c-42459086">[-]</label><label class="expand" for="c-42459086">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Very similar structures, with very different memory requirements and access speeds. I can count on one hand with no fingers the number of times I&#x27;ve seen an array used.<p>That is obvious when you actually check the access speed of arrays and find out it is about half that of lists on small integers (under 256), and worse on non-small integers. That is literally the opposite trade off of what you want in 99.99% of cases.<p>Deques are even less of a consideration, they’re unrolled linked lists so random access is impossible and iteration is slower, you use a deque when you need <i>a deque</i> (or at least a fifo), aka when you need to routinely manipulate the head of the collection.</div><br/></div></div><div id="42458754" class="c"><input type="checkbox" id="c-42458754" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458180">parent</a><span>|</span><a href="#42459086">prev</a><span>|</span><a href="#42458394">next</a><span>|</span><label class="collapse" for="c-42458754">[-]</label><label class="expand" for="c-42458754">[1 more]</label></div><br/><div class="children"><div class="content">You are saying that the potential gains are less than an order of magnitude. That mkes them a pretty hard sell in most instances.</div><br/></div></div><div id="42458394" class="c"><input type="checkbox" id="c-42458394" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458180">parent</a><span>|</span><a href="#42458754">prev</a><span>|</span><a href="#42458845">next</a><span>|</span><label class="collapse" for="c-42458394">[-]</label><label class="expand" for="c-42458394">[3 more]</label></div><br/><div class="children"><div class="content">&gt; which can matter in a hot loop:<p>163ns - 31.9ns == 131.1ns<p>This will need to happen 7.6 million times to save me 1 CPU second.  On AWS lambda with 1GB of memory this will cost you a whopping: $0.0000166667.<p>The point is,  you&#x27;re not even wrong,  but there are vanishingly few cases where it would actually matter to the bottom line in practice.  You&#x27;re taking an absolutist point of view to a discipline which thoroughly rejects it.<p>This is what I love about the cloud.  It forces you to confront what your efforts are actually worth by placing a specific value on all of these commodities.  In my experience they&#x27;re often worth very little given that none of us have the scale of problems where this would show actual returns.</div><br/><div id="42458800" class="c"><input type="checkbox" id="c-42458800" checked=""/><div class="controls bullet"><span class="by">zrm</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458394">parent</a><span>|</span><a href="#42458793">next</a><span>|</span><label class="collapse" for="c-42458800">[-]</label><label class="expand" for="c-42458800">[1 more]</label></div><br/><div class="children"><div class="content">Reaching the scale where it shows actual returns isn&#x27;t all that difficult. You need it to happen 7.6 million times to save 1 CPU second, but each CPU core can execute it nearly that many times every second.<p>Probably you don&#x27;t leave it generating only random numbers all day, but suppose you do generate a good few, so that it&#x27;s 1% of your total compute budget, and you have only a modest load, using on average four CPU cores at any given time. Then saving that amount of computation will have saved you something like $15&#x2F;year in compute, recurring. Which isn&#x27;t actually that bad a return for ten seconds worth of choosing the right function.<p>There are also a lot of even fairly small entities for which four cores is peanuts and they&#x27;re running a hundred or a thousand at once, which quickly turns up the price.<p>And even the things with internet scale aren&#x27;t all that rare. Suppose you&#x27;re making a contribution to the mainline Linux kernel. It will run on billions of devices, possibly for decades. Even if it doesn&#x27;t run very often, that&#x27;s still a lot of cycles, and some of the kernel code <i>does</i> run very often. Likewise code in popular web browsers, javascript on popular websites or in popular libraries, etc.<p>You don&#x27;t have to work for Google to make a contribution to zlib and that kind of stuff has the weight of the world on it.</div><br/></div></div><div id="42458793" class="c"><input type="checkbox" id="c-42458793" checked=""/><div class="controls bullet"><span class="by">norir</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458394">parent</a><span>|</span><a href="#42458800">prev</a><span>|</span><a href="#42458845">next</a><span>|</span><label class="collapse" for="c-42458793">[-]</label><label class="expand" for="c-42458793">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but the cumulative effects of pervasive mediocre to bad decisions do add up. And it isn&#x27;t just about cloud compute cost. Your own time is stolen by the slow ci jobs that you inevitably get stuck waiting for. For me, I prioritize my own personal
happiness in my work and this mindset taken too far makes me unhappy.</div><br/></div></div></div></div></div></div><div id="42458845" class="c"><input type="checkbox" id="c-42458845" checked=""/><div class="controls bullet"><span class="by">jjav</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42457970">parent</a><span>|</span><a href="#42458180">prev</a><span>|</span><a href="#42457840">next</a><span>|</span><label class="collapse" for="c-42458845">[-]</label><label class="expand" for="c-42458845">[5 more]</label></div><br/><div class="children"><div class="content">&gt; You may make your function 2x faster, at a cost of additional complexity, but you never run it enough in a year for it to pay itself back.<p>&quot;You&quot; is both singular and plural, which is often the problem with this thinking.<p>Is it worth spending a month of engineering time to make a page load in 50ms instead of 2s? Seems like a lot of engineering time for a noticeable but somewhat minor improvement.<p>But now, what if you have a million users who do this operation 100x&#x2F;day? <i>Absolutely</i> worth it!<p>For example, I sure wish atlassian would spend a tiny bit of effort into making jira faster. Even if it is 1 second per ticket, since I&#x27;m viewing 100+ tickets per day that adds up. And there&#x27;s many hundreds of us at the company doing the same thing, it really adds up.</div><br/><div id="42459461" class="c"><input type="checkbox" id="c-42459461" checked=""/><div class="controls bullet"><span class="by">xlii</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458845">parent</a><span>|</span><a href="#42459327">next</a><span>|</span><label class="collapse" for="c-42459461">[-]</label><label class="expand" for="c-42459461">[1 more]</label></div><br/><div class="children"><div class="content">Nit:
50ms vs 2000ms is 40x speed increase, i.e. ~1.5 order of magnitude.<p>I still keep words of my database optimization lecturer who said that by his experience optimization below 1 OOM aren’t worth it and most „good ones” are 3+<p>&gt; Absolutely worth it!<p>Long reaching assumption. Even the biggest companies have limited resources (even if vast). Would you rather improve load times by 2x (from 500ms to 250ms) or improve checkout reliability from 99% to 99.5%? And there is much more to consider on some levels (e.g. planning for thermal efficiency is fun).<p>Software development is always a game of choice.</div><br/></div></div><div id="42459327" class="c"><input type="checkbox" id="c-42459327" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458845">parent</a><span>|</span><a href="#42459461">prev</a><span>|</span><a href="#42458875">next</a><span>|</span><label class="collapse" for="c-42459327">[-]</label><label class="expand" for="c-42459327">[1 more]</label></div><br/><div class="children"><div class="content">Most of the time they just move the expensive processing to the user&#x27;s browser so they don&#x27;t have to pay for it :)</div><br/></div></div><div id="42458875" class="c"><input type="checkbox" id="c-42458875" checked=""/><div class="controls bullet"><span class="by">ddtaylor</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458845">parent</a><span>|</span><a href="#42459327">prev</a><span>|</span><a href="#42459354">next</a><span>|</span><label class="collapse" for="c-42458875">[-]</label><label class="expand" for="c-42458875">[1 more]</label></div><br/><div class="children"><div class="content">&gt; 50ms instead of 2s<p>In the past I believe Google was very adament that page load time perception was very important to other metrics.</div><br/></div></div><div id="42459354" class="c"><input type="checkbox" id="c-42459354" checked=""/><div class="controls bullet"><span class="by">sfn42</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458845">parent</a><span>|</span><a href="#42458875">prev</a><span>|</span><a href="#42457840">next</a><span>|</span><label class="collapse" for="c-42459354">[-]</label><label class="expand" for="c-42459354">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re probably not going to achieve that with the kind of optimization described in this article though.</div><br/></div></div></div></div></div></div></div></div><div id="42457840" class="c"><input type="checkbox" id="c-42457840" checked=""/><div class="controls bullet"><span class="by">purplesyringa</span><span>|</span><a href="#42457506">parent</a><span>|</span><a href="#42457759">prev</a><span>|</span><a href="#42459326">next</a><span>|</span><label class="collapse" for="c-42457840">[-]</label><label class="expand" for="c-42457840">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s good food for thought. My hardware is quite old, but I agree that the numbers seem somewhat suspicious.<p>I believe that RAM can only be RMW&#x27;d in cache lines, so modifying just 8 bytes still requires 64 bytes to be transmitted. I&#x27;m assuming the 50 GB&#x2F;s throughput is half-duplex, and 25 GB&#x2F;s over 64 bytes is ~400 Melems&#x2F;s, somewhat closer to my result.<p>I tried using non-temporal stores in the straightforward algorithm, but to my surprise, this led to a significant decrease of performance across all input lengths.<p>&gt; When your data is indeed big and the performance actually matters, consider doing something completely different.<p>I&#x27;m not sure what you mean by this. Scaling across machines is just ignoring the problem. What do you mean by &quot;something completely different&quot;?</div><br/></div></div><div id="42459326" class="c"><input type="checkbox" id="c-42459326" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#42457506">parent</a><span>|</span><a href="#42457840">prev</a><span>|</span><a href="#42458927">next</a><span>|</span><label class="collapse" for="c-42459326">[-]</label><label class="expand" for="c-42459326">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In my 3 years old laptop, system memory (dual channel DDR4-3200) delivers about 50 GB &#x2F; second.<p>That&#x27;s almost certainly for (mostly) sequential access.<p>When you just want a couple bytes here and there, and access isn&#x27;t pipelined and prefetch doesn&#x27;t accelerate your use case, the real world bandwidth is going to be significantly less.</div><br/></div></div><div id="42458927" class="c"><input type="checkbox" id="c-42458927" checked=""/><div class="controls bullet"><span class="by">tc4v</span><span>|</span><a href="#42457506">parent</a><span>|</span><a href="#42459326">prev</a><span>|</span><a href="#42458058">next</a><span>|</span><label class="collapse" for="c-42458927">[-]</label><label class="expand" for="c-42458927">[2 more]</label></div><br/><div class="children"><div class="content">cache misses are slow because of latency, not because of throughput.</div><br/><div id="42459295" class="c"><input type="checkbox" id="c-42459295" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458927">parent</a><span>|</span><a href="#42458058">next</a><span>|</span><label class="collapse" for="c-42459295">[-]</label><label class="expand" for="c-42459295">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the point of the person you replied to that the article author wasn&#x27;t able to eliminate latency because if they were they&#x27;d be constrained by throughput but they are not?</div><br/></div></div></div></div><div id="42458058" class="c"><input type="checkbox" id="c-42458058" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#42457506">parent</a><span>|</span><a href="#42458927">prev</a><span>|</span><a href="#42457692">next</a><span>|</span><label class="collapse" for="c-42458058">[-]</label><label class="expand" for="c-42458058">[6 more]</label></div><br/><div class="children"><div class="content">And my GPU delivers 1TB&#x2F;s. Massive difference &lt;3<p>I wish we could get those kind of speeds on system RAM.</div><br/><div id="42458092" class="c"><input type="checkbox" id="c-42458092" checked=""/><div class="controls bullet"><span class="by">winter_blue</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458058">parent</a><span>|</span><a href="#42457692">next</a><span>|</span><label class="collapse" for="c-42458092">[-]</label><label class="expand" for="c-42458092">[5 more]</label></div><br/><div class="children"><div class="content">To&#x2F;from what sort of devices could the GPU read or write at 1TB&#x2F;sec, besides main memory?<p>The fastest consumer SSDs top out at several GB&#x2F;sec (I guess with massive hardware RAID they could be faster, but not sure if they’d be 1TB&#x2F;sec fast).<p>Even a network adapter that does 10 Gbit&#x2F;sec is only recently becoming slightly more common for the average consumer. Not sure if any consumer adapters in the 10 or 1Tbit&#x2F;sec range exist at all.</div><br/><div id="42459196" class="c"><input type="checkbox" id="c-42459196" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458092">parent</a><span>|</span><a href="#42459615">next</a><span>|</span><label class="collapse" for="c-42459196">[-]</label><label class="expand" for="c-42459196">[1 more]</label></div><br/><div class="children"><div class="content">Consumer? Even in the &quot;if-you-need-to-ask-what-it-costs-you-can&#x27;t-afford-it&quot; world of frontier HPC systems, were only getting teasers of 0.8 Tbit&#x2F;sec NICs this year.<p>As you say, only the GPU, maybe RAM, and the NIC will be able to churn data at these speeds. There is a reason why Mellanox (Nvidia) has developed GPUDirect RDMA, so the GPU and NIC can talk directly to each other.<p><a href="https:&#x2F;&#x2F;www.servethehome.com&#x2F;this-is-the-next-gen-nvidia-connectx-8-supernic-for-800gbps-networking&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.servethehome.com&#x2F;this-is-the-next-gen-nvidia-con...</a></div><br/></div></div><div id="42459615" class="c"><input type="checkbox" id="c-42459615" checked=""/><div class="controls bullet"><span class="by">simoncion</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458092">parent</a><span>|</span><a href="#42459196">prev</a><span>|</span><a href="#42458195">next</a><span>|</span><label class="collapse" for="c-42459615">[-]</label><label class="expand" for="c-42459615">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ...besides main memory?<p>Main memory <i>is</i> an important thing to have fast, though. The faster (and lower-wallclock-latency) it is, the less time your system spends waiting around when it needs to swap things in and out of it. It&#x27;s my understanding that programs that need to be fast (like many video games) take pains to preemptively load data into RAM from disk, and (when appropriate for the program) from main RAM into VRAM. If main RAM&#x27;s transfer speed was equal to or greater than VRAM&#x27;s, and it access latency was a small fraction of a frame render time, (presumably) some of that preloading complexity could go away.<p>&gt; I guess with massive hardware RAID they could be faster...<p>This section of the comment is for folks who haven&#x27;t been paying attention to how fast storage has gotten: It&#x27;s nowhere near 1TB per second, but...<p>I have four 4TB SATA-attached Crucial MX500s set up in LVM2 RAID 0. This array is a bit faster than a 10gbit link. (That is, I get 1.5GByte&#x2F;s transfer rate off of the thing.) Even a single non-garbage U.2-attached (or (barf) M.2-attached) device can saturate a 10Gbit link.</div><br/></div></div><div id="42458195" class="c"><input type="checkbox" id="c-42458195" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458092">parent</a><span>|</span><a href="#42459615">prev</a><span>|</span><a href="#42457692">next</a><span>|</span><label class="collapse" for="c-42458195">[-]</label><label class="expand" for="c-42458195">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Not sure if any consumer adapters in the 10 or 1Tbit&#x2F;sec range exist at all.<p>Further, what exactly is a consumer going to plug a 1Tbit&#x2F;sec adapter into? Your little home ATT fiber connection isn’t going to come close to using that available bandwidth.</div><br/><div id="42459632" class="c"><input type="checkbox" id="c-42459632" checked=""/><div class="controls bullet"><span class="by">simoncion</span><span>|</span><a href="#42457506">root</a><span>|</span><a href="#42458195">parent</a><span>|</span><a href="#42457692">next</a><span>|</span><label class="collapse" for="c-42459632">[-]</label><label class="expand" for="c-42459632">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Further, what exactly is a consumer going to plug a 1Tbit&#x2F;sec adapter into?<p>Another similarly-equipped machine on the LAN, and the switch(es) between them.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42457692" class="c"><input type="checkbox" id="c-42457692" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#42457506">prev</a><span>|</span><a href="#42459055">next</a><span>|</span><label class="collapse" for="c-42457692">[-]</label><label class="expand" for="c-42457692">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Cache is seen as an optimization for small data: if it fits in L2, it’s going to be processed faster</i><p>Nobody worth their salt believes just this and nothing else.<p>Yes, if the data fits entirely into a given cache, that&#x27;s a nice case that&#x27;s easy to reason about. No matter what access pattern is applied to the data, it doesn&#x27;t matter because it&#x27;s in the cache.<p>Hopefully everyone working with caches understands that they provide a potential speedup when <i>not</i> everything fits into the cache, and that this depends on the pattern of access (mainly, does it exhibit &quot;locality&quot;). Moreover, this case is extremely important.<p>The article gives an example of exactly that: improving the locality of access.<p>If you don&#x27;t know this, you don&#x27;t know one of the first facts about caching.<p>There is something else to know about: you can&#x27;t tell by size alone whether a given data set will fit into a cache. The problem is that caches are not always fully associative.  In a set associative cache, a given block of data cannot be stored in any cache line: it is assigned to a small set of possible cache lines. Then within a set, the cache lines are dynamically allocated and tagged. A given bunch of working which appears to be just smaller than the cache might be arranged in such a poor way in memory that it doesn&#x27;t map to all of the cache&#x27;s sets. And so, it actually does not fit into the cache.</div><br/><div id="42459073" class="c"><input type="checkbox" id="c-42459073" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#42457692">parent</a><span>|</span><a href="#42457908">next</a><span>|</span><label class="collapse" for="c-42459073">[-]</label><label class="expand" for="c-42459073">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Nobody worth their salt believes just this and nothing else.... and that this depends on the pattern of access (mainly, does it exhibit &quot;locality&quot;).... If you don&#x27;t know this, you don&#x27;t know one of the first facts about caching.<p>Not necessarily specific to this issue, but I&#x27;ve found that surprisingly many people out there are not &quot;worth their salt&quot; in areas where you&#x27;d really expect them to be.</div><br/></div></div><div id="42457908" class="c"><input type="checkbox" id="c-42457908" checked=""/><div class="controls bullet"><span class="by">purplesyringa</span><span>|</span><a href="#42457692">parent</a><span>|</span><a href="#42459073">prev</a><span>|</span><a href="#42459055">next</a><span>|</span><label class="collapse" for="c-42457908">[-]</label><label class="expand" for="c-42457908">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true, perhaps my wording is off. I believe that the devil in the details. Sure, knowing that better access patterns result in better performance is common. But the fact that the access pattern can be optimized when the problem is literally &quot;access RAM at these random locations&quot; is counterintuitive, IMO.</div><br/><div id="42458674" class="c"><input type="checkbox" id="c-42458674" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42457692">root</a><span>|</span><a href="#42457908">parent</a><span>|</span><a href="#42459055">next</a><span>|</span><label class="collapse" for="c-42458674">[-]</label><label class="expand" for="c-42458674">[1 more]</label></div><br/><div class="children"><div class="content">When you have locality, prefetch can mask the latency of getting the next object, regardless of whether everything fits in cache.</div><br/></div></div></div></div></div></div><div id="42459055" class="c"><input type="checkbox" id="c-42459055" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#42457692">prev</a><span>|</span><a href="#42458999">next</a><span>|</span><label class="collapse" for="c-42459055">[-]</label><label class="expand" for="c-42459055">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a little surprising that this works at all, since the partitioning step in the radix sort is itself the same kind of sharding operation. But I guess that&#x27;s because it allows for working on smaller pieces at a time that fit in whatever cache they need to.<p>&gt; Python can&#x27;t really reserve space for lists, but pretend `reserve` did that anyway.<p>FWIW, you can pre-fill a list with e.g. `None` values, and then <i>replacing</i> those values won&#x27;t cause resizes or relocations (of the <i>list</i>&#x27;s memory - the elements are still indirected). But of course you&#x27;d then need to keep track of the count of &quot;real&quot; elements yourself.<p>But of course, tricks like this are going to be counterproductive in Python anyway, because of all the indirection inherent in the system. They&#x27;ll also get worse if you actually do have objects (unless perhaps they&#x27;re statically, constantly sized, and in a language that can avoid indirection in that case) with a &quot;group ID&quot; attribute, rather than integers to which you can apply some hash function. Some test results, trying to approximate the Rust code in Python but with such objects:<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;zahlman&#x2F;c1d2e98eac57cbb853ce2af515fecc23" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;zahlman&#x2F;c1d2e98eac57cbb853ce2af515fe...</a><p>And as I expected, the results on my (admittedly underpowered) machine are terrible (output is time in seconds):<p><pre><code>    $ .&#x2F;shard.py 
    Naive: 1.1765959519980242
    Presized: 2.254509582002356
    Library sort &#x2F; groupby: 6.990680840001005
    Simple radixing first: 3.571575194997422
</code></pre>
It&#x27;s important to understand the domain. For Pythonistas, simple really is better than complex. And identifying the bottlenecks and moving them to a faster language is also important (Numpy isn&#x27;t successful by accident). The naive result here is still, if I&#x27;m understanding the graphs right, dozens of times worse than what any of the Rust code achieves.<p>(Edit: merely &quot;several&quot; times worse. I forgot that I told `timeit` to run 10 iterations over the same input, so each test processes ~10M elements.)</div><br/><div id="42459193" class="c"><input type="checkbox" id="c-42459193" checked=""/><div class="controls bullet"><span class="by">purplesyringa</span><span>|</span><a href="#42459055">parent</a><span>|</span><a href="#42458999">next</a><span>|</span><label class="collapse" for="c-42459193">[-]</label><label class="expand" for="c-42459193">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s a little surprising that this works at all, since the partitioning step in the radix sort is itself the same kind of sharding operation.<p>The key property here is the number of groups. When sharding data to n groups, only n locations have to be stored in cache -- the tails of the groups. In my radix sort implementation, this is just 256 locations, which works well with cache.</div><br/></div></div></div></div><div id="42458999" class="c"><input type="checkbox" id="c-42458999" checked=""/><div class="controls bullet"><span class="by">awanderingmind</span><span>|</span><a href="#42459055">prev</a><span>|</span><a href="#42459138">next</a><span>|</span><label class="collapse" for="c-42458999">[-]</label><label class="expand" for="c-42458999">[3 more]</label></div><br/><div class="children"><div class="content">This was a great read, thanks. OP, readers might benefit from having it explicitly mentioned that while the pseudocode is in Python, actual Python code will likely not benefit from such an approach because of how memory is fragmented in the standard Python implementation - e.g. this discussion: <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;49786441&#x2F;python-get-processors-l1-cache" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;49786441&#x2F;python-get-proc...</a><p>I am tempted to actually test this (i.e. see if there&#x27;s a speedup in Python), but I don&#x27;t have the time right now.</div><br/><div id="42459061" class="c"><input type="checkbox" id="c-42459061" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#42458999">parent</a><span>|</span><a href="#42459138">next</a><span>|</span><label class="collapse" for="c-42459061">[-]</label><label class="expand" for="c-42459061">[2 more]</label></div><br/><div class="children"><div class="content">I should have looked at the comments before I started writing the code. I&#x27;d have replied to you otherwise :) (see <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42459055">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42459055</a> .)</div><br/><div id="42459090" class="c"><input type="checkbox" id="c-42459090" checked=""/><div class="controls bullet"><span class="by">awanderingmind</span><span>|</span><a href="#42458999">root</a><span>|</span><a href="#42459061">parent</a><span>|</span><a href="#42459138">next</a><span>|</span><label class="collapse" for="c-42459090">[-]</label><label class="expand" for="c-42459090">[1 more]</label></div><br/><div class="children"><div class="content">Haha thanks</div><br/></div></div></div></div></div></div><div id="42459138" class="c"><input type="checkbox" id="c-42459138" checked=""/><div class="controls bullet"><span class="by">xigency</span><span>|</span><a href="#42458999">prev</a><span>|</span><a href="#42457398">next</a><span>|</span><label class="collapse" for="c-42459138">[-]</label><label class="expand" for="c-42459138">[1 more]</label></div><br/><div class="children"><div class="content">There are actually some algorithms specifically designed to optimize usage of cache resources without knowing the specific features of the cache.<p>&quot;Cache Oblivious algorithms&quot; <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cache-oblivious_algorithm" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cache-oblivious_algorithm</a></div><br/></div></div><div id="42457398" class="c"><input type="checkbox" id="c-42457398" checked=""/><div class="controls bullet"><span class="by">CountHackulus</span><span>|</span><a href="#42459138">prev</a><span>|</span><a href="#42459233">next</a><span>|</span><label class="collapse" for="c-42457398">[-]</label><label class="expand" for="c-42457398">[1 more]</label></div><br/><div class="children"><div class="content">Actual benchmarks and graphs. Thank you so much for that.</div><br/></div></div><div id="42459233" class="c"><input type="checkbox" id="c-42459233" checked=""/><div class="controls bullet"><span class="by">lifeisstillgood</span><span>|</span><a href="#42457398">prev</a><span>|</span><a href="#42458480">next</a><span>|</span><label class="collapse" for="c-42459233">[-]</label><label class="expand" for="c-42459233">[3 more]</label></div><br/><div class="children"><div class="content">Anaesthetists are required to undergo days of retraining per year because the field, and the safety numbers, keep moving.<p>To do this researchers publish findings and professionals aggregate those into useful training materials<p>Who &#x2F; where is the useful training materials for software devs? It really cannot be just blogs.<p>That’s a VC investment that has ROI across the industry - tell a16z how to measure that and we are quids in</div><br/><div id="42459515" class="c"><input type="checkbox" id="c-42459515" checked=""/><div class="controls bullet"><span class="by">tialaramex</span><span>|</span><a href="#42459233">parent</a><span>|</span><a href="#42458480">next</a><span>|</span><label class="collapse" for="c-42459515">[-]</label><label class="expand" for="c-42459515">[2 more]</label></div><br/><div class="children"><div class="content">Anaesthetists also undergo many <i>years</i> of training <i>prior</i> to taking post. Typically they&#x27;ve got say 10 years training before they actually do the job, much of it specialised to this job in particular (and the rest in general or intensive care medicine)<p>If you&#x27;re lucky a Software Engineer has a three year degree in CS, and probably only a semester at best was studying &quot;Software Engineering&quot; and even that might focus on something you don&#x27;t care about, such as formal methods.<p>It is <i>entirely possible</i> that your junior engineers have never maintained a sizeable codebase for more than a few weeks, have never co-operated on software with more than a handful of other people, and have never used most of the common software libraries you use every day, regardless of whether these are in-house (so how could they) or widely available.<p>For example maybe you do lots of web apps fronting SQL Server databases in C#. Your new hire has six months of C#, they half-remember a course doing SQL on an Oracle database, and all their web knowledge is in Javascript. Do they know version control? Kinda. Have they used a test framework before? Well, they did in Java but never in C#.<p>The &quot;All Your Tests Are Terrible&quot; talk begins by pointing out that probably they&#x27;re strictly wrong because you <i>don&#x27;t have any fucking tests</i>. All of the rest of the talk is about the hideous tests that you&#x27;re unhappy to find because you forget that somebody could just not have bothered to write any tests at all.</div><br/><div id="42459537" class="c"><input type="checkbox" id="c-42459537" checked=""/><div class="controls bullet"><span class="by">globnomulous</span><span>|</span><a href="#42459233">root</a><span>|</span><a href="#42459515">parent</a><span>|</span><a href="#42458480">next</a><span>|</span><label class="collapse" for="c-42459537">[-]</label><label class="expand" for="c-42459537">[1 more]</label></div><br/><div class="children"><div class="content">Hey, I resent and agree with this!</div><br/></div></div></div></div></div></div><div id="42458480" class="c"><input type="checkbox" id="c-42458480" checked=""/><div class="controls bullet"><span class="by">shmerl</span><span>|</span><a href="#42459233">prev</a><span>|</span><a href="#42457739">next</a><span>|</span><label class="collapse" for="c-42458480">[-]</label><label class="expand" for="c-42458480">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; The RAM myth is a belief that modern computer memory resembles perfect random-access memory. Cache is seen as an optimization for small data</i><p>The post doesn&#x27;t seem to answer what the memory actually resembles. If it&#x27;s not resembling a random access memory, then what is it resembling?</div><br/><div id="42459106" class="c"><input type="checkbox" id="c-42459106" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#42458480">parent</a><span>|</span><a href="#42457739">next</a><span>|</span><label class="collapse" for="c-42459106">[-]</label><label class="expand" for="c-42459106">[1 more]</label></div><br/><div class="children"><div class="content">It resembles a hierarchy wherein a small fraction of memory - a &quot;cache&quot; region - can be accessed much faster than the rest. With careful planning, the programmer can increase the odds that the necessary information for the next step of an algorithm, at any given point, is within that small region. (This is a simplification; there are actually several layers of cache between a CPU and the most general part of the RAM.)</div><br/></div></div></div></div><div id="42457739" class="c"><input type="checkbox" id="c-42457739" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#42458480">prev</a><span>|</span><a href="#42458062">next</a><span>|</span><label class="collapse" for="c-42457739">[-]</label><label class="expand" for="c-42457739">[4 more]</label></div><br/><div class="children"><div class="content">Exemplar code in python, shows the benefit in rust. o---kaaaaay.<p>So the outcome is &quot;for this compiled rust, around 1m records it gets better&quot;<p>But you didn&#x27;t actually prove a general case, you proved &quot;for a good optimising rust compiler&quot; didn&#x27;t you?<p>Maybe I&#x27;m over-thinking it. Maybe this does just become simple, working-set-locality stuff.<p>I could take the lesson: for less than 10m things, on modern hardware with more than 4 cores and more than 4GB stop worrying and just code in the idiom which works for you.</div><br/><div id="42457863" class="c"><input type="checkbox" id="c-42457863" checked=""/><div class="controls bullet"><span class="by">purplesyringa</span><span>|</span><a href="#42457739">parent</a><span>|</span><a href="#42458062">next</a><span>|</span><label class="collapse" for="c-42457863">[-]</label><label class="expand" for="c-42457863">[3 more]</label></div><br/><div class="children"><div class="content">Uh? The post very clearly says: &quot;I’m using Python as pseudocode; pretend I used your favorite low-level language&quot;. The goal is to show what&#x27;s possible when you do your best, of course I&#x27;m not going to use Python for anything beyond demonstrating ideas.<p>&gt; But you didn&#x27;t actually prove a general case, you proved &quot;for a good optimising rust compiler&quot; didn&#x27;t you?<p>Again, that was never my goal. I chose Rust because I&#x27;ve stumbled upon this problem while working on a Rust library. I could&#x27;ve chosen C++, or C, or maybe even Go -- the result would&#x27;ve been the same, and I checked codegen to make sure of that.<p>&gt; I could take the lesson: for less than 10m things, on modern hardware with more than 4 cores and more than 4GB stop worrying and just code in the idiom which works for you.<p>The number of cores and RAM capacity has nothing to do with this. It&#x27;s all about how well data fits in cache, and &quot;less than 10m things&quot; are likely to fit in L3 anyway. If your main takeaway from &quot;here&#x27;s how to process large data&quot; was &quot;I don&#x27;t need to worry about this for small data&quot;, well, I don&#x27;t know what to say.</div><br/><div id="42458034" class="c"><input type="checkbox" id="c-42458034" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#42457739">root</a><span>|</span><a href="#42457863">parent</a><span>|</span><a href="#42458759">next</a><span>|</span><label class="collapse" for="c-42458034">[-]</label><label class="expand" for="c-42458034">[1 more]</label></div><br/><div class="children"><div class="content">Large and Small are so contextual. I&#x27;m processing 350m events&#x2F;day in 24h splits, and I managed to stop worrying about locality of reference because I&#x27;m the sole occupant of the machine. When I did worry about it, I found radix tree, awk hash and perl&#x2F;python hash&#x2F;dict pretty much all occupied much the same space and time but a tuned C implementation got 2-3x faster than any of them. Somebody else pointed out memory resident for most of this would be faster still but you have to then work to process 24h of things against a single memory instance. Which means buying into IPC to get the data &quot;into&quot; that memory.<p>It interested me you didn&#x27;t show the idea in rust. That was the only point I was making: Python as pseudocode to think things in documents is fine with me.<p>But overall, I liked your outcome. I just think it&#x27;s important to remember large and small are very contextual. Your large case looks to me to be &gt;5m things and for an awful lot of people doing stupid things, 5m is bigger than they&#x27;ll ever see. If the target was only people who routinely deal in hundreds of millions of things, then sure.</div><br/></div></div><div id="42458759" class="c"><input type="checkbox" id="c-42458759" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42457739">root</a><span>|</span><a href="#42457863">parent</a><span>|</span><a href="#42458034">prev</a><span>|</span><a href="#42458062">next</a><span>|</span><label class="collapse" for="c-42458759">[-]</label><label class="expand" for="c-42458759">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The number of cores and RAM capacity has nothing to do with this. It&#x27;s all about how well data fits in cache, and &quot;less than 10m things&quot; are likely to fit in L3 anyway.<p>What matters is locality since that allows prefetch to mask latency. If you have this, then you are in a good place even if your data does not fit in the L3 cache. What you did demonstrates the benefits that locality gives from the effect on prefetch. Fitting in L3 cache helps, but not as much as prefetch does. If you do not believe me, test a random access pattern on things in L3 cache vs a sequential access pattern. The sequential access pattern will win every time, because L3 cache is relatively slow and prefetch masks that latency.<p>I have seen options for disabling prefetch and cache as BIOS options (although I only remember the option for disabling cache in ancient systems). If you could get one, you could do some experiments to see which will matter more.</div><br/></div></div></div></div></div></div><div id="42458062" class="c"><input type="checkbox" id="c-42458062" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#42457739">prev</a><span>|</span><a href="#42459548">next</a><span>|</span><label class="collapse" for="c-42458062">[-]</label><label class="expand" for="c-42458062">[1 more]</label></div><br/><div class="children"><div class="content">I was expecting something about NUMA performance, temporal memory access instructions, shared versus global versus register memory on GPUs, SRAM, and so on. There&#x27;s an article about all these things waiting to be written. This article is instead about memory-hierarchy-aware access pattern optimization, which is important, but not the whole story.</div><br/></div></div><div id="42459548" class="c"><input type="checkbox" id="c-42459548" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#42458062">prev</a><span>|</span><label class="collapse" for="c-42459548">[-]</label><label class="expand" for="c-42459548">[1 more]</label></div><br/><div class="children"><div class="content">Tl;dr: cache is the new RAM; RAM is the new disk; disk is the new tape.</div><br/></div></div></div></div></div></div></div></body></html>