<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724144466050" as="style"/><link rel="stylesheet" href="styles.css?v=1724144466050"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://theconversation.com/what-is-model-collapse-an-expert-explains-the-rumours-about-an-impending-ai-doom-236415">&#x27;Model collapse&#x27;? An expert explains the rumours about an impending AI doom</a> <span class="domain">(<a href="https://theconversation.com">theconversation.com</a>)</span></div><div class="subtext"><span>ColinWright</span> | <span>5 comments</span></div><br/><div><div id="41298179" class="c"><input type="checkbox" id="c-41298179" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#41298177">next</a><span>|</span><label class="collapse" for="c-41298179">[-]</label><label class="expand" for="c-41298179">[1 more]</label></div><br/><div class="children"><div class="content">&gt; To train GPT-3, OpenAI needed over 650 billion English words of text – about 200x more than the entire English Wikipedia.<p>Since, I assume, humans don&#x27;t need this much training (?) this area would seem to be ripe to explore — can you achieve similar training with a fraction of the data needed for GPT-3.</div><br/></div></div><div id="41298177" class="c"><input type="checkbox" id="c-41298177" checked=""/><div class="controls bullet"><span class="by">antklan</span><span>|</span><a href="#41298179">prev</a><span>|</span><a href="#41298125">next</a><span>|</span><label class="collapse" for="c-41298177">[-]</label><label class="expand" for="c-41298177">[2 more]</label></div><br/><div class="children"><div class="content">This can be replicated on a small scale by using two LLMs (which can be two instances of the same LLM). Start with a human prompt, then feed the answer of LLM-1 as the prompt to LLM-2, then feed that answer of LLM-2 to LLM-1 and so forth.<p>The answers soon converge to some boring, bland repetition that isn&#x27;t even logical.<p>No intelligence here, which means that code emitting LLMs are just stealing human IP that they happened to have read.</div><br/><div id="41298188" class="c"><input type="checkbox" id="c-41298188" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#41298177">parent</a><span>|</span><a href="#41298125">next</a><span>|</span><label class="collapse" for="c-41298188">[-]</label><label class="expand" for="c-41298188">[1 more]</label></div><br/><div class="children"><div class="content">I too was trained stealing human &quot;IP&quot;. It just took decades (perhaps is continuing still). Maybe slow, ongoing AI is the future.</div><br/></div></div></div></div><div id="41298125" class="c"><input type="checkbox" id="c-41298125" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41298177">prev</a><span>|</span><label class="collapse" for="c-41298125">[-]</label><label class="expand" for="c-41298125">[1 more]</label></div><br/><div class="children"><div class="content">This could use a bit more nuance around training from AI. While the naive approaches will experience worse replies, there are many documented cases where the quality improves instead. Star, self-reflection, groups of agents, and likely others I don&#x27;t know about all improve the results using only the same model&#x27;s output.</div><br/></div></div></div></div></div></div></div></body></html>