<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1738486854908" as="style"/><link rel="stylesheet" href="styles.css?v=1738486854908"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.quantamagazine.org/chatbot-software-begins-to-face-fundamental-limitations-20250131/">Recent results show that LLMs struggle with compositional tasks</a> <span class="domain">(<a href="https://www.quantamagazine.org">www.quantamagazine.org</a>)</span></div><div class="subtext"><span>marban</span> | <span>117 comments</span></div><br/><div><div id="42906389" class="c"><input type="checkbox" id="c-42906389" checked=""/><div class="controls bullet"><span class="by">moolimon</span><span>|</span><a href="#42905986">next</a><span>|</span><label class="collapse" for="c-42906389">[-]</label><label class="expand" for="c-42906389">[4 more]</label></div><br/><div class="children"><div class="content">The main thesis here seems to be that LLMs behave like almost all other machine learning models, in that they are doing pattern matching on their input data, and short circuiting to a statistically likely result. Chain of thought reasoning is still bound by this basic property of reflexive pattern matching, except the LLM is forced to go through a process of iteratively refining the domain it does matching on.<p>Chain of thought is interesting, because you can combine it with reinforcement learning to get models to solve (seemingly) arbitrarily hard problems. This comes with the caveat that you need some reward model for all RL. This means you need a clear definition of success, and some way of rewarding being closer to success, to actually solve those problems.<p>Framing transformer based models as pattern matchers makes all the sense in the world. Pattern matching is obviously vital to human problem solving skills too. Interesting to think about what structures human intelligence has that these models don&#x27;t. For one, humans can integrate absolutely gargantuan amounts of information extremely efficiently.</div><br/><div id="42907092" class="c"><input type="checkbox" id="c-42907092" checked=""/><div class="controls bullet"><span class="by">huijzer</span><span>|</span><a href="#42906389">parent</a><span>|</span><a href="#42906565">next</a><span>|</span><label class="collapse" for="c-42907092">[-]</label><label class="expand" for="c-42907092">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Framing transformer based models as pattern matchers makes all the sense in the world. Pattern matching is obviously vital to human problem solving skills too. Interesting to think about what structures human intelligence has that these models don&#x27;t. For one, humans can integrate absolutely gargantuan amounts of information extremely efficiently.<p>What is also a benefit for humans, I think, is that people are typically much more selective. LLMs train to predict anything on the internet, so for example for finance that includes clickbait articles which have a lifetime of about 2 hours. Experts would probably reject any information in these articles and instead try to focus on high quality sources only.<p>Similarly, a math researcher will probably have read a completely set of sources throughout the life than, say, a lawyer.<p>I’m not sure it’s a fundamental difference, but current models do seem to not specialize from the start unlike humans. And that might be in the way of learning the best representations. I know from ice hockey for example, that you can see within 3 seconds whether someone played ice hockey from young age or not. Same with language. People can usually hear an accent within seconds. Relatedly, I&#x27;ve used OpenAI&#x27;s text to speech a while back and the Dutch voice had an American accent. What this means is that even if you ask LLMs about Buffett&#x27;s strategy, maybe they have a &quot;clickbait accent&quot; too. So with the current approach to training, the models might never reach absolute expert performance.</div><br/></div></div><div id="42906565" class="c"><input type="checkbox" id="c-42906565" checked=""/><div class="controls bullet"><span class="by">drakenot</span><span>|</span><a href="#42906389">parent</a><span>|</span><a href="#42907092">prev</a><span>|</span><a href="#42905986">next</a><span>|</span><label class="collapse" for="c-42906565">[-]</label><label class="expand" for="c-42906565">[2 more]</label></div><br/><div class="children"><div class="content">With DeepSeek-R1-Zero, their usage of RL didn&#x27;t have reward functions really that indicated progress towards the goal afaik.<p>It was &quot;correct structure, wrong answer&quot;, &quot;correct answer&quot;, &quot;wrong answer&quot;.  This was for Math &amp; Coding, where they could verify answers deterministically.</div><br/></div></div></div></div><div id="42905986" class="c"><input type="checkbox" id="c-42905986" checked=""/><div class="controls bullet"><span class="by">geoffhill</span><span>|</span><a href="#42906389">prev</a><span>|</span><a href="#42907147">next</a><span>|</span><label class="collapse" for="c-42905986">[-]</label><label class="expand" for="c-42905986">[21 more]</label></div><br/><div class="children"><div class="content">Idk, `o3-mini-high` was able to pop this Prolog code out in about 20 seconds:<p><pre><code>  solve(WaterDrinker, ZebraOwner) :-
      % H01: Five houses with positions 1..5.
      Houses = [ house(1, _, norwegian, _, _, _),  % H10: Norwegian lives in the first house.
                 house(2, blue, _, _, _, _),       % H15: Since the Norwegian lives next to the blue house,
                 house(3, _, _, milk, _, _),        %       and house1 is Norwegian, house2 must be blue.
                 house(4, _, _, _, _, _),
                 house(5, _, _, _, _, _) ],
  
      % H02: The Englishman lives in the red house.
      member(house(_, red, englishman, _, _, _), Houses),
      % H03: The Spaniard owns the dog.
      member(house(_, _, spaniard, _, dog, _), Houses),
      % H04: Coffee is drunk in the green house.
      member(house(_, green, _, coffee, _, _), Houses),
      % H05: The Ukrainian drinks tea.
      member(house(_, _, ukrainian, tea, _, _), Houses),
      % H06: The green house is immediately to the right of the ivory house.
      right_of(house(_, green, _, _, _, _), house(_, ivory, _, _, _, _), Houses),
      % H07: The Old Gold smoker owns snails.
      member(house(_, _, _, _, snails, old_gold), Houses),
      % H08: Kools are smoked in the yellow house.
      member(house(_, yellow, _, _, _, kools), Houses),
      % H11: The man who smokes Chesterfields lives in the house next to the man with the fox.
      next_to(house(_, _, _, _, _, chesterfields), house(_, _, _, _, fox, _), Houses),
      % H12: Kools are smoked in a house next to the house where the horse is kept.
      next_to(house(_, _, _, _, horse, _), house(_, _, _, _, _, kools), Houses),
      % H13: The Lucky Strike smoker drinks orange juice.
      member(house(_, _, _, orange_juice, _, lucky_strike), Houses),
      % H14: The Japanese smokes Parliaments.
      member(house(_, _, japanese, _, _, parliaments), Houses),
      % (H09 is built in: Milk is drunk in the middle house, i.e. house3.)
      
      % Finally, find out:
      % Q1: Who drinks water?
      member(house(_, _, WaterDrinker, water, _, _), Houses),
      % Q2: Who owns the zebra?
      member(house(_, _, ZebraOwner, _, zebra, _), Houses).
  
  right_of(Right, Left, Houses) :-
      nextto(Left, Right, Houses).
  
  next_to(X, Y, Houses) :-
      nextto(X, Y, Houses);
      nextto(Y, X, Houses).
</code></pre>
Seems ok to me.<p><pre><code>   ?- solve(WaterDrinker, ZebraOwner).
   WaterDrinker = norwegian,
   ZebraOwner = japanese .</code></pre></div><br/><div id="42906399" class="c"><input type="checkbox" id="c-42906399" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#42905986">parent</a><span>|</span><a href="#42906041">next</a><span>|</span><label class="collapse" for="c-42906399">[-]</label><label class="expand" for="c-42906399">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s because it uses a long CoT. The actual paper [1] [2] talks about the limitations of decoder-only transformers predicting the reply directly, although it also establishes the benefits of CoT for composition.<p>This is all known for a long time and makes intuitive sense - you can&#x27;t squeeze more computation from it than it can provide. The authors just formally proved it (which is no small deal). And Quanta is being dramatic with conclusions and headlines, as always.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.02975" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.02975</a><p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42889786">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42889786</a></div><br/><div id="42907026" class="c"><input type="checkbox" id="c-42907026" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906399">parent</a><span>|</span><a href="#42907056">next</a><span>|</span><label class="collapse" for="c-42907026">[-]</label><label class="expand" for="c-42907026">[1 more]</label></div><br/><div class="children"><div class="content">LLMs using CoT are also decoder-only, it&#x27;s not a paradigm shift like people want to claim now to don&#x27;t say they were wrong: it&#x27;s still next token prediction, that is forced to explore more possibilities in the space it contains. And with R1-Zero we also know that LLMs can train themselves to do so.</div><br/></div></div><div id="42907056" class="c"><input type="checkbox" id="c-42907056" checked=""/><div class="controls bullet"><span class="by">teruakohatu</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906399">parent</a><span>|</span><a href="#42907026">prev</a><span>|</span><a href="#42907015">next</a><span>|</span><label class="collapse" for="c-42907056">[-]</label><label class="expand" for="c-42907056">[1 more]</label></div><br/><div class="children"><div class="content">gpt-4o, asked to produce swi-prolog code, gets the same result using a very similar code. gpt4-turbo can do it with slightly less nice code. gpt-3.5-turbo struggled to get the syntax correct but I think with some better prompting could manage it.<p>COT is defiantly optional. Although I am sure all LLM have seen this problem explained and solved in training data.</div><br/></div></div><div id="42907015" class="c"><input type="checkbox" id="c-42907015" checked=""/><div class="controls bullet"><span class="by">mycall</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906399">parent</a><span>|</span><a href="#42907056">prev</a><span>|</span><a href="#42906655">next</a><span>|</span><label class="collapse" for="c-42907015">[-]</label><label class="expand" for="c-42907015">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t include Encoder-Decoder Transformer Fusion for machine translation, or Encoder-Only like text classification, named entity recognition or BERT.</div><br/></div></div><div id="42906655" class="c"><input type="checkbox" id="c-42906655" checked=""/><div class="controls bullet"><span class="by">leonidasv</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906399">parent</a><span>|</span><a href="#42907015">prev</a><span>|</span><a href="#42906041">next</a><span>|</span><label class="collapse" for="c-42906655">[-]</label><label class="expand" for="c-42906655">[1 more]</label></div><br/><div class="children"><div class="content">Also, notice that the original study is from 2023.</div><br/></div></div></div></div><div id="42906041" class="c"><input type="checkbox" id="c-42906041" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905986">parent</a><span>|</span><a href="#42906399">prev</a><span>|</span><a href="#42907087">next</a><span>|</span><label class="collapse" for="c-42906041">[-]</label><label class="expand" for="c-42906041">[6 more]</label></div><br/><div class="children"><div class="content">Solving it with Prolog is neat, and a very realistic way of how LLMs with tools should be expected to handle this kind of thing.</div><br/><div id="42906271" class="c"><input type="checkbox" id="c-42906271" checked=""/><div class="controls bullet"><span class="by">EdwardDiego</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906041">parent</a><span>|</span><a href="#42907087">next</a><span>|</span><label class="collapse" for="c-42906271">[-]</label><label class="expand" for="c-42906271">[5 more]</label></div><br/><div class="children"><div class="content">I would&#x27;ve been very surprised if Prolog to solve this wasn&#x27;t something that the model had already ingested.<p>Early AI hype cycles, after all, is where Prolog, like Lisp, shone.</div><br/><div id="42906572" class="c"><input type="checkbox" id="c-42906572" checked=""/><div class="controls bullet"><span class="by">plasticeagle</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906271">parent</a><span>|</span><a href="#42906581">next</a><span>|</span><label class="collapse" for="c-42906572">[-]</label><label class="expand" for="c-42906572">[1 more]</label></div><br/><div class="children"><div class="content">Indeed.<p><a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;9252656&#x2F;einsteins-riddle-prolog" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;9252656&#x2F;einsteins-riddle...</a></div><br/></div></div><div id="42906581" class="c"><input type="checkbox" id="c-42906581" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906271">parent</a><span>|</span><a href="#42906572">prev</a><span>|</span><a href="#42907087">next</a><span>|</span><label class="collapse" for="c-42906581">[-]</label><label class="expand" for="c-42906581">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m certain models like o3-mini are capable of writing Prolog of this quality for puzzles they haven&#x27;t seen before - it feels like a very straight-forward conversion operation for them.</div><br/><div id="42906908" class="c"><input type="checkbox" id="c-42906908" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906581">parent</a><span>|</span><a href="#42907020">next</a><span>|</span><label class="collapse" for="c-42906908">[-]</label><label class="expand" for="c-42906908">[1 more]</label></div><br/><div class="children"><div class="content">My comment got eaten by HN, but I think LLMs should be used as the glue between logic systems like prolog, with inductive, deductive and abductive reasoning being handed off to a tool. LLMs are great at pattern matching, but forcing them to reason seems like an out of envelope use.<p>Prolog would be how I would solve puzzles like that as well. It is like calling someone weak for using a spreadsheet or a calculator.<p>Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14618" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14618</a></div><br/></div></div><div id="42907020" class="c"><input type="checkbox" id="c-42907020" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906581">parent</a><span>|</span><a href="#42906908">prev</a><span>|</span><a href="#42907087">next</a><span>|</span><label class="collapse" for="c-42907020">[-]</label><label class="expand" for="c-42907020">[1 more]</label></div><br/><div class="children"><div class="content">I’ve used DeepSeek for verifying a couple gnarly boolean conditions in home assistant with z3 and it did a good job, though it didn’t one shot it.</div><br/></div></div></div></div></div></div></div></div><div id="42907087" class="c"><input type="checkbox" id="c-42907087" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#42905986">parent</a><span>|</span><a href="#42906041">prev</a><span>|</span><a href="#42906332">next</a><span>|</span><label class="collapse" for="c-42907087">[-]</label><label class="expand" for="c-42907087">[2 more]</label></div><br/><div class="children"><div class="content">Science is not in the proving of it.<p>It’s in the disproving of it, and in the finding of the terms that help others understand the limits.<p>I dont know why it took me so long to come to that sentence. Yes, everyone can trot out their core proof.<p>The research is motivated by these examples in the first place.</div><br/><div id="42907266" class="c"><input type="checkbox" id="c-42907266" checked=""/><div class="controls bullet"><span class="by">Agraillo</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42907087">parent</a><span>|</span><a href="#42906332">next</a><span>|</span><label class="collapse" for="c-42907266">[-]</label><label class="expand" for="c-42907266">[1 more]</label></div><br/><div class="children"><div class="content">Good point. LLMs can be treated as &quot;theories&quot; and then they definitely meet falsifiability [1] allowing researchers finding &quot;black swans&quot; for years to come. Theories in this case can be different. But if the theory is of logical or symbolic solver then Wolfram&#x27;s Mathematica may be struggle with understanding the human language as an input, but when evaluating the results, well, I think Stephen (Wolfram) can sleep soundly, at least for now<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Falsifiability" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Falsifiability</a></div><br/></div></div></div></div><div id="42906332" class="c"><input type="checkbox" id="c-42906332" checked=""/><div class="controls bullet"><span class="by">lsy</span><span>|</span><a href="#42905986">parent</a><span>|</span><a href="#42907087">prev</a><span>|</span><a href="#42906639">next</a><span>|</span><label class="collapse" for="c-42906332">[-]</label><label class="expand" for="c-42906332">[4 more]</label></div><br/><div class="children"><div class="content">If the LLM’s user indicates that the input can and should be translated as a logic problem, and then the user runs that definition in an external Prolog solver, what’s the LLM really doing here? Probabilistically mapping a logic problem to Prolog? That’s not quite the LLM solving the problem.</div><br/><div id="42906394" class="c"><input type="checkbox" id="c-42906394" checked=""/><div class="controls bullet"><span class="by">xyzzy123</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906332">parent</a><span>|</span><a href="#42907024">next</a><span>|</span><label class="collapse" for="c-42906394">[-]</label><label class="expand" for="c-42906394">[1 more]</label></div><br/><div class="children"><div class="content">Do you feel differently if it runs the prolog in a tool call?</div><br/></div></div><div id="42907024" class="c"><input type="checkbox" id="c-42907024" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906332">parent</a><span>|</span><a href="#42906394">prev</a><span>|</span><a href="#42906350">next</a><span>|</span><label class="collapse" for="c-42907024">[-]</label><label class="expand" for="c-42907024">[1 more]</label></div><br/><div class="children"><div class="content">But the problem is solved. Depends what you care about.</div><br/></div></div><div id="42906350" class="c"><input type="checkbox" id="c-42906350" checked=""/><div class="controls bullet"><span class="by">endofreach</span><span>|</span><a href="#42905986">root</a><span>|</span><a href="#42906332">parent</a><span>|</span><a href="#42907024">prev</a><span>|</span><a href="#42906639">next</a><span>|</span><label class="collapse" for="c-42906350">[-]</label><label class="expand" for="c-42906350">[1 more]</label></div><br/><div class="children"><div class="content">Psst, don&#x27;t tell my clients that it&#x27;s not actually me but the languages syntax i use, that&#x27;s solving their problem.</div><br/></div></div></div></div><div id="42906639" class="c"><input type="checkbox" id="c-42906639" checked=""/><div class="controls bullet"><span class="by">est</span><span>|</span><a href="#42905986">parent</a><span>|</span><a href="#42906332">prev</a><span>|</span><a href="#42906339">next</a><span>|</span><label class="collapse" for="c-42906639">[-]</label><label class="expand" for="c-42906639">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say not only LLM stuggle with these kind of problems, 99% of humans do.</div><br/></div></div><div id="42906339" class="c"><input type="checkbox" id="c-42906339" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42905986">parent</a><span>|</span><a href="#42906639">prev</a><span>|</span><a href="#42906418">next</a><span>|</span><label class="collapse" for="c-42906339">[-]</label><label class="expand" for="c-42906339">[1 more]</label></div><br/><div class="children"><div class="content">The LLM doesn&#x27;t understand it&#x27;s doing this, though. It pattern matched against your &quot;steering&quot; in a way that generalized. And it didn&#x27;t hallucinate in this particular case. That&#x27;s still cherry picking, and you wouldn&#x27;t trust this to turn a $500k screw.<p>I feel like we&#x27;re at 2004 Darpa Grand Challenge level, but we&#x27;re nowhere near solving all of the issues required to run this on public streets. It&#x27;s impressive, but leaves an enormous amount to be desired.<p>I think we&#x27;ll get there, but I don&#x27;t think it&#x27;ll be in just a few short years. The companies hyping that this accelerated timeline is just around the corner are doing so out of existential need to keep the funding flowing.</div><br/></div></div><div id="42906418" class="c"><input type="checkbox" id="c-42906418" checked=""/><div class="controls bullet"><span class="by">tuatoru</span><span>|</span><a href="#42905986">parent</a><span>|</span><a href="#42906339">prev</a><span>|</span><a href="#42907147">next</a><span>|</span><label class="collapse" for="c-42906418">[-]</label><label class="expand" for="c-42906418">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    solve (make me a sandwich)
</code></pre>
Moravec&#x27;s Paradox is still a thing.</div><br/></div></div></div></div><div id="42906683" class="c"><input type="checkbox" id="c-42906683" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42907147">prev</a><span>|</span><a href="#42905745">next</a><span>|</span><label class="collapse" for="c-42906683">[-]</label><label class="expand" for="c-42906683">[17 more]</label></div><br/><div class="children"><div class="content">LLMs keep showing more and more they are the wonder of AI that we awaited for decades: talking machines that every two months do progresses that two months before were anticipated impossible because of &lt;put here some limit that was actually in the prejudice of the skeptical AI community&gt; (just stochastic parrots, no reasoning possible without symbolic representations, there are no longer tokens, ...)<p>At the same time, part of the scientific community continues to diminish what was accomplished and the steps that are being made. A few months ago LeCun arrived to tell new researchers to move away from LLMs since they are a dead end: imagine the disservice he made to the surely non-zero folks that followed the advice, putting themselves out of the AI research that matters. (Incidentally, this skepticism of the Meta AI head must have something to do with the fact that Meta, despite the huge efforts allocated, produced the worst LLM among Anthropic, OpenAI, DeepSeek -- I bet Zuckerberg is asking questions lately).<p>It&#x27;s very hard to explain this behavior if not by psychological denial.<p>[EDIT: you can&#x27;t see the score of this comment, but I can: it&#x27;s incredible how it goes from 3, to -2, to 1, and so forth. The community is split in two, and it is pretty sad since this is not a matter of taste or political inclination: there must be a single truth]</div><br/><div id="42906734" class="c"><input type="checkbox" id="c-42906734" checked=""/><div class="controls bullet"><span class="by">ramblerman</span><span>|</span><a href="#42906683">parent</a><span>|</span><a href="#42906995">next</a><span>|</span><label class="collapse" for="c-42906734">[-]</label><label class="expand" for="c-42906734">[4 more]</label></div><br/><div class="children"><div class="content">I get the sentiment, but I actually think some skepticism in the system is healthy.<p>Billions are flowing towards LLMS, and Sam Altman will overpromise AGI is just around the corner and the days of jobs are gone to fill his coffers to anyone that will listen.<p>Additionally if we begin to use these things in real production environments where mistakes matters, knowing the exact limitations is key.<p>None of this takes away from the fact that these are exciting times.</div><br/><div id="42906766" class="c"><input type="checkbox" id="c-42906766" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906734">parent</a><span>|</span><a href="#42906816">next</a><span>|</span><label class="collapse" for="c-42906766">[-]</label><label class="expand" for="c-42906766">[1 more]</label></div><br/><div class="children"><div class="content">Yes there is another part of the community that overhypes everything. But I can expect that from a CEO of an AI company (especially if he is Altman), but from researches? Also the fact that LLMs may reach superhuman expertise in certain fields in a short timeframe (a few years), since reinforcement learning is starting to be applied to LLMs may no longer be a totally crazy position. If it is possible to extend considerably the same approach seen in R1-Zero there could be low hanging fruits around the corner.</div><br/></div></div><div id="42906816" class="c"><input type="checkbox" id="c-42906816" checked=""/><div class="controls bullet"><span class="by">comeonbro</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906734">parent</a><span>|</span><a href="#42906766">prev</a><span>|</span><a href="#42906995">next</a><span>|</span><label class="collapse" for="c-42906816">[-]</label><label class="expand" for="c-42906816">[2 more]</label></div><br/><div class="children"><div class="content">This article is about things which aren&#x27;t limitations anymore!<p>You are applauding it as pushback for pushback&#x27;s sake, but it&#x27;s an article about limitations in biplane construction, published after we&#x27;d already landed on the moon.</div><br/><div id="42907182" class="c"><input type="checkbox" id="c-42907182" checked=""/><div class="controls bullet"><span class="by">suddenlybananas</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906816">parent</a><span>|</span><a href="#42906995">next</a><span>|</span><label class="collapse" for="c-42907182">[-]</label><label class="expand" for="c-42907182">[1 more]</label></div><br/><div class="children"><div class="content">Is there any evidence that these fundamental issues with compositionality have been resolved or are you just asserting it? Has the paper been replicated with a CoT model and had a positive result?</div><br/></div></div></div></div></div></div><div id="42906995" class="c"><input type="checkbox" id="c-42906995" checked=""/><div class="controls bullet"><span class="by">raindear</span><span>|</span><a href="#42906683">parent</a><span>|</span><a href="#42906734">prev</a><span>|</span><a href="#42906798">next</a><span>|</span><label class="collapse" for="c-42906995">[-]</label><label class="expand" for="c-42906995">[1 more]</label></div><br/><div class="children"><div class="content">Progress is what happens thanks to AI skeptics busy defining model limitations. The limitations set attractive bars to pass.</div><br/></div></div><div id="42906798" class="c"><input type="checkbox" id="c-42906798" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42906683">parent</a><span>|</span><a href="#42906995">prev</a><span>|</span><a href="#42906787">next</a><span>|</span><label class="collapse" for="c-42906798">[-]</label><label class="expand" for="c-42906798">[5 more]</label></div><br/><div class="children"><div class="content">It is becoming more and more important to determine for ourselves what is true and what is not. No person is right on most things, even when they are an expert in that thing. The biggest trap, is to believe someone because they are passionate, that they say it with conviction. Ignore most of the out of band signaling, take what they are saying and then also see if you can corroborate with another source.<p>There are so many people who are wrong about so may things.<p>I really appreciate that you are making your dev with ai videos, it shows people different, more humanistic ways of operating with AI.<p>Most of what I use AI for is to understand and relearn things I only thought I knew. This I think, is the most powerful use of AI, not in the code writing or the image generation, but in understanding and synthesis.<p>There is that hilarious tautological statement, &quot;it is easy if you know it&quot;.<p>This video <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TPLPpz6dD3A" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TPLPpz6dD3A</a> shows how to use AI to be a personal tutor using the Socratic Method.  This is what people should be using AI for, have it test you for things you <i>think</i> you are already good at and you will find huge gaps in your own understanding. Now go apply it to things you have no clue about.<p>Speaking of parrots, a large volume of the anti AI sentiment, even here is by people repeating half truths they don&#x27;t understand, confidently, about what AI cannot do. One would need a pretty tight formal case to prove such things.<p>Everyone should be playing, learning and exploring with these new tools, not shutting each other down.</div><br/><div id="42906829" class="c"><input type="checkbox" id="c-42906829" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906798">parent</a><span>|</span><a href="#42906822">next</a><span>|</span><label class="collapse" for="c-42906829">[-]</label><label class="expand" for="c-42906829">[3 more]</label></div><br/><div class="children"><div class="content">Yes, the stochastic parrots story is one of the most strong instances in recent times where experts in a field are made blind by their own expertise (the mental model they have of certain things) to the point of being incapable of seeing trivial evidences.</div><br/><div id="42906883" class="c"><input type="checkbox" id="c-42906883" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906829">parent</a><span>|</span><a href="#42906822">next</a><span>|</span><label class="collapse" for="c-42906883">[-]</label><label class="expand" for="c-42906883">[2 more]</label></div><br/><div class="children"><div class="content">There’s a certain irony in hearing someone describe an LLM as a &quot;stochastic parrot&quot; for the ten-thousandth time when the only reason they’re doing so is that they’ve seen a sufficient number of other people using the exact same term (so now it&#x27;s in their proverbial training data).</div><br/><div id="42906912" class="c"><input type="checkbox" id="c-42906912" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906883">parent</a><span>|</span><a href="#42906822">next</a><span>|</span><label class="collapse" for="c-42906912">[-]</label><label class="expand" for="c-42906912">[1 more]</label></div><br/><div class="children"><div class="content">All they do is predict the next word!</div><br/></div></div></div></div></div></div><div id="42906822" class="c"><input type="checkbox" id="c-42906822" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906798">parent</a><span>|</span><a href="#42906829">prev</a><span>|</span><a href="#42906787">next</a><span>|</span><label class="collapse" for="c-42906822">[-]</label><label class="expand" for="c-42906822">[1 more]</label></div><br/><div class="children"><div class="content">Another trope that stands out is that someone will take a model, run a battery of tests against it and then make general statements about what LLMs can and cannot do without understanding their architecture, the training data, and the training itself.<p>And then they dress it up to sound scientific, when really they are making hasty generalizations to support a preconceived bias.</div><br/></div></div></div></div><div id="42906787" class="c"><input type="checkbox" id="c-42906787" checked=""/><div class="controls bullet"><span class="by">kubb</span><span>|</span><a href="#42906683">parent</a><span>|</span><a href="#42906798">prev</a><span>|</span><a href="#42906811">next</a><span>|</span><label class="collapse" for="c-42906787">[-]</label><label class="expand" for="c-42906787">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry but they don&#x27;t &quot;do progress that was anticipated impossible&quot;, especially not every two months.<p>They were predicted to end the software engineering profession for almost four years already. And it just doesn&#x27;t happen, even though they can bang out a perfect to-do list in React in a matter of seconds.<p>LLMs have incremental improvements on the quality of their responses as measured by benchmarks. The speed and cost of inference has also been improving. Despite that there was no major breakthrough since GPT 3.<p>People keep trying to make them reason, and keep failing at it.</div><br/><div id="42906878" class="c"><input type="checkbox" id="c-42906878" checked=""/><div class="controls bullet"><span class="by">throw310822</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906787">parent</a><span>|</span><a href="#42906811">next</a><span>|</span><label class="collapse" for="c-42906878">[-]</label><label class="expand" for="c-42906878">[3 more]</label></div><br/><div class="children"><div class="content">&gt; They were predicted to end the software engineering profession for almost four years already<p>ChatGPT was launched on November 30 2022. Two years and two months ago. The fact that in such a short timeframe you&#x27;re talking about missed predictions is absurd, but telling of the accelerated timeframe in which we&#x27;re living. The fact is that currently AI and LLMs are going through a phase of explosive improvement, to the point we can expect enormous improvements in capabilities every six months or so.</div><br/><div id="42906979" class="c"><input type="checkbox" id="c-42906979" checked=""/><div class="controls bullet"><span class="by">kubb</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906878">parent</a><span>|</span><a href="#42907240">next</a><span>|</span><label class="collapse" for="c-42906979">[-]</label><label class="expand" for="c-42906979">[1 more]</label></div><br/><div class="children"><div class="content">And what has enormously improved since ChatGPTs launch? Maybe you should ask it what it &quot;thinks&quot; about the hype surrounding it.</div><br/></div></div><div id="42907240" class="c"><input type="checkbox" id="c-42907240" checked=""/><div class="controls bullet"><span class="by">Capricorn2481</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906878">parent</a><span>|</span><a href="#42906979">prev</a><span>|</span><a href="#42906811">next</a><span>|</span><label class="collapse" for="c-42907240">[-]</label><label class="expand" for="c-42907240">[1 more]</label></div><br/><div class="children"><div class="content">&gt; to the point we can expect enormous improvements in capabilities every six months or so<p>Not really, we just can see we&#x27;ve had improvements. That is not evidence of upcoming improvement.</div><br/></div></div></div></div></div></div><div id="42906811" class="c"><input type="checkbox" id="c-42906811" checked=""/><div class="controls bullet"><span class="by">rbranson</span><span>|</span><a href="#42906683">parent</a><span>|</span><a href="#42906787">prev</a><span>|</span><a href="#42905745">next</a><span>|</span><label class="collapse" for="c-42906811">[-]</label><label class="expand" for="c-42906811">[2 more]</label></div><br/><div class="children"><div class="content">Did you read the article? Dziri and Peng are not the “skeptical AI community,” they are in fact die hard AI researchers. This is like saying people who run benchmarks to find performance problems in code are skeptics or haters.</div><br/><div id="42907008" class="c"><input type="checkbox" id="c-42907008" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42906683">root</a><span>|</span><a href="#42906811">parent</a><span>|</span><a href="#42905745">next</a><span>|</span><label class="collapse" for="c-42907008">[-]</label><label class="expand" for="c-42907008">[1 more]</label></div><br/><div class="children"><div class="content">I read the article: it does not look like very good research: It&#x27;s simple to find flaws in LLMs reasoning &#x2F; compositional capabilities looking at problems that are at the limit of what they can do now, or just picking problems that are very far from their computational model, or submitting riddles. But there is no good analysis of the limitations, nor inspection of how&#x2F;how much better recently LLMs got exactly at this kind of problems. Also the article is full of uninformative and obvious things to show how LLMs fail in stupid tasks such as multiplication between large numbers.<p>But the most absurd thing is to take a tiny transformer, provide a proof that it cannot do a certain thing, and hint at the potential idea that the proof may apply to larger ones, in a discipline where we know that size is everything, and while large LLMs show more and more to be able to solve similar problems.</div><br/></div></div></div></div></div></div><div id="42905745" class="c"><input type="checkbox" id="c-42905745" checked=""/><div class="controls bullet"><span class="by">klodolph</span><span>|</span><a href="#42906683">prev</a><span>|</span><a href="#42906402">next</a><span>|</span><label class="collapse" for="c-42905745">[-]</label><label class="expand" for="c-42905745">[21 more]</label></div><br/><div class="children"><div class="content">When one of these limitations gets spelled out in an article, it feels like six months later, somebody has a demo of a chatbot without that particular limitation.<p>These limitations don’t seem in any way “fundamental” to me. I’m sure there are a ton of people gluing LLMs to SAT solvers as we speak.</div><br/><div id="42905910" class="c"><input type="checkbox" id="c-42905910" checked=""/><div class="controls bullet"><span class="by">chefandy</span><span>|</span><a href="#42905745">parent</a><span>|</span><a href="#42905779">next</a><span>|</span><label class="collapse" for="c-42905910">[-]</label><label class="expand" for="c-42905910">[17 more]</label></div><br/><div class="children"><div class="content">Could you give an example of something we recently solved that was considered an unsolvable problem six months beforehand? I don’t have any specific examples, but it seems like most of the huge breakthrough discoveries I’ve seen announced end up being overstated and for practical usage, our choice of LLM-driven tools is only marginally better than they were a couple of years ago. It seems like the preponderance of practical advancement in recent times has come from the tooling&#x2F;interface improvements rather than generating miracles from the models themselves. But it could be that I just don’t have the right use cases.</div><br/><div id="42905984" class="c"><input type="checkbox" id="c-42905984" checked=""/><div class="controls bullet"><span class="by">munchler</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42905910">parent</a><span>|</span><a href="#42906007">next</a><span>|</span><label class="collapse" for="c-42905984">[-]</label><label class="expand" for="c-42905984">[13 more]</label></div><br/><div class="children"><div class="content">Take a look at the ARC Prize, which is a test for achieving &quot;AGI&quot; created in 2019 by François Chollet. Scroll down halfway on the home page and ponder the steep yellow line on the graph. That&#x27;s what OpenAI o3 recently achieved.<p>[0] <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;</a><p>[1] <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;oai-o3-pub-breakthrough" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;oai-o3-pub-breakthrough</a></div><br/><div id="42906130" class="c"><input type="checkbox" id="c-42906130" checked=""/><div class="controls bullet"><span class="by">mrshadowgoose</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42905984">parent</a><span>|</span><a href="#42906190">next</a><span>|</span><label class="collapse" for="c-42906130">[-]</label><label class="expand" for="c-42906130">[3 more]</label></div><br/><div class="children"><div class="content">Reviewing the actual problems is highly recommended: <a href="https:&#x2F;&#x2F;kts.github.io&#x2F;arc-viewer&#x2F;" rel="nofollow">https:&#x2F;&#x2F;kts.github.io&#x2F;arc-viewer&#x2F;</a><p>They&#x27;re not particularly difficult, but clearly require reasoning to solve.</div><br/><div id="42906294" class="c"><input type="checkbox" id="c-42906294" checked=""/><div class="controls bullet"><span class="by">UnlockedSecrets</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42906130">parent</a><span>|</span><a href="#42906190">next</a><span>|</span><label class="collapse" for="c-42906294">[-]</label><label class="expand" for="c-42906294">[2 more]</label></div><br/><div class="children"><div class="content">unless you train directly against solving those problems... in which case how could you theoretically design a test that could stand against training directly against the answer sheet?</div><br/><div id="42906322" class="c"><input type="checkbox" id="c-42906322" checked=""/><div class="controls bullet"><span class="by">munchler</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42906294">parent</a><span>|</span><a href="#42906190">next</a><span>|</span><label class="collapse" for="c-42906322">[-]</label><label class="expand" for="c-42906322">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why they keep the evaluation set private: &quot;Submit a solution which scores 85% on the ARC-AGI private evaluation set and win $600K.&quot;<p>[0] <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;guide" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;guide</a></div><br/></div></div></div></div></div></div><div id="42906190" class="c"><input type="checkbox" id="c-42906190" checked=""/><div class="controls bullet"><span class="by">EdwardDiego</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42905984">parent</a><span>|</span><a href="#42906130">prev</a><span>|</span><a href="#42906695">next</a><span>|</span><label class="collapse" for="c-42906190">[-]</label><label class="expand" for="c-42906190">[8 more]</label></div><br/><div class="children"><div class="content">So we&#x27;re only 12% from AGI?<p>I&#x27;m dubious tbh. Given we still can&#x27;t simulate a nematode.</div><br/><div id="42906248" class="c"><input type="checkbox" id="c-42906248" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42906190">parent</a><span>|</span><a href="#42907227">next</a><span>|</span><label class="collapse" for="c-42906248">[-]</label><label class="expand" for="c-42906248">[3 more]</label></div><br/><div class="children"><div class="content">ARC creator François Chollet says: <a href="https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;fchollet.bsky.social&#x2F;post&#x2F;3les3izgdj22j" rel="nofollow">https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;fchollet.bsky.social&#x2F;post&#x2F;3les3izgd...</a><p><i>I don&#x27;t think people really appreciate how simple ARC-AGI-1 was, and what solving it really means.</i><p><i>It was designed as the simplest, most basic assessment of fluid intelligence possible. Failure to pass signifies a near-total inability to adapt or problem-solve in unfamiliar situations.</i><p><i>Passing it means your system exhibits non-zero fluid intelligence -- you&#x27;re finally looking at something that isn&#x27;t pure memorized skill. But it says rather little about how intelligent your system is, or how close to human intelligence it is.</i></div><br/><div id="42906962" class="c"><input type="checkbox" id="c-42906962" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42906248">parent</a><span>|</span><a href="#42907227">next</a><span>|</span><label class="collapse" for="c-42906962">[-]</label><label class="expand" for="c-42906962">[2 more]</label></div><br/><div class="children"><div class="content">&gt; designed as the simplest, most basic assessment of fluid intelligence possible.<p>This was the goal, but that doesn&#x27;t say what the test itself is. Try to get a human to solve this problem without their visual cortex, they couldn&#x27;t do it.  Stating your goal for a thing, doesn&#x27;t make the thing that goal.<p>AI researchers designing intelligence tests are like programmers designing their own cryptography.<p>How about we have people skilled in neuropsychology, psychometrics and cognitive psychology do what they are good at.</div><br/><div id="42907058" class="c"><input type="checkbox" id="c-42907058" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42906962">parent</a><span>|</span><a href="#42907227">next</a><span>|</span><label class="collapse" for="c-42907058">[-]</label><label class="expand" for="c-42907058">[1 more]</label></div><br/><div class="children"><div class="content">&gt; How about we have people skilled in neuropsychology, psychometrics and cognitive psychology do what they are good at.<p>Disagree. The thing that we will eventually call AGI will not be human. No need to have human-specific evaluations unless you’re aiming for an artificial human and not just an artificial intelligence.</div><br/></div></div></div></div></div></div><div id="42906206" class="c"><input type="checkbox" id="c-42906206" checked=""/><div class="controls bullet"><span class="by">munchler</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42906190">parent</a><span>|</span><a href="#42907227">prev</a><span>|</span><a href="#42906490">next</a><span>|</span><label class="collapse" for="c-42906206">[-]</label><label class="expand" for="c-42906206">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why I put &quot;AGI&quot; in quotes. The point is that six months ago, no one expected an LLM to score this well.</div><br/><div id="42906274" class="c"><input type="checkbox" id="c-42906274" checked=""/><div class="controls bullet"><span class="by">EdwardDiego</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42906206">parent</a><span>|</span><a href="#42906490">next</a><span>|</span><label class="collapse" for="c-42906274">[-]</label><label class="expand" for="c-42906274">[1 more]</label></div><br/><div class="children"><div class="content">Fair enough.</div><br/></div></div></div></div><div id="42906490" class="c"><input type="checkbox" id="c-42906490" checked=""/><div class="controls bullet"><span class="by">wslh</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42906190">parent</a><span>|</span><a href="#42906206">prev</a><span>|</span><a href="#42906695">next</a><span>|</span><label class="collapse" for="c-42906490">[-]</label><label class="expand" for="c-42906490">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the 12% impact is significant, especially in a societal context, because it represents a shift in how people access and process information. Even without AGI, the comparison between LLMs and search engines is crucial. LLMs provide synthesized, conversational responses rather than just indexing and ranking web pages. This shift reduces the need for users to evaluate multiple sources manually, which has far-reaching implications.</div><br/></div></div></div></div><div id="42906695" class="c"><input type="checkbox" id="c-42906695" checked=""/><div class="controls bullet"><span class="by">0x008</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42905984">parent</a><span>|</span><a href="#42906190">prev</a><span>|</span><a href="#42906007">next</a><span>|</span><label class="collapse" for="c-42906695">[-]</label><label class="expand" for="c-42906695">[1 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI&#x27;s new o3 system - trained on the ARC-AGI-1 Public Training set…<p>Sounds fishy to me</div><br/></div></div></div></div><div id="42906007" class="c"><input type="checkbox" id="c-42906007" checked=""/><div class="controls bullet"><span class="by">liamwire</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42905910">parent</a><span>|</span><a href="#42905984">prev</a><span>|</span><a href="#42905972">next</a><span>|</span><label class="collapse" for="c-42906007">[-]</label><label class="expand" for="c-42906007">[1 more]</label></div><br/><div class="children"><div class="content">Not quite what you asked for, but it seems tangentially related and you might find it interesting: <a href="https:&#x2F;&#x2F;r0bk.github.io&#x2F;killedbyllm&#x2F;" rel="nofollow">https:&#x2F;&#x2F;r0bk.github.io&#x2F;killedbyllm&#x2F;</a></div><br/></div></div><div id="42905972" class="c"><input type="checkbox" id="c-42905972" checked=""/><div class="controls bullet"><span class="by">gallerdude</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42905910">parent</a><span>|</span><a href="#42906007">prev</a><span>|</span><a href="#42905939">next</a><span>|</span><label class="collapse" for="c-42905972">[-]</label><label class="expand" for="c-42905972">[1 more]</label></div><br/><div class="children"><div class="content">Completely disagree… there are a crazy amount of cases that didn’t work, until the models scaled to a point they magically did.<p>Best example I can think of is the ARC AGI benchmark. It was seen to measure human-like intelligence through special symmetries and abstract patterns.<p>From GPT-2 to GPT-4 there was basically had no progress, then o1 got about 20%. Now o3 has basically solved the benchmark.</div><br/></div></div><div id="42905939" class="c"><input type="checkbox" id="c-42905939" checked=""/><div class="controls bullet"><span class="by">intelkishan</span><span>|</span><a href="#42905745">root</a><span>|</span><a href="#42905910">parent</a><span>|</span><a href="#42905972">prev</a><span>|</span><a href="#42905779">next</a><span>|</span><label class="collapse" for="c-42905939">[-]</label><label class="expand" for="c-42905939">[1 more]</label></div><br/><div class="children"><div class="content">Performance of OpenAI o3 in the ARC-AGI challenge fits the bill, however the model is not released publicly.</div><br/></div></div></div></div><div id="42905779" class="c"><input type="checkbox" id="c-42905779" checked=""/><div class="controls bullet"><span class="by">wslh</span><span>|</span><a href="#42905745">parent</a><span>|</span><a href="#42905910">prev</a><span>|</span><a href="#42906679">next</a><span>|</span><label class="collapse" for="c-42905779">[-]</label><label class="expand" for="c-42905779">[1 more]</label></div><br/><div class="children"><div class="content">SMT solvers really.</div><br/></div></div><div id="42906679" class="c"><input type="checkbox" id="c-42906679" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#42905745">parent</a><span>|</span><a href="#42905779">prev</a><span>|</span><a href="#42905909">next</a><span>|</span><label class="collapse" for="c-42906679">[-]</label><label class="expand" for="c-42906679">[1 more]</label></div><br/><div class="children"><div class="content">Came here to say the same thing. Bet o3 and Claude 3.5 Opus will crush this task by the end of 2025.</div><br/></div></div><div id="42905909" class="c"><input type="checkbox" id="c-42905909" checked=""/><div class="controls bullet"><span class="by">xigency</span><span>|</span><a href="#42905745">parent</a><span>|</span><a href="#42906679">prev</a><span>|</span><a href="#42906402">next</a><span>|</span><label class="collapse" for="c-42905909">[-]</label><label class="expand" for="c-42905909">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been slacking but yeah it&#x27;s on my list.</div><br/></div></div></div></div><div id="42906402" class="c"><input type="checkbox" id="c-42906402" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#42905745">prev</a><span>|</span><a href="#42906024">next</a><span>|</span><label class="collapse" for="c-42906402">[-]</label><label class="expand" for="c-42906402">[2 more]</label></div><br/><div class="children"><div class="content">One must now ask whether research results are analyzing pure LLMs (eg. gpt-series) or LLM synthesis engines (eg. o-series, r-series). In this case, the headline is summarizing a paper originally published in 2023 and does not necessarily have bearing on new synthesis engines. In fact, evidence strongly suggests the opposite given o3&#x27;s significant performance on ARC-AGI-1 which requires on-the-fly composition capability.</div><br/><div id="42906488" class="c"><input type="checkbox" id="c-42906488" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#42906402">parent</a><span>|</span><a href="#42906024">next</a><span>|</span><label class="collapse" for="c-42906488">[-]</label><label class="expand" for="c-42906488">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s Quanta being misleading. They mention several papers but end up with this [1] which talks about decoder-only transformers, not LLMs in general, chatbots, or LLM synthesis engines, whatever that means. The paper also proves that CoT-like planning lets you squeeze more computation from a transformer, which is... obvious? but formally proven this time. Models trained to do CoT don&#x27;t have some magical on-the-fly compositional ability, they just invest more computation (could be dozens millions of tokens in case of o3 solving the tasks from that benchmark).<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.02975" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.02975</a></div><br/></div></div></div></div><div id="42906024" class="c"><input type="checkbox" id="c-42906024" checked=""/><div class="controls bullet"><span class="by">WillAdams</span><span>|</span><a href="#42906402">prev</a><span>|</span><a href="#42905748">next</a><span>|</span><label class="collapse" for="c-42906024">[-]</label><label class="expand" for="c-42906024">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps applying:<p><a href="https:&#x2F;&#x2F;www.doc.ic.ac.uk&#x2F;~re14&#x2F;Evans-R-2020-PhD-Thesis.pdf" rel="nofollow">https:&#x2F;&#x2F;www.doc.ic.ac.uk&#x2F;~re14&#x2F;Evans-R-2020-PhD-Thesis.pdf</a><p>&quot;Kant’s Cognitive Architecture&quot;<p>will help?<p>&gt; ...we provide a precise formalization of what it means<p>&gt; to “make sense” of a sensory sequence. According to our definition, making sense means constructing<p>&gt; a symbolic causal theory that explains the sensory sequence and satisfies a set of unity conditions<p>&gt; that were inspired by Kant’s discussion in the first half of the Critique of Pure Reason. According to<p>&gt; our interpretation, making sense of sensory input is a type of program synthesis, but it is unsupervised<p>&gt; program synthesis.</div><br/></div></div><div id="42905748" class="c"><input type="checkbox" id="c-42905748" checked=""/><div class="controls bullet"><span class="by">changoplatanero</span><span>|</span><a href="#42906024">prev</a><span>|</span><a href="#42907068">next</a><span>|</span><label class="collapse" for="c-42905748">[-]</label><label class="expand" for="c-42905748">[13 more]</label></div><br/><div class="children"><div class="content">By the time these academic studies get published they are usually already several months out of date. o3-mini was released yesterday and if one wants to know about the limitations of current technology they are much better to check twitter than some research paper</div><br/><div id="42905938" class="c"><input type="checkbox" id="c-42905938" checked=""/><div class="controls bullet"><span class="by">FuckButtons</span><span>|</span><a href="#42905748">parent</a><span>|</span><a href="#42906470">next</a><span>|</span><label class="collapse" for="c-42905938">[-]</label><label class="expand" for="c-42905938">[1 more]</label></div><br/><div class="children"><div class="content">I think the breathless hype train of twitter is probably the worst place to get an actually grounded take on what the real world implications of the technology is.<p>Seeing the 100th example of an llm generating some toy code for which there are a vast number of examples of approximately similar things in the training corpus doesn’t give you a clearer view of what is or isn’t possible.</div><br/></div></div><div id="42906470" class="c"><input type="checkbox" id="c-42906470" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#42905748">parent</a><span>|</span><a href="#42905938">prev</a><span>|</span><a href="#42905927">next</a><span>|</span><label class="collapse" for="c-42906470">[-]</label><label class="expand" for="c-42906470">[1 more]</label></div><br/><div class="children"><div class="content">Or neither. Try it yourself.<p>For me, LLMs still don’t meet basic usefulness and are a net negative when I try to use them. I push code daily for my job.</div><br/></div></div><div id="42905927" class="c"><input type="checkbox" id="c-42905927" checked=""/><div class="controls bullet"><span class="by">Xmd5a</span><span>|</span><a href="#42905748">parent</a><span>|</span><a href="#42906470">prev</a><span>|</span><a href="#42906010">next</a><span>|</span><label class="collapse" for="c-42905927">[-]</label><label class="expand" for="c-42905927">[1 more]</label></div><br/><div class="children"><div class="content">The paper is recent and being discussed here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42889786">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42889786</a></div><br/></div></div><div id="42906010" class="c"><input type="checkbox" id="c-42906010" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42905748">parent</a><span>|</span><a href="#42905927">prev</a><span>|</span><a href="#42907068">next</a><span>|</span><label class="collapse" for="c-42906010">[-]</label><label class="expand" for="c-42906010">[9 more]</label></div><br/><div class="children"><div class="content">It fundamentally does not matter. Matrix multiplication does not erase the truth of Godel and Turing.</div><br/><div id="42906242" class="c"><input type="checkbox" id="c-42906242" checked=""/><div class="controls bullet"><span class="by">kadoban</span><span>|</span><a href="#42905748">root</a><span>|</span><a href="#42906010">parent</a><span>|</span><a href="#42906241">next</a><span>|</span><label class="collapse" for="c-42906242">[-]</label><label class="expand" for="c-42906242">[6 more]</label></div><br/><div class="children"><div class="content">Godel and Turing just proved that there are some true things that can&#x27;t be proved, and things that cannot be computed. They didn&#x27;t show where those boundaries are.<p>They certainly didn&#x27;t show those boundaries to be below human cognition level.</div><br/><div id="42906284" class="c"><input type="checkbox" id="c-42906284" checked=""/><div class="controls bullet"><span class="by">andrewflnr</span><span>|</span><a href="#42905748">root</a><span>|</span><a href="#42906242">parent</a><span>|</span><a href="#42906360">next</a><span>|</span><label class="collapse" for="c-42906284">[-]</label><label class="expand" for="c-42906284">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the grounded take is that Turing and Gödel apply just as much to human intelligence. If not, someone please go ahead and use this to physically prove the existence of an immortal, hypercomputational soul.</div><br/></div></div><div id="42906360" class="c"><input type="checkbox" id="c-42906360" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42905748">root</a><span>|</span><a href="#42906242">parent</a><span>|</span><a href="#42906284">prev</a><span>|</span><a href="#42906241">next</a><span>|</span><label class="collapse" for="c-42906360">[-]</label><label class="expand" for="c-42906360">[4 more]</label></div><br/><div class="children"><div class="content">Godel proved that there are unprovable statements. Turing showed that certain classes of problems can only be solved by machines with infinite tapes. This no bounded LLM can possibly solve every turing complete problem. Only theoretically infinite chain of thought can possibly get us that power.<p>Godel then tells us that, if we have such a system, there are things where this system may get stuck.<p>Indeed this is what we see in chain of thought models. If you give them an impossible problem they either give up or produce a seemingly infinite series of tokens before emitting the &lt;&#x2F;think&gt; tag.<p>Turing tells us that examining any set of matrices modeling a finite state machine over an infinite token stream is the halting problem.</div><br/><div id="42906423" class="c"><input type="checkbox" id="c-42906423" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#42905748">root</a><span>|</span><a href="#42906360">parent</a><span>|</span><a href="#42906241">next</a><span>|</span><label class="collapse" for="c-42906423">[-]</label><label class="expand" for="c-42906423">[3 more]</label></div><br/><div class="children"><div class="content">Theoretical computability is of dubious practical relevance.<p>Consider two problems:<p>Problem A is not computable<p>Problem B is computable in principle, but, even for trivially sized inputs, the best possible algorithm requires time and&#x2F;or space we’ll never have in practice, orders of magnitude too large for our physical universe<p>From a theoretical computer science perspective, there is a huge difference between A and B. From a practical perspective, there is none whatsoever.<p>The real question is “can AIs do anything humans can do?” And appealing to what Turing machines can or can’t do is irrelevant, because there are a literally infinite number of problems which a Turing machine can solve, but no human nor AI ever could</div><br/><div id="42906802" class="c"><input type="checkbox" id="c-42906802" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42905748">root</a><span>|</span><a href="#42906423">parent</a><span>|</span><a href="#42906241">next</a><span>|</span><label class="collapse" for="c-42906802">[-]</label><label class="expand" for="c-42906802">[2 more]</label></div><br/><div class="children"><div class="content">So the article is about what humans v LLMs can do, except in the article, LLM is taken to mean just a single output auto regressive model (no chain of thought). Since an LLM has a constant number of steps at each token generation, no it cannot do everything a human can. Humans can choose when to think and can ponder the next action interminably. That&#x27;s my point. When we force LLMs to commit to a particular answer by forcing an output at each token generation, the class of problems they can solve is trivially less than the equivalent human.</div><br/><div id="42907065" class="c"><input type="checkbox" id="c-42907065" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#42905748">root</a><span>|</span><a href="#42906802">parent</a><span>|</span><a href="#42906241">next</a><span>|</span><label class="collapse" for="c-42907065">[-]</label><label class="expand" for="c-42907065">[1 more]</label></div><br/><div class="children"><div class="content">I agree that a raw autoregressive LLM model with just a single output is almost necessarily less capable than humans. Not only can we ponder (chain of thought style), we also have various means available to us to check our work – e.g. for a coding problem, we can write the code, see if it compiles and runs and passes our tests, and if it doesn&#x27;t, we can look at the error messages, add debugging, try some changes, and do that iteratively until we hopefully reach a solution–or else we give up – which the constraint &quot;single output&quot; denies.<p>I don&#x27;t think anyone is actually expecting &quot;AGI&quot; to be achieved by a model labouring under such extreme limitations as a single output autoregressive LLM is. If instead we are talking about an AI agent with not just chain of thought, but also function calling to invoke various tools (including to write and run code), the ability to store and retrieve information with a RAG, etc – well, current versions of that aren&#x27;t &quot;AGI&quot; either, but it seems much more plausible that they might eventually evolve into it.<p>I don&#x27;t think we need to invoke Turing or Gödel in order to make the point I just made, and I think doing so is more distracting with irrelevancies than actually enlightening.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42906241" class="c"><input type="checkbox" id="c-42906241" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#42905748">root</a><span>|</span><a href="#42906010">parent</a><span>|</span><a href="#42906242">prev</a><span>|</span><a href="#42907068">next</a><span>|</span><label class="collapse" for="c-42906241">[-]</label><label class="expand" for="c-42906241">[2 more]</label></div><br/><div class="children"><div class="content">Who is trying to “erase the truth of Gödel and Turing”? (Well, some cranks are, but I don’t think that’s who you are talking about.)<p>Gödel and Turing’s results do not appear to give any reason that a computer program can’t do what a person can do.</div><br/><div id="42906344" class="c"><input type="checkbox" id="c-42906344" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42905748">root</a><span>|</span><a href="#42906241">parent</a><span>|</span><a href="#42907068">next</a><span>|</span><label class="collapse" for="c-42906344">[-]</label><label class="expand" for="c-42906344">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not the point.  Computer program with a finite number of steps (an auto regressive LLM without chain of thought) has a limit in what it can reason in one step. This article does a lot of wordcelling to show this obvious point.</div><br/></div></div></div></div></div></div></div></div><div id="42906217" class="c"><input type="checkbox" id="c-42906217" checked=""/><div class="controls bullet"><span class="by">mohsen1</span><span>|</span><a href="#42907068">prev</a><span>|</span><a href="#42906686">next</a><span>|</span><label class="collapse" for="c-42906217">[-]</label><label class="expand" for="c-42906217">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;679f0353-12bc-8007-91cf-dd63d52044a0" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;679f0353-12bc-8007-91cf-dd63d52044...</a><p>O1 Pro gets the answer with proper reasoning<p>Can’t tell if this is dude to data contamination or it really figured it out?<p>How can we form the question in another way to avoid data contamination?</div><br/><div id="42906324" class="c"><input type="checkbox" id="c-42906324" checked=""/><div class="controls bullet"><span class="by">mohsen1</span><span>|</span><a href="#42906217">parent</a><span>|</span><a href="#42906381">next</a><span>|</span><label class="collapse" for="c-42906324">[-]</label><label class="expand" for="c-42906324">[1 more]</label></div><br/><div class="children"><div class="content">With modified prompt to make less look like the original prompt it thought 6x more:<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;679f086d-a758-8007-b240-38e684303786" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;679f086d-a758-8007-b240-38e6843037...</a></div><br/></div></div></div></div><div id="42906686" class="c"><input type="checkbox" id="c-42906686" checked=""/><div class="controls bullet"><span class="by">comeonbro</span><span>|</span><a href="#42906217">prev</a><span>|</span><a href="#42905736">next</a><span>|</span><label class="collapse" for="c-42906686">[-]</label><label class="expand" for="c-42906686">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Chatbot Software Begins to Face Fundamental Limitations<p>&gt; Recent results show that large language models struggle with compositional tasks, suggesting a hard limit to their abilities.<p>Your first question with anything like this should always be <i>WHICH MODELS</i>:<p>&gt; For our experiments, we evaluate the performance of 6 LLMs: GPT4 (gpt-4) [58], ChatGPT
(GPT3.5-turbo) [57], GPT3 (text-davinci-003) [11], FlanT5 [17] and LLaMa [75].<p>This is <i>ancient</i>.  This research was done <i>centuries</i> ago.  This is research about the possibility of isotopes, written about radium in 1903, published in 1946.  It is a criminal level of journalistic malpractice to leave uninformed readers with the impression that this is where AI stands <i>yesterday</i>.</div><br/></div></div><div id="42905736" class="c"><input type="checkbox" id="c-42905736" checked=""/><div class="controls bullet"><span class="by">d0mine</span><span>|</span><a href="#42906686">prev</a><span>|</span><a href="#42905915">next</a><span>|</span><label class="collapse" for="c-42905736">[-]</label><label class="expand" for="c-42905736">[6 more]</label></div><br/><div class="children"><div class="content">&gt; multilayer transformers indeed cannot solve certain complicated compositional tasks<p>&gt; chain-of-thought prompting essentially turns a large problem into a sequence of smaller problems, making it possible for transformers to tackle more complex compositional tasks<p>--- [out of order]<p>&gt; the model could be trained on 20-digit numbers and still reliably (with 98% accuracy) add 100-digit numbers, whereas a model trained without the extra positional embedding was only about 3% accurate</div><br/><div id="42905882" class="c"><input type="checkbox" id="c-42905882" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#42905736">parent</a><span>|</span><a href="#42905915">next</a><span>|</span><label class="collapse" for="c-42905882">[-]</label><label class="expand" for="c-42905882">[5 more]</label></div><br/><div class="children"><div class="content">98% seems pretty damn unreliable to me given that a computer ought to be able to do that addition with probably about 12 nines instead of 1 without really putting much effort into reliability.</div><br/><div id="42905989" class="c"><input type="checkbox" id="c-42905989" checked=""/><div class="controls bullet"><span class="by">bruce511</span><span>|</span><a href="#42905736">root</a><span>|</span><a href="#42905882">parent</a><span>|</span><a href="#42905918">next</a><span>|</span><label class="collapse" for="c-42905989">[-]</label><label class="expand" for="c-42905989">[2 more]</label></div><br/><div class="children"><div class="content">This speaks to the psychology of it all.<p>Over my life I&#x27;ve been &quot;trained&quot; that humans are unreliable and subject-to-change while computers are deterministic and will give the same output every time (given the same input.)<p>LLMs though are very human. Ask it the same question and you&#x27;ll get different answers. Sometimes grammatically different, sometimes factually different.<p>This is partly why hallucinations freaked people out. I expect a correct, and deterministic answer. And it just made stuff up. Then it made up URLs to validate the answer.<p>In other words you have computers behaving like humans. Which was indeed the whole point of them to begin with.<p>But psychologically this is disturbing.</div><br/><div id="42907193" class="c"><input type="checkbox" id="c-42907193" checked=""/><div class="controls bullet"><span class="by">suddenlybananas</span><span>|</span><a href="#42905736">root</a><span>|</span><a href="#42905989">parent</a><span>|</span><a href="#42905918">next</a><span>|</span><label class="collapse" for="c-42907193">[-]</label><label class="expand" for="c-42907193">[1 more]</label></div><br/><div class="children"><div class="content">If you genuinely believe that LLM hallucinations are like human behaviour, you must not interact a lot with humans.</div><br/></div></div></div></div><div id="42905918" class="c"><input type="checkbox" id="c-42905918" checked=""/><div class="controls bullet"><span class="by">szvsw</span><span>|</span><a href="#42905736">root</a><span>|</span><a href="#42905882">parent</a><span>|</span><a href="#42905989">prev</a><span>|</span><a href="#42905915">next</a><span>|</span><label class="collapse" for="c-42905918">[-]</label><label class="expand" for="c-42905918">[2 more]</label></div><br/><div class="children"><div class="content">Can you add 100 digit numbers reliably in a short amount of time over a large sample size? How about without a piece of paper and a pen? Can you even remember a 100 digit number? It’s likely quite difficult…<p>This is not about comparing to reliability “of a computer” but rather to a process which responds to a wide array of natural language queries (both human processes an algorithmic processes)… and obviously the reliability goes up once tool use is included.</div><br/><div id="42906776" class="c"><input type="checkbox" id="c-42906776" checked=""/><div class="controls bullet"><span class="by">achierius</span><span>|</span><a href="#42905736">root</a><span>|</span><a href="#42905918">parent</a><span>|</span><a href="#42905915">next</a><span>|</span><label class="collapse" for="c-42906776">[-]</label><label class="expand" for="c-42906776">[1 more]</label></div><br/><div class="children"><div class="content">With pen and paper? Yes, definitely. Without? LLMs have scratch space, so they&#x27;re not operating without either.</div><br/></div></div></div></div></div></div></div></div><div id="42905915" class="c"><input type="checkbox" id="c-42905915" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905736">prev</a><span>|</span><a href="#42906040">next</a><span>|</span><label class="collapse" for="c-42905915">[-]</label><label class="expand" for="c-42905915">[18 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s &quot;Einstein’s puzzle&quot; from this paper: <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;341189675_Is_Einstein&#x27;s_Puzzle_Over-Specified" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;341189675_Is_Einste...</a><p><pre><code>  H01 There are five houses.
  H02 The Englishman lives in the red house.
  H03 The Spaniard owns the dog.
  H04 Coffee is drunk in the green house.
  H05 The Ukrainian drinks tea.
  H06 The green house is immediately to the right of the ivory house.
  H07 The Old Gold smoker owns snails.
  H08 Kools are smoked in the yellow house.
  H09 Milk is drunk in the middle house.
  H10 The Norwegian lives in the first house.
  H11 The man who smokes Chesterfields lives in the house next to the man with the fox.
  H12 Kools are smoked in a house next to the house where the horse is kept.
  H13 The Lucky Strike smoker drinks orange juice.
  H14 The Japanese smokes Parliaments.
  H15 The Norwegian lives next to the blue house.
  Now,
  Q1 Who drinks water?
  Q2 Who owns the zebra?</code></pre></div><br/><div id="42905958" class="c"><input type="checkbox" id="c-42905958" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905915">parent</a><span>|</span><a href="#42905964">next</a><span>|</span><label class="collapse" for="c-42905958">[-]</label><label class="expand" for="c-42905958">[11 more]</label></div><br/><div class="children"><div class="content">I ran a few replacements because the puzzle is probably in the training data somewhere:<p><pre><code>  Englishman -&gt; Scotsman
  Spaniard -&gt; Welshman
  dog -&gt; parrot
  Coffee -&gt; Kombucha
  green -&gt; scarlet
  tea -&gt; butterscotch
  Ukrainian -&gt; Romanian
  ivory -&gt; pink
  snails -&gt; scorpions
  yellow -&gt; turquoise
  Milk -&gt; Red Bull
  Norwegian -&gt; Brazilian
  fox -&gt; bear
  horse -&gt; mule
  orange juice -&gt; rum
  Japanese -&gt; German
  blue -&gt; brown
  red -&gt; purple
</code></pre>
Here&#x27;s my new prompt after replacing those words:<p><pre><code>  H01 There are five huts.
  H02 The Scotsman lives in the purple hut.
  H03 The Welshman owns the parrot.
  H04 Kombucha is drunk in the scarlet hut.
  H05 The Romanian drinks butterscotch.
  H06 The scarlet hut is immediately to the right of the pink hut.
  H07 The Old Gold smoker owns scorpions.
  H08 Kools are smoked in the turquoise hut.
  H09 Red Bull is drunk in the middle hut.
  H10 The Brazilian lives in the first hut.
  H11 The man who smokes Chesterfields lives in the hut next to the man with the bear.
  H12 Kools are smoked in a hut next to the hut where the mule is kept.
  H13 The Lucky Strike smoker drinks rum.
  H14 The German smokes Parliaments.
  H15 The Brazilian lives next to the brown hut.
  Now,
  Q1 Who drinks water?
  Q2 Who owns the zebra?

</code></pre>
Then I ran the new prompt through o3-mini-high and it gave this answer (after a thought process that looks legit to me, like it actually did reason through the puzzle and not just spit out an answer it had seen):<p><pre><code>  Q1: Who drinks water?
  A: The Brazilian (in Hut 1) drinks water.

  Q2: Who owns the zebra?
  A: The German (in Hut 5) owns the zebra.
</code></pre>
Undoing my replacements, that&#x27;s the Norwegian drinking water and the Japanese person owning a zebra. Which I think is the solution from the original paper.<p>Here&#x27;s my o3-mini-high transcript: <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;679efae7-8544-8006-960b-a744cc1feac0" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;679efae7-8544-8006-960b-a744cc1fea...</a></div><br/><div id="42906006" class="c"><input type="checkbox" id="c-42906006" checked=""/><div class="controls bullet"><span class="by">mbernstein</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42905958">parent</a><span>|</span><a href="#42906549">next</a><span>|</span><label class="collapse" for="c-42906006">[-]</label><label class="expand" for="c-42906006">[3 more]</label></div><br/><div class="children"><div class="content">I just did a pass with some replacements with o1 and it very much still recognized it as the Einstein riddle and actually seems to have cheated a bit :)<p>&quot;Revisiting assumptions<p>Considering &quot;Camels&quot; might be a mistake for &quot;Kools,&quot; leading to confusion. This inconsistency complicates solving the puzzle, showing the need for careful brand assignment.&quot;<p>Tracking puzzle progress<p>I’m mapping out various house and nationality combinations, but the classic conclusion is the Norwegian drinks water and the Japanese owns the zebra.<p>Analyzing the arrangement<p>I’m working through the classic puzzle structure and noting variations, while consistently identifying the Norwegian drinking water and the Japanese owning the zebra as the final solution.&quot;</div><br/><div id="42906034" class="c"><input type="checkbox" id="c-42906034" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42906006">parent</a><span>|</span><a href="#42906549">next</a><span>|</span><label class="collapse" for="c-42906034">[-]</label><label class="expand" for="c-42906034">[2 more]</label></div><br/><div class="children"><div class="content">Hah, that&#x27;s fun. My o3-mini-high transcript didn&#x27;t hint that it recognized the puzzle and looked legit when I scanned through them, but I&#x27;m still very suspicious since this is evidently such a classic puzzle.<p>I should have changed the cigarette brands to something else too.</div><br/><div id="42906388" class="c"><input type="checkbox" id="c-42906388" checked=""/><div class="controls bullet"><span class="by">valenterry</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42906034">parent</a><span>|</span><a href="#42906549">next</a><span>|</span><label class="collapse" for="c-42906388">[-]</label><label class="expand" for="c-42906388">[1 more]</label></div><br/><div class="children"><div class="content">Try flipping the order, adding a few nonsense steps and combining 2 steps into one and also splitting a single step into two. And then see what happens and post it here. :-)</div><br/></div></div></div></div></div></div><div id="42906549" class="c"><input type="checkbox" id="c-42906549" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42905958">parent</a><span>|</span><a href="#42906006">prev</a><span>|</span><a href="#42906969">next</a><span>|</span><label class="collapse" for="c-42906549">[-]</label><label class="expand" for="c-42906549">[3 more]</label></div><br/><div class="children"><div class="content">I tried it against deepseek-r1-distill-llama-70b running on Groq (which is really fast) and it didn&#x27;t get the right answer: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;487c4c074cd6ad163dba061e1e594923" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;487c4c074cd6ad163dba061e1e594...</a><p>I ran it like this:<p><pre><code>  llm -m groq&#x2F;deepseek-r1-distill-llama-70b &#x27;
    H01 There are five huts.
    H02 The Scotsman lives in the purple hut.
    H03 The Welshman owns the parrot.
    H04 Kombucha is drunk in the scarlet hut.
    H05 The Romanian drinks butterscotch.
    H06 The scarlet hut is immediately to the right of the pink hut.
    H07 The Old Gold smoker owns scorpions.
    H08 Kools are smoked in the turquoise hut.
    H09 Red Bull is drunk in the middle hut.
    H10 The Brazilian lives in the first hut.
    H11 The man who smokes Chesterfields lives in the hut next to the man with the bear.
    H12 Kools are smoked in a hut next to the hut where the mule is kept.
    H13 The Lucky Strike smoker drinks rum.
    H14 The German smokes Parliaments.
    H15 The Brazilian lives next to the brown hut.
    Now,
    Q1 Who drinks water?
    Q2 Who owns the zebra?&#x27;
</code></pre>
Using this plugin: <a href="https:&#x2F;&#x2F;github.com&#x2F;angerman&#x2F;llm-groq">https:&#x2F;&#x2F;github.com&#x2F;angerman&#x2F;llm-groq</a></div><br/><div id="42906627" class="c"><input type="checkbox" id="c-42906627" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42906549">parent</a><span>|</span><a href="#42906969">next</a><span>|</span><label class="collapse" for="c-42906627">[-]</label><label class="expand" for="c-42906627">[2 more]</label></div><br/><div class="children"><div class="content">Full DeepSeek R1 - accessed through the DeepSeek API (their &quot;deepseek-reasoner&quot; model) - got the right answer: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;f77be3bbc720e1314235d42593562cc6" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;f77be3bbc720e1314235d42593562...</a></div><br/><div id="42907188" class="c"><input type="checkbox" id="c-42907188" checked=""/><div class="controls bullet"><span class="by">UncleEntity</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42906627">parent</a><span>|</span><a href="#42906969">next</a><span>|</span><label class="collapse" for="c-42907188">[-]</label><label class="expand" for="c-42907188">[1 more]</label></div><br/><div class="children"><div class="content">Whatever model is behind chat.deepseek.com got it in 348 seconds.<p>It amazes me they don&#x27;t time that thing out after, IDK, 5 minutes of computation time.</div><br/></div></div></div></div></div></div><div id="42906969" class="c"><input type="checkbox" id="c-42906969" checked=""/><div class="controls bullet"><span class="by">ta12653421</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42905958">parent</a><span>|</span><a href="#42906549">prev</a><span>|</span><a href="#42906066">next</a><span>|</span><label class="collapse" for="c-42906969">[-]</label><label class="expand" for="c-42906969">[1 more]</label></div><br/><div class="children"><div class="content">ClaudeAI responded:
&gt;&gt;&gt;
After working through all constraints:
Q1: Who drinks water?
The German drinks water.
Q2: Who owns the zebra?
The Scotsman owns the zebra.
&lt;&lt;&lt;</div><br/></div></div><div id="42906066" class="c"><input type="checkbox" id="c-42906066" checked=""/><div class="controls bullet"><span class="by">RossBencina</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42905958">parent</a><span>|</span><a href="#42906969">prev</a><span>|</span><a href="#42905964">next</a><span>|</span><label class="collapse" for="c-42906066">[-]</label><label class="expand" for="c-42906066">[3 more]</label></div><br/><div class="children"><div class="content">&gt; the puzzle is probably in the training data somewhere<p>Given that these models can perform translation I&#x27;m not sure why you think renaming things is sufficient to put your version out of distribution.</div><br/><div id="42906160" class="c"><input type="checkbox" id="c-42906160" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42906066">parent</a><span>|</span><a href="#42905964">next</a><span>|</span><label class="collapse" for="c-42906160">[-]</label><label class="expand" for="c-42906160">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t - but like I said, I reviewed the thought process in the transcript and it looked legit to me.<p>I&#x27;m not sure what else I could do here to be honest, without coming up with a completely new puzzle that captures the same kind of challenge as the original. I&#x27;m not nearly patient enough to do that!</div><br/><div id="42906368" class="c"><input type="checkbox" id="c-42906368" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42906160">parent</a><span>|</span><a href="#42905964">next</a><span>|</span><label class="collapse" for="c-42906368">[-]</label><label class="expand" for="c-42906368">[1 more]</label></div><br/><div class="children"><div class="content">The thought process is basically like asking it to think in French. Same training data LOL</div><br/></div></div></div></div></div></div></div></div><div id="42906033" class="c"><input type="checkbox" id="c-42906033" checked=""/><div class="controls bullet"><span class="by">lgats</span><span>|</span><a href="#42905915">parent</a><span>|</span><a href="#42905964">prev</a><span>|</span><a href="#42905945">next</a><span>|</span><label class="collapse" for="c-42906033">[-]</label><label class="expand" for="c-42906033">[1 more]</label></div><br/><div class="children"><div class="content">Interesting o3-mini produces a correct response:
<a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;679efd3f-8e88-800d-9aa8-e3fadccf7c66" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;679efd3f-8e88-800d-9aa8-e3fadccf7c...</a></div><br/></div></div><div id="42905945" class="c"><input type="checkbox" id="c-42905945" checked=""/><div class="controls bullet"><span class="by">gorgoiler</span><span>|</span><a href="#42905915">parent</a><span>|</span><a href="#42906033">prev</a><span>|</span><a href="#42905985">next</a><span>|</span><label class="collapse" for="c-42905945">[-]</label><label class="expand" for="c-42905945">[2 more]</label></div><br/><div class="children"><div class="content">I am reminded of this Reddit post from November last year, except this is fifth grade math homework:<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;mildlyinfuriating&#x2F;comments&#x2F;1gippqm&#x2F;fifth_grade_math_homework_has_kid_in_tears_and&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;mildlyinfuriating&#x2F;comments&#x2F;1gippqm&#x2F;...</a><p>The puzzle is a grid of simultaneous equations with 9 unknowns:<p><pre><code>  9 + ? - 6 = ? - 3
  -   =   -   =   x
  4 + ? x 5 - ? = 6
  &#x2F;   +   +   -   -
  8 - 5 = ? - ? x 2
  x   -   -   -   x
  ? + 1 + 7 - 4 = 8
  =   -   =   +   =
  7 = ? x 3 - ? - 2</code></pre></div><br/><div id="42907304" class="c"><input type="checkbox" id="c-42907304" checked=""/><div class="controls bullet"><span class="by">UncleEntity</span><span>|</span><a href="#42905915">root</a><span>|</span><a href="#42905945">parent</a><span>|</span><a href="#42905985">next</a><span>|</span><label class="collapse" for="c-42907304">[-]</label><label class="expand" for="c-42907304">[1 more]</label></div><br/><div class="children"><div class="content">Deepthink thought for 644 seconds and says the answer is 4.<p>Probably didn&#x27;t help that I just pasted it with no explanation, the spaces between the columns really threw it off.</div><br/></div></div></div></div><div id="42905985" class="c"><input type="checkbox" id="c-42905985" checked=""/><div class="controls bullet"><span class="by">dweekly</span><span>|</span><a href="#42905915">parent</a><span>|</span><a href="#42905945">prev</a><span>|</span><a href="#42905959">next</a><span>|</span><label class="collapse" for="c-42905985">[-]</label><label class="expand" for="c-42905985">[1 more]</label></div><br/><div class="children"><div class="content">(Not fair because it is in the training set, but o3-mini-high gives a thorough answer after 66s)<p>Below is a step‐by‐step reasoning that leads to the well‐known solution of the “Zebra Puzzle.”<p>Step 1. Number the Houses 1 to 5<p>We assume the houses are arranged in a row from left to right, with House 1 on the far left and House 5 on the far right.<p>Step 2. Assign the Easy Clues
 1. H10: “The Norwegian lives in the first house.”
→ House 1: Nationality = Norwegian.
 2. H15: “The Norwegian lives next to the blue house.”
House 1 has only one neighbor (House 2), so
→ House 2: Color = Blue.
 3. H09: “Milk is drunk in the middle house.”
→ House 3: Beverage = Milk.<p>Step 3. Determine the House Colors with the Ivory&#x2F;Green Pair
 • H06: “The green house is immediately to the right of the ivory house.”
There are two possible placements for this pair: either Houses (3,4) or Houses (4,5).
The option (3,4) would force House 3 to be ivory, but later clues will assign House 3 a color (red, see H02) for the Englishman.
So we take the pair as Houses 4 and 5:
 • House 4: Color = Ivory.
 • House 5: Color = Green.
 • H04: “Coffee is drunk in the green house.”
→ House 5: Beverage = Coffee.
 • The remaining colors for Houses 1–3 are now Red, Yellow, and (already used) Blue.
H02: “The Englishman lives in the red house.”
Since House 1 is taken by the Norwegian and House 2 is blue, the only possibility is:
→ House 3: Color = Red, Nationality = Englishman.
 • The only color left for House 1 is Yellow.
H08: “Kools are smoked in the yellow house.”
→ House 1: Color = Yellow, Cigarette = Kools.<p>Step 4. Assign the Remaining Nationalities and Beverages<p>The nationalities so far:
 • House 1: Norwegian
 • House 3: Englishman<p>Remaining nationalities: Ukrainian, Spaniard, Japanese.
 • H05: “The Ukrainian drinks tea.”
House 2 already is blue and unassigned for nationality and beverage, so:
→ House 2: Nationality = Ukrainian, Beverage = Tea.<p>Now for Houses 4 and 5, the remaining nationalities are Spaniard and Japanese.
 • H03: “The Spaniard owns the dog.”
And
 • H14: “The Japanese smokes Parliaments.”<p>We still have the beverages and cigarettes to assign. Notice that House 5 (green) already has its beverage (coffee). It is natural then to assign:
 • House 5: Nationality = Japanese, Cigarette = Parliaments.
This leaves:
 • House 4: Nationality = Spaniard, and by H03,
→ House 4: Pet = Dog.<p>Step 5. Use the Clues about Cigarettes and Pets<p>The remaining cigarette brands to assign (across Houses 2, 3, and 4) are: Old Gold, Chesterfields, and Lucky Strike.
 • H13: “The Lucky Strike smoker drinks orange juice.”
House 5 already drinks coffee; House 3 has milk; House 2 has tea; so the only house with an unassigned beverage is House 4.
→ House 4: Beverage = Orange juice, Cigarette = Lucky Strike.
 • Now, the remaining cigarettes for Houses 2 and 3 are: Old Gold and Chesterfields.
 • H07: “The Old Gold smoker owns snails.”
 • H11: “The man who smokes Chesterfields lives in the house next to the man with the fox.”
 • H12: “Kools are smoked in a house next to the house where the horse is kept.”
House 1 (which smokes Kools) has only one neighbor, House 2, so:
→ House 2: Pet = Horse.<p>Now, consider Houses 2 and 3 for the remaining cigarette assignments:
 • If House 2 were the Old Gold smoker, then by H07 it would have to own snails—but it already has the horse.
So, House 2 must smoke Chesterfields, and consequently,
→ House 2: Cigarette = Chesterfields.
 • This forces:
→ House 3: Cigarette = Old Gold, and by H07,
→ House 3: Pet = Snails.<p>Finally, H11: “The man who smokes Chesterfields lives in the house next to the man with the fox.”
House 2 (smoking Chesterfields) is adjacent to Houses 1 and 3.
House 3 already has snails, so
→ House 1: must have the fox.
→ House 1: Pet = Fox.<p>The only pet left is the zebra. The only house without a pet now is House 5, so:
→ House 5: Pet = Zebra.<p>Step 6. Assign the Final Beverage<p>At this point, the beverages are:
 • House 2: Tea
 • House 3: Milk
 • House 5: Coffee
 • House 4: Orange juice<p>The only beverage not yet used is water. The only house still without a beverage is House 1, so:
→ House 1: Beverage = Water.<p>Final House Assignments
 • House 1:
Color: Yellow
Nationality: Norwegian
Beverage: Water
Cigarette: Kools
Pet: Fox
 • House 2:
Color: Blue
Nationality: Ukrainian
Beverage: Tea
Cigarette: Chesterfields
Pet: Horse
 • House 3:
Color: Red
Nationality: Englishman
Beverage: Milk
Cigarette: Old Gold
Pet: Snails
 • House 4:
Color: Ivory
Nationality: Spaniard
Beverage: Orange juice
Cigarette: Lucky Strike
Pet: Dog
 • House 5:
Color: Green
Nationality: Japanese
Beverage: Coffee
Cigarette: Parliaments
Pet: Zebra<p>Cross Check Against the Clues
 • H02: Englishman in the red house → House 3 is red and English.
 • H03: Spaniard owns the dog → House 4 (Spaniard) has a dog.
 • H04: Coffee in the green house → House 5 (green) has coffee.
 • H05: Ukrainian drinks tea → House 2 (Ukrainian) has tea.
 • H06: Green house is immediately right of the ivory house → Houses 4 (ivory) and 5 (green).
 • H07: Old Gold smoker owns snails → House 3 (Old Gold) has snails.
 • H08: Kools in the yellow house → House 1 (yellow) smokes Kools.
 • H09: Milk in the middle house → House 3 has milk.
 • H10: Norwegian in House 1 → correct.
 • H11: Chesterfields smoker (House 2) lives next to fox owner (House 1).
 • H12: Kools (House 1) next to horse (House 2).
 • H13: Lucky Strike smoker (House 4) drinks orange juice.
 • H14: Japanese smokes Parliaments → House 5.
 • H15: Norwegian (House 1) lives next to blue house (House 2).<p>All clues are satisfied.<p>Final Answers
 • Q1: Who drinks water?
→ The Norwegian in House 1 drinks water.
 • Q2: Who owns the zebra?
→ The Japanese in House 5 owns the zebra.<p>Sources and Further Reading
 • For a classic discussion of this puzzle and its variations, see “Einstein’s Riddle” as presented in various logic puzzle collections and articles such as those found on Brainzilla or in books on lateral thinking puzzles.<p>This structured reasoning confirms the answers with a high degree of certainty.</div><br/></div></div></div></div><div id="42906040" class="c"><input type="checkbox" id="c-42906040" checked=""/><div class="controls bullet"><span class="by">apsec112</span><span>|</span><a href="#42905915">prev</a><span>|</span><a href="#42906243">next</a><span>|</span><label class="collapse" for="c-42906040">[-]</label><label class="expand" for="c-42906040">[4 more]</label></div><br/><div class="children"><div class="content">&quot;showing that multilayer transformers indeed cannot solve certain complicated compositional tasks. Basically, some compositional problems will always be beyond the ability of transformer-based LLMs.&quot;<p>Pretty sure this is just false and the paper doesn&#x27;t show this. I could be misunderstanding, but it looks like the result is only about a single token&#x2F;forward pass, not a reasoning model with many thousands of tokens like o1&#x2F;o3</div><br/><div id="42906065" class="c"><input type="checkbox" id="c-42906065" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42906040">parent</a><span>|</span><a href="#42906243">next</a><span>|</span><label class="collapse" for="c-42906065">[-]</label><label class="expand" for="c-42906065">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure that the statement &quot;some compositional problems will always be beyond the ability of transformer-based LLMs&quot; is even controversial to be honest.<p>There&#x27;s a reason all of the AI labs have been leaning hard into tool use and (more recently) inference-scaling compute (o1&#x2F;o3&#x2F;Gemini Thinking&#x2F;R1 etc) recently - those are just some of the techniques you can apply to move beyond the unsurprising limitations of purely guessing-the-next-token.</div><br/><div id="42906179" class="c"><input type="checkbox" id="c-42906179" checked=""/><div class="controls bullet"><span class="by">apsec112</span><span>|</span><a href="#42906040">root</a><span>|</span><a href="#42906065">parent</a><span>|</span><a href="#42906243">next</a><span>|</span><label class="collapse" for="c-42906179">[-]</label><label class="expand" for="c-42906179">[2 more]</label></div><br/><div class="children"><div class="content">o3 is still a transformer-based LLM, just one with a different loss function</div><br/><div id="42906518" class="c"><input type="checkbox" id="c-42906518" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42906040">root</a><span>|</span><a href="#42906179">parent</a><span>|</span><a href="#42906243">next</a><span>|</span><label class="collapse" for="c-42906518">[-]</label><label class="expand" for="c-42906518">[1 more]</label></div><br/><div class="children"><div class="content">Huh, yeah that&#x27;s a good point. The various distilled R1 models are definitely regular transformer-based LLMs because the GGUF file versions of them work without any upgrades to the underlying llama.cpp library.</div><br/></div></div></div></div></div></div></div></div><div id="42906243" class="c"><input type="checkbox" id="c-42906243" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42906040">prev</a><span>|</span><a href="#42906003">next</a><span>|</span><label class="collapse" for="c-42906243">[-]</label><label class="expand" for="c-42906243">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they’re not training them on the right thing, maybe math can be a different “mode” just lime sound or images</div><br/></div></div><div id="42906003" class="c"><input type="checkbox" id="c-42906003" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42906243">prev</a><span>|</span><a href="#42905819">next</a><span>|</span><label class="collapse" for="c-42906003">[-]</label><label class="expand" for="c-42906003">[1 more]</label></div><br/><div class="children"><div class="content">This is ultimately a basic adaptation of the pigeonhole principle and is not surprising. A finite system of matrix multiplications cannot be turing complete. You cannot expect one trip through a series of matrix multiplications and bias additions and a final sampling at the end which commits it to a certain answer to ever produce a correct answer. It&#x27;s a mathematical impossibility. No talk of quantum woo, emergent phenomenen, or whatever other pseudo-science has arisen to explain AI intelligence can get around this simple truth of mathematics.<p>However, chain of thought reasoning where token streams can continue ad infinitum could potentially solve large swaths of problems whose ordinary solutions require turing machines. It could also solve problems that cannot generally be solved by turing machines, but where you only need solutions for a few classes of problems.<p>Either way, even with chain of thought, you would expect that... in some instances, the model output diverges and does not complete. And unsurprisingly... this is exactly what you see with the DeepSeek models (and other CoT models) when you pose it difficult questions. It will never emit the &lt;&#x2F;think&gt; tag.</div><br/></div></div><div id="42905819" class="c"><input type="checkbox" id="c-42905819" checked=""/><div class="controls bullet"><span class="by">novemp</span><span>|</span><a href="#42906003">prev</a><span>|</span><label class="collapse" for="c-42905819">[-]</label><label class="expand" for="c-42905819">[1 more]</label></div><br/><div class="children"><div class="content">It feels like every few weeks someone puts out a new article breathlessly talking about how bad the fancy autocorrect bot is at math and logic like it&#x27;s brand new information. How have we as a society not gotten this through our skulls yet?</div><br/></div></div></div></div></div></div></div></body></html>