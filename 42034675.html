<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730710881540" as="style"/><link rel="stylesheet" href="styles.css?v=1730710881540"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/karthink/gptel">gptel: a simple LLM client for Emacs</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>michaelsbradley</span> | <span>24 comments</span></div><br/><div><div id="42035930" class="c"><input type="checkbox" id="c-42035930" checked=""/><div class="controls bullet"><span class="by">sourcepluck</span><span>|</span><a href="#42037363">next</a><span>|</span><label class="collapse" for="c-42035930">[-]</label><label class="expand" for="c-42035930">[2 more]</label></div><br/><div class="children"><div class="content">In this post yesterday <a href="https:&#x2F;&#x2F;justine.lol&#x2F;lex&#x2F;" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;lex&#x2F;</a> there is this quote:<p>&gt; The new highlighter and chatbot interface has made llamafile so pleasant for me to use, combined with the fact that open weights models like gemma 27b it have gotten so good, that it&#x27;s become increasingly rare that I&#x27;ll feel tempted to use Claude these days.<p>Leaving me tempted more than ever to see if I can integrate some sort of LLM workflow locally. I would only consider doing it locally, and I&#x27;ve an older computer, so I didn&#x27;t think this would be possible till reading that post yesterday.<p>The only thing I thought was - how would I work it in to Emacs? And then today, this post. It looks very well integrated. Has anyone any experience using gemma 27b it with llamafile and gptel?  I know very little about the whole space, really.</div><br/><div id="42036353" class="c"><input type="checkbox" id="c-42036353" checked=""/><div class="controls bullet"><span class="by">whartung</span><span>|</span><a href="#42035930">parent</a><span>|</span><a href="#42037363">next</a><span>|</span><label class="collapse" for="c-42036353">[-]</label><label class="expand" for="c-42036353">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m on an Intel iMac, and llama can&#x27;t leverage its GPU. So, it&#x27;s 1990s slow. It&#x27;s literally like talking to a machine in the 90s, from the slow response time to the 1200-2400 baud output.<p>It&#x27;s easy to give tasks to, hard to have a conversation with. Just paste the task in and let it churn, come back to it later.</div><br/></div></div></div></div><div id="42037363" class="c"><input type="checkbox" id="c-42037363" checked=""/><div class="controls bullet"><span class="by">BaculumMeumEst</span><span>|</span><a href="#42035930">prev</a><span>|</span><a href="#42035721">next</a><span>|</span><label class="collapse" for="c-42037363">[-]</label><label class="expand" for="c-42037363">[5 more]</label></div><br/><div class="children"><div class="content">gptel is great because it does exactly what you would expect and stays up to date with new models from anthropic and openai. Before settling on gptel, I went through FOUR programs that had a lot of buzz but were not being kept up to date with new models!<p>gptel has joined magit and undo-tree in being so damn useful and reliable that they are keeping me from ditching emacs, even though I want to.</div><br/><div id="42037481" class="c"><input type="checkbox" id="c-42037481" checked=""/><div class="controls bullet"><span class="by">__mharrison__</span><span>|</span><a href="#42037363">parent</a><span>|</span><a href="#42035721">next</a><span>|</span><label class="collapse" for="c-42037481">[-]</label><label class="expand" for="c-42037481">[4 more]</label></div><br/><div class="children"><div class="content">Mentioning that it is on par with magit is a strong recommendation. Going to try this out tomorrow.</div><br/><div id="42037620" class="c"><input type="checkbox" id="c-42037620" checked=""/><div class="controls bullet"><span class="by">marci</span><span>|</span><a href="#42037363">root</a><span>|</span><a href="#42037481">parent</a><span>|</span><a href="#42035721">next</a><span>|</span><label class="collapse" for="c-42037620">[-]</label><label class="expand" for="c-42037620">[3 more]</label></div><br/><div class="children"><div class="content">Gptel leverages Magit&#x27;s interface.</div><br/><div id="42038984" class="c"><input type="checkbox" id="c-42038984" checked=""/><div class="controls bullet"><span class="by">johanvts</span><span>|</span><a href="#42037363">root</a><span>|</span><a href="#42037620">parent</a><span>|</span><a href="#42035721">next</a><span>|</span><label class="collapse" for="c-42038984">[-]</label><label class="expand" for="c-42038984">[2 more]</label></div><br/><div class="children"><div class="content">Yes, its called transient and it’s pretty simple to use for your own projects. I wrote a transient for my work tasks and now ex. Creating a new youtrack issue is effortless.</div><br/><div id="42039692" class="c"><input type="checkbox" id="c-42039692" checked=""/><div class="controls bullet"><span class="by">ossusermivami</span><span>|</span><a href="#42037363">root</a><span>|</span><a href="#42038984">parent</a><span>|</span><a href="#42035721">next</a><span>|</span><label class="collapse" for="c-42039692">[-]</label><label class="expand" for="c-42039692">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s built-in emacs as well now!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42035721" class="c"><input type="checkbox" id="c-42035721" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#42037363">prev</a><span>|</span><a href="#42037295">next</a><span>|</span><label class="collapse" for="c-42035721">[-]</label><label class="expand" for="c-42035721">[7 more]</label></div><br/><div class="children"><div class="content">This is really sweet. I&#x27;ve only recently started dabbing into AI-assisted programming, and I think this integration into Emacs is really smooth.<p>What would be really neat is to add REPL-like functionality to an LLM buffer so that code generated by the LLM can be evaluated right away in place.</div><br/><div id="42038528" class="c"><input type="checkbox" id="c-42038528" checked=""/><div class="controls bullet"><span class="by">karthink</span><span>|</span><a href="#42035721">parent</a><span>|</span><a href="#42035834">next</a><span>|</span><label class="collapse" for="c-42038528">[-]</label><label class="expand" for="c-42038528">[1 more]</label></div><br/><div class="children"><div class="content">For a REPL-like interface, you could try the chatgpt-shell package.  It can execute code generated by the LLM.  It too does this by using org-babel though, it just calls org-babel functions under the hood.  It&#x27;s also OpenAI-only right now, although the author plans to add support for the other major APIs.<p>gptel has a buffer-centric design because it tries to get out of your way and integrate with your regular Emacs usage.  (For example, it&#x27;s even available _in_ the minibuffer, in that you can call it in the middle of calling another command, and fill the minibuffer prompt itself with the text from an LLM response.)</div><br/></div></div><div id="42035834" class="c"><input type="checkbox" id="c-42035834" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#42035721">parent</a><span>|</span><a href="#42038528">prev</a><span>|</span><a href="#42037295">next</a><span>|</span><label class="collapse" for="c-42035834">[-]</label><label class="expand" for="c-42035834">[5 more]</label></div><br/><div class="children"><div class="content">gptel can output org-babel.</div><br/><div id="42035958" class="c"><input type="checkbox" id="c-42035958" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42035721">root</a><span>|</span><a href="#42035834">parent</a><span>|</span><a href="#42037295">next</a><span>|</span><label class="collapse" for="c-42035958">[-]</label><label class="expand" for="c-42035958">[4 more]</label></div><br/><div class="children"><div class="content">Indeed; fire up gptel-mode in an Org Mode buffer, and you&#x27;ll get to work with Org Mode, including code blocks with whatever evaluation support you have configured in your Emacs.<p>Also I really like the design of the chat feature - the interactive chat buffer is still just a plain Markdown buffer, which you can simply <i>save to file</i> to persist the conversation. Unlike with typical interactive buffers (e.g. shell), nothing actually breaks - gptel-mode just appends the chat settings to the buffer in the standard Emacs fashion (key&#x2F;value comments at the bottom of the file), so to continue from where you left off, you just need to open file and run M-x gptel.<p>(This also means you can just run M-x gptel in a random Markdown buffer - or an Org Mode buffer, if you want aforementioned org-babel functionality; as long as gptel minor mode is active, saving the buffer will also update persisted chat configuration.)</div><br/><div id="42036401" class="c"><input type="checkbox" id="c-42036401" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#42035721">root</a><span>|</span><a href="#42035958">parent</a><span>|</span><a href="#42037295">next</a><span>|</span><label class="collapse" for="c-42036401">[-]</label><label class="expand" for="c-42036401">[3 more]</label></div><br/><div class="children"><div class="content">Org code blocks are great but not quite the same as having a REPL. But like I said above, I think this is really a great piece of software. I can definitely see this being a game changer in my daily work with Emacs.</div><br/><div id="42036579" class="c"><input type="checkbox" id="c-42036579" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42035721">root</a><span>|</span><a href="#42036401">parent</a><span>|</span><a href="#42036941">next</a><span>|</span><label class="collapse" for="c-42036579">[-]</label><label class="expand" for="c-42036579">[1 more]</label></div><br/><div class="children"><div class="content">Used the right way, Org mode code blocks are <i>better</i>, though setting things up to allow this can be tricky, and so I rarely bother.<p>What I mean is: the first difference between a REPL and an Org Mode block (of non-elisp code[0]) is that in REPL, you eval code sequentially <i>in the same runtime session</i>; in contrast, org-babel will happily run each execution in a fresh interpreter&#x2F;runtime, unless steps are taken to keep a shared, persistent session. But once you get that working (which may be more or less tricky, depending on the language), your Org Mode file effectively becomes a REPL with editable scrollback.<p>This may not be what you want in many cases, but it is very helpful when you&#x27;re collaborating with an LLM - being able to freely edit and reshape the entire conversation history is useful in keeping the model on point, and costs in check.<p>--<p>[0] - Emacs Lisp snippets run directly on your Emacs, so your current instance <i>is</i> your session. It&#x27;s nice that you get a shared session for free, but it also sucks, as there only ever is <i>one</i> session, shared by all elisp code you run. Good luck keeping your variables from leaking out to the global scope and possibly overwriting something.</div><br/></div></div><div id="42036941" class="c"><input type="checkbox" id="c-42036941" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#42035721">root</a><span>|</span><a href="#42036401">parent</a><span>|</span><a href="#42036579">prev</a><span>|</span><a href="#42037295">next</a><span>|</span><label class="collapse" for="c-42036941">[-]</label><label class="expand" for="c-42036941">[1 more]</label></div><br/><div class="children"><div class="content">org-babel-eval-in-repl</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42037295" class="c"><input type="checkbox" id="c-42037295" checked=""/><div class="controls bullet"><span class="by">joeevans1000</span><span>|</span><a href="#42035721">prev</a><span>|</span><a href="#42036691">next</a><span>|</span><label class="collapse" for="c-42037295">[-]</label><label class="expand" for="c-42037295">[3 more]</label></div><br/><div class="children"><div class="content">Forgive my naiveté in these questions:<p>Can one get the memory of context now available on the online&#x2F;web chatgpt?<p>I find the list of conversations on the left of the web version are a way to start new lines of thinking. Can&#x2F;how can I get that workflow going in gptel? Is there a better way to organize than what the web version provides?<p>Thanks to all who have made this!</div><br/><div id="42037724" class="c"><input type="checkbox" id="c-42037724" checked=""/><div class="controls bullet"><span class="by">karthink</span><span>|</span><a href="#42037295">parent</a><span>|</span><a href="#42037609">next</a><span>|</span><label class="collapse" for="c-42037724">[-]</label><label class="expand" for="c-42037724">[1 more]</label></div><br/><div class="children"><div class="content">I think the web chat history is separate from API use, so you can&#x27;t combine them.  OpenAI claims not to retain a history of your API queries and responses.<p>For organizing LLM chat logs in Emacs, there are many solutions.  Here are a few:<p>As a basic solution, chats are just text buffers&#x2F;files, so you can simply store your conversations in files in a single directory.  You can then see them in dired etc -- and they are ripgrep-able, can be integrated into Org-roam or your choice of knowledge management system.<p>If you use Org mode, you can have branching conversations in gptel where each path through the document&#x27;s outline tree is a separate conversation branch.  This way you can explore tangential topics while retaining the lineage of the conversation that led to them, while excluding the other branches.  This keeps the context window from blowing up and your API inference costs (if any) down.<p>If you use Org mode, you can limit the scope of the conversation to the current heading by assigning a topic (gptel-set-topic).  This way you can have multiple independent conversations in one file&#x2F;buffer instead of one per buffer.  (This works in tandem with the previous solution.)<p>-----<p>Tools tend to compose very well in Emacs.  So there are probably many other solutions folks have come up with to organize their LLM chat history.  For instance, any feature to handle collections of files or give you an outline&#x2F;ToC view of your Markdown&#x2F;Org documents should work well with the above -- and there are dozens of extensions like these.</div><br/></div></div><div id="42037609" class="c"><input type="checkbox" id="c-42037609" checked=""/><div class="controls bullet"><span class="by">marci</span><span>|</span><a href="#42037295">parent</a><span>|</span><a href="#42037724">prev</a><span>|</span><a href="#42036691">next</a><span>|</span><label class="collapse" for="c-42037609">[-]</label><label class="expand" for="c-42037609">[1 more]</label></div><br/><div class="children"><div class="content">I suppose the simplest would be to have to split to two windows with the same buffer with &quot;Ctrl x 3&quot;, and press &quot;Shift+Tab&quot; to only have the headline. That is if all the conversation are on the same file.<p>If they are not on the same file, maybe something simple like deft (<a href="https:&#x2F;&#x2F;github.com&#x2F;jrblevin&#x2F;deft">https:&#x2F;&#x2F;github.com&#x2F;jrblevin&#x2F;deft</a>) or more complex like org-roam (<a href="https:&#x2F;&#x2F;github.com&#x2F;org-roam&#x2F;org-roam">https:&#x2F;&#x2F;github.com&#x2F;org-roam&#x2F;org-roam</a>)</div><br/></div></div></div></div><div id="42036691" class="c"><input type="checkbox" id="c-42036691" checked=""/><div class="controls bullet"><span class="by">jfdi</span><span>|</span><a href="#42037295">prev</a><span>|</span><a href="#42034697">next</a><span>|</span><label class="collapse" for="c-42036691">[-]</label><label class="expand" for="c-42036691">[5 more]</label></div><br/><div class="children"><div class="content">Anyone know of a similar option for vi&#x2F;m? (Not neovim etc).<p>Been searching and have found some but nothing stands out yet.</div><br/><div id="42039319" class="c"><input type="checkbox" id="c-42039319" checked=""/><div class="controls bullet"><span class="by">setopt</span><span>|</span><a href="#42036691">parent</a><span>|</span><a href="#42039607">next</a><span>|</span><label class="collapse" for="c-42039319">[-]</label><label class="expand" for="c-42039319">[1 more]</label></div><br/><div class="children"><div class="content">Seems some of these are for Vim too, but I haven’t tried them yet:
<a href="https:&#x2F;&#x2F;github.com&#x2F;jkitching&#x2F;awesome-vim-llm-plugins">https:&#x2F;&#x2F;github.com&#x2F;jkitching&#x2F;awesome-vim-llm-plugins</a>
Scanning the list quickly, dense-analysis&#x2F;neural perhaps sticks out since it’s written by the author of ALE, which is a very high-quality plugin.<p>Another option that is perhaps more the Unix way, is to run an LLM client in a terminal split (there are lots of CLI clients), and then use vim-slime to send code and text to that split.<p>Personally I’m still using ChatGPT in the browser and mobile app. Would love to try something else, but the OpenAI API key seems to cost extra, and something like llama probably takes time to setup right.</div><br/></div></div><div id="42039607" class="c"><input type="checkbox" id="c-42039607" checked=""/><div class="controls bullet"><span class="by">hedari</span><span>|</span><a href="#42036691">parent</a><span>|</span><a href="#42039319">prev</a><span>|</span><a href="#42037391">next</a><span>|</span><label class="collapse" for="c-42039607">[-]</label><label class="expand" for="c-42039607">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been happy with vim-ai plugin: <a href="https:&#x2F;&#x2F;github.com&#x2F;madox2&#x2F;vim-ai">https:&#x2F;&#x2F;github.com&#x2F;madox2&#x2F;vim-ai</a></div><br/></div></div><div id="42037391" class="c"><input type="checkbox" id="c-42037391" checked=""/><div class="controls bullet"><span class="by">chungy</span><span>|</span><a href="#42036691">parent</a><span>|</span><a href="#42039607">prev</a><span>|</span><a href="#42034697">next</a><span>|</span><label class="collapse" for="c-42037391">[-]</label><label class="expand" for="c-42037391">[2 more]</label></div><br/><div class="children"><div class="content">You could use Emacs in evil-mode.</div><br/><div id="42038292" class="c"><input type="checkbox" id="c-42038292" checked=""/><div class="controls bullet"><span class="by">pxc</span><span>|</span><a href="#42036691">root</a><span>|</span><a href="#42037391">parent</a><span>|</span><a href="#42034697">next</a><span>|</span><label class="collapse" for="c-42038292">[-]</label><label class="expand" for="c-42038292">[1 more]</label></div><br/><div class="children"><div class="content">Seconding this recommendation! I&#x27;ve never been a super advanced vim user, but Evil is the best and most complete vim emulator I&#x27;ve ever used, and I try them on every editor or IDE I ever run.</div><br/></div></div></div></div></div></div><div id="42034697" class="c"><input type="checkbox" id="c-42034697" checked=""/><div class="controls bullet"><span class="by">michaelsbradley</span><span>|</span><a href="#42036691">prev</a><span>|</span><label class="collapse" for="c-42034697">[-]</label><label class="expand" for="c-42034697">[1 more]</label></div><br/><div class="children"><div class="content">And here’s a nice writeup:<p><i>gptel: Mindblowing integration between Emacs and ChatGPT</i><p><a href="https:&#x2F;&#x2F;www.blogbyben.com&#x2F;2024&#x2F;08&#x2F;gptel-mindblowing-integration-between.html" rel="nofollow">https:&#x2F;&#x2F;www.blogbyben.com&#x2F;2024&#x2F;08&#x2F;gptel-mindblowing-integrat...</a></div><br/></div></div></div></div></div></div></div></body></html>