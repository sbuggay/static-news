<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736758873826" as="style"/><link rel="stylesheet" href="styles.css?v=1736758873826"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/TabbyML/tabby">Tabby: Self-hosted AI coding assistant</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>saikatsg</span> | <span>83 comments</span></div><br/><div><div id="42680343" class="c"><input type="checkbox" id="c-42680343" checked=""/><div class="controls bullet"><span class="by">nikkwong</span><span>|</span><a href="#42679254">next</a><span>|</span><label class="collapse" for="c-42680343">[-]</label><label class="expand" for="c-42680343">[9 more]</label></div><br/><div class="children"><div class="content">Maybe a good product but terrible company to interview with. I went through several rounds and was basically ghosted after the 4th with no explanation or follow up. The last interview was to write an blog post for their blog which I submitted and then didn’t hear back until continuously nagging months later. It was pretty disheartening since all of the interviews were some form of a take-home and I spent a combined total of ~10 hours or more.</div><br/><div id="42680766" class="c"><input type="checkbox" id="c-42680766" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#42680343">parent</a><span>|</span><a href="#42681279">next</a><span>|</span><label class="collapse" for="c-42680766">[-]</label><label class="expand" for="c-42680766">[4 more]</label></div><br/><div class="children"><div class="content">&gt; The last interview was to write an blog post for their blog<p>Where you applying as a Software Dev.? Because that&#x27;s not a software (or an interview) assignment.</div><br/><div id="42680840" class="c"><input type="checkbox" id="c-42680840" checked=""/><div class="controls bullet"><span class="by">nikkwong</span><span>|</span><a href="#42680343">root</a><span>|</span><a href="#42680766">parent</a><span>|</span><a href="#42681279">next</a><span>|</span><label class="collapse" for="c-42680840">[-]</label><label class="expand" for="c-42680840">[3 more]</label></div><br/><div class="children"><div class="content">Yes I was applying for software engineer. I think they wanted engineers who were good at explaining the product to users.</div><br/><div id="42680917" class="c"><input type="checkbox" id="c-42680917" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#42680343">root</a><span>|</span><a href="#42680840">parent</a><span>|</span><a href="#42681279">next</a><span>|</span><label class="collapse" for="c-42680917">[-]</label><label class="expand" for="c-42680917">[2 more]</label></div><br/><div class="children"><div class="content">Sure. Writing and a good command of the language is important. There are multiple ways to showcase that. Writing a blog post for <i>their</i> blog is not one of them.</div><br/><div id="42680969" class="c"><input type="checkbox" id="c-42680969" checked=""/><div class="controls bullet"><span class="by">nikkwong</span><span>|</span><a href="#42680343">root</a><span>|</span><a href="#42680917">parent</a><span>|</span><a href="#42681279">next</a><span>|</span><label class="collapse" for="c-42680969">[-]</label><label class="expand" for="c-42680969">[1 more]</label></div><br/><div class="children"><div class="content">I was willing to jump through hoops—I really wanted the job.</div><br/></div></div></div></div></div></div></div></div><div id="42681279" class="c"><input type="checkbox" id="c-42681279" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#42680343">parent</a><span>|</span><a href="#42680766">prev</a><span>|</span><a href="#42680632">next</a><span>|</span><label class="collapse" for="c-42681279">[-]</label><label class="expand" for="c-42681279">[1 more]</label></div><br/><div class="children"><div class="content">Hope they paid for the work.</div><br/></div></div><div id="42680632" class="c"><input type="checkbox" id="c-42680632" checked=""/><div class="controls bullet"><span class="by">NetOpWibby</span><span>|</span><a href="#42680343">parent</a><span>|</span><a href="#42681279">prev</a><span>|</span><a href="#42679254">next</a><span>|</span><label class="collapse" for="c-42680632">[-]</label><label class="expand" for="c-42680632">[3 more]</label></div><br/><div class="children"><div class="content">Were you at least paid?</div><br/><div id="42680743" class="c"><input type="checkbox" id="c-42680743" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#42680343">root</a><span>|</span><a href="#42680632">parent</a><span>|</span><a href="#42679254">next</a><span>|</span><label class="collapse" for="c-42680743">[-]</label><label class="expand" for="c-42680743">[2 more]</label></div><br/><div class="children"><div class="content">you know that paid interview processes are not the norm, &quot;at least&quot; is unlikely</div><br/><div id="42680839" class="c"><input type="checkbox" id="c-42680839" checked=""/><div class="controls bullet"><span class="by">nikkwong</span><span>|</span><a href="#42680343">root</a><span>|</span><a href="#42680743">parent</a><span>|</span><a href="#42679254">next</a><span>|</span><label class="collapse" for="c-42680839">[-]</label><label class="expand" for="c-42680839">[1 more]</label></div><br/><div class="children"><div class="content">If I was paid, I probably wouldn&#x27;t be complaining publicly. :-) It&#x27;s probably better for both interests if these types of engagements are paid.</div><br/></div></div></div></div></div></div></div></div><div id="42679254" class="c"><input type="checkbox" id="c-42679254" checked=""/><div class="controls bullet"><span class="by">st3fan</span><span>|</span><a href="#42680343">prev</a><span>|</span><a href="#42677193">next</a><span>|</span><label class="collapse" for="c-42679254">[-]</label><label class="expand" for="c-42679254">[30 more]</label></div><br/><div class="children"><div class="content">The demo on the homepage for the completion of the findMaxElement function is a good example of what is to come. Or maybe where we are at now?<p>The six lines of Python suggested for that function can also be replaced with a simple “return max(arr)”. The suggested code works but is absolute junior level.<p>I am terrified of what is to come. Not just horrible code but also how people who blindly “autocomplete” this code are going to stall in their skill level progress.<p>You may score some story points but did you actually get any better at your craft?</div><br/><div id="42679612" class="c"><input type="checkbox" id="c-42679612" checked=""/><div class="controls bullet"><span class="by">shcheklein</span><span>|</span><a href="#42679254">parent</a><span>|</span><a href="#42679961">next</a><span>|</span><label class="collapse" for="c-42679612">[-]</label><label class="expand" for="c-42679612">[14 more]</label></div><br/><div class="children"><div class="content">On the other hand it might become a next level of abstraction.<p>Machine -&gt; Asm -&gt; C -&gt; Python -&gt; LLM (Human language)<p>It compiles human prompt into some intermediate code (in this case Python). Probably initial version of CPython was not perfect at all, and engineers were also terrified. If we are lucky this new &quot;compiler&quot; will be becoming better and better, more efficient. Never perfect, but people will be paying the same price they are already paying for not dealing directly with ASM.</div><br/><div id="42679900" class="c"><input type="checkbox" id="c-42679900" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679612">parent</a><span>|</span><a href="#42680142">next</a><span>|</span><label class="collapse" for="c-42679900">[-]</label><label class="expand" for="c-42679900">[10 more]</label></div><br/><div class="children"><div class="content">&gt; Machine -&gt; Asm -&gt; C -&gt; Python -&gt; LLM (Human language)<p>Something that you neglected to mention is, with every abstraction layer up to Python, everything is predictable and repeatable. With LLMs, we can give the exact same instructions, and not be guaranteed the same code.</div><br/><div id="42681426" class="c"><input type="checkbox" id="c-42681426" checked=""/><div class="controls bullet"><span class="by">zurn</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679900">parent</a><span>|</span><a href="#42680349">next</a><span>|</span><label class="collapse" for="c-42681426">[-]</label><label class="expand" for="c-42681426">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &gt; Machine -&gt; Asm -&gt; C -&gt; Python -&gt; LLM (Human language)<p>&gt; Something that you neglected to mention is, with every abstraction layer up to Python, everything is predictable and repeatable.<p>As long as you consider C and dragons flying out of your nose predictable.<p>(Insert similar quip about hardware)</div><br/></div></div><div id="42680349" class="c"><input type="checkbox" id="c-42680349" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679900">parent</a><span>|</span><a href="#42681426">prev</a><span>|</span><a href="#42681265">next</a><span>|</span><label class="collapse" for="c-42680349">[-]</label><label class="expand" for="c-42680349">[4 more]</label></div><br/><div class="children"><div class="content">I’m not sure why that matters here. Users want code that solves their business need. In general most don’t care about repeatability if someone else tries to solve their problem.<p>The question that matters is: can businesses solve their problems cheaper for the same quality, or at lower quality while beating the previous Pareto-optimal cost&#x2F;quality frontier.</div><br/><div id="42680580" class="c"><input type="checkbox" id="c-42680580" checked=""/><div class="controls bullet"><span class="by">thesz</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42680349">parent</a><span>|</span><a href="#42681265">next</a><span>|</span><label class="collapse" for="c-42680580">[-]</label><label class="expand" for="c-42680580">[3 more]</label></div><br/><div class="children"><div class="content">Recognizable repetition can be abstracted, reducing code base and its (running) support cost.<p>The question that matters is: will businesses crumble due to overproduction of same (or lower) quality code sooner or later.</div><br/><div id="42680701" class="c"><input type="checkbox" id="c-42680701" checked=""/><div class="controls bullet"><span class="by">chii</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42680580">parent</a><span>|</span><a href="#42681265">next</a><span>|</span><label class="collapse" for="c-42680701">[-]</label><label class="expand" for="c-42680701">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The question that matters is: will businesses crumble due to overproduction of same (or lower) quality code sooner or later.<p>but why doesn&#x27;t that happen today? Cheap code can be had by hiring in cheap locations (outsourced for example).<p>The reality is that customers are the ultimate arbiters, and if it satisfies them, the business will not collapse. And i have not seen a single customer demonstrate that they care about the quality of the code base behind the product they enjoy paying for.</div><br/><div id="42680973" class="c"><input type="checkbox" id="c-42680973" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42680701">parent</a><span>|</span><a href="#42681265">next</a><span>|</span><label class="collapse" for="c-42680973">[-]</label><label class="expand" for="c-42680973">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Cheap code can be had by hiring in cheap locations (outsourced for example).<p>If you outsource and like what you get, you would assume the place you outsourced to can help provide continued support. What assurance do you have with LLMs? A working solution doesn&#x27;t mean it can be easily maintained and&#x2F;or evolved.<p>&gt; And i have not seen a single customer demonstrate that they care about the quality of the code base behind the product they enjoy paying for.<p>That is true, but they will complain if bugs cannot be fixed and features are added. It is true that customers don&#x27;t care, and they shouldn&#x27;t, until it does matter, of course.<p>The challenge with software development isn&#x27;t necessarily with the first iteration, but rather it is with continued support. Where I think LLMs can really shine is in providing domain experts (those who understand the problem) with a better way to demonstrate their needs.</div><br/></div></div></div></div></div></div></div></div><div id="42681265" class="c"><input type="checkbox" id="c-42681265" checked=""/><div class="controls bullet"><span class="by">omgwtfbyobbq</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679900">parent</a><span>|</span><a href="#42680349">prev</a><span>|</span><a href="#42680946">next</a><span>|</span><label class="collapse" for="c-42681265">[-]</label><label class="expand" for="c-42681265">[1 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t some models deterministic with temperature set to 0?</div><br/></div></div><div id="42680946" class="c"><input type="checkbox" id="c-42680946" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679900">parent</a><span>|</span><a href="#42681265">prev</a><span>|</span><a href="#42680147">next</a><span>|</span><label class="collapse" for="c-42680946">[-]</label><label class="expand" for="c-42680946">[1 more]</label></div><br/><div class="children"><div class="content"><i>With LLMs, we can give the exact same instructions, and not be guaranteed the same code.</i><p>That&#x27;s something we&#x27;ll have to give up and get over.<p>See also: understanding how the underlying code actually works.  You don&#x27;t need to know assembly to use a high-level programming language (although it certainly doesn&#x27;t hurt), and you won&#x27;t need to know a high-level programming language to write the functional specs in English that the code generator model uses.<p>I say bring it on.  50+ years was long enough to keep doing things the same way.</div><br/></div></div><div id="42680147" class="c"><input type="checkbox" id="c-42680147" checked=""/><div class="controls bullet"><span class="by">12345hn6789</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679900">parent</a><span>|</span><a href="#42680946">prev</a><span>|</span><a href="#42680142">next</a><span>|</span><label class="collapse" for="c-42680147">[-]</label><label class="expand" for="c-42680147">[2 more]</label></div><br/><div class="children"><div class="content">assuming you have full control over which compiler youre using for each step ;)<p>What&#x27;s to say LLMs will not have a &quot;compiler&quot; interface in the future that will reign in their variance</div><br/><div id="42680272" class="c"><input type="checkbox" id="c-42680272" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42680147">parent</a><span>|</span><a href="#42680142">next</a><span>|</span><label class="collapse" for="c-42680272">[-]</label><label class="expand" for="c-42680272">[1 more]</label></div><br/><div class="children"><div class="content">&gt; assuming you have full control over which compiler youre using for each step ;)<p>With existing tools, we know if we need to do something, we can. The issue with LLMs, is they are very much black boxes.<p>&gt; What&#x27;s to say LLMs will not have a &quot;compiler&quot; interface in the future that will reign in their variance<p>Honestly, having a compiler interface for LLMs isn&#x27;t a bad idea...for some use cases. What I don&#x27;t see us being able to do is use natural language to build complex apps in a deterministic manner. Solving this problem would require turning LLMs into deterministic machines, which I don&#x27;t believe will be an easy task, given how LLMs work today.<p>I&#x27;m a strong believer in that LLMs will change how we develop and create software development tools. In the past, you would need Google and Microsoft level of funding to integrate natural language into a tool, but with LLMs, we can easily have LLMs parse input and have it map to deterministic functions in days.</div><br/></div></div></div></div></div></div><div id="42680142" class="c"><input type="checkbox" id="c-42680142" checked=""/><div class="controls bullet"><span class="by">vages</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679612">parent</a><span>|</span><a href="#42679900">prev</a><span>|</span><a href="#42679717">next</a><span>|</span><label class="collapse" for="c-42680142">[-]</label><label class="expand" for="c-42680142">[1 more]</label></div><br/><div class="children"><div class="content">It may be a “level of abstraction”, but not a good one, because it is imprecise.<p>When you want to make changes to the code (which is what we spend most of our time on), you’ll have to either (1) modify the prompt and accept the risk of using the new code or (2) modify the original code, which you can’t do unless you know the lower level of abstraction.<p>Recommended reading: <a href="https:&#x2F;&#x2F;ian-cooper.writeas.com&#x2F;is-ai-a-silver-bullet" rel="nofollow">https:&#x2F;&#x2F;ian-cooper.writeas.com&#x2F;is-ai-a-silver-bullet</a></div><br/></div></div><div id="42679717" class="c"><input type="checkbox" id="c-42679717" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679612">parent</a><span>|</span><a href="#42680142">prev</a><span>|</span><a href="#42679730">next</a><span>|</span><label class="collapse" for="c-42679717">[-]</label><label class="expand" for="c-42679717">[1 more]</label></div><br/><div class="children"><div class="content">Yup!<p>No goal to become a programmer– But I like to build programs.<p>Build a rather complex AI-ecosystem simulator with me as the director and GPT-4 now Claude 3.5 as the programmer.<p>Would never have been able to do this beforehand.</div><br/></div></div><div id="42679730" class="c"><input type="checkbox" id="c-42679730" checked=""/><div class="controls bullet"><span class="by">saurik</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679612">parent</a><span>|</span><a href="#42679717">prev</a><span>|</span><a href="#42679961">next</a><span>|</span><label class="collapse" for="c-42679730">[-]</label><label class="expand" for="c-42679730">[1 more]</label></div><br/><div class="children"><div class="content">I think there is a big difference between an abstraction layer that can improve -- one where you maybe write &quot;code&quot; in prompts and then have a compiler build through real code, allowing that compiler to get better over time -- and an interactive tool that locks bad decisions autocompleted today into both your codebase and your brain, involving you still working at the lower layer but getting low quality &quot;help&quot; in your editor. I am totally pro- compilers and high-level languages, but I think the idea of writing assembly with the help of a partial compiler where you kind of write stuff and then copy&#x2F;paste the result into your assembly file with some munging to fix issues is dumb.<p>By all means, though: if someone gets us to the point where the &quot;code&quot; I am checking in is a bunch of English -- for which I will likely need a law degree in addition to an engineering background to not get evil genie with a cursed paw results from it trying to figure out what I must have meant from what I said :&#x2F; -- I will think that&#x27;s pretty cool and will actually be a new layer of abstraction in the same class as compiler... and like, if at that point I don&#x27;t use it, it will only be because I think it is somehow dangerous to humanity itself (and even then I will admit that it is probably more effective)... but we aren&#x27;t there yet and &quot;we&#x27;re on the way there&quot; doesn&#x27;t count anywhere near as much as people often want it to ;P.</div><br/></div></div></div></div><div id="42679961" class="c"><input type="checkbox" id="c-42679961" checked=""/><div class="controls bullet"><span class="by">tippytippytango</span><span>|</span><a href="#42679254">parent</a><span>|</span><a href="#42679612">prev</a><span>|</span><a href="#42680240">next</a><span>|</span><label class="collapse" for="c-42679961">[-]</label><label class="expand" for="c-42679961">[1 more]</label></div><br/><div class="children"><div class="content">This is self correcting. Code of this quality won&#x27;t let you ship things. You are forced to understand the last 20%-30% of details the LLM can&#x27;t help you with to pass all your tests. But, it also turns out, to understand the 20% of details the LLM couldn&#x27;t handle, you need to understand the 80% the LLM <i>could</i> handle.<p>I&#x27;m just not worried about this, LLMs don&#x27;t ship.</div><br/></div></div><div id="42680240" class="c"><input type="checkbox" id="c-42680240" checked=""/><div class="controls bullet"><span class="by">ripped_britches</span><span>|</span><a href="#42679254">parent</a><span>|</span><a href="#42679961">prev</a><span>|</span><a href="#42679828">next</a><span>|</span><label class="collapse" for="c-42680240">[-]</label><label class="expand" for="c-42680240">[2 more]</label></div><br/><div class="children"><div class="content">The most underrated thing I do on nearly every cursor suggestion is to follow up with “are there any better ways to do this?”.</div><br/><div id="42680667" class="c"><input type="checkbox" id="c-42680667" checked=""/><div class="controls bullet"><span class="by">smcnally</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42680240">parent</a><span>|</span><a href="#42679828">next</a><span>|</span><label class="collapse" for="c-42680667">[-]</label><label class="expand" for="c-42680667">[1 more]</label></div><br/><div class="children"><div class="content">A deeper version of the same idea is to ask a second model to check the first model’s answers. aider’s “architect” is an automated version of this approach.<p><a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;usage&#x2F;modes.html#architect-mode-and-the-editor-model" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;usage&#x2F;modes.html#architect-mode-and-...</a></div><br/></div></div></div></div><div id="42679828" class="c"><input type="checkbox" id="c-42679828" checked=""/><div class="controls bullet"><span class="by">svachalek</span><span>|</span><a href="#42679254">parent</a><span>|</span><a href="#42680240">prev</a><span>|</span><a href="#42679990">next</a><span>|</span><label class="collapse" for="c-42679828">[-]</label><label class="expand" for="c-42679828">[7 more]</label></div><br/><div class="children"><div class="content">Keep in mind that this is the stupidest the LLM will ever be and we can expect major improvements every few months. On the other hand junior devs will always be junior devs. At some point python and C++ will be like assembly now, something that’s always out there but not something the vast majority of developers will ever need to read or write.</div><br/><div id="42680081" class="c"><input type="checkbox" id="c-42680081" checked=""/><div class="controls bullet"><span class="by">llamaLord</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679828">parent</a><span>|</span><a href="#42680298">next</a><span>|</span><label class="collapse" for="c-42680081">[-]</label><label class="expand" for="c-42680081">[2 more]</label></div><br/><div class="children"><div class="content">My experience observing commercial LLM&#x27;s since the release of GPT-4 is actually the opposite of this.<p>Sure, they&#x27;ve gotten much cheaper on a per-token basis, but that cost reduction has come with a non-trivial accuracy&#x2F;reliability cost.<p>The problem is, tokens that are 10x cheaper are still useless if what they say is straight up wrong.</div><br/><div id="42680305" class="c"><input type="checkbox" id="c-42680305" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42680081">parent</a><span>|</span><a href="#42680298">next</a><span>|</span><label class="collapse" for="c-42680305">[-]</label><label class="expand" for="c-42680305">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Sure, they&#x27;ve gotten much cheaper on a per-token basis, but that cost reduction has come with a non-trivial accuracy&#x2F;reliability cost.<p>This only holds for OpenAI.</div><br/></div></div></div></div><div id="42680298" class="c"><input type="checkbox" id="c-42680298" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679828">parent</a><span>|</span><a href="#42680081">prev</a><span>|</span><a href="#42679947">next</a><span>|</span><label class="collapse" for="c-42680298">[-]</label><label class="expand" for="c-42680298">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Keep in mind that this is the stupidest the LLM will ever be and we can expect major improvements every few months.<p>We have seen no noticable improvements (at usable prices) for 7 months, when the original Sonnet 3.5 came out.<p>Maybe specialized hardware for LLM inference will improve so rapidly that o1 (full) will be quick and cheap enough a year from now, but it seems extremely unlikely. For the end user, the top models hadn&#x27;t gotten cheaper for kore than a year until the release of Deepseek v3 a few weeks ago. Even that is currently very slow at non-Deepseek providers, and who knows just how subsidized the pricing and speed at Deepseek itself is, given political interests.</div><br/></div></div><div id="42679947" class="c"><input type="checkbox" id="c-42679947" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679828">parent</a><span>|</span><a href="#42680298">prev</a><span>|</span><a href="#42679990">next</a><span>|</span><label class="collapse" for="c-42679947">[-]</label><label class="expand" for="c-42679947">[3 more]</label></div><br/><div class="children"><div class="content">&gt; we can expect major improvements every few months.<p>I&#x27;m not sure this is grounded in reality. We&#x27;ve already seen articles related to how OpenAI is behind schedule with GPT-5. I do believe things will improve over time, mainly due to advancements in hardware. With better hardware, we can better brute force correct answers.<p>&gt; junior devs will always be junior devs<p>Junior developers turn into senior developers over time.</div><br/><div id="42680729" class="c"><input type="checkbox" id="c-42680729" checked=""/><div class="controls bullet"><span class="by">smcnally</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679947">parent</a><span>|</span><a href="#42679990">next</a><span>|</span><label class="collapse" for="c-42680729">[-]</label><label class="expand" for="c-42680729">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m not sure this is grounded in reality. We&#x27;ve already seen articles related to how OpenAI is behind schedule with GPT-5.<p>Progress by Google, meta, Microsoft, Qwen and Deepseek is unhampered by OpenAI’s schedule. Their latest — including Gemini 2.0, Llama 3.3, Phi 4 — and the coding fine tunes that follow are all pretty good.</div><br/><div id="42680881" class="c"><input type="checkbox" id="c-42680881" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42680729">parent</a><span>|</span><a href="#42679990">next</a><span>|</span><label class="collapse" for="c-42680881">[-]</label><label class="expand" for="c-42680881">[1 more]</label></div><br/><div class="children"><div class="content">&gt; unhampered by OpenAI’s schedule<p>Sure, but if the advancements are to catch up to OpenAI, then major improvements by other vendors are nice and all, but I don&#x27;t believe that was what the commenter was implying. Right now the leaders in my opinion are OpenAI and Anthropic and unless they are making major improvements every few months, the industry as a whole is not making major improvements.</div><br/></div></div></div></div></div></div></div></div><div id="42679990" class="c"><input type="checkbox" id="c-42679990" checked=""/><div class="controls bullet"><span class="by">999900000999</span><span>|</span><a href="#42679254">parent</a><span>|</span><a href="#42679828">prev</a><span>|</span><a href="#42680775">next</a><span>|</span><label class="collapse" for="c-42679990">[-]</label><label class="expand" for="c-42679990">[2 more]</label></div><br/><div class="children"><div class="content">LLMs also love to double down on solutions that don&#x27;t work.<p>Case in point, I&#x27;m working on a game that&#x27;s essentially a website right now. Since I&#x27;m very very bad with web design I&#x27;m using an LLM.<p>It&#x27;s perfect 75% of the time. The other 25% it just doesn&#x27;t work. Multiple LLMs will misunderstand basic tasks. Let&#x27;s add properties and invent functions.<p>It&#x27;s like you had hired a college junior who insists their never wrong and keeps pushing non functional code.<p>The entire mindset is whatever it&#x27;s close enough, good luck.<p>God forbid you need to do anything using an uncommon node module or anything like that.</div><br/><div id="42680854" class="c"><input type="checkbox" id="c-42680854" checked=""/><div class="controls bullet"><span class="by">smcnally</span><span>|</span><a href="#42679254">root</a><span>|</span><a href="#42679990">parent</a><span>|</span><a href="#42680775">next</a><span>|</span><label class="collapse" for="c-42680854">[-]</label><label class="expand" for="c-42680854">[1 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs also love to double down on solutions that don&#x27;t work.<p>“Often wrong but never in doubt” is not proprietary to LLMs. It’s off-putting and we want them to be correct and to have humility when they’re wrong. But we should remember LLMs are trained on work created by people, and many of those people have built successful careers being exceedingly confident in solutions that don’t work.</div><br/></div></div></div></div><div id="42680775" class="c"><input type="checkbox" id="c-42680775" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#42679254">parent</a><span>|</span><a href="#42679990">prev</a><span>|</span><a href="#42679503">next</a><span>|</span><label class="collapse" for="c-42680775">[-]</label><label class="expand" for="c-42680775">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I am terrified of what is to come.<p>Don&#x27;t worry. Like everything else in life, you get what you pay for.</div><br/></div></div><div id="42679503" class="c"><input type="checkbox" id="c-42679503" checked=""/><div class="controls bullet"><span class="by">MaKey</span><span>|</span><a href="#42679254">parent</a><span>|</span><a href="#42680775">prev</a><span>|</span><a href="#42679922">next</a><span>|</span><label class="collapse" for="c-42679503">[-]</label><label class="expand" for="c-42679503">[1 more]</label></div><br/><div class="children"><div class="content">The silver lining is that the value of your skills is going up.</div><br/></div></div><div id="42679922" class="c"><input type="checkbox" id="c-42679922" checked=""/><div class="controls bullet"><span class="by">runeblaze</span><span>|</span><a href="#42679254">parent</a><span>|</span><a href="#42679503">prev</a><span>|</span><a href="#42677193">next</a><span>|</span><label class="collapse" for="c-42679922">[-]</label><label class="expand" for="c-42679922">[1 more]</label></div><br/><div class="children"><div class="content">I mean you can treat it as just a general pseudocode-ish implementation of an O(n) find_max algorithm. Tons of people use Python to illustrate algorithms.<p>(Not to hide your point though -- people please review your LLM-generated code!)</div><br/></div></div></div></div><div id="42677193" class="c"><input type="checkbox" id="c-42677193" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42679254">prev</a><span>|</span><a href="#42677322">next</a><span>|</span><label class="collapse" for="c-42677193">[-]</label><label class="expand" for="c-42677193">[6 more]</label></div><br/><div class="children"><div class="content">Never imagined our project would make it to the HN front page on Sunday!<p>Tabby has undergone significant development since its launch two years ago [0]. It is now a comprehensive AI developer platform featuring code completion and a codebase chat, with a team [1] &#x2F; enterprise focus (SSO, Access Control, User Authentication).<p>Tabby&#x27;s adopters [2][3] have discovered that Tabby is the only platform providing a fully self-service onboarding experience as an on-prem offering. It also delivers performance that rivals other options in the market. If you&#x27;re curious, I encourage you to give it a try!<p>[0]: <a href="https:&#x2F;&#x2F;www.tabbyml.com" rel="nofollow">https:&#x2F;&#x2F;www.tabbyml.com</a><p>[1]: <a href="https:&#x2F;&#x2F;demo.tabbyml.com&#x2F;search&#x2F;how-to-add-an-embedding-api-N8Ay31" rel="nofollow">https:&#x2F;&#x2F;demo.tabbyml.com&#x2F;search&#x2F;how-to-add-an-embedding-api-...</a><p>[2]: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;s&#x2F;lznmkWJhAZ" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;s&#x2F;lznmkWJhAZ</a><p>[3]: <a href="https:&#x2F;&#x2F;www.linkedin.com&#x2F;posts&#x2F;kelvinmu_last-week-i-introduced-two-ai-startups-to-activity-7257051111314911232-C3w2" rel="nofollow">https:&#x2F;&#x2F;www.linkedin.com&#x2F;posts&#x2F;kelvinmu_last-week-i-introduc...</a></div><br/><div id="42677432" class="c"><input type="checkbox" id="c-42677432" checked=""/><div class="controls bullet"><span class="by">maille</span><span>|</span><a href="#42677193">parent</a><span>|</span><a href="#42677611">next</a><span>|</span><label class="collapse" for="c-42677432">[-]</label><label class="expand" for="c-42677432">[3 more]</label></div><br/><div class="children"><div class="content">Do you have a plugin for MSVC?</div><br/><div id="42677888" class="c"><input type="checkbox" id="c-42677888" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42677193">root</a><span>|</span><a href="#42677432">parent</a><span>|</span><a href="#42679293">next</a><span>|</span><label class="collapse" for="c-42677888">[-]</label><label class="expand" for="c-42677888">[1 more]</label></div><br/><div class="children"><div class="content">Not yet, consider subscribe <a href="https:&#x2F;&#x2F;github.com&#x2F;TabbyML&#x2F;tabby&#x2F;issues&#x2F;322">https:&#x2F;&#x2F;github.com&#x2F;TabbyML&#x2F;tabby&#x2F;issues&#x2F;322</a> for future updates!</div><br/></div></div><div id="42679293" class="c"><input type="checkbox" id="c-42679293" checked=""/><div class="controls bullet"><span class="by">somberi</span><span>|</span><a href="#42677193">root</a><span>|</span><a href="#42677432">parent</a><span>|</span><a href="#42677888">prev</a><span>|</span><a href="#42677611">next</a><span>|</span><label class="collapse" for="c-42679293">[-]</label><label class="expand" for="c-42679293">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;codespin-ai&#x2F;codespin-vscode-extension">https:&#x2F;&#x2F;github.com&#x2F;codespin-ai&#x2F;codespin-vscode-extension</a></div><br/></div></div></div></div><div id="42677611" class="c"><input type="checkbox" id="c-42677611" checked=""/><div class="controls bullet"><span class="by">tootie</span><span>|</span><a href="#42677193">parent</a><span>|</span><a href="#42677432">prev</a><span>|</span><a href="#42677322">next</a><span>|</span><label class="collapse" for="c-42677611">[-]</label><label class="expand" for="c-42677611">[2 more]</label></div><br/><div class="children"><div class="content">Is it only compatible with Nvidia and Apple? Will this work with an AMD GPU?</div><br/><div id="42678729" class="c"><input type="checkbox" id="c-42678729" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42677193">root</a><span>|</span><a href="#42677611">parent</a><span>|</span><a href="#42677322">next</a><span>|</span><label class="collapse" for="c-42678729">[-]</label><label class="expand" for="c-42678729">[1 more]</label></div><br/><div class="children"><div class="content">Yes - AMD GPU is supported through vulkan backend:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;TabbyML&#x2F;tabby&#x2F;releases&#x2F;tag&#x2F;v0.23.0">https:&#x2F;&#x2F;github.com&#x2F;TabbyML&#x2F;tabby&#x2F;releases&#x2F;tag&#x2F;v0.23.0</a><p><a href="https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;blog&#x2F;2024&#x2F;05&#x2F;01&#x2F;vulkan-support&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;blog&#x2F;2024&#x2F;05&#x2F;01&#x2F;vulkan-support&#x2F;</a></div><br/></div></div></div></div></div></div><div id="42677322" class="c"><input type="checkbox" id="c-42677322" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#42677193">prev</a><span>|</span><a href="#42680316">next</a><span>|</span><label class="collapse" for="c-42677322">[-]</label><label class="expand" for="c-42677322">[4 more]</label></div><br/><div class="children"><div class="content">As someone unfamiliar with local AIs and eager to try, how does the “run tabby in 1 minute”[1] compare to e.g. chatgpt’s free 4o-mini? Can I run that docker command on a medium specced macbook pro and have an AI that is comparably fast and capable? Or are we not there (yet)?<p>Edit: looks like there is a separate page with instructions for macbooks[2] that has more context.<p>&gt; The compute power of M1&#x2F;M2 is limited and is likely to be sufficient only for individual usage. If you require a shared instance for a team, we recommend considering Docker hosting with CUDA or ROCm.<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;TabbyML&#x2F;tabby#run-tabby-in-1-minute">https:&#x2F;&#x2F;github.com&#x2F;TabbyML&#x2F;tabby#run-tabby-in-1-minute</a><p><pre><code>    docker run -it --gpus all -p 8080:8080 -v $HOME&#x2F;.tabby:&#x2F;data tabbyml&#x2F;tabby serve --model StarCoder-1B --device cuda --chat-model Qwen2-1.5B-Instruct
</code></pre>
[2]: <a href="https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;docs&#x2F;quick-start&#x2F;installation&#x2F;apple&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;docs&#x2F;quick-start&#x2F;installation&#x2F;appl...</a></div><br/><div id="42677879" class="c"><input type="checkbox" id="c-42677879" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#42677322">parent</a><span>|</span><a href="#42677511">next</a><span>|</span><label class="collapse" for="c-42677879">[-]</label><label class="expand" for="c-42677879">[2 more]</label></div><br/><div class="children"><div class="content">gpt-4o-mini might not be the best point of reference for what good LLMs can do with code: <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;leaderboards&#x2F;#aider-polyglot-benchmark-results" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;leaderboards&#x2F;#aider-polyglot-benchma...</a><p>A teeny tiny model such as a 1.5B model is really dumb, and <i>not good</i> at interactively generating code in a conversational way, but models in the 3B or less size can do a good job of suggesting tab completions.<p>There are larger &quot;open&quot; models (in the 32B - 70B range) that you can run locally that should be much, much better than gpt-4o-mini at just about everything, including writing code. For a few examples, llama3.3-70b-instruct and qwen2.5-coder-32b-instruct are pretty good. If you&#x27;re really pressed for RAM, qwen2.5-coder-7b-instruct or codegemma-7b-it might be okay for some simple things.<p>&gt; medium specced macbook pro<p>medium specced doesn&#x27;t mean much. How much RAM do you have? Each &quot;B&quot; (billion) of parameters is going to require <i>about</i> 1GB of RAM, as a rule of thumb. (500MB for really heavily quantized models, 2GB for un-quantized models... but, 8-bit quants use 1GB, and that&#x27;s usually fine.)</div><br/><div id="42678228" class="c"><input type="checkbox" id="c-42678228" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#42677322">root</a><span>|</span><a href="#42677879">parent</a><span>|</span><a href="#42677511">next</a><span>|</span><label class="collapse" for="c-42678228">[-]</label><label class="expand" for="c-42678228">[1 more]</label></div><br/><div class="children"><div class="content">Also context size significantly impacts ram&#x2F;vram usage and in programming those chats get big quickly</div><br/></div></div></div></div><div id="42677511" class="c"><input type="checkbox" id="c-42677511" checked=""/><div class="controls bullet"><span class="by">eric-burel</span><span>|</span><a href="#42677322">parent</a><span>|</span><a href="#42677879">prev</a><span>|</span><a href="#42680316">next</a><span>|</span><label class="collapse" for="c-42677511">[-]</label><label class="expand" for="c-42677511">[1 more]</label></div><br/><div class="children"><div class="content">Side question : open source models tend to be less &quot;smart&quot; than private ones, do you intend to compensate by providing a better context (eg query relevant technology docs to feed context)?</div><br/></div></div></div></div><div id="42680316" class="c"><input type="checkbox" id="c-42680316" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#42677322">prev</a><span>|</span><a href="#42681115">next</a><span>|</span><label class="collapse" for="c-42680316">[-]</label><label class="expand" for="c-42680316">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Toggle IDE &#x2F; Extensions telemetry<p>Cannot be turned off in the Community Edition. What does this telemetry data contain?</div><br/></div></div><div id="42681115" class="c"><input type="checkbox" id="c-42681115" checked=""/><div class="controls bullet"><span class="by">chvid</span><span>|</span><a href="#42680316">prev</a><span>|</span><a href="#42679798">next</a><span>|</span><label class="collapse" for="c-42681115">[-]</label><label class="expand" for="c-42681115">[1 more]</label></div><br/><div class="children"><div class="content">All the examples are for code that would otherwise be found in a library. Some of the code is of dubious quality.<p>LLMs - a spam bot for your codebase?</div><br/></div></div><div id="42679798" class="c"><input type="checkbox" id="c-42679798" checked=""/><div class="controls bullet"><span class="by">mlepath</span><span>|</span><a href="#42681115">prev</a><span>|</span><a href="#42677516">next</a><span>|</span><label class="collapse" for="c-42679798">[-]</label><label class="expand" for="c-42679798">[2 more]</label></div><br/><div class="children"><div class="content">Awesome project! I love the idea of not sending my data to a big company and trust their TOS.<p>The effectiveness of coding assistant is directly proportional to context length and the open models you can run on your computer are usually much smaller. Would love to see something more quantified around the usefulness on more complex codebases.</div><br/><div id="42680975" class="c"><input type="checkbox" id="c-42680975" checked=""/><div class="controls bullet"><span class="by">fullstackwife</span><span>|</span><a href="#42679798">parent</a><span>|</span><a href="#42677516">next</a><span>|</span><label class="collapse" for="c-42680975">[-]</label><label class="expand" for="c-42680975">[1 more]</label></div><br/><div class="children"><div class="content">I hope for proliferation of 100% local coding assistants, but for now the recommendation of &quot;Works best on $10K+ GPU&quot; is a show stopper, and we are forced to use the &quot;big company&quot;. :(</div><br/></div></div></div></div><div id="42677516" class="c"><input type="checkbox" id="c-42677516" checked=""/><div class="controls bullet"><span class="by">mjrpes</span><span>|</span><a href="#42679798">prev</a><span>|</span><a href="#42679488">next</a><span>|</span><label class="collapse" for="c-42677516">[-]</label><label class="expand" for="c-42677516">[5 more]</label></div><br/><div class="children"><div class="content">What is the recommended hardware? GPU required? Could this run OK on an older Ryzen APU (Zen 3 with Vega 7 graphics)?</div><br/><div id="42678044" class="c"><input type="checkbox" id="c-42678044" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#42677516">parent</a><span>|</span><a href="#42677870">next</a><span>|</span><label class="collapse" for="c-42678044">[-]</label><label class="expand" for="c-42678044">[2 more]</label></div><br/><div class="children"><div class="content">The usual bottleneck for self-hosted LLMs is memory bandwidth. It doesn&#x27;t really matter if there are integrated graphics or not... the models will run at the same (very slow) speed on CPU-only. Macs are only decent for LLMs because Apple has given Apple Silicon unusually high memory bandwidth, but they&#x27;re still nowhere near as fast as a high-end GPU with <i>extremely</i> fast VRAM.<p>For extremely tiny models like you would use for tab completion, even an old AMD CPU is probably going to do okay.</div><br/><div id="42678179" class="c"><input type="checkbox" id="c-42678179" checked=""/><div class="controls bullet"><span class="by">mjrpes</span><span>|</span><a href="#42677516">root</a><span>|</span><a href="#42678044">parent</a><span>|</span><a href="#42677870">next</a><span>|</span><label class="collapse" for="c-42678179">[-]</label><label class="expand" for="c-42678179">[1 more]</label></div><br/><div class="children"><div class="content">Good to know. It also looks like you can host TabbyML as an on-premise server with docker and serve requests over a private network. Interesting to think that a self-hosted GPU server might become a thing.</div><br/></div></div></div></div><div id="42677870" class="c"><input type="checkbox" id="c-42677870" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42677516">parent</a><span>|</span><a href="#42678044">prev</a><span>|</span><a href="#42679488">next</a><span>|</span><label class="collapse" for="c-42677870">[-]</label><label class="expand" for="c-42677870">[2 more]</label></div><br/><div class="children"><div class="content">Check <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;s&#x2F;lznmkWJhAZ" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;s&#x2F;lznmkWJhAZ</a> to see a local setup with 3090.</div><br/><div id="42678649" class="c"><input type="checkbox" id="c-42678649" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42677516">root</a><span>|</span><a href="#42677870">parent</a><span>|</span><a href="#42679488">next</a><span>|</span><label class="collapse" for="c-42678649">[-]</label><label class="expand" for="c-42678649">[1 more]</label></div><br/><div class="children"><div class="content">That thread doesn&#x27;t seem to mention hardware.  It would be really helpful to just put hardware requirements in the GitHub README.</div><br/></div></div></div></div></div></div><div id="42679488" class="c"><input type="checkbox" id="c-42679488" checked=""/><div class="controls bullet"><span class="by">mindcrime</span><span>|</span><a href="#42677516">prev</a><span>|</span><a href="#42678312">next</a><span>|</span><label class="collapse" for="c-42679488">[-]</label><label class="expand" for="c-42679488">[1 more]</label></div><br/><div class="children"><div class="content">Very cool. I&#x27;m especially happy to see that there is an Eclipse client[1]. One note though: I had to dig around a bit to find the info about the Eclipse client. It&#x27;s not mentioned in the main readme, or in the list of IDE extensions in the docs. Not sure if that&#x27;s an oversight or because it&#x27;s not &quot;ready for prime time&quot; yet or what.<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;TabbyML&#x2F;tabby&#x2F;tree&#x2F;3bd73a8c59a1c21312e8123aa6817d4f2e096041&#x2F;clients&#x2F;eclipse">https:&#x2F;&#x2F;github.com&#x2F;TabbyML&#x2F;tabby&#x2F;tree&#x2F;3bd73a8c59a1c21312e812...</a></div><br/></div></div><div id="42678312" class="c"><input type="checkbox" id="c-42678312" checked=""/><div class="controls bullet"><span class="by">SOLAR_FIELDS</span><span>|</span><a href="#42679488">prev</a><span>|</span><a href="#42677831">next</a><span>|</span><label class="collapse" for="c-42678312">[-]</label><label class="expand" for="c-42678312">[4 more]</label></div><br/><div class="children"><div class="content">&gt; How to utilize multiple NVIDIA GPUs?<p>| Tabby only supports the use of a single GPU. To utilize multiple GPUs, you can initiate multiple Tabby instances and set CUDA_VISIBLE_DEVICES (for cuda) or HIP_VISIBLE_DEVICES (for rocm) accordingly.<p>So using 2 NVLinked GPU&#x27;s with inference is not supported? Or is that situation different because NVLink treats the two GPU as a single one?</div><br/><div id="42678743" class="c"><input type="checkbox" id="c-42678743" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42678312">parent</a><span>|</span><a href="#42677831">next</a><span>|</span><label class="collapse" for="c-42678743">[-]</label><label class="expand" for="c-42678743">[3 more]</label></div><br/><div class="children"><div class="content">&gt; So using 2 NVLinked GPU&#x27;s with inference is not supported?<p>To make better use of multiple GPUs, we suggest employing a dedicated backend for serving the model. Please refer to <a href="https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;docs&#x2F;references&#x2F;models-http-api&#x2F;vllm&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;docs&#x2F;references&#x2F;models-http-api&#x2F;vl...</a> for an example</div><br/><div id="42679665" class="c"><input type="checkbox" id="c-42679665" checked=""/><div class="controls bullet"><span class="by">SOLAR_FIELDS</span><span>|</span><a href="#42678312">root</a><span>|</span><a href="#42678743">parent</a><span>|</span><a href="#42677831">next</a><span>|</span><label class="collapse" for="c-42679665">[-]</label><label class="expand" for="c-42679665">[2 more]</label></div><br/><div class="children"><div class="content">I see. So this is like, I can have tabby be my LLM server with this limitation or I can just turn that feature off and point tabby at my self hosted LLM as any other OpenAI compatible endpoint?</div><br/><div id="42679801" class="c"><input type="checkbox" id="c-42679801" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42678312">root</a><span>|</span><a href="#42679665">parent</a><span>|</span><a href="#42677831">next</a><span>|</span><label class="collapse" for="c-42679801">[-]</label><label class="expand" for="c-42679801">[1 more]</label></div><br/><div class="children"><div class="content">Yes - however, the FIM model requires careful configuration to properly set the prompt template.</div><br/></div></div></div></div></div></div></div></div><div id="42677831" class="c"><input type="checkbox" id="c-42677831" checked=""/><div class="controls bullet"><span class="by">leke</span><span>|</span><a href="#42678312">prev</a><span>|</span><a href="#42678942">next</a><span>|</span><label class="collapse" for="c-42677831">[-]</label><label class="expand" for="c-42677831">[2 more]</label></div><br/><div class="children"><div class="content">So does this run on your personal machine, or can you install it on a local company server and have everyone in the company connect to it?</div><br/><div id="42677857" class="c"><input type="checkbox" id="c-42677857" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42677831">parent</a><span>|</span><a href="#42678942">next</a><span>|</span><label class="collapse" for="c-42677857">[-]</label><label class="expand" for="c-42677857">[1 more]</label></div><br/><div class="children"><div class="content">Tabby is engineered for team usage, intended to be deployed on a shared server. However, with robust local computing resources, you can also run Tabby on your individual machine. Check <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;s&#x2F;lznmkWJhAZ" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;s&#x2F;lznmkWJhAZ</a> to see a local setup with 3090.</div><br/></div></div></div></div><div id="42678942" class="c"><input type="checkbox" id="c-42678942" checked=""/><div class="controls bullet"><span class="by">KronisLV</span><span>|</span><a href="#42677831">prev</a><span>|</span><a href="#42680056">next</a><span>|</span><label class="collapse" for="c-42678942">[-]</label><label class="expand" for="c-42678942">[1 more]</label></div><br/><div class="children"><div class="content">For something similar I use Continue.dev with ollama, it’s always nice to see more tools in the space! But as usual, you need pretty formidable hardware to run the actually good models, like the 32B version of Qwen2.5-coder.</div><br/></div></div><div id="42680056" class="c"><input type="checkbox" id="c-42680056" checked=""/><div class="controls bullet"><span class="by">trevor-e</span><span>|</span><a href="#42678942">prev</a><span>|</span><a href="#42678110">next</a><span>|</span><label class="collapse" for="c-42680056">[-]</label><label class="expand" for="c-42680056">[2 more]</label></div><br/><div class="children"><div class="content">fyi the pricing page has a typo for &quot;Singel Sign-On&quot;</div><br/><div id="42680750" class="c"><input type="checkbox" id="c-42680750" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42680056">parent</a><span>|</span><a href="#42678110">next</a><span>|</span><label class="collapse" for="c-42680750">[-]</label><label class="expand" for="c-42680750">[1 more]</label></div><br/><div class="children"><div class="content">Appreciated! Fixed</div><br/></div></div></div></div><div id="42678110" class="c"><input type="checkbox" id="c-42678110" checked=""/><div class="controls bullet"><span class="by">thedangler</span><span>|</span><a href="#42680056">prev</a><span>|</span><a href="#42680118">next</a><span>|</span><label class="collapse" for="c-42678110">[-]</label><label class="expand" for="c-42678110">[2 more]</label></div><br/><div class="children"><div class="content">How would I tell this to use an api framework it doesn’t know ?</div><br/><div id="42678671" class="c"><input type="checkbox" id="c-42678671" checked=""/><div class="controls bullet"><span class="by">wsxiaoys</span><span>|</span><a href="#42678110">parent</a><span>|</span><a href="#42680118">next</a><span>|</span><label class="collapse" for="c-42678671">[-]</label><label class="expand" for="c-42678671">[1 more]</label></div><br/><div class="children"><div class="content">Tabby comes with builtin RAG support so you can add this api framework to it.<p>Example: <a href="https:&#x2F;&#x2F;demo.tabbyml.com&#x2F;search&#x2F;how-to-configure-sso-in-tabby-wbdVW1" rel="nofollow">https:&#x2F;&#x2F;demo.tabbyml.com&#x2F;search&#x2F;how-to-configure-sso-in-tabb...</a><p>Settings page: <a href="https:&#x2F;&#x2F;demo.tabbyml.com&#x2F;settings&#x2F;providers&#x2F;doc" rel="nofollow">https:&#x2F;&#x2F;demo.tabbyml.com&#x2F;settings&#x2F;providers&#x2F;doc</a></div><br/></div></div></div></div><div id="42680118" class="c"><input type="checkbox" id="c-42680118" checked=""/><div class="controls bullet"><span class="by">nbzso</span><span>|</span><a href="#42678110">prev</a><span>|</span><a href="#42677965">next</a><span>|</span><label class="collapse" for="c-42680118">[-]</label><label class="expand" for="c-42680118">[1 more]</label></div><br/><div class="children"><div class="content">I will go out on a limb and predict that in the next 10 years AI code assistant will be forbidden:)</div><br/></div></div><div id="42677965" class="c"><input type="checkbox" id="c-42677965" checked=""/><div class="controls bullet"><span class="by">d--b</span><span>|</span><a href="#42680118">prev</a><span>|</span><a href="#42678925">next</a><span>|</span><label class="collapse" for="c-42677965">[-]</label><label class="expand" for="c-42677965">[1 more]</label></div><br/><div class="children"><div class="content">Didn’t you mean to name it Spacey?</div><br/></div></div><div id="42677632" class="c"><input type="checkbox" id="c-42677632" checked=""/><div class="controls bullet"><span class="by">jslakro</span><span>|</span><a href="#42678925">prev</a><span>|</span><a href="#42676768">next</a><span>|</span><label class="collapse" for="c-42677632">[-]</label><label class="expand" for="c-42677632">[3 more]</label></div><br/><div class="children"><div class="content">Duplicated <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35470915">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35470915</a></div><br/><div id="42678609" class="c"><input type="checkbox" id="c-42678609" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42677632">parent</a><span>|</span><a href="#42676768">next</a><span>|</span><label class="collapse" for="c-42678609">[-]</label><label class="expand" for="c-42678609">[2 more]</label></div><br/><div class="children"><div class="content">Not a dupe, as that was nearly two years ago.  <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsfaq.html#reposts">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsfaq.html#reposts</a></div><br/><div id="42679858" class="c"><input type="checkbox" id="c-42679858" checked=""/><div class="controls bullet"><span class="by">jslakro</span><span>|</span><a href="#42677632">root</a><span>|</span><a href="#42678609">parent</a><span>|</span><a href="#42676768">next</a><span>|</span><label class="collapse" for="c-42679858">[-]</label><label class="expand" for="c-42679858">[1 more]</label></div><br/><div class="children"><div class="content">In that case I&#x27;m going to start reposting all good old links.</div><br/></div></div></div></div></div></div><div id="42676768" class="c"><input type="checkbox" id="c-42676768" checked=""/><div class="controls bullet"><span class="by">thecal</span><span>|</span><a href="#42677632">prev</a><span>|</span><label class="collapse" for="c-42676768">[-]</label><label class="expand" for="c-42676768">[6 more]</label></div><br/><div class="children"><div class="content">Unfortunate name. Can you connect Tabby to the OpenAI-compatible TabbyAPI? <a href="https:&#x2F;&#x2F;github.com&#x2F;theroyallab&#x2F;tabbyAPI">https:&#x2F;&#x2F;github.com&#x2F;theroyallab&#x2F;tabbyAPI</a></div><br/><div id="42677365" class="c"><input type="checkbox" id="c-42677365" checked=""/><div class="controls bullet"><span class="by">Medox</span><span>|</span><a href="#42676768">parent</a><span>|</span><a href="#42677041">next</a><span>|</span><label class="collapse" for="c-42677365">[-]</label><label class="expand" for="c-42677365">[2 more]</label></div><br/><div class="children"><div class="content">I though that Tabby, the ssh client [1], got AI capabilities...<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;Eugeny&#x2F;tabby">https:&#x2F;&#x2F;github.com&#x2F;Eugeny&#x2F;tabby</a></div><br/></div></div><div id="42677041" class="c"><input type="checkbox" id="c-42677041" checked=""/><div class="controls bullet"><span class="by">mbernstein</span><span>|</span><a href="#42676768">parent</a><span>|</span><a href="#42677365">prev</a><span>|</span><label class="collapse" for="c-42677041">[-]</label><label class="expand" for="c-42677041">[3 more]</label></div><br/><div class="children"><div class="content">At least per Github, the TabbyML project is older than the TabbyAPI project.</div><br/><div id="42677300" class="c"><input type="checkbox" id="c-42677300" checked=""/><div class="controls bullet"><span class="by">mynameisvlad</span><span>|</span><a href="#42676768">root</a><span>|</span><a href="#42677041">parent</a><span>|</span><label class="collapse" for="c-42677300">[-]</label><label class="expand" for="c-42677300">[2 more]</label></div><br/><div class="children"><div class="content">Also, <i>wildly</i> more popular, to the tune of several magnitudes more forks and stars. If anything, this question should be asked of the TabbyAPI project.</div><br/><div id="42677569" class="c"><input type="checkbox" id="c-42677569" checked=""/><div class="controls bullet"><span class="by">karolist</span><span>|</span><a href="#42676768">root</a><span>|</span><a href="#42677300">parent</a><span>|</span><label class="collapse" for="c-42677569">[-]</label><label class="expand" for="c-42677569">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what&#x27;s going on with TabbyAPI&#x27;s github metrics, but exl2 quants are very popular among nvidia local LLM crowd and TabbyAPI comes in tons of reddit posts of people using it. Might be just my bubble, not saying they&#x27;re not accurate, just generally surprised such a useful project has under 1k stars. On the flip side, LLMs will hallucinate about TabbyML if you ask it TabbyAPI related questions, so I&#x27;d agree the naming is unfortunate.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>