<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683363668492" as="style"/><link rel="stylesheet" href="styles.css?v=1683363668492"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2305.01625">Unlimiformer: Long-Range Transformers with Unlimited Length Input</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>shishy</span> | <span>73 comments</span></div><br/><div><div id="35833502" class="c"><input type="checkbox" id="c-35833502" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#35834924">next</a><span>|</span><label class="collapse" for="c-35833502">[-]</label><label class="expand" for="c-35833502">[25 more]</label></div><br/><div class="children"><div class="content">1. This is not exact attention, but an approximation of it. Specifically, they use k-nearest neighbors to retrieve the top-k most similar tokens, out of an &quot;unlimited-length input&quot; say of size N, where k &lt;&lt; N.<p>2. This idea is quite similar to retrieval transformers and Hopfield networks which have been known and published for several years now. It&#x27;s not really that novel.<p>3. Due to the preceding points, the title can easily mislead people. It&#x27;s not really a conventional transformer, and it&#x27;s not a breakthrough.<p>4. This paper is a preprint and not peer-reviewed.<p>&quot;I generally don&#x27;t enjoy seeing preprints like this going to the top of Hacker News. This would be a higher quality submission if the paper was peer-reviewed or put into a greater context, like a blog post discussion or something like that.&quot;<p>Let me retract this and say something a bit nicer :) 
I personally think there this specific preprint making it to the top of HN is potentially harmful, because of the hype around LLMs, the diverse audience of readers here, and the specific title that implies a claim of &quot;transformer with unlimited context length&quot;, when this is misleading. I don&#x27;t have anything against preprints in general - a lot of work outside of the peer-review process ends up being very impactful.</div><br/><div id="35834332" class="c"><input type="checkbox" id="c-35834332" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#35833502">parent</a><span>|</span><a href="#35834581">next</a><span>|</span><label class="collapse" for="c-35834332">[-]</label><label class="expand" for="c-35834332">[1 more]</label></div><br/><div class="children"><div class="content">After a very quick read, that&#x27;s my understanding too: It&#x27;s just KNN search with some bells and whistles. So I agree on points 1-3.<p>When something works well, I don&#x27;t care much about point 4.<p>Personally, I&#x27;ve had only mixed success with KNN search on long sequences. Maybe I haven&#x27;t done it right? I don&#x27;t know. In my experience, nothing seems to work quite as well as explicit token-token interactions by some form of attention, which as we all know is too costly for long sequences (O(n²)). Lately I&#x27;ve been playing with <a href="https:&#x2F;&#x2F;github.com&#x2F;hazyresearch&#x2F;safari">https:&#x2F;&#x2F;github.com&#x2F;hazyresearch&#x2F;safari</a> , which uses a lot less compute and seems promising, though it reminds me of things like FNet. Otherwise, for long sequences I&#x27;ve yet to find something better than <a href="https:&#x2F;&#x2F;github.com&#x2F;HazyResearch&#x2F;flash-attention">https:&#x2F;&#x2F;github.com&#x2F;HazyResearch&#x2F;flash-attention</a> for n×n interactions and <a href="https:&#x2F;&#x2F;github.com&#x2F;glassroom&#x2F;heinsen_routing">https:&#x2F;&#x2F;github.com&#x2F;glassroom&#x2F;heinsen_routing</a> for n×m interactions. If anyone has other suggestions, I&#x27;d love to hear about them.</div><br/></div></div><div id="35834581" class="c"><input type="checkbox" id="c-35834581" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35833502">parent</a><span>|</span><a href="#35834332">prev</a><span>|</span><a href="#35834316">next</a><span>|</span><label class="collapse" for="c-35834581">[-]</label><label class="expand" for="c-35834581">[6 more]</label></div><br/><div class="children"><div class="content">&gt; I generally don&#x27;t enjoy seeing preprints like this going to the top of Hacker News. This would be a higher quality submission if the paper was peer-reviewed or put into a greater context, like a blog post discussion or something like that.<p>This opinion seems totally backwards to me. I&#x27;m not sure what you think peer-reviewed means? Also I prefer full preprints than blog posts. But then again, I have no idea why ones like the daily blogposts of Seth Godin (to pick on one randomly, sorry it&#x27;s not personal) so often go to the top of hacker news. Maybe opinions like yours explains it?</div><br/><div id="35834640" class="c"><input type="checkbox" id="c-35834640" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834581">parent</a><span>|</span><a href="#35836524">next</a><span>|</span><label class="collapse" for="c-35834640">[-]</label><label class="expand" for="c-35834640">[3 more]</label></div><br/><div class="children"><div class="content">&gt; This opinion seems totally backwards to me.<p>I agree.<p>&gt; I&#x27;m not sure what you think peer-reviewed means?<p>Posting to HN is a form of peer-review, typically far better than the form of &quot;peer-review&quot; coopted by journal publishers.</div><br/><div id="35834696" class="c"><input type="checkbox" id="c-35834696" checked=""/><div class="controls bullet"><span class="by">pyth0</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834640">parent</a><span>|</span><a href="#35835676">next</a><span>|</span><label class="collapse" for="c-35834696">[-]</label><label class="expand" for="c-35834696">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Posting to HN is a form of peer-review, typically far better than the form of &quot;peer-review&quot; coopted by journal publishers.<p>This is a rather self-aggrandizing view, and I think it speaks to the level of ego that underpins a lot of the discussion on here.</div><br/></div></div><div id="35835676" class="c"><input type="checkbox" id="c-35835676" checked=""/><div class="controls bullet"><span class="by">xg15</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834640">parent</a><span>|</span><a href="#35834696">prev</a><span>|</span><a href="#35836524">next</a><span>|</span><label class="collapse" for="c-35835676">[-]</label><label class="expand" for="c-35835676">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s redefining what &quot;peer-review&quot; is. And I&#x27;ll take credentialism over some board of anonymous internet people, I&#x27;m sorry.<p>I mean, hypothetically, this whole thread could be stuffed with sock puppet accounts of the author. How would you know?</div><br/></div></div></div></div><div id="35836524" class="c"><input type="checkbox" id="c-35836524" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834581">parent</a><span>|</span><a href="#35834640">prev</a><span>|</span><a href="#35834316">next</a><span>|</span><label class="collapse" for="c-35836524">[-]</label><label class="expand" for="c-35836524">[2 more]</label></div><br/><div class="children"><div class="content">I got let in on the secret of what &quot;peer review&quot; actually looks like for microbiology papers, and... it&#x27;s just three PhD students in a trenchcoat, posing as the PI who&#x27;s too busy reviewing papers.<p>Except of course for Nature-level papers, but most people never get to review papers like that</div><br/><div id="35838913" class="c"><input type="checkbox" id="c-35838913" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35836524">parent</a><span>|</span><a href="#35834316">next</a><span>|</span><label class="collapse" for="c-35838913">[-]</label><label class="expand" for="c-35838913">[1 more]</label></div><br/><div class="children"><div class="content">PhD students are typically more knowledgeable than the PI, so this is probably optimal. Perhaps a good Postdoc is  better.  PIs are often way outdated in their knowledge and don&#x27;t have the time or energy to stay abreast the developments in the field.  That&#x27;s what PhDs are for - they have to prove themselves in developing what should be their most important work of their lifetime before switching to what is essentially management, rather than science, unfortunately.</div><br/></div></div></div></div></div></div><div id="35834316" class="c"><input type="checkbox" id="c-35834316" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35833502">parent</a><span>|</span><a href="#35834581">prev</a><span>|</span><a href="#35833881">next</a><span>|</span><label class="collapse" for="c-35834316">[-]</label><label class="expand" for="c-35834316">[6 more]</label></div><br/><div class="children"><div class="content">There&#x27;s nothing really wrong with a preprint making it to the top - there can be genuinely good work that stays in preprint for quite some time.
I believe the original ELMo work that spurred the Sesame street gang is <i>still</i> in preprint despite its importance in NLP (:shocked Pikachu face: not a transformer?!).<p>But yes, you&#x27;re correct in this instance that it&#x27;s not necessarily &#x27;huge news&#x27; since it is highly similar to a long list of the Reformer (LSH-based), Performer (FAVOR**), FNet (Fourier-based), Routing Transformer, Sparse Transformer, Longformer (task specific sparse), blockbert,  XLNet&#x2F;xfmr-xl (slide + relative PE), BP-Transformer (binary partition), BigBird (global and rand attn),  RWKV which is..., etc.<p>** FAVOR actually is innovative and different in this space, but towards <i>similar</i> ends anyway</div><br/><div id="35834977" class="c"><input type="checkbox" id="c-35834977" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834316">parent</a><span>|</span><a href="#35833881">next</a><span>|</span><label class="collapse" for="c-35834977">[-]</label><label class="expand" for="c-35834977">[5 more]</label></div><br/><div class="children"><div class="content">How come you know the efficient-transformers family, when I ask questions about transformers in ML interviews nobody has heard of them. Can&#x27;t figure out why it&#x27;s not common knowledge. For years all the transformer papers were about reducing O( N^2 )</div><br/><div id="35838599" class="c"><input type="checkbox" id="c-35838599" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834977">parent</a><span>|</span><a href="#35838941">next</a><span>|</span><label class="collapse" for="c-35838599">[-]</label><label class="expand" for="c-35838599">[1 more]</label></div><br/><div class="children"><div class="content">Two reasons: They were only recently implemented in large production models and are not part of ye-standard-ML-coursera.
And I mean, for half the papers claiming that their particular efficiency variant reduces O(n^2) to whatever without performance loss, we found that in practice it ain&#x27;t quite so shiny.<p>Anyone who has been for whatever reason reading the papers since 2017 has invariably read dozens of these papers.<p>Anyone who has heard of GPT-x in 202x and started from there probably didn&#x27;t.<p>This will likely change with implementation of memory retrieval, some form of linear attention etc. in many productions models, and the democratization of some decoder models... although I have been thinking this for a while.<p>Don&#x27;t get me wrong, you want to hire the people who know these papers, especially if they started after 2017 :-)</div><br/></div></div><div id="35838941" class="c"><input type="checkbox" id="c-35838941" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834977">parent</a><span>|</span><a href="#35838599">prev</a><span>|</span><a href="#35835915">next</a><span>|</span><label class="collapse" for="c-35838941">[-]</label><label class="expand" for="c-35838941">[1 more]</label></div><br/><div class="children"><div class="content">The reason they don&#x27;t know them is they&#x27;re not serious researchers or practitioners of ML - it&#x27;s as simple as that.  Anyone in this area should have this exceedingly basic common knowledge.</div><br/></div></div><div id="35835915" class="c"><input type="checkbox" id="c-35835915" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834977">parent</a><span>|</span><a href="#35838941">prev</a><span>|</span><a href="#35836838">next</a><span>|</span><label class="collapse" for="c-35835915">[-]</label><label class="expand" for="c-35835915">[1 more]</label></div><br/><div class="children"><div class="content">To be fair ML is (used to be?) pretty broad, so unless someone is actively keeping up with the sota in the high-data sequence modeling area it&#x27;s quite possible to miss. I know ML teams which were entirely made up of OSML practicioners, because that was the most commonly useful until recently.</div><br/></div></div><div id="35836838" class="c"><input type="checkbox" id="c-35836838" checked=""/><div class="controls bullet"><span class="by">Nimitz14</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834977">parent</a><span>|</span><a href="#35835915">prev</a><span>|</span><a href="#35833881">next</a><span>|</span><label class="collapse" for="c-35836838">[-]</label><label class="expand" for="c-35836838">[1 more]</label></div><br/><div class="children"><div class="content">Why learn something noone is using.</div><br/></div></div></div></div></div></div><div id="35833881" class="c"><input type="checkbox" id="c-35833881" checked=""/><div class="controls bullet"><span class="by">dhruvdh</span><span>|</span><a href="#35833502">parent</a><span>|</span><a href="#35834316">prev</a><span>|</span><a href="#35835193">next</a><span>|</span><label class="collapse" for="c-35833881">[-]</label><label class="expand" for="c-35833881">[5 more]</label></div><br/><div class="children"><div class="content">I generally don&#x27;t enjoy something being diminished on account of being &quot;not really that novel&quot;.<p>Your comment essentially says - this is not a high quality submission because readers might not actually read it, which is no fault of the work, or submitter.</div><br/><div id="35834047" class="c"><input type="checkbox" id="c-35834047" checked=""/><div class="controls bullet"><span class="by">MasterScrat</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35833881">parent</a><span>|</span><a href="#35834208">next</a><span>|</span><label class="collapse" for="c-35834047">[-]</label><label class="expand" for="c-35834047">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Your comment essentially says - this is not a high quality submission because readers might not actually read it<p>I&#x27;d argue that on average, most readers won&#x27;t have a good enough understanding, or read the paper far enough, to understand that the reality is closer to &quot;it&#x27;s not a breakthrough&quot; rather than &quot;Transformers with Unlimited Length Input&quot;.<p>So, I wholeheartedly welcome this type of hype-breaking leading comment.</div><br/><div id="35834464" class="c"><input type="checkbox" id="c-35834464" checked=""/><div class="controls bullet"><span class="by">jjoonathan</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834047">parent</a><span>|</span><a href="#35834481">next</a><span>|</span><label class="collapse" for="c-35834464">[-]</label><label class="expand" for="c-35834464">[1 more]</label></div><br/><div class="children"><div class="content">Agreed 100%. Not only do I appreciate &quot;well actually&quot; comments, I think they are the single most useful aspect of forum discussions.<p>The headline will always be &quot;BATTERY BREAKTHROUGH PROMISES TO ROCKET ELON MUSK TESLA TO THE MOON!!!&quot; and while it&#x27;s easy to know that <i>some</i> amount of cold water is necessary you need to spend a nontrivial amount of attention and have a nontrivial amount of knowledge to figure out just how much cold water. It&#x27;s a useful thing to outsource. Did a research group see outperformance in an experiment with 1% probability of translating into production? Or is CATL scaling up a production process? The &quot;well actually&quot; comment will contextualize for you. If there&#x27;s a &quot;well actually&quot; reply to the &quot;well actually&quot; comment, that tells you something too. Upvotes&#x2F;downvotes dial in the distributed consensus.<p>It&#x27;s far from perfect, but I&#x27;d challenge detractors to point to a more effective method for large-scale democratic truth seeking.</div><br/></div></div><div id="35834481" class="c"><input type="checkbox" id="c-35834481" checked=""/><div class="controls bullet"><span class="by">swores</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35834047">parent</a><span>|</span><a href="#35834464">prev</a><span>|</span><a href="#35834208">next</a><span>|</span><label class="collapse" for="c-35834481">[-]</label><label class="expand" for="c-35834481">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible to approve of the &quot;hype-breaking&quot; (aka TLDRing &#x2F; ELI5ing so that HN comment readers can understand the degree to which it&#x27;s interesting for those of us not close enough to the field to understand that for ourselves) without agreeing that that same comment should also complain that preprints shouldn&#x27;t be submitted to &#x2F; upvoted on HN.<p>That&#x27;s how I feel, anyway. I&#x27;d rather have seen a comment that has the same explanations in it but just generally less grumpy! Saying stuff like &quot;It&#x27;s not really that novel.&quot; doesn&#x27;t really contribute much, when it could either be explained why it isn&#x27;t novel by explaining how similar it is to something earlier that can be referenced, or thinking about what if anything <i>is</i> novel in this research - assuming it isn&#x27;t being accused of just replicating something already done.</div><br/></div></div></div></div><div id="35834208" class="c"><input type="checkbox" id="c-35834208" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35833881">parent</a><span>|</span><a href="#35834047">prev</a><span>|</span><a href="#35835193">next</a><span>|</span><label class="collapse" for="c-35834208">[-]</label><label class="expand" for="c-35834208">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t have to be someone&#x27;s fault to not be a good suited submission.</div><br/></div></div></div></div><div id="35835193" class="c"><input type="checkbox" id="c-35835193" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#35833502">parent</a><span>|</span><a href="#35833881">prev</a><span>|</span><a href="#35833535">next</a><span>|</span><label class="collapse" for="c-35835193">[-]</label><label class="expand" for="c-35835193">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, these complaints (other than 4) apply to the vast majority of papers. #4 is just false. It has already been viewed by other lab members (peers) and open publication is peer reviewing. The &quot;peer review system&quot; (publishing to conferences&#x2F;journals) is relatively new and I think ML demonstrates all the problems with the system (yay hype).<p>Novelty is especially a joke. ViTs are &quot;just&quot; NLP encoding transformers. T2I models are &quot;just&quot; NLP models connected to generative models. Diffusion models are &quot;just&quot; whitening models. GPT3 is just GPT2 with more layers and more data which is just GPT with more layers and more data. We can go even deeper if we pull from math and physics works. But that doesn&#x27;t mean these works haven&#x27;t been highly fruitful and useful. I&#x27;m happy all of these have been published.<p>&gt; because of the hype around LLMs<p>I too hate the hype, but it is often bimodal. There are people who are far too critical and people who are far too accepting. The harm is not preprints or people reading papers, the harm is people who have no business&#x2F;qualifications evaluating works confidently spouting out critiques. It is people not understanding that researchers are just critical of one another&#x27;s work by default and that doesn&#x27;t mean it shouldn&#x27;t have been published.<p>It is well known that reviewers are good at identifying bad papers but not good at identifying good papers[0,1]. Which let&#x27;s be honest, that means reviewers just have high reject rates in a noisy system. Making publication as a metric for merit a highly noisy one at best.<p>As for the paper:<p>Many LLMs and large models are using attention approximations. Nor is the kNN technique particularly new. My main complaints are a lack of comparisons for Figure 3 and 4, but I&#x27;m not a NLP person so I don&#x27;t even know if there&#x27;s some other good works that can compare better (BART is a common baseline). But generative models are (unfortunately not notoriously known) extremely difficult to evaluate. Paper seems fine to me. It is useful to the community. I don&#x27;t like the name either, but their input is limited by computer memory, not the model. I would want to see more on this. Not a NLP person all I can say is that this looks neither like a strong reject nor a strong accept. I&#x27;ll leave it to the community to determine if they want more experiments for the conference publication or not but the work seems useful.<p>[0] <a href="https:&#x2F;&#x2F;inverseprobability.com&#x2F;talks&#x2F;notes&#x2F;the-neurips-experiment-snsf.html" rel="nofollow">https:&#x2F;&#x2F;inverseprobability.com&#x2F;talks&#x2F;notes&#x2F;the-neurips-exper...</a><p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2109.09774" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2109.09774</a></div><br/></div></div><div id="35833535" class="c"><input type="checkbox" id="c-35833535" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#35833502">parent</a><span>|</span><a href="#35835193">prev</a><span>|</span><a href="#35834374">next</a><span>|</span><label class="collapse" for="c-35833535">[-]</label><label class="expand" for="c-35833535">[4 more]</label></div><br/><div class="children"><div class="content">&gt; This idea is quite similar to retrieval transformers and Hopfield networks which have been known and published for several years now. It&#x27;s not really that novel.<p>Is it? I had thought retrieval transformers &quot;merely&quot; used retrieval as a backend of sorts rather than a substitute for the attention itself?</div><br/><div id="35833815" class="c"><input type="checkbox" id="c-35833815" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35833535">parent</a><span>|</span><a href="#35834374">next</a><span>|</span><label class="collapse" for="c-35833815">[-]</label><label class="expand" for="c-35833815">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, RETRO [0] embeds all an entire question&#x2F;prompt, and searches for similar text passages with k-NN, then does further processing. This can kind of be understood as attention on paragraphs.
This preprint instead does k-NN and calls it attention on single tokens. So not the same. But similar.<p>[0] <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-retrieval-transformer&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-retrieval-transformer...</a></div><br/><div id="35833854" class="c"><input type="checkbox" id="c-35833854" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35833815">parent</a><span>|</span><a href="#35833897">next</a><span>|</span><label class="collapse" for="c-35833854">[-]</label><label class="expand" for="c-35833854">[1 more]</label></div><br/><div class="children"><div class="content">Ah, I see - thanks for the clarification.</div><br/></div></div><div id="35833897" class="c"><input type="checkbox" id="c-35833897" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#35833502">root</a><span>|</span><a href="#35833815">parent</a><span>|</span><a href="#35833854">prev</a><span>|</span><a href="#35834374">next</a><span>|</span><label class="collapse" for="c-35833897">[-]</label><label class="expand" for="c-35833897">[1 more]</label></div><br/><div class="children"><div class="content">retro doesn&#x27;t attend itself, which is a big difference</div><br/></div></div></div></div></div></div></div></div><div id="35834924" class="c"><input type="checkbox" id="c-35834924" checked=""/><div class="controls bullet"><span class="by">GistNoesis</span><span>|</span><a href="#35833502">prev</a><span>|</span><a href="#35833233">next</a><span>|</span><label class="collapse" for="c-35834924">[-]</label><label class="expand" for="c-35834924">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve read the paper quickly, the main idea is simple and interesting, but maybe a little dubious (it&#x27;s kind of an accuracy for memory trade-off).<p>In the transformer architecture one has to compute QKT.<p>QKT=(hd * Wq * WkT)heT  (equation (2) page 3 in the paper).<p>Where hd is the hidden state of the decoder, and he is the hidden state of the encoder, and Wq and Wd are some parameters matrices, and T denotes the transposition operation.<p>By grouping the calculation this way, in a transformer encoder-decoder architecture, they can build and use only a single index (you index the he vectors using a vector database) for all the decoder layers queries. Instead of having to build 2 * L * H indices (with L the number of layers of the decoder and H the number of head in the decoder).<p>But what makes it a little dubious, is that this transformation mean you make your near neighbor queries in a space of dimension &quot;dimension of the hidden state&quot;, instead of &quot;dimension of a head&quot; that is H times smaller.<p>So if you had to build 2 * L * H indices each index would be H times smaller.<p>So you only gain a factor 2 * L. But the trade-off is that you are doing a near neighbor search in higher dimension where you are then subjected to the curse of dimensionality (the higher the dimension the more similar all points are to each other). Whereas the whole point of projections in transformer is to lower the dimension so that the knn search make more sense. So to get the same accuracy, your near-neighbor search engine will have to work a lot harder.<p>Also as an approximation of the transformer, because it&#x27;s using some knn search, it comes with the problems associated with it (for example harder to train because more sparse, and a tendency to hyperfocus), but it can be complemented with low-rank linearization of the attention to also have the neural net act on the gist rather than the closest neighbors.</div><br/><div id="35836293" class="c"><input type="checkbox" id="c-35836293" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#35834924">parent</a><span>|</span><a href="#35835474">next</a><span>|</span><label class="collapse" for="c-35836293">[-]</label><label class="expand" for="c-35836293">[2 more]</label></div><br/><div class="children"><div class="content">This is a nitpick but also it&#x27;s been a few years since I was taking academic ML and linear algebra courses. However regarding this part.<p>&gt; So you only gain a factor 2 * L. But the trade-off is that you are doing a near neighbor search in higher dimension where you are then subjected to the curse of dimensionality (the higher the dimension the more similar all points are to each other).<p>I thought that the curse of dimensionality meant that in higher dimension, points got farther apart</div><br/><div id="35836772" class="c"><input type="checkbox" id="c-35836772" checked=""/><div class="controls bullet"><span class="by">dwaltrip</span><span>|</span><a href="#35834924">root</a><span>|</span><a href="#35836293">parent</a><span>|</span><a href="#35835474">next</a><span>|</span><label class="collapse" for="c-35836772">[-]</label><label class="expand" for="c-35836772">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Curse_of_dimensionality" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Curse_of_dimensionality</a><p>Sounds like you are both right?</div><br/></div></div></div></div><div id="35835474" class="c"><input type="checkbox" id="c-35835474" checked=""/><div class="controls bullet"><span class="by">numeri</span><span>|</span><a href="#35834924">parent</a><span>|</span><a href="#35836293">prev</a><span>|</span><a href="#35833233">next</a><span>|</span><label class="collapse" for="c-35835474">[-]</label><label class="expand" for="c-35835474">[1 more]</label></div><br/><div class="children"><div class="content">This technique can be added on to any encoder–decoder Transformer model post-training, so the added training difficulties you mention don&#x27;t apply. It honestly is a very interesting approach to me – the main issue I see (which they discuss in the paper) is in pure latency. If you&#x27;re using a large enough vector database, it will be on the CPU, and transferring hidden states from GPU to CPU and then the embeddings back from CPU to GPU is going to eat up a ton of time.</div><br/></div></div></div></div><div id="35833233" class="c"><input type="checkbox" id="c-35833233" checked=""/><div class="controls bullet"><span class="by">space_fountain</span><span>|</span><a href="#35834924">prev</a><span>|</span><a href="#35833101">next</a><span>|</span><label class="collapse" for="c-35833233">[-]</label><label class="expand" for="c-35833233">[3 more]</label></div><br/><div class="children"><div class="content">As I understand it the approach here is to use an approximate nearest neighbor database to retrieve highly relevant tokens from across large documents using the existing attention heads. So each attention head retrieves context from entire document. They say this can work without fine tuning, but performance improves with it. This is apparently extending this piece of prior work, but they&#x27;ve managed to re-range the linear algebra of attention so they only need one database for all attention heads across all layers of the model. I&#x27;m a bit confused how attention would here for layers below the top and a bit confused about how position is encoded for tokens across a long document like this.</div><br/><div id="35834470" class="c"><input type="checkbox" id="c-35834470" checked=""/><div class="controls bullet"><span class="by">im3w1l</span><span>|</span><a href="#35833233">parent</a><span>|</span><a href="#35833101">next</a><span>|</span><label class="collapse" for="c-35834470">[-]</label><label class="expand" for="c-35834470">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand how this could work. Like if you select a small fixed  number of tokens from a large document won&#x27;t you necessarily lose a lot of important data?</div><br/><div id="35838896" class="c"><input type="checkbox" id="c-35838896" checked=""/><div class="controls bullet"><span class="by">MagicMoonlight</span><span>|</span><a href="#35833233">root</a><span>|</span><a href="#35834470">parent</a><span>|</span><a href="#35833101">next</a><span>|</span><label class="collapse" for="c-35838896">[-]</label><label class="expand" for="c-35838896">[1 more]</label></div><br/><div class="children"><div class="content">When you read a book do you remember every word? Even every chapter?<p>You only need the important concepts, not individual words</div><br/></div></div></div></div></div></div><div id="35833101" class="c"><input type="checkbox" id="c-35833101" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#35833233">prev</a><span>|</span><a href="#35835506">next</a><span>|</span><label class="collapse" for="c-35833101">[-]</label><label class="expand" for="c-35833101">[3 more]</label></div><br/><div class="children"><div class="content">I think infiniformer would&#x27;ve sounded better. The bench scores seem pretty marginal.</div><br/><div id="35833265" class="c"><input type="checkbox" id="c-35833265" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#35833101">parent</a><span>|</span><a href="#35835506">next</a><span>|</span><label class="collapse" for="c-35833265">[-]</label><label class="expand" for="c-35833265">[2 more]</label></div><br/><div class="children"><div class="content">Pretty marginal score gains once a week is all you need.</div><br/><div id="35833387" class="c"><input type="checkbox" id="c-35833387" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#35833101">root</a><span>|</span><a href="#35833265">parent</a><span>|</span><a href="#35835506">next</a><span>|</span><label class="collapse" for="c-35833387">[-]</label><label class="expand" for="c-35833387">[1 more]</label></div><br/><div class="children"><div class="content">Only so long as a) the gains are real, and not overfitting the test dataset, and b) you don&#x27;t balloon in complexity, so that stacking approaches becomes impossible to manage.<p>Point (a) is extremely hard to discern, especially when people are chasing third-significant-digit gains on common benchmarks; it&#x27;s essentially multiple-testing false discovery in action. I&#x27;ve seen whole families of methods fail to transfer to new domains...<p>Point (b) is also a real issue. As you increase the number of bells and whistles, each with their own hyperparameters with non-linear impacts on model quality, it becomes impossible to say what&#x27;s working or not.<p>In practice, i think we see some cycles of baroque incremental improvements, followed by someone spending a year stripping away the bullshit and getting something simple that outperforms the pack, essentially because it&#x27;s easier to do hyperparam search over simpler models once you figure out the bits that actually matter.</div><br/></div></div></div></div></div></div><div id="35835506" class="c"><input type="checkbox" id="c-35835506" checked=""/><div class="controls bullet"><span class="by">chrgy</span><span>|</span><a href="#35833101">prev</a><span>|</span><a href="#35833061">next</a><span>|</span><label class="collapse" for="c-35835506">[-]</label><label class="expand" for="c-35835506">[2 more]</label></div><br/><div class="children"><div class="content">In the age of transformers , lets ask a transformer to summarize this paper:<p>The Unlimiformer paper is about a new way to make computer programs that can summarize really long pieces of text. Normally, when you ask a computer program to summarize something, it can only handle a certain amount of text at once. But with Unlimiformer, the program can handle as much text as you want!<p>The way Unlimiformer works is by using a special technique called a &quot;k-nearest-neighbor index&quot; to help the program pay attention to the most important parts of the text. This makes it possible for the program to summarize even really long documents without losing important information.<p>Overall, Unlimiformer is an exciting new development in natural language processing that could make it easier for computers to understand and summarize large amounts of text.</div><br/><div id="35836550" class="c"><input type="checkbox" id="c-35836550" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#35835506">parent</a><span>|</span><a href="#35833061">next</a><span>|</span><label class="collapse" for="c-35836550">[-]</label><label class="expand" for="c-35836550">[1 more]</label></div><br/><div class="children"><div class="content">Said transformer as it handled the article&#x27;s length anyway: <i>sensible chuckle</i></div><br/></div></div></div></div><div id="35833061" class="c"><input type="checkbox" id="c-35833061" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#35835506">prev</a><span>|</span><a href="#35834198">next</a><span>|</span><label class="collapse" for="c-35833061">[-]</label><label class="expand" for="c-35833061">[3 more]</label></div><br/><div class="children"><div class="content">What does it mean for ChatGPT and likes? Can they employ this method to virtually get rid of context tokens limit?</div><br/><div id="35833360" class="c"><input type="checkbox" id="c-35833360" checked=""/><div class="controls bullet"><span class="by">Kranar</span><span>|</span><a href="#35833061">parent</a><span>|</span><a href="#35834198">next</a><span>|</span><label class="collapse" for="c-35833360">[-]</label><label class="expand" for="c-35833360">[2 more]</label></div><br/><div class="children"><div class="content">Yes it looks like it can use this method. This method is a preprocessor and post-processor that can be used on an existing GPT model to augment it to handle unlimited tokens.</div><br/><div id="35838885" class="c"><input type="checkbox" id="c-35838885" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#35833061">root</a><span>|</span><a href="#35833360">parent</a><span>|</span><a href="#35834198">next</a><span>|</span><label class="collapse" for="c-35838885">[-]</label><label class="expand" for="c-35838885">[1 more]</label></div><br/><div class="children"><div class="content">And <i>that</i> makes it pretty notable compared to all the linear attention&#x2F;retrieval schemes that didn&#x27;t pan out. Not saying this will pan out, but we&#x27;ll know more without waiting six months for the model to train.</div><br/></div></div></div></div></div></div><div id="35834198" class="c"><input type="checkbox" id="c-35834198" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#35833061">prev</a><span>|</span><a href="#35836917">next</a><span>|</span><label class="collapse" for="c-35834198">[-]</label><label class="expand" for="c-35834198">[2 more]</label></div><br/><div class="children"><div class="content">Is this how Kagi&#x27;s &quot;universal summarizer&quot; works? They wrote a lot of copy about how it&#x27;s able to summarize websites and documents of arbitrary length, while not revealing how on Earth this actually works. It <i>does</i> seem to work, though.</div><br/><div id="35836566" class="c"><input type="checkbox" id="c-35836566" checked=""/><div class="controls bullet"><span class="by">KaoruAoiShiho</span><span>|</span><a href="#35834198">parent</a><span>|</span><a href="#35836917">next</a><span>|</span><label class="collapse" for="c-35836566">[-]</label><label class="expand" for="c-35836566">[1 more]</label></div><br/><div class="children"><div class="content">Could that not just be some kind of langchain like system?</div><br/></div></div></div></div><div id="35836917" class="c"><input type="checkbox" id="c-35836917" checked=""/><div class="controls bullet"><span class="by">logophobia</span><span>|</span><a href="#35834198">prev</a><span>|</span><a href="#35836393">next</a><span>|</span><label class="collapse" for="c-35836917">[-]</label><label class="expand" for="c-35836917">[1 more]</label></div><br/><div class="children"><div class="content">An alternative which I&#x27;ve used with some succes are structured state space models: <a href="https:&#x2F;&#x2F;srush.github.io&#x2F;annotated-s4&#x2F;" rel="nofollow">https:&#x2F;&#x2F;srush.github.io&#x2F;annotated-s4&#x2F;</a>. A very different approach that works well for quite a few types of problems.</div><br/></div></div><div id="35836393" class="c"><input type="checkbox" id="c-35836393" checked=""/><div class="controls bullet"><span class="by">opportune</span><span>|</span><a href="#35836917">prev</a><span>|</span><a href="#35837991">next</a><span>|</span><label class="collapse" for="c-35836393">[-]</label><label class="expand" for="c-35836393">[1 more]</label></div><br/><div class="children"><div class="content">This seems like a definite attention optimization but I think the fundamental problem with attention is that it doesn’t handle state in a way that scales well.<p>Personally I think the RNN&#x2F;LSTM state handling approach is going to be something we revisit when trying to advance past transformers. It handles state in a way that generalizes and scales better (it should in theory learn an attention-like mechanism anyway, and state is independent of input size).<p>It may be harder to train, and require further improvements, but it really seems more like an engineering or cost problem than a theoretical one. But I’m only an amateur and not an expert. Maybe continued improvement on attention will approach generalized state handling in a way that efficiently trains better than improvements on more generalized stateful approaches improve training.</div><br/></div></div><div id="35837991" class="c"><input type="checkbox" id="c-35837991" checked=""/><div class="controls bullet"><span class="by">intalentive</span><span>|</span><a href="#35836393">prev</a><span>|</span><a href="#35835025">next</a><span>|</span><label class="collapse" for="c-35837991">[-]</label><label class="expand" for="c-35837991">[1 more]</label></div><br/><div class="children"><div class="content">The ML community keeps rediscovering the work of Steve Grossberg. This is very similar to his decades-old ART  model.</div><br/></div></div><div id="35835025" class="c"><input type="checkbox" id="c-35835025" checked=""/><div class="controls bullet"><span class="by">nephanth</span><span>|</span><a href="#35837991">prev</a><span>|</span><a href="#35833833">next</a><span>|</span><label class="collapse" for="c-35835025">[-]</label><label class="expand" for="c-35835025">[2 more]</label></div><br/><div class="children"><div class="content">Btw, why do transformers have a limit input size in the first place? I&#x27;m pretty sure the self-attention mechanisms scale (although with bad complexity) to arbitrary sizes</div><br/><div id="35835605" class="c"><input type="checkbox" id="c-35835605" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35835025">parent</a><span>|</span><a href="#35833833">next</a><span>|</span><label class="collapse" for="c-35835605">[-]</label><label class="expand" for="c-35835605">[1 more]</label></div><br/><div class="children"><div class="content">&gt;(although with bad complexity)<p>Because of exactly that.<p>Also the attention mechanism is baked in during pretraining. So whatever max context length you want increases the compute cost of training by at least a function of said &quot;bad complexity.&quot; Even just 4096 tokens of max context is much more expensive to train than 2048. So if we want models with 8k, 32k, or more context then the training costs get out of hand quickly.</div><br/></div></div></div></div><div id="35833833" class="c"><input type="checkbox" id="c-35833833" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35835025">prev</a><span>|</span><a href="#35838113">next</a><span>|</span><label class="collapse" for="c-35833833">[-]</label><label class="expand" for="c-35833833">[4 more]</label></div><br/><div class="children"><div class="content">Other times this was put on hacker news:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35823039" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35823039</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35803470" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35803470</a></div><br/><div id="35834604" class="c"><input type="checkbox" id="c-35834604" checked=""/><div class="controls bullet"><span class="by">swores</span><span>|</span><a href="#35833833">parent</a><span>|</span><a href="#35838113">next</a><span>|</span><label class="collapse" for="c-35834604">[-]</label><label class="expand" for="c-35834604">[3 more]</label></div><br/><div class="children"><div class="content">While I appreciate your intent and effort - I don&#x27;t think it&#x27;s actually useful to link to other submissions unless either they have comments (ideally only if there&#x27;s at least one interesting comment, but at least more than no comments at all), or if it&#x27;s a submission of the same subject but to a different source link - in which case it&#x27;s probably more useful to just link the alternative source, if it&#x27;s worth reading, rather than potentially split the discussion into separate comment threads if the other is empty.<p>Linking to a different submission of the same link with 0 comments doesn&#x27;t add anything.</div><br/><div id="35834678" class="c"><input type="checkbox" id="c-35834678" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35833833">root</a><span>|</span><a href="#35834604">parent</a><span>|</span><a href="#35838113">next</a><span>|</span><label class="collapse" for="c-35834678">[-]</label><label class="expand" for="c-35834678">[2 more]</label></div><br/><div class="children"><div class="content">I must have submitted it at the wrong time of day.</div><br/><div id="35834838" class="c"><input type="checkbox" id="c-35834838" checked=""/><div class="controls bullet"><span class="by">swores</span><span>|</span><a href="#35833833">root</a><span>|</span><a href="#35834678">parent</a><span>|</span><a href="#35838113">next</a><span>|</span><label class="collapse" for="c-35834838">[-]</label><label class="expand" for="c-35834838">[1 more]</label></div><br/><div class="children"><div class="content">Sure or just random luck, maybe this submission just happened to take place when the only few people who care about this subject happened to come online, or vice versa for bad luck before etc.<p>But unlike sites like Reddit, with the exception of self &#x2F; ask HN &#x2F; etc posts, nobody really pays attention to who the submitter is, so enjoy the conversation finally breaking out on it as consolation for not getting karma points, but skip linking to dead submissions :)<p>FYI, if you ever submit something that fails to get any traction &#x2F; upvotes, then I&#x27;ve seen mods say before (@dang will hopefully correct me if I&#x27;m wrong) that a) it&#x27;s OK to try submitting a second time maybe after a day or so (but not keep submitting over and over) or b) send the mods an email with a brief reason why it&#x27;s a link that should interest HN readers for it to be potentially added to a &quot;second chance pool&quot;.  Though in the case of this link, between three of you it was posted two days ago, one day ago, and today which has finally got a bit more notice, so worked out alright in the end :)</div><br/></div></div></div></div></div></div></div></div><div id="35838113" class="c"><input type="checkbox" id="c-35838113" checked=""/><div class="controls bullet"><span class="by">jfisher4024</span><span>|</span><a href="#35833833">prev</a><span>|</span><a href="#35835724">next</a><span>|</span><label class="collapse" for="c-35838113">[-]</label><label class="expand" for="c-35838113">[1 more]</label></div><br/><div class="children"><div class="content">Neubig is the real deal. I’d take this paper seriously.</div><br/></div></div><div id="35835724" class="c"><input type="checkbox" id="c-35835724" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#35838113">prev</a><span>|</span><a href="#35833438">next</a><span>|</span><label class="collapse" for="c-35835724">[-]</label><label class="expand" for="c-35835724">[2 more]</label></div><br/><div class="children"><div class="content">Input should be the Internet then.</div><br/><div id="35835863" class="c"><input type="checkbox" id="c-35835863" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35835724">parent</a><span>|</span><a href="#35833438">next</a><span>|</span><label class="collapse" for="c-35835863">[-]</label><label class="expand" for="c-35835863">[1 more]</label></div><br/><div class="children"><div class="content">Pricing: $0.1 per nano token.</div><br/></div></div></div></div><div id="35833438" class="c"><input type="checkbox" id="c-35833438" checked=""/><div class="controls bullet"><span class="by">j2kun</span><span>|</span><a href="#35835724">prev</a><span>|</span><a href="#35833311">next</a><span>|</span><label class="collapse" for="c-35833438">[-]</label><label class="expand" for="c-35833438">[6 more]</label></div><br/><div class="children"><div class="content">Appendix C of the PDF has an example book summary of The Brothers Karamazov, with noted minor factual errors<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.01625.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.01625.pdf</a><p>&gt; Unlimiformer summary:<p>&gt; The first part of the novel focuses on the question of whether or not the Russian nobleman,
Dmitri Fyodorovitch, has killed his father. In the town of Ivanovna, the lieutenant-colonel of
the Mushenkhanovitch is accused of the murder of his brother Ivanovitch. The lieutenant-incommand, Vasilyevitch, takes the form of a dog, and the two men–the two men and the woman
who are questioned by the court-martial–murphy. The two men cry out to the God of Russia for
help in their quest to save the town. The man, afraid of the wrath of the God, hands the dog a
bunch of letters that are supposed to be proof of his love for his brother. The old man–the one
who had killed his mother, and then found the letter–arrives. He reads it–asked the old man
to forgive him for the murder and then takes the dog away. The other men, all of whom are
prisoners, demand that the man confess his crime to the court. The first and most important
thing they tell the court is that they love the man. The court acquits the man and sentences the
man to death. The second man–an old officer of the town, Alekandrovitch–askes to tell them
the same thing. The third man–in the process of confessing his crime–is Vashenka, a drunk
man who has been sent to the town to kill his father, for reasons which are not entirely clear
to the people. The woman’s servant, Evgenyevna, is also the one who has told the court the
story of the Medvedevitch’s murder, for the good old man’s and the young man’s love. The
three men, who are separated for the first time, are laughing at the man’s attempt to seduce
Mitya. The young man, in the meantime, is conscripted into the town-side. He tells the court
that he loves her, but he has yet to tell her the true story. The men, in this room, demand a
man to kill her, and she will not betray them. The women, in their own country, are rebelling
against the man who had sent them three thousand roubles, and they will not allow the man of
the people to see them. They will not let the man in the town be allowed to see the man–or
Dmitriovitch; he will have her husband killed him. He will not tell the people who love him.
The next man, named Vashenovitch, arrives, and takes the man away. They all begin to laugh at
the fact that he has succeeded in seducing and entrusting his brother Dmitri. He is then taken
away to the old woman’s house, where the governor-side-of-the-world, and his sister, Arkadin,
is being punished. The priestesses and the baron are shocked, for they have been so virtuous and
well-suited. The only thing they will be able to do is kill the priest. They threaten to burn the
priestess to death, for she has been so wicked and libidinous that she has not yet seen the priest,
for her husband. The priests–ostensibly convinced that she is a woman who loves the priest and
has been punished for her love and for allowing the priest to marry her. The last man, Yakivitch,
arrives at the house, and, after a long day of drinking and then some of the men–is killed. He
and the priest are ordered to leave the town so that the priest can finally be reunited with the
people of the old lady. The final man, the commander of the St. Petersburg town of Arkadina, is
sentenced to death for the crime of having killed and then the lieutenant of the governor, for
taking the money. The commander, the former lieutenant-delegation of the People’s Army, is
summarily executed, and all the men, except for the commander, have been summarily punished
for their crime. The entire town is shocked and, in a very dramatic way, the priestesses plead for
the forgiveness of the man, for allowing them to kill and imprison Ivan. They plead for their
brother to be restored as well, for all the people they have loved, and for the priestor to tell the
story</div><br/><div id="35835357" class="c"><input type="checkbox" id="c-35835357" checked=""/><div class="controls bullet"><span class="by">edflsafoiewq</span><span>|</span><a href="#35833438">parent</a><span>|</span><a href="#35833666">next</a><span>|</span><label class="collapse" for="c-35835357">[-]</label><label class="expand" for="c-35835357">[2 more]</label></div><br/><div class="children"><div class="content">That summary hardly inspires confidence, it&#x27;s awful.</div><br/><div id="35836572" class="c"><input type="checkbox" id="c-35836572" checked=""/><div class="controls bullet"><span class="by">KaoruAoiShiho</span><span>|</span><a href="#35833438">root</a><span>|</span><a href="#35835357">parent</a><span>|</span><a href="#35833666">next</a><span>|</span><label class="collapse" for="c-35836572">[-]</label><label class="expand" for="c-35836572">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s because their model sucks (very old, not SOTA) not because the idea in this paper doesn&#x27;t work.</div><br/></div></div></div></div><div id="35833666" class="c"><input type="checkbox" id="c-35833666" checked=""/><div class="controls bullet"><span class="by">timy2shoes</span><span>|</span><a href="#35833438">parent</a><span>|</span><a href="#35835357">prev</a><span>|</span><a href="#35833311">next</a><span>|</span><label class="collapse" for="c-35833666">[-]</label><label class="expand" for="c-35833666">[3 more]</label></div><br/><div class="children"><div class="content">Just like the book, that summary was too long; didn’t read.</div><br/><div id="35833857" class="c"><input type="checkbox" id="c-35833857" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35833438">root</a><span>|</span><a href="#35833666">parent</a><span>|</span><a href="#35833311">next</a><span>|</span><label class="collapse" for="c-35833857">[-]</label><label class="expand" for="c-35833857">[2 more]</label></div><br/><div class="children"><div class="content">Sounds like your context window is too short.</div><br/><div id="35834533" class="c"><input type="checkbox" id="c-35834533" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#35833438">root</a><span>|</span><a href="#35833857">parent</a><span>|</span><a href="#35833311">next</a><span>|</span><label class="collapse" for="c-35834533">[-]</label><label class="expand" for="c-35834533">[1 more]</label></div><br/><div class="children"><div class="content">because internet?</div><br/></div></div></div></div></div></div></div></div><div id="35833311" class="c"><input type="checkbox" id="c-35833311" checked=""/><div class="controls bullet"><span class="by">ztratar</span><span>|</span><a href="#35833438">prev</a><span>|</span><a href="#35833124">next</a><span>|</span><label class="collapse" for="c-35833311">[-]</label><label class="expand" for="c-35833311">[2 more]</label></div><br/><div class="children"><div class="content">Given the model performance is thus affected by a k-nearest neighbor, but those algorithms are proving not great for baseline vector search, how well will this actually work?<p>It seems mostly like a vertically integrated vector DB + existing LLM call, but correct me if I&#x27;m wrong. There are of course some performance gains with that, but the holy grail of &quot;understanding&quot; at unlimited length still seems unsolved.</div><br/><div id="35833776" class="c"><input type="checkbox" id="c-35833776" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#35833311">parent</a><span>|</span><a href="#35833124">next</a><span>|</span><label class="collapse" for="c-35833776">[-]</label><label class="expand" for="c-35833776">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the performance (as in the capacity of retrieval, not performance as compute&#x2F;memory usage) of kNN mostly given by the quality of the vectors&#x2F;embeddings themselves?<p>Most vector DBs use (at least) some kind of KNN anyways.</div><br/></div></div></div></div><div id="35833124" class="c"><input type="checkbox" id="c-35833124" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#35833311">prev</a><span>|</span><a href="#35835516">next</a><span>|</span><label class="collapse" for="c-35833124">[-]</label><label class="expand" for="c-35833124">[5 more]</label></div><br/><div class="children"><div class="content">Hang on, how unlimited is unlimited here? Surely the immediate thing you&#x27;d do with this is just <i>never</i> delete any prior inputs so it becomes defacto long term memory for the model?</div><br/><div id="35833184" class="c"><input type="checkbox" id="c-35833184" checked=""/><div class="controls bullet"><span class="by">shishy</span><span>|</span><a href="#35833124">parent</a><span>|</span><a href="#35833272">next</a><span>|</span><label class="collapse" for="c-35833184">[-]</label><label class="expand" for="c-35833184">[1 more]</label></div><br/><div class="children"><div class="content">Last paragraph touches on that:<p>The length of inputs is theoretically bounded by
the memory limitations of the computer used. More
practically, using a CPU datastore is many times
slower than a GPU datastore because of slower
search and the need to transfer retrieved embed-
dings to the GPU... (continues)</div><br/></div></div><div id="35833272" class="c"><input type="checkbox" id="c-35833272" checked=""/><div class="controls bullet"><span class="by">0xDEF</span><span>|</span><a href="#35833124">parent</a><span>|</span><a href="#35833184">prev</a><span>|</span><a href="#35835516">next</a><span>|</span><label class="collapse" for="c-35833272">[-]</label><label class="expand" for="c-35833272">[3 more]</label></div><br/><div class="children"><div class="content">The limit is RAM but GPU RAM is much faster than computer RAM.</div><br/><div id="35833751" class="c"><input type="checkbox" id="c-35833751" checked=""/><div class="controls bullet"><span class="by">davrosthedalek</span><span>|</span><a href="#35833124">root</a><span>|</span><a href="#35833272">parent</a><span>|</span><a href="#35835516">next</a><span>|</span><label class="collapse" for="c-35833751">[-]</label><label class="expand" for="c-35833751">[2 more]</label></div><br/><div class="children"><div class="content">Is that really the limit? There is no real restriction that everything is in memory at the same time, right? You could maybe stream from SSD?</div><br/><div id="35834577" class="c"><input type="checkbox" id="c-35834577" checked=""/><div class="controls bullet"><span class="by">capableweb</span><span>|</span><a href="#35833124">root</a><span>|</span><a href="#35833751">parent</a><span>|</span><a href="#35835516">next</a><span>|</span><label class="collapse" for="c-35834577">[-]</label><label class="expand" for="c-35834577">[1 more]</label></div><br/><div class="children"><div class="content">Create a swapfile and you essentially trade disk space for memory space.</div><br/></div></div></div></div></div></div></div></div><div id="35833948" class="c"><input type="checkbox" id="c-35833948" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#35835516">prev</a><span>|</span><label class="collapse" for="c-35833948">[-]</label><label class="expand" for="c-35833948">[4 more]</label></div><br/><div class="children"><div class="content">The attention mechanism corresponds to the Hopf algebraic convolution, a generalization of the commonly known convolution.<p>I&#x27;m in the process of implementing a framework based on this idea.<p>I have written a paper on this recently, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.01834" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.01834</a><p>I have a discord channel <a href="https:&#x2F;&#x2F;discord.cofunctional.ai" rel="nofollow">https:&#x2F;&#x2F;discord.cofunctional.ai</a>.</div><br/><div id="35834321" class="c"><input type="checkbox" id="c-35834321" checked=""/><div class="controls bullet"><span class="by">capableweb</span><span>|</span><a href="#35833948">parent</a><span>|</span><label class="collapse" for="c-35834321">[-]</label><label class="expand" for="c-35834321">[3 more]</label></div><br/><div class="children"><div class="content">You never just work on something until it&#x27;s being ready to be shared, and then share it once? It has to be shared before it&#x27;s even a little bit usable, with just some vague words about what it might be?</div><br/><div id="35834365" class="c"><input type="checkbox" id="c-35834365" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#35833948">root</a><span>|</span><a href="#35834321">parent</a><span>|</span><label class="collapse" for="c-35834365">[-]</label><label class="expand" for="c-35834365">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m gauging interest and looking for potential users. Steve Blank and all that.</div><br/><div id="35834504" class="c"><input type="checkbox" id="c-35834504" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#35833948">root</a><span>|</span><a href="#35834365">parent</a><span>|</span><label class="collapse" for="c-35834504">[-]</label><label class="expand" for="c-35834504">[1 more]</label></div><br/><div class="children"><div class="content">The first step to crossing the chasm is finding those innovators and learning if you are solving a problem!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>