<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683968448910" as="style"/><link rel="stylesheet" href="styles.css?v=1683968448910"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://glean.io/blog-posts/why-i-stopped-worrying-and-learned-to-love-denormalized-tables">I stopped worrying and learned to love denormalized tables</a> <span class="domain">(<a href="https://glean.io">glean.io</a>)</span></div><div class="subtext"><span>vndrewlee</span> | <span>16 comments</span></div><br/><div><div id="35925019" class="c"><input type="checkbox" id="c-35925019" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#35926760">next</a><span>|</span><label class="collapse" for="c-35925019">[-]</label><label class="expand" for="c-35925019">[1 more]</label></div><br/><div class="children"><div class="content">Well...<p>1) Normalisation at all costs is foolish - if the cost exceeds the value, then don&#x27;t do it. That isn&#x27;t complicated. Denormalised data sometimes points at design flaws, but even then all systems have design flaws and they don&#x27;t automatically need to be fixed. Quality is expensive, like every other property (even doing things the cheap way is expensive, ironically - software is all about managing costs).<p>2) <i>For any given user</i> it is better to have denormalised data where the data model is perfectly aligned to their use case. <i>For a system with multiple users</i> it is better to have normalised data. And the corollary is that any data important enough to be recorded is probably valuable enough that it will eventually have multiple interested users even if the person building the system swears that this time is different - so they should normalise their data. Brownie point to anyone who has reached enlightenment and understands the you of 12 months hence is a different user with different needs of the data.</div><br/></div></div><div id="35926760" class="c"><input type="checkbox" id="c-35926760" checked=""/><div class="controls bullet"><span class="by">sergioisidoro</span><span>|</span><a href="#35925019">prev</a><span>|</span><a href="#35924788">next</a><span>|</span><label class="collapse" for="c-35926760">[-]</label><label class="expand" for="c-35926760">[1 more]</label></div><br/><div class="children"><div class="content">As someone who today has to maintain a database with a lot of denormalised data, do this only if your database is pretty much write once (the author&#x27;s use case).<p>For anything else you might feel that it&#x27;s saving you time and performance by not needing to join tables, but you&#x27;re just shooting yourself in the foot with a delayed effect. You&#x27;re just moving complexity from the read to the write operation with a multiplication effect.</div><br/></div></div><div id="35924788" class="c"><input type="checkbox" id="c-35924788" checked=""/><div class="controls bullet"><span class="by">MilStdJunkie</span><span>|</span><a href="#35926760">prev</a><span>|</span><a href="#35924841">next</a><span>|</span><label class="collapse" for="c-35924788">[-]</label><label class="expand" for="c-35924788">[1 more]</label></div><br/><div class="children"><div class="content">For the third time this week, in relatively unrelated fields of computation science, I&#x27;m reminded of the quote: &quot;Duplication is less expensive than the wrong abstraction&quot;.<p>An awful lot of the time, a table schema is a <i>terrible</i> abstraction of the actual series it is designed to record. Sometimes it&#x27;s designed under constraints that exist only to self-sustain the abstraction. Some of them have viable reasoning, some don&#x27;t. How these structures sustain themselves for . . decades . . is a mystery to me. These non-relational movements, in part represented by the OP article, are (in part) attempts to shift the computing from data to the actual programmatic area. Because the real world doesn&#x27;t have schemas - although that&#x27;s still, incredibly, a source of intense disagreement.<p>Just an interesting thing that keeps cropping up. I wonder what the formal, &quot;scientific&quot; name for this is?</div><br/></div></div><div id="35924841" class="c"><input type="checkbox" id="c-35924841" checked=""/><div class="controls bullet"><span class="by">makeitdouble</span><span>|</span><a href="#35924788">prev</a><span>|</span><a href="#35926718">next</a><span>|</span><label class="collapse" for="c-35924841">[-]</label><label class="expand" for="c-35924841">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like the author is calling a kind of materialized&#x2F;persisted view &quot;denormalized tables&quot;. The actual DB tables stay untouched and fully normalized.<p>It sure makes sense to love them, views are great. I don&#x27;t know why they need a new name.<p>&gt; Transformation tools such as dbt (Data Build Tool) have revolutionized the management and maintenance of denormalized tables. With dbt, we can establish clear relationships between table abstractions, create denormalized analytics datasets on top of them, and ensure data integrity and consistency with tests.</div><br/></div></div><div id="35926718" class="c"><input type="checkbox" id="c-35926718" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#35924841">prev</a><span>|</span><a href="#35924885">next</a><span>|</span><label class="collapse" for="c-35926718">[-]</label><label class="expand" for="c-35926718">[1 more]</label></div><br/><div class="children"><div class="content">Hmm as far as I can remember that&#x27;s what they told us at uni. After hammering normal forms into our heads for a while they added &quot;and you can carefully denormalize for speed&quot;.</div><br/></div></div><div id="35924885" class="c"><input type="checkbox" id="c-35924885" checked=""/><div class="controls bullet"><span class="by">redsaz</span><span>|</span><a href="#35926718">prev</a><span>|</span><a href="#35924854">next</a><span>|</span><label class="collapse" for="c-35924885">[-]</label><label class="expand" for="c-35924885">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s important to note that the author uses denormalized tables for data analysis only. It was never outright stated &quot;don&#x27;t do this for source-of-truth, authoritative data,&quot; but yeah, don&#x27;t do this for source-of-truth, authoritative data (in general).</div><br/></div></div><div id="35924854" class="c"><input type="checkbox" id="c-35924854" checked=""/><div class="controls bullet"><span class="by">hakunin</span><span>|</span><a href="#35924885">prev</a><span>|</span><a href="#35925940">next</a><span>|</span><label class="collapse" for="c-35924854">[-]</label><label class="expand" for="c-35924854">[1 more]</label></div><br/><div class="children"><div class="content">While this talks mostly about data warehousing, oftentimes denormalization is useful for everyday web app data storage. If your web app (usually on Postgres) is mostly frequent reads and rare writes (most web apps are) — there&#x27;s no excuse for your pages to load slower than a static site. Store your data as normalized as you want, add a denormalized materialized view, update it on writes, render pages based on the view.<p>Of course I&#x27;m talking about 95% of the apps where this is acceptable, not 5% where table locking can cause problems, leading to the need for concurrent update handling.</div><br/></div></div><div id="35925940" class="c"><input type="checkbox" id="c-35925940" checked=""/><div class="controls bullet"><span class="by">samtho</span><span>|</span><a href="#35924854">prev</a><span>|</span><a href="#35926125">next</a><span>|</span><label class="collapse" for="c-35925940">[-]</label><label class="expand" for="c-35925940">[1 more]</label></div><br/><div class="children"><div class="content">Given that initially defining your db schema is a one-time thing and we generally don’t change it very often after, I can’t fully get behind what the article suggests. However, the one thing I tend to do is add a text or json field called “extra” to my main tables that just stores a JSON map with fields I want to add to the record but don’t need to necessarily query by.</div><br/></div></div><div id="35926125" class="c"><input type="checkbox" id="c-35926125" checked=""/><div class="controls bullet"><span class="by">therufa</span><span>|</span><a href="#35925940">prev</a><span>|</span><a href="#35926098">next</a><span>|</span><label class="collapse" for="c-35926125">[-]</label><label class="expand" for="c-35926125">[1 more]</label></div><br/><div class="children"><div class="content">Why isn&#x27;t the author just using a document store instead? 
This entire post is defeating the purpose of RDBMS&#x27;.
What happens when data needs to be updated? Would that require a number of n update statements in order to change a username for instance?</div><br/></div></div><div id="35926098" class="c"><input type="checkbox" id="c-35926098" checked=""/><div class="controls bullet"><span class="by">tlarkworthy</span><span>|</span><a href="#35926125">prev</a><span>|</span><a href="#35926543">next</a><span>|</span><label class="collapse" for="c-35926098">[-]</label><label class="expand" for="c-35926098">[1 more]</label></div><br/><div class="children"><div class="content">You have to denormalize in very common cases even for mostly OLTP workloads in order to get sortable data into an index. Consider a cloud storage product with folders connecting with a many to one to documents. The product wants to display the most recently used folders ordered by their inner document modification date.<p>Because composite indexes commonly can&#x27;t span tables, you have to push the last modification date up to the folder row in order to get the data in the right place to build the obvious index.<p>Denormalization is a normal and expected optimization to scale a relational database.</div><br/></div></div><div id="35926543" class="c"><input type="checkbox" id="c-35926543" checked=""/><div class="controls bullet"><span class="by">fbn79</span><span>|</span><a href="#35926098">prev</a><span>|</span><a href="#35925404">next</a><span>|</span><label class="collapse" for="c-35926543">[-]</label><label class="expand" for="c-35926543">[1 more]</label></div><br/><div class="children"><div class="content">Denormalized table is just an hard to mantain  (materialized)view over normalized tables.</div><br/></div></div><div id="35925404" class="c"><input type="checkbox" id="c-35925404" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#35926543">prev</a><span>|</span><a href="#35926294">next</a><span>|</span><label class="collapse" for="c-35925404">[-]</label><label class="expand" for="c-35925404">[1 more]</label></div><br/><div class="children"><div class="content">Okay, so someone who analyzes reads, who&#x27;s never written software that needs consistent writes, is in favor of denormalized data. Let&#x27;s see how this post updates in ten years with parts 4 thru 9 where they go from realizing their data is inconsistent to writing some monstrous beast to try to normalize it.</div><br/></div></div><div id="35926294" class="c"><input type="checkbox" id="c-35926294" checked=""/><div class="controls bullet"><span class="by">revskill</span><span>|</span><a href="#35925404">prev</a><span>|</span><a href="#35925628">next</a><span>|</span><label class="collapse" for="c-35926294">[-]</label><label class="expand" for="c-35926294">[1 more]</label></div><br/><div class="children"><div class="content">What is dbt ?  How&#x27;s it related to denormalized tables ? The article is a bit confusing to me. What&#x27;re alternatives ?</div><br/></div></div><div id="35925628" class="c"><input type="checkbox" id="c-35925628" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#35926294">prev</a><span>|</span><a href="#35924705">next</a><span>|</span><label class="collapse" for="c-35925628">[-]</label><label class="expand" for="c-35925628">[1 more]</label></div><br/><div class="children"><div class="content">Yeah for an OLTP system until you hit certain scale normalized is fine and actually really needed.<p>For OLAP reporting and analysis denormalized is the way to go for sure. Reducing the number of joins needed (preferably to 0) makes things go very fast.</div><br/></div></div><div id="35924705" class="c"><input type="checkbox" id="c-35924705" checked=""/><div class="controls bullet"><span class="by">berkle4455</span><span>|</span><a href="#35925628">prev</a><span>|</span><label class="collapse" for="c-35924705">[-]</label><label class="expand" for="c-35924705">[1 more]</label></div><br/><div class="children"><div class="content">Every time someone praises dbt, a view sheds another tear.</div><br/></div></div></div></div></div></div></div></body></html>