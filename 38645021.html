<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702630857248" as="style"/><link rel="stylesheet" href="styles.css?v=1702630857248"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/intel-ceo-attacks-nvidia-on-ai-the-entire-industry-is-motivated-to-eliminate-the-cuda-market">Intel CEO: &#x27;The entire industry is motivated to eliminate the CUDA market&#x27;</a> <span class="domain">(<a href="https://www.tomshardware.com">www.tomshardware.com</a>)</span></div><div class="subtext"><span>rbanffy</span> | <span>291 comments</span></div><br/><div><div id="38645864" class="c"><input type="checkbox" id="c-38645864" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#38645594">next</a><span>|</span><label class="collapse" for="c-38645864">[-]</label><label class="expand" for="c-38645864">[104 more]</label></div><br/><div class="children"><div class="content">As another commenter said, it&#x27;s CUDA. Intel and AMD and whoever can turn out chips reasonably fast, but nobody gets that it&#x27;s the software and ecosystem. You have to out-compete the ecosystem. You can pick up a used Mi100 that performs almost like an A100 for 5x less money on eBay for example. Why is it 5x less? Because the software incompatibilities mean you&#x27;ll spend a ton of time getting it to work compared to an Nvidia GPU.<p>Google is barely limping along with it&#x27;s XLA interface to pytorch providing researchers a decent compatibility path. Same with Intel.<p>Any company in this space should basically setup a giant test suite of IDK, every model on hugging face and just start brute force fixing the issues. Then maybe they can sell some chips!<p>Intel is basically doing the same shit they always do here, announcing some open initiative and then doing literally the bare minimum to support it. 99% chance openvino goes nowhere. OpenAIs Triton already seems more popular, at least I&#x27;ve heard it referenced a lot more than openvino.</div><br/><div id="38646485" class="c"><input type="checkbox" id="c-38646485" checked=""/><div class="controls bullet"><span class="by">lacker</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38652240">next</a><span>|</span><label class="collapse" for="c-38646485">[-]</label><label class="expand" for="c-38646485">[34 more]</label></div><br/><div class="children"><div class="content">The funny thing to me is that so much of the &quot;AI software ecosystem&quot; is just PyTorch. You don&#x27;t need to develop some new framework and make it popular. You don&#x27;t need to support a zillion end libraries. Just literally support PyTorch.<p>If PyTorch worked fine on Intel GPUs, a lot of people would be happy to switch.</div><br/><div id="38646746" class="c"><input type="checkbox" id="c-38646746" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646485">parent</a><span>|</span><a href="#38646630">next</a><span>|</span><label class="collapse" for="c-38646746">[-]</label><label class="expand" for="c-38646746">[18 more]</label></div><br/><div class="children"><div class="content">But you can&#x27;t support Pytorch without a proper foundation in place. They don&#x27;t need to support zillion _end_ libraries, sure, but they do need to have at least a very good set of standard libraries, equivalent of Cublas, Curand etc.<p>And they don&#x27;t. My work recently had me working with rocRAND (Rocm&#x27;s answer to Curand). It was frankly pretty bad- the design, performance (50% slower in places that don&#x27;t make any sense because generating random numbers is not exact that complicated), and documentation (God it was awful).<p>Now, that&#x27;s a small slice of the larger pie. But imagine if this trend continues for other libraries.</div><br/><div id="38647311" class="c"><input type="checkbox" id="c-38647311" checked=""/><div class="controls bullet"><span class="by">singhrac</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646746">parent</a><span>|</span><a href="#38646864">next</a><span>|</span><label class="collapse" for="c-38647311">[-]</label><label class="expand" for="c-38647311">[6 more]</label></div><br/><div class="children"><div class="content">Generating random numbers is a bit complicated! I wrote some of the samplers in Pytorch (probably replaced by now) and some of the underlying pseudo-random algorithms that work correctly in parallel are not exactly easy... running the same PRNG with the same seed on all your cores will produce the same result, which is probably NOT what you want from your API.<p>But, to be honest, it&#x27;s not that hard either. I&#x27;m surprised their API is 2x slower, Philox is 10 years old now and I don&#x27;t think there&#x27;s a licensing fee?</div><br/><div id="38649428" class="c"><input type="checkbox" id="c-38649428" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647311">parent</a><span>|</span><a href="#38648811">next</a><span>|</span><label class="collapse" for="c-38649428">[-]</label><label class="expand" for="c-38649428">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Generating random numbers is a bit complicated!<p>I know! I just wrote a whole paper and published a library on this!<p>But really, perhaps not as much as many from outside might think. The core of a Philox implementation can be around 50 lines of C++ [1], with all the bells and whistles maybe around 300-400. That implementation&#x27;s performance equals CuRAND&#x27;s , sometimes even surpasses it! (the API is designed to avoid maintaining any rng states on device memory, something curand forces you to do).<p>&gt; running the same PRNG with the same seed on all your cores will produce the same result<p>You&#x27;re right. Solution here is to utilize multiple generator objects, one per thread, ensuring each produces statistically independent random streams. Some good algorithms  (Philox for example), allow you to use any set of unique values as seeds for your threads (e.g. thread id).<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;msu-sparta&#x2F;OpenRAND&#x2F;blob&#x2F;main&#x2F;include&#x2F;openrand&#x2F;philox.h#L102">https:&#x2F;&#x2F;github.com&#x2F;msu-sparta&#x2F;OpenRAND&#x2F;blob&#x2F;main&#x2F;include&#x2F;ope...</a></div><br/><div id="38650021" class="c"><input type="checkbox" id="c-38650021" checked=""/><div class="controls bullet"><span class="by">carterschonwald</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649428">parent</a><span>|</span><a href="#38648811">next</a><span>|</span><label class="collapse" for="c-38650021">[-]</label><label class="expand" for="c-38650021">[1 more]</label></div><br/><div class="children"><div class="content">Cool! I’ll have a lookseee. I’ve my own experiments in this space.</div><br/></div></div></div></div><div id="38648811" class="c"><input type="checkbox" id="c-38648811" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647311">parent</a><span>|</span><a href="#38649428">prev</a><span>|</span><a href="#38650995">next</a><span>|</span><label class="collapse" for="c-38648811">[-]</label><label class="expand" for="c-38648811">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if the next generation chips are going to just have a dedicated hardware RNG per-core if that&#x27;s an issue?</div><br/></div></div><div id="38650995" class="c"><input type="checkbox" id="c-38650995" checked=""/><div class="controls bullet"><span class="by">paulmd</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647311">parent</a><span>|</span><a href="#38648811">prev</a><span>|</span><a href="#38646864">next</a><span>|</span><label class="collapse" for="c-38650995">[-]</label><label class="expand" for="c-38650995">[2 more]</label></div><br/><div class="children"><div class="content">for GPGPU, the better approach is CBRNG like random123.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;DEShawResearch&#x2F;random123">https:&#x2F;&#x2F;github.com&#x2F;DEShawResearch&#x2F;random123</a><p>if you accept the principles of encryption, then the bits of the output of crypt(key, message) should be totally uncorrelated to the output of crypt(key, message+1).  and this requires no state other than knowing the key and the position in the sequence.<p>the direct-port analogy is that you have an array of CuRand generators, generator index G is equivalent to key G, and you have a fixed start offset for the particular simulation.<p>moreover, you can then define the key in relation to your actual data.  the mental shift from what you&#x27;re talking about is that in this model, a PRNG isn&#x27;t something that belongs to the executing thread.  <i>every element</i> can get its own PRNG and keystream.  And if you use a contextually-meaningful value for the element key, then you already &quot;know&quot; the key from your existing data.  And this significantly improves determinism of the simulation etc because PRNG output is tied to the simulation state, not which thread it happens to be scheduled on.<p>(note that the property of cryptographic non-correlation is NOT guaranteed across keystreams - (key, counter) is NOT guaranteed to be uncorrelated to (key+1, counter), because that&#x27;s not how encryption usually is used.  with a decent crypto, it should still be very good, but, it&#x27;s not guaranteed to be attack-resistant&#x2F;etc.  so notionally if you use a different key index for every element, element N isn&#x27;t guaranteed to be uncorrelated to element N+1 at the same place in the keystream.  If this is really important then maybe you want to pass your array indexes through a key-spreading function etc.)<p>there are several benefits to doing it like this.  first off obviously you get a keystream for each element of interest.  but also there is no real state per-thread either - the key can be determined by looking at the element, but generating a new value doesn&#x27;t change the key&#x2F;keystream.  so there is nothing to store and update, and you can have arbitrary numbers of generators used at any given time.  Also, since this computation is purely mathematical&#x2F;&quot;pure function&quot;, it doesn&#x27;t really consume any memory-bandwidth to speak of, and since computation time is usually not the limiting element in GPGPU simulations this effectively makes RNG usage &quot;free&quot;.  my experience is that this <i>increases</i> performance vs CuRand, even while using less VRAM, even just directly porting the &quot;1 execution thread = 1 generator&quot; idiom.<p>Also, by storing &quot;epoch numbers&quot; (each iteration of the sim, etc), or calculating this based on predictions of PRNG consumption (&quot;each iteration uses at most 16 random numbers&quot;), you can fast-forward or rewind the PRNG to arbitrary times, and you can use this to lookahead or lookback on previous events from the keystream, meaning it serves as a massively potent form of compression as well.  Why store data in memory and use up your precious VRAM, when you could simply recompute it on-demand from the original part of the original keystream used to generate it in the first place?  (assuming proper &quot;object ownership&quot; of events ofc!) And this actually is pretty much free in performance terms, since it&#x27;s a &quot;pure function&quot; based on the function parameters, and the GPGPU almost certainly has an excess of computation available.<p>--<p>In the extreme case, you should be able to theoretically &quot;walk&quot; huge parts of the keystream and find specific events you need, even if there is no other reference to what happened at that particular time in the past.  Like why not just walk through parts of the keystream until you find the event that matches your target criteria?  Remember since this is basically pure math, it&#x27;s generated on-demand by mathing it out, it&#x27;s pretty much free, and computation is cheap compared to cache&#x2F;memory or notarizing.<p>(ie this is a weird form of &quot;inverted-index searching&quot;, analogous to Elastic&#x2F;Solr&#x27;s transformers and how this allows a large number of individual transformers (which do their own searching&#x2F;indexing for each query, which will be generally unindexable operations like fulltext etc) to listen to a single IO stream as blocks are broadcast from the disk in big sequential streaming batches.  Instead of SSD batch reads you&#x27;d be aiming for computation batch reads from a long range within a keystream.  (And this is supposition but I think you can also trade back and forth between generator space and index hitrate by pinning certain bits in the output right?)<p>--<p>Anyway I don&#x27;t know how much that maps to your particular use-case but that&#x27;s the best advice I can give.  Procedural generation using a rewindable, element-specific keystream is a very potent form of compression, and very cheap.  But, even if all you are doing is just avoiding having to store a bunch of CuRand instances in VRAM... that&#x27;s still an <i>enormous</i> win even if you directly port your existing application to simply use the globalThreadIdx like it was a CuRand stateful instance being loaded&#x2F;saved back to VRAM.  Like I said, my experience is that because you&#x27;re changing mutation to computation, this runs faster and also uses less VRAM, it is both smaller and better and probably also statistically better randomness (especially if you choose the &quot;hard&quot; algorithms instead of the &quot;optimized&quot; versions like threefish instead of threefry etc).  The bit distribution patterns of cryptographic algorithms is something that a lot of people pay very very close attention to, you are turning a science toy implementation into a gatling gun there simply by modeling your task and the RNG slightly differently.<p>That is the reason why you shouldn&#x27;t do the &quot;just download random numbers&quot;, as a sibling comment mentions (probably a joke) - that consumes VRAM, or at least system memory (and pcie bandwidth).  and you know what&#x27;s usually way more available as a resource in most GPGPU applications than VRAM or PCIe bandwidth?  pure ALU&#x2F;FPU computation time.<p>buddy, everyone has random numbers, they come with the fucking xbox. ;)</div><br/><div id="38651277" class="c"><input type="checkbox" id="c-38651277" checked=""/><div class="controls bullet"><span class="by">paulmd</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650995">parent</a><span>|</span><a href="#38646864">next</a><span>|</span><label class="collapse" for="c-38651277">[-]</label><label class="expand" for="c-38651277">[1 more]</label></div><br/><div class="children"><div class="content">thinking this through a little bit, you are launching a series of gradient-descent work tasks, right?  taskId is your counter value, weightIdx is your key value (RNG stream).  That&#x27;s how I&#x27;d port that.  Ideally you want to define some maximum PRNG usage for each stage of the program, which allows you to establish fixed offsets from the epoch value for a given event.  Divide your keystream in whatever advantageous way, based on (highly-compressible) epoch counters and event offsets from that value.<p>in practice, assuming a gradient-descent event needs a lot of random numbers, having one keystream for a single GD event might be too much and that&#x27;s where key-spreading comes in.  if you take the &quot;weightIdx W at GradientDescentIdx G&quot; as the key, you can have a whole global keystream-<i>space</i> for that descent stage.  And the key-spreading-function lets you go between your composite key and a practical one.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Key_derivation_function" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Key_derivation_function</a><p>(again, like threefry, there is notionally no need for this to be <i>cryptographically</i> secure in most cases, as long as it spreads in ways that your CBRNG crypto algorithm can tolerate without bit-correlation.  there is no need to do 2 million rounds here either etc.  You should actually pick reasonable parameters here for fast performance, but good enough keyspreading for your needs.)<p>I&#x27;ve been out of this for a long time, I&#x27;ve been told I&#x27;m out of date before and GPGPUs might not behave exactly this way anymore, so please just take it in the spirit it&#x27;s offered, can&#x27;t guarantee this is right but I&#x27;ve specifically gazed into the abyss the CuRand situation a decade ago and this was what I managed to come up with.  I do feel your pain on the stateful RNG situation, managing state per-execution-thread is awful and destroys simulation reproducibility, and managing a PRNG context for each possible element is often infeasible.  What a waste of VRAM and bandwidth and mutation&#x2F;cache etc.<p>And I think that cryptographic&#x2F;pseudo-cryptographic PRNG models are frankly just a much better horse to hook your wagon to than scientific&#x2F;academic ones, even apart from all the other advantages.  Like there&#x27;s just not any way mersenne twister or w&#x2F;e is better than threefish, sorry academia<p>--<p>edit: Real-world sim programs are usually very low-intensity and have effectively unlimited amounts of compute to spare, they just ride on bandwidth (sort&#x2F;search or sort&#x2F;prefix-scan&#x2F;search algorithms with global scope building blocks often work well).<p>And tbh that&#x27;s why tensor is so amazing, it&#x27;s super effective at math intensity and computational focus, and that&#x27;s what GPUs do well, augmented by things like sparse models etc.  Make your random not-math task into dense or sparse (but optimized) GPGPU math, plus you get a solution (reasonable optimum) to an intractible problem in realtime.  The experienced salesman usually finds a reasonable optimum, but we pay him in GEMM&#x2F;BLAS&#x2F;Tensor compute time instead of dollars.<p>Sort&#x2F;search or sort&#x2F;prefix-sum&#x2F;search often works really well in deterministic programs too.  Do you ever have a &quot;myGroup[groupIdx].addObj(objIdx) stage?  that&#x27;s a sort and prefix-sum operation right there, and both of those ops run super well on GPGPU.</div><br/></div></div></div></div></div></div><div id="38646864" class="c"><input type="checkbox" id="c-38646864" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646746">parent</a><span>|</span><a href="#38647311">prev</a><span>|</span><a href="#38651966">next</a><span>|</span><label class="collapse" for="c-38646864">[-]</label><label class="expand" for="c-38646864">[6 more]</label></div><br/><div class="children"><div class="content">Folks also underestimate how complex these libraries are. There are dozens of projects to make BLAS alternatives which give up after ~3-6 months when they realize that this project will take years to be successful.</div><br/><div id="38648832" class="c"><input type="checkbox" id="c-38648832" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646864">parent</a><span>|</span><a href="#38651966">next</a><span>|</span><label class="collapse" for="c-38648832">[-]</label><label class="expand" for="c-38648832">[5 more]</label></div><br/><div class="children"><div class="content">How does that work? Why not pick up where the previous team left off instead of everyone starting new ones?  Or are they all targeting different backends and hardware?</div><br/><div id="38649052" class="c"><input type="checkbox" id="c-38649052" checked=""/><div class="controls bullet"><span class="by">haltist</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648832">parent</a><span>|</span><a href="#38651966">next</a><span>|</span><label class="collapse" for="c-38649052">[-]</label><label class="expand" for="c-38649052">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a compiler problem and there is no money in compilers [1]. If someone made an intermediate representation for AI graphs and then wrote a compiler from that intermediate format into whatever backend was the deployment target then they might be able to charge money for support and bug fixes but that would be it. It&#x27;s not the kind of business anyone wants to be in so there is no good intermediate format and compiler that is platform agnostic.<p>1: <a href="https:&#x2F;&#x2F;tinygrad.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tinygrad.org&#x2F;</a></div><br/><div id="38650895" class="c"><input type="checkbox" id="c-38650895" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649052">parent</a><span>|</span><a href="#38651966">next</a><span>|</span><label class="collapse" for="c-38650895">[-]</label><label class="expand" for="c-38650895">[3 more]</label></div><br/><div class="children"><div class="content">JAX is a compiler.</div><br/><div id="38651193" class="c"><input type="checkbox" id="c-38651193" checked=""/><div class="controls bullet"><span class="by">haltist</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650895">parent</a><span>|</span><a href="#38651182">next</a><span>|</span><label class="collapse" for="c-38651193">[-]</label><label class="expand" for="c-38651193">[1 more]</label></div><br/><div class="children"><div class="content">So are TensorFlow and PyTorch. All AI&#x2F;ML frameworks have to translate high-level tensor programs into executable artifacts for the given hardware and they&#x27;re all given away for free because there is no way to make money with them. It&#x27;s all open source and free. So the big tech companies subsidize the compilers because they want hardware to be the moat. It&#x27;s why the running joke is that I need $80B to build AGI. The software is cheap&#x2F;free, the hardware costs money.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38651966" class="c"><input type="checkbox" id="c-38651966" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646746">parent</a><span>|</span><a href="#38646864">prev</a><span>|</span><a href="#38649714">next</a><span>|</span><label class="collapse" for="c-38651966">[-]</label><label class="expand" for="c-38651966">[2 more]</label></div><br/><div class="children"><div class="content">I honestly don&#x27;t see why it&#x27;s so hard. On my project we wrote our own gemm kernels from scratch so llama.cpp didn&#x27;t need to depend on cublas anymore. Only took a few days and a few hundred lines of code. We had to trade away 5% performance.</div><br/><div id="38652362" class="c"><input type="checkbox" id="c-38652362" checked=""/><div class="controls bullet"><span class="by">soulbadguy</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38651966">parent</a><span>|</span><a href="#38649714">next</a><span>|</span><label class="collapse" for="c-38652362">[-]</label><label class="expand" for="c-38652362">[1 more]</label></div><br/><div class="children"><div class="content">For a given set kernels, and a limited set of architectures, the problem is relatively easy.<p>But covering all the important kernels acros all the crazy architecture out there and with relatively good performance and numerical accuracy ... Much harder</div><br/></div></div></div></div><div id="38649714" class="c"><input type="checkbox" id="c-38649714" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646746">parent</a><span>|</span><a href="#38651966">prev</a><span>|</span><a href="#38648794">next</a><span>|</span><label class="collapse" for="c-38649714">[-]</label><label class="expand" for="c-38649714">[1 more]</label></div><br/><div class="children"><div class="content">If you haven&#x27;t already, please consider filing issues on the rocrand GitHub repo for the problems you encountered. The rocrand library is being actively developed and your feedback would be valuable for guiding improvements.</div><br/></div></div><div id="38648794" class="c"><input type="checkbox" id="c-38648794" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646746">parent</a><span>|</span><a href="#38649714">prev</a><span>|</span><a href="#38646630">next</a><span>|</span><label class="collapse" for="c-38648794">[-]</label><label class="expand" for="c-38648794">[2 more]</label></div><br/><div class="children"><div class="content">Instead of generating pseudorandom numbers you can just download files of them.<p><a href="https:&#x2F;&#x2F;archive.random.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.random.org&#x2F;</a></div><br/><div id="38650372" class="c"><input type="checkbox" id="c-38650372" checked=""/><div class="controls bullet"><span class="by">hcrean</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648794">parent</a><span>|</span><a href="#38646630">next</a><span>|</span><label class="collapse" for="c-38650372">[-]</label><label class="expand" for="c-38650372">[1 more]</label></div><br/><div class="children"><div class="content">Or you could just re-use the same number; no one can prove it is not random.<p><a href="https:&#x2F;&#x2F;xkcd.com&#x2F;221&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;xkcd.com&#x2F;221&#x2F;</a></div><br/></div></div></div></div></div></div><div id="38646630" class="c"><input type="checkbox" id="c-38646630" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646485">parent</a><span>|</span><a href="#38646746">prev</a><span>|</span><a href="#38647334">next</a><span>|</span><label class="collapse" for="c-38646630">[-]</label><label class="expand" for="c-38646630">[1 more]</label></div><br/><div class="children"><div class="content">This is a big reason why AMD did this deal with PyTorch...<p><a href="https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;experience-power-pytorch-2.0&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;experience-power-pytorch-2.0&#x2F;</a></div><br/></div></div><div id="38647334" class="c"><input type="checkbox" id="c-38647334" checked=""/><div class="controls bullet"><span class="by">singhrac</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646485">parent</a><span>|</span><a href="#38646630">prev</a><span>|</span><a href="#38647024">next</a><span>|</span><label class="collapse" for="c-38647334">[-]</label><label class="expand" for="c-38647334">[8 more]</label></div><br/><div class="children"><div class="content">Just to point out it does, kind of: <a href="https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;intel-extension-for-pytorch">https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;intel-extension-for-pytorch</a><p>I&#x27;ve asked before if they&#x27;ll merge it back into PyTorch main and include it in the CI, not sure if they&#x27;ve done that yet.<p>In this case I think the biggest bottleneck is just that they don&#x27;t have a fast enough card that can compete with having a 3090 or an A100. And Gaudi is stuck on a different software platform which doesn&#x27;t seem as flexible as an A100.</div><br/><div id="38647551" class="c"><input type="checkbox" id="c-38647551" checked=""/><div class="controls bullet"><span class="by">spacemanspiff01</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647334">parent</a><span>|</span><a href="#38647024">next</a><span>|</span><label class="collapse" for="c-38647551">[-]</label><label class="expand" for="c-38647551">[7 more]</label></div><br/><div class="children"><div class="content">They could compete on ram, if the software was there. Just having a low cost alternative to the 4060ti would allow them to break into the student&#x2F;hobbies&#x2F;open source market.<p>I tried the a770, but returned it. Half the stuff does not work. They have the CPU side and GPU development on different branches (GPU seems to be ~6 months behind CPU) and often you have to compile it yourself, (if you want torchvision or torchaudio) it also currently on 2.0.1 of pytorch so somewhat lagging, and does not have most of the performance analysis software available. You also, do need to modify your pytorch code, often more than just replacing cuda for xpu as the device.
They are also doing all development internally, then pushing intermittently to public. A lot of this would not be as bad if there was a better idea of feature timeline, or if they made their CI public. (Trying to build it myself involved a extremely hacky bash script, that inevitably failed halfway through.)</div><br/><div id="38648817" class="c"><input type="checkbox" id="c-38648817" checked=""/><div class="controls bullet"><span class="by">selfhoster11</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647551">parent</a><span>|</span><a href="#38649153">next</a><span>|</span><label class="collapse" for="c-38648817">[-]</label><label class="expand" for="c-38648817">[4 more]</label></div><br/><div class="children"><div class="content">The amount of VRAM is the absolute killer USP for the current large AI model hobbyist segment. Something that had just as much VRAM as a 3090 but at half the speed and half the price would sell like hot cakes.</div><br/><div id="38649920" class="c"><input type="checkbox" id="c-38649920" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648817">parent</a><span>|</span><a href="#38649153">next</a><span>|</span><label class="collapse" for="c-38649920">[-]</label><label class="expand" for="c-38649920">[3 more]</label></div><br/><div class="children"><div class="content">You are describing the ebay market for used nvidia tesla cards. The k80, p40, or m40 are widely available and sell for ~$100 with 24gb vram. The m10 even has 32gb! The problem for ai hobbyists is it won&#x27;t take long to realize how many apis use the &quot;optical flow&quot; pathways and so on nvidia they&#x27;ll only run at acceptable speeds on rtx hardware, assuming they run at all. Cuda versions are pinned to hardware to some extent.</div><br/><div id="38652112" class="c"><input type="checkbox" id="c-38652112" checked=""/><div class="controls bullet"><span class="by">selfhoster11</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649920">parent</a><span>|</span><a href="#38650462">next</a><span>|</span><label class="collapse" for="c-38652112">[-]</label><label class="expand" for="c-38652112">[1 more]</label></div><br/><div class="children"><div class="content">Out of these three, only P40 is worth the effort to get running vs the capabilities they offer. That&#x27;s also before considering that other than software hacks or configuration tweaks, those cards require specialised cooling shrouds for adequate cooling in tower-style cases.<p>If your time or personal energy is worth &gt;$0, these cards work out to much more than $100. And you can&#x27;t even file the time burnt on getting them to run as any kind of transferable experience.<p>That&#x27;s not to say I don&#x27;t recommend getting them - I have a P4 family card and will get at least one more, but I&#x27;m not kidding myself that the use isn&#x27;t very limited.</div><br/></div></div><div id="38650462" class="c"><input type="checkbox" id="c-38650462" checked=""/><div class="controls bullet"><span class="by">icelancer</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649920">parent</a><span>|</span><a href="#38652112">prev</a><span>|</span><a href="#38649153">next</a><span>|</span><label class="collapse" for="c-38650462">[-]</label><label class="expand" for="c-38650462">[1 more]</label></div><br/><div class="children"><div class="content">Yep. I have a fleet of P40s that are good at what they do (Whisper ASR primarily) but anything even remotely new... nah. fp16 support is missing so you need P100 cards, and usually that means you are accepting 16GB of VRAM rather than 24GB.<p>Still some cool hardware.</div><br/></div></div></div></div></div></div><div id="38649153" class="c"><input type="checkbox" id="c-38649153" checked=""/><div class="controls bullet"><span class="by">singhrac</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647551">parent</a><span>|</span><a href="#38648817">prev</a><span>|</span><a href="#38651338">next</a><span>|</span><label class="collapse" for="c-38649153">[-]</label><label class="expand" for="c-38649153">[1 more]</label></div><br/><div class="children"><div class="content">This is pretty disappointing to hear. I’m really surprised they can’t even get a clean build script for users, let alone integrate into the regular Pytorch releases.</div><br/></div></div><div id="38651338" class="c"><input type="checkbox" id="c-38651338" checked=""/><div class="controls bullet"><span class="by">paulmd</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647551">parent</a><span>|</span><a href="#38649153">prev</a><span>|</span><a href="#38647024">next</a><span>|</span><label class="collapse" for="c-38651338">[-]</label><label class="expand" for="c-38651338">[1 more]</label></div><br/><div class="children"><div class="content">oh no, I just bought a refurbed A770 16GB for tinkering with GPGPU lol.  It was $220, return?</div><br/></div></div></div></div></div></div><div id="38647024" class="c"><input type="checkbox" id="c-38647024" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646485">parent</a><span>|</span><a href="#38647334">prev</a><span>|</span><a href="#38649203">next</a><span>|</span><label class="collapse" for="c-38647024">[-]</label><label class="expand" for="c-38647024">[1 more]</label></div><br/><div class="children"><div class="content">PyTorch includes some Vulkan compat already (though mostly tested on Android, not on desktop&#x2F;server platforms), and they&#x27;re sort of planning to work on OpenCL 3.0 compat, which would in turn lead to broad-based hardware support via Mesa&#x27;s RustiCL driver.<p>(They don&#x27;t advertise this as &quot;support&quot; because they have higher standards for what that term means.  PyTorch includes a zillion different &quot;operators&quot; and some of them might be unimplemented still.  Besides performance is still lacking compared to CUDA, Rocm or Metal on leading hardware - so only useful for toy models.)</div><br/></div></div><div id="38649203" class="c"><input type="checkbox" id="c-38649203" checked=""/><div class="controls bullet"><span class="by">ndneighbor</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646485">parent</a><span>|</span><a href="#38647024">prev</a><span>|</span><a href="#38652240">next</a><span>|</span><label class="collapse" for="c-38649203">[-]</label><label class="expand" for="c-38649203">[5 more]</label></div><br/><div class="children"><div class="content">OneAPI isn&#x27;t bad for PyTorch, the performance isn&#x27;t there yet but you can tell it&#x27;s an extremely top priority for Intel.</div><br/><div id="38651231" class="c"><input type="checkbox" id="c-38651231" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649203">parent</a><span>|</span><a href="#38649406">next</a><span>|</span><label class="collapse" for="c-38651231">[-]</label><label class="expand" for="c-38651231">[1 more]</label></div><br/><div class="children"><div class="content">But this is the the thing.  Speaking as someone who dabbles in this area rather than any kind of expert, it’s baffling to me that people like Intel are making press releases and public statements rather than (I don’t know) putting in the frikkin work to make performance of the one library that people actually use decent.<p>You have a massive organization full of gazillions of engineers many of whom are really excellent.  Before you open your mouth in public and say something is a priority, deploy a lot of them against this and manifest that priority by actually doing the thing that is necessary so people can use your stuff.<p>It’s really hard to take them seriously when they haven’t (yet) done that.</div><br/></div></div><div id="38649406" class="c"><input type="checkbox" id="c-38649406" checked=""/><div class="controls bullet"><span class="by">flakiness</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649203">parent</a><span>|</span><a href="#38651231">prev</a><span>|</span><a href="#38652240">next</a><span>|</span><label class="collapse" for="c-38649406">[-]</label><label class="expand" for="c-38649406">[3 more]</label></div><br/><div class="children"><div class="content">Intel has to do it by themselves. NVIDIA just lets Meta&#x2F;OpenAI&#x2F;Google engineers do it for them. Such a handicapped fight.</div><br/><div id="38651240" class="c"><input type="checkbox" id="c-38651240" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649406">parent</a><span>|</span><a href="#38650391">next</a><span>|</span><label class="collapse" for="c-38651240">[-]</label><label class="expand" for="c-38651240">[1 more]</label></div><br/><div class="children"><div class="content">It wasn’t always like this.  Nvidia did the initial heavy lifting to get cuda off the ground to a point where other people could use it.</div><br/></div></div><div id="38650391" class="c"><input type="checkbox" id="c-38650391" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649406">parent</a><span>|</span><a href="#38651240">prev</a><span>|</span><a href="#38652240">next</a><span>|</span><label class="collapse" for="c-38650391">[-]</label><label class="expand" for="c-38650391">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s because CUDA is a clear, well-functioning library and Intel has no equivalent. It makes any &quot;you just have to get Pytorch working&quot; a little less plausible.</div><br/></div></div></div></div></div></div></div></div><div id="38652240" class="c"><input type="checkbox" id="c-38652240" checked=""/><div class="controls bullet"><span class="by">nox100</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38646485">prev</a><span>|</span><a href="#38646396">next</a><span>|</span><label class="collapse" for="c-38652240">[-]</label><label class="expand" for="c-38652240">[1 more]</label></div><br/><div class="children"><div class="content">Is Mojo trying to solve this?<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SEwTjZvy8vw" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SEwTjZvy8vw</a></div><br/></div></div><div id="38646396" class="c"><input type="checkbox" id="c-38646396" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38652240">prev</a><span>|</span><a href="#38651479">next</a><span>|</span><label class="collapse" for="c-38646396">[-]</label><label class="expand" for="c-38646396">[24 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not just Intel. Open initiatives and consortiums (the phase two of the same) are always the losers ganging up hoping that it will give them the leg up they don&#x27;t have. If you&#x27;re older you&#x27;ll have seen this play out over and over in the industry - the history of Unix vs. Windows NT from the 1990s was full of actions like this, networking is going through it again for the nth time (this time with UltraEthernet) and so on. OpenGl was probably the most successful approach, barely worked, and didn&#x27;t help any of the players who were not on the road to victory already. Unix 95 didn&#x27;t work, unix 98 didn&#x27;t work, etc.</div><br/><div id="38647099" class="c"><input type="checkbox" id="c-38647099" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646396">parent</a><span>|</span><a href="#38651479">next</a><span>|</span><label class="collapse" for="c-38647099">[-]</label><label class="expand" for="c-38647099">[23 more]</label></div><br/><div class="children"><div class="content">You&#x27;re just listing the ones that didn&#x27;t knock it out of the park.<p>TCP&#x2F;IP completely displaced IPX to the point that most people don&#x27;t even remember what it was. Nobody uses WINS anymore, even Microsoft uses DNS. It&#x27;s rare to find an operating system that <i>doesn&#x27;t</i> implement the POSIX API.<p>The past is littered with the corpses of proprietary technologies displaced by open standards. Because customers don&#x27;t actually want vendor-locked technology. They tolerate it when it&#x27;s the only viable alternative, but make the open option <i>good</i> and the proprietary one will be on its way out.</div><br/><div id="38649889" class="c"><input type="checkbox" id="c-38649889" checked=""/><div class="controls bullet"><span class="by">tormeh</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647099">parent</a><span>|</span><a href="#38651165">next</a><span>|</span><label class="collapse" for="c-38649889">[-]</label><label class="expand" for="c-38649889">[6 more]</label></div><br/><div class="children"><div class="content">Open source generally wins once the state of the art has stopped moving. When a field is still experiencing rapid change closed source solutions generally do better than open source ones. Until we somehow figure out a relatively static set of requirements for running and training LLMs I wouldn’t expect any open source solution to win.</div><br/><div id="38650539" class="c"><input type="checkbox" id="c-38650539" checked=""/><div class="controls bullet"><span class="by">kelipso</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649889">parent</a><span>|</span><a href="#38651165">next</a><span>|</span><label class="collapse" for="c-38650539">[-]</label><label class="expand" for="c-38650539">[5 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t really make sense. Pretty much all LLMs are trained in pytorch, which is open source. LLMs only reached the state it is now because many academic conferences insisted that paper submissions have open source code attached to it. So much of the ML&#x2F;AI ecosystem is open source. Pretty much only CUDA is not open source.</div><br/><div id="38651933" class="c"><input type="checkbox" id="c-38651933" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650539">parent</a><span>|</span><a href="#38650761">next</a><span>|</span><label class="collapse" for="c-38651933">[-]</label><label class="expand" for="c-38651933">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Pretty much only CUDA is not open source.<p>What stops Intel to make their own CUDA and plug in into pytorch?</div><br/><div id="38652088" class="c"><input type="checkbox" id="c-38652088" checked=""/><div class="controls bullet"><span class="by">nemothekid</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38651933">parent</a><span>|</span><a href="#38651988">next</a><span>|</span><label class="collapse" for="c-38652088">[-]</label><label class="expand" for="c-38652088">[1 more]</label></div><br/><div class="children"><div class="content">CUDA is huge and nvidia spent a ton in a lot of &quot;dead end&quot; use cases optimizing it. There have been experiments with CUDA translation layers with decent performance[1]. There are two things that most projects hit:<p>1. The CUDA API is huge; I&#x27;m sure Intel&#x2F;AMD will focus on what they need to implement pytorch and ignore every other use case ensuring that CUDA always has the leg up in any new frontier<p>2. Nvidia actually cares about developer experience. The most prominent example is  Geohotz with tinygrad - where AMD examples didn&#x27;t even work or had glaring compiler bugs. You will find nvidia engineer in github issues for CUDA projects. Intel&#x2F;AMD hasn&#x27;t made that level of investment and thats important because GPUs tend to be more fickle than CPUs.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;vosen&#x2F;ZLUDA">https:&#x2F;&#x2F;github.com&#x2F;vosen&#x2F;ZLUDA</a></div><br/></div></div><div id="38651988" class="c"><input type="checkbox" id="c-38651988" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38651933">parent</a><span>|</span><a href="#38652088">prev</a><span>|</span><a href="#38650761">next</a><span>|</span><label class="collapse" for="c-38651988">[-]</label><label class="expand" for="c-38651988">[1 more]</label></div><br/><div class="children"><div class="content">The same shit as always, patents and copyright.</div><br/></div></div></div></div></div></div></div></div><div id="38651165" class="c"><input type="checkbox" id="c-38651165" checked=""/><div class="controls bullet"><span class="by">binkHN</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647099">parent</a><span>|</span><a href="#38649889">prev</a><span>|</span><a href="#38647589">next</a><span>|</span><label class="collapse" for="c-38651165">[-]</label><label class="expand" for="c-38651165">[1 more]</label></div><br/><div class="children"><div class="content">You didn&#x27;t have a choice when it came to protocols for the Internet; it&#x27;s TCP&#x2F;IP and DNS or you don&#x27;t get to play. Everyone was running dual stack to support their LAN and Internet and you had no choice with one of them. So, everything went TCP&#x2F;IP and reduced overall complexity.</div><br/></div></div><div id="38647589" class="c"><input type="checkbox" id="c-38647589" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647099">parent</a><span>|</span><a href="#38651165">prev</a><span>|</span><a href="#38647777">next</a><span>|</span><label class="collapse" for="c-38647589">[-]</label><label class="expand" for="c-38647589">[7 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s rare to find an operating system that doesn&#x27;t implement the POSIX API.<p>Except that it has barelly improved beyond CLI and daemons, still thinks terminals are the only hardware, everything else that matters isn&#x27;t part of it, not even more modern networking protocols that aren&#x27;t exposed in socket configurations.</div><br/><div id="38647677" class="c"><input type="checkbox" id="c-38647677" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647589">parent</a><span>|</span><a href="#38647777">next</a><span>|</span><label class="collapse" for="c-38647677">[-]</label><label class="expand" for="c-38647677">[6 more]</label></div><br/><div class="children"><div class="content">Its purpose was to create compatibility between Unix vendors so developers could write software compatible with different flavors. The primary market for Unix vendors is servers, which to this day are still about CLI and daemons, and POSIX systems continue to have dominant market share in that market.</div><br/><div id="38651987" class="c"><input type="checkbox" id="c-38651987" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647677">parent</a><span>|</span><a href="#38651746">next</a><span>|</span><label class="collapse" for="c-38651987">[-]</label><label class="expand" for="c-38651987">[1 more]</label></div><br/><div class="children"><div class="content">Maybe it&#x27;s POSIX that is holding new developments back because people think it&#x27;s good enough. It&#x27;s not &#x27;60s anymore.
I would have expected to have totally new paradigms in 2023 if you asked me 23 years ago. Even NT kernel seems more modern.<p>While POSIX was state of the art when it was invented, it shouldn&#x27;t be today.<p>Lots of research was thrown in recycle bin because &quot;Hey, we have POSIX, why reinvent the wheel?&quot;, up to the point that nobody wants to do operating systems research today, because they don&#x27;t want their hard work to get thrown into the same recycle bin.<p>I think that people who invented POSIX were innovators and have they live today, they would come with a totally new paradigm, more fit to today&#x27;s needs and knowledge.</div><br/></div></div><div id="38651746" class="c"><input type="checkbox" id="c-38651746" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647677">parent</a><span>|</span><a href="#38651987">prev</a><span>|</span><a href="#38648091">next</a><span>|</span><label class="collapse" for="c-38651746">[-]</label><label class="expand" for="c-38651746">[1 more]</label></div><br/><div class="children"><div class="content">Nah, POSIX on servers is only relevant enough for language runtimes and compilers, which then use their own package managers and cloud APIs for everything else.<p>Alongside a cloud shell, which yeah, we now have a VT100 running on a browser window.<p>There is a reason why there are USENIX papers on the loss of POSIX relevance.</div><br/></div></div><div id="38648091" class="c"><input type="checkbox" id="c-38648091" checked=""/><div class="controls bullet"><span class="by">nvm0n2</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647677">parent</a><span>|</span><a href="#38651746">prev</a><span>|</span><a href="#38647777">next</a><span>|</span><label class="collapse" for="c-38648091">[-]</label><label class="expand" for="c-38648091">[3 more]</label></div><br/><div class="children"><div class="content">Arguably the dominant APIs in the server space are the cloud APIs not POSIX.</div><br/><div id="38648346" class="c"><input type="checkbox" id="c-38648346" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648091">parent</a><span>|</span><a href="#38647777">next</a><span>|</span><label class="collapse" for="c-38648346">[-]</label><label class="expand" for="c-38648346">[2 more]</label></div><br/><div class="children"><div class="content">Many of which are also open, like OpenStack or K8s, or have third party implementations, like Ceph implementing the Amazon S3 API.</div><br/><div id="38650023" class="c"><input type="checkbox" id="c-38650023" checked=""/><div class="controls bullet"><span class="by">natbennett</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648346">parent</a><span>|</span><a href="#38647777">next</a><span>|</span><label class="collapse" for="c-38650023">[-]</label><label class="expand" for="c-38650023">[1 more]</label></div><br/><div class="children"><div class="content">Also all reimplementations of proprietary technology.<p>The S3 API is a <i>really</i> good example of the “OSS only becomes dominant when development slows down” principle. As a friend of mine who has had to support a lot of local blob storage says, “On the gates of hell are emblazoned — S3 compatible.”</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38647777" class="c"><input type="checkbox" id="c-38647777" checked=""/><div class="controls bullet"><span class="by">paulddraper</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647099">parent</a><span>|</span><a href="#38647589">prev</a><span>|</span><a href="#38649534">next</a><span>|</span><label class="collapse" for="c-38647777">[-]</label><label class="expand" for="c-38647777">[6 more]</label></div><br/><div class="children"><div class="content">I think OP&#x27;s point was less that the tech didn&#x27;t work (e.g. OpenGL was fantastically successful) and that it didn&#x27;t produce a good outcome for the &quot;loser&quot; companies that supported it.</div><br/><div id="38648280" class="c"><input type="checkbox" id="c-38648280" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647777">parent</a><span>|</span><a href="#38649534">next</a><span>|</span><label class="collapse" for="c-38648280">[-]</label><label class="expand" for="c-38648280">[5 more]</label></div><br/><div class="children"><div class="content">The point of the open standard is to untether your prospective customers from the incumbent. That means nobody is going to monopolize that technology anymore, but that works out fine when it&#x27;s not the thing you&#x27;re trying to sell -- AMD and Intel aren&#x27;t trying to sell software libraries, they&#x27;re trying to sell GPUs.<p>And this strategy regularly works out for companies. It&#x27;s Commoditize Your Complement.<p>If you&#x27;re Intel you support Linux and other open source software so you can sell hardware that competes with vertically integrated vendors like DEC. This has gone <i>very</i> well for Intel -- proprietary RISC server architectures are basically dead, and Linux dominates much of the server market in which case they don&#x27;t have to share their margins with Microsoft. The main survivor is IBM, which is another company that has embraced open standards. It might have also worked out for Sun but they failed to make competitive <i>hardware</i>, which is not optional.<p>We see this all over the place. Google&#x27;s most successful &quot;messaging service&quot; is Gmail, using standard SMTP. It&#x27;s rare to the point of notability for a modern internet service to use all proprietary networking protocols instead of standard HTTP and TCP and DNS, but many of them are extremely successful.<p>And some others are barely scraping by, but they <i>exist</i>, which they wouldn&#x27;t if there wasn&#x27;t a standard they could use instead of a proprietary system they were locked out of.</div><br/><div id="38648482" class="c"><input type="checkbox" id="c-38648482" checked=""/><div class="controls bullet"><span class="by">paulddraper</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648280">parent</a><span>|</span><a href="#38649534">next</a><span>|</span><label class="collapse" for="c-38648482">[-]</label><label class="expand" for="c-38648482">[4 more]</label></div><br/><div class="children"><div class="content">&gt; The point of the open standard is to untether your prospective customers from the incumbent.<p>That is the point.<p>But FWIW the incumbent adopts it and dominates anyway. (Though you now are technically &quot;untethered.&quot;)</div><br/><div id="38648631" class="c"><input type="checkbox" id="c-38648631" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648482">parent</a><span>|</span><a href="#38649534">next</a><span>|</span><label class="collapse" for="c-38648631">[-]</label><label class="expand" for="c-38648631">[3 more]</label></div><br/><div class="children"><div class="content">&gt; But FWIW the incumbent adopts it and dominates anyway. (Though you now are technically &quot;untethered.&quot;)<p>That&#x27;s assuming the incumbent&#x27;s advantage isn&#x27;t rooted in the lock-in.<p>If ML was suddenly untethered from CUDA, now you&#x27;re competing on hardware. Intel would still have mediocre GPUs, but AMD&#x27;s are competitive, and Intel&#x27;s could be in the near future if they execute competently.<p>The open standard doesn&#x27;t automatically give you the win, but it puts you in the ring.<p>And either of them have the potential to gain an advantage over Nvidia by integrating GPUs with their x86_64 CPUs, e.g. so the CPU and GPU can share memory, avoiding copying over PCIe and giving the CPU direct access to HBM. They could even put a cut down but compatible version of the technology in every commodity PC by default, giving them a huge installed base of hardware that encourages developers to target it.</div><br/><div id="38650839" class="c"><input type="checkbox" id="c-38650839" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648631">parent</a><span>|</span><a href="#38649534">next</a><span>|</span><label class="collapse" for="c-38650839">[-]</label><label class="expand" for="c-38650839">[2 more]</label></div><br/><div class="children"><div class="content">If the software side no longer mattered, I would expect all three vendors would magically start competing on available RAM. A slower card with double today&#x27;s RAM would absolutely sell.</div><br/><div id="38651661" class="c"><input type="checkbox" id="c-38651661" checked=""/><div class="controls bullet"><span class="by">sakras</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650839">parent</a><span>|</span><a href="#38649534">next</a><span>|</span><label class="collapse" for="c-38651661">[-]</label><label class="expand" for="c-38651661">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A slower card with double today&#x27;s RAM would absolutely sell<p>Absolutely, SQL analytics people (like me) have been itching for a viable GPU for analytics for years now. The price&#x2F;performance just isn&#x27;t there yet because there&#x27;s such a bias towards high compute and low memory.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38649534" class="c"><input type="checkbox" id="c-38649534" checked=""/><div class="controls bullet"><span class="by">gamblor956</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647099">parent</a><span>|</span><a href="#38647777">prev</a><span>|</span><a href="#38651479">next</a><span>|</span><label class="collapse" for="c-38649534">[-]</label><label class="expand" for="c-38649534">[2 more]</label></div><br/><div class="children"><div class="content">Windows still uses WINS and NetBIOS when DNS is unavailable.</div><br/><div id="38651188" class="c"><input type="checkbox" id="c-38651188" checked=""/><div class="controls bullet"><span class="by">binkHN</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649534">parent</a><span>|</span><a href="#38651479">next</a><span>|</span><label class="collapse" for="c-38651188">[-]</label><label class="expand" for="c-38651188">[1 more]</label></div><br/><div class="children"><div class="content">In a business-class network, even one running Windows, if DNS breaks &quot;everything&quot; is going to break.</div><br/></div></div></div></div></div></div></div></div><div id="38651479" class="c"><input type="checkbox" id="c-38651479" checked=""/><div class="controls bullet"><span class="by">goldenshale</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38646396">prev</a><span>|</span><a href="#38647250">next</a><span>|</span><label class="collapse" for="c-38651479">[-]</label><label class="expand" for="c-38651479">[1 more]</label></div><br/><div class="children"><div class="content">Seriously, why don&#x27;t they just dedicate a group to creating the best pytorch backend possible?  Proving it there will gain researcher traction and prove that their hardware is worth porting the other stuff over to.</div><br/></div></div><div id="38647250" class="c"><input type="checkbox" id="c-38647250" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38651479">prev</a><span>|</span><a href="#38646336">next</a><span>|</span><label class="collapse" for="c-38647250">[-]</label><label class="expand" for="c-38647250">[7 more]</label></div><br/><div class="children"><div class="content">Nvidia is probably ten times more scared of this guy <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov">https:&#x2F;&#x2F;github.com&#x2F;ggerganov</a> than Intel or AMD.</div><br/><div id="38648019" class="c"><input type="checkbox" id="c-38648019" checked=""/><div class="controls bullet"><span class="by">ebb_earl_co</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647250">parent</a><span>|</span><a href="#38646336">next</a><span>|</span><label class="collapse" for="c-38648019">[-]</label><label class="expand" for="c-38648019">[6 more]</label></div><br/><div class="children"><div class="content">Can you expand on this? This is my first time seeing this guy’s work</div><br/><div id="38648261" class="c"><input type="checkbox" id="c-38648261" checked=""/><div class="controls bullet"><span class="by">pbronez</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648019">parent</a><span>|</span><a href="#38646336">next</a><span>|</span><label class="collapse" for="c-38648261">[-]</label><label class="expand" for="c-38648261">[5 more]</label></div><br/><div class="children"><div class="content">He’s the main developer of Llama.cpp, which allows you to run a wide range of open-weights models on a wide range of non-NVIDIA processors.</div><br/><div id="38650068" class="c"><input type="checkbox" id="c-38650068" checked=""/><div class="controls bullet"><span class="by">fl0id</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38648261">parent</a><span>|</span><a href="#38646336">next</a><span>|</span><label class="collapse" for="c-38650068">[-]</label><label class="expand" for="c-38650068">[4 more]</label></div><br/><div class="children"><div class="content">but it&#x27;s all inference, and most of Nvidias moat is in training afaik.</div><br/><div id="38651636" class="c"><input type="checkbox" id="c-38651636" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650068">parent</a><span>|</span><a href="#38650879">next</a><span>|</span><label class="collapse" for="c-38651636">[-]</label><label class="expand" for="c-38651636">[1 more]</label></div><br/><div class="children"><div class="content">There is an example of training <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;1f0bccb27929e261744c979bc75114955da49e98&#x2F;examples&#x2F;train-text-from-scratch">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;1f0bccb27929e261...</a><p>But that&#x27;s absolutely false about the Nvidia moat being only training. Llama.cpp makes it far more practical run inference on a variety of devices. Including ones with or without Nvidia hardware.</div><br/></div></div><div id="38650879" class="c"><input type="checkbox" id="c-38650879" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650068">parent</a><span>|</span><a href="#38651636">prev</a><span>|</span><a href="#38646336">next</a><span>|</span><label class="collapse" for="c-38650879">[-]</label><label class="expand" for="c-38650879">[2 more]</label></div><br/><div class="children"><div class="content">People have really bizarre overdramatic misunderstandings of llama.cpp because they used it a few times to cook their laptop. This one really got me giggling though.</div><br/><div id="38651657" class="c"><input type="checkbox" id="c-38651657" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650879">parent</a><span>|</span><a href="#38646336">next</a><span>|</span><label class="collapse" for="c-38651657">[-]</label><label class="expand" for="c-38651657">[1 more]</label></div><br/><div class="children"><div class="content">I am integrating llama.cpp into my application. I just went through one of their text generation examples line-by-line and converted it into my own class.<p>This is a leading-edge software library that provides a huge boost for non-Nvidia hardware in terms of inference capability with quantized models.<p>If you don&#x27;t understand that, then you have missed an important development in the space of machine learning.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38646336" class="c"><input type="checkbox" id="c-38646336" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38647250">prev</a><span>|</span><a href="#38645947">next</a><span>|</span><label class="collapse" for="c-38646336">[-]</label><label class="expand" for="c-38646336">[22 more]</label></div><br/><div class="children"><div class="content">This right here. Until Intel (and&#x2F;or AMD) get serious about the software side and actually invest the money CUDA isn&#x27;t going anywhere. Intel will make noises about various initiatives in that direction and then a quarter or two later they&#x27;ll make big cuts in those divisions. They need to make a multi-year commitment and do some serious hiring (and they&#x27;ll need to raise their salaries to market rates to do this) if they want to play in the CUDA space.</div><br/><div id="38646668" class="c"><input type="checkbox" id="c-38646668" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646336">parent</a><span>|</span><a href="#38651594">next</a><span>|</span><label class="collapse" for="c-38646668">[-]</label><label class="expand" for="c-38646668">[20 more]</label></div><br/><div class="children"><div class="content">Literally, every single announcement (and action) coming out of AMD these days is that they are serious about the software. I don&#x27;t see any reason at this point to doubt them.<p>The larger issue is that they need to fix the access to their high end GPUs. You can&#x27;t rent a MI250... or even a MI300x (yet, I&#x27;m working on that myself!). But that said, you can&#x27;t rent an H100 either... there are none available.</div><br/><div id="38649460" class="c"><input type="checkbox" id="c-38649460" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646668">parent</a><span>|</span><a href="#38652049">next</a><span>|</span><label class="collapse" for="c-38649460">[-]</label><label class="expand" for="c-38649460">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Literally, every single announcement (and action) coming out of AMD these days is that they are serious about the software. I don&#x27;t see any reason at this point to doubt them.<p>They&#x27;re having to announce it so much <i>because</i> people are rightly sceptical. Talk is cheap, and their software has sucked for years. Have they given concrete proof of their commitment, e.g. they&#x27;ve spent X dollars or hired Y people to work on it (or big names Z and W)?</div><br/><div id="38649871" class="c"><input type="checkbox" id="c-38649871" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649460">parent</a><span>|</span><a href="#38651590">prev</a><span>|</span><a href="#38652049">next</a><span>|</span><label class="collapse" for="c-38649871">[-]</label><label class="expand" for="c-38649871">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. Time will tell.<p>MI300x and ROCm 6 and their support of projects like Pytorch, are all good steps in the right direction. HuggingFace now supports ROCm.</div><br/></div></div></div></div><div id="38652049" class="c"><input type="checkbox" id="c-38652049" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646668">parent</a><span>|</span><a href="#38649460">prev</a><span>|</span><a href="#38647674">next</a><span>|</span><label class="collapse" for="c-38652049">[-]</label><label class="expand" for="c-38652049">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Literally, every single announcement (and action) coming out of AMD these days is that they are serious about the software. I don&#x27;t see any reason at this point to doubt them.<p>I have a bridge to sell.</div><br/><div id="38652159" class="c"><input type="checkbox" id="c-38652159" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38652049">parent</a><span>|</span><a href="#38647674">next</a><span>|</span><label class="collapse" for="c-38652159">[-]</label><label class="expand" for="c-38652159">[1 more]</label></div><br/><div class="children"><div class="content">It is way, way better in the last year or so. Perfectly reasonable cards for inference if you actually understand the stack and know how to use it. Is nVidia faster? Sure, but at twice the price for 20-30% gains. If that makes sense for you, keep paying the tax.</div><br/></div></div></div></div><div id="38647674" class="c"><input type="checkbox" id="c-38647674" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646668">parent</a><span>|</span><a href="#38652049">prev</a><span>|</span><a href="#38651594">next</a><span>|</span><label class="collapse" for="c-38647674">[-]</label><label class="expand" for="c-38647674">[14 more]</label></div><br/><div class="children"><div class="content">&lt;removed&gt;</div><br/><div id="38647699" class="c"><input type="checkbox" id="c-38647699" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647674">parent</a><span>|</span><a href="#38647805">next</a><span>|</span><label class="collapse" for="c-38647699">[-]</label><label class="expand" for="c-38647699">[2 more]</label></div><br/><div class="children"><div class="content">ROCm is open source.<p>and: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pVl25BbczLI" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pVl25BbczLI</a></div><br/></div></div><div id="38647805" class="c"><input type="checkbox" id="c-38647805" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647674">parent</a><span>|</span><a href="#38647699">prev</a><span>|</span><a href="#38647720">next</a><span>|</span><label class="collapse" for="c-38647805">[-]</label><label class="expand" for="c-38647805">[10 more]</label></div><br/><div class="children"><div class="content">What @rowan removed were these two comments:<p>&quot;AMD is not serious, and neither is Intel for that matter. Their software are piles of proprietary garbage fires. They may say they are serious but literally nothing indicates they are.&quot;<p>&quot;Yes, and ROCm also doesn&#x27;t work on anything non-AMD. In fact it doesn&#x27;t even work on all recent AMD gpus. T&quot;</div><br/><div id="38649227" class="c"><input type="checkbox" id="c-38649227" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647805">parent</a><span>|</span><a href="#38647720">next</a><span>|</span><label class="collapse" for="c-38649227">[-]</label><label class="expand" for="c-38649227">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not too polite to repost what people removed. Errors on the internet shouldn&#x27;t haunt people forever.<p>However, my experience is that the comments about AMD are spot-on, with the exception of the word &quot;proprietary.&quot;<p>Intel hasn&#x27;t gotten serious yet, and has a good track record in other domains (compilers, numerical libraries, etc.). They&#x27;ve been flailing for a while, but I&#x27;m curious if they&#x27;ll come up with something okay.</div><br/><div id="38650337" class="c"><input type="checkbox" id="c-38650337" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649227">parent</a><span>|</span><a href="#38650508">next</a><span>|</span><label class="collapse" for="c-38650337">[-]</label><label class="expand" for="c-38650337">[1 more]</label></div><br/><div class="children"><div class="content">As a long time user of intels scientific compiler&#x2F;accelerator stack, I&#x27;m not sure if I&#x27;d call it a &quot;good track record&quot;. Once you get all their libs working they tend to be fairly well optimized but they&#x27;re always a huge hassle to install, configure, and distribute. And when I say a hassle to install, I&#x27;m talking about hours to run their installers.<p>They have a track record of taking open projects, adding proprietary extensions, and then requiring those extensions to work with other tools. This sounds fine, but they are very slow&#x2F;never update the base libs. From version to version they&#x27;ll muck with deep dependencies, sometimes they&#x27;ll even ship different rules on different platforms (I dare you to try to static link openmp in a recent version of oneapi targeting windows). If you ship a few tools (let&#x27;s say A and B) that use the same dynamic lib, it&#x27;s a royal pain to make sure they don&#x27;t conflict with each other if you update software A but not B. Ranting about consumer junk aside, their cluster focused tooling on Linux tends to be quite good, especially compared to amd.</div><br/></div></div><div id="38650508" class="c"><input type="checkbox" id="c-38650508" checked=""/><div class="controls bullet"><span class="by">Qwertious</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649227">parent</a><span>|</span><a href="#38650337">prev</a><span>|</span><a href="#38649792">next</a><span>|</span><label class="collapse" for="c-38650508">[-]</label><label class="expand" for="c-38650508">[4 more]</label></div><br/><div class="children"><div class="content">&gt;It&#x27;s not too polite to repost what people removed. Errors on the internet shouldn&#x27;t haunt people forever.<p>IMO the comment deletion system handles deleting your own comment wrong - it should grey out the comment and strikethrough it, and label it &quot;comment disavowed&quot; or something with the username removed, but it shouldn&#x27;t actually delete the comment.<p>Deleting the comment damages the history, and makes the comment chain hard to follow.</div><br/><div id="38650653" class="c"><input type="checkbox" id="c-38650653" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650508">parent</a><span>|</span><a href="#38649792">next</a><span>|</span><label class="collapse" for="c-38650653">[-]</label><label class="expand" for="c-38650653">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure history should be sacred. In the world of science, this is important, but otherwise, we didn&#x27;t used to live in a universe where every embarrassing thing we did in middle school would haunt us in our old age.<p>I feel bad for the younger generation. Privacy is important, and more so than &quot;the history.&quot;</div><br/><div id="38650868" class="c"><input type="checkbox" id="c-38650868" checked=""/><div class="controls bullet"><span class="by">code_biologist</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650653">parent</a><span>|</span><a href="#38649792">next</a><span>|</span><label class="collapse" for="c-38650868">[-]</label><label class="expand" for="c-38650868">[2 more]</label></div><br/><div class="children"><div class="content">Long ago, under a different account, I emailed dang about removing some comment content that became too identifying only years after the comments in question. He did so and was very gracious about it. dang, you&#x27;re cool af and you make HN a great place!</div><br/><div id="38650899" class="c"><input type="checkbox" id="c-38650899" checked=""/><div class="controls bullet"><span class="by">krapp</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650868">parent</a><span>|</span><a href="#38649792">next</a><span>|</span><label class="collapse" for="c-38650899">[-]</label><label class="expand" for="c-38650899">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re being very obsequious about having to personally get approval from a moderator to do on your behalf something every other forum lets you do on your own by default.</div><br/></div></div></div></div></div></div></div></div><div id="38649792" class="c"><input type="checkbox" id="c-38649792" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649227">parent</a><span>|</span><a href="#38650508">prev</a><span>|</span><a href="#38647720">next</a><span>|</span><label class="collapse" for="c-38649792">[-]</label><label class="expand" for="c-38649792">[3 more]</label></div><br/><div class="children"><div class="content">Normally, I wouldn&#x27;t do that, but this time I felt like both comments were intentionally inflammatory and then the context for my response was lost.<p>&quot;literally nothing&quot; is also wrong given that they just had a large press announcement on Dec 6th (yt as part of my response below), where they spent 2 hours saying (and showing) they are serious.<p>The second comment was made and then immediately deleted, in a way that was to send me a message directly. It is what irked me enough to post their comments back.</div><br/><div id="38651433" class="c"><input type="checkbox" id="c-38651433" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649792">parent</a><span>|</span><a href="#38647720">next</a><span>|</span><label class="collapse" for="c-38651433">[-]</label><label class="expand" for="c-38651433">[2 more]</label></div><br/><div class="children"><div class="content">I deleted it because I was in an extremely bad mood and later realized it was simply wrong of me to post it and vent my unrelated frustration in those comments. I think it&#x27;s in extremely bad taste to repost what I wrote when I made the clear choice to delete it.</div><br/><div id="38652080" class="c"><input type="checkbox" id="c-38652080" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38651433">parent</a><span>|</span><a href="#38647720">next</a><span>|</span><label class="collapse" for="c-38652080">[-]</label><label class="expand" for="c-38652080">[1 more]</label></div><br/><div class="children"><div class="content">If you didn&#x27;t say it, the rocks would cry out. Personally, I&#x27;ll believe AMD is <i>maybe</i> serious about Rocm if they make it a year without breaking their Debian repository.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38647720" class="c"><input type="checkbox" id="c-38647720" checked=""/><div class="controls bullet"><span class="by">mepian</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38647674">parent</a><span>|</span><a href="#38647805">prev</a><span>|</span><a href="#38651594">next</a><span>|</span><label class="collapse" for="c-38647720">[-]</label><label class="expand" for="c-38647720">[1 more]</label></div><br/><div class="children"><div class="content">How’s SYCL proprietary exactly?</div><br/></div></div></div></div></div></div><div id="38651594" class="c"><input type="checkbox" id="c-38651594" checked=""/><div class="controls bullet"><span class="by">viewtransform</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646336">parent</a><span>|</span><a href="#38646668">prev</a><span>|</span><a href="#38645947">next</a><span>|</span><label class="collapse" for="c-38651594">[-]</label><label class="expand" for="c-38651594">[1 more]</label></div><br/><div class="children"><div class="content">AMD made a presentation on their AI software strategy at Microsoft Ignite two weeks ago. Worth a watch for the slides and live demo<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;7jqZBTduhAQ?t=61" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;7jqZBTduhAQ?t=61</a></div><br/></div></div></div></div><div id="38645947" class="c"><input type="checkbox" id="c-38645947" checked=""/><div class="controls bullet"><span class="by">_the_inflator</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38646336">prev</a><span>|</span><a href="#38646670">next</a><span>|</span><label class="collapse" for="c-38645947">[-]</label><label class="expand" for="c-38645947">[1 more]</label></div><br/><div class="children"><div class="content">I agree. That’s what MS didn’t understand with cloud and Linux at first.<p>There is more than just a hardware layer to adoption.<p>CUDA is a platform is an ecosystem is also some sort of attitude. It won’t go away. Companies invested a lot into it.</div><br/></div></div><div id="38646670" class="c"><input type="checkbox" id="c-38646670" checked=""/><div class="controls bullet"><span class="by">Keyframe</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38645947">prev</a><span>|</span><a href="#38646553">next</a><span>|</span><label class="collapse" for="c-38646670">[-]</label><label class="expand" for="c-38646670">[2 more]</label></div><br/><div class="children"><div class="content">Guys need to do ye olde embrace, extend maneuver. What wine did, what Javas of the world did. CUDA driver API and CUDA runtime API either translation or implementation layer that offers compatibility and speed. I see no way around it at this point, for now.</div><br/><div id="38650272" class="c"><input type="checkbox" id="c-38650272" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38646670">parent</a><span>|</span><a href="#38646553">next</a><span>|</span><label class="collapse" for="c-38650272">[-]</label><label class="expand" for="c-38650272">[1 more]</label></div><br/><div class="children"><div class="content">They could do that. That would eliminate Nvidia&#x27;s monopoly. AMD has made gestures in that direction with Hip. But they ultimately don&#x27;t want to do that - Hip support is half-assed and inconsistent. AMD creates and abandons a variety of APIs. So the conclusion is the other chip makers whine about Nvidia&#x27;s monopoly but don&#x27;t want to end it - they just want maneuver to get their smaller monopolies of some sort or other.</div><br/></div></div></div></div><div id="38646553" class="c"><input type="checkbox" id="c-38646553" checked=""/><div class="controls bullet"><span class="by">papichulo2023</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38646670">prev</a><span>|</span><a href="#38650632">next</a><span>|</span><label class="collapse" for="c-38646553">[-]</label><label class="expand" for="c-38646553">[1 more]</label></div><br/><div class="children"><div class="content">Wasnt Intel the biggest supporter of OpenCV? I dont know any open source heavely supported by Nvidia</div><br/></div></div><div id="38650632" class="c"><input type="checkbox" id="c-38650632" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38646553">prev</a><span>|</span><a href="#38652106">next</a><span>|</span><label class="collapse" for="c-38650632">[-]</label><label class="expand" for="c-38650632">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but nobody gets that it&#x27;s the software and ecosystem.<p>Really? Because I’m confident Intel knows exactly what it’s about. Have you looked at their contributions to Linux and open source in general?  They employ thousands of software developers.</div><br/><div id="38651750" class="c"><input type="checkbox" id="c-38651750" checked=""/><div class="controls bullet"><span class="by">madaxe_again</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38650632">parent</a><span>|</span><a href="#38652106">next</a><span>|</span><label class="collapse" for="c-38651750">[-]</label><label class="expand" for="c-38651750">[1 more]</label></div><br/><div class="children"><div class="content">Which is why intel left nvidia and everyone else in the dust with CUDA, which they developed.<p>Oh, wait…</div><br/></div></div></div></div><div id="38652106" class="c"><input type="checkbox" id="c-38652106" checked=""/><div class="controls bullet"><span class="by">MrBuddyCasino</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38650632">prev</a><span>|</span><a href="#38647680">next</a><span>|</span><label class="collapse" for="c-38652106">[-]</label><label class="expand" for="c-38652106">[1 more]</label></div><br/><div class="children"><div class="content">They still don&#x27;t get that those who are serious about hardware, must make their own software.</div><br/></div></div><div id="38649369" class="c"><input type="checkbox" id="c-38649369" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38647680">prev</a><span>|</span><a href="#38650874">next</a><span>|</span><label class="collapse" for="c-38649369">[-]</label><label class="expand" for="c-38649369">[3 more]</label></div><br/><div class="children"><div class="content">Both AMD and Intel (and Qualcomm to some degree) just don&#x27;t seem to get how you beat NVIDIA.<p>If they want to grab a piece of NVIDIA&#x27;s pie, they do NOT need to build something better than an H100 right away. There are a million consumers who are happy with a 4090 or 4080 or even 3080 and would love for something that&#x27;s equally capable at half price, and moreover, actually available for purchase, from Amazon&#x2F;NewEgg&#x2F;wherever, and without a &quot;call for pricing&quot; button. AMD and Intel are much better at making their chips available for purchase than NVIDIA. But that&#x27;s not enough.<p>What they DO need to do to take a piece of NVIDIA&#x27;s pie is to build &quot;intelcc&quot;, &quot;amdcc&quot;, and &quot;qualcommcc&quot; that accept the EXACT SAME code that people feed to &quot;nvcc&quot; so that it compiles as-is, with not a single function prototype being different, no questions asked, and works on the target hardware. It needs to just be a drop-in replacement for CUDA.<p>When that is done, recompiling PyTorch and everything else to use other chips will be trivial.</div><br/><div id="38652064" class="c"><input type="checkbox" id="c-38652064" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649369">parent</a><span>|</span><a href="#38651435">next</a><span>|</span><label class="collapse" for="c-38652064">[-]</label><label class="expand" for="c-38652064">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Both AMD and Intel (and Qualcomm to some degree) just don&#x27;t seem to get how you beat NVIDIA.<p>That&#x27;s simple, just do what Nvidia did. Be better than the competition.</div><br/></div></div><div id="38651435" class="c"><input type="checkbox" id="c-38651435" checked=""/><div class="controls bullet"><span class="by">flamedoge</span><span>|</span><a href="#38645864">root</a><span>|</span><a href="#38649369">parent</a><span>|</span><a href="#38652064">prev</a><span>|</span><a href="#38650874">next</a><span>|</span><label class="collapse" for="c-38651435">[-]</label><label class="expand" for="c-38651435">[1 more]</label></div><br/><div class="children"><div class="content">thats what hip is though. a recompiler for cuda code. its not good enough for ptx assembly to amdgpu yet</div><br/></div></div></div></div><div id="38646313" class="c"><input type="checkbox" id="c-38646313" checked=""/><div class="controls bullet"><span class="by">claytonjy</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38650874">prev</a><span>|</span><a href="#38649443">next</a><span>|</span><label class="collapse" for="c-38646313">[-]</label><label class="expand" for="c-38646313">[1 more]</label></div><br/><div class="children"><div class="content">huh, I didn&#x27;t even know openvino supported anything but CPUs! TIL</div><br/></div></div><div id="38649443" class="c"><input type="checkbox" id="c-38649443" checked=""/><div class="controls bullet"><span class="by">2OEH8eoCRo0</span><span>|</span><a href="#38645864">parent</a><span>|</span><a href="#38646313">prev</a><span>|</span><a href="#38645594">next</a><span>|</span><label class="collapse" for="c-38649443">[-]</label><label class="expand" for="c-38649443">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the software incompatibilities mean you&#x27;ll spend a ton of time getting it to work compared to an Nvidia GPU.<p>Leverage LLMs to port the SW</div><br/></div></div></div></div><div id="38645594" class="c"><input type="checkbox" id="c-38645594" checked=""/><div class="controls bullet"><span class="by">brindlejim</span><span>|</span><a href="#38645864">prev</a><span>|</span><a href="#38645601">next</a><span>|</span><label class="collapse" for="c-38645594">[-]</label><label class="expand" for="c-38645594">[42 more]</label></div><br/><div class="children"><div class="content">Fun fact: More than half of all engineers at NVIDIA are software engineers. Jensen has deliberately and strategically built a powerful software stack on top of his GPUs, and he&#x27;s spent decades doing it.<p>Until Intel finds a CEO who is as technical and strategic, as opposed to the bean-counters, I doubt that they will manage to organize a successful counterattack on CUDA.</div><br/><div id="38647740" class="c"><input type="checkbox" id="c-38647740" checked=""/><div class="controls bullet"><span class="by">tester756</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38652319">next</a><span>|</span><label class="collapse" for="c-38647740">[-]</label><label class="expand" for="c-38647740">[12 more]</label></div><br/><div class="children"><div class="content">&gt;finds a CEO who is as technical and strategic, as opposed to the bean-counters<p>Did you just call Gelsinger a &quot;non-technical&quot;? wow, how out of touch with reality<p>&gt;Gelsinger first joined Intel at 18 years old in 1979 just after earning an associate degree from Lincoln Tech.[9] He spent much of his career with the company in Oregon,[12] where he maintains a home.[13] In 1987, he co-authored his first book about programming the 80386 microprocessor.[14][1] Gelsinger was the lead architect of the 4th generation 80486 processor[1] introduced in 1989.[9] At age 32, he was named the youngest vice president in Intel&#x27;s history.[7] Mentored by Intel CEO Andrew Grove, Gelsinger became the company&#x27;s CTO in 2001, leading key technology developments, including Wi-Fi, USB, Intel Core and Intel Xeon processors, and 14 chip projects.[2][15] He launched the Intel Developer Forum conference as a counterpart to Microsoft&#x27;s WinHEC.</div><br/><div id="38652202" class="c"><input type="checkbox" id="c-38652202" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38647740">parent</a><span>|</span><a href="#38649007">next</a><span>|</span><label class="collapse" for="c-38652202">[-]</label><label class="expand" for="c-38652202">[1 more]</label></div><br/><div class="children"><div class="content">Gelsinger is a typical hardware engineer out of his depth competing against what is effctively a software play. This is a recurring theme in the industry where you have successful hardware companies with strong hardware focused leadership fail over time because they don&#x27;t get software.<p>I used to work at Nokia Research. The problem was on full display during the period Apple made it&#x27;s entry into mobile. We had plenty of great software people throughout the company. But the leadership had grown up in a world where Nokia was basically making and selling hardware. Radio engineers and hardware engineers basically. They did not get software all that well. And of course what Apple did was executing really well on software for what was initially a nice but not particularly impressive bit of hardware. It&#x27;s the software that made the difference. The hardware excellence came later. And the software only got better over time. Nokia never recovered from that. And they tried really hard to fix the software. It failed. They couldn&#x27;t do it. Symbian was a train wreck and flopped hard in the market.<p>Intel is facing the same issue here. Their hardware is only useful if there&#x27;s great software to do something with it. The whole point of hardware is running software. And Intel is not in the software business so they need others to do that for them. Similar to Nokia, Apple came along and showed the world that you don&#x27;t need Intel hardware to deliver a great software experience. Now their competitor NVidia is basically stealing their thunder in the AI and 3D graphics market. Intel wants in but just like they failed to get into the mobile market (they tried, with Nokia even), their efforts to enter this market are also crippled by their software ineptness.<p>This is a lesson that many IOT companies struggle with as well. Great hardware but they typically struggle with their software ecosystems and unlocking the value of the hardware.  So much so that one Finnish software company in this space (Wirepas), has been running an absolute genius marketing campaign with the beautiful slogan &quot;Most IOT is shit&quot;. Check out their website. Some very nice Finnish humor on display there. Their blunt message is that most hardware focused IOT companies are hopelessly clumsy on the software front and they of course provide a solution.</div><br/></div></div><div id="38649007" class="c"><input type="checkbox" id="c-38649007" checked=""/><div class="controls bullet"><span class="by">brindlejim</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38647740">parent</a><span>|</span><a href="#38652202">prev</a><span>|</span><a href="#38650062">next</a><span>|</span><label class="collapse" for="c-38649007">[-]</label><label class="expand" for="c-38649007">[7 more]</label></div><br/><div class="children"><div class="content">Intel spent more than a decade under Otellini, Krzanich and Swan. Bean counters.  Gelsinger was appointed out of desperation, but the problem runs much deeper. I doubt that culture is gone. It has already cost Intel many opportunities.</div><br/><div id="38649583" class="c"><input type="checkbox" id="c-38649583" checked=""/><div class="controls bullet"><span class="by">alexey-salmin</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38649007">parent</a><span>|</span><a href="#38649088">next</a><span>|</span><label class="collapse" for="c-38649583">[-]</label><label class="expand" for="c-38649583">[2 more]</label></div><br/><div class="children"><div class="content">Otellini wasn&#x27;t an engineer but still he made the historical x86-mac deal, pushed like crazy for x86-android and owned the top500 with xeon phi.<p>The downfall began with Krzanich who had no goal besides raising the stock price and no strategy other than cutting long-term projects and other costs that got in the way. What a shame.</div><br/><div id="38651072" class="c"><input type="checkbox" id="c-38651072" checked=""/><div class="controls bullet"><span class="by">selimthegrim</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38649583">parent</a><span>|</span><a href="#38649088">next</a><span>|</span><label class="collapse" for="c-38651072">[-]</label><label class="expand" for="c-38651072">[1 more]</label></div><br/><div class="children"><div class="content">Krzanich started out as an engineer</div><br/></div></div></div></div><div id="38649088" class="c"><input type="checkbox" id="c-38649088" checked=""/><div class="controls bullet"><span class="by">tester756</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38649007">parent</a><span>|</span><a href="#38649583">prev</a><span>|</span><a href="#38650564">next</a><span>|</span><label class="collapse" for="c-38649088">[-]</label><label class="expand" for="c-38649088">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Intel spent more than a decade under Otellini, Krzanich and Swan. Bean counters.<p>It still doesn&#x27;t change mistake in your original message.<p>&gt;Gelsinger was appointed out of desperation, but the problem runs much deeper.<p>How much &quot;much deeper&quot;? VPs? middle level managers? engineers?<p>The example goes from the top, so if he can change the culture at the top, it will eventually get deeper.</div><br/><div id="38650741" class="c"><input type="checkbox" id="c-38650741" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38649088">parent</a><span>|</span><a href="#38650352">next</a><span>|</span><label class="collapse" for="c-38650741">[-]</label><label class="expand" for="c-38650741">[1 more]</label></div><br/><div class="children"><div class="content">&gt;It still doesn&#x27;t change mistake in your original message.<p>Precisely. The problem I found on HN is that It is hard to have any meaningful discussion on anything Hardware. Especially when it is mixed with business or economics models.<p>I was greedy and was hoping Intel could fall closer to $20 in early 2023 before I load up more of their stock. Otherwise I would put more money where my mouth is.</div><br/></div></div><div id="38650352" class="c"><input type="checkbox" id="c-38650352" checked=""/><div class="controls bullet"><span class="by">jbm</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38649088">parent</a><span>|</span><a href="#38650741">prev</a><span>|</span><a href="#38650564">next</a><span>|</span><label class="collapse" for="c-38650352">[-]</label><label class="expand" for="c-38650352">[1 more]</label></div><br/><div class="children"><div class="content">&gt; as technical <i>and strategic</i><p>It seems that this is an AND not an OR.</div><br/></div></div></div></div><div id="38650564" class="c"><input type="checkbox" id="c-38650564" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38649007">parent</a><span>|</span><a href="#38649088">prev</a><span>|</span><a href="#38650062">next</a><span>|</span><label class="collapse" for="c-38650564">[-]</label><label class="expand" for="c-38650564">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Gelsinger was appointed out of desperation,<p>You will need to observe Intel more closely. It was not out of desperation. And Gelsinger is more technical and strategic than you implies.</div><br/></div></div></div></div><div id="38650062" class="c"><input type="checkbox" id="c-38650062" checked=""/><div class="controls bullet"><span class="by">natbennett</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38647740">parent</a><span>|</span><a href="#38649007">prev</a><span>|</span><a href="#38650840">next</a><span>|</span><label class="collapse" for="c-38650062">[-]</label><label class="expand" for="c-38650062">[2 more]</label></div><br/><div class="children"><div class="content">If you read carefully you’ll see the comment is “as technical and strategic.”<p>That’s very different from “non-technical.”<p>He was clearly capable of leading a team to develop a new processor but that’s not the issue here.</div><br/></div></div><div id="38650840" class="c"><input type="checkbox" id="c-38650840" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38647740">parent</a><span>|</span><a href="#38650062">prev</a><span>|</span><a href="#38652319">next</a><span>|</span><label class="collapse" for="c-38650840">[-]</label><label class="expand" for="c-38650840">[1 more]</label></div><br/><div class="children"><div class="content">1989 is 34 years ago.</div><br/></div></div></div></div><div id="38652319" class="c"><input type="checkbox" id="c-38652319" checked=""/><div class="controls bullet"><span class="by">seanalltogether</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38647740">prev</a><span>|</span><a href="#38646194">next</a><span>|</span><label class="collapse" for="c-38652319">[-]</label><label class="expand" for="c-38652319">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia GPU moat has always been their software. Game ready drivers are a big deal for each AAA game launch and they always help to push their fps numbers on reviewers charts. I feel like for 20 years I&#x27;ve been reading people online complain about ATI&#x2F;AMD drivers and how they want to go back to an Nvidia card the next chance they get.</div><br/></div></div><div id="38646194" class="c"><input type="checkbox" id="c-38646194" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38652319">prev</a><span>|</span><a href="#38646064">next</a><span>|</span><label class="collapse" for="c-38646194">[-]</label><label class="expand" for="c-38646194">[12 more]</label></div><br/><div class="children"><div class="content">Gelsinger is saying &quot;the entire industry&quot; and that seems likely to be a simple fact. Every single player, other than Nvidia, has an incentive to minimise the importance of CUDA as a proprietary technology. That is a lot more programmers than Nvidia can afford to employ.<p>Even if Intel falls over its own feet, the incentives to bring in more chip manufacturers are huge. It&#x27;ll happen, the only question is whether the timeframe is months, years or a decade. My guess is shorter timeframes, this seems to mostly be matrix multiplication and there is suddenly a lot of money and attention on the matter. And AMD&#x27;s APU play [0] is starting to reach the high end of the market with the MI300A which is an interesting development.<p>[0] EDIT: For anyone not following that story, they&#x27;ve been unifying system and GPU memory; so if I&#x27;ve understood this correctly there isn&#x27;t any need to &quot;copy data to the GPU&quot; any more on those chips. Basically the CPU will now have big extensions for doing matrix math. Seems likely to catch on. Historically they&#x27;ve been adding that tech to low-end CPU so it isn&#x27;t useful for AI work, now they&#x27;re adding it to the big ones.</div><br/><div id="38646899" class="c"><input type="checkbox" id="c-38646899" checked=""/><div class="controls bullet"><span class="by">amadeuspagel</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646194">parent</a><span>|</span><a href="#38650354">next</a><span>|</span><label class="collapse" for="c-38646899">[-]</label><label class="expand" for="c-38646899">[5 more]</label></div><br/><div class="children"><div class="content">&gt; That is a lot more programmers than Nvidia can afford to employ.<p>How many programmers one can employ is determined by profits, and Nvidia has monopoly profits thanks to CUDA, while &quot;the entire industry&quot; can at best hope to create some commiditized alternative to CUDA. Companies with real market power can beat entire industries of commodity manufacturers, Apple is the prime example.</div><br/><div id="38647233" class="c"><input type="checkbox" id="c-38647233" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646899">parent</a><span>|</span><a href="#38650171">next</a><span>|</span><label class="collapse" for="c-38647233">[-]</label><label class="expand" for="c-38647233">[3 more]</label></div><br/><div class="children"><div class="content">AMD and Intel together have more revenue than Nvidia, even without considering any other player in the industry or any community contributions they get from being open source.</div><br/><div id="38647468" class="c"><input type="checkbox" id="c-38647468" checked=""/><div class="controls bullet"><span class="by">julienfr112</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38647233">parent</a><span>|</span><a href="#38650171">next</a><span>|</span><label class="collapse" for="c-38647468">[-]</label><label class="expand" for="c-38647468">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not about revenue, it&#x27;s about investment. It is closely related to future profit. Not so much to current revenue ...</div><br/><div id="38647542" class="c"><input type="checkbox" id="c-38647542" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38647468">parent</a><span>|</span><a href="#38650171">next</a><span>|</span><label class="collapse" for="c-38647542">[-]</label><label class="expand" for="c-38647542">[1 more]</label></div><br/><div class="children"><div class="content">Profit is revenue minus costs. Investment is costs. If you&#x27;re reinvesting everything you take in your current-year profit would be zero <i>because</i> you&#x27;re making large investments in the future.</div><br/></div></div></div></div></div></div><div id="38650171" class="c"><input type="checkbox" id="c-38650171" checked=""/><div class="controls bullet"><span class="by">tiahura</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646899">parent</a><span>|</span><a href="#38647233">prev</a><span>|</span><a href="#38650354">next</a><span>|</span><label class="collapse" for="c-38650171">[-]</label><label class="expand" for="c-38650171">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re forgetting Microsoft.<p><a href="https:&#x2F;&#x2F;www.cnbc.com&#x2F;2023&#x2F;12&#x2F;06&#x2F;meta-and-microsoft-to-buy-amds-new-ai-chip-as-alternative-to-nvidia.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.cnbc.com&#x2F;2023&#x2F;12&#x2F;06&#x2F;meta-and-microsoft-to-buy-am...</a></div><br/></div></div></div></div><div id="38650354" class="c"><input type="checkbox" id="c-38650354" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646194">parent</a><span>|</span><a href="#38646899">prev</a><span>|</span><a href="#38646589">next</a><span>|</span><label class="collapse" for="c-38650354">[-]</label><label class="expand" for="c-38650354">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Gelsinger is saying &quot;the entire industry&quot; and that seems likely to be a simple fact. Every single player, other than Nvidia, has an incentive to minimise the importance of CUDA as a proprietary technology. That is a lot more programmers than Nvidia can afford to employ.<p>I mean, this statement is technically true, but it&#x27;s true for <i>any</i> proprietary technology. If things work like this then we won&#x27;t have any industry where proprietary techs&#x2F;formats are prevalent.</div><br/><div id="38650558" class="c"><input type="checkbox" id="c-38650558" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38650354">parent</a><span>|</span><a href="#38646589">next</a><span>|</span><label class="collapse" for="c-38650558">[-]</label><label class="expand" for="c-38650558">[3 more]</label></div><br/><div class="children"><div class="content">I suppose, but it is a practical matter here. CUDA is a library for memory management and matrix math targeted at researchers, hyper-productive devs and enthusiasts. It looks like it&#x27;ll be highly capital intensive, requiring hardware that runs in some of the biggest, nastiest, OSS-friendliest data-centres in the world who all design their own silicon. The generations of AMD GPU that matter - the ones out and on people&#x27;s machines - aren&#x27;t supported for high quality GPGPU compute right now. Alright, that means CUDA is a massive edge right now. But that doesn&#x27;t look like a defensible moat.<p>I was interested in being part of this AI thing, what stopped me wasn&#x27;t lack of CUDA, it was that my AMD card reliably crashes under load doing compute workloads. Then when I see George Hotz having a go, the problem isn&#x27;t lack of CUDA; it was that his AMD card crashed under compute workloads (technically I think it was running the demo suite). That is only anecdata, but 2 for 2 is almost a significant number of people with the small number of players and lack of big money in AI historically.<p>Lacking CUDA specifically might be a problem here, but I&#x27;ve never seen AMD fall down at that point. I&#x27;ve only ever see them fall down at basic driver bugs. And I don&#x27;t see how CUDA would matter all that much because I can implement most of what I need math-wise in code. If I see a specific list of common complaints maybe I&#x27;ll change my mind, but I&#x27;m just not detecting where the huge complexity is. I can see CUDA maintaining an edge for years because it is convenient, but I really don&#x27;t see how it can stay essential. The card can already do the workload in theory and in practice assuming the code path doesn&#x27;t bug out. I really don&#x27;t need CUDA, all I want rocBLAS to not crash. I suspect that&#x27;d go a long way in practice.</div><br/><div id="38651465" class="c"><input type="checkbox" id="c-38651465" checked=""/><div class="controls bullet"><span class="by">flamedoge</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38650558">parent</a><span>|</span><a href="#38646589">next</a><span>|</span><label class="collapse" for="c-38651465">[-]</label><label class="expand" for="c-38651465">[2 more]</label></div><br/><div class="children"><div class="content">AMD could use testers(cough clients i mean) like you. Jokes aside, please report bugs to rocm github..</div><br/><div id="38652083" class="c"><input type="checkbox" id="c-38652083" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38651465">parent</a><span>|</span><a href="#38646589">next</a><span>|</span><label class="collapse" for="c-38652083">[-]</label><label class="expand" for="c-38652083">[1 more]</label></div><br/><div class="children"><div class="content">Unless their hardware is on the official support list, I wouldn&#x27;t be too hopeful for a quick resolution. Still, it&#x27;s even less likely to get fixed if it&#x27;s not reported.<p>If nothing else, I would be curious to know more about the issue. Personally, I want to know how well ROCm functions on every AMD GPU.</div><br/></div></div></div></div></div></div></div></div><div id="38646589" class="c"><input type="checkbox" id="c-38646589" checked=""/><div class="controls bullet"><span class="by">droopyEyelids</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646194">parent</a><span>|</span><a href="#38650354">prev</a><span>|</span><a href="#38646064">next</a><span>|</span><label class="collapse" for="c-38646589">[-]</label><label class="expand" for="c-38646589">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not an expert here, but with:<p>&gt; That is a lot more programmers than Nvidia can afford to employ<p>How do you account for the increased complexity those developers have to deal with in an environment where there are multiple companies with conflicting incentives working on the standard?<p>My gut reaction is to worry if this is one of those problems like &quot;9 people working together can&#x27;t have a baby in one month&quot;.</div><br/><div id="38647169" class="c"><input type="checkbox" id="c-38647169" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646589">parent</a><span>|</span><a href="#38646064">next</a><span>|</span><label class="collapse" for="c-38647169">[-]</label><label class="expand" for="c-38647169">[1 more]</label></div><br/><div class="children"><div class="content">I actually find that a really interesting question with a really interesting answer - the scaling properties of large groups of people are unintuitive. In this case, my guess would be high market complexity, and the entire userbase to ignore that complexity in favour of 1-2 vendors with simple and cheap options. So the market overall will just settle on de-facto standards.<p>Of course, based on what we see right now that standard would be Nvidia&#x27;s CUDA; but while CUDA is impressive I don&#x27;t think running neural nets requires that level of complexity. We&#x27;re not talking about GUIs which are one of the stickiest and most complicated blocks of software we know about, or complex platform-specific operations. I&#x27;d expect that the need for specialist libraries to do inference to go away in time and CUDA to be mainly useful for researching GPU applications to new problems. Training will likely just come down to raw ops&#x2F;second in hardware rather than software.<p>It isn&#x27;t like this stuff can&#x27;t already run on other cards. AMD cards can run stable diffusion or LLMs. The issue is just that AMD drivers tend to crash. That is simultaneously a huge and a tiny problem - if they focus on it it won&#x27;t be around for long. CUDA is an advantage, but not a moat.</div><br/></div></div></div></div></div></div><div id="38646064" class="c"><input type="checkbox" id="c-38646064" checked=""/><div class="controls bullet"><span class="by">natbennett</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38646194">prev</a><span>|</span><a href="#38646013">next</a><span>|</span><label class="collapse" for="c-38646064">[-]</label><label class="expand" for="c-38646064">[1 more]</label></div><br/><div class="children"><div class="content">I find the hero-worship of Pat Gelsinger — examples in sibling comments — really weird. My impression of him at VMware was very beancounter-y, not especially technical, and too caught up in personal vendettas and status games to make good technical leadership decisions.<p>Granted, I may have just gotten off on the wrong foot. The first thing he said to Pivotal during the acquisition announcement was, “You were our cousins, but you’re now more like children.” So the whole tone was just weird.</div><br/></div></div><div id="38646013" class="c"><input type="checkbox" id="c-38646013" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38646064">prev</a><span>|</span><a href="#38652081">next</a><span>|</span><label class="collapse" for="c-38646013">[-]</label><label class="expand" for="c-38646013">[2 more]</label></div><br/><div class="children"><div class="content">This was true for Intel for at least 10 years and I’m pretty sure for much longer than that. It was probably true for nvidia for about as long as they exist.<p>Hardware without software is just expensive sand. Every semiconductor company knows this. Intel was the one to perfect the whole package with x86 in the first place…<p>In the GPU compute space CUDA <i>is</i> x86. It’s ubiquitous, de facto standard and will be disrupted. Question is if it takes a year or a decade.</div><br/><div id="38647967" class="c"><input type="checkbox" id="c-38647967" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646013">parent</a><span>|</span><a href="#38652081">next</a><span>|</span><label class="collapse" for="c-38647967">[-]</label><label class="expand" for="c-38647967">[1 more]</label></div><br/><div class="children"><div class="content">The stereotype is hardware engineers all think software is easy. So while semiconductor firms know software is important, they&#x27;re often optimistic about the ease of creating it.<p>Cuda is enormous, very complicated and fits together relatively well. All the semiconductor startups have a business plan about being transparent drop in replacements for cuda systems, built by some tens of software engineers in a year or two. That only really makes sense if you&#x27;ve totally misjudged the problem difficulty.</div><br/></div></div></div></div><div id="38652081" class="c"><input type="checkbox" id="c-38652081" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38646013">prev</a><span>|</span><a href="#38645819">next</a><span>|</span><label class="collapse" for="c-38652081">[-]</label><label class="expand" for="c-38652081">[1 more]</label></div><br/><div class="children"><div class="content">Ultimately it comes down to Intel and AMD being penny wise, pound foolish. They&#x27;re unwilling to hire a lot of quality software engineers, because they&#x27;re expensive, so they just continue to fall behind NVidia.</div><br/></div></div><div id="38645819" class="c"><input type="checkbox" id="c-38645819" checked=""/><div class="controls bullet"><span class="by">mi_lk</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38652081">prev</a><span>|</span><a href="#38649728">next</a><span>|</span><label class="collapse" for="c-38645819">[-]</label><label class="expand" for="c-38645819">[9 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t know Pat Gelsinger do you?</div><br/><div id="38646644" class="c"><input type="checkbox" id="c-38646644" checked=""/><div class="controls bullet"><span class="by">ActionHank</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38645819">parent</a><span>|</span><a href="#38646595">next</a><span>|</span><label class="collapse" for="c-38646644">[-]</label><label class="expand" for="c-38646644">[7 more]</label></div><br/><div class="children"><div class="content">We&#x27;re in 2023, he&#x27;s been in the CEO seat for 2 years already. He&#x27;s had plenty of time to show the world his intent and where they are going. All that has happened is they launched a very mid GPU and have yielded more ground to AMD. Meanwhile AMD continue to eat away at Intel&#x27;s talent pool, market share, and still managed to push into the AI space.<p>He should be sweating.</div><br/><div id="38646828" class="c"><input type="checkbox" id="c-38646828" checked=""/><div class="controls bullet"><span class="by">pgeorgi</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646644">parent</a><span>|</span><a href="#38648059">next</a><span>|</span><label class="collapse" for="c-38646828">[-]</label><label class="expand" for="c-38646828">[3 more]</label></div><br/><div class="children"><div class="content">&gt; for 2 years already ... All that has happened is they launched a very mid GPU<p>Hardware development cycles are closer to 5 years. So while he might have gotten some adjustments done on the designs so far, if he turned the ship around it&#x27;ll take a while longer to materialize.<p>The software side is more agile, so any tea leave reading to discern what Gelsinger&#x27;s strategy looks like is best done over there.</div><br/><div id="38649108" class="c"><input type="checkbox" id="c-38649108" checked=""/><div class="controls bullet"><span class="by">jorvi</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646828">parent</a><span>|</span><a href="#38648059">next</a><span>|</span><label class="collapse" for="c-38649108">[-]</label><label class="expand" for="c-38649108">[2 more]</label></div><br/><div class="children"><div class="content">Not only that, for a “first” (not sure how much of Larrabee was salvaged) discrete GPU attempt, Intel Arc is fantastic. Look at the first GPUs Nvidia and ATI launched.<p>It’s only when you put them up against Nvidia and AMDs comes-with-decades-of-experience offerings that Intel’s GPUs seem less than stellar.</div><br/><div id="38649708" class="c"><input type="checkbox" id="c-38649708" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38649108">parent</a><span>|</span><a href="#38648059">next</a><span>|</span><label class="collapse" for="c-38649708">[-]</label><label class="expand" for="c-38649708">[1 more]</label></div><br/><div class="children"><div class="content">Yeah Arc is incredible in how much it accomplished as a first attempt and as long as they keep at it without chopping it up into a bunch of artificially limited market segments then it&#x27;ll probably be incredibly competitive in a few generations.</div><br/></div></div></div></div></div></div><div id="38648059" class="c"><input type="checkbox" id="c-38648059" checked=""/><div class="controls bullet"><span class="by">signatoremo</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646644">parent</a><span>|</span><a href="#38646828">prev</a><span>|</span><a href="#38650580">next</a><span>|</span><label class="collapse" for="c-38648059">[-]</label><label class="expand" for="c-38648059">[2 more]</label></div><br/><div class="children"><div class="content">His intent is “5 nodes in 4 years” - [0]. The goal is to reclaim the node leadership from TSMC by 2025.<p>They announced the first chips based on Intel 4 today, which is more or less equivalent to TSMC’s 5nm.<p>They may fail, but the goal is clear and ambitious.<p>[0] - <a href="https:&#x2F;&#x2F;www.xda-developers.com&#x2F;intel-roadmap-2025-explainer&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.xda-developers.com&#x2F;intel-roadmap-2025-explainer&#x2F;</a></div><br/></div></div><div id="38650580" class="c"><input type="checkbox" id="c-38650580" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38646644">parent</a><span>|</span><a href="#38648059">prev</a><span>|</span><a href="#38646595">next</a><span>|</span><label class="collapse" for="c-38650580">[-]</label><label class="expand" for="c-38650580">[1 more]</label></div><br/><div class="children"><div class="content">&gt;All that has happened is they launched a very mid GPU and have yielded more ground to AMD.<p>And you dont blame that to Raja Koduri but to Gelsinger?</div><br/></div></div></div></div><div id="38646595" class="c"><input type="checkbox" id="c-38646595" checked=""/><div class="controls bullet"><span class="by">drexlspivey</span><span>|</span><a href="#38645594">root</a><span>|</span><a href="#38645819">parent</a><span>|</span><a href="#38646644">prev</a><span>|</span><a href="#38649728">next</a><span>|</span><label class="collapse" for="c-38646595">[-]</label><label class="expand" for="c-38646595">[1 more]</label></div><br/><div class="children"><div class="content">or Lisa Su</div><br/></div></div></div></div><div id="38649728" class="c"><input type="checkbox" id="c-38649728" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38645819">prev</a><span>|</span><a href="#38645987">next</a><span>|</span><label class="collapse" for="c-38649728">[-]</label><label class="expand" for="c-38649728">[1 more]</label></div><br/><div class="children"><div class="content">Intel has over 15,000 software engineers, per their website. I couldn&#x27;t find a number for NVIDIA, but it looks like they have a bit above 26k total employees.<p>So, its very likely Intel has more software engineers than NVIDIA. Intel has far more products than NVIDIA though, so NVIDIA almost certainly has more software engineers working on GPU.</div><br/></div></div><div id="38645987" class="c"><input type="checkbox" id="c-38645987" checked=""/><div class="controls bullet"><span class="by">papichulo2023</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38649728">prev</a><span>|</span><a href="#38645953">next</a><span>|</span><label class="collapse" for="c-38645987">[-]</label><label class="expand" for="c-38645987">[1 more]</label></div><br/><div class="children"><div class="content">Are you saying they able to develop without a PM every 5 engineers? Insane.</div><br/></div></div><div id="38645953" class="c"><input type="checkbox" id="c-38645953" checked=""/><div class="controls bullet"><span class="by">sulam</span><span>|</span><a href="#38645594">parent</a><span>|</span><a href="#38645987">prev</a><span>|</span><a href="#38645601">next</a><span>|</span><label class="collapse" for="c-38645953">[-]</label><label class="expand" for="c-38645953">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, they should get someone who actually architected a successful chip, like the 486. Maybe a boomerang who used to be CTO. Get rid of this beancounter and hire someone like that!!<p>&#x2F;S</div><br/></div></div></div></div><div id="38645601" class="c"><input type="checkbox" id="c-38645601" checked=""/><div class="controls bullet"><span class="by">bartwr</span><span>|</span><a href="#38645594">prev</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38645601">[-]</label><label class="expand" for="c-38645601">[41 more]</label></div><br/><div class="children"><div class="content">If they create a better tool chain, ecosystem, and programming experience than CUDA and compatible with all computational platforms at their peak performance - awesome! Everyone wins!<p>Until then, it&#x27;s a bit funny claim, especially considering what a failure OpenCL was (programmer&#x27;s experience and fading support). Or trying to do GPGPU with compute shaders in DX&#x2F;GL&#x2F;Vulkan.
Are they really &quot;motivated&quot;? Because they had so many years and the results are miserable... And I don&#x27;t think they invested even a fraction of what got invested into CUDA. Put your money where your mouth is.</div><br/><div id="38647318" class="c"><input type="checkbox" id="c-38647318" checked=""/><div class="controls bullet"><span class="by">Figs</span><span>|</span><a href="#38645601">parent</a><span>|</span><a href="#38645631">next</a><span>|</span><label class="collapse" for="c-38647318">[-]</label><label class="expand" for="c-38647318">[15 more]</label></div><br/><div class="children"><div class="content">I wish AMD or Intel would just ship a giant honking CPU with <i>1000s of cores</i> that <i>doesn&#x27;t need</i> any special purpose programming languages to utilize. Screw co-processors. Screw trying to make yet another fucked up special purpose language -- whether that&#x27;s C&#x2F;C++-with-quirks or a half-assed Python clone or whatever. Nuts to that. Just ship more cores and let me use real threads in regular programming languages.</div><br/><div id="38649626" class="c"><input type="checkbox" id="c-38649626" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38647318">parent</a><span>|</span><a href="#38651488">next</a><span>|</span><label class="collapse" for="c-38649626">[-]</label><label class="expand" for="c-38649626">[6 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t work if you&#x27;re going against GPUs. All the nice goodies we are accustomed to on large desktop x86 machines with gigantic caches and huge branch predictor area and OOO execution engines -- the features that yield the performance profile we expect -- simply do not translate or scale up to thousands of cores per die. To scale that up, you need to redesign the microarchitecture in a fundamental way to allow more compute-per-mm^2 of area, but at that point none of the original software will work in any meaningful capacity because the pipeline is so radically different, it might as well be a different architecture entirely. That means you might as well just write an entirely different software stack, too, and if you&#x27;re rewriting the software, well, a different ISA is actually the easy part. And no, shoving sockets on the mobo does not change this; it doesn&#x27;t matter if it&#x27;s a single die or multi socket. The same dynamics apply.</div><br/><div id="38649808" class="c"><input type="checkbox" id="c-38649808" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649626">parent</a><span>|</span><a href="#38651488">next</a><span>|</span><label class="collapse" for="c-38649808">[-]</label><label class="expand" for="c-38649808">[5 more]</label></div><br/><div class="children"><div class="content">While the first &gt;1000 core x86 processor is probably a little ways out, Intel is releasing a 288-core x86 processor in the first half of 2024 (Sierra Forest). I assume AMD will have something similarly high core in 2024-25 as well.</div><br/><div id="38649966" class="c"><input type="checkbox" id="c-38649966" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649808">parent</a><span>|</span><a href="#38651369">next</a><span>|</span><label class="collapse" for="c-38649966">[-]</label><label class="expand" for="c-38649966">[1 more]</label></div><br/><div class="children"><div class="content">To be clear, you can probably make a 1000 core x86 machine, and those 1000 cores can probably even be pretty powerful. I don&#x27;t doubt that. I think Azure even has crazy 8-socket multi-sled systems doing hundreds of cores, today. But this thread is about CUDA. Sierra Forest will get absolutely obliterated by a single A100 in basically any workload where you could reasonably choose between the two as options. I&#x27;m not saying they can&#x27;t exist. Just that they will be (very) bad in this specific competition. I made an edit to my comment to reflect that.<p>But what you mention is important, and also a reason for the ultimate demise of e.g. Xeon Phi. Intel surely realized they could just scale their existing Xeon designs up-and-out further than expected. Like from a product&#x2F;SKU standpoint, what is the point of having a 300 core Phi where every core is slow as shit, when you have a 100 core 4-socket Xeon design on the horizon, using an existing battle-tested design that you ship billions of dollars worth every year? Especially when the 300 core Xeon fails completely against the competition. By the time Phi died, they were already doing 100-cores-per-socket systems. They essentially realized any market they <i>could</i> have had would be served better by the existing Xeon line and by playing to their existing strengths.</div><br/></div></div><div id="38651369" class="c"><input type="checkbox" id="c-38651369" checked=""/><div class="controls bullet"><span class="by">dahart</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649808">parent</a><span>|</span><a href="#38649966">prev</a><span>|</span><a href="#38651488">next</a><span>|</span><label class="collapse" for="c-38651369">[-]</label><label class="expand" for="c-38651369">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Intel is releasing a 288-core x86<p>This made me wonder a couple of things-<p>What kind of workloads and problems is that best suited for? It’s a lot of cores for a CPU, but for pure math&#x2F;compute, like with AI training and inference and with graphics, 288 cores is like ~1.5% of the number of threads of a modern GPU, right? Doesn’t it take particular kinds of problems to make a 288 core CPU attractive?<p>I also wondered if the ratio of the highest core count CPU to GPU has been relatively flat for a while? Which way is it trending- which of CPUs or GPUs are getting more cores faster?</div><br/><div id="38652264" class="c"><input type="checkbox" id="c-38652264" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38651369">parent</a><span>|</span><a href="#38651488">next</a><span>|</span><label class="collapse" for="c-38652264">[-]</label><label class="expand" for="c-38652264">[2 more]</label></div><br/><div class="children"><div class="content">You could do sparse deep learning with much, much larger models with these CPUs. As paradoxical as it might sound, sparse deep learning gets more compute bound as you add more cores.</div><br/><div id="38652345" class="c"><input type="checkbox" id="c-38652345" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38652264">parent</a><span>|</span><a href="#38651488">next</a><span>|</span><label class="collapse" for="c-38652345">[-]</label><label class="expand" for="c-38652345">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be curious to learn more about how it&#x27;s compute bound and what specifically is compute bound. On modern H100s you need ~600 fp8 operations per byte loaded from memory in order to be compute bound, and that&#x27;s with full 128-byte loads each time. Even integer&#x2F;fp32 vector operations need quite a few operations to be compute bound (~20 for vector fp32).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38651488" class="c"><input type="checkbox" id="c-38651488" checked=""/><div class="controls bullet"><span class="by">dahart</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38647318">parent</a><span>|</span><a href="#38649626">prev</a><span>|</span><a href="#38648058">next</a><span>|</span><label class="collapse" for="c-38651488">[-]</label><label class="expand" for="c-38651488">[1 more]</label></div><br/><div class="children"><div class="content">Apple might be sort-of trying to build the honking CPU, but it still requires different language extensions and a mix of different programming models.<p>And what you suggest could be done, but it would likely flop commercially if you made it today, which is why they aren’t doing it. SIMD machines are faster on homogenous workloads, by a <i>lot</i>. It would be a bummer to develop a CPU with thousands of cores that is still tens or hundreds of times slower than a comparably priced GPU.<p>SIMD isn’t going away anytime soon, or maybe ever. When the workload is embarrassingly parallel, it’s cheaper and more efficient to use SIMD over general purpose cores. Specialized chiplets and co-processors are on the rise too, co-inciding with the wane of Moore’s law; specialization is often the lowest hanging fruit for improving efficiency now.<p>There’s going to be plenty of demand for general programmers but maybe worth keeping in mind the kinds of opportunities that are opening up for people who can learn and develop special purpose hardware and software.</div><br/></div></div><div id="38648058" class="c"><input type="checkbox" id="c-38648058" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38647318">parent</a><span>|</span><a href="#38651488">prev</a><span>|</span><a href="#38647615">next</a><span>|</span><label class="collapse" for="c-38648058">[-]</label><label class="expand" for="c-38648058">[1 more]</label></div><br/><div class="children"><div class="content">Well, that is what a GPU is. Cuda &#x2F; openmp etc are attempts at conveniently programming a mixed cpu&#x2F;gpu system.<p>If you don&#x27;t want that, program the GPU directly in assembly or C++ or whatever. A kernel is a thread - program counter, register file, independent execution from the other threads.<p>There isn&#x27;t a Linux kernel equivalent sitting between you and the hardware so it&#x27;s very like bare metal x64 programming, but you could put a kernel abstraction on it if you wanted.<p>Core isn&#x27;t very well defined, but if we go with &quot;number of independent program counters live at the same time&quot; it&#x27;s a few thousand.<p>X64 cores are vaguely equivalent to GCN compute units, 100 or so if either in a 300W envelope. X64 has two threads and a load of branch prediction &#x2F; speculation hardware. GCN has 80 threads and swaps between them each cycle. Same sort of idea, different allocation of silicon.</div><br/></div></div><div id="38647615" class="c"><input type="checkbox" id="c-38647615" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38647318">parent</a><span>|</span><a href="#38648058">prev</a><span>|</span><a href="#38645631">next</a><span>|</span><label class="collapse" for="c-38647615">[-]</label><label class="expand" for="c-38647615">[6 more]</label></div><br/><div class="children"><div class="content">It was called Larrabee and XeonPhi, they botched it, and the only thing left from that effort is AVX.</div><br/><div id="38647908" class="c"><input type="checkbox" id="c-38647908" checked=""/><div class="controls bullet"><span class="by">vkazanov</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38647615">parent</a><span>|</span><a href="#38645631">next</a><span>|</span><label class="collapse" for="c-38647908">[-]</label><label class="expand" for="c-38647908">[5 more]</label></div><br/><div class="children"><div class="content">I used to play with these toys 7-8 years ago. We tried everything, and it was bad at it all.<p>Traditional compute? The cores were too weak.<p>Number crunching? Okay-ish but gpus were better.<p>Useless stuff.</div><br/><div id="38651639" class="c"><input type="checkbox" id="c-38651639" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38647908">parent</a><span>|</span><a href="#38649069">next</a><span>|</span><label class="collapse" for="c-38651639">[-]</label><label class="expand" for="c-38651639">[1 more]</label></div><br/><div class="children"><div class="content">Hence why &quot; they botched it&quot;.</div><br/></div></div><div id="38649069" class="c"><input type="checkbox" id="c-38649069" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38647908">parent</a><span>|</span><a href="#38651639">prev</a><span>|</span><a href="#38645631">next</a><span>|</span><label class="collapse" for="c-38649069">[-]</label><label class="expand" for="c-38649069">[3 more]</label></div><br/><div class="children"><div class="content">They seemed exceedingly hard to use well but interestingly capable &amp; full of promise. And they were made in a much more primitive software age.<p>I&#x27;d love to hear about what didn&#x27;t work. OpenMP support seemed ok maybe but OpenMP is just a platform, figuring out software architectures that&#x27;s mechanistically sympathetic to the system is hard. It would be so interesting to see what Xeon Phi might have been if we had Calcite or Velox or OpenXLA or other execution engine&#x2F;optimizers that can orchestrate usage. The possibility of something like Phi seems so much higher now.<p>There&#x27;s such a consensus around Phi tanking, and yes, some people came and tried and failed.  But most of those lessons, of why it wasn&#x27;t working (or was!) never survived the era, never were turned into stories &amp; research that illuminates what Phi really was. My feeling is that most people were staying the course on GPU stuff, and that there weren&#x27;t that many people trying Phi. I&#x27;d like more than the heresay heaped at Phi&#x27;s feed to judge by.</div><br/><div id="38649794" class="c"><input type="checkbox" id="c-38649794" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649069">parent</a><span>|</span><a href="#38645631">next</a><span>|</span><label class="collapse" for="c-38649794">[-]</label><label class="expand" for="c-38649794">[2 more]</label></div><br/><div class="children"><div class="content">Some observations:<p>- Very bad performance at existing x86 workloads, so a major selling point was basically not there in practice, because extracting any meaningful performance required a software rewrite anyway. This was an important adoption criteria; if they outright said &quot;All your existing workloads are compatible, but will perform like complete dogshit&quot;, why would anyone bother? Compatibility was a big selling point that ended up meaning little in practice, unfortunately.<p>- Not actually what x86 <i>users</i> wanted. This was at the height of &quot;Intel stagnation&quot; and while I think they were experimenting with lots of stuff, well, in this case, they were serving a market that didn&#x27;t really want what they had (or at least wasn&#x27;t <i>convinced</i> they wanted it).<p>- GPU creators weren&#x27;t sitting idle and twiddling their thumbs. Nvidia was continuously improving performance and programmability of their GPUs across all segments (gaming, HPC, datacenters, scientific workloads) while this was all happening. They improved their compilers, programming models, and microarchitecture. They did not sit by on any of these fronts.<p>Ironically the main living legacy of Phi is AVX-512, which people <i>did</i> and still <i>do</i> want. But that kind of gives it all away, doesn&#x27;t it? People didn&#x27;t want a new massively multicore microarchitecture. They wanted new vector instructions that were flexible and easier to program than what they had -- and AVX-512 is really much better. They wanted the things they were already doing to get better, <i>not</i> things that were like, effectively a different market.<p>Anyway, the most important point is probably the last one, honestly. Like we could talk a lot about compiler optimizations or autovectorization. But really, the market that Phi was trying to occupy just wasn&#x27;t actually that big, and in the end, GPUs got better at things they were bad at, quicker than Phi got better at things it was bad at. It&#x27;s not dissimilar to Optane. Technically interesting, and I mourn its death, but the competition simply improved faster than the adoption rate of the new thing, and so flash is what we have.<p>Once you factor in that you have to rewrite software to get meaningful performance uplift, the rest sort of falls into place. Keep in mind that if you have a $10,000 chip and you can only extract 50% of the performance, you more or less have just $5,000 on fire for nothing in return. You might as well go all the way and use a GPU because at least then you&#x27;re getting more ops&#x2F;mm^2 of silicon.</div><br/><div id="38650179" class="c"><input type="checkbox" id="c-38650179" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649794">parent</a><span>|</span><a href="#38645631">next</a><span>|</span><label class="collapse" for="c-38650179">[-]</label><label class="expand" for="c-38650179">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree anywhere but I don&#x27;t think any of these statements actually condemn Xeon Phi outright. It didn&#x27;t work at the time, and doing it with so little software support to tile out workloads well was a big &amp; possibly bad gambit, but I&#x27;m so unsure we can condemn the architecture. There seems to be so few folks who made good attempts and succeeded or failed &amp; wrote about it.<p>I tend to think there was tons of untapped potential still on the table. And that a failure to adopt potential isn&#x27;t purely Intel alone&#x27;s fault. The story we are commenting on is about the rest-of-industry trying to figure out enduring joint strategies, and much of this is chipmaker provided, but it is also informed and helped by plenty of consumers also pouring energy in to figure out what&#x27;s working and not, trying to push the bounds.<p>Agreed that anyone going in thinking Xeon Phi would be viable for running a boring everyday x86 workload was going to be sad. To me the promise seemed clear that existing toolchains &amp; code would work, but it was always clear to me there were a bunch of little punycores &amp; massive SIMD units and that doing anything not SIMD intensive wasn&#x27;t going to go well at all. But what&#x27;s the current trend? Intel and AMD are both actively building not punycores but smaller cores, with Sierra Forest and Bergamo. E-cores are the grown up Atom we saw here.<p>Yes the GPGPU folks were winning. They had a huge head start, were the default option. And Intel was having trouble delivering nodes. So yes, Xeon Phi was getting trounced for real reasons. But they weren&#x27;t architectural issues! It just means the Xeon Phi premise was becoming increasingly handicapped.<p>As I said I broadly agree everywhere. Your core point about giving the market more of what it already does is well taken, is a river of wisdom we see again and again. But I do think conservative thinking, iterating along, is dangerous thinking that obstructs us from seeing real value &amp; possibility before us. Maybe Intel could have made a better ML chip than the GPGPU market has gotten for years, had things gone differently; I think the industry could perhaps have been glad they had veered onto a new course, but the barriers to that happening &amp; the slow down in Intel delivery &amp; the difficulty bootstrapping new software were all horrible encumberances which were rightly more than was worth bearing together.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38645631" class="c"><input type="checkbox" id="c-38645631" checked=""/><div class="controls bullet"><span class="by">ttoinou</span><span>|</span><a href="#38645601">parent</a><span>|</span><a href="#38647318">prev</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38645631">[-]</label><label class="expand" for="c-38645631">[25 more]</label></div><br/><div class="children"><div class="content">What&#x27;s wrong with compute shaders ?</div><br/><div id="38652356" class="c"><input type="checkbox" id="c-38652356" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645631">parent</a><span>|</span><a href="#38645850">next</a><span>|</span><label class="collapse" for="c-38652356">[-]</label><label class="expand" for="c-38652356">[1 more]</label></div><br/><div class="children"><div class="content">Compute shaders are not capable of using modern GPU features like tensor cores or many of the other features needed to feed tensor cores data fast enough (e.g. TMA&#x2F;cp.async.shared)</div><br/></div></div><div id="38645850" class="c"><input type="checkbox" id="c-38645850" checked=""/><div class="controls bullet"><span class="by">bartwr</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645631">parent</a><span>|</span><a href="#38652356">prev</a><span>|</span><a href="#38646638">next</a><span>|</span><label class="collapse" for="c-38645850">[-]</label><label class="expand" for="c-38645850">[8 more]</label></div><br/><div class="children"><div class="content">I shipped a dozen products with them (mostly video games), so there&#x27;s nothing &quot;wrong&quot; that would make them unusable.
But programming them and setting up the graphics pipe (and all the passes, structured buffers, compiling, binding, weird errors, and synchronization) is a huge PITA as compared to the convenience of CUDA.
Compilers are way less mature, especially on some platforms <i>cough</i>.
Some GPU capabilities are not exposed.
No real composability or libraries.
No proper debugging.</div><br/><div id="38646555" class="c"><input type="checkbox" id="c-38646555" checked=""/><div class="controls bullet"><span class="by">pcwalton</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645850">parent</a><span>|</span><a href="#38647758">next</a><span>|</span><label class="collapse" for="c-38646555">[-]</label><label class="expand" for="c-38646555">[1 more]</label></div><br/><div class="children"><div class="content">These days, some game engines have done pretty well at making compute shaders easy to use (such as Bevy [1] -- disclaimer, I contribute to that engine). But telling the scientific&#x2F;financial&#x2F;etc. community that they need to run their code inside a game engine to get a decent experience is a hard sell. It&#x27;s not a great situation compared to how easy it is on NVIDIA&#x27;s stack.<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;bevyengine&#x2F;bevy&#x2F;blob&#x2F;main&#x2F;examples&#x2F;shader&#x2F;compute_shader_game_of_life.rs">https:&#x2F;&#x2F;github.com&#x2F;bevyengine&#x2F;bevy&#x2F;blob&#x2F;main&#x2F;examples&#x2F;shader...</a></div><br/></div></div><div id="38647758" class="c"><input type="checkbox" id="c-38647758" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645850">parent</a><span>|</span><a href="#38646555">prev</a><span>|</span><a href="#38648625">next</a><span>|</span><label class="collapse" for="c-38647758">[-]</label><label class="expand" for="c-38647758">[5 more]</label></div><br/><div class="children"><div class="content">I have recently published an AI-related open-source project entirely based on compute shaders <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml</a> and I’m super happy with the workflow. Possible to implement very complicated things without compiling a single line of C++, the software is mostly in C#.<p>&gt; setting up the graphics pipe<p>I’ve picked D3D11, as opposed to D3D12 or Vulkan. The 11 is significantly higher level, and much easier to use.<p>&gt; compiling, binding<p>The compiler is design-time, I ship them compiled, and integrated into the IDE. I solved the bindings with a simple code generation tool, which parses HLSL and generates C#.<p>&gt; No proper debugging<p>I partially agree but still, we have renderdoc.</div><br/><div id="38649001" class="c"><input type="checkbox" id="c-38649001" checked=""/><div class="controls bullet"><span class="by">raphlinus</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38647758">parent</a><span>|</span><a href="#38648625">next</a><span>|</span><label class="collapse" for="c-38649001">[-]</label><label class="expand" for="c-38649001">[4 more]</label></div><br/><div class="children"><div class="content">I understand why you&#x27;ve picked D3D11, but people have to understand that comes with serious limitations. There are no subgroups, which also means no cooperative matrix multiplication (&quot;tensor cores&quot;). For throughput in machine learning inference in particular, there&#x27;s no way D3D11 can compete with either CUDA or a more modern compute shader stack, such as one based on Vulkan 1.3.</div><br/><div id="38649298" class="c"><input type="checkbox" id="c-38649298" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649001">parent</a><span>|</span><a href="#38648625">next</a><span>|</span><label class="collapse" for="c-38649298">[-]</label><label class="expand" for="c-38649298">[3 more]</label></div><br/><div class="children"><div class="content">&gt; no subgroups<p>Indeed, in D3D they are called “wave intrinsics” and require D3D12. But that’s IMO a reasonable price to pay for hardware compatibility.<p>&gt; no cooperative matrix multiplication<p>Matrix multiplication compute shader which uses group shared memory for cooperative loads: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;blob&#x2F;master&#x2F;Mistral&#x2F;MistralShaders&#x2F;mulMatTiled.hlsl">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;blob&#x2F;master&#x2F;Mistral&#x2F;Mistral...</a><p>&gt; tensor cores<p>When running inference on end-user computers, for many practical applications users don’t care about throughput. They only have a single audio stream &#x2F; chat &#x2F; picture being generated, their batch size is a small number often just 1, and they mostly care about latency, not throughput. Under these conditions inference is guaranteed to bottleneck on memory bandwidth, as opposed to compute. For use cases like that, tensor cores are useless.<p>&gt; there&#x27;s no way D3D11 can compete with either CUDA<p>My D3D11 port of Whisper outperformed original CUDA-based implementation running on the same GPU: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Whisper&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Whisper&#x2F;</a></div><br/><div id="38649575" class="c"><input type="checkbox" id="c-38649575" checked=""/><div class="controls bullet"><span class="by">raphlinus</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649298">parent</a><span>|</span><a href="#38648625">next</a><span>|</span><label class="collapse" for="c-38649575">[-]</label><label class="expand" for="c-38649575">[2 more]</label></div><br/><div class="children"><div class="content">Sure. It&#x27;s a tradeoff space. Gain portability and ergonomics, lose throughput. For applications that <i>are</i> throttled by TOPS at low precisions (ie most ML inferencing) then the performance drop from not being able to use tensor cores is going to be unacceptable. Glad you found something that works for you, but it certainly doesn&#x27;t spell the end of CUDA.</div><br/><div id="38649873" class="c"><input type="checkbox" id="c-38649873" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649575">parent</a><span>|</span><a href="#38648625">next</a><span>|</span><label class="collapse" for="c-38649873">[-]</label><label class="expand" for="c-38649873">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ie most ML inferencing<p>Most ML inferencing is throttled with memory, not compute. This certainly applies to both Whisper and Mistral models.<p>&gt; it certainly doesn&#x27;t spell the end of CUDA<p>No, because traditional HPC. Some people in the industry spent many man-years developing very complicated compute kernels, which are very expensive to port.<p>AI is another story. Not too hard to port from CUDA to compute shaders, because the GPU-running code is rather simple.<p>Moreover, it can help with performance just by removing abstraction layers. I think the reason why compute shaders-based Whisper outperformed CUDA-based version on the same GPU, these implementations do slightly different things. Unlike Python and Torch, compute shaders actually program GPUs as opposed to calling libraries with tons of abstractions layers inside them. This saves memory bandwidth storing and then loading temporary tensors.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38648625" class="c"><input type="checkbox" id="c-38648625" checked=""/><div class="controls bullet"><span class="by">jzl</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645850">parent</a><span>|</span><a href="#38647758">prev</a><span>|</span><a href="#38646638">next</a><span>|</span><label class="collapse" for="c-38648625">[-]</label><label class="expand" for="c-38648625">[1 more]</label></div><br/><div class="children"><div class="content">This. It&#x27;s crazy how primitive the GPU development process still is in the year 2023. Yeah it&#x27;s gotten better, but there&#x27;s still a massive gap with traditional development.</div><br/></div></div></div></div><div id="38646638" class="c"><input type="checkbox" id="c-38646638" checked=""/><div class="controls bullet"><span class="by">mschuetz</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645631">parent</a><span>|</span><a href="#38645850">prev</a><span>|</span><a href="#38645836">next</a><span>|</span><label class="collapse" for="c-38646638">[-]</label><label class="expand" for="c-38646638">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kinda like building Legos vs building actual Skyscrapers. The gap between compute shaders and CUDA is massive. At least it feels massive because CUDA has some key features that compute shaders lack, and which make it so much easier to build complex, powerful and fast applications.<p>One of the features that would get compute shaders far ahead compared to now would be pointers and pointer casting - Just let me have a byte buffer and easily cast the bytes to whatever I want. Another would be function pointers. These two are pretty much the main reason I had to stop doing a project in OpenGL&#x2F;Vulkan, and start using CUDA. There are so many more, however, that make life easier like cooperative groups with device-wide sync, being able to allocate a single buffer with all the GPU memory, recursion, etc.<p>Khronos should start supporting C++20 for shaders (basically what CUDA is) and stop the glsl or spirv nonsense.</div><br/><div id="38648040" class="c"><input type="checkbox" id="c-38648040" checked=""/><div class="controls bullet"><span class="by">gmueckl</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38646638">parent</a><span>|</span><a href="#38647652">next</a><span>|</span><label class="collapse" for="c-38648040">[-]</label><label class="expand" for="c-38648040">[1 more]</label></div><br/><div class="children"><div class="content">You might argue for forking off from glsl and SPIR-V for complex compute workloads, but lightweight, fast compilers for a simple language like glsl do solve issues for graphics. Some graphics use cases don&#x27;t get around shipping a shader compiler to the user. The number of possible shader configurations is often either insanely large or just impossible to enumerate, so on the fly compilation is really the only thing you can do.</div><br/></div></div><div id="38647652" class="c"><input type="checkbox" id="c-38647652" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38646638">parent</a><span>|</span><a href="#38648040">prev</a><span>|</span><a href="#38645836">next</a><span>|</span><label class="collapse" for="c-38647652">[-]</label><label class="expand" for="c-38647652">[1 more]</label></div><br/><div class="children"><div class="content">Ironically, most people use HLSL with Vulkan, because Khronos doesn&#x27;t have a budget nor the people to improve GLSL.<p>So yet another thing where Khronos APIs are dependent on DirectX evolution.<p>It used to be that AMD and NVidia would first implement new stuff on DirectX in collaboration with Microsoft, have them as extensions in OpenGL, and eventually as standard features.<p>Now even the shading language is part of it.</div><br/></div></div></div></div><div id="38645836" class="c"><input type="checkbox" id="c-38645836" checked=""/><div class="controls bullet"><span class="by">johncolanduoni</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645631">parent</a><span>|</span><a href="#38646638">prev</a><span>|</span><a href="#38645736">next</a><span>|</span><label class="collapse" for="c-38645836">[-]</label><label class="expand" for="c-38645836">[1 more]</label></div><br/><div class="children"><div class="content">For GPGPU tasks, they lack a lot of useful features that CUDA has like the ability to allocate memory and launch kernels from the GPU. They also generally require you to write your GPU and CPU portions of an algorithm in different languages, while CUDA allows you to intermix your code and share data structures and simple functions between the two.</div><br/></div></div><div id="38645736" class="c"><input type="checkbox" id="c-38645736" checked=""/><div class="controls bullet"><span class="by">fluxem</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645631">parent</a><span>|</span><a href="#38645836">prev</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38645736">[-]</label><label class="expand" for="c-38645736">[11 more]</label></div><br/><div class="children"><div class="content">CUDA = C++ on GPUs. Compute shader - subset of C with a weird quirks.</div><br/><div id="38645932" class="c"><input type="checkbox" id="c-38645932" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645736">parent</a><span>|</span><a href="#38646674">next</a><span>|</span><label class="collapse" for="c-38645932">[-]</label><label class="expand" for="c-38645932">[1 more]</label></div><br/><div class="children"><div class="content">There are existing efforts to compile SYCL to Vulkan compute shaders.  Plenty of &quot;weird quirks&quot; involved since they&#x27;re based on different underlying varieties of SPIR-V (&quot;kernels&quot; vs. &quot;shaders&quot;) and seem to have evolved independently in other ways (Vulkan does not have the amount of support for numerical computation that OpenCL&#x2F;SYCL has) - but nothing too terrible or anything that couldn&#x27;t be addressed by future Vulkan extensions.</div><br/></div></div><div id="38646674" class="c"><input type="checkbox" id="c-38646674" checked=""/><div class="controls bullet"><span class="by">mschuetz</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38645736">parent</a><span>|</span><a href="#38645932">prev</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38646674">[-]</label><label class="expand" for="c-38646674">[9 more]</label></div><br/><div class="children"><div class="content">A subset that lacks pointers, which makes compute shaders a toy language next to CUDA.</div><br/><div id="38648975" class="c"><input type="checkbox" id="c-38648975" checked=""/><div class="controls bullet"><span class="by">raphlinus</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38646674">parent</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38648975">[-]</label><label class="expand" for="c-38648975">[8 more]</label></div><br/><div class="children"><div class="content">Vulkan 1.3 has pointers, thanks to buffer device address[1]. It took a while to get there, and earlier pointer support was flawed. I also don&#x27;t know of any major applications that use this.<p>Modern Vulkan is looking pretty good now. Cooperative matrix multiplication has also landed (as a widely supported extension), and I think it&#x27;s fair to say it&#x27;s gone past OpenCL.<p>Whether we get significant adoption of all this I think is too early to say, but I think it&#x27;s a plausible foundation for real stuff. It&#x27;s no longer just a toy.<p>[1] <a href="https:&#x2F;&#x2F;community.arm.com&#x2F;arm-community-blogs&#x2F;b&#x2F;graphics-gaming-and-vr-blog&#x2F;posts&#x2F;vulkan-buffer-device-address" rel="nofollow noreferrer">https:&#x2F;&#x2F;community.arm.com&#x2F;arm-community-blogs&#x2F;b&#x2F;graphics-gam...</a></div><br/><div id="38651809" class="c"><input type="checkbox" id="c-38651809" checked=""/><div class="controls bullet"><span class="by">mschuetz</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38648975">parent</a><span>|</span><a href="#38649241">next</a><span>|</span><label class="collapse" for="c-38651809">[-]</label><label class="expand" for="c-38651809">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Vulkan 1.3 has pointers, thanks to buffer device address[1].<p>&gt; [1] <a href="https:&#x2F;&#x2F;community.arm.com&#x2F;arm-community-blogs&#x2F;b&#x2F;graphics-gam" rel="nofollow noreferrer">https:&#x2F;&#x2F;community.arm.com&#x2F;arm-community-blogs&#x2F;b&#x2F;graphics-gam</a>...<p>&quot;Using a pointer in a shader - In Vulkan GLSL, there is the GL_EXT_buffer_reference extension &quot;<p>That extension is utter garbage. I tried it. It was the last thing I tried before giving up on GLSL&#x2F;Vulkan and switching to CUDA. It was the nail in the coffin that made me go &quot;okay, if that&#x27;s the best Vulkan can do, then I need to switch to CUDA&quot;. It&#x27;s incredibly cumbersome, confusing and verbose.<p>What&#x27;s needed are regular, simple, C-like pointers.</div><br/></div></div><div id="38649241" class="c"><input type="checkbox" id="c-38649241" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38648975">parent</a><span>|</span><a href="#38651809">prev</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38649241">[-]</label><label class="expand" for="c-38649241">[6 more]</label></div><br/><div class="children"><div class="content">Is IREE the main runtime doing Vulkan or are there others? Who should we be listening to (oh wise @raphlinus)?<p>It&#x27;s been awesome seeing folks like Keras 3.0 kicking out broad Intercompatibility across JAX, TF, Pytorch, powered by flexible executuon engines. Looking forward to seeing more Vulkan based runs getting socialized benchmarked &amp; compared. <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38446353">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38446353</a></div><br/><div id="38649383" class="c"><input type="checkbox" id="c-38649383" checked=""/><div class="controls bullet"><span class="by">raphlinus</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649241">parent</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38649383">[-]</label><label class="expand" for="c-38649383">[5 more]</label></div><br/><div class="children"><div class="content">The two I know of are IREE and Kompute[1]. I&#x27;m not sure how much momentum the latter has, I don&#x27;t see it referenced much. There&#x27;s also a growing body of work that uses Vulkan indirectly through WebGPU. This is currently lagging in performance due to lack of subgroups and cooperative matrix mult, but I see that gap closing. There I think wonnx[2] has the most momentum, but I am aware of other efforts.<p>[1]: <a href="https:&#x2F;&#x2F;kompute.cc&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;kompute.cc&#x2F;</a><p>[2]: <a href="https:&#x2F;&#x2F;github.com&#x2F;webonnx&#x2F;wonnx">https:&#x2F;&#x2F;github.com&#x2F;webonnx&#x2F;wonnx</a></div><br/><div id="38649461" class="c"><input type="checkbox" id="c-38649461" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649383">parent</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38649461">[-]</label><label class="expand" for="c-38649461">[4 more]</label></div><br/><div class="children"><div class="content">How feasible would it be to target Vulkan 1.3 or such from standard SYCL (as first seen in Sylkan, for earlier Vulkan Compute)? Is it still lacking the numerical properties for some math functions that OpenCL and SYCL seem to expect?</div><br/><div id="38649532" class="c"><input type="checkbox" id="c-38649532" checked=""/><div class="controls bullet"><span class="by">raphlinus</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649461">parent</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38649532">[-]</label><label class="expand" for="c-38649532">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a really good question. I don&#x27;t know enough about SYCL to be able to tell you the answer, but I&#x27;ve heard rumblings that it may be the thing to watch. I think there may be some other limitations, for example SYCL 2020 depends on unified shared memory, and that is definitely not something you can depend on in compute shader land (in some cases you can get some of it, for example with resizable BAR, but it depends).<p>In researching this answer, I came across a really interesting thread[1] on diagnosing performance problems with USM in SYCL (running on AMD HIP in this case). It&#x27;s a good tour of why this is hard, and why for the vast majority of users it&#x27;s far better to just use CUDA and not have to deal with any of this bullshit - things pretty much just work.<p>When targeting compute shaders, you pretty much have to manage buffers manually, and also do copying between host and device memory explicitly (when needed - on hardware such as Apple Silicon, you prefer to not copy). I personally don&#x27;t have a problem with this, as I like things being explicit, but it is definitely one of the ergonomic advantages of modern CUDA, and one of the reasons why fully automated conversion to other runtimes is not going to work well.<p>[1]: <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;76700305&#x2F;4000-performance-decrease-in-sycl-when-using-unified-shared-memory-instead-of-d" rel="nofollow noreferrer">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;76700305&#x2F;4000-performanc...</a></div><br/><div id="38649715" class="c"><input type="checkbox" id="c-38649715" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649532">parent</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38649715">[-]</label><label class="expand" for="c-38649715">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;enccs.github.io&#x2F;sycl-workshop&#x2F;unified-shared-memory&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;enccs.github.io&#x2F;sycl-workshop&#x2F;unified-shared-memory&#x2F;</a> seems to suggest that USM is still a hardware-specific feature in SYCL 2020, so compatibility with hardware that requires a buffer copying approach is still maintained.  Is this incorrect?</div><br/><div id="38649993" class="c"><input type="checkbox" id="c-38649993" checked=""/><div class="controls bullet"><span class="by">raphlinus</span><span>|</span><a href="#38645601">root</a><span>|</span><a href="#38649715">parent</a><span>|</span><a href="#38648784">next</a><span>|</span><label class="collapse" for="c-38649993">[-]</label><label class="expand" for="c-38649993">[1 more]</label></div><br/><div class="children"><div class="content">Good call. So this doesn&#x27;t look like a blocker to SYCL compatibility. I&#x27;m interested in learning more about this.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38648784" class="c"><input type="checkbox" id="c-38648784" checked=""/><div class="controls bullet"><span class="by">TheAlchemist</span><span>|</span><a href="#38645601">prev</a><span>|</span><a href="#38645197">next</a><span>|</span><label class="collapse" for="c-38648784">[-]</label><label class="expand" for="c-38648784">[4 more]</label></div><br/><div class="children"><div class="content">Can anybody with a deep knowledge of the AI space, explain to me what&#x27;s the real moat of CUDA ?<p>It&#x27;s clear to everybody that it&#x27;s not the hardware but the software - which is the CUDA ecosystem.<p>I&#x27;ve played a bit in the past with ML, but at the level of understanding I had - training some models, tweaking things, I was using higher level libraries and as far as I know, it&#x27;s pretty much an if statement in those libraries to decide which backend use.<p>So let&#x27;s suppose Intel and others does manage to implement a viable competitor - am I wrong in thinking that the transitions for many users would be seamless ? 
That&#x27;s probably not the case for researchers and people pushing the boundaries, but for most companies, my understanding is there would be not a lot of migration costs involved ?</div><br/><div id="38650711" class="c"><input type="checkbox" id="c-38650711" checked=""/><div class="controls bullet"><span class="by">bdd8f1df777b</span><span>|</span><a href="#38648784">parent</a><span>|</span><a href="#38651539">next</a><span>|</span><label class="collapse" for="c-38650711">[-]</label><label class="expand" for="c-38650711">[1 more]</label></div><br/><div class="children"><div class="content">Your understanding is correct, but the predicates are not easy at all. The amount of work going into CUDA is enormous, and NVIDIA is not standing still waiting for their competitors to catch up.</div><br/></div></div><div id="38651539" class="c"><input type="checkbox" id="c-38651539" checked=""/><div class="controls bullet"><span class="by">fnbr</span><span>|</span><a href="#38648784">parent</a><span>|</span><a href="#38650711">prev</a><span>|</span><a href="#38650629">next</a><span>|</span><label class="collapse" for="c-38651539">[-]</label><label class="expand" for="c-38651539">[1 more]</label></div><br/><div class="children"><div class="content">You need performance in the high level libraries to match, on a flops&#x2F;$ basis. That’s “it”. That’s easier said than done, though. Even google’s TPUs still struggle to match H100s at flops&#x2F;$, and they’re really annoying to use unless you’re using Jax.</div><br/></div></div><div id="38650629" class="c"><input type="checkbox" id="c-38650629" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#38648784">parent</a><span>|</span><a href="#38651539">prev</a><span>|</span><a href="#38645197">next</a><span>|</span><label class="collapse" for="c-38650629">[-]</label><label class="expand" for="c-38650629">[1 more]</label></div><br/><div class="children"><div class="content">I see the situation as a lot like the original IBM PC wars. Originally you had the IBM PC and a bunch of &quot;compatibles&quot; that weren&#x27;t drop-in compatible but half-assed compatible - many programs needed to be re-compiled to run on them. And other large American companies made these - they didn&#x27;t expect to commodify the PC, they just wanted a small piece of a big market.<p>The actual PC clones, pure drop-in compatibles, were made in Taiwan and they took over the market. Which is to say that large companies don&#x27;t want a commodified market where prices are low and everyone competes on a level playing field - which is what &quot;seamless transition&quot; gets you. So that&#x27;s why none of these companies are working to create that.</div><br/></div></div></div></div><div id="38645197" class="c"><input type="checkbox" id="c-38645197" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38648784">prev</a><span>|</span><a href="#38649164">next</a><span>|</span><label class="collapse" for="c-38645197">[-]</label><label class="expand" for="c-38645197">[33 more]</label></div><br/><div class="children"><div class="content">Intel and AMD have had years to provide similar capabilities on top of OpenCL.<p>Maybe they should look into their own failures first.</div><br/><div id="38645374" class="c"><input type="checkbox" id="c-38645374" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#38645197">parent</a><span>|</span><a href="#38646328">next</a><span>|</span><label class="collapse" for="c-38645374">[-]</label><label class="expand" for="c-38645374">[11 more]</label></div><br/><div class="children"><div class="content">SYCL is a better analog to Cuda than OpenCL, and Intel have their own implementation of that. Don&#x27;t really see anyone writing anything in SYCL though, and when I looked into trying it out it was a bit of a mess with different implementations, each supporting their own subset of OSs and hardware.<p><a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;tools&#x2F;oneapi&#x2F;data-parallel-c-plus-plus.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;tools&#x2F;onea...</a></div><br/><div id="38646591" class="c"><input type="checkbox" id="c-38646591" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645374">parent</a><span>|</span><a href="#38645477">next</a><span>|</span><label class="collapse" for="c-38646591">[-]</label><label class="expand" for="c-38646591">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Don&#x27;t really see anyone writing anything in SYCL though<p>My work involves writing software that runs on many GPU platforms at once. So far we have been going through the Kokkos route, but SYCL is looking pretty good to me recent days. There is some consolidation happening in this space (Codeplay gave up working on their own implementation and merged with Intel). It was pretty easy to setup on my Linux machine for Nvidia card. Documentation is very good and professional, unlike AMD&#x27;s, which can be frankly horrible at times. And Intel has a good track record with software.<p>I genuinely believe if someone is going to dethrone CUDA, at this point SYCL (oneAPI) is a far more likely candidate than Rocm&#x2F;HIP.</div><br/></div></div><div id="38645477" class="c"><input type="checkbox" id="c-38645477" checked=""/><div class="controls bullet"><span class="by">rbanffy</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645374">parent</a><span>|</span><a href="#38646591">prev</a><span>|</span><a href="#38646931">next</a><span>|</span><label class="collapse" for="c-38645477">[-]</label><label class="expand" for="c-38645477">[4 more]</label></div><br/><div class="children"><div class="content">I am unfamiliar with the implementation, but why would it be difficult to implement a Cuda-compatible software layer on top of other platforms?<p>This would be the first step. Then, if we want to move away from Cuda into hardware that&#x27;s as ubiquitous and performant as Nvidia&#x27;s (or better), someone would need to write an abstraction layer that&#x27;s more convenient to use than Cuda. I did play a little bit with Cuda and OpenCL, but not enough to hate either.</div><br/><div id="38645929" class="c"><input type="checkbox" id="c-38645929" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645477">parent</a><span>|</span><a href="#38648308">next</a><span>|</span><label class="collapse" for="c-38645929">[-]</label><label class="expand" for="c-38645929">[1 more]</label></div><br/><div class="children"><div class="content">Because that is an herculean task, given the hardware semantics, the amount of languages that target PTX, the graphical debugging tools that expose every little detail of the cards like debugging on the CPU, and the libraries ecosystem.<p>Any Cuda-compatible software layer only has two options, be a second class CUDA implementation by being compatible with a subset like AMD ROCm and HIP effforts, or be compatible with everything always playing catchup.<p>The only way is to use middleware that just like in 3D APIs, abstract the actual compute API being used, as man language bindings are doing nowadays.</div><br/></div></div><div id="38648308" class="c"><input type="checkbox" id="c-38648308" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645477">parent</a><span>|</span><a href="#38645929">prev</a><span>|</span><a href="#38646931">next</a><span>|</span><label class="collapse" for="c-38648308">[-]</label><label class="expand" for="c-38648308">[2 more]</label></div><br/><div class="children"><div class="content">Worth noting that cuda seems prone to using inline ptx assembly and that latter is really obnoxious to deal with on other platforms.<p>Implementing programming languages and runtimes is pretty difficult in general. Note that cuda doesn&#x27;t have the same semantics as c++ despite looking kind of similar.  Wherever you differ from expected behaviour people consider it a bug, and implementing based on cuda&#x27;s docs wouldn&#x27;t get you the behaviour people expect.<p>Pretty horrendous task overall. It would be much better for people to stop developing programs that only run on a gnarly proprietary language.</div><br/><div id="38651656" class="c"><input type="checkbox" id="c-38651656" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38648308">parent</a><span>|</span><a href="#38646931">next</a><span>|</span><label class="collapse" for="c-38651656">[-]</label><label class="expand" for="c-38651656">[1 more]</label></div><br/><div class="children"><div class="content">CUDA was redesigned to follow C++&#x27;s memory model introduced in C++11, yet another compatibility pain point with other hardware vendors.</div><br/></div></div></div></div></div></div><div id="38646931" class="c"><input type="checkbox" id="c-38646931" checked=""/><div class="controls bullet"><span class="by">hjabird</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645374">parent</a><span>|</span><a href="#38645477">prev</a><span>|</span><a href="#38645837">next</a><span>|</span><label class="collapse" for="c-38646931">[-]</label><label class="expand" for="c-38646931">[1 more]</label></div><br/><div class="children"><div class="content">SYCL is gaining traction, especially in the HPC community since it can target AMD, Nvidia and Intel hardware with one codebase. A fun fact is the GROMACS (a major application for molecular dynamics, and big consumer of HPC time) recommends SYCL for running on AMD hardware!</div><br/></div></div><div id="38645837" class="c"><input type="checkbox" id="c-38645837" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645374">parent</a><span>|</span><a href="#38646931">prev</a><span>|</span><a href="#38646328">next</a><span>|</span><label class="collapse" for="c-38645837">[-]</label><label class="expand" for="c-38645837">[4 more]</label></div><br/><div class="children"><div class="content">SYCL builds on top of OpenCL, it is basically the reboot of OpenCL C++, after OpenCL 2.0 SPIR failure.<p>Yet another example on how Intel and AMD failed to take up on CUDA.</div><br/><div id="38647073" class="c"><input type="checkbox" id="c-38647073" checked=""/><div class="controls bullet"><span class="by">hjabird</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645837">parent</a><span>|</span><a href="#38646328">next</a><span>|</span><label class="collapse" for="c-38647073">[-]</label><label class="expand" for="c-38647073">[3 more]</label></div><br/><div class="children"><div class="content">SYCL isn&#x27;t based on OpenCL.<p>SYCL (SYCL-2020 spec) supports multiple backends, including Nvidia&#x27;s CUDA, AMD&#x27;s HIP, OpenCL, Intel&#x27;s Level-zero, and also running on the host CPU. This can either be done with Intel&#x27;s DPC++ w&#x2F; Codeplay&#x27;s plugins, or using AdaptiveCpp (aka. hipSYCL, aka openSYCL). OpenCL is just another backend.<p>It is also a very long way from OpenCL C++. The code is a single C++ file, and you don&#x27;t need to write any special kernel language. The vast majority of SYCL is just C++, so -if you avoid a couple of features- you can use SYCL in library-only form without even any special compiler! This is possible for instance with AdaptiveCpp.</div><br/><div id="38647137" class="c"><input type="checkbox" id="c-38647137" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647073">parent</a><span>|</span><a href="#38647314">next</a><span>|</span><label class="collapse" for="c-38647137">[-]</label><label class="expand" for="c-38647137">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty much based on the same underlying featureset.  Which is why trying to target Vulkan Compute from it is messy enough, whereas OpenCL is a natural target.</div><br/></div></div><div id="38647314" class="c"><input type="checkbox" id="c-38647314" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647073">parent</a><span>|</span><a href="#38647137">prev</a><span>|</span><a href="#38646328">next</a><span>|</span><label class="collapse" for="c-38647314">[-]</label><label class="expand" for="c-38647314">[1 more]</label></div><br/><div class="children"><div class="content">Now, but that isn&#x27;t how it started after the OpenCL 3.0 reboot, where 3.0 == 1.0.<p>Also some of that work, we have to thank Codeplay for, before their acquistion from Intel.</div><br/></div></div></div></div></div></div></div></div><div id="38646328" class="c"><input type="checkbox" id="c-38646328" checked=""/><div class="controls bullet"><span class="by">rnk</span><span>|</span><a href="#38645197">parent</a><span>|</span><a href="#38645374">prev</a><span>|</span><a href="#38648220">next</a><span>|</span><label class="collapse" for="c-38646328">[-]</label><label class="expand" for="c-38646328">[1 more]</label></div><br/><div class="children"><div class="content">Similar capabilities, similar performance and similar (actually better performance). They don&#x27;t have any of these at this time. People don&#x27;t want to buy slower and maybe cheaper computation on nvidia hardware - why would they want to do this on intel hardware? Your app will have to change, it just seems like an obvious non-starter.<p>I&#x27;m not an expert here, am I missing something? Saying the x86 industry is motivated to move away from what nvidia provides, intel needs to tick some of these &#x27;better somehow&#x27; boxes.</div><br/></div></div><div id="38648220" class="c"><input type="checkbox" id="c-38648220" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#38645197">parent</a><span>|</span><a href="#38646328">prev</a><span>|</span><a href="#38646136">next</a><span>|</span><label class="collapse" for="c-38648220">[-]</label><label class="expand" for="c-38648220">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve both implemented opencl. Nvidia has too. The industry could have built on that common language. Instead, it built on cuda, and complains that the other hardware vendors don&#x27;t have cuda.<p>I attribute this to opencl being the common subset a bunch of companies could agree could be implemented. I wrote some code that compiles as cuda, opencl, C++ and openmp, and the entire exercise was repeatedly &quot;what, opencl can&#x27;t do that either? damn it&quot;.</div><br/></div></div><div id="38646136" class="c"><input type="checkbox" id="c-38646136" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#38645197">parent</a><span>|</span><a href="#38648220">prev</a><span>|</span><a href="#38645879">next</a><span>|</span><label class="collapse" for="c-38646136">[-]</label><label class="expand" for="c-38646136">[1 more]</label></div><br/><div class="children"><div class="content">Intel tried with OneAPI years ago. Turns out they were decades behind, so catching up takes a while…</div><br/></div></div><div id="38645879" class="c"><input type="checkbox" id="c-38645879" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645197">parent</a><span>|</span><a href="#38646136">prev</a><span>|</span><a href="#38645357">next</a><span>|</span><label class="collapse" for="c-38645879">[-]</label><label class="expand" for="c-38645879">[16 more]</label></div><br/><div class="children"><div class="content">AMD definitely has and is doubling down on ROCm.</div><br/><div id="38645991" class="c"><input type="checkbox" id="c-38645991" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645879">parent</a><span>|</span><a href="#38645357">next</a><span>|</span><label class="collapse" for="c-38645991">[-]</label><label class="expand" for="c-38645991">[15 more]</label></div><br/><div class="children"><div class="content">Perhaps they&#x27;re doubling down, but even doubling down is not enough to say that they&#x27;re serious about it since they&#x27;ve been so neglectful for so many years - for example, right now they explicitly say that many of AMD GPUs are not supported by ROCm; if they&#x27;re not willing to put their money where their mouth is and do the legwork to ensure support for powerful cards they sold just a few years ago, how can they say you should rely on their platform?<p>Unless a random gamer with a random AMD GPU can go to amd.com and download pre-packaged, officially supported tools that work out of the box on their machine and after a few clicks have working GPU-accelerated pytorch (which IMHO isn&#x27;t the case, but admittedly I haven&#x27;t tried this year) then their &quot;doubling down&quot; isn&#x27;t even meeting table stakes.</div><br/><div id="38646722" class="c"><input type="checkbox" id="c-38646722" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645991">parent</a><span>|</span><a href="#38647504">next</a><span>|</span><label class="collapse" for="c-38646722">[-]</label><label class="expand" for="c-38646722">[13 more]</label></div><br/><div class="children"><div class="content">People argue for ROCm to support older cards because that is all they have accessible to them. AMD has lagged on getting expensive cards into the hands of end users because they&#x27;ve focused only on building super computers.<p>I predict that access to the newer cards is a more likely scenario. Right now, you can&#x27;t rent a MI250 or even MI300x, but that is going to change quickly. Azure is going to have them, as well as others (I know this, cause that&#x27;s what I&#x27;m building now).</div><br/><div id="38650785" class="c"><input type="checkbox" id="c-38650785" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38646722">parent</a><span>|</span><a href="#38647354">next</a><span>|</span><label class="collapse" for="c-38650785">[-]</label><label class="expand" for="c-38650785">[2 more]</label></div><br/><div class="children"><div class="content">Well yeah. Before I go renting a super GPU in the cloud, I&#x27;d like to get my feet wet with the 5 year old but reasonably well specced AMD GPU (Vega 48) in my iMac...but I can&#x27;t. It&#x27;s more rational for me to get an fancy 2021 GPU or a Jetson and stick it in an enclosure or build a Linux box around it. At least I know CUDA is a mature ecosystem and is going to be around for a while, so whatever time I invest in it is likely to pay for itself.<p>I get your point about AMD not wanting to spend money on supporting old hardware, but how do they expect to build a market without a fan base?</div><br/><div id="38651358" class="c"><input type="checkbox" id="c-38651358" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38650785">parent</a><span>|</span><a href="#38647354">next</a><span>|</span><label class="collapse" for="c-38651358">[-]</label><label class="expand" for="c-38651358">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I get your point about AMD not wanting to spend money on supporting old hardware, but how do they expect to build a market without a fan base?</i><p>Look, I get it. You&#x27;re right. They do need to work on building their market and they really screwed the pooch on the AI boat. The developer flywheel is hugely important and they missed out on that. That said, we can&#x27;t expect them to go back in time, but we can keep moving forward. Having enough people making noise about wanting to play with their hardware is certainly a step in the right direction.</div><br/></div></div></div></div><div id="38647354" class="c"><input type="checkbox" id="c-38647354" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38646722">parent</a><span>|</span><a href="#38650785">prev</a><span>|</span><a href="#38647504">next</a><span>|</span><label class="collapse" for="c-38647354">[-]</label><label class="expand" for="c-38647354">[10 more]</label></div><br/><div class="children"><div class="content">&gt; People argue for ROCm to support older cards because that is all they have accessible to them.<p>What they really need is to support the less expensive cards, of which the older cards are a large subset. There are a lot of people who will make contributions and fix bugs if they can actually use the thing. Some CS student at the university has to pay tuition and therefore only has an old RX570, and that isn&#x27;t going to change in the next couple years, but that kind of student could fix some of the software bugs currently preventing the company from selling more expensive GPUs to large institutions. If the stack supported their hardware.</div><br/><div id="38651070" class="c"><input type="checkbox" id="c-38651070" checked=""/><div class="controls bullet"><span class="by">mappu</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647354">parent</a><span>|</span><a href="#38647416">next</a><span>|</span><label class="collapse" for="c-38651070">[-]</label><label class="expand" for="c-38651070">[1 more]</label></div><br/><div class="children"><div class="content">ROCm works on RX570 (&quot;gfx803&quot;, including RX470+RX580 too).<p>Support was dropped upstream, but only because AMD no longer regularly test it. The code is still there and downstream distributors (like if you just apt-get install libamdhip64-5 &amp;&amp; pip3 install torch) usually flip it enabled again.<p><a href="https:&#x2F;&#x2F;salsa.debian.org&#x2F;rocm-team&#x2F;community&#x2F;team-project&#x2F;-&#x2F;wikis&#x2F;Supported-GPU-list#trixie-sid" rel="nofollow noreferrer">https:&#x2F;&#x2F;salsa.debian.org&#x2F;rocm-team&#x2F;community&#x2F;team-project&#x2F;-&#x2F;...</a><p>There is a scary red square in the table, but in my experience it worked completely fine for Stable Diffusion.</div><br/></div></div><div id="38647416" class="c"><input type="checkbox" id="c-38647416" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647354">parent</a><span>|</span><a href="#38651070">prev</a><span>|</span><a href="#38648821">next</a><span>|</span><label class="collapse" for="c-38647416">[-]</label><label class="expand" for="c-38647416">[7 more]</label></div><br/><div class="children"><div class="content">I ran 130,000 RX470-580 cards, so I know them quite well. Those cards aren&#x27;t going to do anything useful with AI&#x2F;ML. That technology is just too old and things are moving too quickly. It isn&#x27;t just the card, but the mobo, disks, ram, networking...</div><br/><div id="38650800" class="c"><input type="checkbox" id="c-38650800" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647416">parent</a><span>|</span><a href="#38647472">next</a><span>|</span><label class="collapse" for="c-38650800">[-]</label><label class="expand" for="c-38650800">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s fine for corporate customers, but how do you expect kids and hobbyists to learn the basics without spending thousands on an A6000 or something?</div><br/><div id="38651372" class="c"><input type="checkbox" id="c-38651372" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38650800">parent</a><span>|</span><a href="#38647472">next</a><span>|</span><label class="collapse" for="c-38651372">[-]</label><label class="expand" for="c-38651372">[1 more]</label></div><br/><div class="children"><div class="content">I believe strongly in &quot;where there is a will, there is a way.&quot;<p>Those kids and hobbyists can&#x27;t even rent time on high end AMD hardware today. I see that as one piece of the puzzle that I&#x27;m personally dedicating my time&#x2F;resources to resolving.</div><br/></div></div></div></div><div id="38647472" class="c"><input type="checkbox" id="c-38647472" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647416">parent</a><span>|</span><a href="#38650800">prev</a><span>|</span><a href="#38648821">next</a><span>|</span><label class="collapse" for="c-38647472">[-]</label><label class="expand" for="c-38647472">[4 more]</label></div><br/><div class="children"><div class="content">RX570 is going to do ML faster than a typical desktop CPU. That&#x27;s all you need for the person who has one to want to use it for Llama or Stable Diffusion, and then want to improve the software for the thing they&#x27;re now using.<p>Not everything is a huge datacenter.</div><br/><div id="38647643" class="c"><input type="checkbox" id="c-38647643" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647472">parent</a><span>|</span><a href="#38648821">next</a><span>|</span><label class="collapse" for="c-38647643">[-]</label><label class="expand" for="c-38647643">[3 more]</label></div><br/><div class="children"><div class="content">What is nonsense is that you think that AMD should dedicate limited resources to supporting a 6 year old card with only 4-8gb of ram (the ones I ran had 8).<p>I didn&#x27;t say they are bad cards... they are just outdated at this point.<p>If you really want to put your words to action... let me know. I&#x27;ll put you in touch with someone to buy 130,000 of these cards, and you can sell them to every college kid out there... until then, I wouldn&#x27;t hold AMD over the coals for not wanting to put effort into something like that when they are already lagging behind on their AI efforts as it is. I&#x27;d personally rather see them catch up a bit first.</div><br/><div id="38647784" class="c"><input type="checkbox" id="c-38647784" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647643">parent</a><span>|</span><a href="#38648821">next</a><span>|</span><label class="collapse" for="c-38647784">[-]</label><label class="expand" for="c-38647784">[2 more]</label></div><br/><div class="children"><div class="content">8GB is enough for Stable Diffusion or Llama 13B q4. They&#x27;re outdated, but non-outdated GPUs are still <i>expensive</i>, so they&#x27;re all many people can afford.<p>&gt; I&#x27;ll put you in touch with someone to buy 130,000 of these cards, and you can sell them to every college kid out there...<p>Just sell them on eBay? They still go for $50-$100 each, so you&#x27;re sitting on several million dollars worth of GPUs.<p>&gt; I&#x27;d personally rather see them catch up a bit first.<p>Growing the community is how you catch up. That doesn&#x27;t happen if people can&#x27;t afford the only GPUs you support.</div><br/><div id="38649809" class="c"><input type="checkbox" id="c-38649809" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647784">parent</a><span>|</span><a href="#38648821">next</a><span>|</span><label class="collapse" for="c-38649809">[-]</label><label class="expand" for="c-38649809">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Growing the community is how you catch up.<p>Agreed 100%.<p>&gt; That doesn&#x27;t happen if people can&#x27;t afford the only GPUs you support.<p>On this part, we are going to have to agree to disagree. I feel like being able to at least affordably rent time on the high end GPUs is another alternative to buying them. As I mentioned above, that is something I&#x27;m actively working on.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38648821" class="c"><input type="checkbox" id="c-38648821" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38647354">parent</a><span>|</span><a href="#38647416">prev</a><span>|</span><a href="#38647504">next</a><span>|</span><label class="collapse" for="c-38648821">[-]</label><label class="expand" for="c-38648821">[1 more]</label></div><br/><div class="children"><div class="content">The older cards are going to be supported by Mesa and RustiCL for the foreseeable future.  Rocm is not the only game in town, far from it.</div><br/></div></div></div></div></div></div><div id="38647504" class="c"><input type="checkbox" id="c-38647504" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645991">parent</a><span>|</span><a href="#38646722">prev</a><span>|</span><a href="#38645357">next</a><span>|</span><label class="collapse" for="c-38647504">[-]</label><label class="expand" for="c-38647504">[1 more]</label></div><br/><div class="children"><div class="content">They did recently add support for 7900xt<p><a href="https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;Radeon-RX-7900-XT-ROCm-PyTorch" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;Radeon-RX-7900-XT-ROCm-PyTorch</a></div><br/></div></div></div></div></div></div><div id="38645357" class="c"><input type="checkbox" id="c-38645357" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#38645197">parent</a><span>|</span><a href="#38645879">prev</a><span>|</span><a href="#38649164">next</a><span>|</span><label class="collapse" for="c-38645357">[-]</label><label class="expand" for="c-38645357">[2 more]</label></div><br/><div class="children"><div class="content">Apple as well. <i>Everyone&#x27;s</i> failure to commit in this situation enabled a highly-integrated competitor to clean up shop. It&#x27;s funny how much clearer OpenCL&#x27;s value prop is in hindsight...</div><br/><div id="38645867" class="c"><input type="checkbox" id="c-38645867" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38645197">root</a><span>|</span><a href="#38645357">parent</a><span>|</span><a href="#38649164">next</a><span>|</span><label class="collapse" for="c-38645867">[-]</label><label class="expand" for="c-38645867">[1 more]</label></div><br/><div class="children"><div class="content">Apple created OpenCL, and after disagreements with Khronos on how to move OpenCL forward, they stopped caring.<p>Apple also doesn&#x27;t care about HPC, and has their own stuff on top of Metal Compute Shaders, just like Microsoft has DirectML.</div><br/></div></div></div></div></div></div><div id="38649164" class="c"><input type="checkbox" id="c-38649164" checked=""/><div class="controls bullet"><span class="by">lvl102</span><span>|</span><a href="#38645197">prev</a><span>|</span><a href="#38645866">next</a><span>|</span><label class="collapse" for="c-38649164">[-]</label><label class="expand" for="c-38649164">[1 more]</label></div><br/><div class="children"><div class="content">Intel has not done anything in the past ten years. They wasted billions on barely functional GPUs. They sat on CPU monopoly and slowed innovations while churning out profit.<p>At least Nvidia built something to facilitate advances in AI. They made a bold bet that paid off.</div><br/></div></div><div id="38645866" class="c"><input type="checkbox" id="c-38645866" checked=""/><div class="controls bullet"><span class="by">bragr</span><span>|</span><a href="#38649164">prev</a><span>|</span><a href="#38645153">next</a><span>|</span><label class="collapse" for="c-38645866">[-]</label><label class="expand" for="c-38645866">[10 more]</label></div><br/><div class="children"><div class="content">People don&#x27;t seem particularly motivated to move away from CUDA to me. I&#x27;ve been poking around various models and tooling over the last couple months, and they pretty much all have something like<p><pre><code>  device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
</code></pre>
and I&#x27;ve yet to see a single one implement the AMD NN middleware: <a href="https:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;developer&#x2F;zendnn.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;developer&#x2F;zendnn.html</a></div><br/><div id="38646776" class="c"><input type="checkbox" id="c-38646776" checked=""/><div class="controls bullet"><span class="by">kaelinl</span><span>|</span><a href="#38645866">parent</a><span>|</span><a href="#38646044">next</a><span>|</span><label class="collapse" for="c-38646776">[-]</label><label class="expand" for="c-38646776">[3 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t used it personally, but my understanding is that AMD&#x2F;ROCm-accelerated backends for PyTorch overload the &quot;cuda&quot; device and module so that ROCm shows up as CUDA for feature testing. They want to make a transition seamless, and lots of existing code checks for CUDA, so they do what&#x27;s necessary to make that existing code run.</div><br/><div id="38651245" class="c"><input type="checkbox" id="c-38651245" checked=""/><div class="controls bullet"><span class="by">mappu</span><span>|</span><a href="#38645866">root</a><span>|</span><a href="#38646776">parent</a><span>|</span><a href="#38647020">next</a><span>|</span><label class="collapse" for="c-38651245">[-]</label><label class="expand" for="c-38651245">[1 more]</label></div><br/><div class="children"><div class="content">That seems to be the case for me out-of-the-box.<p><pre><code>    $ python3 -m venv .
    $ .&#x2F;bin&#x2F;pip3 install torch --index-url https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;rocm5.6
    $ .&#x2F;bin&#x2F;python3 -c &#x27;import torch; print([torch.cuda.is_available(), torch.cuda.get_device_name(0)])&#x27;
    [True, &#x27;AMD Radeon RX 6600 XT&#x27;]</code></pre></div><br/></div></div><div id="38647020" class="c"><input type="checkbox" id="c-38647020" checked=""/><div class="controls bullet"><span class="by">bragr</span><span>|</span><a href="#38645866">root</a><span>|</span><a href="#38646776">parent</a><span>|</span><a href="#38651245">prev</a><span>|</span><a href="#38646044">next</a><span>|</span><label class="collapse" for="c-38647020">[-]</label><label class="expand" for="c-38647020">[1 more]</label></div><br/><div class="children"><div class="content">Which tools actually pull down those packages though? If anything that&#x27;s a counter point to the Intel CEO: industry is adopting CUDA as the defacto standard.</div><br/></div></div></div></div><div id="38646044" class="c"><input type="checkbox" id="c-38646044" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#38645866">parent</a><span>|</span><a href="#38646776">prev</a><span>|</span><a href="#38646733">next</a><span>|</span><label class="collapse" for="c-38646044">[-]</label><label class="expand" for="c-38646044">[5 more]</label></div><br/><div class="children"><div class="content">This is a bit of a chicken and egg problem, because the developers of these projects probably don&#x27;t even have relevant AMD devices to test that, because why would they?<p>The only reasonable way to break the cycle would be proactive intervention by AMD to contribute code and testing  and easy &quot;works out of the box&quot; installation to all the major popular projects to add AMD support so they can sell more their hardware later, but I&#x27;m not seeing AMD doing that.</div><br/><div id="38646756" class="c"><input type="checkbox" id="c-38646756" checked=""/><div class="controls bullet"><span class="by">mschuetz</span><span>|</span><a href="#38645866">root</a><span>|</span><a href="#38646044">parent</a><span>|</span><a href="#38648387">next</a><span>|</span><label class="collapse" for="c-38646756">[-]</label><label class="expand" for="c-38646756">[2 more]</label></div><br/><div class="children"><div class="content">CUDA is just way to good in terms of UX with nothing platform-independent coming close to it. I wish there was a competitor, but there simply isn&#x27;t.<p>I wish AMD&amp;Intel would extend compute shaders with pointers&amp;pointer casting, arbitrary large buffers instead of just 4GB, device-wide sync, and function pointers. Those are kinda my must-have functionality. Even better, just use C++ for compute shaders.</div><br/><div id="38648402" class="c"><input type="checkbox" id="c-38648402" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#38645866">root</a><span>|</span><a href="#38646756">parent</a><span>|</span><a href="#38648387">next</a><span>|</span><label class="collapse" for="c-38648402">[-]</label><label class="expand" for="c-38648402">[1 more]</label></div><br/><div class="children"><div class="content">Freestanding C++ can be compiled to amdgpu or nvptx using clang. There are rough edges but it&#x27;s usable.</div><br/></div></div></div></div><div id="38648387" class="c"><input type="checkbox" id="c-38648387" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#38645866">root</a><span>|</span><a href="#38646044">parent</a><span>|</span><a href="#38646756">prev</a><span>|</span><a href="#38646564">next</a><span>|</span><label class="collapse" for="c-38648387">[-]</label><label class="expand" for="c-38648387">[1 more]</label></div><br/><div class="children"><div class="content">Like the pytorch one? Or the onnx? Or Triton? There&#x27;s loads of these machine learning things, each of which built their thing assuming Nvidia, to which AMD is stoically working to implement support for their hardware. It&#x27;s a task of unbounded scope with unclear return on investment and they&#x27;re doing it anyway.</div><br/></div></div><div id="38646564" class="c"><input type="checkbox" id="c-38646564" checked=""/><div class="controls bullet"><span class="by">lainga</span><span>|</span><a href="#38645866">root</a><span>|</span><a href="#38646044">parent</a><span>|</span><a href="#38648387">prev</a><span>|</span><a href="#38646733">next</a><span>|</span><label class="collapse" for="c-38646564">[-]</label><label class="expand" for="c-38646564">[1 more]</label></div><br/><div class="children"><div class="content">Maybe the AMD device will just have to announce it&#x27;s (KHTML, like Gecko).</div><br/></div></div></div></div><div id="38646733" class="c"><input type="checkbox" id="c-38646733" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38645866">parent</a><span>|</span><a href="#38646044">prev</a><span>|</span><a href="#38645153">next</a><span>|</span><label class="collapse" for="c-38646733">[-]</label><label class="expand" for="c-38646733">[1 more]</label></div><br/><div class="children"><div class="content">You didn&#x27;t watch the announcement on the MI300x. You don&#x27;t have to change that line to use AMD.</div><br/></div></div></div></div><div id="38645153" class="c"><input type="checkbox" id="c-38645153" checked=""/><div class="controls bullet"><span class="by">mngdtt</span><span>|</span><a href="#38645866">prev</a><span>|</span><a href="#38646450">next</a><span>|</span><label class="collapse" for="c-38645153">[-]</label><label class="expand" for="c-38645153">[19 more]</label></div><br/><div class="children"><div class="content">Funny, because the entire industry is motivated to move away from x86 too. I wonder if he&#x27;d like that to happen.</div><br/><div id="38645212" class="c"><input type="checkbox" id="c-38645212" checked=""/><div class="controls bullet"><span class="by">rbanffy</span><span>|</span><a href="#38645153">parent</a><span>|</span><a href="#38646450">next</a><span>|</span><label class="collapse" for="c-38645212">[-]</label><label class="expand" for="c-38645212">[18 more]</label></div><br/><div class="children"><div class="content">While we are at it, why not both? Also, can we move away from Windows and Office as well? And SharePoint, please? And add good search capabilities to Confluence.</div><br/><div id="38650971" class="c"><input type="checkbox" id="c-38650971" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645212">parent</a><span>|</span><a href="#38645983">next</a><span>|</span><label class="collapse" for="c-38650971">[-]</label><label class="expand" for="c-38650971">[1 more]</label></div><br/><div class="children"><div class="content">Office and x86 are both things I like because 3rd parties can create compatible ware.</div><br/></div></div><div id="38645983" class="c"><input type="checkbox" id="c-38645983" checked=""/><div class="controls bullet"><span class="by">ryanjshaw</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645212">parent</a><span>|</span><a href="#38650971">prev</a><span>|</span><a href="#38645622">next</a><span>|</span><label class="collapse" for="c-38645983">[-]</label><label class="expand" for="c-38645983">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve got bad news for you. AD, Office, SharePoint, Teams, BI, etc. are becoming unified in the cloud around Power Platform.<p>Any new business problem in a Microsoft shop is going to go with a Power Platform&#x2F;Teams&#x2F;SharePoint Online solution as first prize.<p>Personally, I like it, even if it has some annoying challenges. I&#x27;ve not seen anything else come close in terms of rapid application development, integration and deployment; there&#x27;s very little to license and what there is left falls under existing procurement relationships for most big enterprises that rely on Microsoft already.</div><br/><div id="38648628" class="c"><input type="checkbox" id="c-38648628" checked=""/><div class="controls bullet"><span class="by">rqtwteye</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645983">parent</a><span>|</span><a href="#38645622">next</a><span>|</span><label class="collapse" for="c-38648628">[-]</label><label class="expand" for="c-38648628">[3 more]</label></div><br/><div class="children"><div class="content">But don&#x27;t forget: It&#x27;s Sharepoint all the way down.</div><br/><div id="38649522" class="c"><input type="checkbox" id="c-38649522" checked=""/><div class="controls bullet"><span class="by">EdSharkey</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38648628">parent</a><span>|</span><a href="#38645622">next</a><span>|</span><label class="collapse" for="c-38649522">[-]</label><label class="expand" for="c-38649522">[2 more]</label></div><br/><div class="children"><div class="content">And on the way down, there was an unholy congress between SharePoint and OneDrive to create a new eldritch horror.</div><br/><div id="38651140" class="c"><input type="checkbox" id="c-38651140" checked=""/><div class="controls bullet"><span class="by">selimthegrim</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38649522">parent</a><span>|</span><a href="#38645622">next</a><span>|</span><label class="collapse" for="c-38651140">[-]</label><label class="expand" for="c-38651140">[1 more]</label></div><br/><div class="children"><div class="content">I suppose it was always going to be HR software</div><br/></div></div></div></div></div></div></div></div><div id="38645622" class="c"><input type="checkbox" id="c-38645622" checked=""/><div class="controls bullet"><span class="by">beembeem</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645212">parent</a><span>|</span><a href="#38645983">prev</a><span>|</span><a href="#38645271">next</a><span>|</span><label class="collapse" for="c-38645622">[-]</label><label class="expand" for="c-38645622">[2 more]</label></div><br/><div class="children"><div class="content">Are you me? This list enumerates many specific pains I&#x27;ve had over the years. Adobe Acrobat might be next on this list.</div><br/><div id="38645869" class="c"><input type="checkbox" id="c-38645869" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645622">parent</a><span>|</span><a href="#38645271">next</a><span>|</span><label class="collapse" for="c-38645869">[-]</label><label class="expand" for="c-38645869">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re just one of the borg that has existed and used computers and all felt the exact same thing.</div><br/></div></div></div></div><div id="38645271" class="c"><input type="checkbox" id="c-38645271" checked=""/><div class="controls bullet"><span class="by">znpy</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645212">parent</a><span>|</span><a href="#38645622">prev</a><span>|</span><a href="#38646450">next</a><span>|</span><label class="collapse" for="c-38645271">[-]</label><label class="expand" for="c-38645271">[10 more]</label></div><br/><div class="children"><div class="content">Why not move away from confluence as well?</div><br/><div id="38645370" class="c"><input type="checkbox" id="c-38645370" checked=""/><div class="controls bullet"><span class="by">DarmokJalad1701</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645271">parent</a><span>|</span><a href="#38645365">next</a><span>|</span><label class="collapse" for="c-38645370">[-]</label><label class="expand" for="c-38645370">[7 more]</label></div><br/><div class="children"><div class="content">Move away from everything. Retvrn to monke.</div><br/><div id="38645510" class="c"><input type="checkbox" id="c-38645510" checked=""/><div class="controls bullet"><span class="by">mckirk</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645370">parent</a><span>|</span><a href="#38645662">next</a><span>|</span><label class="collapse" for="c-38645510">[-]</label><label class="expand" for="c-38645510">[5 more]</label></div><br/><div class="children"><div class="content">No, further, return to sea. Exist as peaceful algae.</div><br/><div id="38645858" class="c"><input type="checkbox" id="c-38645858" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645510">parent</a><span>|</span><a href="#38645567">next</a><span>|</span><label class="collapse" for="c-38645858">[-]</label><label class="expand" for="c-38645858">[1 more]</label></div><br/><div class="children"><div class="content">Why oh why didn&#x27;t I take the blue pill?</div><br/></div></div><div id="38645567" class="c"><input type="checkbox" id="c-38645567" checked=""/><div class="controls bullet"><span class="by">rbanffy</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645510">parent</a><span>|</span><a href="#38645858">prev</a><span>|</span><a href="#38645662">next</a><span>|</span><label class="collapse" for="c-38645567">[-]</label><label class="expand" for="c-38645567">[3 more]</label></div><br/><div class="children"><div class="content">Coral, please. I don&#x27;t want to be floating too much.</div><br/><div id="38645889" class="c"><input type="checkbox" id="c-38645889" checked=""/><div class="controls bullet"><span class="by">eric__cartman</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645567">parent</a><span>|</span><a href="#38645839">next</a><span>|</span><label class="collapse" for="c-38645889">[-]</label><label class="expand" for="c-38645889">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d rather be a unicellular microorganism. Only having one cell to worry about sounds pretty chill.</div><br/></div></div><div id="38645839" class="c"><input type="checkbox" id="c-38645839" checked=""/><div class="controls bullet"><span class="by">datadrivenangel</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645567">parent</a><span>|</span><a href="#38645889">prev</a><span>|</span><a href="#38645662">next</a><span>|</span><label class="collapse" for="c-38645839">[-]</label><label class="expand" for="c-38645839">[1 more]</label></div><br/><div class="children"><div class="content">Go with the flow man.</div><br/></div></div></div></div></div></div><div id="38645662" class="c"><input type="checkbox" id="c-38645662" checked=""/><div class="controls bullet"><span class="by">2358452</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645370">parent</a><span>|</span><a href="#38645510">prev</a><span>|</span><a href="#38645365">next</a><span>|</span><label class="collapse" for="c-38645662">[-]</label><label class="expand" for="c-38645662">[1 more]</label></div><br/><div class="children"><div class="content">No, it&#x27;s the <i>other</i> direction (away from monke): go toward open source :)</div><br/></div></div></div></div><div id="38645365" class="c"><input type="checkbox" id="c-38645365" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645271">parent</a><span>|</span><a href="#38645370">prev</a><span>|</span><a href="#38646450">next</a><span>|</span><label class="collapse" for="c-38645365">[-]</label><label class="expand" for="c-38645365">[2 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t we all do that when they discontinued on-premise and then followed that up with a massive data loss in their hosted instance.</div><br/><div id="38647406" class="c"><input type="checkbox" id="c-38647406" checked=""/><div class="controls bullet"><span class="by">nineteen999</span><span>|</span><a href="#38645153">root</a><span>|</span><a href="#38645365">parent</a><span>|</span><a href="#38646450">next</a><span>|</span><label class="collapse" for="c-38647406">[-]</label><label class="expand" for="c-38647406">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the way we do SaaS over here in Australia. Kicks tyres. She&#x27;ll be right mate.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38646450" class="c"><input type="checkbox" id="c-38646450" checked=""/><div class="controls bullet"><span class="by">lacker</span><span>|</span><a href="#38645153">prev</a><span>|</span><a href="#38646061">next</a><span>|</span><label class="collapse" for="c-38646450">[-]</label><label class="expand" for="c-38646450">[1 more]</label></div><br/><div class="children"><div class="content">Okay, well, hurry up and do it already then. I don&#x27;t need CUDA. Just make PyTorch run nicely on an Intel GPU that outperforms an Nvidia GPU of the same cost and I&#x27;ll happily buy an Intel GPU.</div><br/></div></div><div id="38646061" class="c"><input type="checkbox" id="c-38646061" checked=""/><div class="controls bullet"><span class="by">jonahrd</span><span>|</span><a href="#38646450">prev</a><span>|</span><a href="#38651429">next</a><span>|</span><label class="collapse" for="c-38646061">[-]</label><label class="expand" for="c-38646061">[4 more]</label></div><br/><div class="children"><div class="content">I absolutely understand why NVIDIA is incentivized to maintain CUDA dominance, and I absolutely understand that within these market parameters, AMD&#x2F;Intel&#x2F;others have dropped the ball.<p>However it&#x27;s worth noting that in the end, it&#x27;s the consumer that loses out when these technical&#x2F;capability moats maintain a de facto sort of monopoly on certain use cases.</div><br/><div id="38646341" class="c"><input type="checkbox" id="c-38646341" checked=""/><div class="controls bullet"><span class="by">nemothekid</span><span>|</span><a href="#38646061">parent</a><span>|</span><a href="#38646774">next</a><span>|</span><label class="collapse" for="c-38646341">[-]</label><label class="expand" for="c-38646341">[2 more]</label></div><br/><div class="children"><div class="content">&gt;<i>However it&#x27;s worth noting that in the end, it&#x27;s the consumer that loses out when these technical&#x2F;capability moats maintain a de facto sort of monopoly on certain use cases.</i><p>I think in most cases you are right, but for CUDA, the consumer is winning. CUDA isn&#x27;t some special secret complex algorithm just for nvidia GPUs. nvidia just spent a decade giving a fuck about the developer experience across a wide range of industries. CUDA isn&#x27;t just AI, it&#x27;s also physics, numerical modeling, cryptography, biology and more. They found a thousand use cases and spent time listening to customers and built a platform around it - it just turned out AI ended up being a huge money maker.<p>The problem is, and will continue to be, that Intel and AMD will only see the &quot;AI&quot; money bags and ignore every other part of platform, from debugging, compilers, language integration, GUIs, and even bug fixing. If Intel is saying here &quot;we want to eliminate CUDA by investing billions into OpenCL and ensure OpenCL has a top of line developer experience and platform&quot; than I&#x27;d be excited. But what I&#x27;m reading is &quot;we will replace a few function calls for CUDA in Pytorch&quot;, which might be fun for a while up until you have to debug some performance issue and you realize you can get in touch with a CUDA engineer on github instead of emailing some dead Intel mailing list.</div><br/><div id="38650572" class="c"><input type="checkbox" id="c-38650572" checked=""/><div class="controls bullet"><span class="by">dopeboy</span><span>|</span><a href="#38646061">root</a><span>|</span><a href="#38646341">parent</a><span>|</span><a href="#38646774">next</a><span>|</span><label class="collapse" for="c-38650572">[-]</label><label class="expand" for="c-38650572">[1 more]</label></div><br/><div class="children"><div class="content">Well said. It sounds like NVIDIA went through the hard work of listening and building for years (decades!). I can imagine some top level execs looking at that success and looking to replicate the tech, studying the use cases that customers want.</div><br/></div></div></div></div><div id="38646774" class="c"><input type="checkbox" id="c-38646774" checked=""/><div class="controls bullet"><span class="by">mschuetz</span><span>|</span><a href="#38646061">parent</a><span>|</span><a href="#38646341">prev</a><span>|</span><a href="#38651429">next</a><span>|</span><label class="collapse" for="c-38646774">[-]</label><label class="expand" for="c-38646774">[1 more]</label></div><br/><div class="children"><div class="content">AMD&#x2F;Intel&#x2F;Khronos are free to compete with something better. NVIDIA isn&#x27;t preventing them from doing so. And in that regard, CUDA is a massive benefit to consumers, because the alternatives are really bad.</div><br/></div></div></div></div><div id="38651429" class="c"><input type="checkbox" id="c-38651429" checked=""/><div class="controls bullet"><span class="by">sharts</span><span>|</span><a href="#38646061">prev</a><span>|</span><a href="#38646847">next</a><span>|</span><label class="collapse" for="c-38651429">[-]</label><label class="expand" for="c-38651429">[1 more]</label></div><br/><div class="children"><div class="content">Yeah that&#x27;s never gonna happen. The competitors to NVIDIA are pretty trash at actually pumping out coherent and consistent software ecosystems.<p>NVIDIA seems to be able to do software markedly better than all the other hardware folks. And software is the real product that end users actually interact with.</div><br/></div></div><div id="38646847" class="c"><input type="checkbox" id="c-38646847" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#38651429">prev</a><span>|</span><a href="#38651883">next</a><span>|</span><label class="collapse" for="c-38646847">[-]</label><label class="expand" for="c-38646847">[2 more]</label></div><br/><div class="children"><div class="content">It looks like both Pat Gelsinger and Lisa Su have no clue about software and can&#x27;t even delegate this. They expect &quot;the community&quot; to do the software for their extremely complicated hardware.<p>If you see the presentations by Nvidia&#x27;s Bill Daly [1] it shows they&#x27;ve been evolving hardware and software together. The CUDA programming model was a massive bet a long time ago. And they deservedly won ML&#x2F;AI.<p>Unless Intel and AMD do a radical change it&#x27;s game over for them. They will lose to ARM and Nvidia.<p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=kLiwvnr4L80" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=kLiwvnr4L80</a></div><br/><div id="38649244" class="c"><input type="checkbox" id="c-38649244" checked=""/><div class="controls bullet"><span class="by">lvl102</span><span>|</span><a href="#38646847">parent</a><span>|</span><a href="#38651883">next</a><span>|</span><label class="collapse" for="c-38649244">[-]</label><label class="expand" for="c-38649244">[1 more]</label></div><br/><div class="children"><div class="content">It’s not that they don’t understand the problem. It’s just that there’s a massive land grab right now and no one wants to waste resources reinventing the wheels (of parallel computing) which is actually a lot harder than people make it out to be.</div><br/></div></div></div></div><div id="38651883" class="c"><input type="checkbox" id="c-38651883" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38646847">prev</a><span>|</span><a href="#38651859">next</a><span>|</span><label class="collapse" for="c-38651883">[-]</label><label class="expand" for="c-38651883">[1 more]</label></div><br/><div class="children"><div class="content">Can Intel show us a GPU that beats Nvidia A100? Can they show us a framework for that GPU that is better than CUDA and which most companies are willing to adopt?<p>No? Then is just talk.</div><br/></div></div><div id="38651859" class="c"><input type="checkbox" id="c-38651859" checked=""/><div class="controls bullet"><span class="by">physicsguy</span><span>|</span><a href="#38651883">prev</a><span>|</span><a href="#38646706">next</a><span>|</span><label class="collapse" for="c-38651859">[-]</label><label class="expand" for="c-38651859">[1 more]</label></div><br/><div class="children"><div class="content">Then build some decent library support.<p>Intel should have an advantage here as MKL and OneAPI are quite good, but the AMD’s roc libraries are not very good.</div><br/></div></div><div id="38646706" class="c"><input type="checkbox" id="c-38646706" checked=""/><div class="controls bullet"><span class="by">MangoCoffee</span><span>|</span><a href="#38651859">prev</a><span>|</span><a href="#38645289">next</a><span>|</span><label class="collapse" for="c-38646706">[-]</label><label class="expand" for="c-38646706">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s Intel. I take whatever Intel says with a grant of salt.<p>Intel pushed 14nm to 14nm+++ and used backhand tactics to push OEMs not to use AMD. how long did Intel stay on some trend until it fizzled for Intel?<p>while Nvidia has been working on CUDA forever?</div><br/></div></div><div id="38645289" class="c"><input type="checkbox" id="c-38645289" checked=""/><div class="controls bullet"><span class="by">sbininit</span><span>|</span><a href="#38646706">prev</a><span>|</span><a href="#38651848">next</a><span>|</span><label class="collapse" for="c-38645289">[-]</label><label class="expand" for="c-38645289">[5 more]</label></div><br/><div class="children"><div class="content">Sure, provide a 100% CUDA-compatible chip, then do the microsoft-style EEE.</div><br/><div id="38645502" class="c"><input type="checkbox" id="c-38645502" checked=""/><div class="controls bullet"><span class="by">rbanffy</span><span>|</span><a href="#38645289">parent</a><span>|</span><a href="#38645749">next</a><span>|</span><label class="collapse" for="c-38645502">[-]</label><label class="expand" for="c-38645502">[2 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t a 100% Cuda-compatible toolchain suffice? Then you could write your software for Cuda and the software environment would run it on whatever is on your computer.</div><br/><div id="38645902" class="c"><input type="checkbox" id="c-38645902" checked=""/><div class="controls bullet"><span class="by">ladberg</span><span>|</span><a href="#38645289">root</a><span>|</span><a href="#38645502">parent</a><span>|</span><a href="#38645749">next</a><span>|</span><label class="collapse" for="c-38645902">[-]</label><label class="expand" for="c-38645902">[1 more]</label></div><br/><div class="children"><div class="content">CUDA the language itself isn&#x27;t necessarily the biggest obstacle for porting code, it&#x27;s the usage of low level hardware features. You&#x27;d have to make a chip that&#x27;s pretty dang close to Nvidia&#x27;s before most hand-written CUDA code could run on it performantly.</div><br/></div></div></div></div><div id="38645722" class="c"><input type="checkbox" id="c-38645722" checked=""/><div class="controls bullet"><span class="by">poisonborz</span><span>|</span><a href="#38645289">parent</a><span>|</span><a href="#38645749">prev</a><span>|</span><a href="#38651848">next</a><span>|</span><label class="collapse" for="c-38645722">[-]</label><label class="expand" for="c-38645722">[1 more]</label></div><br/><div class="children"><div class="content">Oracle: oh no, my precious API patents!</div><br/></div></div></div></div><div id="38651848" class="c"><input type="checkbox" id="c-38651848" checked=""/><div class="controls bullet"><span class="by">trynumber9</span><span>|</span><a href="#38645289">prev</a><span>|</span><a href="#38645726">next</a><span>|</span><label class="collapse" for="c-38651848">[-]</label><label class="expand" for="c-38651848">[1 more]</label></div><br/><div class="children"><div class="content">For some context, Gelsinger was also behind Intel&#x27;s first GPGPU attempts back around 2008. Before he was appointed CEO of Intel he was at VMWare but did an interview with the Computer History Museum in 2019 and said this
<a href="https:&#x2F;&#x2F;youtu.be&#x2F;MxZe1i8z-8Y?t=780" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;MxZe1i8z-8Y?t=780</a><p>He has wanted GPGPU or at least massively parallel compute for a long time and says it would be 10 years of work to make that happen.<p>And secondly, do you really think he&#x27;s wrong about this part:
&quot;As inferencing occurs, hey, once you&#x27;ve trained the model… There is no CUDA dependency,&quot; Gelsinger continued. &quot;It&#x27;s all about, can you run that model well?&quot;<p>Why run inferencing on an expensive H100? Gaudi seems like a pretty good idea.</div><br/></div></div><div id="38645726" class="c"><input type="checkbox" id="c-38645726" checked=""/><div class="controls bullet"><span class="by">davesque</span><span>|</span><a href="#38651848">prev</a><span>|</span><a href="#38651036">next</a><span>|</span><label class="collapse" for="c-38645726">[-]</label><label class="expand" for="c-38645726">[1 more]</label></div><br/><div class="children"><div class="content">Prove it. CUDA has dominated the GPGPU scene for years now. Competitors should have had plenty of time to catch up if they had wanted to.</div><br/></div></div><div id="38651036" class="c"><input type="checkbox" id="c-38651036" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#38645726">prev</a><span>|</span><a href="#38649659">next</a><span>|</span><label class="collapse" for="c-38651036">[-]</label><label class="expand" for="c-38651036">[1 more]</label></div><br/><div class="children"><div class="content">OpenCL was a perfectly good alternative to CUDA.  Unfortunately due to the short-sightedness of programmers using the proprietary CUDA, here we are with CUDA dominant.</div><br/></div></div><div id="38649659" class="c"><input type="checkbox" id="c-38649659" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#38651036">prev</a><span>|</span><a href="#38646231">next</a><span>|</span><label class="collapse" for="c-38649659">[-]</label><label class="expand" for="c-38649659">[1 more]</label></div><br/><div class="children"><div class="content">My hunch is that there is some fundamental misunderstanding of what CUDA is and why it works. Its been like 6 years now of well funded attempts to displace CUDA, and there has been very little progress. In fact I&#x27;d argue its gone backwards as things like FlashAttention have further locked in the benefits of CUDA &amp; Nvidia chips.</div><br/></div></div><div id="38646231" class="c"><input type="checkbox" id="c-38646231" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#38649659">prev</a><span>|</span><a href="#38645621">next</a><span>|</span><label class="collapse" for="c-38646231">[-]</label><label class="expand" for="c-38646231">[1 more]</label></div><br/><div class="children"><div class="content">He&#x27;s not wrong. An open alternative to CUDA would be really nice. But so far neither Intel or AMD have put enough effort&#x2F;investment into actually eliminating the CUDA market.</div><br/></div></div><div id="38645621" class="c"><input type="checkbox" id="c-38645621" checked=""/><div class="controls bullet"><span class="by">eddiewithzato</span><span>|</span><a href="#38646231">prev</a><span>|</span><a href="#38645986">next</a><span>|</span><label class="collapse" for="c-38645621">[-]</label><label class="expand" for="c-38645621">[1 more]</label></div><br/><div class="children"><div class="content">These tech companies really had no vision (and risk taking) like NVIDIA.</div><br/></div></div><div id="38645986" class="c"><input type="checkbox" id="c-38645986" checked=""/><div class="controls bullet"><span class="by">barryrandall</span><span>|</span><a href="#38645621">prev</a><span>|</span><a href="#38650955">next</a><span>|</span><label class="collapse" for="c-38645986">[-]</label><label class="expand" for="c-38645986">[1 more]</label></div><br/><div class="children"><div class="content">No they&#x27;re not. Some industry participants covet CUDA&#x27;s market share and ecosystem power, but not enough to work together on compelling, interoperable alternatives.</div><br/></div></div><div id="38650955" class="c"><input type="checkbox" id="c-38650955" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38645986">prev</a><span>|</span><a href="#38650975">next</a><span>|</span><label class="collapse" for="c-38650955">[-]</label><label class="expand" for="c-38650955">[1 more]</label></div><br/><div class="children"><div class="content">Why hasn’t CUDA turned into an x86 type thing where competitors can be compatible, and well… compete?</div><br/></div></div><div id="38650975" class="c"><input type="checkbox" id="c-38650975" checked=""/><div class="controls bullet"><span class="by">isatty</span><span>|</span><a href="#38650955">prev</a><span>|</span><a href="#38646012">next</a><span>|</span><label class="collapse" for="c-38650975">[-]</label><label class="expand" for="c-38650975">[1 more]</label></div><br/><div class="children"><div class="content">Then why isn’t the CEO, the person with the power do so, putting his money where his mouth is?</div><br/></div></div><div id="38646012" class="c"><input type="checkbox" id="c-38646012" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#38650975">prev</a><span>|</span><a href="#38646497">next</a><span>|</span><label class="collapse" for="c-38646012">[-]</label><label class="expand" for="c-38646012">[7 more]</label></div><br/><div class="children"><div class="content">Having the relatively unique experience of trying to sell an alternative deep learning stack, and then selling that company to Intel, color me skeptical. AMA.</div><br/><div id="38646245" class="c"><input type="checkbox" id="c-38646245" checked=""/><div class="controls bullet"><span class="by">marssaxman</span><span>|</span><a href="#38646012">parent</a><span>|</span><a href="#38646089">next</a><span>|</span><label class="collapse" for="c-38646245">[-]</label><label class="expand" for="c-38646245">[2 more]</label></div><br/><div class="children"><div class="content">Having worked for a deep learning startup which was acquired by Intel, I am likewise skeptical.<p>(Do we know each other?)</div><br/><div id="38646281" class="c"><input type="checkbox" id="c-38646281" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#38646012">root</a><span>|</span><a href="#38646245">parent</a><span>|</span><a href="#38646089">next</a><span>|</span><label class="collapse" for="c-38646281">[-]</label><label class="expand" for="c-38646281">[1 more]</label></div><br/><div class="children"><div class="content">Hey! Yep, this is Choong.</div><br/></div></div></div></div><div id="38646089" class="c"><input type="checkbox" id="c-38646089" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#38646012">parent</a><span>|</span><a href="#38646245">prev</a><span>|</span><a href="#38646497">next</a><span>|</span><label class="collapse" for="c-38646089">[-]</label><label class="expand" for="c-38646089">[4 more]</label></div><br/><div class="children"><div class="content">Well, can you elaborate on what exactly you observed&#x2F;felt from Intel that makes you skeptical about these claims?</div><br/><div id="38646202" class="c"><input type="checkbox" id="c-38646202" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#38646012">root</a><span>|</span><a href="#38646089">parent</a><span>|</span><a href="#38646497">next</a><span>|</span><label class="collapse" for="c-38646202">[-]</label><label class="expand" for="c-38646202">[3 more]</label></div><br/><div class="children"><div class="content">Across the semi industry the prevailing attitude is that the chip is the important part and software is a nice add-on that you build as a one-off or that &quot;the community&quot; will support. This has changed much more slowly than I thought it would given NVIDIA&#x27;s demonstration of what is possible when software is viewed as a core part of the product. Claiming the CUDA moat is weak suggests that it&#x27;s business as usual over there.</div><br/><div id="38650609" class="c"><input type="checkbox" id="c-38650609" checked=""/><div class="controls bullet"><span class="by">dopeboy</span><span>|</span><a href="#38646012">root</a><span>|</span><a href="#38646202">parent</a><span>|</span><a href="#38646497">next</a><span>|</span><label class="collapse" for="c-38650609">[-]</label><label class="expand" for="c-38650609">[2 more]</label></div><br/><div class="children"><div class="content">Does the solution lie in genuinely investing in a CUDA alternative?</div><br/><div id="38652050" class="c"><input type="checkbox" id="c-38652050" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#38646012">root</a><span>|</span><a href="#38650609">parent</a><span>|</span><a href="#38646497">next</a><span>|</span><label class="collapse" for="c-38652050">[-]</label><label class="expand" for="c-38652050">[1 more]</label></div><br/><div class="children"><div class="content">In my opinion the main issues are a layer up from that, it&#x27;s more of a customer experience and brand building exercise to solve by actually improving the tools people use. Today NVIDIA is the default if you want to get work done and not have a lot of tooling issues. Nobody is going to forego that experience to work with flaky slow products from a late number three entrant but that&#x27;s more or less the perception today. What&#x27;s weird is they actually have the high ground in an important area: They ship more GPUs than just about everyone and have for a long time, their integrated graphics drivers are pretty good (virtualization!), and they have a deep bench of performance engineering talent. NVIDIA structurally has no way to reach the same volume. Intel with some investment of time could be the most productive platform for AI developers but it would take creativity that is hard in a big company and acknowledging that in that effort they&#x27;re starting as a distant #3.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38646497" class="c"><input type="checkbox" id="c-38646497" checked=""/><div class="controls bullet"><span class="by">rafaelturk</span><span>|</span><a href="#38646012">prev</a><span>|</span><a href="#38650760">next</a><span>|</span><label class="collapse" for="c-38646497">[-]</label><label class="expand" for="c-38646497">[1 more]</label></div><br/><div class="children"><div class="content">Intel&#x27;s proposing the replacement of Nvidia CUDA across the entire industry is undeniably a bold move, yet it carries echoes of Steve Ballmer&#x27;s dismissive stance towards the iPhone in its early days. At first glance, it might appear as a epic statement, but it seems to be of pure fear.</div><br/></div></div><div id="38650760" class="c"><input type="checkbox" id="c-38650760" checked=""/><div class="controls bullet"><span class="by">rasz</span><span>|</span><a href="#38646497">prev</a><span>|</span><a href="#38645696">next</a><span>|</span><label class="collapse" for="c-38650760">[-]</label><label class="expand" for="c-38650760">[1 more]</label></div><br/><div class="children"><div class="content">The ship has sailed 10 years ago. I signed up for Uni GPGPU (probably called something like &#x27;Heterogeneous Parallel Programming&#x27;) course back then, and only during first lecture learned that all hardware and material was provided by NVIDIA free of charge. Result was pure CUDA class with no alternatives ever mentioned, just like the good old days of Word and Excel College courses.<p>Nvidia is really good at building&#x2F;buying moats:<p>PhysX where lack of Nvidia GPU used to default you into unoptimized FPU slow path on SSE capable CPUs <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;gaming&#x2F;2010&#x2F;07&#x2F;did-nvidia-cripple-its-cpu-gaming-physics-library-to-spite-intel&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;arstechnica.com&#x2F;gaming&#x2F;2010&#x2F;07&#x2F;did-nvidia-cripple-it...</a> <a href="https:&#x2F;&#x2F;www.realworldtech.com&#x2F;physx87&#x2F;3&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.realworldtech.com&#x2F;physx87&#x2F;3&#x2F;</a> &quot;For Nvidia, decreasing the baseline CPU performance by using x87 instructions and a single thread makes GPUs look better.&quot;<p>&quot;The Way It’s Meant To Be Played&quot; program paying of studios to directly cripple AMD. Ubisoft retracting DX10.1 patch is one example <a href="https:&#x2F;&#x2F;techreport.com&#x2F;news&#x2F;14707&#x2F;ubisoft-comments-on-assassins-creed-dx10-1-controversy-updated&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;techreport.com&#x2F;news&#x2F;14707&#x2F;ubisoft-comments-on-assass...</a><p>&quot;GameWorks&quot; program where Nvidia went one step further paying game studios to directly embed Nvidia crippleware libraries in games. <a href="https:&#x2F;&#x2F;techreport.com&#x2F;review&#x2F;21404&#x2F;crysis-2-tessellation-too-much-of-a-good-thing&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;techreport.com&#x2F;review&#x2F;21404&#x2F;crysis-2-tessellation-to...</a> <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;gaming&#x2F;2015&#x2F;05&#x2F;amd-says-nvidias-gameworks-completely-sabotaged-witcher-3-performance&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;arstechnica.com&#x2F;gaming&#x2F;2015&#x2F;05&#x2F;amd-says-nvidias-game...</a> <a href="https:&#x2F;&#x2F;wccftech.com&#x2F;fight-nvidias-gameworks-continues-amd-call-program-tragic&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;wccftech.com&#x2F;fight-nvidias-gameworks-continues-amd-c...</a></div><br/></div></div><div id="38645696" class="c"><input type="checkbox" id="c-38645696" checked=""/><div class="controls bullet"><span class="by">anshumankmr</span><span>|</span><a href="#38650760">prev</a><span>|</span><a href="#38651078">next</a><span>|</span><label class="collapse" for="c-38645696">[-]</label><label class="expand" for="c-38645696">[1 more]</label></div><br/><div class="children"><div class="content">cuda isn&#x27;t just the cuda cores but also the parallel programming library that Nvidia developed (which runs under the hood for stuff like Tensforflow and Pytorch)<p>I dabbled a bit with it in college for a course<p>replacing it ain&#x27;t gonna be easy (or even a desired thing for some)</div><br/></div></div><div id="38651078" class="c"><input type="checkbox" id="c-38651078" checked=""/><div class="controls bullet"><span class="by">throwaway5959</span><span>|</span><a href="#38645696">prev</a><span>|</span><label class="collapse" for="c-38651078">[-]</label><label class="expand" for="c-38651078">[1 more]</label></div><br/><div class="children"><div class="content">Dude the entire industry is motivated to move away from x86. CUDA works and will remain the unofficial standard until something better is provided, which isn’t going to come out of a company like Intel.</div><br/></div></div></div></div></div></div></div></body></html>