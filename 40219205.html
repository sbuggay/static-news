<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714640483936" as="style"/><link rel="stylesheet" href="styles.css?v=1714640483936"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/KindXiaoming/pykan">Kolmogorov-Arnold Networks</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>sumo43</span> | <span>111 comments</span></div><br/><div><div id="40222212" class="c"><input type="checkbox" id="c-40222212" checked=""/><div class="controls bullet"><span class="by">GistNoesis</span><span>|</span><a href="#40220240">next</a><span>|</span><label class="collapse" for="c-40222212">[-]</label><label class="expand" for="c-40222212">[8 more]</label></div><br/><div class="children"><div class="content">I quickly skimmed the paper, got inspired to simplify it, and created some Pytorch Layer :<p><a href="https:&#x2F;&#x2F;github.com&#x2F;GistNoesis&#x2F;FourierKAN&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;GistNoesis&#x2F;FourierKAN&#x2F;</a><p>The core is really just a few lines.<p>In the paper they use some spline interpolation to represent 1d function that they sum. Their code seemed aimed at smaller sizes. Instead I chose a different representation, aka fourier coefficients that are used to interpolate the functions of individual coordinates.<p>It should give an idea of Kolmogorov-Arnold networks representation power, it should probably converge easier than their spline version but spline version have less operations.<p>Of course, if my code doesn&#x27;t work, it doesn&#x27;t mean theirs doesn&#x27;t.<p>Feel free to experiment and publish paper if you want.</div><br/><div id="40233557" class="c"><input type="checkbox" id="c-40233557" checked=""/><div class="controls bullet"><span class="by">sevagh</span><span>|</span><a href="#40222212">parent</a><span>|</span><a href="#40228108">next</a><span>|</span><label class="collapse" for="c-40233557">[-]</label><label class="expand" for="c-40233557">[1 more]</label></div><br/><div class="children"><div class="content">Does your code work? Did you train it? Any graphs?<p>&gt;Of course, if my code doesn&#x27;t work, it doesn&#x27;t mean theirs doesn&#x27;t.<p>But, _does_ it work?</div><br/></div></div><div id="40228108" class="c"><input type="checkbox" id="c-40228108" checked=""/><div class="controls bullet"><span class="by">agnosticmantis</span><span>|</span><a href="#40222212">parent</a><span>|</span><a href="#40233557">prev</a><span>|</span><a href="#40224332">next</a><span>|</span><label class="collapse" for="c-40228108">[-]</label><label class="expand" for="c-40228108">[4 more]</label></div><br/><div class="children"><div class="content">How GPU-friendly is this class of models?</div><br/><div id="40231478" class="c"><input type="checkbox" id="c-40231478" checked=""/><div class="controls bullet"><span class="by">cloudhan</span><span>|</span><a href="#40222212">root</a><span>|</span><a href="#40228108">parent</a><span>|</span><a href="#40224332">next</a><span>|</span><label class="collapse" for="c-40231478">[-]</label><label class="expand" for="c-40231478">[3 more]</label></div><br/><div class="children"><div class="content">Very unfriendly.<p>The symbolic library (type of activations) requires a branching at the very core of the kernel. GPU will need to serialized on these operations warp-wise.<p>To optimize, you might want to do a scan operation beforehand and dispatch to activation funcs in a warp specialized way, this, however, makes the global memory read&#x2F;write non-coalesced.<p>You then may sort the input based on type of activations and store it in that order, this makes the gmem IO coalesced but requires gather and scatter as pre and post processing.</div><br/><div id="40233271" class="c"><input type="checkbox" id="c-40233271" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40222212">root</a><span>|</span><a href="#40231478">parent</a><span>|</span><a href="#40224332">next</a><span>|</span><label class="collapse" for="c-40233271">[-]</label><label class="expand" for="c-40233271">[2 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t it be faster to calculate every function type and then just multiply them by 0s or 1s to keep the active ones?</div><br/><div id="40233542" class="c"><input type="checkbox" id="c-40233542" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#40222212">root</a><span>|</span><a href="#40233271">parent</a><span>|</span><a href="#40224332">next</a><span>|</span><label class="collapse" for="c-40233542">[-]</label><label class="expand" for="c-40233542">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s pretty much how branching on GPUs already works.</div><br/></div></div></div></div></div></div></div></div><div id="40224332" class="c"><input type="checkbox" id="c-40224332" checked=""/><div class="controls bullet"><span class="by">itsthecourier</span><span>|</span><a href="#40222212">parent</a><span>|</span><a href="#40228108">prev</a><span>|</span><a href="#40220240">next</a><span>|</span><label class="collapse" for="c-40224332">[-]</label><label class="expand" for="c-40224332">[2 more]</label></div><br/><div class="children"><div class="content">you really are a pragmatic programmer, Noesis</div><br/><div id="40227458" class="c"><input type="checkbox" id="c-40227458" checked=""/><div class="controls bullet"><span class="by">GistNoesis</span><span>|</span><a href="#40222212">root</a><span>|</span><a href="#40224332">parent</a><span>|</span><a href="#40220240">next</a><span>|</span><label class="collapse" for="c-40227458">[-]</label><label class="expand" for="c-40227458">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. I like simple things.<p>Sums and products can get you surprisingly far.<p>Conceptually it&#x27;s simpler to think about and optimize. But you can also write it use einsum to do the sum product reductions (I&#x27;ve updated some comment to show how) to use less memory, but it&#x27;s more intimidating.<p>You can probably use KeOps library to fuse it further (einsum would get in the way).<p>But the best is probably a custom kernel. Once you have written it as sums and product, it&#x27;s just iterating. Like the core is 5 lines, but you have to add roughly 500 lines of low-level wrapping code to do cuda parallelisation, c++ to python, various types, manual derivatives. And then you have to add various checks so that there are no buffer overflows. And then you can optimize for special hardware operations like tensor cores. Making sure along the way that no numerical errors where introduced.<p>So there are a lot more efforts involved, and it&#x27;s usually only worth it if the layer is promising, but hopefully AI should be able to autocomplete these soon.</div><br/></div></div></div></div></div></div><div id="40220240" class="c"><input type="checkbox" id="c-40220240" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#40222212">prev</a><span>|</span><a href="#40231653">next</a><span>|</span><label class="collapse" for="c-40220240">[-]</label><label class="expand" for="c-40220240">[10 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve spent some time playing with their Jupyter notebooks. The most useful (to me, anyway) is their Example_3_classfication.ipynb ([1]).<p>It works as advertised with the parameters selected by the authors, but if we modified the network shape in the second half of the tutorial (Classification formulation) from (2, 2) to (2, 2, 2), it fails to generalize. The training loss gets down to 1e-9, while test loss stays around 3e-1. Getting to larger network sizes does not help either.<p>I would really like to see a bigger example with many more parameters and more data complexity and if it could be trained at all. MNIST would be a good start.<p>Update: I increased the training dataset size 100x, and that helps with the overfitting, but now I can&#x27;t get training loss below 1e-2. Still iterating on it; a GPU acceleration would really help - right now, my progress is limited by the speed of my CPU.<p>1. <a href="https:&#x2F;&#x2F;github.com&#x2F;KindXiaoming&#x2F;pykan&#x2F;blob&#x2F;master&#x2F;tutorials&#x2F;Example_3_classfication.ipynb">https:&#x2F;&#x2F;github.com&#x2F;KindXiaoming&#x2F;pykan&#x2F;blob&#x2F;master&#x2F;tutorials&#x2F;...</a></div><br/><div id="40220925" class="c"><input type="checkbox" id="c-40220925" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#40220240">parent</a><span>|</span><a href="#40233633">next</a><span>|</span><label class="collapse" for="c-40220925">[-]</label><label class="expand" for="c-40220925">[7 more]</label></div><br/><div class="children"><div class="content">Update2: got it to 100% training accuracy, 99% test accuracy with (2, 2, 2) shape.<p>Changes:<p>1. Increased the training set from 1000 to 100k samples. This solved overfitting.<p>2. In the dataset generation, slightly reduced noise (0.1 -&gt; 0.07) so that classes don&#x27;t overlap. With an overlap, naturally, it&#x27;s impossible to hit 100%.<p>3. Most important &amp; specific to KANs: train for 30 steps with grid=5 (5 segments for each activation function), then 30 steps with grid=10 (and initializing from the previous model), and then 30 steps with grid=20. This is idiomatic to KANs and covered in the Example_1_function_fitting.ipynb: <a href="https:&#x2F;&#x2F;github.com&#x2F;KindXiaoming&#x2F;pykan&#x2F;blob&#x2F;master&#x2F;tutorials&#x2F;Example_1_function_fitting.ipynb">https:&#x2F;&#x2F;github.com&#x2F;KindXiaoming&#x2F;pykan&#x2F;blob&#x2F;master&#x2F;tutorials&#x2F;...</a><p>Overall, my impressions are:<p>- it works!<p>- the reference implementation is very slow. A GPU implementation is dearly needed.<p>- it feels like it&#x27;s a bit too non-linear and training is not as stable as it&#x27;s with MLP + ReLU.<p>- Scaling is not guaranteed to work well. Really need to see if MNIST is possible to solve with this approach.<p>I will definitely keep an eye on this development.</div><br/><div id="40221296" class="c"><input type="checkbox" id="c-40221296" checked=""/><div class="controls bullet"><span class="by">thom</span><span>|</span><a href="#40220240">root</a><span>|</span><a href="#40220925">parent</a><span>|</span><a href="#40226091">next</a><span>|</span><label class="collapse" for="c-40221296">[-]</label><label class="expand" for="c-40221296">[5 more]</label></div><br/><div class="children"><div class="content">This makes me wonder what you could achieve if instead of iteratively growing the grid, or worrying about pruning or regularization, you governed network topology with some sort of evolutionary algorithm.</div><br/><div id="40221997" class="c"><input type="checkbox" id="c-40221997" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#40220240">root</a><span>|</span><a href="#40221296">parent</a><span>|</span><a href="#40221313">next</a><span>|</span><label class="collapse" for="c-40221997">[-]</label><label class="expand" for="c-40221997">[2 more]</label></div><br/><div class="children"><div class="content">You can do much better by growing an AST with memoization and non-linear regression. So much so, the EVO folks gave a best paper to a non-EVO, deterministic algorithm at their conference<p><a href="https:&#x2F;&#x2F;seminars.math.binghamton.edu&#x2F;ComboSem&#x2F;worm-chiu.pge_gecco2013.pdf" rel="nofollow">https:&#x2F;&#x2F;seminars.math.binghamton.edu&#x2F;ComboSem&#x2F;worm-chiu.pge_...</a> (author)</div><br/><div id="40232538" class="c"><input type="checkbox" id="c-40232538" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#40220240">root</a><span>|</span><a href="#40221997">parent</a><span>|</span><a href="#40221313">next</a><span>|</span><label class="collapse" for="c-40232538">[-]</label><label class="expand" for="c-40232538">[1 more]</label></div><br/><div class="children"><div class="content">Code for the curious: <a href="https:&#x2F;&#x2F;github.com&#x2F;verdverm&#x2F;pypge">https:&#x2F;&#x2F;github.com&#x2F;verdverm&#x2F;pypge</a></div><br/></div></div></div></div><div id="40221313" class="c"><input type="checkbox" id="c-40221313" checked=""/><div class="controls bullet"><span class="by">gxyt6gfy5t</span><span>|</span><a href="#40220240">root</a><span>|</span><a href="#40221296">parent</a><span>|</span><a href="#40221997">prev</a><span>|</span><a href="#40226091">next</a><span>|</span><label class="collapse" for="c-40221313">[-]</label><label class="expand" for="c-40221313">[2 more]</label></div><br/><div class="children"><div class="content">Believe there is a Google paper out there that tried that</div><br/><div id="40222018" class="c"><input type="checkbox" id="c-40222018" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#40220240">root</a><span>|</span><a href="#40221313">parent</a><span>|</span><a href="#40226091">next</a><span>|</span><label class="collapse" for="c-40222018">[-]</label><label class="expand" for="c-40222018">[1 more]</label></div><br/><div class="children"><div class="content">1000s, there is a whole field and set of conferences. You can find more by searching &quot;Genetic Programming&quot; or &quot;Symbolic Regression&quot;<p>KAN, with the library of variables and math operators, very much resembles this family of algos, problems, and limitations. The lowest hanging fruit they usually leave on the proverbial tree is that you can use fast regression techniques for the constants and coefficients. No need to leave it up to random perturbations or gradient descent. What you really need to figure out is the form or shape of the model, rather than leaving it up to the human (in KAN)</div><br/></div></div></div></div></div></div><div id="40226091" class="c"><input type="checkbox" id="c-40226091" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40220240">root</a><span>|</span><a href="#40220925">parent</a><span>|</span><a href="#40221296">prev</a><span>|</span><a href="#40233633">next</a><span>|</span><label class="collapse" for="c-40226091">[-]</label><label class="expand" for="c-40226091">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Increased the training set from 1000 to 100k samples. This solved overfitting.<p>Solved over fitting or created more? Even if your sets are completely disjoint with something like two moons the more data you have the lower the variance.</div><br/></div></div></div></div><div id="40233633" class="c"><input type="checkbox" id="c-40233633" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#40220240">parent</a><span>|</span><a href="#40220925">prev</a><span>|</span><a href="#40230067">next</a><span>|</span><label class="collapse" for="c-40233633">[-]</label><label class="expand" for="c-40233633">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible to run it on CUDA. One of their examples shows how. But I found it&#x27;s slower than on CPU. I&#x27;m actually not really surprised since running something on GPU is not a guarantee that it&#x27;s gonna be fast, especially when lots of branching is involved.<p>Unfortunately, I had to modify KAN.py and KANLayer.py to make it work as not all relevant tensor are put on the correct device. In some places the formatting even suggests that there was previously a device argument.</div><br/></div></div><div id="40230067" class="c"><input type="checkbox" id="c-40230067" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#40220240">parent</a><span>|</span><a href="#40233633">prev</a><span>|</span><a href="#40231653">next</a><span>|</span><label class="collapse" for="c-40230067">[-]</label><label class="expand" for="c-40230067">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I would really like to see a bigger example<p>This.   I don&#x27;t think toy examples are useful for modern ML techniques.    If you tested big ideas in ML (transformers, LSTM&#x27;s, ADAM) on a training dataset of 50 numbers trying to fit a y=sin(x) curve, I think you&#x27;d wrongly throw these ideas out.</div><br/></div></div></div></div><div id="40231653" class="c"><input type="checkbox" id="c-40231653" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40220240">prev</a><span>|</span><a href="#40223328">next</a><span>|</span><label class="collapse" for="c-40231653">[-]</label><label class="expand" for="c-40231653">[1 more]</label></div><br/><div class="children"><div class="content">There exists a Kolmogorov-Arnold inspired model in classical statistics called GAMs (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Generalized_additive_model" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Generalized_additive_model</a>), developed by Hastie and Tibshirani as an extension of GLMs (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Generalized_linear_model" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Generalized_linear_model</a>).<p>GLMs in turn generalize logistic-, linear and other popular regression models.<p>Neural GAMs with learned basis functions have already been proposed, so I&#x27;m a bit surprised that the prior art is not mentioned in this new paper. Previous applications  focused more on interpretability.</div><br/></div></div><div id="40223328" class="c"><input type="checkbox" id="c-40223328" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40231653">prev</a><span>|</span><a href="#40220706">next</a><span>|</span><label class="collapse" for="c-40223328">[-]</label><label class="expand" for="c-40223328">[24 more]</label></div><br/><div class="children"><div class="content">It&#x27;s so <i>refreshing</i> to come across new AI research different from the usual &quot;we modified a transformer in this and that way and got slightly better results on this and that benchmark.&quot; All those new papers proposing incremental improvements are important, but... everyone is getting a bit tired of them. Also,  anecdotal evidence and recent work suggest we&#x27;re starting to run into fundamental limits inherent to transformers, so we may well need new alternatives.[a]<p>The best thing about this new work is that it&#x27;s not an either&#x2F;or proposition. The proposed &quot;learnable spline interpolations as activation functions&quot; can be used <i>in conventional DNNs</i>, to improve their expressivity. Now we just have to test the stuff to see if it really works better.<p>Very nice. Thank you for sharing this work here!<p>---<p>[a] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40179232">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40179232</a></div><br/><div id="40225436" class="c"><input type="checkbox" id="c-40225436" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">parent</a><span>|</span><a href="#40225676">next</a><span>|</span><label class="collapse" for="c-40225436">[-]</label><label class="expand" for="c-40225436">[18 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a ton actually. Just they tend to go through extra rounds of review (or never make it...) and never make it to HN unless there&#x27;s special circumstances (this one is MIT and CIT). Unfortunately we&#x27;ve let PR become a very powerful force (it&#x27;s always been a thing, but seems more influential now). We can fight against this by up voting things like this and if you&#x27;re a reviewee, not focusing on sota (it&#x27;s clearly been gamed and clearly leading us in the wrong direction)</div><br/><div id="40228543" class="c"><input type="checkbox" id="c-40228543" checked=""/><div class="controls bullet"><span class="by">abhgh</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40225436">parent</a><span>|</span><a href="#40233572">next</a><span>|</span><label class="collapse" for="c-40228543">[-]</label><label class="expand" for="c-40228543">[13 more]</label></div><br/><div class="children"><div class="content">Yes seconding this. If you want a broad view of ML IMHO the best places to look at are conference proceedings. The typical review process is imperfect so that still doesn&#x27;t show you all the interesting work out there (which you mention), but it is still a start wrt diversity of research. I follow LLMs closely but then going through proceedings means I come across exciting research like these [1],[2],[3].<p>References:<p>[1] A grad.-based way to optimize axis-parallel and oblique decision trees: the <i>Tree Alternating Optimization (TAO)</i> algorithm <a href="https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2018&#x2F;file&#x2F;185c29dc24325934ee377cfda20e414c-Paper.pdf" rel="nofollow">https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2018&#x2F;file&#x2F;1...</a>. An extension was the <i>softmax tree</i> <a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2021.emnlp-main.838&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;2021.emnlp-main.838&#x2F;</a>.<p>[2] XAI explains models, but can you recommend corrective actions? <i>FACE: feasible and Actionable Counterfactual Explanations</i> <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1909.09369" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1909.09369</a>, <i>Algorithmic Recourse: from Counterfactual Explanations to Interventions</i> <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2002.06278" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2002.06278</a><p>[3] <i>OBOE: Collaborative Filtering for AutoML Model Selection</i> <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1808.03233" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1808.03233</a></div><br/><div id="40229071" class="c"><input type="checkbox" id="c-40229071" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40228543">parent</a><span>|</span><a href="#40233572">next</a><span>|</span><label class="collapse" for="c-40229071">[-]</label><label class="expand" for="c-40229071">[12 more]</label></div><br/><div class="children"><div class="content">Honestly, these days I just rely on arxiv. The conferences are so noisy that it is hard to really tell what&#x27;s useful and what&#x27;s crap. Twitter is a bit better but still a crap shoot. So as far as it seems to me, there&#x27;s no real good signal to use to differentiate. And what&#x27;s the point of journals&#x2F;conferences if not to provide some reasonable signal? If it is a slot machine, it is useless.<p>And I feel like we&#x27;re far too dismissive of instances we see where good papers get rejected. We&#x27;re too dismissive of the collusion rings. What am I putting in all this time to write and all this time to review (and be an emergency reviewer) if we aren&#x27;t going to take some basic steps forward? Fuck, I&#x27;ve saved a Welling paper from rejection from two reviewers who admitted to not knowing PDEs, and this was a workshop (should have been accepted into the main conference). I think review works for those already successful, who can pÌ¶aÌ¶yÌ¶ &quot;perform more experiments when requested&quot; their way out of review hell, but we&#x27;re ignoring a lot of good work simply for lack of mÌ¶oÌ¶nÌ¶eÌ¶yÌ¶ compute. It slows down our progress to reach AGI.</div><br/><div id="40232617" class="c"><input type="checkbox" id="c-40232617" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40229071">parent</a><span>|</span><a href="#40231514">next</a><span>|</span><label class="collapse" for="c-40232617">[-]</label><label class="expand" for="c-40232617">[2 more]</label></div><br/><div class="children"><div class="content">I agree with almost all you said except that Twitter is better than top conferences, and I take a contrarian view that reviewers slow down AGI with requests for additional experiments. Without going into specifics, which you can probably guess based on your background, too many ideas that work well, even optimally, at small scale fail horribly at large scale. Other ideas that work at super specialized settings donât transfer or donât generalize. The saving of two or three dimensions for exact symmetry operations is super important when you deal with handful of dimensions and is often irrelevant or slowing down training a lot when you already deal with tens of thousands of dimensions.  Correlations in huge multimodal datasets are way more complicated than most humans can grasp and we will not get to AGI before we can have a large enough group of people dealing with such data routinely.  It is very likely detrimental for our progress to AGI that we lack abundant hardware for academics and hobbyists to contribute frontier experiments, however we donât do anybody a favor by increasing the entropy of the publications in the huge ML conferences. This particular work listed in HN stands out despite lack of scaling and will probably make it in a top conference (perhaps with some additional background citations) but not everything that is merely novel should simply make it to ICLR or neurIPS or ICML, otherwise we could have a million papers in each in a few years from today and nobody would be the wiser.</div><br/><div id="40233595" class="c"><input type="checkbox" id="c-40233595" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40232617">parent</a><span>|</span><a href="#40231514">next</a><span>|</span><label class="collapse" for="c-40233595">[-]</label><label class="expand" for="c-40233595">[1 more]</label></div><br/><div class="children"><div class="content">&gt; too many ideas that work well, even optimally, at small scale fail horribly at large scale.<p>Not that I disagree, but I don&#x27;t think that&#x27;s a reason to not publish. There&#x27;s another way to rephrase what you&#x27;ve said<p><pre><code>  many ideas that work well at small scales do not trivially work at large scales
</code></pre>
But this is true for many works, even transformers. You don&#x27;t just scale by turning up model parameters and data. You can, but generally more things are going on. So why hold these works back because of that? There may be nuggets in there that may be of value and people may learn how to scale them. Just because they don&#x27;t scale (now or ever) doesn&#x27;t mean they aren&#x27;t of value (and let&#x27;s be honest, if they don&#x27;t scale, this is a real killer for the &quot;scale is all you need&quot; people)<p>&gt; Other ideas that work at super specialized settings donât transfer or donât generalize.<p>It is also hard to tell if these are hyper-parameter settings. Not that I disagree with you, but it is hard to tell.<p>&gt; Correlations in huge multimodal datasets are way more complicated than most humans can grasp and we will not get to AGI before we can have a large enough group of people dealing with such data routinely.<p>I&#x27;m not sure I understand your argument here. The people I know that work at scale often have the worst understanding of large data. Not understanding the differences between density in a normal distribution and a uniform. Thinking that LERPing in a normal yields representative data. Or cosine simularity and orthogonality. IME people that work at scale benefit from being able to throw compute at problems.<p>&gt; we donât do anybody a favor by increasing the entropy of the publications in the huge ML conferences<p>You and I have very different ideas as to what constitutes information gain. I would say a majority of people studying two models (LLMs and diffusion) results in lower gain, not more.<p>And as I&#x27;ve said above, I don&#x27;t care about novelty. It&#x27;s a meaningless term. (and I wish to god people would read the fucking conference reviewer guidelines as they constantly violate them when discussing novelty)</div><br/></div></div></div></div><div id="40231514" class="c"><input type="checkbox" id="c-40231514" checked=""/><div class="controls bullet"><span class="by">versteegen</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40229071">parent</a><span>|</span><a href="#40232617">prev</a><span>|</span><a href="#40229266">next</a><span>|</span><label class="collapse" for="c-40231514">[-]</label><label class="expand" for="c-40231514">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve saved a Welling paper from rejection from two reviewers who admitted to not knowing PDEs<p>Thank you for fighting the good fight.<p>This is why I love OpenReview, I can spot and ignore nonsensical reviewer criticisms and ratings and look for the insightful comments and rebuttals. Many reviewers do put in a lot of very valuable work reading and critiquing most of which would go to waste if not made public.</div><br/><div id="40233536" class="c"><input type="checkbox" id="c-40233536" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40231514">parent</a><span>|</span><a href="#40233506">next</a><span>|</span><label class="collapse" for="c-40233536">[-]</label><label class="expand" for="c-40233536">[1 more]</label></div><br/><div class="children"><div class="content">I like OR too and I wish we would just post to there instead. It has everything we need, and I see no value from the venues. No one wants to act in good faith and they have every incentive not to.<p>And I gotta say, I&#x27;m not going to put up a fight much longer. As soon as I get out of my PhD I intend to just post to OR.</div><br/></div></div></div></div><div id="40229266" class="c"><input type="checkbox" id="c-40229266" checked=""/><div class="controls bullet"><span class="by">abhgh</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40229071">parent</a><span>|</span><a href="#40231514">prev</a><span>|</span><a href="#40233572">next</a><span>|</span><label class="collapse" for="c-40229266">[-]</label><label class="expand" for="c-40229266">[6 more]</label></div><br/><div class="children"><div class="content">Yes arxiv is a good first source too. I mentioned conferences as a way to get exposed to diversity, but not necessarily (sadly) merit. It has been my experience as an author and reviewer both that review quality has plummeted over the years for the most part. As a reviewer I had to struggle with the ills of &quot;commission and omission&quot; both, i.e., (a) convince other reviewers to see an idea (from a trendy area such as in-context learning) as not novel (because it has been done before, even in the area of LLMs), and (b) see an idea as novel, which wouldn&#x27;t haven&#x27;t seemed so initially because some reviewers weren&#x27;t aware of the background or impact of anything non-LLM, or god forbid, non-DL. As an author this has personally affected me because I had to work on my PhD remotely, so I didn&#x27;t have access to a bunch of compute and I deliberately picked a non-DL area, and I had to pay the price for that in terms of multiple rejections, reviewer ghosting, journals not responding for years (yes, years).</div><br/><div id="40229546" class="c"><input type="checkbox" id="c-40229546" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40229266">parent</a><span>|</span><a href="#40233572">next</a><span>|</span><label class="collapse" for="c-40229546">[-]</label><label class="expand" for="c-40229546">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve stopped considering novelty at all. The only thing I now consider is if the precise technique has been done before. If not, well I&#x27;ve seen pretty small things change results dramatically. The pattern I&#x27;ve seen that scares me more is that when authors do find simple but effective changes, they end up convoluting the ideas because simplicity and clarity is often confused with novelty. And honestly, revisiting ideas is useful as our environments change. So I don&#x27;t want to discourage this type of work.<p>Personally, this has affected me as a late PhD student. Late in the literal sense as I&#x27;m not getting my work pushed out (even some SOTA stuff) because of factors like these and my department insists something is wrong with me but will not read my papers, the reviews, or suggest what I need to do besides &quot;publish more.&quot; (Literally told to me, &quot;try publishing 5 papers a year, one should get in.&quot;) You&#x27;ll laugh at this, I pushed a paper into a workshop and a major complaint was that I didn&#x27;t give enough background on StyleGAN because &quot;not everyone would be familiar with the architecture.&quot; (while I can understand the comment, 8 pages is not much room when you gotta show pictures on several datasets. My appendix was quite lengthy and included all requested information). We just used a GAN as a proxy because diffusion is much more expensive to train (most common complaints are &quot;not enough datasets&quot; and &quot;how&#x27;s it scale&quot;). I think this is the reason so many universities use pretrained networks instead of training things from scratch, which just railroads research.<p>(I also got a paper double desk rejected. First because it was &quot;already published.&quot; Took a 2 months for them to realize it was arxiv only. Then they fixed that and rejected again because &quot;didn&#x27;t cite relevant works&quot; with no mention of what those works were... I&#x27;ve obviously lost all faith in the review process)</div><br/><div id="40232939" class="c"><input type="checkbox" id="c-40232939" checked=""/><div class="controls bullet"><span class="by">abhgh</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40229546">parent</a><span>|</span><a href="#40232644">next</a><span>|</span><label class="collapse" for="c-40232939">[-]</label><label class="expand" for="c-40232939">[2 more]</label></div><br/><div class="children"><div class="content">Sorry to hear that. My experiences haven&#x27;t been very different. I really can&#x27;t tell if the current review process is the least bad among alternatives or is there something better (if so, what is it?).</div><br/><div id="40233523" class="c"><input type="checkbox" id="c-40233523" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40232939">parent</a><span>|</span><a href="#40232644">next</a><span>|</span><label class="collapse" for="c-40233523">[-]</label><label class="expand" for="c-40233523">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry to hear that too. I really wish there was something that could be done. I imagine a lot of graduate students are in complicated situations because of this.<p>As for alternatives: I don&#x27;t see why we don&#x27;t just push to OpenReview and call it a day. We can link our code, it has revisions, and people can comment and review. I don&#x27;t see what the advantage of having 1-3 referees who don&#x27;t want to read my paper and have no interest in it but have strong incentives to reject it is any meaningful signal of value. I&#x27;ll take arxiv over their opinions.</div><br/></div></div></div></div><div id="40232644" class="c"><input type="checkbox" id="c-40232644" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40229546">parent</a><span>|</span><a href="#40232939">prev</a><span>|</span><a href="#40233572">next</a><span>|</span><label class="collapse" for="c-40232644">[-]</label><label class="expand" for="c-40232644">[2 more]</label></div><br/><div class="children"><div class="content">Sorry to hear all this (after writing my other sibling comment).  Please donât lose faith in the review process. It is still useful. Until the AGI can be better reviewers, which is hopefully not too far in the future.</div><br/><div id="40233493" class="c"><input type="checkbox" id="c-40233493" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40232644">parent</a><span>|</span><a href="#40233572">next</a><span>|</span><label class="collapse" for="c-40233493">[-]</label><label class="expand" for="c-40233493">[1 more]</label></div><br/><div class="children"><div class="content">For me to regain faith in the review process I need to actually see some semblance of the review process working.<p>So far, instead, I&#x27;ve seen:<p><pre><code>  - Banning social media posting so that only big tech and collusion positing can happen to &quot;protect the little guy&quot;
  - Undoing the ban to lots of complaints
  - Instituting a no LLM policy with no teeth and no method to actually verify
  - Instituting a high school track to get those rich kids in sooner
</code></pre>
Until I see such changes like &quot;we&#x27;re going to focus on review quality&quot; I&#x27;m going to continue thinking it is a scam. They get paid by my tax dollars, by private companies, and I volunteer time, for what...? Something a LLM could have actually done better? I&#x27;m seeing great papers from big (and small) labs get turned down while terrible papers are getting accepted. Collusion rings go unpunished. And methods get more and more convoluted as everyone tries to game the system.<p>You think of all people, we, ML, would understand reward hacking. But until we admit it, we can&#x27;t solve it. And if we can&#x27;t solve it here, how the hell are we going to convince anyone we&#x27;re going to create safe AGI?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40233572" class="c"><input type="checkbox" id="c-40233572" checked=""/><div class="controls bullet"><span class="by">sevagh</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40225436">parent</a><span>|</span><a href="#40228543">prev</a><span>|</span><a href="#40228240">next</a><span>|</span><label class="collapse" for="c-40233572">[-]</label><label class="expand" for="c-40233572">[2 more]</label></div><br/><div class="children"><div class="content">For example, I find Spike Neural Networks to be cool, but until they reach SOTA, how can they displace conventional neural networks?</div><br/><div id="40233912" class="c"><input type="checkbox" id="c-40233912" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40233572">parent</a><span>|</span><a href="#40228240">next</a><span>|</span><label class="collapse" for="c-40233912">[-]</label><label class="expand" for="c-40233912">[1 more]</label></div><br/><div class="children"><div class="content">Compare how much time has been spent studying the two different architectures. Who knows if SNNs can displace other stuff, but I wouldn&#x27;t rely on SOTA for being the benchmark. Progress has to be made and it isn&#x27;t made in leaps and bounds. If you find them cool, study them more. Maybe you&#x27;ll stumble onto something. Maybe you&#x27;ll find an edge in a niche domain (and maybe you find that that edge can generalize more than you initially thought).<p>Stop worrying about displacing conventional networks and start worrying about understanding things. We chip away at this together, as a community. There&#x27;s a lot we need to learn and a lot that needs to be explored. Why tie anyone&#x27;s hands behind their backs?</div><br/></div></div></div></div><div id="40228240" class="c"><input type="checkbox" id="c-40228240" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40225436">parent</a><span>|</span><a href="#40233572">prev</a><span>|</span><a href="#40225676">next</a><span>|</span><label class="collapse" for="c-40228240">[-]</label><label class="expand" for="c-40228240">[2 more]</label></div><br/><div class="children"><div class="content">&gt; never make it to HN unless there&#x27;s special circumstances<p>Yes, I agree. The two most common patterns I&#x27;ve noticed in research that does show up on HN are: 1) It outright improves, or has the potential to improve, applications currently used in production by many HN readers. In other words, it&#x27;s not just navel-gazing. 2) The authors and&#x2F;or their organizations are well-known, as you suggest.</div><br/><div id="40228992" class="c"><input type="checkbox" id="c-40228992" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40228240">parent</a><span>|</span><a href="#40225676">next</a><span>|</span><label class="collapse" for="c-40228992">[-]</label><label class="expand" for="c-40228992">[1 more]</label></div><br/><div class="children"><div class="content">What bothers me the most is that comments will float to the top of a link that&#x27;s an arxiv paper or uni press where people will talk about how something is still in a prototype stage and not production yet&#x2F;has a ways to go to production. While this is fine, that&#x27;s also the context of works like these. But it is the same thing that I see in reviews. I&#x27;ve had works myself killed because reviewers treat the paper as a product rather than... you know... research.</div><br/></div></div></div></div></div></div><div id="40225676" class="c"><input type="checkbox" id="c-40225676" checked=""/><div class="controls bullet"><span class="by">beagle3</span><span>|</span><a href="#40223328">parent</a><span>|</span><a href="#40225436">prev</a><span>|</span><a href="#40223792">next</a><span>|</span><label class="collapse" for="c-40225676">[-]</label><label class="expand" for="c-40225676">[1 more]</label></div><br/><div class="children"><div class="content">I read a book on NNs by Robert Hecht Nielsen in 1989, during the NN hype of the time (I believe it was the 2nd hype cycle, the first beginning with Rosenblattâs original hardware perceptron and dying with Minsky and Pappertâs âPerceptronsâ manuscript a decade or two earlier).<p>Everything described was laughably basic by modern standards, but the motivation given in that book was the Kolmogorov representation theorem: a modest 3 layer networks with the right activation function <i>can</i> represent any continuous m-to-n function.<p>Most research back then focused on 3 layer networks, possibly for that reason. Sigmoid activation was king, and vanishing gradients the main issue. It took 2 decades until AlexNet brought NN research back from the AI winter of the 1990âs</div><br/></div></div><div id="40223792" class="c"><input type="checkbox" id="c-40223792" checked=""/><div class="controls bullet"><span class="by">glebnovikov</span><span>|</span><a href="#40223328">parent</a><span>|</span><a href="#40225676">prev</a><span>|</span><a href="#40220706">next</a><span>|</span><label class="collapse" for="c-40223792">[-]</label><label class="expand" for="c-40223792">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Everyone is getting tired of those papers.<p>This is science as is :)<p>95% percent will produce mediocre-to-nice improvements to what we already have so there were reserachers that eventually grow up and do something really exciting</div><br/><div id="40225450" class="c"><input type="checkbox" id="c-40225450" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40223328">root</a><span>|</span><a href="#40223792">parent</a><span>|</span><a href="#40225459">next</a><span>|</span><label class="collapse" for="c-40225450">[-]</label><label class="expand" for="c-40225450">[1 more]</label></div><br/><div class="children"><div class="content">Nothing wrong with incremental improvements. Giant leaps (almost always) only happen because of a lack of your niche domain expertise. And I mean niche niche</div><br/></div></div></div></div></div></div><div id="40220706" class="c"><input type="checkbox" id="c-40220706" checked=""/><div class="controls bullet"><span class="by">montebicyclelo</span><span>|</span><a href="#40223328">prev</a><span>|</span><a href="#40222655">next</a><span>|</span><label class="collapse" for="c-40220706">[-]</label><label class="expand" for="c-40220706">[2 more]</label></div><br/><div class="children"><div class="content">The success we&#x27;re seeing with neural networks is tightly coupled with the ability to scale - the algorithm itself works at scale (more layers), but it also scales well with hardware, (neural nets mostly consist of matrix multiplications, and GPUs have specialised matrix multiplication acceleration) - one of the most impactful neural network papers, AlexNet, was impactful because it showed that NNs could be put on the GPU, scaled and accelerated, to great effect.<p>It&#x27;s not clear from the paper how well this algorithm will scale, both in terms of the algorithm itself (does it still train well with more layers?), and ability to make use of hardware acceleration, (e.g. it&#x27;s not clear to me that the structure, with its per-weight activation functions, can make use of fast matmul acceleration).<p>It&#x27;s an interesting idea, that seems to work well and have nice properties on a smaller scale; but whether it&#x27;s a good architecture for imagenet, LLMs, etc. is not clear at this stage.</div><br/><div id="40220823" class="c"><input type="checkbox" id="c-40220823" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#40220706">parent</a><span>|</span><a href="#40222655">next</a><span>|</span><label class="collapse" for="c-40220823">[-]</label><label class="expand" for="c-40220823">[1 more]</label></div><br/><div class="children"><div class="content">&gt; with its per-weight activation functions<p>Sounds like something which could be approximated by a DCT (discrete cosine transform). JPEG compression does this, and there are hardware accelerations for it.<p>&gt; can make use of fast matmul acceleration<p>Maybe not, but matmul acceleration was done in hardware because it&#x27;s useful for some problems (graphics initially).<p>So if these per weight activations functions really work, people will be quick to figure out how to run them in hardware.</div><br/></div></div></div></div><div id="40222655" class="c"><input type="checkbox" id="c-40222655" checked=""/><div class="controls bullet"><span class="by">ubj</span><span>|</span><a href="#40220706">prev</a><span>|</span><a href="#40227122">next</a><span>|</span><label class="collapse" for="c-40222655">[-]</label><label class="expand" for="c-40222655">[2 more]</label></div><br/><div class="children"><div class="content">Very interesting! Kolmogorov neutral networks can represent discontinuous functions [1], but I&#x27;ve wondered about how practically applicable they are. This repo seems to show that they have some use after all.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00049" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00049</a></div><br/><div id="40231434" class="c"><input type="checkbox" id="c-40231434" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#40222655">parent</a><span>|</span><a href="#40227122">next</a><span>|</span><label class="collapse" for="c-40231434">[-]</label><label class="expand" for="c-40231434">[1 more]</label></div><br/><div class="children"><div class="content">Not for discontinuous functions, as your paper explains, we know that g exists for discontinuous bounded, but nothing to find it with.<p>&gt; A practical construction of g in cases with discontinuous bounded and un-
bounded functions is not yet known. For such cases Theorem 2.1 gives only a theoretical understanding of the representation problem. This is because for the representation of discontinuous bounded functions we have derived (2.1) from the fact that the range of the operator Zâ is the whole space of bounded functions B(Id). This fact directly gives us a formula (2.1) but does not tell how the bounded one-variable function g is attained. For the representation of unbounded functions we have used a linear extension of the functional F , existence of which is based on Zornâs lemma (see, e.g., [19, Ch. 3]). Application of Zornâs lemma provides no mechanism for practical construction of such an extension. Zornâs lemma helps to assert only its existence.<p>If you look at the OP post arxiv link, you will see they are using splines .<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.19756" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.19756</a><p>Still interesting and potentially useful, but not useful for discontinuous functions without further discoveries.<p>If I am wrong please provide a link, it is of great interest to me.</div><br/></div></div></div></div><div id="40227122" class="c"><input type="checkbox" id="c-40227122" checked=""/><div class="controls bullet"><span class="by">Lichtso</span><span>|</span><a href="#40222655">prev</a><span>|</span><a href="#40220203">next</a><span>|</span><label class="collapse" for="c-40227122">[-]</label><label class="expand" for="c-40227122">[8 more]</label></div><br/><div class="children"><div class="content">1. Interestingly the foundations of this approach and MLP were invented &#x2F; discovered around the same time about 66 years ago:<p>1957: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kolmogorov%E2%80%93Arnold_representation_theorem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kolmogorov%E2%80%93Arnold_repr...</a><p>1958: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multilayer_perceptron" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multilayer_perceptron</a><p>2. Another advantage of this approach is that it has only one class of parameters (the coefficients of the local activation functions) as opposed to MLP which has three classes of parameters (weights, biases, and the globally uniform activation function).<p>3. Everybody is talking transformers. I want to see diffusion models with this approach.</div><br/><div id="40229957" class="c"><input type="checkbox" id="c-40229957" checked=""/><div class="controls bullet"><span class="by">tripplyons</span><span>|</span><a href="#40227122">parent</a><span>|</span><a href="#40228022">next</a><span>|</span><label class="collapse" for="c-40229957">[-]</label><label class="expand" for="c-40229957">[1 more]</label></div><br/><div class="children"><div class="content">To your 3rd point, most diffusion models already use a transformer-based architecture (U-Net with self attention and cross attention, Vision Transformer, Diffusion Transformer, etc.).</div><br/></div></div><div id="40228022" class="c"><input type="checkbox" id="c-40228022" checked=""/><div class="controls bullet"><span class="by">trwm</span><span>|</span><a href="#40227122">parent</a><span>|</span><a href="#40229957">prev</a><span>|</span><a href="#40228750">next</a><span>|</span><label class="collapse" for="c-40228022">[-]</label><label class="expand" for="c-40228022">[3 more]</label></div><br/><div class="children"><div class="content">Biases are just weights on an always on input.<p>There isn&#x27;t much difference between weights of a linear sum and coefficients of a spline.</div><br/><div id="40228194" class="c"><input type="checkbox" id="c-40228194" checked=""/><div class="controls bullet"><span class="by">Lichtso</span><span>|</span><a href="#40227122">root</a><span>|</span><a href="#40228022">parent</a><span>|</span><a href="#40228750">next</a><span>|</span><label class="collapse" for="c-40228194">[-]</label><label class="expand" for="c-40228194">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Biases are just weights on an always on input.<p>Granted, however this approach does not require that constant-one input either.<p>&gt; There isn&#x27;t much difference between weights of a linear sum and coefficients of a function.<p>Yes, the trained function coefficients of this approach are the equivalent to the trained weights of MLP. Still this approach does not require the globally uniform activation function of MLP.</div><br/><div id="40228409" class="c"><input type="checkbox" id="c-40228409" checked=""/><div class="controls bullet"><span class="by">trwm</span><span>|</span><a href="#40227122">root</a><span>|</span><a href="#40228194">parent</a><span>|</span><a href="#40228750">next</a><span>|</span><label class="collapse" for="c-40228409">[-]</label><label class="expand" for="c-40228409">[1 more]</label></div><br/><div class="children"><div class="content">At this point this is a distinction without a difference.<p>The only question is if splines are more efficient than lines at describing general functions at the billion to trillion parameter count.</div><br/></div></div></div></div></div></div><div id="40228750" class="c"><input type="checkbox" id="c-40228750" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#40227122">parent</a><span>|</span><a href="#40228022">prev</a><span>|</span><a href="#40228431">next</a><span>|</span><label class="collapse" for="c-40228750">[-]</label><label class="expand" for="c-40228750">[1 more]</label></div><br/><div class="children"><div class="content">Yes, #2 is a difference. But what makes it an advantage?<p>One might argue this via parsimony (Occamâs razor). Is this your thinking? &#x2F; Anything else?</div><br/></div></div><div id="40228431" class="c"><input type="checkbox" id="c-40228431" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40227122">parent</a><span>|</span><a href="#40228750">prev</a><span>|</span><a href="#40220203">next</a><span>|</span><label class="collapse" for="c-40228431">[-]</label><label class="expand" for="c-40228431">[2 more]</label></div><br/><div class="children"><div class="content">I may be wrong but with midern llms biases arenât really used any more.</div><br/><div id="40229971" class="c"><input type="checkbox" id="c-40229971" checked=""/><div class="controls bullet"><span class="by">tripplyons</span><span>|</span><a href="#40227122">root</a><span>|</span><a href="#40228431">parent</a><span>|</span><a href="#40220203">next</a><span>|</span><label class="collapse" for="c-40229971">[-]</label><label class="expand" for="c-40229971">[1 more]</label></div><br/><div class="children"><div class="content">From what I remember, larger LLMs like PaLM don&#x27;t use biases for training stability, but smaller ones tend to still use them.</div><br/></div></div></div></div></div></div><div id="40220203" class="c"><input type="checkbox" id="c-40220203" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#40227122">prev</a><span>|</span><a href="#40231574">next</a><span>|</span><label class="collapse" for="c-40220203">[-]</label><label class="expand" for="c-40220203">[5 more]</label></div><br/><div class="children"><div class="content">From the preprint - 100 input dimensions is considered &quot;high&quot;, and most problems considered have 5 or fewer input dimensions. This is typical of physics-inspired settings I&#x27;ve seen considered in ML. The next step would be demonstrating them on MNIST, which, at 784 dimensions is tiny by modern standards.</div><br/><div id="40221265" class="c"><input type="checkbox" id="c-40221265" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#40220203">parent</a><span>|</span><a href="#40231574">next</a><span>|</span><label class="collapse" for="c-40221265">[-]</label><label class="expand" for="c-40221265">[4 more]</label></div><br/><div class="children"><div class="content">In actual business processes there are lots of ML problems with fewer than 100 input dimensions. But for most of them decision trees are still competitive with neural networks or even outperform them.</div><br/><div id="40222136" class="c"><input type="checkbox" id="c-40222136" checked=""/><div class="controls bullet"><span class="by">galangalalgol</span><span>|</span><a href="#40220203">root</a><span>|</span><a href="#40221265">parent</a><span>|</span><a href="#40228041">next</a><span>|</span><label class="collapse" for="c-40222136">[-]</label><label class="expand" for="c-40222136">[1 more]</label></div><br/><div class="children"><div class="content">The aid to explainability seems at least somewhat compelling. Understanding what a random forest did isn&#x27;t always easy. And if what you want isn&#x27;t the model but the closed form of what the model does, this could be quite useful. When those hundred input dimensions interact nonlinearly in a million ways thats nice. Or more likely I&#x27;d use it when I don&#x27;t want to find a pencil to derive the closed form of what I&#x27;m trying to do.</div><br/></div></div><div id="40228041" class="c"><input type="checkbox" id="c-40228041" checked=""/><div class="controls bullet"><span class="by">trwm</span><span>|</span><a href="#40220203">root</a><span>|</span><a href="#40221265">parent</a><span>|</span><a href="#40222136">prev</a><span>|</span><a href="#40231574">next</a><span>|</span><label class="collapse" for="c-40228041">[-]</label><label class="expand" for="c-40228041">[2 more]</label></div><br/><div class="children"><div class="content">Business processes don&#x27;t need deep learning in the first place. It is just there because hype.</div><br/><div id="40228879" class="c"><input type="checkbox" id="c-40228879" checked=""/><div class="controls bullet"><span class="by">JoshCole</span><span>|</span><a href="#40220203">root</a><span>|</span><a href="#40228041">parent</a><span>|</span><a href="#40231574">next</a><span>|</span><label class="collapse" for="c-40228879">[-]</label><label class="expand" for="c-40228879">[1 more]</label></div><br/><div class="children"><div class="content">Competent companies tend to put a lot of effort into building data analysis tools.  There will often be A&#x2F;B or QRT frameworks in place allowing deployment of two models, for example, the new deep learning model, and the old rule based system.  By using the results from these experiments in conjunction with typical offline and online evaluation metrics one can begin to make statements about the impact of model performance on revenue. Naturally model performance is tracked through many offline and online metrics.  So people can and do say things like &quot;if this model is x% more accurate then that translates to $y million dollars in monthly revenue&quot; with great confidence.<p>Lets call someone working at such a company Bob.<p>A restatement of your claim is that Bob decided to launch a model to live because of hype rather than because he could justify his promotion by pointing to the millions of dollars in increased revenue his switch produced.  Bob of course did not make his decision based on hype.  He made his decision because there were evaluation criteria in place for the launch.  He was literally not allowed to launch things that didn&#x27;t improve the system according to the evaluation criteria.  As Bob didn&#x27;t want to be fired for not doing anything at the company, he was forced to use a tool that worked to improve the evaluation according to the criteria that was specified.  So he used the tool that worked.  Hype might provide motivation to experiment, but it doesn&#x27;t justify a launch.<p>I say this as someone whose literally seen transitions from decision trees to deep learning models on &lt; 100 feature models which had multi-million dollar monthly revenue impacts.</div><br/></div></div></div></div></div></div></div></div><div id="40231574" class="c"><input type="checkbox" id="c-40231574" checked=""/><div class="controls bullet"><span class="by">cloudhan</span><span>|</span><a href="#40220203">prev</a><span>|</span><a href="#40222800">next</a><span>|</span><label class="collapse" for="c-40231574">[-]</label><label class="expand" for="c-40231574">[1 more]</label></div><br/><div class="children"><div class="content">This reminds me of Weight Agnostic Neural Networks <a href="https:&#x2F;&#x2F;weightagnostic.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;weightagnostic.github.io&#x2F;</a></div><br/></div></div><div id="40222800" class="c"><input type="checkbox" id="c-40222800" checked=""/><div class="controls bullet"><span class="by">reynoldss</span><span>|</span><a href="#40231574">prev</a><span>|</span><a href="#40219890">next</a><span>|</span><label class="collapse" for="c-40222800">[-]</label><label class="expand" for="c-40222800">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps a hasty comment but linear combinations of B-splines are yet another (higher-degree) B-spline. Isn&#x27;t this simply fitting high degree B-splines to functions?</div><br/><div id="40225550" class="c"><input type="checkbox" id="c-40225550" checked=""/><div class="controls bullet"><span class="by">Lichtso</span><span>|</span><a href="#40222800">parent</a><span>|</span><a href="#40219890">next</a><span>|</span><label class="collapse" for="c-40225550">[-]</label><label class="expand" for="c-40225550">[1 more]</label></div><br/><div class="children"><div class="content">That would be true for a single node &#x2F; single layer. But once the output of one layer is fed into the input of the next it is not just a linear combination of splines anymore.</div><br/></div></div></div></div><div id="40219890" class="c"><input type="checkbox" id="c-40219890" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#40222800">prev</a><span>|</span><a href="#40232131">next</a><span>|</span><label class="collapse" for="c-40219890">[-]</label><label class="expand" for="c-40219890">[4 more]</label></div><br/><div class="children"><div class="content">Feels like someone stuffed splines into decision trees.</div><br/><div id="40221182" class="c"><input type="checkbox" id="c-40221182" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#40219890">parent</a><span>|</span><a href="#40222089">next</a><span>|</span><label class="collapse" for="c-40221182">[-]</label><label class="expand" for="c-40221182">[1 more]</label></div><br/><div class="children"><div class="content">Itâs Monte Carlo all the way down</div><br/></div></div><div id="40222089" class="c"><input type="checkbox" id="c-40222089" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#40219890">parent</a><span>|</span><a href="#40221182">prev</a><span>|</span><a href="#40232131">next</a><span>|</span><label class="collapse" for="c-40222089">[-]</label><label class="expand" for="c-40222089">[2 more]</label></div><br/><div class="children"><div class="content">splines, yes.<p>I&#x27;m not seeing decision trees, though. Am I missing something?<p>&gt; &quot;KANsâ nodes simply sum incoming signals without applying any non-linearities.&quot; (page 2 of the PDF)</div><br/><div id="40225762" class="c"><input type="checkbox" id="c-40225762" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#40219890">root</a><span>|</span><a href="#40222089">parent</a><span>|</span><a href="#40232131">next</a><span>|</span><label class="collapse" for="c-40225762">[-]</label><label class="expand" for="c-40225762">[1 more]</label></div><br/><div class="children"><div class="content">I definitely think I&#x27;m projecting and maybe seeing things that aren&#x27;t there. If you replaced splines with linear weights, it kind of looks like a decision tree to me.</div><br/></div></div></div></div></div></div><div id="40232131" class="c"><input type="checkbox" id="c-40232131" checked=""/><div class="controls bullet"><span class="by">coderenegade</span><span>|</span><a href="#40219890">prev</a><span>|</span><a href="#40219640">next</a><span>|</span><label class="collapse" for="c-40232131">[-]</label><label class="expand" for="c-40232131">[2 more]</label></div><br/><div class="children"><div class="content">I was under the impression that graph neural nets already trained learnable functions on graph edges rather than nodes, albeit typically on a fully connected graph. Is there any comparison to just a basic GNN here?</div><br/><div id="40233625" class="c"><input type="checkbox" id="c-40233625" checked=""/><div class="controls bullet"><span class="by">vindex10</span><span>|</span><a href="#40232131">parent</a><span>|</span><a href="#40219640">next</a><span>|</span><label class="collapse" for="c-40233625">[-]</label><label class="expand" for="c-40233625">[1 more]</label></div><br/><div class="children"><div class="content">Bayesian networks learn probability functions, but looks like only their tabulated versions:<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bayesian_network#Graphical_model" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bayesian_network#Graphical_mod...</a><p>&gt;  Each node is associated with a probability function that takes, as input, a particular set of values for the node&#x27;s parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if m parent nodes represent m Boolean variables, then the probability function could be represented by a table of 2^m  entries, one entry for each of the 2^m possible parent combinations.</div><br/></div></div></div></div><div id="40219640" class="c"><input type="checkbox" id="c-40219640" checked=""/><div class="controls bullet"><span class="by">diwank</span><span>|</span><a href="#40232131">prev</a><span>|</span><a href="#40233139">next</a><span>|</span><label class="collapse" for="c-40219640">[-]</label><label class="expand" for="c-40219640">[5 more]</label></div><br/><div class="children"><div class="content">Itâd be really cool to see a transformer with the MLP layers swapped for KANs and then compare its scaling properties with vanilla transformers</div><br/><div id="40219995" class="c"><input type="checkbox" id="c-40219995" checked=""/><div class="controls bullet"><span class="by">gautam5669</span><span>|</span><a href="#40219640">parent</a><span>|</span><a href="#40224506">next</a><span>|</span><label class="collapse" for="c-40219995">[-]</label><label class="expand" for="c-40219995">[1 more]</label></div><br/><div class="children"><div class="content">This is the first thought came to my mind too.<p>Given its sparse, Will this be just replacement for MoE.</div><br/></div></div><div id="40224506" class="c"><input type="checkbox" id="c-40224506" checked=""/><div class="controls bullet"><span class="by">bart1ett</span><span>|</span><a href="#40219640">parent</a><span>|</span><a href="#40219995">prev</a><span>|</span><a href="#40220987">next</a><span>|</span><label class="collapse" for="c-40224506">[-]</label><label class="expand" for="c-40224506">[1 more]</label></div><br/><div class="children"><div class="content">After trying this out with the fourier implementation above, swapping MLP&#x2F;Attention Linear layers for KANs (all, or even a few layers) produces diverging loss. KANs don&#x27;t require normalization for good forward pass dynamics, but may be trickier to train in a deep net.</div><br/></div></div><div id="40220987" class="c"><input type="checkbox" id="c-40220987" checked=""/><div class="controls bullet"><span class="by">mp187</span><span>|</span><a href="#40219640">parent</a><span>|</span><a href="#40224506">prev</a><span>|</span><a href="#40233139">next</a><span>|</span><label class="collapse" for="c-40220987">[-]</label><label class="expand" for="c-40220987">[2 more]</label></div><br/><div class="children"><div class="content">Why was this your first thought? Is a limiting factor to transformers the MLP layer? I thought the bottleneck was in the renormalization part.</div><br/><div id="40222676" class="c"><input type="checkbox" id="c-40222676" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#40219640">root</a><span>|</span><a href="#40220987">parent</a><span>|</span><a href="#40233139">next</a><span>|</span><label class="collapse" for="c-40222676">[-]</label><label class="expand" for="c-40222676">[1 more]</label></div><br/><div class="children"><div class="content">At small input size, yes the MLP dominates compute.  At large input attention matters more</div><br/></div></div></div></div></div></div><div id="40233139" class="c"><input type="checkbox" id="c-40233139" checked=""/><div class="controls bullet"><span class="by">nu91</span><span>|</span><a href="#40219640">prev</a><span>|</span><a href="#40221216">next</a><span>|</span><label class="collapse" for="c-40233139">[-]</label><label class="expand" for="c-40233139">[1 more]</label></div><br/><div class="children"><div class="content">I am curious to know if this type of network can help with causal inference.</div><br/></div></div><div id="40221216" class="c"><input type="checkbox" id="c-40221216" checked=""/><div class="controls bullet"><span class="by">yobbo</span><span>|</span><a href="#40233139">prev</a><span>|</span><a href="#40228998">next</a><span>|</span><label class="collapse" for="c-40221216">[-]</label><label class="expand" for="c-40221216">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;kindxiaoming.github.io&#x2F;pykan&#x2F;intro.html" rel="nofollow">https:&#x2F;&#x2F;kindxiaoming.github.io&#x2F;pykan&#x2F;intro.html</a><p>At the end of this example, they recover the symbolic formula that generated their training set: exp(xâÂ² + sin(3.14xâ)).<p>It&#x27;s like a computation graph with a library of &quot;activation functions&quot; that is optimised, and then pruned. You can recover good symbolic formulas from the pruned graph.<p>Maybe not meaningful for MNIST.</div><br/><div id="40225764" class="c"><input type="checkbox" id="c-40225764" checked=""/><div class="controls bullet"><span class="by">beagle3</span><span>|</span><a href="#40221216">parent</a><span>|</span><a href="#40228998">next</a><span>|</span><label class="collapse" for="c-40225764">[-]</label><label class="expand" for="c-40225764">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if Breimanâs ACE (alternating conditional expectation) is useful as a building block here.<p>It will easily recover this formula, because it is separable under the log transformation (which ACE recovers as well).<p>But ACE doesnât work well on unseparable problems - not sure how well KAN will.</div><br/></div></div></div></div><div id="40228998" class="c"><input type="checkbox" id="c-40228998" checked=""/><div class="controls bullet"><span class="by">kevmo314</span><span>|</span><a href="#40221216">prev</a><span>|</span><a href="#40219548">next</a><span>|</span><label class="collapse" for="c-40228998">[-]</label><label class="expand" for="c-40228998">[1 more]</label></div><br/><div class="children"><div class="content">This seems very similar in concept to the finite element method. Nice to see patterns across fields like that.</div><br/></div></div><div id="40219548" class="c"><input type="checkbox" id="c-40219548" checked=""/><div class="controls bullet"><span class="by">Maro</span><span>|</span><a href="#40228998">prev</a><span>|</span><a href="#40228105">next</a><span>|</span><label class="collapse" for="c-40219548">[-]</label><label class="expand" for="c-40219548">[2 more]</label></div><br/><div class="children"><div class="content">Interesting!<p>Would this approach (with non-linear learning) still be able to utilize GPUs to speed up training?</div><br/><div id="40219635" class="c"><input type="checkbox" id="c-40219635" checked=""/><div class="controls bullet"><span class="by">diwank</span><span>|</span><a href="#40219548">parent</a><span>|</span><a href="#40228105">next</a><span>|</span><label class="collapse" for="c-40219635">[-]</label><label class="expand" for="c-40219635">[1 more]</label></div><br/><div class="children"><div class="content">Seconded. Iâm guessing you could create an implementation that is able to do that and then write optimised triton&#x2F;cuda kernels to accelerate them but need to investigate further</div><br/></div></div></div></div><div id="40228105" class="c"><input type="checkbox" id="c-40228105" checked=""/><div class="controls bullet"><span class="by">ComplexSystems</span><span>|</span><a href="#40219548">prev</a><span>|</span><a href="#40228033">next</a><span>|</span><label class="collapse" for="c-40228105">[-]</label><label class="expand" for="c-40228105">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting! Could existing MLP-style neural networks be put into this form?</div><br/></div></div><div id="40228033" class="c"><input type="checkbox" id="c-40228033" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#40228105">prev</a><span>|</span><a href="#40224455">next</a><span>|</span><label class="collapse" for="c-40228033">[-]</label><label class="expand" for="c-40228033">[1 more]</label></div><br/><div class="children"><div class="content">doesn&#x27;t KA representation require continuous univariate functions? do B-splines actually cover the space of all continuous functions?  wouldn&#x27;t... MLPs be better for the learnable activation functions?</div><br/></div></div><div id="40224455" class="c"><input type="checkbox" id="c-40224455" checked=""/><div class="controls bullet"><span class="by">SpaceManNabs</span><span>|</span><a href="#40228033">prev</a><span>|</span><a href="#40224081">next</a><span>|</span><label class="collapse" for="c-40224455">[-]</label><label class="expand" for="c-40224455">[3 more]</label></div><br/><div class="children"><div class="content">How does back propagation work now? Do these suffer from vanishing or exploding gradients?</div><br/><div id="40226155" class="c"><input type="checkbox" id="c-40226155" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#40224455">parent</a><span>|</span><a href="#40227168">next</a><span>|</span><label class="collapse" for="c-40226155">[-]</label><label class="expand" for="c-40226155">[1 more]</label></div><br/><div class="children"><div class="content">At page 6 it explains how they did back propagation <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.19756" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.19756</a> (and in page 2 it says that previous efforts to leverage Kolmogorov-Arnold representation failed to use backpropagation), so maybe using backpropagation to train multilayer networks with this architecture is their main contribution?<p>&gt; Unsurprisingly, the possibility of using Kolmogorov-Arnold representation theorem to build neuralnetworks has been studied [8, 9, 10, 11, 12, 13]. However, most work has stuck with the original depth-2 width-(2n + 1) representation, and did not have the chance to leverage more modern techniques (e.g., back propagation) to train the networks. Our contribution lies in generalizing the original Kolmogorov-Arnold representation to arbitrary widths and depths, revitalizing and contextualizing it in todayâs deep learning world, as well as using extensive empirical experiments to highlight its potential role as a foundation model for AI + Science due to its accuracy and interpretability.</div><br/></div></div><div id="40227168" class="c"><input type="checkbox" id="c-40227168" checked=""/><div class="controls bullet"><span class="by">goggy_googy</span><span>|</span><a href="#40224455">parent</a><span>|</span><a href="#40226155">prev</a><span>|</span><a href="#40224081">next</a><span>|</span><label class="collapse" for="c-40227168">[-]</label><label class="expand" for="c-40227168">[1 more]</label></div><br/><div class="children"><div class="content">No, the activations are a combination of the basis function and the spline function. It&#x27;s a little unclear to me still how the grid works, but it seems like this shouldn&#x27;t suffer anymore than a generic relu MLP.</div><br/></div></div></div></div><div id="40224081" class="c"><input type="checkbox" id="c-40224081" checked=""/><div class="controls bullet"><span class="by">keynesyoudigit</span><span>|</span><a href="#40224455">prev</a><span>|</span><a href="#40219546">next</a><span>|</span><label class="collapse" for="c-40224081">[-]</label><label class="expand" for="c-40224081">[3 more]</label></div><br/><div class="children"><div class="content">Eli5: why aren&#x27;t these more popular and broadly used?</div><br/><div id="40225055" class="c"><input type="checkbox" id="c-40225055" checked=""/><div class="controls bullet"><span class="by">OisinMoran</span><span>|</span><a href="#40224081">parent</a><span>|</span><a href="#40219546">next</a><span>|</span><label class="collapse" for="c-40225055">[-]</label><label class="expand" for="c-40225055">[2 more]</label></div><br/><div class="children"><div class="content">Because they have just been invented!</div><br/><div id="40227472" class="c"><input type="checkbox" id="c-40227472" checked=""/><div class="controls bullet"><span class="by">wbeckler</span><span>|</span><a href="#40224081">root</a><span>|</span><a href="#40225055">parent</a><span>|</span><a href="#40219546">next</a><span>|</span><label class="collapse" for="c-40227472">[-]</label><label class="expand" for="c-40227472">[1 more]</label></div><br/><div class="children"><div class="content">60 years ago</div><br/></div></div></div></div></div></div><div id="40219546" class="c"><input type="checkbox" id="c-40219546" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#40224081">prev</a><span>|</span><a href="#40220702">next</a><span>|</span><label class="collapse" for="c-40219546">[-]</label><label class="expand" for="c-40219546">[1 more]</label></div><br/><div class="children"><div class="content">Looks super interesting<p>I wonder how many more new architectures are going to be found in the next few years</div><br/></div></div><div id="40220702" class="c"><input type="checkbox" id="c-40220702" checked=""/><div class="controls bullet"><span class="by">arianvanp</span><span>|</span><a href="#40219546">prev</a><span>|</span><a href="#40219935">next</a><span>|</span><label class="collapse" for="c-40220702">[-]</label><label class="expand" for="c-40220702">[1 more]</label></div><br/><div class="children"><div class="content">This really reminds me of petrinets but an analog version? But instead of places and discrete tokens we have activation functions and signals. You can only trigger a transition if an activation function (place) has the right signal (tokens).</div><br/></div></div><div id="40219935" class="c"><input type="checkbox" id="c-40219935" checked=""/><div class="controls bullet"><span class="by">ALittleLight</span><span>|</span><a href="#40220702">prev</a><span>|</span><a href="#40225448">next</a><span>|</span><label class="collapse" for="c-40219935">[-]</label><label class="expand" for="c-40219935">[17 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t assess this, but I do worry that overnight some algorithmic advance will enhance LLMs by orders of magnitude and the next big model to get trained is suddenly 10,000x better than GPT-4 and nobody&#x27;s ready for it.</div><br/><div id="40220191" class="c"><input type="checkbox" id="c-40220191" checked=""/><div class="controls bullet"><span class="by">6mian</span><span>|</span><a href="#40219935">parent</a><span>|</span><a href="#40226834">next</a><span>|</span><label class="collapse" for="c-40220191">[-]</label><label class="expand" for="c-40220191">[11 more]</label></div><br/><div class="children"><div class="content">What to be worried about? Technical progress will happen, sometimes by sudden jumps. Some company will become a leader, competitors will catch up after a while.</div><br/><div id="40220310" class="c"><input type="checkbox" id="c-40220310" checked=""/><div class="controls bullet"><span class="by">cess11</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40220191">parent</a><span>|</span><a href="#40226834">next</a><span>|</span><label class="collapse" for="c-40220310">[-]</label><label class="expand" for="c-40220310">[10 more]</label></div><br/><div class="children"><div class="content">&quot;Technical progress&quot; has been destroying our habitat for centuries, causing lots of other species to go extinct. Pretty much the entire planet surface has been &#x27;technically progressed&#x27;, spreading plastics, climate change and whatnot over the entirety of it.<p>Are you assuming that this particular &quot;progress&quot; would be relatively innocent?</div><br/><div id="40220549" class="c"><input type="checkbox" id="c-40220549" checked=""/><div class="controls bullet"><span class="by">6mian</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40220310">parent</a><span>|</span><a href="#40221138">next</a><span>|</span><label class="collapse" for="c-40220549">[-]</label><label class="expand" for="c-40220549">[6 more]</label></div><br/><div class="children"><div class="content">On the other hand, the same &quot;technical progress&quot; (if we&#x27;re putting machine learning, deforestation, and mining in the same bag) gave you medicine, which turns many otherwise deadly diseases into inconveniences and allows you to work less than 12 hrs&#x2F;7 days per week to not die from hunger in a large portion of the world. A few hundred years ago, unless you were born into the lucky 0.01% of the ruling population, working from dawn to sunset was the norm for a lot more people than now.<p>I&#x27;m not assuming that something 10k x better than GPT-4 will be good or bad; I don&#x27;t know. I was just curious what exactly to be worried about. I think in the current state, LLMs are already advanced enough for bad uses like article generation for SEO, spam, scams, etc., and I wonder if an order of magnitude better model would allow for something worse.</div><br/><div id="40220778" class="c"><input type="checkbox" id="c-40220778" checked=""/><div class="controls bullet"><span class="by">cess11</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40220549">parent</a><span>|</span><a href="#40221138">next</a><span>|</span><label class="collapse" for="c-40220778">[-]</label><label class="expand" for="c-40220778">[5 more]</label></div><br/><div class="children"><div class="content">Where did you learn that history?<p>What do you mean by &quot;better&quot;?</div><br/><div id="40221152" class="c"><input type="checkbox" id="c-40221152" checked=""/><div class="controls bullet"><span class="by">6mian</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40220778">parent</a><span>|</span><a href="#40221138">next</a><span>|</span><label class="collapse" for="c-40221152">[-]</label><label class="expand" for="c-40221152">[4 more]</label></div><br/><div class="children"><div class="content">I had a European peasant in the 1600-1700s in mind when I wrote about the amount of work. During the season, they worked all day; off-season, they had &quot;free time&quot; that went into taking care of the household, inventory, etc., so it&#x27;s still work. Can&#x27;t quickly find a reliable source in English I could link, so I can be wrong here.<p>&quot;Better&quot; was referring to what OP wrote in the top comment. I guess 10x faster, 10x longer context, and 100x less prone to hallucinations would make a good &quot;10k x better&quot; than GPT-4.</div><br/><div id="40224048" class="c"><input type="checkbox" id="c-40224048" checked=""/><div class="controls bullet"><span class="by">cess11</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40221152">parent</a><span>|</span><a href="#40221138">next</a><span>|</span><label class="collapse" for="c-40224048">[-]</label><label class="expand" for="c-40224048">[3 more]</label></div><br/><div class="children"><div class="content">Sorry, I can&#x27;t fit that with what you wrote earlier:
&quot;12 hrs&#x2F;7 days per week to not die from hunger&quot;.<p>Those peasants payed taxes, i.e. some of their work was exploited by an army or a priest rather than hunger, and as you mention, they did not work &quot;12 hrs&#x2F;7 days per week&quot;.<p>Do you have a better example?</div><br/><div id="40232632" class="c"><input type="checkbox" id="c-40232632" checked=""/><div class="controls bullet"><span class="by">yazzku</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40224048">parent</a><span>|</span><a href="#40221138">next</a><span>|</span><label class="collapse" for="c-40232632">[-]</label><label class="expand" for="c-40232632">[2 more]</label></div><br/><div class="children"><div class="content">This entire line of argument is just pointless.</div><br/><div id="40232887" class="c"><input type="checkbox" id="c-40232887" checked=""/><div class="controls bullet"><span class="by">cess11</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40232632">parent</a><span>|</span><a href="#40221138">next</a><span>|</span><label class="collapse" for="c-40232887">[-]</label><label class="expand" for="c-40232887">[1 more]</label></div><br/><div class="children"><div class="content">You probably placed this wrong? I&#x27;m not driving a line of argument here.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40221138" class="c"><input type="checkbox" id="c-40221138" checked=""/><div class="controls bullet"><span class="by">vladms</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40220310">parent</a><span>|</span><a href="#40220549">prev</a><span>|</span><a href="#40226834">next</a><span>|</span><label class="collapse" for="c-40221138">[-]</label><label class="expand" for="c-40221138">[3 more]</label></div><br/><div class="children"><div class="content">Many species went extinct during Earth&#x27;s history. Evolution requires quite aggressive competition.<p>The way the habitat got destroyed by humans is stupid because it might put us in danger. You can call me &quot;speciesist&quot; but I do care more for humans rather than for a particular other specie.<p>So I think progress should be geared towards human species survival and if possible preventing other species extinction. Some of the current developments are a bit too much on the side of &quot;I don&#x27;t care about anyone&#x27;s survival&quot; (which is stupid and inefficient).</div><br/><div id="40222573" class="c"><input type="checkbox" id="c-40222573" checked=""/><div class="controls bullet"><span class="by">lccerina</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40221138">parent</a><span>|</span><a href="#40224076">next</a><span>|</span><label class="collapse" for="c-40222573">[-]</label><label class="expand" for="c-40222573">[1 more]</label></div><br/><div class="children"><div class="content">If other species die, we follow shortly. This anthropocentric view really ignore how much of our food chain exists because of other animals surviving despite human activities.</div><br/></div></div><div id="40224076" class="c"><input type="checkbox" id="c-40224076" checked=""/><div class="controls bullet"><span class="by">cess11</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40221138">parent</a><span>|</span><a href="#40222573">prev</a><span>|</span><a href="#40226834">next</a><span>|</span><label class="collapse" for="c-40224076">[-]</label><label class="expand" for="c-40224076">[1 more]</label></div><br/><div class="children"><div class="content">Evolution is the result of catastrophies and atrocities. You use the word as if it has positive connotations, which I find weird.<p>How do you come to the conclusion &quot;stupid&quot; rather than evil? Aren&#x27;t we very aware of the consequences of how we are currently organising human societies, and have been for a long time?</div><br/></div></div></div></div></div></div></div></div><div id="40226834" class="c"><input type="checkbox" id="c-40226834" checked=""/><div class="controls bullet"><span class="by">snewman</span><span>|</span><a href="#40219935">parent</a><span>|</span><a href="#40220191">prev</a><span>|</span><a href="#40220169">next</a><span>|</span><label class="collapse" for="c-40226834">[-]</label><label class="expand" for="c-40226834">[2 more]</label></div><br/><div class="children"><div class="content">I think this is unlikely. There has never (in the visible fossil record) been a mutation that suddenly made tigers an order of magnitude stronger and faster, or humans an order of magnitude more intelligent. It&#x27;s been a long time (if ever?) since chip transistor density made a multiple-order-of-magnitude leap. Any complex optimized system has many limiting factors and it&#x27;s unlikely that all of them would leap forward at once. The current generation of LLMs are not as complex or optimized as tigers or humans, but they&#x27;re far enough along that changing one thing is unlikely to result in a giant leap.<p>If and when something radically better comes along, say an alternative to back-propagation that is more like the way our brains learn, it will need a lot of scaling and refinement to catch up with the then-current LLM.</div><br/><div id="40227457" class="c"><input type="checkbox" id="c-40227457" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40226834">parent</a><span>|</span><a href="#40220169">next</a><span>|</span><label class="collapse" for="c-40227457">[-]</label><label class="expand" for="c-40227457">[1 more]</label></div><br/><div class="children"><div class="content">Comparing it to evolution and SNPs isn&#x27;t really a good analogy. Novel network architectures are much larger changes, maybe comparable to new organelles or metabolic pathways? And those have caused catastrophic changes. Evolution also operates on much longer time-scales due to its blind parallel search.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Oxygen_catastrophe" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Oxygen_catastrophe</a></div><br/></div></div></div></div><div id="40220169" class="c"><input type="checkbox" id="c-40220169" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#40219935">parent</a><span>|</span><a href="#40226834">prev</a><span>|</span><a href="#40225448">next</a><span>|</span><label class="collapse" for="c-40220169">[-]</label><label class="expand" for="c-40220169">[3 more]</label></div><br/><div class="children"><div class="content">&gt;some algorithmic advance will enhance LLMs by orders of magnitude<p>I would worry if I&#x27;d own Nvidia shares.</div><br/><div id="40220564" class="c"><input type="checkbox" id="c-40220564" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40220169">parent</a><span>|</span><a href="#40225448">next</a><span>|</span><label class="collapse" for="c-40220564">[-]</label><label class="expand" for="c-40220564">[2 more]</label></div><br/><div class="children"><div class="content">Actually, that would be fantastic for NVIDIA shares;<p>1. A new architecture would make all&#x2F;most of these upcoming Transformer accelerators obsolete =&gt; back to GPUs.<p>2. Higher performance LLMs on GPUs =&gt; we can speed up LLMs with 1T+ parameters. So, LLMs become more useful, so more of GPUs would be purchased.</div><br/><div id="40227096" class="c"><input type="checkbox" id="c-40227096" checked=""/><div class="controls bullet"><span class="by">mindcrime</span><span>|</span><a href="#40219935">root</a><span>|</span><a href="#40220564">parent</a><span>|</span><a href="#40225448">next</a><span>|</span><label class="collapse" for="c-40227096">[-]</label><label class="expand" for="c-40227096">[1 more]</label></div><br/><div class="children"><div class="content"><i>1. A new architecture would make all&#x2F;most of these upcoming Transformer accelerators obsolete =&gt; back to GPUs.</i><p>There&#x27;s no guarantee that that is what would happen. The right (or wrong, depending on your POV) algorithmic breakthrough might make GPU&#x27;s obsolete for AI, by making CPU&#x27;s (or analog computing units, or DSP&#x27;s, or &quot;other&quot;) the preferred platform to run AI.</div><br/></div></div></div></div></div></div></div></div><div id="40220742" class="c"><input type="checkbox" id="c-40220742" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#40225962">prev</a><span>|</span><label class="collapse" for="c-40220742">[-]</label><label class="expand" for="c-40220742">[1 more]</label></div><br/><div class="children"><div class="content">Looks very interesting, but my guess would be that this would run into the problem of exploding&#x2F;vanishing gradients at larger depths, just like TanH or sigmoid networks do.</div><br/></div></div></div></div></div></div></div></body></html>