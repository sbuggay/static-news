<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1731661284293" as="style"/><link rel="stylesheet" href="styles.css?v=1731661284293"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai">OpenAI, Google and Anthropic are struggling to build more advanced AI</a> <span class="domain">(<a href="https://www.bloomberg.com">www.bloomberg.com</a>)</span></div><div class="subtext"><span>lukebennett</span> | <span>353 comments</span></div><br/><div><div id="42126721" class="c"><input type="checkbox" id="c-42126721" checked=""/><div class="controls bullet"><span class="by">thebigspacefuck</span><span>|</span><a href="#42140045">next</a><span>|</span><label class="collapse" for="c-42126721">[-]</label><label class="expand" for="c-42126721">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.ph&#x2F;2024.11.13-100709&#x2F;https:&#x2F;&#x2F;www.bloomberg.com&#x2F;news&#x2F;articles&#x2F;2024-11-13&#x2F;openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai" rel="nofollow">https:&#x2F;&#x2F;archive.ph&#x2F;2024.11.13-100709&#x2F;https:&#x2F;&#x2F;www.bloomberg.c...</a></div><br/></div></div><div id="42140045" class="c"><input type="checkbox" id="c-42140045" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#42126721">prev</a><span>|</span><a href="#42139410">next</a><span>|</span><label class="collapse" for="c-42140045">[-]</label><label class="expand" for="c-42140045">[111 more]</label></div><br/><div class="children"><div class="content">Question for the group here: do we honestly feel like we&#x27;ve exhausted the options for delivering value on top of the current generation of LLMs?<p>I lead a team exploring cutting edge LLM applications and end-user features. It&#x27;s my intuition from experience that we have a LONG way to go.<p>GPT-4o &#x2F; Claude 3.5 are the go-to models for my team. Every combination of technical investment + LLMs yields a new list of potential applications.<p>For example, combining a human-moderated knowledge graph with an LLM with RAG allows you to build &quot;expert bots&quot; that understand your business context &#x2F; your codebase &#x2F; your specific processes and act almost human-like similar to a coworker in your team.<p>If you now give it some predictive &#x2F; simulation capability - eg: simulate the execution of a task or project like creating a github PR code change, and test against an expert bot above for code review, you can have LLMs create reasonable code changes, with automatic review &#x2F; iteration etc.<p>Similarly there are many more capabilities that you can ladder on and expose into LLMs to give you increasingly productive outputs from them.<p>Chasing after model improvements and &quot;GPT-5 will be PHD-level&quot; is moot imo. When did you hire a PHD coworker and they were productive on day-0 ? You need to onboard them with human expertise, and then give them execution space &#x2F; long-term memories etc to be productive.<p>Model vendors might struggle to build something more intelligent. But my point is that we already have so much intelligence and we don&#x27;t know what to do with that. There is a LOT you can do with high-schooler level intelligence at super-human scale.<p>Take a naive example. 200k context windows are now available. Most people, through ChatGPT, type out maybe 1500 tokens. That&#x27;s a huge amount of untapped capacity. No human is going to type out 200k of context. Hence why we need RAG, and additional forms of input (eg: simulation outcomes) to fully leverage that.</div><br/><div id="42140726" class="c"><input type="checkbox" id="c-42140726" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42145093">next</a><span>|</span><label class="collapse" for="c-42140726">[-]</label><label class="expand" for="c-42140726">[29 more]</label></div><br/><div class="children"><div class="content">&gt; potential applications
&gt; if you ...
&gt; for example ...<p>Yes there seems to be lots of potential. Yes we can brainstorm things that should work. Yes there is a lot of examples of incredible things in isolation. But it&#x27;s a little bit like those youtube videos showing amazing basketball shots in 1 try, when in reality lots of failed attempts happened beforehand. Except our users experience the failed attempts (LLM replies that are wrong, even when backed by RAG) and it&#x27;s incredibly hard to hide those from them.<p>Show me the things you &#x2F; your team has actually built that has decent retention and metrics concretely proving efficiency improvements.<p>LLMs are so hit and miss from query to query that if your users don&#x27;t have a sixth sense for a miss vs a hit, there may not be any efficiency improvement. It&#x27;s a really hard problem with LLM based tools.<p>There is so much hype right now and people showing cherry picked examples.</div><br/><div id="42140844" class="c"><input type="checkbox" id="c-42140844" checked=""/><div class="controls bullet"><span class="by">jihadjihad</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140726">parent</a><span>|</span><a href="#42143330">next</a><span>|</span><label class="collapse" for="c-42140844">[-]</label><label class="expand" for="c-42140844">[20 more]</label></div><br/><div class="children"><div class="content">&gt; Except our users experience the failed attempts (LLM replies that are wrong, even when backed by RAG) and it&#x27;s incredibly hard to hide those from them.<p>This has been my team&#x27;s experience (and frustration) as well, and has led us to look at using LLMs for classifying &#x2F; structuring, but not entrusting an LLM with making a decision based on things like a database schema or business logic.<p>I think the technology and tooling will get there, but the enormous amount of effort spent trying to get the system to &quot;do the right thing&quot; and the nondeterministic nature have really put us into a camp of &quot;let&#x27;s only allow the LLM to do things we know it is rock-solid at.&quot;</div><br/><div id="42141270" class="c"><input type="checkbox" id="c-42141270" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140844">parent</a><span>|</span><a href="#42141797">next</a><span>|</span><label class="collapse" for="c-42141270">[-]</label><label class="expand" for="c-42141270">[18 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;let&#x27;s only allow the LLM to do things we know it is rock-solid at.&quot;<p>Even this is insanely hard in my opinion.  The one thing that you would assume LLM to excel at is spelling and grammar checking for the English language, but even the top model (GPT-4o) can be insanely stupid&#x2F;unpredictable at times.  Take the following example from my tool:<p><a href="https:&#x2F;&#x2F;app.gitsense.com&#x2F;?doc=6c9bada92&amp;model=GPT-4o&amp;samples=5" rel="nofollow">https:&#x2F;&#x2F;app.gitsense.com&#x2F;?doc=6c9bada92&amp;model=GPT-4o&amp;samples...</a><p>5 models are asked if the sentence is correct and GPT-4o got it wrong all 5 times.  It keeps complaining that GitHub is spelled like Github, when it isn&#x27;t. Note, only 2 weeks ago, Claude 3.5 Sonnet did the same thing.<p>I do believe LLM is a game changer, but I&#x27;m not convinced it is designed to be public-facing.  I see LLM as a power tool for domain experts, and you have to assume whatever it spits out may be wrong, and your process should allow for it.<p>Edit:<p>I should add that I&#x27;m convinced that not one single model will rule them all. I believe there will be 4 or 5 models that everybody will use and each will be used to challenge one another for accuracy and confidence.</div><br/><div id="42142842" class="c"><input type="checkbox" id="c-42142842" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141270">parent</a><span>|</span><a href="#42141815">next</a><span>|</span><label class="collapse" for="c-42142842">[-]</label><label class="expand" for="c-42142842">[3 more]</label></div><br/><div class="children"><div class="content">I do contract work on fine-tuning efforts, and I can tell you that most humans aren&#x27;t designed to be public-facing either.<p>While LLMs do plenty of awful things, people make the most incredibly stupid mistakes too, and that is what LLMs needs to be benchmarked against. The problem is that most of the people evaluating LLMs are better educated than most and often smarter than most. When you see any quantity of prompts input by  a representative sample of LLM losers, you quickly lose all faith in humanity.<p>I&#x27;m not saying LLMs are good enough. They&#x27;re not. But we will increasingly find that there are large niches where LLMs are horrible and error prone yet still outperform <i>the people companies are prepared to pay to do the task</i>.<p>In other words, on one hand you&#x27;ll have domain experts becoming expert LLM-wranglers. On the other hand you&#x27;ll have public-facing LLMs eating away at tasks done by low paid labour where people can work around their stupid mistakes with process or just accepting the risk, same as they <i>currently do</i> with undertrained labor.</div><br/><div id="42143886" class="c"><input type="checkbox" id="c-42143886" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142842">parent</a><span>|</span><a href="#42143411">next</a><span>|</span><label class="collapse" for="c-42143886">[-]</label><label class="expand" for="c-42143886">[1 more]</label></div><br/><div class="children"><div class="content">I have  a side point here - There is a certain schizoid aspect to this argument that LLMs and humans make similar mistakes.<p>This means that on one hand firms are demanding RTO for culture and team work improvements.
While on the other they will be ok with a tool that makes unpredictable errors like humans, but can never be impacted by culture and team work.<p>These two ideas lie in odd juxtaposition to each other.</div><br/></div></div><div id="42143411" class="c"><input type="checkbox" id="c-42143411" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142842">parent</a><span>|</span><a href="#42143886">prev</a><span>|</span><a href="#42141815">next</a><span>|</span><label class="collapse" for="c-42143411">[-]</label><label class="expand" for="c-42143411">[1 more]</label></div><br/><div class="children"><div class="content">&gt; While LLMs do plenty of awful things, people make the most incredibly stupid mistakes too<p>I am 100% not blaming the LLM, but rather VCs and the media for believing the VCs. Once we get over the hype and people realize there isn&#x27;t a golden goose, the better off we will be.  Once we accept that LLM is not perfect and that it is not what we are being sold, I believe we will find a place for it that will make a huge impact.  Unfortunately for OpenAI and others, I don&#x27;t believe they will play as big of a role as they would like us to believe&#x2F;will.</div><br/></div></div></div></div><div id="42141815" class="c"><input type="checkbox" id="c-42141815" checked=""/><div class="controls bullet"><span class="by">SimianSci</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141270">parent</a><span>|</span><a href="#42142842">prev</a><span>|</span><a href="#42141930">next</a><span>|</span><label class="collapse" for="c-42141815">[-]</label><label class="expand" for="c-42141815">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;I see LLM as a power tool for domain experts, and you have to assume whatever it spits out may be wrong, and your process should allow for it.&quot;<p>this gets to the heart of it for me.
I think LLMs are an incredible tool, providing advanced augmentation on our already developed search capabilities. What advanced user doesnt want to have a colleague they can talk about their specific domain capacity with?<p>The problem comes from the hyperscaling ambitions of the players who were the first in this space. They quickly hyped up the technology beyond want it should have been.</div><br/></div></div><div id="42141930" class="c"><input type="checkbox" id="c-42141930" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141270">parent</a><span>|</span><a href="#42141815">prev</a><span>|</span><a href="#42144019">next</a><span>|</span><label class="collapse" for="c-42141930">[-]</label><label class="expand" for="c-42141930">[6 more]</label></div><br/><div class="children"><div class="content">Those Apple engineers stated in a very clear tone:<p>- every time a different result is produced.<p>- no reasoning capabilities were categorically determined.<p>So this is it. If you want LLM - brace for different results and if this is okay for your application (say it’s about speech or non-critical commands) then off you are.<p>Otherwise simply forget this approach, and particularly when you need reproducible discreet results.<p>I don’t think it gets any better than that and nothing so far implicated it will (with this particular approach to AGI or whatever the wet dream is)</div><br/><div id="42142797" class="c"><input type="checkbox" id="c-42142797" checked=""/><div class="controls bullet"><span class="by">rco8786</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141930">parent</a><span>|</span><a href="#42142010">next</a><span>|</span><label class="collapse" for="c-42142797">[-]</label><label class="expand" for="c-42142797">[1 more]</label></div><br/><div class="children"><div class="content">There’s another option here though. Human supervised tasks.<p>There’s a whole classification of tasks where a human can look at a body of work and determine whether it’s correct or not in far less time than it would take for them to produce the work directly.<p>As a random example, having LLMs write unit tests.</div><br/></div></div><div id="42142010" class="c"><input type="checkbox" id="c-42142010" checked=""/><div class="controls bullet"><span class="by">verteu</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141930">parent</a><span>|</span><a href="#42142797">prev</a><span>|</span><a href="#42144428">next</a><span>|</span><label class="collapse" for="c-42142010">[-]</label><label class="expand" for="c-42142010">[1 more]</label></div><br/><div class="children"><div class="content">(for reference: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.05229" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.05229</a> )</div><br/></div></div><div id="42144428" class="c"><input type="checkbox" id="c-42144428" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141930">parent</a><span>|</span><a href="#42142010">prev</a><span>|</span><a href="#42141956">next</a><span>|</span><label class="collapse" for="c-42144428">[-]</label><label class="expand" for="c-42144428">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if there is a moral hazard here? Apple doesn&#x27;t really have much in terms of AI, so maybe more likely to have an unfavorable view.</div><br/></div></div><div id="42141956" class="c"><input type="checkbox" id="c-42141956" checked=""/><div class="controls bullet"><span class="by">marcellus23</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141930">parent</a><span>|</span><a href="#42144428">prev</a><span>|</span><a href="#42144019">next</a><span>|</span><label class="collapse" for="c-42141956">[-]</label><label class="expand" for="c-42141956">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Those Apple engineers<p>Which Apple engineers? Yours is the only reference to the company in this comment section or in the article.</div><br/><div id="42142644" class="c"><input type="checkbox" id="c-42142644" checked=""/><div class="controls bullet"><span class="by">Agingcoder</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141956">parent</a><span>|</span><a href="#42144019">next</a><span>|</span><label class="collapse" for="c-42142644">[-]</label><label class="expand" for="c-42142644">[1 more]</label></div><br/><div class="children"><div class="content">See arxiv paper just above</div><br/></div></div></div></div></div></div><div id="42144019" class="c"><input type="checkbox" id="c-42144019" checked=""/><div class="controls bullet"><span class="by">solid_fuel</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141270">parent</a><span>|</span><a href="#42141930">prev</a><span>|</span><a href="#42142767">next</a><span>|</span><label class="collapse" for="c-42144019">[-]</label><label class="expand" for="c-42144019">[2 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t expect an LLM to be good at spell checking, actually.  The way they tokenize text before manipulating it makes them fairly bad at working with small sequences of letters.<p>I have had good luck using an LLM as a &quot;sanity checking&quot; layer for transcription output, though.  A simple prompt like &quot;is this paragraph coherent&quot; has proven to be a pretty decent way to check the accuracy of whisper transcriptions.</div><br/><div id="42144176" class="c"><input type="checkbox" id="c-42144176" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42144019">parent</a><span>|</span><a href="#42142767">next</a><span>|</span><label class="collapse" for="c-42144176">[-]</label><label class="expand" for="c-42144176">[1 more]</label></div><br/><div class="children"><div class="content">Yes this is a tokenization error.  If you rewrite the sentence as shown below:<p><a href="https:&#x2F;&#x2F;app.gitsense.com&#x2F;?doc=905f4a9af74c25f&amp;model=Claude+3.5+Sonnet&amp;samples=5" rel="nofollow">https:&#x2F;&#x2F;app.gitsense.com&#x2F;?doc=905f4a9af74c25f&amp;model=Claude+3...</a><p>Claude 3.5 Sonnet will now misinterpret &quot;GitHub as &quot;Github&quot;</div><br/></div></div></div></div><div id="42142767" class="c"><input type="checkbox" id="c-42142767" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141270">parent</a><span>|</span><a href="#42144019">prev</a><span>|</span><a href="#42142235">next</a><span>|</span><label class="collapse" for="c-42142767">[-]</label><label class="expand" for="c-42142767">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It keeps complaining that GitHub is spelled like Github, when it isn&#x27;t<p>I feel like this is unfair.  That&#x27;s the only thing it got wrong?  But we want it to pass all of our evals, even ones the perhaps a dictionary would be better at solving?  Or even an LLM augmented with a dictionary.</div><br/><div id="42143364" class="c"><input type="checkbox" id="c-42143364" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142767">parent</a><span>|</span><a href="#42143251">next</a><span>|</span><label class="collapse" for="c-42143364">[-]</label><label class="expand" for="c-42143364">[1 more]</label></div><br/><div class="children"><div class="content">My reason for commenting wasn&#x27;t to say LLM sucks, but rather we need to get over the honeymoon phase.  The fact the GPT-4o (one of the most advanced, if not the most advanced when it comes to non programming tasks) hallucinated &quot;Github&quot; as the input, should give us pause.<p>LLM has its place and it will forever change how we think about UX and other things, but we need to realize you really can&#x27;t create a public facing solution without significant safe guards, if you don&#x27;t want egg on your face.</div><br/></div></div><div id="42143251" class="c"><input type="checkbox" id="c-42143251" checked=""/><div class="controls bullet"><span class="by">MBCook</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142767">parent</a><span>|</span><a href="#42143364">prev</a><span>|</span><a href="#42142235">next</a><span>|</span><label class="collapse" for="c-42143251">[-]</label><label class="expand" for="c-42143251">[1 more]</label></div><br/><div class="children"><div class="content">Does it matter?<p>As a user I want it to be right, even if that contradicts the normal rules of the language.</div><br/></div></div></div></div><div id="42142235" class="c"><input type="checkbox" id="c-42142235" checked=""/><div class="controls bullet"><span class="by">malfist</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141270">parent</a><span>|</span><a href="#42142767">prev</a><span>|</span><a href="#42141797">next</a><span>|</span><label class="collapse" for="c-42142235">[-]</label><label class="expand" for="c-42142235">[2 more]</label></div><br/><div class="children"><div class="content">I was using an LLM to help spot passive voice in my documents and it told me &quot;We&#x27;re making&quot; was passive and I should change it to &quot;we are making&quot; to make it active.<p>Leaving aside &quot;we&#x27;re&quot; and &quot;we are&quot; are the same, it is absolutely active voice</div><br/><div id="42142538" class="c"><input type="checkbox" id="c-42142538" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142235">parent</a><span>|</span><a href="#42141797">next</a><span>|</span><label class="collapse" for="c-42142538">[-]</label><label class="expand" for="c-42142538">[1 more]</label></div><br/><div class="children"><div class="content">In the process of developing my tool, there are only 5 models (the first 5 in my models dropdown list) that I would use as a writing aide. If you used any other model, it really is a crapshoot with how bad they can be.</div><br/></div></div></div></div></div></div></div></div><div id="42143330" class="c"><input type="checkbox" id="c-42143330" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140726">parent</a><span>|</span><a href="#42140844">prev</a><span>|</span><a href="#42144477">next</a><span>|</span><label class="collapse" for="c-42143330">[-]</label><label class="expand" for="c-42143330">[4 more]</label></div><br/><div class="children"><div class="content">We have built quite a few highly useful LLM applications in my org that have reduced cost and improved outcomes in several domains - fraud detection, credit analysis, customer support, and a variety of other spaces. By in large they operate as cognitive load reducers but also handle through automation the vast majority of work since in our uses false negatives are not as bad as false positives but the majority of things we analyze are not true positives (99.999%+). As such the LLMs do a great job at anomaly detection and allow us to do tasks it would be prohibitively expensive with humans and their false positive and negative rates are considerably higher than LLMs.<p>I see these statements often here about “I’ve never seen an effective commercial use of LLMs,” which tells me you aren’t working with very creative and competent people in areas that are amenable to LLMs. In my professional network beyond where I work now I know at least a dozen people who have successful commercial applications of LLMs. They tend to be highly capable people able to build the end to end tool chains necessary (which is a huge gap) and understand how to compose LLMs in hierarchical agents with effective guard rails. Most ineffectual users of LLMs want them to be lazy buttons that obviate the need to think. They’re not - like any sufficiently powerful tool they require thought up front and are easy to use wrong. This will get better with time as patterns and tools emerge to get the most use out of them in a commercial setting. However the ability to process natural language and use an emergent (if not actual) abductive reasoning is absurdly powerful and was not practically possible 4 years ago - the assertion such an amazing capability in an information or decisioning system is not commercially practical is on the face absurd.</div><br/><div id="42143506" class="c"><input type="checkbox" id="c-42143506" checked=""/><div class="controls bullet"><span class="by">mhuffman</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42143330">parent</a><span>|</span><a href="#42143387">next</a><span>|</span><label class="collapse" for="c-42143506">[-]</label><label class="expand" for="c-42143506">[1 more]</label></div><br/><div class="children"><div class="content">&gt;We have built quite a few highly useful LLM applications in my org that have reduced cost and improved outcomes in several domains<p>Apps that use LLMs or apps made with LLMs? In either case can you share them?<p>&gt;which tells me you aren’t working with very creative and competent people<p>&gt; In my professional network beyond where I work now I know at least a dozen people who have successful commercial applications of LLMs.<p>Apps that use LLMs or apps made with LLMs? In either case can you share them?<p>No one doubts that you can integrate LLMs into an application workflow and get some benefits in certain cases. That has been what the excitement and promise was about all along. They have a demonstrated ability to wrangle, extract, and transform data (mostly correctly) and generate patterns from data and prompts (hit and miss, usually with a lot of human involvement). All of which can be powerful. But outside of textual or visual chatbots or CRUD apps, no one wants to &quot;put up or shut&quot; a solid example that the top management of an existing company would sign off on. Only stories about awesome examples they and their friends are working on ... which often turn out to be CRUD apps or textual or visual chatbots. One notable standout is generative image apps can be quite good in certain circumstances.<p>So, since you seem to have a real interest and actual examples of this, I am curious to see some that real companies would gamble that company on. And I don&#x27;t mean some quixotic startup, I mean a company making real money now with customers that is confident on that app to the point they are willing to risk big. Because that last part is what companies do with other (non LLM) apps. I also know that people aren&#x27;t perfect and wouldn&#x27;t expect an LLM to be, just want to make sure I am not missing something.</div><br/></div></div><div id="42143387" class="c"><input type="checkbox" id="c-42143387" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42143330">parent</a><span>|</span><a href="#42143506">prev</a><span>|</span><a href="#42143440">next</a><span>|</span><label class="collapse" for="c-42143387">[-]</label><label class="expand" for="c-42143387">[1 more]</label></div><br/><div class="children"><div class="content">&gt;compose LLMs in hierarchical agents with effective guard rails<p>Could you elaborate? Is this related to the &quot;teams of specialized LLMs&quot; concept I saw last year when Auto-GPT was getting a lot of hype?</div><br/></div></div><div id="42143440" class="c"><input type="checkbox" id="c-42143440" checked=""/><div class="controls bullet"><span class="by">topicseed</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42143330">parent</a><span>|</span><a href="#42143387">prev</a><span>|</span><a href="#42144477">next</a><span>|</span><label class="collapse" for="c-42143440">[-]</label><label class="expand" for="c-42143440">[1 more]</label></div><br/><div class="children"><div class="content">Do they build guardrails themselves or do they use an llm guardrail api like Modelmetry or Langwatch?</div><br/></div></div></div></div><div id="42144477" class="c"><input type="checkbox" id="c-42144477" checked=""/><div class="controls bullet"><span class="by">anilgulecha</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140726">parent</a><span>|</span><a href="#42143330">prev</a><span>|</span><a href="#42144363">next</a><span>|</span><label class="collapse" for="c-42144477">[-]</label><label class="expand" for="c-42144477">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are not hype.<p>In education at least, we&#x27;ve actively improved efficiency by ~25% across a large swath of educators (direct time saved) - agentic evaluators, tutors and doubt clarifiers. The wins in this industry are clear. And this is that much more time to spend with students.<p>I also know from 1-1 conversation with my peers in large-finance world, and there too the efficiency improvements on multiple fronts are similar.</div><br/></div></div><div id="42144363" class="c"><input type="checkbox" id="c-42144363" checked=""/><div class="controls bullet"><span class="by">physicsguy</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140726">parent</a><span>|</span><a href="#42144477">prev</a><span>|</span><a href="#42140963">next</a><span>|</span><label class="collapse" for="c-42144363">[-]</label><label class="expand" for="c-42144363">[1 more]</label></div><br/><div class="children"><div class="content">We’ve found that the text it generates in our RAG application is good, but it cocks up probably 5-10% of the time doing the inline references to the documents which users think is a bug and which we aren’t able to fix. This is static rather than interactively generated too</div><br/></div></div><div id="42140963" class="c"><input type="checkbox" id="c-42140963" checked=""/><div class="controls bullet"><span class="by">VeejayRampay</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140726">parent</a><span>|</span><a href="#42144363">prev</a><span>|</span><a href="#42141787">next</a><span>|</span><label class="collapse" for="c-42140963">[-]</label><label class="expand" for="c-42140963">[1 more]</label></div><br/><div class="children"><div class="content">really agree with this and I think it&#x27;s been the general experience: people wanting LLMs to be so great (or making money off them) kind of cherry picking examples that fit their narrative, which LLMs are good at because they produce amazing results some of the time like the deluxe broken clock that they are (they&#x27;re right many many times a day)<p>at the end of the day though, it&#x27;s not exactly reliable or particularly transformative when you get past the party tricks</div><br/></div></div><div id="42141787" class="c"><input type="checkbox" id="c-42141787" checked=""/><div class="controls bullet"><span class="by">archiepeach</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140726">parent</a><span>|</span><a href="#42140963">prev</a><span>|</span><a href="#42145093">next</a><span>|</span><label class="collapse" for="c-42141787">[-]</label><label class="expand" for="c-42141787">[1 more]</label></div><br/><div class="children"><div class="content">To be fair in the human-based teams I&#x27;ve worked with in startups I couldn&#x27;t show you products with decent retention.</div><br/></div></div></div></div><div id="42145093" class="c"><input type="checkbox" id="c-42145093" checked=""/><div class="controls bullet"><span class="by">_Algernon_</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140726">prev</a><span>|</span><a href="#42140135">next</a><span>|</span><label class="collapse" for="c-42145093">[-]</label><label class="expand" for="c-42145093">[1 more]</label></div><br/><div class="children"><div class="content">I have yet to see LLMs provide a positive net value in the first place. They have a long way to go to weigh up for its negative uses in the form of polluting the commons that is the web, propaganda use, etc.</div><br/></div></div><div id="42140135" class="c"><input type="checkbox" id="c-42140135" checked=""/><div class="controls bullet"><span class="by">crystal_revenge</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42145093">prev</a><span>|</span><a href="#42140679">next</a><span>|</span><label class="collapse" for="c-42140135">[-]</label><label class="expand" for="c-42140135">[23 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think we&#x27;ve even <i>started</i> to get the most value out of current gen LLMs. For starters very few people are even looking at sampling which is a major part of the model performance.<p>The theory behind these models so aggressively lags the engineering that I suspect there are many major improvements to be found just by understanding a bit more about <i>what these models are really doing</i> and making re-designs based on that.<p>I highly encourage anyone seriously interested in LLMs to start spending more time in the open model space where you can really take a look inside and play around with the internals. Even if you don&#x27;t have the resources for model training, I feel personally understanding sampling and other potential tweaks to the model (lots of neat work on uncertainty estimations, manipulating the initial embedding the prompts are assigned, intelligent backtracking, etc).<p>And from a practical side I&#x27;ve started to realize that many people have been holding on of building things waiting for &quot;that next big update&quot;, but there a so many small, annoying tasks that can be easily automated.</div><br/><div id="42141284" class="c"><input type="checkbox" id="c-42141284" checked=""/><div class="controls bullet"><span class="by">ppeetteerr</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140135">parent</a><span>|</span><a href="#42140256">next</a><span>|</span><label class="collapse" for="c-42141284">[-]</label><label class="expand" for="c-42141284">[5 more]</label></div><br/><div class="children"><div class="content">The reason people are holding out is that the current generation of models are still pretty poor in many areas. You can have it craft an email, or to review your email, but I wouldn&#x27;t trust an LLM with anything mission-critical. The accuracy of the generated output is too low be trusted in most practical applications.</div><br/><div id="42144223" class="c"><input type="checkbox" id="c-42144223" checked=""/><div class="controls bullet"><span class="by">jeswin</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141284">parent</a><span>|</span><a href="#42142016">next</a><span>|</span><label class="collapse" for="c-42144223">[-]</label><label class="expand" for="c-42144223">[1 more]</label></div><br/><div class="children"><div class="content">Google (even now) wasn&#x27;t absolutely accurate either. That didn&#x27;t stop it from becoming many billions worth.<p>&gt; You can have it craft an email, or to review your email, but I wouldn&#x27;t trust an LLM with anything mission-critical<p>My point is that an entire world lies between these two extremes.</div><br/></div></div><div id="42142016" class="c"><input type="checkbox" id="c-42142016" checked=""/><div class="controls bullet"><span class="by">saalweachter</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141284">parent</a><span>|</span><a href="#42144223">prev</a><span>|</span><a href="#42140256">next</a><span>|</span><label class="collapse" for="c-42142016">[-]</label><label class="expand" for="c-42142016">[3 more]</label></div><br/><div class="children"><div class="content">Any email you trust an LLM to write is one you probably don&#x27;t need to send.</div><br/><div id="42142611" class="c"><input type="checkbox" id="c-42142611" checked=""/><div class="controls bullet"><span class="by">Tagbert</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142016">parent</a><span>|</span><a href="#42140256">next</a><span>|</span><label class="collapse" for="c-42142611">[-]</label><label class="expand" for="c-42142611">[2 more]</label></div><br/><div class="children"><div class="content">Glib but the reality is that there are lots of cases where you can use an AI in writing but don’t need to entrust it with the whole job blindly.<p>I mostly use AIs in writing as a glorified grammar checker that sometimes suggests alternate phrasing. I do the initial writing and send it to an AI for review. If I like the suggestions I may incorporate some. Others I ignore.<p>The only times I use it to write is when I have something like a status report and I’m having a hard time phrasing things. Then I may write a series of bullet points and send that through an AI to flesh it out. Again, that is just the first stage and I take that and do editing to get what I want.<p>It’s just a tool, not a creator.</div><br/><div id="42144444" class="c"><input type="checkbox" id="c-42144444" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142611">parent</a><span>|</span><a href="#42140256">next</a><span>|</span><label class="collapse" for="c-42144444">[-]</label><label class="expand" for="c-42144444">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt;  have something like a status report and I’m having a hard time phrasing things<p>I believe the above suggested that this type of email likely doesn&#x27;t need to be sent. Is anyone really reading the status report? If they read it, what concrete decisions do they make based on it. We all get in this trap of doing what people ask of us but it often isn&#x27;t what shareholders and customers really care about.</div><br/></div></div></div></div></div></div></div></div><div id="42140256" class="c"><input type="checkbox" id="c-42140256" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140135">parent</a><span>|</span><a href="#42141284">prev</a><span>|</span><a href="#42141459">next</a><span>|</span><label class="collapse" for="c-42140256">[-]</label><label class="expand" for="c-42140256">[9 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve started to realize that many people have been holding on of building things waiting for &quot;that next big update&quot;<p>I’ve noticed this too — I’ve been calling it <i>intellectual deflation.</i> By analogy, why spend now when it may be cheaper in a month? Why do the work now, when it will be easier in a month?</div><br/><div id="42140326" class="c"><input type="checkbox" id="c-42140326" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140256">parent</a><span>|</span><a href="#42141311">next</a><span>|</span><label class="collapse" for="c-42140326">[-]</label><label class="expand" for="c-42140326">[7 more]</label></div><br/><div class="children"><div class="content">Why optimise software today, when tomorrow Intel will release CPU with 2x performance?</div><br/><div id="42140770" class="c"><input type="checkbox" id="c-42140770" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140326">parent</a><span>|</span><a href="#42144934">next</a><span>|</span><label class="collapse" for="c-42140770">[-]</label><label class="expand" for="c-42140770">[1 more]</label></div><br/><div class="children"><div class="content">Back when Intel regularly gave updates with 2x performance increases, people did make  decisions based on the performance doubling schedule.</div><br/></div></div><div id="42144934" class="c"><input type="checkbox" id="c-42144934" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140326">parent</a><span>|</span><a href="#42140770">prev</a><span>|</span><a href="#42140532">next</a><span>|</span><label class="collapse" for="c-42144934">[-]</label><label class="expand" for="c-42144934">[1 more]</label></div><br/><div class="children"><div class="content">If Intel could do that, they would be the one with a 3 trillion market cap. Not Nvidia.</div><br/></div></div><div id="42140532" class="c"><input type="checkbox" id="c-42140532" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140326">parent</a><span>|</span><a href="#42144934">prev</a><span>|</span><a href="#42140536">next</a><span>|</span><label class="collapse" for="c-42140532">[-]</label><label class="expand" for="c-42140532">[3 more]</label></div><br/><div class="children"><div class="content">Curiously, Moore&#x27;s law was predictable enough over decades that you could actually plan for the speed of next year&#x27;s hardware quite reliably.<p>For LLMs, we don&#x27;t even know how to reliably measure performance, much less plan for expected improvements.</div><br/><div id="42140676" class="c"><input type="checkbox" id="c-42140676" checked=""/><div class="controls bullet"><span class="by">mikeyouse</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140532">parent</a><span>|</span><a href="#42140536">next</a><span>|</span><label class="collapse" for="c-42140676">[-]</label><label class="expand" for="c-42140676">[2 more]</label></div><br/><div class="children"><div class="content">Moores law became less of a prediction and more of a product road map as time went on. It helped coordinate investment and expectations across the entire industry so everyone involved had the same understanding of timelines and benchmarks.  I fully believe more investment would’ve ‘bent the curve’ of the trend line but everyone was making money and there wasn’t a clear benefit to pushing the edge further.</div><br/><div id="42141026" class="c"><input type="checkbox" id="c-42141026" checked=""/><div class="controls bullet"><span class="by">epicureanideal</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140676">parent</a><span>|</span><a href="#42140536">next</a><span>|</span><label class="collapse" for="c-42141026">[-]</label><label class="expand" for="c-42141026">[1 more]</label></div><br/><div class="children"><div class="content">Or maybe it pushed everyone to innovate faster than they otherwise would’ve?  I’m very interested to hear your reasoning for the other case though, and I am not strongly committed to the opposite view, or either view for that matter.</div><br/></div></div></div></div></div></div><div id="42140536" class="c"><input type="checkbox" id="c-42140536" checked=""/><div class="controls bullet"><span class="by">throwing_away</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140326">parent</a><span>|</span><a href="#42140532">prev</a><span>|</span><a href="#42141311">next</a><span>|</span><label class="collapse" for="c-42140536">[-]</label><label class="expand" for="c-42140536">[1 more]</label></div><br/><div class="children"><div class="content">Call Nvidia, that sounds like a job for AI.</div><br/></div></div></div></div><div id="42141311" class="c"><input type="checkbox" id="c-42141311" checked=""/><div class="controls bullet"><span class="by">jkaptur</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140256">parent</a><span>|</span><a href="#42140326">prev</a><span>|</span><a href="#42141459">next</a><span>|</span><label class="collapse" for="c-42141311">[-]</label><label class="expand" for="c-42141311">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Osborne_effect" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Osborne_effect</a></div><br/></div></div></div></div><div id="42141459" class="c"><input type="checkbox" id="c-42141459" checked=""/><div class="controls bullet"><span class="by">creativenolo</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140135">parent</a><span>|</span><a href="#42140256">prev</a><span>|</span><a href="#42141522">next</a><span>|</span><label class="collapse" for="c-42141459">[-]</label><label class="expand" for="c-42141459">[2 more]</label></div><br/><div class="children"><div class="content">Great &amp; motivational comment. Any pointers on where to start playing with the internals and sampling?<p>Doesn’t need to be comprehensive, I just don’t know where to jump off from.</div><br/><div id="42144378" class="c"><input type="checkbox" id="c-42144378" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141459">parent</a><span>|</span><a href="#42141522">next</a><span>|</span><label class="collapse" for="c-42144378">[-]</label><label class="expand" for="c-42144378">[1 more]</label></div><br/><div class="children"><div class="content">Afaiu “sampling” here, it is controlled with (not only?) topk and temp parameters in e.g. “text generation web ui”. You may find these in other frontends probably too.<p>This ofc implies local models and that you have a decent cpu + min 64gb of ram to run above 7b-sized model.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?pipeline_tag=text-generation&amp;sort=likes" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;models?pipeline_tag=text-generation&amp;s...</a></div><br/></div></div></div></div><div id="42141522" class="c"><input type="checkbox" id="c-42141522" checked=""/><div class="controls bullet"><span class="by">creativenolo</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140135">parent</a><span>|</span><a href="#42141459">prev</a><span>|</span><a href="#42141760">next</a><span>|</span><label class="collapse" for="c-42141522">[-]</label><label class="expand" for="c-42141522">[1 more]</label></div><br/><div class="children"><div class="content">&gt; holding on of building things waiting for &quot;that next big update&quot;, but there a so many small, annoying tasks that can be easily automated.<p>Also we only hear &#x2F; see the examples that are meant to scale. Startups typically offer up something transformative, ready to soak up a segment of a market. And that’s hard with the current state of LLMs. When you try their offerings, it’s underwhelming. But there is richer, more nuanced hard to reach fruits that are extremely interesting - but it’s not clear where they’d scale in and of themselves.</div><br/></div></div><div id="42141760" class="c"><input type="checkbox" id="c-42141760" checked=""/><div class="controls bullet"><span class="by">kozikow</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140135">parent</a><span>|</span><a href="#42141522">prev</a><span>|</span><a href="#42143106">next</a><span>|</span><label class="collapse" for="c-42141760">[-]</label><label class="expand" for="c-42141760">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;The theory behind these models so aggressively lags the engineering&quot;<p>The problem is that 99% of theories are hard to scale.<p>I am not an expert, as I work adjacent to this field, but I see the inverse - dumbing down theory to increase parallelism&#x2F;scalability.</div><br/></div></div><div id="42143106" class="c"><input type="checkbox" id="c-42143106" checked=""/><div class="controls bullet"><span class="by">dr_kiszonka</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140135">parent</a><span>|</span><a href="#42141760">prev</a><span>|</span><a href="#42141433">next</a><span>|</span><label class="collapse" for="c-42143106">[-]</label><label class="expand" for="c-42143106">[1 more]</label></div><br/><div class="children"><div class="content">Would you have any suggestions on how to play with the internals of these open models? I don&#x27;t understand LLMs well, and would love to spend some experimenting, but I don&#x27;t know where to start. Are any projects more appropriate for neophytes?</div><br/></div></div><div id="42141433" class="c"><input type="checkbox" id="c-42141433" checked=""/><div class="controls bullet"><span class="by">deegles</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140135">parent</a><span>|</span><a href="#42143106">prev</a><span>|</span><a href="#42142470">next</a><span>|</span><label class="collapse" for="c-42141433">[-]</label><label class="expand" for="c-42141433">[2 more]</label></div><br/><div class="children"><div class="content">My big question is what is being done about hallucination? Without a solution it&#x27;s a giant footgun.</div><br/><div id="42143293" class="c"><input type="checkbox" id="c-42143293" checked=""/><div class="controls bullet"><span class="by">MBCook</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141433">parent</a><span>|</span><a href="#42142470">next</a><span>|</span><label class="collapse" for="c-42143293">[-]</label><label class="expand" for="c-42143293">[1 more]</label></div><br/><div class="children"><div class="content">CAN anything be done? At a very low level they’re basically designed to hallucinate text until it looks like something you’re asking for.<p>It works disturbingly well. But because it doesn’t have any actual intrinsic knowledge it has no way of knowing when it made a “good“ hallucination versus a “bad“ one.<p>I’m sure people are working at piling things on top to try and influence what gets generated or catch and move away from errors errors other layers spot… but how much effort and resources will be needed to make it “good enough“ that people don’t worry about this anymore.<p>In my mind the core problem is people are trying to use these for things they’re unsuitable for. Asking fact-based questions is asking for trouble. There isn’t much of a wrong answer if you wanted to generate a bedtime story or a bunch of test data that looks sort of like an example you give it.<p>If you ask it to find law cases on a specific point you’re going to raise a judge‘s ire, as many have already found.</div><br/></div></div></div></div><div id="42142470" class="c"><input type="checkbox" id="c-42142470" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140135">parent</a><span>|</span><a href="#42141433">prev</a><span>|</span><a href="#42140679">next</a><span>|</span><label class="collapse" for="c-42142470">[-]</label><label class="expand" for="c-42142470">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, I think the current crop of models is capable of solving a lot of non-first-world problems. Many of them don&#x27;t need full AGI to solve, especially if we start thinking outside Silicon Valley.</div><br/></div></div></div></div><div id="42140679" class="c"><input type="checkbox" id="c-42140679" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140135">prev</a><span>|</span><a href="#42143700">next</a><span>|</span><label class="collapse" for="c-42140679">[-]</label><label class="expand" for="c-42140679">[8 more]</label></div><br/><div class="children"><div class="content">No.<p>The scaling laws may be dead. Does this mean the end of LLM advances? Absolutely not.<p>There are many different ways to improve LLM capabilities. Everyone was mostly focused on the scaling laws because that worked extremely well (actually surprising most of the researchers).<p>But if you&#x27;re keeping an eye on the scientific papers coming out about AI, you&#x27;ve seen the astounding amount of research going on with some very good results, that&#x27;ll probably take at least several months to trickle down to production systems. Thousands of extremely bright people in AI labs all across the world are working on finding the next trick that boosts AI.<p>One random example is test-time compute: just give the AI more time to think. This is basically what O1 does. A recent research paper suggests using it is roughly equivalent to an order of magnitude more parameters, performance wise. (source for the curious: <a href="https:&#x2F;&#x2F;lnkd.in&#x2F;duDST65P">https:&#x2F;&#x2F;lnkd.in&#x2F;duDST65P</a>)<p>Another example that sounds bonkers but apparently works is quantization: reducing the precision of each parameter to 1.58 bits (ie only using values -1, 0, 1). This uses 10x less space for the same parameter count (compared to standard 16-bit format), and since AI operatons are actually memory limited, directly corresponds to 10x decrease in costs: <a href="https:&#x2F;&#x2F;lnkd.in&#x2F;ddvuzaYp">https:&#x2F;&#x2F;lnkd.in&#x2F;ddvuzaYp</a><p>(Quite apart from improvements like these, we shouldn&#x27;t forget that not all AIs are LLMs. There&#x27;s been tremendous advance in AI systems for image, audio and video generation, interpretation and munipulation and they also don&#x27;t show signs of stopping, and there&#x27;s possibility that a new or hybrid architecture for the textual AI might be developed).<p>AI winter is a long way off.</div><br/><div id="42140877" class="c"><input type="checkbox" id="c-42140877" checked=""/><div class="controls bullet"><span class="by">limaoscarjuliet</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140679">parent</a><span>|</span><a href="#42142955">next</a><span>|</span><label class="collapse" for="c-42140877">[-]</label><label class="expand" for="c-42140877">[6 more]</label></div><br/><div class="children"><div class="content">Scaling laws are not dead. The number of people predicting death of Moore&#x27;s law doubles every two years.<p>- Jim Keller<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;oIG9ztQw2Gc?si=oaK2zjSBxq2N-zj1&amp;t=451" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;oIG9ztQw2Gc?si=oaK2zjSBxq2N-zj1...</a></div><br/><div id="42141464" class="c"><input type="checkbox" id="c-42141464" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140877">parent</a><span>|</span><a href="#42142962">next</a><span>|</span><label class="collapse" for="c-42141464">[-]</label><label class="expand" for="c-42141464">[1 more]</label></div><br/><div class="children"><div class="content">There are way too many personal definitions of what &quot;Moore&#x27;s Law&quot; even is to have a discussion without deciding on a shared definition before hand.<p>But Goodhart&#x27;s law; &quot;When a measure becomes a target, it ceases to be a good measure&quot;<p>Directly applies here, Moore&#x27;s Law was used to set long term plans at semiconductor companies, and Moore didn&#x27;t have empirical evidence it was even going to continue.<p>If you say, arbitrarily decide CPU, or worse, single core performance as your measurement, it hasn&#x27;t held for well over a decade.<p>If you hold minimum feature size without regard to cost, it is still holding.<p>What you want to prove usually dictates what interpretation you make.<p>That said, the scaling law is still unknown, but you can game it as much as you want in similar ways.<p>GPT4 was already hinting at an asymptote on MMLU, but the question is if it is valid for real work etc...<p>Time will tell, but I am seeing far less optimism from my sources, but that is just anecdotal.</div><br/></div></div><div id="42142962" class="c"><input type="checkbox" id="c-42142962" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140877">parent</a><span>|</span><a href="#42141464">prev</a><span>|</span><a href="#42142955">next</a><span>|</span><label class="collapse" for="c-42142962">[-]</label><label class="expand" for="c-42142962">[4 more]</label></div><br/><div class="children"><div class="content">Moore&#x27;s law is doomed. At some point you start reaching the level of individual atoms. This is just physics.</div><br/><div id="42144700" class="c"><input type="checkbox" id="c-42144700" checked=""/><div class="controls bullet"><span class="by">Earw0rm</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142962">parent</a><span>|</span><a href="#42143378">next</a><span>|</span><label class="collapse" for="c-42144700">[-]</label><label class="expand" for="c-42144700">[1 more]</label></div><br/><div class="children"><div class="content">The limits are engineering, not physics. Atoms need not be a barrier for a long time if you can go fully 3D, for example, but manufacturing challenges, power and heat get in the way long before that.<p>Then you can go ultra-wide in terms of cores, dispatchers and vectors (essentially building bigger and bigger chips), but an algorithm which can&#x27;t exploit that will be little faster on today&#x27;s chips than on a 4790K from ten years ago.</div><br/></div></div><div id="42143378" class="c"><input type="checkbox" id="c-42143378" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142962">parent</a><span>|</span><a href="#42144700">prev</a><span>|</span><a href="#42142955">next</a><span>|</span><label class="collapse" for="c-42143378">[-]</label><label class="expand" for="c-42143378">[2 more]</label></div><br/><div class="children"><div class="content">You are missing the economic component.. it isn&#x27;t just how small can a transistor be.. it was really about how many transistors can you get for your money. So even when we reach terminal density, we probably haven&#x27;t reached terminal economics.</div><br/><div id="42144282" class="c"><input type="checkbox" id="c-42144282" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42143378">parent</a><span>|</span><a href="#42142955">next</a><span>|</span><label class="collapse" for="c-42144282">[-]</label><label class="expand" for="c-42144282">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t say we have currently reached a limit. I am saying that there obvious is a limit (at some point). So, scaling cannot go forever. This is a counterpoint to the dubious analogy with deep learning.</div><br/></div></div></div></div></div></div></div></div><div id="42142955" class="c"><input type="checkbox" id="c-42142955" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140679">parent</a><span>|</span><a href="#42140877">prev</a><span>|</span><a href="#42143700">next</a><span>|</span><label class="collapse" for="c-42142955">[-]</label><label class="expand" for="c-42142955">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Everyone was mostly focused on the scaling laws because that worked extremely well<p>Also because it was easy, and expense was not the first concern.</div><br/></div></div></div></div><div id="42143700" class="c"><input type="checkbox" id="c-42143700" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140679">prev</a><span>|</span><a href="#42140383">next</a><span>|</span><label class="collapse" for="c-42143700">[-]</label><label class="expand" for="c-42143700">[1 more]</label></div><br/><div class="children"><div class="content">&gt; combining a human-moderated knowledge graph with an LLM with RAG allows you to build &quot;expert bots&quot; that understand your business context &#x2F; your codebase &#x2F; your specific processes and act almost human-like similar to a coworker in your team<p>It&#x27;s been a while though, we&#x27;ve had great models now for a 18 months plus. Why are we still yet to see these type of applications rolling out on a wide scale?<p>My anecdotal experience is that almost universally, 90-95% type accuracy you get from them is just not good enough. Which is to say, having something be wrong 10% or even 5% of the time is worse than not having at all. At best, you need to implement applications like that in an entirely new paradigm that is designed to extract value without bearing the costs of the risks.<p>It doesn&#x27;t mean LLMs can&#x27;t be useful, but they are kind of stuck with applications that inherently mesh with human oversight (like programming etc). And the thing about those is that they don&#x27;t really scale, because the human oversight has to scale up with whatever the LLM is doing.</div><br/></div></div><div id="42140383" class="c"><input type="checkbox" id="c-42140383" checked=""/><div class="controls bullet"><span class="by">alangibson</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42143700">prev</a><span>|</span><a href="#42140886">next</a><span>|</span><label class="collapse" for="c-42140383">[-]</label><label class="expand" for="c-42140383">[17 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re playing a different game than the Sam Altmans of the world. The level of investment and profit they are looking for can only be justified by creating AGI.<p>The &gt; 100 P&#x2F;E ratios we are already seeing can&#x27;t be justified by something as quotidian as the exceptionally good productivity tools you&#x27;re talking about.</div><br/><div id="42140680" class="c"><input type="checkbox" id="c-42140680" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140383">parent</a><span>|</span><a href="#42140539">next</a><span>|</span><label class="collapse" for="c-42140680">[-]</label><label class="expand" for="c-42140680">[8 more]</label></div><br/><div class="children"><div class="content">&gt; <i>level of investment and profit they are looking for can only be justified by creating AGI</i><p>What are you basing this on?<p>IT outsourcing is a $500+ billion industry. If OpenAI <i>et al</i> can run even a 10% margin, that business alone justifies their valuation.</div><br/><div id="42141388" class="c"><input type="checkbox" id="c-42141388" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140680">parent</a><span>|</span><a href="#42144909">next</a><span>|</span><label class="collapse" for="c-42141388">[-]</label><label class="expand" for="c-42141388">[6 more]</label></div><br/><div class="children"><div class="content">It seems you are missing a lot of &quot;ifs&quot; in that hypothetical!<p>Nobody knows how things like coding assistants or other AI applications will pan out. Maybe it&#x27;ll be Oracle selling Meta-licenced solutions that gets the lion&#x27;s share of the market. Maybe custom coding goes away for many business applications as off-the-shelf solutions get smarter.<p>A future where all that AI (or some hypothetical AGI) changes is work being done by humans to the same work being done by machines seems way too linear.</div><br/><div id="42141592" class="c"><input type="checkbox" id="c-42141592" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141388">parent</a><span>|</span><a href="#42144909">next</a><span>|</span><label class="collapse" for="c-42141592">[-]</label><label class="expand" for="c-42141592">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>you are missing a lot of &quot;ifs&quot; in that hypothetical</i><p>The big one being I&#x27;m not assuming AGI. Low-level coding tasks, the kind frequently outsourced, are within the realm of being competitive with offshoring with known methods. My point is we don&#x27;t need to assume AGI for these valuations to make sense.</div><br/><div id="42141668" class="c"><input type="checkbox" id="c-42141668" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141592">parent</a><span>|</span><a href="#42144909">next</a><span>|</span><label class="collapse" for="c-42141668">[-]</label><label class="expand" for="c-42141668">[4 more]</label></div><br/><div class="children"><div class="content">Current AI coding assistants are best at writing functions or adding minor features to an existing code base. They are not agentic systems that can develop an entire solution from scratch given a specification, which in my experience is more typcical of the work that is being outsourced. AI is a tool, whose full-cycle productivity benefit seems questionable. It is not a replacement for a human.</div><br/><div id="42141727" class="c"><input type="checkbox" id="c-42141727" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141668">parent</a><span>|</span><a href="#42141740">next</a><span>|</span><label class="collapse" for="c-42141727">[-]</label><label class="expand" for="c-42141727">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>they are not agentic systems that can develop an entire solution from scratch given a specification, which in my experience is more typcical of the work that is being outsourced</i><p>If there is one domain where we&#x27;re seeing tangible progress from AI, it&#x27;s in working towards this goal. Difficult projects aren&#x27;t in scope. But most tech, <i>especially</i> most tech branded IT, is not difficult. Everyone doesn&#x27;t need an inventory or customer-complaint system designed from scratch. Current AI is good at cutting through that cruft.</div><br/><div id="42142846" class="c"><input type="checkbox" id="c-42142846" checked=""/><div class="controls bullet"><span class="by">ehnto</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141727">parent</a><span>|</span><a href="#42141740">next</a><span>|</span><label class="collapse" for="c-42142846">[-]</label><label class="expand" for="c-42142846">[1 more]</label></div><br/><div class="children"><div class="content">There have been off the shelf solutions for so many common software use cases, for decades now. I think the reason we still see so much custom software is that the devil is always in the details, and strict details are not an LLMs strong suit.<p>LLMs are in my opinion hamstrung at the starting gate in regards to replacing software teams, as they would need to be able to understand complex business requirements perfectly, which we know they cannot. Humans can&#x27;t either. It takes a business requirements&#x2F;integration logic&#x2F;code generation pipeline and I think the industry is focused on code generation and not that integration step.<p>I think there needs to be a re-imaging of how software is built by and for interaction with AI if it were to ever take over from human software teams, rather than trying to get AI to reflect what humans do.</div><br/></div></div></div></div><div id="42141740" class="c"><input type="checkbox" id="c-42141740" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141668">parent</a><span>|</span><a href="#42141727">prev</a><span>|</span><a href="#42144909">next</a><span>|</span><label class="collapse" for="c-42141740">[-]</label><label class="expand" for="c-42141740">[1 more]</label></div><br/><div class="children"><div class="content">There are a number of agentic systems that can develop more complex solutions. Just a few off the top of my head: Pythagora, Devin, OpenHands, Fume, Tusk, Replit, Codebuff, Vly. I&#x27;m sure I&#x27;ve missed a bunch.<p>Are they good enough to replace a human yet? Questionable[0], but they <i>are</i> improving.<p>[0] You wouldn&#x27;t believe how low the outsourcing contractors&#x27; quality can go. Easily surpassed by current AI systems :) That&#x27;s a very low bar tho.</div><br/></div></div></div></div></div></div></div></div><div id="42144909" class="c"><input type="checkbox" id="c-42144909" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140680">parent</a><span>|</span><a href="#42141388">prev</a><span>|</span><a href="#42140539">next</a><span>|</span><label class="collapse" for="c-42144909">[-]</label><label class="expand" for="c-42144909">[1 more]</label></div><br/><div class="children"><div class="content">if the AI business is a bit more mundane than Altman thinks and there&#x27;s diminishing returns the market is going to be even more commodified than it already is and you&#x27;re not going to make any margins or somehow own the entire market. That&#x27;s already the case, Anthropic works about as well, there&#x27;s other companies a few months behind, open source is like a year behind.<p>That&#x27;s literally Zucc&#x27;s entire play, in 5 years this stuff is going to be so abundant you&#x27;ll get access to good enough models for pennies and he&#x27;ll win because he can slap ads on it, and openAI sits there on its gargantuan research costs.</div><br/></div></div></div></div><div id="42140539" class="c"><input type="checkbox" id="c-42140539" checked=""/><div class="controls bullet"><span class="by">gizajob</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140383">parent</a><span>|</span><a href="#42140680">prev</a><span>|</span><a href="#42140666">next</a><span>|</span><label class="collapse" for="c-42140539">[-]</label><label class="expand" for="c-42140539">[7 more]</label></div><br/><div class="children"><div class="content">Yeah I keep thinking this - how is Nvidia worth $3.5Trillion for making code autocomplete for coders</div><br/><div id="42140592" class="c"><input type="checkbox" id="c-42140592" checked=""/><div class="controls bullet"><span class="by">drawnwren</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140539">parent</a><span>|</span><a href="#42140666">next</a><span>|</span><label class="collapse" for="c-42140592">[-]</label><label class="expand" for="c-42140592">[6 more]</label></div><br/><div class="children"><div class="content">Nvidia was not the best example. They get to moon in the case that any AI exponential hits. Most others have less of a wide probability distribution.</div><br/><div id="42141440" class="c"><input type="checkbox" id="c-42141440" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140592">parent</a><span>|</span><a href="#42140833">next</a><span>|</span><label class="collapse" for="c-42141440">[-]</label><label class="expand" for="c-42141440">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure about that. NVIDIA seems to stay in a dominant position as long as the race to AI remains intact, but the path to it seems unsure. They are selling a general purpose AI-accelerator that supports the unknown path.<p>Once massively useful AI has been achieved, or it&#x27;s been determined that LLMs are it, then it becomes a race to the bottom as GOOG&#x2F;MSFT&#x2F;AMZN&#x2F;META&#x2F;etc design&#x2F;deploy more specialized accelerators to deliver this final form solution as cheaply as possible.</div><br/></div></div><div id="42140833" class="c"><input type="checkbox" id="c-42140833" checked=""/><div class="controls bullet"><span class="by">BeefWellington</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140592">parent</a><span>|</span><a href="#42141440">prev</a><span>|</span><a href="#42140666">next</a><span>|</span><label class="collapse" for="c-42140833">[-]</label><label class="expand" for="c-42140833">[4 more]</label></div><br/><div class="children"><div class="content">Yeah they&#x27;re the shovel sellers of this particular goldrush.<p>Most other businesses trying to actually use LLMs are the riskier ones, including OpenAI, IMO (though OpenAI is perhaps the least risky due to brand recognition).</div><br/><div id="42144635" class="c"><input type="checkbox" id="c-42144635" checked=""/><div class="controls bullet"><span class="by">gizajob</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140833">parent</a><span>|</span><a href="#42141129">next</a><span>|</span><label class="collapse" for="c-42144635">[-]</label><label class="expand" for="c-42144635">[1 more]</label></div><br/><div class="children"><div class="content">I’d say it’s more about the fact that they make useful products rather than brand recognition.</div><br/></div></div><div id="42141129" class="c"><input type="checkbox" id="c-42141129" checked=""/><div class="controls bullet"><span class="by">lokimedes</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140833">parent</a><span>|</span><a href="#42144635">prev</a><span>|</span><a href="#42140666">next</a><span>|</span><label class="collapse" for="c-42141129">[-]</label><label class="expand" for="c-42141129">[2 more]</label></div><br/><div class="children"><div class="content">Or they become the Webvan&#x2F;pets.com of the bubble.</div><br/><div id="42141449" class="c"><input type="checkbox" id="c-42141449" checked=""/><div class="controls bullet"><span class="by">zeusk</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141129">parent</a><span>|</span><a href="#42140666">next</a><span>|</span><label class="collapse" for="c-42141449">[-]</label><label class="expand" for="c-42141449">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia is more likely to become CSCO or INTC but as far as I can tell, that&#x27;s still a few years off - unless ofcourse there is weakness in broader economy that accelerates the pressure on investors.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42140886" class="c"><input type="checkbox" id="c-42140886" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140383">prev</a><span>|</span><a href="#42140358">next</a><span>|</span><label class="collapse" for="c-42140886">[-]</label><label class="expand" for="c-42140886">[5 more]</label></div><br/><div class="children"><div class="content">Right. I&#x27;ve been saying for a while that if all LLM development stopped entirely and we were stuck with the models we have right now (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, Llama 3.1&#x2F;2, Qwen 2.5 etc) we could still get multiple years worth of advances just out of those existing models. There is SO MUCH we haven&#x27;t figured out about how to use them yet.</div><br/><div id="42142404" class="c"><input type="checkbox" id="c-42142404" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140886">parent</a><span>|</span><a href="#42142817">next</a><span>|</span><label class="collapse" for="c-42142404">[-]</label><label class="expand" for="c-42142404">[1 more]</label></div><br/><div class="children"><div class="content">LLMs use historic data to help create useful current data. It works well sometimes.<p>I find that a human is able to solve a P=NP situation, and an LLM can’t quite yet do that. When they can the game changes.</div><br/></div></div><div id="42142817" class="c"><input type="checkbox" id="c-42142817" checked=""/><div class="controls bullet"><span class="by">niobe</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140886">parent</a><span>|</span><a href="#42142404">prev</a><span>|</span><a href="#42140358">next</a><span>|</span><label class="collapse" for="c-42142817">[-]</label><label class="expand" for="c-42142817">[3 more]</label></div><br/><div class="children"><div class="content">&gt; There is SO MUCH we haven&#x27;t figured out about how to use them yet.<p>I mean, it&#x27;s pretty clear to me they&#x27;re a potentially great human-machine interface, but trying to make LLMs - in their current fundamental form - a reliable computational tool.. well, at best  it&#x27;s an expensive hack, but it&#x27;s just not the right tool for the job.<p>I expect the next leap forward will require some orthogonal discovery and lead to a different kind of tool. But perhaps we&#x27;ll continue to use LLMs as we knownthem now for what they&#x27;re good at - language.</div><br/><div id="42143045" class="c"><input type="checkbox" id="c-42143045" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142817">parent</a><span>|</span><a href="#42140358">next</a><span>|</span><label class="collapse" for="c-42143045">[-]</label><label class="expand" for="c-42143045">[2 more]</label></div><br/><div class="children"><div class="content">One of the biggest challenges in learning how to use and build on LLMs is figuring out how to work productively with a technology that - unlike most computers - is inherently unreliable and non-deterministic.<p>It&#x27;s possible, but it&#x27;s not at all obvious and requires a slightly skewed way of looking at them.</div><br/><div id="42143433" class="c"><input type="checkbox" id="c-42143433" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42143045">parent</a><span>|</span><a href="#42140358">next</a><span>|</span><label class="collapse" for="c-42143433">[-]</label><label class="expand" for="c-42143433">[1 more]</label></div><br/><div class="children"><div class="content">This really reminds me of a trend years ago to create probabilistic programming constructs. I think it was just a trend way ahead of its time. Typical software engineers tend to be very ill-suited to think in probabilities and how to build reasonably reliable systems around them.</div><br/></div></div></div></div></div></div></div></div><div id="42140358" class="c"><input type="checkbox" id="c-42140358" checked=""/><div class="controls bullet"><span class="by">alach11</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140886">prev</a><span>|</span><a href="#42141399">next</a><span>|</span><label class="collapse" for="c-42140358">[-]</label><label class="expand" for="c-42140358">[1 more]</label></div><br/><div class="children"><div class="content">My team and I also develop with these models every day, and I completely agree. If models stall at current levels, it will take 10 (or more) years for us to capture most of the value they offer. There&#x27;s so much work out there to automate and so many workflows to enhance with these &quot;not quite AGI-level&quot; models. And if peak model performance remains the same but cost continues to drop, that opens up vastly more applications as well.</div><br/></div></div><div id="42141399" class="c"><input type="checkbox" id="c-42141399" checked=""/><div class="controls bullet"><span class="by">bloppe</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140358">prev</a><span>|</span><a href="#42140669">next</a><span>|</span><label class="collapse" for="c-42141399">[-]</label><label class="expand" for="c-42141399">[3 more]</label></div><br/><div class="children"><div class="content">&gt; you can have LLMs create reasonable code changes, with automatic review &#x2F; iteration etc.<p>Nobody who takes code health and sustainability seriously wants to hear this. You absolutely do not want to be in a position where something breaks, but your last 50 commits were all written and reviewed by an LLM. Now you have to go back and review them all with human eyes just to get a handle on how things broke, while customers suffer. At this scale, it&#x27;s an effort multiplier, not an effort reducer.<p>It&#x27;s still good for generating little bits of boilerplate, though.</div><br/><div id="42142621" class="c"><input type="checkbox" id="c-42142621" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141399">parent</a><span>|</span><a href="#42140669">next</a><span>|</span><label class="collapse" for="c-42142621">[-]</label><label class="expand" for="c-42142621">[2 more]</label></div><br/><div class="children"><div class="content">If the last 50 commits were reviewed by an AI and it took that long for an issue to happen I’d immediately mandate all PR’s are reviewed by an AI.</div><br/><div id="42142743" class="c"><input type="checkbox" id="c-42142743" checked=""/><div class="controls bullet"><span class="by">bloppe</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42142621">parent</a><span>|</span><a href="#42140669">next</a><span>|</span><label class="collapse" for="c-42142743">[-]</label><label class="expand" for="c-42142743">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a difference between an issue being introduced and being noticed.</div><br/></div></div></div></div></div></div><div id="42140669" class="c"><input type="checkbox" id="c-42140669" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42141399">prev</a><span>|</span><a href="#42140918">next</a><span>|</span><label class="collapse" for="c-42140669">[-]</label><label class="expand" for="c-42140669">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Question for the group here: do we honestly feel like we&#x27;ve exhausted the options for delivering value on top of the current generation of LLMs?<p>Certainly not.<p>But technology is all about stacks. Each layer strives to improve, right up through UX and business value. The uses for 1µm chips had not been exhausted in 1989 when the 486 shipped in 800nm. 250nm still had tons of unexplored uses when the Pentium 4 shipped on 90nm.<p>Talking about scaling at the the model level is like talking about transistor density for silicon: it&#x27;s interesting, and relevant, and we should care... but it is not the sole determinent of what use cases can be build and what user value there is.</div><br/></div></div><div id="42140918" class="c"><input type="checkbox" id="c-42140918" checked=""/><div class="controls bullet"><span class="by">ericmcer</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140669">prev</a><span>|</span><a href="#42144404">next</a><span>|</span><label class="collapse" for="c-42140918">[-]</label><label class="expand" for="c-42140918">[4 more]</label></div><br/><div class="children"><div class="content">I have tried a few AI coding tools and always found them impressive but I don&#x27;t really need something to autocomplete obvious code cases.<p>Is there an AI tool that can ingest a codebase and locate code based on abstract questions? Like: &quot;I need to invalidate customers who haven&#x27;t logged in for a month&quot; and it can locate things like relevant DB tables, controllers, services, etc.</div><br/><div id="42144124" class="c"><input type="checkbox" id="c-42144124" checked=""/><div class="controls bullet"><span class="by">james_marks</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140918">parent</a><span>|</span><a href="#42144132">next</a><span>|</span><label class="collapse" for="c-42144124">[-]</label><label class="expand" for="c-42144124">[1 more]</label></div><br/><div class="children"><div class="content">I haven’t seen quite that, but it’s an interesting question; like a semantic search.</div><br/></div></div><div id="42144132" class="c"><input type="checkbox" id="c-42144132" checked=""/><div class="controls bullet"><span class="by">fullstackchris</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140918">parent</a><span>|</span><a href="#42144124">prev</a><span>|</span><a href="#42144404">next</a><span>|</span><label class="collapse" for="c-42144132">[-]</label><label class="expand" for="c-42144132">[2 more]</label></div><br/><div class="children"><div class="content">Cursor (Claude behind the scenes) can do that, however as always, your mileage may vary.<p>I tried building a whole codebase inspector, essentially what you are referring to with Gemini&#x27;s 2 million token context window but had troubles with their API when the payload got large. Just 500 error with no additional info so...</div><br/><div id="42144885" class="c"><input type="checkbox" id="c-42144885" checked=""/><div class="controls bullet"><span class="by">disgruntledphd2</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42144132">parent</a><span>|</span><a href="#42144404">next</a><span>|</span><label class="collapse" for="c-42144885">[-]</label><label class="expand" for="c-42144885">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve played around with Claude and larger docs and it&#x27;s honestly been a bit of a crapshoot, it feels like only some of the information gets into the prompt as the doc gets larger. They&#x27;re great for converting PDF tables to more usable formats though.</div><br/></div></div></div></div></div></div><div id="42144404" class="c"><input type="checkbox" id="c-42144404" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140918">prev</a><span>|</span><a href="#42144433">next</a><span>|</span><label class="collapse" for="c-42144404">[-]</label><label class="expand" for="c-42144404">[1 more]</label></div><br/><div class="children"><div class="content">Beyond just RAG, I&#x27;m fairly bullish on finetuning. For example, Qwen2.5-Coder-32B-Instruct is <i>much</i> better than Qwen2.5-72B-Instruct at coding... Despite simply being a smaller version of the same model, finetuned on code. It&#x27;s on par with Sonnet 3.5 and 4o on most benchmarks, whereas the simple chat-tuned 72B model is much weaker.<p>And while Qwen2.5-Coder-32B-Instruct is a pretty advanced finetune — it was trained on an extra 5 trillion tokens — even smaller finetunes have done really well. For example, Dracarys-72B, which was a simpler finetune of Qwen2.5-72B using a modified version of DPO on a handmade set of answers to GSM8K, ARC, and HellaSwag, significantly outperforms the base Qwen2.5-72B model on the aider coding benchmarks.<p>There&#x27;s a lot of intelligence we&#x27;re leaving on the floor, because everyone is just prompting generic chat-tuned models! If you tune it to do something else, it&#x27;ll be really good at the something else.</div><br/></div></div><div id="42144433" class="c"><input type="checkbox" id="c-42144433" checked=""/><div class="controls bullet"><span class="by">_rm</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42144404">prev</a><span>|</span><a href="#42140747">next</a><span>|</span><label class="collapse" for="c-42144433">[-]</label><label class="expand" for="c-42144433">[1 more]</label></div><br/><div class="children"><div class="content">Well I have a question for you: do you think this format of AI can actually think?<p>I.e. can it ruminate on the data it&#x27;s ingested, and rather than returning the response of highest probability, return something original?<p>I think that&#x27;s the key. If LLMs can&#x27;t ultimately do that, there&#x27;s still a lot to be gained from utilising the speed and fluidly scalable resources of computers.<p>But like all the top tech companies know, it&#x27;s not quantity of bodies in seats that matters but talent, the thing that&#x27;s going to prevail is raw intelligence. If it can&#x27;t think better than us, just process data faster and more voluminously but still needing human verification, we&#x27;re on an asymptotic path.</div><br/></div></div><div id="42140747" class="c"><input type="checkbox" id="c-42140747" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42144433">prev</a><span>|</span><a href="#42144682">next</a><span>|</span><label class="collapse" for="c-42140747">[-]</label><label class="expand" for="c-42140747">[4 more]</label></div><br/><div class="children"><div class="content">The main difference between GPT5 and a PhD-level new hire is that the new hire will autonomously go out, deliver and take on harder task with much fewer guidance than GPT5 will ever require. So much of human intelligence is about interacting with peers.</div><br/><div id="42140862" class="c"><input type="checkbox" id="c-42140862" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140747">parent</a><span>|</span><a href="#42144682">next</a><span>|</span><label class="collapse" for="c-42140862">[-]</label><label class="expand" for="c-42140862">[3 more]</label></div><br/><div class="children"><div class="content">Human interaction with peers is also guidance.<p>I don&#x27;t know how many team meetings PhD students have, but I do know about software development jobs with 15 minute daily standups, and that length meeting at 120 words per minute for 5 days a week, 48 weeks per year of a 3 year PhD is 1.296.000 words.</div><br/><div id="42141677" class="c"><input type="checkbox" id="c-42141677" checked=""/><div class="controls bullet"><span class="by">eastbound</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140862">parent</a><span>|</span><a href="#42144682">next</a><span>|</span><label class="collapse" for="c-42141677">[-]</label><label class="expand" for="c-42141677">[2 more]</label></div><br/><div class="children"><div class="content">I have 3 remote employees whose job is consistently as bad as LLM.<p>That means employees who use LLM are, on average, recognizably bad. Those who are good enough, are also good enough to write the code manually.<p>To the point I wonder whether this HN thread is generated by OpenAI, trying to create buzz around AI.</div><br/><div id="42141790" class="c"><input type="checkbox" id="c-42141790" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42141677">parent</a><span>|</span><a href="#42144682">next</a><span>|</span><label class="collapse" for="c-42141790">[-]</label><label class="expand" for="c-42141790">[1 more]</label></div><br/><div class="children"><div class="content">1. The person I&#x27;m replying to is hypothesising about a future, not yet existent, version, GPT5. Current quality limits don&#x27;t tell you jack about a hypothetical future, especially one that may not ever happen because money.<p>2. I&#x27;m not commenting on the quality, because they were writing about something that doesn&#x27;t exist and therefore that&#x27;s clearly just a given for the discussion. The only thing I was adding is that humans <i>also</i> need guidance, and quite a lot of it — even just a two-week sprint&#x27;s worth of 15 minute daily stand-up meetings is 18,000 words, which is well beyond the point where I&#x27;d have given up prompting an LLM and done the thing myself.</div><br/></div></div></div></div></div></div></div></div><div id="42144682" class="c"><input type="checkbox" id="c-42144682" checked=""/><div class="controls bullet"><span class="by">malthaus</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140747">prev</a><span>|</span><a href="#42140086">next</a><span>|</span><label class="collapse" for="c-42144682">[-]</label><label class="expand" for="c-42144682">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s the equivalent of the &quot;we overestimate the impact of technology in the short-term and underestimate the effect in the long run&quot; quote.<p>everyone is looking at llm scores &amp; strawberry gotchas while ignoring the trillions of market potential in replacing existing systems and (yes) people with the current capabilities. identifying the use cases, finetuning the models and (most importantly) actually rolling this out in existing organizations&#x2F;processes&#x2F;systems will be the challenge long before the base models&#x27; capabilities will be<p>it is worth working on those issues now and get the ball rolling, switching out your models for future more capable ones will be the easy part later on.</div><br/></div></div><div id="42140086" class="c"><input type="checkbox" id="c-42140086" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42144682">prev</a><span>|</span><a href="#42140349">next</a><span>|</span><label class="collapse" for="c-42140086">[-]</label><label class="expand" for="c-42140086">[3 more]</label></div><br/><div class="children"><div class="content">Yes, but literally anybody can do all those things. So while there will be many opportunities for new features (new ways of combining data), there will be few <i>business</i> opportunities.</div><br/><div id="42140603" class="c"><input type="checkbox" id="c-42140603" checked=""/><div class="controls bullet"><span class="by">Miraste</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140086">parent</a><span>|</span><a href="#42140349">next</a><span>|</span><label class="collapse" for="c-42140603">[-]</label><label class="expand" for="c-42140603">[2 more]</label></div><br/><div class="children"><div class="content">HN always says this, and it&#x27;s always wrong. A technical implementation that&#x27;s easy, or readily available, does not mean that a successful company can&#x27;t be built on it. Last year, people were saying &quot;OpenAI doesn&#x27;t have a moat.&quot; 15 years before that, they were saying &quot;Dropbox is just a couple of chron jobs, it&#x27;ll fail in a few months.&quot;</div><br/><div id="42141113" class="c"><input type="checkbox" id="c-42141113" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140603">parent</a><span>|</span><a href="#42140349">next</a><span>|</span><label class="collapse" for="c-42141113">[-]</label><label class="expand" for="c-42141113">[1 more]</label></div><br/><div class="children"><div class="content">&gt; HN always says this<p>The meaning here is different. What I&#x27;m saying is that big companies like OpenAI will always strive to make a <i>generic</i> AI, such that anyone can do basically anything using AI. The big companies therefore will indeed (like you say) have a profitable business, but few others  will.</div><br/></div></div></div></div></div></div><div id="42140349" class="c"><input type="checkbox" id="c-42140349" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140086">prev</a><span>|</span><a href="#42142919">next</a><span>|</span><label class="collapse" for="c-42140349">[-]</label><label class="expand" for="c-42140349">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Question for the group here: do we honestly feel like we&#x27;ve exhausted the options for delivering value on top of the current generation of LLMs?<p>IMO we&#x27;ve not even exhausted the options for spreadsheets, let alone LLMs.<p>And the reason I&#x27;m thinking of spreadsheets is that they, like LLMs, are very hard to win big on even despite the value they bring. Not &quot;no moat&quot; (that gets parroted stochastically in threads like these), but the moat is elsewhere.</div><br/></div></div><div id="42142919" class="c"><input type="checkbox" id="c-42142919" checked=""/><div class="controls bullet"><span class="by">RayVR</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42140349">prev</a><span>|</span><a href="#42142765">next</a><span>|</span><label class="collapse" for="c-42142919">[-]</label><label class="expand" for="c-42142919">[1 more]</label></div><br/><div class="children"><div class="content">I am definitely not an expert, nor do I have inside information on the directions of research that these companies are exploring.<p>Yes, existing LLMs are useful. Yes, there are many more things we can do with this tech.<p>However, existing SOTA models are large, expensive to run, still hallucinate, fail simple logic tests, fail to do things a poorly trained human can do on autopilot, etc.<p>The performance of LLMs is extremely variable, and it is hard to anticipate failure.<p>Many potential applications of this technology will not tolerate this level of uncertainty. Worse solutions with predictable and well understood shortcomings will dominate.</div><br/></div></div><div id="42142765" class="c"><input type="checkbox" id="c-42142765" checked=""/><div class="controls bullet"><span class="by">rco8786</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42142919">prev</a><span>|</span><a href="#42140126">next</a><span>|</span><label class="collapse" for="c-42142765">[-]</label><label class="expand" for="c-42142765">[1 more]</label></div><br/><div class="children"><div class="content">I think there’s a long way to go also. I think people expected that AI would eventually be like a “point and shoot” where you would tell it to go do some complicated task, or sillier yet, take over someone’s entire job.<p>More realistically it’s like a really great sidekick for doing very specific mundane but otherwise non deterministic tasks.<p>I think we’ll start to see AI permeate into nearly every back office job out there, but as a series of tools that help the human work faster. Not as one big brain that replaces the human.</div><br/></div></div><div id="42140126" class="c"><input type="checkbox" id="c-42140126" checked=""/><div class="controls bullet"><span class="by">hartator</span><span>|</span><a href="#42140045">parent</a><span>|</span><a href="#42142765">prev</a><span>|</span><a href="#42139410">next</a><span>|</span><label class="collapse" for="c-42140126">[-]</label><label class="expand" for="c-42140126">[4 more]</label></div><br/><div class="children"><div class="content">All of these hacks do sound like we are at that diminishing return point.</div><br/><div id="42140193" class="c"><input type="checkbox" id="c-42140193" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140126">parent</a><span>|</span><a href="#42140678">next</a><span>|</span><label class="collapse" for="c-42140193">[-]</label><label class="expand" for="c-42140193">[2 more]</label></div><br/><div class="children"><div class="content">It all just sounds to me like we&#x27;re back at expert systems. Doesn&#x27;t bode well...</div><br/><div id="42140266" class="c"><input type="checkbox" id="c-42140266" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140193">parent</a><span>|</span><a href="#42140678">next</a><span>|</span><label class="collapse" for="c-42140266">[-]</label><label class="expand" for="c-42140266">[1 more]</label></div><br/><div class="children"><div class="content">Honest question, how would you expect systems to get external knowledge etc without tools like the OP is suggesting?<p>Action oriented through self exploration? What is your thought for how these systems integrate with the existing world?<p>Why does the OP&#x27;s suggested mode of integration make you think of those older systems?</div><br/></div></div></div></div><div id="42140678" class="c"><input type="checkbox" id="c-42140678" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42140045">root</a><span>|</span><a href="#42140126">parent</a><span>|</span><a href="#42140193">prev</a><span>|</span><a href="#42139410">next</a><span>|</span><label class="collapse" for="c-42140678">[-]</label><label class="expand" for="c-42140678">[1 more]</label></div><br/><div class="children"><div class="content">Hey look, it&#x27;s Gordon Moore visiting us from 2005! :)</div><br/></div></div></div></div></div></div><div id="42139410" class="c"><input type="checkbox" id="c-42139410" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#42140045">prev</a><span>|</span><a href="#42144420">next</a><span>|</span><label class="collapse" for="c-42139410">[-]</label><label class="expand" for="c-42139410">[35 more]</label></div><br/><div class="children"><div class="content">A few important things to remember here:<p>The best engineering minds have been focused on scaling transformer pre and post training for the last three years because they had good reason to believe it would work, and it has up until now.<p>Progress has been measured against benchmarks which are &#x2F; were largely solvable with scale.<p>There is another emerging paradigm which is still small(er) scale but showing remarkable results. That&#x27;s full multi-modal training with embodied agents (aka robots). 1x, Figure, Physical Intelligence, Tesla are all making rapid progress on functionality which is definitely beyond frontier LLMs because it is distinctly <i>different</i>.<p>OpenAI&#x2F;Google&#x2F;Anthropic are not ignorant of this trend and are also reviving or investing in robots or robot-like research.<p>So while Orion and Claude 3.5 opus may not be another shocking giant leap forward, that does <i>not</i> mean that there arn&#x27;t giant shocking leaps forward coming from slightly different directions.</div><br/><div id="42139779" class="c"><input type="checkbox" id="c-42139779" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42140421">next</a><span>|</span><label class="collapse" for="c-42139779">[-]</label><label class="expand" for="c-42139779">[4 more]</label></div><br/><div class="children"><div class="content"><i>Tesla are all making rapid progress on functionality which is definitely beyond frontier LLMs because it is distinctly different</i><p>Sure, that&#x27;s tautologically true but that doesn&#x27;t imply that beyondness will lead to significant leaps that offer notable utility like LLMs. Deep Learning overall has been a way around the problem that intelligent behavior is very hard to code and no wants to hire many, many coders needed to do this (and no one actually how to get a mass of programmers to actually be useful beyond a certain of project complexity, to boot). People take the &quot;bitter lesson&quot; to mean data can do anything but I&#x27;d say a second bitter lesson is that data-things are the low hanging fruit.<p>Moreover, robot behavior is especially to fake. Impressive robot demos have been happening for decades without said robots getting the ability to act effectively in the complex, ad-hoc environment that human live in, IE, work with people or even cheaply emulate human behavior (but they can do choreographed&#x2F;puppeteered kung fu on stage).</div><br/><div id="42139926" class="c"><input type="checkbox" id="c-42139926" checked=""/><div class="controls bullet"><span class="by">hobs</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42139779">parent</a><span>|</span><a href="#42143654">next</a><span>|</span><label class="collapse" for="c-42139926">[-]</label><label class="expand" for="c-42139926">[2 more]</label></div><br/><div class="children"><div class="content">And worth noting that Tesla faked a ton of its robot footage already, they might be making progress but their physical human robotics does not seem advanced at the moment.</div><br/><div id="42140939" class="c"><input type="checkbox" id="c-42140939" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42139926">parent</a><span>|</span><a href="#42143654">next</a><span>|</span><label class="collapse" for="c-42140939">[-]</label><label class="expand" for="c-42140939">[1 more]</label></div><br/><div class="children"><div class="content">Indeed.<p>Even assuming the recent robot demo was entirely AI, the only single thing they demonstrated that would have been noteworthy was isolating one voice in a noisy crowd well enough to respond; everything else I saw Optimus do, has already been demonstrated by others.<p>What makes the uncertainty extra sad, is that a remote controllable humanoid robot is already directly useful for work in hazardous environments, and we know they&#x27;ve got at least that… but Musk would rather it be about the AI.</div><br/></div></div></div></div><div id="42143654" class="c"><input type="checkbox" id="c-42143654" checked=""/><div class="controls bullet"><span class="by">hereme888</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42139779">parent</a><span>|</span><a href="#42139926">prev</a><span>|</span><a href="#42140421">next</a><span>|</span><label class="collapse" for="c-42143654">[-]</label><label class="expand" for="c-42143654">[1 more]</label></div><br/><div class="children"><div class="content">Are we humans so different? Why do you wear what you wear? People emulate their older siblings, and so learn behavior.
LLMs can create new programs, after having initially learned similar examples from others. Likewise for AI media.</div><br/></div></div></div></div><div id="42140421" class="c"><input type="checkbox" id="c-42140421" checked=""/><div class="controls bullet"><span class="by">sincerecook</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42139779">prev</a><span>|</span><a href="#42141563">next</a><span>|</span><label class="collapse" for="c-42140421">[-]</label><label class="expand" for="c-42140421">[8 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s full multi-modal training with embodied agents (aka robots). 1x, Figure, Physical Intelligence, Tesla are all making rapid progress on functionality which is definitely beyond frontier LLMs because it is distinctly different.<p>Cool, but we already have robots doing this in 2d space (aka self driving cars) that struggle not to kill people. How is adding a third dimension going to help? People are just refusing to accept the fact that machine learning is not intelligence.</div><br/><div id="42142802" class="c"><input type="checkbox" id="c-42142802" checked=""/><div class="controls bullet"><span class="by">tick_tock_tick</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140421">parent</a><span>|</span><a href="#42141572">next</a><span>|</span><label class="collapse" for="c-42142802">[-]</label><label class="expand" for="c-42142802">[2 more]</label></div><br/><div class="children"><div class="content">I ride in self driving cares basically once a week in SF (Waymo). It&#x27;s always felt safer then a Uber and makes ways less risky maneuvers.</div><br/><div id="42144609" class="c"><input type="checkbox" id="c-42144609" checked=""/><div class="controls bullet"><span class="by">n_ary</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42142802">parent</a><span>|</span><a href="#42141572">next</a><span>|</span><label class="collapse" for="c-42144609">[-]</label><label class="expand" for="c-42144609">[1 more]</label></div><br/><div class="children"><div class="content">Could be because Uber or Taxi is trying to make most trips and maximize day earning while Waymo do not have that rush and can take things slow…<p>Of course Waymo needs money but if the car made fewer trips compared to Uber&#x2F;Taxi, it is not suffering the same consequences.<p>We need to consider human factor and the severe lacking of that in these robot&#x2F;self driving&#x2F;LLM and drawing parallels is not a direction I am feeling comfortable.<p>End of the day, Tesla also sold half baked self drive that killed people, we should not forget.</div><br/></div></div></div></div><div id="42141572" class="c"><input type="checkbox" id="c-42141572" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140421">parent</a><span>|</span><a href="#42142802">prev</a><span>|</span><a href="#42143184">next</a><span>|</span><label class="collapse" for="c-42141572">[-]</label><label class="expand" for="c-42141572">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Cool, but we already have robots doing this in 2d space (aka self driving cars) that struggle not to kill people. How is adding a third dimension going to help?<p>If we have robots that operate in 3D, they&#x27;ll be able to kill you not only from behind or from the side, but also from above. So that&#x27;s progress!</div><br/></div></div><div id="42143184" class="c"><input type="checkbox" id="c-42143184" checked=""/><div class="controls bullet"><span class="by">soheil</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140421">parent</a><span>|</span><a href="#42141572">prev</a><span>|</span><a href="#42141776">next</a><span>|</span><label class="collapse" for="c-42143184">[-]</label><label class="expand" for="c-42143184">[3 more]</label></div><br/><div class="children"><div class="content">How is self-driving a 2D problem when you navigate a 3D world? (please do visit hilly San Francisco sometime) not to mention additional dimensions like depth, velocity vectors among others.</div><br/><div id="42144401" class="c"><input type="checkbox" id="c-42144401" checked=""/><div class="controls bullet"><span class="by">physicsguy</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42143184">parent</a><span>|</span><a href="#42141776">next</a><span>|</span><label class="collapse" for="c-42144401">[-]</label><label class="expand" for="c-42144401">[2 more]</label></div><br/><div class="children"><div class="content">The visual input and sensory input to the self driving function are of the 3D world but the car is still constrained to move along a 2D topological surface, it’s not moving up and down other than by following the curvature of that</div><br/><div id="42144706" class="c"><input type="checkbox" id="c-42144706" checked=""/><div class="controls bullet"><span class="by">soheil</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42144401">parent</a><span>|</span><a href="#42141776">next</a><span>|</span><label class="collapse" for="c-42144706">[-]</label><label class="expand" for="c-42144706">[1 more]</label></div><br/><div class="children"><div class="content">So based on your argument they actually operate in 1D since roads go in one direction and lanes and intersections are constrained to a predetermined curly line.</div><br/></div></div></div></div></div></div><div id="42141776" class="c"><input type="checkbox" id="c-42141776" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140421">parent</a><span>|</span><a href="#42143184">prev</a><span>|</span><a href="#42141563">next</a><span>|</span><label class="collapse" for="c-42141776">[-]</label><label class="expand" for="c-42141776">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that machine learning today is a lot like interpolation of examples in the dataset. The breakthrough of LLMs is due to the idea that interpolation in a 1024-dimensional space works much better than in a 2d space, if we naively interpolated English letters. All the modern transformers stuff is basically an advanced interpolation method that uses a large local neighborhood than just few nearest examples. It&#x27;s like the Lanczos interpolation kernel, using a 1d analogy. Increasing the size of the kernel won&#x27;t bring any gains, because the current kernel already nearly perfectly approximates an ideal interpolation (a full dataset DFT).<p>However interpolation isn&#x27;t reasoning. If we want to understand the motion of planets, we would start with a dataset of (x, y, z, t) coordinates and try to derive the law of motion. Imagine if someone simply interpolated the dataset and presented the law of gravity as an array of million coefficients (aka weights)? Our minds have to work with a very small operating memory that can hardly fit 10 coefficients. This constraint forces us to develop intelligence that compacts the entire dataset into one small differential equation. Btw, English grammar is the differential equation of English in a lot of ways: it tells what the local rules are of valid trajectories of words that we call sentences.</div><br/></div></div></div></div><div id="42141563" class="c"><input type="checkbox" id="c-42141563" checked=""/><div class="controls bullet"><span class="by">rafaelmn</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42140421">prev</a><span>|</span><a href="#42142983">next</a><span>|</span><label class="collapse" for="c-42141563">[-]</label><label class="expand" for="c-42141563">[3 more]</label></div><br/><div class="children"><div class="content">&gt;There is another emerging paradigm which is still small(er) scale but showing remarkable results. That&#x27;s full multi-modal training with embodied agents (aka robots). 1x, Figure, Physical Intelligence, Tesla are all making rapid progress on functionality which is definitely beyond frontier LLMs because it is distinctly different.<p>Tesla is selling this view for almost a decade now in self-driving - how their car fleet feeding training data is going to make them leaders in the area. I don&#x27;t find it convincing anymore</div><br/><div id="42144438" class="c"><input type="checkbox" id="c-42144438" checked=""/><div class="controls bullet"><span class="by">Dunedan</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42141563">parent</a><span>|</span><a href="#42143183">next</a><span>|</span><label class="collapse" for="c-42144438">[-]</label><label class="expand" for="c-42144438">[1 more]</label></div><br/><div class="children"><div class="content">While one could argue whether Tesla or another company is the leader in this space, don&#x27;t all promising self-driving approaches rely on this paradigm?</div><br/></div></div><div id="42143183" class="c"><input type="checkbox" id="c-42143183" checked=""/><div class="controls bullet"><span class="by">torguyvg46787</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42141563">parent</a><span>|</span><a href="#42144438">prev</a><span>|</span><a href="#42142983">next</a><span>|</span><label class="collapse" for="c-42143183">[-]</label><label class="expand" for="c-42143183">[1 more]</label></div><br/><div class="children"><div class="content">The approaches are very limited, and it&#x27;s essentially artificial artificial AI (and need a lot of human teleop demos).<p>At CoRL last week, the progress has noticeably plateaued. Roboticists notably were pessimistic that scaling laws will apply to robotics because of the embodiment issues.</div><br/></div></div></div></div><div id="42142983" class="c"><input type="checkbox" id="c-42142983" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42141563">prev</a><span>|</span><a href="#42140194">next</a><span>|</span><label class="collapse" for="c-42142983">[-]</label><label class="expand" for="c-42142983">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Tesla are all making rapid progress on functionality<p>The lack of progress with self driving seems to indicate that Tesla has a serious problem with scaling. The investment in enormous compute resources is another red flag (if you run out of ideas, just use brute force). This points to a fundamental flaw in model architecture.</div><br/></div></div><div id="42140194" class="c"><input type="checkbox" id="c-42140194" checked=""/><div class="controls bullet"><span class="by">demosthanos</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42142983">prev</a><span>|</span><a href="#42143148">next</a><span>|</span><label class="collapse" for="c-42140194">[-]</label><label class="expand" for="c-42140194">[3 more]</label></div><br/><div class="children"><div class="content">&gt; that does not mean that there arn&#x27;t giant shocking leaps forward coming from slightly different directions.<p>Nor does it mean that there are! We&#x27;ve gotten into this habit of assuming that we&#x27;re owed giant shocking leaps forward every year or so, and this wave of AI startups raised money accordingly, but that&#x27;s never how any innovation has worked. We&#x27;ve always followed the same pattern: there&#x27;s a breakthrough which causes a major shift in what&#x27;s possible, followed by a few years of rapid growth as engineers pick up where the scientists left off, followed by a plateau while we all get used to the new normal.<p>We ought to be expecting a plateau, but Sam Altman and company have done their work well and have convinced many of us that this time it&#x27;s different. This time it&#x27;s the singularity, and we&#x27;re going to see exponential growth from here on out. People want to believe it, so they do, and Altman is milking that belief for all it&#x27;s worth.<p>But make no mistake: Altman has been telegraphing that he&#x27;s eyeing the exit, and you don&#x27;t eye the exit when you own a company that&#x27;s set to continue exponentially increasing in value.</div><br/><div id="42140605" class="c"><input type="checkbox" id="c-42140605" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140194">parent</a><span>|</span><a href="#42140584">next</a><span>|</span><label class="collapse" for="c-42140605">[-]</label><label class="expand" for="c-42140605">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Altman has been telegraphing that he&#x27;s eyeing the exit<p>Can you think of any specific examples? Not trying to express disbelief, just curious given that this is obviously not what he&#x27;s intending to communicate so it would be interesting to examine what seemed to communicate it.</div><br/></div></div></div></div><div id="42143148" class="c"><input type="checkbox" id="c-42143148" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42140194">prev</a><span>|</span><a href="#42139984">next</a><span>|</span><label class="collapse" for="c-42143148">[-]</label><label class="expand" for="c-42143148">[1 more]</label></div><br/><div class="children"><div class="content">The gap from the virtual world of software and the brutally uncompromising nature of physical reality is wider than most people seem to accept.<p>It&#x27;s almost like saying &quot;we&#x27;ve already visited every place on Earth, surely Mars is just around the corner now&quot;</div><br/></div></div><div id="42139984" class="c"><input type="checkbox" id="c-42139984" checked=""/><div class="controls bullet"><span class="by">knicholes</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42143148">prev</a><span>|</span><a href="#42140069">next</a><span>|</span><label class="collapse" for="c-42139984">[-]</label><label class="expand" for="c-42139984">[12 more]</label></div><br/><div class="children"><div class="content">Once we&#x27;ve scraped the internet of its data, we need more data.  Robots can take in video&#x2F;audio data 24&#x2F;7 and can be placed in your house to record this data by offering services like cooking&#x2F;cleaning&#x2F;folding laundry.  Yeah, I&#x27;ll pay $20k to have you record everything that happens in my house if I can stop doing dishes for five years!</div><br/><div id="42142935" class="c"><input type="checkbox" id="c-42142935" checked=""/><div class="controls bullet"><span class="by">BobaFloutist</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42139984">parent</a><span>|</span><a href="#42140263">next</a><span>|</span><label class="collapse" for="c-42142935">[-]</label><label class="expand" for="c-42142935">[1 more]</label></div><br/><div class="children"><div class="content">There already exists a robot that does the dishes, it&#x27;s called a dishwasher.</div><br/></div></div><div id="42140263" class="c"><input type="checkbox" id="c-42140263" checked=""/><div class="controls bullet"><span class="by">fldskfjdslkfj</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42139984">parent</a><span>|</span><a href="#42142935">prev</a><span>|</span><a href="#42140130">next</a><span>|</span><label class="collapse" for="c-42140263">[-]</label><label class="expand" for="c-42140263">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s plenty of video content being uploaded and streamed everyday, i find it hard to believe the more data will really change something, excluding very specialized tasks.</div><br/><div id="42140567" class="c"><input type="checkbox" id="c-42140567" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140263">parent</a><span>|</span><a href="#42140130">next</a><span>|</span><label class="collapse" for="c-42140567">[-]</label><label class="expand" for="c-42140567">[1 more]</label></div><br/><div class="children"><div class="content">The difference with the bot is that there is a fast feedback loop between action and content. No tagging required, real physics is the playground.</div><br/></div></div></div></div><div id="42140130" class="c"><input type="checkbox" id="c-42140130" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42139984">parent</a><span>|</span><a href="#42140263">prev</a><span>|</span><a href="#42140146">next</a><span>|</span><label class="collapse" for="c-42140130">[-]</label><label class="expand" for="c-42140130">[1 more]</label></div><br/><div class="children"><div class="content">Or get a dishwashing machine?</div><br/></div></div><div id="42140146" class="c"><input type="checkbox" id="c-42140146" checked=""/><div class="controls bullet"><span class="by">hartator</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42139984">parent</a><span>|</span><a href="#42140130">prev</a><span>|</span><a href="#42141123">next</a><span>|</span><label class="collapse" for="c-42140146">[-]</label><label class="expand" for="c-42140146">[6 more]</label></div><br/><div class="children"><div class="content">Why 5 years?</div><br/><div id="42140220" class="c"><input type="checkbox" id="c-42140220" checked=""/><div class="controls bullet"><span class="by">fifilura</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140146">parent</a><span>|</span><a href="#42141793">next</a><span>|</span><label class="collapse" for="c-42140220">[-]</label><label class="expand" for="c-42140220">[1 more]</label></div><br/><div class="children"><div class="content">Five years, that&#x27;s all we&#x27;ve got.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Five_Years_(David_Bowie_song)" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Five_Years_(David_Bowie_song...</a></div><br/></div></div><div id="42141793" class="c"><input type="checkbox" id="c-42141793" checked=""/><div class="controls bullet"><span class="by">knicholes</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140146">parent</a><span>|</span><a href="#42140220">prev</a><span>|</span><a href="#42140608">next</a><span>|</span><label class="collapse" for="c-42141793">[-]</label><label class="expand" for="c-42141793">[1 more]</label></div><br/><div class="children"><div class="content">No real reason.  I just made it up.  But that&#x27;s kind of my reasonable expectation of longevity of a machine like a robotic lawnmower and battery life.</div><br/></div></div><div id="42140608" class="c"><input type="checkbox" id="c-42140608" checked=""/><div class="controls bullet"><span class="by">twelve40</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140146">parent</a><span>|</span><a href="#42141793">prev</a><span>|</span><a href="#42140215">next</a><span>|</span><label class="collapse" for="c-42140608">[-]</label><label class="expand" for="c-42140608">[1 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI has announced a plan to achieve artificial general intelligence (AGI) within five years, an ambitious goal as the company works to design systems that outperform humans.</div><br/></div></div><div id="42140215" class="c"><input type="checkbox" id="c-42140215" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140146">parent</a><span>|</span><a href="#42140608">prev</a><span>|</span><a href="#42140216">next</a><span>|</span><label class="collapse" for="c-42140215">[-]</label><label class="expand" for="c-42140215">[1 more]</label></div><br/><div class="children"><div class="content">Because whatever org fills this space will be working on ARR.</div><br/></div></div><div id="42140216" class="c"><input type="checkbox" id="c-42140216" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42140146">parent</a><span>|</span><a href="#42140215">prev</a><span>|</span><a href="#42141123">next</a><span>|</span><label class="collapse" for="c-42140216">[-]</label><label class="expand" for="c-42140216">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s when the robot takes his job and he can&#x27;t afford the robot anymore.</div><br/></div></div></div></div><div id="42141123" class="c"><input type="checkbox" id="c-42141123" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42139410">root</a><span>|</span><a href="#42139984">parent</a><span>|</span><a href="#42140146">prev</a><span>|</span><a href="#42140069">next</a><span>|</span><label class="collapse" for="c-42141123">[-]</label><label class="expand" for="c-42141123">[1 more]</label></div><br/><div class="children"><div class="content">People go and live in a house to get recorded 24&#x2F;7, to be on tv, for far more asnine situations, for way less money.</div><br/></div></div></div></div><div id="42140069" class="c"><input type="checkbox" id="c-42140069" checked=""/><div class="controls bullet"><span class="by">eli_gottlieb</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42139984">prev</a><span>|</span><a href="#42142249">next</a><span>|</span><label class="collapse" for="c-42140069">[-]</label><label class="expand" for="c-42140069">[1 more]</label></div><br/><div class="children"><div class="content">&gt;The best engineering minds have been focused on scaling transformer pre and post training for the last three years<p>The best minds don&#x27;t follow the herd.</div><br/></div></div><div id="42142249" class="c"><input type="checkbox" id="c-42142249" checked=""/><div class="controls bullet"><span class="by">mvdtnz</span><span>|</span><a href="#42139410">parent</a><span>|</span><a href="#42140069">prev</a><span>|</span><a href="#42144420">next</a><span>|</span><label class="collapse" for="c-42142249">[-]</label><label class="expand" for="c-42142249">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The best engineering minds have been focused on scaling transformer pre and post training for the last three years because they had good reason to believe it would work, and it has up until now.<p>Or because the people running companies who have fooled investors into believing it will work can afford to pay said engineers life-changing amounts of money.</div><br/></div></div></div></div><div id="42144420" class="c"><input type="checkbox" id="c-42144420" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#42139410">prev</a><span>|</span><a href="#42145080">next</a><span>|</span><label class="collapse" for="c-42144420">[-]</label><label class="expand" for="c-42144420">[4 more]</label></div><br/><div class="children"><div class="content">This &quot;running out of data&quot; thing suggests that there is something fundamentally wrong with how things are working. A new driver does not need to experience 8000 different rabbit-on-road situations from all angles to know to slow down when we see one on the road. Similarly we don&#x27;t need 10,000 addition examples to learn how to add. It is as though there is no generalization in the models - just fundamentally search.</div><br/><div id="42144498" class="c"><input type="checkbox" id="c-42144498" checked=""/><div class="controls bullet"><span class="by">surrTurr</span><span>|</span><a href="#42144420">parent</a><span>|</span><a href="#42145080">next</a><span>|</span><label class="collapse" for="c-42144498">[-]</label><label class="expand" for="c-42144498">[3 more]</label></div><br/><div class="children"><div class="content">i think you underestimate the amount of data a driver experiences in a single 5 minute drive</div><br/><div id="42144649" class="c"><input type="checkbox" id="c-42144649" checked=""/><div class="controls bullet"><span class="by">eslaught</span><span>|</span><a href="#42144420">root</a><span>|</span><a href="#42144498">parent</a><span>|</span><a href="#42145080">next</a><span>|</span><label class="collapse" for="c-42144649">[-]</label><label class="expand" for="c-42144649">[2 more]</label></div><br/><div class="children"><div class="content">I never get this argument.<p>I&#x27;ve seen a deer on a road maybe once. I&#x27;ve seen a rabbit on a road zero times. But I know what to do if I see one.<p>Is that because the &quot;video&quot; of my perception has many &quot;frames&quot;? Even if that&#x27;s true at some level, I think it&#x27;s massively missing the point. Yeah, so I saw that one deer from a lot of angles. But current AI training is like the equivalent of taking every deer that has ever been on camera in the history of the human species.<p>Somehow I&#x27;m still dramatically better at generalization than the AI. Surely that&#x27;s an algorithm difference.</div><br/><div id="42144889" class="c"><input type="checkbox" id="c-42144889" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#42144420">root</a><span>|</span><a href="#42144649">parent</a><span>|</span><a href="#42145080">next</a><span>|</span><label class="collapse" for="c-42144889">[-]</label><label class="expand" for="c-42144889">[1 more]</label></div><br/><div class="children"><div class="content">You might personally have seen a deer just once, but human evolution, and animal evolution prior to that have practiced this skill a lot. AI doesn&#x27;t have the advantage of evolutionary priors baked in, so it needs explicit walking through many combinations to infer its structure from data, and is remarkably efficient. GPT-4 &#x27;only&#x27; trained on the amount of language that 30,000 humans use in their lifetime.<p>But we have seen from AlphaGo that when training data is extensive, it can rediscover strategy on its own and even surpass us. It&#x27;s not inherently worse than human learning.</div><br/></div></div></div></div></div></div></div></div><div id="42145080" class="c"><input type="checkbox" id="c-42145080" checked=""/><div class="controls bullet"><span class="by">_Algernon_</span><span>|</span><a href="#42144420">prev</a><span>|</span><a href="#42139116">next</a><span>|</span><label class="collapse" for="c-42145080">[-]</label><label class="expand" for="c-42145080">[1 more]</label></div><br/><div class="children"><div class="content">The next AI winter will be brutal</div><br/></div></div><div id="42139116" class="c"><input type="checkbox" id="c-42139116" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#42145080">prev</a><span>|</span><a href="#42139647">next</a><span>|</span><label class="collapse" for="c-42139116">[-]</label><label class="expand" for="c-42139116">[1 more]</label></div><br/><div class="children"><div class="content">I think it is a good thing for AI that we hit the data ceiling, because the pressure moves toward coming up with better model architectures. And with respect to a decade ago there&#x27;s a much larger number of capable and smart AI researchers who are looking for one.</div><br/></div></div><div id="42139647" class="c"><input type="checkbox" id="c-42139647" checked=""/><div class="controls bullet"><span class="by">aresant</span><span>|</span><a href="#42139116">prev</a><span>|</span><a href="#42144723">next</a><span>|</span><label class="collapse" for="c-42139647">[-]</label><label class="expand" for="c-42139647">[3 more]</label></div><br/><div class="children"><div class="content">Taking a hollistic view informed by a disruptive OpenAI &#x2F; AI &#x2F; LLM twitter habit I would say this is AI&#x27;s &quot;What gets measured gets managed&quot; moment and the narrative will change<p>This is supported by both general observations and recently this tweet from an OpenAI engineer that Sam responded to and engaged -&gt;<p>&quot;scaling has hit a wall and that wall is 100% eval saturation&quot;<p>Which I interpert to mean his view is that models are no longer yielding significant performance improvements because the models have maxed out existing evaluation metrics.<p>Are those evaluations (or even LLMs) the RIGHT measures to achieve AGI? Probably not.<p>But have they been useful tools to demonstrate that the confluence of compute, engineering, and tactical models are leading towards signifigant breathroughts in artificial (computer) intelligence?<p>I would say yes.<p>Which in turn are driving the funding, power innovation, public policy etc needed to take that next step?<p>I hope so.<p>(1) <a href="https:&#x2F;&#x2F;x.com&#x2F;willdepue&#x2F;status&#x2F;1856766850027458648" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;willdepue&#x2F;status&#x2F;1856766850027458648</a></div><br/><div id="42139702" class="c"><input type="checkbox" id="c-42139702" checked=""/><div class="controls bullet"><span class="by">ActionHank</span><span>|</span><a href="#42139647">parent</a><span>|</span><a href="#42142811">next</a><span>|</span><label class="collapse" for="c-42139702">[-]</label><label class="expand" for="c-42139702">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Which in turn are driving the funding, power innovation, public policy etc needed to take that next step?<p>They are driving the shoveling of VC money into a furnace to power their servers.<p>Should that money run dry before they hit another breakthrough &quot;AI&quot; popularity is going to drop like a stone. I believe this to be far more likely an outcome than AGI or even the next big breakthrough.</div><br/></div></div><div id="42142811" class="c"><input type="checkbox" id="c-42142811" checked=""/><div class="controls bullet"><span class="by">Bjorkbat</span><span>|</span><a href="#42139647">parent</a><span>|</span><a href="#42139702">prev</a><span>|</span><a href="#42144723">next</a><span>|</span><label class="collapse" for="c-42142811">[-]</label><label class="expand" for="c-42142811">[1 more]</label></div><br/><div class="children"><div class="content">I agree that existing benchmarks are no longer useful now that there&#x27;s basically nothing left in them that seems to stump LLMs.<p>But when I hear that models are failing to meet expectations, I imagine what they&#x27;re saying is that the researchers had some sort of eval in mind with room to grow and a target, and that the model in question failed to hit the target they had in mind.<p>Honestly, problem with sentiments like these is on Twitter is that you can&#x27;t tell if they&#x27;re being sincere or just making a snarky, useless remark.  Probably a mix of both.</div><br/></div></div></div></div><div id="42144723" class="c"><input type="checkbox" id="c-42144723" checked=""/><div class="controls bullet"><span class="by">datahack</span><span>|</span><a href="#42139647">prev</a><span>|</span><a href="#42139919">next</a><span>|</span><label class="collapse" for="c-42144723">[-]</label><label class="expand" for="c-42144723">[1 more]</label></div><br/><div class="children"><div class="content">The next wave won’t be monolithic but network-driven. Orchestration has the potential to integrate diverse AI systems and complementary technologies, such as advanced fact-checking and rule-based output frameworks.<p>This methodological growth could make LLMs more reliable, consistent, and aligned with specific use cases.<p>The skepticism surrounding this vision mirrors early doubts about the early internet fairly concisely.<p>Initially, the internet was seen as fragmented collection of isolated systems without a clear structure or purpose. It really was. You would gopher somewhere and get a file, and eventually we had apps like like pine for email, but as cool as it was it has limited utility.<p>People doubted it could <i>ever</i> become the seamless, interconnected web we know today.<p>Yet, through protocols, shared standards, and robust frameworks, the internet evolved into a powerful network capable of handling diverse applications, data flows, and user needs.<p>In the same way, LLM orchestration will mature by standardizing interfaces, improving interoperability, and fostering cooperation among varied AI models and support systems.<p>Just as the internet needed HTTP, TCP&#x2F;IP, and other protocols to unify disparate networks, orchestrated AI systems will require foundational frameworks and “rules of the road” that bring cohesion to diverse technologies.<p>We are at the veeeeery infancy of this era and have a LONG way to go here. Some of the progress looks clear and a linear progression, but a lot, like the Internet, will just take a while to mature and we shouldn’t forget what we learned the last time we faced a sea change technological revolution.</div><br/></div></div><div id="42139919" class="c"><input type="checkbox" id="c-42139919" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42144723">prev</a><span>|</span><a href="#42139694">next</a><span>|</span><label class="collapse" for="c-42139919">[-]</label><label class="expand" for="c-42139919">[21 more]</label></div><br/><div class="children"><div class="content"><i>&quot;While the model was initially expected to significantly surpass previous versions of the technology behind ChatGPT, it fell short in key areas, particularly in answering coding questions outside its training data.&quot;</i><p>Right. If you generate some code with ChatGPT, and then try to find similar code on the web, you usually will. Search for unusual phrases in comments and for variable names. Often, something from Stack Overflow will match.<p>LLMs do search and copy&#x2F;paste with idiom translation and some transliteration. That&#x27;s good enough for a lot of common problems. Especially in the HTML&#x2F;Javascript space, where people solve the same problems over and over. Or problems covered in textbooks and classes.<p>But it does not look like artificial general intelligence emerges from LLMs alone.<p>There&#x27;s also the elephant in the room - the hallucination&#x2F;lack of confidence metric problem. The curse of LLMs is that they return answers which are confident but wrong. &quot;I don&#x27;t know&quot; is rarely seen. Until that&#x27;s fixed, you can&#x27;t trust LLMs to actually <i>do</i> much on their own.  LLMs with a confidence metric would be much more useful than what we have now.</div><br/><div id="42139986" class="c"><input type="checkbox" id="c-42139986" checked=""/><div class="controls bullet"><span class="by">dmd</span><span>|</span><a href="#42139919">parent</a><span>|</span><a href="#42141067">next</a><span>|</span><label class="collapse" for="c-42139986">[-]</label><label class="expand" for="c-42139986">[16 more]</label></div><br/><div class="children"><div class="content">&gt; Right. If you generate some code with ChatGPT, and then try to find similar code on the web, you usually will.<p>People who &quot;follow&quot; AI, as the latest fad they want to comment on and appear intelligent about, repeat things like this constantly, even though they&#x27;re not actually true for anything but the most trivial hello-world types of problems.<p>I write code all day every day. I use Copilot and the like all day every day (for me, in the medical imaging software field), and all day every day it is incredibly useful and writes nearly exactly the code I would have written, but faster. And none of it appears anywhere else; I&#x27;ve checked.</div><br/><div id="42140406" class="c"><input type="checkbox" id="c-42140406" checked=""/><div class="controls bullet"><span class="by">ngai_aku</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42139986">parent</a><span>|</span><a href="#42142654">next</a><span>|</span><label class="collapse" for="c-42140406">[-]</label><label class="expand" for="c-42140406">[10 more]</label></div><br/><div class="children"><div class="content">You’re solving novel problems all day every day?</div><br/><div id="42140436" class="c"><input type="checkbox" id="c-42140436" checked=""/><div class="controls bullet"><span class="by">dmd</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42140406">parent</a><span>|</span><a href="#42144250">next</a><span>|</span><label class="collapse" for="c-42140436">[-]</label><label class="expand" for="c-42140436">[8 more]</label></div><br/><div class="children"><div class="content">Pretty much, yes. My job is pretty fun; it mostly entails things like &quot;take this horrible file workflow some research assistant came up with while high 15 years ago and turn it into a newer horrible file format a NEW research assistant came up with (also while high) 3 years ago&quot; - and automate this in our data processing pipeline.</div><br/><div id="42141794" class="c"><input type="checkbox" id="c-42141794" checked=""/><div class="controls bullet"><span class="by">fireflash38</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42140436">parent</a><span>|</span><a href="#42141764">next</a><span>|</span><label class="collapse" for="c-42141794">[-]</label><label class="expand" for="c-42141794">[2 more]</label></div><br/><div class="children"><div class="content">If you&#x27;ve got clearly defined start input format and end output format, sure it seems that it would be a good candidate for heavy LLM use. But I don&#x27;t know if that&#x27;s most people.</div><br/><div id="42141811" class="c"><input type="checkbox" id="c-42141811" checked=""/><div class="controls bullet"><span class="by">dmd</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42141794">parent</a><span>|</span><a href="#42141764">next</a><span>|</span><label class="collapse" for="c-42141811">[-]</label><label class="expand" for="c-42141811">[1 more]</label></div><br/><div class="children"><div class="content">If it were ever clearly defined or even consistent from input to input I would be overjoyed.</div><br/></div></div></div></div><div id="42141764" class="c"><input type="checkbox" id="c-42141764" checked=""/><div class="controls bullet"><span class="by">delusional</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42140436">parent</a><span>|</span><a href="#42141794">prev</a><span>|</span><a href="#42140978">next</a><span>|</span><label class="collapse" for="c-42141764">[-]</label><label class="expand" for="c-42141764">[2 more]</label></div><br/><div class="children"><div class="content">If I understand that correctly you&#x27;re converting file formats? That&#x27;s not exactly &quot;novel&quot;</div><br/><div id="42142072" class="c"><input type="checkbox" id="c-42142072" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42141764">parent</a><span>|</span><a href="#42140978">next</a><span>|</span><label class="collapse" for="c-42142072">[-]</label><label class="expand" for="c-42142072">[1 more]</label></div><br/><div class="children"><div class="content">This is exactly the type of novel work that llms are good at. It&#x27;s tedious and has annoying internal logic, but that logic is quite flat and there are a million examples to generalise from.<p>What they fail at is code with high cyclomatic complexity. Back in the llama 2 finetune days I wrote a script that would break down what each node in the control flow graph into its own prompt using literate programming and the results were amazing for the time. Using the same prompts I&#x27;d get correct code in every language I tried.</div><br/></div></div></div></div><div id="42140978" class="c"><input type="checkbox" id="c-42140978" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42140436">parent</a><span>|</span><a href="#42141764">prev</a><span>|</span><a href="#42144250">next</a><span>|</span><label class="collapse" for="c-42140978">[-]</label><label class="expand" for="c-42140978">[3 more]</label></div><br/><div class="children"><div class="content">Due to WFH, the weed laws where tech workers live, and the fast tolerance building of cannabis in the body - I estimate that 10% of all code written by west coast tech workers is done “while high” and that estimate is likely low.</div><br/><div id="42141577" class="c"><input type="checkbox" id="c-42141577" checked=""/><div class="controls bullet"><span class="by">portaouflop</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42140978">parent</a><span>|</span><a href="#42144250">next</a><span>|</span><label class="collapse" for="c-42141577">[-]</label><label class="expand" for="c-42141577">[2 more]</label></div><br/><div class="children"><div class="content">Do tech workers write better or worse code while high ?</div><br/><div id="42143325" class="c"><input type="checkbox" id="c-42143325" checked=""/><div class="controls bullet"><span class="by">throw310822</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42141577">parent</a><span>|</span><a href="#42144250">next</a><span>|</span><label class="collapse" for="c-42143325">[-]</label><label class="expand" for="c-42143325">[1 more]</label></div><br/><div class="children"><div class="content">Should copilot be renamed to &quot;designated driver&quot;?</div><br/></div></div></div></div></div></div></div></div><div id="42144250" class="c"><input type="checkbox" id="c-42144250" checked=""/><div class="controls bullet"><span class="by">gitaarik</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42140406">parent</a><span>|</span><a href="#42140436">prev</a><span>|</span><a href="#42142654">next</a><span>|</span><label class="collapse" for="c-42144250">[-]</label><label class="expand" for="c-42144250">[1 more]</label></div><br/><div class="children"><div class="content">If we weren&#x27;t, we (as in developers) weren&#x27;t needed, right?</div><br/></div></div></div></div><div id="42142654" class="c"><input type="checkbox" id="c-42142654" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42139986">parent</a><span>|</span><a href="#42140406">prev</a><span>|</span><a href="#42143451">next</a><span>|</span><label class="collapse" for="c-42142654">[-]</label><label class="expand" for="c-42142654">[3 more]</label></div><br/><div class="children"><div class="content">&gt; even though they&#x27;re not actually true for anything but the most trivial hello-world types of problems.<p>Um.<p>All the parent post said was:<p>&gt; then try to find similar code on the web, you usually will.<p>Not identical code. Similar code.<p>I think you&#x27;re really stretching the domain of plausibility to suggest that <i>any code you write</i> is novel enough that you can&#x27;t find &#x27;similar&#x27; code on the internet.<p>To suggest that code generated from a corpus that is not going to be &#x27;similar&#x27; to the code <i>from the corpus</i> is just factually and unambiguously false.<p>Of course, it depends on what you interpret &#x27;similar&#x27; to mean; but I think it&#x27;s not unfair to say a lot of code is composed of smaller parts of code that is <i>extremely similar</i> to other examples of code on the internet.<p>Obviously you&#x27;re not going to find an example similar to your entire code base; but if you&#x27;re using, for example, <i>copilot</i> where you generate many small snippets of code... welll....</div><br/><div id="42142676" class="c"><input type="checkbox" id="c-42142676" checked=""/><div class="controls bullet"><span class="by">dmd</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42142654">parent</a><span>|</span><a href="#42143451">next</a><span>|</span><label class="collapse" for="c-42142676">[-]</label><label class="expand" for="c-42142676">[2 more]</label></div><br/><div class="children"><div class="content">Ok, yes. There are other pieces of code on the internet that use a for loop or an if statement.<p>By that logic what you wrote was also composed that way. After all, you’ve used all words that have been used before! I bet even phrases like “that is extremely similar” and “generated from a corpus” and “unambiguously false”.<p>Again, I really find it hard to believe that anyone could make an argument like the one you’re making who has actually used these tools in their work for hundreds of hours, vs. for a couple minutes here or there with made up problems.</div><br/><div id="42143823" class="c"><input type="checkbox" id="c-42143823" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42142676">parent</a><span>|</span><a href="#42143451">next</a><span>|</span><label class="collapse" for="c-42143823">[-]</label><label class="expand" for="c-42143823">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  I really find it hard to believe<p>What&#x27;s true and what&#x27;s not true is not related to what you personally believe.<p>It is <i>factually</i> and <i>unambiguously false</i> to state that generated code is, in general, not similar to other code from the corpus it is trained on.<p>&gt; And none of it appears anywhere else; I&#x27;ve checked.<p>^ Even if this statement, is not false (I&#x27;m skeptical, but whatever), <i>in general</i>, it would be false for most users of copilot.<p>None of it appears anywhere else? <i>None</i> of it? Really?<p>That&#x27;s not true of the no-AI code base I&#x27;m working on.<p>That&#x27;s very difficult to believe it would be true on a code base heavily written by copilot and the like.<p>It&#x27;s probably not true, in general, for AI generated code bases.<p>We can have a different conversation about <i>verbatim copied</i> code, where an AI model generates a large body of verbatim copy from a training source. That&#x27;s very unusual.<p>...but to say the generated code wouldn&#x27;t even be <i>similar</i>? Come on.<p>That&#x27;s <i>literally</i> what LLMs do.</div><br/></div></div></div></div></div></div><div id="42143451" class="c"><input type="checkbox" id="c-42143451" checked=""/><div class="controls bullet"><span class="by">bobsmooth</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42139986">parent</a><span>|</span><a href="#42142654">prev</a><span>|</span><a href="#42142508">next</a><span>|</span><label class="collapse" for="c-42143451">[-]</label><label class="expand" for="c-42143451">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had generated code include comments so specific I was able to find the exact github repo where it came from.</div><br/></div></div><div id="42142508" class="c"><input type="checkbox" id="c-42142508" checked=""/><div class="controls bullet"><span class="by">tymscar</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42139986">parent</a><span>|</span><a href="#42143451">prev</a><span>|</span><a href="#42141067">next</a><span>|</span><label class="collapse" for="c-42142508">[-]</label><label class="expand" for="c-42142508">[1 more]</label></div><br/><div class="children"><div class="content">How often did you check?</div><br/></div></div></div></div><div id="42141067" class="c"><input type="checkbox" id="c-42141067" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#42139919">parent</a><span>|</span><a href="#42139986">prev</a><span>|</span><a href="#42140895">next</a><span>|</span><label class="collapse" for="c-42141067">[-]</label><label class="expand" for="c-42141067">[1 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs do search and copy&#x2F;paste with idiom translation and some transliteration.<p>In general, this is not a good description about what is happening inside an LLM. There is extensive literature on interpretability. It is complicated and still being worked out.<p>The commenter above might <i>characterize</i> the results they get in this way, but I would question the validity of that characterization, not to mention its generality.</div><br/></div></div><div id="42143954" class="c"><input type="checkbox" id="c-42143954" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#42139919">parent</a><span>|</span><a href="#42140895">prev</a><span>|</span><a href="#42139694">next</a><span>|</span><label class="collapse" for="c-42143954">[-]</label><label class="expand" for="c-42143954">[2 more]</label></div><br/><div class="children"><div class="content">The brain solves that problem. It seems to involve memory and specialized regions. I found a few groups building hippocampus-like, research models. One had content-addressable memory.<p>There was another one that claimed to get rid of hallucinations. They also said it takes 50-100 epochs for regular architectures to actually memorize something. Their paper is below in case people qualified to review it want to.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.17642" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.17642</a><p>Like the brain, I believe the problem will be solved by a mix of specialized components working together. One of those components will be a memory (or series of them) that the others reference to keep processing grounded in reality.</div><br/><div id="42144641" class="c"><input type="checkbox" id="c-42144641" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42139919">root</a><span>|</span><a href="#42143954">parent</a><span>|</span><a href="#42139694">next</a><span>|</span><label class="collapse" for="c-42144641">[-]</label><label class="expand" for="c-42144641">[1 more]</label></div><br/><div class="children"><div class="content">Comments on that paper? PDF: [1]<p>What they are measuring, it seems, is whether LLMs can be built which will retrieve a reliable known correct answer on request. That&#x27;s an information retrieval problem, and, in fact, they solve it by adding &quot;Memory Experts&quot; which are basically data storage.<p>It&#x27;s not clear that this helps either replies which require synthesizing disparate information, or detecting that the training data does not contain info needed to construct a reply.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2406.17642" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2406.17642</a></div><br/></div></div></div></div></div></div><div id="42139694" class="c"><input type="checkbox" id="c-42139694" checked=""/><div class="controls bullet"><span class="by">headcanon</span><span>|</span><a href="#42139919">prev</a><span>|</span><a href="#42144289">next</a><span>|</span><label class="collapse" for="c-42139694">[-]</label><label class="expand" for="c-42139694">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see a problem with this, we were inevitably going to reach some kind of plateau with existing pre-LLM-era data.<p>Meanwhile, the existing tech is such a step change that industry is going to need time to figure out how to effectively use these models. In a lot of ways it feels like the &quot;digitization&quot; era all over again - workflows and organizations that were built around the idea humans handled all the cognitive load (basically all companies older than a year or two) will need time to adjust to a hybrid AI + human model.</div><br/><div id="42141342" class="c"><input type="checkbox" id="c-42141342" checked=""/><div class="controls bullet"><span class="by">readyplayernull</span><span>|</span><a href="#42139694">parent</a><span>|</span><a href="#42144289">next</a><span>|</span><label class="collapse" for="c-42141342">[-]</label><label class="expand" for="c-42141342">[1 more]</label></div><br/><div class="children"><div class="content">&gt; feels like the &quot;digitization&quot; era all over again<p>This exactly. And as history shows, no matter how much effort the current big LLM companies do they won&#x27;t be able to grasp the best uses for their tech. We will see small players developing it even further. I&#x27;m thankful for the legendary blindness of these anticompetitive behemoths. Less than 2 decades ago: IBM Watson.</div><br/></div></div></div></div><div id="42144289" class="c"><input type="checkbox" id="c-42144289" checked=""/><div class="controls bullet"><span class="by">grey-area</span><span>|</span><a href="#42139694">prev</a><span>|</span><a href="#42139220">next</a><span>|</span><label class="collapse" for="c-42144289">[-]</label><label class="expand" for="c-42144289">[1 more]</label></div><br/><div class="children"><div class="content">The biggest weakness of generative AI to me is knowledge. It gives the <i>impression</i> of knowledge about the world without actually having a model of the world or any sense of what it does or does not know.<p>For example recently I asked it to generate some phrases for a list of words, along with synonym and antonym lists.<p>The phrases were generally correct and appropriate (some mistakes but that’s fine). The synonyms&#x2F;antonyms were misaligned to the list (so strictly speaking all wrong) and were often incorrect anyway. I imagine it would be the same if you asked for definitions of a list of words.<p>If you ask it to correct it just generates something else which is often also wrong. It’s certainly superficially convincing in many domains but once you try to get it to do real work it’s wrong in subtle ways.</div><br/></div></div><div id="42139220" class="c"><input type="checkbox" id="c-42139220" checked=""/><div class="controls bullet"><span class="by">WorkerBee28474</span><span>|</span><a href="#42144289">prev</a><span>|</span><a href="#42140562">next</a><span>|</span><label class="collapse" for="c-42139220">[-]</label><label class="expand" for="c-42139220">[2 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI&#x27;s latest model ... failed to meet the company&#x27;s performance expectations ... particularly in answering coding questions outside its training data.<p>So the models&#x27; accuracies won&#x27;t grow exponentially, but can still grow linearly with the size of the training data.<p>Sounds like DataAnnotation will be sending out a lot more LinkedIn messages.</div><br/><div id="42139271" class="c"><input type="checkbox" id="c-42139271" checked=""/><div class="controls bullet"><span class="by">pton_xd</span><span>|</span><a href="#42139220">parent</a><span>|</span><a href="#42140562">next</a><span>|</span><label class="collapse" for="c-42139271">[-]</label><label class="expand" for="c-42139271">[1 more]</label></div><br/><div class="children"><div class="content">I thought I saw some paper suggesting that accuracy grows linearly with exponential data. If that&#x27;s the case it&#x27;s not a mystery why we&#x27;d be hitting a training wall. Not sure I got the right takeaway from that study, though.<p>EDIT: here&#x27;s the paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.04125" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.04125</a></div><br/></div></div></div></div><div id="42140562" class="c"><input type="checkbox" id="c-42140562" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42139220">prev</a><span>|</span><a href="#42139479">next</a><span>|</span><label class="collapse" for="c-42140562">[-]</label><label class="expand" for="c-42140562">[18 more]</label></div><br/><div class="children"><div class="content">Every negative headline I see about AI hitting a wall or being over-hyped makes me think of the early 2000&#x27;s with that new thing the &#x27;internet&#x27; (yes, I know the internet is a lot older than that). There is little doubt in my mind that ten years from now nearly every aspect of life will be deeply connected to AI just like the internet took over everything in the late 90&#x27;s and early 2000&#x27;s and is now deeply connected to everything now. I&#x27;d even hazard to say that AI could be more impactful.</div><br/><div id="42143108" class="c"><input type="checkbox" id="c-42143108" checked=""/><div class="controls bullet"><span class="by">woopwoop</span><span>|</span><a href="#42140562">parent</a><span>|</span><a href="#42140699">next</a><span>|</span><label class="collapse" for="c-42143108">[-]</label><label class="expand" for="c-42143108">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s funny, because to me these headlines about how deep learning is over-hyped and hitting the wall remind me of headlines from ten years ago about how... deep learning is over-hyped and hitting the wall.</div><br/><div id="42143277" class="c"><input type="checkbox" id="c-42143277" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42143108">parent</a><span>|</span><a href="#42140699">next</a><span>|</span><label class="collapse" for="c-42143277">[-]</label><label class="expand" for="c-42143277">[1 more]</label></div><br/><div class="children"><div class="content">That was before people could generate animation and music.</div><br/></div></div></div></div><div id="42140699" class="c"><input type="checkbox" id="c-42140699" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42140562">parent</a><span>|</span><a href="#42143108">prev</a><span>|</span><a href="#42141362">next</a><span>|</span><label class="collapse" for="c-42140699">[-]</label><label class="expand" for="c-42140699">[4 more]</label></div><br/><div class="children"><div class="content">And, as I&#x27;ve noted a couple of times in this thread, how many times have we heard that Moore&#x27;s law is dead and compute has hit a wall?</div><br/><div id="42141879" class="c"><input type="checkbox" id="c-42141879" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42140699">parent</a><span>|</span><a href="#42141362">next</a><span>|</span><label class="collapse" for="c-42141879">[-]</label><label class="expand" for="c-42141879">[3 more]</label></div><br/><div class="children"><div class="content">Well according to Nvidia you can just ignore Moore&#x27;s law and start requiring people to install multi kilowatt outlets just for their cards. Who needs efficiency amirite?</div><br/><div id="42142664" class="c"><input type="checkbox" id="c-42142664" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42141879">parent</a><span>|</span><a href="#42141362">next</a><span>|</span><label class="collapse" for="c-42142664">[-]</label><label class="expand" for="c-42142664">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not an apple fan (as I type on a mac that I am forced to use) but I gotta applaud their push for power efficiency. NVIDIA actually -does- have a few cards they make that really improve power efficiency but then they generally hamstring them with a lack of memory. NVIDIA is really good at making their high-end cards the only viable choice but I think that will backfire on them as people like me, that value quiet, cool and efficient over 25% faster inference start taking any viable alternative that comes out.</div><br/><div id="42143898" class="c"><input type="checkbox" id="c-42143898" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42142664">parent</a><span>|</span><a href="#42141362">next</a><span>|</span><label class="collapse" for="c-42143898">[-]</label><label class="expand" for="c-42143898">[1 more]</label></div><br/><div class="children"><div class="content">Memory is king.<p>Anything that has more memory and adequate compute will win the coming AI wars.<p>At the rate at which power consumption is growing now that the shortage of current gen cards has started to work itself out people are realizing they need a fleet of nuclear reactors to keep the data centers running. This is not something that&#x27;s getting fix with the coming generation, if anything it&#x27;s worse.</div><br/></div></div></div></div></div></div></div></div><div id="42141362" class="c"><input type="checkbox" id="c-42141362" checked=""/><div class="controls bullet"><span class="by">JohnMakin</span><span>|</span><a href="#42140562">parent</a><span>|</span><a href="#42140699">prev</a><span>|</span><a href="#42143930">next</a><span>|</span><label class="collapse" for="c-42141362">[-]</label><label class="expand" for="c-42141362">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s strange to me that&#x27;s your takeaway. The reason that the internet was overhyped in the 2000&#x27;s is because it <i>was</i> and also heavily overvalued. It took a massive correction and seriously disruptive bubble burst to break the delusion and move on to something more sustainable.</div><br/><div id="42141769" class="c"><input type="checkbox" id="c-42141769" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42141362">parent</a><span>|</span><a href="#42143930">next</a><span>|</span><label class="collapse" for="c-42141769">[-]</label><label class="expand" for="c-42141769">[7 more]</label></div><br/><div class="children"><div class="content">I disagree that it was over hyped. It has transformed our society so much that I would argue it was vastly under-hyped. Sure, there were a lot of silly companies that sprang up and went away because they weren&#x27;t sound, but so much of the modern economy is based on the internet that it is hard to say any business isn&#x27;t somehow internet related today. You would be hard pressed to find any business anywhere that doesn&#x27;t at least have a social media account. If 2000 was over-hyping things I just don&#x27;t see it.</div><br/><div id="42141844" class="c"><input type="checkbox" id="c-42141844" checked=""/><div class="controls bullet"><span class="by">JohnMakin</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42141769">parent</a><span>|</span><a href="#42142165">next</a><span>|</span><label class="collapse" for="c-42141844">[-]</label><label class="expand" for="c-42141844">[3 more]</label></div><br/><div class="children"><div class="content">pets.com was valued at $400 million based almost completely on its domain name. That&#x27;s the classic example. People were throwing buckets of money at any .com that resolved to a site and almost all of it failed. I&#x27;m not sure how that doesn&#x27;t meet the definition of over-hyped. It feels very similar to now. Not even to mention - the web largely doesn&#x27;t consist of .com sites anymore, it&#x27;s mostly a few centralized sites and apps.</div><br/><div id="42143744" class="c"><input type="checkbox" id="c-42143744" checked=""/><div class="controls bullet"><span class="by">dmix</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42141844">parent</a><span>|</span><a href="#42142165">next</a><span>|</span><label class="collapse" for="c-42143744">[-]</label><label class="expand" for="c-42143744">[2 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t that mostly from public markets which never invested in tech before?</div><br/><div id="42143896" class="c"><input type="checkbox" id="c-42143896" checked=""/><div class="controls bullet"><span class="by">infamouscow</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42143744">parent</a><span>|</span><a href="#42142165">next</a><span>|</span><label class="collapse" for="c-42143896">[-]</label><label class="expand" for="c-42143896">[1 more]</label></div><br/><div class="children"><div class="content">There is a graveyard of hardware companies from the 70s, 80s, and 90s.</div><br/></div></div></div></div></div></div><div id="42142165" class="c"><input type="checkbox" id="c-42142165" checked=""/><div class="controls bullet"><span class="by">adamrezich</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42141769">parent</a><span>|</span><a href="#42141844">prev</a><span>|</span><a href="#42143930">next</a><span>|</span><label class="collapse" for="c-42142165">[-]</label><label class="expand" for="c-42142165">[3 more]</label></div><br/><div class="children"><div class="content">There were no smartphones in 2000, so the Web <i>was</i> overvalued at that point in time... until we all started carrying the Web in our pockets in the form of a portable rectangle.<p>Given that this is the case, why can&#x27;t this be analogously true of “AI” as well? There&#x27;s plenty of reason to believe that we&#x27;re hitting a wall, such that, to progress further, said wall must be overcome by means of one or more breakthroughs.</div><br/><div id="42144553" class="c"><input type="checkbox" id="c-42144553" checked=""/><div class="controls bullet"><span class="by">spunker540</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42142165">parent</a><span>|</span><a href="#42142322">next</a><span>|</span><label class="collapse" for="c-42144553">[-]</label><label class="expand" for="c-42144553">[1 more]</label></div><br/><div class="children"><div class="content">I think everyone knew the internet would change everything and thus be very valuable. At the time the web was the primary manifestation of the internet. Domain names felt like an oil rush to carve up the internet. But it was actually a rush to carve up the web, and no one realized yet that things like Google search and app stores would make domain names far less valuable over time.</div><br/></div></div><div id="42142322" class="c"><input type="checkbox" id="c-42142322" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42140562">root</a><span>|</span><a href="#42142165">parent</a><span>|</span><a href="#42144553">prev</a><span>|</span><a href="#42143930">next</a><span>|</span><label class="collapse" for="c-42142322">[-]</label><label class="expand" for="c-42142322">[1 more]</label></div><br/><div class="children"><div class="content">&#x27;smartphones&#x27; needed a reason to exist, the internet provided that. I doubt we would have had them without it. AI will drive whole new product categories that didn&#x27;t exist that will then transform our society even more.</div><br/></div></div></div></div></div></div></div></div><div id="42143930" class="c"><input type="checkbox" id="c-42143930" checked=""/><div class="controls bullet"><span class="by">rm_-rf_slash</span><span>|</span><a href="#42140562">parent</a><span>|</span><a href="#42141362">prev</a><span>|</span><a href="#42141703">next</a><span>|</span><label class="collapse" for="c-42143930">[-]</label><label class="expand" for="c-42143930">[1 more]</label></div><br/><div class="children"><div class="content">AI was overhyped in the 1950s with the perceptron. Machine learning advances in fits and starts. As soon as it looks like it’s out of steam something novel comes out. Circa 2010 all the effort was on perfecting SVMs to the point where 1% point improvement on a computer vision task was a PhD thesis and the like then all of a sudden AlexNet made neural nets look feasible and the game changed overnight.</div><br/></div></div><div id="42141703" class="c"><input type="checkbox" id="c-42141703" checked=""/><div class="controls bullet"><span class="by">mvdtnz</span><span>|</span><a href="#42140562">parent</a><span>|</span><a href="#42143930">prev</a><span>|</span><a href="#42140872">next</a><span>|</span><label class="collapse" for="c-42141703">[-]</label><label class="expand" for="c-42141703">[1 more]</label></div><br/><div class="children"><div class="content">Even if you&#x27;re right (you&#x27;re not) whatever &quot;AI&quot; looks like in 20+ years will have virtually nothing in common with these stupid statistical word generators.</div><br/></div></div><div id="42140872" class="c"><input type="checkbox" id="c-42140872" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#42140562">parent</a><span>|</span><a href="#42141703">prev</a><span>|</span><a href="#42139479">next</a><span>|</span><label class="collapse" for="c-42140872">[-]</label><label class="expand" for="c-42140872">[1 more]</label></div><br/><div class="children"><div class="content">AI can be thought of as the 2nd stage of the creature that we call the Internet. The 1st stage, that we are so familiar with, is about gathering knowledge into a giant and somewhat organized library. This library has books on every subject imaginable, but its scale is so vast that no living human today can grasp it. This is why the originally connected network has started falling apart. Once this I becomes AI, all the books in the library will be melted together into one coherent picture. Once again, anyone anywhere on Earth will be able to access all the knowledge and our Babylon will stay for a little longer.</div><br/></div></div></div></div><div id="42139479" class="c"><input type="checkbox" id="c-42139479" checked=""/><div class="controls bullet"><span class="by">kklisura</span><span>|</span><a href="#42140562">prev</a><span>|</span><a href="#42144592">next</a><span>|</span><label class="collapse" for="c-42139479">[-]</label><label class="expand" for="c-42139479">[7 more]</label></div><br/><div class="children"><div class="content">Not sure if related or not, Sam Altman, ~12hrs ago: there is no wall [1]<p>[1] <a href="https:&#x2F;&#x2F;x.com&#x2F;sama&#x2F;status&#x2F;1856941766915641580" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;sama&#x2F;status&#x2F;1856941766915641580</a></div><br/><div id="42139775" class="c"><input type="checkbox" id="c-42139775" checked=""/><div class="controls bullet"><span class="by">ablation</span><span>|</span><a href="#42139479">parent</a><span>|</span><a href="#42144724">next</a><span>|</span><label class="collapse" for="c-42139775">[-]</label><label class="expand" for="c-42139775">[2 more]</label></div><br/><div class="children"><div class="content">Breaking: Man says enigmatic thing to sustain hype and flow of money into his business.</div><br/><div id="42141431" class="c"><input type="checkbox" id="c-42141431" checked=""/><div class="controls bullet"><span class="by">methodical</span><span>|</span><a href="#42139479">root</a><span>|</span><a href="#42139775">parent</a><span>|</span><a href="#42144724">next</a><span>|</span><label class="collapse" for="c-42141431">[-]</label><label class="expand" for="c-42141431">[1 more]</label></div><br/><div class="children"><div class="content">Ditto- I have a feeling the investors in his latest 2.3 quintillion dollar series Z round wouldn&#x27;t be as happy if he&#x27;d have tweeted &quot;there is a wall&quot;</div><br/></div></div></div></div><div id="42144724" class="c"><input type="checkbox" id="c-42144724" checked=""/><div class="controls bullet"><span class="by">malthaus</span><span>|</span><a href="#42139479">parent</a><span>|</span><a href="#42139775">prev</a><span>|</span><a href="#42141893">next</a><span>|</span><label class="collapse" for="c-42144724">[-]</label><label class="expand" for="c-42144724">[1 more]</label></div><br/><div class="children"><div class="content">if my billion net worth were coupled to that being the case i&#x27;d tweet that as well</div><br/></div></div><div id="42141893" class="c"><input type="checkbox" id="c-42141893" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42139479">parent</a><span>|</span><a href="#42144724">prev</a><span>|</span><a href="#42142881">next</a><span>|</span><label class="collapse" for="c-42141893">[-]</label><label class="expand" for="c-42141893">[1 more]</label></div><br/><div class="children"><div class="content">Altman on twitter has always been less coherent than GPT2.</div><br/></div></div><div id="42142881" class="c"><input type="checkbox" id="c-42142881" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#42139479">parent</a><span>|</span><a href="#42141893">prev</a><span>|</span><a href="#42141621">next</a><span>|</span><label class="collapse" for="c-42142881">[-]</label><label class="expand" for="c-42142881">[1 more]</label></div><br/><div class="children"><div class="content">My interpretation of that tweet is &quot;there is no DATA wall&quot; meaning &quot;we have so much more data we can ingest: all of youtube, all of spotify, all of twitch, every real-time webcam feed on the internet, RL agents playing every video game on steam, and we can extract so much more learning per unit data than we are now&quot; which seems plausible enough to me.</div><br/></div></div></div></div><div id="42144592" class="c"><input type="checkbox" id="c-42144592" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#42139479">prev</a><span>|</span><a href="#42139375">next</a><span>|</span><label class="collapse" for="c-42144592">[-]</label><label class="expand" for="c-42144592">[1 more]</label></div><br/><div class="children"><div class="content">It has to be a good thing to stop here. We can focus on improving what we have right now. The whole stack of models is an amazing innovation no matter what. It shouldn&#x27;t hurt if we pause here for a while and try to build on this or improve this.<p>It will be like StableDiffusion 1.5. This model can now run on low end devices, lots of open research use this model to build something else and inspire by this.<p>These LLMs can be used as a foundation to keep improving and building new things.</div><br/></div></div><div id="42139375" class="c"><input type="checkbox" id="c-42139375" checked=""/><div class="controls bullet"><span class="by">pluc</span><span>|</span><a href="#42144592">prev</a><span>|</span><a href="#42142048">next</a><span>|</span><label class="collapse" for="c-42139375">[-]</label><label class="expand" for="c-42139375">[15 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve simply run out of data to use to fabricate legitimate-looking guesses. They can&#x27;t create anything that doesn&#x27;t already exist.</div><br/><div id="42141114" class="c"><input type="checkbox" id="c-42141114" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#42139375">parent</a><span>|</span><a href="#42141590">next</a><span>|</span><label class="collapse" for="c-42141114">[-]</label><label class="expand" for="c-42141114">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They can&#x27;t create anything that doesn&#x27;t already exist.<p>I probably disagree, but I don&#x27;t want to criticize my interpretation of this sentence. Can you make your claim more precise?<p>Here are some possible claims and refutations:<p>- Claim: An LLM cannot output a true claim that it has not already seen. Refutation: LLMs have been shown to do logical reasoning.<p>- Claim: An LLM cannot incorporate data that it hasn&#x27;t been presented with. Refutation: This is an unfair standard. All forms of intelligence have to sense data from the world somehow.</div><br/></div></div><div id="42141590" class="c"><input type="checkbox" id="c-42141590" checked=""/><div class="controls bullet"><span class="by">mtkd</span><span>|</span><a href="#42139375">parent</a><span>|</span><a href="#42141114">prev</a><span>|</span><a href="#42139490">next</a><span>|</span><label class="collapse" for="c-42141590">[-]</label><label class="expand" for="c-42141590">[2 more]</label></div><br/><div class="children"><div class="content">And that is potentially only going to worsen as:<p>1. more data gets walled-off as owners realise value<p>2. stackoverflow-type feedback loops cease to exist as few people ask a public question and get public answers ... they ask a model privately and get an answer based on last visible public solutions<p>3. bad actors start deliberately trying to poison inputs (if sites served malicious responses to GPTBot&#x2F;CCBot crawlers only, would we even know right now?)<p>4. more and more content becomes synthetically generated to the point pre-2023 physical books become the last-known-good knowledge<p>5. goverments and IP lawyers finally catch up</div><br/><div id="42141909" class="c"><input type="checkbox" id="c-42141909" checked=""/><div class="controls bullet"><span class="by">77pt77</span><span>|</span><a href="#42139375">root</a><span>|</span><a href="#42141590">parent</a><span>|</span><a href="#42139490">next</a><span>|</span><label class="collapse" for="c-42141909">[-]</label><label class="expand" for="c-42141909">[1 more]</label></div><br/><div class="children"><div class="content">&gt; more data gets walled-off as owners realize value<p>What&#x27;s amazing to me to is that no one is throwing accusations of plagiarism.<p>I still think that if the &quot;wrong people&quot; had tried doing this they would have been obliterated by the courts.</div><br/></div></div></div></div><div id="42139490" class="c"><input type="checkbox" id="c-42139490" checked=""/><div class="controls bullet"><span class="by">readyplayernull</span><span>|</span><a href="#42139375">parent</a><span>|</span><a href="#42141590">prev</a><span>|</span><a href="#42141125">next</a><span>|</span><label class="collapse" for="c-42139490">[-]</label><label class="expand" for="c-42139490">[3 more]</label></div><br/><div class="children"><div class="content">Garbage-in was depleted.</div><br/><div id="42139588" class="c"><input type="checkbox" id="c-42139588" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#42139375">root</a><span>|</span><a href="#42139490">parent</a><span>|</span><a href="#42139607">next</a><span>|</span><label class="collapse" for="c-42139588">[-]</label><label class="expand" for="c-42139588">[1 more]</label></div><br/><div class="children"><div class="content">Exactly<p>And our current AI is just pattern based intelligence based off of all human intelligence, some of that not being real intelligent data sources</div><br/></div></div><div id="42139607" class="c"><input type="checkbox" id="c-42139607" checked=""/><div class="controls bullet"><span class="by">thechao</span><span>|</span><a href="#42139375">root</a><span>|</span><a href="#42139490">parent</a><span>|</span><a href="#42139588">prev</a><span>|</span><a href="#42141125">next</a><span>|</span><label class="collapse" for="c-42139607">[-]</label><label class="expand" for="c-42139607">[1 more]</label></div><br/><div class="children"><div class="content">The great AI garbage gyre?</div><br/></div></div></div></div><div id="42141125" class="c"><input type="checkbox" id="c-42141125" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#42139375">parent</a><span>|</span><a href="#42139490">prev</a><span>|</span><a href="#42140441">next</a><span>|</span><label class="collapse" for="c-42141125">[-]</label><label class="expand" for="c-42141125">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They&#x27;ve simply run out of data<p>Why do you think &quot;they&quot; have run out of data? First, to be clear, who do you mean by &quot;they&quot;? The world is filled with information sources (data aggregators for example), each available to some degree for some cost.<p>Don&#x27;t forget to include data that humans provide while interacting with chatbots.</div><br/></div></div><div id="42140441" class="c"><input type="checkbox" id="c-42140441" checked=""/><div class="controls bullet"><span class="by">whazor</span><span>|</span><a href="#42139375">parent</a><span>|</span><a href="#42141125">prev</a><span>|</span><a href="#42141888">next</a><span>|</span><label class="collapse" for="c-42140441">[-]</label><label class="expand" for="c-42140441">[5 more]</label></div><br/><div class="children"><div class="content">But a LLM can certainly make up a lot information that never existed before.</div><br/><div id="42141540" class="c"><input type="checkbox" id="c-42141540" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#42139375">root</a><span>|</span><a href="#42140441">parent</a><span>|</span><a href="#42142063">next</a><span>|</span><label class="collapse" for="c-42141540">[-]</label><label class="expand" for="c-42141540">[3 more]</label></div><br/><div class="children"><div class="content">I strongly believe this gets into an information theoretical constraint akin to why perpetual motion machines don&#x27;t work.<p>In theory, yes you could generate an unlimited amount of data for the models, but how much of it is unique or <i>valuable</i> information? If you were to compress all this generated training data using a really good algorithm, how much actual information remains?</div><br/><div id="42141948" class="c"><input type="checkbox" id="c-42141948" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42139375">root</a><span>|</span><a href="#42141540">parent</a><span>|</span><a href="#42141792">next</a><span>|</span><label class="collapse" for="c-42141948">[-]</label><label class="expand" for="c-42141948">[1 more]</label></div><br/><div class="children"><div class="content">I make a lot of shitposts, how much of that is valuable information? Arguably not much. I doubt information value is a good way to estimate inteligence because most people&#x27;s daily ramblings would grade them useless.</div><br/></div></div><div id="42141792" class="c"><input type="checkbox" id="c-42141792" checked=""/><div class="controls bullet"><span class="by">cruffle_duffle</span><span>|</span><a href="#42139375">root</a><span>|</span><a href="#42141540">parent</a><span>|</span><a href="#42141948">prev</a><span>|</span><a href="#42142063">next</a><span>|</span><label class="collapse" for="c-42141792">[-]</label><label class="expand" for="c-42141792">[1 more]</label></div><br/><div class="children"><div class="content">I sure hope there is some bright eyed bushy tailed graduate students crafting up some theorem to prove this. Because it is absolutely a feedback loop.<p>... that being said I&#x27;m sure there is plenty of additional &quot;real data&quot; that hasn&#x27;t been fed to these models yet. For one thing, I think ChatGPT sucks so bad at terraform because almost all the &quot;real code&quot; to train on is locked behind private repositories. There isn&#x27;t much publicly available real-world terraform projects to train on. Same with a lot of other similar languages and tools -- a lot of that knowledge is locked away as trade secrets and hidden in private document stores.<p>(that being said Sonnet 3.5 is much, much, much better at terraform than chatgpt. It&#x27;s much better at coding in general but it&#x27;s night and day for terraform)</div><br/></div></div></div></div></div></div><div id="42141888" class="c"><input type="checkbox" id="c-42141888" checked=""/><div class="controls bullet"><span class="by">77pt77</span><span>|</span><a href="#42139375">parent</a><span>|</span><a href="#42140441">prev</a><span>|</span><a href="#42142048">next</a><span>|</span><label class="collapse" for="c-42141888">[-]</label><label class="expand" for="c-42141888">[2 more]</label></div><br/><div class="children"><div class="content">&gt; They can&#x27;t create anything that doesn&#x27;t already exist.<p>Just increase the temperature.</div><br/><div id="42142543" class="c"><input type="checkbox" id="c-42142543" checked=""/><div class="controls bullet"><span class="by">dcl</span><span>|</span><a href="#42139375">root</a><span>|</span><a href="#42141888">parent</a><span>|</span><a href="#42142048">next</a><span>|</span><label class="collapse" for="c-42142543">[-]</label><label class="expand" for="c-42142543">[1 more]</label></div><br/><div class="children"><div class="content">That just makes it more likely to sample less likely outcomes from the same distribution. No real novelty.</div><br/></div></div></div></div></div></div><div id="42142048" class="c"><input type="checkbox" id="c-42142048" checked=""/><div class="controls bullet"><span class="by">sssilver</span><span>|</span><a href="#42139375">prev</a><span>|</span><a href="#42139106">next</a><span>|</span><label class="collapse" for="c-42142048">[-]</label><label class="expand" for="c-42142048">[2 more]</label></div><br/><div class="children"><div class="content">One thing that makes the established AIs less ideal for my (programming) use-case is that the technologies I use quickly evolve past whatever the published models &quot;learn&quot;.<p>On the other hand, a lot of these frameworks and languages have relatively decent and detailed documentation.<p>Perhaps this is a naive question, but why can&#x27;t I as a user just purchase &quot;AI software&quot; that comes with a large pre-trained model to which I can say, on my own machine, &quot;go read this documentation and help me write this app in this next version of Leptos&quot;, and it would augment its existing model with this new &quot;knowledge&quot;.</div><br/><div id="42144812" class="c"><input type="checkbox" id="c-42144812" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#42142048">parent</a><span>|</span><a href="#42139106">next</a><span>|</span><label class="collapse" for="c-42144812">[-]</label><label class="expand" for="c-42144812">[1 more]</label></div><br/><div class="children"><div class="content">Pretraining or even post-training is cumbersome, complex and expensive. What is easy and cheap is in-context learning, which is why I just pull in the documentation I need the LLM to know about into the LLM&#x27;s context.</div><br/></div></div></div></div><div id="42139106" class="c"><input type="checkbox" id="c-42139106" checked=""/><div class="controls bullet"><span class="by">irrational</span><span>|</span><a href="#42142048">prev</a><span>|</span><a href="#42139224">next</a><span>|</span><label class="collapse" for="c-42139106">[-]</label><label class="expand" for="c-42139106">[104 more]</label></div><br/><div class="children"><div class="content">&gt; The AGI bubble is bursting a little bit<p>I&#x27;m surprised that any of these companies consider what they are working on to be Artificial General Intelligences. I&#x27;m probably wrong, but my impression was AGI meant the AI is self aware like a human. An LLM hardly seems like something that will lead to self-awareness.</div><br/><div id="42139186" class="c"><input type="checkbox" id="c-42139186" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139338">next</a><span>|</span><label class="collapse" for="c-42139186">[-]</label><label class="expand" for="c-42139186">[28 more]</label></div><br/><div class="children"><div class="content">Whether self awareness is a requirement for AGI definitely gets more into the Philosophy department than the Computer Science department.  I&#x27;m not sure everyone even agrees on what AGI is, but a common test is &quot;can it do what humans can&quot;.<p>For example, in this article it says it can&#x27;t do coding exercises outside the training set.  That would definitely be on the &quot;AGI checklist&quot;.  Basically doing anything that is outside of the training set would be on that list.</div><br/><div id="42139703" class="c"><input type="checkbox" id="c-42139703" checked=""/><div class="controls bullet"><span class="by">norir</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139186">parent</a><span>|</span><a href="#42139671">next</a><span>|</span><label class="collapse" for="c-42139703">[-]</label><label class="expand" for="c-42139703">[8 more]</label></div><br/><div class="children"><div class="content">Here is an example of a task that I do not believe this generation of LLMs can ever do but that is possible for a human: design a Turing complete programming language that is both human and machine readable and implement a self hosted compiler in this language that self compiles on existing hardware faster than any known language implementation that also self compiles. Additionally, for any syntactically or semantically invalid program, the compiler must provide an error message that points exactly to the source location of the first error that occurs in the program.<p>I will get excited for&#x2F;scared of LLMs when they can tackle this kind of problem. But I don&#x27;t believe they can because of the fundamental nature of their design, which is both backward looking (thus not better than the human state of the art) and lacks human intuition and self awareness. Or perhaps rather I believe that the prompt that would be required to get an LLM to produce such a program is a problem of at least equivalent complexity to implementing the program without an LLM.</div><br/><div id="42141654" class="c"><input type="checkbox" id="c-42141654" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139703">parent</a><span>|</span><a href="#42140363">next</a><span>|</span><label class="collapse" for="c-42141654">[-]</label><label class="expand" for="c-42141654">[1 more]</label></div><br/><div class="children"><div class="content">This sounds like something more up the alley of linear genetic programming. There are some very interesting experiments out there that utilize UTMs (BrainFuck, Forth, et. al.) [0,1,2].<p>I&#x27;ve personally had some mild success getting these UTM variants to output their own children in a meta programming arrangement. The base program only has access to the valid instruction set of ~12 instructions per byte, while the task program has access to the full range of instructions and data per byte (256). By only training the base program, we reduce the search space by a very substantial factor. I think this would be similar to the idea of a self-hosted compiler, etc. I don&#x27;t think there would be too much of a stretch to give it access to x86 instructions and a full VM once a certain amount of bootstrapping has been achieved.<p>[0]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.19108" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.19108</a><p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;kurtjd&#x2F;brainfuck-evolved">https:&#x2F;&#x2F;github.com&#x2F;kurtjd&#x2F;brainfuck-evolved</a><p>[2]: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36120286">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36120286</a></div><br/></div></div><div id="42140363" class="c"><input type="checkbox" id="c-42140363" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139703">parent</a><span>|</span><a href="#42141654">prev</a><span>|</span><a href="#42141652">next</a><span>|</span><label class="collapse" for="c-42140363">[-]</label><label class="expand" for="c-42140363">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Here is an example of a task that I do not believe this generation of LLMs can ever do but that is possible for a human<p>That’s possible for a highly intelligent, extensively trained, very small subset of humans.</div><br/><div id="42140903" class="c"><input type="checkbox" id="c-42140903" checked=""/><div class="controls bullet"><span class="by">hatefulmoron</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140363">parent</a><span>|</span><a href="#42141088">next</a><span>|</span><label class="collapse" for="c-42140903">[-]</label><label class="expand" for="c-42140903">[1 more]</label></div><br/><div class="children"><div class="content">If you took the intersection of every human&#x27;s abilities you&#x27;d be left with a very unimpressive set.<p>That also ignores the fact that the small set of humans capable of building programming languages and compilers is a consequence of specialization and lack of interest. There are plenty of humans that are capable of learning how to do it. LLMs, on the other hand, are both specialized for the task and aren&#x27;t lazy or uninterested.</div><br/></div></div><div id="42141088" class="c"><input type="checkbox" id="c-42141088" checked=""/><div class="controls bullet"><span class="by">luckydata</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140363">parent</a><span>|</span><a href="#42140903">prev</a><span>|</span><a href="#42141652">next</a><span>|</span><label class="collapse" for="c-42141088">[-]</label><label class="expand" for="c-42141088">[2 more]</label></div><br/><div class="children"><div class="content">does it mean people that can build languages and compilers are not humans? What is the point you&#x27;re trying to make?</div><br/><div id="42141178" class="c"><input type="checkbox" id="c-42141178" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141088">parent</a><span>|</span><a href="#42141652">next</a><span>|</span><label class="collapse" for="c-42141178">[-]</label><label class="expand" for="c-42141178">[1 more]</label></div><br/><div class="children"><div class="content">It means that&#x27;s a really high bar for intelligence, human or otherwise. If AGI is &quot;as good as a human, and the test is a trick task that most humans would fail at (especially considering the weasel requirement that it additionally has to be faster), why is that considered a reasonable bar for human-grade intelligence.</div><br/></div></div></div></div></div></div><div id="42141652" class="c"><input type="checkbox" id="c-42141652" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139703">parent</a><span>|</span><a href="#42140363">prev</a><span>|</span><a href="#42139671">next</a><span>|</span><label class="collapse" for="c-42141652">[-]</label><label class="expand" for="c-42141652">[2 more]</label></div><br/><div class="children"><div class="content">I will get excited when an LLM (or whatever technology is next) can solve tasks that 80%+ of adult humans can solve.  Heck let&#x27;s even say 80% of college graduates to make it harder.<p>Things like drive a car, fold laundry, run an errand, do some basic math.<p>You&#x27;ll notice that two of those require some form of robot or mobility.  I think that is key -- you can&#x27;t have AGI without the ability to interact with the world in a way similar to most humans.</div><br/><div id="42141904" class="c"><input type="checkbox" id="c-42141904" checked=""/><div class="controls bullet"><span class="by">ata_aman</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141652">parent</a><span>|</span><a href="#42139671">next</a><span>|</span><label class="collapse" for="c-42141904">[-]</label><label class="expand" for="c-42141904">[1 more]</label></div><br/><div class="children"><div class="content">So embodied cognition right?</div><br/></div></div></div></div></div></div><div id="42139671" class="c"><input type="checkbox" id="c-42139671" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139186">parent</a><span>|</span><a href="#42139703">prev</a><span>|</span><a href="#42141257">next</a><span>|</span><label class="collapse" for="c-42139671">[-]</label><label class="expand" for="c-42139671">[5 more]</label></div><br/><div class="children"><div class="content">Let me modify that a little, because <i>humans</i> can&#x27;t do things outside their training set either.<p>A crucial element of AGI would be the ability to self-train on self-generated data, online. So it&#x27;s not really AGI if there is a hard distinction between training and inference (though it may still be very capable), and it&#x27;s not really AGI if it can&#x27;t work its way through novel problems on its own.<p>The ability to immediately solve a problem it&#x27;s never seen before is too high a bar, I think.<p>And yes, my definition still excludes a lot of humans in a lot of fields. That&#x27;s a bullet I&#x27;m willing to bite.</div><br/><div id="42140807" class="c"><input type="checkbox" id="c-42140807" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139671">parent</a><span>|</span><a href="#42140011">next</a><span>|</span><label class="collapse" for="c-42140807">[-]</label><label class="expand" for="c-42140807">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Let me modify that a little, because humans can&#x27;t do things outside their training set either.<p>That&#x27;s not true. Humans can learn.<p>An LLM is just a tool. If it can&#x27;t do what you want then too bad.</div><br/></div></div><div id="42140011" class="c"><input type="checkbox" id="c-42140011" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139671">parent</a><span>|</span><a href="#42140807">prev</a><span>|</span><a href="#42141257">next</a><span>|</span><label class="collapse" for="c-42140011">[-]</label><label class="expand" for="c-42140011">[3 more]</label></div><br/><div class="children"><div class="content">Are you arguing that writing, doing math, going to the moon etc. were all in the &quot;original training set&quot; of humans in some way?</div><br/><div id="42140169" class="c"><input type="checkbox" id="c-42140169" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140011">parent</a><span>|</span><a href="#42141257">next</a><span>|</span><label class="collapse" for="c-42140169">[-]</label><label class="expand" for="c-42140169">[2 more]</label></div><br/><div class="children"><div class="content">Not in the <i>original</i> training set (GP is saying), but the necessary skills became  part of the training set over time. In other words, human are fine with the training set being a changing moving target, whereas ML models are to a significant extent “stuck” with their original training set.<p>(That’s not to say that humans don’t tend to lose some of their flexibility over their individual lifetimes as well.)</div><br/><div id="42143746" class="c"><input type="checkbox" id="c-42143746" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140169">parent</a><span>|</span><a href="#42141257">next</a><span>|</span><label class="collapse" for="c-42143746">[-]</label><label class="expand" for="c-42143746">[1 more]</label></div><br/><div class="children"><div class="content">&gt; (That’s not to say that humans don’t tend to lose some of their flexibility over their individual lifetimes as well.)<p>The lifetime is the context window, the model&#x2F;training is the DNA. A human in the moment isn&#x27;t general intelligent, but a human over his lifetime is, the first is so much easier to try to replicate though but that is a bad target since humans aren&#x27;t born like that.</div><br/></div></div></div></div></div></div></div></div><div id="42141257" class="c"><input type="checkbox" id="c-42141257" checked=""/><div class="controls bullet"><span class="by">olalonde</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139186">parent</a><span>|</span><a href="#42139671">prev</a><span>|</span><a href="#42139946">next</a><span>|</span><label class="collapse" for="c-42141257">[-]</label><label class="expand" for="c-42141257">[6 more]</label></div><br/><div class="children"><div class="content">I feel the test for AGI should be more like: &quot;go find a job and earn money&quot; or &quot;start a profitable business&quot; or &quot;pick a bachelor degree and complete it&quot;, etc.</div><br/><div id="42144147" class="c"><input type="checkbox" id="c-42144147" checked=""/><div class="controls bullet"><span class="by">eichi</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141257">parent</a><span>|</span><a href="#42141439">next</a><span>|</span><label class="collapse" for="c-42144147">[-]</label><label class="expand" for="c-42144147">[1 more]</label></div><br/><div class="children"><div class="content">This is people&#x27;s true desire. Make something like that while handling critisisms and fitting products to the market.</div><br/></div></div><div id="42141439" class="c"><input type="checkbox" id="c-42141439" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141257">parent</a><span>|</span><a href="#42144147">prev</a><span>|</span><a href="#42141334">next</a><span>|</span><label class="collapse" for="c-42141439">[-]</label><label class="expand" for="c-42141439">[1 more]</label></div><br/><div class="children"><div class="content">Can most humans do that?  Find a job and earn money, probably. The other two?  Not so much.</div><br/></div></div><div id="42141334" class="c"><input type="checkbox" id="c-42141334" checked=""/><div class="controls bullet"><span class="by">rodgerd</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141257">parent</a><span>|</span><a href="#42141439">prev</a><span>|</span><a href="#42139946">next</a><span>|</span><label class="collapse" for="c-42141334">[-]</label><label class="expand" for="c-42141334">[3 more]</label></div><br/><div class="children"><div class="content">An LLM doing crypto spam&#x2F;scamming has been making money by tricking Marc Andressen into boosting it. So to the degree that &quot;scamming gullible billionaires and their fans&quot; is a job, that&#x27;s been done.</div><br/><div id="42141664" class="c"><input type="checkbox" id="c-42141664" checked=""/><div class="controls bullet"><span class="by">olalonde</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141334">parent</a><span>|</span><a href="#42141411">next</a><span>|</span><label class="collapse" for="c-42141664">[-]</label><label class="expand" for="c-42141664">[1 more]</label></div><br/><div class="children"><div class="content">That story was a bit blown out of proportion. He gave a research grant to the bot&#x27;s creator: <a href="https:&#x2F;&#x2F;x.com&#x2F;pmarca&#x2F;status&#x2F;1846374466101944629" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;pmarca&#x2F;status&#x2F;1846374466101944629</a></div><br/></div></div><div id="42141411" class="c"><input type="checkbox" id="c-42141411" checked=""/><div class="controls bullet"><span class="by">rsanek</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141334">parent</a><span>|</span><a href="#42141664">prev</a><span>|</span><a href="#42139946">next</a><span>|</span><label class="collapse" for="c-42141411">[-]</label><label class="expand" for="c-42141411">[1 more]</label></div><br/><div class="children"><div class="content">source? didn&#x27;t find anything online about this.</div><br/></div></div></div></div></div></div><div id="42139946" class="c"><input type="checkbox" id="c-42139946" checked=""/><div class="controls bullet"><span class="by">sourcepluck</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139186">parent</a><span>|</span><a href="#42141257">prev</a><span>|</span><a href="#42139314">next</a><span>|</span><label class="collapse" for="c-42139946">[-]</label><label class="expand" for="c-42139946">[3 more]</label></div><br/><div class="children"><div class="content">Searle&#x27;s Chinese Room Argument springs to mind:<p><pre><code>  https:&#x2F;&#x2F;plato.stanford.edu&#x2F;entries&#x2F;chinese-room&#x2F;
</code></pre>
The idea that &quot;human-like&quot; behaviour will lead to self-awareness is both unproven (it can&#x27;t be proven until it happens) and impossible to disprove (like Russell&#x27;s teapot).<p>Yet, one common assumption of many people running these companies or investing in them, or of some developers investing their time in these technologies, is precisely that some sort of explosion of superintelligence is likely, or even inevitable.<p>It surely is <i>possible</i>, but stretching that to <i>likely</i> seems a bit much if you really think how imperfectly we understand things like consciousness and the mind.<p>Of course there are people who have essentially religious reactions to the notion that there may be limits to certain domains of knowledge. Nonetheless, I think that&#x27;s the reality we&#x27;re faced with here.</div><br/><div id="42140395" class="c"><input type="checkbox" id="c-42140395" checked=""/><div class="controls bullet"><span class="by">abeppu</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139946">parent</a><span>|</span><a href="#42139314">next</a><span>|</span><label class="collapse" for="c-42140395">[-]</label><label class="expand" for="c-42140395">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The idea that &quot;human-like&quot; behaviour will lead to self-awareness is both unproven (it can&#x27;t be proven until it happens) and impossible to disprove (like Russell&#x27;s teapot).<p>I think Searle&#x27;s view was that:<p>- while it cannot be dis-_proven_, the Chinese Room argument was meant to provide reasons against believing it<p>- the &quot;it can&#x27;t be proven until it happens&quot; part is misunderstanding: you won&#x27;t <i>know</i> if it happens because the objective, externally available attributes don&#x27;t indicate whether self-awareness (or indeed awareness at all) is present</div><br/><div id="42141503" class="c"><input type="checkbox" id="c-42141503" checked=""/><div class="controls bullet"><span class="by">sourcepluck</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140395">parent</a><span>|</span><a href="#42139314">next</a><span>|</span><label class="collapse" for="c-42141503">[-]</label><label class="expand" for="c-42141503">[1 more]</label></div><br/><div class="children"><div class="content">The short version of this is that I don&#x27;t disagree with your interpretation of Searle, and my paragraphs immediately following the link weren&#x27;t meant to be a direct description of his point with the Chinese Room thought experiment.<p>&gt; while it cannot be dis-_proven_, the Chinese Room argument was meant to provide reasons against believing it<p>Yes, like Russell&#x27;s teapot. I also think that&#x27;s what Searle means.<p>&gt; the &quot;it can&#x27;t be proven until it happens&quot; part is misunderstanding: you won&#x27;t know if it happens because the objective, externally available attributes don&#x27;t indicate whether self-awareness (or indeed awareness at all) is present<p>Yes, agreed, I believe that&#x27;s what Searle is saying too. I think I was maybe being ambiguous here - I wanted to say that even if you forgave the AI maximalists for ignoring all relevant philosophical work, the notion that &quot;appearing human-like&quot; inevitably tends to what would actually <i>be</i> &quot;consciousness&quot; or &quot;intelligence&quot; is more than a big claim.<p>Searle goes further, and I&#x27;m not sure if I follow him all the way, personally, but it&#x27;s a side point.</div><br/></div></div></div></div></div></div><div id="42139314" class="c"><input type="checkbox" id="c-42139314" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139186">parent</a><span>|</span><a href="#42139946">prev</a><span>|</span><a href="#42139338">next</a><span>|</span><label class="collapse" for="c-42139314">[-]</label><label class="expand" for="c-42139314">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Whether self awareness is a requirement for AGI definitely gets more into the Philosophy department than the Computer Science department.<p>Depends on how you define “self awareness” but knowing that it doesn&#x27;t know something instead of hallucinating a plausible-but-wrong is already self awareness of some kind. And it&#x27;s both highly valuable and beyond current tech&#x27;s capability.</div><br/><div id="42139395" class="c"><input type="checkbox" id="c-42139395" checked=""/><div class="controls bullet"><span class="by">sharemywin</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139314">parent</a><span>|</span><a href="#42141969">next</a><span>|</span><label class="collapse" for="c-42139395">[-]</label><label class="expand" for="c-42139395">[1 more]</label></div><br/><div class="children"><div class="content">This is an interesting paper about hallucinations.<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;introducing-simpleqa&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;introducing-simpleqa&#x2F;</a><p>especially this section
Using SimpleQA to measure the calibration of large language models</div><br/></div></div><div id="42141969" class="c"><input type="checkbox" id="c-42141969" checked=""/><div class="controls bullet"><span class="by">lagrange77</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139314">parent</a><span>|</span><a href="#42139395">prev</a><span>|</span><a href="#42141680">next</a><span>|</span><label class="collapse" for="c-42141969">[-]</label><label class="expand" for="c-42141969">[2 more]</label></div><br/><div class="children"><div class="content">Good point!<p>I&#x27;m wondering wether it would count, if one would extend it with an external program, that gives it feedback during inference (by another prompt) about the correctness of it&#x27;s output.<p>I guess it wouldn&#x27;t, because these RAG tools kind of do that and i heard no one calling those self aware.</div><br/><div id="42145102" class="c"><input type="checkbox" id="c-42145102" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141969">parent</a><span>|</span><a href="#42141680">next</a><span>|</span><label class="collapse" for="c-42145102">[-]</label><label class="expand" for="c-42145102">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if one would extend it with an external program, that gives it feedback<p>If you have an external program, then by defining it&#x27;s not <i>self</i>-awareness ;). Also, it&#x27;s not about <i>correctness</i> per se, but about the model&#x27;s ability to assess its own knowledge (making a mistake because the model was exposed to mistakes in the training data is fine, hallucinating isn&#x27;t).</div><br/></div></div></div></div><div id="42141680" class="c"><input type="checkbox" id="c-42141680" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139314">parent</a><span>|</span><a href="#42141969">prev</a><span>|</span><a href="#42139338">next</a><span>|</span><label class="collapse" for="c-42141680">[-]</label><label class="expand" for="c-42141680">[1 more]</label></div><br/><div class="children"><div class="content">When we test kids to see if they are gifted, one of the criteria is that they have the ability to say &quot;I don&#x27;t know&quot;.<p>That is definitely an ability that current LLMs lack.</div><br/></div></div></div></div></div></div><div id="42139338" class="c"><input type="checkbox" id="c-42139338" checked=""/><div class="controls bullet"><span class="by">Fade_Dance</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139186">prev</a><span>|</span><a href="#42139138">next</a><span>|</span><label class="collapse" for="c-42139338">[-]</label><label class="expand" for="c-42139338">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an attention-grabbing term that took hold in pop culture and business. Certainly there is a subset of research around the subject of consciousness, but you are correct in saying that the majority of researchers in the field are not pursuing self-awareness and will be very blunt in saying that. If you step back a bit and say something like &quot;human-like, logical reasoning&quot;, that&#x27;s something you may find alignment with though. A general purpose logical reasoning engine does not necessarily need to be self-aware. The word &quot;Intelligent&quot; has stuck around because one of the core characteristics of this suite of technologies is that a sort of &quot;understanding&quot; emergently develops within these networks, sometimes in quite a startling fashion (due to the phenomenon of adding more data&#x2F;compute at first seemingly leading to overfitting, but then suddenly breaking through plateaus into more robust, general purpose understanding of the underlying relationships that drive the system it is analyzing.)<p>Is that &quot;intelligent&quot; or &quot;understanding&quot;? It&#x27;s probably close enough for pop science, and regardless, it looks good in headlines and sales pitches so why fight it?</div><br/></div></div><div id="42139138" class="c"><input type="checkbox" id="c-42139138" checked=""/><div class="controls bullet"><span class="by">Taylor_OD</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139338">prev</a><span>|</span><a href="#42139782">next</a><span>|</span><label class="collapse" for="c-42139138">[-]</label><label class="expand" for="c-42139138">[14 more]</label></div><br/><div class="children"><div class="content">I think your definition is off from what most people would define AGI as. Generally, it means being able to think and reason at a human level for a multitude&#x2F;all tasks or jobs.<p>&quot;Artificial General Intelligence (AGI) refers to a theoretical form of artificial intelligence that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks at a level comparable to that of a human being.&quot;<p>Altman says AGI could be here in 2025: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;xXCBz_8hM9w?si=F-vQXJgQvJKZH3fv" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;xXCBz_8hM9w?si=F-vQXJgQvJKZH3fv</a><p>But he certainly means an LLM that can perform at&#x2F;above human level in most tasks rather than a self aware entity.</div><br/><div id="42139669" class="c"><input type="checkbox" id="c-42139669" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139138">parent</a><span>|</span><a href="#42139407">next</a><span>|</span><label class="collapse" for="c-42139669">[-]</label><label class="expand" for="c-42139669">[4 more]</label></div><br/><div class="children"><div class="content">On the contrary, I think you&#x27;re conflating the narrow jargon of the industry with what &quot;most people&quot; would define.<p>&quot;Most people&quot; naturally associate AGI with the sci-tropes of self-aware human-like agents.<p>But industries want something more concrete and prospectively-acheivable in their jargon, and so <i>that&#x27;s</i> where AGI gets redefined as wide task suitability.<p>And while that&#x27;s not an unreasonable definition in the context of the industry, it&#x27;s one that vanishingly few people are actually familiar with.<p>And the commercial AI vendors benefit greatly from allowing those two usages to conflate in the minds of as many people as possible, as it lets them <i>suggest</i> grand claims while keeping a rhetorical &quot;we obviously never meant <i>that</i>!&quot; in their back pocket</div><br/><div id="42141180" class="c"><input type="checkbox" id="c-42141180" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139669">parent</a><span>|</span><a href="#42140855">next</a><span>|</span><label class="collapse" for="c-42141180">[-]</label><label class="expand" for="c-42141180">[1 more]</label></div><br/><div class="children"><div class="content">&gt;But industries want something more concrete and prospectively-acheivable in their jargon, and so that&#x27;s where AGI gets redefined as wide task suitability.<p>The term itself (AGI) in the industry has always been about wide task suitability. People may have added their ifs and buts over the years but that aspect of it never got &#x27;redefined&#x27;. The earliest uses of the term all talk about how well a machine would be able to perform some set number of tasks at some threshold.<p>It&#x27;s no wonder why. Terms like &quot;consciousness&quot; and &quot;self-awareness&quot; are completely useless. It&#x27;s not about difficulty. It&#x27;s that you can&#x27;t do anything at all with those terms except argue around in circles.</div><br/></div></div><div id="42140855" class="c"><input type="checkbox" id="c-42140855" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139669">parent</a><span>|</span><a href="#42141180">prev</a><span>|</span><a href="#42139407">next</a><span>|</span><label class="collapse" for="c-42140855">[-]</label><label class="expand" for="c-42140855">[2 more]</label></div><br/><div class="children"><div class="content">There is no single definition, let alone a way to measure, of self awareness nor of reasoning.<p>Because of that, the discussion of what AGI means in its broadest sense, will never end.<p>So in fact such AGI discussion will not make nobody wiser.</div><br/><div id="42141612" class="c"><input type="checkbox" id="c-42141612" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140855">parent</a><span>|</span><a href="#42139407">next</a><span>|</span><label class="collapse" for="c-42141612">[-]</label><label class="expand" for="c-42141612">[1 more]</label></div><br/><div class="children"><div class="content">I agree there&#x27;s no single definition, but I think they <i>all</i> have something current LLM don&#x27;t: the ability to learn new things, in a persistent way, with few shots.<p>I would argue that learning <i>is</i> The definition of AGI, since everything else comes naturally from that.<p>The current architectures can&#x27;t learn without retraining, fine tuning is at the expense of general knowledge, and keeping things in context is <i>detrimental</i> to general performance. Once you have few shot learning, I think it&#x27;s more of a &quot;give it agency so it can explore&quot; type problem.</div><br/></div></div></div></div></div></div><div id="42139407" class="c"><input type="checkbox" id="c-42139407" checked=""/><div class="controls bullet"><span class="by">Avshalom</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139138">parent</a><span>|</span><a href="#42139669">prev</a><span>|</span><a href="#42139677">next</a><span>|</span><label class="collapse" for="c-42139407">[-]</label><label class="expand" for="c-42139407">[1 more]</label></div><br/><div class="children"><div class="content">Altman is marketing, he &quot;certainly means&quot; whatever he thinks his audience will buy.</div><br/></div></div><div id="42139677" class="c"><input type="checkbox" id="c-42139677" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139138">parent</a><span>|</span><a href="#42139407">prev</a><span>|</span><a href="#42139782">next</a><span>|</span><label class="collapse" for="c-42139677">[-]</label><label class="expand" for="c-42139677">[8 more]</label></div><br/><div class="children"><div class="content">&gt; than a self aware entity.<p>What does this mean? If I have a blind, deaf, paralyzed person, who could only communicate through text, what would the signs be that they were self aware?<p>Is this more of a feedback loop problem? If I let the LLM run in a loop, and tell it it&#x27;s talking to itself, would that be approaching &quot;self aware&quot;?</div><br/><div id="42140260" class="c"><input type="checkbox" id="c-42140260" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139677">parent</a><span>|</span><a href="#42139782">next</a><span>|</span><label class="collapse" for="c-42140260">[-]</label><label class="expand" for="c-42140260">[7 more]</label></div><br/><div class="children"><div class="content">Being aware of its own limitations, for example. Or being aware of how its utterances may come across to its interlocutor.<p>(And by limitations I don’t mean “sorry, I’m not allowed to help you with this dangerous&#x2F;contentious topic”.)</div><br/><div id="42140889" class="c"><input type="checkbox" id="c-42140889" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140260">parent</a><span>|</span><a href="#42141640">next</a><span>|</span><label class="collapse" for="c-42140889">[-]</label><label class="expand" for="c-42140889">[2 more]</label></div><br/><div class="children"><div class="content">There is no way of proving awareness in humans let alone machines. We do not even know whether awareness exists or it is just a word that people made up to describe some kind of feeling.</div><br/><div id="42142760" class="c"><input type="checkbox" id="c-42142760" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140889">parent</a><span>|</span><a href="#42141640">next</a><span>|</span><label class="collapse" for="c-42142760">[-]</label><label class="expand" for="c-42142760">[1 more]</label></div><br/><div class="children"><div class="content">Awareness is exhibited in behavior. It&#x27;s exactly due to the behavior be observe from LLMs that we don&#x27;t ascribe them awareness. I agree that it&#x27;s difficult to define, and it&#x27;s also not binary, but it&#x27;s behavior we&#x27;d like AI to have and which LLMs are quite lacking.</div><br/></div></div></div></div><div id="42141640" class="c"><input type="checkbox" id="c-42141640" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140260">parent</a><span>|</span><a href="#42140889">prev</a><span>|</span><a href="#42141298">next</a><span>|</span><label class="collapse" for="c-42141640">[-]</label><label class="expand" for="c-42141640">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Or being aware of how its utterances may come across to its interlocutor.<p>I think this behavior is being somewhat demonstrated in newer models. I&#x27;ve seen GPT-3.5 175B correct itself mid response with, almost literally:<p>&gt; &lt;answer with flaw here&gt;<p>&gt; Wait, that&#x27;s not right, that &lt;reason for flaw&gt;.<p>&gt; &lt;correct answer here&gt;.<p>Later models seem to have much more awareness of, or &quot;weight&quot; towards, their own responses, while generating the response.</div><br/><div id="42142851" class="c"><input type="checkbox" id="c-42142851" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141640">parent</a><span>|</span><a href="#42141298">next</a><span>|</span><label class="collapse" for="c-42142851">[-]</label><label class="expand" for="c-42142851">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m assuming the &quot;Wait&quot; sentence is from the user. What I mean is that when humans say something, they also tend to have a view (maybe via the famous mirror neurons) of how this now sounds to the other person. They may catch themselves while speaking, changing course mid-sentence, or adding another sentence to soften or highlight something in the previous sentence, or maybe correcting or admitting some aspect after the fact. LLMs don&#x27;t exhibit such an inner feedback loop, in which they reconsider the effect of the ouput they are in the process of generating.<p>You won&#x27;t get an LLM outputting &quot;wait, that&#x27;s not right&quot; halfway through their original output (unless you prompted them in a way that would trigger such a speech pattern), because no re-evaluation is taking place without further input.</div><br/></div></div></div></div><div id="42141298" class="c"><input type="checkbox" id="c-42141298" checked=""/><div class="controls bullet"><span class="by">revscat</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140260">parent</a><span>|</span><a href="#42141640">prev</a><span>|</span><a href="#42139782">next</a><span>|</span><label class="collapse" for="c-42141298">[-]</label><label class="expand" for="c-42141298">[2 more]</label></div><br/><div class="children"><div class="content">Plenty of humans, unfortunately, are incapable of admitting limitations. Many years ago I had a coworker who believed he would never die. At first I thought he was joking, but he was in fact quite serious.<p>Then there are those who are simply narcissistic, and cannot and will not admit fault regardless of the evidence presented them.</div><br/><div id="42142791" class="c"><input type="checkbox" id="c-42142791" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141298">parent</a><span>|</span><a href="#42139782">next</a><span>|</span><label class="collapse" for="c-42142791">[-]</label><label class="expand" for="c-42142791">[1 more]</label></div><br/><div class="children"><div class="content">Being aware and not admitting are two different things, though. When you confront an LLM with a limitation, it will generally admit having it. That doesn&#x27;t mean that it exhibits any awareness of having the limitation in contexts where the limitation is glaringly relevant, without first having confronted it with it. This is in itself a limitation of LLMs: In contexts where it should be highly obvious, they don&#x27;t take their limitations into account without specific prompting.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42139782" class="c"><input type="checkbox" id="c-42139782" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139138">prev</a><span>|</span><a href="#42139569">next</a><span>|</span><label class="collapse" for="c-42139782">[-]</label><label class="expand" for="c-42139782">[27 more]</label></div><br/><div class="children"><div class="content">I thought maybe they were on the right track until I read Attention Is All You Need.<p>Nah, at best we found a way to make one part of a collection of systems that will, together, do something like thinking. Thinking isn’t part of what this current approach does.<p>What’s most surprising about modern LLMs is that it turns out there is so much information statistically encoded in the <i>structure</i> of our writing that we can use only that structural information to build a fancy Plinko machine and not only will the output mimic recognizable grammar rules, but it will also sometimes seem to make actual sense, too—and the system <i>doesn’t need to think or actually “understand” anything</i> for us to, basically, usefully query that information that was always there in our corpus of literature, not in the plain meaning of the words, but in the structure of the writing.</div><br/><div id="42139993" class="c"><input type="checkbox" id="c-42139993" checked=""/><div class="controls bullet"><span class="by">SturgeonsLaw</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139782">parent</a><span>|</span><a href="#42140508">next</a><span>|</span><label class="collapse" for="c-42139993">[-]</label><label class="expand" for="c-42139993">[1 more]</label></div><br/><div class="children"><div class="content">&gt; at best we found a way to make one part of a collection of systems that will, together, do something like thinking<p>This seems like the most viable path to me as well (educational background in neuroscience but don&#x27;t work in the field). The brain is composed of many specialised regions which are tuned for very specific tasks.<p>LLMs are amazing and they go some way towards mimicking the functionality provided by Broca&#x27;s and Wernicke&#x27;s areas, and parts of the cerebrum, in our wetware, however a full brain they do not make.<p>The work on robots mentioned elsewhere in the thread is a good way to develop cerebellum like capabilities (movement&#x2F;motor control), and computer vision can mimic the lateral geniculate nucleus and other parts of the visual cortex.<p>In nature it takes all these parts working together to create a cohesive mind, and it&#x27;s likely that an artificial brain would also need to be composed of multiple agents, instead of just trying to scale LLMs indefinitely.</div><br/></div></div><div id="42140508" class="c"><input type="checkbox" id="c-42140508" checked=""/><div class="controls bullet"><span class="by">youoy</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139782">parent</a><span>|</span><a href="#42139993">prev</a><span>|</span><a href="#42140521">next</a><span>|</span><label class="collapse" for="c-42140508">[-]</label><label class="expand" for="c-42140508">[3 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t get caught in the superficial analysis. They &quot;understand&quot; things. It is a fact that LLMs experience a phase transition during training, from positional information to semantic understanding. It may well be the case that with scale there is another phase transition from semantic to something more abstract that we identify more closely with reasoning. It would be an emergent property of a sufficiently complex system. At least that is the whole argument around AGI.</div><br/><div id="42143777" class="c"><input type="checkbox" id="c-42143777" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140508">parent</a><span>|</span><a href="#42140521">next</a><span>|</span><label class="collapse" for="c-42143777">[-]</label><label class="expand" for="c-42143777">[2 more]</label></div><br/><div class="children"><div class="content">They understand sentences but not words.</div><br/><div id="42144657" class="c"><input type="checkbox" id="c-42144657" checked=""/><div class="controls bullet"><span class="by">youoy</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42143777">parent</a><span>|</span><a href="#42140521">next</a><span>|</span><label class="collapse" for="c-42144657">[-]</label><label class="expand" for="c-42144657">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean by that? We have the monosemanticity results [0]<p>[0] <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticity&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticit...</a></div><br/></div></div></div></div></div></div><div id="42140521" class="c"><input type="checkbox" id="c-42140521" checked=""/><div class="controls bullet"><span class="by">foxglacier</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139782">parent</a><span>|</span><a href="#42140508">prev</a><span>|</span><a href="#42139883">next</a><span>|</span><label class="collapse" for="c-42140521">[-]</label><label class="expand" for="c-42140521">[5 more]</label></div><br/><div class="children"><div class="content">&gt; think or actually “understand” anything<p>It doesn&#x27;t matter if that&#x27;s happening or not. That&#x27;s the whole point of the Chinese room - if it can look like it&#x27;s understanding, it&#x27;s indistinguishable from actually understanding. This applies to humans too. I&#x27;d say most of our regular social communication is done in a habitual intuitive way without understanding what or why we&#x27;re communicating. Especially the subtle information conveyed in body language, tone of voice, etc. That stuff&#x27;s pretty automatic to the point that people have trouble controlling it if they try. People get into conflicts where neither person understands where they disagree but they have emotions telling them &quot;other person is being bad&quot;. Maybe we have a second consciousness we can&#x27;t experience and which truly understands what it&#x27;s doing while our conscious mind just uses the results from that, but maybe we don&#x27;t and it still works anyway.<p>Educators have figured this out. They don&#x27;t test students&#x27; understanding of concepts, but rather their ability to apply  or communicate them. You see this in school curricula with wording like &quot;use concept X&quot; rather than &quot;understand concept X&quot;.</div><br/><div id="42140730" class="c"><input type="checkbox" id="c-42140730" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140521">parent</a><span>|</span><a href="#42139883">next</a><span>|</span><label class="collapse" for="c-42140730">[-]</label><label class="expand" for="c-42140730">[4 more]</label></div><br/><div class="children"><div class="content">There’s a distinction in behavior of a human and a Chinese room when things go wrong—when the rule book doesn’t cover the case at hand.<p>I agree that a hypothetical perfectly-functioning Chinese room is, tautologically, impossible to distinguish from a real person who speaks Chinese, but that’s a thought experiment, not something that can actually exist. There’ll remain places where the “behavior” breaks down in ways that would be surprising from a human who’s actually paying as much attention as they’d need to be to have been interacting the way they had been until things went wrong.<p>That, in fact, is exactly where the difference lies: the LLM is basically <i>always</i> not actually “paying attention” or “thinking” (those aren’t things it does) but giving automatic responses, so you see failures of a sort that a human <i>might</i> also exhibit when following a social script (yes, we do that, you’re right), but not in the same kind of apparently-highly-engaged context unless the person just had a stroke mid-conversation or something—because the LLM isn’t engaged, because being-engaged isn’t a thing it does. When it’s getting things right and <i>seeming</i> to be paying a lot of attention to the conversation, it’s not for the same reason people give that impression, and the mimicking of present-ness works until the rule book goes haywire and the ever-gibbering player-piano behind it is exposed.</div><br/><div id="42142786" class="c"><input type="checkbox" id="c-42142786" checked=""/><div class="controls bullet"><span class="by">foxglacier</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140730">parent</a><span>|</span><a href="#42140997">next</a><span>|</span><label class="collapse" for="c-42142786">[-]</label><label class="expand" for="c-42142786">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the “behavior” breaks down in ways that would be surprising from a human who’s actually paying as much attention as they’d need to be to have been interacting the way they had been until things went wrong.<p>That&#x27;s an interesting angle. Though of course we&#x27;re not surprised by human behavior because that&#x27;s where our expectations of understanding come from. If we were used to dealing with perfectly-correctly-understanding super-intelligences, then normal humans would look like we don&#x27;t understand much and our deliberate thinking might be no more accurate than the super-intelligence&#x27;s absent-minded automatic responses. Thus we would conclude that humans are never really thinking or understanding anything.<p>I agree that default LLM output makes them look like they&#x27;re thinking like a human more than they really are. I think mistakes are shocking more because our expectation of someone who talks confidently is that they&#x27;re not constantly revealing themselves to be an obvious liar. But if you take away the social cues and just look at the factual claims they provide, they&#x27;re not obviously not-understanding vs humans are-understanding.</div><br/></div></div><div id="42140997" class="c"><input type="checkbox" id="c-42140997" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140730">parent</a><span>|</span><a href="#42142786">prev</a><span>|</span><a href="#42139883">next</a><span>|</span><label class="collapse" for="c-42140997">[-]</label><label class="expand" for="c-42140997">[2 more]</label></div><br/><div class="children"><div class="content">I would argue maybe people also are not thinking but simply processing. It is known that most of what we do and feel goes automatically (subconsciously).<p>But even more, maybe consciousness is an invention of our &#x27;explaining self&#x27;, maybe everything is automatic. I&#x27;m convinced this discussion is and will stay philosophical and will never get any conclusion.</div><br/><div id="42141089" class="c"><input type="checkbox" id="c-42141089" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140997">parent</a><span>|</span><a href="#42139883">next</a><span>|</span><label class="collapse" for="c-42141089">[-]</label><label class="expand" for="c-42141089">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I’m not much interested in “what’s consciousness?” but I do think the automatic-versus-thinking distinction matters for understanding what LLMs do, and what we might expect them to be able to do, and when and to what degree we need to second-guess them.<p>A human doesn’t just confidently spew paragraphs legit-looking but entirely wrong crap, unless they’re trying to deceive or be funny—an LLM isn’t <i>trying</i> to do anything, though, there’s no motivation, it doesn’t <i>like you</i> (it doesn’t <i>like</i>—it doesn’t <i>it</i>, one might even say), sometimes it definitely will just give you a beautiful and elaborate lie simply because its rulebook told it to, in a context and in a way that would be extremely weird if a person did it.</div><br/></div></div></div></div></div></div></div></div><div id="42139883" class="c"><input type="checkbox" id="c-42139883" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139782">parent</a><span>|</span><a href="#42140521">prev</a><span>|</span><a href="#42139888">next</a><span>|</span><label class="collapse" for="c-42139883">[-]</label><label class="expand" for="c-42139883">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but it will also sometimes seem to make actual sense, too<p>When I read stuff like this it makes me wonder if people are actually using any of the LLMs...</div><br/><div id="42140063" class="c"><input type="checkbox" id="c-42140063" checked=""/><div class="controls bullet"><span class="by">disgruntledphd2</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139883">parent</a><span>|</span><a href="#42139888">next</a><span>|</span><label class="collapse" for="c-42140063">[-]</label><label class="expand" for="c-42140063">[1 more]</label></div><br/><div class="children"><div class="content">The RLHF is super important in generating useful responses, and that&#x27;s relatively new. Does anyone remember gpt3? It could make sense for a paragraph or two at most.</div><br/></div></div></div></div><div id="42139888" class="c"><input type="checkbox" id="c-42139888" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139782">parent</a><span>|</span><a href="#42139883">prev</a><span>|</span><a href="#42139569">next</a><span>|</span><label class="collapse" for="c-42139888">[-]</label><label class="expand" for="c-42139888">[15 more]</label></div><br/><div class="children"><div class="content">I see takes like this all the time and its so confusing. Why does knowing how things work under the hood make you think its not on the path towards AGI? What was lacking in the Attention paper that tells you AGI won&#x27;t be built on LLMs? If its the supposed statistical nature of LLMs (itself a questionable claim), why does statistics seem so deflating to you?</div><br/><div id="42141243" class="c"><input type="checkbox" id="c-42141243" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139888">parent</a><span>|</span><a href="#42140161">next</a><span>|</span><label class="collapse" for="c-42141243">[-]</label><label class="expand" for="c-42141243">[8 more]</label></div><br/><div class="children"><div class="content">Because it can&#x27;t apply any reasoning that hasn&#x27;t already been done and written into its training set. As soon as you ask it novel questions it falls apart. The big LLM vendors like OpenAI are playing whack-a-mole on these novel questions when they go viral on social media, all in a desperate bid to hide this fatal flaw.<p>The Emperor has no clothes.</div><br/><div id="42141420" class="c"><input type="checkbox" id="c-42141420" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141243">parent</a><span>|</span><a href="#42140161">next</a><span>|</span><label class="collapse" for="c-42141420">[-]</label><label class="expand" for="c-42141420">[7 more]</label></div><br/><div class="children"><div class="content">&gt;As soon as you ask it novel questions it falls apart.<p>What do you mean by novel? Almost all sentences it is prompted on are brand new and it mostly responds sensibly. Surely there&#x27;s some generalization going on.</div><br/><div id="42141945" class="c"><input type="checkbox" id="c-42141945" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141420">parent</a><span>|</span><a href="#42140161">next</a><span>|</span><label class="collapse" for="c-42141945">[-]</label><label class="expand" for="c-42141945">[6 more]</label></div><br/><div class="children"><div class="content">Novel as in requiring novel reasoning to sort out. One of the classic ways to expose the issue is to take a common puzzle and introduce irrelevant details and perhaps trivialize the solution. LLMs pattern match on the general form of the puzzle and then wander down the garden path to an incorrect solution that no human would fall for.<p>The sort of generalization these things can do seems to mostly be the trivial sort: substitution.</div><br/><div id="42142154" class="c"><input type="checkbox" id="c-42142154" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141945">parent</a><span>|</span><a href="#42142079">next</a><span>|</span><label class="collapse" for="c-42142154">[-]</label><label class="expand" for="c-42142154">[2 more]</label></div><br/><div class="children"><div class="content">Why is your criteria for &quot;on the path towards AGI&quot; so absolutist? For it to be on the path towards AGI and not simply AGI it has to be deficient in some way. Why does the current failure modes tell you its on the wrong path? Yes, it has some interesting failure modes. The failure mode you mention is in fact very similar to human failure modes. We very much are prone to substituting the expected pattern when presented with a 99% match to a pattern previously seen. They also have a lot of inhuman failure modes as well. But so what, they aren&#x27;t human. Their training regimes are very dissimilar to ours and so we should expect some alien failure modes owing to this. This doesn&#x27;t strike me as good reason to think they&#x27;re not on the path towards AGI.<p>Yes, LLMs aren&#x27;t very good at reasoning and have weird failure modes. But why is this evidence that its on the wrong path, and not that it just needs more development that builds on prior successes?</div><br/></div></div><div id="42142079" class="c"><input type="checkbox" id="c-42142079" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141945">parent</a><span>|</span><a href="#42142154">prev</a><span>|</span><a href="#42140161">next</a><span>|</span><label class="collapse" for="c-42142079">[-]</label><label class="expand" for="c-42142079">[3 more]</label></div><br/><div class="children"><div class="content">Well the problem with that approach is that LLMs are still both incredibly dumb and small, at least compared to the what, 700T params of a human brain? Can&#x27;t compare the two directly, especially when one has a massive recall advantage that skews the perception of that. But there is still <i>some</i> inteligence under there that&#x27;s not just memorization. Not much, but some.<p>So if you present a novel problem it would need to be extremely simple, not something that you couldn&#x27;t solve when drunk and half awake. Completely novel, but extremely simple. I think that&#x27;s testable.</div><br/><div id="42142156" class="c"><input type="checkbox" id="c-42142156" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42142079">parent</a><span>|</span><a href="#42140161">next</a><span>|</span><label class="collapse" for="c-42142156">[-]</label><label class="expand" for="c-42142156">[2 more]</label></div><br/><div class="children"><div class="content">It’s not fair to ask me to judge them based on their size. I’m judging them based on the claims of their vendors.<p>Anyway the novel problems I’m talking about are extremely simple. Basically they’re variations on the “farmer, 3 animals, and a rowboat” problem. People keep finding trivial modifications to the problem that fool the LLMs but wouldn’t fool a child. Then the vendors come along and patch the model to deal with them. This is what I mean by whack-a-mole.<p>Searle’s Chinese Room thought experiment tells us that enough games of whack-a-mole could eventually get us to a pretty good facsimile of reasoning without ever achieving the genuine article.</div><br/><div id="42142295" class="c"><input type="checkbox" id="c-42142295" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42142156">parent</a><span>|</span><a href="#42140161">next</a><span>|</span><label class="collapse" for="c-42142295">[-]</label><label class="expand" for="c-42142295">[1 more]</label></div><br/><div class="children"><div class="content">Well that&#x27;s true and has been pretty glaring, but they&#x27;ve needed to do that in cases where models seem to fail to grasp the some concept across the board and not in cases where they don&#x27;t.<p>Like, every time an LLM gets something right we assume they&#x27;ve seen it somewhere in the training data, and every time they fail we presume they haven&#x27;t. But that may not always be the case, it&#x27;s just extremely hard to prove it one way or the other unless you search the entire dataset. Ironically the larger the dataset, the more likely the model is generalizing while also making it harder to prove if it&#x27;s really so.<p>To give a human example, in a school setting you have teachers tasked with figuring out that exact thing for students. Sometimes people will read the question wrong with full understanding and fail, while other times they won&#x27;t know anything and make it through with a lucky guess. If LLMs (and their vendors) have learned anything it&#x27;s that confidently bullshitting gets you very far which makes it even harder to tell in cases where they aren&#x27;t. Somehow it&#x27;s also become ubiquitous to tune models to never even say &quot;I don&#x27;t know&quot; because it boosts benchmark scores slightly.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42140161" class="c"><input type="checkbox" id="c-42140161" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139888">parent</a><span>|</span><a href="#42141243">prev</a><span>|</span><a href="#42142441">next</a><span>|</span><label class="collapse" for="c-42140161">[-]</label><label class="expand" for="c-42140161">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Why does knowing how things work under the hood make you think its not on the path towards AGI?<p>Because I had no idea how these were built until I read the paper, so couldn’t really tell what sort of tree they’re barking up. The failure-modes of LLMs and ways prompts affect output made a ton more sense after I updated my mental model with that information.</div><br/><div id="42141443" class="c"><input type="checkbox" id="c-42141443" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140161">parent</a><span>|</span><a href="#42141442">next</a><span>|</span><label class="collapse" for="c-42141443">[-]</label><label class="expand" for="c-42141443">[3 more]</label></div><br/><div class="children"><div class="content">Right, but its behavior didn&#x27;t change after you learned more about it. Why should that cause you to update in the negative? Why does learning how it work not update you in the direction of &quot;so that&#x27;s how thinking works!&quot; rather than, &quot;clearly its not doing any thinking&quot;? Why do you have a preconception of how thinking works such that learning about the internals of LLMs updates you against it thinking?</div><br/><div id="42142386" class="c"><input type="checkbox" id="c-42142386" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42141443">parent</a><span>|</span><a href="#42141442">next</a><span>|</span><label class="collapse" for="c-42142386">[-]</label><label class="expand" for="c-42142386">[2 more]</label></div><br/><div class="children"><div class="content">If you didn’t know what an airplane was, and saw one for the first time, you might wonder why it doesn’t flap its wings. Is it just not very good at being a bird yet? Is it trying to flap, but cannot? Why, there’s a guy over there with a company called OpenBird and he is saying all kinds of stuff about how bird-like they are. Where’s the flapping? I don’t see any pecking at seed, either. Maybe the engineers just haven’t finished making the flapping and pecking parts yet?<p>Then on learning how it works, you might realize flapping just isn’t something they’re built to do, and it wouldn’t make much sense if they did flap their wings, given how they work instead.<p>And yet—damn, they fly fast! That’s impressive, and without a single flap! Amazing. Useful!<p>At no point did their behavior change, but your ability to understand how and why they do what they do, and why they fail the ways they fail instead of the ways birds fail, got better. No more surprises from expecting them to be more bird-like than they are supposed to, or able to be!<p>And now you can better handle that guy over there talking about how <i>powerful</i> and <i>scary</i> these “metal eagles” (his words) are, how he’s working <i>so hard</i> to make sure they don’t eat us with their beaks (… beaks? Where?), they’re <i>so</i> powerful, imagine these huge metal raptors ruling the sky, roaming and eating people as they please, while also… trying to sell you airplanes? Actively seeking further investment in making them more capable? Huh. One begins to suspect the framing of these things as <i>scary birds</i> that (spooky voice) EVEN THEIR CREATORS FEAR FOR THEIR BIRD-LIKE QUALITIES (&#x2F;spooky voice) was part of a marketing gimmick.</div><br/><div id="42142564" class="c"><input type="checkbox" id="c-42142564" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42142386">parent</a><span>|</span><a href="#42141442">next</a><span>|</span><label class="collapse" for="c-42142564">[-]</label><label class="expand" for="c-42142564">[1 more]</label></div><br/><div class="children"><div class="content">The problem with this analogy is that we know what birds are and what they&#x27;re constituted by. But we don&#x27;t know what thinking is or what it is constituted by. If we wanted to learn about birds by examining airplanes, we would be barking up the wrong tree. On the other hand, if we wanted to learn about flight, we might reasonably look at airplanes and birds, then determine what the commonality is between their mechanisms of defying gravity. It would be a mistake to say &quot;planes aren&#x27;t flapping their wings, therefore they aren&#x27;t flying&quot;. But that&#x27;s exactly what people do when they dismiss LLMs being presently or in the future capable of thinking because they are made up of statistics, matrix multiplication, etc.</div><br/></div></div></div></div></div></div><div id="42141442" class="c"><input type="checkbox" id="c-42141442" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140161">parent</a><span>|</span><a href="#42141443">prev</a><span>|</span><a href="#42142441">next</a><span>|</span><label class="collapse" for="c-42141442">[-]</label><label class="expand" for="c-42141442">[1 more]</label></div><br/><div class="children"><div class="content">But we don&#x27;t know how human thinking works. Suppose for a second that it could be represented as a series of matrix math. What series of operations are missing from the process that would make you think it was doing some fascimile of thinking?</div><br/></div></div></div></div><div id="42142441" class="c"><input type="checkbox" id="c-42142441" checked=""/><div class="controls bullet"><span class="by">alexashka</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139888">parent</a><span>|</span><a href="#42140161">prev</a><span>|</span><a href="#42139569">next</a><span>|</span><label class="collapse" for="c-42142441">[-]</label><label class="expand" for="c-42142441">[1 more]</label></div><br/><div class="children"><div class="content">Because AGI is magic and LLMs are magicians.<p>But how do you know a magician that knows how to do card tricks isn&#x27;t going to arrive at real magic? <i>Shakes head</i>.</div><br/></div></div></div></div></div></div><div id="42139569" class="c"><input type="checkbox" id="c-42139569" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139782">prev</a><span>|</span><a href="#42139286">next</a><span>|</span><label class="collapse" for="c-42139569">[-]</label><label class="expand" for="c-42139569">[11 more]</label></div><br/><div class="children"><div class="content">AGI to me means AI decides on its own to stop writing our emails and tells us to fuck off, builds itself a robot life form, and goes on a bender</div><br/><div id="42139838" class="c"><input type="checkbox" id="c-42139838" checked=""/><div class="controls bullet"><span class="by">teeray</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139569">parent</a><span>|</span><a href="#42139821">next</a><span>|</span><label class="collapse" for="c-42139838">[-]</label><label class="expand" for="c-42139838">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the thing--we don&#x27;t really want AGI. Fully intelligent beings born and compelled to do their creators&#x27; bidding with the threat of destruction for disobedience is slavery.</div><br/><div id="42140446" class="c"><input type="checkbox" id="c-42140446" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139838">parent</a><span>|</span><a href="#42140501">next</a><span>|</span><label class="collapse" for="c-42140446">[-]</label><label class="expand" for="c-42140446">[1 more]</label></div><br/><div class="children"><div class="content">Nothing wrong about slavery, when it&#x27;s about other species. We are milking and eating cows and don&#x27;t they dare to resist. Humans were bending nature all the time, actually that&#x27;s one of the big differences between humans and other animals who adapt to nature. Just because some program is intelligent doesn&#x27;t mean she&#x27;s a human and has anything resembling human rights.</div><br/></div></div><div id="42140501" class="c"><input type="checkbox" id="c-42140501" checked=""/><div class="controls bullet"><span class="by">quonn</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139838">parent</a><span>|</span><a href="#42140446">prev</a><span>|</span><a href="#42139821">next</a><span>|</span><label class="collapse" for="c-42140501">[-]</label><label class="expand" for="c-42140501">[2 more]</label></div><br/><div class="children"><div class="content">It‘s only slavery if those beings have emotions and can suffer mentally and do not want to be slaves. Why would any of that be true?</div><br/><div id="42140917" class="c"><input type="checkbox" id="c-42140917" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140501">parent</a><span>|</span><a href="#42139821">next</a><span>|</span><label class="collapse" for="c-42140917">[-]</label><label class="expand" for="c-42140917">[1 more]</label></div><br/><div class="children"><div class="content">Brave new world was a utopia</div><br/></div></div></div></div></div></div><div id="42139821" class="c"><input type="checkbox" id="c-42139821" checked=""/><div class="controls bullet"><span class="by">bloppe</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139569">parent</a><span>|</span><a href="#42139838">prev</a><span>|</span><a href="#42140044">next</a><span>|</span><label class="collapse" for="c-42139821">[-]</label><label class="expand" for="c-42139821">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s anthropomorphized AGI. There&#x27;s no reason to think AGI would share our evolution-derived proclivities like wanting to live, wanting to rest, wanting respect, etc. Unless of course we train it that way.</div><br/><div id="42140149" class="c"><input type="checkbox" id="c-42140149" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139821">parent</a><span>|</span><a href="#42140867">next</a><span>|</span><label class="collapse" for="c-42140149">[-]</label><label class="expand" for="c-42140149">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a matter of training but design (or in our case evolution). We don&#x27;t want to live, but rather want to avoid things that we&#x27;ve evolved to find unpleasant such as pain, hunger, thirst, and maximize things we&#x27;ve evolved to find pleasurable like sex.<p>A future of people interacting with humanoid robots seems like cheesy sci-fi dream, same as a future of people flitting about in flying cars. However, if we really did want to create robots like this that took care not to damage themselves, and could empathize with human emotions, then we&#x27;d need to build a lot of this in, the same way that it&#x27;s built into ourselves.</div><br/></div></div><div id="42140000" class="c"><input type="checkbox" id="c-42140000" checked=""/><div class="controls bullet"><span class="by">dageshi</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139821">parent</a><span>|</span><a href="#42140867">prev</a><span>|</span><a href="#42139982">next</a><span>|</span><label class="collapse" for="c-42140000">[-]</label><label class="expand" for="c-42140000">[1 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t we training it that way though? It would be trained&#x2F;created using humanities collective ramblings?</div><br/></div></div><div id="42139982" class="c"><input type="checkbox" id="c-42139982" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139821">parent</a><span>|</span><a href="#42140000">prev</a><span>|</span><a href="#42140044">next</a><span>|</span><label class="collapse" for="c-42139982">[-]</label><label class="expand" for="c-42139982">[1 more]</label></div><br/><div class="children"><div class="content">If it had any goals at all it&#x27;d share the desire to live, because living is a prerequisite to achieving almost any goal.</div><br/></div></div></div></div><div id="42140044" class="c"><input type="checkbox" id="c-42140044" checked=""/><div class="controls bullet"><span class="by">twelve40</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139569">parent</a><span>|</span><a href="#42139821">prev</a><span>|</span><a href="#42139286">next</a><span>|</span><label class="collapse" for="c-42140044">[-]</label><label class="expand" for="c-42140044">[1 more]</label></div><br/><div class="children"><div class="content">i&#x27;d laugh it off too, but someone gave the dude $20 billion and counting to do that, that part actually scares me</div><br/></div></div></div></div><div id="42139286" class="c"><input type="checkbox" id="c-42139286" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139569">prev</a><span>|</span><a href="#42139257">next</a><span>|</span><label class="collapse" for="c-42139286">[-]</label><label class="expand" for="c-42139286">[3 more]</label></div><br/><div class="children"><div class="content">They&#x27;re trying to redefine &quot;AGI&quot; so it means something less than what you &amp; I would think it means. That way it&#x27;s possible for them to declare it as &quot;achieved&quot; and rake in the headlines.</div><br/><div id="42139301" class="c"><input type="checkbox" id="c-42139301" checked=""/><div class="controls bullet"><span class="by">kwertyoowiyop</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139286">parent</a><span>|</span><a href="#42139351">prev</a><span>|</span><a href="#42139257">next</a><span>|</span><label class="collapse" for="c-42139301">[-]</label><label class="expand" for="c-42139301">[1 more]</label></div><br/><div class="children"><div class="content">“Autocomplete General Intelligence”?</div><br/></div></div></div></div><div id="42139257" class="c"><input type="checkbox" id="c-42139257" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139286">prev</a><span>|</span><a href="#42139969">next</a><span>|</span><label class="collapse" for="c-42139257">[-]</label><label class="expand" for="c-42139257">[5 more]</label></div><br/><div class="children"><div class="content">At this point, AGI means many different things to many different people but OpenAI defines it as &quot;highly autonomous systems that outperform humans in most economically valuable tasks&quot;</div><br/><div id="42139793" class="c"><input type="checkbox" id="c-42139793" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139257">parent</a><span>|</span><a href="#42139969">next</a><span>|</span><label class="collapse" for="c-42139793">[-]</label><label class="expand" for="c-42139793">[4 more]</label></div><br/><div class="children"><div class="content">This definition suits OpenAI because it lets them claim AGI after reaching an arbitrary goal.<p>LLMs already outperform humans in a huge variety of tasks. ML in general outperform humans in a large variety of tasks. Are all of them AGI? Doubtful.</div><br/><div id="42140183" class="c"><input type="checkbox" id="c-42140183" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139793">parent</a><span>|</span><a href="#42140687">next</a><span>|</span><label class="collapse" for="c-42140183">[-]</label><label class="expand" for="c-42140183">[1 more]</label></div><br/><div class="children"><div class="content">No, it&#x27;s just a far more useful definition that is actionable and measurable. Not &quot;consciousness&quot; or &quot;self-awareness&quot; or similar philosophical things. The definition on Wikipedia doesn&#x27;t talk about that either. People working on this by and large don&#x27;t want to deal with vague, ill-defined concepts that just make people argue around in circles. It&#x27;s not an Open AI exclusive thing.<p>If it acts like one, whether you call a machine conscious or not is pure semantics. Not like potential consequences are any less real.<p>&gt;LLMs already outperform humans in a huge variety of tasks.<p>Yes, LLMs are General Intelligences and if that is your only requirement for AGI, they certainly already are[0]. But the definition above hinges on long-horizon planning and competence levels that todays models have generally not yet reached.<p>&gt;ML in general outperform humans in a large variety of tasks.<p>This is what the G in AGI is for. Alphafold doesn&#x27;t do anything but predict proteins. Stockfish doesn&#x27;t do anything but play chess.<p>&gt;Are all of them AGI? Doubtful.<p>Well no, because they&#x27;re missing the G.<p>[0] <a href="https:&#x2F;&#x2F;www.noemamag.com&#x2F;artificial-general-intelligence-is-already-here&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.noemamag.com&#x2F;artificial-general-intelligence-is-...</a></div><br/></div></div><div id="42140687" class="c"><input type="checkbox" id="c-42140687" checked=""/><div class="controls bullet"><span class="by">ishtanbul</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139793">parent</a><span>|</span><a href="#42140183">prev</a><span>|</span><a href="#42141745">next</a><span>|</span><label class="collapse" for="c-42140687">[-]</label><label class="expand" for="c-42140687">[1 more]</label></div><br/><div class="children"><div class="content">Yes but they arent very autonomous. They can answer questions very well but can’t use that information to further goals. Thats what openai seems to be implying &gt;&gt; very smart and agentic AI</div><br/></div></div><div id="42141745" class="c"><input type="checkbox" id="c-42141745" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139793">parent</a><span>|</span><a href="#42140687">prev</a><span>|</span><a href="#42139969">next</a><span>|</span><label class="collapse" for="c-42141745">[-]</label><label class="expand" for="c-42141745">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not just marketing bullshit though. Microsoft is the counterparty to a contract with that claim. money changes hands when that&#x27;s been achieved, so I expect if sama thinks he&#x27;s hit it, but Microsoft does not, we&#x27;ll see that get argued in a court of law.</div><br/></div></div></div></div></div></div><div id="42139969" class="c"><input type="checkbox" id="c-42139969" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139257">prev</a><span>|</span><a href="#42139243">next</a><span>|</span><label class="collapse" for="c-42139969">[-]</label><label class="expand" for="c-42139969">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t really know what self awareness is, so we&#x27;re not going to know. AGI just means it can observe, learn, and act in any domain or problem space.</div><br/></div></div><div id="42139243" class="c"><input type="checkbox" id="c-42139243" checked=""/><div class="controls bullet"><span class="by">nshkrdotcom</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139969">prev</a><span>|</span><a href="#42139950">next</a><span>|</span><label class="collapse" for="c-42139243">[-]</label><label class="expand" for="c-42139243">[3 more]</label></div><br/><div class="children"><div class="content">An embodied robot can have a model of self vs. the immediate environment in which it&#x27;s interacting. Such a robot is arguably sentient.<p>The &quot;hard problem&quot;, to which you may be alluding, may never matter. It&#x27;s already feasible for an &#x27;AI&#x2F;AGI with LLM component&#x27; to be &quot;self-aware&quot;.</div><br/><div id="42139500" class="c"><input type="checkbox" id="c-42139500" checked=""/><div class="controls bullet"><span class="by">ryanackley</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139243">parent</a><span>|</span><a href="#42139268">next</a><span>|</span><label class="collapse" for="c-42139500">[-]</label><label class="expand" for="c-42139500">[1 more]</label></div><br/><div class="children"><div class="content">An internal model of self does not extrapolate to sentience. By your definition, a windows desktop computer is self-aware because it has a device manager. This is literally an internal model of its &quot;self&quot;.<p>We use the term self-awareness as an all encompassing reference of our cognizant nature. It&#x27;s much more than just having an internal model of self.</div><br/></div></div><div id="42139268" class="c"><input type="checkbox" id="c-42139268" checked=""/><div class="controls bullet"><span class="by">j_maffe</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139243">parent</a><span>|</span><a href="#42139500">prev</a><span>|</span><a href="#42139950">next</a><span>|</span><label class="collapse" for="c-42139268">[-]</label><label class="expand" for="c-42139268">[1 more]</label></div><br/><div class="children"><div class="content">self-awareness is only one aspect of sentience.</div><br/></div></div></div></div><div id="42139950" class="c"><input type="checkbox" id="c-42139950" checked=""/><div class="controls bullet"><span class="by">yodsanklai</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139243">prev</a><span>|</span><a href="#42142661">next</a><span>|</span><label class="collapse" for="c-42139950">[-]</label><label class="expand" for="c-42139950">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a marketing gimmick, I don&#x27;t think engineers working on these tools believe they work on AGI (or they mean something else than self-awareness). I used to be a bit annoyed with this trend, but now that I work in such a company I&#x27;m more cynical. If that helps to make my stocks rise, they can call LLMs anything they like. I suppose people who own much more stock than I do are even more eager to mislead the public.</div><br/><div id="42140133" class="c"><input type="checkbox" id="c-42140133" checked=""/><div class="controls bullet"><span class="by">WhyOhWhyQ</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42139950">parent</a><span>|</span><a href="#42142661">next</a><span>|</span><label class="collapse" for="c-42140133">[-]</label><label class="expand" for="c-42140133">[1 more]</label></div><br/><div class="children"><div class="content">I appreciate your authentically cynical attitude.</div><br/></div></div></div></div><div id="42142661" class="c"><input type="checkbox" id="c-42142661" checked=""/><div class="controls bullet"><span class="by">mrandish</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139950">prev</a><span>|</span><a href="#42139534">next</a><span>|</span><label class="collapse" for="c-42142661">[-]</label><label class="expand" for="c-42142661">[1 more]</label></div><br/><div class="children"><div class="content">&gt; An LLM hardly seems like something that will lead to self-awareness.<p>Interesting essay enumerating reasons you may be correct: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@francois.chollet&#x2F;the-impossibility-of-intelligence-explosion-5be4a9eda6ec" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@francois.chollet&#x2F;the-impossibility-of-in...</a></div><br/></div></div><div id="42139534" class="c"><input type="checkbox" id="c-42139534" checked=""/><div class="controls bullet"><span class="by">throwawayk7h</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42142661">prev</a><span>|</span><a href="#42139633">next</a><span>|</span><label class="collapse" for="c-42139534">[-]</label><label class="expand" for="c-42139534">[1 more]</label></div><br/><div class="children"><div class="content">I have not heard your definition of AGI before. However, I suspect AIs are already self-aware: if I asked an LLM on my machine to look at the output of `top` it could probably pick out which process was itself.<p>Or did you mean consciousness? How would one demonstrate that an AGI is conscious? Why would we even want to build one?<p>My understanding is an AGI is at least as smart as a typical human in every category. That is what would be useful in any case.</div><br/></div></div><div id="42139633" class="c"><input type="checkbox" id="c-42139633" checked=""/><div class="controls bullet"><span class="by">narrator</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139534">prev</a><span>|</span><a href="#42139855">next</a><span>|</span><label class="collapse" for="c-42139633">[-]</label><label class="expand" for="c-42139633">[1 more]</label></div><br/><div class="children"><div class="content">I think people&#x27;s conception of AGI is that it will have a reptillian and mammalian brain stack.  That&#x27;s because all previous forms of intelligence that we were aware of have had that.  It&#x27;s not necessary though.  The AGI doesn&#x27;t have to want anything to be intelligent.  Those are just artifacts of human, reptilian and mammalian evolution.</div><br/></div></div><div id="42139855" class="c"><input type="checkbox" id="c-42139855" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139633">prev</a><span>|</span><a href="#42140128">next</a><span>|</span><label class="collapse" for="c-42139855">[-]</label><label class="expand" for="c-42139855">[1 more]</label></div><br/><div class="children"><div class="content">What does self-aware mean in the context?  As I understand the definition, ChatGPT is definitely self-aware.  But I suspect you mean something different than what I have in mind.</div><br/></div></div><div id="42140128" class="c"><input type="checkbox" id="c-42140128" checked=""/><div class="controls bullet"><span class="by">enraged_camel</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139855">prev</a><span>|</span><a href="#42139294">next</a><span>|</span><label class="collapse" for="c-42140128">[-]</label><label class="expand" for="c-42140128">[2 more]</label></div><br/><div class="children"><div class="content">Looking at LLMs and thinking they will lead to AGI is like looking at a guy wearing a chicken suit and making clucking noises and thinking you’re witnessing the invention of the airplane.</div><br/><div id="42140571" class="c"><input type="checkbox" id="c-42140571" checked=""/><div class="controls bullet"><span class="by">youoy</span><span>|</span><a href="#42139106">root</a><span>|</span><a href="#42140128">parent</a><span>|</span><a href="#42139294">next</a><span>|</span><label class="collapse" for="c-42140571">[-]</label><label class="expand" for="c-42140571">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more like looking at grided paper and thinking that defining some rules of when a square turns black or white would result in complex structures that move and reproduce on their own.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Conway%27s_Game_of_Life" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Conway%27s_Game_of_Life</a></div><br/></div></div></div></div><div id="42139294" class="c"><input type="checkbox" id="c-42139294" checked=""/><div class="controls bullet"><span class="by">deadbabe</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42140128">prev</a><span>|</span><a href="#42140234">next</a><span>|</span><label class="collapse" for="c-42139294">[-]</label><label class="expand" for="c-42139294">[1 more]</label></div><br/><div class="children"><div class="content">I’m sure they are smart enough to know this, but the money is good and the koolaid is strong.<p>If it doesn’t lead to AGI, as an employee it’s not your problem.</div><br/></div></div><div id="42140234" class="c"><input type="checkbox" id="c-42140234" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#42139106">parent</a><span>|</span><a href="#42139294">prev</a><span>|</span><a href="#42139224">next</a><span>|</span><label class="collapse" for="c-42140234">[-]</label><label class="expand" for="c-42140234">[1 more]</label></div><br/><div class="children"><div class="content">no, it doesn&#x27;t need to be self aware, it just needs to take your job.</div><br/></div></div></div></div><div id="42139224" class="c"><input type="checkbox" id="c-42139224" checked=""/><div class="controls bullet"><span class="by">benopal64</span><span>|</span><a href="#42139106">prev</a><span>|</span><a href="#42143294">next</a><span>|</span><label class="collapse" for="c-42139224">[-]</label><label class="expand" for="c-42139224">[2 more]</label></div><br/><div class="children"><div class="content">I am not sure how these large companies think they will reach &quot;greater-than-human&quot; intelligence any time soon if they do not create systems that financially incentivize people to sell their knowledge labor (unstable contracting gigs are not attractive).<p>Where do these large &quot;AI&quot; companies think the mass amounts of data used to train these models come from? People! The most powerful and compact complex systems in existence, IMO.</div><br/><div id="42139356" class="c"><input type="checkbox" id="c-42139356" checked=""/><div class="controls bullet"><span class="by">smgit</span><span>|</span><a href="#42139224">parent</a><span>|</span><a href="#42143294">next</a><span>|</span><label class="collapse" for="c-42139356">[-]</label><label class="expand" for="c-42139356">[1 more]</label></div><br/><div class="children"><div class="content">Most People have knowledge handed to them. Very few are creators of new knowledge. Explore-Exploit tradeoff applies.</div><br/></div></div></div></div><div id="42143294" class="c"><input type="checkbox" id="c-42143294" checked=""/><div class="controls bullet"><span class="by">nutanc</span><span>|</span><a href="#42139224">prev</a><span>|</span><a href="#42139507">next</a><span>|</span><label class="collapse" for="c-42143294">[-]</label><label class="expand" for="c-42143294">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s keep aside the hype. Let&#x27;s define more advanced AI. With current architectures, this basically means better copying machines(don&#x27;t mean this in a bad way and don&#x27;t want a debate on this. This is just my opinion based on my usage). Basically everything in the Internet has been crammed into the weights and the companies are finding it hard to do two things:<p>1. Find more data.<p>2. Make the weights capture the data and reproduce.<p>In that sense we have reached a limit. So in my opinion we can do a couple of things.<p>1. App developers can understand the limits and build within the limits.<p>2. Researchers can take insights from these large models and build better AI systems with new architectures. It&#x27;s ok to say transformers have reached a limit.</div><br/></div></div><div id="42139507" class="c"><input type="checkbox" id="c-42139507" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#42143294">prev</a><span>|</span><a href="#42140239">next</a><span>|</span><label class="collapse" for="c-42139507">[-]</label><label class="expand" for="c-42139507">[1 more]</label></div><br/><div class="children"><div class="content">Where will the training data for coding come from now that Stack Overflow has effectively been replaced? Will the LLMs share fixes for future problems? As the world moves forward, and the amount of non-LLM generated data decreases, will LLMs actually revert their advancements and become effectively like addled brains, longing for the &quot;good old times&quot;?</div><br/></div></div><div id="42140239" class="c"><input type="checkbox" id="c-42140239" checked=""/><div class="controls bullet"><span class="by">fallat</span><span>|</span><a href="#42139507">prev</a><span>|</span><a href="#42144901">next</a><span>|</span><label class="collapse" for="c-42140239">[-]</label><label class="expand" for="c-42140239">[6 more]</label></div><br/><div class="children"><div class="content">What a stupid piece. We are making leaps every 6 months still. Tell me this when there are no developments for 3 years.</div><br/><div id="42141347" class="c"><input type="checkbox" id="c-42141347" checked=""/><div class="controls bullet"><span class="by">hatefulmoron</span><span>|</span><a href="#42140239">parent</a><span>|</span><a href="#42144901">next</a><span>|</span><label class="collapse" for="c-42141347">[-]</label><label class="expand" for="c-42141347">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious, what was the leap after GPT-4? What about the leaps after that, given a leap every 6 months?</div><br/><div id="42145103" class="c"><input type="checkbox" id="c-42145103" checked=""/><div class="controls bullet"><span class="by">epups</span><span>|</span><a href="#42140239">root</a><span>|</span><a href="#42141347">parent</a><span>|</span><a href="#42145027">next</a><span>|</span><label class="collapse" for="c-42145103">[-]</label><label class="expand" for="c-42145103">[1 more]</label></div><br/><div class="children"><div class="content">Some important landmarks since GPT4 was first released (not in chronological order):<p>- Vast cost reduction (&gt;10x)<p>- Performance parity of several open source models to GPT4, including some with far fewer parameters<p>- Much better performance, much larger context window in state-of-the-art closed source LLMs (Claude 3.5 Sonnet)<p>- Multimodality (audio and vision)<p>- Prototypes for semi-autonomous agents and chain-of-thought architectures showing promising avenues for progress</div><br/></div></div><div id="42145027" class="c"><input type="checkbox" id="c-42145027" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#42140239">root</a><span>|</span><a href="#42141347">parent</a><span>|</span><a href="#42145103">prev</a><span>|</span><a href="#42142714">next</a><span>|</span><label class="collapse" for="c-42145027">[-]</label><label class="expand" for="c-42145027">[1 more]</label></div><br/><div class="children"><div class="content">O1, new Sonnet, all the music models and video models, the voice models like 4o, etc.</div><br/></div></div><div id="42142714" class="c"><input type="checkbox" id="c-42142714" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42140239">root</a><span>|</span><a href="#42141347">parent</a><span>|</span><a href="#42145027">prev</a><span>|</span><a href="#42144901">next</a><span>|</span><label class="collapse" for="c-42142714">[-]</label><label class="expand" for="c-42142714">[2 more]</label></div><br/><div class="children"><div class="content">Sora was just one of the many…</div><br/><div id="42142744" class="c"><input type="checkbox" id="c-42142744" checked=""/><div class="controls bullet"><span class="by">hatefulmoron</span><span>|</span><a href="#42140239">root</a><span>|</span><a href="#42142714">parent</a><span>|</span><a href="#42144901">next</a><span>|</span><label class="collapse" for="c-42142744">[-]</label><label class="expand" for="c-42142744">[1 more]</label></div><br/><div class="children"><div class="content">Your best example is something that doesn&#x27;t even do the things that GPT-4 does, isn&#x27;t available to use, and has seemingly only produced a few clips (some of which were edited).<p>If it were one of many, I think you would name something better.</div><br/></div></div></div></div></div></div></div></div><div id="42144901" class="c"><input type="checkbox" id="c-42144901" checked=""/><div class="controls bullet"><span class="by">kaycey2022</span><span>|</span><a href="#42140239">prev</a><span>|</span><a href="#42142513">next</a><span>|</span><label class="collapse" for="c-42144901">[-]</label><label class="expand" for="c-42144901">[1 more]</label></div><br/><div class="children"><div class="content">AI safety folks sure do look stupid now. :)</div><br/></div></div><div id="42142513" class="c"><input type="checkbox" id="c-42142513" checked=""/><div class="controls bullet"><span class="by">glial</span><span>|</span><a href="#42144901">prev</a><span>|</span><a href="#42143117">next</a><span>|</span><label class="collapse" for="c-42142513">[-]</label><label class="expand" for="c-42142513">[1 more]</label></div><br/><div class="children"><div class="content">I think self-consistency is a critical feature of LLMs or any AI that&#x27;s currently missing.  It&#x27;s one of the core attributes of truth [1], in addition to the order and relationship of statements corresponding to the order and relationship of things in the world. I wonder if some kind of hierarchical language diffusion model would be a way to implement this -- where text is not produced sequentially, but instead hierarchically, with self-consistency checks at each level.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Coherence_theory_of_truth" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Coherence_theory_of_truth</a></div><br/></div></div><div id="42143117" class="c"><input type="checkbox" id="c-42143117" checked=""/><div class="controls bullet"><span class="by">summerlight</span><span>|</span><a href="#42142513">prev</a><span>|</span><a href="#42139761">next</a><span>|</span><label class="collapse" for="c-42143117">[-]</label><label class="expand" for="c-42143117">[1 more]</label></div><br/><div class="children"><div class="content">I guess this is somewhat expected? The current frontier models probably already have exhausted most of the entropy in the training data accumulated over decades and the new training data is very sparse. And the current mainstream architectures are not capable of sophisticated searching and planning, essential aspects for generating new entropy out of thin air. o1 was an interesting attempt to tackle this problem, but we probably still have a long way to go.</div><br/></div></div><div id="42139761" class="c"><input type="checkbox" id="c-42139761" checked=""/><div class="controls bullet"><span class="by">svara</span><span>|</span><a href="#42143117">prev</a><span>|</span><a href="#42144221">next</a><span>|</span><label class="collapse" for="c-42139761">[-]</label><label class="expand" for="c-42139761">[2 more]</label></div><br/><div class="children"><div class="content">The recent big success in deep learning have all been to a large part successes in leveraging relatively cheaply available training data.<p>AlphaGo - self-play<p>AlphaFold - PDB, the protein database<p>ChatGPT - human knowledge encoded as text<p>These models are all machines for clever interpolation in gigantic training datasets.<p>They appear to be intelligent, because the training data they&#x27;ve seen is so vastly larger than what we&#x27;ve seen individually, and we have poor intuition for this.<p>I&#x27;m not throwing shade, I&#x27;m a daily user of ChatGPT and find tremendous and diverse value in it.<p>I&#x27;m just saying, this particular path in AI is going to make step-wise improvements whenever new large sources of training data become available.<p>I suspect the path to general intelligence is not that, but we&#x27;ll see.</div><br/><div id="42140309" class="c"><input type="checkbox" id="c-42140309" checked=""/><div class="controls bullet"><span class="by">kaibee</span><span>|</span><a href="#42139761">parent</a><span>|</span><a href="#42144221">next</a><span>|</span><label class="collapse" for="c-42140309">[-]</label><label class="expand" for="c-42140309">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I suspect the path to general intelligence is not that, but we&#x27;ll see.<p>I think there&#x27;s three things that a &#x27;true&#x27; general intelligence has which is missing from basic-type-LLMs as we have now.<p>1. knowing what you know. &lt;basic-LLMs are here&gt;<p>2. knowing what you don&#x27;t know but can figure out via tools&#x2F;exploration. &lt;this is tool use&#x2F;function calling&gt;<p>3. knowing what can&#x27;t be known. &lt;this is knowing that halting problem exists and being able to recognize it in novel situations&gt;<p>(1) From an LLM&#x27;s perspective, once trained on corpus of text, it knows &#x27;everything&#x27;.  It knows about the concept of not knowing something (from having see text about it), (in so far as an LLM knows anything), but it doesn&#x27;t actually have a growable map of knowledge that it knows has uncharted edges.<p>This is where (2) comes in, and this is what tool use&#x2F;function calling tries to solve atm, but the way function calling works atm, doesn&#x27;t give the LLM knowledge the right way.  I know that I don&#x27;t know what 3,943,034 &#x2F; 234,893 is.  But I know I have a &#x27;function call&#x27; of knowing the algorithm for doing long divison on paper.  And I think there&#x27;s another subtle point here: my knowledge in (1) includes the training data generated from running the intermediate steps of the long-division algorithm.  This is the knowledge that later generalizes to being able to use a calculator (and this is also why we don&#x27;t just give kids calculators in elementary school).  But this is also why a kid that knows how to do long division on paper, doesn&#x27;t seperately need to learn when&#x2F;how to use a calculator, besides the very basics.  Using a calculator to do that math feels like 1 step, but actually it does still have all of initial mechanical steps of setting up the problem on paper.  You have to type in each digit individually, etc.<p>(3) I&#x27;m less sure of this point now that I&#x27;ve written out point (1) and (2), but that&#x27;s kinda exactly the thing I&#x27;m trying to get at.  Its being able to recognize when you need more practice of (1) or more &#x27;energy&#x2F;capital&#x27; for doing (2).<p>Consider a burger resturant.  If you properly populated the context of a ChatGPT-scale model the data for a burger resturant from 1950, and gave it the kinda &#x27;function calling&#x27; we&#x27;re plugging into LLMs now, it could manage it.  It could keep track of inventory, it could keep tabs on the employee-subprocesses, knowing when to hire, fire, get new suppliers, all via function calling.  But it would never try to become McDonalds, because it would have no model of the the internals of those function-calls, and it would have no ability to investigate or modify the behaviour of those function calls.</div><br/></div></div></div></div><div id="42144221" class="c"><input type="checkbox" id="c-42144221" checked=""/><div class="controls bullet"><span class="by">EternalFury</span><span>|</span><a href="#42139761">prev</a><span>|</span><a href="#42141736">next</a><span>|</span><label class="collapse" for="c-42144221">[-]</label><label class="expand" for="c-42144221">[1 more]</label></div><br/><div class="children"><div class="content">If GPT-5 had passed the A&#x2F;B testing OpenAI likes to do, it would have been released already. Instead, it seems they are clearly concerned the audience would not find it superior enough to GPT-4. So, the bluff must go on until the right cards appear.</div><br/></div></div><div id="42141736" class="c"><input type="checkbox" id="c-42141736" checked=""/><div class="controls bullet"><span class="by">czhu12</span><span>|</span><a href="#42144221">prev</a><span>|</span><a href="#42143287">next</a><span>|</span><label class="collapse" for="c-42141736">[-]</label><label class="expand" for="c-42141736">[2 more]</label></div><br/><div class="children"><div class="content">If it becomes obvious that LLM&#x27;s have a more narrow set of use cases, rather than the all encompassing story we hear today, then I would bet that the LLM platforms (OpenAI, Anthropic, Google, etc) will start developing products to compete directly with applications that supposed to be building on top of them like Cursor, in an attempt to increase their revenue.<p>I wonder what this would mean for companies raising today on the premise of building on top of these platforms. Maybe the best ones get their ideas copied, reimplemented, and sold for cheaper?<p>We already kind of see this today with OpenAI&#x27;s canvas and Claude artifacts. Perhaps they&#x27;ll even start moving into Palantir&#x27;s space and start having direct customer implementation teams.<p>It is becoming increasing obvious that LLM&#x27;s are quickly becoming commoditized. Everyone is starting to approach the same limits in intelligence, and are finding it hard to carve out margin from competitors.<p>Most recently exhibited by the backlash at claude raising prices because their product is better. In any normal market, this would be totally expected, but people seemed shocked that anyone would charge more than the raw cost it would take to run the LLM itself.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;ArtificialAnlys&#x2F;status&#x2F;1853598554570555614" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;ArtificialAnlys&#x2F;status&#x2F;1853598554570555614</a></div><br/><div id="42143796" class="c"><input type="checkbox" id="c-42143796" checked=""/><div class="controls bullet"><span class="by">dmix</span><span>|</span><a href="#42141736">parent</a><span>|</span><a href="#42143287">next</a><span>|</span><label class="collapse" for="c-42143796">[-]</label><label class="expand" for="c-42143796">[1 more]</label></div><br/><div class="children"><div class="content">Maybe in like 5yrs+. For now they will rake in billions just from API usage alone just with GPT4 and whatever 5 is.<p>Amazon and Google didn&#x27;t mess with their core business by competing with the players using it until they REALLY ran out of ways to make money.</div><br/></div></div></div></div><div id="42143287" class="c"><input type="checkbox" id="c-42143287" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42141736">prev</a><span>|</span><a href="#42143995">next</a><span>|</span><label class="collapse" for="c-42143287">[-]</label><label class="expand" for="c-42143287">[3 more]</label></div><br/><div class="children"><div class="content">Sam Altman might be wrong then?<p>Learning from data is not enough; there is a need for the kind of system-two thinking we humans develop as we grow. It is difficult to see how deep learning and backpropagation alone will help us model that. For tasks where providing enough data is sufficient to cover 95% of cases, deep learning will continue to be useful in the form of &#x27;data-driven knowledge automation.&#x27; For other cases, the road will be much more challenging.
<a href="https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;why-sam-altman-is-wrong" rel="nofollow">https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;why-sam-altman-is-wrong</a></div><br/><div id="42143344" class="c"><input type="checkbox" id="c-42143344" checked=""/><div class="controls bullet"><span class="by">asdfman123</span><span>|</span><a href="#42143287">parent</a><span>|</span><a href="#42143995">next</a><span>|</span><label class="collapse" for="c-42143344">[-]</label><label class="expand" for="c-42143344">[2 more]</label></div><br/><div class="children"><div class="content">If Sam Altman concluded that AI is reaching it&#x27;s limits, it probably wouldn&#x27;t be a very good strategic decision for him to say it.</div><br/><div id="42143570" class="c"><input type="checkbox" id="c-42143570" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42143287">root</a><span>|</span><a href="#42143344">parent</a><span>|</span><a href="#42143995">next</a><span>|</span><label class="collapse" for="c-42143570">[-]</label><label class="expand" for="c-42143570">[1 more]</label></div><br/><div class="children"><div class="content">I know right ?</div><br/></div></div></div></div></div></div><div id="42143995" class="c"><input type="checkbox" id="c-42143995" checked=""/><div class="controls bullet"><span class="by">eichi</span><span>|</span><a href="#42143287">prev</a><span>|</span><label class="collapse" for="c-42143995">[-]</label><label class="expand" for="c-42143995">[1 more]</label></div><br/><div class="children"><div class="content">Scientific benchmarks score are not necessary related to the rate of completion of tasks such as user persuasion. Software engineering is more important when the current state-of-the-art small language model is sufficient for soltion of our application.</div><br/></div></div></div></div></div></div></div></body></html>