<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686301251749" as="style"/><link rel="stylesheet" href="styles.css?v=1686301251749"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://simonwillison.net/2023/Jun/8/gpt-tokenizers/">Understanding GPT tokenizers</a> <span class="domain">(<a href="https://simonwillison.net">simonwillison.net</a>)</span></div><div class="subtext"><span>simonw</span> | <span>91 comments</span></div><br/><div><div id="36249869" class="c"><input type="checkbox" id="c-36249869" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#36251042">next</a><span>|</span><label class="collapse" for="c-36249869">[-]</label><label class="expand" for="c-36249869">[7 more]</label></div><br/><div class="children"><div class="content">A few extra notes on tokens.<p>You don&#x27;t have to use tiktoken if you aren&#x27;t actually tokenizing things. The token lists are just text files that consist of the characters base64 encoded followed by the numeric ID. If you want to explore the list you can just download them and decode them yourself.<p>I find that sorting tokens by length makes it a bit easier to get a feel for what&#x27;s in there.<p>GPT-4 has a token vocabulary about twice the size of GPT-3.5.<p>The most interesting thing to me about the GPT-4 token list is how dominated it is by non-natural languages. It&#x27;s not as simple as English tokenizing more efficiently than Spanish because of frequency. The most common language after English is code. A huge number of tokens are allocated to even not very common things found in code, like &quot;ValidateAntiForgeryToken&quot; or &quot;_InternalArray&quot;. From eyeballing the list I&#x27;d guess about half the tokens seem to be from source code.<p>My guess is that it&#x27;s not a coincidence that GPT-4 both trained on a lot of code and is also the leading model. I suspect we&#x27;re going to discover at some point, or maybe OpenAI already did, that training on code isn&#x27;t just a neat trick to get an LLM that can knock out scripts. Maybe it&#x27;s fundamentally useful to train the model to reason logically and think clearly. The highly structured and unambiguous yet also complex thought that code represents is probably a great way for the model to really level up its thought processes. Ilya Sutskever mentioned in an interview that one of the bottlenecks they face on training something smarter than GPT-4 is getting access to &quot;more complex thought&quot;. If this is true then it&#x27;s possible the Microsoft collaboration will prove an enduring competitive advantage for OpenAI, as it gives them access to the bulk GitHub corpus which is probably quite hard to scrape otherwise.</div><br/><div id="36254268" class="c"><input type="checkbox" id="c-36254268" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#36249869">parent</a><span>|</span><a href="#36251122">next</a><span>|</span><label class="collapse" for="c-36254268">[-]</label><label class="expand" for="c-36254268">[2 more]</label></div><br/><div class="children"><div class="content">Here is the list of the 100k GPT-4 tokens as text file.<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;s-macke&#x2F;ae83f6afb89794350f8d9a1ad8a09193" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;s-macke&#x2F;ae83f6afb89794350f8d9a1ad8a0...</a><p>Yes, a lot of tokens are just for code.<p>Edit: 
Here as raw link for the poor mobile devices:<p><a href="https:&#x2F;&#x2F;gist.githubusercontent.com&#x2F;s-macke&#x2F;ae83f6afb89794350f8d9a1ad8a09193&#x2F;raw&#x2F;7efdf09500e1865483e0b2a97f1c5408a540d7f5&#x2F;gpt-4-tokens.txt" rel="nofollow">https:&#x2F;&#x2F;gist.githubusercontent.com&#x2F;s-macke&#x2F;ae83f6afb89794350...</a></div><br/><div id="36254760" class="c"><input type="checkbox" id="c-36254760" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#36249869">root</a><span>|</span><a href="#36254268">parent</a><span>|</span><a href="#36251122">next</a><span>|</span><label class="collapse" for="c-36254760">[-]</label><label class="expand" for="c-36254760">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for this! That was very nice and thoughtful.<p>There’s something poetic about ULL being a token, but NULL not being one.</div><br/></div></div></div></div><div id="36251122" class="c"><input type="checkbox" id="c-36251122" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36249869">parent</a><span>|</span><a href="#36254268">prev</a><span>|</span><a href="#36250657">next</a><span>|</span><label class="collapse" for="c-36251122">[-]</label><label class="expand" for="c-36251122">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I suspect we&#x27;re going to discover at some point, or maybe OpenAI already did, that training on code isn&#x27;t just a neat trick to get an LLM that can knock out scripts.<p>This is a thing that&#x27;s already fairly well known<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.07128" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.07128</a></div><br/></div></div><div id="36250657" class="c"><input type="checkbox" id="c-36250657" checked=""/><div class="controls bullet"><span class="by">base698</span><span>|</span><a href="#36249869">parent</a><span>|</span><a href="#36251122">prev</a><span>|</span><a href="#36252590">next</a><span>|</span><label class="collapse" for="c-36250657">[-]</label><label class="expand" for="c-36250657">[1 more]</label></div><br/><div class="children"><div class="content">I saw this when 3.5 came out: <a href="https:&#x2F;&#x2F;yaofu.notion.site&#x2F;How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1" rel="nofollow">https:&#x2F;&#x2F;yaofu.notion.site&#x2F;How-does-GPT-Obtain-its-Ability-Tr...</a><p>Haven&#x27;t followed up on all the comments in it but speculates on why chain of thought improves when training on code.</div><br/></div></div><div id="36251044" class="c"><input type="checkbox" id="c-36251044" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#36249869">parent</a><span>|</span><a href="#36252590">prev</a><span>|</span><a href="#36251042">next</a><span>|</span><label class="collapse" for="c-36251044">[-]</label><label class="expand" for="c-36251044">[1 more]</label></div><br/><div class="children"><div class="content">If anyone else is looking for the list parent is mentioning  I assume it is: <a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;encodings&#x2F;r50k_base.tiktoken" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;encodings&#x2F;r50k_ba...</a></div><br/></div></div></div></div><div id="36251042" class="c"><input type="checkbox" id="c-36251042" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#36249869">prev</a><span>|</span><a href="#36249024">next</a><span>|</span><label class="collapse" for="c-36251042">[-]</label><label class="expand" for="c-36251042">[2 more]</label></div><br/><div class="children"><div class="content">I frequently see people surprised about the kinds of &quot;simple&quot; mistakes LLMs make when given tasks that involve letters, syllables, word lengths, rhyming, etc. They don&#x27;t do well at character oriented tasks which seem trivial to us, like &quot;write me a sentence with only 5 letter words&quot;.<p>All of these problems stem from the fact that LLMs don&#x27;t &quot;see&quot; the actual letters&#x2F;characters in the text they are consuming and producing. They are only dealing in tokens, each of which usually blends together multiple letters.<p>The fact that they can sometimes or partially succeed at character-oriented tasks is the actual surprise. They are presumably using meta-knowledge about specific words. For example, they may have learned by rote that &quot;cat&quot; has 3 letters. Or they just know as a fact that &quot;moon&quot; and &quot;tune&quot; rhyme, without having any sense of what it means to pronounce them.</div><br/><div id="36253228" class="c"><input type="checkbox" id="c-36253228" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36251042">parent</a><span>|</span><a href="#36249024">next</a><span>|</span><label class="collapse" for="c-36253228">[-]</label><label class="expand" for="c-36253228">[1 more]</label></div><br/><div class="children"><div class="content">Yup! I wrote a whole paper and constrained text generation studio about this which has gotten some positive attention:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Hellisotherpeople&#x2F;Constrained-Text-Generation-Studio">https:&#x2F;&#x2F;github.com&#x2F;Hellisotherpeople&#x2F;Constrained-Text-Genera...</a><p><a href="https:&#x2F;&#x2F;replicate.com&#x2F;blog&#x2F;turn-your-llm-into-a-poet">https:&#x2F;&#x2F;replicate.com&#x2F;blog&#x2F;turn-your-llm-into-a-poet</a></div><br/></div></div></div></div><div id="36249024" class="c"><input type="checkbox" id="c-36249024" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36251042">prev</a><span>|</span><a href="#36254061">next</a><span>|</span><label class="collapse" for="c-36249024">[-]</label><label class="expand" for="c-36249024">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s the Observable notebook I built to explore how the tokenizers work: <a href="https:&#x2F;&#x2F;observablehq.com&#x2F;@simonw&#x2F;gpt-tokenizer" rel="nofollow">https:&#x2F;&#x2F;observablehq.com&#x2F;@simonw&#x2F;gpt-tokenizer</a></div><br/></div></div><div id="36254061" class="c"><input type="checkbox" id="c-36254061" checked=""/><div class="controls bullet"><span class="by">szopa</span><span>|</span><a href="#36249024">prev</a><span>|</span><a href="#36254644">next</a><span>|</span><label class="collapse" for="c-36254061">[-]</label><label class="expand" for="c-36254061">[1 more]</label></div><br/><div class="children"><div class="content">How I wish this post had appeared a few days earlier... I am writing on my own library for some agent experiments (in go, to make my life more interesting I guess), and knowing the number of tokens is important to implement a token buffer memory (as you approach the model&#x27;s context window size, you prune enough messages from the beginning of the conversation that the whole thing keeps some given size, in tokens). While there&#x27;s a nice native library in go for OpenAI models (<a href="https:&#x2F;&#x2F;github.com&#x2F;tiktoken-go&#x2F;tokenizer">https:&#x2F;&#x2F;github.com&#x2F;tiktoken-go&#x2F;tokenizer</a>), the only library I found for Hugging Face models (and Claude, they published their tokenizer spec in the same JSON format) calls into HF&#x27;s Rust implementation, which makes it challenging as a dependency in Go. What is more, any tokenizer needs to keep some representation of its vocabulary in memory. So, in the end I removed the true tokenizers, and ended up using an approximate version (just split it in on spaces and multiply by a factor I determined experimentally for  the models I use using the real tokenizer, with a little  extra for safety). If it turns out someone needs the real thing they can always provide their own token counter). I was actually rather happy with this result: I have less dependencies, and use less memory. But to get there I needed to do a deep dive too understand BPE tokenizers :)<p>(The library, if anyone is interested: <a href="https:&#x2F;&#x2F;github.com&#x2F;ryszard&#x2F;agency">https:&#x2F;&#x2F;github.com&#x2F;ryszard&#x2F;agency</a>.)</div><br/></div></div><div id="36254644" class="c"><input type="checkbox" id="c-36254644" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#36254061">prev</a><span>|</span><a href="#36250954">next</a><span>|</span><label class="collapse" for="c-36254644">[-]</label><label class="expand" for="c-36254644">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone have any understanding why these models don&#x27;t simply output unicode, chunked into 12 or 16 bit words or whatever? The token lists are 100.000 tokens long, 65336 positions derived from bit sequences shouldn&#x27;t be a problem right?</div><br/></div></div><div id="36250954" class="c"><input type="checkbox" id="c-36250954" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#36254644">prev</a><span>|</span><a href="#36249285">next</a><span>|</span><label class="collapse" for="c-36250954">[-]</label><label class="expand" for="c-36250954">[1 more]</label></div><br/><div class="children"><div class="content">Computerphile has an interesting video about Glitch Tokens, but as it goes with Computerphile videos it&#x27;s also a good introduction to the topic.<p><a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=WO2X3oZEJOA">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=WO2X3oZEJOA</a></div><br/></div></div><div id="36249285" class="c"><input type="checkbox" id="c-36249285" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#36250954">prev</a><span>|</span><a href="#36249379">next</a><span>|</span><label class="collapse" for="c-36249285">[-]</label><label class="expand" for="c-36249285">[17 more]</label></div><br/><div class="children"><div class="content">I really really wish someone would try tokenizing off of a phonetic representation rather than textual one.  I think it would be interesting to compare the output</div><br/><div id="36251713" class="c"><input type="checkbox" id="c-36251713" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#36249285">parent</a><span>|</span><a href="#36252243">next</a><span>|</span><label class="collapse" for="c-36251713">[-]</label><label class="expand" for="c-36251713">[1 more]</label></div><br/><div class="children"><div class="content">I can see the theoretical advantages of such a concept, but I think a key limitation is that we don&#x27;t have appropriate amounts of data with <i>accurate</i> phonetic representation.<p>The potential advantage of using a phonetic representation is that it can have different relevant information than written spelling does. However, if you take the written spelling and pass it through some rules that transform it to what the phonetic representation <i>might</i> be... that transformation can only destroy information, not add it; you&#x27;d just be better off using the source data directly.<p>Now if at some point we get to a place where most of the training data is audio (i.e. the quantity of spoken words in available audio data becomes larger than current written data on internet and in libraries), then phonetic representation would make all sense, being closer to the source data.<p>But if we&#x27;re talking about purely tokenization - I think your suggestion is effectively halfway towards morphologically based tokenization, splitting into morphemes (which tend to map to semantics), and that is getting explored. The problem is, for an apples-to-apples comparison you need equally sized models and changes to tokenization require a complete retraining of the model; so doing a comparison on GPT-3 or GPT-4 scale is very expensive (too expensive for &quot;would be interesting&quot; to justify it), and measuring the effect on small models won&#x27;t necessarily be very indicative of how it will affect large models.</div><br/></div></div><div id="36252243" class="c"><input type="checkbox" id="c-36252243" checked=""/><div class="controls bullet"><span class="by">emmelaich</span><span>|</span><a href="#36249285">parent</a><span>|</span><a href="#36251713">prev</a><span>|</span><a href="#36249320">next</a><span>|</span><label class="collapse" for="c-36252243">[-]</label><label class="expand" for="c-36252243">[1 more]</label></div><br/><div class="children"><div class="content">I would like to see what happens when you go the other way.  Extremely naive tokening, for instance none at all.  Just a stream of bytes or nybbles.<p>It might take far more training but also it might avoid any biases introduced by tokenisation.<p>[edit - see @api had the same question]</div><br/></div></div><div id="36249320" class="c"><input type="checkbox" id="c-36249320" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36249285">parent</a><span>|</span><a href="#36252243">prev</a><span>|</span><a href="#36252687">next</a><span>|</span><label class="collapse" for="c-36249320">[-]</label><label class="expand" for="c-36249320">[9 more]</label></div><br/><div class="children"><div class="content">it doesn&#x27;t matter, the &#x27;bitter lesson&#x27; as coined by Rich Sutton is that stacking more layers with more parameters and compute and dataset size is going to swamp any kind of clever &#x27;feature engineering&#x27; like trying to be clever about phonetic tokens. Karpathy for example just wants to go back to byte tokens.</div><br/><div id="36249618" class="c"><input type="checkbox" id="c-36249618" checked=""/><div class="controls bullet"><span class="by">nsinreal</span><span>|</span><a href="#36249285">root</a><span>|</span><a href="#36249320">parent</a><span>|</span><a href="#36249435">next</a><span>|</span><label class="collapse" for="c-36249618">[-]</label><label class="expand" for="c-36249618">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but how much extra layers and computing power do you need? Of course, phonetic tokens are awkward idea, but there is a reason why word &quot;human&quot; is encoded as only one token.</div><br/></div></div><div id="36249435" class="c"><input type="checkbox" id="c-36249435" checked=""/><div class="controls bullet"><span class="by">spywaregorilla</span><span>|</span><a href="#36249285">root</a><span>|</span><a href="#36249320">parent</a><span>|</span><a href="#36249618">prev</a><span>|</span><a href="#36251374">next</a><span>|</span><label class="collapse" for="c-36249435">[-]</label><label class="expand" for="c-36249435">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that is intuitive at all. &quot;Clever feature engineering&quot; like trying to create columns from calculations of tabular data, sure. You&#x27;re not going to move the needle. But the basic representation of unstructured data like text could very believably alter the need for parameters, layers, and calculation speed by orders of magnitude.</div><br/><div id="36249472" class="c"><input type="checkbox" id="c-36249472" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36249285">root</a><span>|</span><a href="#36249435">parent</a><span>|</span><a href="#36249865">next</a><span>|</span><label class="collapse" for="c-36249472">[-]</label><label class="expand" for="c-36249472">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;I don&#x27;t think that is intuitive at all.&quot;<p>That&#x27;s exactly the point. Every intuition is always on the side of feature engineering.</div><br/></div></div><div id="36249865" class="c"><input type="checkbox" id="c-36249865" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36249285">root</a><span>|</span><a href="#36249435">parent</a><span>|</span><a href="#36249472">prev</a><span>|</span><a href="#36251374">next</a><span>|</span><label class="collapse" for="c-36249865">[-]</label><label class="expand" for="c-36249865">[2 more]</label></div><br/><div class="children"><div class="content">You would be wrong at the scales we are talking about.<p>The whole point is that it is unintuitive.</div><br/><div id="36254357" class="c"><input type="checkbox" id="c-36254357" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#36249285">root</a><span>|</span><a href="#36249865">parent</a><span>|</span><a href="#36251374">next</a><span>|</span><label class="collapse" for="c-36254357">[-]</label><label class="expand" for="c-36254357">[1 more]</label></div><br/><div class="children"><div class="content">If you replace a tokenizer with 5 bytes per token on average by a byte-level representation, you now need 5 times as much memory and (depending on the specifics of the attention mechanism) 11 to 25 times as much compute.<p>At the scales we&#x27;re talking about, that&#x27;s quite a hefty price to pay, and it doesn&#x27;t even take into account that you might need more layers to replace the processing that was implicitly done by the tokenizer.</div><br/></div></div></div></div></div></div><div id="36251374" class="c"><input type="checkbox" id="c-36251374" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#36249285">root</a><span>|</span><a href="#36249320">parent</a><span>|</span><a href="#36249435">prev</a><span>|</span><a href="#36249570">next</a><span>|</span><label class="collapse" for="c-36251374">[-]</label><label class="expand" for="c-36251374">[1 more]</label></div><br/><div class="children"><div class="content">I think that’s part of the reason I would love to see somebody try it, because intuitively I think it would make a difference, but it may not.<p>To me it’s like changing the periodic table, at the macroscopic scale it may or may not make a difference.</div><br/></div></div><div id="36249570" class="c"><input type="checkbox" id="c-36249570" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36249285">root</a><span>|</span><a href="#36249320">parent</a><span>|</span><a href="#36251374">prev</a><span>|</span><a href="#36253333">next</a><span>|</span><label class="collapse" for="c-36249570">[-]</label><label class="expand" for="c-36249570">[1 more]</label></div><br/><div class="children"><div class="content">Most current implementations can&#x27;t count syllables at all, so it would get you at least that far.</div><br/></div></div><div id="36253333" class="c"><input type="checkbox" id="c-36253333" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36249285">root</a><span>|</span><a href="#36249320">parent</a><span>|</span><a href="#36249570">prev</a><span>|</span><a href="#36252687">next</a><span>|</span><label class="collapse" for="c-36253333">[-]</label><label class="expand" for="c-36253333">[1 more]</label></div><br/><div class="children"><div class="content">If we don&#x27;t fix up issues caused by the tokenizers, than techniques which literally remove superfluous computation (i.e. through filters of the LLM probability distribution) are useful as a stop-gap.<p>Switching to bytes is the ultimate fix, but for the interim, if you want reliable rhyming with an LLM, you need filter-assisted decoding: <a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be-poets-too-an-ai" rel="nofollow">https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be...</a>
and replicas post about this work: <a href="https:&#x2F;&#x2F;replicate.com&#x2F;blog&#x2F;turn-your-llm-into-a-poet">https:&#x2F;&#x2F;replicate.com&#x2F;blog&#x2F;turn-your-llm-into-a-poet</a></div><br/></div></div></div></div><div id="36252687" class="c"><input type="checkbox" id="c-36252687" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#36249285">parent</a><span>|</span><a href="#36249320">prev</a><span>|</span><a href="#36253236">next</a><span>|</span><label class="collapse" for="c-36252687">[-]</label><label class="expand" for="c-36252687">[1 more]</label></div><br/><div class="children"><div class="content">Probably better to skip that and go for characters or bytes, since it can simply learn morphemes or phonemes from the smallest structure available.  Alas, the context size problem is the main pressure against this.</div><br/></div></div><div id="36253236" class="c"><input type="checkbox" id="c-36253236" checked=""/><div class="controls bullet"><span class="by">LudwigNagasena</span><span>|</span><a href="#36249285">parent</a><span>|</span><a href="#36252687">prev</a><span>|</span><a href="#36252063">next</a><span>|</span><label class="collapse" for="c-36253236">[-]</label><label class="expand" for="c-36253236">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean by phonetic representation? Sound files?</div><br/></div></div><div id="36252063" class="c"><input type="checkbox" id="c-36252063" checked=""/><div class="controls bullet"><span class="by">justanotheratom</span><span>|</span><a href="#36249285">parent</a><span>|</span><a href="#36253236">prev</a><span>|</span><a href="#36250128">next</a><span>|</span><label class="collapse" for="c-36252063">[-]</label><label class="expand" for="c-36252063">[1 more]</label></div><br/><div class="children"><div class="content">I wonder what Whisper&#x27;s token vocabulary looks (sounds) like..</div><br/></div></div><div id="36250128" class="c"><input type="checkbox" id="c-36250128" checked=""/><div class="controls bullet"><span class="by">bpiche</span><span>|</span><a href="#36249285">parent</a><span>|</span><a href="#36252063">prev</a><span>|</span><a href="#36253276">next</a><span>|</span><label class="collapse" for="c-36250128">[-]</label><label class="expand" for="c-36250128">[1 more]</label></div><br/><div class="children"><div class="content">spacy&#x27;s sense2vec gets pretty close to that<p><a href="https:&#x2F;&#x2F;spacy.io&#x2F;universe&#x2F;project&#x2F;sense2vec&#x2F;" rel="nofollow">https:&#x2F;&#x2F;spacy.io&#x2F;universe&#x2F;project&#x2F;sense2vec&#x2F;</a><p>granted, it is 8 years old, but it&#x27;s still interesting</div><br/></div></div><div id="36253276" class="c"><input type="checkbox" id="c-36253276" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36249285">parent</a><span>|</span><a href="#36250128">prev</a><span>|</span><a href="#36249379">next</a><span>|</span><label class="collapse" for="c-36253276">[-]</label><label class="expand" for="c-36253276">[1 more]</label></div><br/><div class="children"><div class="content">I agree with you, and I&#x27;m SHOCKED at how little work there actually is in phonetics within the NLP community. Consider that most of the phonetic tools that I am using to enforce rhyming or similar syntactic constrained in constrained text generation studio (<a href="https:&#x2F;&#x2F;github.com&#x2F;Hellisotherpeople&#x2F;Constrained-Text-Generation-Studio">https:&#x2F;&#x2F;github.com&#x2F;Hellisotherpeople&#x2F;Constrained-Text-Genera...</a>) were built circa 2014, such as the CMU rhyming dictionary. In most cases, I could not find better modern implementations of these tools.<p>I did learn an awful lot about phonetic representations and matching algorithms. Things like &quot;soundex&quot; and &quot;double metaphone&quot; now make sense to me and are fascinating to read about.</div><br/></div></div></div></div><div id="36249379" class="c"><input type="checkbox" id="c-36249379" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#36249285">prev</a><span>|</span><a href="#36250330">next</a><span>|</span><label class="collapse" for="c-36249379">[-]</label><label class="expand" for="c-36249379">[6 more]</label></div><br/><div class="children"><div class="content">Worth mentioning the many other consequences of BPE tokenization: gwern.net&#x2F;gpt-3#bpes <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;t9svvNPNmFf5Qa3TA&#x2F;mysteries-of-mode-collapse?commentId=tHhsnntni7WHFzR3x" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;t9svvNPNmFf5Qa3TA&#x2F;mysteries-...</a></div><br/><div id="36250119" class="c"><input type="checkbox" id="c-36250119" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#36249379">parent</a><span>|</span><a href="#36253286">next</a><span>|</span><label class="collapse" for="c-36250119">[-]</label><label class="expand" for="c-36250119">[4 more]</label></div><br/><div class="children"><div class="content">In the article on your blog, you wrote:<p><i>&quot;GPT-3 rhymes reasonably well and often when appropriate, but the improvement is much smaller on rhyming than it is on pretty much everything else. Apparently it is easier for GPT-3 to learn things like arithmetic and spreadsheets than it is to learn how to rhyme.&quot;</i><p>I&#x27;ve experimented extensively with Claude, and a bit with Claude+, ChatGPT (GPT 3.5) and GPT4 on poe.com, and I&#x27;ve had not the slightest problem in getting them to rhyme.  However, once they&#x27;ve started writing rhyming poetry it&#x27;s hard to get them to stop rhyming.  They seem to have formed a strong association between rhyming and poetry.  I&#x27;ve also been unable to get them to obey a specific rhyming scheme like ABBAB.</div><br/><div id="36252179" class="c"><input type="checkbox" id="c-36252179" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#36249379">root</a><span>|</span><a href="#36250119">parent</a><span>|</span><a href="#36250277">next</a><span>|</span><label class="collapse" for="c-36252179">[-]</label><label class="expand" for="c-36252179">[2 more]</label></div><br/><div class="children"><div class="content">&gt; However, once they&#x27;ve started writing rhyming poetry it&#x27;s hard to get them to stop rhyming. They seem to have formed a strong association between rhyming and poetry. I&#x27;ve also been unable to get them to obey a specific rhyming scheme like ABBAB.<p>Correct and commonly observed (eg. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11064" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11064</a> ). (At least, for GPT models. I don&#x27;t know as much about the Anthropic models as I should, although I understand they do still use a BPE tokenization, unfortunately.) My theory is that it is a surprising interaction of BPEs with RLHF: <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;t9svvNPNmFf5Qa3TA&#x2F;mysteries-of-mode-collapse?commentId=tHhsnntni7WHFzR3x" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;t9svvNPNmFf5Qa3TA&#x2F;mysteries-...</a><p>As much as they look like they can, they <i>can&#x27;t</i> rhyme because of BPEs still. What they have done in lieu of genuine phonetic understanding is, more or less, memorized a ton of rhyme-pairs: they only have a vast patchwork of half-understood phonetics discerned dimly through the lossy compression of BPEs and memorized pairs. If you don&#x27;t force them out of the memorized space and let them write without interruption, they look like they understand, but they still don&#x27;t.<p>Then RLHF punishes them for any incorrect poetry, so they never leave the memorized space on their own because that&#x27;s the only way to guarantee correct rhyming poetry. And since there is no way for it to tell the difference between &#x27;rhymes but I don&#x27;t know that it rhymes because BPEs&#x27; and &#x27;deliberately nonrhyming poetry&#x27;, much less what the difference is between &#x27;ABBAB&#x27; and &#x27;AABBAA&#x27;, it just always does rhyming quatrains etc. Why take the risk?<p>Also applies to jokes and joke explanations: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.04563" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.04563</a> It can&#x27;t understand properly what is a joke or not, because it&#x27;s blind to what makes a vast number of jokes work, so it just memorizes a few safe jokes and assumes anything presented to it as a joke must be one of the countless jokes that it can&#x27;t understand &amp; makes up its best guess.</div><br/><div id="36253190" class="c"><input type="checkbox" id="c-36253190" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#36249379">root</a><span>|</span><a href="#36252179">parent</a><span>|</span><a href="#36250277">next</a><span>|</span><label class="collapse" for="c-36253190">[-]</label><label class="expand" for="c-36253190">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if having access to characters actually helps rhyming in English all that much, as English rules of pronunciation are essentially rote-learned anyway. If it were not rote-learning, then it might make different mistakes, for example expecting two words to rhyme because they end with the same suffix.<p>Perhaps it would be more effective to ask it to produce poems in the format: <i>English0 IPA0 English1 IPA1</i>, where each line is produced in both semantic and phonetic representations. This would give it the context necessary to “see” the rhymes without having to mess around with the tokenization.</div><br/></div></div></div></div><div id="36250277" class="c"><input type="checkbox" id="c-36250277" checked=""/><div class="controls bullet"><span class="by">burnished</span><span>|</span><a href="#36249379">root</a><span>|</span><a href="#36250119">parent</a><span>|</span><a href="#36252179">prev</a><span>|</span><a href="#36253286">next</a><span>|</span><label class="collapse" for="c-36250277">[-]</label><label class="expand" for="c-36250277">[1 more]</label></div><br/><div class="children"><div class="content">That seems incredibly challenging, I&#x27;d expect some fundamental difficulty due to rhyming being determined by how a word sounds and not what it means.</div><br/></div></div></div></div><div id="36253286" class="c"><input type="checkbox" id="c-36253286" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36249379">parent</a><span>|</span><a href="#36250119">prev</a><span>|</span><a href="#36250330">next</a><span>|</span><label class="collapse" for="c-36253286">[-]</label><label class="expand" for="c-36253286">[1 more]</label></div><br/><div class="children"><div class="content">My paper, titled &quot;Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio&quot;, is cited in that gwern article!<p><a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be-poets-too-an-ai" rel="nofollow">https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be...</a></div><br/></div></div></div></div><div id="36250330" class="c"><input type="checkbox" id="c-36250330" checked=""/><div class="controls bullet"><span class="by">TechBro8615</span><span>|</span><a href="#36249379">prev</a><span>|</span><a href="#36252733">next</a><span>|</span><label class="collapse" for="c-36250330">[-]</label><label class="expand" for="c-36250330">[1 more]</label></div><br/><div class="children"><div class="content">Kudos to simonw for all the LLM content you&#x27;ve been publishing. I like reading your perspective and notes on your own learning experiences.</div><br/></div></div><div id="36252733" class="c"><input type="checkbox" id="c-36252733" checked=""/><div class="controls bullet"><span class="by">jamesk_au</span><span>|</span><a href="#36250330">prev</a><span>|</span><a href="#36250098">next</a><span>|</span><label class="collapse" for="c-36252733">[-]</label><label class="expand" for="c-36252733">[4 more]</label></div><br/><div class="children"><div class="content">Can anyone comment on how the limits on GPT-X’s token space translate to limits on its vocabulary (with corresponding limits on understanding input and generating output)?<p>For example, is GPT-4’s list of ~100k tokens sufficient to understand and generate every non-obsolete word in the English language (per, say, a standard dictionary)? Or even every word in the training data?<p>If not, do we have examples of ordinary words that it is impossible for GPT-4 ever to understand or generate? What happens when it encounters those words and is unable to tokenize them; are they simply ignored (eg omitted from the input vector, or set to 0 or some sort of null token)?</div><br/><div id="36252870" class="c"><input type="checkbox" id="c-36252870" checked=""/><div class="controls bullet"><span class="by">wolfgang42</span><span>|</span><a href="#36252733">parent</a><span>|</span><a href="#36252759">next</a><span>|</span><label class="collapse" for="c-36252870">[-]</label><label class="expand" for="c-36252870">[1 more]</label></div><br/><div class="children"><div class="content">IIRC from poking around in the LLaMA internals (I assume ChatGPT is the same since it’s the obvious way to handle this): the token list has a complete set of tokens of length 1. This means that in the degenerate case where the tokenizer can’t compose the text out of any other tokens it’ll still be processable, just as a collection of single-character tokens that the language model presumably has vaguer associations for. (Which I imagine doesn’t actually affect things significantly; if you added more tokens for less-frequently-seen strings, it still wouldn’t have much of an idea what to do with them.)</div><br/></div></div><div id="36252759" class="c"><input type="checkbox" id="c-36252759" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36252733">parent</a><span>|</span><a href="#36252870">prev</a><span>|</span><a href="#36250098">next</a><span>|</span><label class="collapse" for="c-36252759">[-]</label><label class="expand" for="c-36252759">[2 more]</label></div><br/><div class="children"><div class="content">My current understanding is that the lack of a token for a specific word does nothing to prevent that word from being &quot;understood&quot; or produced in output - GPT-4 is very capable of consuming and producing text in languages such as Spanish despite most Spanish words not corresponding to a single token.</div><br/><div id="36252860" class="c"><input type="checkbox" id="c-36252860" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#36252733">root</a><span>|</span><a href="#36252759">parent</a><span>|</span><a href="#36250098">next</a><span>|</span><label class="collapse" for="c-36252860">[-]</label><label class="expand" for="c-36252860">[1 more]</label></div><br/><div class="children"><div class="content">For Russian text, it degrades to, basically, 1 character = 1 token, due to the tokenization issues discussed in the article, yet it produces absolutely coherent text, almost same as in English. In my tests, its Russian output is worse than English output, though, something like 80% quality I&#x27;d say. I&#x27;m not an LLM expert but I have a theory that, being mostly trained on English text, its thought processes actually happen in English (the part of the model which was trained on English text) and for Russian, it&#x27;s able to map English to Russian and back thanks to its language translation ability, because I&#x27;ve noticed sometimes it produces slightly awkward sentences whose word choice makes sense in English (calques?) and not as much in Russian.</div><br/></div></div></div></div></div></div><div id="36250098" class="c"><input type="checkbox" id="c-36250098" checked=""/><div class="controls bullet"><span class="by">throwaway2016a</span><span>|</span><a href="#36252733">prev</a><span>|</span><a href="#36249266">next</a><span>|</span><label class="collapse" for="c-36250098">[-]</label><label class="expand" for="c-36250098">[8 more]</label></div><br/><div class="children"><div class="content">Pardon the n00b question, but...<p>How does this relate to vectors? It was my understanding that the tokens were vectors and this seems to show them as an integer.<p>It&#x27;s probably a really obvious question to anyone who knows AI but I figured if I have it someone else does too.</div><br/><div id="36250265" class="c"><input type="checkbox" id="c-36250265" checked=""/><div class="controls bullet"><span class="by">nighthawk454</span><span>|</span><a href="#36250098">parent</a><span>|</span><a href="#36250151">next</a><span>|</span><label class="collapse" for="c-36250265">[-]</label><label class="expand" for="c-36250265">[4 more]</label></div><br/><div class="children"><div class="content">The tokens are an integer. The first layer of the model is an &#x27;embedding&#x27;, which is essentially a giant lookup table. So if a string gets tokenized to Token #3, that means get the vector in row 3 of the embedding table. (Those vectors are learned during model training.)<p>More completely, you can think of the integers as being implicitly a one-hot vector encoding. So say you have a vocab size of 20,000 and you want Token #3. The one-hot vector would be a 20,000 length vector of zeros with a one in position 3. This vector is then multiplied against the embedding table&#x2F;matrix. Although in practice this is equivalent to just selecting one row directly, so it&#x27;s implemented as such and there&#x27;s no reason to explicitly make the large one-hot vectors.</div><br/><div id="36251006" class="c"><input type="checkbox" id="c-36251006" checked=""/><div class="controls bullet"><span class="by">CuriousSkeptic</span><span>|</span><a href="#36250098">root</a><span>|</span><a href="#36250265">parent</a><span>|</span><a href="#36250151">next</a><span>|</span><label class="collapse" for="c-36251006">[-]</label><label class="expand" for="c-36251006">[3 more]</label></div><br/><div class="children"><div class="content">Kind of refreshing to see this perspective on lookup vs matrix multiplication, specially with the bias towards the latter as more natural.<p>Is there some reference table somewhere mapping more code idioms like this to equivalent nn representations?</div><br/><div id="36254481" class="c"><input type="checkbox" id="c-36254481" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36250098">root</a><span>|</span><a href="#36251006">parent</a><span>|</span><a href="#36252784">next</a><span>|</span><label class="collapse" for="c-36254481">[-]</label><label class="expand" for="c-36254481">[1 more]</label></div><br/><div class="children"><div class="content">Andrej covers this in <a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;nn-zero-to-hero">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;nn-zero-to-hero</a>. He explains things in multiple ways, both the matrix multiplications as well as the &quot;programmer&#x27;s&quot; way of thinking of it - i.e. the lookups. The downside is it takes a while to get through those lectures. I would say for each 1 hour you need another 10 to looks stuff up and practice, unless you are fresh out of calculus and linear algebra classes.<p>Other idioms I can think of, in my words:<p>Softmax = take the maximum (but in a differentiable way)<p>tanh&#x2F;sigmoid&#x2F;relu = a switch. &quot;activation&quot;<p>cross entropy loss = average(-log(probability you gave to the right answer)). Averaged over the current batch you are training on for this step. (Sorry that is still quite mathy).</div><br/></div></div><div id="36252784" class="c"><input type="checkbox" id="c-36252784" checked=""/><div class="controls bullet"><span class="by">nighthawk454</span><span>|</span><a href="#36250098">root</a><span>|</span><a href="#36251006">parent</a><span>|</span><a href="#36254481">prev</a><span>|</span><a href="#36250151">next</a><span>|</span><label class="collapse" for="c-36252784">[-]</label><label class="expand" for="c-36252784">[1 more]</label></div><br/><div class="children"><div class="content">Hmm not that I know of, but that would be neat! A lot of the frameworks and model code treat this sort of thing as ‘implementation details’. Which is disappointing because I think it adds perspective and intuition.<p>One other example would be how multi-head attention is implemented with a single matrix. You don’t actually create matrices for each of the N ‘heads’ separately. It’s a logical distinction</div><br/></div></div></div></div></div></div><div id="36250151" class="c"><input type="checkbox" id="c-36250151" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#36250098">parent</a><span>|</span><a href="#36250265">prev</a><span>|</span><a href="#36250870">next</a><span>|</span><label class="collapse" for="c-36250151">[-]</label><label class="expand" for="c-36250151">[1 more]</label></div><br/><div class="children"><div class="content">Very basic overview: A token is assigned a number, that number gets passed into the encoder model with other token numbers, and the encoder model transforms those number sequences into embeddings (vectors)</div><br/></div></div><div id="36250870" class="c"><input type="checkbox" id="c-36250870" checked=""/><div class="controls bullet"><span class="by">RC_ITR</span><span>|</span><a href="#36250098">parent</a><span>|</span><a href="#36250151">prev</a><span>|</span><a href="#36250322">next</a><span>|</span><label class="collapse" for="c-36250870">[-]</label><label class="expand" for="c-36250870">[1 more]</label></div><br/><div class="children"><div class="content">After training, tokens are vectors, but the number of unique vectors is limited by your vocabulary size (i.e. Should &#x27;The&#x27; get a vector or should &#x27;Th&#x27; and &#x27;e&#x27; each get their own vector?).<p>This step is deciding which clusters of letters (or whatever) get a vector and then giving them a scalar unique ID for conveniences&#x27; sake.<p>The training then determines what that vector actually is.</div><br/></div></div><div id="36250322" class="c"><input type="checkbox" id="c-36250322" checked=""/><div class="controls bullet"><span class="by">z3c0</span><span>|</span><a href="#36250098">parent</a><span>|</span><a href="#36250870">prev</a><span>|</span><a href="#36249266">next</a><span>|</span><label class="collapse" for="c-36250322">[-]</label><label class="expand" for="c-36250322">[1 more]</label></div><br/><div class="children"><div class="content">The integers represent a position within a vector of &quot;all known tokens&quot;. Typically, following a simple bag-of-words approach, each position in the vector would be toggled to 1 or 0 based on the presence of a token in a given document. Since most vectors would be almost completely zeroed, the simpler way to represent these vectors is through a list of positions in the now abstracted vector, aka a sparse vector, ie a list of integers.<p>In the case of more advanced language models like LLMs, a given token can be paired with many other features of the token (such as dependencies or parts-of-speech) to make an integer represent one of many permutations on the same word based on its usage.</div><br/></div></div></div></div><div id="36249266" class="c"><input type="checkbox" id="c-36249266" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36250098">prev</a><span>|</span><a href="#36249195">next</a><span>|</span><label class="collapse" for="c-36249266">[-]</label><label class="expand" for="c-36249266">[2 more]</label></div><br/><div class="children"><div class="content">I just want to say i love your pet pelican names Pelly, Beaky, SkyDancer, Scoop, and Captain Gulliver.</div><br/><div id="36249328" class="c"><input type="checkbox" id="c-36249328" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36249266">parent</a><span>|</span><a href="#36249195">next</a><span>|</span><label class="collapse" for="c-36249328">[-]</label><label class="expand" for="c-36249328">[1 more]</label></div><br/><div class="children"><div class="content">Captain Gulliver is genuinely an excellent name for a pelican!</div><br/></div></div></div></div><div id="36249261" class="c"><input type="checkbox" id="c-36249261" checked=""/><div class="controls bullet"><span class="by">hsjqllzlfkf</span><span>|</span><a href="#36249195">prev</a><span>|</span><a href="#36251271">next</a><span>|</span><label class="collapse" for="c-36249261">[-]</label><label class="expand" for="c-36249261">[18 more]</label></div><br/><div class="children"><div class="content">Could anyone who&#x27;s an expert comment why there seems to be such a focus on discussing tokenizers? It seems every other day there&#x27;s a new article or implementation of a tokenizer on HN. But downstream from that, rarely anything. As a non-expert I would have thought to tokenizing is just one step.</div><br/><div id="36249288" class="c"><input type="checkbox" id="c-36249288" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36249261">parent</a><span>|</span><a href="#36249723">next</a><span>|</span><label class="collapse" for="c-36249288">[-]</label><label class="expand" for="c-36249288">[3 more]</label></div><br/><div class="children"><div class="content">The reason it&#x27;s trending today is because of the phenomenon of Glitch Tokens. They thought all Glitch Tokens had been removed by GPT-4 but apparently one is still left. If you go down the rabbit hole on Glitch Tokens it gets ... really really weird.</div><br/><div id="36251005" class="c"><input type="checkbox" id="c-36251005" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249288">parent</a><span>|</span><a href="#36249723">next</a><span>|</span><label class="collapse" for="c-36251005">[-]</label><label class="expand" for="c-36251005">[2 more]</label></div><br/><div class="children"><div class="content">But does the tokenizer have anything to do with Glitch Tokens?  Glitch Tokens seem more like a function of the neural network.  I&#x27;m saying this with only a surface level understanding of glitch tokens.</div><br/><div id="36254017" class="c"><input type="checkbox" id="c-36254017" checked=""/><div class="controls bullet"><span class="by">GeneralMayhem</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36251005">parent</a><span>|</span><a href="#36249723">next</a><span>|</span><label class="collapse" for="c-36254017">[-]</label><label class="expand" for="c-36254017">[1 more]</label></div><br/><div class="children"><div class="content">It does a bit, because the fact that they&#x27;re able to persist is sort of an artifact of how naive the tokenizer is (it&#x27;s a counting operation based on n-grams), and that it runs as a separate step. There&#x27;s no feedback from the transformer to the tokenizer to say &quot;hey, this token is actually pretty meaningless, maybe try again on that one&quot;. That means that strings of characters that are common but very low semantic value, like the example of Reddit usernames that mostly post on &#x2F;r&#x2F;counting, will be included in the model&#x27;s vocabulary even though they&#x27;re not interesting.<p>When humans see extremely low-information-density data, we can forget it. And the model can too, but only kind of - it can forget (or rather, never learn) what the &quot;word&quot; means, but it can&#x27;t forget <i>that it&#x27;s a word</i>.</div><br/></div></div></div></div></div></div><div id="36249723" class="c"><input type="checkbox" id="c-36249723" checked=""/><div class="controls bullet"><span class="by">SkyPuncher</span><span>|</span><a href="#36249261">parent</a><span>|</span><a href="#36249288">prev</a><span>|</span><a href="#36249279">next</a><span>|</span><label class="collapse" for="c-36249723">[-]</label><label class="expand" for="c-36249723">[11 more]</label></div><br/><div class="children"><div class="content">Tokens are the primitives that most LLMs (and broadly a lot of NLP) works with. While, you and I would expect whole-words to be tokens, many tokens are shorter - 3 to 4 characters - and don&#x27;t always match the sentence structure you and I expect.<p>This can create some interesting challenges and unexpected behavior. It also makes certain things, like vectorization, a challenge since tokens may not map 1:1 with the words you intend to weight them against.</div><br/><div id="36249849" class="c"><input type="checkbox" id="c-36249849" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249723">parent</a><span>|</span><a href="#36249971">next</a><span>|</span><label class="collapse" for="c-36249849">[-]</label><label class="expand" for="c-36249849">[2 more]</label></div><br/><div class="children"><div class="content">You are using the word vectorization in an idiosyncratic way, you are referring to the process of embedding words?</div><br/><div id="36250501" class="c"><input type="checkbox" id="c-36250501" checked=""/><div class="controls bullet"><span class="by">SkyPuncher</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249849">parent</a><span>|</span><a href="#36249971">next</a><span>|</span><label class="collapse" for="c-36250501">[-]</label><label class="expand" for="c-36250501">[1 more]</label></div><br/><div class="children"><div class="content">Because I’m not an expert in this area.  I know it well enough to build products around it, but it’s not my deep area of expertise.<p>Just trying to provide an example.</div><br/></div></div></div></div><div id="36249971" class="c"><input type="checkbox" id="c-36249971" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249723">parent</a><span>|</span><a href="#36249849">prev</a><span>|</span><a href="#36249888">next</a><span>|</span><label class="collapse" for="c-36249971">[-]</label><label class="expand" for="c-36249971">[3 more]</label></div><br/><div class="children"><div class="content">&gt; While, you and I would expect whole-words to be tokens, many tokens are shorter - 3 to 4 characters - and don&#x27;t always match the sentence structure you and I expect.<p>There is a phenomenon called Broca&#x27;s Aphasia which is, essentially, the inability to connect words into sentences. This mostly prevents the patient from communicating via language. But patients with this condition can reveal quite a bit about the structure of the language they can no longer speak.<p>One example discussed in <i>The Language Instinct</i> is someone who works at (and was injured at) a mill. He is unable to produce utterances that are more than one word long, though he seems to do well at understanding what people say to him. One of his single-word utterances, describing the mill where he works, is &quot;Four hundred tons a day!&quot;.<p>This is the opposite of what you describe, a single token that is longer than one word in the base language instead of being shorter. But it appears to be the same kind of thing.<p>By the way, if you study a highly inflectional language such as Latin or Russian, you will lose the assumption that interpretive tokens should be whole words. You&#x27;d still expect them to align closely with sentence structure, though.</div><br/><div id="36252989" class="c"><input type="checkbox" id="c-36252989" checked=""/><div class="controls bullet"><span class="by">wolfgang42</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249971">parent</a><span>|</span><a href="#36250675">next</a><span>|</span><label class="collapse" for="c-36252989">[-]</label><label class="expand" for="c-36252989">[1 more]</label></div><br/><div class="children"><div class="content">You can observe (what I assume is) the same tokenization phenomenon in people who are struggling to speak (for example because they’re distracted by something or not native speakers): stock fragments will come out all at once, and less common words will get split, usually on affixes or at the join point of compound words.</div><br/></div></div><div id="36250675" class="c"><input type="checkbox" id="c-36250675" checked=""/><div class="controls bullet"><span class="by">doormatt</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249971">parent</a><span>|</span><a href="#36252989">prev</a><span>|</span><a href="#36249888">next</a><span>|</span><label class="collapse" for="c-36250675">[-]</label><label class="expand" for="c-36250675">[1 more]</label></div><br/><div class="children"><div class="content">&gt;One of his single-word utterances, describing the mill where he works, is &quot;Four hundred tons a day!&quot;.<p>I&#x27;m sorry, but I&#x27;m lost on how that&#x27;s a single-word utterance.</div><br/></div></div></div></div><div id="36249888" class="c"><input type="checkbox" id="c-36249888" checked=""/><div class="controls bullet"><span class="by">hsjqllzlfkf</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249723">parent</a><span>|</span><a href="#36249971">prev</a><span>|</span><a href="#36249279">next</a><span>|</span><label class="collapse" for="c-36249888">[-]</label><label class="expand" for="c-36249888">[5 more]</label></div><br/><div class="children"><div class="content">Your answer explains what tokenizers are, which isn&#x27;t what I asked. You also told me something interesting about tokenizers, which is also not what I asked. Can you tell me anything NOT about tokenized? This is my point.</div><br/><div id="36249937" class="c"><input type="checkbox" id="c-36249937" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249888">parent</a><span>|</span><a href="#36250537">next</a><span>|</span><label class="collapse" for="c-36249937">[-]</label><label class="expand" for="c-36249937">[2 more]</label></div><br/><div class="children"><div class="content">The reason it&#x27;s not discussed much is that what goes on downstream of tokenization is extremely opaque. It&#x27;s lots of layers of the transformer network so the overall structure is documented but what exactly those numbers mean is hard to figure out.<p>There&#x27;s an article here where the structure of an image generation network is explored a bit:<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;sparse-transformer" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;sparse-transformer</a><p>They have a visualization of what the different layers are paying attention to.<p>There are also some good explanations of transformers elsewhere online. This one is old but I found it helpful:<p><a href="http:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;" rel="nofollow">http:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;</a></div><br/><div id="36250026" class="c"><input type="checkbox" id="c-36250026" checked=""/><div class="controls bullet"><span class="by">hsjqllzlfkf</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249937">parent</a><span>|</span><a href="#36250537">next</a><span>|</span><label class="collapse" for="c-36250026">[-]</label><label class="expand" for="c-36250026">[1 more]</label></div><br/><div class="children"><div class="content">This was my suspicion, thank you.</div><br/></div></div></div></div><div id="36250537" class="c"><input type="checkbox" id="c-36250537" checked=""/><div class="controls bullet"><span class="by">SkyPuncher</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249888">parent</a><span>|</span><a href="#36249937">prev</a><span>|</span><a href="#36252934">next</a><span>|</span><label class="collapse" for="c-36250537">[-]</label><label class="expand" for="c-36250537">[1 more]</label></div><br/><div class="children"><div class="content">With all due respect, this feels like asking me to talk about math without talking about numbers.<p>Tokens are so closely tied to modern LLMs that’s it’s basically impossible to not talk about them. They’re getting a lot of attention because they are the primitive. They’re the thing of most interest for improving performance.</div><br/></div></div><div id="36252934" class="c"><input type="checkbox" id="c-36252934" checked=""/><div class="controls bullet"><span class="by">wolfgang42</span><span>|</span><a href="#36249261">root</a><span>|</span><a href="#36249888">parent</a><span>|</span><a href="#36250537">prev</a><span>|</span><a href="#36249279">next</a><span>|</span><label class="collapse" for="c-36252934">[-]</label><label class="expand" for="c-36252934">[1 more]</label></div><br/><div class="children"><div class="content">You asked why there was a focus on discussing tokenizers, and got an answer explaining why tokenizers are something people would want to discuss.</div><br/></div></div></div></div></div></div><div id="36249279" class="c"><input type="checkbox" id="c-36249279" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36249261">parent</a><span>|</span><a href="#36249723">prev</a><span>|</span><a href="#36249551">next</a><span>|</span><label class="collapse" for="c-36249279">[-]</label><label class="expand" for="c-36249279">[1 more]</label></div><br/><div class="children"><div class="content">I just think they&#x27;re interesting.<p>From a practical point of view they only really matter in that we have to think carefully about how to use our token budget.</div><br/></div></div><div id="36253307" class="c"><input type="checkbox" id="c-36253307" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36249261">parent</a><span>|</span><a href="#36249551">prev</a><span>|</span><a href="#36251271">next</a><span>|</span><label class="collapse" for="c-36253307">[-]</label><label class="expand" for="c-36253307">[1 more]</label></div><br/><div class="children"><div class="content">Most of the shitty behavior of LLMs on syntactic and lexical tasks are due to the tokenizer and not due to the LLM itself. Having even tiny changes in tokenization has massive downstream effects on LLM behavior.</div><br/></div></div></div></div><div id="36251271" class="c"><input type="checkbox" id="c-36251271" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#36249261">prev</a><span>|</span><a href="#36249934">next</a><span>|</span><label class="collapse" for="c-36251271">[-]</label><label class="expand" for="c-36251271">[1 more]</label></div><br/><div class="children"><div class="content">It is working fine if GPT 4 said it but glitches if it comes from user end: <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;ngdotkQ" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;ngdotkQ</a></div><br/></div></div><div id="36249934" class="c"><input type="checkbox" id="c-36249934" checked=""/><div class="controls bullet"><span class="by">ywmario</span><span>|</span><a href="#36251271">prev</a><span>|</span><a href="#36249668">next</a><span>|</span><label class="collapse" for="c-36249934">[-]</label><label class="expand" for="c-36249934">[1 more]</label></div><br/><div class="children"><div class="content">I have been under the impression that the embedded vector is the one actually matters. Token is just another format.</div><br/></div></div><div id="36249668" class="c"><input type="checkbox" id="c-36249668" checked=""/><div class="controls bullet"><span class="by">bluepoint</span><span>|</span><a href="#36249934">prev</a><span>|</span><a href="#36250040">next</a><span>|</span><label class="collapse" for="c-36249668">[-]</label><label class="expand" for="c-36249668">[8 more]</label></div><br/><div class="children"><div class="content">So the space character is part of the token?</div><br/><div id="36249758" class="c"><input type="checkbox" id="c-36249758" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36249668">parent</a><span>|</span><a href="#36251410">next</a><span>|</span><label class="collapse" for="c-36249758">[-]</label><label class="expand" for="c-36249758">[5 more]</label></div><br/><div class="children"><div class="content">Yup. Most common words have several tokens - the word, the word with a capital letter, the word with a leading space and sometimes the word all in caps too.<p>Try searching for different words using the search box here: <a href="https:&#x2F;&#x2F;observablehq.com&#x2F;@simonw&#x2F;gpt-tokenizer#cell-135" rel="nofollow">https:&#x2F;&#x2F;observablehq.com&#x2F;@simonw&#x2F;gpt-tokenizer#cell-135</a></div><br/><div id="36250359" class="c"><input type="checkbox" id="c-36250359" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36249668">root</a><span>|</span><a href="#36249758">parent</a><span>|</span><a href="#36251410">next</a><span>|</span><label class="collapse" for="c-36250359">[-]</label><label class="expand" for="c-36250359">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if the embeddings could be explicitly configured to account for these “symmetries”. E.g.: instead of storing seperate full copies of the “variants”, maybe keep a reduced representation with a common prefix and only a small subset of the embedding vector that is allowed to be learned?<p>This could force the model to correctly learn how to capitalise, make all-caps, etc…</div><br/><div id="36251011" class="c"><input type="checkbox" id="c-36251011" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#36249668">root</a><span>|</span><a href="#36250359">parent</a><span>|</span><a href="#36251410">next</a><span>|</span><label class="collapse" for="c-36251011">[-]</label><label class="expand" for="c-36251011">[3 more]</label></div><br/><div class="children"><div class="content">There was some discussion of doing this for RKVW, but I don&#x27;t think it has actually been implemented yet.<p>The goal is simply to speed up training slightly, it wouldn&#x27;t actually make a difference to the final performance of a model as big as GPT-4 (except maybe decrease the prevalence of glitch tokens)</div><br/><div id="36251150" class="c"><input type="checkbox" id="c-36251150" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36249668">root</a><span>|</span><a href="#36251011">parent</a><span>|</span><a href="#36251410">next</a><span>|</span><label class="collapse" for="c-36251150">[-]</label><label class="expand" for="c-36251150">[2 more]</label></div><br/><div class="children"><div class="content">&gt; wouldn&#x27;t actually make a difference to the final performance<p>Doesn&#x27;t that assume that the embeddings learned are in some sense &quot;perfect&quot;? Is that actually the case in practice?<p>I would expect the learned embeddings to have some errors, especially for the rarer ones that have few examples available for the model to learn from.<p>I also thought that explicitly accounting for symmetries always improved model performance, because then it doesn&#x27;t waste parameters learning things that aren&#x27;t unique and interesting pieces of information.</div><br/><div id="36251492" class="c"><input type="checkbox" id="c-36251492" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#36249668">root</a><span>|</span><a href="#36251150">parent</a><span>|</span><a href="#36251410">next</a><span>|</span><label class="collapse" for="c-36251492">[-]</label><label class="expand" for="c-36251492">[1 more]</label></div><br/><div class="children"><div class="content">Thing is, when you consider the tasks you actually want to optimize the models for, quite a few things mentioned in this discussion - e.g. correctly learn how to capitalise, make all-caps, count syllables, act on specific counts of letters - fall in the category of uninteresting things you don&#x27;t want to waste parameters on. Sure, they&#x27;d help with some trick questions that refer to the peculiarities of how exactly we encode stuff in letters, but that&#x27;s the whole thing we want to abstract away, going beyond textual encoding (or verbal encoding or pictures as rectangles of pixels) towards what the utterance means - like, not only we want to abstract away from spelling mistakes or variations, but also much larger changes  to text like different grammar structures to say the same thing, or even saying the same thing in a different language in a different alphabet.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36251410" class="c"><input type="checkbox" id="c-36251410" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#36249668">parent</a><span>|</span><a href="#36249758">prev</a><span>|</span><a href="#36249927">next</a><span>|</span><label class="collapse" for="c-36251410">[-]</label><label class="expand" for="c-36251410">[1 more]</label></div><br/><div class="children"><div class="content">You have to represent spaces in <i>some</i> way (you want to make a distinction between therapist and the rapist), different tokenizers do it differently - one option is to include space as part of the token, another commonly used option is to include the <i>lack</i> of space as part of the token by adding a specific mark representing &quot;the word goes on&quot; at the end.</div><br/></div></div><div id="36249927" class="c"><input type="checkbox" id="c-36249927" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36249668">parent</a><span>|</span><a href="#36251410">prev</a><span>|</span><a href="#36250040">next</a><span>|</span><label class="collapse" for="c-36249927">[-]</label><label class="expand" for="c-36249927">[1 more]</label></div><br/><div class="children"><div class="content">This can vary by BPE tokenizer. The original GPT-2&#x2F;GPT-3 was weirder about it.</div><br/></div></div></div></div><div id="36250040" class="c"><input type="checkbox" id="c-36250040" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#36249668">prev</a><span>|</span><a href="#36251687">next</a><span>|</span><label class="collapse" for="c-36250040">[-]</label><label class="expand" for="c-36250040">[4 more]</label></div><br/><div class="children"><div class="content">Has anyone ever tried a GPT trained on, say, 256 tokens representing bytes in a byte stream or even more simply binary digits?<p>I imagine there are efficiency trade-offs but I just wonder if it works at all.</div><br/><div id="36251573" class="c"><input type="checkbox" id="c-36251573" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#36250040">parent</a><span>|</span><a href="#36251028">next</a><span>|</span><label class="collapse" for="c-36251573">[-]</label><label class="expand" for="c-36251573">[1 more]</label></div><br/><div class="children"><div class="content">Sure, the concept has been explored; for example see the classic 2015 Karpathy&#x27;s <a href="http:&#x2F;&#x2F;karpathy.github.io&#x2F;2015&#x2F;05&#x2F;21&#x2F;rnn-effectiveness&#x2F;" rel="nofollow">http:&#x2F;&#x2F;karpathy.github.io&#x2F;2015&#x2F;05&#x2F;21&#x2F;rnn-effectiveness&#x2F;</a> as a cool description of a character-level model.<p>IIRC the early papers on subword tokenization also sometimes included explicit comparisons with character-level models, but people don&#x27;t do it nowadays because there&#x27;s a clear consensus on the expected outcome - yes, it works, but it&#x27;s simply worse.<p>Technically it&#x27;s the exact outcome that you get if you put in a vocabulary size of 256 (and do tokenization on byte-level, not unicode), so it&#x27;s just an extreme case of vocabulary size choice, and there&#x27;s enough research on how vocabulary size affects stuff to assume that 256 is not an optimal size.<p>You can do it for exploring capabilities though - see &quot;Bytes is all you need&quot; <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36176756" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36176756</a> discussion on trying to abstract away complex file formats by just passing the bytes of the file to the neural network directly - again, it obviously works worse, but it kind of works.</div><br/></div></div><div id="36251028" class="c"><input type="checkbox" id="c-36251028" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#36250040">parent</a><span>|</span><a href="#36251573">prev</a><span>|</span><a href="#36250125">next</a><span>|</span><label class="collapse" for="c-36251028">[-]</label><label class="expand" for="c-36251028">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure it would work, but there are obvious downsides (slower and less history) with few upsides (simpler, no glitch tokens)</div><br/></div></div><div id="36250125" class="c"><input type="checkbox" id="c-36250125" checked=""/><div class="controls bullet"><span class="by">sandinmyjoints</span><span>|</span><a href="#36250040">parent</a><span>|</span><a href="#36251028">prev</a><span>|</span><a href="#36251687">next</a><span>|</span><label class="collapse" for="c-36250125">[-]</label><label class="expand" for="c-36250125">[1 more]</label></div><br/><div class="children"><div class="content">Not a GPT, but I think Megabyte does that.</div><br/></div></div></div></div><div id="36251687" class="c"><input type="checkbox" id="c-36251687" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#36250040">prev</a><span>|</span><a href="#36250697">next</a><span>|</span><label class="collapse" for="c-36251687">[-]</label><label class="expand" for="c-36251687">[3 more]</label></div><br/><div class="children"><div class="content">I didnt fully understand tokens, and I went down this fun rabbit hole with GPT:<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;b8f06d5e-f2d9-47d7-9c60-69b088ceb135" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;b8f06d5e-f2d9-47d7-9c60-69b088...</a> - it turned into me asking it to help me with an &quot;understanding AI&quot; book definition, I learned a LOT in that thread.</div><br/><div id="36251853" class="c"><input type="checkbox" id="c-36251853" checked=""/><div class="controls bullet"><span class="by">plewd</span><span>|</span><a href="#36251687">parent</a><span>|</span><a href="#36250697">next</a><span>|</span><label class="collapse" for="c-36251853">[-]</label><label class="expand" for="c-36251853">[2 more]</label></div><br/><div class="children"><div class="content">Asking ChatGPT to develop a learning path&#x2F;syllabus to learn a topic seems really effective, I&#x27;ve never thought to try that before.</div><br/><div id="36251927" class="c"><input type="checkbox" id="c-36251927" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#36251687">root</a><span>|</span><a href="#36251853">parent</a><span>|</span><a href="#36250697">next</a><span>|</span><label class="collapse" for="c-36251927">[-]</label><label class="expand" for="c-36251927">[1 more]</label></div><br/><div class="children"><div class="content">Read that thread -- the summaries and glossary it gives are very helpful.<p>My brother needed to take a certification test for his (non-technical) job, and he had a bunch of dead-trees to study...<p>So I asked chatGPT to summarize each section of the study material (a national test for a trade) -- which it did<p>I then asked it for smaple questions which would reflect the test for each section, and it did.</div><br/></div></div></div></div></div></div><div id="36250697" class="c"><input type="checkbox" id="c-36250697" checked=""/><div class="controls bullet"><span class="by">tabtab</span><span>|</span><a href="#36251687">prev</a><span>|</span><a href="#36250855">next</a><span>|</span><label class="collapse" for="c-36250697">[-]</label><label class="expand" for="c-36250697">[2 more]</label></div><br/><div class="children"><div class="content">Replace them with corresponding emoji&#x27;s, and reinvent hieroglyphics :-)</div><br/><div id="36250725" class="c"><input type="checkbox" id="c-36250725" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#36250697">parent</a><span>|</span><a href="#36250855">next</a><span>|</span><label class="collapse" for="c-36250725">[-]</label><label class="expand" for="c-36250725">[1 more]</label></div><br/><div class="children"><div class="content">Rebus ftw : <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rebus" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rebus</a></div><br/></div></div></div></div><div id="36250855" class="c"><input type="checkbox" id="c-36250855" checked=""/><div class="controls bullet"><span class="by">buggythebug</span><span>|</span><a href="#36250697">prev</a><span>|</span><label class="collapse" for="c-36250855">[-]</label><label class="expand" for="c-36250855">[1 more]</label></div><br/><div class="children"><div class="content">Like Arcade tokens?</div><br/></div></div></div></div></div></div></div></body></html>