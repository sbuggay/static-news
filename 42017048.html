<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730538068481" as="style"/><link rel="stylesheet" href="styles.css?v=1730538068481"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2410.23168">TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>og_kalu</span> | <span>24 comments</span></div><br/><div><div id="42017601" class="c"><input type="checkbox" id="c-42017601" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#42020432">next</a><span>|</span><label class="collapse" for="c-42017601">[-]</label><label class="expand" for="c-42017601">[9 more]</label></div><br/><div class="children"><div class="content">The authors factorize every weight matrix with an attention mechanism:<p><pre><code>  weight = attention(token_query, weight_keys, weight_values).
</code></pre>
In other words, they query weight_keys to fetch the weight_values, and mix them to compute each weight on the spot.<p>Increasing model size becomes a matter of adding more weight_keys and weight_values, and incrementally training them.<p>Simple, clever, and it seems to work well. Beautiful.</div><br/><div id="42018030" class="c"><input type="checkbox" id="c-42018030" checked=""/><div class="controls bullet"><span class="by">szcs</span><span>|</span><a href="#42017601">parent</a><span>|</span><a href="#42018277">next</a><span>|</span><label class="collapse" for="c-42018030">[-]</label><label class="expand" for="c-42018030">[5 more]</label></div><br/><div class="children"><div class="content">There is a particularly nice geometric interpretation of attention I just realised recently in a flash of enlightenment, best explained with an interactive Desmos plot (black dot is draggable):<p><a href="https:&#x2F;&#x2F;www.desmos.com&#x2F;calculator&#x2F;3rtqsyapxo" rel="nofollow">https:&#x2F;&#x2F;www.desmos.com&#x2F;calculator&#x2F;3rtqsyapxo</a><p>The above assumes the columns of K are normalised but bear with me. K and V together form a vector database. V are the payloads, each row containing a vector of data. K describes the position of these points in space, on the surface of a hypershpere. The query vector describes the query into the database: the vector direction describes the point in space that&#x27;s being queried, the vector magnitude describes the radius of the query. The result is the weighted average of vectors from V, weighted by their distance from the query vector scaled by the query radius (which has a smooth Gaussian falloff). A recent paper from Nvidia I recommend, which derives a significant speedup by normalising vectors to a hypershpere: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2410.01131v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2410.01131v1</a></div><br/><div id="42022166" class="c"><input type="checkbox" id="c-42022166" checked=""/><div class="controls bullet"><span class="by">jdthedisciple</span><span>|</span><a href="#42017601">root</a><span>|</span><a href="#42018030">parent</a><span>|</span><a href="#42019105">next</a><span>|</span><label class="collapse" for="c-42022166">[-]</label><label class="expand" for="c-42022166">[3 more]</label></div><br/><div class="children"><div class="content">It looks fascinating, but i don&#x27;t understand it. I&#x27;m haven&#x27;t gone yet deeply into the theory of attention networks.<p>Can you explain the desmos plot in simple terms?</div><br/><div id="42024406" class="c"><input type="checkbox" id="c-42024406" checked=""/><div class="controls bullet"><span class="by">szcs</span><span>|</span><a href="#42017601">root</a><span>|</span><a href="#42022166">parent</a><span>|</span><a href="#42019105">next</a><span>|</span><label class="collapse" for="c-42024406">[-]</label><label class="expand" for="c-42024406">[2 more]</label></div><br/><div class="children"><div class="content">Attention is a 3 matrix product, s(QK)V where s is softmax. Each matrix has as many rows (Q and V) or columns (K) as many tokens you have in your context. The plot looks at the processing of a single row of Q (predicting a single token from previous ones) called q. q is a 2 element vector and is visualised as the draggable dot (imagine a line from the origin to the dot). The K matrix is shown as green dots, each previous token in the context window is represented as a separate dot. The distance of a blue dot from a corresponding green dot represents how much information from that token gets mixed into the output of the query. The green dots form a hypersphere, a 1D manifold in 2D space. In a real network it would be more like e.g. a 127D manifold in 128D space but the analogy works there as well. You can see how the query gathers information stored on the surface of the manifold by specifying a region and volume of space specified through q&#x27;s direction and magnitude respectively.</div><br/><div id="42024938" class="c"><input type="checkbox" id="c-42024938" checked=""/><div class="controls bullet"><span class="by">jdthedisciple</span><span>|</span><a href="#42017601">root</a><span>|</span><a href="#42024406">parent</a><span>|</span><a href="#42019105">next</a><span>|</span><label class="collapse" for="c-42024938">[-]</label><label class="expand" for="c-42024938">[1 more]</label></div><br/><div class="children"><div class="content">oh wow that makes more sense now!<p>and what are the orange dots? sorry if I missed that</div><br/></div></div></div></div></div></div><div id="42019105" class="c"><input type="checkbox" id="c-42019105" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#42017601">root</a><span>|</span><a href="#42018030">parent</a><span>|</span><a href="#42022166">prev</a><span>|</span><a href="#42018277">next</a><span>|</span><label class="collapse" for="c-42019105">[-]</label><label class="expand" for="c-42019105">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I believe this intuition first introduced by the Neural Turing Machine line-of-work and later simplified into AIAYN paper (NTM maintains &quot;external memory&quot; a.k.a. weight_keys, weight_values here).<p>Disclaimer: these are from my memory, which can be wrong entirely.</div><br/></div></div></div></div><div id="42018277" class="c"><input type="checkbox" id="c-42018277" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42017601">parent</a><span>|</span><a href="#42018030">prev</a><span>|</span><a href="#42020432">next</a><span>|</span><label class="collapse" for="c-42018277">[-]</label><label class="expand" for="c-42018277">[3 more]</label></div><br/><div class="children"><div class="content">I believe there have been studies showing that the attention mechanism allows estimation of gradients for one-shot learning (i.e, based on what you tell the model you want in the input, it will use attention to &#x27;update&#x27; the weights of the linear layers to &#x27;learn&#x27; new information). This seems to be taking that one step further and just using attention for the weight estimations itself. The key insight here is that by adding more tokens to the weight estimation calculation, you can get more degrees of freedom.<p>Total aside, but imagining how many levels of functions are present in the calculation of each activation here, and thinking about how regular old differentiation and gradient descent actually work to train these nested parameters, is truly amazing, in my opinion.</div><br/><div id="42018546" class="c"><input type="checkbox" id="c-42018546" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#42017601">root</a><span>|</span><a href="#42018277">parent</a><span>|</span><a href="#42020432">next</a><span>|</span><label class="collapse" for="c-42018546">[-]</label><label class="expand" for="c-42018546">[2 more]</label></div><br/><div class="children"><div class="content">Yeah. This thing is &quot;assembling a different transformer&quot; on the spot for each token.<p>If one thinks about it for more than a moment, it&#x27;s kind of incredible that it works.</div><br/><div id="42019806" class="c"><input type="checkbox" id="c-42019806" checked=""/><div class="controls bullet"><span class="by">0-_-0</span><span>|</span><a href="#42017601">root</a><span>|</span><a href="#42018546">parent</a><span>|</span><a href="#42020432">next</a><span>|</span><label class="collapse" for="c-42019806">[-]</label><label class="expand" for="c-42019806">[1 more]</label></div><br/><div class="children"><div class="content">I think the same about regular neutral networks</div><br/></div></div></div></div></div></div></div></div><div id="42020432" class="c"><input type="checkbox" id="c-42020432" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#42017601">prev</a><span>|</span><a href="#42020695">next</a><span>|</span><label class="collapse" for="c-42020432">[-]</label><label class="expand" for="c-42020432">[3 more]</label></div><br/><div class="children"><div class="content">I would like to see a comparison for the inference time compute between a regular transformer and this. Iâm assuming token&#x2F;s is lower since you need to compute the weights of the model for each token prior to the actual attention calculations for the sequence position.</div><br/><div id="42024354" class="c"><input type="checkbox" id="c-42024354" checked=""/><div class="controls bullet"><span class="by">paraschopra</span><span>|</span><a href="#42020432">parent</a><span>|</span><a href="#42020782">next</a><span>|</span><label class="collapse" for="c-42024354">[-]</label><label class="expand" for="c-42024354">[1 more]</label></div><br/><div class="children"><div class="content">Why would it be higher? You can keep KV cache precomputed like before.</div><br/></div></div><div id="42020782" class="c"><input type="checkbox" id="c-42020782" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42020432">parent</a><span>|</span><a href="#42024354">prev</a><span>|</span><a href="#42020695">next</a><span>|</span><label class="collapse" for="c-42020782">[-]</label><label class="expand" for="c-42020782">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that figure 5 in the paper? It&#x27;s for training not inference, but presumably if training is faster then inference would be too. Because they don&#x27;t increase the dimension of the text tokens when scaling up, which reduces the compute needed for attention. But potentially limits how well the text token attention can keep track of things, because it&#x27;s got less space for passing things along.</div><br/></div></div></div></div><div id="42020695" class="c"><input type="checkbox" id="c-42020695" checked=""/><div class="controls bullet"><span class="by">goldenshale</span><span>|</span><a href="#42020432">prev</a><span>|</span><a href="#42019519">next</a><span>|</span><label class="collapse" for="c-42020695">[-]</label><label class="expand" for="c-42020695">[1 more]</label></div><br/><div class="children"><div class="content">This is a great idea.  Being able to dynamically scale up model sizes as datasets and use cases expand without needing to retrain from scratch could enable a Cambrian explosion of interesting stuff building on top of a Llama type model trained in this way.</div><br/></div></div><div id="42019519" class="c"><input type="checkbox" id="c-42019519" checked=""/><div class="controls bullet"><span class="by">davesque</span><span>|</span><a href="#42020695">prev</a><span>|</span><a href="#42019701">next</a><span>|</span><label class="collapse" for="c-42019519">[-]</label><label class="expand" for="c-42019519">[4 more]</label></div><br/><div class="children"><div class="content">Seems like a big deal. I feel like this could enable a new level of modularity and compatibility between publicly available weight sets, assuming they use similar channel dimensions. Maybe it also provides a nice formalism for thinking about fine tuning, where you could adopt certain heuristics for adding&#x2F;removing key-value pairs from the Pattention layers.<p>One interesting thing to note: sounds like model scaling happens on the fly by adding key-value pairs as rows in the K and V matrices on the Pattention layer. That suggests that weights represented by tokens in the first rows may be more important than weights in later rows. There may be a lot you could do with that ordering of weights in terms of pruning and such.</div><br/><div id="42020472" class="c"><input type="checkbox" id="c-42020472" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#42019519">parent</a><span>|</span><a href="#42019701">next</a><span>|</span><label class="collapse" for="c-42020472">[-]</label><label class="expand" for="c-42020472">[3 more]</label></div><br/><div class="children"><div class="content">Unless Iâm reading it wrong I donât think rows matter. Attention doesnât take into account sequence position natively, thatâs why positional encodings exist.</div><br/><div id="42020694" class="c"><input type="checkbox" id="c-42020694" checked=""/><div class="controls bullet"><span class="by">davesque</span><span>|</span><a href="#42019519">root</a><span>|</span><a href="#42020472">parent</a><span>|</span><a href="#42019701">next</a><span>|</span><label class="collapse" for="c-42020694">[-]</label><label class="expand" for="c-42020694">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m talking about the rows in the new K and V matrices introduced by the paper, not rows in the input sequence. The ordering of rows in the new K and V matrices does matter in the sense that rows that appear further down were added later in the training process to add new parameter tokens during scaling. So those newer parameters <i>may</i> represent knowledge that is less fundamental and more about fine tuning on the training set.</div><br/><div id="42024361" class="c"><input type="checkbox" id="c-42024361" checked=""/><div class="controls bullet"><span class="by">paraschopra</span><span>|</span><a href="#42019519">root</a><span>|</span><a href="#42020694">parent</a><span>|</span><a href="#42019701">next</a><span>|</span><label class="collapse" for="c-42024361">[-]</label><label class="expand" for="c-42024361">[1 more]</label></div><br/><div class="children"><div class="content">But after adding new rows, I think entire network is retrained.</div><br/></div></div></div></div></div></div></div></div><div id="42019701" class="c"><input type="checkbox" id="c-42019701" checked=""/><div class="controls bullet"><span class="by">c0g</span><span>|</span><a href="#42019519">prev</a><span>|</span><a href="#42021430">next</a><span>|</span><label class="collapse" for="c-42019701">[-]</label><label class="expand" for="c-42019701">[1 more]</label></div><br/><div class="children"><div class="content">Surprised not to see a comparison to <a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;augmenting-self-attention-with-persistent" rel="nofollow">https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;augmenting-self-attention-w...</a></div><br/></div></div><div id="42021430" class="c"><input type="checkbox" id="c-42021430" checked=""/><div class="controls bullet"><span class="by">mentalically</span><span>|</span><a href="#42019701">prev</a><span>|</span><a href="#42020816">next</a><span>|</span><label class="collapse" for="c-42021430">[-]</label><label class="expand" for="c-42021430">[1 more]</label></div><br/><div class="children"><div class="content">Eventually people will figure out how to nest neural networks in the nodes and edges of an arbitrary graph.</div><br/></div></div><div id="42020816" class="c"><input type="checkbox" id="c-42020816" checked=""/><div class="controls bullet"><span class="by">davesque</span><span>|</span><a href="#42021430">prev</a><span>|</span><a href="#42020976">next</a><span>|</span><label class="collapse" for="c-42020816">[-]</label><label class="expand" for="c-42020816">[1 more]</label></div><br/><div class="children"><div class="content">Seems like a lot of existing models could be converted to this token parameter representation.</div><br/></div></div><div id="42020976" class="c"><input type="checkbox" id="c-42020976" checked=""/><div class="controls bullet"><span class="by">ml_thoughts</span><span>|</span><a href="#42020816">prev</a><span>|</span><a href="#42020745">next</a><span>|</span><label class="collapse" for="c-42020976">[-]</label><label class="expand" for="c-42020976">[1 more]</label></div><br/><div class="children"><div class="content">This seems closely related to the &quot;Mixtral&quot; approach of a mixture-of-experts transformer [1]... I&#x27;m not claiming the approach is not original, it just helped me understand what was going on.<p>Consider a case of two &quot;experts&quot; or two &quot;value parameter tokens.&quot;<p>The mixture of experts has a &quot;router&quot; network that provides a weight to each expert (through a softmax) conditional on an input. The output is a (sparse) weighted sum of the outputs of the experts.<p>The TokenFormer has an &quot;attention&quot; layer combines the token and a key value to provide a weight to each &quot;value parameter&quot; token. A(B+C) = AB + AC definitionally, so this is like applying a weighted sum of distinct transformations.<p>I think the differences are: a) where the non-linearity hits (the above description doesn&#x27;t consider an activation function), b) this attention softmax is not (necessarily) sparse, c) that &quot;mixtral&quot; networks only replace the feed-forward components of the layer, and d) that extending a &quot;mixtral&quot; approach would require re-training the &quot;router&quot; layers.<p>It seems like (d) is maybe the nicest feature here... my intuition would think (a) doesn&#x27;t matter much, (b) is debatable (how close a sparse-MoE can approximate a dense-MoE), (c) has probably been tried (guessing the ffwd limitation was just &quot;more-bang-for-buck-given-parameters&quot; not an oversight)...<p>... I wonder, though, if there might be diminishing returns here (I believe that Mixture-of-Experts tends to struggle with imbalanced &quot;winner-take-all&quot; dynamics, since &quot;early&quot; winners get more gradient signal to improve their weights) and how different this would have been from going from 3x7B to a 8x7B to a 24x7B training approach (with a &quot;retrain routing networks&quot; step).<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.04088" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.04088</a></div><br/></div></div><div id="42020745" class="c"><input type="checkbox" id="c-42020745" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42020976">prev</a><span>|</span><a href="#42019937">next</a><span>|</span><label class="collapse" for="c-42020745">[-]</label><label class="expand" for="c-42020745">[1 more]</label></div><br/><div class="children"><div class="content">Seems this would naturally translate into a mixture of experts by using a &quot;hard&quot; attention function so that only a fixed amount of weight tokens get included in the calculation.</div><br/></div></div><div id="42019937" class="c"><input type="checkbox" id="c-42019937" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#42020745">prev</a><span>|</span><label class="collapse" for="c-42019937">[-]</label><label class="expand" for="c-42019937">[1 more]</label></div><br/><div class="children"><div class="content">This could be revolutionary. The PPL&#x2F;compute graphs are damning. If the Transformer is a function, then the TokenFormer feels like a higher-order function. Perhaps this approach is a natural direction for producing System Two reasoning? There&#x27;s so much to digest here...</div><br/></div></div></div></div></div></div></div></body></html>