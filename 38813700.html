<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704013264123" as="style"/><link rel="stylesheet" href="styles.css?v=1704013264123"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://jamesg.blog/2023/12/29/compression-adventures/">Adventures with compression</a> <span class="domain">(<a href="https://jamesg.blog">jamesg.blog</a>)</span></div><div class="subtext"><span>zerojames</span> | <span>12 comments</span></div><br/><div><div id="38822390" class="c"><input type="checkbox" id="c-38822390" checked=""/><div class="controls bullet"><span class="by">pitdicker</span><span>|</span><a href="#38821455">next</a><span>|</span><label class="collapse" for="c-38822390">[-]</label><label class="expand" for="c-38822390">[1 more]</label></div><br/><div class="children"><div class="content">From <a href="http:&#x2F;&#x2F;prize.hutter1.net&#x2F;" rel="nofollow">http:&#x2F;&#x2F;prize.hutter1.net&#x2F;</a>:<p>Being able to compress well is closely related to intelligence as explained below. While intelligence is a slippery concept, file sizes are hard numbers. Wikipedia is an extensive snapshot of Human Knowledge. If you can compress the first 1GB of Wikipedia better than your predecessors, your (de)compressor likely has to be smart(er). The intention of this prize is to encourage development of intelligent compressors&#x2F;programs as a path to AGI.<p>The Task:
Losslessly compress the 1GB file enwik9 to less than 114MB. More precisely:<p>- Create a Linux or Windows compressor comp.exe of size S1 that compresses enwik9 to archive.exe of size S2 such that S:=S1+S2 &lt; L := 114&#x27;156&#x27;155 (previous record).<p>- If run, archive.exe produces (without input from other sources) a 10^9 byte file that is identical to enwik9.<p>- If we can verify your claim, you are eligible for a prize of 500&#x27;000€×(1-S&#x2F;L). Minimum claim is 5&#x27;000€ (1% improvement).<p>- Restrictions: Must run in ≲50 hours using a single CPU core and &lt;10GB RAM and &lt;100GB HDD on our test machine.</div><br/></div></div><div id="38821455" class="c"><input type="checkbox" id="c-38821455" checked=""/><div class="controls bullet"><span class="by">d_burfoot</span><span>|</span><a href="#38822390">prev</a><span>|</span><a href="#38822333">next</a><span>|</span><label class="collapse" for="c-38821455">[-]</label><label class="expand" for="c-38821455">[6 more]</label></div><br/><div class="children"><div class="content">Yes, compression is definitely a rabbit hole - essentially an infinite rabbit hole. Optimal compression requires perfect understanding of the underlying material. So to compress English text well, you need to understand English very well - it&#x27;s grammar, morphology, semantics, etc.<p>I crawled very deep into this particular rabbit hole. I built text compressors that exploited linguistic structure. I was able to quantify improvements in my understanding of text by observing improvements in the compression rate. The beautiful aspect of this project is its rigor: lossless compression is such a demanding challenge that you cannot possibly deceive yourself. If you have a bug in your code, you will either decode the wrong result or fail to get a codelength reduction. Using this methodology I was able to build a statistical parser without using any labeled training data.<p>Unfortunately for me, the advent of GPT and other DNN-based LLMs made this research obsolete. It may still be interesting for humans to build theories of syntax and grammar, but GPT has this information encoded implicitly in its neural weights in a much more sophisticated way than linguistic theories can achieve.</div><br/><div id="38821906" class="c"><input type="checkbox" id="c-38821906" checked=""/><div class="controls bullet"><span class="by">refset</span><span>|</span><a href="#38821455">parent</a><span>|</span><a href="#38821728">next</a><span>|</span><label class="collapse" for="c-38821906">[-]</label><label class="expand" for="c-38821906">[1 more]</label></div><br/><div class="children"><div class="content">Compression is, ultimately, AI.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38399753">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38399753</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31923231">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31923231</a></div><br/></div></div><div id="38821728" class="c"><input type="checkbox" id="c-38821728" checked=""/><div class="controls bullet"><span class="by">xnx</span><span>|</span><a href="#38821455">parent</a><span>|</span><a href="#38821906">prev</a><span>|</span><a href="#38822333">next</a><span>|</span><label class="collapse" for="c-38821728">[-]</label><label class="expand" for="c-38821728">[4 more]</label></div><br/><div class="children"><div class="content">&gt; GPT has this information encoded implicitly in its neural weights in a much more sophisticated way than linguistic theories can achieve<p>&quot;More data beats clever algorithms&quot; strikes again.</div><br/><div id="38822165" class="c"><input type="checkbox" id="c-38822165" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#38821455">root</a><span>|</span><a href="#38821728">parent</a><span>|</span><a href="#38822333">next</a><span>|</span><label class="collapse" for="c-38822165">[-]</label><label class="expand" for="c-38822165">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t gpt like 800gb big? I can compress a 1gb file to 0 bytes if i am allowed 1gb of additional data.</div><br/><div id="38822200" class="c"><input type="checkbox" id="c-38822200" checked=""/><div class="controls bullet"><span class="by">ChadNauseam</span><span>|</span><a href="#38821455">root</a><span>|</span><a href="#38822165">parent</a><span>|</span><a href="#38822333">next</a><span>|</span><label class="collapse" for="c-38822200">[-]</label><label class="expand" for="c-38822200">[2 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t if you have to pick the 1gb of additional data before you see the 1gb file you&#x27;re supposed to compress (which is the situation gpt is in)</div><br/><div id="38822463" class="c"><input type="checkbox" id="c-38822463" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#38821455">root</a><span>|</span><a href="#38822200">parent</a><span>|</span><a href="#38822333">next</a><span>|</span><label class="collapse" for="c-38822463">[-]</label><label class="expand" for="c-38822463">[1 more]</label></div><br/><div class="children"><div class="content">Although in this case, we know the data comes from wikipedia and i assume gpt was trained on wikipedia, so it really does seem unfair.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38822333" class="c"><input type="checkbox" id="c-38822333" checked=""/><div class="controls bullet"><span class="by">rcthompson</span><span>|</span><a href="#38821455">prev</a><span>|</span><a href="#38822519">next</a><span>|</span><label class="collapse" for="c-38822333">[-]</label><label class="expand" for="c-38822333">[2 more]</label></div><br/><div class="children"><div class="content">Random compression-related story time:<p>I once had to run a program hundreds of times and capture all the stack traces to help the author catch a stochastic bug. I zipped up all the stack traces and sent the resulting file to the author. The compressed file was only a few dozen MB, and the uncompressed stack traces all together were only a few hundred MB... on my ZFS filesystem with compression enabled. When the author unzipped the file on their end, they were surprised to find around 10 GB of very repetitive stack traces.</div><br/><div id="38822484" class="c"><input type="checkbox" id="c-38822484" checked=""/><div class="controls bullet"><span class="by">waltbosz</span><span>|</span><a href="#38822333">parent</a><span>|</span><a href="#38822519">next</a><span>|</span><label class="collapse" for="c-38822484">[-]</label><label class="expand" for="c-38822484">[1 more]</label></div><br/><div class="children"><div class="content">My favorite uncompressed file size surprise comes from the world of Dreamcast game piracy.<p>Pirates would publish game disc images in a ZIP file that was say 100mb. When uncompressed the iso file would be 750mb. The reason for the huge compression ratio was because the iso would contain a 600mb empty file, which compressed down to almost nothing.<p>The purpose was so the when the image was burned to a cdr, the game data would be burned on the outer part of the physical media, and the empty file burned to the inner. The goal was to speed up read times when playing a game.</div><br/></div></div></div></div><div id="38822519" class="c"><input type="checkbox" id="c-38822519" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#38822333">prev</a><span>|</span><label class="collapse" for="c-38822519">[-]</label><label class="expand" for="c-38822519">[2 more]</label></div><br/><div class="children"><div class="content">So has anyone tried lossless compression with an LLM?<p>Method: create a list of next token predictions, ordered by describing probability, and encode the actual token as its position in the ordered list (so, eg, the most likely token is encoded as zero, second most likely as 1, etc).<p>If the LLM is good, this should provide an excellent encoding.</div><br/><div id="38822643" class="c"><input type="checkbox" id="c-38822643" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#38822519">parent</a><span>|</span><label class="collapse" for="c-38822643">[-]</label><label class="expand" for="c-38822643">[1 more]</label></div><br/><div class="children"><div class="content">Yes, next token prediction is wxactly what you need for compression. Many papers on this, recent one<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.10668.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.10668.pdf</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>