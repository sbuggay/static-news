<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1694509266329" as="style"/><link rel="stylesheet" href="styles.css?v=1694509266329"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/12/HonestHypothesisTestingNonStandardErrors.pdf">Non-Standard Errors [pdf]</a> <span class="domain">(<a href="https://statmodeling.stat.columbia.edu">statmodeling.stat.columbia.edu</a>)</span></div><div class="subtext"><span>luu</span> | <span>11 comments</span></div><br/><div><div id="37474284" class="c"><input type="checkbox" id="c-37474284" checked=""/><div class="controls bullet"><span class="by">derbOac</span><span>|</span><a href="#37474182">next</a><span>|</span><label class="collapse" for="c-37474284">[-]</label><label class="expand" for="c-37474284">[3 more]</label></div><br/><div class="children"><div class="content">Their &quot;team quality&quot; variable caused me to tune out really quickly. Sure, they found out it didn&#x27;t explain much but it was enough to change the way I approached the rest of it. Reinforcing one academic problem while attacking another seems like two steps forward and one step back.<p>This phenomenon — variability in analytic results on the same sample — has been demonstrated before.</div><br/><div id="37474644" class="c"><input type="checkbox" id="c-37474644" checked=""/><div class="controls bullet"><span class="by">nequo</span><span>|</span><a href="#37474284">parent</a><span>|</span><a href="#37477662">next</a><span>|</span><label class="collapse" for="c-37474644">[-]</label><label class="expand" for="c-37474644">[1 more]</label></div><br/><div class="children"><div class="content">They describe how they constructed their measure of “team quality.” They took the first principal component of five measures: publications in top journals, self-assessed expertise in the literature, experience with big data, academic seniority, and team size.<p>These sound fairly reasonable to me. Publishing in the top journals is easier with the right connections which makes the first measure questionable. Tenure is also harder to achieve for women which makes the fourth measure questionable. But even if these measures are driven by connections and gender norms, they capture something that is interesting to study further if there’s an association found.</div><br/></div></div></div></div><div id="37474182" class="c"><input type="checkbox" id="c-37474182" checked=""/><div class="controls bullet"><span class="by">themusicgod1</span><span>|</span><a href="#37474284">prev</a><span>|</span><label class="collapse" for="c-37474182">[-]</label><label class="expand" for="c-37474182">[7 more]</label></div><br/><div class="children"><div class="content">&gt; a bazillion authors<p>was this necessary?</div><br/><div id="37474225" class="c"><input type="checkbox" id="c-37474225" checked=""/><div class="controls bullet"><span class="by">yellowstuff</span><span>|</span><a href="#37474182">parent</a><span>|</span><a href="#37474347">next</a><span>|</span><label class="collapse" for="c-37474225">[-]</label><label class="expand" for="c-37474225">[1 more]</label></div><br/><div class="children"><div class="content">Yes. It&#x27;s an analysis of the differences in results generated by different teams of academics researching the same data. That required lots of researchers, who all deserve credit for their contributions.</div><br/></div></div><div id="37474347" class="c"><input type="checkbox" id="c-37474347" checked=""/><div class="controls bullet"><span class="by">labcomputer</span><span>|</span><a href="#37474182">parent</a><span>|</span><a href="#37474225">prev</a><span>|</span><a href="#37477664">next</a><span>|</span><label class="collapse" for="c-37474347">[-]</label><label class="expand" for="c-37474347">[4 more]</label></div><br/><div class="children"><div class="content">People complain about the weirdest thi... the first <i>five</i> pages of the paper contain nothing but the list of authors and their affiliations?!?</div><br/><div id="37476152" class="c"><input type="checkbox" id="c-37476152" checked=""/><div class="controls bullet"><span class="by">csb6</span><span>|</span><a href="#37474182">root</a><span>|</span><a href="#37474347">parent</a><span>|</span><a href="#37477664">next</a><span>|</span><label class="collapse" for="c-37476152">[-]</label><label class="expand" for="c-37476152">[3 more]</label></div><br/><div class="children"><div class="content">From the abstract:<p>&gt; We claim that EGP variation across researchers adds uncertainty: non-standard errors. To study them, we let 164 teams test six hypotheses on the same sample.</div><br/><div id="37476335" class="c"><input type="checkbox" id="c-37476335" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#37474182">root</a><span>|</span><a href="#37476152">parent</a><span>|</span><a href="#37477664">next</a><span>|</span><label class="collapse" for="c-37476335">[-]</label><label class="expand" for="c-37476335">[2 more]</label></div><br/><div class="children"><div class="content">So those are the subjects, not co-authors.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>