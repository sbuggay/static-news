<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686128463708" as="style"/><link rel="stylesheet" href="styles.css?v=1686128463708"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/kdave/btrfsmaintenance">Scripts for Btrfs Maintenance</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>pantalaimon</span> | <span>37 comments</span></div><br/><div><div id="36220445" class="c"><input type="checkbox" id="c-36220445" checked=""/><div class="controls bullet"><span class="by">EscapeFromNY</span><span>|</span><a href="#36221004">next</a><span>|</span><label class="collapse" for="c-36220445">[-]</label><label class="expand" for="c-36220445">[24 more]</label></div><br/><div class="children"><div class="content">Was I supposed to be maintaining my btrfs partition all this time? I just formatted my disk as btrfs when I bought my laptop and haven&#x27;t thought about it since.</div><br/><div id="36221465" class="c"><input type="checkbox" id="c-36221465" checked=""/><div class="controls bullet"><span class="by">xtracto</span><span>|</span><a href="#36220445">parent</a><span>|</span><a href="#36220550">next</a><span>|</span><label class="collapse" for="c-36221465">[-]</label><label class="expand" for="c-36221465">[8 more]</label></div><br/><div class="children"><div class="content">Just recently I had a worst-case nightmare scenario with a single BTRFS  partition 5TB disk (no RAID or anything crazy).  It was a single BTRFS disk, single partition external USB disk running under Linux Mint.<p>At some point after I restarted the computer and it wouldn&#x27;t mount the partition saying that the path for mounting was already being used (even after restarting??), but the disk was not mounted.  Searching around apparently there is some kind of bug where Linux will &quot;cache&quot; the BTRFS UUID [1].<p>At the time, the &quot;solution&quot; I read was to change the UUID of the BTRFS partition running btrfstune. Which I did... and it supposedly changed successfully. Except that after trying to mount the partition again, it will tell me that there was an error in the partition: the tree had mismatching UUIDs :-&#x2F;. I tried updating the UUID several times, and the command ended in success, but the error remained. My BTRFS disk was officially broken...<p>After spending several hours trying to fix the issue following the BTRFS documentation (pretty shitty TBH), in the end, I ended up having to do `btrfs restore -iv ...`  to extract the data from the disk into some other external disk (formatted NTFS this time!!).  The command is still going, about 2 weeks later and I am slowly recovering my 5TB of data.<p>But the one thing clear to me is that I don&#x27;t trust BTRFS or it&#x27;s admin commands AT ALL after this experience. Once I finish recovering my data, I&#x27;ll nuke the partition, format the disk in NTFS and forget about this sour experience.<p>[1] <a href="https:&#x2F;&#x2F;unix.stackexchange.com&#x2F;questions&#x2F;603528&#x2F;why-am-i-getting-this-file-exists-error-trying-to-mount-unmounted-filesystem" rel="nofollow">https:&#x2F;&#x2F;unix.stackexchange.com&#x2F;questions&#x2F;603528&#x2F;why-am-i-get...</a></div><br/><div id="36223540" class="c"><input type="checkbox" id="c-36223540" checked=""/><div class="controls bullet"><span class="by">Arnavion</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36221465">parent</a><span>|</span><a href="#36221712">next</a><span>|</span><label class="collapse" for="c-36223540">[-]</label><label class="expand" for="c-36223540">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Searching around apparently there is some kind of bug where Linux will &quot;cache&quot; the BTRFS UUID [1].<p>It&#x27;s not a bug. It&#x27;s just a fact that every BTRFS filesystem visible to the kernel (mounted or not) should have a unique ID. Otherwise operations on one filesystem (including mounting the partition that contains it) can end up applying to the other filesystem.<p>&gt;At the time, the &quot;solution&quot; I read was to change the UUID of the BTRFS partition running btrfstune.<p>The solution would&#x27;ve been to find out why you have two partitions with the same filesystem ID and remove one of them. One example is that if you have the BTRFS filesystem on an LVM partition and then you clone the LVM partition, you end up with two BTRFS filesystems with the same ID. I accidentally encountered it while converting an unencrypted BTRFS parition into an encrypted one by dd&#x27;ing it into a new LUKS device - when I tried to mount the encrypted partition it ended mounting the unencrypted one.<p>Since you say it&#x27;s an external USB drive, perhaps it disconnected and reconnected uncleanly and the kernel thought there were two of that drive connected at the same time.</div><br/></div></div><div id="36221712" class="c"><input type="checkbox" id="c-36221712" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36221465">parent</a><span>|</span><a href="#36223540">prev</a><span>|</span><a href="#36223931">next</a><span>|</span><label class="collapse" for="c-36221712">[-]</label><label class="expand" for="c-36221712">[5 more]</label></div><br/><div class="children"><div class="content">I can echo&#x2F;support this claim - I wandered down the same rabbit hole.<p>It doesn&#x27;t even get much better with many drives and avoiding the docs&#x2F;tooling<p>BTRFS arrays will <i>consistently</i> corrupt <i>with my reset button</i> -- RAID10 on gen4 NVMe drives... while <i>LVM&#x2F;dm-raid</i> + traditional file systems are absolutely fine</div><br/><div id="36221867" class="c"><input type="checkbox" id="c-36221867" checked=""/><div class="controls bullet"><span class="by">xtracto</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36221712">parent</a><span>|</span><a href="#36222236">next</a><span>|</span><label class="collapse" for="c-36221867">[-]</label><label class="expand" for="c-36221867">[2 more]</label></div><br/><div class="children"><div class="content">&gt;BTRFS arrays will consistently corrupt with my reset button<p>YES!!!  Sorry to beat a dead horse, but it has been so frustrating for me. I thought I did something wrong, like, how could the partition break for doing nothing! Where did I fucked up?     I cannot imagine the experience with a RAIDed BTRFS array <i>shudders</i></div><br/><div id="36222159" class="c"><input type="checkbox" id="c-36222159" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36221867">parent</a><span>|</span><a href="#36222236">next</a><span>|</span><label class="collapse" for="c-36222159">[-]</label><label class="expand" for="c-36222159">[1 more]</label></div><br/><div class="children"><div class="content">I had a BTRFS raid corrupt because one of my HDD had a firmware bug and was silently discarding writes.<p>Sure... HDDs shouldn&#x27;t do that, but it&#x27;s literally the job of a BTRFS raid to protect me from such issues. The data was all still intact on the other drive. I could see it with dd, but BTRFS refused to read it.</div><br/></div></div></div></div><div id="36222236" class="c"><input type="checkbox" id="c-36222236" checked=""/><div class="controls bullet"><span class="by">curt15</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36221712">parent</a><span>|</span><a href="#36221867">prev</a><span>|</span><a href="#36223931">next</a><span>|</span><label class="collapse" for="c-36222236">[-]</label><label class="expand" for="c-36222236">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t copy-on-write supposed to be inherently resilient to crashes[1]? Why would btrfs be more susceptible to resets than traditional filesystems?<p>[1]: <a href="https:&#x2F;&#x2F;unix.stackexchange.com&#x2F;questions&#x2F;634050&#x2F;in-what-ways-is-the-cow-filesystem-an-improvement-over-the-journaling-filesystem" rel="nofollow">https:&#x2F;&#x2F;unix.stackexchange.com&#x2F;questions&#x2F;634050&#x2F;in-what-ways...</a></div><br/><div id="36222667" class="c"><input type="checkbox" id="c-36222667" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36222236">parent</a><span>|</span><a href="#36223931">next</a><span>|</span><label class="collapse" for="c-36222667">[-]</label><label class="expand" for="c-36222667">[1 more]</label></div><br/><div class="children"><div class="content">I doubt that because of the experience I just outlined<p>In theory perhaps, but absolutely not in practice. Making things atomic is tricky.</div><br/></div></div></div></div></div></div><div id="36223931" class="c"><input type="checkbox" id="c-36223931" checked=""/><div class="controls bullet"><span class="by">MaKey</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36221465">parent</a><span>|</span><a href="#36221712">prev</a><span>|</span><a href="#36220550">next</a><span>|</span><label class="collapse" for="c-36223931">[-]</label><label class="expand" for="c-36223931">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure you can fault BTRFS for the issue you&#x27;ve experienced. It sounds like your system had a problem which lead to you breaking your (likely perfectly fine) BTRFS file system.</div><br/></div></div></div></div><div id="36220550" class="c"><input type="checkbox" id="c-36220550" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#36220445">parent</a><span>|</span><a href="#36221465">prev</a><span>|</span><a href="#36220497">next</a><span>|</span><label class="collapse" for="c-36220550">[-]</label><label class="expand" for="c-36220550">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not benefiting from it as much as you could be; others have mentioned scrubbing<p>That will read all of the data from the storage device(s) checking for coherency.  In the case of redundancy <i>(ie: RAID1)</i> and corruption&#x2F;rot, would use the other copy to make things whole<p>Beyond that and trim, most of what&#x27;s in here is fairly specific. ie: avoiding ENOSPC or dealing with array changes<p><i>edit:</i> I think running this more than ~monthly is overzealous... and only really meaningful if you have redundancy<p>If memory serves, the checksums are validated when you read things anyway - so I question doing passes too aggressively. I&#x27;ll accept a bit for bitrot</div><br/></div></div><div id="36220497" class="c"><input type="checkbox" id="c-36220497" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36220445">parent</a><span>|</span><a href="#36220550">prev</a><span>|</span><a href="#36220505">next</a><span>|</span><label class="collapse" for="c-36220497">[-]</label><label class="expand" for="c-36220497">[8 more]</label></div><br/><div class="children"><div class="content">Since you have checkums available, you might as well do a periodic &quot;scrub&quot; and see if any of your data has been corrupted. Trim is useful for SSDs because it can make your drive&#x27;s firmware more effective at wear leveling. Balance is pretty workload-specific and I don&#x27;t have a heuristic for when it might make a difference. But being super unbalanced might cause performance issues.</div><br/><div id="36220693" class="c"><input type="checkbox" id="c-36220693" checked=""/><div class="controls bullet"><span class="by">EscapeFromNY</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220497">parent</a><span>|</span><a href="#36220511">next</a><span>|</span><label class="collapse" for="c-36220693">[-]</label><label class="expand" for="c-36220693">[2 more]</label></div><br/><div class="children"><div class="content">Scrubbing sounds useful. I&#x27;ll start doing that every so often.<p>I thought trim was enabled by default (<a href="https:&#x2F;&#x2F;wiki.archlinux.org&#x2F;title&#x2F;Btrfs#SSD_TRIM" rel="nofollow">https:&#x2F;&#x2F;wiki.archlinux.org&#x2F;title&#x2F;Btrfs#SSD_TRIM</a>). Does fstrim do something more than that or is it the same thing?</div><br/><div id="36221191" class="c"><input type="checkbox" id="c-36221191" checked=""/><div class="controls bullet"><span class="by">sweettea</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220693">parent</a><span>|</span><a href="#36220511">next</a><span>|</span><label class="collapse" for="c-36221191">[-]</label><label class="expand" for="c-36221191">[1 more]</label></div><br/><div class="children"><div class="content">If there is no discard work to do, a fstrim on btrfs will do nothing. Discard=async, the new default, should be enough to trim deleted data without using fstrim. But nothing bad happens by using both.</div><br/></div></div></div></div><div id="36220511" class="c"><input type="checkbox" id="c-36220511" checked=""/><div class="controls bullet"><span class="by">kevincox</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220497">parent</a><span>|</span><a href="#36220693">prev</a><span>|</span><a href="#36220505">next</a><span>|</span><label class="collapse" for="c-36220511">[-]</label><label class="expand" for="c-36220511">[5 more]</label></div><br/><div class="children"><div class="content">IIUC the filesystem should balance itself. So unless you have added a new device to a mostly full filesystem you should be fine.</div><br/><div id="36220566" class="c"><input type="checkbox" id="c-36220566" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220511">parent</a><span>|</span><a href="#36220505">next</a><span>|</span><label class="collapse" for="c-36220566">[-]</label><label class="expand" for="c-36220566">[4 more]</label></div><br/><div class="children"><div class="content">If you delete a bunch of files it might get unbalanced? That wouldn&#x27;t degrade performance though I think. Or it is one of those FS&#x27;s that makes sure everything is balanced on disk anyway?</div><br/><div id="36220602" class="c"><input type="checkbox" id="c-36220602" checked=""/><div class="controls bullet"><span class="by">kevincox</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220566">parent</a><span>|</span><a href="#36222089">next</a><span>|</span><label class="collapse" for="c-36220602">[-]</label><label class="expand" for="c-36220602">[1 more]</label></div><br/><div class="children"><div class="content">In theory yes. But it is unlikely. The files are very likely balanced before you delete them so it should be mostly balanced after. It will also self-correct the difference after a bit more use.<p>Unbalanced drives can affect performance as one drive will have to do more of the work often bottlenecking. But except for extreme cases it is probably a rounding error.</div><br/></div></div><div id="36222089" class="c"><input type="checkbox" id="c-36222089" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220566">parent</a><span>|</span><a href="#36220602">prev</a><span>|</span><a href="#36220505">next</a><span>|</span><label class="collapse" for="c-36222089">[-]</label><label class="expand" for="c-36222089">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand the details but I know one case it can fix problems is after deleting files to free up space on a nearly full disk.</div><br/><div id="36224392" class="c"><input type="checkbox" id="c-36224392" checked=""/><div class="controls bullet"><span class="by">tremon</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36222089">parent</a><span>|</span><a href="#36220505">next</a><span>|</span><label class="collapse" for="c-36224392">[-]</label><label class="expand" for="c-36224392">[1 more]</label></div><br/><div class="children"><div class="content">Btrfs has separate allocation pools for data and metadata. If you delete files, the freed-up space is returned to the data pool but that does not make it generally-available for use in the metadata pool. So in the case where the entire disk space is already allocated to one of the allocation pools, none of the pools can grow beyond their current size.<p>One of the effects of balancing the tree is to release unused space from the allocation pools to general availability, solving that problem.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36220505" class="c"><input type="checkbox" id="c-36220505" checked=""/><div class="controls bullet"><span class="by">kevincox</span><span>|</span><a href="#36220445">parent</a><span>|</span><a href="#36220497">prev</a><span>|</span><a href="#36221004">next</a><span>|</span><label class="collapse" for="c-36220505">[-]</label><label class="expand" for="c-36220505">[6 more]</label></div><br/><div class="children"><div class="content">Running an occasional scrub is probably a good idea. I don&#x27;t think any of the other commands mentioned here should be required. Naturally the disk should be balanced as it is written so you shouldn&#x27;t need explicit balances unless you added a new drive when they others were quite full.</div><br/><div id="36220946" class="c"><input type="checkbox" id="c-36220946" checked=""/><div class="controls bullet"><span class="by">Gigachad</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220505">parent</a><span>|</span><a href="#36221004">next</a><span>|</span><label class="collapse" for="c-36220946">[-]</label><label class="expand" for="c-36220946">[5 more]</label></div><br/><div class="children"><div class="content">If it was a good idea, I assume it should already be happening for me. What would be the point of mindlessly running some command periodically?</div><br/><div id="36221583" class="c"><input type="checkbox" id="c-36221583" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220946">parent</a><span>|</span><a href="#36221370">next</a><span>|</span><label class="collapse" for="c-36221583">[-]</label><label class="expand" for="c-36221583">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If it was a good idea, I assume it should already be happening for me.<p>It would, if you were running a well-designed&#x2F;maintained operating system. Unfortunately many Linux distributions make poor decisions.</div><br/></div></div><div id="36221370" class="c"><input type="checkbox" id="c-36221370" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220946">parent</a><span>|</span><a href="#36221583">prev</a><span>|</span><a href="#36221134">next</a><span>|</span><label class="collapse" for="c-36221370">[-]</label><label class="expand" for="c-36221370">[1 more]</label></div><br/><div class="children"><div class="content">Does your car wash itself? &#x2F;s<p>It very well may be! Depending on your distribution... they may already have similar services&#x2F;timers for periodic scrubbing<p>On Fedora they aren&#x27;t provided; but <i>are</i> with Arch<p>Unless you run an array it&#x27;s fairly meaningless, beyond being an administrative-reporting tool. It can&#x27;t fix failed checksums without mirrors or parity<p>I think the checksums are validated on read anyway, so it&#x27;s mostly to mitigate bitrot - very specific to &#x27;glacial&#x27; storage</div><br/></div></div><div id="36221134" class="c"><input type="checkbox" id="c-36221134" checked=""/><div class="controls bullet"><span class="by">kevincox</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36220946">parent</a><span>|</span><a href="#36221370">prev</a><span>|</span><a href="#36221004">next</a><span>|</span><label class="collapse" for="c-36221134">[-]</label><label class="expand" for="c-36221134">[2 more]</label></div><br/><div class="children"><div class="content">Because different use cases have different requirements. So having the maintenance command be separate from the filesystem gives ultimate flexibility.</div><br/><div id="36221833" class="c"><input type="checkbox" id="c-36221833" checked=""/><div class="controls bullet"><span class="by">Gigachad</span><span>|</span><a href="#36220445">root</a><span>|</span><a href="#36221134">parent</a><span>|</span><a href="#36221004">next</a><span>|</span><label class="collapse" for="c-36221833">[-]</label><label class="expand" for="c-36221833">[1 more]</label></div><br/><div class="children"><div class="content">The OP comment didn&#x27;t give any context for what you should be thinking to make a judgement call on. Just blindly &quot;run this command periodically&quot;. If I&#x27;m not making any actual decision, it should just be run for me by default.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36221004" class="c"><input type="checkbox" id="c-36221004" checked=""/><div class="controls bullet"><span class="by">chasil</span><span>|</span><a href="#36220445">prev</a><span>|</span><a href="#36221906">next</a><span>|</span><label class="collapse" for="c-36221004">[-]</label><label class="expand" for="c-36221004">[11 more]</label></div><br/><div class="children"><div class="content">Why is nobody mentioning?:<p><pre><code>  btrfs fi defrag -r &#x2F;
</code></pre>
This is a recovery that ZFS cannot make.<p><a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;login&#x2F;articles&#x2F;login_summer17_02_conway.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;login&#x2F;articles&#x2F;login_sum...</a></div><br/><div id="36221322" class="c"><input type="checkbox" id="c-36221322" checked=""/><div class="controls bullet"><span class="by">nubinetwork</span><span>|</span><a href="#36221004">parent</a><span>|</span><a href="#36221722">next</a><span>|</span><label class="collapse" for="c-36221322">[-]</label><label class="expand" for="c-36221322">[6 more]</label></div><br/><div class="children"><div class="content">ZFS doesn&#x27;t need it, if the file is accessed enough, it stays in cache.  If the file isn&#x27;t important enough, who cares that it takes a little bit longer to load into memory?</div><br/><div id="36221444" class="c"><input type="checkbox" id="c-36221444" checked=""/><div class="controls bullet"><span class="by">chasil</span><span>|</span><a href="#36221004">root</a><span>|</span><a href="#36221322">parent</a><span>|</span><a href="#36221722">next</a><span>|</span><label class="collapse" for="c-36221444">[-]</label><label class="expand" for="c-36221444">[5 more]</label></div><br/><div class="children"><div class="content">If a ZFS dataset goes over 80% utilization, I understand it will switch from &quot;first fit&quot; to &quot;best fit&quot; which can permanently and severely impact performance.<p><a href="https:&#x2F;&#x2F;serverfault.com&#x2F;questions&#x2F;733817&#x2F;does-the-max-80-use-target-suggested-for-zfs-for-performance-reasons-apply-to-s" rel="nofollow">https:&#x2F;&#x2F;serverfault.com&#x2F;questions&#x2F;733817&#x2F;does-the-max-80-use...</a></div><br/><div id="36221834" class="c"><input type="checkbox" id="c-36221834" checked=""/><div class="controls bullet"><span class="by">LeoPanthera</span><span>|</span><a href="#36221004">root</a><span>|</span><a href="#36221444">parent</a><span>|</span><a href="#36221858">next</a><span>|</span><label class="collapse" for="c-36221834">[-]</label><label class="expand" for="c-36221834">[2 more]</label></div><br/><div class="children"><div class="content">I actually tested this, and could not measure any performance hit at all until I hit 95% full.<p>It sped right back up again when I deleted some stuff.<p>I wouldn&#x27;t worry about it.</div><br/><div id="36222195" class="c"><input type="checkbox" id="c-36222195" checked=""/><div class="controls bullet"><span class="by">chasil</span><span>|</span><a href="#36221004">root</a><span>|</span><a href="#36221834">parent</a><span>|</span><a href="#36221858">next</a><span>|</span><label class="collapse" for="c-36222195">[-]</label><label class="expand" for="c-36222195">[1 more]</label></div><br/><div class="children"><div class="content">Interesting test. Thanks for finding this out.</div><br/></div></div></div></div><div id="36221858" class="c"><input type="checkbox" id="c-36221858" checked=""/><div class="controls bullet"><span class="by">seized</span><span>|</span><a href="#36221004">root</a><span>|</span><a href="#36221444">parent</a><span>|</span><a href="#36221834">prev</a><span>|</span><a href="#36221722">next</a><span>|</span><label class="collapse" for="c-36221858">[-]</label><label class="expand" for="c-36221858">[2 more]</label></div><br/><div class="children"><div class="content">You can ZFS send back and forth between pools or to a new dataset to defrag&#x2F;fix that, so it&#x27;s not &quot;permanent&quot; as in can&#x27;t ever be fixed. It also depends on workload, write once&#x2F;read many is different from VM disk files.</div><br/><div id="36222127" class="c"><input type="checkbox" id="c-36222127" checked=""/><div class="controls bullet"><span class="by">chasil</span><span>|</span><a href="#36221004">root</a><span>|</span><a href="#36221858">parent</a><span>|</span><a href="#36221722">next</a><span>|</span><label class="collapse" for="c-36222127">[-]</label><label class="expand" for="c-36222127">[1 more]</label></div><br/><div class="children"><div class="content">Btrfs can &quot;rebalance&quot; in place. This has had bugs in the past, but seems safe now.<p>On the side of ZFS, you can take one volume of a mirrorset, move it to another system and mount it in degraded mode, and copy a bunch of new files to it. When you return the updated drive to the full set and mount it, it will resilver the older drive with the newer content automatically.<p>That&#x27;s a killer feature. With btrfs, you have to rebalance to mirror the new files, which takes a long time as every block must be rewritten between the mirrors. ZFS knows how to move just the new files in a resilver.</div><br/></div></div></div></div></div></div></div></div><div id="36221386" class="c"><input type="checkbox" id="c-36221386" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#36221004">parent</a><span>|</span><a href="#36221722">prev</a><span>|</span><a href="#36221906">next</a><span>|</span><label class="collapse" for="c-36221386">[-]</label><label class="expand" for="c-36221386">[3 more]</label></div><br/><div class="children"><div class="content">Defrag breaks COW so if you use COW a lot (e.g. snapshots) you don&#x27;t want to do it.</div><br/><div id="36224285" class="c"><input type="checkbox" id="c-36224285" checked=""/><div class="controls bullet"><span class="by">fsckboy</span><span>|</span><a href="#36221004">root</a><span>|</span><a href="#36221386">parent</a><span>|</span><a href="#36221906">next</a><span>|</span><label class="collapse" for="c-36224285">[-]</label><label class="expand" for="c-36224285">[2 more]</label></div><br/><div class="children"><div class="content">what do you mean it breaks? does it purge* the older versions, or does something break?<p>* (yes, DEC reference)</div><br/><div id="36224516" class="c"><input type="checkbox" id="c-36224516" checked=""/><div class="controls bullet"><span class="by">jraph</span><span>|</span><a href="#36221004">root</a><span>|</span><a href="#36224285">parent</a><span>|</span><a href="#36221906">next</a><span>|</span><label class="collapse" for="c-36224516">[-]</label><label class="expand" for="c-36224516">[1 more]</label></div><br/><div class="children"><div class="content">It deduplicates everything. Don&#x27;t defrag Btrfs lightly, unless you ran out on other solutions and only do it if you know what you are doing.</div><br/></div></div></div></div></div></div></div></div><div id="36221906" class="c"><input type="checkbox" id="c-36221906" checked=""/><div class="controls bullet"><span class="by">seized</span><span>|</span><a href="#36221004">prev</a><span>|</span><label class="collapse" for="c-36221906">[-]</label><label class="expand" for="c-36221906">[1 more]</label></div><br/><div class="children"><div class="content">I just scrub my ZFS pools once in a while and that&#x27;s it... OpenIndiana autosnap is the rest of it which is built in. Then Monit to check pool health and alert me automatically.</div><br/></div></div></div></div></div></div></div></body></html>