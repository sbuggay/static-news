<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734166872139" as="style"/><link rel="stylesheet" href="styles.css?v=1734166872139"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/?_fb_noscript=1">Byte Latent Transformer: Patches Scale Better Than Tokens</a> <span class="domain">(<a href="https://ai.meta.com">ai.meta.com</a>)</span></div><div class="subtext"><span>zxexz</span> | <span>3 comments</span></div><br/><div><div id="42415641" class="c"><input type="checkbox" id="c-42415641" checked=""/><div class="controls bullet"><span class="by">qouteall</span><span>|</span><a href="#42415587">next</a><span>|</span><label class="collapse" for="c-42415641">[-]</label><label class="expand" for="c-42415641">[1 more]</label></div><br/><div class="children"><div class="content">Related quote from Karpathy:<p>Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.<p>• Why can&#x27;t LLM spell words? Tokenization.<p>• Why can&#x27;t LLM do super simple string processing tasks like reversing a string? Tokenization.<p>• Why is LLM worse at non-English languages (e.g. Japanese)? Tokenization.<p>• Why is LLM bad at simple arithmetic? Tokenization.<p>• Why did GPT-2 have more than necessary trouble coding in Python? Tokenization.<p>• Why did my LLM abruptly halt when it sees the string &quot;&lt;|endoftext|&gt;&quot;? Tokenization.<p>• What is this weird warning I get about a &quot;trailing whitespace&quot;? Tokenization.<p>• Why the LLM break if I ask it about &quot;SolidGoldMagikarp&quot;? Tokenization.<p>• Why should I prefer to use YAML over JSON with LLMs? Tokenization.<p>• Why is LLM not actually end-to-end language modeling? Tokenization.<p>• What is the real root of suffering? Tokenization.</div><br/></div></div><div id="42415587" class="c"><input type="checkbox" id="c-42415587" checked=""/><div class="controls bullet"><span class="by">bloomingkales</span><span>|</span><a href="#42415641">prev</a><span>|</span><label class="collapse" for="c-42415587">[-]</label><label class="expand" for="c-42415587">[1 more]</label></div><br/><div class="children"><div class="content">I thought we’re supposed to be plateauing!?</div><br/></div></div></div></div></div></div></div></body></html>