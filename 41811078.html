<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728723674136" as="style"/><link rel="stylesheet" href="styles.css?v=1728723674136"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/samuel-vitorino/lm.rs">Lm.rs: Minimal CPU LLM inference in Rust with no dependency</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>littlestymaar</span> | <span>60 comments</span></div><br/><div><div id="41812637" class="c"><input type="checkbox" id="c-41812637" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41813738">next</a><span>|</span><label class="collapse" for="c-41812637">[-]</label><label class="expand" for="c-41812637">[17 more]</label></div><br/><div class="children"><div class="content">This is impressive. I just ran the 1.2G llama3.2-1b-it-q80.lmrs on a M2 64GB MacBook and it felt speedy and used 1000% of CPU across 13 threads (according to Activity Monitor).<p><pre><code>    cd &#x2F;tmp
    git clone https:&#x2F;&#x2F;github.com&#x2F;samuel-vitorino&#x2F;lm.rs
    cd lm.rs
    RUSTFLAGS=&quot;-C target-cpu=native&quot; cargo build --release --bin chat
    curl -LO &#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;samuel-vitorino&#x2F;Llama-3.2-1B-Instruct-Q8_0-LMRS&#x2F;resolve&#x2F;main&#x2F;tokenizer.bin?download=true&#x27;
    curl -LO &#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;samuel-vitorino&#x2F;Llama-3.2-1B-Instruct-Q8_0-LMRS&#x2F;resolve&#x2F;main&#x2F;llama3.2-1b-it-q80.lmrs?download=true&#x27;
    .&#x2F;target&#x2F;release&#x2F;chat --model llama3.2-1b-it-q80.lmrs</code></pre></div><br/><div id="41812737" class="c"><input type="checkbox" id="c-41812737" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#41812637">parent</a><span>|</span><a href="#41812692">next</a><span>|</span><label class="collapse" for="c-41812737">[-]</label><label class="expand" for="c-41812737">[14 more]</label></div><br/><div class="children"><div class="content">Not sure how to formulate this, but what does this mean in the sense of how &quot;smart&quot; it is compared to the latest chatgpt version?</div><br/><div id="41812810" class="c"><input type="checkbox" id="c-41812810" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41812737">parent</a><span>|</span><a href="#41812836">next</a><span>|</span><label class="collapse" for="c-41812810">[-]</label><label class="expand" for="c-41812810">[5 more]</label></div><br/><div class="children"><div class="content">The model I&#x27;m running here is Llama 3.2 1B, the smallest on-device model I&#x27;ve tried that has given me good results.<p>The fact that a 1.2GB download can do as well as this is honestly astonishing to me - but it&#x27;s going to laughably poor in comparison to something like GPT-4o - which I&#x27;m guessing is measured in the 100s of GBs.<p>You can try out Llama 3.2 1B yourself directly in your browser (it will fetch about 1GB of data) at <a href="https:&#x2F;&#x2F;chat.webllm.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;chat.webllm.ai&#x2F;</a></div><br/><div id="41815271" class="c"><input type="checkbox" id="c-41815271" checked=""/><div class="controls bullet"><span class="by">iknowstuff</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41812810">parent</a><span>|</span><a href="#41812836">next</a><span>|</span><label class="collapse" for="c-41815271">[-]</label><label class="expand" for="c-41815271">[4 more]</label></div><br/><div class="children"><div class="content">anyone else think 4o is kinda garbage compared to the older gpt4? as well as o1-preview and probably o1-mini.<p>gpt4 tends to be more accurate than 4o for me.</div><br/><div id="41815707" class="c"><input type="checkbox" id="c-41815707" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41815271">parent</a><span>|</span><a href="#41816462">next</a><span>|</span><label class="collapse" for="c-41815707">[-]</label><label class="expand" for="c-41815707">[1 more]</label></div><br/><div class="children"><div class="content">I sort of do, especially against OG GPT-4 (before turbo)<p>4o is a bit too lobotomized for my taste. If you try to engage in conversation, nearly every answer after the first starts with &quot;You&#x27;re absolutely right&quot;. Bro, I don&#x27;t know if I&#x27;m right, that&#x27;s why I&#x27;m asking a question!<p>It&#x27;s somehow better in _some_ scenarios but I feel like it&#x27;s also objectively worse in others so it ends up being a wash. It paradoxically looks bad relative to GPT-4 but also makes GPT-4 feel worse when you go back to it...<p>o1-preview has been growing on me despite its answers also being very formulaic (relative to the OG GPT-3.5 and GPT-4 models which had more &quot;freedom&quot; in how they answered)</div><br/></div></div><div id="41816462" class="c"><input type="checkbox" id="c-41816462" checked=""/><div class="controls bullet"><span class="by">iammrpayments</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41815271">parent</a><span>|</span><a href="#41815707">prev</a><span>|</span><a href="#41816231">next</a><span>|</span><label class="collapse" for="c-41816462">[-]</label><label class="expand" for="c-41816462">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I use 4o for customer support in multiple languages and sometimes I have to tell it to reply using the customer language, while gpt4 could easily infer it.</div><br/></div></div><div id="41816231" class="c"><input type="checkbox" id="c-41816231" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41815271">parent</a><span>|</span><a href="#41816462">prev</a><span>|</span><a href="#41812836">next</a><span>|</span><label class="collapse" for="c-41816231">[-]</label><label class="expand" for="c-41816231">[1 more]</label></div><br/><div class="children"><div class="content">gpt-4o is a weak version of gpt-4 with &quot;steps-instructions&quot;. Gpt-4 is just too expensive which is why openAI is releasing all these mini versions.</div><br/></div></div></div></div></div></div><div id="41812836" class="c"><input type="checkbox" id="c-41812836" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41812737">parent</a><span>|</span><a href="#41812810">prev</a><span>|</span><a href="#41816507">next</a><span>|</span><label class="collapse" for="c-41812836">[-]</label><label class="expand" for="c-41812836">[7 more]</label></div><br/><div class="children"><div class="content">The implementation has no control on “how smart” the model is, and when it comes to llama 1B, it&#x27;s not very smart by current standard (but it would still have blown everyone&#x27;s mind just a few years back).</div><br/><div id="41813260" class="c"><input type="checkbox" id="c-41813260" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41812836">parent</a><span>|</span><a href="#41816507">next</a><span>|</span><label class="collapse" for="c-41813260">[-]</label><label class="expand" for="c-41813260">[6 more]</label></div><br/><div class="children"><div class="content">The implementation absolutely can influence the outputs.<p>If you have a sloppy implementations which somehow accumulates a lot of error in it&#x27;s floating point math, you will get worse results.<p>It&#x27;s rarely talked about, but it&#x27;s a real thing. Floating point addition and multiplication is non-associative and the order of operations affects the correctness and performance. Developers might (unknowningly) trade performance for correctness. And it matters a lot more in the low precision modes we operate today. Just try different methods of summing a vector containing 9,999 fp16 ones in fp16. Hint: it will never be 9,999.0 and you won&#x27;t get close to the best approximation if you do it in a naive loop.</div><br/><div id="41813548" class="c"><input type="checkbox" id="c-41813548" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41813260">parent</a><span>|</span><a href="#41814379">next</a><span>|</span><label class="collapse" for="c-41813548">[-]</label><label class="expand" for="c-41813548">[2 more]</label></div><br/><div class="children"><div class="content">How well does bf16 work in comparison?</div><br/><div id="41813659" class="c"><input type="checkbox" id="c-41813659" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41813548">parent</a><span>|</span><a href="#41814379">next</a><span>|</span><label class="collapse" for="c-41813659">[-]</label><label class="expand" for="c-41813659">[1 more]</label></div><br/><div class="children"><div class="content">Even worse, I&#x27;d say since it has fewer bits for the fraction. At least in the example i was mentioning, where you run into precision limits, not into range limits.<p>I believe bf16 was primarily designed as a storage format, since it just needs 16 zero bits added to be a valid fp32.</div><br/></div></div></div></div><div id="41814379" class="c"><input type="checkbox" id="c-41814379" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41813260">parent</a><span>|</span><a href="#41813548">prev</a><span>|</span><a href="#41813313">next</a><span>|</span><label class="collapse" for="c-41814379">[-]</label><label class="expand" for="c-41814379">[2 more]</label></div><br/><div class="children"><div class="content">I thought all current implementations accumulate into a fp32 instead of accumulating in fp16.</div><br/><div id="41814901" class="c"><input type="checkbox" id="c-41814901" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41814379">parent</a><span>|</span><a href="#41813313">next</a><span>|</span><label class="collapse" for="c-41814901">[-]</label><label class="expand" for="c-41814901">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t looked at all implementations, but the hardware (tensor cores as well as cuda cores) allows you to accumulate at fp16 precision.</div><br/></div></div></div></div><div id="41813313" class="c"><input type="checkbox" id="c-41813313" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41813260">parent</a><span>|</span><a href="#41814379">prev</a><span>|</span><a href="#41816507">next</a><span>|</span><label class="collapse" for="c-41813313">[-]</label><label class="expand" for="c-41813313">[1 more]</label></div><br/><div class="children"><div class="content">TIL, thanks.</div><br/></div></div></div></div></div></div></div></div><div id="41812692" class="c"><input type="checkbox" id="c-41812692" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812637">parent</a><span>|</span><a href="#41812737">prev</a><span>|</span><a href="#41813738">next</a><span>|</span><label class="collapse" for="c-41812692">[-]</label><label class="expand" for="c-41812692">[2 more]</label></div><br/><div class="children"><div class="content">Could you try with<p><pre><code>    .&#x2F;target&#x2F;release&#x2F;chat --model llama3.2-1b-it-q80.lmrs --show-metrics
</code></pre>
To know how many token&#x2F;s you get?</div><br/><div id="41812840" class="c"><input type="checkbox" id="c-41812840" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41812637">root</a><span>|</span><a href="#41812692">parent</a><span>|</span><a href="#41813738">next</a><span>|</span><label class="collapse" for="c-41812840">[-]</label><label class="expand" for="c-41812840">[1 more]</label></div><br/><div class="children"><div class="content">Nice, just tried that with &quot;tell me a long tall tale&quot; as the prompt and got:<p><pre><code>    Speed: 26.41 tok&#x2F;s
</code></pre>
Full output: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;6f25fca5c664b84fdd4b72b0918546e0" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;6f25fca5c664b84fdd4b72b091854...</a></div><br/></div></div></div></div></div></div><div id="41813738" class="c"><input type="checkbox" id="c-41813738" checked=""/><div class="controls bullet"><span class="by">jll29</span><span>|</span><a href="#41812637">prev</a><span>|</span><a href="#41812524">next</a><span>|</span><label class="collapse" for="c-41813738">[-]</label><label class="expand" for="c-41813738">[3 more]</label></div><br/><div class="children"><div class="content">This is beautifully written, thanks for sharing.<p>I could see myself using some of the source code in the classroom to explain
how transformers &quot;really&quot; work; code is more concrete&#x2F;detailed than all those
pictures of attention heads etc.<p>Two points of minor criticism&#x2F;suggestions for improvement:<p>- libraries should not print to stdout, as that output may detroy application output (imagine I want to use the library in a text editor to offer style checking). So best to write to a string buffer owned by a logging class instance associated with a lm.rs object.<p>- Is it possible to do all this without &quot;unsafe&quot; without twisting one&#x27;s arm? I see there are uses of &quot;unsafe&quot; e.g. to force data alignment in the model reader.<p>Again, thanks and very impressive!</div><br/><div id="41815249" class="c"><input type="checkbox" id="c-41815249" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#41813738">parent</a><span>|</span><a href="#41812524">next</a><span>|</span><label class="collapse" for="c-41815249">[-]</label><label class="expand" for="c-41815249">[2 more]</label></div><br/><div class="children"><div class="content">&gt; best to write to a string buffer<p>It&#x27;s best to call a user callback. That way logs can be, for example, displayed in a GUI.</div><br/><div id="41816987" class="c"><input type="checkbox" id="c-41816987" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#41813738">root</a><span>|</span><a href="#41815249">parent</a><span>|</span><a href="#41812524">next</a><span>|</span><label class="collapse" for="c-41816987">[-]</label><label class="expand" for="c-41816987">[1 more]</label></div><br/><div class="children"><div class="content">A good logging framework has all the hooks you need</div><br/></div></div></div></div></div></div><div id="41812524" class="c"><input type="checkbox" id="c-41812524" checked=""/><div class="controls bullet"><span class="by">J_Shelby_J</span><span>|</span><a href="#41813738">prev</a><span>|</span><a href="#41812438">next</a><span>|</span><label class="collapse" for="c-41812524">[-]</label><label class="expand" for="c-41812524">[1 more]</label></div><br/><div class="children"><div class="content">Neat.<p>FYI I have a whole bunch of rust tools[0] for loading models and other LLM tasks. For example auto selecting the largest quant based on memory available, extracting a tokenizer from a gguf, prompting, etc. You could use this to remove some of the python dependencies you have.<p>Currently to support llama.cpp, but this is pretty neat too. Any plans to support grammars?<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;ShelbyJenkins&#x2F;llm_client">https:&#x2F;&#x2F;github.com&#x2F;ShelbyJenkins&#x2F;llm_client</a></div><br/></div></div><div id="41812438" class="c"><input type="checkbox" id="c-41812438" checked=""/><div class="controls bullet"><span class="by">wyldfire</span><span>|</span><a href="#41812524">prev</a><span>|</span><a href="#41812615">next</a><span>|</span><label class="collapse" for="c-41812438">[-]</label><label class="expand" for="c-41812438">[15 more]</label></div><br/><div class="children"><div class="content">The title is less clear than it could be IMO.<p>When I saw &quot;no dependency&quot; I thought maybe it could be no_std (llama.c is relatively lightweight in this regard).  But it&#x27;s definitely not `no_std` and in fact seems like it has several dependencies.  Perhaps all of them are rust dependencies?</div><br/><div id="41812628" class="c"><input type="checkbox" id="c-41812628" checked=""/><div class="controls bullet"><span class="by">saghm</span><span>|</span><a href="#41812438">parent</a><span>|</span><a href="#41813416">next</a><span>|</span><label class="collapse" for="c-41812628">[-]</label><label class="expand" for="c-41812628">[3 more]</label></div><br/><div class="children"><div class="content">The readme seems to indicate that it expects pytorch alongside several other Python dependencies in a requirements.txt file (which is the only place I can find any form of the word &quot;dependency&quot; on the page). I&#x27;m very confused by the characterization in the title here given that it doesn&#x27;t seem to be claimed at all by the project itself (which simple has the subtitle &quot;Minimal LLM inference in Rust&quot;).<p>From the git history, it looks like the username of the person who posted this here is someone who has contributed to the project but isn&#x27;t the primary author. If they could elaborate on what exactly they mean by saying this has &quot;zero dependencies&quot;, that might be helpful.</div><br/><div id="41812763" class="c"><input type="checkbox" id="c-41812763" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41812628">parent</a><span>|</span><a href="#41813416">next</a><span>|</span><label class="collapse" for="c-41812763">[-]</label><label class="expand" for="c-41812763">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The readme seems to indicate that it expects pytorch alongside several other Python dependencies in a requirements.txt file<p>That&#x27;s only if you want to convert the model yourself, you don&#x27;t need that if you use the converted weights on the author&#x27;s huggingface page (in “prepared-models” table of the README).<p>&gt; From the git history, it looks like the username of the person who posted this here is someone who has contributed to the project but isn&#x27;t the primary author.<p>Yup that&#x27;s correct, so far I&#x27;ve only authored the dioxus GUI app.<p>&gt; If they could elaborate on what exactly they mean by saying this has &quot;zero dependencies&quot;, that might be helpful.<p>See my other response: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41812665">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41812665</a></div><br/><div id="41813346" class="c"><input type="checkbox" id="c-41813346" checked=""/><div class="controls bullet"><span class="by">J_Shelby_J</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41812763">parent</a><span>|</span><a href="#41813416">next</a><span>|</span><label class="collapse" for="c-41813346">[-]</label><label class="expand" for="c-41813346">[1 more]</label></div><br/><div class="children"><div class="content">What do you think about implementing your gui for other rust LLM projects? I’m looking for a front end for my project: <a href="https:&#x2F;&#x2F;github.com&#x2F;ShelbyJenkins&#x2F;llm_client">https:&#x2F;&#x2F;github.com&#x2F;ShelbyJenkins&#x2F;llm_client</a></div><br/></div></div></div></div></div></div><div id="41813416" class="c"><input type="checkbox" id="c-41813416" checked=""/><div class="controls bullet"><span class="by">ctz</span><span>|</span><a href="#41812438">parent</a><span>|</span><a href="#41812628">prev</a><span>|</span><a href="#41812665">next</a><span>|</span><label class="collapse" for="c-41813416">[-]</label><label class="expand" for="c-41813416">[1 more]</label></div><br/><div class="children"><div class="content">The original may have made sense, eg &quot;no hardware dependency&quot;, or &quot;no GPU dependency&quot;.  Unfortunately HN deletes words from titles with no rhyme or reason, and no transparency.</div><br/></div></div><div id="41812665" class="c"><input type="checkbox" id="c-41812665" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812438">parent</a><span>|</span><a href="#41813416">prev</a><span>|</span><a href="#41812629">next</a><span>|</span><label class="collapse" for="c-41812665">[-]</label><label class="expand" for="c-41812665">[4 more]</label></div><br/><div class="children"><div class="content">Titles are hard.<p>What I wanted to express is that it doesn&#x27;t have any pytorch or Cuda or onnx or whatever deep learning dependency and that all the logic is self contained.<p>To be totally transparent it has 5 Rust dependencies by default, two of them should be feature gated for the <i>chat</i> (chrono and clap), and then there are 3 utility crates that are used to get a little bit more performance out of the hardware (`rayon` for easier parallelization, `wide` for helping with SIMD, and `memmap2` for memory mapping of the model file).</div><br/><div id="41813366" class="c"><input type="checkbox" id="c-41813366" checked=""/><div class="controls bullet"><span class="by">J_Shelby_J</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41812665">parent</a><span>|</span><a href="#41812629">next</a><span>|</span><label class="collapse" for="c-41813366">[-]</label><label class="expand" for="c-41813366">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, hard to not be overly verbose. “No massive dependencies with long build times and deep abstractions!” Is not as catchy.</div><br/><div id="41813626" class="c"><input type="checkbox" id="c-41813626" checked=""/><div class="controls bullet"><span class="by">0x457</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41813366">parent</a><span>|</span><a href="#41812629">next</a><span>|</span><label class="collapse" for="c-41813626">[-]</label><label class="expand" for="c-41813626">[2 more]</label></div><br/><div class="children"><div class="content">No dependencies in this case (and pretty much any rust project) means: to build you need rustc+cargo and to use you just need resulting binary.<p>As in you don&#x27;t need to have C compiler, python, dynamic libraries. &quot;pure rust&quot; would be a better way to describe it.</div><br/><div id="41813984" class="c"><input type="checkbox" id="c-41813984" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41813626">parent</a><span>|</span><a href="#41812629">next</a><span>|</span><label class="collapse" for="c-41813984">[-]</label><label class="expand" for="c-41813984">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a little bit more than pure Rust: to build the library there&#x27;s basically only two dependencies (rayon and wide) which bring only 14 transitive dependencies (anyone who&#x27;s built even simple Rust program knows that this is a very small number).<p>And there&#x27;s more, Rayon and wide are only needed for performance and we could trivially put them behind a feature flag and have zero dependency and have the library work in a no-std context actually, but it would be so slow it would have no use at all so I don&#x27;t really think that makes sense to do except in order to win an argument…</div><br/></div></div></div></div></div></div></div></div><div id="41812629" class="c"><input type="checkbox" id="c-41812629" checked=""/><div class="controls bullet"><span class="by">vitaminka</span><span>|</span><a href="#41812438">parent</a><span>|</span><a href="#41812665">prev</a><span>|</span><a href="#41812615">next</a><span>|</span><label class="collapse" for="c-41812629">[-]</label><label class="expand" for="c-41812629">[6 more]</label></div><br/><div class="children"><div class="content">is rust cargo basically like npm at this point? like how on earth is sixteen dependencies means no dependencies lol</div><br/><div id="41813811" class="c"><input type="checkbox" id="c-41813811" checked=""/><div class="controls bullet"><span class="by">tormeh</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41812629">parent</a><span>|</span><a href="#41812802">next</a><span>|</span><label class="collapse" for="c-41813811">[-]</label><label class="expand" for="c-41813811">[4 more]</label></div><br/><div class="children"><div class="content">Yes, basically. Someone who is a dependency maximalist (never write any code that can be replaced by a dependency) then you can easily end up with a thousand dependencies. I don&#x27;t like things being that way, but others do.<p>It&#x27;s worth noting that Rust&#x27;s std library is really small, and you therefore need more dependencies in Rust than in some other languages like Python. There are some &quot;blessed&quot; crates though, like the ones maintained by the rust-lang team themselves (<a href="https:&#x2F;&#x2F;crates.io&#x2F;teams&#x2F;github:rust-lang:libs" rel="nofollow">https:&#x2F;&#x2F;crates.io&#x2F;teams&#x2F;github:rust-lang:libs</a> and <a href="https:&#x2F;&#x2F;crates.io&#x2F;teams&#x2F;github:rust-lang-nursery:libs" rel="nofollow">https:&#x2F;&#x2F;crates.io&#x2F;teams&#x2F;github:rust-lang-nursery:libs</a>). Also, when you add a dependency like Tokio, Axum, or Polars, these are often ecosystems of crates rather than singular crates.<p>Tl;dr: Good package managers end up encouraging micro-dependencies and dependency bloat because these things are now painless. Cargo is one of these good package managers.</div><br/><div id="41814460" class="c"><input type="checkbox" id="c-41814460" checked=""/><div class="controls bullet"><span class="by">jll29</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41813811">parent</a><span>|</span><a href="#41812802">next</a><span>|</span><label class="collapse" for="c-41814460">[-]</label><label class="expand" for="c-41814460">[3 more]</label></div><br/><div class="children"><div class="content">How about designing a &quot;proper&quot; standard library for Rust (comparable to Java&#x27;s or CommonLISP&#x27;s), to ensure a richer experience, avoiding dependency explosions, and also to ensure things are written in a uniform interface style? Is that something the Rust folks are considering or actively working on?<p>EDIT: nobody is helped by 46 regex libraries, none of which implements Unicode fully, for example (not an example taken from the Rust community).</div><br/><div id="41815306" class="c"><input type="checkbox" id="c-41815306" checked=""/><div class="controls bullet"><span class="by">pornel</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41814460">parent</a><span>|</span><a href="#41814654">next</a><span>|</span><label class="collapse" for="c-41815306">[-]</label><label class="expand" for="c-41815306">[1 more]</label></div><br/><div class="children"><div class="content">The particular mode of distribution of code as a traditional standard library has downsides:<p>- it&#x27;s inevitably going to accumulate mistakes&#x2F;obsolete&#x2F;deprecated stuff over time, because there can be only one version of it, and it needs to be backwards compatible.<p>- it makes porting the language to new platforms harder, since there&#x27;s more stuff promised to work as standard.<p>- to reduce risk of having the above problems, stdlib usually sticks to basic lowest-common-denominator APIs, lagging behind the state of the art, creating a dilemma between using standard impl vs better but 3rd party impls (and large programs end up with both)<p>- with a one-size-fits-all it&#x27;s easy to add bloat from unnecessary features. Not all programs want to embed megabytes of Unicode metadata for a regex.<p>The goal of having common trustworthy code can be achieved in many other ways, such as having (de-facto) standard individual dependencies to choose from. Packages that aren&#x27;t built-in can be versioned independently, and included only when necessary.</div><br/></div></div><div id="41814654" class="c"><input type="checkbox" id="c-41814654" checked=""/><div class="controls bullet"><span class="by">tormeh</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41814460">parent</a><span>|</span><a href="#41815306">prev</a><span>|</span><a href="#41812802">next</a><span>|</span><label class="collapse" for="c-41814654">[-]</label><label class="expand" for="c-41814654">[1 more]</label></div><br/><div class="children"><div class="content">Just use the rust-lang org&#x27;s regex crate. It&#x27;s fascinating that you managed to pick one of like 3 high-level use-cases that are covered by official rust-lang crates.</div><br/></div></div></div></div></div></div><div id="41812802" class="c"><input type="checkbox" id="c-41812802" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812438">root</a><span>|</span><a href="#41812629">parent</a><span>|</span><a href="#41813811">prev</a><span>|</span><a href="#41812615">next</a><span>|</span><label class="collapse" for="c-41812802">[-]</label><label class="expand" for="c-41812802">[1 more]</label></div><br/><div class="children"><div class="content">&gt; like how on earth is sixteen dependencies means no dependencies lol<p>You&#x27;re counting optional dependencies used in the binaries which isn&#x27;t fair (obviously the GUI app or the backend of the webui are going to have dependencies!). But yes 3 dependencies isn&#x27;t literally no dependency.</div><br/></div></div></div></div></div></div><div id="41812615" class="c"><input type="checkbox" id="c-41812615" checked=""/><div class="controls bullet"><span class="by">gip</span><span>|</span><a href="#41812438">prev</a><span>|</span><a href="#41817001">next</a><span>|</span><label class="collapse" for="c-41812615">[-]</label><label class="expand" for="c-41812615">[2 more]</label></div><br/><div class="children"><div class="content">Great! Did something similar some time ago [0] but the performance was underwhelming compared to C&#x2F;C++ code running on CPU (which points to my lack of understanding of how to make Rust fast). Would be nice to have some benchmarks of the different Rust implementations.<p>Implementing LLM inference should&#x2F;could really become the new &quot;hello world!&quot; for serious programmers out there :)<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;gip&#x2F;yllama.rs">https:&#x2F;&#x2F;github.com&#x2F;gip&#x2F;yllama.rs</a></div><br/><div id="41816977" class="c"><input type="checkbox" id="c-41816977" checked=""/><div class="controls bullet"><span class="by">flaneur2020</span><span>|</span><a href="#41812615">parent</a><span>|</span><a href="#41817001">next</a><span>|</span><label class="collapse" for="c-41816977">[-]</label><label class="expand" for="c-41816977">[1 more]</label></div><br/><div class="children"><div class="content">i also had a similar &#x27;hello world&#x27; experience some time ago with [0] :). i manually used some SIMD instructions, and it seems the performance could align with llama.cpp. it appears that the key to performance is:<p>1. using SIMD on quantized matrix multiplication
2. using a busy loop instead of condition variables when splitting work among threads.<p>(however, i haven&#x27;t had more free time to continue working on inferencing quantized models on GPU (with Vulkan), and it hasn&#x27;t been updated for a long time since then.)<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;crabml&#x2F;crabml">https:&#x2F;&#x2F;github.com&#x2F;crabml&#x2F;crabml</a></div><br/></div></div></div></div><div id="41817001" class="c"><input type="checkbox" id="c-41817001" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#41812615">prev</a><span>|</span><a href="#41817037">next</a><span>|</span><label class="collapse" for="c-41817001">[-]</label><label class="expand" for="c-41817001">[2 more]</label></div><br/><div class="children"><div class="content">This is cool (and congrats on writing your first Rust lib!), but Metal&#x2F;Cuda support is a must for serious local usage.</div><br/><div id="41817166" class="c"><input type="checkbox" id="c-41817166" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41817001">parent</a><span>|</span><a href="#41817037">next</a><span>|</span><label class="collapse" for="c-41817166">[-]</label><label class="expand" for="c-41817166">[1 more]</label></div><br/><div class="children"><div class="content">Using Cuda is a non starter because it would go against the purpose of this project, but I (not the main author but contributor) am experimenting with wgpu to get some kind of GPU acceleration.<p>I&#x27;m not sure it goes anywhere though, because the main author want to keep the complexity under control.</div><br/></div></div></div></div><div id="41817037" class="c"><input type="checkbox" id="c-41817037" checked=""/><div class="controls bullet"><span class="by">aravindputrevu</span><span>|</span><a href="#41817001">prev</a><span>|</span><a href="#41812454">next</a><span>|</span><label class="collapse" for="c-41817037">[-]</label><label class="expand" for="c-41817037">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, I appreciate the rust community‘s enthu to rewrite most the stuff.</div><br/></div></div><div id="41812454" class="c"><input type="checkbox" id="c-41812454" checked=""/><div class="controls bullet"><span class="by">lucgagan</span><span>|</span><a href="#41817037">prev</a><span>|</span><a href="#41816245">next</a><span>|</span><label class="collapse" for="c-41812454">[-]</label><label class="expand" for="c-41812454">[6 more]</label></div><br/><div class="children"><div class="content">Correct me if I am wrong, but these implementations are all CPU bound?, i.e. if I have a good GPU, I should look for alternatives.</div><br/><div id="41812550" class="c"><input type="checkbox" id="c-41812550" checked=""/><div class="controls bullet"><span class="by">bt1a</span><span>|</span><a href="#41812454">parent</a><span>|</span><a href="#41816210">next</a><span>|</span><label class="collapse" for="c-41812550">[-]</label><label class="expand" for="c-41812550">[1 more]</label></div><br/><div class="children"><div class="content">You are correct. This project is &quot;on the CPU&quot;, so it will not utilize your GPU for computation. If you would like to try out a Rust framework that does support GPUs, Candle <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;candle&#x2F;tree&#x2F;main">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;candle&#x2F;tree&#x2F;main</a> may be worth exploring</div><br/></div></div><div id="41816210" class="c"><input type="checkbox" id="c-41816210" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#41812454">parent</a><span>|</span><a href="#41812550">prev</a><span>|</span><a href="#41812589">next</a><span>|</span><label class="collapse" for="c-41816210">[-]</label><label class="expand" for="c-41816210">[1 more]</label></div><br/><div class="children"><div class="content">CPU, yes, but more importantly memory bandwidth.<p>An RTX 3090 (as one example) has nearly 1TB&#x2F;s of memory bandwidth. You&#x27;d need at least 12 channels of the fastest proof-of-concept DDR5 on the planet to equal that.<p>If you have a discrete GPU, use an implementation that utilizes it because it&#x27;s a completely different story.<p>Apple Silicon boasts impressive numbers on LLM inference because it has a unified CPU-GPU high-bandwidth (400GB&#x2F;s IIRC) memory architecture.</div><br/></div></div><div id="41812589" class="c"><input type="checkbox" id="c-41812589" checked=""/><div class="controls bullet"><span class="by">J_Shelby_J</span><span>|</span><a href="#41812454">parent</a><span>|</span><a href="#41816210">prev</a><span>|</span><a href="#41812495">next</a><span>|</span><label class="collapse" for="c-41812589">[-]</label><label class="expand" for="c-41812589">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Depending on gpu 10-20x difference.<p>For rust you have the llama.cpp wrappers like llm_client (mine), and the candle based projects mistral.rs, and Kalosm.<p>Although, my project does try and provide an implementation of mistral.rs, I haven’t fully migrated from llama.cpp. A full rust implementation would be nice for quick install times (among other reasons). Right now my crate has to clone and build. It’s automated for mac, pc, and Linux but it adds about a minute of build time.</div><br/></div></div><div id="41812535" class="c"><input type="checkbox" id="c-41812535" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812454">parent</a><span>|</span><a href="#41812495">prev</a><span>|</span><a href="#41816245">next</a><span>|</span><label class="collapse" for="c-41812535">[-]</label><label class="expand" for="c-41812535">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s all implemented on the CPU, yes, there&#x27;s no GPU acceleration whatsoever (at the moment at least).<p>&gt; if I have a good GPU, I should look for alternatives.<p>If you actually want to run it, even just on the CPU, you should look for an alternative (and the alternative is called llama.cpp) this is more of an educational resource about how things work when you remove all the layers of complexity in the ecosystem.<p>LLM are somewhat magic in how effective they can be, but in terms of code it&#x27;s really simple.</div><br/></div></div></div></div><div id="41816245" class="c"><input type="checkbox" id="c-41816245" checked=""/><div class="controls bullet"><span class="by">nikolayasdf123</span><span>|</span><a href="#41812454">prev</a><span>|</span><a href="#41815636">next</a><span>|</span><label class="collapse" for="c-41816245">[-]</label><label class="expand" for="c-41816245">[2 more]</label></div><br/><div class="children"><div class="content">how does this compare to <a href="https:&#x2F;&#x2F;github.com&#x2F;EricLBuehler&#x2F;mistral.rs">https:&#x2F;&#x2F;github.com&#x2F;EricLBuehler&#x2F;mistral.rs</a> ?</div><br/><div id="41817197" class="c"><input type="checkbox" id="c-41817197" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41816245">parent</a><span>|</span><a href="#41815636">next</a><span>|</span><label class="collapse" for="c-41817197">[-]</label><label class="expand" for="c-41817197">[1 more]</label></div><br/><div class="children"><div class="content">Much simpler codebase because it has much less features. It doesn&#x27;t aim to be a llama.cpp competitor AFAIK.</div><br/></div></div></div></div><div id="41815636" class="c"><input type="checkbox" id="c-41815636" checked=""/><div class="controls bullet"><span class="by">dcreater</span><span>|</span><a href="#41816245">prev</a><span>|</span><a href="#41812151">next</a><span>|</span><label class="collapse" for="c-41815636">[-]</label><label class="expand" for="c-41815636">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the value of this compared to llama.cpp?</div><br/><div id="41816915" class="c"><input type="checkbox" id="c-41816915" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#41815636">parent</a><span>|</span><a href="#41815648">next</a><span>|</span><label class="collapse" for="c-41816915">[-]</label><label class="expand" for="c-41816915">[1 more]</label></div><br/><div class="children"><div class="content">Easier to integrate with other Rust projects maybe?</div><br/></div></div><div id="41815648" class="c"><input type="checkbox" id="c-41815648" checked=""/><div class="controls bullet"><span class="by">kvakkefly</span><span>|</span><a href="#41815636">parent</a><span>|</span><a href="#41816915">prev</a><span>|</span><a href="#41812151">next</a><span>|</span><label class="collapse" for="c-41815648">[-]</label><label class="expand" for="c-41815648">[1 more]</label></div><br/><div class="children"><div class="content">Cleaner codebase because of fewer features!</div><br/></div></div></div></div><div id="41812151" class="c"><input type="checkbox" id="c-41812151" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#41815636">prev</a><span>|</span><a href="#41815645">next</a><span>|</span><label class="collapse" for="c-41812151">[-]</label><label class="expand" for="c-41812151">[3 more]</label></div><br/><div class="children"><div class="content">This is really cool.<p>It&#x27;s already using Dioxus (neat). I wonder if WASM could be put on the roadmap.<p>If this could run a lightweight LLM like RWKV in the browser, then the browser unlocks a whole class of new capabilities without calling any SaaS APIs.</div><br/><div id="41812525" class="c"><input type="checkbox" id="c-41812525" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#41812151">parent</a><span>|</span><a href="#41813804">next</a><span>|</span><label class="collapse" for="c-41812525">[-]</label><label class="expand" for="c-41812525">[1 more]</label></div><br/><div class="children"><div class="content">I was poking at this a bit here<p><a href="https:&#x2F;&#x2F;github.com&#x2F;maedoc&#x2F;rwkv.js">https:&#x2F;&#x2F;github.com&#x2F;maedoc&#x2F;rwkv.js</a><p>using the Rwkv.cpp compiled with emscripten, but I didn’t quite figure out the tokenizers part (yet, only spent about an hour on it)<p>Nevertheless I am pretty sure the 1.6b rwkv6 would be totally usable offline browser only.  It’s not capable enough for general chat but for rag etc it could be quite enough</div><br/></div></div><div id="41813804" class="c"><input type="checkbox" id="c-41813804" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812151">parent</a><span>|</span><a href="#41812525">prev</a><span>|</span><a href="#41815645">next</a><span>|</span><label class="collapse" for="c-41813804">[-]</label><label class="expand" for="c-41813804">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I wonder if WASM could be put on the roadmap.<p>The library itself should be able to compile to WASM with very little change: <i>rayon</i> and <i>wide</i> the only mandatory dependencies support wasm out of the box, and to get rid of <i>memmap2</i> by replacing the `Mmap` type in <i>transformer.rs</i> with `&amp;[u8]`.<p>That being said, RWKV is a completely different architecture so it should be reimplemented entierly and is not likely to be part of the roadmap ever (not the main author so I can&#x27;t say for sure, but I really doubt it).</div><br/></div></div></div></div><div id="41815645" class="c"><input type="checkbox" id="c-41815645" checked=""/><div class="controls bullet"><span class="by">kvakkefly</span><span>|</span><a href="#41812151">prev</a><span>|</span><a href="#41811889">next</a><span>|</span><label class="collapse" for="c-41815645">[-]</label><label class="expand" for="c-41815645">[1 more]</label></div><br/><div class="children"><div class="content">Would love to see a wasm version of this!</div><br/></div></div><div id="41811889" class="c"><input type="checkbox" id="c-41811889" checked=""/><div class="controls bullet"><span class="by">fuddle</span><span>|</span><a href="#41815645">prev</a><span>|</span><a href="#41813928">next</a><span>|</span><label class="collapse" for="c-41811889">[-]</label><label class="expand" for="c-41811889">[2 more]</label></div><br/><div class="children"><div class="content">Nice work, it would be great to see some benchmarks comparing it to llm.c.</div><br/><div id="41814170" class="c"><input type="checkbox" id="c-41814170" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41811889">parent</a><span>|</span><a href="#41813928">next</a><span>|</span><label class="collapse" for="c-41814170">[-]</label><label class="expand" for="c-41814170">[1 more]</label></div><br/><div class="children"><div class="content">I doubt it would compare favorably at the moment, I don&#x27;t think it&#x27;s particularly well optimized besides using rayon to get CPU parallelism and wide for a bit of SIMD.<p>It&#x27;s good enough to get pretty good performance for little effort, but I don&#x27;t think it would win a benchmark race either.</div><br/></div></div></div></div><div id="41813928" class="c"><input type="checkbox" id="c-41813928" checked=""/><div class="controls bullet"><span class="by">marques576</span><span>|</span><a href="#41811889">prev</a><span>|</span><label class="collapse" for="c-41813928">[-]</label><label class="expand" for="c-41813928">[1 more]</label></div><br/><div class="children"><div class="content">Such a talented guy!</div><br/></div></div></div></div></div></div></div></body></html>