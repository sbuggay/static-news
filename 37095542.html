<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691830860179" as="style"/><link rel="stylesheet" href="styles.css?v=1691830860179"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/BerriAI/litellm/blob/main/cookbook/proxy-server/readme.md">Show HN: Open-source proxy server for Llama2, GPT-4, Claude2 with Logging,Cache</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>ij23</span> | <span>20 comments</span></div><br/><div><div id="37098103" class="c"><input type="checkbox" id="c-37098103" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#37097106">next</a><span>|</span><label class="collapse" for="c-37098103">[-]</label><label class="expand" for="c-37098103">[1 more]</label></div><br/><div class="children"><div class="content">Can you say more about the semantic caching, I can&#x27;t seem to find much in the docs.<p>edit: Found the cache notebook[1] and the calls to the cache and distance eval in the code[2]<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;main&#x2F;cookbook&#x2F;liteLLM_ChromaDB_Cache.ipynb">https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;main&#x2F;cookbook&#x2F;liteLL...</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;80d77fed7123af22201158528feadb83ab9a95bc&#x2F;cookbook&#x2F;proxy-server&#x2F;main.py#L124">https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;80d77fed7123af222011...</a></div><br/></div></div><div id="37097106" class="c"><input type="checkbox" id="c-37097106" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#37098103">prev</a><span>|</span><a href="#37096625">next</a><span>|</span><label class="collapse" for="c-37097106">[-]</label><label class="expand" for="c-37097106">[3 more]</label></div><br/><div class="children"><div class="content">This would be super useful if it supported local&#x2F;in-K8-cluster models.<p>Most production use cases probably need some sort of fall back of small -&gt; medium -&gt; large -&gt; GPT4.<p>Given the costs + low quota limits, I would be surprised if any significant portion of the market is falling back from one expensive proprietary* API to another.<p>With the rapid improvements in model servers like llama.cpp and vllm.ai, providing an abstraction layer for “fastest model server of the month” would be useful.</div><br/><div id="37097261" class="c"><input type="checkbox" id="c-37097261" checked=""/><div class="controls bullet"><span class="by">ij23</span><span>|</span><a href="#37097106">parent</a><span>|</span><a href="#37096625">next</a><span>|</span><label class="collapse" for="c-37097261">[-]</label><label class="expand" for="c-37097261">[2 more]</label></div><br/><div class="children"><div class="content">What local&#x2F;in-K8-cluster models servers would you recommend adding ?<p>Should we add support for llama.cpp and vllm.ai in the proxy server ? Or should we assume you can host them on your own infra and the proxy server requests your hosted model ?</div><br/><div id="37097385" class="c"><input type="checkbox" id="c-37097385" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#37097106">root</a><span>|</span><a href="#37097261">parent</a><span>|</span><a href="#37096625">next</a><span>|</span><label class="collapse" for="c-37097385">[-]</label><label class="expand" for="c-37097385">[1 more]</label></div><br/><div class="children"><div class="content">IMO don’t try to be the one stop shop to host models. There are too many players with all sorts of advancements (eg: stopping grammar, continuous batching, novel quantization etc.) and you won’t be able to keep up.<p>There is a ton of boilerplate around the actual model server that’s just busy work , but if done wrong can be a huge performance suck. Solve that.<p>Build the proxy that works with the most model servers out there. Do it in a way that once you have mindshare, the model server makers will be find it easy to put up a PR so that they can claim your proxy supports their server.<p>Don’t take a hard dependency on non-OSS stuff - being able to build an “on-prem” solution (read “deployed into customer’s VPC”) is table stakes for anyone to use your offering for a lot of enterprise use cases.<p>Edit: another unsolved problem - different models need slightly different prompts to solve the same problem well…</div><br/></div></div></div></div></div></div><div id="37096625" class="c"><input type="checkbox" id="c-37096625" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37097106">prev</a><span>|</span><a href="#37096039">next</a><span>|</span><label class="collapse" for="c-37096625">[-]</label><label class="expand" for="c-37096625">[3 more]</label></div><br/><div class="children"><div class="content">The idea of an LLM proxy is super compelling. There&#x27;s a lot of powerful ideas baked into the proxy form factor – I think you&#x27;ve listed out quite a few of them. It reminds me a bit of what Cloudflare did for the web: both making it faster and safer&#x2F;easier. Have you considered local LLMs at all for Llama 2? A few people and I have been working on <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama&#x2F;</a> and was thinking it would be helpful to be able to augment it with a proxy layer like this. Not only that, but it might help folks dynamically choose to run locally (vs against a cloud LLM) for certain prompts.</div><br/><div id="37096748" class="c"><input type="checkbox" id="c-37096748" checked=""/><div class="controls bullet"><span class="by">detente18</span><span>|</span><a href="#37096625">parent</a><span>|</span><a href="#37096039">next</a><span>|</span><label class="collapse" for="c-37096748">[-]</label><label class="expand" for="c-37096748">[2 more]</label></div><br/><div class="children"><div class="content">Hey @jmorgan - we love ollama!<p>Re: LocalLLM&#x27;s like Llama2 - yes we support self-deployed models, through Huggingface, Replicate, TogetherAI integrations.<p>We&#x27;re missing support for locally deployed models - and would love the help!<p>Re: ollama - we spent a couple hours trying to integrate ollama. We had a couple issues though, would love to try and support it. Got time to chat sometime this&#x2F;next week? I think this would be an awesome addition.</div><br/><div id="37096979" class="c"><input type="checkbox" id="c-37096979" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37096625">root</a><span>|</span><a href="#37096748">parent</a><span>|</span><a href="#37096039">next</a><span>|</span><label class="collapse" for="c-37096979">[-]</label><label class="expand" for="c-37096979">[1 more]</label></div><br/><div class="children"><div class="content">Great. Let&#x27;s chat!</div><br/></div></div></div></div></div></div><div id="37096039" class="c"><input type="checkbox" id="c-37096039" checked=""/><div class="controls bullet"><span class="by">breckenedge</span><span>|</span><a href="#37096625">prev</a><span>|</span><a href="#37096881">next</a><span>|</span><label class="collapse" for="c-37096039">[-]</label><label class="expand" for="c-37096039">[2 more]</label></div><br/><div class="children"><div class="content">Does it support the function-calling API? <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;function-calling-and-other-api-updates" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;function-calling-and-other-api-updat...</a></div><br/><div id="37096722" class="c"><input type="checkbox" id="c-37096722" checked=""/><div class="controls bullet"><span class="by">detente18</span><span>|</span><a href="#37096039">parent</a><span>|</span><a href="#37096881">next</a><span>|</span><label class="collapse" for="c-37096722">[-]</label><label class="expand" for="c-37096722">[1 more]</label></div><br/><div class="children"><div class="content">Hey @breckenedge yes it does! Exactly the same way as the openai-python sdk. Here&#x27;s our code tutorial for it  <a href="https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;main&#x2F;cookbook&#x2F;liteLLM_function_calling.ipynb">https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;main&#x2F;cookbook&#x2F;liteLL...</a></div><br/></div></div></div></div><div id="37096881" class="c"><input type="checkbox" id="c-37096881" checked=""/><div class="controls bullet"><span class="by">deet</span><span>|</span><a href="#37096039">prev</a><span>|</span><a href="#37096662">next</a><span>|</span><label class="collapse" for="c-37096881">[-]</label><label class="expand" for="c-37096881">[2 more]</label></div><br/><div class="children"><div class="content">Is user auth (and tracking token spend) within scope of this or is that better handled at a layer in front of this?</div><br/><div id="37096925" class="c"><input type="checkbox" id="c-37096925" checked=""/><div class="controls bullet"><span class="by">detente18</span><span>|</span><a href="#37096881">parent</a><span>|</span><a href="#37096662">next</a><span>|</span><label class="collapse" for="c-37096925">[-]</label><label class="expand" for="c-37096925">[1 more]</label></div><br/><div class="children"><div class="content">Hi @deet - Yes it is! This automatically stores the cost per query to the Supabase table - here&#x27;s how: <a href="https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;80d77fed7123af22201158528feadb83ab9a95bc&#x2F;litellm&#x2F;integrations&#x2F;supabase.py#L74">https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;litellm&#x2F;blob&#x2F;80d77fed7123af222011...</a><p>If you have ideas for improvement - we&#x27;d love a ticket&#x2F;PR!</div><br/></div></div></div></div><div id="37096662" class="c"><input type="checkbox" id="c-37096662" checked=""/><div class="controls bullet"><span class="by">romanzubenko</span><span>|</span><a href="#37096881">prev</a><span>|</span><a href="#37097078">next</a><span>|</span><label class="collapse" for="c-37096662">[-]</label><label class="expand" for="c-37096662">[3 more]</label></div><br/><div class="children"><div class="content">Reminds me of analytics.js which later turned into Segment and 3b acquisition</div><br/><div id="37096998" class="c"><input type="checkbox" id="c-37096998" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37096662">parent</a><span>|</span><a href="#37096725">next</a><span>|</span><label class="collapse" for="c-37096998">[-]</label><label class="expand" for="c-37096998">[1 more]</label></div><br/><div class="children"><div class="content">I do think this is an apt analogy. I&#x27;ve heard a counterpoint that there won&#x27;t be enough &quot;destinations&quot; for this to work, but then it&#x27;s not hard to imagine a single order of magnitude more LLM &quot;destinations&quot; than today (all of which were launched in the last 12 months).<p>There&#x27;s also the fact that the data being sent to these LLM &quot;destinations&quot; could be significantly more valuable (or contain significantly more sensitive information) than the average segment identity or track objects.</div><br/></div></div><div id="37096725" class="c"><input type="checkbox" id="c-37096725" checked=""/><div class="controls bullet"><span class="by">detente18</span><span>|</span><a href="#37096662">parent</a><span>|</span><a href="#37096998">prev</a><span>|</span><a href="#37097078">next</a><span>|</span><label class="collapse" for="c-37096725">[-]</label><label class="expand" for="c-37096725">[1 more]</label></div><br/><div class="children"><div class="content">Thanks - that&#x27;s very kind.</div><br/></div></div></div></div><div id="37097078" class="c"><input type="checkbox" id="c-37097078" checked=""/><div class="controls bullet"><span class="by">ada1981</span><span>|</span><a href="#37096662">prev</a><span>|</span><a href="#37096686">next</a><span>|</span><label class="collapse" for="c-37097078">[-]</label><label class="expand" for="c-37097078">[2 more]</label></div><br/><div class="children"><div class="content">So I use my own accounts to access this correct? I didn’t see in the documentation where I provide my credentials. I’ll look again…</div><br/><div id="37097251" class="c"><input type="checkbox" id="c-37097251" checked=""/><div class="controls bullet"><span class="by">ij23</span><span>|</span><a href="#37097078">parent</a><span>|</span><a href="#37096686">next</a><span>|</span><label class="collapse" for="c-37097251">[-]</label><label class="expand" for="c-37097251">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you use your own API keys. You can set them as env variables. 
Either set them as os.environ[&#x27;OPENAI_API_KEY&#x27;] or set them in .env files:
<a href="https:&#x2F;&#x2F;litellm.readthedocs.io&#x2F;en&#x2F;latest&#x2F;supported&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;litellm.readthedocs.io&#x2F;en&#x2F;latest&#x2F;supported&#x2F;</a></div><br/></div></div></div></div><div id="37096686" class="c"><input type="checkbox" id="c-37096686" checked=""/><div class="controls bullet"><span class="by">downvotetruth</span><span>|</span><a href="#37097078">prev</a><span>|</span><label class="collapse" for="c-37096686">[-]</label><label class="expand" for="c-37096686">[3 more]</label></div><br/><div class="children"><div class="content">There are unaddressed github issues like #29 added by whoiskatrin who is not listed as a contributor yet telephone #s and emails are listed on the readme- motive for that is ambiguous and is not likely to result in an optimal outcome.</div><br/><div id="37096715" class="c"><input type="checkbox" id="c-37096715" checked=""/><div class="controls bullet"><span class="by">detente18</span><span>|</span><a href="#37096686">parent</a><span>|</span><label class="collapse" for="c-37096715">[-]</label><label class="expand" for="c-37096715">[2 more]</label></div><br/><div class="children"><div class="content">Hey @downvotetruth, the issues&#x2F;PR&#x27;s are actually ours (the creators). We&#x27;re actively working + using this repo, and use these as ways to keep track of ongoing work. We&#x27;re super open to contributions and help though!</div><br/></div></div></div></div></div></div></div></div></div></body></html>