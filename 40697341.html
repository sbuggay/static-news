<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718614868161" as="style"/><link rel="stylesheet" href="styles.css?v=1718614868161"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://stratechery.com/2024/an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems/">An Interview with AMD CEO Lisa Su About Solving Hard Problems</a> <span class="domain">(<a href="https://stratechery.com">stratechery.com</a>)</span></div><div class="subtext"><span>wallflower</span> | <span>27 comments</span></div><br/><div><div id="40703517" class="c"><input type="checkbox" id="c-40703517" checked=""/><div class="controls bullet"><span class="by">NKosmatos</span><span>|</span><a href="#40703129">next</a><span>|</span><label class="collapse" for="c-40703517">[-]</label><label class="expand" for="c-40703517">[1 more]</label></div><br/><div class="children"><div class="content">I was todays years old, when I found out that Lisa Su (CEO of AMD) and Jensen Huang (co-founder and CEO of NVIDIA) are relatives!
If you can&#x27;t do a merge, it&#x27;s good to have family onboard ;-)</div><br/></div></div><div id="40703129" class="c"><input type="checkbox" id="c-40703129" checked=""/><div class="controls bullet"><span class="by">djvu97</span><span>|</span><a href="#40703517">prev</a><span>|</span><a href="#40703120">next</a><span>|</span><label class="collapse" for="c-40703129">[-]</label><label class="expand" for="c-40703129">[2 more]</label></div><br/><div class="children"><div class="content">Was I the only one who was expecting Lisa Su to attempt Leetcode hard in this interview?</div><br/><div id="40703260" class="c"><input type="checkbox" id="c-40703260" checked=""/><div class="controls bullet"><span class="by">blitzar</span><span>|</span><a href="#40703129">parent</a><span>|</span><a href="#40703120">next</a><span>|</span><label class="collapse" for="c-40703260">[-]</label><label class="expand" for="c-40703260">[1 more]</label></div><br/><div class="children"><div class="content">It is an interview with a CEO - expect platitudes and fortune cookie wisdom that contradict their own (corporate) behaviours.</div><br/></div></div></div></div><div id="40703120" class="c"><input type="checkbox" id="c-40703120" checked=""/><div class="controls bullet"><span class="by">refurb</span><span>|</span><a href="#40703129">prev</a><span>|</span><a href="#40702743">next</a><span>|</span><label class="collapse" for="c-40703120">[-]</label><label class="expand" for="c-40703120">[9 more]</label></div><br/><div class="children"><div class="content">I’m not anti-CEO, I think they play an important role, but why would you interview a CEO about hard technological problems?</div><br/><div id="40703190" class="c"><input type="checkbox" id="c-40703190" checked=""/><div class="controls bullet"><span class="by">DanielHB</span><span>|</span><a href="#40703120">parent</a><span>|</span><a href="#40703426">next</a><span>|</span><label class="collapse" for="c-40703190">[-]</label><label class="expand" for="c-40703190">[1 more]</label></div><br/><div class="children"><div class="content">She worked as a researcher in the field for decades. Moore&#x27;s law only kept up because of some of the techniques she developed.<p>&gt; During her time at IBM,[6] Su played a &quot;critical role&quot;[7] in developing the &quot;recipe&quot;[2] to make copper connections work with semiconductor chips instead of aluminum, &quot;solving the problem of preventing copper impurities from contaminating the devices during production&quot;.[7] Working with various IBM design teams on the details of the device, Su explained, &quot;my specialty was not in copper, but I migrated to where the problems were&quot;.[6] The copper technology was launched in 1998,[7] resulting in new industry standards[22] and chips that were up to 20% faster than the conventional versions.[6][7]</div><br/></div></div><div id="40703426" class="c"><input type="checkbox" id="c-40703426" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#40703120">parent</a><span>|</span><a href="#40703190">prev</a><span>|</span><a href="#40703156">next</a><span>|</span><label class="collapse" for="c-40703426">[-]</label><label class="expand" for="c-40703426">[1 more]</label></div><br/><div class="children"><div class="content">AMD was close to ruin when Lisa took over. It had completely lost the graphics and x64 wars and was limping by on low margin games consoles.<p>Since then Epyc has broken Intel. Anyone buying Xeon&#x27;s today is at credible risk of being fired over it when someone notices the power bill relative to their competition.<p>The graphics ramp is behind Nvidia in mind share and market share. The ROCm software stack gets a lot of grief on here. Nevertheless, Nvidia lost the frontier and el cap bids and now Microsoft is running GPT4 on AMD hardware. Sounds pretty good to me given it&#x27;s one product line of many.<p>If turning a multinational semiconductor firm away from the edge of bankruptcy and into the profitable conglomerate it is today doesn&#x27;t qualify as &quot;solving a hard problem&quot; I don&#x27;t know what would. It&#x27;s way beyond what I&#x27;d be capable of doing.</div><br/></div></div><div id="40703156" class="c"><input type="checkbox" id="c-40703156" checked=""/><div class="controls bullet"><span class="by">yen223</span><span>|</span><a href="#40703120">parent</a><span>|</span><a href="#40703426">prev</a><span>|</span><a href="#40703420">next</a><span>|</span><label class="collapse" for="c-40703156">[-]</label><label class="expand" for="c-40703156">[2 more]</label></div><br/><div class="children"><div class="content">Lisa Su is one of the (surprisingly rare) tech CEOs who comes from an engineering background.</div><br/><div id="40703463" class="c"><input type="checkbox" id="c-40703463" checked=""/><div class="controls bullet"><span class="by">automatic6131</span><span>|</span><a href="#40703120">root</a><span>|</span><a href="#40703156">parent</a><span>|</span><a href="#40703420">next</a><span>|</span><label class="collapse" for="c-40703463">[-]</label><label class="expand" for="c-40703463">[1 more]</label></div><br/><div class="children"><div class="content">And so is her first cousin (once removed) Jensen Huang of Nvidia!</div><br/></div></div></div></div><div id="40703420" class="c"><input type="checkbox" id="c-40703420" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#40703120">parent</a><span>|</span><a href="#40703156">prev</a><span>|</span><a href="#40702743">next</a><span>|</span><label class="collapse" for="c-40703420">[-]</label><label class="expand" for="c-40703420">[4 more]</label></div><br/><div class="children"><div class="content">It says in the article that he was asked by readers to do a Lisa Su interview. The headline is a little misleading, they don&#x27;t talk much about technological problems. The interview is a soft light tour of her career and a few attempts to get her to talk about the present day business. Interviews with someone like Musk or Jensen are much more technically intense.<p>Honestly this interview feels bearish for AMD. Su&#x27;s performance is not good. Thompson repeatedly pushes her to reflect on past mistakes, but it&#x27;s just not happening. The reason why AMD has fallen so far behind NVIDIA shine through clear as day and it doesn&#x27;t look like it&#x27;s going to get fixed anytime soon.<p>Su&#x27;s problem and therefore AMD&#x27;s problem is that she doesn&#x27;t want to think about software at all. Hardware is all she knows and she states that openly. Nor does she seem to consider this a weakness. The problem goes back to the very start of her career. The interview opens with Thompson saying she faced a choice between computer science and electronics engineering at MIT, and she picked EE because it was harder. Is that true? She&#x27;s nowhere in AI due to lack of sufficient skilled devs so now would be a good time to talk up the importance of software, but no, she laughs and says sure! CS seemed easy to her because you &quot;just&quot; write software instead of &quot;building things&quot;, whereas in electronics your stuff &quot;has to work&quot;. End of answer.<p>He tries to get a comment on the (in hindsight) not great design tradeoffs made by the Cell processor, which was hard to program for and so held back the PS3 at critical points in its lifecycle. It was a long time ago so there&#x27;s been plenty of time to reflect on it, yet her only thought is <i>&quot;Perhaps one could say, if you look in hindsight, programmability is so important&quot;</i>. That&#x27;s it! In hindsight, programmability of your CPU is important! Then she immediately returns to hardware again, and saying how proud she was of the leaps in <i>hardware</i> made over the PS generations.<p>He asks her if she&#x27;d stayed at IBM and taken over there, would she have avoided Gerstner&#x27;s mistake of ignoring the cloud? Her answer is <i>&quot;I don’t know that I would’ve been on that path. I was a semiconductor person, I am a semiconductor person.&quot;</i> - again, she seems to just reject on principle the idea that she would think about software, networking or systems architecture because she defines herself as an electronics person.<p>Later Thompson tries harder to ram the point home, asking her <i>&quot;Where is the software piece of this? You can’t just be a hardware cowboy ... What is the reticence to software at AMD and how have you worked to change that?&quot;</i> and she just point-blank denies AMD has ever had a problem with software. Later she claims everything works out of the box with AMD and seems to imply that ROCm hardly matters because everyone is just programming against PyTorch anyway!<p>The final blow comes when he asks her about ChatGPT. A pivotal moment that catapults her competitor to absolute dominance, apparently catching AMD unaware. Thompson asks her what her response was. Was she surprised? Maybe she realized this was an all hands to deck moment? What did NVIDIA do right that you missed? Answer: no, we always knew and have always been good at AI. NVIDIA did nothing different to us.<p>The whole interview is just astonishing. Put under pressure to reflect on her market position, again and again Su retreats to outright denial and management waffle about &quot;product arcs&quot;. It seems to be her go-to safe space. It&#x27;s certainly possible she just decided to play it all as low key as possible and not say anything interesting to protect the share price, but if I was an analyst looking for signs of a quick turnaround in strategy there&#x27;s no sign of that here.</div><br/><div id="40703613" class="c"><input type="checkbox" id="c-40703613" checked=""/><div class="controls bullet"><span class="by">DanielHB</span><span>|</span><a href="#40703120">root</a><span>|</span><a href="#40703420">parent</a><span>|</span><a href="#40703448">next</a><span>|</span><label class="collapse" for="c-40703613">[-]</label><label class="expand" for="c-40703613">[1 more]</label></div><br/><div class="children"><div class="content">In my point of view AMD is going down not because nVidia, but because of ARM and Qualcomm. AMD Ryzen x64 cash cow is going to start declining soon both in the server and consumer space.<p>I saw this clear as day when M1 Macbooks came out and Amazon AWS Graviton servers becoming more popular and cheaper. It was inevitable that the PC world was going to move to ARM soon, in fact I am surprised that it took this long to get viable ARM PC laptops (only this year).<p>So unless AMD has some secret ARM or RISC-V research division close to launch a product I don&#x27;t see how it is going to survive long term.</div><br/></div></div><div id="40703448" class="c"><input type="checkbox" id="c-40703448" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#40703120">root</a><span>|</span><a href="#40703420">parent</a><span>|</span><a href="#40703613">prev</a><span>|</span><a href="#40702743">next</a><span>|</span><label class="collapse" for="c-40703448">[-]</label><label class="expand" for="c-40703448">[2 more]</label></div><br/><div class="children"><div class="content">Electrical engineers do generally think software is easy. Even when their day is a horror show of TCL and verilog. In fairness I think hardware is horrendously difficult, so maybe they&#x27;re not wrong.</div><br/><div id="40703553" class="c"><input type="checkbox" id="c-40703553" checked=""/><div class="controls bullet"><span class="by">DanielHB</span><span>|</span><a href="#40703120">root</a><span>|</span><a href="#40703448">parent</a><span>|</span><a href="#40702743">next</a><span>|</span><label class="collapse" for="c-40703553">[-]</label><label class="expand" for="c-40703553">[1 more]</label></div><br/><div class="children"><div class="content">Most bachelor&#x27;s and master&#x27;s level CS is comparatively easier than EE because EE requires much more hard math. The theory at least, but project-wise CS is more demanding. I had two EE roommates in college, their exams were HARD, but their home-projects were easy compared to CS (less projects overall as well).<p>I remember one exam my roommate complaining about was about getting all the formulas he needed into his scientific calculator before the exam even started. If you understood how to derive all the formulas and knew how to put them in the calculator and how to use them you passed the exam. I think it was analog circuit processing exam but I might be wrong.<p>Research-level in computer science can get very hard as well though. A lot of it is more pure mathematics than engineering.</div><br/></div></div></div></div></div></div></div></div><div id="40702743" class="c"><input type="checkbox" id="c-40702743" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#40703120">prev</a><span>|</span><label class="collapse" for="c-40702743">[-]</label><label class="expand" for="c-40702743">[14 more]</label></div><br/><div class="children"><div class="content">And yet they still can&#x27;t solve the problem of their GPU driver&#x2F;software stack for ML being much worse than NVidia&#x27;s. It seems like the first step is easy: pay more for engineers. AMD pays engineers significantly less than NVidia, and it&#x27;s presumably quite hard to build a competitive software stack while paying so much less. You get what you pay for.</div><br/><div id="40702889" class="c"><input type="checkbox" id="c-40702889" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#40702743">parent</a><span>|</span><a href="#40702750">next</a><span>|</span><label class="collapse" for="c-40702889">[-]</label><label class="expand" for="c-40702889">[5 more]</label></div><br/><div class="children"><div class="content">Everyone does software poorly, hardware companies more so.</div><br/><div id="40702950" class="c"><input type="checkbox" id="c-40702950" checked=""/><div class="controls bullet"><span class="by">jajko</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40702889">parent</a><span>|</span><a href="#40702750">next</a><span>|</span><label class="collapse" for="c-40702950">[-]</label><label class="expand" for="c-40702950">[4 more]</label></div><br/><div class="children"><div class="content">Well this is glaringly obvious to whole world, and Nvidia managed to get it right. Surely a feat that can be repeated elsewhere when enough will is spread over some time. And it would make them grow massively, something no shareholder ever frowns upon.</div><br/><div id="40703066" class="c"><input type="checkbox" id="c-40703066" checked=""/><div class="controls bullet"><span class="by">rf15</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40702950">parent</a><span>|</span><a href="#40702750">next</a><span>|</span><label class="collapse" for="c-40703066">[-]</label><label class="expand" for="c-40703066">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Nvidia managed to get it right<p>I don&#x27;t think they did. If you work in the space and watched it develop over the years, you can see that there&#x27;s been (and still are) plenty of jank and pain points to be found. Their spread mostly comes from their dominant market position before, afaik.</div><br/><div id="40703125" class="c"><input type="checkbox" id="c-40703125" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40703066">parent</a><span>|</span><a href="#40702750">next</a><span>|</span><label class="collapse" for="c-40703125">[-]</label><label class="expand" for="c-40703125">[2 more]</label></div><br/><div class="children"><div class="content">They succeeded not because they are perfect but because they are the least bad. By far.</div><br/><div id="40703462" class="c"><input type="checkbox" id="c-40703462" checked=""/><div class="controls bullet"><span class="by">DanielHB</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40703125">parent</a><span>|</span><a href="#40702750">next</a><span>|</span><label class="collapse" for="c-40703462">[-]</label><label class="expand" for="c-40703462">[1 more]</label></div><br/><div class="children"><div class="content">Also CUDA first released in 2007, I remember researching GPUs at the time and wondering what the hell CUDA was (I was a teenager). They were VERY early to the party and has A LOT of time to improve. Everyone is catching up to ~10 years of a headstart.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40702750" class="c"><input type="checkbox" id="c-40702750" checked=""/><div class="controls bullet"><span class="by">partypete</span><span>|</span><a href="#40702743">parent</a><span>|</span><a href="#40702889">prev</a><span>|</span><a href="#40702998">next</a><span>|</span><label class="collapse" for="c-40702750">[-]</label><label class="expand" for="c-40702750">[1 more]</label></div><br/><div class="children"><div class="content">Came here to say this. They only just recently got an AMD GPU on MLPerf thanks to a (different company), Tinycorp by George Hotz. I guess basic ML performance is too hard a problem.</div><br/></div></div><div id="40702998" class="c"><input type="checkbox" id="c-40702998" checked=""/><div class="controls bullet"><span class="by">cepth</span><span>|</span><a href="#40702743">parent</a><span>|</span><a href="#40702750">prev</a><span>|</span><a href="#40702775">next</a><span>|</span><label class="collapse" for="c-40702998">[-]</label><label class="expand" for="c-40702998">[3 more]</label></div><br/><div class="children"><div class="content">A couple of thoughts here.<p>* AMD&#x27;s traditional target market for its GPUs has been HPC as opposed to deep learning&#x2F;&quot;AI&quot; customers.<p>For example, look at the supercomputers at the national labs. AMD has won quite a few high profile bids with the national labs in recent years:<p>- Frontier (deployment begun in 2021) (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Frontier_(supercomputer)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Frontier_(supercomputer)</a>) - used at Oak Ridge for modeling nuclear reactors, materials science, biology, etc.<p>- El Capitan (2023) (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;El_Capitan_(supercomputer)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;El_Capitan_(supercomputer)</a>) - Livermore national lab<p>AMD GPUs are pretty well represented on the TOP500 list (<a href="https:&#x2F;&#x2F;top500.org&#x2F;lists&#x2F;top500&#x2F;list&#x2F;2024&#x2F;06&#x2F;" rel="nofollow">https:&#x2F;&#x2F;top500.org&#x2F;lists&#x2F;top500&#x2F;list&#x2F;2024&#x2F;06&#x2F;</a>), which tends to feature computers used by major national-level labs for scientific research. AMD CPUs are even moreso represented.<p>* HPC tends to focus exclusively on FP64 computation, since rounding errors in that kind of use-case are a much bigger deal than in DL (see for example <a href="https:&#x2F;&#x2F;hal.science&#x2F;hal-02486753&#x2F;document" rel="nofollow">https:&#x2F;&#x2F;hal.science&#x2F;hal-02486753&#x2F;document</a>). NVIDIA innovations like TensorFloat, mixed precision, custom silicon (e.g., the &quot;transformer engine&quot;) are of limited interest to HPC customers. It&#x27;s no surprise that AMD didn&#x27;t pursue similar R&amp;D, given who they were selling GPUs to.<p>* People tend to forget that less than a decade ago, AMD as a company had a few quarters of cash left before the company would&#x27;ve been bankrupt. When Lisa Su took over as CEO in 2014, AMD market share for all CPUs was 23.4% (even lower in the more lucrative datacenter market). This would bottom out at 17.8% in 2016 (<a href="https:&#x2F;&#x2F;www.trefis.com&#x2F;data&#x2F;companies&#x2F;AMD,.INTC&#x2F;no-login-required&#x2F;TfU13vlN&#x2F;Intel-vs-AMD-How-Have-Revenues-Key-Operating-Metrics-Changed-Over-Recent-Years" rel="nofollow">https:&#x2F;&#x2F;www.trefis.com&#x2F;data&#x2F;companies&#x2F;AMD,.INTC&#x2F;no-login-req...</a>).<p>AMD&#x27;s &quot;Zen moment&quot; didn&#x27;t arrive until March 2017. And it wasn&#x27;t until Zen 2 (July 2019), that major datacenter customers began to adopt AMD CPUs again.<p>* In interviews with key AMD figures like Mark Papermaster and Forrest Norrod, they&#x27;ve mentioned how in the years leading up to the Zen release, all other R&amp;D was slashed to the bone. You can see (<a href="https:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;267873&#x2F;amds-expenditure-on-research-and-development-since-2001" rel="nofollow">https:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;267873&#x2F;amds-expenditure-...</a>) that AMD R&amp;D spending didn&#x27;t surpass its previous peak (on a nominal dollar, not even inflation-adjusted, basis) until 2020.<p>There was barely enough money to fund the CPUs that would stop the company from going bankrupt, much less fund GPU hardware and software development.<p>* By the time AMD could afford to spend on GPU development, CUDA was the entrenched leader. CUDA was first released in 2003(!), ROCm not until 2016. AMD is playing from behind, and had to make various concessions. The ROCm API is designed around CUDA API verbs&#x2F;nouns. AMD funded ZLUDA, intended to be a &quot;translation layer&quot; so that CUDA programs can run as a drop-in on ROCm.<p>* There&#x27;s a chicken-and-egg problem here.<p>1) There&#x27;s only one major cloud (Azure) that has ready access to AMD&#x27;s datacenter-grade GPUs (the Instinct series).<p>2) I suspect a substantial portion of their datacenter revenue still comes from traditional HPC customers, who have no need for the ROCm stack.<p>3) The lack of a ROCm developer ecosystem means that development and bug fixes come much slower than they would for CUDA. For example, the mainline TensorFlow release was broken on ROCm for a while (you had to install the nightly release).<p>4) But, things are improving (slowly). ROCm 6 works substantially better than ROCm 5 did for me. PyTorch and TensorFlow benchmark suites will run.<p>Trust me, I share the frustration around the semi-broken state that ROCm is in for deep learning applications. As an owner of various NVIDIA GPUs (from consumer laptop&#x2F;desktop cards to datacenter accelerators), in 90% of cases things just work on CUDA.<p>On ROCm, as of today it definitely doesn&#x27;t &quot;just work&quot;. I put together a guide for Framework laptop owners to get ROCm working on the AMD GPU that ships as an optional add-in (<a href="https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;installing-rocm-hiplib-on-ubuntu-22-04&#x2F;50412&#x2F;2" rel="nofollow">https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;installing-rocm-hiplib-on-ubu...</a>). This took a lot of head banging, and the parsing of obscure blogs and Github issues.<p>TL;DR, if you consider where AMD GPUs were just a few years ago, things are much better now. But, it still takes too much effort for the average developer to get started on ROCm today.</div><br/><div id="40703394" class="c"><input type="checkbox" id="c-40703394" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40702998">parent</a><span>|</span><a href="#40703274">next</a><span>|</span><label class="collapse" for="c-40703394">[-]</label><label class="expand" for="c-40703394">[1 more]</label></div><br/><div class="children"><div class="content">Small correction: CUDA was first released in 2007 and of course Nvidia was also aiming at HPC before the AlexNet moment.</div><br/></div></div><div id="40703274" class="c"><input type="checkbox" id="c-40703274" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40702998">parent</a><span>|</span><a href="#40703394">prev</a><span>|</span><a href="#40702775">next</a><span>|</span><label class="collapse" for="c-40703274">[-]</label><label class="expand" for="c-40703274">[1 more]</label></div><br/><div class="children"><div class="content">Good summary. There was also the 2010&#x27;s multivendor HSA and OpenCL software evolution directions that ended up losing other vendors on the way and many customers turned out to accept the proprietary Cuda.</div><br/></div></div></div></div><div id="40702775" class="c"><input type="checkbox" id="c-40702775" checked=""/><div class="controls bullet"><span class="by">wordofx</span><span>|</span><a href="#40702743">parent</a><span>|</span><a href="#40702998">prev</a><span>|</span><label class="collapse" for="c-40702775">[-]</label><label class="expand" for="c-40702775">[4 more]</label></div><br/><div class="children"><div class="content">And yet people seem to work just fine with ML on AMD GPUs when they aren’t thinking about Jensen.</div><br/><div id="40702815" class="c"><input type="checkbox" id="c-40702815" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40702775">parent</a><span>|</span><label class="collapse" for="c-40702815">[-]</label><label class="expand" for="c-40702815">[3 more]</label></div><br/><div class="children"><div class="content">Which AMD GPUs? Most consumer AMD GPUs don&#x27;t even support ROCm.</div><br/><div id="40703294" class="c"><input type="checkbox" id="c-40703294" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40702815">parent</a><span>|</span><a href="#40703351">next</a><span>|</span><label class="collapse" for="c-40703294">[-]</label><label class="expand" for="c-40703294">[1 more]</label></div><br/><div class="children"><div class="content">ROCm 6.0 and 6.1 list RDNA3 (gfx1100) and RDNA2 (gfx1030) in their supported architectures list: <a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;compatibility&#x2F;compatibility-matrix.html" rel="nofollow">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;compatibility&#x2F;compatibil...</a><p>Although &quot;official&quot; &#x2F; validated support^ is only for PRO W6800&#x2F;V620 for RDNA2 and RDNA3 RX 7900&#x27;s for consumer. Based on lots of reports you can probably just HSA_OVERRIDE_GFX_VERSION override for other RDNA2&#x2F;3 cards and it&#x27;ll probably just work. I can get GPU-accelerate ROCm for LLM inferencing on my Radeon 780M iGPU for example w&#x2F; ROCm 6.0 and HSA_OVERRIDE_GFX_VERSION=11.0.0<p>(In the past some people also built custom versions of ROCm for older architectures (eg ROC_ENABLE_PRE_VEGA=1) but I have no idea if those work still or not.)<p>^ <a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;projects&#x2F;install-on-linux&#x2F;en&#x2F;latest&#x2F;reference&#x2F;system-requirements.html" rel="nofollow">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;projects&#x2F;install-on-linux&#x2F;en&#x2F;lates...</a></div><br/></div></div><div id="40703351" class="c"><input type="checkbox" id="c-40703351" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#40702743">root</a><span>|</span><a href="#40702815">parent</a><span>|</span><a href="#40703294">prev</a><span>|</span><label class="collapse" for="c-40703351">[-]</label><label class="expand" for="c-40703351">[1 more]</label></div><br/><div class="children"><div class="content">Debian, Arch and Gentoo have ROCm built for consumer GPUs. Thus so do their derivatives. Anything gfx9 or later is likely to be fine and gfx8 has a decent chance of working. The <a href="https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;ROCm">https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;ROCm</a> source has build scripts these days.<p>At least some of the internal developers largely work on consumer hardware. It&#x27;s not as solid as the enterprise gear but it&#x27;s also very cheap so overall that seems reasonable to me. I&#x27;m using a pair of 6900XT, with a pair of VII&#x27;s in a backup machine.<p>For turn key proprietary stuff where you really like the happy path foreseen by your vendor, in classic mainframe style, team green is who you want.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>