<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708246851043" as="style"/><link rel="stylesheet" href="styles.css?v=1708246851043"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ollama.com/blog/windows-preview">Ollama is now available on Windows in preview</a> <span class="domain">(<a href="https://ollama.com">ollama.com</a>)</span></div><div class="subtext"><span>pentagrama</span> | <span>133 comments</span></div><br/><div><div id="39412058" class="c"><input type="checkbox" id="c-39412058" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39411901">next</a><span>|</span><label class="collapse" for="c-39412058">[-]</label><label class="expand" for="c-39412058">[4 more]</label></div><br/><div class="children"><div class="content">I am running this on my desktop, using Open-WebUI for the front-end. I have a collection of a dozen or so fine-tunes of Mistral and a few other models. They are good enough for chatting and doing some information extraction tasks. The Open-WebUI app looks a lot like chatGPT. You can even search your conversations.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui">https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui</a></div><br/><div id="39412763" class="c"><input type="checkbox" id="c-39412763" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#39412058">parent</a><span>|</span><a href="#39413651">next</a><span>|</span><label class="collapse" for="c-39412763">[-]</label><label class="expand" for="c-39412763">[1 more]</label></div><br/><div class="children"><div class="content">For anyone else who missed the announcement a few hours ago, open-webui is the rebranding of the project formerly known as ollama-webui [0].<p>I can vouch for it as a solid frontend for Ollama. It works really well and has had an astounding pace of development. Every few weeks I pull the latest docker images and am always surprised by how much has improved.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui&#x2F;discussions&#x2F;764">https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui&#x2F;discussions&#x2F;764</a></div><br/></div></div><div id="39413651" class="c"><input type="checkbox" id="c-39413651" checked=""/><div class="controls bullet"><span class="by">scratchyone</span><span>|</span><a href="#39412058">parent</a><span>|</span><a href="#39412763">prev</a><span>|</span><a href="#39411901">next</a><span>|</span><label class="collapse" for="c-39413651">[-]</label><label class="expand" for="c-39413651">[2 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, what&#x27;re you using the fine-tunes for? Do you fine-tune them on your own data or are they just publicly available models you use for different tasks?</div><br/><div id="39413752" class="c"><input type="checkbox" id="c-39413752" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39412058">root</a><span>|</span><a href="#39413651">parent</a><span>|</span><a href="#39411901">next</a><span>|</span><label class="collapse" for="c-39413752">[-]</label><label class="expand" for="c-39413752">[1 more]</label></div><br/><div class="children"><div class="content">I am just loading GGUF models from HuggingFace that have good scores in the benchmarks, and running my private eval set from  my current project. Some of the merged models are surprisingly good compared with simple fine-tunes.</div><br/></div></div></div></div></div></div><div id="39411901" class="c"><input type="checkbox" id="c-39411901" checked=""/><div class="controls bullet"><span class="by">Klaster_1</span><span>|</span><a href="#39412058">prev</a><span>|</span><a href="#39412916">next</a><span>|</span><label class="collapse" for="c-39411901">[-]</label><label class="expand" for="c-39411901">[38 more]</label></div><br/><div class="children"><div class="content">As usual, no AMD GPU support mentioned. What a sad state of affair, I regret going with AMD this time.</div><br/><div id="39413715" class="c"><input type="checkbox" id="c-39413715" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39411997">next</a><span>|</span><label class="collapse" for="c-39413715">[-]</label><label class="expand" for="c-39413715">[3 more]</label></div><br/><div class="children"><div class="content">AMD GPU support is definitely an important part of the project roadmap (sorry this isn&#x27;t better published in a ROADMAP.md or similar for the project – will do that soon).<p>A few of the maintainers of the project are from the Toronto area, the original home of ATI technologies [1], and so we personally want to see Ollama work well on AMD GPUs :).<p>One of the test machines we use to work on AMD support for Ollama is running a Radeon RX 7900XT, and it&#x27;s quite fast. Definitely comparable to a high-end GeForce 40 series GPU.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ATI_Technologies" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ATI_Technologies</a></div><br/><div id="39414423" class="c"><input type="checkbox" id="c-39414423" checked=""/><div class="controls bullet"><span class="by">FirmwareBurner</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413715">parent</a><span>|</span><a href="#39411997">next</a><span>|</span><label class="collapse" for="c-39414423">[-]</label><label class="expand" for="c-39414423">[2 more]</label></div><br/><div class="children"><div class="content">What about AMD APUs with RDNA graphics? ANy chance of getting Olama for them?</div><br/><div id="39415173" class="c"><input type="checkbox" id="c-39415173" checked=""/><div class="controls bullet"><span class="by">spookie</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39414423">parent</a><span>|</span><a href="#39411997">next</a><span>|</span><label class="collapse" for="c-39415173">[-]</label><label class="expand" for="c-39415173">[1 more]</label></div><br/><div class="children"><div class="content">I suppose it comes down to ROCm support.
<a href="https:&#x2F;&#x2F;docs.amd.com&#x2F;en&#x2F;docs-5.7.1&#x2F;release&#x2F;windows_support.html" rel="nofollow">https:&#x2F;&#x2F;docs.amd.com&#x2F;en&#x2F;docs-5.7.1&#x2F;release&#x2F;windows_support.h...</a></div><br/></div></div></div></div></div></div><div id="39411997" class="c"><input type="checkbox" id="c-39411997" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39413715">prev</a><span>|</span><a href="#39413491">next</a><span>|</span><label class="collapse" for="c-39411997">[-]</label><label class="expand" for="c-39411997">[19 more]</label></div><br/><div class="children"><div class="content">Same.  I really want AMD to succeed because as a long time Linux user I have strong distaste for Nvidia and the hell they put me through.  I paid <i>a lot</i> for a beastly AMD card in the hopes that it would be shortly behind Nvidia and that has most definitely not been the case, and I blame AMD for not putting the resources behind it.<p>AMD, you can change, but you need to start NOW.</div><br/><div id="39412031" class="c"><input type="checkbox" id="c-39412031" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39411997">parent</a><span>|</span><a href="#39412039">next</a><span>|</span><label class="collapse" for="c-39412031">[-]</label><label class="expand" for="c-39412031">[6 more]</label></div><br/><div class="children"><div class="content">Hi, we’ve been working to support AMD GPUs directly via ROCm. It’s still under development but if you build from source it does work:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;development.md#linux-rocm-amd">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;development....</a></div><br/><div id="39412114" class="c"><input type="checkbox" id="c-39412114" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412031">parent</a><span>|</span><a href="#39412814">next</a><span>|</span><label class="collapse" for="c-39412114">[-]</label><label class="expand" for="c-39412114">[4 more]</label></div><br/><div class="children"><div class="content">Every time I try to run anything through ROCm, my machine kernel-panics.<p>I’m not blaming you for this, but I’m also sticking with nvidia.</div><br/><div id="39412204" class="c"><input type="checkbox" id="c-39412204" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412114">parent</a><span>|</span><a href="#39416271">next</a><span>|</span><label class="collapse" for="c-39412204">[-]</label><label class="expand" for="c-39412204">[2 more]</label></div><br/><div class="children"><div class="content">Really sorry about this. Do you happen to have logs for us to look into? This is definitely not the way we want</div><br/><div id="39413714" class="c"><input type="checkbox" id="c-39413714" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412204">parent</a><span>|</span><a href="#39416271">next</a><span>|</span><label class="collapse" for="c-39413714">[-]</label><label class="expand" for="c-39413714">[1 more]</label></div><br/><div class="children"><div class="content">To be clearer, it isn&#x27;t Ollama-specific. I first encountered the issue with Stable Diffusion, and it&#x27;s remained since, but the GPU that causes it isn&#x27;t currently inside any machine; I replaced it with a 3090 a few days ago.</div><br/></div></div></div></div><div id="39416271" class="c"><input type="checkbox" id="c-39416271" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412114">parent</a><span>|</span><a href="#39412204">prev</a><span>|</span><a href="#39412814">next</a><span>|</span><label class="collapse" for="c-39416271">[-]</label><label class="expand" for="c-39416271">[1 more]</label></div><br/><div class="children"><div class="content">And you&#x27;re the lucky one getting the chance to kernel panic with ROCm. AMD drops ROCm support for their consumer GPUs so fast it&#x27;ll make your head spin. I bought my GPU for $230 in 2020 and by 2021 AMD had dropped support for it. Just a bit under 4 years after the card&#x27;s release on market.</div><br/></div></div></div></div><div id="39412814" class="c"><input type="checkbox" id="c-39412814" checked=""/><div class="controls bullet"><span class="by">agartner</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412031">parent</a><span>|</span><a href="#39412114">prev</a><span>|</span><a href="#39412039">next</a><span>|</span><label class="collapse" for="c-39412814">[-]</label><label class="expand" for="c-39412814">[1 more]</label></div><br/><div class="children"><div class="content">Working well for me on a 7900XT with ROCm 6 and Linux 6.7.5 thanks!</div><br/></div></div></div></div><div id="39412039" class="c"><input type="checkbox" id="c-39412039" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39411997">parent</a><span>|</span><a href="#39412031">prev</a><span>|</span><a href="#39412693">next</a><span>|</span><label class="collapse" for="c-39412039">[-]</label><label class="expand" for="c-39412039">[3 more]</label></div><br/><div class="children"><div class="content">Ollama is a model-management app that runs on top of llama.cpp so you should ask there about AMD support.</div><br/><div id="39412299" class="c"><input type="checkbox" id="c-39412299" checked=""/><div class="controls bullet"><span class="by">progman32</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412039">parent</a><span>|</span><a href="#39413040">next</a><span>|</span><label class="collapse" for="c-39412299">[-]</label><label class="expand" for="c-39412299">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been running llama.cpp with full GPU acceleration on my AMD card, using the text-generation-webui install script on kubuntu. Same with stable diffusion using a1111. AMD&#x27;s compute stack is indeed quite broken and is more fragile, but it does work using most modern cards.<p>The kernel panics though... Yeah, I had those on my Radeon vii before I upgraded.</div><br/></div></div><div id="39413040" class="c"><input type="checkbox" id="c-39413040" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412039">parent</a><span>|</span><a href="#39412299">prev</a><span>|</span><a href="#39412693">next</a><span>|</span><label class="collapse" for="c-39413040">[-]</label><label class="expand" for="c-39413040">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp has had ROCm support for a long time</div><br/></div></div></div></div><div id="39412693" class="c"><input type="checkbox" id="c-39412693" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39411997">parent</a><span>|</span><a href="#39412039">prev</a><span>|</span><a href="#39413491">next</a><span>|</span><label class="collapse" for="c-39412693">[-]</label><label class="expand" for="c-39412693">[9 more]</label></div><br/><div class="children"><div class="content">What problems have you had with AMD and in what fashion do they fall short of Nvidia?</div><br/><div id="39412803" class="c"><input type="checkbox" id="c-39412803" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412693">parent</a><span>|</span><a href="#39413491">next</a><span>|</span><label class="collapse" for="c-39412803">[-]</label><label class="expand" for="c-39412803">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had no end of difficulty installing the Pro drivers and&#x2F;or ROCm.  The &quot;solution&quot; that was recommended was to install a different distro (I use Fedora and installing CentOS or Ubuntu was recommended).  When I finally <i>could</i> get it installed, I got kernel panics and my system frequently became unbootable.  Then once it was installed, getting user space programs to recognize it was the next major pain point.</div><br/><div id="39415972" class="c"><input type="checkbox" id="c-39415972" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412803">parent</a><span>|</span><a href="#39413451">next</a><span>|</span><label class="collapse" for="c-39415972">[-]</label><label class="expand" for="c-39415972">[2 more]</label></div><br/><div class="children"><div class="content">On Fedora 40, I believe you can install llama.cpp&#x27;s ROCm dependencies with:<p><pre><code>    dnf install hipcc rocm-hip-devel rocblas-devel hipblas-devel</code></pre></div><br/><div id="39416699" class="c"><input type="checkbox" id="c-39416699" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39415972">parent</a><span>|</span><a href="#39413451">next</a><span>|</span><label class="collapse" for="c-39416699">[-]</label><label class="expand" for="c-39416699">[1 more]</label></div><br/><div class="children"><div class="content">So, after a bit of experimentation, it seems that Fedora is built primarily for RDNA 3 while Debian is built for RDNA 2 and earlier. These are llama-cpp build instructions for Fedora: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;cgmb&#x2F;bb661fccaf041d3649f9a90560826ebc" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;cgmb&#x2F;bb661fccaf041d3649f9a90560826eb...</a>. These are llama-cpp build instructions for Debian: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;cgmb&#x2F;be113c04cd740425f637aa33c3e4ea33" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;cgmb&#x2F;be113c04cd740425f637aa33c3e4ea3...</a>.</div><br/></div></div></div></div><div id="39413451" class="c"><input type="checkbox" id="c-39413451" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412803">parent</a><span>|</span><a href="#39415972">prev</a><span>|</span><a href="#39413491">next</a><span>|</span><label class="collapse" for="c-39413451">[-]</label><label class="expand" for="c-39413451">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using Nvidia and it stopped being challenging in about 2006. I hear perpetually that Nvidia is horrible and I should try AMD. The 2 times I did admitted a long time ago it was... not great.</div><br/><div id="39413524" class="c"><input type="checkbox" id="c-39413524" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413451">parent</a><span>|</span><a href="#39413491">next</a><span>|</span><label class="collapse" for="c-39413524">[-]</label><label class="expand" for="c-39413524">[4 more]</label></div><br/><div class="children"><div class="content">Do you use Ubuntu LTS?  If so, then indeed Nvidia is not a problem.<p>But if you run a distro that has anywhere near new kernels such as Fedora and Arch, you&#x27;ll be constantly in fear of receiving new kernel updates.  And every so often the packages will be broken and you&#x27;ll have to use Nvidia&#x27;s horrible installer.  Oh and every once in a while they&#x27;ll subtly drop support for older cards and you&#x27;ll need to move to the legacy package, but the way you&#x27;ll find out is that your system suddenly doesn&#x27;t boot and you just happen to think about it being the old Nvidia card so you Kagi that and discover the change.</div><br/><div id="39413893" class="c"><input type="checkbox" id="c-39413893" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413524">parent</a><span>|</span><a href="#39415212">next</a><span>|</span><label class="collapse" for="c-39413893">[-]</label><label class="expand" for="c-39413893">[1 more]</label></div><br/><div class="children"><div class="content">I found it much easier to make ROCm&#x2F;AMD work for AI (including on an laptop) than getting nvidia work with Xorg on an optimus laptop with an intel iGPU&#x2F;nvidia dGPU. I swore off nvidia at that point.</div><br/></div></div><div id="39415212" class="c"><input type="checkbox" id="c-39415212" checked=""/><div class="controls bullet"><span class="by">spookie</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413524">parent</a><span>|</span><a href="#39413893">prev</a><span>|</span><a href="#39414629">next</a><span>|</span><label class="collapse" for="c-39415212">[-]</label><label class="expand" for="c-39415212">[1 more]</label></div><br/><div class="children"><div class="content">Try to use the runfile provided by Nvidia and use DKMS. The biggest issue is just that flatpaks aren&#x27;t really updated for CUDA drivers, but you can just not use them if your distro isn&#x27;t old or niche.</div><br/></div></div><div id="39414629" class="c"><input type="checkbox" id="c-39414629" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413524">parent</a><span>|</span><a href="#39415212">prev</a><span>|</span><a href="#39413491">next</a><span>|</span><label class="collapse" for="c-39414629">[-]</label><label class="expand" for="c-39414629">[1 more]</label></div><br/><div class="children"><div class="content">Changing kernels automatically as new releases came out was never an optimal strategy even if its what you get by default in Arch. Notably arch has linux-lts presently at 6.6 whereas mainline is 6.7.<p>Instead of treating it like a dice roll and living in existential dread at the entirely predictable peril of Linus cutting releases that necessarily occasionally front run NVIDIA which releases less frequently I simply don&#x27;t install kernels first released yesterday, pull in major kernel version updates daily, don&#x27;t remove the old kernel automatically when the new one is installed, and automatically make snapshots on update against any sort of issue that might obtain.<p>If that seems like too much work one could simply at least keep the prior kernel version around and reboot and your only out 45 seconds of your life. This actually seems like a good idea no matter what.<p>I don&#x27;t think I have used nvidia&#x27;s installer since 2003 on Fedora &quot;Core&quot;–as the nomenclature used to be—One. One simply doesn&#x27;t need to. Also generally speaking one doesn&#x27;t need to use a legacy package until a card is over 10 years old. For instance the oldest consumer card unsupported right now is a 600 series from 2012.<p>If you still own a 2012 GPU you should probably put it where it belongs in the trash but when you get to the sort of computers that require legacy support which is 2009-2012 you are apt to need to worry about other matters like distros that still support 32 bit, simple environments like xfce, software that works well in ram constrained environments. Needing to install a slightly different driver seems tractable.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39413491" class="c"><input type="checkbox" id="c-39413491" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39411997">prev</a><span>|</span><a href="#39413180">next</a><span>|</span><label class="collapse" for="c-39413491">[-]</label><label class="expand" for="c-39413491">[1 more]</label></div><br/><div class="children"><div class="content">llamafile has amd gpu support. on windows, it only depends on the graphics driver, thanks to our tinyBLAS library.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;releases&#x2F;tag&#x2F;0.6.2">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;releases&#x2F;tag&#x2F;0.6.2</a><p>By default it opens a browser tab with a chat gui. You can run it as a cli chatbot like ollama as follows:<p><a href="https:&#x2F;&#x2F;justine.lol&#x2F;oneliners&#x2F;#chat" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;oneliners&#x2F;#chat</a></div><br/></div></div><div id="39413180" class="c"><input type="checkbox" id="c-39413180" checked=""/><div class="controls bullet"><span class="by">peppermint_gum</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39413491">prev</a><span>|</span><a href="#39412979">next</a><span>|</span><label class="collapse" for="c-39413180">[-]</label><label class="expand" for="c-39413180">[2 more]</label></div><br/><div class="children"><div class="content">AMD clearly believes that this newfangled &quot;GPU compute&quot; fad will pass soon, so there&#x27;s no point to invest in it.<p>This is one of the worst acts of self-sabotage I have ever seen in the tech business.</div><br/><div id="39413621" class="c"><input type="checkbox" id="c-39413621" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413180">parent</a><span>|</span><a href="#39412979">next</a><span>|</span><label class="collapse" for="c-39413621">[-]</label><label class="expand" for="c-39413621">[1 more]</label></div><br/><div class="children"><div class="content">Zen4 AVX512 must be really good then.</div><br/></div></div></div></div><div id="39412979" class="c"><input type="checkbox" id="c-39412979" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39413180">prev</a><span>|</span><a href="#39412006">next</a><span>|</span><label class="collapse" for="c-39412979">[-]</label><label class="expand" for="c-39412979">[7 more]</label></div><br/><div class="children"><div class="content">As others have mentioned, Ollama uses Llama.CPP under the hood and they recently released Vulkan support which is supposed to work with AMD GPUs. I was able to use llama.cpu compiled with Vulkan support with my app [1] and make it run on an AMD laptop but I was unable to make it work with Ollama as it makes some assumptions about how it goes about searching for available GPUs on a machine.<p>[1]: <a href="https:&#x2F;&#x2F;msty.app" rel="nofollow">https:&#x2F;&#x2F;msty.app</a></div><br/><div id="39413913" class="c"><input type="checkbox" id="c-39413913" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412979">parent</a><span>|</span><a href="#39413731">next</a><span>|</span><label class="collapse" for="c-39413913">[-]</label><label class="expand" for="c-39413913">[2 more]</label></div><br/><div class="children"><div class="content">ROCm is preferred over vulkan for AMD GPUs, performance wise. Using OpenCL or Vulkan should only be for older cards or weird setups.</div><br/><div id="39414523" class="c"><input type="checkbox" id="c-39414523" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413913">parent</a><span>|</span><a href="#39413731">next</a><span>|</span><label class="collapse" for="c-39414523">[-]</label><label class="expand" for="c-39414523">[1 more]</label></div><br/><div class="children"><div class="content">That’s good to know. Thank you!</div><br/></div></div></div></div><div id="39413731" class="c"><input type="checkbox" id="c-39413731" checked=""/><div class="controls bullet"><span class="by">Kelteseth</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39412979">parent</a><span>|</span><a href="#39413913">prev</a><span>|</span><a href="#39413109">next</a><span>|</span><label class="collapse" for="c-39413731">[-]</label><label class="expand" for="c-39413731">[3 more]</label></div><br/><div class="children"><div class="content">I got a Windows defender Virus alert after executing your app.</div><br/><div id="39414522" class="c"><input type="checkbox" id="c-39414522" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413731">parent</a><span>|</span><a href="#39413109">next</a><span>|</span><label class="collapse" for="c-39414522">[-]</label><label class="expand" for="c-39414522">[2 more]</label></div><br/><div class="children"><div class="content">Ugh! Probably because it’s an exe app? Not sure how to go around about that. I am looking into getting it signed just like the counterpart MacOS app. Thank you for the heads up and sorry about the false positive.</div><br/><div id="39416241" class="c"><input type="checkbox" id="c-39416241" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39414522">parent</a><span>|</span><a href="#39413109">next</a><span>|</span><label class="collapse" for="c-39416241">[-]</label><label class="expand" for="c-39416241">[1 more]</label></div><br/><div class="children"><div class="content">Ironically Ollama is also struggling with this sort of thing, see <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2519">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2519</a><p>Code signing helps by having an avenue by which you can establish reliable reputation, and then using VirusTotal to check for AV flags and using the AV vendor&#x27;s whitelist request form is the second part, over time your reputation increases and you don&#x27;t get flagged as malware.<p>It seems to be much more likely with AI stuff, apparently due to use of CUDA or something (&#x2F;shrug)</div><br/></div></div></div></div></div></div></div></div><div id="39412006" class="c"><input type="checkbox" id="c-39412006" checked=""/><div class="controls bullet"><span class="by">accelbred</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39412979">prev</a><span>|</span><a href="#39412015">next</a><span>|</span><label class="collapse" for="c-39412006">[-]</label><label class="expand" for="c-39412006">[1 more]</label></div><br/><div class="children"><div class="content">Ollama has a opencl backend. I&#x27;m on Linux and clblast works great with AMD cards. As far as I remember opencl on Windows did not have that much issues, but its been a while.</div><br/></div></div><div id="39412015" class="c"><input type="checkbox" id="c-39412015" checked=""/><div class="controls bullet"><span class="by">gerwim</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39412006">prev</a><span>|</span><a href="#39412725">next</a><span>|</span><label class="collapse" for="c-39412015">[-]</label><label class="expand" for="c-39412015">[1 more]</label></div><br/><div class="children"><div class="content">Maybe there’s proper support soon in AI landscape [0].<p>[0]: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39344815">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39344815</a></div><br/></div></div><div id="39412725" class="c"><input type="checkbox" id="c-39412725" checked=""/><div class="controls bullet"><span class="by">RealStickman_</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39412015">prev</a><span>|</span><a href="#39413455">next</a><span>|</span><label class="collapse" for="c-39412725">[-]</label><label class="expand" for="c-39412725">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had success using my AMD GPU with the OpenCL backend for llamacpp. The ROCm backend had pretty bad performance though.</div><br/></div></div><div id="39413455" class="c"><input type="checkbox" id="c-39413455" checked=""/><div class="controls bullet"><span class="by">vdaea</span><span>|</span><a href="#39411901">parent</a><span>|</span><a href="#39412725">prev</a><span>|</span><a href="#39412916">next</a><span>|</span><label class="collapse" for="c-39413455">[-]</label><label class="expand" for="c-39413455">[2 more]</label></div><br/><div class="children"><div class="content">AMD is the underdog, and that&#x27;s what happens when you choose the underdog.</div><br/><div id="39415357" class="c"><input type="checkbox" id="c-39415357" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#39411901">root</a><span>|</span><a href="#39413455">parent</a><span>|</span><a href="#39412916">next</a><span>|</span><label class="collapse" for="c-39415357">[-]</label><label class="expand" for="c-39415357">[1 more]</label></div><br/><div class="children"><div class="content">I would argue we are well past the point of calling AMD an underdog.</div><br/></div></div></div></div></div></div><div id="39412916" class="c"><input type="checkbox" id="c-39412916" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39411901">prev</a><span>|</span><a href="#39413915">next</a><span>|</span><label class="collapse" for="c-39412916">[-]</label><label class="expand" for="c-39412916">[23 more]</label></div><br/><div class="children"><div class="content">If anyone is looking for a nice Chat UI on top of Ollama that supports both online models and local models, I’ve been working on an app [1] that is offline and privacy focused. I just released Windows support this morning.<p>[1]: <a href="https:&#x2F;&#x2F;msty.app" rel="nofollow">https:&#x2F;&#x2F;msty.app</a></div><br/><div id="39413472" class="c"><input type="checkbox" id="c-39413472" checked=""/><div class="controls bullet"><span class="by">vdaea</span><span>|</span><a href="#39412916">parent</a><span>|</span><a href="#39416207">next</a><span>|</span><label class="collapse" for="c-39413472">[-]</label><label class="expand" for="c-39413472">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m trying the Windows version. What really sticks out is that buttons don&#x27;t have tooltips. It&#x27;s impossible to know what they do if you don&#x27;t click them.<p>Also in the conversation view you have two buttons &quot;New Chat&quot; and &quot;Add Chat&quot; which do two different things but they both have the same keybind ^T</div><br/><div id="39414081" class="c"><input type="checkbox" id="c-39414081" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39413472">parent</a><span>|</span><a href="#39416207">next</a><span>|</span><label class="collapse" for="c-39414081">[-]</label><label class="expand" for="c-39414081">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for the feedback. I will get them resolved soon.</div><br/><div id="39414284" class="c"><input type="checkbox" id="c-39414284" checked=""/><div class="controls bullet"><span class="by">wlesieutre</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39414081">parent</a><span>|</span><a href="#39416207">next</a><span>|</span><label class="collapse" for="c-39414284">[-]</label><label class="expand" for="c-39414284">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m getting a lot of jank with the hovery-sidebar: <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;VXZXL94" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;VXZXL94</a><p>Personally I&#x27;d rather have the sidebar be toggled on click, instead of having such a huge animation every time my mouse passes by. And if it&#x27;s such an important part of the UI that requiring a click is too much of a barrier, then it&#x27;d be better to build that functionality into a permanent sidebar rather than a buried under a level of sidebar buttons.<p>The sidebar on my Finder windows for example are about 150px wide, always visible, and fit more content than all three of Msty&#x27;s interchanging sidebars put together.<p>If I had a lot of previous conversations that might not be true anymore, but a single level sidebar with subheadings still works fine for things like Music where I can have a long list of playlists. If it&#x27;s too many conversations to reasonably include in an always visible list then maybe they go into a [More] section.<p>Current UI feels like I had to think a bit too much to understand how it&#x27;s organized.</div><br/><div id="39414336" class="c"><input type="checkbox" id="c-39414336" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39414284">parent</a><span>|</span><a href="#39416207">next</a><span>|</span><label class="collapse" for="c-39414336">[-]</label><label class="expand" for="c-39414336">[1 more]</label></div><br/><div class="children"><div class="content">Ugh! That’s not the experience I wanted people to have. Sorry about that and I will be working on making the experience better. Feedback like yours really helps so thank you very much.</div><br/></div></div></div></div></div></div></div></div><div id="39416207" class="c"><input type="checkbox" id="c-39416207" checked=""/><div class="controls bullet"><span class="by">sumedh</span><span>|</span><a href="#39412916">parent</a><span>|</span><a href="#39413472">prev</a><span>|</span><a href="#39413921">next</a><span>|</span><label class="collapse" for="c-39416207">[-]</label><label class="expand" for="c-39416207">[1 more]</label></div><br/><div class="children"><div class="content">Is this similar to LLM Studio?</div><br/></div></div><div id="39413921" class="c"><input type="checkbox" id="c-39413921" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#39412916">parent</a><span>|</span><a href="#39416207">prev</a><span>|</span><a href="#39413251">next</a><span>|</span><label class="collapse" for="c-39413921">[-]</label><label class="expand" for="c-39413921">[6 more]</label></div><br/><div class="children"><div class="content">btw, it triggers Program:Win32&#x2F;Wacapew.C!ml detection</div><br/><div id="39414107" class="c"><input type="checkbox" id="c-39414107" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39413921">parent</a><span>|</span><a href="#39413251">next</a><span>|</span><label class="collapse" for="c-39414107">[-]</label><label class="expand" for="c-39414107">[5 more]</label></div><br/><div class="children"><div class="content">Hmmm… it’s a false positive. Is it Windows Defender or something else? Is it when you open the app or when you setup local AI? Not sure where I would send a request for it to be not flagged.</div><br/><div id="39414155" class="c"><input type="checkbox" id="c-39414155" checked=""/><div class="controls bullet"><span class="by">js4ever</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39414107">parent</a><span>|</span><a href="#39413251">next</a><span>|</span><label class="collapse" for="c-39414155">[-]</label><label class="expand" for="c-39414155">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s probably an issue with the tool you used to create the installer.<p>Few options: use another tool like the one included in visual studio, sign your exe with a certificate. Or publish it on the windows marketplace.<p>Now you understand why real desktop applications died a decade ago and now 99.99% of apps are using a web UI</div><br/><div id="39414308" class="c"><input type="checkbox" id="c-39414308" checked=""/><div class="controls bullet"><span class="by">dom96</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39414155">parent</a><span>|</span><a href="#39414223">next</a><span>|</span><label class="collapse" for="c-39414308">[-]</label><label class="expand" for="c-39414308">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say it&#x27;s more an issue with the anti-virus that is flagging this as a virus when it isn&#x27;t one. We should expect better out of AV software. I&#x27;ve personally seen many instances of false positives across various software that was definitely not a virus.</div><br/><div id="39416256" class="c"><input type="checkbox" id="c-39416256" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39414308">parent</a><span>|</span><a href="#39414223">next</a><span>|</span><label class="collapse" for="c-39416256">[-]</label><label class="expand" for="c-39416256">[1 more]</label></div><br/><div class="children"><div class="content">There seems to be something about what these AI apps do that causes the false positives, because Ollama itself <i>also</i> triggers Windows defender <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2519">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;2519</a></div><br/></div></div></div></div><div id="39414223" class="c"><input type="checkbox" id="c-39414223" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39414155">parent</a><span>|</span><a href="#39414308">prev</a><span>|</span><a href="#39413251">next</a><span>|</span><label class="collapse" for="c-39414223">[-]</label><label class="expand" for="c-39414223">[1 more]</label></div><br/><div class="children"><div class="content">That’s true and unfortunate. The MacOS installer is signed and I will be looking into signing the Windows installer. Thank you for your suggestions. My last experience of getting and signing a Windows installer was awful and goes back to what you were saying about desktop app dying a decade ago.</div><br/></div></div></div></div></div></div></div></div><div id="39413251" class="c"><input type="checkbox" id="c-39413251" checked=""/><div class="controls bullet"><span class="by">haliskerbas</span><span>|</span><a href="#39412916">parent</a><span>|</span><a href="#39413921">prev</a><span>|</span><a href="#39413406">next</a><span>|</span><label class="collapse" for="c-39413251">[-]</label><label class="expand" for="c-39413251">[4 more]</label></div><br/><div class="children"><div class="content">Off topic but what did you use to make your landing page?</div><br/><div id="39413273" class="c"><input type="checkbox" id="c-39413273" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39413251">parent</a><span>|</span><a href="#39413406">next</a><span>|</span><label class="collapse" for="c-39413273">[-]</label><label class="expand" for="c-39413273">[3 more]</label></div><br/><div class="children"><div class="content">It’s a Nuxt static app but all hand rolled, no builder or anything like that if that’s what you are asking.</div><br/><div id="39413285" class="c"><input type="checkbox" id="c-39413285" checked=""/><div class="controls bullet"><span class="by">haliskerbas</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39413273">parent</a><span>|</span><a href="#39413406">next</a><span>|</span><label class="collapse" for="c-39413285">[-]</label><label class="expand" for="c-39413285">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what I was looking for, I couldn&#x27;t find any elements or class names that I recognized in the code from the popular frameworks. Looks great!</div><br/><div id="39413331" class="c"><input type="checkbox" id="c-39413331" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39413285">parent</a><span>|</span><a href="#39413406">next</a><span>|</span><label class="collapse" for="c-39413331">[-]</label><label class="expand" for="c-39413331">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! I probably spent way too much time tweaking it. Haha! I am glad someone liked all the small details I was sweating on.</div><br/></div></div></div></div></div></div></div></div><div id="39413406" class="c"><input type="checkbox" id="c-39413406" checked=""/><div class="controls bullet"><span class="by">vorticalbox</span><span>|</span><a href="#39412916">parent</a><span>|</span><a href="#39413251">prev</a><span>|</span><a href="#39413889">next</a><span>|</span><label class="collapse" for="c-39413406">[-]</label><label class="expand" for="c-39413406">[2 more]</label></div><br/><div class="children"><div class="content">Any plans for a Linux client?</div><br/><div id="39413663" class="c"><input type="checkbox" id="c-39413663" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39413406">parent</a><span>|</span><a href="#39413889">next</a><span>|</span><label class="collapse" for="c-39413663">[-]</label><label class="expand" for="c-39413663">[1 more]</label></div><br/><div class="children"><div class="content">Yes. I already have a local build that I am testing. Going to release it in a week or so.</div><br/></div></div></div></div><div id="39413889" class="c"><input type="checkbox" id="c-39413889" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#39412916">parent</a><span>|</span><a href="#39413406">prev</a><span>|</span><a href="#39413915">next</a><span>|</span><label class="collapse" for="c-39413889">[-]</label><label class="expand" for="c-39413889">[5 more]</label></div><br/><div class="children"><div class="content">adding gemini API?</div><br/><div id="39414084" class="c"><input type="checkbox" id="c-39414084" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39413889">parent</a><span>|</span><a href="#39413915">next</a><span>|</span><label class="collapse" for="c-39414084">[-]</label><label class="expand" for="c-39414084">[4 more]</label></div><br/><div class="children"><div class="content">Yes. I signed up for the API a couple of days ago and I am in the waitlist.</div><br/><div id="39415162" class="c"><input type="checkbox" id="c-39415162" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39414084">parent</a><span>|</span><a href="#39413915">next</a><span>|</span><label class="collapse" for="c-39415162">[-]</label><label class="expand" for="c-39415162">[3 more]</label></div><br/><div class="children"><div class="content">gemini-pro is available without a wait list. Go to AI studio to get a key.</div><br/><div id="39416126" class="c"><input type="checkbox" id="c-39416126" checked=""/><div class="controls bullet"><span class="by">BOOSTERHIDROGEN</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39415162">parent</a><span>|</span><a href="#39413915">next</a><span>|</span><label class="collapse" for="c-39416126">[-]</label><label class="expand" for="c-39416126">[2 more]</label></div><br/><div class="children"><div class="content">Wait how to get that ? I just submit email on waitlist</div><br/><div id="39417021" class="c"><input type="checkbox" id="c-39417021" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#39412916">root</a><span>|</span><a href="#39416126">parent</a><span>|</span><a href="#39413915">next</a><span>|</span><label class="collapse" for="c-39417021">[-]</label><label class="expand" for="c-39417021">[1 more]</label></div><br/><div class="children"><div class="content">waitlist is for ultra or 1.5, whatever, 1.0 pro is available<p>aistudio.google.com</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39413915" class="c"><input type="checkbox" id="c-39413915" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#39412916">prev</a><span>|</span><a href="#39413784">next</a><span>|</span><label class="collapse" for="c-39413915">[-]</label><label class="expand" for="c-39413915">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious what people think of the non-open-source LM Studio (<a href="https:&#x2F;&#x2F;lmstudio.ai" rel="nofollow">https:&#x2F;&#x2F;lmstudio.ai</a>) compared to Ollama.</div><br/><div id="39414093" class="c"><input type="checkbox" id="c-39414093" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#39413915">parent</a><span>|</span><a href="#39413784">next</a><span>|</span><label class="collapse" for="c-39414093">[-]</label><label class="expand" for="c-39414093">[3 more]</label></div><br/><div class="children"><div class="content">Likes:<p>* Super easy setup<p>* One-click download and load models&#x2F;weights<p>* Works great<p>Dislikes:<p>* throws weights (in Windows) in &#x2F;users&#x2F;username&#x2F;.cache in a proprietary directory structure, eating up tens of gigs without telling you or letting you share them with other clients<p>* won&#x27;t let you import models you download yourself<p>* Search function is terrible<p>* I hate how it deals with instance settings</div><br/><div id="39415264" class="c"><input type="checkbox" id="c-39415264" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#39413915">root</a><span>|</span><a href="#39414093">parent</a><span>|</span><a href="#39415633">next</a><span>|</span><label class="collapse" for="c-39415264">[-]</label><label class="expand" for="c-39415264">[1 more]</label></div><br/><div class="children"><div class="content">&gt; * won&#x27;t let you import models you download yourself<p>you can drop GGUF in the models folder following its structure and LM Studio will pick it up.<p>What I wish LMS and others improve on is downloading models. At the very least they should support resume and retry of failed downloads. Also multistream would help. Huggingface CDN isn&#x27;t the most reliable and redownloading failed multigigabytes models isn&#x27;t fun. Of course I could do it manually but then it&#x27;s not &quot;one-click download&quot;.</div><br/></div></div><div id="39415633" class="c"><input type="checkbox" id="c-39415633" checked=""/><div class="controls bullet"><span class="by">spamfilter247</span><span>|</span><a href="#39413915">root</a><span>|</span><a href="#39414093">parent</a><span>|</span><a href="#39415264">prev</a><span>|</span><a href="#39413784">next</a><span>|</span><label class="collapse" for="c-39415633">[-]</label><label class="expand" for="c-39415633">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone know where it stores GGUFs on macOS?</div><br/></div></div></div></div></div></div><div id="39413784" class="c"><input type="checkbox" id="c-39413784" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413915">prev</a><span>|</span><a href="#39412564">next</a><span>|</span><label class="collapse" for="c-39413784">[-]</label><label class="expand" for="c-39413784">[31 more]</label></div><br/><div class="children"><div class="content">What is the rationale for so many of these ‘run it locally’ AI ports to run <i>as a server</i>?<p>Have developers forgotten that it’s actually possible to run code inside your UI process?<p>We see the same thing with stable diffusion runners as well as LLM hosts.<p>I don’t like running background services locally if I don’t need to. Why do these implementations all seem to operate that way?</div><br/><div id="39413879" class="c"><input type="checkbox" id="c-39413879" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39414021">next</a><span>|</span><label class="collapse" for="c-39413879">[-]</label><label class="expand" for="c-39413879">[7 more]</label></div><br/><div class="children"><div class="content">This is a really interesting question. I think there&#x27;s definitely a world for both deployment models. Maybe a good analogy is database engines: both SQLite (a library) and Postgres (a long-running service) have widespread use cases with tradeoffs.</div><br/><div id="39413908" class="c"><input type="checkbox" id="c-39413908" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413879">parent</a><span>|</span><a href="#39414021">next</a><span>|</span><label class="collapse" for="c-39413908">[-]</label><label class="expand" for="c-39413908">[6 more]</label></div><br/><div class="children"><div class="content">But these are typically filling the usecases of <i>productivity</i> applications, not ‘engines’.<p>Microsoft Word doesn’t run its grammar checker as an external service and shunt JSON over a localhost socket to get spelling and style suggestions.<p>Photoshop doesn’t install a background service to host filters.<p>The closest pattern I can think of is the ‘language servers’ model used by IDEs to handle autosuggest - see <a href="https:&#x2F;&#x2F;microsoft.github.io&#x2F;language-server-protocol&#x2F;" rel="nofollow">https:&#x2F;&#x2F;microsoft.github.io&#x2F;language-server-protocol&#x2F;</a> - but the point of that is to enable many to many interop - multiple languages supporting multiple IDEs. Is that the expected usecase for local language assistants and image generators?</div><br/><div id="39414488" class="c"><input type="checkbox" id="c-39414488" checked=""/><div class="controls bullet"><span class="by">bri3d</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413908">parent</a><span>|</span><a href="#39414982">next</a><span>|</span><label class="collapse" for="c-39414488">[-]</label><label class="expand" for="c-39414488">[3 more]</label></div><br/><div class="children"><div class="content">Funny choice of example. You’ve always been able to use Word as a remote spellchecker over COM, and as of Windows 8, spellchecking is available system wide and runs in a separate process (again over COM) for sandboxing reasons.<p>JSON over TCP is perhaps a silly IPC mechanism for local services, but this kind of composition doesn’t seem unreasonable to me.</div><br/><div id="39415613" class="c"><input type="checkbox" id="c-39415613" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39414488">parent</a><span>|</span><a href="#39414982">next</a><span>|</span><label class="collapse" for="c-39415613">[-]</label><label class="expand" for="c-39415613">[2 more]</label></div><br/><div class="children"><div class="content">&gt; use Word as a remote spellchecker over COM<p>That&#x27;s not how COM works. You can load Word&#x27;s spellchecker <i>into your process</i>.<p>Windows added a spellchecking API in Windows 8. I&#x27;ve not dug into the API in detail, but don&#x27;t see any indication that spellchecker providers run in a separate process (you can probably build one that works that way, but it&#x27;s not intrinsic to the provider model).</div><br/><div id="39415770" class="c"><input type="checkbox" id="c-39415770" checked=""/><div class="controls bullet"><span class="by">bri3d</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39415613">parent</a><span>|</span><a href="#39414982">next</a><span>|</span><label class="collapse" for="c-39415770">[-]</label><label class="expand" for="c-39415770">[1 more]</label></div><br/><div class="children"><div class="content">Are you not familiar with out of process COM servers? A lot of Office automation is out of process, even inside of Office itself. Admittedly I’m not sure about the grammar checker specifically.<p>As for the Spellcheck API, external providers are explicitly out of proc: <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;win32&#x2F;intl&#x2F;about-the-spell-checker-api" rel="nofollow">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;win32&#x2F;intl&#x2F;about-t...</a><p>Anyway, my point still stands - building desktop apps using composition over RPC is neither new nor a bad idea, although HTTP might not be the best RPC mechanism (although… neither was COM…)</div><br/></div></div></div></div></div></div><div id="39414982" class="c"><input type="checkbox" id="c-39414982" checked=""/><div class="controls bullet"><span class="by">pseudosavant</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413908">parent</a><span>|</span><a href="#39414488">prev</a><span>|</span><a href="#39414021">next</a><span>|</span><label class="collapse" for="c-39414982">[-]</label><label class="expand" for="c-39414982">[2 more]</label></div><br/><div class="children"><div class="content">The language server pattern is actually a very good comparison. The web service + web UI approach enables you do use different local and&#x2F;or cloud AI services. That is why most of these servers&#x2F;services support the OpenAI API.</div><br/><div id="39415635" class="c"><input type="checkbox" id="c-39415635" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39414982">parent</a><span>|</span><a href="#39414021">next</a><span>|</span><label class="collapse" for="c-39415635">[-]</label><label class="expand" for="c-39415635">[1 more]</label></div><br/><div class="children"><div class="content">Which means most of these servers limit themselves to the capabilities exposed by the OpenAI API.</div><br/></div></div></div></div></div></div></div></div><div id="39414021" class="c"><input type="checkbox" id="c-39414021" checked=""/><div class="controls bullet"><span class="by">psytrx</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39413879">prev</a><span>|</span><a href="#39413805">next</a><span>|</span><label class="collapse" for="c-39414021">[-]</label><label class="expand" for="c-39414021">[1 more]</label></div><br/><div class="children"><div class="content">In addition to the initial loading time noted by the other posters:<p>You may want to use the same inference engine or even the same LLM for multiple purposes in multiple applications.<p>Also, which is a huge factor in my opinion, is getting your machine, environment and OS into a state that can&#x27;t run the models efficiently. It wasn&#x27;t trivial to me. Putting all this complexity inside a container (and therefore &quot;server&quot;) helps tremendously, a) in setting everything up initially and b) keeping up with the constant improvements and updates that are happening regularly.</div><br/></div></div><div id="39413805" class="c"><input type="checkbox" id="c-39413805" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39414021">prev</a><span>|</span><a href="#39413896">next</a><span>|</span><label class="collapse" for="c-39413805">[-]</label><label class="expand" for="c-39413805">[6 more]</label></div><br/><div class="children"><div class="content">It doesn’t make sense to load the weights on the fly- that is gigabits of memory that has to be shuffled around. Instead, you have a long running process that serves up lots of predictions<p>(edit: someday soon, probably to multiple clients too!)</div><br/><div id="39413817" class="c"><input type="checkbox" id="c-39413817" checked=""/><div class="controls bullet"><span class="by">nightfly</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413805">parent</a><span>|</span><a href="#39413814">next</a><span>|</span><label class="collapse" for="c-39413817">[-]</label><label class="expand" for="c-39413817">[4 more]</label></div><br/><div class="children"><div class="content">So better to have GiBs of memory consumed by it constantly?</div><br/><div id="39413840" class="c"><input type="checkbox" id="c-39413840" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413817">parent</a><span>|</span><a href="#39413814">next</a><span>|</span><label class="collapse" for="c-39413840">[-]</label><label class="expand" for="c-39413840">[3 more]</label></div><br/><div class="children"><div class="content">If you don’t have that memory to spare you can’t run this locally anyways, and keeping it in memory is the only way to have a fast experience. Paying the model loading cost repeatedly sucks.</div><br/><div id="39413878" class="c"><input type="checkbox" id="c-39413878" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413840">parent</a><span>|</span><a href="#39413814">next</a><span>|</span><label class="collapse" for="c-39413878">[-]</label><label class="expand" for="c-39413878">[2 more]</label></div><br/><div class="children"><div class="content">Why would linking llama.cpp into a UI application lead to incurring the model loading cost repeatedly?</div><br/><div id="39413925" class="c"><input type="checkbox" id="c-39413925" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413878">parent</a><span>|</span><a href="#39413814">next</a><span>|</span><label class="collapse" for="c-39413925">[-]</label><label class="expand" for="c-39413925">[1 more]</label></div><br/><div class="children"><div class="content">It would be loaded repeatedly if the ui is opened and closed repeatedly. You can achieve the same “long running server + short running ui window” with multiple threads or processes all linked into one binary if you want of course. This way (with a separate server) seems simpler to me (and has the added benefit that multiple applications could easily call into the “server” if needed)</div><br/></div></div></div></div></div></div></div></div><div id="39413814" class="c"><input type="checkbox" id="c-39413814" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413805">parent</a><span>|</span><a href="#39413817">prev</a><span>|</span><a href="#39413896">next</a><span>|</span><label class="collapse" for="c-39413814">[-]</label><label class="expand" for="c-39413814">[1 more]</label></div><br/><div class="children"><div class="content">Local UI applications <i>are</i> long running processes normally</div><br/></div></div></div></div><div id="39413896" class="c"><input type="checkbox" id="c-39413896" checked=""/><div class="controls bullet"><span class="by">imiric</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39413805">prev</a><span>|</span><a href="#39414191">next</a><span>|</span><label class="collapse" for="c-39413896">[-]</label><label class="expand" for="c-39413896">[1 more]</label></div><br/><div class="children"><div class="content">This is a good thing IMO. I don&#x27;t have a very powerful laptop or workstation, but do have a multi-GPU headless server. These projects allow me to experiment with LLMs on my server, and expose an API and web UI to my LAN.</div><br/></div></div><div id="39414191" class="c"><input type="checkbox" id="c-39414191" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39413896">prev</a><span>|</span><a href="#39414859">next</a><span>|</span><label class="collapse" for="c-39414191">[-]</label><label class="expand" for="c-39414191">[1 more]</label></div><br/><div class="children"><div class="content">In addition to everything that everyone else has said: I run Ollama on a large gaming PC for speed but want to be able to use the models from elsewhere in the house. So I run Open-WebUI at chat.domain.example and Ollama at api.chat.domain.example (both only accessible within my local network).<p>With this setup I can use my full-speed local models from both my laptop and my phone with the web UI, and my raspberry pi that&#x27;s running my experimental voice assistant can query Ollama through the API endpoints, all at the full speed enabled by my gaming GPU.<p>The same logic goes for my Stable Diffusion setup.</div><br/></div></div><div id="39414859" class="c"><input type="checkbox" id="c-39414859" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39414191">prev</a><span>|</span><a href="#39416468">next</a><span>|</span><label class="collapse" for="c-39414859">[-]</label><label class="expand" for="c-39414859">[1 more]</label></div><br/><div class="children"><div class="content">Because it adds flexibility. By decoupling the frontend from the backend it&#x27;s much easier for other devs not directly affiliated with the server repo (e.g. Ollama) to design new frontends that can connect to it.<p>I also think it allows experts to focus on what they are good at. Some people have a really keen eye for aesthetics and can design amazing front and experiences, and some people are the exact opposite and prefer to work on the backend.<p>Additionally, since it runs as a server, I can place it on a powerful headless machine that I have and can access that easily from significantly less powerful devices such as my phone and laptop.</div><br/></div></div><div id="39416468" class="c"><input type="checkbox" id="c-39416468" checked=""/><div class="controls bullet"><span class="by">andersa</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39414859">prev</a><span>|</span><a href="#39413953">next</a><span>|</span><label class="collapse" for="c-39416468">[-]</label><label class="expand" for="c-39416468">[1 more]</label></div><br/><div class="children"><div class="content">I personally find it very useful, because it allows me to run the inference server on a powerful remote server while running the UI locally on a laptop or tablet.</div><br/></div></div><div id="39413953" class="c"><input type="checkbox" id="c-39413953" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39416468">prev</a><span>|</span><a href="#39415059">next</a><span>|</span><label class="collapse" for="c-39413953">[-]</label><label class="expand" for="c-39413953">[3 more]</label></div><br/><div class="children"><div class="content">Because running it locally really means running it on a cloud server that you own and is called by other server that you own. This gives you the ability to make the interfaces lightweight and most importantly to not pay premiums to model servers.</div><br/><div id="39413985" class="c"><input type="checkbox" id="c-39413985" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413953">parent</a><span>|</span><a href="#39415059">next</a><span>|</span><label class="collapse" for="c-39413985">[-]</label><label class="expand" for="c-39413985">[2 more]</label></div><br/><div class="children"><div class="content">No, running it locally means running it on my laptop.<p>My Mac M2 is quite capable of running stable diffusion XL models and 30M parameter. LLMs under llama.cpp.<p>What I don’t like is the trend towards the way to do that being to open up network listeners with no authentication on them.</div><br/><div id="39414092" class="c"><input type="checkbox" id="c-39414092" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39413985">parent</a><span>|</span><a href="#39415059">next</a><span>|</span><label class="collapse" for="c-39414092">[-]</label><label class="expand" for="c-39414092">[1 more]</label></div><br/><div class="children"><div class="content">Bind to localhost then</div><br/></div></div></div></div></div></div><div id="39415059" class="c"><input type="checkbox" id="c-39415059" checked=""/><div class="controls bullet"><span class="by">Kuinox</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39413953">prev</a><span>|</span><a href="#39415012">next</a><span>|</span><label class="collapse" for="c-39415059">[-]</label><label class="expand" for="c-39415059">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll probably uses that because the Rust binding to llamacpp doesn&#x27;t works on windows (well, cpu only works, so not usable).  
Python is broken (can&#x27;t install the deps)<p>Also mind that loading theses models take dozens of seconds, and you can only load one at a time on your machine, so if you have multiple programs that want to run theses models, it make sense to delegate this job to another program that the user can control.</div><br/></div></div><div id="39415012" class="c"><input type="checkbox" id="c-39415012" checked=""/><div class="controls bullet"><span class="by">ijustlovemath</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39415059">prev</a><span>|</span><a href="#39414510">next</a><span>|</span><label class="collapse" for="c-39415012">[-]</label><label class="expand" for="c-39415012">[1 more]</label></div><br/><div class="children"><div class="content">Not mentioned yet: you can &quot;mitm&quot; existing APIs, like OpenAI, so that you can use existing applications with Ollama without changing your code.<p>Really clever, IMO! I was also mystified by the choice until I saw that use case.</div><br/></div></div><div id="39414510" class="c"><input type="checkbox" id="c-39414510" checked=""/><div class="controls bullet"><span class="by">kaliqt</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39415012">prev</a><span>|</span><a href="#39414319">next</a><span>|</span><label class="collapse" for="c-39414510">[-]</label><label class="expand" for="c-39414510">[1 more]</label></div><br/><div class="children"><div class="content">Heavy compute. Often you might need to outsource the model to another PC and also because it&#x27;s heavy compute and general models, multiple apps use the same model at the same time.</div><br/></div></div><div id="39414319" class="c"><input type="checkbox" id="c-39414319" checked=""/><div class="controls bullet"><span class="by">justsomehnguy</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39414510">prev</a><span>|</span><a href="#39413883">next</a><span>|</span><label class="collapse" for="c-39414319">[-]</label><label class="expand" for="c-39414319">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I don’t like running background services locally if I don’t need to. Why do these implementations all seem to operate that way?<p>Because it&#x27;s now a simple REST-like query to interact with that server.<p>Default model of running the binary and capturing it&#x27;s output would mean you would reload everything <i>each time</i>. Of course, you can write a master process what would actually perform the queries and have a separate executable for querying that master process... wait, you just invented a server.</div><br/><div id="39414435" class="c"><input type="checkbox" id="c-39414435" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39414319">parent</a><span>|</span><a href="#39413883">next</a><span>|</span><label class="collapse" for="c-39414435">[-]</label><label class="expand" for="c-39414435">[3 more]</label></div><br/><div class="children"><div class="content">I’m not sure what this ‘default model of running a binary and capturing its output’ is that you’re talking about.<p>Aren’t people mostly running browser frontends in front of these to provide a persistent UI - a chat interface or an image workspace or something?<p>sure, if you’re running a lot of little command line tools that need access to an LLM a server makes sense but what I don’t understand is why that isn’t a <i>niche</i> way of distributing these things - instead it seems to be the default.</div><br/><div id="39415050" class="c"><input type="checkbox" id="c-39415050" checked=""/><div class="controls bullet"><span class="by">justsomehnguy</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39414435">parent</a><span>|</span><a href="#39413883">next</a><span>|</span><label class="collapse" for="c-39415050">[-]</label><label class="expand" for="c-39415050">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  I’m not sure what this ‘default model of running a binary and capturing its output’ is that you’re talking about.<p>Did you ever used a computer?<p><pre><code>    PS C:\Users\Administrator\AppData\Local\Programs\Ollama&gt; .&#x2F;ollama.exe run llama2:7b &quot;say hello&quot; --verbose
    Hello! How can I help you today?

    total duration:       35.9150092s
    load duration:        1.7888ms
    prompt eval duration: 1.941793s
    prompt eval rate:     0.00 tokens&#x2F;s
    eval count:           10 token(s)
    eval duration:        16.988289s
    eval rate:            0.59 tokens&#x2F;s
</code></pre>
But I feel like you are here just to troll around without a merit or a target.</div><br/><div id="39415535" class="c"><input type="checkbox" id="c-39415535" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#39413784">root</a><span>|</span><a href="#39415050">parent</a><span>|</span><a href="#39413883">next</a><span>|</span><label class="collapse" for="c-39415535">[-]</label><label class="expand" for="c-39415535">[1 more]</label></div><br/><div class="children"><div class="content">If you just check out <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a> and run make, you’ll wind up with an executable called ‘main’ that lets you run any gguf language model you choose. Then:<p>.&#x2F;main -m .&#x2F;models&#x2F;30B&#x2F;llama-30b.Q4_K_M.gguf --prompt “say hello”<p>On my M2 MacBook, the first run takes a few seconds before it produces anything, but after that subsequent runs start outputting tokens immediately.<p>You can run LLM models right inside a short lived process.<p>But the <i>majority</i> of humans don’t want to use a single execution of a command line to access LLM completions. They want to run a program that lets them interact with an LLM. And to do that they will likely start and leave running a long-lived process with UI state - which can also serve as a host for a longer lived LLM context.<p>Neither usecase particularly seems to need a server to function. My curiosity about why people are packaging these things up like that is completely genuine.<p>Last run of llama.cpp main off my command line:<p><pre><code>   llama_print_timings:        load time =     871.43 ms
   llama_print_timings:      sample time =      20.39 ms &#x2F;   259 runs   (    0.08 ms per token, 12702.31 tokens per second)
   llama_print_timings: prompt eval time =     397.77 ms &#x2F;     3 tokens (  132.59 ms per token,     7.54 tokens per second)
   llama_print_timings:        eval time =   20079.05 ms &#x2F;   258 runs   (   77.83 ms per token,    12.85 tokens per second)
   llama_print_timings:       total time =   20534.77 ms &#x2F;   261 tokens</code></pre></div><br/></div></div></div></div></div></div></div></div><div id="39413883" class="c"><input type="checkbox" id="c-39413883" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39414319">prev</a><span>|</span><a href="#39414752">next</a><span>|</span><label class="collapse" for="c-39413883">[-]</label><label class="expand" for="c-39413883">[1 more]</label></div><br/><div class="children"><div class="content">The main reason I see is to use the same AI engine for multiple things like VSCode plugins, UI apps, etc.<p>That being said I use LM Studio which runs as a UI and allows you to start a local server for coding and editor plugins.<p>I can run Deepseek Coder in VSCode locally on an M1 Max and it’s actually useful. It’ll just eat the battery quickly if it’s not plugged in since it really slams the GPU. It’s about the only thing I use that will make the M1 make audible fan noise.</div><br/></div></div><div id="39414752" class="c"><input type="checkbox" id="c-39414752" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39413784">parent</a><span>|</span><a href="#39413883">prev</a><span>|</span><a href="#39412564">next</a><span>|</span><label class="collapse" for="c-39414752">[-]</label><label class="expand" for="c-39414752">[1 more]</label></div><br/><div class="children"><div class="content">You have a beefy computer with lots of vram for testing locally, and then once that’s running you want to use the same thing from other computers or from web servers etc. that can’t run the models themselves.</div><br/></div></div></div></div><div id="39412564" class="c"><input type="checkbox" id="c-39412564" checked=""/><div class="controls bullet"><span class="by">hat_tr1ck</span><span>|</span><a href="#39413784">prev</a><span>|</span><a href="#39411985">next</a><span>|</span><label class="collapse" for="c-39412564">[-]</label><label class="expand" for="c-39412564">[6 more]</label></div><br/><div class="children"><div class="content">Had no idea Windows users had no access to Ollama, feels like only a few years ago we Mac users would have been the ones having to wait</div><br/><div id="39412704" class="c"><input type="checkbox" id="c-39412704" checked=""/><div class="controls bullet"><span class="by">mil22</span><span>|</span><a href="#39412564">parent</a><span>|</span><a href="#39412718">next</a><span>|</span><label class="collapse" for="c-39412704">[-]</label><label class="expand" for="c-39412704">[2 more]</label></div><br/><div class="children"><div class="content">It has worked just fine under WSL for many months now, including full GPU support, though that&#x27;s not as convenient for most. Native Windows support is icing on the cake.</div><br/><div id="39413685" class="c"><input type="checkbox" id="c-39413685" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39412564">root</a><span>|</span><a href="#39412704">parent</a><span>|</span><a href="#39412718">next</a><span>|</span><label class="collapse" for="c-39413685">[-]</label><label class="expand" for="c-39413685">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, WSL has surprisingly good GPU passthrough and AVX instruction support, which makes running models fast albeit the virtualization layer. WSL comes with it&#x27;s own setup steps and performance considerations (not to mention quite a few folks are still using WSL 1 in their workflow), and so a lot of folks asked for a pre-built Windows version that runs natively!</div><br/></div></div></div></div><div id="39412718" class="c"><input type="checkbox" id="c-39412718" checked=""/><div class="controls bullet"><span class="by">hu3</span><span>|</span><a href="#39412564">parent</a><span>|</span><a href="#39412704">prev</a><span>|</span><a href="#39411985">next</a><span>|</span><label class="collapse" for="c-39412718">[-]</label><label class="expand" for="c-39412718">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been running Ollama in Windows WSL for some time now.<p>It&#x27;s x86 Linux after all. Everything just works.</div><br/><div id="39412913" class="c"><input type="checkbox" id="c-39412913" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39412564">root</a><span>|</span><a href="#39412718">parent</a><span>|</span><a href="#39412847">next</a><span>|</span><label class="collapse" for="c-39412913">[-]</label><label class="expand" for="c-39412913">[1 more]</label></div><br/><div class="children"><div class="content">There’s some magic with wsl gpu drivers.</div><br/></div></div></div></div></div></div><div id="39411985" class="c"><input type="checkbox" id="c-39411985" checked=""/><div class="controls bullet"><span class="by">trelane</span><span>|</span><a href="#39412564">prev</a><span>|</span><a href="#39413122">next</a><span>|</span><label class="collapse" for="c-39411985">[-]</label><label class="expand" for="c-39411985">[1 more]</label></div><br/><div class="children"><div class="content">Looks like it&#x27;s already available on Linux &amp; Mac. The change is that they&#x27;re adding Windows: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama</a></div><br/></div></div><div id="39413122" class="c"><input type="checkbox" id="c-39413122" checked=""/><div class="controls bullet"><span class="by">tydunn</span><span>|</span><a href="#39411985">prev</a><span>|</span><a href="#39414237">next</a><span>|</span><label class="collapse" for="c-39413122">[-]</label><label class="expand" for="c-39413122">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been playing around with it for the last couple days on my Windows machine, using it for local tab-autocomplete in VS Code, and it&#x27;s been just as good as it is on my Mac</div><br/><div id="39413474" class="c"><input type="checkbox" id="c-39413474" checked=""/><div class="controls bullet"><span class="by">notsylver</span><span>|</span><a href="#39413122">parent</a><span>|</span><a href="#39414237">next</a><span>|</span><label class="collapse" for="c-39413474">[-]</label><label class="expand" for="c-39413474">[3 more]</label></div><br/><div class="children"><div class="content">What do you use for tab-autocomplete in VS Code? I&#x27;ve been trying to find something that can replace copilot, just because it sounds fun. Everyhing I&#x27;ve found seems more aimed at entering a prompt and having it refactor code, not completing as you write with no other input.</div><br/><div id="39414856" class="c"><input type="checkbox" id="c-39414856" checked=""/><div class="controls bullet"><span class="by">sqs</span><span>|</span><a href="#39413122">root</a><span>|</span><a href="#39413474">parent</a><span>|</span><a href="#39413508">next</a><span>|</span><label class="collapse" for="c-39414856">[-]</label><label class="expand" for="c-39414856">[1 more]</label></div><br/><div class="children"><div class="content">Cody (<a href="https:&#x2F;&#x2F;github.com&#x2F;sourcegraph&#x2F;cody">https:&#x2F;&#x2F;github.com&#x2F;sourcegraph&#x2F;cody</a>) supports using Ollama for autocomplete in VS Code. See the release notes at <a href="https:&#x2F;&#x2F;sourcegraph.com&#x2F;blog&#x2F;cody-vscode-1.1.0-release" rel="nofollow">https:&#x2F;&#x2F;sourcegraph.com&#x2F;blog&#x2F;cody-vscode-1.1.0-release</a> for instructions. And soon it&#x27;ll support Ollama for chat&#x2F;refactoring as well (<a href="https:&#x2F;&#x2F;twitter.com&#x2F;sqs&#x2F;status&#x2F;1750045006382162346&#x2F;video&#x2F;1" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;sqs&#x2F;status&#x2F;1750045006382162346&#x2F;video&#x2F;1</a>).<p>Disclaimer: I work on Cody and hacked on this feature.</div><br/></div></div><div id="39413508" class="c"><input type="checkbox" id="c-39413508" checked=""/><div class="controls bullet"><span class="by">tydunn</span><span>|</span><a href="#39413122">root</a><span>|</span><a href="#39413474">parent</a><span>|</span><a href="#39414856">prev</a><span>|</span><a href="#39414237">next</a><span>|</span><label class="collapse" for="c-39413508">[-]</label><label class="expand" for="c-39413508">[1 more]</label></div><br/><div class="children"><div class="content">I use Continue.dev&#x27;s new tab-autocomplete [1] (disclaimer: I am one of the authors of this open-source project)<p>[1] <a href="https:&#x2F;&#x2F;continue.dev&#x2F;docs&#x2F;walkthroughs&#x2F;tab-autocomplete">https:&#x2F;&#x2F;continue.dev&#x2F;docs&#x2F;walkthroughs&#x2F;tab-autocomplete</a></div><br/></div></div></div></div></div></div><div id="39414237" class="c"><input type="checkbox" id="c-39414237" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#39413122">prev</a><span>|</span><a href="#39413440">next</a><span>|</span><label class="collapse" for="c-39414237">[-]</label><label class="expand" for="c-39414237">[4 more]</label></div><br/><div class="children"><div class="content">I just ran this on my new Mac Mini (installing the llama2 model) and got a full-blown kernel panic. What?!</div><br/><div id="39414504" class="c"><input type="checkbox" id="c-39414504" checked=""/><div class="controls bullet"><span class="by">asabla</span><span>|</span><a href="#39414237">parent</a><span>|</span><a href="#39413440">next</a><span>|</span><label class="collapse" for="c-39414504">[-]</label><label class="expand" for="c-39414504">[3 more]</label></div><br/><div class="children"><div class="content">This may happen if you chose a model which is larger then your available unified memory.<p>Which version of llama2 did you choose? And how much unified memory do you have?</div><br/><div id="39414638" class="c"><input type="checkbox" id="c-39414638" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#39414237">root</a><span>|</span><a href="#39414504">parent</a><span>|</span><a href="#39413440">next</a><span>|</span><label class="collapse" for="c-39414638">[-]</label><label class="expand" for="c-39414638">[2 more]</label></div><br/><div class="children"><div class="content">Just the default (7B parameters?), which apparently requires at least 8GB of RAM, which is how much I have. It seems to be running fine now... it&#x27;s just the installation process that caused it to completely die.</div><br/><div id="39416220" class="c"><input type="checkbox" id="c-39416220" checked=""/><div class="controls bullet"><span class="by">risho</span><span>|</span><a href="#39414237">root</a><span>|</span><a href="#39414638">parent</a><span>|</span><a href="#39413440">next</a><span>|</span><label class="collapse" for="c-39416220">[-]</label><label class="expand" for="c-39416220">[1 more]</label></div><br/><div class="children"><div class="content">if it requires 8gb of ram and you have 8gb of ram that leaves nothing left for the other processes that need to run and the gpu which also shares the ram.</div><br/></div></div></div></div></div></div></div></div><div id="39413440" class="c"><input type="checkbox" id="c-39413440" checked=""/><div class="controls bullet"><span class="by">orion138</span><span>|</span><a href="#39414237">prev</a><span>|</span><a href="#39415349">next</a><span>|</span><label class="collapse" for="c-39413440">[-]</label><label class="expand" for="c-39413440">[1 more]</label></div><br/><div class="children"><div class="content">Is this people’s preferred way to run and develop on top of local models?<p>Any other must learn tools?</div><br/></div></div><div id="39415349" class="c"><input type="checkbox" id="c-39415349" checked=""/><div class="controls bullet"><span class="by">waldrews</span><span>|</span><a href="#39413440">prev</a><span>|</span><a href="#39412085">next</a><span>|</span><label class="collapse" for="c-39415349">[-]</label><label class="expand" for="c-39415349">[2 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t seem to get it to see my NVIDIA Quadro P3000 GPU.  Is it too old to make a difference?</div><br/><div id="39415589" class="c"><input type="checkbox" id="c-39415589" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39415349">parent</a><span>|</span><a href="#39412085">next</a><span>|</span><label class="collapse" for="c-39415589">[-]</label><label class="expand" for="c-39415589">[1 more]</label></div><br/><div class="children"><div class="content">Ollama should support anything CUDA compute capability 5+ (P3000 is 6.1) <a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;cuda-gpus" rel="nofollow">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;cuda-gpus</a>. Possible to shoot me an email? (in my HN bio). The `server` logs should have information regarding GPU detection in the first 10-20 lines or so that can help debug. Sorry!</div><br/></div></div></div></div><div id="39412085" class="c"><input type="checkbox" id="c-39412085" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39415349">prev</a><span>|</span><a href="#39414301">next</a><span>|</span><label class="collapse" for="c-39412085">[-]</label><label class="expand" for="c-39412085">[9 more]</label></div><br/><div class="children"><div class="content">This is the first time I use llama2 (I used the 13B one) -- and it absolutely struggles with Chinese despite it says &quot;I am capable of understanding Chinese and English&quot; when asking &quot;what languages do you understand&quot;.<p>Is this expected?<p>See an example below, it can&#x27;t stay in Chinese at all.<p><pre><code>    &gt;&gt;&gt; 你知道海豚吗

    Ah, 海豚 (hǎitún) is a type of dolphin!  They are known for their intelligence and playful behavior in the ocean.
    Is there anything else you would like to know or discuss?

    &gt;&gt;&gt; 请用中文回答

    Ah, I see!  As a 13b model, I can only communicate in Chinese. Here&#x27;s my answer:

    海豚是一种智能和活泼的 marine mammal他们主要生活在海洋中。它们有着柔软的皮服、圆润的脸和小的耳朵。他们是 ocean 中的一 种美丽和 интерес的生物很多人喜欢去看他们的表演。</code></pre></div><br/><div id="39415239" class="c"><input type="checkbox" id="c-39415239" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39412085">parent</a><span>|</span><a href="#39412118">next</a><span>|</span><label class="collapse" for="c-39415239">[-]</label><label class="expand" for="c-39415239">[1 more]</label></div><br/><div class="children"><div class="content">Never, ever make assumptions about what the model can and cannot do based on what it tells you itself. This is one area where they hallucinate <i>a lot</i>, even top-of-the-line stuff like GPT-4 (e.g. it will happily &quot;translate&quot; languages that it has very little understanding of).</div><br/></div></div><div id="39412118" class="c"><input type="checkbox" id="c-39412118" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#39412085">parent</a><span>|</span><a href="#39415239">prev</a><span>|</span><a href="#39414678">next</a><span>|</span><label class="collapse" for="c-39412118">[-]</label><label class="expand" for="c-39412118">[1 more]</label></div><br/><div class="children"><div class="content">There is the qwen 1.5 model from Alibaba team.<p><a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;qwen">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;qwen</a><p>ollama run qwen:0.5b
ollama run qwen:1.8b
ollama run qwen:4b
ollama run qwen:7b
ollama run qwen:14b
ollama run qwen:72b<p>I would only recommend smaller parameter sizes if you are fine tuning with it.</div><br/></div></div><div id="39414678" class="c"><input type="checkbox" id="c-39414678" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#39412085">parent</a><span>|</span><a href="#39412118">prev</a><span>|</span><a href="#39416246">next</a><span>|</span><label class="collapse" for="c-39414678">[-]</label><label class="expand" for="c-39414678">[1 more]</label></div><br/><div class="children"><div class="content">Its Japanese ability is even worse... and by that I mean it&#x27;s basically nonexistent. You have to really persuade it to speak the language, and even then it&#x27;s very reluctant and outputs complete gibberish most of the time.<p>Interestingly, trying the &#x27;llama2:text&#x27; (the raw model without the fine tuning for chat) gives much better results, although still quite weird. Maybe the fine tuning process — since it presumably focuses on English — destroys what little Japanese ability was in there to begin with.<p>(of course, none of this is surprising; as far as I know it doesn&#x27;t claim to be able to communicate in Japanese.)</div><br/></div></div><div id="39416246" class="c"><input type="checkbox" id="c-39416246" checked=""/><div class="controls bullet"><span class="by">cyp0633</span><span>|</span><a href="#39412085">parent</a><span>|</span><a href="#39414678">prev</a><span>|</span><a href="#39412773">next</a><span>|</span><label class="collapse" for="c-39416246">[-]</label><label class="expand" for="c-39416246">[1 more]</label></div><br/><div class="children"><div class="content">Mistral-7B answers in Chinese only when I explicitly tells it to do so</div><br/></div></div><div id="39412773" class="c"><input type="checkbox" id="c-39412773" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39412085">parent</a><span>|</span><a href="#39416246">prev</a><span>|</span><a href="#39415196">next</a><span>|</span><label class="collapse" for="c-39412773">[-]</label><label class="expand" for="c-39412773">[2 more]</label></div><br/><div class="children"><div class="content">get yourself a proper Chinese model from China, they are hosted in the Ollama model zoo as well</div><br/><div id="39412904" class="c"><input type="checkbox" id="c-39412904" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39412085">root</a><span>|</span><a href="#39412773">parent</a><span>|</span><a href="#39415196">next</a><span>|</span><label class="collapse" for="c-39412904">[-]</label><label class="expand" for="c-39412904">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true. I was more just out of curiosity because ChatGPT has <i>great</i> Chinese capability even the 3.5 version.</div><br/></div></div></div></div><div id="39415196" class="c"><input type="checkbox" id="c-39415196" checked=""/><div class="controls bullet"><span class="by">charcircuit</span><span>|</span><a href="#39412085">parent</a><span>|</span><a href="#39412773">prev</a><span>|</span><a href="#39414120">next</a><span>|</span><label class="collapse" for="c-39415196">[-]</label><label class="expand" for="c-39415196">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Is this expected?<p>Yes, the training was primarily focused on English text and performance on English prompts. Only 0.13% of the training data was Chinese.<p>&gt;Does Llama 2 support other languages outside of English?<p>&gt;The model was primarily trained on English with a bit of additional data from 27 other languages. We do not expect the same level of performance in these languages as in English.<p><a href="https:&#x2F;&#x2F;llama.meta.com&#x2F;llama2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llama.meta.com&#x2F;llama2&#x2F;</a></div><br/></div></div><div id="39414120" class="c"><input type="checkbox" id="c-39414120" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#39412085">parent</a><span>|</span><a href="#39415196">prev</a><span>|</span><a href="#39414301">next</a><span>|</span><label class="collapse" for="c-39414120">[-]</label><label class="expand" for="c-39414120">[1 more]</label></div><br/><div class="children"><div class="content">Give Yi a shot.</div><br/></div></div></div></div><div id="39414301" class="c"><input type="checkbox" id="c-39414301" checked=""/><div class="controls bullet"><span class="by">justsomehnguy</span><span>|</span><a href="#39412085">prev</a><span>|</span><a href="#39413095">next</a><span>|</span><label class="collapse" for="c-39414301">[-]</label><label class="expand" for="c-39414301">[1 more]</label></div><br/><div class="children"><div class="content">JUST as I wanted to dabble on that and try myself installing all those ... requirements.<p>And now this article.<p>Tested, yes, it&#x27;s amusing on how simple it is and it works.<p>The only trouble I see is what again there is no option to select the destination of the installer (so if you have a server and multiple users they all end with a personal copy, instead of the global one).</div><br/></div></div></div></div></div></div></div></body></html>