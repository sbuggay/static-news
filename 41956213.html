<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730365268680" as="style"/><link rel="stylesheet" href="styles.css?v=1730365268680"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.nextplatform.com/2024/10/25/cerebras-trains-llama-models-to-leap-over-gpus/">Cerebras Trains Llama Models to Leap over GPUs</a> <span class="domain">(<a href="https://www.nextplatform.com">www.nextplatform.com</a>)</span></div><div class="subtext"><span>rbanffy</span> | <span>27 comments</span></div><br/><div><div id="42003352" class="c"><input type="checkbox" id="c-42003352" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42003688">next</a><span>|</span><label class="collapse" for="c-42003352">[-]</label><label class="expand" for="c-42003352">[15 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  1x MI300x has 192GB HBM3.

  1x MI325x has 256GB HBM3e.
</code></pre>
They cost less, you can fit more into a rack and you can buy&#x2F;deploy at least the 300&#x27;s today and 325&#x27;s early next year. AMD and library software performance for AI is improving daily [0].<p>I&#x27;m still trying to wrap my head around how these companies think they are going to do well in this market without more memory.<p>[0] <a href="https:&#x2F;&#x2F;blog.vllm.ai&#x2F;2024&#x2F;10&#x2F;23&#x2F;vllm-serving-amd.html" rel="nofollow">https:&#x2F;&#x2F;blog.vllm.ai&#x2F;2024&#x2F;10&#x2F;23&#x2F;vllm-serving-amd.html</a></div><br/><div id="42003394" class="c"><input type="checkbox" id="c-42003394" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#42003352">parent</a><span>|</span><a href="#42003473">next</a><span>|</span><label class="collapse" for="c-42003394">[-]</label><label class="expand" for="c-42003394">[2 more]</label></div><br/><div class="children"><div class="content">Groq and Cerebras only make sense at massive scale which is why I guess they pivoted to being API providers so they can amortize the hardware over many customers.</div><br/><div id="42003494" class="c"><input type="checkbox" id="c-42003494" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003394">parent</a><span>|</span><a href="#42003473">next</a><span>|</span><label class="collapse" for="c-42003494">[-]</label><label class="expand" for="c-42003494">[1 more]</label></div><br/><div class="children"><div class="content">Correct except that massive scale doesn&#x27;t work cause it just uses up exponentially more power&#x2F;space&#x2F;resources.<p>They also have a very limited use case... if things ever shift away from LLM&#x27;s and into another form of engineering that their hardware does not support, what are they going to do? Just keep deploying hardware?<p>Slippery slope.</div><br/></div></div></div></div><div id="42003473" class="c"><input type="checkbox" id="c-42003473" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#42003352">parent</a><span>|</span><a href="#42003394">prev</a><span>|</span><a href="#42004182">next</a><span>|</span><label class="collapse" for="c-42003473">[-]</label><label class="expand" for="c-42003473">[11 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m still trying to wrap my head around how these companies think they are going to do well in this market without more memory.<p>Cerebras and Groq provide the fastest (by an order of magnitude) inference. This is very useful for certain workflows, which require low-latency feedback: audio chat with LLM, robotics, etc.<p>Outside that narrow niche, AMD stuff seems to be the only contender to NVIDIA, at the moment.</div><br/><div id="42003512" class="c"><input type="checkbox" id="c-42003512" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003473">parent</a><span>|</span><a href="#42004329">next</a><span>|</span><label class="collapse" for="c-42003512">[-]</label><label class="expand" for="c-42003512">[9 more]</label></div><br/><div class="children"><div class="content">&gt; Cerebras and Groq provide the fastest (by an order of magnitude) inference.<p>Only on smaller models, their numbers are all 70b in the article.<p>Those numbers also need to be adjusted for the comparable amounts of capex+opex costs. If the costs are so high that they have to subsidize the usage&#x2F;results, then they are just going to run out of money, fast.</div><br/><div id="42003586" class="c"><input type="checkbox" id="c-42003586" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003512">parent</a><span>|</span><a href="#42004329">next</a><span>|</span><label class="collapse" for="c-42003586">[-]</label><label class="expand" for="c-42003586">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Only on smaller models, their numbers are all 70b in the article.<p>No, they are 5x-10x faster for all the model sizes (because it&#x27;s all just running from SRAM and they have more of it than NVIDIA&#x2F;AMD), even though they benchmarked just up to 70B.<p>&gt; Those numbers also need to be adjusted for the comparable amounts of capex+opex costs. If the costs are so high that they have to subsidize the usage&#x2F;results, then they are just going to run out of money, fast.<p>True. Although, for some workloads, fast enough inference is a strict prerequisite and GPUs just don&#x27;t cut it.</div><br/><div id="42003823" class="c"><input type="checkbox" id="c-42003823" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003586">parent</a><span>|</span><a href="#42003642">next</a><span>|</span><label class="collapse" for="c-42003823">[-]</label><label class="expand" for="c-42003823">[5 more]</label></div><br/><div class="children"><div class="content">576 CS-3 nodes costs around $900 million, which is $1.56 million per node.<p>It takes 4 nodes to serve one 70B model or $6.24m. It is unclear how many requests they can serve concurrently for this, they only report on tokens.<p>A cluster of 128 MI300x, which has a combined 24,576GB and can serve up a whole ton of models and users, 4 racks total, is in the ~$5m range, if you don&#x27;t go big on networking&#x2F;disk&#x2F;ram (which you don&#x27;t need for inference anyway).<p>While speed might be an issue here, I don&#x27;t think people are going to be able to justify the price for the speed (always the tradeoff) unless they can get their costs down significantly.</div><br/><div id="42003879" class="c"><input type="checkbox" id="c-42003879" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003823">parent</a><span>|</span><a href="#42004229">next</a><span>|</span><label class="collapse" for="c-42003879">[-]</label><label class="expand" for="c-42003879">[1 more]</label></div><br/><div class="children"><div class="content">You are right assuming that model capabilities are determined only by model size. But consider that OpenAI is saying they have a way of scaling intelligence with inference time compute, not just model size. If that proves out, reducing latency per output token potentially becomes as valuable as or even possibly more valuable than scaling model size. Speed <i>becomes</i> intelligence. And Cerebras has 1&#x2F;10 the latency per token of anything else.</div><br/></div></div><div id="42004229" class="c"><input type="checkbox" id="c-42004229" checked=""/><div class="controls bullet"><span class="by">campers</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003823">parent</a><span>|</span><a href="#42003879">prev</a><span>|</span><a href="#42003918">next</a><span>|</span><label class="collapse" for="c-42004229">[-]</label><label class="expand" for="c-42004229">[1 more]</label></div><br/><div class="children"><div class="content">On Google Cloud a server with 8 TPU v5e will do 2175 token&#x2F;seconds on Llama2 70B.<p><a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;compute&#x2F;updates-to-ai-hypercomputer-software-stack&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;compute&#x2F;updates-to-ai...</a><p>From <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;pricing" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;pricing</a> and <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;vertex-ai&#x2F;pricing#prediction-prices" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;vertex-ai&#x2F;pricing#prediction-prices</a> (search for ct5lp-hightpu-8t on the page) the cost for that appears to be $11.04&#x2F;hr which is just under $100k for a year. Or half that on a 3-year commit.<p>That seems like a better deal than millions for a few CS-3 nodes.<p>And they&#x27;ve just announced the v6 TPU:<p><pre><code>  Compared to TPU v5e, Trillium delivers: 
  Over 4x improvement in training performance 
  Up to 3x increase in inference throughput 
  A 67% increase in energy efficiency
  An impressive 4.7x increase in peak compute performance per chip 
  Double the High Bandwidth Memory (HBM) capacity 
  Double the Interchip Interconnect (ICI) bandwidth 
</code></pre>
<a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;compute&#x2F;trillium-sixth-generation-tpu-is-in-preview&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;compute&#x2F;trillium-sixt...</a></div><br/></div></div><div id="42003918" class="c"><input type="checkbox" id="c-42003918" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003823">parent</a><span>|</span><a href="#42004229">prev</a><span>|</span><a href="#42004188">next</a><span>|</span><label class="collapse" for="c-42003918">[-]</label><label class="expand" for="c-42003918">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re correct on $&#x2F;bandwidth. The point about low latency continues to be ignored, though.</div><br/></div></div><div id="42004188" class="c"><input type="checkbox" id="c-42004188" checked=""/><div class="controls bullet"><span class="by">arisAlexis</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003823">parent</a><span>|</span><a href="#42003918">prev</a><span>|</span><a href="#42003642">next</a><span>|</span><label class="collapse" for="c-42004188">[-]</label><label class="expand" for="c-42004188">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a benchmark or shower thought?</div><br/></div></div></div></div><div id="42003642" class="c"><input type="checkbox" id="c-42003642" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003586">parent</a><span>|</span><a href="#42003823">prev</a><span>|</span><a href="#42004329">next</a><span>|</span><label class="collapse" for="c-42003642">[-]</label><label class="expand" for="c-42003642">[2 more]</label></div><br/><div class="children"><div class="content">Cerebras had less chip perimeter to hook up external memory I&#x2F;O and is memory capacity limited with just SRAM. SRAM circuit size hasn&#x27;t been scaling nearly as well as logic on recent nodes, but if scaling there had continued to from when Cerebras started it may have worked out better.<p>They&#x27;ll probably still have to do advanced packaging putting HBM on top to save things.<p>They could maybe enable some cool real time inference stuff like VR SORA, but that doesn&#x27;t seem like much of a product market for the cost yet.<p>Maybe something heavy on inference iteration like an o1 style model that trades training time for more inference, used to process earnings reports the fastest or something zero sum latency war like that will be a viable market.  A real time use case that may be viable with cerebras first could be with flexible robotics in ad hoc latency sensitive environments, maybe warfare.<p>If models keep lasting ~year timescales could we ever see people going with ROM chips for the weights instead of memory?  Has density and speed kept up there?  Lots of stuff uses identical elements to help make the masks more cheaply, so I don&#x27;t think you could use something like EUV for ROM where every few um^2 of die is distinct.</div><br/><div id="42003799" class="c"><input type="checkbox" id="c-42003799" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003642">parent</a><span>|</span><a href="#42004329">next</a><span>|</span><label class="collapse" for="c-42003799">[-]</label><label class="expand" for="c-42003799">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If models keep lasting ~year timescales could we ever see people going with ROM chips for the weights instead of memory?<p>Before ROM, there&#x27;s a step where HBM for weights is replaced with Flash or Optane (but still high bandwidth, on top of the chip) and KV cache lives in SRAM - for small batch sizes, that would actually be decently cheap. In this case, even if weights change weekly, it&#x27;s not a big deal at all.</div><br/></div></div></div></div></div></div></div></div><div id="42004329" class="c"><input type="checkbox" id="c-42004329" checked=""/><div class="controls bullet"><span class="by">hhdhdbdb</span><span>|</span><a href="#42003352">root</a><span>|</span><a href="#42003473">parent</a><span>|</span><a href="#42003512">prev</a><span>|</span><a href="#42004182">next</a><span>|</span><label class="collapse" for="c-42004329">[-]</label><label class="expand" for="c-42004329">[1 more]</label></div><br/><div class="children"><div class="content">How is this a narrow niche?<p>Chain of thought type operations is in this &quot;niche&quot;.<p>Also anything where the value is in the follow up chat not the one shot.</div><br/></div></div></div></div><div id="42004182" class="c"><input type="checkbox" id="c-42004182" checked=""/><div class="controls bullet"><span class="by">arisAlexis</span><span>|</span><a href="#42003352">parent</a><span>|</span><a href="#42003473">prev</a><span>|</span><a href="#42003688">next</a><span>|</span><label class="collapse" for="c-42004182">[-]</label><label class="expand" for="c-42004182">[1 more]</label></div><br/><div class="children"><div class="content">The article explains in depth the issues with memory, did you read through ?</div><br/></div></div></div></div><div id="42003688" class="c"><input type="checkbox" id="c-42003688" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#42003352">prev</a><span>|</span><a href="#42002870">next</a><span>|</span><label class="collapse" for="c-42003688">[-]</label><label class="expand" for="c-42003688">[1 more]</label></div><br/><div class="children"><div class="content">Title is about training.... article about inference</div><br/></div></div><div id="42002870" class="c"><input type="checkbox" id="c-42002870" checked=""/><div class="controls bullet"><span class="by">asdf1145</span><span>|</span><a href="#42003688">prev</a><span>|</span><a href="#42003656">next</a><span>|</span><label class="collapse" for="c-42002870">[-]</label><label class="expand" for="c-42002870">[6 more]</label></div><br/><div class="children"><div class="content">clickbait title: inference is not training</div><br/><div id="42002949" class="c"><input type="checkbox" id="c-42002949" checked=""/><div class="controls bullet"><span class="by">mentalically</span><span>|</span><a href="#42002870">parent</a><span>|</span><a href="#42003656">next</a><span>|</span><label class="collapse" for="c-42002949">[-]</label><label class="expand" for="c-42002949">[5 more]</label></div><br/><div class="children"><div class="content">The value proposition of Cerebras is that they can compile existing graphs to their hardware and allow inference at lower costs and higher efficiencies. The title does not say anything about creating or optimizing new architectures from scratch.</div><br/><div id="42003207" class="c"><input type="checkbox" id="c-42003207" checked=""/><div class="controls bullet"><span class="by">germanjoey</span><span>|</span><a href="#42002870">root</a><span>|</span><a href="#42002949">parent</a><span>|</span><a href="#42003656">next</a><span>|</span><label class="collapse" for="c-42003207">[-]</label><label class="expand" for="c-42003207">[4 more]</label></div><br/><div class="children"><div class="content">the title says &quot;Cerebras Trains Llama Models&quot;...</div><br/><div id="42003359" class="c"><input type="checkbox" id="c-42003359" checked=""/><div class="controls bullet"><span class="by">mentalically</span><span>|</span><a href="#42002870">root</a><span>|</span><a href="#42003207">parent</a><span>|</span><a href="#42003656">next</a><span>|</span><label class="collapse" for="c-42003359">[-]</label><label class="expand" for="c-42003359">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s correct and if you read the whole thing you will realize that it is followed by &quot;... to leap over GPUs&quot; which indicates that they&#x27;re not literally referring to optimizing the weights of the graph on a new architecture or freshly initialized variables on existing ones.</div><br/><div id="42003580" class="c"><input type="checkbox" id="c-42003580" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42002870">root</a><span>|</span><a href="#42003359">parent</a><span>|</span><a href="#42003656">next</a><span>|</span><label class="collapse" for="c-42003580">[-]</label><label class="expand" for="c-42003580">[2 more]</label></div><br/><div class="children"><div class="content">This is as clickbaity as it gets.<p>Trains has no other sensible interpretation in the context of LLM models.  My impression was that they trained the models to be better than the models trained by GPUs, presumably because they trained faster and managed to train for longer than Meta, but this interpretation was far from the content.<p>Also interesting to see the ommission of deepinfra from the price table, presumably because it would be cheaper than Cerebras, though I didnt even bother to check at that point because I hate these cheap clickbaity pieces that attempt to enrich some player at the cost of everyone’s time or money.<p>Good luck with their IPO. We need competition but we dont need confusion.</div><br/><div id="42003619" class="c"><input type="checkbox" id="c-42003619" checked=""/><div class="controls bullet"><span class="by">mentalically</span><span>|</span><a href="#42002870">root</a><span>|</span><a href="#42003580">parent</a><span>|</span><a href="#42003656">next</a><span>|</span><label class="collapse" for="c-42003619">[-]</label><label class="expand" for="c-42003619">[1 more]</label></div><br/><div class="children"><div class="content">What are you confused about? Their value proposition is very simple and obvious, custom hardware with a compiler that transforms existing graphs into a format that can run at lower cost and higher efficiency because it utilizes a special instruction set only available on Cerebras silicon.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42003656" class="c"><input type="checkbox" id="c-42003656" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#42002870">prev</a><span>|</span><a href="#42002904">next</a><span>|</span><label class="collapse" for="c-42003656">[-]</label><label class="expand" for="c-42003656">[1 more]</label></div><br/><div class="children"><div class="content">Why is nobody mentioning that there is no such thing as Llama 3.2 70B</div><br/></div></div><div id="42002904" class="c"><input type="checkbox" id="c-42002904" checked=""/><div class="controls bullet"><span class="by">7e</span><span>|</span><a href="#42003656">prev</a><span>|</span><a href="#42003170">next</a><span>|</span><label class="collapse" for="c-42002904">[-]</label><label class="expand" for="c-42002904">[1 more]</label></div><br/><div class="children"><div class="content">&quot;It would be interesting to see what the delta in accuracy is for these benchmarks.&quot;<p>^ the entirety of it</div><br/></div></div><div id="42003170" class="c"><input type="checkbox" id="c-42003170" checked=""/><div class="controls bullet"><span class="by">asdf1145</span><span>|</span><a href="#42002904">prev</a><span>|</span><a href="#42002921">next</a><span>|</span><label class="collapse" for="c-42003170">[-]</label><label class="expand" for="c-42003170">[1 more]</label></div><br/><div class="children"><div class="content">did they release MLPerf data yet or wouldn&#x27;t help their IPO?</div><br/></div></div><div id="42002921" class="c"><input type="checkbox" id="c-42002921" checked=""/><div class="controls bullet"><span class="by">7e</span><span>|</span><a href="#42003170">prev</a><span>|</span><label class="collapse" for="c-42002921">[-]</label><label class="expand" for="c-42002921">[1 more]</label></div><br/><div class="children"><div class="content">&quot;So, the delta in price&#x2F;performance between Cerebras and the Hoppers in the cloud when buying iron is 2.75X but for renting iron it is 5.2X, which seems to imply that Cerebras is taking a pretty big haircut when it rents out capacity. That kind of delta between renting out capacity and selling it is not a business model, it is a loss leader from a startup trying to make a point.&quot;<p>As always, it is about TCO, not who can make the biggest monster chip.</div><br/></div></div></div></div></div></div></div></body></html>