<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721984466086" as="style"/><link rel="stylesheet" href="styles.css?v=1721984466086"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://gpuopen.com/download/publications/Real-Time_Procedural_Generation_with_GPU_Work_Graphs-GPUOpen_preprint.pdf">Real-Time Procedural Generation with GPU Work Graphs [pdf]</a> <span class="domain">(<a href="https://gpuopen.com">gpuopen.com</a>)</span></div><div class="subtext"><span>ibobev</span> | <span>14 comments</span></div><br/><div><div id="41075581" class="c"><input type="checkbox" id="c-41075581" checked=""/><div class="controls bullet"><span class="by">zellyn</span><span>|</span><a href="#41073619">next</a><span>|</span><label class="collapse" for="c-41075581">[-]</label><label class="expand" for="c-41075581">[1 more]</label></div><br/><div class="children"><div class="content">Figure 1… :slow clap:<p>For those who don&#x27;t want to click through:<p>(a) Without our system -- (empty courtyard)
(b) With our system -- (same picture but with procedural generated content added)</div><br/></div></div><div id="41073619" class="c"><input type="checkbox" id="c-41073619" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#41075581">prev</a><span>|</span><a href="#41073878">next</a><span>|</span><label class="collapse" for="c-41073619">[-]</label><label class="expand" for="c-41073619">[1 more]</label></div><br/><div class="children"><div class="content">Related: <a href="https:&#x2F;&#x2F;gpuopen.com&#x2F;learn&#x2F;work_graphs_mesh_nodes&#x2F;work_graphs_mesh_nodes-intro&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gpuopen.com&#x2F;learn&#x2F;work_graphs_mesh_nodes&#x2F;work_graphs...</a></div><br/></div></div><div id="41073878" class="c"><input type="checkbox" id="c-41073878" checked=""/><div class="controls bullet"><span class="by">lainga</span><span>|</span><a href="#41073619">prev</a><span>|</span><a href="#41074623">next</a><span>|</span><label class="collapse" for="c-41073878">[-]</label><label class="expand" for="c-41073878">[3 more]</label></div><br/><div class="children"><div class="content">Work graphs are in Vulkan (pg. 3)?! Since when? Where can I learn about this? Excitement</div><br/><div id="41073966" class="c"><input type="checkbox" id="c-41073966" checked=""/><div class="controls bullet"><span class="by">elabajaba</span><span>|</span><a href="#41073878">parent</a><span>|</span><a href="#41073927">next</a><span>|</span><label class="collapse" for="c-41073966">[-]</label><label class="expand" for="c-41073966">[1 more]</label></div><br/><div class="children"><div class="content">IIRC they&#x27;re an AMD extension (VK_AMDX_shader_enqueue) that only works with a specific beta driver currently.<p>I wouldn&#x27;t really say they&#x27;re usable in vulkan at this point.</div><br/></div></div><div id="41073927" class="c"><input type="checkbox" id="c-41073927" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#41073878">parent</a><span>|</span><a href="#41073966">prev</a><span>|</span><a href="#41074623">next</a><span>|</span><label class="collapse" for="c-41073927">[-]</label><label class="expand" for="c-41073927">[1 more]</label></div><br/><div class="children"><div class="content">I think Vulkan support is still AMD-only.<p><a href="https:&#x2F;&#x2F;gpuopen.com&#x2F;gpu-work-graphs-in-vulkan&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gpuopen.com&#x2F;gpu-work-graphs-in-vulkan&#x2F;</a></div><br/></div></div></div></div><div id="41074623" class="c"><input type="checkbox" id="c-41074623" checked=""/><div class="controls bullet"><span class="by">doug-moen</span><span>|</span><a href="#41073878">prev</a><span>|</span><a href="#41073979">next</a><span>|</span><label class="collapse" for="c-41074623">[-]</label><label class="expand" for="c-41074623">[6 more]</label></div><br/><div class="children"><div class="content">GPUs are Turing complete computers. Why is it necessary to add a GPU programming design pattern like this as an extension to Vulkan? Can I just implement this design pattern in an existing GPU programming language right now, without waiting for Vulkan extensions to be implemented for all the relevant GPU models? If this isn&#x27;t possible right now, then what&#x27;s missing from existing GPU languages that prevents this kind of flexibility?</div><br/><div id="41074928" class="c"><input type="checkbox" id="c-41074928" checked=""/><div class="controls bullet"><span class="by">zeusk</span><span>|</span><a href="#41074623">parent</a><span>|</span><a href="#41075018">next</a><span>|</span><label class="collapse" for="c-41074928">[-]</label><label class="expand" for="c-41074928">[1 more]</label></div><br/><div class="children"><div class="content">Because every GPU on the market has a very different internal organization and instruction set.<p>They are able to execute any shader using standardized programming constructs (Vulkan, DirectX, Metal) that both the OS and driver understand. While the API and OS manage device&#x2F;app context, the driver manages the device itself and the job of compiling shaders or standard calls to their GPUs specific instructions or layout.</div><br/></div></div><div id="41075018" class="c"><input type="checkbox" id="c-41075018" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#41074623">parent</a><span>|</span><a href="#41074928">prev</a><span>|</span><a href="#41074687">next</a><span>|</span><label class="collapse" for="c-41075018">[-]</label><label class="expand" for="c-41075018">[1 more]</label></div><br/><div class="children"><div class="content">They are Turing complete in the context of a single thread. But, a single thread does not control the entire GPU. For example: In multi-vendor APIs, a thread cannot spawn another thread.<p>The execution model of GPUs is complex and only specified at the high level in order to give manufacturers freedom in their implementations. Work Graphs are an extension of the execution model.</div><br/></div></div><div id="41074687" class="c"><input type="checkbox" id="c-41074687" checked=""/><div class="controls bullet"><span class="by">ttoinou</span><span>|</span><a href="#41074623">parent</a><span>|</span><a href="#41075018">prev</a><span>|</span><a href="#41076485">next</a><span>|</span><label class="collapse" for="c-41074687">[-]</label><label class="expand" for="c-41074687">[2 more]</label></div><br/><div class="children"><div class="content">I’m wondering the same. Maybe it’d be easier if we could upload our own ‘sequential program’ to a little CPU inside the GPU that would be code making calls to the real GPU. This way delays to native GPU code would be minimized and we wouldn’t need to mimick creating new features &#x2F; GPU paradigms</div><br/><div id="41075632" class="c"><input type="checkbox" id="c-41075632" checked=""/><div class="controls bullet"><span class="by">01HNNWZ0MV43FF</span><span>|</span><a href="#41074623">root</a><span>|</span><a href="#41074687">parent</a><span>|</span><a href="#41076485">next</a><span>|</span><label class="collapse" for="c-41075632">[-]</label><label class="expand" for="c-41075632">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t hardly wait for GPGPU wasm</div><br/></div></div></div></div><div id="41076485" class="c"><input type="checkbox" id="c-41076485" checked=""/><div class="controls bullet"><span class="by">jms55</span><span>|</span><a href="#41074623">parent</a><span>|</span><a href="#41074687">prev</a><span>|</span><a href="#41073979">next</a><span>|</span><label class="collapse" for="c-41076485">[-]</label><label class="expand" for="c-41076485">[1 more]</label></div><br/><div class="children"><div class="content">GPUs are setup as large amounts of SIMD blocks of threads with some shared units like registers, cache, ALU, etc for every few blocks of threads.<p>The typical way you schedule GPU work is to dispatch a very large amount of work all running the same program. So you&#x27;d spawn 10 million units of work, to run on 10 thousand thread blocks. Due to differing memory accesses per unit of work, each threadblock will complete their work at a different time. Whenever a threadblock is stuck waiting for memory accesses to complete, or has finished it&#x27;s work, the GPU scheduler gives it a new unit of work to work on.<p>If you graph &quot;occupancy&quot; as the percentage of threadblocks busy doing work, then you&#x27;d see a spin-up period as threadblocks are filled with work, a steady period where all threadblocks are busy, and then a spin-down period as there&#x27;s gradually less work available than the number of threadblocks.<p>If you wanted to run two programs (e.g., check meshes to determine what&#x27;s visible and cull invisible meshes, then draw the remaining visible meshes), then you&#x27;d have a hard gap in between. Threadblocks would spin-up with culling work, work steadily, spin-down until 0 work is left, spin-up with mesh drawing work, work steadily, and then spin-down again. The spin-down period in between the two passes is bad for performance.<p>Rather than having the GPU go completely idle, wouldn&#x27;t it be better if, as the GPU is running out of culling work to execute (available work is less than the number of threadblocks available to perform work), you could fill in the gaps by immediately starting the mesh drawing work for meshes that have already passed culling? That way there would be no idle time between passes. Workgraphs let you do this, by specifying execution not as monolithic passes, but as nodes that perform 1 unit of work and produce output for other nodes to consume.<p>Another benefit is memory allocation required. With the two distinct passes model, you need to allocate the worse-case amount of memory to hold the first pass output (input to the second pass). If you have 10,000 meshes, then you need to allocate space to be able to draw 10,000 meshes for the worst case that they&#x27;re all visible - there is no runtime memory allocation on the GPU. With workgraphs, the GPU can allocate a reasonable estimate of how much memory it needs for node 1&#x27;s output (input for node 2), and if the output buffer is full, the GPU can simply stop scheduling node 1 work, and start scheduling node 2 work to pop from the buffer and free up space.<p>As for whether you can do this with the existing GPU pass-based model, more or less yeah. You can build your own queue and use global atomics to control producer&#x2F;consumer synchronization. It&#x27;s called persistent threads. You might even do better than the GPU&#x27;s built-in scheduling depending on the task, if you hyper-optimize and tune your code. However, it&#x27;s a lot harder, and dependent on specific implicit behavior of the GPU&#x27;s scheduler. If the GPU is not smart enough to sleep threadblocks waiting for the atomic &quot;lock&quot; and schedule threadblocks that are holding the lock, then you get a deadlock and your computer freezes until the GPU driver kills your program.<p>The big reason workgraphs are getting introduced is for Unreal Engine 5&#x27;s Nanite renderer that came out a few years ago. It uses the persistent threads technique to traverse a BVH, doing cull checks on each node, with passing nodes getting their children pushed onto the queue. BVHs (trees) can be unbalanced, so Nanite uses the persistent threads technique to dynamically load-balance nodes amongst the available threadblocks. With workgroups, Nanite could instead define it as a graph, with culling nodes outputting work for more culling nodes, and have the driver handle the load balancing.</div><br/></div></div></div></div><div id="41073979" class="c"><input type="checkbox" id="c-41073979" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#41074623">prev</a><span>|</span><label class="collapse" for="c-41073979">[-]</label><label class="expand" for="c-41073979">[2 more]</label></div><br/><div class="children"><div class="content">Does this work on Metal</div><br/><div id="41075858" class="c"><input type="checkbox" id="c-41075858" checked=""/><div class="controls bullet"><span class="by">dagmx</span><span>|</span><a href="#41073979">parent</a><span>|</span><label class="collapse" for="c-41075858">[-]</label><label class="expand" for="c-41075858">[1 more]</label></div><br/><div class="children"><div class="content">Metal’s closest equivalent is Indirect Command Buffers.<p><a href="https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;metal&#x2F;indirect_command_encoding&#x2F;encoding_indirect_command_buffers_on_the_gpu" rel="nofollow">https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;metal&#x2F;indirect_com...</a><p>However, there is no direct equivalent to work graphs. This DirectX blog goes over the differences in DirectX’s equivalent to indirect calls and work graphs better<p><a href="https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;directx&#x2F;d3d12-work-graphs&#x2F;" rel="nofollow">https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;directx&#x2F;d3d12-work-graphs&#x2F;</a><p>That said, a lot of what the AMD paper talks about can be done without work graphs, just with more care required.</div><br/></div></div></div></div></div></div></div></div></div></body></html>