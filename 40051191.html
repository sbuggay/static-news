<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713344471025" as="style"/><link rel="stylesheet" href="styles.css?v=1713344471025"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://aliramadhan.me/2024/03/31/trillion-rows.html">Loading a trillion rows of weather data into TimescaleDB</a>Â <span class="domain">(<a href="https://aliramadhan.me">aliramadhan.me</a>)</span></div><div class="subtext"><span>PolarizedPoutin</span> | <span>110 comments</span></div><br/><div><div id="40062095" class="c"><input type="checkbox" id="c-40062095" checked=""/><div class="controls bullet"><span class="by">gingerwizard</span><span>|</span><a href="#40052172">next</a><span>|</span><label class="collapse" for="c-40062095">[-]</label><label class="expand" for="c-40062095">[1 more]</label></div><br/><div class="children"><div class="content">Few in this thread have suggested ClickHouse would do well here. We tested 1 trillon rows recently, albeit much simpler data - <a href="https:&#x2F;&#x2F;clickhouse.com&#x2F;blog&#x2F;clickhouse-1-trillion-row-challenge" rel="nofollow">https:&#x2F;&#x2F;clickhouse.com&#x2F;blog&#x2F;clickhouse-1-trillion-row-challe...</a><p>This is a good dataset though and the level of detail in the post is appreciated. I&#x27;ll give ClicKHouse a go on the same...<p>Disclaimer: I work for ClickHouse</div><br/></div></div><div id="40052172" class="c"><input type="checkbox" id="c-40052172" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#40062095">prev</a><span>|</span><a href="#40054707">next</a><span>|</span><label class="collapse" for="c-40052172">[-]</label><label class="expand" for="c-40052172">[30 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve done a good amount of geospatial analysis for work.<p>One thing you quickly realize with geospatial data is that it&#x27;s incredibly nuanced. You have to be quite careful about understanding which coordinate reference system (CRS) and for visualization which projection is being used. The CRS is somewhat paranoia inducing if you don&#x27;t have great infrastructure setup with the right tools to carry that metadata with your geospatial data.<p>I&#x27;ve tested everything AWS has to offer, Postgres&#x2F;PostGIS, Spark&#x2F;DB, Snowflake, Trini, and ARCGis.<p>I&#x27;m convinced the best tool for large scale geospatial work is Google BigQuery and it&#x27;s not even close. It took an expensive multi hour query running on PostGIS deployed on an enormous m6a EC2 instance to less than 5 seconds that ran in the BigQuery free tier. It does make sense if you think about it, Google was early with Maps, but it is just stunning how much better they are in this specific niche domain.<p>This was using publicly available FEMA data that Snowflake and AWS services would just choke on, because the geometry column exceeded their maximum byte size. Spark doesn&#x27;t even have geospatial data types and the open source packages that add support leave a lot to be desired.<p>This guy is running on-prem, so maybe this made sense, but I just would never bother. The storage for BQ would probably be less than $100&#x2F;months for 20 TB.</div><br/><div id="40052567" class="c"><input type="checkbox" id="c-40052567" checked=""/><div class="controls bullet"><span class="by">ingenieroariel</span><span>|</span><a href="#40052172">parent</a><span>|</span><a href="#40053765">next</a><span>|</span><label class="collapse" for="c-40052567">[-]</label><label class="expand" for="c-40052567">[10 more]</label></div><br/><div class="children"><div class="content">I went through a similar phase with a process that started with global OSM and Whosonfirst to process a pipeline. Google costs kept going up (7k a month with airflow + bigquery) and I was able to replace it with a one time $7k hardware purchase. We were able to do it since the process was using H3 indices early on and the resulting intermediate datasets all fit on ram.<p>System is a Mac Studio with 128GB + Asahi Linux + mmapped parquet files and DuckDB, it also runs airflow for us and with Nix can be used to accelerate developer builds and run the airflow tasks for the data team.<p>GCP is nice when it is free&#x2F;cheap but they keep tabs on what you are doing and may surprise you at any point in time with ever higher bills without higher usage.</div><br/><div id="40059372" class="c"><input type="checkbox" id="c-40059372" checked=""/><div class="controls bullet"><span class="by">nojvek</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40052567">parent</a><span>|</span><a href="#40053699">next</a><span>|</span><label class="collapse" for="c-40059372">[-]</label><label class="expand" for="c-40059372">[1 more]</label></div><br/><div class="children"><div class="content">DuckDB is the real magic. On an nvme disk with decent amount of RAM, it goes brrrrrr.<p>I would love it if somehow Postgres got duckdb powered columnstore tables.<p>I know hydra.so is doing columnstores.<p>DuckDB being able to query parquet files directly is a big win IMO.<p>I wish we could bulk insert parquet files into stock PG.</div><br/></div></div><div id="40053699" class="c"><input type="checkbox" id="c-40053699" checked=""/><div class="controls bullet"><span class="by">jfim</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40052567">parent</a><span>|</span><a href="#40059372">prev</a><span>|</span><a href="#40053804">next</a><span>|</span><label class="collapse" for="c-40053699">[-]</label><label class="expand" for="c-40053699">[5 more]</label></div><br/><div class="children"><div class="content">BigQuery is nice but it&#x27;s definitely a major foot-gun in terms of cost. It&#x27;s surprisingly easy to rack up high costs with say a misconfigured dashboard or a developer just testing stuff.</div><br/><div id="40061025" class="c"><input type="checkbox" id="c-40061025" checked=""/><div class="controls bullet"><span class="by">mrgaro</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40053699">parent</a><span>|</span><a href="#40054257">next</a><span>|</span><label class="collapse" for="c-40061025">[-]</label><label class="expand" for="c-40061025">[1 more]</label></div><br/><div class="children"><div class="content">Definitively agree here. Once the data is in BigQuery, people will start doing ad-hoc queries and building Grafana dashboards on top of it.<p>And sooner or later (usually sooner) somebody will build a fancy Grafana dashboard and set it to refresh every 5 second and you will not notice it until it&#x27;s too late.</div><br/></div></div><div id="40054257" class="c"><input type="checkbox" id="c-40054257" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40053699">parent</a><span>|</span><a href="#40061025">prev</a><span>|</span><a href="#40053804">next</a><span>|</span><label class="collapse" for="c-40054257">[-]</label><label class="expand" for="c-40054257">[3 more]</label></div><br/><div class="children"><div class="content">Frankly I think this is just a sign that it&#x27;s a power tool for power users.</div><br/><div id="40054431" class="c"><input type="checkbox" id="c-40054431" checked=""/><div class="controls bullet"><span class="by">carlhjerpe</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40054257">parent</a><span>|</span><a href="#40053804">next</a><span>|</span><label class="collapse" for="c-40054431">[-]</label><label class="expand" for="c-40054431">[2 more]</label></div><br/><div class="children"><div class="content">Sadly my colleagues aren&#x27;t always &quot;power users&quot;</div><br/><div id="40058061" class="c"><input type="checkbox" id="c-40058061" checked=""/><div class="controls bullet"><span class="by">brailsafe</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40054431">parent</a><span>|</span><a href="#40053804">next</a><span>|</span><label class="collapse" for="c-40058061">[-]</label><label class="expand" for="c-40058061">[1 more]</label></div><br/><div class="children"><div class="content">Nobody starts as a power user</div><br/></div></div></div></div></div></div></div></div><div id="40053804" class="c"><input type="checkbox" id="c-40053804" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40052567">parent</a><span>|</span><a href="#40053699">prev</a><span>|</span><a href="#40055216">next</a><span>|</span><label class="collapse" for="c-40053804">[-]</label><label class="expand" for="c-40053804">[1 more]</label></div><br/><div class="children"><div class="content">That is a very cool setup!<p>My org would never allow that as we&#x27;re in a highly regulated and security conscious space.<p>Totally agree about the BQ costs. The free tier is great and I think pretty generous, but if you&#x27;re not very careful with enforcing table creation only with partitioning and clustering as much as possible, and don&#x27;t enforce some training for devs on how to deal with columnar DB&#x27;s if they&#x27;re not familiar, the bills can get pretty crazy quickly.</div><br/></div></div><div id="40055216" class="c"><input type="checkbox" id="c-40055216" checked=""/><div class="controls bullet"><span class="by">hawk_</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40052567">parent</a><span>|</span><a href="#40053804">prev</a><span>|</span><a href="#40053765">next</a><span>|</span><label class="collapse" for="c-40055216">[-]</label><label class="expand" for="c-40055216">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  and may surprise you at any point in time with ever higher bills without higher usage.<p>What? really? Do they change your pricing plan? How can they charge more for the same usage?</div><br/><div id="40058396" class="c"><input type="checkbox" id="c-40058396" checked=""/><div class="controls bullet"><span class="by">ingenieroariel</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40055216">parent</a><span>|</span><a href="#40053765">next</a><span>|</span><label class="collapse" for="c-40058396">[-]</label><label class="expand" for="c-40058396">[1 more]</label></div><br/><div class="children"><div class="content">When you queried their &#x27;Open Data&#x27; datasets and linked with your own it was absurdly cheap for some time. Granted we used our hacking skills to make sure the really big queries ran in the free tier and only smaller datasets got in the private tables.<p>I kept getting emails about small changes and the bills got bigger all over the place including BigQuery and how they dealt with queries on public datasets. Bill got higher.<p>There is a non zero chance I conflated things. But from my point of view: I created a system and let it running for years - afterwards bills got higher out of the blue and I moved out.</div><br/></div></div></div></div></div></div><div id="40053765" class="c"><input type="checkbox" id="c-40053765" checked=""/><div class="controls bullet"><span class="by">johnymontana</span><span>|</span><a href="#40052172">parent</a><span>|</span><a href="#40052567">prev</a><span>|</span><a href="#40052758">next</a><span>|</span><label class="collapse" for="c-40053765">[-]</label><label class="expand" for="c-40053765">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Spark doesn&#x27;t even have geospatial data types and the open source packages that add support leave a lot to be desired.<p>Could you say more about this? I&#x27;m curious if you&#x27;ve compared Apache Sedona [0] and what specifically you found lacking? I currently work at Wherobots [1], founded by the creators of Apache Sedona and would love to hear any feedback.<p>[0] <a href="https:&#x2F;&#x2F;sedona.apache.org&#x2F;latest&#x2F;" rel="nofollow">https:&#x2F;&#x2F;sedona.apache.org&#x2F;latest&#x2F;</a><p>[1] <a href="https:&#x2F;&#x2F;wherobots.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wherobots.com&#x2F;</a></div><br/></div></div><div id="40052758" class="c"><input type="checkbox" id="c-40052758" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#40052172">parent</a><span>|</span><a href="#40053765">prev</a><span>|</span><a href="#40052193">next</a><span>|</span><label class="collapse" for="c-40052758">[-]</label><label class="expand" for="c-40052758">[4 more]</label></div><br/><div class="children"><div class="content">Do you mind linking the specific dataset? I agree very wide columns break a lot of tools but other columnar postgres forks should support this no problem. It sounds like you didn&#x27;t use Redshift, which I find surprising as it directly competes with BQ. Redshift has &quot;super&quot; columns that can be very large, even larger than the maximum supported by BigQuery.<p>I constantly see folks finding out the hard way that PostGIS is <i>really</i> hard to beat. The fact that Trini&#x2F;Presto and Spark have languished here is particularly telling.</div><br/><div id="40053694" class="c"><input type="checkbox" id="c-40053694" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40052758">parent</a><span>|</span><a href="#40052193">next</a><span>|</span><label class="collapse" for="c-40053694">[-]</label><label class="expand" for="c-40053694">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s FEMA&#x27;s NFHL. I can&#x27;t recall the specific layer of the GDB file, but you could probably figure it out. Try loading up Iowa into redshift and if that works for you I&#x27;d be quite surprised.<p>My org has a very large AWS spend and we got to have a chat with some of their SWE&#x27;s that work on the geospatial processing features for Redshift and Athena. We described what we needed and they said our only option was to aggregate the data first or drop the offending rows. Obviously we&#x27;re not interested in compromising our work just to use a specific tool, so we opted for better tools.<p>The crux of the issue was that the large problem column was the geometry itself. Specifically, MultiPolygon. You need to use the geometry datatype for this[1]. However, our MultiPolygon column was 10&#x27;s to 100&#x27;s of MB&#x27;s. Well outside the max size for the Super datatype from what I can tell as it looks like that&#x27;s 16 MB.<p>[1]: <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;redshift&#x2F;latest&#x2F;dg&#x2F;GeometryType-function.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;redshift&#x2F;latest&#x2F;dg&#x2F;GeometryType-...</a></div><br/><div id="40055990" class="c"><input type="checkbox" id="c-40055990" checked=""/><div class="controls bullet"><span class="by">beeboobaa3</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40053694">parent</a><span>|</span><a href="#40059179">next</a><span>|</span><label class="collapse" for="c-40055990">[-]</label><label class="expand" for="c-40055990">[1 more]</label></div><br/><div class="children"><div class="content">Just split it out into multiple polygons, one per row. If you&#x27;re using a relational database, do as relational databases do.</div><br/></div></div><div id="40059179" class="c"><input type="checkbox" id="c-40059179" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40053694">parent</a><span>|</span><a href="#40055990">prev</a><span>|</span><a href="#40052193">next</a><span>|</span><label class="collapse" for="c-40059179">[-]</label><label class="expand" for="c-40059179">[1 more]</label></div><br/><div class="children"><div class="content">The description you provide appears too large for even BigQuery.</div><br/></div></div></div></div></div></div><div id="40052193" class="c"><input type="checkbox" id="c-40052193" checked=""/><div class="controls bullet"><span class="by">Cthulhu_</span><span>|</span><a href="#40052172">parent</a><span>|</span><a href="#40052758">prev</a><span>|</span><a href="#40053542">next</a><span>|</span><label class="collapse" for="c-40052193">[-]</label><label class="expand" for="c-40052193">[1 more]</label></div><br/><div class="children"><div class="content">I hear so much good things about BigTable &#x2F; BigQuery, it&#x27;s a shame I&#x27;ve had no opportunity to use it yet.</div><br/></div></div><div id="40053542" class="c"><input type="checkbox" id="c-40053542" checked=""/><div class="controls bullet"><span class="by">detourdog</span><span>|</span><a href="#40052172">parent</a><span>|</span><a href="#40052193">prev</a><span>|</span><a href="#40053563">next</a><span>|</span><label class="collapse" for="c-40053542">[-]</label><label class="expand" for="c-40053542">[2 more]</label></div><br/><div class="children"><div class="content">Iâm glad to hear this first hand experience. Iâm pretty sure that want to build and refine my own geospatial data horde.<p>I wonder if you think a longterm project is better rolling their own. The second priority is that I expect it all to be locally hosted.<p>Thanks for any considerations.</div><br/><div id="40053767" class="c"><input type="checkbox" id="c-40053767" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40053542">parent</a><span>|</span><a href="#40053563">next</a><span>|</span><label class="collapse" for="c-40053767">[-]</label><label class="expand" for="c-40053767">[1 more]</label></div><br/><div class="children"><div class="content">Frankly I think for long term hoarding BQ is hard to beat. The storage costs are pretty reasonable and you never pay for compute until you actually run a query, so if you&#x27;re mostly just hoarding, well, you&#x27;re probably going to save a lot of time, money, and effort in the long run.</div><br/></div></div></div></div><div id="40053563" class="c"><input type="checkbox" id="c-40053563" checked=""/><div class="controls bullet"><span class="by">rtkwe</span><span>|</span><a href="#40052172">parent</a><span>|</span><a href="#40053542">prev</a><span>|</span><a href="#40052738">next</a><span>|</span><label class="collapse" for="c-40053563">[-]</label><label class="expand" for="c-40053563">[1 more]</label></div><br/><div class="children"><div class="content">Even more fun I bet would be if you&#x27;re getting data with different reference spheroids.</div><br/></div></div><div id="40052738" class="c"><input type="checkbox" id="c-40052738" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40052172">parent</a><span>|</span><a href="#40053563">prev</a><span>|</span><a href="#40053514">next</a><span>|</span><label class="collapse" for="c-40052738">[-]</label><label class="expand" for="c-40052738">[2 more]</label></div><br/><div class="children"><div class="content">Thank you for the insights! Yeah I&#x27;m still not sure how Postgres&#x2F;PostGIS will scale for me, but good to know that BigQuery does this nicely.<p>This is not something I&#x27;m productionizing (at least not yet?) and I&#x27;m giving myself zero budget since it&#x27;s a side project, but if&#x2F;when I do I&#x27;ll definitely look into BigQuery!</div><br/><div id="40053742" class="c"><input type="checkbox" id="c-40053742" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40052738">parent</a><span>|</span><a href="#40053514">next</a><span>|</span><label class="collapse" for="c-40053742">[-]</label><label class="expand" for="c-40053742">[1 more]</label></div><br/><div class="children"><div class="content">Good luck! They have some really great tutorials on how to get started with BQ and geospatial data. One other nuance of BigQuery that doesn&#x27;t seem to apply to many other tools in this space is that you can enable partitioning on your tables in addition to clustering on the geometry (Geography in BQ) column.<p><a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;bigquery&#x2F;docs&#x2F;geospatial-data#partitioning_and_clustering_geospatial_data" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;bigquery&#x2F;docs&#x2F;geospatial-data#parti...</a></div><br/></div></div></div></div><div id="40053514" class="c"><input type="checkbox" id="c-40053514" checked=""/><div class="controls bullet"><span class="by">winrid</span><span>|</span><a href="#40052172">parent</a><span>|</span><a href="#40052738">prev</a><span>|</span><a href="#40054707">next</a><span>|</span><label class="collapse" for="c-40053514">[-]</label><label class="expand" for="c-40053514">[8 more]</label></div><br/><div class="children"><div class="content">M6a is not even remotely enormous. Also were you using EBS?</div><br/><div id="40053613" class="c"><input type="checkbox" id="c-40053613" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40053514">parent</a><span>|</span><a href="#40054707">next</a><span>|</span><label class="collapse" for="c-40053613">[-]</label><label class="expand" for="c-40053613">[7 more]</label></div><br/><div class="children"><div class="content">m6a.48xlarge: 192 vCPU &amp; 768 GiB<p>If that&#x27;s not a large node for you, well you&#x27;re living in a different world from me. Yes to using EBS.</div><br/><div id="40054370" class="c"><input type="checkbox" id="c-40054370" checked=""/><div class="controls bullet"><span class="by">winrid</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40053613">parent</a><span>|</span><a href="#40054755">next</a><span>|</span><label class="collapse" for="c-40054370">[-]</label><label class="expand" for="c-40054370">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the info. The issue is using EBS. If you used an instance with NVME drives it would probably have been faster than BQ (and you aren&#x27;t billed per-query...). I would suggest an R or I4 class instance for this, m6 is not good for the money here.<p>You would just have to setup replication for backups, but this could just be rsync to EBS or some other replication solution depending on your database.</div><br/></div></div><div id="40054755" class="c"><input type="checkbox" id="c-40054755" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40053613">parent</a><span>|</span><a href="#40054370">prev</a><span>|</span><a href="#40054707">next</a><span>|</span><label class="collapse" for="c-40054755">[-]</label><label class="expand" for="c-40054755">[5 more]</label></div><br/><div class="children"><div class="content">Unless youâre CPU-bound (unlikely), the r-family is usually a better fit for RDBMS, IME. 8x RAM:vCPU ratio, compared to 4x for m-family.<p>Then thereâs the x-family, which can go up to bonkers levels, like 4 TiB of RAM (and local NVMe).<p>As a sibling comment mentioned, though, if you can fit the data into local storage, thatâs the way to do it. EBS latency, even with io2.blockexpress, simply cannot compete.<p>That said, if I did the math correctly based on the 71 GB&#x2F;31 days footnote, youâre looking at about 2.15 PB to load the entire thing, so, uh, good luck.</div><br/><div id="40056223" class="c"><input type="checkbox" id="c-40056223" checked=""/><div class="controls bullet"><span class="by">ayewo</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40054755">parent</a><span>|</span><a href="#40054707">next</a><span>|</span><label class="collapse" for="c-40056223">[-]</label><label class="expand" for="c-40056223">[4 more]</label></div><br/><div class="children"><div class="content">In your experience, is the r-family better than the c-family for running an RDBMS?</div><br/><div id="40057693" class="c"><input type="checkbox" id="c-40057693" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40056223">parent</a><span>|</span><a href="#40056796">next</a><span>|</span><label class="collapse" for="c-40057693">[-]</label><label class="expand" for="c-40057693">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Most RDBMS are memory-bound (indexes, connection overhead, buffer poolâ¦), so the more, the better.<p>At my last job, I switched a large-ish (100K QPS at peak) m5 MySQL instance to a smaller (in AWS numbering) r6i that was cheaper, despite having the same amount of RAM and being  generation newer. That, combined with careful tuning and testing, resulted in queries speeding up 20-40%, AND we then had plenty of room for vertical scaling again if necessary.</div><br/></div></div><div id="40056796" class="c"><input type="checkbox" id="c-40056796" checked=""/><div class="controls bullet"><span class="by">aPoCoMiLogin</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40056223">parent</a><span>|</span><a href="#40057693">prev</a><span>|</span><a href="#40054707">next</a><span>|</span><label class="collapse" for="c-40056796">[-]</label><label class="expand" for="c-40056796">[2 more]</label></div><br/><div class="children"><div class="content">things are changing recently in aws, but few years ago R-family instances had one of the fastest uplink to EBS. for example only the larges M4 instance (m4.16xlarge) has 10gbps uplink, versus R5b where it starts from 10gbps (for the lowest tier) and ends on 60gbps @ 260k IOPS. you can very easily choke EBS with DB.<p>EDIT: only newer C instances have comparable uplink to EBS, C5 or C4 (and some C6) starts from ~4.7gbps. just compare the EBS bandwidth column in <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;</a></div><br/><div id="40057720" class="c"><input type="checkbox" id="c-40057720" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#40052172">root</a><span>|</span><a href="#40056796">parent</a><span>|</span><a href="#40054707">next</a><span>|</span><label class="collapse" for="c-40057720">[-]</label><label class="expand" for="c-40057720">[1 more]</label></div><br/><div class="children"><div class="content">The other fun thing about AWS instances is that the network uplink speed isnât always whatâs advertised. There is of course the âUp to X Gbpsâ levels (30 minutes of rated speed guaranteed every 24 hours), but there are also other limits, like cross-region speed being capped at 50% of rated capacity.<p>This rarely matters until it does, like if youâre setting up an active-active DB across regions. Then itâs a fun surprise.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40054707" class="c"><input type="checkbox" id="c-40054707" checked=""/><div class="controls bullet"><span class="by">jamesgresql</span><span>|</span><a href="#40052172">prev</a><span>|</span><a href="#40055220">next</a><span>|</span><label class="collapse" for="c-40054707">[-]</label><label class="expand" for="c-40054707">[2 more]</label></div><br/><div class="children"><div class="content">This is super cool! I run DevlRel @ Timescale, and I love seeing our community create well written posts like this!<p>My initial reaction is that I think one of the reasons you&#x27;re seeing a hypertable being slower is almost certainly that it creates an index on the timestamp column by default. You don&#x27;t have an index on your standard table which lets it go faster.<p>You can use create_hypertable with create_default_indexes=&gt;false to skip creating the index, or you can just drop the index before you ingest data. You&#x27;ll eventually want that index - but it&#x27;s best created after ingestion in a one-shot load like this.<p>I&#x27;d also be interested in how the HDD you&#x27;re reading data from is holding up in some of the highly parallel setups?</div><br/><div id="40057555" class="c"><input type="checkbox" id="c-40057555" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40054707">parent</a><span>|</span><a href="#40055220">next</a><span>|</span><label class="collapse" for="c-40057555">[-]</label><label class="expand" for="c-40057555">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for reading and for your kind words!<p>Ah I did not know about the `create_default_indexes=&gt;false` and that a time index is created by default for hypertables. I&#x27;ll add a note to explain this! Also curious to benchmark inserting without the time index then creating it manually.<p>Even with 32 workers I think the HDD was fine. I did monitor disk usage through btop and the SSD that Postgres lived on seemed to be more of a bottleneck than the HDD. So my conclusion was that a faster SSD for Postgres would be a better investment than moving the data from HDD to SSD.</div><br/></div></div></div></div><div id="40055220" class="c"><input type="checkbox" id="c-40055220" checked=""/><div class="controls bullet"><span class="by">rabernat</span><span>|</span><a href="#40054707">prev</a><span>|</span><a href="#40055213">next</a><span>|</span><label class="collapse" for="c-40055220">[-]</label><label class="expand" for="c-40055220">[4 more]</label></div><br/><div class="children"><div class="content">Great post! Hi Ali!<p>I think what&#x27;s missing here is an analysis of what is gained by moving the weather data into a RDBMS. The motivation is to speed up queries. But what&#x27;s the baseline?<p>As someone very familiar with this tech landscape (maintainer of Xarray and Zarr, founder of <a href="https:&#x2F;&#x2F;earthmover.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;earthmover.io&#x2F;</a>),  I know that serverless solutions + object storage can deliver very low latency performance (sub second) for timeseries queries on weather data--much faster than the 30 minutes cited here--_if_ the data are chunked appropriately in Zarr. Given the difficulty of data ingestion described in this post, it&#x27;s worth seriously evaluating those solutions before going down the RDBMS path.</div><br/><div id="40057760" class="c"><input type="checkbox" id="c-40057760" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40055220">parent</a><span>|</span><a href="#40056187">next</a><span>|</span><label class="collapse" for="c-40057760">[-]</label><label class="expand" for="c-40057760">[1 more]</label></div><br/><div class="children"><div class="content">Hey Ryan and thank you for the feedback!<p>I agree that storing the data is appropriately chunked Zarr files is almost surely going to be faster, simpler to set up, and take up less space. Could even put up an API in front of it to get &quot;queries&quot;.<p>I also agree that I haven&#x27;t motivated the RDBMS approach much. This is mainly because I took this approach with Postgres + Timescale since I wanted to learn to work with them, and playing around with ERA5 data seemed like the most fun way. Maybe it&#x27;s the allure of weather data being big enough to pose a challenge here.<p>I don&#x27;t have anything to back this up but I wonder if the RDBMS approach, with properly tuned and indexed TimescaleDB + PostGIS (non-trivial to set up), can speed up complex spatio-temporal queries, e.g. computing the 99th percentile of summer temperatures in Chile from 1940-1980, in case many different Zarr chunks have to be read to find this data. I like the idea of setting up different tables to cache these kinds of statistics, but it&#x27;s not that hard to do with Zarr either.<p>I&#x27;m benchmarking queries and indexes next so I might know more then!</div><br/></div></div><div id="40056187" class="c"><input type="checkbox" id="c-40056187" checked=""/><div class="controls bullet"><span class="by">ohmahjong</span><span>|</span><a href="#40055220">parent</a><span>|</span><a href="#40057760">prev</a><span>|</span><a href="#40055213">next</a><span>|</span><label class="collapse" for="c-40056187">[-]</label><label class="expand" for="c-40056187">[2 more]</label></div><br/><div class="children"><div class="content">This is a bit off-topic but I&#x27;m interested in the same space you are in.<p>There seems to be an inherent pull between large chunks (great for visualising large extents and larger queries) vs smaller chunks for point-based or timeseries queries. It&#x27;s possible but not very cost-effective to maintain separately-chunked versions of these large geospatial datasets. I have heard of &quot;kerchunk&quot; being used to try and get the best of both, but then I _think_ you lose out on the option of compressing the data and it introduces quite a lot of complexity.<p>What are your thoughts on how to strike that balance between use cases?</div><br/><div id="40056515" class="c"><input type="checkbox" id="c-40056515" checked=""/><div class="controls bullet"><span class="by">rabernat</span><span>|</span><a href="#40055220">root</a><span>|</span><a href="#40056187">parent</a><span>|</span><a href="#40055213">next</a><span>|</span><label class="collapse" for="c-40056515">[-]</label><label class="expand" for="c-40056515">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s possible but not very cost-effective to maintain separately-chunked versions of these large geospatial datasets.<p>Like all things in tech, it&#x27;s about tradeoffs. S3 storage costs about $275 TB a year. Typical weather datasets are ~10 TB. If you&#x27;re running a business that uses weather data in operations to make money, you could easily afford to make 2-3 copies that are optimized for different query patterns. We see many teams doing this today in production. That&#x27;s still much cheaper (and more flexible) than putting the same volume of data in a RDBMS, given the relative cost of S3 vs. persistent disks.<p>The real hidden costs of all of these solutions is the developer time operating the data pipelines for the transformation.</div><br/></div></div></div></div></div></div><div id="40055213" class="c"><input type="checkbox" id="c-40055213" checked=""/><div class="controls bullet"><span class="by">counters</span><span>|</span><a href="#40055220">prev</a><span>|</span><a href="#40052450">next</a><span>|</span><label class="collapse" for="c-40055213">[-]</label><label class="expand" for="c-40055213">[5 more]</label></div><br/><div class="children"><div class="content">Why?<p>Most weather and climate datasets - including ERA5 - are highly structured on regular latitude-longitude grids. Even if you were solely doing timeseries analyses for specific locations plucked from this grid, the strength of this sort of dataset is its intrinsic spatiotemporal structure and context, and it makes very little sense to completely destroy the dataset&#x27;s structure unless you were solely and exclusively to extract point timeseries. And even then, you&#x27;d probably want to decimate the data pretty dramatically, since there is very little use case for, say, a point timeseries of surface temperature in the middle of the ocean!<p>The vast majority of research and operational applications of datasets like ERA5 are probably better suited by leveraging cloud-optimized replicas of the original dataset, such as ARCO-ERA5 published on the Google Public Datasets program [1]. These versions of the dataset preserve the original structure, and chunk it in ways that are amenable to massively parallel access via cloud storage. In almost any case I&#x27;ve encountered in my career, a generically chunked Zarr-based archive of a dataset like this will be more than performant enough for the majority of use cases that one might care about.<p>[1]: <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;storage&#x2F;docs&#x2F;public-datasets&#x2F;era5" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;storage&#x2F;docs&#x2F;public-datasets&#x2F;era5</a></div><br/><div id="40057662" class="c"><input type="checkbox" id="c-40057662" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40055213">parent</a><span>|</span><a href="#40055303">next</a><span>|</span><label class="collapse" for="c-40057662">[-]</label><label class="expand" for="c-40057662">[2 more]</label></div><br/><div class="children"><div class="content">The main reason why was that it&#x27;s a personal project and I wanted to do everything on my home server so that I wouldn&#x27;t have to pay for cloud resources, and so that I could learn Postgres, TimescaleDB, and eventuallly PostGIS.<p>But as rabernat pointed out in his comment, pulling out a long time series from the cloud replica is also slow. And I know I eventually want to perform complex spatio-temporal queries, e.g. computing the 99% percentile of summer temperatures in Chile from 1940-1980.<p>I don&#x27;t doubt that a cloud replica can be faster, but it&#x27;s at odds with my budget of $0 haha.</div><br/><div id="40057961" class="c"><input type="checkbox" id="c-40057961" checked=""/><div class="controls bullet"><span class="by">roter</span><span>|</span><a href="#40055213">root</a><span>|</span><a href="#40057662">parent</a><span>|</span><a href="#40055303">next</a><span>|</span><label class="collapse" for="c-40057961">[-]</label><label class="expand" for="c-40057961">[1 more]</label></div><br/><div class="children"><div class="content">I too need to do percentiles. One option is loop through the grids but bin&#x2F;histogram it. You&#x27;ll get a really good 99% from a 1 Kelvin bin width.<p>Also, I&#x27;ve found the diurnal profile from ERA5 analysis can be abysmal in some locations. ERA5-Land is much better, high resolution, though only available over... er... land.<p>To your point about not relying on cloud. Noted in the Google option [1] link above:<p>&gt; Update Frequency: The ERA5 dataset is currently not refreshed in the Google Cloud Public Dataset Program. The program provides ERA5 data spanning from 1940 to May 2023.<p>Another alternative, Amazon [2], also deprecated:<p>&gt; The provider of this dataset will no longer maintain this dataset. We are open to talking with anyone else who might be willing to provide this dataset to the community.<p>[2] <a href="https:&#x2F;&#x2F;registry.opendata.aws&#x2F;ecmwf-era5&#x2F;" rel="nofollow">https:&#x2F;&#x2F;registry.opendata.aws&#x2F;ecmwf-era5&#x2F;</a></div><br/></div></div></div></div><div id="40055303" class="c"><input type="checkbox" id="c-40055303" checked=""/><div class="controls bullet"><span class="by">rabernat</span><span>|</span><a href="#40055213">parent</a><span>|</span><a href="#40057662">prev</a><span>|</span><a href="#40055341">next</a><span>|</span><label class="collapse" for="c-40055303">[-]</label><label class="expand" for="c-40055303">[1 more]</label></div><br/><div class="children"><div class="content">True, but in fact, the Google ERA5 public data suffers from the exact chunking problem described in the post: it&#x27;s optimized for spatial queries, not timeseries queries. I just ran a benchmark, and it took me 20 minutes to pull a timeseries of a single variable at a single point!<p>This highlights the needs for timeseries-optimized chunking if that is your anticipated usage pattern.</div><br/></div></div><div id="40055341" class="c"><input type="checkbox" id="c-40055341" checked=""/><div class="controls bullet"><span class="by">orhmeh09</span><span>|</span><a href="#40055213">parent</a><span>|</span><a href="#40055303">prev</a><span>|</span><a href="#40052450">next</a><span>|</span><label class="collapse" for="c-40055341">[-]</label><label class="expand" for="c-40055341">[1 more]</label></div><br/><div class="children"><div class="content">That might be nice if someone would do it and teach others to use it. Some labs have an RDBMS-based pipeline with published algorithms and data that nobody wants to try to reimplement (and which nobody would be paid to do). About the best improvement we could get was moving from an ancient version of MySQL to Postgres + PostGIS. I think Timescale would have helped. There were other reasons also to run locally due to privacy, cluster access, funds etc.</div><br/></div></div></div></div><div id="40052450" class="c"><input type="checkbox" id="c-40052450" checked=""/><div class="controls bullet"><span class="by">carderne</span><span>|</span><a href="#40055213">prev</a><span>|</span><a href="#40058654">next</a><span>|</span><label class="collapse" for="c-40052450">[-]</label><label class="expand" for="c-40052450">[4 more]</label></div><br/><div class="children"><div class="content">Hey OP (assuming you&#x27;re the author), you might be interested in this similar experiment I did about four years ago, same dataset, same target, similar goal!<p><a href="https:&#x2F;&#x2F;rdrn.me&#x2F;optimising-sql&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rdrn.me&#x2F;optimising-sql&#x2F;</a><p>Similar sequence of investigations, but using regular Postgres rather than Timescale. With my setup I got another ~3x speedup over COPY by copying binary data directly (assuming your data is already in memory).</div><br/><div id="40052798" class="c"><input type="checkbox" id="c-40052798" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40052450">parent</a><span>|</span><a href="#40058654">next</a><span>|</span><label class="collapse" for="c-40052798">[-]</label><label class="expand" for="c-40052798">[3 more]</label></div><br/><div class="children"><div class="content">Wish I saw this before I started haha! I left a footnote about why I didn&#x27;t try binary copy (basically someone else found its performance disappointing) but it sounds like I should give it a try.<p>footnote: <a href="https:&#x2F;&#x2F;aliramadhan.me&#x2F;2024&#x2F;03&#x2F;31&#x2F;trillion-rows.html#fn:copy-binary-note" rel="nofollow">https:&#x2F;&#x2F;aliramadhan.me&#x2F;2024&#x2F;03&#x2F;31&#x2F;trillion-rows.html#fn:copy...</a></div><br/><div id="40053377" class="c"><input type="checkbox" id="c-40053377" checked=""/><div class="controls bullet"><span class="by">carderne</span><span>|</span><a href="#40052450">root</a><span>|</span><a href="#40052798">parent</a><span>|</span><a href="#40058654">next</a><span>|</span><label class="collapse" for="c-40053377">[-]</label><label class="expand" for="c-40053377">[2 more]</label></div><br/><div class="children"><div class="content">Yeah I imagine it depends where the data is coming from and what exactly it looks like (num fields, dtypes...?). What I did was source data -&gt; Numpy Structured Array [0] -&gt; Postgres binary [1]. Bit of a pain getting it into the required shape, but if you follow the links the code should get you going (sorry no type hints!).<p>[0] <a href="https:&#x2F;&#x2F;rdrn.me&#x2F;optimising-sampling&#x2F;#round-10-off-the-deep-end" rel="nofollow">https:&#x2F;&#x2F;rdrn.me&#x2F;optimising-sampling&#x2F;#round-10-off-the-deep-e...</a>
[1] In the original blog I linked.</div><br/><div id="40053861" class="c"><input type="checkbox" id="c-40053861" checked=""/><div class="controls bullet"><span class="by">anentropic</span><span>|</span><a href="#40052450">root</a><span>|</span><a href="#40053377">parent</a><span>|</span><a href="#40058654">next</a><span>|</span><label class="collapse" for="c-40053861">[-]</label><label class="expand" for="c-40053861">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to hear from anyone who&#x27;s done the same in MySQL</div><br/></div></div></div></div></div></div></div></div><div id="40058654" class="c"><input type="checkbox" id="c-40058654" checked=""/><div class="controls bullet"><span class="by">d416</span><span>|</span><a href="#40052450">prev</a><span>|</span><a href="#40052516">next</a><span>|</span><label class="collapse" for="c-40058654">[-]</label><label class="expand" for="c-40058654">[1 more]</label></div><br/><div class="children"><div class="content">âIs a relational database even appropriate for gridded weather data? No idea but weâll find out.â<p>Love this. Itâs the exact opposite of all the other âwell actuallyâ mainstream tech posts and I am here for all of it. Props for keeping the reader fully engaged on the journey.</div><br/></div></div><div id="40052516" class="c"><input type="checkbox" id="c-40052516" checked=""/><div class="controls bullet"><span class="by">RyanHamilton</span><span>|</span><a href="#40058654">prev</a><span>|</span><a href="#40056457">next</a><span>|</span><label class="collapse" for="c-40052516">[-]</label><label class="expand" for="c-40052516">[2 more]</label></div><br/><div class="children"><div class="content">If you want to plot time-series charts or many other charts directly from sql queries, qStudio is a free SQL IDE and works with everything including TimescaleDB: <a href="https:&#x2F;&#x2F;www.timestored.com&#x2F;qstudio&#x2F;database&#x2F;timescale" rel="nofollow">https:&#x2F;&#x2F;www.timestored.com&#x2F;qstudio&#x2F;database&#x2F;timescale</a>   Disclaimer, I am the author.</div><br/><div id="40056494" class="c"><input type="checkbox" id="c-40056494" checked=""/><div class="controls bullet"><span class="by">ayewo</span><span>|</span><a href="#40052516">parent</a><span>|</span><a href="#40056457">next</a><span>|</span><label class="collapse" for="c-40056494">[-]</label><label class="expand" for="c-40056494">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the process for adding support for other databases to your tool qStudio?<p>I&#x27;m thinking perhaps you could add support for Timeplus [1]? Timeplus is a streaming-first database built on ClickHouse. The core DB engine Timeplus Proton is open source [2].<p>It seems that qStudio is open source [3] and written in Java and will need a JDBC driver to add support for a new RDBMS? If yes, Timeplus Proton has an open source JDBC driver [4] based on ClickHouse&#x27;s driver but with modifications added for streaming use cases.<p>1: <a href="https:&#x2F;&#x2F;www.timeplus.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.timeplus.com&#x2F;</a><p>2: <a href="https:&#x2F;&#x2F;github.com&#x2F;timeplus-io&#x2F;proton">https:&#x2F;&#x2F;github.com&#x2F;timeplus-io&#x2F;proton</a><p>3: <a href="https:&#x2F;&#x2F;github.com&#x2F;timeseries&#x2F;qstudio">https:&#x2F;&#x2F;github.com&#x2F;timeseries&#x2F;qstudio</a><p>4: <a href="https:&#x2F;&#x2F;github.com&#x2F;timeplus-io&#x2F;proton-java-driver">https:&#x2F;&#x2F;github.com&#x2F;timeplus-io&#x2F;proton-java-driver</a></div><br/></div></div></div></div><div id="40056457" class="c"><input type="checkbox" id="c-40056457" checked=""/><div class="controls bullet"><span class="by">postgresperf</span><span>|</span><a href="#40052516">prev</a><span>|</span><a href="#40052119">next</a><span>|</span><label class="collapse" for="c-40056457">[-]</label><label class="expand" for="c-40056457">[3 more]</label></div><br/><div class="children"><div class="content">Contributor to the PG bulk loading docs you referenced here.  Good survey of the techniques here.  I&#x27;ve done a good bit of this trying to speed up loading the Open Street Map database.  Presentation at <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=BCMnu7xay2Y" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=BCMnu7xay2Y</a> for my last public update.  Since then the advance of hardware, GIS improvements in PG15, and osm2pgsql adopting their middle-way-node-index-id-shift technique (makes the largest but rarely used index 1&#x2F;32 the size) have gotten my times to load the planet set below 4 hours.<p>One suggestion aimed at the author here:  some of your experiments are taking out WAL writing in a sort of indirect way, using pg_bulkload and COPY.  There&#x27;s one thing you could try that wasn&#x27;t documented yet when my buddy Craig Ringer wrote the SO post you linked to:  you can just turn off the WAL in the configuration.  Yes, you will lose the tables in progress if there&#x27;s a crash, and when things run for weeks those happen.  With time scale data, it&#x27;s not hard to structure the loading so you&#x27;ll only lose the last chunk of work when that happens.  WAL data isn&#x27;t really necessary for bulk loading.  Crash, clean up the right edge of the loaded data, start back up.<p>Here&#x27;s the full set of postgresql.conf settings I run to disable the WAL and other overhead:<p>wal_level = minimal
 max_wal_senders = 0
 synchronous_commit = off
 fsync = off
 full_page_writes = off
 autovacuum = off
 checkpoint_timeout = 60min<p>Finally, when loading in big chunks, to keep the vacuum work down I&#x27;d normally turn off autovac as above then issue periodic VACUUM FREEZE commands running behind the currently loading date partition.  (Talking normal PG here)  That skips some work of the intermediate step the database normally frets about where new transactions are written but not visible to everyone yet.</div><br/><div id="40061208" class="c"><input type="checkbox" id="c-40061208" checked=""/><div class="controls bullet"><span class="by">kabes</span><span>|</span><a href="#40056457">parent</a><span>|</span><a href="#40052119">next</a><span>|</span><label class="collapse" for="c-40061208">[-]</label><label class="expand" for="c-40061208">[2 more]</label></div><br/><div class="children"><div class="content">Do you have more info on the GIS improvements in PG15?</div><br/><div id="40061908" class="c"><input type="checkbox" id="c-40061908" checked=""/><div class="controls bullet"><span class="by">postgresperf</span><span>|</span><a href="#40056457">root</a><span>|</span><a href="#40061208">parent</a><span>|</span><a href="#40052119">next</a><span>|</span><label class="collapse" for="c-40061908">[-]</label><label class="expand" for="c-40061908">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a whole talk about it we had in our conference: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TG28lRoailE" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TG28lRoailE</a><p>Short version is GIS indexes are notably smaller and build faster in PG15 than earlier versions.  It&#x27;s a major version to version PG improvement for these workloads.</div><br/></div></div></div></div></div></div><div id="40052119" class="c"><input type="checkbox" id="c-40052119" checked=""/><div class="controls bullet"><span class="by">lawn</span><span>|</span><a href="#40056457">prev</a><span>|</span><a href="#40059159">next</a><span>|</span><label class="collapse" for="c-40052119">[-]</label><label class="expand" for="c-40052119">[2 more]</label></div><br/><div class="children"><div class="content">What an interesting post!<p>&gt; At a sustained ~462k inserts per second, weâre waiting ~20 days for our ~754 billion rows which is not bad I guess  Itâs less time than it took me to write this post.<p>Hah, as I&#x27;ve been gravitating more to writing larger and more in depth blog posts I can relate to the surprising effort it can require.</div><br/><div id="40052518" class="c"><input type="checkbox" id="c-40052518" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40052119">parent</a><span>|</span><a href="#40059159">next</a><span>|</span><label class="collapse" for="c-40052518">[-]</label><label class="expand" for="c-40052518">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! Yeah haha some of the benchmarks took several hours (and a few re-runs) and there was a lot of learning done along the way.</div><br/></div></div></div></div><div id="40059159" class="c"><input type="checkbox" id="c-40059159" checked=""/><div class="controls bullet"><span class="by">to11mtm</span><span>|</span><a href="#40052119">prev</a><span>|</span><a href="#40054058">next</a><span>|</span><label class="collapse" for="c-40059159">[-]</label><label class="expand" for="c-40059159">[1 more]</label></div><br/><div class="children"><div class="content">As someone who used to do some GIS hacking in an office job[0] before I was a &#x27;Software Developer&#x2F;engineer&#x27; this is super cool.<p>[0] - Honestly some of the coolest stuff I ever got to do in it&#x27;s own right. Building tools that could move data between AutoCAD, Microstation, and Google Earth while also importing other bits with metadata from Trimble units[1]. Also it was probably the most I ever used math in my entire career [2], so there&#x27;s <i>that</i>.<p>[1] - I wound up finding a custom font maker, and one of our folks made a font library with the symbols, made it easy to write a parser too :D<p>[2] - One of those PDFs I always seem to wind up having floating on a hard drive is the USGS &#x27;Map Projections, a working manual&#x27;. At one point I used it as a reference to implement a C# library to handle transforms between coordinate systems... alas it was internal.</div><br/></div></div><div id="40054058" class="c"><input type="checkbox" id="c-40054058" checked=""/><div class="controls bullet"><span class="by">tmiku</span><span>|</span><a href="#40059159">prev</a><span>|</span><a href="#40061498">next</a><span>|</span><label class="collapse" for="c-40054058">[-]</label><label class="expand" for="c-40054058">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I think it would be cool to have historical weather data from around the world to analyze for signals of climate change weâve already had rather than think about potential future change.<p>This is a very good instinct! A pretty major portion of modern climate science is paleoclimatology, with a goal of reaching far beyond reliable human measurements. A lot of earth&#x27;s previous climate states were wildly different from the range of conditions we have experienced in the past 10,000 years, and a better climate record is essential to predicting the effects of massive carbon emission.<p>Ice cores from Antarctica&#x2F;Greenland are the most famous instance of this, but there&#x27;s a lot of other cool ones - there are chemical records of climate change in cave stalactites, ocean floor sediments, etc.</div><br/><div id="40057018" class="c"><input type="checkbox" id="c-40057018" checked=""/><div class="controls bullet"><span class="by">randrus</span><span>|</span><a href="#40054058">parent</a><span>|</span><a href="#40061498">next</a><span>|</span><label class="collapse" for="c-40057018">[-]</label><label class="expand" for="c-40057018">[1 more]</label></div><br/><div class="children"><div class="content">Somewhat relevant (which I posted a while back):<p><a href="https:&#x2F;&#x2F;climate.metoffice.cloud&#x2F;" rel="nofollow">https:&#x2F;&#x2F;climate.metoffice.cloud&#x2F;</a></div><br/></div></div></div></div><div id="40061498" class="c"><input type="checkbox" id="c-40061498" checked=""/><div class="controls bullet"><span class="by">dcreater</span><span>|</span><a href="#40054058">prev</a><span>|</span><a href="#40051812">next</a><span>|</span><label class="collapse" for="c-40061498">[-]</label><label class="expand" for="c-40061498">[1 more]</label></div><br/><div class="children"><div class="content">Nice! Can you upload the data to hf, oxen.ai, kaggle or something?</div><br/></div></div><div id="40051812" class="c"><input type="checkbox" id="c-40051812" checked=""/><div class="controls bullet"><span class="by">semiquaver</span><span>|</span><a href="#40061498">prev</a><span>|</span><a href="#40058425">next</a><span>|</span><label class="collapse" for="c-40051812">[-]</label><label class="expand" for="c-40051812">[8 more]</label></div><br/><div class="children"><div class="content">Any idea why hypertable insert rates were slower? I though hypertables were supposed to _increase_ insert rates?</div><br/><div id="40052186" class="c"><input type="checkbox" id="c-40052186" checked=""/><div class="controls bullet"><span class="by">perrygeo</span><span>|</span><a href="#40051812">parent</a><span>|</span><a href="#40051894">next</a><span>|</span><label class="collapse" for="c-40052186">[-]</label><label class="expand" for="c-40052186">[3 more]</label></div><br/><div class="children"><div class="content">Hypertable insert rates are faster and more predictable <i>over time</i>. Each individual insert might incur a small bit of extra overhead, but they scale forever since each temporal chunk is indexed separately vs a regular table where the entire index needs to fit in memory. This is a case where you can&#x27;t make meaningful inferences from micro-benchmarks (they tested 20k rows, you probably need 200M to start seeing the diff)</div><br/><div id="40052280" class="c"><input type="checkbox" id="c-40052280" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40051812">root</a><span>|</span><a href="#40052186">parent</a><span>|</span><a href="#40052267">next</a><span>|</span><label class="collapse" for="c-40052280">[-]</label><label class="expand" for="c-40052280">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the insight! It is true that I started with a micro-benchmark of 20k rows for slower inserts, but I also did some longer benchmarks with ~772 million rows.</div><br/></div></div></div></div><div id="40051894" class="c"><input type="checkbox" id="c-40051894" checked=""/><div class="controls bullet"><span class="by">leros</span><span>|</span><a href="#40051812">parent</a><span>|</span><a href="#40052186">prev</a><span>|</span><a href="#40051949">next</a><span>|</span><label class="collapse" for="c-40051894">[-]</label><label class="expand" for="c-40051894">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t data inserted into basically a normal Postgres table with hypertable extensions? I don&#x27;t know the details of Timescale but that sounds like it would incur a cost of a normal Postgres insert, plus potentially extra work at insert time, plus extra work in the background to manage the hypertable.</div><br/><div id="40054468" class="c"><input type="checkbox" id="c-40054468" checked=""/><div class="controls bullet"><span class="by">rahkiin</span><span>|</span><a href="#40051812">root</a><span>|</span><a href="#40051894">parent</a><span>|</span><a href="#40051949">next</a><span>|</span><label class="collapse" for="c-40054468">[-]</label><label class="expand" for="c-40054468">[1 more]</label></div><br/><div class="children"><div class="content">Not entirely. A hypertable is a postgres table chunked over time. There is the assumption that most data and queries are time-relevant, but also that older data is less relevant than new data.<p>Indexes are per chunk. So if the query analyzer understands you only touch 2023 it can omit looking at any chunk that is from other years and keep those out of memory. Same with the indexes.</div><br/></div></div></div></div><div id="40051949" class="c"><input type="checkbox" id="c-40051949" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40051812">parent</a><span>|</span><a href="#40051894">prev</a><span>|</span><a href="#40058425">next</a><span>|</span><label class="collapse" for="c-40051949">[-]</label><label class="expand" for="c-40051949">[2 more]</label></div><br/><div class="children"><div class="content">Yeah I&#x27;m curious about this too. Been meaning to ask on the Timescale forums. My only guess is that there&#x27;s some small extra overhead due to hypertable chunking.<p>I know Timescale has a blog post from 2017 claiming a 20x higher insert rate but that&#x27;s for inserting into a table with an index. The general wisdom for loading huge amounts of data seems to be that you should insert into a table with no indexes then build them later though. So with no index, inserting into a hypertable seems a bit slower.<p>Timescale blog post: <a href="https:&#x2F;&#x2F;medium.com&#x2F;timescale&#x2F;timescaledb-vs-6a696248104e" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;timescale&#x2F;timescaledb-vs-6a696248104e</a></div><br/><div id="40052062" class="c"><input type="checkbox" id="c-40052062" checked=""/><div class="controls bullet"><span class="by">h4kor</span><span>|</span><a href="#40051812">root</a><span>|</span><a href="#40051949">parent</a><span>|</span><a href="#40058425">next</a><span>|</span><label class="collapse" for="c-40052062">[-]</label><label class="expand" for="c-40052062">[1 more]</label></div><br/><div class="children"><div class="content">Timescale hypertables automatically have an index on the timestamp. To make this more comparable you could create the same index on the normal table and test the ingestion rate.</div><br/></div></div></div></div></div></div><div id="40058425" class="c"><input type="checkbox" id="c-40058425" checked=""/><div class="controls bullet"><span class="by">islandert</span><span>|</span><a href="#40051812">prev</a><span>|</span><a href="#40054640">next</a><span>|</span><label class="collapse" for="c-40058425">[-]</label><label class="expand" for="c-40058425">[2 more]</label></div><br/><div class="children"><div class="content">If you don&#x27;t have access to COPY if the postgres instance is managed, I&#x27;ve had a lot of luck with encoding a batch of rows as a JSON string, sending the string as a single query parameter, and using `json_to_recordset` to turn the JSON back into a list of rows in the db.<p>I haven&#x27;t compared how this performs compared to using a low-level sql library but it outperforms everything else I&#x27;ve tried in sqlalchemy.</div><br/><div id="40059682" class="c"><input type="checkbox" id="c-40059682" checked=""/><div class="controls bullet"><span class="by">twoodfin</span><span>|</span><a href="#40058425">parent</a><span>|</span><a href="#40054640">next</a><span>|</span><label class="collapse" for="c-40059682">[-]</label><label class="expand" for="c-40059682">[1 more]</label></div><br/><div class="children"><div class="content">When I see suggestions like this (totally reasonable hack!), I do have to wonder what happened to JDBCâs âaddBatch()&#x2F;executeBatch()â, introduced over 25 years ago.<p><a href="https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;api&#x2F;java&#x2F;sql&#x2F;PreparedStatement.html#addBatch--" rel="nofollow">https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;api&#x2F;java&#x2F;sql&#x2F;PreparedS...</a><p>Did modern APIs &amp; protocols simply fail to carry this evolutionary strand forward?</div><br/></div></div></div></div><div id="40054640" class="c"><input type="checkbox" id="c-40054640" checked=""/><div class="controls bullet"><span class="by">tonymet</span><span>|</span><a href="#40058425">prev</a><span>|</span><a href="#40052081">next</a><span>|</span><label class="collapse" for="c-40054640">[-]</label><label class="expand" for="c-40054640">[8 more]</label></div><br/><div class="children"><div class="content">I encourage people to look into the ERA5 dataset provenance especially when you approach the observations made toward the &quot;pre industrial date&quot; of 1850 .<p>Remember that modern global surface temperatures are collected by satellites, and the dataset is comingled with recordings observed visually &amp; made by hand using buckets by sailors who were not primarily academic researchers.  Segments of high resolution, low noise data (satellites) are mixed with low resolution, low coverage, high noise records (hand records on a boat made surrounding the united kingdom).<p>My point is to be in awe of the technical aspects of this effort but also keep in mind that we are only making copies of low resolution, noisy manuscripts from sailors 170 years ago.</div><br/><div id="40058860" class="c"><input type="checkbox" id="c-40058860" checked=""/><div class="controls bullet"><span class="by">shoyer</span><span>|</span><a href="#40054640">parent</a><span>|</span><a href="#40056598">next</a><span>|</span><label class="collapse" for="c-40058860">[-]</label><label class="expand" for="c-40058860">[2 more]</label></div><br/><div class="children"><div class="content">ERA5 covers 1940 to present. That&#x27;s well before the satellite era (and the earlier data absolutely has more quality issues) but there&#x27;s nothing from 170 years ago.</div><br/><div id="40058994" class="c"><input type="checkbox" id="c-40058994" checked=""/><div class="controls bullet"><span class="by">tonymet</span><span>|</span><a href="#40054640">root</a><span>|</span><a href="#40058860">parent</a><span>|</span><a href="#40056598">next</a><span>|</span><label class="collapse" for="c-40058994">[-]</label><label class="expand" for="c-40058994">[1 more]</label></div><br/><div class="children"><div class="content">Similar noise issues apply. Most of the other surface temp models have to cover 1850</div><br/></div></div></div></div><div id="40056598" class="c"><input type="checkbox" id="c-40056598" checked=""/><div class="controls bullet"><span class="by">relaxing</span><span>|</span><a href="#40054640">parent</a><span>|</span><a href="#40058860">prev</a><span>|</span><a href="#40052081">next</a><span>|</span><label class="collapse" for="c-40056598">[-]</label><label class="expand" for="c-40056598">[5 more]</label></div><br/><div class="children"><div class="content">Ok? And whatâs your point in pointing out that?</div><br/><div id="40058615" class="c"><input type="checkbox" id="c-40058615" checked=""/><div class="controls bullet"><span class="by">tonymet</span><span>|</span><a href="#40054640">root</a><span>|</span><a href="#40056598">parent</a><span>|</span><a href="#40058669">next</a><span>|</span><label class="collapse" for="c-40058615">[-]</label><label class="expand" for="c-40058615">[3 more]</label></div><br/><div class="children"><div class="content">The data is noisy so be careful when using it for research.  Always account for the provenance of the records when working with &quot;data&quot;.</div><br/><div id="40059001" class="c"><input type="checkbox" id="c-40059001" checked=""/><div class="controls bullet"><span class="by">relaxing</span><span>|</span><a href="#40054640">root</a><span>|</span><a href="#40058615">parent</a><span>|</span><a href="#40058669">next</a><span>|</span><label class="collapse" for="c-40059001">[-]</label><label class="expand" for="c-40059001">[2 more]</label></div><br/><div class="children"><div class="content">So like basically every data science effort.</div><br/></div></div></div></div><div id="40058669" class="c"><input type="checkbox" id="c-40058669" checked=""/><div class="controls bullet"><span class="by">tonymet</span><span>|</span><a href="#40054640">root</a><span>|</span><a href="#40056598">parent</a><span>|</span><a href="#40058615">prev</a><span>|</span><a href="#40052081">next</a><span>|</span><label class="collapse" for="c-40058669">[-]</label><label class="expand" for="c-40058669">[1 more]</label></div><br/><div class="children"><div class="content">one of the project&#x27;s goals was to load the data and make predictions.  The page covered the data loading part, but not the methods and error tolerance in the predictions</div><br/></div></div></div></div></div></div><div id="40052081" class="c"><input type="checkbox" id="c-40052081" checked=""/><div class="controls bullet"><span class="by">hyperman1</span><span>|</span><a href="#40054640">prev</a><span>|</span><a href="#40059322">next</a><span>|</span><label class="collapse" for="c-40052081">[-]</label><label class="expand" for="c-40052081">[5 more]</label></div><br/><div class="children"><div class="content">Two remarks with postgres and lots of data:<p>1) I always wonder of there is a better way than COPY.  I tend to quickly get 100% CPU without saturating I&#x2F;O<p>2) The row overhead seems big.  A row has 23 bytes overhead, this table has 48 bytes data per row, so even without page overhead, we lose ~1&#x2F;3 of our storage.  This is pure data storage, without any index.</div><br/><div id="40052489" class="c"><input type="checkbox" id="c-40052489" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40052081">parent</a><span>|</span><a href="#40059322">next</a><span>|</span><label class="collapse" for="c-40052489">[-]</label><label class="expand" for="c-40052489">[4 more]</label></div><br/><div class="children"><div class="content">1. Yeah to me it seems very hard to saturate I&#x2F;O with Postgres unless maybe you insert into an unlogged table. I guess there&#x27;s quite a bit of overhead to get all the nice stability&#x2F;consistency and crash-resistance.<p>2. That is a good point. I&#x27;m hoping TimescaleDB&#x27;s compression helps here but yeah I don&#x27;t think you&#x27;ll ever get the database size below the data&#x27;s original footprint.</div><br/><div id="40052968" class="c"><input type="checkbox" id="c-40052968" checked=""/><div class="controls bullet"><span class="by">feike</span><span>|</span><a href="#40052081">root</a><span>|</span><a href="#40052489">parent</a><span>|</span><a href="#40053014">next</a><span>|</span><label class="collapse" for="c-40052968">[-]</label><label class="expand" for="c-40052968">[1 more]</label></div><br/><div class="children"><div class="content">Timescaler here, if you configure the timescaledb.compress_segmentby well, and the data suits the compression, you can achieve 20x or more compression.<p>(On some metrics data internally, I have 98% reduction in size of the data).<p>One of the reasons this works is due to only having to pay the per-tuple overhead once per grouped row, which could be as much as a 1000 rows.<p>The other is the compression algorithm, which can be TimescaleDB or plain PostgreSQL TOAST<p><a href="https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;time-series-compression-algorithms-explained&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;time-series-compression-algor...</a>
<a href="https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;storage-toast.html" rel="nofollow">https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;storage-toast.html</a></div><br/></div></div><div id="40053014" class="c"><input type="checkbox" id="c-40053014" checked=""/><div class="controls bullet"><span class="by">hyperman1</span><span>|</span><a href="#40052081">root</a><span>|</span><a href="#40052489">parent</a><span>|</span><a href="#40052968">prev</a><span>|</span><a href="#40059322">next</a><span>|</span><label class="collapse" for="c-40053014">[-]</label><label class="expand" for="c-40053014">[2 more]</label></div><br/><div class="children"><div class="content">If I look into perf, it seems mostly parsing overhead.  I can saturate a create newtable as select from oldtable.  Unfortunately, CSV seems still the lingua franca for transport between DBs.  Maybe some day a more binary oriented transport protocol will appear( e.g parquet?)</div><br/><div id="40053124" class="c"><input type="checkbox" id="c-40053124" checked=""/><div class="controls bullet"><span class="by">feike</span><span>|</span><a href="#40052081">root</a><span>|</span><a href="#40053014">parent</a><span>|</span><a href="#40059322">next</a><span>|</span><label class="collapse" for="c-40053124">[-]</label><label class="expand" for="c-40053124">[1 more]</label></div><br/><div class="children"><div class="content">Many libraries for python, Rust, golang support COPY BINARY.<p>The times I&#x27;ve tested it, the improvement is very small as compared to plain copy, or copy with CSV, whereas it does require more work and thought upfront to ensure the binary actually works correctly.<p><a href="https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;sql-copy.html" rel="nofollow">https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;sql-copy.html</a></div><br/></div></div></div></div></div></div></div></div><div id="40059322" class="c"><input type="checkbox" id="c-40059322" checked=""/><div class="controls bullet"><span class="by">curious_cat_163</span><span>|</span><a href="#40052081">prev</a><span>|</span><a href="#40054329">next</a><span>|</span><label class="collapse" for="c-40059322">[-]</label><label class="expand" for="c-40059322">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The data is output from a climate model run that is constrained to match weather observations.<p>That&#x27;s interesting. Why store it? Why not compute it as needed using the model?<p>FWIW, I am not an expert in this space and if someone is, it would be good to understand it.</div><br/></div></div><div id="40054329" class="c"><input type="checkbox" id="c-40054329" checked=""/><div class="controls bullet"><span class="by">roter</span><span>|</span><a href="#40059322">prev</a><span>|</span><a href="#40053884">next</a><span>|</span><label class="collapse" for="c-40054329">[-]</label><label class="expand" for="c-40054329">[1 more]</label></div><br/><div class="children"><div class="content">I too use the ERA5 reanalysis data and I too need quick time series. As the data comes in [lat, lon] grids, stacked by whatever period you&#x27;ve chosen, e.g. [month of hourly data, lat, lon], it becomes a massive matrix transpose problem if you want 20+ years.<p>What I do is download each netCDF file, transpose, and insert into a massive 3D HDF file organized as [lat, lon, hour]. On my workstation it takes about 30 minutes to create one year for one variable (no parallel I&#x2F;O or processes) but then takes milliseconds to pull a single (lat, lon) location. Initial pain for long-term gain. Simplistic, but I&#x27;m just a climatologist not a database guru.</div><br/></div></div><div id="40053884" class="c"><input type="checkbox" id="c-40053884" checked=""/><div class="controls bullet"><span class="by">hendiatris</span><span>|</span><a href="#40054329">prev</a><span>|</span><a href="#40055965">next</a><span>|</span><label class="collapse" for="c-40053884">[-]</label><label class="expand" for="c-40053884">[1 more]</label></div><br/><div class="children"><div class="content">If youâre going to work with weather data use a columnar database, like BigQuery. If you set things up right your performance will generally be a few seconds for aggregation queries. I setup a data platform like this at my previous company and we were able to vastly outperform our competitors and at a much lower cost.<p>The great thing about this data is it is generally append only, unless errors are found in earlier data sets. But itâs something that usually only happens once a year if at all.</div><br/></div></div><div id="40055965" class="c"><input type="checkbox" id="c-40055965" checked=""/><div class="controls bullet"><span class="by">koliber</span><span>|</span><a href="#40053884">prev</a><span>|</span><a href="#40052821">next</a><span>|</span><label class="collapse" for="c-40055965">[-]</label><label class="expand" for="c-40055965">[1 more]</label></div><br/><div class="children"><div class="content">Curious if he could squeeze more performance by using a different structure to store the same data. Some of these float4 cols could probably be stored as int2. Depending on how many decimal places are needed, can divide the int to get the resulting floating point value.</div><br/></div></div><div id="40053971" class="c"><input type="checkbox" id="c-40053971" checked=""/><div class="controls bullet"><span class="by">sammy2255</span><span>|</span><a href="#40052821">prev</a><span>|</span><a href="#40059394">next</a><span>|</span><label class="collapse" for="c-40053971">[-]</label><label class="expand" for="c-40053971">[1 more]</label></div><br/><div class="children"><div class="content">Clickhouse will eat this for breakfast. And has built-in compression even at the column level</div><br/></div></div><div id="40059394" class="c"><input type="checkbox" id="c-40059394" checked=""/><div class="controls bullet"><span class="by">nojvek</span><span>|</span><a href="#40053971">prev</a><span>|</span><a href="#40055046">next</a><span>|</span><label class="collapse" for="c-40059394">[-]</label><label class="expand" for="c-40059394">[1 more]</label></div><br/><div class="children"><div class="content">This should be a benchmark.<p>Could someone post me to where I can download the whole dataset?</div><br/></div></div><div id="40055046" class="c"><input type="checkbox" id="c-40055046" checked=""/><div class="controls bullet"><span class="by">smellybigbelly</span><span>|</span><a href="#40059394">prev</a><span>|</span><a href="#40052461">next</a><span>|</span><label class="collapse" for="c-40055046">[-]</label><label class="expand" for="c-40055046">[2 more]</label></div><br/><div class="children"><div class="content">Can anyone give some advice on how they run TimeScale in Kubernetes? Iâm seeing they dropped support for their Helm chart.</div><br/><div id="40055324" class="c"><input type="checkbox" id="c-40055324" checked=""/><div class="controls bullet"><span class="by">jamesgresql</span><span>|</span><a href="#40055046">parent</a><span>|</span><a href="#40052461">next</a><span>|</span><label class="collapse" for="c-40055324">[-]</label><label class="expand" for="c-40055324">[1 more]</label></div><br/><div class="children"><div class="content">Hi, we updated our docs with the best options.<p><a href="https:&#x2F;&#x2F;docs.timescale.com&#x2F;self-hosted&#x2F;latest&#x2F;install&#x2F;installation-kubernetes&#x2F;" rel="nofollow">https:&#x2F;&#x2F;docs.timescale.com&#x2F;self-hosted&#x2F;latest&#x2F;install&#x2F;instal...</a><p>I&#x27;d personally recommend StackGres, it&#x27;s a great project.</div><br/></div></div></div></div><div id="40052461" class="c"><input type="checkbox" id="c-40052461" checked=""/><div class="controls bullet"><span class="by">dunefox</span><span>|</span><a href="#40055046">prev</a><span>|</span><a href="#40059411">next</a><span>|</span><label class="collapse" for="c-40052461">[-]</label><label class="expand" for="c-40052461">[2 more]</label></div><br/><div class="children"><div class="content">OT: Does anyone know if DuckDB would be of use here?</div><br/><div id="40052493" class="c"><input type="checkbox" id="c-40052493" checked=""/><div class="controls bullet"><span class="by">wiredfool</span><span>|</span><a href="#40052461">parent</a><span>|</span><a href="#40059411">next</a><span>|</span><label class="collapse" for="c-40052493">[-]</label><label class="expand" for="c-40052493">[1 more]</label></div><br/><div class="children"><div class="content">Not likely, unless it&#x27;s a set of parquet files already.  Clickhouse would be a better bet.</div><br/></div></div></div></div><div id="40059411" class="c"><input type="checkbox" id="c-40059411" checked=""/><div class="controls bullet"><span class="by">Aeroi</span><span>|</span><a href="#40052461">prev</a><span>|</span><a href="#40059534">next</a><span>|</span><label class="collapse" for="c-40059411">[-]</label><label class="expand" for="c-40059411">[1 more]</label></div><br/><div class="children"><div class="content">What did Google use to train GraphCast?</div><br/></div></div><div id="40059534" class="c"><input type="checkbox" id="c-40059534" checked=""/><div class="controls bullet"><span class="by">kinj28</span><span>|</span><a href="#40059411">prev</a><span>|</span><a href="#40052233">next</a><span>|</span><label class="collapse" for="c-40059534">[-]</label><label class="expand" for="c-40059534">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a tangent.<p>I am curious if we query the data to give us temperature at a given time for all lat n long and plot it geo spatially , would the result give anything on  heat distribution of energy received across the lat and long at that point in time?</div><br/></div></div><div id="40052233" class="c"><input type="checkbox" id="c-40052233" checked=""/><div class="controls bullet"><span class="by">rkwasny</span><span>|</span><a href="#40059534">prev</a><span>|</span><a href="#40052787">next</a><span>|</span><label class="collapse" for="c-40052233">[-]</label><label class="expand" for="c-40052233">[13 more]</label></div><br/><div class="children"><div class="content">Yeah, don&#x27;t use TimescaleDB, use ClickHouse - I have 10 years of NOAA climate data on my desktop that I query when I want to go on holidays ;-)</div><br/><div id="40053119" class="c"><input type="checkbox" id="c-40053119" checked=""/><div class="controls bullet"><span class="by">mfreed</span><span>|</span><a href="#40052233">parent</a><span>|</span><a href="#40052554">next</a><span>|</span><label class="collapse" for="c-40053119">[-]</label><label class="expand" for="c-40053119">[3 more]</label></div><br/><div class="children"><div class="content">Our experience is that Clickhouse and Timescale are designed for different workloads, and that Timescale is optimized for many of the time-series workloads people use in production:<p>- <a href="https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;what-is-clickhouse-how-does-it-compare-to-postgresql-and-timescaledb-and-how-does-it-perform-for-time-series-data&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;what-is-clickhouse-how-does-i...</a><p>Sidenote:  Timescale _does_ provide columnar storage.  I don&#x27;t believe that the blog author focused on this as part of insert benchmarks:<p>- Timescale columnar storage: <a href="https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;building-columnar-compression-in-a-row-oriented-database&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;building-columnar-compression...</a><p>- Timescale query vectorization: <a href="https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;teaching-postgres-new-tricks-simd-vectorization-for-faster-analytical-queries&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;teaching-postgres-new-tricks-...</a></div><br/><div id="40053459" class="c"><input type="checkbox" id="c-40053459" checked=""/><div class="controls bullet"><span class="by">rkwasny</span><span>|</span><a href="#40052233">root</a><span>|</span><a href="#40053119">parent</a><span>|</span><a href="#40052554">next</a><span>|</span><label class="collapse" for="c-40053459">[-]</label><label class="expand" for="c-40053459">[2 more]</label></div><br/><div class="children"><div class="content">Well, as a Co-founder and CTO of Timescale, would you say TimescaleDB is a good fit for storing weather data as OP does?</div><br/><div id="40057439" class="c"><input type="checkbox" id="c-40057439" checked=""/><div class="controls bullet"><span class="by">mfreed</span><span>|</span><a href="#40052233">root</a><span>|</span><a href="#40053459">parent</a><span>|</span><a href="#40052554">next</a><span>|</span><label class="collapse" for="c-40057439">[-]</label><label class="expand" for="c-40057439">[1 more]</label></div><br/><div class="children"><div class="content">TimescaleDB primarily serves operational use cases: Developers building products on top of live data, where you are regularly streaming in fresh data, and you often know what many queries look like a priori, because those are powering your live APIs, dashboards, and product experience.<p>That&#x27;s different from a data warehouse or many traditional &quot;OLAP&quot; use cases, where you might dump a big dataset statically, and then people will occasionally do ad-hoc queries against it.  This is the big weather dataset file sitting on your desktop that you occasionally query while on holidays.<p>So it&#x27;s less about &quot;can you store weather data&quot;, but what does that use case look like?  How are the queries shaped?  Are you saving a single dataset for ad-hoc queries across the entire dataset, or continuously streaming in new data, and aging out or de-prioritizing old data?<p>In most of the products we serve, customers are often interested in recent data in a very granular format (&quot;shallow and wide&quot;), or longer historical queries along a well defined axis (&quot;deep and narrow&quot;).<p>For example, this is where the benefits of TimescaleDB&#x27;s segmented columnar compression emerges.  It optimizes for those queries which are very common in your application, e.g., an IoT application that groups by or selected by deviceID, crypto&#x2F;fintech analysis based on the ticker symbol, product analytics based on tenantID, etc.<p>If you look at Clickbench, what most of the queries say are:  Scan ALL the data in your database, and GROUP BY one of the 100 columns in the web analytics logs.<p>- <a href="https:&#x2F;&#x2F;github.com&#x2F;ClickHouse&#x2F;ClickBench&#x2F;blob&#x2F;main&#x2F;clickhouse&#x2F;queries.sql">https:&#x2F;&#x2F;github.com&#x2F;ClickHouse&#x2F;ClickBench&#x2F;blob&#x2F;main&#x2F;clickhous...</a><p>There are almost no time-predicates in the benchmark that Clickhouse created, but perhaps that is not surprising given it was designed for ad-hoc weblog analytics at Yandex.<p>So yes, Timescale serves many products today that use weather data, but has made different choices than Clickhouse (or things like DuckDB, pg_analytics, etc) to serve those more operational use cases.</div><br/></div></div></div></div></div></div><div id="40052554" class="c"><input type="checkbox" id="c-40052554" checked=""/><div class="controls bullet"><span class="by">dangoodmanUT</span><span>|</span><a href="#40052233">parent</a><span>|</span><a href="#40053119">prev</a><span>|</span><a href="#40052283">next</a><span>|</span><label class="collapse" for="c-40052554">[-]</label><label class="expand" for="c-40052554">[3 more]</label></div><br/><div class="children"><div class="content">Agreed, clickhouse is faster and has better features for this</div><br/><div id="40053627" class="c"><input type="checkbox" id="c-40053627" checked=""/><div class="controls bullet"><span class="by">RyanHamilton</span><span>|</span><a href="#40052233">root</a><span>|</span><a href="#40052554">parent</a><span>|</span><a href="#40052283">next</a><span>|</span><label class="collapse" for="c-40053627">[-]</label><label class="expand" for="c-40053627">[2 more]</label></div><br/><div class="children"><div class="content">I agree. Clickhouse is awesomely powerful and fast. I maintain a list of benchmarks, if you know of any speed comparisons please let me know and I will add them to the lsit:
<a href="https:&#x2F;&#x2F;www.timestored.com&#x2F;data&#x2F;time-series-database-benchmarks" rel="nofollow">https:&#x2F;&#x2F;www.timestored.com&#x2F;data&#x2F;time-series-database-benchma...</a></div><br/><div id="40055544" class="c"><input type="checkbox" id="c-40055544" checked=""/><div class="controls bullet"><span class="by">lyapunova</span><span>|</span><a href="#40052233">root</a><span>|</span><a href="#40053627">parent</a><span>|</span><a href="#40052283">next</a><span>|</span><label class="collapse" for="c-40055544">[-]</label><label class="expand" for="c-40055544">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for maintaining benchmarks here. Is there a github repo that might accompany the benchmarks that I could take a look at &#x2F; reproduce?</div><br/></div></div></div></div></div></div><div id="40052283" class="c"><input type="checkbox" id="c-40052283" checked=""/><div class="controls bullet"><span class="by">anentropic</span><span>|</span><a href="#40052233">parent</a><span>|</span><a href="#40052554">prev</a><span>|</span><a href="#40052614">next</a><span>|</span><label class="collapse" for="c-40052283">[-]</label><label class="expand" for="c-40052283">[2 more]</label></div><br/><div class="children"><div class="content">Do you say that because its quicker to insert large batches of rows into Clickhouse, or because it&#x27;s better in other ways?<p>(I&#x27;m currently inserting large batches of rows into MySQL and curious about Clickhouse...)</div><br/><div id="40052427" class="c"><input type="checkbox" id="c-40052427" checked=""/><div class="controls bullet"><span class="by">rkwasny</span><span>|</span><a href="#40052233">root</a><span>|</span><a href="#40052283">parent</a><span>|</span><a href="#40052614">next</a><span>|</span><label class="collapse" for="c-40052427">[-]</label><label class="expand" for="c-40052427">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s better in insert speed, query speed and used disk storage.</div><br/></div></div></div></div><div id="40052614" class="c"><input type="checkbox" id="c-40052614" checked=""/><div class="controls bullet"><span class="by">gonzo41</span><span>|</span><a href="#40052233">parent</a><span>|</span><a href="#40052283">prev</a><span>|</span><a href="#40052438">next</a><span>|</span><label class="collapse" for="c-40052614">[-]</label><label class="expand" for="c-40052614">[1 more]</label></div><br/><div class="children"><div class="content">So click house is a column db. Any thoughts on if the performance would be a wash if you just pivoted the timescale hypertable and indexed the time + column on timescale?</div><br/></div></div><div id="40052438" class="c"><input type="checkbox" id="c-40052438" checked=""/><div class="controls bullet"><span class="by">PolarizedPoutin</span><span>|</span><a href="#40052233">parent</a><span>|</span><a href="#40052614">prev</a><span>|</span><a href="#40052787">next</a><span>|</span><label class="collapse" for="c-40052438">[-]</label><label class="expand" for="c-40052438">[3 more]</label></div><br/><div class="children"><div class="content">Haha very cool use! Yeah reading up on TimescaleDB vs. Clickhouse it seems like columnar storage and Clickhouse will be faster and better compress the time series data. For now I&#x27;m sticking to TimescaleDB to learn Postgres and PostGIS, but might make a TimescaleDB vs. Clickhouse comparison when I switch!</div><br/><div id="40055115" class="c"><input type="checkbox" id="c-40055115" checked=""/><div class="controls bullet"><span class="by">cevian</span><span>|</span><a href="#40052233">root</a><span>|</span><a href="#40052438">parent</a><span>|</span><a href="#40052535">next</a><span>|</span><label class="collapse" for="c-40055115">[-]</label><label class="expand" for="c-40055115">[1 more]</label></div><br/><div class="children"><div class="content">Please note that TimescaleDB also uses columnar storage for its compressed data.<p>Disclosure: I am a TimescaleDB engineer.</div><br/></div></div><div id="40052535" class="c"><input type="checkbox" id="c-40052535" checked=""/><div class="controls bullet"><span class="by">rkwasny</span><span>|</span><a href="#40052233">root</a><span>|</span><a href="#40052438">parent</a><span>|</span><a href="#40055115">prev</a><span>|</span><a href="#40052787">next</a><span>|</span><label class="collapse" for="c-40052535">[-]</label><label class="expand" for="c-40052535">[1 more]</label></div><br/><div class="children"><div class="content">I can replicate your benchmark when I get a moment, the data, is it free to share if I wanted to make an open browser?<p>I have a feeling general public has very limited access to weather data, and graphs in news that state &quot;are we getting hotter on colder&quot; are all sensational and hard to verify.</div><br/></div></div></div></div></div></div><div id="40052787" class="c"><input type="checkbox" id="c-40052787" checked=""/><div class="controls bullet"><span class="by">sigmonsays</span><span>|</span><a href="#40052233">prev</a><span>|</span><label class="collapse" for="c-40052787">[-]</label><label class="expand" for="c-40052787">[2 more]</label></div><br/><div class="children"><div class="content">where can I download the weather data?<p>Is it free or available if I sign up?</div><br/><div id="40054042" class="c"><input type="checkbox" id="c-40054042" checked=""/><div class="controls bullet"><span class="by">roter</span><span>|</span><a href="#40052787">parent</a><span>|</span><label class="collapse" for="c-40054042">[-]</label><label class="expand" for="c-40054042">[1 more]</label></div><br/><div class="children"><div class="content">If I read correctly, they are using the ERA5 dataset [0]. It is freely available and can be downloaded through a Python API called cdsapi.<p>[0] <a href="https:&#x2F;&#x2F;cds.climate.copernicus.eu&#x2F;cdsapp#!&#x2F;dataset&#x2F;reanalysis-era5-single-levels?tab=overview" rel="nofollow">https:&#x2F;&#x2F;cds.climate.copernicus.eu&#x2F;cdsapp#!&#x2F;dataset&#x2F;reanalysi...</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>