<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688374862775" as="style"/><link rel="stylesheet" href="styles.css?v=1688374862775"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://db.cs.cmu.edu/mmap-cidr2022/">Are You Sure You Want to Use MMAP in Your Database Management System? (2022)</a>Â <span class="domain">(<a href="https://db.cs.cmu.edu">db.cs.cmu.edu</a>)</span></div><div class="subtext"><span>nethunters</span> | <span>162 comments</span></div><br/><div><div id="36564309" class="c"><input type="checkbox" id="c-36564309" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36563882">next</a><span>|</span><label class="collapse" for="c-36564309">[-]</label><label class="expand" for="c-36564309">[78 more]</label></div><br/><div class="children"><div class="content">This is a pretty old argument and IMO it&#x27;s far out of date&#x2F;obsolete.<p>Taking full control of your I&#x2F;O and buffer management is great if (a) your developers are all smart and experienced enough to be kernel programmers and (b) your DBMS is the only process running on a machine. In practice, (a) is never true, and (b) is no longer true because everyone is running apps inside containers inside shared VMs. In the modern application&#x2F;server environment, no user level process has accurate information about the total state of the machine, only the kernel (or hypervisor) does and it&#x27;s an exercise in futility to try to manage paging etc at the user level.<p>As Dr. Michael Stonebraker put it: The Traditional RDBMS Wisdom is (Almost Certainly) All Wrong. <a href="https:&#x2F;&#x2F;slideshot.epfl.ch&#x2F;play&#x2F;suri_stonebraker" rel="nofollow noreferrer">https:&#x2F;&#x2F;slideshot.epfl.ch&#x2F;play&#x2F;suri_stonebraker</a> (See the slide at 21:25 into the video). Modern DBMSs spend 96% of their time managing buffers and locks, and only 4% doing actual useful work for the caller.<p>Granted, even using mmap you still need to know wtf you&#x27;re doing. MongoDB&#x27;s original mmap backing store was a poster child for Doing It Wrong, getting all of the reliability problems and none of the performance benefits. LMDB is an example of doing it right: perfect crash-proof reliability, and perfect linear read scalability across arbitrarily many CPUs with zero-copy reads and no wasted effort, and a hot code path that fits into a CPU&#x27;s 32KB L1 instruction cache.</div><br/><div id="36565886" class="c"><input type="checkbox" id="c-36565886" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36564309">parent</a><span>|</span><a href="#36565504">next</a><span>|</span><label class="collapse" for="c-36565886">[-]</label><label class="expand" for="c-36565886">[11 more]</label></div><br/><div class="children"><div class="content">&gt; your DBMS is the only process running on a machine. In practice, (a) is never true, and (b) is no longer true because everyone is running apps inside containers inside shared VMs.<p>There&#x27;s nothing special about kernel programmers.  In fact, if I had to compare, I&#x27;d go with storage people being the more experienced &#x2F; knowledgeable ones.  They have a highly competitive environment, which requires a lot more understanding and inventiveness to succeed, whereas kernel programmers proper don&#x27;t compete -- Linux won many years ago.  Kernel programmers who deal with stuff like drivers or various &quot;extensions&quot; are, largely, in the same group as storage (often time literally the same people).<p>As for &quot;single process&quot; argument... well, if you run a database inside an OS, then, obviously, that will never happen as OS has its own processes to run.  But, if you ignore that -- no DBA worth their salt would put database in the environment where it has to share resources with applications.  People who do that are, probably, Web developers who don&#x27;t have high expectations from their database anyways and would have no idea how to configure &#x2F; tune it for high performance, so, it doesn&#x27;t matter how they run it, they aren&#x27;t the target audience -- they are light years behind on what&#x27;s possible to achieve with their resources.<p>This has nothing to do with mmap though. mmap shouldn&#x27;t be used for storage applications for other reasons. mmap doesn&#x27;t allow their users to precisely control the persistence aspect... which is kind of the central point of databases.  So, it&#x27;s a mostly worthless tool in that context.  Maybe fine for some throw-away work, but definitely not for storing users&#x27; data or database&#x27;s own data.</div><br/><div id="36566050" class="c"><input type="checkbox" id="c-36566050" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565886">parent</a><span>|</span><a href="#36565504">next</a><span>|</span><label class="collapse" for="c-36566050">[-]</label><label class="expand" for="c-36566050">[10 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s nothing special about kernel programmers.<p>Yes, that was a shorthand generalization for &quot;people who&#x27;ve studied computer architecture&quot; - which most application developers never have.<p>&gt; no DBA worth their salt would put database in the environment where it has to share resources with applications.<p>Most applications today are running on smartphones&#x2F;mobile devices. That means they&#x27;re running with local embedded databases - it&#x27;s all about &quot;edge computing&quot;. There&#x27;s far more DBs in use in the world than there are DBAs managing them.<p>&gt; mmap shouldn&#x27;t be used for storage applications for other reasons. mmap doesn&#x27;t allow their users to precisely control the persistence aspect... which is kind of the central point of databases. So, it&#x27;s a mostly worthless tool in that context. Maybe fine for some throw-away work, but definitely not for storing users&#x27; data or database&#x27;s own data.<p>Well, you&#x27;re half right. That&#x27;s why by default LMDB uses a read-only mmap and uses regular (p)write syscalls for writes. But the central point of databases is to be able to persist data <i>such that it can be retrieved again in the future, efficiently</i>. And that&#x27;s where the read characteristics of using mmap are superior.</div><br/><div id="36567587" class="c"><input type="checkbox" id="c-36567587" checked=""/><div class="controls bullet"><span class="by">Johnny555</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566050">parent</a><span>|</span><a href="#36566926">prev</a><span>|</span><a href="#36566534">next</a><span>|</span><label class="collapse" for="c-36567587">[-]</label><label class="expand" for="c-36567587">[5 more]</label></div><br/><div class="children"><div class="content"><i>mmap doesn&#x27;t allow their users to precisely control the persistence aspect</i><p>It&#x27;s been a while since I&#x27;ve dealt with mmap(), but isn&#x27;t this what msync() does? You can synchronously or asynchronously force dirty pages to be flushed to disk without waiting until munmap().</div><br/><div id="36567667" class="c"><input type="checkbox" id="c-36567667" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567587">parent</a><span>|</span><a href="#36566534">next</a><span>|</span><label class="collapse" for="c-36567667">[-]</label><label class="expand" for="c-36567667">[4 more]</label></div><br/><div class="children"><div class="content">msync lets you force a flush so you can control the latest possible moment for a writeout. But the OS can flush before that, and you have no way to detect or control that. So you can only control the late side of the timing, not the early side. And in databases, you usually need writes to be persisted in a specific order; early writes are just as harmful as late writes.</div><br/><div id="36567958" class="c"><input type="checkbox" id="c-36567958" checked=""/><div class="controls bullet"><span class="by">colanderman</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567667">parent</a><span>|</span><a href="#36566534">next</a><span>|</span><label class="collapse" for="c-36567958">[-]</label><label class="expand" for="c-36567958">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;d even take a memory ordering guarantee, something like, within each page, data is read out sequentially as atomic aligned 64-bit reads with acquire ordering. (Though this <i>probably</i> is what you get on AMD64.) As-is, there&#x27;s not even a guarantee against an atomic aligned write being torn when written out.</div><br/><div id="36569806" class="c"><input type="checkbox" id="c-36569806" checked=""/><div class="controls bullet"><span class="by">ayende</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567958">parent</a><span>|</span><a href="#36566534">next</a><span>|</span><label class="collapse" for="c-36569806">[-]</label><label class="expand" for="c-36569806">[2 more]</label></div><br/><div class="children"><div class="content">That is absolutely <i>not</i> what you actually get from the hardware.<p>For fun, there is <i>no</i> guarantee in terms of writing a page in what order it is written. SQLite documents that they <i>assume</i> (but cannot verify) that _sector_ writes are linear, but not atomic.
<a href="https:&#x2F;&#x2F;www.sqlite.org&#x2F;atomiccommit.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sqlite.org&#x2F;atomiccommit.html</a><p>&gt;  If a power failure occurs in the middle of a sector write it might be that part of the sector was modified and another part was left unchanged. The key assumption by SQLite is that if any part of the sector gets changed, then either the first or the last bytes will be changed. So the hardware will never start writing a sector in the middle and work towards the ends. We do not know if this assumption is always true but it seems reasonable.<p>You are talking several levels higher than that, at the page level (composed of multiple sectors).<p>Assume that they reside in _different_ physical locations, and are written at different times. That&#x27;s fun.</div><br/><div id="36570264" class="c"><input type="checkbox" id="c-36570264" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36569806">parent</a><span>|</span><a href="#36566534">next</a><span>|</span><label class="collapse" for="c-36570264">[-]</label><label class="expand" for="c-36570264">[1 more]</label></div><br/><div class="children"><div class="content">Every HDD since the 1980s has guaranteed atomic sector writes:<p>&gt; Currently all hard drive&#x2F;SSD manufacturers guarantee that 512 byte sector
 writes are atomic. As such, failure to write the 106 byte header is not
 something we account for in current LMDB releases. Also, failures of this type
 should result in ECC errors in the disk sector - it should be impossible to
 successfully read a sector that was written incorrectly in the ways you describe.<p>Even in extreme cases, the probability of failure to write the leading 128 out
 of 512 bytes of a sector is nearly nil - even on very old hard drives, before
 512-byte sector write guarantees. We would have to go back nearly 30 years to
 find such a device, e.g.<p><a href="https:&#x2F;&#x2F;archive.org&#x2F;details&#x2F;bitsavers_quantumQuaroductManualJun88_6537414" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.org&#x2F;details&#x2F;bitsavers_quantumQuaroductManual...</a><p>Page 23, Section 2.1
 &quot;No damage or loss of data will occur if power is applied or removed during
 drive operation, except that data may be lost in the sector being written at
 the time of power loss.&quot;<p><pre><code>  From the specs on page 15, the data transfer rate to&#x2F;from the platters is
 1.25MB&#x2F;sec, so the time to write one full sector is 0.4096ms; the time to
 write the leading 128 bytes of the sector is thus 1&#x2F;4 of that: 0.10ms. You
 would have to be very very unlucky to have a power failure hit the drive
 within this .1ms window of time. Fast-forward to present day and it&#x27;s simply
 not an issue.
</code></pre>
^ above quoted from <a href="https:&#x2F;&#x2F;lists.openldap.org&#x2F;hyperkitty&#x2F;list&#x2F;openldap-devel@openldap.org&#x2F;message&#x2F;XXA2SN6HZ2FXDSE73GYONWKDJHLIB5RT&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lists.openldap.org&#x2F;hyperkitty&#x2F;list&#x2F;openldap-devel@op...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="36566534" class="c"><input type="checkbox" id="c-36566534" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566050">parent</a><span>|</span><a href="#36567587">prev</a><span>|</span><a href="#36565504">next</a><span>|</span><label class="collapse" for="c-36566534">[-]</label><label class="expand" for="c-36566534">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Most applications today are running on smartphones&#x2F;mobile devices.<p>That&#x27;s patently false. There are about 8 bn. people.  Even if everyone has a smartphone or two, it&#x27;s nothing compared to the total of all devices that can be called &quot;computer&quot;.  I think that &quot;smart TV&quot; alone will beat the number of smartphones.  But even that is a drop in a bucket when it comes to the total of running programs on Earth &#x2F; its orbit.<p>But, that&#x27;s beside the point. Smartphones aren&#x27;t designed to run database servers.  Even if they indeed were the majority, they&#x27;d still be irrelevant for this conversation because they are a wrong platform for deploying databases.  In other words, it doesn&#x27;t matter how people deploy databases to smartphones -- they have no hopes of achieving good performance, and whether they use mmap or not is of no consequences -- they&#x27;ve lost the race before they even qualified for it.<p>&gt; LMDB<p>Are we talking about this? <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lightning_Memory-Mapped_Database" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lightning_Memory-Mapped_Databa...</a> If so, this is irrelevant for databases in general.<p>&gt; LMDB databases may have only one writer at a time<p>(Taken from the page above) -- this isn&#x27;t a serious contender for database server space.  It&#x27;s a toy database.  You shouldn&#x27;t give general advice based on whatever this system does or doesn&#x27;t.</div><br/><div id="36569324" class="c"><input type="checkbox" id="c-36569324" checked=""/><div class="controls bullet"><span class="by">ricardo81</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566534">parent</a><span>|</span><a href="#36569693">next</a><span>|</span><label class="collapse" for="c-36569324">[-]</label><label class="expand" for="c-36569324">[1 more]</label></div><br/><div class="children"><div class="content">&gt;irrelevant for databases in general<p>It&#x27;s one of the databases compared in the paper<p>OP is one of the authors of LMDB</div><br/></div></div><div id="36569693" class="c"><input type="checkbox" id="c-36569693" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566534">parent</a><span>|</span><a href="#36569324">prev</a><span>|</span><a href="#36565504">next</a><span>|</span><label class="collapse" for="c-36569693">[-]</label><label class="expand" for="c-36569693">[1 more]</label></div><br/><div class="children"><div class="content">Smart TVs are also all running SQLite</div><br/></div></div></div></div></div></div></div></div><div id="36565504" class="c"><input type="checkbox" id="c-36565504" checked=""/><div class="controls bullet"><span class="by">gavinray</span><span>|</span><a href="#36564309">parent</a><span>|</span><a href="#36565886">prev</a><span>|</span><a href="#36568460">next</a><span>|</span><label class="collapse" for="c-36565504">[-]</label><label class="expand" for="c-36565504">[32 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, how many databases have you written?<p>This is co-authored by Pavlo, Viktor Leiss, with feedback from Neumann. I&#x27;m sorry, but if someone on the internet claims to know better than those 3, you&#x27;re going to need some monumental evidence of your credibility.<p>Additionally, what you link here:<p><pre><code>  &gt; ... (See the slide at 21:25 into the video). Modern DBMSs spend 96% of their time managing buffers and locks, and only 4% doing actual useful work for the caller.
</code></pre>
Is discussing &quot;Main Memory&quot; databases. These databases do no I&#x2F;O outside of potential initial reads, because all of the data fits in-memory!<p>These databases represent a small portion of contemporary DBMS usage when compared to traditional RDBMS.<p>All you have to do is look at the bandwidth and reads&#x2F;sec from the paper when using O_DIRECT &quot;pread()&quot;s versus mmap&#x27;ed IO.</div><br/><div id="36565661" class="c"><input type="checkbox" id="c-36565661" checked=""/><div class="controls bullet"><span class="by">LAC-Tech</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565504">parent</a><span>|</span><a href="#36567649">next</a><span>|</span><label class="collapse" for="c-36565661">[-]</label><label class="expand" for="c-36565661">[30 more]</label></div><br/><div class="children"><div class="content">This is a classic appeal to authority. Let&#x27;s play the argument, not the man.<p>(My understanding is that the GP wrote LMDB, works on openLDAP, and was a maintainer for BerkelyDB for a number of years. But even if he&#x27;d only written &#x27;hello, world!&#x27; I&#x27;m much more interested in the specific arguments).</div><br/><div id="36566074" class="c"><input type="checkbox" id="c-36566074" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565661">parent</a><span>|</span><a href="#36566403">next</a><span>|</span><label class="collapse" for="c-36566074">[-]</label><label class="expand" for="c-36566074">[3 more]</label></div><br/><div class="children"><div class="content">Correct, and thank you. I wrote LMDB, wrote a lot of OpenLDAP, and worked on BerkeleyDB for many years. And actually Andy Pavlo invited me to CMU to give a lecture on LMDB a few years back. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tEa5sAh-kVk">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tEa5sAh-kVk</a><p>Andy and I have had this debate going for a long time already.</div><br/><div id="36566179" class="c"><input type="checkbox" id="c-36566179" checked=""/><div class="controls bullet"><span class="by">gavinray</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566074">parent</a><span>|</span><a href="#36566403">next</a><span>|</span><label class="collapse" for="c-36566179">[-]</label><label class="expand" for="c-36566179">[2 more]</label></div><br/><div class="children"><div class="content">Well, I eat my shorts.<p>Isn&#x27;t LMBD closer to an embedded key-value store than an RDBMS, though? Also there&#x27;s a section in the paper that mentions it&#x27;s single-writer.</div><br/><div id="36566203" class="c"><input type="checkbox" id="c-36566203" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566179">parent</a><span>|</span><a href="#36566403">next</a><span>|</span><label class="collapse" for="c-36566203">[-]</label><label class="expand" for="c-36566203">[1 more]</label></div><br/><div class="children"><div class="content">Yes, LMDB is an embedded key&#x2F;value store but it can be used as the backing store of any other DB model you care for. E.g. as a backend to MySQL, or SQLite, or OpenLDAP, or whatever.</div><br/></div></div></div></div></div></div><div id="36566403" class="c"><input type="checkbox" id="c-36566403" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565661">parent</a><span>|</span><a href="#36566074">prev</a><span>|</span><a href="#36566107">next</a><span>|</span><label class="collapse" for="c-36566403">[-]</label><label class="expand" for="c-36566403">[5 more]</label></div><br/><div class="children"><div class="content">I think the real argument is more nuanced. Where you see mmap() fail badly on Linux, even for read-only workloads, is under a few specific conditions: very large storage volumes, highly concurrent access, non-trivial access patterns (e.g. high-dimensionality access methods). Most people do not operate data models under these conditions, but if you do then you can achieve large integer factor gains in throughput by not using mmap().<p>Interestingly, most of the reason for these problems has to do with theoretical limitations of cache replacement algorithms as drivers of I&#x2F;O scheduling. There are alternative approaches to scheduling I&#x2F;O that work much better in these cases but mmap() canât express them, so in those cases bypassing mmap() offers large gains.</div><br/><div id="36567136" class="c"><input type="checkbox" id="c-36567136" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566403">parent</a><span>|</span><a href="#36567332">next</a><span>|</span><label class="collapse" for="c-36567136">[-]</label><label class="expand" for="c-36567136">[2 more]</label></div><br/><div class="children"><div class="content">GP wrote a key-value store called LMDB that is constrained to a single writer, and often used for small databases that fit entirely in memory but need to persist to disk.  There&#x27;s a whole different world for more scalable databases.</div><br/><div id="36567590" class="c"><input type="checkbox" id="c-36567590" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567136">parent</a><span>|</span><a href="#36567332">next</a><span>|</span><label class="collapse" for="c-36567590">[-]</label><label class="expand" for="c-36567590">[1 more]</label></div><br/><div class="children"><div class="content">&quot;fit entirely in memory&quot; is not a requirement. LMDB is not a main-memory database, it is an on-disk database that uses memory mapping.</div><br/></div></div></div></div><div id="36567332" class="c"><input type="checkbox" id="c-36567332" checked=""/><div class="controls bullet"><span class="by">LAC-Tech</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566403">parent</a><span>|</span><a href="#36567136">prev</a><span>|</span><a href="#36567663">next</a><span>|</span><label class="collapse" for="c-36567332">[-]</label><label class="expand" for="c-36567332">[1 more]</label></div><br/><div class="children"><div class="content">Can you explain &quot;high-dimensionality access methods&quot; to me? (Or if it&#x27;s too big for an HN comment, maybe recommend a paper).</div><br/></div></div><div id="36567663" class="c"><input type="checkbox" id="c-36567663" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566403">parent</a><span>|</span><a href="#36567332">prev</a><span>|</span><a href="#36566107">next</a><span>|</span><label class="collapse" for="c-36567663">[-]</label><label class="expand" for="c-36567663">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d imagine same kind of worst case access would also be a problem doing IO the &quot;classical&quot; way</div><br/></div></div></div></div><div id="36566107" class="c"><input type="checkbox" id="c-36566107" checked=""/><div class="controls bullet"><span class="by">gavinray</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565661">parent</a><span>|</span><a href="#36566403">prev</a><span>|</span><a href="#36566322">next</a><span>|</span><label class="collapse" for="c-36566107">[-]</label><label class="expand" for="c-36566107">[20 more]</label></div><br/><div class="children"><div class="content">The argument is that:<p>- Queries can trigger blocking page faults when accessing (transparently) evicted pages, causing unexpected I&#x2F;O stalls<p>- mmap() complicates transactionality and error-handling<p>- Page table contention, single-threaded page eviction, and TLB shootdowns become bottlenecks</div><br/><div id="36566186" class="c"><input type="checkbox" id="c-36566186" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566107">parent</a><span>|</span><a href="#36566322">next</a><span>|</span><label class="collapse" for="c-36566186">[-]</label><label class="expand" for="c-36566186">[19 more]</label></div><br/><div class="children"><div class="content">1 - for reading any uncached data, the I&#x2F;O stalls are unavoidable. Whatever client requested that data is going to have to wait regardless.<p>2 - complexity? this is simply false. LMDB&#x27;s ACID txns using MVCC are much simpler than any &quot;traditional&quot; approach.<p>3 - contention is a red herring since this approach is already single-writer, as is common for most embedded k&#x2F;v stores these days. You lose more perf by trying to make the write path multi-threaded, in lock contention and cache thrashing.</div><br/><div id="36567192" class="c"><input type="checkbox" id="c-36567192" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566186">parent</a><span>|</span><a href="#36566322">next</a><span>|</span><label class="collapse" for="c-36567192">[-]</label><label class="expand" for="c-36567192">[18 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kind of disingenuous to talk about how great your concurrency system is when you only allow a single writer.  RCU (which I imagine your system is isomorphic to) is pretty simple compared to what many DB engines use to do ACID transactions that involve both reads and writes.</div><br/><div id="36567625" class="c"><input type="checkbox" id="c-36567625" checked=""/><div class="controls bullet"><span class="by">jeffffff</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567192">parent</a><span>|</span><a href="#36567483">next</a><span>|</span><label class="collapse" for="c-36567625">[-]</label><label class="expand" for="c-36567625">[7 more]</label></div><br/><div class="children"><div class="content">take a look at <a href="http:&#x2F;&#x2F;nms.csail.mit.edu&#x2F;~stavros&#x2F;pubs&#x2F;OLTP_sigmod08.pdf" rel="nofollow noreferrer">http:&#x2F;&#x2F;nms.csail.mit.edu&#x2F;~stavros&#x2F;pubs&#x2F;OLTP_sigmod08.pdf</a> - the overhead of coordinating multiple writers often makes multi-writer databases slower than single-writer databases. remember, everything has to be serialized when it goes to the write ahead log, so as long as you can do the database updates as fast as you can write to the log then concurrent writers are of no benefit.</div><br/><div id="36567763" class="c"><input type="checkbox" id="c-36567763" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567625">parent</a><span>|</span><a href="#36567483">next</a><span>|</span><label class="collapse" for="c-36567763">[-]</label><label class="expand" for="c-36567763">[6 more]</label></div><br/><div class="children"><div class="content">This is another cool example of a toy database that is again very small:<p>&gt; The database size for one warehouse is
approximately 100 MB (we experiment with five warehouses for
a total size of 500MB).<p>It is not surprising that when your database basically fits in RAM, serializing on one writer is worth doing, because it just plainly reduces contention.  You basically gain nothing in a DB engine from multi-writer transactions when this is the case.  A large part of a write (the vast majority of write latency) in many systems with a large database comes from reading the index up to the point where you plan to write.  If that tree is in RAM, there is no work here, and you instead incur overhead on consistency of that tree by having multiple writers.<p>I&#x27;m not suggesting that these results are useless.  They are useful <i>for people whose databases are small</i> because they are meaningfully better than RocksDB&#x2F;LevelDB which implicitly assume that your database is a *lot* bigger than RAM.</div><br/><div id="36569826" class="c"><input type="checkbox" id="c-36569826" checked=""/><div class="controls bullet"><span class="by">ayende</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567763">parent</a><span>|</span><a href="#36567796">next</a><span>|</span><label class="collapse" for="c-36569826">[-]</label><label class="expand" for="c-36569826">[1 more]</label></div><br/><div class="children"><div class="content">I am using the same rough model, and I&#x27;m using that on a 1.5 TB db running on Raspberry PI very successfully.<p>Pretty much all storage libraries written in the past couple of decades are using single writer. Note that single writer doesn&#x27;t mean single transaction. Merging transactions is easy and highly profitable, after all.</div><br/></div></div><div id="36567796" class="c"><input type="checkbox" id="c-36567796" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567763">parent</a><span>|</span><a href="#36569826">prev</a><span>|</span><a href="#36567483">next</a><span>|</span><label class="collapse" for="c-36567796">[-]</label><label class="expand" for="c-36567796">[4 more]</label></div><br/><div class="children"><div class="content">&gt; RocksDB&#x2F;LevelDB which implicitly assume that your database is a <i>lot</i> bigger than RAM.<p>Where are you getting that assumption from? LevelDB was built to be used in Google Chrome, not for multi-TB DBs. RocksDB was optimized specifically for in-memory workloads.</div><br/><div id="36567830" class="c"><input type="checkbox" id="c-36567830" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567796">parent</a><span>|</span><a href="#36567483">next</a><span>|</span><label class="collapse" for="c-36567830">[-]</label><label class="expand" for="c-36567830">[3 more]</label></div><br/><div class="children"><div class="content">I worked with the Bigtable folks at Google.  LevelDB&#x27;s design is ripped straight from BigTable, which was designed with that assumption in mind.  I&#x27;m also pretty sure it was not designed specifically for Google Chrome&#x27;s use case - it was written to be a general key-value storage engine based on BigTable, and Google Chrome was the first customer.<p>RocksDB is Facebook&#x27;s offshoot of LevelDB, basically keeping the core architecture of the storage engine (but multithreading it), and is used internally at Facebook as the backing store for many of their database systems.  I have never heard from anyone that RocksDB was optimized for in-memory workloads at all, and I think most benchmarks can conclusively say the opposite: both of those DB engines are pretty bad for workloads that fit in memory.</div><br/><div id="36570134" class="c"><input type="checkbox" id="c-36570134" checked=""/><div class="controls bullet"><span class="by">earthnail</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567830">parent</a><span>|</span><a href="#36567922">next</a><span>|</span><label class="collapse" for="c-36570134">[-]</label><label class="expand" for="c-36570134">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used RocksDB for an in-memory K&#x2F;V store of ~600GB in size and it worked really well. Not saying it&#x27;s the best choice out there but it did the job very well for us. And in particular because our dataset was always growing and we needed the option to fallback to disk if needed, RocksDB worked very well.<p>Was a PITA to optimise though; tons of options and little insight into which ones work.</div><br/></div></div><div id="36567922" class="c"><input type="checkbox" id="c-36567922" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567830">parent</a><span>|</span><a href="#36570134">prev</a><span>|</span><a href="#36567483">next</a><span>|</span><label class="collapse" for="c-36567922">[-]</label><label class="expand" for="c-36567922">[1 more]</label></div><br/><div class="children"><div class="content">I think we&#x27;ve gone off on a tangent. At any rate, both LevelDB and RocksDB are still single-writer so whatever point seems to have been lost along the way.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36567483" class="c"><input type="checkbox" id="c-36567483" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567192">parent</a><span>|</span><a href="#36567625">prev</a><span>|</span><a href="#36568550">next</a><span>|</span><label class="collapse" for="c-36567483">[-]</label><label class="expand" for="c-36567483">[9 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need more than single-writer concurrency if your write txns are fast enough.<p>Our experience with OpenLDAP was that multi-writer concurrency cost too much overhead. Even though you may be writing primary records to independent regions of the DB, if you&#x27;re indexing any of that data (which all real DBs do, for query perf) you wind up getting a lot of contention in the indices. That leads to row locking conflicts, txn rollbacks, and retries. With a single writer txn model, you never get conflicts, never need rollbacks.</div><br/><div id="36568154" class="c"><input type="checkbox" id="c-36568154" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567483">parent</a><span>|</span><a href="#36567513">next</a><span>|</span><label class="collapse" for="c-36568154">[-]</label><label class="expand" for="c-36568154">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You don&#x27;t need more than single-writer concurrency if your write txns are fast enough.<p>This only works on systems with sufficiently slow storage. If your server has a bunch of NVMe, which is a pretty normal database config these days, you will be hard-pressed to get anywhere close to the theoretical throughput of the storage with a single writer. That requires 10+ GB&#x2F;s sustained. It is a piece of cake with multiple writers and a good architecture.<p>Writes through indexing can be sustained at this rate (assuming appropriate data structures), most of the technical challenge is driving the network at the necessary rate in my experience.</div><br/><div id="36570312" class="c"><input type="checkbox" id="c-36570312" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36568154">parent</a><span>|</span><a href="#36567513">next</a><span>|</span><label class="collapse" for="c-36570312">[-]</label><label class="expand" for="c-36570312">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s all just false. Just because you&#x27;re single-writer at the application level doesn&#x27;t mean the OS isn&#x27;t queueing enough writes to saturate storage at the device level. We&#x27;ve benchmarked plenty of high speed NVMe devices, like Intel Optane SSDs, etc. showing this.
<a href="http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;optanessd&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;optanessd&#x2F;</a></div><br/></div></div></div></div><div id="36567513" class="c"><input type="checkbox" id="c-36567513" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567483">parent</a><span>|</span><a href="#36568154">prev</a><span>|</span><a href="#36568550">next</a><span>|</span><label class="collapse" for="c-36567513">[-]</label><label class="expand" for="c-36567513">[6 more]</label></div><br/><div class="children"><div class="content">That&#x27;s probably because your OpenLDAP benchmarks used a tiny database.  If you have multi-terabyte databases, you will start to see huge gains from a multi-writer setup because you will be regularly be loading pages from disk, rather than keeping almost all of your btree&#x2F;LSM tree in RAM.</div><br/><div id="36567540" class="c"><input type="checkbox" id="c-36567540" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567513">parent</a><span>|</span><a href="#36568550">next</a><span>|</span><label class="collapse" for="c-36567540">[-]</label><label class="expand" for="c-36567540">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, no. Not with a DB 50x larger than RAM, anyway.<p><a href="http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;hyperdex&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;hyperdex&#x2F;</a><p>RAM is relatively cheap too, there&#x27;s no real reason to be running multi-TB databases at greater than a 50x ratio.</div><br/><div id="36567643" class="c"><input type="checkbox" id="c-36567643" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567540">parent</a><span>|</span><a href="#36568550">next</a><span>|</span><label class="collapse" for="c-36567643">[-]</label><label class="expand" for="c-36567643">[4 more]</label></div><br/><div class="children"><div class="content">Sorry, but &quot;50x larger than RAM&quot; is a pretty small DB - that&#x27;s an 800 GB database on a machine with 16 GB of RAM.  I usually have seen machines with 500-1000x ratios of flash to RAM.  &quot;RAM is relatively cheap&quot; is also false when you&#x27;re storing truly huge amounts of data, which is how the systems you compare yourself to (LevelDB, etc) are usually deployed.  Note that RAM is now the single greatest cost when buying servers.<p>&gt; Now that the total database is 50 times larger than RAM, around half of the key lookups will require a disk I&#x2F;O.<p>That is an insanely high cache hit rate, which should have probably set off your &quot;unrepresentative benchmark&quot; detector.  I am also a little surprised at the lack of a random writes benchmark.  I get that this is marketing material, though.</div><br/><div id="36567700" class="c"><input type="checkbox" id="c-36567700" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567643">parent</a><span>|</span><a href="#36568550">next</a><span>|</span><label class="collapse" for="c-36567700">[-]</label><label class="expand" for="c-36567700">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I am also a little surprised at the lack of a random writes benchmark.<p>Eh? This was 20% random writes, 80% random reads. LMDB is for read-heavy workloads.<p>&gt; That is an insanely high cache hit rate, which should have probably set off your &quot;unrepresentative benchmark&quot; detector.<p>No, that is normal for a B+tree; the root page and most of the branch pages will always be in cache. This is why you can get excellent efficiency and performance from a DB without tuning to a specific workload.</div><br/><div id="36567774" class="c"><input type="checkbox" id="c-36567774" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567700">parent</a><span>|</span><a href="#36568550">next</a><span>|</span><label class="collapse" for="c-36567774">[-]</label><label class="expand" for="c-36567774">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Eh? This was 20% random writes, 80% random reads. LMDB is for read-heavy workloads.<p>The page says &quot;updates,&quot; not &quot;writes.&quot;  Updates are a constrained form of write where you are writing to an existing key.  Updates, importantly, do not affect your index structure, while writes do.<p>&gt; No, that is normal for a B+tree; the root page and most of the branch pages will always be in cache. This is why you can get excellent efficiency and performance from a DB without tuning to a specific workload.<p>It is normal for a <i>small</i> B+tree relative to the memory size available on the machine.  The &quot;small&quot; was the unrepresentative part of the benchmark, not the &quot;B+tree.&quot;</div><br/><div id="36567892" class="c"><input type="checkbox" id="c-36567892" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567774">parent</a><span>|</span><a href="#36568550">next</a><span>|</span><label class="collapse" for="c-36567892">[-]</label><label class="expand" for="c-36567892">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The page says &quot;updates,&quot; not &quot;writes.&quot; Updates are a constrained form of write where you are writing to an existing key. Updates, importantly, do not affect your index structure, while writes do.<p>OK, I see your point. It would only have made things even worse for LevelDB here to do an Add&#x2F;Delete workload because its garbage compaction passes would have had to do a lot more work.<p>&gt; It is normal for a small B+tree relative to the memory size available on the machine. The &quot;small&quot; was the unrepresentative part of the benchmark, not the &quot;B+tree.&quot;<p>This was 100 million records, and a 5-level deep tree. To get to 6 levels deep it would be about 10 billion records. Most of the branch pages would still fit in RAM; most queries would require at most 1 more I&#x2F;O than the 5-level case. The cost is still better than any other approach.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36568550" class="c"><input type="checkbox" id="c-36568550" checked=""/><div class="controls bullet"><span class="by">AdamProut</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567192">parent</a><span>|</span><a href="#36567483">prev</a><span>|</span><a href="#36566322">next</a><span>|</span><label class="collapse" for="c-36568550">[-]</label><label class="expand" for="c-36568550">[1 more]</label></div><br/><div class="children"><div class="content">Yeah for workloads with any long running write transactions a single writer design is a pretty big limitation. Say some long running data load (or a big bulk deletion) running along with some faster high throughput key value writes - the big data load would block all the faster key-value writes when it runs.<p>No &quot;mainstream&quot; database I&#x27;m aware of has a global single writer design.</div><br/></div></div></div></div></div></div></div></div><div id="36566322" class="c"><input type="checkbox" id="c-36566322" checked=""/><div class="controls bullet"><span class="by">jemfinch</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565661">parent</a><span>|</span><a href="#36566107">prev</a><span>|</span><a href="#36567649">next</a><span>|</span><label class="collapse" for="c-36566322">[-]</label><label class="expand" for="c-36566322">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Taking full control of your I&#x2F;O and buffer management is great if (a) your developers are all smart and experienced enough to be kernel programmers&quot; is already an appeal to authority in itself.<p>We shouldn&#x27;t apply a higher bar to the counterargument than we applied to the argument in the first place.</div><br/></div></div></div></div><div id="36567649" class="c"><input type="checkbox" id="c-36567649" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565504">parent</a><span>|</span><a href="#36565661">prev</a><span>|</span><a href="#36568460">next</a><span>|</span><label class="collapse" for="c-36567649">[-]</label><label class="expand" for="c-36567649">[1 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, do you have anything actually useful to add or are just throwing appeals to authority because you don&#x27;t ?</div><br/></div></div></div></div><div id="36568460" class="c"><input type="checkbox" id="c-36568460" checked=""/><div class="controls bullet"><span class="by">sakras</span><span>|</span><a href="#36564309">parent</a><span>|</span><a href="#36565504">prev</a><span>|</span><a href="#36565060">next</a><span>|</span><label class="collapse" for="c-36568460">[-]</label><label class="expand" for="c-36568460">[1 more]</label></div><br/><div class="children"><div class="content">Can you comment on what the paper gets wrong? It says that scalability with mmap is poor due to page table contention and others. How does LMDB manage to scale well with mmap? Is page table contention just not an issue in practice?</div><br/></div></div><div id="36565060" class="c"><input type="checkbox" id="c-36565060" checked=""/><div class="controls bullet"><span class="by">tadfisher</span><span>|</span><a href="#36564309">parent</a><span>|</span><a href="#36568460">prev</a><span>|</span><a href="#36567333">next</a><span>|</span><label class="collapse" for="c-36565060">[-]</label><label class="expand" for="c-36565060">[2 more]</label></div><br/><div class="children"><div class="content">Maybe someone should pull LMDB&#x27;s mmap&#x2F;paging system into a usable library. I&#x27;d love to use the k&#x2F;v store part of course, but I keep hitting the default key size limitation and would prefer not to link statically.</div><br/><div id="36566088" class="c"><input type="checkbox" id="c-36566088" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565060">parent</a><span>|</span><a href="#36567333">next</a><span>|</span><label class="collapse" for="c-36566088">[-]</label><label class="expand" for="c-36566088">[1 more]</label></div><br/><div class="children"><div class="content">It wouldn&#x27;t be much use without the B+tree as well; it&#x27;s the B+tree&#x27;s cache friendliness that allows applications to run so efficiently without the OS knowing any specifics of the app&#x27;s usage patterns.</div><br/></div></div></div></div><div id="36567333" class="c"><input type="checkbox" id="c-36567333" checked=""/><div class="controls bullet"><span class="by">ori_b</span><span>|</span><a href="#36564309">parent</a><span>|</span><a href="#36565060">prev</a><span>|</span><a href="#36566978">next</a><span>|</span><label class="collapse" for="c-36567333">[-]</label><label class="expand" for="c-36567333">[3 more]</label></div><br/><div class="children"><div class="content">Do you have benchmarks of lmdb when the working set is much larger than memory? I couldn&#x27;t find any.<p>In my experience -- and in line with the article -- mmap works fine with small working sets. It seems that most benchmarks of lmdb have relatively small data sets.</div><br/><div id="36567522" class="c"><input type="checkbox" id="c-36567522" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567333">parent</a><span>|</span><a href="#36566978">next</a><span>|</span><label class="collapse" for="c-36567522">[-]</label><label class="expand" for="c-36567522">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Do you have benchmarks of lmdb when the working set is much larger than memory? I couldn&#x27;t find any.<p>Where did you look? This is a sample using DB 5x and 50x larger than RAM 
<a href="http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;hyperdex&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;hyperdex&#x2F;</a><p>There are plenty of other larger-than-RAM benchmarks there.</div><br/><div id="36567973" class="c"><input type="checkbox" id="c-36567973" checked=""/><div class="controls bullet"><span class="by">ori_b</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567522">parent</a><span>|</span><a href="#36566978">next</a><span>|</span><label class="collapse" for="c-36567973">[-]</label><label class="expand" for="c-36567973">[1 more]</label></div><br/><div class="children"><div class="content">Hm. That seems to be comparing against a 2013 era leveldb, which at the time also used mmap. (It&#x27;s since switched the default for performance reasons)<p>It&#x27;s also strange to me that there&#x27;s no transition in performance when the data set size grows beyond cache.</div><br/></div></div></div></div></div></div><div id="36566978" class="c"><input type="checkbox" id="c-36566978" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#36564309">parent</a><span>|</span><a href="#36567333">prev</a><span>|</span><a href="#36564527">next</a><span>|</span><label class="collapse" for="c-36566978">[-]</label><label class="expand" for="c-36566978">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Taking full control of your I&#x2F;O and buffer management is great if (a) your developers are all smart and experienced enough to be kernel programmers and (b) your DBMS is the only process running on a machine. In practice, (a) is never true, and (b) is no longer true because everyone is running apps inside containers inside shared VMs.<p>The article is about DBMS developers. For DBMS developers, &quot;in practice&quot; (a) and (b) are usually true I think.</div><br/></div></div><div id="36564527" class="c"><input type="checkbox" id="c-36564527" checked=""/><div class="controls bullet"><span class="by">danappelxx</span><span>|</span><a href="#36564309">parent</a><span>|</span><a href="#36566978">prev</a><span>|</span><a href="#36563882">next</a><span>|</span><label class="collapse" for="c-36564527">[-]</label><label class="expand" for="c-36564527">[27 more]</label></div><br/><div class="children"><div class="content">Who is deploying databases in containers?</div><br/><div id="36564571" class="c"><input type="checkbox" id="c-36564571" checked=""/><div class="controls bullet"><span class="by">orbz</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36564527">parent</a><span>|</span><a href="#36565981">next</a><span>|</span><label class="collapse" for="c-36564571">[-]</label><label class="expand" for="c-36564571">[21 more]</label></div><br/><div class="children"><div class="content">A disturbingly large number of deployments Iâve seen using Kubernetes or docker compose have databases deployed as such.</div><br/><div id="36565088" class="c"><input type="checkbox" id="c-36565088" checked=""/><div class="controls bullet"><span class="by">spockz</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36564571">parent</a><span>|</span><a href="#36564909">next</a><span>|</span><label class="collapse" for="c-36565088">[-]</label><label class="expand" for="c-36565088">[7 more]</label></div><br/><div class="children"><div class="content">Given the ability to deploy pods to dedicated nodes based on label selectors, what is the actual performance impact of running a database in a container on a bare metal host with mounted volume versus running that same process with say systemd on that same node? Basically, shouldnât the overhead of running a container be minimal?</div><br/><div id="36566040" class="c"><input type="checkbox" id="c-36566040" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565088">parent</a><span>|</span><a href="#36564909">next</a><span>|</span><label class="collapse" for="c-36566040">[-]</label><label class="expand" for="c-36566040">[6 more]</label></div><br/><div class="children"><div class="content">The problem is kubelet likes to spike in memory &#x2F; CPU &#x2F; network usage. It&#x27;s not a well-behaved program to put alongside a database. It&#x27;s not written with an eye for resource utilization.<p>Also, it brings nothing of value to the table, but requires a lot of dance around it to keep it going.  I.e. if you are a decent DBA, you don&#x27;t have a problem setting up a node to run your database of choice, you would be probably opposed to using pre-packaged Docker images anyways.<p>Also, Kubernetes sucks at managing storage... basically, it doesn&#x27;t offer anything that&#x27;d be useful to a DBA.  Things that <i>might</i> be useful come as CSI... and, obviously, it&#x27;s better &#x2F; easier to not use a CSI, but to interface directly with the storage you want instead.<p>That&#x27;s not to say that storage products don&#x27;t offer these CSI... so, a legitimate question would be why would anyone do that? -- and the answer is -- not because it&#x27;s useful, but because a lot of people think they need &#x2F; want it.  Instead of fighting stupidity, why not make an extra buck?</div><br/><div id="36566395" class="c"><input type="checkbox" id="c-36566395" checked=""/><div class="controls bullet"><span class="by">FridgeSeal</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566040">parent</a><span>|</span><a href="#36566923">next</a><span>|</span><label class="collapse" for="c-36566395">[-]</label><label class="expand" for="c-36566395">[4 more]</label></div><br/><div class="children"><div class="content">I run DBâs on K8s, not because I donât know what Iâm doing, but because most of the trade offs are worth it.<p>If I run a db workload in K8s, itâs a tiny fraction of the operational overhead, and not a massively noticeable performance loss.<p>I would absolutely <i>love</i> a way to deploy and manage dbâs as easily as K8s with fewer of the quite significant issues that have mentioned, so if you know of something that is better behaved around singular workloads, but keeps the simple deploys, the resiliency, the ease of networking and config deployments, the ease of monitoring, etc, I am all ears.</div><br/><div id="36566471" class="c"><input type="checkbox" id="c-36566471" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566395">parent</a><span>|</span><a href="#36567598">next</a><span>|</span><label class="collapse" for="c-36566471">[-]</label><label class="expand" for="c-36566471">[2 more]</label></div><br/><div class="children"><div class="content">If you think that deploying anything with Kubernetes is simple... well, I have bad news for you.<p>It&#x27;s simple, until you hit a problem. And then it becomes a lot worse than if you had never touched it.  You are now in the stage of a person who&#x27;d never made backups and never had a failure that required them to restore from backups, and you are wondering why would anyone do it. Adverse events are rare, and you may go like this for years, or, perhaps the rest of your life... unfortunately, your experience will not translate into a general advice.<p>But, again, you just might be in the camp where performance doesn&#x27;t matter. Nor does uptime matter, nor does your data have very high value... and in that case it&#x27;s OK to use tools that don&#x27;t offer any of that, and save you some time.  But, you cannot advise others based on that perspective.  Or, at least, not w&#x2F;o mentioning the downsides.</div><br/><div id="36569258" class="c"><input type="checkbox" id="c-36569258" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566471">parent</a><span>|</span><a href="#36567598">next</a><span>|</span><label class="collapse" for="c-36569258">[-]</label><label class="expand" for="c-36569258">[1 more]</label></div><br/><div class="children"><div class="content">Everyone running databases in production knows how to take backups and restore from them.  K8s or not, even using your cloud provider&#x27;s database&#x27;s built-in backups is hardly safe.  One click of the &quot;delete instance&quot; button (or nowadays, an exciting fuck up in IaC code), and your backups are gone!  Not to mention the usual cloud provider problems of &quot;oops your credit card bounced&quot; or &quot;the algorithm decided we don&#x27;t like your line of business&quot;.  You have to have backups, they have to be &quot;off site&quot;, and you have to try restoring them every few months.  There is pretty much no platform that gives you that for free.<p>I am not sure what complexity Kubernetes adds in this situation.  Anything Kubernetes can do to you, your cloud provider (or a poorly aimed fire extinguisher) can do to you.  You have to be ready for a disaster no matter the platform.</div><br/></div></div></div></div><div id="36567598" class="c"><input type="checkbox" id="c-36567598" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566395">parent</a><span>|</span><a href="#36566471">prev</a><span>|</span><a href="#36566923">next</a><span>|</span><label class="collapse" for="c-36567598">[-]</label><label class="expand" for="c-36567598">[1 more]</label></div><br/><div class="children"><div class="content">If you run in the cloud, any of the major cloud providers can take that undifferentiated heavy lifting off your hands (Amazon RDS etc.).</div><br/></div></div></div></div><div id="36566923" class="c"><input type="checkbox" id="c-36566923" checked=""/><div class="controls bullet"><span class="by">lokar</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566040">parent</a><span>|</span><a href="#36566395">prev</a><span>|</span><a href="#36564909">next</a><span>|</span><label class="collapse" for="c-36566923">[-]</label><label class="expand" for="c-36566923">[1 more]</label></div><br/><div class="children"><div class="content">If you care about perf you would pin the kubelet and all other overhead workload to one core, and mask that off for your workload.</div><br/></div></div></div></div></div></div><div id="36564909" class="c"><input type="checkbox" id="c-36564909" checked=""/><div class="controls bullet"><span class="by">danappelxx</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36564571">parent</a><span>|</span><a href="#36565088">prev</a><span>|</span><a href="#36565981">next</a><span>|</span><label class="collapse" for="c-36564909">[-]</label><label class="expand" for="c-36564909">[13 more]</label></div><br/><div class="children"><div class="content">IMO if youâre concerned about performance and yet are deploying databases this way â mmap should not even be on the radar.</div><br/><div id="36565076" class="c"><input type="checkbox" id="c-36565076" checked=""/><div class="controls bullet"><span class="by">charcircuit</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36564909">parent</a><span>|</span><a href="#36565981">next</a><span>|</span><label class="collapse" for="c-36565076">[-]</label><label class="expand" for="c-36565076">[12 more]</label></div><br/><div class="children"><div class="content">How would containers even hurt performance? How does the database no longer having the ability to see other processes on the machine somehow make it slower?</div><br/><div id="36566151" class="c"><input type="checkbox" id="c-36566151" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565076">parent</a><span>|</span><a href="#36565248">next</a><span>|</span><label class="collapse" for="c-36566151">[-]</label><label class="expand" for="c-36566151">[5 more]</label></div><br/><div class="children"><div class="content">There are many &quot;holes&quot; in these containers.<p>1. fsync. You cannot &quot;divide&quot; it between containers. Whoever does it, stalls I&#x2F;O for everyone else.<p>2. Context switches. Unless you do a lot of configurations <i>outside</i> of container runtime, you cannot ensure exclusive access to the number of CPU cores you need.<p>3. Networking has the same problem. You would either have to dedicate a whole NIC or SRI-OV-style virtual NIC to your database server.  Otherwise just the amount of chatter that goes on through the control plane of something like Kubernetes will be a noticeable disadvantage.  Again, containers don&#x27;t help here, they only get in the way as to get that kind of exclusive network access you need <i>more</i> configuration on the host, and, possible an CNI to deal with it.<p>4. kubelet is not optimized to get out of your way. It needs a lot of resources and may spike, hindering or outright stalling database process.<p>5. Kubernetes sucks at managing memory-intensive processes.  It doesn&#x27;t work (well or at all) with swap (which, again, cannot be properly divided between containers).  It doesn&#x27;t integrate well with OOM killer (it cannot replace it, so any configurations you make inside Kubernetes are kind of irrelevant, because system&#x27;s OOM killer will do how it pleases, ignoring Kubernetes).<p>---<p>Bottom line... Kubernetes is lame from infrastructure perspective.  It&#x27;s written for Web developers.  To make things appear simpler for them, while sacrificing a lot of resources and hiding a lot of actual complexity... which is impossible to hide, and which, in an even of failure will come to bite you.  You don&#x27;t want that kind of program near your database.</div><br/><div id="36566962" class="c"><input type="checkbox" id="c-36566962" checked=""/><div class="controls bullet"><span class="by">lokar</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566151">parent</a><span>|</span><a href="#36566577">next</a><span>|</span><label class="collapse" for="c-36566962">[-]</label><label class="expand" for="c-36566962">[2 more]</label></div><br/><div class="children"><div class="content">My background is more borg then k8s, butâ¦<p>Alway allocate whole cores, just mask them off<p>Dedicate physical IO devices for sensitive workloads<p>You can have per cgroup swap if you want, but imo swap is not useful<p>I think all of this is possible in k8s</div><br/><div id="36568162" class="c"><input type="checkbox" id="c-36568162" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566962">parent</a><span>|</span><a href="#36566577">next</a><span>|</span><label class="collapse" for="c-36568162">[-]</label><label class="expand" for="c-36568162">[1 more]</label></div><br/><div class="children"><div class="content">Whole core masking is not quite as easy as it should be, predominantly because the API is designed to hand wave away actual cores. The way you typically solve this is to go the other way and claim exclusive cores for the orchestrator and other overhead.</div><br/></div></div></div></div><div id="36566577" class="c"><input type="checkbox" id="c-36566577" checked=""/><div class="controls bullet"><span class="by">FridgeSeal</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566151">parent</a><span>|</span><a href="#36566962">prev</a><span>|</span><a href="#36568142">next</a><span>|</span><label class="collapse" for="c-36566577">[-]</label><label class="expand" for="c-36566577">[1 more]</label></div><br/><div class="children"><div class="content">As these are obviously very real issues, and Kubernetes also isnât going away imminently, how many of these can be fixed&#x2F;improved with different design on the application front?<p>Would using direct-Io APIâs fix most of the fsync issues? If workloads pin their stuff to specific cores can we incite some of the overhead here? (Assuming weâre only running a single dedicated workload + kubelet on the node).<p>&gt; You would either have to dedicate a whole NIC or SRI-OV-style virtual NIC to your database server<p>Tbh Iâve no idea we could do this with commodity cloud servers, nor do I know how, but Iâm terribly interested in knowing how, do you know if thereâs like a âdummyâs guide to better networkingâ? Haha<p>&gt; kubelet is not optimized to get out of your way...Kubernetes sucks at managing memory-intensive processes<p>Definitely agree on both these issues, Iâve blown up the kubelet by overallocating memory before, which basically borked the node until some watchdog process kicked in. Sounds like the better solution here is a kubelet rebuilt to operate more efficiently and more predictably? Is the solution a db-optimised kubelet&#x2F;K8s?</div><br/></div></div><div id="36568142" class="c"><input type="checkbox" id="c-36568142" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566151">parent</a><span>|</span><a href="#36566577">prev</a><span>|</span><a href="#36565248">next</a><span>|</span><label class="collapse" for="c-36568142">[-]</label><label class="expand" for="c-36568142">[1 more]</label></div><br/><div class="children"><div class="content">This is extremely misinformed. No matter how you choose to manage workloads, ultimately you are responsible for tuning and optimization.<p>If you&#x27;re not in control of the system, and thus kubelet, obviously your hands are tied. I&#x27;m not sure anyone is suggesting that for a serious workload.<p>Now to dispell your myths:<p>1. You can assign dedicated storage devices to your database. Outside of mount operations you&#x27;re not going to see much alien fsync activity. This is paranoid.<p>2. You can pin kubelet CPU cores. You can ensure exclusive access to the remaining ones. There are a number of advanced techniques that are not at all necessary if you want to be a control freak, such as creating your own cgroups. This isn&#x27;t &quot;outside&quot; of the runtime. Kubernetes is designed to conform to your managed cgroups. That&#x27;s the whole point. RTFM.<p>3. The general theme of your complaint has nothing to do with kubernetes. There&#x27;s no beating a dedicated NIC and even network fabric. Some cloud providers even allow you to multi-NIC out of the box so this is pretty solvable. Also, like, the dumbest QoS rules can drastically minimize this problem generally. Who cares.<p>4. Nah. RTFM. This is total FUD.<p>5.a. I don&#x27;t understand. Are you sharing resources on the node or not? If you&#x27;re not, then swap works fine. If you are, then this smells like cognitive dissonance and maybe listen to your own advice, but also swap is still very doable. It&#x27;s just disk. swapon to your heart&#x27;s content. But also swap is almost entirely dumb these days. Are you suggesting swapping to your primary IO device? Come on. More FUD.<p>5.b. OOM killer does what it wants. What&#x27;s a better alternative that integrates &quot;well&quot; with the OOM killer? Do you even understand how resource limits work? The OOM killer is only ever a problem if you either do not configure your workload properly (true regardless of execution environment) or you run out of actual memory.<p>Bottom line: come down off your high horse and acknowledge that dedicated resources and kernel tuning is the secret to extreme high performance. I don&#x27;t care how you&#x27;re orchestrating your workloads, the best practices are essentially universal.<p>And to be clear, I&#x27;m not recommending using Kubernetes to run a high performance database but it&#x27;s not really any worse (today) than alternatives.<p>&gt; It&#x27;s written for Web developers. To make things appear simpler for them, while sacrificing a lot of resources and hiding a lot of actual complexity... which is impossible to hide, and which, in an even of failure will come to bite you.<p>What planet are you currently on? This makes no sense. It&#x27;s a set of abstractions and patterns, the intent isn&#x27;t to hide the complexity but to make it manageable at scale. I&#x27;d argue it succeeds at that.<p>Seriously, what is the alternative runtime you&#x27;d prefer here? systemd? hand rolled bash scripts? puppet and ansible? All of the above??</div><br/></div></div></div></div><div id="36565248" class="c"><input type="checkbox" id="c-36565248" checked=""/><div class="controls bullet"><span class="by">danappelxx</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565076">parent</a><span>|</span><a href="#36566151">prev</a><span>|</span><a href="#36565981">next</a><span>|</span><label class="collapse" for="c-36565248">[-]</label><label class="expand" for="c-36565248">[6 more]</label></div><br/><div class="children"><div class="content">Iâll assume the worst case:<p>- lots of containers running on a single host<p>- containers are each isolated in a VM (aka virtualized)<p>- workloads are not homogenous and change often (your neighbor today may not be your neighbor tomorrow)<p>I believe these are fair assumptions if youâre running on generic infrastructure with kubernetes.<p>In this setup, my concerns are pretty much noisy neighbors + throttling. You may get latency spikes out of nowhere and the cause could be any of:<p>- your neighbor is hogging IO (disk or network)<p>- your database spawned too many threads and got throttled by CFS<p>- CFS scheduled your DBs threads on a different CPU and you lost your cache lines<p>In short, the DB does not have stable, predictable performance, which are exactly the characteristics you want it to have. If you ran the DB on a dedicated host you avoid this whole suite of issues.<p>You can alleviate most of this if you make sure the DBâs container gets the entire hostâs resources and doesnât have neighbors.</div><br/><div id="36566150" class="c"><input type="checkbox" id="c-36566150" checked=""/><div class="controls bullet"><span class="by">gcoakes</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565248">parent</a><span>|</span><a href="#36567604">next</a><span>|</span><label class="collapse" for="c-36566150">[-]</label><label class="expand" for="c-36566150">[4 more]</label></div><br/><div class="children"><div class="content">&gt; - containers are each isolated in a VM (aka virtualized)<p>Why are you assuming containers are virtualized? Is there some container runtime that does that as an added security measure? I thought they all use namespaces on Linux.</div><br/><div id="36566197" class="c"><input type="checkbox" id="c-36566197" checked=""/><div class="controls bullet"><span class="by">danappelxx</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566150">parent</a><span>|</span><a href="#36567604">next</a><span>|</span><label class="collapse" for="c-36566197">[-]</label><label class="expand" for="c-36566197">[3 more]</label></div><br/><div class="children"><div class="content">Itâs becoming standard as a security measure. See: Kata containers, Firecracker VM</div><br/><div id="36567613" class="c"><input type="checkbox" id="c-36567613" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36566197">parent</a><span>|</span><a href="#36567604">next</a><span>|</span><label class="collapse" for="c-36567613">[-]</label><label class="expand" for="c-36567613">[2 more]</label></div><br/><div class="children"><div class="content">Not so; neither Kata containers nor Firecracker are in widespread public use today. (Source: I work for AWS and consult regularly with container services customers, who both use AWS and run on premise.)</div><br/><div id="36567812" class="c"><input type="checkbox" id="c-36567812" checked=""/><div class="controls bullet"><span class="by">danappelxx</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36567613">parent</a><span>|</span><a href="#36567604">next</a><span>|</span><label class="collapse" for="c-36567812">[-]</label><label class="expand" for="c-36567812">[1 more]</label></div><br/><div class="children"><div class="content">Ah, good to know!</div><br/></div></div></div></div></div></div></div></div><div id="36567604" class="c"><input type="checkbox" id="c-36567604" checked=""/><div class="controls bullet"><span class="by">charcircuit</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565248">parent</a><span>|</span><a href="#36566150">prev</a><span>|</span><a href="#36565981">next</a><span>|</span><label class="collapse" for="c-36567604">[-]</label><label class="expand" for="c-36567604">[1 more]</label></div><br/><div class="children"><div class="content">None of those are the fault of containers. You can do all of what you said without containers.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36565981" class="c"><input type="checkbox" id="c-36565981" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36564527">parent</a><span>|</span><a href="#36564571">prev</a><span>|</span><a href="#36567945">next</a><span>|</span><label class="collapse" for="c-36565981">[-]</label><label class="expand" for="c-36565981">[2 more]</label></div><br/><div class="children"><div class="content">Nobody who matters.<p>Those who do that don&#x27;t know what they are doing (even if they outnumber the other side hundred to one, they &quot;don&#x27;t count&quot; because they aren&#x27;t aiming for good performance anyways).<p>Well, maybe not quite... of course it&#x27;s possible that someone would want to deploy a database in a container because of the convenience of assembling all dependencies in a single &quot;package&quot;, however, they would never run database on the same node as applications -- that&#x27;s insanity.<p>But, even the idea of deploying a database alongside something like kubelet service is cringe... This service is very &quot;fat&quot; and can spike in memory &#x2F; CPU usage.  I would be very strongly opposed to an idea of running a database on the same VM that runs Kubernetes or any container runtime that requires a service to run it.<p>Obviously, it says nothing about the number of processes that will run on the database node. At the minimum, you&#x27;d want to run some stuff for monitoring, that&#x27;s beside all the system services... but I don&#x27;t think GP meant &quot;one process&quot; literally. Neither that is realistic nor is it necessary.</div><br/><div id="36566136" class="c"><input type="checkbox" id="c-36566136" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36565981">parent</a><span>|</span><a href="#36567945">next</a><span>|</span><label class="collapse" for="c-36566136">[-]</label><label class="expand" for="c-36566136">[1 more]</label></div><br/><div class="children"><div class="content">&gt;but I don&#x27;t think GP meant &quot;one process&quot; literally. Neither that is realistic nor is it necessary.<p>The point was simply about other processes that could be competing for resources - CPU, memory, or I&#x2F;O. It is expensive for a user-level process to perform accounting for all of these resources, and without such accounting you can&#x27;t optimally allocate them.<p>If there are other apps that can suddenly spike memory usage then any careful buffer tuning you&#x27;ve done goes out the window. Likewise for any I&#x2F;O scheduling you&#x27;ve done, etc.</div><br/></div></div></div></div><div id="36567945" class="c"><input type="checkbox" id="c-36567945" checked=""/><div class="controls bullet"><span class="by">didip</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36564527">parent</a><span>|</span><a href="#36565981">prev</a><span>|</span><a href="#36564666">next</a><span>|</span><label class="collapse" for="c-36567945">[-]</label><label class="expand" for="c-36567945">[1 more]</label></div><br/><div class="children"><div class="content">My group and a bunch of my peer groups.<p>And we are running them at the scale that most people canât even imagine.</div><br/></div></div><div id="36564666" class="c"><input type="checkbox" id="c-36564666" checked=""/><div class="controls bullet"><span class="by">huahaiy</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36564527">parent</a><span>|</span><a href="#36567945">prev</a><span>|</span><a href="#36566077">next</a><span>|</span><label class="collapse" for="c-36564666">[-]</label><label class="expand" for="c-36564666">[1 more]</label></div><br/><div class="children"><div class="content">Embedded DB</div><br/></div></div><div id="36566077" class="c"><input type="checkbox" id="c-36566077" checked=""/><div class="controls bullet"><span class="by">morelisp</span><span>|</span><a href="#36564309">root</a><span>|</span><a href="#36564527">parent</a><span>|</span><a href="#36564666">prev</a><span>|</span><a href="#36563882">next</a><span>|</span><label class="collapse" for="c-36566077">[-]</label><label class="expand" for="c-36566077">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m running prod databases in containers so the server infra team doesn&#x27;t have to know anything about how that specific database works or how to upgrade it, they just need to know how to issue generic container start&#x2F;stop commands if they want to do some maintenance.<p>(But just in containers, not in Kubernetes. I&#x27;m not crazy.)</div><br/></div></div></div></div></div></div><div id="36563882" class="c"><input type="checkbox" id="c-36563882" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#36564309">prev</a><span>|</span><a href="#36564297">next</a><span>|</span><label class="collapse" for="c-36563882">[-]</label><label class="expand" for="c-36563882">[7 more]</label></div><br/><div class="children"><div class="content">Another interesting limitation of mmap() is that real-world storage volumes can exceed the virtual address space a CPU can address. A 64-bit CPU may have 64-bit pointers but typically cannot address anywhere close to 64 bits of memory, virtually or physically. A normal buffer pool does not have this limitation. You can get EC2 instances on AWS with more direct-attached storage than addressable virtual address space on the local microarchitecture.</div><br/><div id="36567395" class="c"><input type="checkbox" id="c-36567395" checked=""/><div class="controls bullet"><span class="by">glandium</span><span>|</span><a href="#36563882">parent</a><span>|</span><a href="#36564297">next</a><span>|</span><label class="collapse" for="c-36567395">[-]</label><label class="expand" for="c-36567395">[6 more]</label></div><br/><div class="children"><div class="content">To put concrete numbers: x86-64 is limited to 48 bits for virtual addresses, which is &quot;only&quot; 256TiB (281TB).</div><br/><div id="36569760" class="c"><input type="checkbox" id="c-36569760" checked=""/><div class="controls bullet"><span class="by">stevefan1999</span><span>|</span><a href="#36563882">root</a><span>|</span><a href="#36567395">parent</a><span>|</span><a href="#36567753">next</a><span>|</span><label class="collapse" for="c-36569760">[-]</label><label class="expand" for="c-36569760">[1 more]</label></div><br/><div class="children"><div class="content">Intel now extended the page table level to 5-level making this number not so valid. Granted, PL5 creates more TLB pressure and longer memory access time due to that.</div><br/></div></div><div id="36567753" class="c"><input type="checkbox" id="c-36567753" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36563882">root</a><span>|</span><a href="#36567395">parent</a><span>|</span><a href="#36569760">prev</a><span>|</span><a href="#36567427">next</a><span>|</span><label class="collapse" for="c-36567753">[-]</label><label class="expand" for="c-36567753">[2 more]</label></div><br/><div class="children"><div class="content">All of that is true, but I don&#x27;t think it&#x27;s a realistic concern. You&#x27;re going to be sharding your data across multiple nodes before it gets that large. Nobody wants to sit around backing up or restoring a monolithic 256 TiB database.</div><br/><div id="36568394" class="c"><input type="checkbox" id="c-36568394" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#36563882">root</a><span>|</span><a href="#36567753">parent</a><span>|</span><a href="#36567427">next</a><span>|</span><label class="collapse" for="c-36568394">[-]</label><label class="expand" for="c-36568394">[1 more]</label></div><br/><div class="children"><div class="content">Technically you get quite a bit less than the 256 TB theoretical in practice.<p>It is a realistic concern, Iâve lived it for more than a decade across many orgs, though I shared your opinion at one point. Storage density is massively important for both workload scalability and economic efficiency. Low storage density means buying a ton of server hardware that sits idle under max load and vastly larger clusters than would otherwise be necessary, which have their own costs.<p>When your database is sufficiently large, backup and restore often isnât even a technical possibility so that requirement is a red herring. The kinds of workloads that can be recovered from backup at that scale on a single server, and some can, benefit massively from the economics of running it on a single server. A solution that has 10x the AWS bill for the same workload performance doesnât get chosen.<p>At scale, hardware footprint economics is one of the central business decision drivers. Data isnât getting smaller. It is increasingly ordinary for innocuous organizations to have a single table with a trillion records in it.<p>For better or worse, the market increasingly drives my technical design decisions to optimize for hardware&#x2F;cloud costs above all else, and dense storage is a huge win for that.</div><br/></div></div></div></div><div id="36567427" class="c"><input type="checkbox" id="c-36567427" checked=""/><div class="controls bullet"><span class="by">Svetlitski</span><span>|</span><a href="#36563882">root</a><span>|</span><a href="#36567395">parent</a><span>|</span><a href="#36567753">prev</a><span>|</span><a href="#36564297">next</a><span>|</span><label class="collapse" for="c-36567427">[-]</label><label class="expand" for="c-36567427">[2 more]</label></div><br/><div class="children"><div class="content">Starting with Ice Lake thereâs  support for 5-level paging, which increases this to 128 PiB. Canât say that Iâve ever seen this used in the wild though.</div><br/><div id="36568414" class="c"><input type="checkbox" id="c-36568414" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#36563882">root</a><span>|</span><a href="#36567427">parent</a><span>|</span><a href="#36564297">next</a><span>|</span><label class="collapse" for="c-36568414">[-]</label><label class="expand" for="c-36568414">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, there mostly isnât a use case for it in databases. If you have that much storage youâll need to bypass the kernel cache and scheduler anyway for other reasons. That was true even at the 48-bit limit.</div><br/></div></div></div></div></div></div></div></div><div id="36564297" class="c"><input type="checkbox" id="c-36564297" checked=""/><div class="controls bullet"><span class="by">pjdesno</span><span>|</span><a href="#36563882">prev</a><span>|</span><a href="#36563633">next</a><span>|</span><label class="collapse" for="c-36564297">[-]</label><label class="expand" for="c-36564297">[3 more]</label></div><br/><div class="children"><div class="content">Not just databases - we ran into the same issues when we needed a high-performance caching HTTP reverse proxy for a research project. We were just going to drop in Varnish, which is mmap-based, but performance sucked and we had to write our own.<p>Note that Varnish dates to 2006, in the days of hard disk drives, SCSI, and 2-core server CPUs. Mmap might well have been as good or even better than I&#x2F;O back then - a lot of the issues discussed in this paper (TLB shootdown overhead, single flush thread) get much worse as the core count increases.</div><br/><div id="36565939" class="c"><input type="checkbox" id="c-36565939" checked=""/><div class="controls bullet"><span class="by">Sesse__</span><span>|</span><a href="#36564297">parent</a><span>|</span><a href="#36566465">next</a><span>|</span><label class="collapse" for="c-36565939">[-]</label><label class="expand" for="c-36565939">[1 more]</label></div><br/><div class="children"><div class="content">Varnish&#x27; design wasn&#x27;t very fast even for 2006-era hardware. It _was_ fast compared to Squid, though (which was the only real competitor at the time), and most importantly, much more flexible for the origin server case. But it came from a culture of âthe FreeBSD kernel is so awesome that the best thing userspace can do is to offload as many decisions as humanly possible to the kernelâ, which caused, well, suboptimal performance.<p>AFAIK the persistent backend was dropped pretty early on (eventually replaced with a more traditional read()&#x2F;write()-based one as part of Varnish Plus), and the general recommendation became just to use malloc and hope you didn&#x27;t swap.</div><br/></div></div><div id="36566465" class="c"><input type="checkbox" id="c-36566465" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#36564297">parent</a><span>|</span><a href="#36565939">prev</a><span>|</span><a href="#36563633">next</a><span>|</span><label class="collapse" for="c-36566465">[-]</label><label class="expand" for="c-36566465">[1 more]</label></div><br/><div class="children"><div class="content">Varnish has a file system backed cache that depends on the page cache to keep it fast.<p>What did you differently in your custom one that was faster then varnish?</div><br/></div></div></div></div><div id="36563633" class="c"><input type="checkbox" id="c-36563633" checked=""/><div class="controls bullet"><span class="by">wood_spirit</span><span>|</span><a href="#36564297">prev</a><span>|</span><a href="#36566784">next</a><span>|</span><label class="collapse" for="c-36563633">[-]</label><label class="expand" for="c-36563633">[9 more]</label></div><br/><div class="children"><div class="content">Old timers will recall when using mmap was a prominently promoted selling point for the âno sqlâ dbms.</div><br/><div id="36563958" class="c"><input type="checkbox" id="c-36563958" checked=""/><div class="controls bullet"><span class="by">ren_engineer</span><span>|</span><a href="#36563633">parent</a><span>|</span><a href="#36563723">next</a><span>|</span><label class="collapse" for="c-36563958">[-]</label><label class="expand" for="c-36563958">[2 more]</label></div><br/><div class="children"><div class="content">seems like all databases are moving towards the middle. Postgres has JSON support, MongoDB has transactions and also a columnar extension for OLAP type data. NoSQL seems almost meaningless as a term now. Feels like a move towards a winner takes all multi-modal database that can work with most types of data fairly well. Postgres with all of it&#x27;s specialized extensions seems like it will be the most popular choice. The convenience of not having to manage multiple databases is hard to beat unless performance is exponentially better, Postgres with these extensions can probably be &quot;good enough&quot; for a lot of companies<p>reminds me of how industries typically start out dominated by vertically integrated companies, move to specialized horizontal companies, then generally move back to vertical integration due to efficiency. Car industry started this way with Ford, went away from it, and now Tesla is doing it again. Lots of other examples in other industries</div><br/><div id="36565776" class="c"><input type="checkbox" id="c-36565776" checked=""/><div class="controls bullet"><span class="by">TheGeminon</span><span>|</span><a href="#36563633">root</a><span>|</span><a href="#36563958">parent</a><span>|</span><a href="#36563723">next</a><span>|</span><label class="collapse" for="c-36565776">[-]</label><label class="expand" for="c-36565776">[1 more]</label></div><br/><div class="children"><div class="content">The pendulum swing is common in any system, and is a really effective mechanism for evaluation.<p>You almost always want somewhere in the middle, but itâs often much easier to move back after a large jump in one direction than to push towards the middle.</div><br/></div></div></div></div><div id="36563723" class="c"><input type="checkbox" id="c-36563723" checked=""/><div class="controls bullet"><span class="by">nemo44x</span><span>|</span><a href="#36563633">parent</a><span>|</span><a href="#36563958">prev</a><span>|</span><a href="#36566784">next</a><span>|</span><label class="collapse" for="c-36563723">[-]</label><label class="expand" for="c-36563723">[6 more]</label></div><br/><div class="children"><div class="content">For documents it made access fast since thereâs no joins, etc. that require paging from all over. The problem ended up being updates and compaction issues.</div><br/><div id="36563928" class="c"><input type="checkbox" id="c-36563928" checked=""/><div class="controls bullet"><span class="by">wood_spirit</span><span>|</span><a href="#36563633">root</a><span>|</span><a href="#36563723">parent</a><span>|</span><a href="#36566784">next</a><span>|</span><label class="collapse" for="c-36563928">[-]</label><label class="expand" for="c-36563928">[5 more]</label></div><br/><div class="children"><div class="content">My memory is that the problem was ACID.  The document stores didnât promise to be reliable because apparently that didnât scale.<p>And there was a very well known cartoon video discussion about it with âweb scaleâ and âjust write to dev nullâ and other classics that became memes :)</div><br/><div id="36567009" class="c"><input type="checkbox" id="c-36567009" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#36563633">root</a><span>|</span><a href="#36563928">parent</a><span>|</span><a href="#36564492">next</a><span>|</span><label class="collapse" for="c-36567009">[-]</label><label class="expand" for="c-36567009">[1 more]</label></div><br/><div class="children"><div class="content">Document stores often are reliable and more fault tolerant. But yes they trade ACID.<p>There are some applications that require high throughput (usually write) but can be fine with read consistency.<p>Couple of examples
- consumer facing comment systems where other users are OK to miss your comment by 30 seconds
- timeseries logging where you are usually reading infrequently but writing very much in a denormalized format so joins aren&#x27;t as critical<p>For general CRUD, ACID is important though.</div><br/></div></div><div id="36564492" class="c"><input type="checkbox" id="c-36564492" checked=""/><div class="controls bullet"><span class="by">cratermoon</span><span>|</span><a href="#36563633">root</a><span>|</span><a href="#36563928">parent</a><span>|</span><a href="#36567009">prev</a><span>|</span><a href="#36566784">next</a><span>|</span><label class="collapse" for="c-36564492">[-]</label><label class="expand" for="c-36564492">[3 more]</label></div><br/><div class="children"><div class="content">Did you ever read Pat Helland&#x27;s article, &quot;Life Beyond Distributed Transactions: An apostateâs opinion&quot; <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3012426.3025012" rel="nofollow noreferrer">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3012426.3025012</a>? &quot;This article explores and names some of the practical approaches used in the implementation of large-scale mission-critical applications in a world that rejects distributed transactions.&quot;</div><br/><div id="36564889" class="c"><input type="checkbox" id="c-36564889" checked=""/><div class="controls bullet"><span class="by">wood_spirit</span><span>|</span><a href="#36563633">root</a><span>|</span><a href="#36564492">parent</a><span>|</span><a href="#36566784">next</a><span>|</span><label class="collapse" for="c-36564889">[-]</label><label class="expand" for="c-36564889">[2 more]</label></div><br/><div class="children"><div class="content">No I havenât.  Thanks for the interesting link :)<p>Admittedly I live in a world where big distributed transactions are a given and work fine and sql speeds us up not slows us down.  Iâm guessing sql and acid scaled after all?</div><br/><div id="36566384" class="c"><input type="checkbox" id="c-36566384" checked=""/><div class="controls bullet"><span class="by">cratermoon</span><span>|</span><a href="#36563633">root</a><span>|</span><a href="#36564889">parent</a><span>|</span><a href="#36566784">next</a><span>|</span><label class="collapse" for="c-36566384">[-]</label><label class="expand" for="c-36566384">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Iâm guessing sql and acid scaled after all?<p>Yes and no. Distributed transactions and two-phase commit have been superseded by things like Paxos and Raft, with a variety of consistency models, so the implementation is drastically different.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36566784" class="c"><input type="checkbox" id="c-36566784" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#36563633">prev</a><span>|</span><a href="#36563705">next</a><span>|</span><label class="collapse" for="c-36566784">[-]</label><label class="expand" for="c-36566784">[1 more]</label></div><br/><div class="children"><div class="content">Related:<p><i>Are You Sure You Want to Use MMAP in Your Database Management System? [pdf]</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31504052">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31504052</a> - May 2022 (43 comments)<p><i>Are you sure you want to use MMAP in your database management system? [pdf]</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29936104">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29936104</a> - Jan 2022 (127 comments)</div><br/></div></div><div id="36563705" class="c"><input type="checkbox" id="c-36563705" checked=""/><div class="controls bullet"><span class="by">dist1ll</span><span>|</span><a href="#36566784">prev</a><span>|</span><a href="#36563953">next</a><span>|</span><label class="collapse" for="c-36563705">[-]</label><label class="expand" for="c-36563705">[15 more]</label></div><br/><div class="children"><div class="content">Many general-purpose OS abstractions start leaking when you&#x27;re working on systems-like software.<p>You notice it when web servers are doing kernel bypass to for zero-copy, low-latency networking, or database engines throw away the kernel&#x27;s page cache to implement their own file buffer.</div><br/><div id="36563977" class="c"><input type="checkbox" id="c-36563977" checked=""/><div class="controls bullet"><span class="by">kentonv</span><span>|</span><a href="#36563705">parent</a><span>|</span><a href="#36563874">next</a><span>|</span><label class="collapse" for="c-36563977">[-]</label><label class="expand" for="c-36563977">[1 more]</label></div><br/><div class="children"><div class="content">Yes. I think mmap() is misunderstood as being an advanced tool for systems hackers, but it&#x27;s actually the opposite: it&#x27;s a tool to make application code simpler by leaving the systems stuff to the kernel.<p>With mmap, you get to avoid thinking about how much data to buffer at once, caching data to speed up repeated access, or shedding that cache when memory pressure is high. The kernel does all that. It may not do it in the absolute ideal way for your program but the benefit is you don&#x27;t have to think about these logistics.<p>But if you&#x27;re already writing intense systems code then you can probably do a better job than the kernel by optimizing for your use case.</div><br/></div></div><div id="36563874" class="c"><input type="checkbox" id="c-36563874" checked=""/><div class="controls bullet"><span class="by">arter4</span><span>|</span><a href="#36563705">parent</a><span>|</span><a href="#36563977">prev</a><span>|</span><a href="#36563953">next</a><span>|</span><label class="collapse" for="c-36563874">[-]</label><label class="expand" for="c-36563874">[13 more]</label></div><br/><div class="children"><div class="content">Web servers doing kernel bypass for zero-copy networking? Do you have a specific example in mind? I&#x27;m curious.</div><br/><div id="36564018" class="c"><input type="checkbox" id="c-36564018" checked=""/><div class="controls bullet"><span class="by">dist1ll</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36563874">parent</a><span>|</span><a href="#36563941">next</a><span>|</span><label class="collapse" for="c-36564018">[-]</label><label class="expand" for="c-36564018">[5 more]</label></div><br/><div class="children"><div class="content">The most common example is DPDK [1]. It&#x27;s a framework for building bespoke networking stacks that are usable from userspace, without involving the kernel.<p>You&#x27;ll find DPDK mentioned a lot in the networking&#x2F;HPC&#x2F;data center literature. An example of a backend framework that uses DPDK is the seastar framework [2]. Also, I recently stumbled upon a paper for  efficient RPC networks in data centers [3].<p>If you want to learn more, the p99 conference has tons of speakers talking about some interesting challenges in that space.<p>[1] <a href="https:&#x2F;&#x2F;www.dpdk.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.dpdk.org&#x2F;</a>.<p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;scylladb&#x2F;seastar">https:&#x2F;&#x2F;github.com&#x2F;scylladb&#x2F;seastar</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;erpc-io&#x2F;eRPC">https:&#x2F;&#x2F;github.com&#x2F;erpc-io&#x2F;eRPC</a></div><br/><div id="36564246" class="c"><input type="checkbox" id="c-36564246" checked=""/><div class="controls bullet"><span class="by">arter4</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36564018">parent</a><span>|</span><a href="#36567273">next</a><span>|</span><label class="collapse" for="c-36564246">[-]</label><label class="expand" for="c-36564246">[2 more]</label></div><br/><div class="children"><div class="content">Interesting. I hear a lot more about sendfile(), kTLS and general kernel space tricks than I do about DPDK and userspace networking, but maybe it&#x27;s just me.<p>I do wonder what trend is going to win: bypass the kernel or embrace the kernel for everything?<p>The way I see it, latency decreases either way (as long as you don&#x27;t have to switch back and forth between kernel and user space), but userspace seems better from a security standpoint.<p>Then again, everyone is doing eBPF, so probably the &quot;embrace the kernel&quot; approach is going to win. Who knows.</div><br/><div id="36567152" class="c"><input type="checkbox" id="c-36567152" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36564246">parent</a><span>|</span><a href="#36567273">next</a><span>|</span><label class="collapse" for="c-36567152">[-]</label><label class="expand" for="c-36567152">[1 more]</label></div><br/><div class="children"><div class="content">The people who use DPDK and the like are a lot quieter about it.  The nature of kernel development means that people tend to hear about what you&#x27;re doing, while DPDK and userspace networking tends to happen in more proprietary settings.<p>That said, I&#x27;m not sure many people write webservers in DPDK, since the Kernel is pretty well suited to webservers (sendfile, etc.). Most applications that use kernel-bypass are more specialized.</div><br/></div></div></div></div><div id="36567273" class="c"><input type="checkbox" id="c-36567273" checked=""/><div class="controls bullet"><span class="by">ori_b</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36564018">parent</a><span>|</span><a href="#36564246">prev</a><span>|</span><a href="#36563941">next</a><span>|</span><label class="collapse" for="c-36567273">[-]</label><label class="expand" for="c-36567273">[2 more]</label></div><br/><div class="children"><div class="content">The downside, of course, is that each program owns one instance of the hardware. Applications don&#x27;t share the network card. This isn&#x27;t a general purpose solution.<p>That may be acceptable for your purposes, or it may not.</div><br/></div></div></div></div><div id="36563941" class="c"><input type="checkbox" id="c-36563941" checked=""/><div class="controls bullet"><span class="by">kentonv</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36563874">parent</a><span>|</span><a href="#36564018">prev</a><span>|</span><a href="#36563953">next</a><span>|</span><label class="collapse" for="c-36563941">[-]</label><label class="expand" for="c-36563941">[7 more]</label></div><br/><div class="children"><div class="content">Probably the most common example is sendfile() for writing file contents out to a socket without reading them into userspace:<p><a href="https:&#x2F;&#x2F;man7.org&#x2F;linux&#x2F;man-pages&#x2F;man2&#x2F;sendfile.2.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;man7.org&#x2F;linux&#x2F;man-pages&#x2F;man2&#x2F;sendfile.2.html</a></div><br/><div id="36564191" class="c"><input type="checkbox" id="c-36564191" checked=""/><div class="controls bullet"><span class="by">mrfox321</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36563941">parent</a><span>|</span><a href="#36564164">next</a><span>|</span><label class="collapse" for="c-36564191">[-]</label><label class="expand" for="c-36564191">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that the opposite?  That is, bypassing user space, not kernel space?</div><br/><div id="36564231" class="c"><input type="checkbox" id="c-36564231" checked=""/><div class="controls bullet"><span class="by">kentonv</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36564191">parent</a><span>|</span><a href="#36564164">next</a><span>|</span><label class="collapse" for="c-36564231">[-]</label><label class="expand" for="c-36564231">[1 more]</label></div><br/><div class="children"><div class="content">Oh, hmm, yeah, perhaps OP meant something more like using raw sockets to get packets directly into userspace without relying on the kernel to arrange them into streams?<p>I&#x27;m not very familiar with that though.</div><br/></div></div></div></div><div id="36564164" class="c"><input type="checkbox" id="c-36564164" checked=""/><div class="controls bullet"><span class="by">arter4</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36563941">parent</a><span>|</span><a href="#36564191">prev</a><span>|</span><a href="#36564252">next</a><span>|</span><label class="collapse" for="c-36564164">[-]</label><label class="expand" for="c-36564164">[3 more]</label></div><br/><div class="children"><div class="content">Yes, I knew about sendfile() but I wasnt&#x27;t aware of any web server using that (though I know Kafka uses it).<p>Then I found out Apache supports it via the EnableSendfile directive. Nice.<p>&gt;This directive controls whether httpd may use the sendfile support from the kernel to transmit file contents to the client. By default, when the handling of a request requires no access to the data within a file -- for example, when delivering a static file -- Apache httpd uses sendfile to deliver the file contents without ever reading the file if the OS supports it.</div><br/><div id="36564357" class="c"><input type="checkbox" id="c-36564357" checked=""/><div class="controls bullet"><span class="by">nh2</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36564164">parent</a><span>|</span><a href="#36564269">next</a><span>|</span><label class="collapse" for="c-36564357">[-]</label><label class="expand" for="c-36564357">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much all modern Linux web servers support sendfile(). Examples:<p>* nginx: [1]
* Haskell webserver module: [2]
* caddy: [3]<p>[1]: <a href="https:&#x2F;&#x2F;nginx.org&#x2F;en&#x2F;docs&#x2F;http&#x2F;ngx_http_core_module.html#sendfile" rel="nofollow noreferrer">https:&#x2F;&#x2F;nginx.org&#x2F;en&#x2F;docs&#x2F;http&#x2F;ngx_http_core_module.html#sen...</a>
[2]: <a href="https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;warp-3.3.28&#x2F;docs&#x2F;Network-Wai-Handler-Warp-Internal.html#g:6" rel="nofollow noreferrer">https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;warp-3.3.28&#x2F;docs&#x2F;Network...</a>
[3]: <a href="https:&#x2F;&#x2F;github.com&#x2F;caddyserver&#x2F;caddy&#x2F;pull&#x2F;5022">https:&#x2F;&#x2F;github.com&#x2F;caddyserver&#x2F;caddy&#x2F;pull&#x2F;5022</a></div><br/></div></div><div id="36564269" class="c"><input type="checkbox" id="c-36564269" checked=""/><div class="controls bullet"><span class="by">kentonv</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36564164">parent</a><span>|</span><a href="#36564357">prev</a><span>|</span><a href="#36564252">next</a><span>|</span><label class="collapse" for="c-36564269">[-]</label><label class="expand" for="c-36564269">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d expect most serious web servers support it. I&#x27;ve written one that does (workerd), it&#x27;s not too hard.<p>That said, it&#x27;s tricky to use if the server also does TLS termination... then you need kTLS, which is a much bigger can of worms.</div><br/></div></div></div></div><div id="36564252" class="c"><input type="checkbox" id="c-36564252" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#36563705">root</a><span>|</span><a href="#36563941">parent</a><span>|</span><a href="#36564164">prev</a><span>|</span><a href="#36563953">next</a><span>|</span><label class="collapse" for="c-36564252">[-]</label><label class="expand" for="c-36564252">[1 more]</label></div><br/><div class="children"><div class="content">Sendfile isnât kernel bypass.</div><br/></div></div></div></div></div></div></div></div><div id="36563953" class="c"><input type="checkbox" id="c-36563953" checked=""/><div class="controls bullet"><span class="by">kwohlfahrt</span><span>|</span><a href="#36563705">prev</a><span>|</span><a href="#36564901">next</a><span>|</span><label class="collapse" for="c-36563953">[-]</label><label class="expand" for="c-36563953">[3 more]</label></div><br/><div class="children"><div class="content">It sounds like a lot of the performance issues are TLB-related. Am I right in thinking huge-pages would help here? If so, it&#x27;s a bit unfortunate they didn&#x27;t test this in the paper.<p>Edit: Hm, it might not be possible to mmap files with huge-pages. This LWN article[1] from 5 years ago talks about the work that would be required, but I haven&#x27;t seen any follow-ups.<p>[1]: <a href="https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;718102&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;718102&#x2F;</a></div><br/><div id="36567607" class="c"><input type="checkbox" id="c-36567607" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36563953">parent</a><span>|</span><a href="#36567278">next</a><span>|</span><label class="collapse" for="c-36567607">[-]</label><label class="expand" for="c-36567607">[1 more]</label></div><br/><div class="children"><div class="content">Huge pages aren&#x27;t pageable, so they wouldn&#x27;t be particularly advantageous for a mmap DB anyway, you&#x27;d have to do traditional I&#x2F;O &amp; buffer management for everything.</div><br/></div></div><div id="36567278" class="c"><input type="checkbox" id="c-36567278" checked=""/><div class="controls bullet"><span class="by">ori_b</span><span>|</span><a href="#36563953">parent</a><span>|</span><a href="#36567607">prev</a><span>|</span><a href="#36564901">next</a><span>|</span><label class="collapse" for="c-36567278">[-]</label><label class="expand" for="c-36567278">[1 more]</label></div><br/><div class="children"><div class="content">No, huge pages wouldn&#x27;t help. They would change when the TLB gets flushed, but the flushes would still be there.</div><br/></div></div></div></div><div id="36564901" class="c"><input type="checkbox" id="c-36564901" checked=""/><div class="controls bullet"><span class="by">mpweiher</span><span>|</span><a href="#36563953">prev</a><span>|</span><a href="#36563437">next</a><span>|</span><label class="collapse" for="c-36564901">[-]</label><label class="expand" for="c-36564901">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I definitely would <i>want</i> to use mmap() in my storage system.  And would love to see the limitations that make this tricky addressed.</div><br/></div></div><div id="36563437" class="c"><input type="checkbox" id="c-36563437" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#36564901">prev</a><span>|</span><a href="#36563438">next</a><span>|</span><label class="collapse" for="c-36563437">[-]</label><label class="expand" for="c-36563437">[11 more]</label></div><br/><div class="children"><div class="content">Memory-Mapped Files = access violations when a disk read fails.  If you&#x27;re not prepared to handle those, don&#x27;t use memory-mapped files.  (Access violation exceptions are the same thing that happens when you attempt to read a null pointer)<p>Then there&#x27;s the part with writes being delayed.  Be prepared to deal with blocks not necessarily updating to disk in the order they were written to, and 10 seconds after the fact.  This can make power failures cause inconsistencies.</div><br/><div id="36563654" class="c"><input type="checkbox" id="c-36563654" checked=""/><div class="controls bullet"><span class="by">kentonv</span><span>|</span><a href="#36563437">parent</a><span>|</span><a href="#36563898">next</a><span>|</span><label class="collapse" for="c-36563654">[-]</label><label class="expand" for="c-36563654">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Be prepared to deal with blocks not necessarily updating to disk in the order they were written to, and 10 seconds after the fact. This can make power failures cause inconsistencies.<p>This is not specific to mmap -- regular old write() calls have the same behavior. You need to fsync() (or, with mmap, msync()) to guarantee data is on disk.</div><br/><div id="36569577" class="c"><input type="checkbox" id="c-36569577" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#36563437">root</a><span>|</span><a href="#36563654">parent</a><span>|</span><a href="#36566252">next</a><span>|</span><label class="collapse" for="c-36569577">[-]</label><label class="expand" for="c-36569577">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s fun to remember that fsync() on Linux on ext4 at least offers no real guarantee that the data was successfully written to disk. This happens when write errors from background buffered writes are handled internally by the kernel, and they cleanup the error situation (mark dirty pages clean etc). Since the kernel can&#x27;t know if a later call to fsync() will ever happen, it can&#x27;t just keep the error around. So, when the call does happen, it will not return any error code. I don&#x27;t know for sure, but msync() may well have the same behavior.<p>Here is an LWN article discussing the whole problem as the Postgres team found out about it.<p><a href="https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;752063&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;752063&#x2F;</a></div><br/></div></div><div id="36566252" class="c"><input type="checkbox" id="c-36566252" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36563437">root</a><span>|</span><a href="#36563654">parent</a><span>|</span><a href="#36569577">prev</a><span>|</span><a href="#36563898">next</a><span>|</span><label class="collapse" for="c-36566252">[-]</label><label class="expand" for="c-36566252">[3 more]</label></div><br/><div class="children"><div class="content">&gt; This is not specific to mmap -- regular old write() calls have the same behavior.<p>This is not true.  This depends on how the file was opened. You may request DIRECT | SYNC when opening and the writes are acknowledged when they are actually written.  This is obviously a lot slower than writing to cache, but this is the way for &quot;simple&quot; user-space applications to implement their own cache.<p>In the world of today, you are very rarely writing to something that&#x27;s not network attached, and depending on your appliance, the meaning of acknowledgement from write() differs.  Sometimes it&#x27;s even configurable.  This is why databases also offer various modes of synchronization -- you need to know how your appliance works and configure the database accordingly.</div><br/><div id="36566957" class="c"><input type="checkbox" id="c-36566957" checked=""/><div class="controls bullet"><span class="by">kentonv</span><span>|</span><a href="#36563437">root</a><span>|</span><a href="#36566252">parent</a><span>|</span><a href="#36563898">next</a><span>|</span><label class="collapse" for="c-36566957">[-]</label><label class="expand" for="c-36566957">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This is not true. This depends on how the file was opened. You may request DIRECT | SYNC<p>Well sure, but 99.9% of people don&#x27;t do that (and shouldn&#x27;t, unless they really know what they are doing).<p>&gt; In the world of today, you are very rarely writing to something that&#x27;s not network attached, and depending on your appliance, the meaning of acknowledgement from write() differs.<p>What network-attached storage actually uses O_SYNC behavior without being asked? I&#x27;d be quite surprised if any did this as it would make typical workloads incredibly slow in order to provide a guarantee they didn&#x27;t ask for.</div><br/><div id="36568386" class="c"><input type="checkbox" id="c-36568386" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36563437">root</a><span>|</span><a href="#36566957">parent</a><span>|</span><a href="#36563898">next</a><span>|</span><label class="collapse" for="c-36568386">[-]</label><label class="expand" for="c-36568386">[1 more]</label></div><br/><div class="children"><div class="content">100% of people writing a database know about filesystem options like DIRECT and SYNC, and that is the subject of this paper.<p>Also, most of the network-attached storage we people use is in the form of things like EBS, which is very careful to imitate the behavior of a real disk, but with different performance and some different (albeit <i>very rare</i>) failure modes.</div><br/></div></div></div></div></div></div></div></div><div id="36563898" class="c"><input type="checkbox" id="c-36563898" checked=""/><div class="controls bullet"><span class="by">afr0ck</span><span>|</span><a href="#36563437">parent</a><span>|</span><a href="#36563654">prev</a><span>|</span><a href="#36565028">next</a><span>|</span><label class="collapse" for="c-36563898">[-]</label><label class="expand" for="c-36563898">[2 more]</label></div><br/><div class="children"><div class="content">Linux throws a SIGBUS. A process should anticipate such I&#x2F;O failures by implementing a SIGBUS handler, especially a database server.<p>For the second part of your comment, on Linux systems, there is the msync() system call that can be used to flush the page cache on demand.</div><br/><div id="36566289" class="c"><input type="checkbox" id="c-36566289" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36563437">root</a><span>|</span><a href="#36563898">parent</a><span>|</span><a href="#36565028">next</a><span>|</span><label class="collapse" for="c-36566289">[-]</label><label class="expand" for="c-36566289">[1 more]</label></div><br/><div class="children"><div class="content">&gt; msync() system call that can be used to flush the page cache on demand.<p><i>for everyone</i>, not just the file you mapped to memory.  I.e. the guarantee is that your file will be written, but there&#x27;s no way to do that w&#x2F;o affecting others.  This is not such a hot idea in an environment where multiple threads &#x2F; processes are doing I&#x2F;O.</div><br/></div></div></div></div><div id="36565028" class="c"><input type="checkbox" id="c-36565028" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36563437">parent</a><span>|</span><a href="#36563898">prev</a><span>|</span><a href="#36563695">next</a><span>|</span><label class="collapse" for="c-36565028">[-]</label><label class="expand" for="c-36565028">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how many apps don&#x27;t handle errors from read() anyway.</div><br/></div></div><div id="36563695" class="c"><input type="checkbox" id="c-36563695" checked=""/><div class="controls bullet"><span class="by">sidewndr46</span><span>|</span><a href="#36563437">parent</a><span>|</span><a href="#36565028">prev</a><span>|</span><a href="#36563438">next</a><span>|</span><label class="collapse" for="c-36563695">[-]</label><label class="expand" for="c-36563695">[2 more]</label></div><br/><div class="children"><div class="content">does that get delivered as SIGSEGV to the process or something else?</div><br/><div id="36563769" class="c"><input type="checkbox" id="c-36563769" checked=""/><div class="controls bullet"><span class="by">afr0ck</span><span>|</span><a href="#36563437">root</a><span>|</span><a href="#36563695">parent</a><span>|</span><a href="#36563438">next</a><span>|</span><label class="collapse" for="c-36563769">[-]</label><label class="expand" for="c-36563769">[1 more]</label></div><br/><div class="children"><div class="content">On Linux, it&#x27;s a SIGBUS.</div><br/></div></div></div></div></div></div><div id="36563438" class="c"><input type="checkbox" id="c-36563438" checked=""/><div class="controls bullet"><span class="by">zffr</span><span>|</span><a href="#36563437">prev</a><span>|</span><a href="#36563582">next</a><span>|</span><label class="collapse" for="c-36563438">[-]</label><label class="expand" for="c-36563438">[1 more]</label></div><br/><div class="children"><div class="content">The TLDR is that MMAP sorta does what you want, but DBMSes need more control over how&#x2F;when data is paged in&#x2F;out of memory. Without this extra control, there can be issues with transactional safety, and performance.</div><br/></div></div><div id="36563582" class="c"><input type="checkbox" id="c-36563582" checked=""/><div class="controls bullet"><span class="by">jasonhansel</span><span>|</span><a href="#36563438">prev</a><span>|</span><a href="#36565105">next</a><span>|</span><label class="collapse" for="c-36563582">[-]</label><label class="expand" for="c-36563582">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve become convinced that there are very few, if any, reasons to MMAP a file on disk. It seems to simplify things in the common case, but in the end it adds a massive amount of unnecessary complexity.</div><br/><div id="36563846" class="c"><input type="checkbox" id="c-36563846" checked=""/><div class="controls bullet"><span class="by">vvanders</span><span>|</span><a href="#36563582">parent</a><span>|</span><a href="#36563741">next</a><span>|</span><label class="collapse" for="c-36563846">[-]</label><label class="expand" for="c-36563846">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s incredibly useful in read-only, memory constrained scenarios. I.E. we used to mmap all of our animation data on many rendering engines I worked on where having ~20-50mb of animation data and only &quot;paying&quot; a couple 10s of kb based on usage patterns was very handy. It becomes even more powerful when you have multiple processes sharing that data and the kernel is able to re-use clean pages across processes.<p>From reading the paper most of the concerns are around the write side. LMDB is the primary implementation that I know which leans heavily into mmap but it also comes with a number of constraints there(single writer, read locks can lead to unbounded appending to the WAL, etc). As with any tech choice it&#x27;s about knowing constraints&#x2F;trade-offs and making appropriate choices for your domain.</div><br/></div></div><div id="36563741" class="c"><input type="checkbox" id="c-36563741" checked=""/><div class="controls bullet"><span class="by">AnotherGoodName</span><span>|</span><a href="#36563582">parent</a><span>|</span><a href="#36563846">prev</a><span>|</span><a href="#36565559">next</a><span>|</span><label class="collapse" for="c-36563741">[-]</label><label class="expand" for="c-36563741">[3 more]</label></div><br/><div class="children"><div class="content">Complexity? You mmap it in and then read the multi terrabyte file as if it was an array.<p>The opposite with actual file io sucks in terms of complexity. I get that you can write bespoke code that performs better but mmap is a one liner to turn a file into an array.</div><br/><div id="36564520" class="c"><input type="checkbox" id="c-36564520" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#36563582">root</a><span>|</span><a href="#36563741">parent</a><span>|</span><a href="#36565952">next</a><span>|</span><label class="collapse" for="c-36564520">[-]</label><label class="expand" for="c-36564520">[1 more]</label></div><br/><div class="children"><div class="content">Need to handle the exceptions&#x2F;signals every time a disk read fails.  With classic IO, you know when the read will happen.  But with memory-mapped files, the exception can happen at any time you are reading from the memory range.<p>As for why disk reads fail, yes that&#x27;s a thing.  Less common on internal storage (bad sectors), but more common on removable USB devices or Network drives (especially on wifi).</div><br/></div></div><div id="36565952" class="c"><input type="checkbox" id="c-36565952" checked=""/><div class="controls bullet"><span class="by">Sesse__</span><span>|</span><a href="#36563582">root</a><span>|</span><a href="#36563741">parent</a><span>|</span><a href="#36564520">prev</a><span>|</span><a href="#36565559">next</a><span>|</span><label class="collapse" for="c-36565952">[-]</label><label class="expand" for="c-36565952">[1 more]</label></div><br/><div class="children"><div class="content">Multi-terabyte? Better hope you have lots of spare RAM for all those page structures the kernel has to keep.</div><br/></div></div></div></div><div id="36565559" class="c"><input type="checkbox" id="c-36565559" checked=""/><div class="controls bullet"><span class="by">gavinray</span><span>|</span><a href="#36563582">parent</a><span>|</span><a href="#36563741">prev</a><span>|</span><a href="#36567213">next</a><span>|</span><label class="collapse" for="c-36565559">[-]</label><label class="expand" for="c-36565559">[1 more]</label></div><br/><div class="children"><div class="content">&quot;mmap&quot; in the general case is incredibly useful.<p>There&#x27;s so much you get &quot;for free&quot; and the UX&#x2F;DX of reads&#x2F;writes to it, especially if you&#x27;re primarily operating on structs instead of raw byte&#x2F;string data.<p>(Example, reading a file and &quot;reinterpret_cast&lt;&gt;&quot;&#x27;ing it from bytes to in-memory struct representations)<p>It&#x27;s just that for the _particular_ case of a DBMS that relies on optimal I&#x2F;O and transactionality, the general-purpose kernel implementation of mmap falls short of what you can implement by hand.</div><br/></div></div><div id="36567213" class="c"><input type="checkbox" id="c-36567213" checked=""/><div class="controls bullet"><span class="by">neerajsi</span><span>|</span><a href="#36563582">parent</a><span>|</span><a href="#36565559">prev</a><span>|</span><a href="#36563964">next</a><span>|</span><label class="collapse" for="c-36567213">[-]</label><label class="expand" for="c-36567213">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been thinking for the past few years about how to get a scenario like &#x27;git clone&#x27; of a large repo to go fast. One thought is to memory map the destination files being written by git and then copy&#x2F;unzip the data there. You&#x27;d save a copy versus the staging buffer that you&#x27;d currently be passing to write().  However, the overhead of managing the tlb shootdowns would likely be fatal except for the largest output files.</div><br/></div></div><div id="36563964" class="c"><input type="checkbox" id="c-36563964" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#36563582">parent</a><span>|</span><a href="#36567213">prev</a><span>|</span><a href="#36565105">next</a><span>|</span><label class="collapse" for="c-36563964">[-]</label><label class="expand" for="c-36563964">[1 more]</label></div><br/><div class="children"><div class="content">if you truss starting up a binary, the OS normally mmaps the binary, at least in tests i ran.</div><br/></div></div></div></div><div id="36565105" class="c"><input type="checkbox" id="c-36565105" checked=""/><div class="controls bullet"><span class="by">benlivengood</span><span>|</span><a href="#36563582">prev</a><span>|</span><a href="#36563712">next</a><span>|</span><label class="collapse" for="c-36565105">[-]</label><label class="expand" for="c-36565105">[6 more]</label></div><br/><div class="children"><div class="content">For all of its usefulness in the good old days of rusty disks I wonder if virtual memory is worth having for dedicated databases, caches, and storage heads.  Avoiding TLB flushes entirely sounds like a huge win for massively multithreaded software and memory management in a large shared flat address space doesn&#x27;t sound impossibly hard.</div><br/><div id="36568826" class="c"><input type="checkbox" id="c-36568826" checked=""/><div class="controls bullet"><span class="by">justin_</span><span>|</span><a href="#36565105">parent</a><span>|</span><a href="#36567640">next</a><span>|</span><label class="collapse" for="c-36568826">[-]</label><label class="expand" for="c-36568826">[1 more]</label></div><br/><div class="children"><div class="content">This is the kind of debate that has been going on surrounding virtual memory forever[0][1]. If you can keep everything in memory, then you&#x27;re golden. But eventually you won&#x27;t, and you&#x27;ll need to rely on secondary storage.<p>Is there a performance benefit to be had by managing the memory and paging yourself? Yes. But eventually you will also consider running processes next to your database, for logging, auditing, ingesting data, running backups, etc. Virtual memory across the whole system helps with that, especially if other people will be using your database in ways you can&#x27;t predict. As for the efficiency of MMUs and the OS, seems like for almost all cases it&#x27;s &quot;satisfactory&quot; enough[1].<p>[0] <a href="http:&#x2F;&#x2F;denninginstitute.com&#x2F;pjd&#x2F;PUBS&#x2F;bvm.pdf" rel="nofollow noreferrer">http:&#x2F;&#x2F;denninginstitute.com&#x2F;pjd&#x2F;PUBS&#x2F;bvm.pdf</a><p>[1] From 1969! <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;363626.363629" rel="nofollow noreferrer">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;363626.363629</a></div><br/></div></div><div id="36567640" class="c"><input type="checkbox" id="c-36567640" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36565105">parent</a><span>|</span><a href="#36568826">prev</a><span>|</span><a href="#36563712">next</a><span>|</span><label class="collapse" for="c-36567640">[-]</label><label class="expand" for="c-36567640">[4 more]</label></div><br/><div class="children"><div class="content">The jump in address sizes starts to get too unwieldy. 32 bit addresses were ok, 64 bit addresses start to get clunky, 128 bit would be exorbitant for CPU real estate. There&#x27;s a reason AMD64 still only supported 40 physical address bits when it was introduced, and later only expanded to 48 bits.<p>The reality is there will always be a hierarchy for storage, and paging will always be the best mechanism to deal with it. Because primary memory will always be most expensive, no matter what technology it&#x27;s based on. There will always be something slower, cheaper, and denser that will be used for secondary storage. There will <i>always</i> be cheaper storage. And its capacity will exceed primary, and it will always be most efficient to reference secondary storage in chunks - pages - and not at individual byte addresses.</div><br/><div id="36567794" class="c"><input type="checkbox" id="c-36567794" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#36565105">root</a><span>|</span><a href="#36567640">parent</a><span>|</span><a href="#36563712">next</a><span>|</span><label class="collapse" for="c-36567794">[-]</label><label class="expand" for="c-36567794">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really see what those two things have to do with each other.  When you don&#x27;t use mmap, you manage the disc&lt;-&gt;ram storage virtualisation yourself.  Hardware paging, then, is pure overhead.  The parent doesn&#x27;t argue against layering of storage media, nor against chunking in general.  Only against mmus as a mechanism for implementing it.</div><br/><div id="36568011" class="c"><input type="checkbox" id="c-36568011" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#36565105">root</a><span>|</span><a href="#36567794">parent</a><span>|</span><a href="#36563712">next</a><span>|</span><label class="collapse" for="c-36568011">[-]</label><label class="expand" for="c-36568011">[2 more]</label></div><br/><div class="children"><div class="content">The mention of a large shared flat address space implied no paging, to me. Maybe I just read something into it that wasn&#x27;t there.</div><br/><div id="36568239" class="c"><input type="checkbox" id="c-36568239" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#36565105">root</a><span>|</span><a href="#36568011">parent</a><span>|</span><a href="#36563712">next</a><span>|</span><label class="collapse" for="c-36568239">[-]</label><label class="expand" for="c-36568239">[1 more]</label></div><br/><div class="children"><div class="content">The &#x27;paging&#x27; is implemented in software, not in hardware.  This is how databases not using mmap already work, so mmus are already pure overhead for them.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36563712" class="c"><input type="checkbox" id="c-36563712" checked=""/><div class="controls bullet"><span class="by">AnotherGoodName</span><span>|</span><a href="#36565105">prev</a><span>|</span><a href="#36563792">next</a><span>|</span><label class="collapse" for="c-36563712">[-]</label><label class="expand" for="c-36563712">[11 more]</label></div><br/><div class="children"><div class="content">A well written bespoke function can beat a generalized function at a specific task.<p>If you have the resources to write and maintain the bespoke method great. The large database developers probably have this. For others please don&#x27;t take this link and go around claiming mmap is bad though. That gets tiresome and is misguided. Mmap is a shortcut to access large files in a non linear fashion. It&#x27;s good at that too. Just not as good as a bespoke function.</div><br/><div id="36563766" class="c"><input type="checkbox" id="c-36563766" checked=""/><div class="controls bullet"><span class="by">dist1ll</span><span>|</span><a href="#36563712">parent</a><span>|</span><a href="#36563738">next</a><span>|</span><label class="collapse" for="c-36563766">[-]</label><label class="expand" for="c-36563766">[1 more]</label></div><br/><div class="children"><div class="content">This paper isn&#x27;t aimed at random developers, and it&#x27;s not a criticism of mmap in general.<p>This is an appeal to core database engineers to stop using the wrong tool for the job.</div><br/></div></div><div id="36563738" class="c"><input type="checkbox" id="c-36563738" checked=""/><div class="controls bullet"><span class="by">formerly_proven</span><span>|</span><a href="#36563712">parent</a><span>|</span><a href="#36563766">prev</a><span>|</span><a href="#36563792">next</a><span>|</span><label class="collapse" for="c-36563738">[-]</label><label class="expand" for="c-36563738">[9 more]</label></div><br/><div class="children"><div class="content">mmap can be handy but usually is not a good idea when you care about ACID properties. So it tends to be most useful outside databases.</div><br/><div id="36563848" class="c"><input type="checkbox" id="c-36563848" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#36563712">root</a><span>|</span><a href="#36563738">parent</a><span>|</span><a href="#36563792">next</a><span>|</span><label class="collapse" for="c-36563848">[-]</label><label class="expand" for="c-36563848">[8 more]</label></div><br/><div class="children"><div class="content">Can you give some examples where mmap is useful?</div><br/><div id="36564063" class="c"><input type="checkbox" id="c-36564063" checked=""/><div class="controls bullet"><span class="by">duped</span><span>|</span><a href="#36563712">root</a><span>|</span><a href="#36563848">parent</a><span>|</span><a href="#36563929">next</a><span>|</span><label class="collapse" for="c-36564063">[-]</label><label class="expand" for="c-36564063">[1 more]</label></div><br/><div class="children"><div class="content">I once improved a parser&#x27;s performance a huge amount (iirc, something like 500x) when parsing large (&gt;1GB) text files by mmap&#x27;ing the files instead of reading them into a byte array. It&#x27;s not a magic bullet but it was alright for that application.<p>Another technique that can only be done with mmap is to map two contiguous regions of virtual memory to the same underlying buffer. This allows you to use a ring buffer but only read from&#x2F;write to what looks like a contiguous region of memory.</div><br/></div></div><div id="36563929" class="c"><input type="checkbox" id="c-36563929" checked=""/><div class="controls bullet"><span class="by">dataflow</span><span>|</span><a href="#36563712">root</a><span>|</span><a href="#36563848">parent</a><span>|</span><a href="#36564063">prev</a><span>|</span><a href="#36566340">next</a><span>|</span><label class="collapse" for="c-36563929">[-]</label><label class="expand" for="c-36563929">[2 more]</label></div><br/><div class="children"><div class="content">If your data is likely to already be in the system cache, memory mapping can achieve zero copying of the data, whereas reading will perform at least one memcpy. So there can be a performance advantage depending on the usage pattern.<p>Also, I&#x27;ve never tested this, but I believe mapped files will get flushed as long as the system stays running. So if you only need resilience against abnormal termination rather than system crashes, it seems like a good option?</div><br/><div id="36564794" class="c"><input type="checkbox" id="c-36564794" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#36563712">root</a><span>|</span><a href="#36563929">parent</a><span>|</span><a href="#36566340">next</a><span>|</span><label class="collapse" for="c-36564794">[-]</label><label class="expand" for="c-36564794">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Also, I&#x27;ve never tested this, but I believe mapped files will get flushed as long as the system stays running. So if you only need resilience against abnormal termination rather than system crashes, it seems like a good option?<p>Linux will not lose data written to a MAP_SHARED mapping when the process crashes.<p>But! Linux <i>will</i> synchronously update mtime when starting to write to a currently write protected mapping (e.g. one which was just written out).  This means (a) POSIX is violated (IMO) and (b) what should be a minor fault to enable writes turns into an actual metadata write, which can cause actual synchronous IO.<p>I have an ancient patch set to fix this, but I never got it all the way into upstream Linux.<p>What you <i>can</i> do is mmap a file on a tmpfs as long as you trust yourself to have some other reliable process handle the data even if your application terminates abnormally. This is awkward with a container solution if you need to survive termination of the entire container.</div><br/></div></div></div></div><div id="36566340" class="c"><input type="checkbox" id="c-36566340" checked=""/><div class="controls bullet"><span class="by">whartung</span><span>|</span><a href="#36563712">root</a><span>|</span><a href="#36563848">parent</a><span>|</span><a href="#36563929">prev</a><span>|</span><a href="#36564023">next</a><span>|</span><label class="collapse" for="c-36566340">[-]</label><label class="expand" for="c-36566340">[1 more]</label></div><br/><div class="children"><div class="content">Itâs useful in a world of several processes sharing things. This is much less common today in a world of âsingle processâ containers and VMs as well as monolithic processes using threads or async techniques.<p>However, Java can build a special library file of the core JRE classes that it can mmap into memory with the intent to speed up startup times, mostly for small Java programs.<p>Guile scheme will mmap files that have been compiled to byte code. You can visualize a contrived (especially today) scenario where Guile is used for CGI handlers, having the bulk of their code mapped, the overall memory impact of simultaneous handlers is much lower, as well as start up times.<p>The process model is less common today so the value of this goes down, but it can still have its place.</div><br/></div></div><div id="36564023" class="c"><input type="checkbox" id="c-36564023" checked=""/><div class="controls bullet"><span class="by">fatboy</span><span>|</span><a href="#36563712">root</a><span>|</span><a href="#36563848">parent</a><span>|</span><a href="#36566340">prev</a><span>|</span><a href="#36567628">next</a><span>|</span><label class="collapse" for="c-36564023">[-]</label><label class="expand" for="c-36564023">[1 more]</label></div><br/><div class="children"><div class="content">One place I&#x27;ve seen it used was a lib by a guy called DHoerl for reading images that are too big to fit in memory (this was years ago on iOS).<p>A very over-simplified and probably a bit incorrect description of what it did was to create a smaller version of the image - one that could fit in memory - by sub sampling every nth pixel, which was addressed via mmap.<p>It actually dealt with jpegs so I have no idea how that bit worked as they are not bitmaps.</div><br/></div></div><div id="36567628" class="c"><input type="checkbox" id="c-36567628" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#36563712">root</a><span>|</span><a href="#36563848">parent</a><span>|</span><a href="#36564023">prev</a><span>|</span><a href="#36563896">next</a><span>|</span><label class="collapse" for="c-36567628">[-]</label><label class="expand" for="c-36567628">[1 more]</label></div><br/><div class="children"><div class="content">glibc itself uses mmap under the covers when doing malloc in certain situations. Granted, it&#x27;s anonymous and not file-backed, but it&#x27;s still proven to be performant. See, e.g, mallopt(3).</div><br/></div></div></div></div></div></div></div></div><div id="36563792" class="c"><input type="checkbox" id="c-36563792" checked=""/><div class="controls bullet"><span class="by">SoftTalker</span><span>|</span><a href="#36563712">prev</a><span>|</span><a href="#36563627">next</a><span>|</span><label class="collapse" for="c-36563792">[-]</label><label class="expand" for="c-36563792">[1 more]</label></div><br/><div class="children"><div class="content">This reads more like &quot;don&#x27;t write your own DBMS&quot; than &quot;don&#x27;t use mmap.&quot;</div><br/></div></div><div id="36563627" class="c"><input type="checkbox" id="c-36563627" checked=""/><div class="controls bullet"><span class="by">jFriedensreich</span><span>|</span><a href="#36563792">prev</a><span>|</span><a href="#36563907">next</a><span>|</span><label class="collapse" for="c-36563627">[-]</label><label class="expand" for="c-36563627">[5 more]</label></div><br/><div class="children"><div class="content">maybe a stupid question but what is wrong with coffee and spicy food?</div><br/><div id="36567122" class="c"><input type="checkbox" id="c-36567122" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#36563627">parent</a><span>|</span><a href="#36563651">next</a><span>|</span><label class="collapse" for="c-36567122">[-]</label><label class="expand" for="c-36567122">[1 more]</label></div><br/><div class="children"><div class="content">to put it crudely I think the punchline is the spicy food hurts on the way out, and the coffee makes that happen with greater velocity</div><br/></div></div><div id="36563651" class="c"><input type="checkbox" id="c-36563651" checked=""/><div class="controls bullet"><span class="by">orf</span><span>|</span><a href="#36563627">parent</a><span>|</span><a href="#36567122">prev</a><span>|</span><a href="#36564536">next</a><span>|</span><label class="collapse" for="c-36563651">[-]</label><label class="expand" for="c-36563651">[2 more]</label></div><br/><div class="children"><div class="content">For the majority of the world, nothing. But if your diet consists of fairly bland food then it can result in unpleasant trips to the toilet.</div><br/><div id="36563668" class="c"><input type="checkbox" id="c-36563668" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#36563627">root</a><span>|</span><a href="#36563651">parent</a><span>|</span><a href="#36564536">next</a><span>|</span><label class="collapse" for="c-36563668">[-]</label><label class="expand" for="c-36563668">[1 more]</label></div><br/><div class="children"><div class="content">Acid reflux I thought</div><br/></div></div></div></div><div id="36564536" class="c"><input type="checkbox" id="c-36564536" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#36563627">parent</a><span>|</span><a href="#36563651">prev</a><span>|</span><a href="#36563907">next</a><span>|</span><label class="collapse" for="c-36564536">[-]</label><label class="expand" for="c-36564536">[1 more]</label></div><br/><div class="children"><div class="content">Just doesnât taste good together I think</div><br/></div></div></div></div></div></div></div></div></div></body></html>