<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691053252956" as="style"/><link rel="stylesheet" href="styles.css?v=1691053252956"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="http://datacolada.org/78a">I don&#x27;t use Bayes factors in my research (2019)</a> <span class="domain">(<a href="http://datacolada.org">datacolada.org</a>)</span></div><div class="subtext"><span>jasonhansel</span> | <span>35 comments</span></div><br/><div><div id="36981889" class="c"><input type="checkbox" id="c-36981889" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#36983057">next</a><span>|</span><label class="collapse" for="c-36981889">[-]</label><label class="expand" for="c-36981889">[3 more]</label></div><br/><div class="children"><div class="content">I’ve been working a lot with Bayes factors lately. I don’t want to sound cultish, but I think part of the issue is this stuff doesn’t work “half way”. As soon as you’re talking about the null hypothesis and Bayes factors, you’re mixing up two schools of thought that don’t play nice.<p>Bayes factors work with comparing models. There is no null model. What, 0% effect? Ok, there was a non-zero effect. That model loses since it put the probability of 0% at 1 and everything else at 0. And if you do <i>anything else</i>, you’re encoding some amount of belief into the model, some judgment you’ve made.<p>So, you need to pick two models and compare them. I’m not saying this is right for science. It’s working well for my purposes. One model meaning “as planned”, one model meaning “not as planned”, use the Bayes factor to decide if things are going as planned. But you do need to be explicit about what models you’re comparing. You have to be able to just put some data in and get a probability back, or it’s not going to work.<p>This is what makes this criticism of Bayes factors so unpersuasive. They’re very easy to calculate, but they’re never calculated here! It’s just the ratio of marginal likelihoods, the probability of the data under the model.</div><br/><div id="36982340" class="c"><input type="checkbox" id="c-36982340" checked=""/><div class="controls bullet"><span class="by">LudwigNagasena</span><span>|</span><a href="#36981889">parent</a><span>|</span><a href="#36983057">next</a><span>|</span><label class="collapse" for="c-36982340">[-]</label><label class="expand" for="c-36982340">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Bayes factors work with comparing models.<p>So does the traditional Neyman–Pearson hypothesis testing.<p>&gt; There is no null model.<p>Why can’t there be?<p>&gt; What, 0% effect? Ok, there was a non-zero effect. That model loses since it put the probability of 0% at 1 and everything else at 0%.<p>Well, if your null hypothesis is deterministic and says 0% effect, getting anything other than 0% absolutely will make you reject the null hypothesis. But most of the time hypotheses are not deterministic. Usually you sample random variables.<p>&gt; And if you do anything else, you’re encoding some amount of belief into the model, some judgment you’ve made.<p>Traditional hypothesis testing is a particular case of minimising risk, ie the expected value of your loss given possible models and your decision rule. You don’t assume any belief on the probability of a specific model to be true, thus I think it is incorrect to claim that you encode a belief. You don’t even claim that it <i>can</i> be measured.<p>Of course, that makes it impossible to quantify the risk over all possible models. Thus, you only deal with Type I and Type II errors, which values presumes that the null or alternative hypothesis is correct.<p>If you have a probability measure for models, you can simply average your risks over it and get what is known as Bayes risk. <i>That</i> would be encoding some belief.</div><br/><div id="36982745" class="c"><input type="checkbox" id="c-36982745" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#36981889">root</a><span>|</span><a href="#36982340">parent</a><span>|</span><a href="#36983057">next</a><span>|</span><label class="collapse" for="c-36982745">[-]</label><label class="expand" for="c-36982745">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve sliced up what I&#x27;ve said to the point it doesn&#x27;t really make sense. This is exactly what I said was confusing. I&#x27;m talking about Bayes factors and you&#x27;re talking about null hypothesis testing.<p>I&#x27;ll just answer your question, why there can&#x27;t be a null model. You can have a hypothesis that represents all differences between groups are due to chance. To make this a statistical model, something that can calculate the probability of an event, you have to make assumptions. Maybe it&#x27;s just about the distribution. Maybe it&#x27;s independence. But, it&#x27;s always something. You said it yourself &quot;You don’t assume any belief on the probability of a specific model to be true.&quot; To be a statistical model, to calculate the probability of an event, to calculate marginal likelihoods, to calculate Bayes factors, you have to do that.<p>This is largely a philosophical point. You can have <i>a</i> null model. Something you pick to represent &quot;no effect&quot;. But there&#x27;s not <i>the</i> null, this belief free model that&#x27;s categorically different from a model with priors.<p>If there&#x27;s a belief-free model that can give a marginal likelihood, then I&#x27;m wrong. I&#x27;d also very much like to know about it.</div><br/></div></div></div></div></div></div><div id="36983057" class="c"><input type="checkbox" id="c-36983057" checked=""/><div class="controls bullet"><span class="by">civilized</span><span>|</span><a href="#36981889">prev</a><span>|</span><a href="#36981447">next</a><span>|</span><label class="collapse" for="c-36983057">[-]</label><label class="expand" for="c-36983057">[1 more]</label></div><br/><div class="children"><div class="content">Bayesian methods are not easy to use, I agree. But that&#x27;s because they&#x27;re trying to answer much more meaningful (but harder) questions than frequentist ones, which researchers <i>should</i> be trying to do. You can&#x27;t ignore Bayesian epistemology just by not using Bayesian methods. The underlying considerations in the Bayesian framework will inevitably become relevant to how you interpret your data, whether or not you use a formal Bayesian method.<p>The thing is, formally, frequentist methods don&#x27;t tell you anything. If you get a significant p-value, that means the data you observed wouldn&#x27;t often happen by chance (within your model of the null). This doesn&#x27;t actually tell you if your particular hypothesis should be favored. That requires other considerations, including ones that Simonsohn is negative about in this article.<p>For example, Simonsohn&#x27;s conclusion says:<p>&gt; To use Bayes factors to test hypotheses: you need to be OK with the following two things:<p>&gt; 1. Accepting the null when “the alternative” you consider, and reject, does not represent the theory of interest.<p>&gt; 2. Rejecting a theory after observing an outcome that the theory predicts.<p>He implies that these should be points against Bayes factors. But #2 is something you actually <i>should</i> do sometimes. Demonstrably. In particular, if the data suggests a wildly implausible effect size, that <i>should</i> be a point against your theory and in favor of some more mundane explanation, like noisy data from an underpowered study [1].<p>Not using Bayesian methods is understandable if you don&#x27;t feel comfortable with the very heavy demands they can make on your statistics acumen. But if you&#x27;re a social scientist incentivized to get &quot;sexy&quot; results and you refuse to <i>engage</i> with Bayesian epistemology at all, your career will almost certainly just be a contribution of more noise publications to the replication crisis.<p>[1] <a href="http:&#x2F;&#x2F;www.stat.columbia.edu&#x2F;~gelman&#x2F;presentations&#x2F;ziff.pdf" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.stat.columbia.edu&#x2F;~gelman&#x2F;presentations&#x2F;ziff.pdf</a></div><br/></div></div><div id="36981447" class="c"><input type="checkbox" id="c-36981447" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36983057">prev</a><span>|</span><a href="#36981631">next</a><span>|</span><label class="collapse" for="c-36981447">[-]</label><label class="expand" for="c-36981447">[9 more]</label></div><br/><div class="children"><div class="content">The statistical interpretation of observations is so subtle and complex that it&#x27;s a good idea to assume that any publication from the empirical sciences is complete garbage, until you know for sure that a qualified statistician has supervised the process. A semester of &quot;introduction to statistical methods&quot; (which is all the background that most scientists have) is <i>NOT</i> enough.<p>Imagine a mathematician writing a paper on a medical topic, making all kinds of claims on how things work in the human body – and then that mathematician justifies their expertise by saying &quot;I did a two-week first aid course once, and also, I was really good at biology in school&quot;. This is pretty much how lots of science operates when it comes to interpreting results mathematically.</div><br/><div id="36981759" class="c"><input type="checkbox" id="c-36981759" checked=""/><div class="controls bullet"><span class="by">concinds</span><span>|</span><a href="#36981447">parent</a><span>|</span><a href="#36981511">next</a><span>|</span><label class="collapse" for="c-36981759">[-]</label><label class="expand" for="c-36981759">[5 more]</label></div><br/><div class="children"><div class="content">I read a survey once, that found that a huge number of PhDs&#x2F;researchers in the studied sample gave an incorrect definition for what a &quot;95% confidence interval&quot; (&#x2F;p-value, etc) actually means, and that several popular introductory textbooks defined it incorrectly as well. Wish I bookmarked it.<p>At bare minimum, journals need to require that researchers publish all their data alongside every paper, so statistical analyses can be redone and flaws can be spotted.</div><br/><div id="36982900" class="c"><input type="checkbox" id="c-36982900" checked=""/><div class="controls bullet"><span class="by">bakuninsbart</span><span>|</span><a href="#36981447">root</a><span>|</span><a href="#36981759">parent</a><span>|</span><a href="#36981839">next</a><span>|</span><label class="collapse" for="c-36982900">[-]</label><label class="expand" for="c-36982900">[1 more]</label></div><br/><div class="children"><div class="content">While I agree on the data point, it would kill so much research. It is bad that a lot of research validation basically comes down to &quot;trust me guys&quot;, but with data being both very valuable and often times highly sensitive, it can be really difficult to just publish the data along with the research.<p>A decent compromise would be to at least require meta-data to sufficiently exclude some flaws. A different approach could be to have researchers document and publish the <i>process</i> of th research, similar to a git-repo with the main branch being completely off limits to history-rewriting.</div><br/></div></div><div id="36981839" class="c"><input type="checkbox" id="c-36981839" checked=""/><div class="controls bullet"><span class="by">constantcrying</span><span>|</span><a href="#36981447">root</a><span>|</span><a href="#36981759">parent</a><span>|</span><a href="#36982900">prev</a><span>|</span><a href="#36981847">next</a><span>|</span><label class="collapse" for="c-36981839">[-]</label><label class="expand" for="c-36981839">[1 more]</label></div><br/><div class="children"><div class="content">I think you may be talking about &quot;Mindless statistics&quot; by Gigerenzer. He has some surveys about p-values and how radically wrong they are usually interpreted.<p>&gt;At bare minimum, journals need to require that researchers publish all their data alongside every paper, so statistical analyses can be redone and flaws can be spotted.<p>Absolutely.</div><br/></div></div><div id="36981847" class="c"><input type="checkbox" id="c-36981847" checked=""/><div class="controls bullet"><span class="by">samch93</span><span>|</span><a href="#36981447">root</a><span>|</span><a href="#36981759">parent</a><span>|</span><a href="#36981839">prev</a><span>|</span><a href="#36981789">next</a><span>|</span><label class="collapse" for="c-36981847">[-]</label><label class="expand" for="c-36981847">[1 more]</label></div><br/><div class="children"><div class="content">probably this article:
Hoekstra, R., Morey, R.D., Rouder, J.N. et al. Robust misinterpretation of confidence intervals. Psychon Bull Rev 21, 1157–1164 (2014). <a href="https:&#x2F;&#x2F;doi.org&#x2F;10.3758&#x2F;s13423-013-0572-3" rel="nofollow noreferrer">https:&#x2F;&#x2F;doi.org&#x2F;10.3758&#x2F;s13423-013-0572-3</a><p>another good article on misinterpretation of p-values and confidence intervals is:
Greenland, S., Senn, S.J., Rothman, K.J. et al. Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. Eur J Epidemiol 31, 337–350 (2016). <a href="https:&#x2F;&#x2F;doi.org&#x2F;10.1007&#x2F;s10654-016-0149-3" rel="nofollow noreferrer">https:&#x2F;&#x2F;doi.org&#x2F;10.1007&#x2F;s10654-016-0149-3</a></div><br/></div></div></div></div><div id="36981511" class="c"><input type="checkbox" id="c-36981511" checked=""/><div class="controls bullet"><span class="by">blablabla123</span><span>|</span><a href="#36981447">parent</a><span>|</span><a href="#36981759">prev</a><span>|</span><a href="#36981512">next</a><span>|</span><label class="collapse" for="c-36981511">[-]</label><label class="expand" for="c-36981511">[2 more]</label></div><br/><div class="children"><div class="content">On the other hand it seems there&#x27;s also a lack of testing subjects. It&#x27;s already frequently pointed out how medical results might not represent everyone. I would also assume that e.g. pharmaceutical certification processes do apply more sophisticated statistics.</div><br/><div id="36981585" class="c"><input type="checkbox" id="c-36981585" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36981447">root</a><span>|</span><a href="#36981511">parent</a><span>|</span><a href="#36981512">next</a><span>|</span><label class="collapse" for="c-36981585">[-]</label><label class="expand" for="c-36981585">[1 more]</label></div><br/><div class="children"><div class="content">The ugly truth is that lots of &quot;science&quot; being done today isn&#x27;t actually science – it&#x27;s a performance art that superficially imitates certain behaviors that are associated with real science.<p>And how could it be otherwise? There are nearly 10 million scientists in the world right now. And all of them are pushing out papers as fast as humanly possible. There isn&#x27;t <i>anywhere near</i> enough statistical brainpower available to quality-control all of that. Not to mention that most people with sufficient expertise in statistics have better things to do than micromanaging science grad students who have a hard time comprehending Bayes&#x27; theorem.</div><br/></div></div></div></div><div id="36981512" class="c"><input type="checkbox" id="c-36981512" checked=""/><div class="controls bullet"><span class="by">KRAKRISMOTT</span><span>|</span><a href="#36981447">parent</a><span>|</span><a href="#36981511">prev</a><span>|</span><a href="#36981631">next</a><span>|</span><label class="collapse" for="c-36981512">[-]</label><label class="expand" for="c-36981512">[1 more]</label></div><br/><div class="children"><div class="content">Ah, I see you have not met a biophysicist.</div><br/></div></div></div></div><div id="36981631" class="c"><input type="checkbox" id="c-36981631" checked=""/><div class="controls bullet"><span class="by">edbaskerville</span><span>|</span><a href="#36981447">prev</a><span>|</span><a href="#36982350">next</a><span>|</span><label class="collapse" for="c-36981631">[-]</label><label class="expand" for="c-36981631">[1 more]</label></div><br/><div class="children"><div class="content">I screwed around with trying to compute Bayes factors for models of distributions over set partitions, having been led astray by Bayesian phylogenetic inference methods. It was a waste of time--in practice the epistemology was terrible because the choice of prior distributions had such a huge effect on model comparisons. On top of that, the computations were highly unstable so I had to do a lot of fancy multi-temperature MCMC stuff that never quite worked.<p>Unless your priors are based on actual observations, stick with model selection approaches that are based on measured predictive power, or at least plausible approximations thereof, e.g. Aki Vehtari et al. LOO-CV (approximate leave-one-out cross-validation):<p><a href="https:&#x2F;&#x2F;avehtari.github.io&#x2F;modelselection&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;avehtari.github.io&#x2F;modelselection&#x2F;</a><p><a href="https:&#x2F;&#x2F;mc-stan.org&#x2F;loo&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;mc-stan.org&#x2F;loo&#x2F;</a></div><br/></div></div><div id="36982350" class="c"><input type="checkbox" id="c-36982350" checked=""/><div class="controls bullet"><span class="by">MrManatee</span><span>|</span><a href="#36981631">prev</a><span>|</span><a href="#36983187">next</a><span>|</span><label class="collapse" for="c-36982350">[-]</label><label class="expand" for="c-36982350">[1 more]</label></div><br/><div class="children"><div class="content">If the minimum wage is increased $4, the competing explanations seem to be:<p>1. Change in unemployment is normally distributed with mean 0% and standard deviation 0.606%.<p>2. Change in unemployment is uniformly distributed between 1% and 10%.<p>I don&#x27;t really agree that &quot;(1) vs (2)&quot; is a particularly good formulation of the original question (&quot;Would raising the minimum wage by $4 lead to greater unemployment?&quot;). But if it were, how would the math work out?<p>If we observe that unemployment increases 1%, then yes, that piece of evidence is very slightly in favor of explanation (1). This doesn&#x27;t feel weird or paradoxical to me. But surely we wouldn&#x27;t want to decide the matter based just on that one inconclusive data point? Instead we would want to look at another instance of the same situation. If in that case an increase of, say, 6% would (almost) conclusively settle the matter in favor of (2), and an increase of, say, 0.8% would (absolutely) conclusively settle the matter in favor of (1).</div><br/></div></div><div id="36983187" class="c"><input type="checkbox" id="c-36983187" checked=""/><div class="controls bullet"><span class="by">frankreyes</span><span>|</span><a href="#36982350">prev</a><span>|</span><a href="#36981576">next</a><span>|</span><label class="collapse" for="c-36983187">[-]</label><label class="expand" for="c-36983187">[1 more]</label></div><br/><div class="children"><div class="content">Put the minimum wage at $1,000,000.<p>Then, watch unemployment go up.</div><br/></div></div><div id="36981576" class="c"><input type="checkbox" id="c-36981576" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#36983187">prev</a><span>|</span><a href="#36982671">next</a><span>|</span><label class="collapse" for="c-36981576">[-]</label><label class="expand" for="c-36981576">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Note: By theory I merely mean the rationale for investigating the effect of x on y. A theory can be as simple as “I think people value a mug more once they own it”.<p>Hoo boy, the [2019] is well deserved on this one -- that&#x27;s a dan arielly reference from before The 2021 Accusation and before the recent NPR story refuting his excuse[1].<p>[1]: <a href="https:&#x2F;&#x2F;www.npr.org&#x2F;2023&#x2F;07&#x2F;27&#x2F;1190568472&#x2F;dan-ariely-francesca-gino-harvard-dishonesty-fabricated-data" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.npr.org&#x2F;2023&#x2F;07&#x2F;27&#x2F;1190568472&#x2F;dan-ariely-frances...</a></div><br/><div id="36981791" class="c"><input type="checkbox" id="c-36981791" checked=""/><div class="controls bullet"><span class="by">neonate</span><span>|</span><a href="#36981576">parent</a><span>|</span><a href="#36982671">next</a><span>|</span><label class="collapse" for="c-36981791">[-]</label><label class="expand" for="c-36981791">[3 more]</label></div><br/><div class="children"><div class="content">What is the reference? I don&#x27;t get it.</div><br/><div id="36982025" class="c"><input type="checkbox" id="c-36982025" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#36981576">root</a><span>|</span><a href="#36981791">parent</a><span>|</span><a href="#36981965">next</a><span>|</span><label class="collapse" for="c-36982025">[-]</label><label class="expand" for="c-36982025">[1 more]</label></div><br/><div class="children"><div class="content">I could swear one of his research projects involved asking people to make a mug and pricing it out after they finished. But I guess it was Kahneman that researched mugs? Whoops</div><br/></div></div><div id="36981965" class="c"><input type="checkbox" id="c-36981965" checked=""/><div class="controls bullet"><span class="by">fsckboy</span><span>|</span><a href="#36981576">root</a><span>|</span><a href="#36981791">parent</a><span>|</span><a href="#36982025">prev</a><span>|</span><a href="#36982671">next</a><span>|</span><label class="collapse" for="c-36981965">[-]</label><label class="expand" for="c-36981965">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dan_Ariely#Accusations_of_data_fraud_and_academic_misconduct" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dan_Ariely#Accusations_of_data...</a></div><br/></div></div></div></div></div></div><div id="36982671" class="c"><input type="checkbox" id="c-36982671" checked=""/><div class="controls bullet"><span class="by">fridental</span><span>|</span><a href="#36981576">prev</a><span>|</span><a href="#36982075">next</a><span>|</span><label class="collapse" for="c-36982671">[-]</label><label class="expand" for="c-36982671">[1 more]</label></div><br/><div class="children"><div class="content">So you have just one data point and you want to do statistics about it? No matter what you do, the results won&#x27;t be useful.<p>In Bayesian approach, you start with some distribution that is a wild guess and doesn&#x27;t even need to base on any knowledge besides of the basics how money work and that unemployment cannot be 0% or 100%. Each data point will refine your distribution until at some dataset size, it will converge to something estimating the reality.<p>You might want to watch an amazingly helpful introduction by Richard McElreath here <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=guTdrfycW2Q">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=guTdrfycW2Q</a></div><br/></div></div><div id="36982075" class="c"><input type="checkbox" id="c-36982075" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#36982671">prev</a><span>|</span><a href="#36981524">next</a><span>|</span><label class="collapse" for="c-36982075">[-]</label><label class="expand" for="c-36982075">[1 more]</label></div><br/><div class="children"><div class="content">&gt; wait until you understand Bayes Factors<p>I&#x27;m not sure that piece will help people to understand Bayes Factors: <a href="https:&#x2F;&#x2F;statmodeling.stat.columbia.edu&#x2F;2019&#x2F;09&#x2F;10&#x2F;i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing&#x2F;#comment-1119100" rel="nofollow noreferrer">https:&#x2F;&#x2F;statmodeling.stat.columbia.edu&#x2F;2019&#x2F;09&#x2F;10&#x2F;i-hate-bay...</a><p>&gt; In social science, theory alone will not deliver one [hypothesis to test]<p>I guess it&#x27;s difficult to test a hypothesis when you don&#x27;t really have one.</div><br/></div></div><div id="36981391" class="c"><input type="checkbox" id="c-36981391" checked=""/><div class="controls bullet"><span class="by">Konohamaru</span><span>|</span><a href="#36981524">prev</a><span>|</span><a href="#36981538">next</a><span>|</span><label class="collapse" for="c-36981391">[-]</label><label class="expand" for="c-36981391">[9 more]</label></div><br/><div class="children"><div class="content">Milton Friedman was correct: because the true minimum wage is $0.00 (unemployment), he was correct to compare wage increase to the null hypothesis. The potshot in the opening paragraph (&quot;Milton feels bad about the unemployed but good about his theory.&quot;) is simultaneously an appeal to emotion and a presumptuous ad hominem.</div><br/><div id="36981819" class="c"><input type="checkbox" id="c-36981819" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#36981391">parent</a><span>|</span><a href="#36981807">next</a><span>|</span><label class="collapse" for="c-36981819">[-]</label><label class="expand" for="c-36981819">[2 more]</label></div><br/><div class="children"><div class="content">Potshot? It just seems like a joke, but one that puts this character (a nod to Milton Friedman but not like a serious insert) in a positive light. He’s pleased by being correct but sympathetic since he was right about something bad happening to people (unemployment).</div><br/><div id="36982064" class="c"><input type="checkbox" id="c-36982064" checked=""/><div class="controls bullet"><span class="by">systemvoltage</span><span>|</span><a href="#36981391">root</a><span>|</span><a href="#36981819">parent</a><span>|</span><a href="#36981807">next</a><span>|</span><label class="collapse" for="c-36982064">[-]</label><label class="expand" for="c-36982064">[1 more]</label></div><br/><div class="children"><div class="content">If anyone that’s taking potshots, it’s the author.</div><br/></div></div></div></div><div id="36981807" class="c"><input type="checkbox" id="c-36981807" checked=""/><div class="controls bullet"><span class="by">constantcrying</span><span>|</span><a href="#36981391">parent</a><span>|</span><a href="#36981819">prev</a><span>|</span><a href="#36981538">next</a><span>|</span><label class="collapse" for="c-36981807">[-]</label><label class="expand" for="c-36981807">[6 more]</label></div><br/><div class="children"><div class="content">The point isn&#x27;t that Friedman is right or wrong, but that the statistic model tells him to reject his hypothesis, even though he observed a result consistent with his hypothesis.<p>&gt;is simultaneously an appeal to emotion and a presumptuous ad hominem.<p>I don&#x27;t see how that is the case.</div><br/><div id="36981850" class="c"><input type="checkbox" id="c-36981850" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36981391">root</a><span>|</span><a href="#36981807">parent</a><span>|</span><a href="#36981538">next</a><span>|</span><label class="collapse" for="c-36981850">[-]</label><label class="expand" for="c-36981850">[5 more]</label></div><br/><div class="children"><div class="content">Because it makes Friedman heartless. He feels bad but he still promulgated theories which wreaked the badness he felt bad about. So it goes to character.<p>If he _really_ felt bad, he&#x27;d have done what Norbert Weiner did and move out of the field. He stayed an economist. Not so bad feeling, eh?</div><br/><div id="36981951" class="c"><input type="checkbox" id="c-36981951" checked=""/><div class="controls bullet"><span class="by">constantcrying</span><span>|</span><a href="#36981391">root</a><span>|</span><a href="#36981850">parent</a><span>|</span><a href="#36981538">next</a><span>|</span><label class="collapse" for="c-36981951">[-]</label><label class="expand" for="c-36981951">[4 more]</label></div><br/><div class="children"><div class="content">Predicting a negative effect is extremely common in science effecting humans. No reason to abandon a field.<p>&gt;but he still promulgated theories which wreaked the badness he felt bad about<p>No. His prediction was that increasing the minimum wage leads to increased unemployment. He predicted a negative effect and a negative effect happened.<p><i>None</i> of this has to do with article, which makes a very simple point about statistics.</div><br/><div id="36982148" class="c"><input type="checkbox" id="c-36982148" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36981391">root</a><span>|</span><a href="#36981951">parent</a><span>|</span><a href="#36981538">next</a><span>|</span><label class="collapse" for="c-36982148">[-]</label><label class="expand" for="c-36982148">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>None of this has to do with article, which makes a very simple point about statistics.</i><p>I am addressing a &#x27;how is this ad hom&#x27; question not Friedman&#x27;s character or the article. I seek to explain what view of him would be a critique of character not substance.</div><br/><div id="36982684" class="c"><input type="checkbox" id="c-36982684" checked=""/><div class="controls bullet"><span class="by">constantcrying</span><span>|</span><a href="#36981391">root</a><span>|</span><a href="#36982148">parent</a><span>|</span><a href="#36981538">next</a><span>|</span><label class="collapse" for="c-36982684">[-]</label><label class="expand" for="c-36982684">[2 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t an adhom because it is irrelevant.</div><br/><div id="36982976" class="c"><input type="checkbox" id="c-36982976" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36981391">root</a><span>|</span><a href="#36982684">parent</a><span>|</span><a href="#36981538">next</a><span>|</span><label class="collapse" for="c-36982976">[-]</label><label class="expand" for="c-36982976">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t a very good line of reasoning because it&#x27;s a non sequiteur. At this point I think you&#x27;re trolling more than discussing rationally.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36981538" class="c"><input type="checkbox" id="c-36981538" checked=""/><div class="controls bullet"><span class="by">maxminminmax</span><span>|</span><a href="#36981391">prev</a><span>|</span><label class="collapse" for="c-36981538">[-]</label><label class="expand" for="c-36981538">[2 more]</label></div><br/><div class="children"><div class="content">This is one of the most strawman (to put it mildly) things I have ever read.</div><br/><div id="36981814" class="c"><input type="checkbox" id="c-36981814" checked=""/><div class="controls bullet"><span class="by">constantcrying</span><span>|</span><a href="#36981538">parent</a><span>|</span><label class="collapse" for="c-36981814">[-]</label><label class="expand" for="c-36981814">[1 more]</label></div><br/><div class="children"><div class="content">Where is it?</div><br/></div></div></div></div></div></div></div></div></div></body></html>