<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699434060476" as="style"/><link rel="stylesheet" href="styles.css?v=1699434060476"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://chipsandcheese.com/2023/11/07/core-to-core-latency-data-on-large-systems/">Core to core latency data on large systems</a> <span class="domain">(<a href="https://chipsandcheese.com">chipsandcheese.com</a>)</span></div><div class="subtext"><span>nuriaion</span> | <span>16 comments</span></div><br/><div><div id="38183898" class="c"><input type="checkbox" id="c-38183898" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38187448">prev</a><span>|</span><a href="#38186020">next</a><span>|</span><label class="collapse" for="c-38183898">[-]</label><label class="expand" for="c-38183898">[8 more]</label></div><br/><div class="children"><div class="content">The NUMA nature of recent* chips has made me wonder if there’s ever going to be a movement to start using message passing libraries (like MPI) on shared memory machines.<p>* actually, not even that recent, Zen planted this hope in my brain.</div><br/><div id="38183964" class="c"><input type="checkbox" id="c-38183964" checked=""/><div class="controls bullet"><span class="by">nvartolomei</span><span>|</span><a href="#38183898">parent</a><span>|</span><a href="#38184573">next</a><span>|</span><label class="collapse" for="c-38183964">[-]</label><label class="expand" for="c-38183964">[1 more]</label></div><br/><div class="children"><div class="content">Thread-per-core software architectures are doing this <a href="https:&#x2F;&#x2F;penberg.org&#x2F;papers&#x2F;tpc-ancs19.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;penberg.org&#x2F;papers&#x2F;tpc-ancs19.pdf</a><p>Real world examples are scylladb and Redpanda, both built on the seastar framework (C++ <a href="https:&#x2F;&#x2F;seastar.io&#x2F;message-passing&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;seastar.io&#x2F;message-passing&#x2F;</a>).<p>And for rust there is glommio <a href="https:&#x2F;&#x2F;www.datadoghq.com&#x2F;blog&#x2F;engineering&#x2F;introducing-glommio&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.datadoghq.com&#x2F;blog&#x2F;engineering&#x2F;introducing-glomm...</a></div><br/></div></div><div id="38184573" class="c"><input type="checkbox" id="c-38184573" checked=""/><div class="controls bullet"><span class="by">the_svd_doctor</span><span>|</span><a href="#38183898">parent</a><span>|</span><a href="#38183964">prev</a><span>|</span><a href="#38186088">next</a><span>|</span><label class="collapse" for="c-38184573">[-]</label><label class="expand" for="c-38184573">[1 more]</label></div><br/><div class="children"><div class="content">In HPC it&#x27;s common to do a mix of MPI (message-passing &#x2F; distributed memory) and OpenMP (shared memory) parallelism when running on big multicore (and obviously multi-node) machines. It helps with locality, among other things.</div><br/></div></div><div id="38186088" class="c"><input type="checkbox" id="c-38186088" checked=""/><div class="controls bullet"><span class="by">adapteva</span><span>|</span><a href="#38183898">parent</a><span>|</span><a href="#38184573">prev</a><span>|</span><a href="#38184363">next</a><span>|</span><label class="collapse" for="c-38186088">[-]</label><label class="expand" for="c-38186088">[2 more]</label></div><br/><div class="children"><div class="content">Ten years too early....<p><a href="https:&#x2F;&#x2F;parallella.org&#x2F;2015&#x2F;05&#x2F;25&#x2F;how-the-do-i-program-the-parallella&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;parallella.org&#x2F;2015&#x2F;05&#x2F;25&#x2F;how-the-do-i-program-the-p...</a></div><br/><div id="38186563" class="c"><input type="checkbox" id="c-38186563" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#38183898">root</a><span>|</span><a href="#38186088">parent</a><span>|</span><a href="#38184363">next</a><span>|</span><label class="collapse" for="c-38186563">[-]</label><label class="expand" for="c-38186563">[1 more]</label></div><br/><div class="children"><div class="content">Nope, Parallela was the wrong thing at the time and it&#x27;s still wrong. Cache is good.</div><br/></div></div></div></div><div id="38184363" class="c"><input type="checkbox" id="c-38184363" checked=""/><div class="controls bullet"><span class="by">senderista</span><span>|</span><a href="#38183898">parent</a><span>|</span><a href="#38186088">prev</a><span>|</span><a href="#38185912">next</a><span>|</span><label class="collapse" for="c-38184363">[-]</label><label class="expand" for="c-38184363">[1 more]</label></div><br/><div class="children"><div class="content">A good recent paper on implementing message passing over shared memory:<p>&quot;Message Passing or Shared Memory: Evaluating the Delegation Abstraction for Multicores&quot;<p><a href="https:&#x2F;&#x2F;cs.brown.edu&#x2F;~irina&#x2F;papers&#x2F;2013-opodis.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;cs.brown.edu&#x2F;~irina&#x2F;papers&#x2F;2013-opodis.pdf</a></div><br/></div></div><div id="38185912" class="c"><input type="checkbox" id="c-38185912" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38183898">parent</a><span>|</span><a href="#38184363">prev</a><span>|</span><a href="#38185458">next</a><span>|</span><label class="collapse" for="c-38185912">[-]</label><label class="expand" for="c-38185912">[1 more]</label></div><br/><div class="children"><div class="content">I keep thinking that Rust’s borrow semantics would be pretty good for hinting whether code should run on the same core or could be offloaded to another. Two modules that only communicate via small, read only messages could easily be on separate cores.<p>And on architectures where some cores share faster paths than others, gradations could be scheduled that way.</div><br/></div></div><div id="38185458" class="c"><input type="checkbox" id="c-38185458" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#38183898">parent</a><span>|</span><a href="#38185912">prev</a><span>|</span><a href="#38186020">next</a><span>|</span><label class="collapse" for="c-38185458">[-]</label><label class="expand" for="c-38185458">[1 more]</label></div><br/><div class="children"><div class="content">IMO, MPI is the wrong level to do this on. Most apps should either be using some form of mapreduce or not using parallelism beyond the numa node.</div><br/></div></div></div></div><div id="38186020" class="c"><input type="checkbox" id="c-38186020" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38183898">prev</a><span>|</span><a href="#38183800">next</a><span>|</span><label class="collapse" for="c-38186020">[-]</label><label class="expand" for="c-38186020">[1 more]</label></div><br/><div class="children"><div class="content">I was misreading these charts for too long. Maybe I still am.<p>Am I seeing that none of these processors implement a toroidal communication path?  I thought that was considered basic cluster topology these days so I’m surprised that multi core chips don’t implement it.</div><br/></div></div><div id="38183800" class="c"><input type="checkbox" id="c-38183800" checked=""/><div class="controls bullet"><span class="by">formerly_proven</span><span>|</span><a href="#38186020">prev</a><span>|</span><a href="#38183880">next</a><span>|</span><label class="collapse" for="c-38183800">[-]</label><label class="expand" for="c-38183800">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s almost poetic to have those mid-1990s Pentiums there, with about 2-3x the inter-socket latency of the current state-of-the-art, 30 years later.</div><br/></div></div><div id="38183880" class="c"><input type="checkbox" id="c-38183880" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#38183800">prev</a><span>|</span><a href="#38183057">next</a><span>|</span><label class="collapse" for="c-38183880">[-]</label><label class="expand" for="c-38183880">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting. Now do bandwidth next!</div><br/></div></div><div id="38183057" class="c"><input type="checkbox" id="c-38183057" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#38183880">prev</a><span>|</span><label class="collapse" for="c-38183057">[-]</label><label class="expand" for="c-38183057">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;ll be interesting to see how CXL shakes out. It might end up being not much more than cross socket access! 150ns to go between sockets is about what we see here &amp; is in the realm of what CXL had been promising.<p>Having a super short lightweight protocol like CXL.mem to talk over such fast fabric has so much killer potential.<p>These graphs are always such a delight to see. It&#x27;s a network map, of how well connected cores are, and they reveal so many particular advantages and diaadvantages of the greater systems architecture.</div><br/><div id="38185619" class="c"><input type="checkbox" id="c-38185619" checked=""/><div class="controls bullet"><span class="by">rsaxvc</span><span>|</span><a href="#38183057">parent</a><span>|</span><a href="#38186934">next</a><span>|</span><label class="collapse" for="c-38185619">[-]</label><label class="expand" for="c-38185619">[1 more]</label></div><br/><div class="children"><div class="content">Back in the days before Oracle, Sun would sell you a dual socket Opteron desktop and you could add your own FPGA right on the hypertransport in the second socket.<p>Exciting to see that capability becoming more standardized with CXL.<p>Edit: phrasing.</div><br/></div></div><div id="38186934" class="c"><input type="checkbox" id="c-38186934" checked=""/><div class="controls bullet"><span class="by">loxias</span><span>|</span><a href="#38183057">parent</a><span>|</span><a href="#38185619">prev</a><span>|</span><label class="collapse" for="c-38186934">[-]</label><label class="expand" for="c-38186934">[1 more]</label></div><br/><div class="children"><div class="content">I, too, am excited for CXL.   Not enough people got to _feel_ the awesome of pmem.  I think if more people had, pmem would be in all our laptops, desktops, servers.</div><br/></div></div></div></div></div></div></div></div></div></body></html>