<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713862859130" as="style"/><link rel="stylesheet" href="styles.css?v=1713862859130"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2404.14219">Phi-3 Technical Report</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>varunvummadi</span> | <span>83 comments</span></div><br/><div><div id="40128351" class="c"><input type="checkbox" id="c-40128351" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40128046">next</a><span>|</span><label class="collapse" for="c-40128351">[-]</label><label class="expand" for="c-40128351">[10 more]</label></div><br/><div class="children"><div class="content">Everyone needs to take these benchmark numbers with a big grain of salt. According to what I&#x27;ve read, Phi-2 was much worse than its benchmark numbers suggested. This model follows the same training strategy. Nobody should be assuming these numbers will translate directly into a high ranking on the LMSYS leaderboard, or usefulness in everyday tasks. Let&#x27;s not dethrone Llama 3 until some real world testing can be done.<p>That said, I don&#x27;t think it&#x27;s impossible for a small model to be very good. I see their &quot;synthetic data&quot; as essentially a way of distilling GPT-4 into smaller models. It would be exciting if a large fraction of the performance of huge models could be transferred to small ones! If true, then Chinchilla-optimal training could make sense again, as you could optimally train a ginormous model and then distill it afterward for efficient inference.</div><br/><div id="40128483" class="c"><input type="checkbox" id="c-40128483" checked=""/><div class="controls bullet"><span class="by">bt1a</span><span>|</span><a href="#40128351">parent</a><span>|</span><a href="#40128438">next</a><span>|</span><label class="collapse" for="c-40128483">[-]</label><label class="expand" for="c-40128483">[4 more]</label></div><br/><div class="children"><div class="content">This won&#x27;t dethrone Llama 3, but it&#x27;s equally impressive.<p>They mention this model&#x27;s relative weakness in the TruthfulQA eval, since it&#x27;s more lossy trying to pack &#x27;knowledge&#x27; into a small model relative to problem-solving skills (which shine on MMLU)<p>Regardless - still a very useful thing to have offline and on the fly. Those scores are nothing to scoff at.<p>Given that these pipelines are likely harder harder to imitate than new architectures like Transformers, I assume there has been and will be an intense focus on synthetic data generation and cleansing. Llama 3 used 15T of tokens in its training corpus vs 4.8T in the &quot;scaled-up&quot; version of phi-3. If you made it to the end of this disjointed ramble I&#x27;m sorry</div><br/><div id="40128980" class="c"><input type="checkbox" id="c-40128980" checked=""/><div class="controls bullet"><span class="by">IvanAchlaqullah</span><span>|</span><a href="#40128351">root</a><span>|</span><a href="#40128483">parent</a><span>|</span><a href="#40128438">next</a><span>|</span><label class="collapse" for="c-40128980">[-]</label><label class="expand" for="c-40128980">[3 more]</label></div><br/><div class="children"><div class="content">&gt; TruthfulQA<p>Wait, people still use this benchmark? I hear there&#x27;s a huge flaw on it.<p>For examples, fine-tuning the model on 4chan make it scores better on TruthfulQA. It becomes very offensive afterwards though, for obvious reasons. See GPT-4chan [1]<p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=efPrtcLdcdM" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=efPrtcLdcdM</a></div><br/><div id="40129693" class="c"><input type="checkbox" id="c-40129693" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#40128351">root</a><span>|</span><a href="#40128980">parent</a><span>|</span><a href="#40129233">next</a><span>|</span><label class="collapse" for="c-40129693">[-]</label><label class="expand" for="c-40129693">[1 more]</label></div><br/><div class="children"><div class="content">Couldn&#x27;t it be that training it on 4chan makes it more truthful for some reason?</div><br/></div></div><div id="40129233" class="c"><input type="checkbox" id="c-40129233" checked=""/><div class="controls bullet"><span class="by">hoseja</span><span>|</span><a href="#40128351">root</a><span>|</span><a href="#40128980">parent</a><span>|</span><a href="#40129693">prev</a><span>|</span><a href="#40128438">next</a><span>|</span><label class="collapse" for="c-40129233">[-]</label><label class="expand" for="c-40129233">[1 more]</label></div><br/><div class="children"><div class="content">Looks like a good and useful benchmark.</div><br/></div></div></div></div></div></div><div id="40128438" class="c"><input type="checkbox" id="c-40128438" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40128351">parent</a><span>|</span><a href="#40128483">prev</a><span>|</span><a href="#40128046">next</a><span>|</span><label class="collapse" for="c-40128438">[-]</label><label class="expand" for="c-40128438">[5 more]</label></div><br/><div class="children"><div class="content">Phi-2 wasn&#x27;t chat&#x2F;instruct tuned, so it didn&#x27;t act good in chat, it was a base model. But the benchmark #s were real.</div><br/><div id="40128793" class="c"><input type="checkbox" id="c-40128793" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40128351">root</a><span>|</span><a href="#40128438">parent</a><span>|</span><a href="#40128552">next</a><span>|</span><label class="collapse" for="c-40128793">[-]</label><label class="expand" for="c-40128793">[1 more]</label></div><br/><div class="children"><div class="content">I had a lot of issues trying to get Phi-2 to perform as well as the benchmarks indicated on non-chat tasks.<p>It felt a lot like it was overfitted to the exact type of tasks (ie, not a data leak) in the benchmarks but if you were trying something a bit off track if didn&#x27;t know what to do. At the time my hypothesis was that the small model just didn&#x27;t have the capacity to generalise well enough, but since then Gemma 2B has come out and seems to be ok.<p>So now I have no idea why, but yes: the benchmarks for Phi-2 didn&#x27;t represent how it worked for me on real world tasks where you&#x27;d expect it top be ok.</div><br/></div></div><div id="40128552" class="c"><input type="checkbox" id="c-40128552" checked=""/><div class="controls bullet"><span class="by">irjustin</span><span>|</span><a href="#40128351">root</a><span>|</span><a href="#40128438">parent</a><span>|</span><a href="#40128793">prev</a><span>|</span><a href="#40128046">next</a><span>|</span><label class="collapse" for="c-40128552">[-]</label><label class="expand" for="c-40128552">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty naive so please forgive it&#x27;s a stupid question.<p>To me, what the parent comment is saying is that even though the benchmarks are cool, it&#x27;s not super helpful to the every day person. Because if you can&#x27;t chat with it very well (even for a narrow context) what utility does it have with great benchmarks?</div><br/><div id="40128703" class="c"><input type="checkbox" id="c-40128703" checked=""/><div class="controls bullet"><span class="by">svnt</span><span>|</span><a href="#40128351">root</a><span>|</span><a href="#40128552">parent</a><span>|</span><a href="#40128046">next</a><span>|</span><label class="collapse" for="c-40128703">[-]</label><label class="expand" for="c-40128703">[2 more]</label></div><br/><div class="children"><div class="content">Both are saying the same thing: in order for the base model that is phi to perform well as a chat agent, it would need to be tuned for that purpose before its benchmark results could have real-world value.</div><br/><div id="40128906" class="c"><input type="checkbox" id="c-40128906" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40128351">root</a><span>|</span><a href="#40128703">parent</a><span>|</span><a href="#40128046">next</a><span>|</span><label class="collapse" for="c-40128906">[-]</label><label class="expand" for="c-40128906">[1 more]</label></div><br/><div class="children"><div class="content">From this report. Phi-2 was not instruct tuned indeed.<p>&quot;Our models went through post-training with both supervised instruction fine-tuning, and preference tuning with DPO. We have worked on generating and curating various instruction and preference data. This has improved the model chat capabilities, robustness, as well as its safety.&quot;</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40128046" class="c"><input type="checkbox" id="c-40128046" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#40128351">prev</a><span>|</span><a href="#40128521">next</a><span>|</span><label class="collapse" for="c-40128046">[-]</label><label class="expand" for="c-40128046">[31 more]</label></div><br/><div class="children"><div class="content">Incredible, rivals Llama 3 8B with 3.8B parameters after less than a week of release.<p>And on LMSYS English, Llama 3 8B is on par with GPT-4 (not GPT-4-Turbo), as well as Mistral-Large.<p>Source: <a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?leaderboard" rel="nofollow">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?leaderboard</a> (select English in the dropdown)<p>So we now have an open-source LLM approximately equivalent in quality to GPT-4 that can run on phones? Kinda? Wild.<p>(I&#x27;m sure there&#x27;s a lot of nuance to it, for one these benchmarks are not so hard to game, we&#x27;ll see how the dust settles, but still...)<p>Phi-3-mini 3.8b:  71.2<p>Phi-3-small 7b:   74.9<p>Phi-3-medium 14b: 78.2<p>Phi-2 2.7b:       58.8<p>Mistral 7b:       61.0<p>Gemma 7b:         62.0<p>Llama-3-In 8b:    68.0<p>Mixtral 8x7b:     69.9<p>GPT-3.5 1106:     75.3<p>(these are averages across all tasks for each model, but looking at individual scores shows a similar picture)</div><br/><div id="40128229" class="c"><input type="checkbox" id="c-40128229" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#40128046">parent</a><span>|</span><a href="#40128105">next</a><span>|</span><label class="collapse" for="c-40128229">[-]</label><label class="expand" for="c-40128229">[6 more]</label></div><br/><div class="children"><div class="content">This inductive logic is way overblown.<p>&gt; Incredible, beat Llama 3 8B with 3.8B parameters after less than a week of release.<p>Judging by a single benchmark? Without even trying it out with real world usage?<p>&gt; And on LMSYS English, Llama 3 8B is on par with GPT-4 (not GPT-4-Turbo), as well as Mistral-Large.<p>Any potential caveat in such a leaderboard not withstanding, on that leaderboard alone, there is a huge gap between llama 3 8B and Mistral-Large, let alone any of the GPT-4.<p>By the way, for beating benchmark, &quot;Pretraining on the Test Set Is All You Need&quot;</div><br/><div id="40128277" class="c"><input type="checkbox" id="c-40128277" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128229">parent</a><span>|</span><a href="#40128105">next</a><span>|</span><label class="collapse" for="c-40128277">[-]</label><label class="expand" for="c-40128277">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easy to miss: select English in the dropdown.
The scores are quite different in Overall and in English for LMSYS.<p>As I&#x27;ve stated in other comments, yeah... Agreed, I&#x27;m stretching it a bit. It&#x27;s just that any indication of a 3.8B model being in the vicinity of GPT-4 is huge.<p>I&#x27;m sure that when things are properly measured by third-parties it will show a more sober picture. But still, with good fine-tunes, we&#x27;ll probably get close.<p>It&#x27;s a very significant demonstration of what could be possible soon.</div><br/><div id="40128474" class="c"><input type="checkbox" id="c-40128474" checked=""/><div class="controls bullet"><span class="by">saretup</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128277">parent</a><span>|</span><a href="#40128105">next</a><span>|</span><label class="collapse" for="c-40128474">[-]</label><label class="expand" for="c-40128474">[4 more]</label></div><br/><div class="children"><div class="content">Firstly, English is a highly subjective category.<p>Secondly, Llama 3 usually adds first sentences like âWhat a unique question!â or âWhat an insightful thoughtâ, which might make people like it more than the competition because of the pandering.<p>While Llama 3 is singular in terms of size to quality ratio, calling the 8B model close to GPT4 would be an overstretch.</div><br/><div id="40129033" class="c"><input type="checkbox" id="c-40129033" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128474">parent</a><span>|</span><a href="#40128105">next</a><span>|</span><label class="collapse" for="c-40129033">[-]</label><label class="expand" for="c-40129033">[3 more]</label></div><br/><div class="children"><div class="content">Yes, I don&#x27;t know how people don&#x27;t realize how much cheap tricks works in Chatbot Arena. A single base model produces 100s of ELO difference depending on the way it is tuned. And on most cases, instruction tuning heavily slightly even decreases reasoning ability on standard benchmark. You can see base model scores better in MMLU&#x2F;ARC most of the times in huggingface leaderboard.<p>Even GPT-4-1106 seems to only sounds better than GPT-4-0613 and works for wider range of prompt. But in a well defined prompt and follow up questions I don&#x27;t think there is an improvement in reasoning.</div><br/><div id="40129131" class="c"><input type="checkbox" id="c-40129131" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40129033">parent</a><span>|</span><a href="#40128105">next</a><span>|</span><label class="collapse" for="c-40129131">[-]</label><label class="expand" for="c-40129131">[2 more]</label></div><br/><div class="children"><div class="content">When I tried Phi2 it was just bad. I don&#x27;t know where you got this fantasy from that people accept obviously wrong answers, because of &quot;pandering&quot;.</div><br/><div id="40129583" class="c"><input type="checkbox" id="c-40129583" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40129131">parent</a><span>|</span><a href="#40128105">next</a><span>|</span><label class="collapse" for="c-40129583">[-]</label><label class="expand" for="c-40129583">[1 more]</label></div><br/><div class="children"><div class="content">Obviously correct answer matters more but ~100-200 elo points could be gained just for better writing. Answer would be range of 500 elo in comparison.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40128105" class="c"><input type="checkbox" id="c-40128105" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#40128046">parent</a><span>|</span><a href="#40128229">prev</a><span>|</span><a href="#40129792">next</a><span>|</span><label class="collapse" for="c-40128105">[-]</label><label class="expand" for="c-40128105">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Phi-3-mini 3.8b: 71.2</i><p>Per the paper, phi3-mini (which is english-only) quantised to 4bit uses 1.8gb RAM and outputs 1212 tokens&#x2F;sec (correction: 12 tokens&#x2F;sec) on iOS.<p>A model on par with GPT-3.5 running on phones!<p>(weights haven&#x27;t been released, though)</div><br/><div id="40128205" class="c"><input type="checkbox" id="c-40128205" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128105">parent</a><span>|</span><a href="#40129792">next</a><span>|</span><label class="collapse" for="c-40128205">[-]</label><label class="expand" for="c-40128205">[4 more]</label></div><br/><div class="children"><div class="content">&gt; (weights haven&#x27;t been released, though)<p>Phi-1, Phi-1.5, and Phi-2 have all had their weights released, and those weights are available under the MIT License.<p>Hopefully Microsoft will continue that trend with Phi-3.<p>&gt; outputs 1212 tokens&#x2F;sec on iOS<p>I think you meant &quot;12 tokens&#x2F;sec&quot;, which is still nice, just a little less exciting than a kilotoken&#x2F;sec.</div><br/><div id="40128579" class="c"><input type="checkbox" id="c-40128579" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128205">parent</a><span>|</span><a href="#40128648">next</a><span>|</span><label class="collapse" for="c-40128579">[-]</label><label class="expand" for="c-40128579">[1 more]</label></div><br/><div class="children"><div class="content">Weights will be realised tomorrow, according to one of the tech report authors on Twitter.</div><br/></div></div><div id="40128648" class="c"><input type="checkbox" id="c-40128648" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128205">parent</a><span>|</span><a href="#40128579">prev</a><span>|</span><a href="#40129089">next</a><span>|</span><label class="collapse" for="c-40128648">[-]</label><label class="expand" for="c-40128648">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>you meant 12 tokens&#x2F;sec</i><p>Thanks! The HTML version on archive.is has messed up markup and shows <i>1212</i> instead: <a href="https:&#x2F;&#x2F;archive.is&#x2F;Ndox6" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;Ndox6</a></div><br/></div></div><div id="40129089" class="c"><input type="checkbox" id="c-40129089" checked=""/><div class="controls bullet"><span class="by">intellectronica</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128205">parent</a><span>|</span><a href="#40128648">prev</a><span>|</span><a href="#40129792">next</a><span>|</span><label class="collapse" for="c-40129089">[-]</label><label class="expand" for="c-40129089">[1 more]</label></div><br/><div class="children"><div class="content">Weights are coming tomorrow.</div><br/></div></div></div></div></div></div><div id="40129792" class="c"><input type="checkbox" id="c-40129792" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#40128046">parent</a><span>|</span><a href="#40128105">prev</a><span>|</span><a href="#40129742">next</a><span>|</span><label class="collapse" for="c-40129792">[-]</label><label class="expand" for="c-40129792">[1 more]</label></div><br/><div class="children"><div class="content">Where did you get this from?<p>&gt; So we now have an open-source LLM approximately equivalent in quality to GPT-4 that can run on phones<p>No, not even close ... Even Gemini has huge UX gap comparing to GPT4&#x2F;Opus, 8B I won&#x27;t even attempt this argument.</div><br/></div></div><div id="40129742" class="c"><input type="checkbox" id="c-40129742" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#40128046">parent</a><span>|</span><a href="#40129792">prev</a><span>|</span><a href="#40129308">next</a><span>|</span><label class="collapse" for="c-40129742">[-]</label><label class="expand" for="c-40129742">[1 more]</label></div><br/><div class="children"><div class="content">At a glance, it looks like Phi-3 was trained on an English only, STEM-strong dataset. See how they are not as strong in HumanEval, Trivia, etc. But of course it&#x27;s very good.</div><br/></div></div><div id="40129308" class="c"><input type="checkbox" id="c-40129308" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#40128046">parent</a><span>|</span><a href="#40129742">prev</a><span>|</span><a href="#40128060">next</a><span>|</span><label class="collapse" for="c-40129308">[-]</label><label class="expand" for="c-40129308">[1 more]</label></div><br/><div class="children"><div class="content">On par in some categories. Phi was intended for reasoning, not storing data, due to small size. I mean, it&#x27;s still great, but the smaller it gets, the more facts from outside of the prompts context will not be known at all.</div><br/></div></div><div id="40128060" class="c"><input type="checkbox" id="c-40128060" checked=""/><div class="controls bullet"><span class="by">crakenzak</span><span>|</span><a href="#40128046">parent</a><span>|</span><a href="#40129308">prev</a><span>|</span><a href="#40128075">next</a><span>|</span><label class="collapse" for="c-40128060">[-]</label><label class="expand" for="c-40128060">[7 more]</label></div><br/><div class="children"><div class="content">Canât wait to see some Phi-3 fine tunes! Will be testing this out locally, such a small model that I can run it without quantization.<p>Feels incredible to be living in a time with such neck breaking innovations. What are chances weâll have a &lt;100B parameter GPT4&#x2F;Claude Opus model in the next 5 years?</div><br/><div id="40128854" class="c"><input type="checkbox" id="c-40128854" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128060">parent</a><span>|</span><a href="#40128273">next</a><span>|</span><label class="collapse" for="c-40128854">[-]</label><label class="expand" for="c-40128854">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What are chances weâll have a &lt;100B parameter GPT4&#x2F;Claude Opus model in the next 5 years?<p>In 5 years time we&#x27;ll have adaptive compute and the idea of talking about the parameter count of a model will seem as quaint as talking about the cylinder capacity of a jet engine.</div><br/></div></div><div id="40128273" class="c"><input type="checkbox" id="c-40128273" checked=""/><div class="controls bullet"><span class="by">Deverauxi</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128060">parent</a><span>|</span><a href="#40128854">prev</a><span>|</span><a href="#40128209">next</a><span>|</span><label class="collapse" for="c-40128273">[-]</label><label class="expand" for="c-40128273">[2 more]</label></div><br/><div class="children"><div class="content">5 years? 5 years is a millennia these days.<p>Weâll have small local models beating gpt-4&#x2F;Claude opus in 2024. We already have sub 100b models trading blows with former gpt-4 models, and the future is racing toward us. All these little breakthroughs are piling up.</div><br/><div id="40128446" class="c"><input type="checkbox" id="c-40128446" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128273">parent</a><span>|</span><a href="#40128209">next</a><span>|</span><label class="collapse" for="c-40128446">[-]</label><label class="expand" for="c-40128446">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely not on the first one. Not even close.</div><br/></div></div></div></div><div id="40128209" class="c"><input type="checkbox" id="c-40128209" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128060">parent</a><span>|</span><a href="#40128273">prev</a><span>|</span><a href="#40128595">next</a><span>|</span><label class="collapse" for="c-40128209">[-]</label><label class="expand" for="c-40128209">[1 more]</label></div><br/><div class="children"><div class="content">Is it released?</div><br/></div></div><div id="40128595" class="c"><input type="checkbox" id="c-40128595" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128060">parent</a><span>|</span><a href="#40128209">prev</a><span>|</span><a href="#40128075">next</a><span>|</span><label class="collapse" for="c-40128595">[-]</label><label class="expand" for="c-40128595">[2 more]</label></div><br/><div class="children"><div class="content">We already do. Itâs called LLama 3 70B Instruct.</div><br/><div id="40129462" class="c"><input type="checkbox" id="c-40129462" checked=""/><div class="controls bullet"><span class="by">vitorgrs</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128595">parent</a><span>|</span><a href="#40128075">next</a><span>|</span><label class="collapse" for="c-40129462">[-]</label><label class="expand" for="c-40129462">[1 more]</label></div><br/><div class="children"><div class="content">Llama 3 is awful in non-English. 95% of their training data is in English....<p>GPT is still the king when talking about multiple languages&#x2F;knowledge.</div><br/></div></div></div></div></div></div><div id="40128075" class="c"><input type="checkbox" id="c-40128075" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#40128046">parent</a><span>|</span><a href="#40128060">prev</a><span>|</span><a href="#40128290">next</a><span>|</span><label class="collapse" for="c-40128075">[-]</label><label class="expand" for="c-40128075">[6 more]</label></div><br/><div class="children"><div class="content">&gt;And on LMSYS English, Llama 3 8B is well above GPT-4<p>Source?</div><br/><div id="40128086" class="c"><input type="checkbox" id="c-40128086" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128075">parent</a><span>|</span><a href="#40128290">next</a><span>|</span><label class="collapse" for="c-40128086">[-]</label><label class="expand" for="c-40128086">[5 more]</label></div><br/><div class="children"><div class="content">Right thanks for the reminder, I added it</div><br/><div id="40128110" class="c"><input type="checkbox" id="c-40128110" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128086">parent</a><span>|</span><a href="#40128290">next</a><span>|</span><label class="collapse" for="c-40128110">[-]</label><label class="expand" for="c-40128110">[4 more]</label></div><br/><div class="children"><div class="content">Thanks, I don&#x27;t see them being &quot;well above GPT-4&quot;, merely 1 point? Also, no idea why one would want to exclude GPT-4-Turbo, the flagship &quot;GPT-4&quot; model, but w&#x2F;e.<p>I also don&#x27;t think they &quot;beat Llama 3 8B&quot;; their own abstract says &quot;rivals that of models such as Mixtral 8x7B and GPT-3.5&quot;, &quot;rivals&quot; not even &quot;beats&quot;.<p>Great model, but let&#x27;s not overplay it.</div><br/><div id="40128170" class="c"><input type="checkbox" id="c-40128170" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128110">parent</a><span>|</span><a href="#40128290">next</a><span>|</span><label class="collapse" for="c-40128170">[-]</label><label class="expand" for="c-40128170">[3 more]</label></div><br/><div class="children"><div class="content">In the English category: GPT-4-0314 (ELO 1166), Llama 3 8B Instruct (ELO 1161), Mistral-Large-2402 (ELO 1151), GPT-4-0613 (ELO 1148).<p>You are right, I toned down the language, I got a bit overexcited, and I missed the difference in the versions of GPT-4. And LMSYS is a subjective benchmark for what users prefer, which I&#x27;m sure has weird inherent biases.<p>It&#x27;s just that any signal of an 3.8B model being anywhere in the vicinity of GPT-4 is huge.</div><br/><div id="40128210" class="c"><input type="checkbox" id="c-40128210" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128170">parent</a><span>|</span><a href="#40128290">next</a><span>|</span><label class="collapse" for="c-40128210">[-]</label><label class="expand" for="c-40128210">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, GPT3.5, in a phone, at ~1,000 tokens&#x2F;sec ... nice!</div><br/><div id="40128376" class="c"><input type="checkbox" id="c-40128376" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128210">parent</a><span>|</span><a href="#40128290">next</a><span>|</span><label class="collapse" for="c-40128376">[-]</label><label class="expand" for="c-40128376">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  at ~1,000 tokens&#x2F;sec<p>12 tokens per second.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40128290" class="c"><input type="checkbox" id="c-40128290" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#40128046">parent</a><span>|</span><a href="#40128075">prev</a><span>|</span><a href="#40128521">next</a><span>|</span><label class="collapse" for="c-40128290">[-]</label><label class="expand" for="c-40128290">[3 more]</label></div><br/><div class="children"><div class="content">&gt; So we now have an open-source LLM approximately equivalent in quality to GPT-4 that can run on phones?<p>No, we don&#x27;t. LMsys is just one, very flawed benchmark.</div><br/><div id="40128494" class="c"><input type="checkbox" id="c-40128494" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128290">parent</a><span>|</span><a href="#40128302">next</a><span>|</span><label class="collapse" for="c-40128494">[-]</label><label class="expand" for="c-40128494">[1 more]</label></div><br/><div class="children"><div class="content">Why is LMsys flawed?<p>Many people treat LMsys as gospel because it&#x27;s the only large-scale, up-to-date qualitative benchmark. All the numeric benchmarks seem to miss real-world applicability.</div><br/></div></div><div id="40128302" class="c"><input type="checkbox" id="c-40128302" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#40128046">root</a><span>|</span><a href="#40128290">parent</a><span>|</span><a href="#40128494">prev</a><span>|</span><a href="#40128521">next</a><span>|</span><label class="collapse" for="c-40128302">[-]</label><label class="expand" for="c-40128302">[1 more]</label></div><br/><div class="children"><div class="content">Agreed, but it&#x27;s wild that even one benchmark shows this. Based on what we knew just a few months ago, these models should be so far from each other in every benchmark.</div><br/></div></div></div></div></div></div><div id="40128521" class="c"><input type="checkbox" id="c-40128521" checked=""/><div class="controls bullet"><span class="by">mythz</span><span>|</span><a href="#40128046">prev</a><span>|</span><a href="#40128359">next</a><span>|</span><label class="collapse" for="c-40128521">[-]</label><label class="expand" for="c-40128521">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll believe it till I try it for myself, Phi-2 was the clear worst of the 20 LLMs we evaluated (was also smallest so was expected).<p>But it was slow for its size, generated the longest responses with the most hallucinations, as well as generating the most empty responses. It was also the model ranked with the lowest quality answers.</div><br/></div></div><div id="40128359" class="c"><input type="checkbox" id="c-40128359" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40128521">prev</a><span>|</span><a href="#40128111">next</a><span>|</span><label class="collapse" for="c-40128359">[-]</label><label class="expand" for="c-40128359">[1 more]</label></div><br/><div class="children"><div class="content">This shows the power of synthetic content - 3.3 trillion tokens! This approach can make a model even smaller and more efficient than organic text training, and it will not be able to regurgitate NYT articles because it hasn&#x27;t seen any of them. This is how copyright infringement claims can be placated.</div><br/></div></div><div id="40128111" class="c"><input type="checkbox" id="c-40128111" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#40128359">prev</a><span>|</span><a href="#40129248">next</a><span>|</span><label class="collapse" for="c-40128111">[-]</label><label class="expand" for="c-40128111">[20 more]</label></div><br/><div class="children"><div class="content">If I was Apple I&#x27;d be quaking in my boots. They are getting too far behind to ever catch up. Nokia in 2010 vibes.</div><br/><div id="40128253" class="c"><input type="checkbox" id="c-40128253" checked=""/><div class="controls bullet"><span class="by">PedroBatista</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128256">next</a><span>|</span><label class="collapse" for="c-40128253">[-]</label><label class="expand" for="c-40128253">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;ll just do what they have been doing for ~20 years, they wait, pick the &quot;winner&quot;, polish the &quot;user experience&quot;, call it Apple magic and incorporate that into their product cycles.<p>Some day will be the day their joke book becomes so mediocre it will not stick anymore, but I think they are safe on this one, for now..</div><br/><div id="40129924" class="c"><input type="checkbox" id="c-40129924" checked=""/><div class="controls bullet"><span class="by">fauigerzigerk</span><span>|</span><a href="#40128111">root</a><span>|</span><a href="#40128253">parent</a><span>|</span><a href="#40128256">next</a><span>|</span><label class="collapse" for="c-40129924">[-]</label><label class="expand" for="c-40129924">[1 more]</label></div><br/><div class="children"><div class="content">True for hardware, but their record on software is far less convincing.</div><br/></div></div></div></div><div id="40128256" class="c"><input type="checkbox" id="c-40128256" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128253">prev</a><span>|</span><a href="#40128358">next</a><span>|</span><label class="collapse" for="c-40128256">[-]</label><label class="expand" for="c-40128256">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think MS has a special sauce here, just a willingness to publish. To the extent MS has disclosed the bulk of what they are doing with Phi, it&#x27;s a combination of really nice initial idea &quot;Use written texts + GPT-4 to generate high quality prompts where we know the answer is great because it&#x27;s written down&quot; and engineering.<p>To me this is advancing the state of the art as to the impact of <i>data quality</i>, but it doesn&#x27;t look to me like the phi series have some magical special sauce otherwise. Data of quality and synthetic data creation are not magical moats that Apple can&#x27;t cross.<p>I&#x27;ll say too that I&#x27;m psyched to try Phi-3; the sweet spot for me is a model that can be a local coding assistant and still answer random q&amp;a questions with <i>some</i> sophistication. I&#x27;m skeptical that 3-8b parameter models will bring the high-level of sophistication sometimes needed in this cycle; there&#x27;s still a very large gap with the larger models in daily use, despite some often close benchmark scores.<p>Anyway, Apple-Phi-3 is in no way an impossibility.</div><br/></div></div><div id="40128358" class="c"><input type="checkbox" id="c-40128358" checked=""/><div class="controls bullet"><span class="by">bt1a</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128256">prev</a><span>|</span><a href="#40128131">next</a><span>|</span><label class="collapse" for="c-40128358">[-]</label><label class="expand" for="c-40128358">[1 more]</label></div><br/><div class="children"><div class="content">I tore my hair out developing a SwiftUI app that could run llama.cpp and whisper.cpp simultaneously. Was able to run a Q3_K Mistral 7B along with a smaller whisper model eventually, but grinding through XCode is a nightmare.<p>They&#x27;re working on MLX but it only recently got swift bindings. They just don&#x27;t have the DEVELOPERS DEVELOPERS DEVELOPERS coked out attitude i guess</div><br/></div></div><div id="40128131" class="c"><input type="checkbox" id="c-40128131" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128358">prev</a><span>|</span><a href="#40128276">next</a><span>|</span><label class="collapse" for="c-40128131">[-]</label><label class="expand" for="c-40128131">[1 more]</label></div><br/><div class="children"><div class="content">Did they ever claim to be a powerhouse in foundation models? Did your MacBook or iPhone become obsolete or stop working? They use the models, they don&#x27;t release them because they don&#x27;t hoard data.</div><br/></div></div><div id="40128276" class="c"><input type="checkbox" id="c-40128276" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128131">prev</a><span>|</span><a href="#40128197">next</a><span>|</span><label class="collapse" for="c-40128276">[-]</label><label class="expand" for="c-40128276">[1 more]</label></div><br/><div class="children"><div class="content">The opposite is the case, with all the advancements, even by doing nothing, Apple (like everyone, including hobbyists) is moving closer to the frontier. Hopefully this trend stays alive!</div><br/></div></div><div id="40128197" class="c"><input type="checkbox" id="c-40128197" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128276">prev</a><span>|</span><a href="#40128263">next</a><span>|</span><label class="collapse" for="c-40128197">[-]</label><label class="expand" for="c-40128197">[1 more]</label></div><br/><div class="children"><div class="content">If anything this is good for them. Apple&#x27;s play here has always been getting their devices ready for running LLMs locally. This makes it way easier.</div><br/></div></div><div id="40128263" class="c"><input type="checkbox" id="c-40128263" checked=""/><div class="controls bullet"><span class="by">Deverauxi</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128197">prev</a><span>|</span><a href="#40128228">next</a><span>|</span><label class="collapse" for="c-40128263">[-]</label><label class="expand" for="c-40128263">[1 more]</label></div><br/><div class="children"><div class="content">They have something like 140 billion dollars in cash.<p>Theyâll be fine.</div><br/></div></div><div id="40128228" class="c"><input type="checkbox" id="c-40128228" checked=""/><div class="controls bullet"><span class="by">IncreasePosts</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128263">prev</a><span>|</span><a href="#40128953">next</a><span>|</span><label class="collapse" for="c-40128228">[-]</label><label class="expand" for="c-40128228">[1 more]</label></div><br/><div class="children"><div class="content">How exactly does publicized research lead to them not being able to catch up? I don&#x27;t think anything in this paper is patentable.</div><br/></div></div><div id="40128953" class="c"><input type="checkbox" id="c-40128953" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128228">prev</a><span>|</span><a href="#40128637">next</a><span>|</span><label class="collapse" for="c-40128953">[-]</label><label class="expand" for="c-40128953">[2 more]</label></div><br/><div class="children"><div class="content">Apple&#x27;s advantage is that their devices are safeguarding people from the dangers of AI</div><br/><div id="40129932" class="c"><input type="checkbox" id="c-40129932" checked=""/><div class="controls bullet"><span class="by">fauigerzigerk</span><span>|</span><a href="#40128111">root</a><span>|</span><a href="#40128953">parent</a><span>|</span><a href="#40128637">next</a><span>|</span><label class="collapse" for="c-40129932">[-]</label><label class="expand" for="c-40129932">[1 more]</label></div><br/><div class="children"><div class="content">How so? And what dangers?</div><br/></div></div></div></div><div id="40128637" class="c"><input type="checkbox" id="c-40128637" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128953">prev</a><span>|</span><a href="#40128177">next</a><span>|</span><label class="collapse" for="c-40128637">[-]</label><label class="expand" for="c-40128637">[1 more]</label></div><br/><div class="children"><div class="content">I think that when people release new interesting software products it&#x27;s good for hardware companies.</div><br/></div></div><div id="40128177" class="c"><input type="checkbox" id="c-40128177" checked=""/><div class="controls bullet"><span class="by">thoughtegting</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128637">prev</a><span>|</span><a href="#40128206">next</a><span>|</span><label class="collapse" for="c-40128177">[-]</label><label class="expand" for="c-40128177">[5 more]</label></div><br/><div class="children"><div class="content">If I were apple, I would be developing something in total secrecy and then release something ahead of the rest of competition when people least expect it. very big ifs but siri can be updated everywhere overnight and I dont see them rushing into anything like this</div><br/><div id="40128201" class="c"><input type="checkbox" id="c-40128201" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#40128111">root</a><span>|</span><a href="#40128177">parent</a><span>|</span><a href="#40128206">next</a><span>|</span><label class="collapse" for="c-40128201">[-]</label><label class="expand" for="c-40128201">[4 more]</label></div><br/><div class="children"><div class="content">If I were apple, I would just buy one of the major LLM companies. They have the cash.</div><br/><div id="40128598" class="c"><input type="checkbox" id="c-40128598" checked=""/><div class="controls bullet"><span class="by">bingbingbing777</span><span>|</span><a href="#40128111">root</a><span>|</span><a href="#40128201">parent</a><span>|</span><a href="#40128206">next</a><span>|</span><label class="collapse" for="c-40128598">[-]</label><label class="expand" for="c-40128598">[3 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve been buying AI companies and have nothing to show for it.</div><br/><div id="40128722" class="c"><input type="checkbox" id="c-40128722" checked=""/><div class="controls bullet"><span class="by">elbear</span><span>|</span><a href="#40128111">root</a><span>|</span><a href="#40128598">parent</a><span>|</span><a href="#40129208">next</a><span>|</span><label class="collapse" for="c-40128722">[-]</label><label class="expand" for="c-40128722">[1 more]</label></div><br/><div class="children"><div class="content">Why do you think that is? Do you think their culture is an obstacle or is it something else?</div><br/></div></div><div id="40129208" class="c"><input type="checkbox" id="c-40129208" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#40128111">root</a><span>|</span><a href="#40128598">parent</a><span>|</span><a href="#40128722">prev</a><span>|</span><a href="#40128206">next</a><span>|</span><label class="collapse" for="c-40129208">[-]</label><label class="expand" for="c-40129208">[1 more]</label></div><br/><div class="children"><div class="content">Showing off work in progress is not really their thing.</div><br/></div></div></div></div></div></div></div></div><div id="40128206" class="c"><input type="checkbox" id="c-40128206" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128177">prev</a><span>|</span><a href="#40128156">next</a><span>|</span><label class="collapse" for="c-40128206">[-]</label><label class="expand" for="c-40128206">[1 more]</label></div><br/><div class="children"><div class="content">Eh, I think itâs showing that this class of model is becoming commoditized given there is a new one launching every week.</div><br/></div></div><div id="40128156" class="c"><input type="checkbox" id="c-40128156" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#40128111">parent</a><span>|</span><a href="#40128206">prev</a><span>|</span><a href="#40129248">next</a><span>|</span><label class="collapse" for="c-40128156">[-]</label><label class="expand" for="c-40128156">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t recall Nokia being a 3 <i>trillion</i> dollar company. Your vibes may vary, though.</div><br/></div></div></div></div><div id="40129248" class="c"><input type="checkbox" id="c-40129248" checked=""/><div class="controls bullet"><span class="by">smartmic</span><span>|</span><a href="#40128111">prev</a><span>|</span><a href="#40129900">next</a><span>|</span><label class="collapse" for="c-40129248">[-]</label><label class="expand" for="c-40129248">[4 more]</label></div><br/><div class="children"><div class="content">Hm, roundabout 84 authors of one &quot;scientific&quot; paper. I wonder if this says something about (a) the quality of its content, (b) the path were academic (?) paper publishing goes to, (c) nothing at all, or (d), something entirely else.</div><br/><div id="40129890" class="c"><input type="checkbox" id="c-40129890" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#40129248">parent</a><span>|</span><a href="#40129740">next</a><span>|</span><label class="collapse" for="c-40129890">[-]</label><label class="expand" for="c-40129890">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a tech report. Fair enough to include the whole lab.</div><br/></div></div><div id="40129740" class="c"><input type="checkbox" id="c-40129740" checked=""/><div class="controls bullet"><span class="by">lysecret</span><span>|</span><a href="#40129248">parent</a><span>|</span><a href="#40129890">prev</a><span>|</span><a href="#40129298">next</a><span>|</span><label class="collapse" for="c-40129740">[-]</label><label class="expand" for="c-40129740">[1 more]</label></div><br/><div class="children"><div class="content">Just means you need a big machine and a lot of capital to make advancement. Take a look at any paper coming out of cern.</div><br/></div></div><div id="40129298" class="c"><input type="checkbox" id="c-40129298" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#40129248">parent</a><span>|</span><a href="#40129740">prev</a><span>|</span><a href="#40129900">next</a><span>|</span><label class="collapse" for="c-40129298">[-]</label><label class="expand" for="c-40129298">[1 more]</label></div><br/><div class="children"><div class="content">I have been on far larger author lists :) There&#x27;s probably a whole team for the training data generation and assessment, a whole team for the safety assessment  (section 4), that stuff adds up.</div><br/></div></div></div></div><div id="40129900" class="c"><input type="checkbox" id="c-40129900" checked=""/><div class="controls bullet"><span class="by">maximsicora</span><span>|</span><a href="#40129248">prev</a><span>|</span><a href="#40128639">next</a><span>|</span><label class="collapse" for="c-40129900">[-]</label><label class="expand" for="c-40129900">[1 more]</label></div><br/><div class="children"><div class="content">insane</div><br/></div></div><div id="40128639" class="c"><input type="checkbox" id="c-40128639" checked=""/><div class="controls bullet"><span class="by">abidlabs</span><span>|</span><a href="#40129900">prev</a><span>|</span><a href="#40128057">next</a><span>|</span><label class="collapse" for="c-40128639">[-]</label><label class="expand" for="c-40128639">[1 more]</label></div><br/><div class="children"><div class="content">Hugging Face Paper Page and Discussion: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2404.14219" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;papers&#x2F;2404.14219</a></div><br/></div></div><div id="40128057" class="c"><input type="checkbox" id="c-40128057" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40128639">prev</a><span>|</span><a href="#40128158">next</a><span>|</span><label class="collapse" for="c-40128057">[-]</label><label class="expand" for="c-40128057">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m getting a bit skeptical of MMLU at this point. As far as I can tell it&#x27;s a set of multiple choice questions that hasn&#x27;t been updated since 2020. We have to trust the model providers not to deliberately or accidentally train on it for those scores to be useful.</div><br/><div id="40128120" class="c"><input type="checkbox" id="c-40128120" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#40128057">parent</a><span>|</span><a href="#40128158">next</a><span>|</span><label class="collapse" for="c-40128120">[-]</label><label class="expand" for="c-40128120">[1 more]</label></div><br/><div class="children"><div class="content">At the least, there&#x27;s multiple benchmarks noted in the paper (21!) and the results are consistent across all of them.<p>I&#x27;d trust Microsoft to do decontamination testing, although the paper doesn&#x27;t explicitly mention it other than &quot;The prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for the phi-3 models.&quot;</div><br/></div></div></div></div><div id="40128158" class="c"><input type="checkbox" id="c-40128158" checked=""/><div class="controls bullet"><span class="by">blackoil</span><span>|</span><a href="#40128057">prev</a><span>|</span><a href="#40128480">next</a><span>|</span><label class="collapse" for="c-40128158">[-]</label><label class="expand" for="c-40128158">[1 more]</label></div><br/><div class="children"><div class="content">Has anyone used these&#x2F;similar with fine tune and RAG? How is the performance over a narrow domain for simple queries? Is it good enough for say an informational chat bot?</div><br/></div></div><div id="40128480" class="c"><input type="checkbox" id="c-40128480" checked=""/><div class="controls bullet"><span class="by">ur-whale</span><span>|</span><a href="#40128158">prev</a><span>|</span><a href="#40128116">next</a><span>|</span><label class="collapse" for="c-40128480">[-]</label><label class="expand" for="c-40128480">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a whole lot of Zhangs!</div><br/></div></div><div id="40128116" class="c"><input type="checkbox" id="c-40128116" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40128480">prev</a><span>|</span><label class="collapse" for="c-40128116">[-]</label><label class="expand" for="c-40128116">[9 more]</label></div><br/><div class="children"><div class="content">Less tokens than Llama 3 (3.3T vs 15T) yet better outcome. No doubt more information dense training data. The interesting thing is the use of synthetic data which they don&#x27;t talk about.</div><br/><div id="40128264" class="c"><input type="checkbox" id="c-40128264" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#40128116">parent</a><span>|</span><a href="#40128140">next</a><span>|</span><label class="collapse" for="c-40128264">[-]</label><label class="expand" for="c-40128264">[6 more]</label></div><br/><div class="children"><div class="content">Actually the original Phi papers did talk about their synthetic data strategy, and it&#x27;s very cool -- essentially invert high quality textbook text using GPT-4 to create prompts, where the textbooks supply the answers. There may be more undisclosed, but it remains in my mind as one of the best ideas of the last twelve months -- so smart, and interesting, and apparently, it works well.</div><br/><div id="40128652" class="c"><input type="checkbox" id="c-40128652" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#40128116">root</a><span>|</span><a href="#40128264">parent</a><span>|</span><a href="#40128775">next</a><span>|</span><label class="collapse" for="c-40128652">[-]</label><label class="expand" for="c-40128652">[1 more]</label></div><br/><div class="children"><div class="content">I feel like literal dictionaries would make good training data; wonder if any of them have done that. LLMs are good at faking so it&#x27;s hard to tell by asking them.</div><br/></div></div><div id="40128775" class="c"><input type="checkbox" id="c-40128775" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#40128116">root</a><span>|</span><a href="#40128264">parent</a><span>|</span><a href="#40128652">prev</a><span>|</span><a href="#40128369">next</a><span>|</span><label class="collapse" for="c-40128775">[-]</label><label class="expand" for="c-40128775">[2 more]</label></div><br/><div class="children"><div class="content">Except everything that comes out of an LLM (like GPT4) is highly suspect (at least in my experience).</div><br/><div id="40129922" class="c"><input type="checkbox" id="c-40129922" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#40128116">root</a><span>|</span><a href="#40128775">parent</a><span>|</span><a href="#40128369">next</a><span>|</span><label class="collapse" for="c-40129922">[-]</label><label class="expand" for="c-40129922">[1 more]</label></div><br/><div class="children"><div class="content">1. They need it for style and language, not necessarily for the facts<p>2. Since GPT-4 is seen as the very best general-purpose LLM in existence, it makes sense to emulate its performance with less resources.<p>3. Phi models are also trained with other high-quality data</div><br/></div></div></div></div><div id="40128369" class="c"><input type="checkbox" id="c-40128369" checked=""/><div class="controls bullet"><span class="by">xarope</span><span>|</span><a href="#40128116">root</a><span>|</span><a href="#40128264">parent</a><span>|</span><a href="#40128775">prev</a><span>|</span><a href="#40128894">next</a><span>|</span><label class="collapse" for="c-40128369">[-]</label><label class="expand" for="c-40128369">[1 more]</label></div><br/><div class="children"><div class="content">perhaps that&#x27;s the best path forward? Text and reference books (hopefully unbiased) for answers, and web scraped data for conversational tone.</div><br/></div></div><div id="40128894" class="c"><input type="checkbox" id="c-40128894" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#40128116">root</a><span>|</span><a href="#40128264">parent</a><span>|</span><a href="#40128369">prev</a><span>|</span><a href="#40128140">next</a><span>|</span><label class="collapse" for="c-40128894">[-]</label><label class="expand" for="c-40128894">[1 more]</label></div><br/><div class="children"><div class="content">No they don&#x27;t use textbook text at all despite the paper title. They just asked GPT-4 to generate &quot;textbook quality&quot; content, which doesn&#x27;t even exactly looks like textbook.</div><br/></div></div></div></div><div id="40128140" class="c"><input type="checkbox" id="c-40128140" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#40128116">parent</a><span>|</span><a href="#40128264">prev</a><span>|</span><label class="collapse" for="c-40128140">[-]</label><label class="expand" for="c-40128140">[2 more]</label></div><br/><div class="children"><div class="content">Yes, &quot;chinchilla optimal&quot; is a meme, but 15T might turn out to be too many tokens.</div><br/><div id="40128190" class="c"><input type="checkbox" id="c-40128190" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40128116">root</a><span>|</span><a href="#40128140">parent</a><span>|</span><label class="collapse" for="c-40128190">[-]</label><label class="expand" for="c-40128190">[1 more]</label></div><br/><div class="children"><div class="content">My understanding from this tweet thread [1] is that chinchilla probably overspecified some of the hyperparameters to the model<p>tl;dr I&#x27;m looking forward to having lots of models (ideally models) trained with a wide range of parameters to narrow down &quot;what is actually optimal&quot;<p>I think there is an interesting tradeoff of data quality and data volume, though<p>(Eg if we train with the highest quality 10% of our data, does the model improve if we use the other 90%? What if we increase our data size by 10x?)<p>[1] <a href="https:&#x2F;&#x2F;twitter.com&#x2F;tamaybes&#x2F;status&#x2F;1780639257389904013" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;tamaybes&#x2F;status&#x2F;1780639257389904013</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>