<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737277254133" as="style"/><link rel="stylesheet" href="styles.css?v=1737277254133"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://chipsandcheese.com/p/skymont-intels-e-cores-reach-for-the-sky">Skymont: Intel&#x27;s E-Cores reach for the Sky</a> <span class="domain">(<a href="https://chipsandcheese.com">chipsandcheese.com</a>)</span></div><div class="subtext"><span>ksec</span> | <span>64 comments</span></div><br/><div><div id="42751521" class="c"><input type="checkbox" id="c-42751521" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42752454">next</a><span>|</span><label class="collapse" for="c-42751521">[-]</label><label class="expand" for="c-42751521">[33 more]</label></div><br/><div class="children"><div class="content">Triple decoder is one unique effect. The fact that Intel managed to get them lined up for small loops to do 9x effective instruction issue is basically miraculous IMO. Very well done.<p>Another unique effect is L2 shared between 4 cores. This means that thread communications across those 4 cores has much lower latencies.<p>I&#x27;ve had lots of debates with people online about this design vs Hyperthreading. It seems like the overall discovery from Intel is that highly threaded tasks use less resources (cache, ROPs, etc. etc).<p>Big cores (P cores or AMD Zen5) obviously can split into 2 hyperthread, but what if that division is still too big? E cores are 4 threads of support in roughly the same space as 1 Pcore.<p>This is because L2 cache is shared&#x2F;consolidated, and other resources (ROP buffers, register files, etc. etc.) are just all so much smaller on the Ecore.<p>It&#x27;s an interesting design. I&#x27;d still think that growing the cores to 4way SMT (like Xeon Phi) or 8way SMT (POWER10) would be a more conventional way to split up resources though. But obviously I don&#x27;t work at Intel or can make these kinds of decisions.</div><br/><div id="42753025" class="c"><input type="checkbox" id="c-42753025" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#42751521">parent</a><span>|</span><a href="#42752196">next</a><span>|</span><label class="collapse" for="c-42753025">[-]</label><label class="expand" for="c-42753025">[4 more]</label></div><br/><div class="children"><div class="content"><i>&gt; The fact that Intel managed to get them lined up for small loops to do 9x effective instruction issue is basically miraculous IMO</i><p>Not just small loops. It can reach 9x instruction decode on almost any control flow pattern. It just looks at the next 3 branch targets and starts decoding at each of them. As long as there is a branch every 32ish instructions (presumably a taken branch?), Skymont can keep all three uop queues full and Rename&#x2F;dispatch can consume uops at a sustained rate of 8 uops per cycle.<p>And in typical code, blocks with more than 32 instructions between branches are somewhat rare.<p>But Skymont has a brilliant trick for dealing with long runs of branchless code too: It simply inserts dummy branches into the branch predictor, breaking them into shorter blocks that fit into the 32 entry uop queues. The 3 decoders will start decoding the long block at three different positions, leap-frogging over each-other until the entire block is decoded and stuffed into the queues.<p>This design is absolutely brilliant. It seems to entirely solve the issue decoding X86, with far less resources than a uop cache. I suspect the approach will scale to almost unlimited numbers of decoders running in parallel, shifting the bottlenecks to other parts of the design (branch prediction and everything post decode)</div><br/><div id="42754056" class="c"><input type="checkbox" id="c-42754056" checked=""/><div class="controls bullet"><span class="by">rayiner</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42753025">parent</a><span>|</span><a href="#42752196">next</a><span>|</span><label class="collapse" for="c-42754056">[-]</label><label class="expand" for="c-42754056">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for the explanation. I was wondering how the heck Intel did to make a 9-way decode x86–a low power core of all things. Seems like an elegant approach.</div><br/><div id="42754255" class="c"><input type="checkbox" id="c-42754255" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42754056">parent</a><span>|</span><a href="#42752196">next</a><span>|</span><label class="collapse" for="c-42754255">[-]</label><label class="expand" for="c-42754255">[2 more]</label></div><br/><div class="children"><div class="content">The important bit: Intel E-cores now have 3x decoders each with the ability for 3-wide decode. When they work as a team, they can perform 9 decodes per clock tick (which then bottlenecks to 8 renamed uops in the best case scenario, and more than likely ~4 or ~3 more typical uops).</div><br/><div id="42754639" class="c"><input type="checkbox" id="c-42754639" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42754255">parent</a><span>|</span><a href="#42752196">next</a><span>|</span><label class="collapse" for="c-42754639">[-]</label><label class="expand" for="c-42754639">[1 more]</label></div><br/><div class="children"><div class="content">3-4 uops per cycle is more of an average throughput than a typical throughput.<p>The average is dragged down by many cycles that don&#x27;t decoded&#x2F;rename any uops. Either waiting for bytes to decode (icache miss, etc) or rename is blocked because the ROB is full (probably stalled on a dcache miss).<p>So you want a quite wide frontend so that whenever you are unblocked, you can drag the average up again.</div><br/></div></div></div></div></div></div></div></div><div id="42752196" class="c"><input type="checkbox" id="c-42752196" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#42751521">parent</a><span>|</span><a href="#42753025">prev</a><span>|</span><a href="#42752140">next</a><span>|</span><label class="collapse" for="c-42752196">[-]</label><label class="expand" for="c-42752196">[2 more]</label></div><br/><div class="children"><div class="content">While the frontend of Intel Skymont, which includes instruction fetching and decoding, is very original and unlike to that of any other CPU core, the backend of Skymont, which includes the execution units, is extremely similar to that of Arm Cortex-X4 (which is a.k.a. Neoverse V3 in its server variant and as Neoverse V3AE in its automotive variant).<p>This similarity consists in the fact that both Intel Skymont and Arm Cortex-X4 have the same number of execution units of each kind (and there are many kinds of execution units).<p>Therefore it can be expected that for any application whose performance is limited by the CPU core backend, the CPU cores Intel Skymont and Arm Cortex-X4 (or Neoverse V3) should have very similar performances.<p>Moreover, Intel Skymont and Arm Cortex-X4 have the same die area, i.e. around 1.7 square mm (including with both cores 1 MB of L2 cache in this area). Therefore the 2 cores not only should have about the same performance for backend-limited applications, but they also have the same cost.<p>Before Skymont, all the older Intel Atom cores had been designed to compete with the medium-size Arm Cortex-A7xx cores, even if the Intel Atom cores have always lagged in performance Cortex-A7xx by a year or two. For instance Intel Tremont had a very similar performance to Arm Cortex-A76, while Intel Gracemont and Crestmont have an extremely similar core backend with the series of Cortex-A78 to Cortex-A725 (like Gracemont and Crestmont, the 5 cores in the series Cortex-A78, Cortex-A710, Cortex-A715, Cortex-A720 and Cortex-A725 have only insignificant differences in the execution units).<p>With Skymont, Intel has made a jump in E-core size, positioning it as a match for Cortex-X, not for Cortex-A7xx, like its predecessors.</div><br/><div id="42755127" class="c"><input type="checkbox" id="c-42755127" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752196">parent</a><span>|</span><a href="#42752140">next</a><span>|</span><label class="collapse" for="c-42755127">[-]</label><label class="expand" for="c-42755127">[1 more]</label></div><br/><div class="children"><div class="content">&gt;positioning it as a match for Cortex-X<p>Well the recent Cortex X5 or 925 is already at around 3.4mm2 so that comparison isn&#x27;t exactly accurate. But I would love to test and see results on Skymont compared to X4. But I dont think they are available yet ( as an individual core ).<p>I am really looking forward to Clearwater Forest which is Skymont on 18A for Server.<p>And I know I am going to sound crazy but I wouldn&#x27;t mind a small SoC based on Skymont and Xe2 Graphics for Smartphone to Tablets.</div><br/></div></div></div></div><div id="42752140" class="c"><input type="checkbox" id="c-42752140" checked=""/><div class="controls bullet"><span class="by">trynumber9</span><span>|</span><a href="#42751521">parent</a><span>|</span><a href="#42752196">prev</a><span>|</span><a href="#42751667">next</a><span>|</span><label class="collapse" for="c-42752140">[-]</label><label class="expand" for="c-42752140">[2 more]</label></div><br/><div class="children"><div class="content">Skymont is an improvement but...<p>Skymont area efficiency should be compared to Zen 5C on 3nm. It has higher IPC, SMT with dual decoders - one for each thread, and full rate AVX-512 execution.<p>AMD didn&#x27;t have major difficulties in scaling down their SMT cores to achieve similar performance per area. But Intel went with different approach. At the cost of having different ISA support on each core in consumer devices and having to produce an SMT version of their P cores for servers anyway.</div><br/><div id="42752315" class="c"><input type="checkbox" id="c-42752315" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752140">parent</a><span>|</span><a href="#42751667">next</a><span>|</span><label class="collapse" for="c-42752315">[-]</label><label class="expand" for="c-42752315">[1 more]</label></div><br/><div class="children"><div class="content">It should be noted that Intel Skymont has the same area and it should also have the same performance for any backend-limited application with Arm Cortex-X4 (a.k.a. Neoverse V3) (both use 1.7 square mm in the &quot;3 nm&quot; TSMC fabrication process, while a Zen 5 compact might have an almost double area in the less dense &quot;4 nm&quot; process, with full vector pipelines, and a 3 square mm area with reduced vector pipelines, in the same less dense process).<p>Arm Cortex-X4 has the best performance per area of among the cores designed by Arm. Cortex-X925 has a double area in comparison with Cortex-X4, which results in a much lower performance per area. Cortex-A725 is smaller in area, but the area ratio is likely to be smaller than the performance ratio (for most kinds of execution units Cortex-X4 has a double number, while for some it has only a 50% or a 33% advantage), so it is likely that the performance per area of Cortex-A725 is worse than for Cortex-X4 and for Skymont.<p>For any programs that benefit from vector instructions, Zen 5 compact will have a much better performance per area than Intel Skymont and Arm Cortex-X4.<p>For programs that execute mostly irregular integer and pointer operations, there are chances for Intel Skymont and Arm Cortex-X4 to achieve better performance per area, but this is uncertain.<p>Intel Skymont and Arm Cortex-X4 have a greater number of integer&#x2F;pointer execution units per area than Zen 5 compact, even if Zen 5 compact were made with a TSMC process equally dense, which is not the case today.<p>Despite that, the execution units of Zen 5 compact will be busy a much higher percentage of the time, for several reasons. Zen 5 is better balanced, it has more resources for ensuring out-of-order and multithreaded execution, it has better cache memories. All these factors result in a higher IPC for Zen 5.<p>It is not clear whether the better IPC of Zen 5 is enough to compensate its greater area, when performing only irregular integer and pointer operations. Most likely is that in such cases Intel Skymont and Arm Cortex-X4 remain with a small advantage in performance per area, i.e. in performance per dollar, because the advantage in IPC of Zen 5 (when using SMT) may be in the range of 10% to 50%, while the advantage in area of Intel Skymont and Arm Cortex-X4 might be somewhere between 50% and 70%, had they been made with the same TSMC process.<p>On the other hand, for any program that can be accelerated with vector instructions, Zen 5 compact will crush in performance per area (i.e. in performance per dollar) any core designed by Intel or Arm.</div><br/></div></div></div></div><div id="42751667" class="c"><input type="checkbox" id="c-42751667" checked=""/><div class="controls bullet"><span class="by">01HNNWZ0MV43FF</span><span>|</span><a href="#42751521">parent</a><span>|</span><a href="#42752140">prev</a><span>|</span><a href="#42751930">next</a><span>|</span><label class="collapse" for="c-42751667">[-]</label><label class="expand" for="c-42751667">[16 more]</label></div><br/><div class="children"><div class="content">&gt; It seems like the overall discovery from Intel is that highly threaded tasks use less resources (cache, ROPs, etc. etc).<p>Does that mean if I can take a single-threaded program and split it into multiple threads, it might use less power? I have been telling myself that the only reason to use threads is to get more CPU power or to call blocking APIs. If they&#x27;re actually more power-efficient, that would change how I weigh threads vs. async</div><br/><div id="42751756" class="c"><input type="checkbox" id="c-42751756" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42751667">parent</a><span>|</span><a href="#42751930">next</a><span>|</span><label class="collapse" for="c-42751756">[-]</label><label class="expand" for="c-42751756">[15 more]</label></div><br/><div class="children"><div class="content">Not... quite. I think you&#x27;ve got the cause-and-effect backwards.<p>Programmers who happen to write multiple-threaded programs don&#x27;t need powerful cores, they want more cores. A Blender programmer calculating cloth physics would rather have 4x weaker cores than 1x P-core.<p>Programmers who happen to write powerful singled-threaded programs need powerful cores. For example, AMD&#x27;s &quot;X3D&quot; line of CPUs famously have 96MB of L3 cache, and video games that are on these very-powerful cores have much better performance.<p>Its not &quot;Programmers should change their code to fit the machine&quot;. From Intel&#x27;s perspective, CPU designers should change their core designs to match the different kinds of programmers. Single-threaded (or low-thread) programmers... largely represented by the Video Game programmers... want P-cores. But not very much of them.<p>Multithreaded programmers... represented by Graphics and a few others... want E-cores. Splitting a P-core into &quot;only&quot; 2 threads is not sufficient, they want 4x or even 8x more cores. Because there&#x27;s multiple communities of programmers out there, dedicating design teams to creating entirely different cores is a worthwhile endeavor.<p>--------<p>&gt; Does that mean if I can take a single-threaded program and split it into multiple threads, it might use less power? I have been telling myself that the only reason to use threads is to get more CPU power or to call blocking APIs. If they&#x27;re actually more power-efficient, that would change how I weigh threads vs. async<p>Power-efficiency is going to be incredibly difficult moving forward.<p>It should be noted that E-cores are not very power-efficient though. They&#x27;re area efficient, IE Cheaper for Intel to make. Intel can sell 4x as many E-cores for roughly the same price&#x2F;area as 1x P-core.<p>E-cores are cost-efficient cores. I think they happen to use slightly less power, but I&#x27;m not convinced that power-efficiency is their particular design goal.<p>If your code benefits from cache (ie: big cores), its probable that the lowest power-cost would be to run on large caches (like P-cores or Zen5 or Zen5 X3D). Communicating with RAM is always more power than just communicating with caches after all.<p>If your code does NOT benefit from cache (ie: Blender regularly has 100GB+ scenes for complex movies), then all of those spare resources on P-cores are useless, as nothing fits anyway and the core will be spending almost all of its time waiting on RAM to do anything. So the E-core will be more power efficient in this case.</div><br/><div id="42752572" class="c"><input type="checkbox" id="c-42752572" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42751756">parent</a><span>|</span><a href="#42753201">next</a><span>|</span><label class="collapse" for="c-42752572">[-]</label><label class="expand" for="c-42752572">[4 more]</label></div><br/><div class="children"><div class="content">&gt; A Blender programmer calculating cloth physics would rather have 4x weaker cores than 1x P-core.<p>Don’t they really want GPU threads for that? You wouldn’t get by with just weaker cores.</div><br/><div id="42753392" class="c"><input type="checkbox" id="c-42753392" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752572">parent</a><span>|</span><a href="#42753201">next</a><span>|</span><label class="collapse" for="c-42753392">[-]</label><label class="expand" for="c-42753392">[3 more]</label></div><br/><div class="children"><div class="content">Cloth physics in Blender are stored in RAM (as scenes and models can grow very large, too large for a GPU).<p>Figuring out which verticies for a physics simulation need to be sent to the GPU would be time, effort, and PCIe traffic _NOT_ running the cloth physics.<p>Furthermore, once all the data is figured out and blocked out in cache, its... cached. Cloth physics only interacts with a small number of close, nearby objects. Yeah, you _could_ calculate this small portion and send it to the GPU, but CPU is really good at just automatically traversing trees and storing the most recently used stuff in L1, L2, and L3 caches automatically (without any need of special code).<p>All in all, I expect something like Cloth physics (which is a calculation Blender currently does on CPU-only), is best done CPU only. Not because GPUs are bad at this algorithm... but instead because PCIe transfers are too slow and cloth physics is just too easily cached &#x2F; benefited by various CPU features.<p>It&#x27;d be a lot of effort to translate all that code to GPU and you likely won&#x27;t get large gains (like Raytracing&#x2F;Cycles&#x2F;Rendering gets for GPU Compute).</div><br/><div id="42753599" class="c"><input type="checkbox" id="c-42753599" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42753392">parent</a><span>|</span><a href="#42753201">next</a><span>|</span><label class="collapse" for="c-42753599">[-]</label><label class="expand" for="c-42753599">[2 more]</label></div><br/><div class="children"><div class="content">NVIDIA&#x27;s physX has its own cloth physics abstractions: <a href="https:&#x2F;&#x2F;docs.nvidia.com&#x2F;gameworks&#x2F;content&#x2F;gameworkslibrary&#x2F;physx&#x2F;guide&#x2F;Manual&#x2F;Cloth.html" rel="nofollow">https:&#x2F;&#x2F;docs.nvidia.com&#x2F;gameworks&#x2F;content&#x2F;gameworkslibrary&#x2F;p...</a>, so I&#x27;m sure it is a thing we do on GPUs already, if only for games. These are old demos anyways:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=80vKqJSAmIc" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=80vKqJSAmIc</a><p>I wonder what the difference is between the cloth physics you are talking about and the one NVIDIA has been doing for I think more than a decade now? Is it scale? It sounds like, at least, there are alternatives that do it on the GPU and there are questions if Blender will do it on the GPU:<p><a href="https:&#x2F;&#x2F;blenderartists.org&#x2F;t&#x2F;any-plans-to-make-cloth-simulation-gpu-based&#x2F;1554378" rel="nofollow">https:&#x2F;&#x2F;blenderartists.org&#x2F;t&#x2F;any-plans-to-make-cloth-simulat...</a></div><br/><div id="42753848" class="c"><input type="checkbox" id="c-42753848" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42753599">parent</a><span>|</span><a href="#42753201">next</a><span>|</span><label class="collapse" for="c-42753848">[-]</label><label class="expand" for="c-42753848">[1 more]</label></div><br/><div class="children"><div class="content">Cloth &#x2F; Hair physics in those games were graphics-only physics.<p>They could collide with any mesh that was inside of the GPU&#x27;s memory. But those calculations cannot work on any information stored on CPU RAM. Well... not efficiently anyway.<p>---------<p>When the Cloth simulator in Blender runs, it generates all kinds of information the CPU needs for other steps. In effect, Blender&#x27;s cloth physics serves as an input to animation frames, which is all CPU-side information.<p>Again: i know cloth physics executes on GPUs very well in isolation. But I&#x27;d be surprised if BLENDER&#x27;s specific cloth physics would ever be efficient on a GPU. Because as it turns out, calculations kind of don&#x27;t matter in the big-picture. There&#x27;s a lot of other things you need to do after those calculations (animations, key frames, and other such interactions). And if all that information is stored randomly in 100GB of CPU RAM, it&#x27;d be very hard to untangle that data and get it to a GPU (and back).<p>In a Video Game PHYSX setting, you just display the cloth physics to the screen. In Blender, a 3d animation program, you have to do a lot more with all that information and touch many other data-structures.<p>PCIe is very slow compared to RAM.</div><br/></div></div></div></div></div></div></div></div><div id="42753201" class="c"><input type="checkbox" id="c-42753201" checked=""/><div class="controls bullet"><span class="by">rasz</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42751756">parent</a><span>|</span><a href="#42752572">prev</a><span>|</span><a href="#42752304">next</a><span>|</span><label class="collapse" for="c-42753201">[-]</label><label class="expand" for="c-42753201">[5 more]</label></div><br/><div class="children"><div class="content">&gt;A Blender programmer calculating cloth physics would rather have 4x weaker cores than 1x P-core.<p>Nah, Blender programmer will prefer one core with AVX-512 instead of 4 without it.</div><br/><div id="42755005" class="c"><input type="checkbox" id="c-42755005" checked=""/><div class="controls bullet"><span class="by">dkjaudyeqooe</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42753201">parent</a><span>|</span><a href="#42753399">next</a><span>|</span><label class="collapse" for="c-42755005">[-]</label><label class="expand" for="c-42755005">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just more parallelism, they&#x27;ll take their parallelism wherever they can get it.<p>It&#x27;s to be seen if the future is more SIMD or more smaller general processors. Arguably the latter are more flexible but maybe not as efficientas the former.</div><br/></div></div><div id="42753399" class="c"><input type="checkbox" id="c-42753399" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42753201">parent</a><span>|</span><a href="#42755005">prev</a><span>|</span><a href="#42752304">next</a><span>|</span><label class="collapse" for="c-42753399">[-]</label><label class="expand" for="c-42753399">[3 more]</label></div><br/><div class="children"><div class="content">I mean, eventually yeah.<p>I like Zen5 as much as the next guy, but it should be noted that even today&#x27;s most recent version of Blender is AVX (256-bit) only. That means E-cores remain the optimal core to work with for a lot of Blender stuff.<p>Hopefully AMD Zen5 AVX512 becomes more popular. Maybe it&#x27;d become more popular as Intel rolls out AVX10 (somewhat compatible instruction set)</div><br/><div id="42753861" class="c"><input type="checkbox" id="c-42753861" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42753399">parent</a><span>|</span><a href="#42752304">next</a><span>|</span><label class="collapse" for="c-42753861">[-]</label><label class="expand" for="c-42753861">[2 more]</label></div><br/><div class="children"><div class="content">Would blender benefit from the bits of AVX-512 other than the width? I would think the approximate sqrt instructions might be useful.</div><br/><div id="42753881" class="c"><input type="checkbox" id="c-42753881" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42753861">parent</a><span>|</span><a href="#42752304">next</a><span>|</span><label class="collapse" for="c-42753881">[-]</label><label class="expand" for="c-42753881">[1 more]</label></div><br/><div class="children"><div class="content">AVX512 is one of the best instruction sets I&#x27;ve seen. No joke.<p>There&#x27;s all kinds of things AVX512 would help out in Blender. But those ways are incompatible with older AVX2 or SSE code. The question is if Blender will be willing to support SSE, AVX, and AVX512 code paths. Each new codepath is more maintenance and more effort.<p>AVX512 has more registers: not just 32x 512-bit registers (AVX normally has 16x 256-bit registers), but also the kmask registers (64-bits that take the place of old boolean logic that used to be done on the 256-bit registers). This alone should give far more optimizations for the compiler to automatically find.<p>There&#x27;s also VPCOMPRESSB and VPEXPANDB, Conflict-detection, and other instructions that make new SIMD data-structures far more efficient to implement. But this requires deep understanding that very few programmers have yet.</div><br/></div></div></div></div></div></div></div></div><div id="42752304" class="c"><input type="checkbox" id="c-42752304" checked=""/><div class="controls bullet"><span class="by">delusional</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42751756">parent</a><span>|</span><a href="#42753201">prev</a><span>|</span><a href="#42751930">next</a><span>|</span><label class="collapse" for="c-42752304">[-]</label><label class="expand" for="c-42752304">[5 more]</label></div><br/><div class="children"><div class="content">&gt; A Blender programmer calculating cloth physics would rather have 4x weaker cores than 1x P-core.<p>Is this true? In most of my work I&#x27;d usually rather have a single serializable thread of execution. Any parallelism usually comes with added overhead of synchronization, and added mental overhead of having to think about parallel execution. If I could freely pick between 4 IPC worth of single core or 1 IPC per core with 4 cores I&#x27;d pretty much always pick a single core. The catch is that we&#x27;re usually not trading like for like. Meaning I can get 3 IPC worth of single core or 4 IPC spread over 4 cores. Now I suddenly have to weigh the overhead and analyze my options.<p>Would you ever rather have multiple cores or an equivalent single core? Intuitively it feels like there&#x27;s some mathematics here.</div><br/><div id="42753421" class="c"><input type="checkbox" id="c-42753421" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752304">parent</a><span>|</span><a href="#42752483">next</a><span>|</span><label class="collapse" for="c-42753421">[-]</label><label class="expand" for="c-42753421">[1 more]</label></div><br/><div class="children"><div class="content">E-cores can typically execute ~4-instructions per clock tick in highly optimized code.<p>P-cores go up to like... 6-instructions. Better yes, but not dramatically better. The real issue is that P-cores have far more resources than E-cores: deeper reorder buffers to perform more out-of-order execution. Deeper branch prediction, more register files, larger caches, etc. etc.<p>So P-cores should be hitting the max of 6-instructions per clock tick on more kinds of code. E-cores have much smaller caches (and other resources) so they&#x27;ll run out and start stalling out to memory-limitations, which is like 0.1 instructions per clock tick or slower.<p>----------<p>But guess what? If a fancy P-core is memory-bound (like a lot of Blender code, due to the large-scale dozens+ GBs nature of modern 3d scenes), then those fancy P-cores run out of resources and are 0.1 IPC as well.<p>If both P-cores and E-cores are stalled out waiting on memory, you&#x27;d rather have 32x E-Cores all executing at 0.1 IPC, rather than only 8x P-cores executing at 0.1 IPC.<p>Its going to be a complex world moving forward: modern CPUs are growing far more complex and its not clear what the tradeoffs will be. But this reality of E-core and P-cores stalling out and waiting on memory is just how modern code works in too many cases.<p>And remember, its 4x E-cores are equivalent in area&#x2F;costs to 1x P-core. So there&#x27;s no contest in terms of overall instructions-per-second for E-core vs P-cores. The E-cores simply are better, even if the individual threads run slower.</div><br/></div></div><div id="42752483" class="c"><input type="checkbox" id="c-42752483" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752304">parent</a><span>|</span><a href="#42753421">prev</a><span>|</span><a href="#42752461">next</a><span>|</span><label class="collapse" for="c-42752483">[-]</label><label class="expand" for="c-42752483">[1 more]</label></div><br/><div class="children"><div class="content">Indeed a single thread is most simple to reason about, but if you have a single task that can already use 2 cores uniformly, going to 8 cores (assuming enough workload) should be a pretty clean 4x speedup (as long as you don&#x27;t run into memory bandwidth limits, but that&#x27;d cap the single-threaded code too).<p>But the performance difference between E-core and P-core perf is way less than 4x; the OP article shows a 1.6x&#x2F;1.7x difference in SPEC for skymont vs lion cove, and 1.3x&#x2F;1.7x for crestmont vs redwood code; and some searching around for past generations gives numbers around 1.4x.<p>Increasing core counts being a much more area- and energy-efficient way for hardware to provide more total performance than making the individual cores faster is a pretty fundamental thing.</div><br/></div></div><div id="42752461" class="c"><input type="checkbox" id="c-42752461" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752304">parent</a><span>|</span><a href="#42752483">prev</a><span>|</span><a href="#42752747">next</a><span>|</span><label class="collapse" for="c-42752461">[-]</label><label class="expand" for="c-42752461">[1 more]</label></div><br/><div class="children"><div class="content">For stuff like path tracing you have to work very hard not to trash the caches, so you&#x27;re often just waiting for memory.<p>That&#x27;s why such workloads gets a near linear scaling when using hyper-threads, unlike workloads like LLMs which are memory bandwidth bound.</div><br/></div></div><div id="42752747" class="c"><input type="checkbox" id="c-42752747" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752304">parent</a><span>|</span><a href="#42752461">prev</a><span>|</span><a href="#42751930">next</a><span>|</span><label class="collapse" for="c-42752747">[-]</label><label class="expand" for="c-42752747">[1 more]</label></div><br/><div class="children"><div class="content">Obviously it is easier to write any program as a single sequential thread, because you do not need to think about the dependencies between program statements. When you append a statement, you assume that all previous statements have been already executed, so the new statement can access without worries any data it needs.<p>The problem is that the speed of a single thread is limited and there exists no chance to increase it by significant amounts.<p>As long as we will continue to use silicon, there will be negligible increases in clock frequency. Switching to other semiconductors might bring us a double clock frequency in 10 years from now, but there will never be again a decade like that from 1993 to 2003, when the clock frequencies have increased 50 times.<p>The slow yearly increase in instructions per clock cycle is obtained by making the hardware do more and more of the work that has not been done by the programmer or the compiler, i.e. by extracting from the sequential program the separate chains of dependent instructions that should have been written as distinct threads, in order to execute them concurrently.<p>This division of a single instruction sequence into separate threads is extremely inefficient when done at runtime by hardware. Because of this the CPU cores with very high IPC have lower and lower performance per area and per power   with the increase of the IPC. Low performance per area and per power means low multithreaded performance.<p>So the CPU cores with very good single-threaded performance, like Intel Lion Cove or Arm Cortex-X925 have very poor multi-threaded performance and using many of them in a CPU would be futile, because in the same limited area one could put many more small CPU cores, achieving a much higher total performance.<p>This is why such big CPU cores that are good for single-threaded applications must be paired with smaller CPU cores, like Intel Skymont or Arm Cortex-X4, in order to obtain a good multi-threaded performance.<p>Writing the program as a single thread is easy and of course it should always be done so when the achieved performance is good enough on the current big superscalar CPU cores.<p>On the other hand, whenever the performance is not sufficient, there is no way to increase it a lot otherwise than by decomposing the work that must be done into multiple concurrent activities.<p>The easy case is that of iterations, which frequently provide large amounts of work that can be done concurrently. Moreover, with iterations there are many tools that can create concurrent threads automatically, like OpenMP or NVIDIA CUDA.<p>Where there are no iterations, one may need to do much more work to identify the dependencies between activities, in order to determine which may be executed concurrently, because they do not have functional dependencies between them.<p>However, when an entire program consists of a single chain of dependent instructions, which may happen e.g. when computing certain kinds of hash functions over a file, you are doomed. There is no way to increase the performance of that program.<p>Nevertheless even in such cases one can question whether the specification of the program is truly what the end user needs. For instance, when computing a hash over a file, the actual goal is normally not the computation of the hash, but to verify whether the file is the same as another (where the other file may be a past version of the same file, to detect modification, or an apparently distinct file coming from another source, when deduplication is desired). In such cases, it does not really matter which hash function is used, so it may be acceptable to replace the hash algorithm with another that allows concurrent computation, solving the performance problem.<p>Similar reformulations of the problem that must be solved may help in other cases where initially it appears that it is not possible to decompose the workload into concurrent tasks.</div><br/></div></div></div></div></div></div></div></div><div id="42751930" class="c"><input type="checkbox" id="c-42751930" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#42751521">parent</a><span>|</span><a href="#42751667">prev</a><span>|</span><a href="#42752200">next</a><span>|</span><label class="collapse" for="c-42751930">[-]</label><label class="expand" for="c-42751930">[2 more]</label></div><br/><div class="children"><div class="content">I see &quot;ROP&quot; and immediately think of Return Oriented Programming and exploits...</div><br/><div id="42751949" class="c"><input type="checkbox" id="c-42751949" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42751930">parent</a><span>|</span><a href="#42752200">next</a><span>|</span><label class="collapse" for="c-42751949">[-]</label><label class="expand" for="c-42751949">[1 more]</label></div><br/><div class="children"><div class="content">Lulz, I got some wires crossed. The CPU resource I meant to say was ROB: Reorder Buffer.<p>I don&#x27;t know why I wrote ROP. You&#x27;re right, ROP means return oriented programming. A completely different thing.</div><br/></div></div></div></div><div id="42752200" class="c"><input type="checkbox" id="c-42752200" checked=""/><div class="controls bullet"><span class="by">scrubs</span><span>|</span><a href="#42751521">parent</a><span>|</span><a href="#42751930">prev</a><span>|</span><a href="#42753142">next</a><span>|</span><label class="collapse" for="c-42752200">[-]</label><label class="expand" for="c-42752200">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Another unique effect is L2 shared between 4 cores. This means that thread communications across those 4 cores has much lower latencies.&quot;<p>@dragontamer solid point. Consider a in memory ring shared between two threads. There&#x27;s huge difference in throughput and latency if the threads share L2 (on same core) or when on different cores all down to the relative slowness of L3.<p>Are there other cpus (arm, graviton?) that have similarly shared L2 caches?</div><br/><div id="42753436" class="c"><input type="checkbox" id="c-42753436" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752200">parent</a><span>|</span><a href="#42753142">next</a><span>|</span><label class="collapse" for="c-42753436">[-]</label><label class="expand" for="c-42753436">[1 more]</label></div><br/><div class="children"><div class="content">Hyperthreading actually shares L1 caches between two threads (after all, two threads are running in the same L1 cache and core).<p>I believe SMT4 and SMT8 cores from IBM Power10 also have L1 caches shared (8 threads on one core), and thus benefit from communication speeds.<p>But you&#x27;re right in that this is a very obscure performance quirk. I&#x27;m honestly not aware of any practical code that takes advantage of this. E-cores are perhaps the most &quot;natural&quot; implementation of high-speed core-to-core communications though.</div><br/></div></div></div></div><div id="42753142" class="c"><input type="checkbox" id="c-42753142" checked=""/><div class="controls bullet"><span class="by">Salgat</span><span>|</span><a href="#42751521">parent</a><span>|</span><a href="#42752200">prev</a><span>|</span><a href="#42752001">next</a><span>|</span><label class="collapse" for="c-42753142">[-]</label><label class="expand" for="c-42753142">[2 more]</label></div><br/><div class="children"><div class="content">What we desperately need before we get too deep into this is stronger support in languages for heterogeneous cores in an architecture agnostic way. Some way to annotate that certain threads should run on certain types of cores (and close together in memory hierarchy) without getting too deep into implementation details.</div><br/><div id="42754839" class="c"><input type="checkbox" id="c-42754839" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42753142">parent</a><span>|</span><a href="#42752001">next</a><span>|</span><label class="collapse" for="c-42754839">[-]</label><label class="expand" for="c-42754839">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think so.  I don&#x27;t trust software authors to make the right choice, and the most tilted examples of where a core will usually need a bigger core can afford to wait for the scheduler to figure it out.<p>And if you want to be close together in the memory hierarchy, does that mean close to the RAM that you can easily get to?  And you want allocations from there?  If you really want that, you can use numa(3).<p>&gt; without getting too deep into implementation details.<p>Every microarchitecture is a special case about what you win by being close to things, and how it plays with contention and distances to other things.  You either don&#x27;t care and trust the infrastructure, or you want to micromanage it all, IMO.</div><br/></div></div></div></div><div id="42752001" class="c"><input type="checkbox" id="c-42752001" checked=""/><div class="controls bullet"><span class="by">Zardoz84</span><span>|</span><a href="#42751521">parent</a><span>|</span><a href="#42753142">prev</a><span>|</span><a href="#42752454">next</a><span>|</span><label class="collapse" for="c-42752001">[-]</label><label class="expand" for="c-42752001">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve had lots of debates with people online about this design vs Hyperthreading. It seems like the overall discovery from Intel is that highly threaded tasks use less resources (cache, ROPs, etc. etc).<p>AMD did something similar before. Anyone don&#x27;t remember the Bulldozer cores sharing resources between pair of cores ?</div><br/><div id="42753443" class="c"><input type="checkbox" id="c-42753443" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42751521">root</a><span>|</span><a href="#42752001">parent</a><span>|</span><a href="#42752454">next</a><span>|</span><label class="collapse" for="c-42753443">[-]</label><label class="expand" for="c-42753443">[1 more]</label></div><br/><div class="children"><div class="content">AMD&#x27;s modern SMT implementation is just better than Bulldozer&#x27;s decoder sharing.<p>Modern Zen5 has very good SMT implementations. Back 10 years ago, people mostly talked about Bulldozer&#x27;s design to make fun of it. It always was intriguing to me but it just never seemed to be practical in any workflow.</div><br/></div></div></div></div></div></div><div id="42752454" class="c"><input type="checkbox" id="c-42752454" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#42751521">prev</a><span>|</span><a href="#42752085">next</a><span>|</span><label class="collapse" for="c-42752454">[-]</label><label class="expand" for="c-42752454">[12 more]</label></div><br/><div class="children"><div class="content">Intel advertising the fact that their schedulers can keep MS Teams confined to the efficiency cores... what a sad reflection of how bloated Teams is.<p>We make a single Electron-like app grow, cancer-like, to do everything from messaging and videoconferencing to shared drive browsing and editing, and as a result we have to contain it.</div><br/><div id="42752873" class="c"><input type="checkbox" id="c-42752873" checked=""/><div class="controls bullet"><span class="by">notepad0x90</span><span>|</span><a href="#42752454">parent</a><span>|</span><a href="#42755019">next</a><span>|</span><label class="collapse" for="c-42752873">[-]</label><label class="expand" for="c-42752873">[7 more]</label></div><br/><div class="children"><div class="content">It can run in your browser too.The electron part isn&#x27;t the bloat but the web part. Web devs keep using framework on top of frameworks and the bloat is endless. Lack of good native UX kits forces devs to use web-based kits. Qt has a nice idea with qml but aside from some limitations, it is mostly C++ (yes, pyqt,etc.. exist).<p>Native UI kits should be able to do better than web-based kits. But I suspect just as with the web, the problem is consistency. The one thing the web does right is deliver consistent UI experience across various hardware with less dev time. It all comes down to which method has least amounts of friction for devs? Large tech companies spent a lot of time and money in dev tooling for their web services, so web based approaches to solve problems inherently have to be taken for even trivial apps (not that teams is one).<p>Open source native UX kits that work consistently across platforms and languages would solve much of this. Unfortunately, the open source community is stuck on polishing gtk and qt.</div><br/><div id="42754829" class="c"><input type="checkbox" id="c-42754829" checked=""/><div class="controls bullet"><span class="by">knubie</span><span>|</span><a href="#42752454">root</a><span>|</span><a href="#42752873">parent</a><span>|</span><a href="#42753320">next</a><span>|</span><label class="collapse" for="c-42754829">[-]</label><label class="expand" for="c-42754829">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The electron part isn&#x27;t the bloat but the web part.<p>The bloat part is the bloat. Web apps can made to be perfectly performant if you are diligent, and native apps can be made to be bloated and slow if you&#x27;re not.</div><br/></div></div><div id="42753320" class="c"><input type="checkbox" id="c-42753320" checked=""/><div class="controls bullet"><span class="by">jenadine</span><span>|</span><a href="#42752454">root</a><span>|</span><a href="#42752873">parent</a><span>|</span><a href="#42754829">prev</a><span>|</span><a href="#42753179">next</a><span>|</span><label class="collapse" for="c-42753320">[-]</label><label class="expand" for="c-42753320">[1 more]</label></div><br/><div class="children"><div class="content">There are a bunch of new native UI toolkit as well, such as Slint [<a href="https:&#x2F;&#x2F;slint.dev" rel="nofollow">https:&#x2F;&#x2F;slint.dev</a>]</div><br/></div></div><div id="42753179" class="c"><input type="checkbox" id="c-42753179" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42752454">root</a><span>|</span><a href="#42752873">parent</a><span>|</span><a href="#42753320">prev</a><span>|</span><a href="#42753618">next</a><span>|</span><label class="collapse" for="c-42753179">[-]</label><label class="expand" for="c-42753179">[3 more]</label></div><br/><div class="children"><div class="content">Sometimes I find myself wistfully thinking of  
Java Swing applets with native theme settings.</div><br/><div id="42753331" class="c"><input type="checkbox" id="c-42753331" checked=""/><div class="controls bullet"><span class="by">notepad0x90</span><span>|</span><a href="#42752454">root</a><span>|</span><a href="#42753179">parent</a><span>|</span><a href="#42753618">next</a><span>|</span><label class="collapse" for="c-42753331">[-]</label><label class="expand" for="c-42753331">[2 more]</label></div><br/><div class="children"><div class="content">They say nostalgia is always deceptive, but I miss ~java2 era UX.</div><br/><div id="42753408" class="c"><input type="checkbox" id="c-42753408" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42752454">root</a><span>|</span><a href="#42753331">parent</a><span>|</span><a href="#42753618">next</a><span>|</span><label class="collapse" for="c-42753408">[-]</label><label class="expand" for="c-42753408">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it was Java itself, but many operating systems simply had a much stronger set of UX that was cohesively being followed.<p>Yet here we are, in an era where you can encounter multiple choices and you don&#x27;t know whether it&#x27;s a single select versus a multi-select tick box. And then there&#x27;s something with a boolean state, and it&#x27;s not clear which color means it&#x27;s currently active. Then you hit alt-F for the File menu order to quit your browser in frustration, but the web page blocks you because it has decided that means that you&#x27;re going to &quot;Favorite&quot; whatever you&#x27;re looking at.</div><br/></div></div></div></div></div></div><div id="42753618" class="c"><input type="checkbox" id="c-42753618" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#42752454">root</a><span>|</span><a href="#42752873">parent</a><span>|</span><a href="#42753179">prev</a><span>|</span><a href="#42755019">next</a><span>|</span><label class="collapse" for="c-42753618">[-]</label><label class="expand" for="c-42753618">[1 more]</label></div><br/><div class="children"><div class="content">&gt;But I suspect just as with the web, the problem is consistency.<p>That is indeed the &quot;problem&quot; at its core. People are lazy, operating systems aren&#x27;t consistent enough.<p>Operating systems came about to abstract all the differences of countless hardware away, but that is no longer good enough. Now people want to abstract away that abstraction: Chrome.<p>Chrome is the abstraction layer to Windows, MacOS, iOS, Linux, Android, BSD, C++, HTML, PHP, Ruby, Rust, Python, desktops, laptops, smartphones, tablets, and all the other things. You develop for Chrome and everyone uses Chrome and everyone on both sides gets the same thing for a singular effort.<p>If I were to step away from all I know and care about computers and see as an uncaring man, I have to admit: It makes perfect sense. Fuck all that noise. Code for Chrome and use by Chrome. The world can&#x27;t be simpler.</div><br/></div></div></div></div><div id="42755019" class="c"><input type="checkbox" id="c-42755019" checked=""/><div class="controls bullet"><span class="by">gymbeaux</span><span>|</span><a href="#42752454">parent</a><span>|</span><a href="#42752873">prev</a><span>|</span><a href="#42752782">next</a><span>|</span><label class="collapse" for="c-42755019">[-]</label><label class="expand" for="c-42755019">[1 more]</label></div><br/><div class="children"><div class="content">Intel and Windows have been birds of a feather for 40 years, and it frustrates me that to this day Intel is still designing its CPUs around Windows. Yes, Windows is “#1” in market share. It’s just sad that the world accepts the ads in Windows 11, Teams being an electron app, x86 when ARM is the clear winner (at least for laptops), all the other Windows nonsense.</div><br/></div></div><div id="42752782" class="c"><input type="checkbox" id="c-42752782" checked=""/><div class="controls bullet"><span class="by">sho_hn</span><span>|</span><a href="#42752454">parent</a><span>|</span><a href="#42755019">prev</a><span>|</span><a href="#42753256">next</a><span>|</span><label class="collapse" for="c-42752782">[-]</label><label class="expand" for="c-42752782">[1 more]</label></div><br/><div class="children"><div class="content">These days you need a CPU, a GPU, a NPU and a TPU (not Tensor, but Teams Processing Unit).<p>In my case, the TPU is a seperate Mac that also does Outlook, and the real work gets done on the Linux laptop. I refer to this as the airgap to protect my sanity.</div><br/></div></div><div id="42753256" class="c"><input type="checkbox" id="c-42753256" checked=""/><div class="controls bullet"><span class="by">p_ing</span><span>|</span><a href="#42752454">parent</a><span>|</span><a href="#42752782">prev</a><span>|</span><a href="#42752643">next</a><span>|</span><label class="collapse" for="c-42753256">[-]</label><label class="expand" for="c-42753256">[1 more]</label></div><br/><div class="children"><div class="content">Teams is technically WebView2 and not Electron… with a bunch of native platform code.</div><br/></div></div><div id="42752643" class="c"><input type="checkbox" id="c-42752643" checked=""/><div class="controls bullet"><span class="by">BonoboIO</span><span>|</span><a href="#42752454">parent</a><span>|</span><a href="#42753256">prev</a><span>|</span><a href="#42752085">next</a><span>|</span><label class="collapse" for="c-42752643">[-]</label><label class="expand" for="c-42752643">[1 more]</label></div><br/><div class="children"><div class="content">It’s amazing what computers can do today and how absolutely inefficient they are used.</div><br/></div></div></div></div><div id="42752085" class="c"><input type="checkbox" id="c-42752085" checked=""/><div class="controls bullet"><span class="by">rwmj</span><span>|</span><a href="#42752454">prev</a><span>|</span><a href="#42752664">next</a><span>|</span><label class="collapse" for="c-42752085">[-]</label><label class="expand" for="c-42752085">[11 more]</label></div><br/><div class="children"><div class="content">Slightly off topic, but if I&#x27;m aiming to get the fastest &#x27;make -jN&#x27; for some random C project (such as the kernel) should I set N = #P [threads] + #E, or just the #P, or something else?  Basically, is there a case where using the E cores slows a compile down?  Or is power management a factor?<p>I timed it on the single Intel machine I have access to with E-cores and setting N = #P + #E was in fact the fastest, but I wonder if that&#x27;s a general rule.</div><br/><div id="42752456" class="c"><input type="checkbox" id="c-42752456" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#42752085">parent</a><span>|</span><a href="#42752141">next</a><span>|</span><label class="collapse" for="c-42752456">[-]</label><label class="expand" for="c-42752456">[1 more]</label></div><br/><div class="children"><div class="content">On my tests on an AMD Zen 3 (a 5900X), I have determined that with SMT disabled, the best performance is with N+1 threads, where N is the number of cores.<p>On the other hand, with SMT enabled, the best performance has been obtained with 2N threads, where N is the same as above, i.e. with the same number of threads as supported in hardware.<p>For example, on a 12C&#x2F;24T 5900X, it is best to use &quot;make -j13&quot; if SMT is disabled, but &quot;make -j24&quot; if SMT is enabled.<p>For software project compilation, enabling SMT is always a win, which is not always the case for other applications, i.e. for those where the execution time is dominated by optimized loops.<p>Similarly, I expect that for the older Meteor Lake, Raptor Lake and Alder Lake CPUs the best compilation speed is achieved with 2 x #P + #E threads, even if this should improve the compilation time by only something like 10% over that achieved with #P + #E threads. At least the published compilation benchmarks are consistent with this expectation.<p>EDIT:
I see from your other posting that you have used a notation that has confused me, i.e. that by #P you have meant the number of SMT threads that can be executed on P cores, not the number of P cores.<p>Therefore what I have written as 2 x #P + #E is the same as what you have written as #P + #E.<p>So your results are the same with what I have obtained on AMD CPUs. With SMT enabled, the optimal number of threads is the total number of threads supported by the CPU, where the SMT cores support multiple threads.<p>Only with SMT disabled an extra thread over the number of cores becomes beneficial.<p>The reason for this difference in behavior is that with SMT disabled, any thread that is stalled by waiting for I&#x2F;O leaves a CPU core idle, slowing the progress. With SMT enabled, when any thread is stalled, the threads are redistributed to balance the workload and no core remains idle. Only 1 of the P cores runs a thread instead of 2, which reduces its throughput by only a few percent. Avoiding this small loss in throughput by running an extra thread does not provide enough additional performance to compensate the additional overhead caused by an extra thread.</div><br/></div></div><div id="42752141" class="c"><input type="checkbox" id="c-42752141" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#42752085">parent</a><span>|</span><a href="#42752456">prev</a><span>|</span><a href="#42752108">next</a><span>|</span><label class="collapse" for="c-42752141">[-]</label><label class="expand" for="c-42752141">[2 more]</label></div><br/><div class="children"><div class="content">Power management is a factor because the cores &quot;steal&quot; power from each other. However the E-cores are more efficient so slowing down P-cores and giving some of their power to the E-cores increases the efficiency and performance of the chip overall. In general you&#x27;re better off using all the cores.</div><br/><div id="42752686" class="c"><input type="checkbox" id="c-42752686" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#42752085">root</a><span>|</span><a href="#42752141">parent</a><span>|</span><a href="#42752108">next</a><span>|</span><label class="collapse" for="c-42752686">[-]</label><label class="expand" for="c-42752686">[1 more]</label></div><br/><div class="children"><div class="content">I suggest this depends on the exact model you are using. On Alder&#x2F;Raptor Lake, the E-cores run at 4.5GHz which is completely futile, and in doing so they heat their adjacent P-cores, because 2x E-core clusters can easily draw 135W or more. This can significantly cut into the headroom of the nearby P. Arrow Lake-S has rearranged the E-cores.</div><br/></div></div></div></div><div id="42752108" class="c"><input type="checkbox" id="c-42752108" checked=""/><div class="controls bullet"><span class="by">saurik</span><span>|</span><a href="#42752085">parent</a><span>|</span><a href="#42752141">prev</a><span>|</span><a href="#42752664">next</a><span>|</span><label class="collapse" for="c-42752108">[-]</label><label class="expand" for="c-42752108">[7 more]</label></div><br/><div class="children"><div class="content">Did you test at least +1 if not *1.5 or something? I would expect you to occasionally get blocked on disk I&#x2F;O and would want some spare work sitting hot to switch in.</div><br/><div id="42752119" class="c"><input type="checkbox" id="c-42752119" checked=""/><div class="controls bullet"><span class="by">rwmj</span><span>|</span><a href="#42752085">root</a><span>|</span><a href="#42752108">parent</a><span>|</span><a href="#42752664">next</a><span>|</span><label class="collapse" for="c-42752119">[-]</label><label class="expand" for="c-42752119">[6 more]</label></div><br/><div class="children"><div class="content">Let me test that now.  Note I only have 1 Intel machine so any results are very specific to this laptop.<p><pre><code>  -j           time (mean ± σ)
  12 (#P+#E)   130.889 s ±  4.072 s
  13 (..+1)    135.049 s ±  2.270 s
   4 (#P)      179.845 s ±  1.783 s
   8 (#E)      141.669 s ±  3.441 s
</code></pre>
Machine: 13th Gen Intel(R) Core(TM) i7-1365U;
2 x P-cores (4 threads), 8 x E-cores</div><br/><div id="42752557" class="c"><input type="checkbox" id="c-42752557" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42752085">root</a><span>|</span><a href="#42752119">parent</a><span>|</span><a href="#42752664">next</a><span>|</span><label class="collapse" for="c-42752557">[-]</label><label class="expand" for="c-42752557">[5 more]</label></div><br/><div class="children"><div class="content">Your processor has two P cores, and ten cores total, not twelve. The HyperThreading (SMT) does not make the two P cores into four cores. Your experiment with 4 threads will most likely result in using both P cores and two E cores, as no sane OS would double up threads on the P cores before the E cores were full with one thread each.</div><br/><div id="42752610" class="c"><input type="checkbox" id="c-42752610" checked=""/><div class="controls bullet"><span class="by">rwmj</span><span>|</span><a href="#42752085">root</a><span>|</span><a href="#42752557">parent</a><span>|</span><a href="#42752692">next</a><span>|</span><label class="collapse" for="c-42752610">[-]</label><label class="expand" for="c-42752610">[2 more]</label></div><br/><div class="children"><div class="content">The hyperthreading should cover up memory latency, since the workload (compiling qemu) might not fit into L3 cache.  Although I take your point that it doesn&#x27;t magically create two core-equivalents.</div><br/><div id="42753268" class="c"><input type="checkbox" id="c-42753268" checked=""/><div class="controls bullet"><span class="by">gonzo</span><span>|</span><a href="#42752085">root</a><span>|</span><a href="#42752610">parent</a><span>|</span><a href="#42752692">next</a><span>|</span><label class="collapse" for="c-42753268">[-]</label><label class="expand" for="c-42753268">[1 more]</label></div><br/><div class="children"><div class="content">“Hyperthreading” is a write pipe hack.<p>If the core stalls on a write then the other thread gets run.</div><br/></div></div></div></div><div id="42752692" class="c"><input type="checkbox" id="c-42752692" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#42752085">root</a><span>|</span><a href="#42752557">parent</a><span>|</span><a href="#42752610">prev</a><span>|</span><a href="#42752664">next</a><span>|</span><label class="collapse" for="c-42752692">[-]</label><label class="expand" for="c-42752692">[2 more]</label></div><br/><div class="children"><div class="content">I am sure rwmj was smart enough to use `taskset` to make this experiment meaningful.</div><br/><div id="42752704" class="c"><input type="checkbox" id="c-42752704" checked=""/><div class="controls bullet"><span class="by">rwmj</span><span>|</span><a href="#42752085">root</a><span>|</span><a href="#42752692">parent</a><span>|</span><a href="#42752664">next</a><span>|</span><label class="collapse" for="c-42752704">[-]</label><label class="expand" for="c-42752704">[1 more]</label></div><br/><div class="children"><div class="content">Hehe, if only :-(  However I do want to know what&#x27;s best with the default Linux scheduler and just using &#x27;make&#x27; rather than more complicated commands.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42752664" class="c"><input type="checkbox" id="c-42752664" checked=""/><div class="controls bullet"><span class="by">happycube</span><span>|</span><a href="#42752085">prev</a><span>|</span><a href="#42752917">next</a><span>|</span><label class="collapse" for="c-42752664">[-]</label><label class="expand" for="c-42752664">[2 more]</label></div><br/><div class="children"><div class="content">Sounds like a nice core, but intel&#x27;s gone from &quot;fries itself&quot; broken in Raptor Lake to merely a broken cache&#x2F;memory archecture in Lunar Lake.<p>None of the integer improvements make it onto the screen, as it were.<p>edit:  I got Lunar and Arrow Lake&#x27;s issues mixed up, but still there should be a lot more integer uplift on paper.</div><br/><div id="42752765" class="c"><input type="checkbox" id="c-42752765" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#42752664">parent</a><span>|</span><a href="#42752917">next</a><span>|</span><label class="collapse" for="c-42752765">[-]</label><label class="expand" for="c-42752765">[1 more]</label></div><br/><div class="children"><div class="content">What do you consider broken about Lunar Lake? It looks good to me. The E-cores are on a separate island to allow the ring to power off and that does lead to lower performance but it&#x27;s still good enough IMO.</div><br/></div></div></div></div><div id="42752917" class="c"><input type="checkbox" id="c-42752917" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#42752664">prev</a><span>|</span><a href="#42753493">next</a><span>|</span><label class="collapse" for="c-42752917">[-]</label><label class="expand" for="c-42752917">[4 more]</label></div><br/><div class="children"><div class="content">I still don’t quite get how the cpu knows what is low priority or background. Or is that steered at OS level a bit like cpu pinning ?</div><br/><div id="42753121" class="c"><input type="checkbox" id="c-42753121" checked=""/><div class="controls bullet"><span class="by">tredre3</span><span>|</span><a href="#42752917">parent</a><span>|</span><a href="#42752924">next</a><span>|</span><label class="collapse" for="c-42753121">[-]</label><label class="expand" for="c-42753121">[2 more]</label></div><br/><div class="children"><div class="content">When the P&#x2F;E model was introduced by Intel, there was a fairly long transition period where both Windows and Linux performed unpredictably poorly for most compute-intensive work loads, to the point where the advice was to disable the E cores entirely if you were gaming or doing anything remotely CPU-intensive or if your OS was never going to be updated (Win 7&#x2F;8, many LTS Linux).<p>It&#x27;s not entirely clear to me why it took a while to add support on Linux because the kernel already supported big.LITTLE and from the scheduler&#x27;s point of view it&#x27;s the same thing as Intel&#x27;s P&#x2F;E cores. I guess the patch must&#x27;ve been simple but it just took very long to trickle down to common distributions?</div><br/><div id="42753714" class="c"><input type="checkbox" id="c-42753714" checked=""/><div class="controls bullet"><span class="by">baobun</span><span>|</span><a href="#42752917">root</a><span>|</span><a href="#42753121">parent</a><span>|</span><a href="#42752924">next</a><span>|</span><label class="collapse" for="c-42753714">[-]</label><label class="expand" for="c-42753714">[1 more]</label></div><br/><div class="children"><div class="content">Not very surprisingly but IME running VMs you still want to pin (at least on Linux).</div><br/></div></div></div></div><div id="42752924" class="c"><input type="checkbox" id="c-42752924" checked=""/><div class="controls bullet"><span class="by">computerliker</span><span>|</span><a href="#42752917">parent</a><span>|</span><a href="#42753121">prev</a><span>|</span><a href="#42753493">next</a><span>|</span><label class="collapse" for="c-42752924">[-]</label><label class="expand" for="c-42752924">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Scheduling_(computing)" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Scheduling_(computing)</a></div><br/></div></div></div></div><div id="42753493" class="c"><input type="checkbox" id="c-42753493" checked=""/><div class="controls bullet"><span class="by">taspeotis</span><span>|</span><a href="#42752917">prev</a><span>|</span><label class="collapse" for="c-42753493">[-]</label><label class="expand" for="c-42753493">[1 more]</label></div><br/><div class="children"><div class="content">(2024)</div><br/></div></div></div></div></div></div></div></body></html>