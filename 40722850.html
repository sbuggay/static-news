<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718787673584" as="style"/><link rel="stylesheet" href="styles.css?v=1718787673584"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://3tilley.github.io/posts/simple-ipc-ping-pong/">IPC in Rust – A Ping Pong Comparison</a> <span class="domain">(<a href="https://3tilley.github.io">3tilley.github.io</a>)</span></div><div class="subtext"><span>todsacerdoti</span> | <span>28 comments</span></div><br/><div><div id="40724088" class="c"><input type="checkbox" id="c-40724088" checked=""/><div class="controls bullet"><span class="by">gamozolabs</span><span>|</span><a href="#40723935">next</a><span>|</span><label class="collapse" for="c-40724088">[-]</label><label class="expand" for="c-40724088">[8 more]</label></div><br/><div class="children"><div class="content">I wrote an extremely fast hot polled pipe in Rust for QEMU instrumentation. I’m sure there’s room to improve but it’s effectively bottlenecking on the uarch. <a href="https:&#x2F;&#x2F;github.com&#x2F;MarginResearch&#x2F;cannoli&#x2F;blob&#x2F;main&#x2F;mempipe&#x2F;src&#x2F;lib.rs">https:&#x2F;&#x2F;github.com&#x2F;MarginResearch&#x2F;cannoli&#x2F;blob&#x2F;main&#x2F;mempipe&#x2F;...</a><p>This was specifically designed for one producer (the QEMU processor), and many consumers (processing the trace of instructions and memory accesses). I can&#x27;t remember what specific tunings I did to help with this specific model. Data structures like this can always be tuned slightly based on your workload, by changing structure shapes, shared cache lines, etc.<p>With about 3-4 consumers QEMU was not really blocking on the reporting of every single instruction executed, which is really cool. This requires a low noise system, just having a browser open can almost half these numbers since there&#x27;s just more cache coherency bus traffic occuring. If having a browser open doesn&#x27;t affect your benchmark, it&#x27;s probably not bottlenecking on the uarch yet ;)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;MarginResearch&#x2F;cannoli&#x2F;blob&#x2F;main&#x2F;.assets&#x2F;perf_graph.png">https:&#x2F;&#x2F;github.com&#x2F;MarginResearch&#x2F;cannoli&#x2F;blob&#x2F;main&#x2F;.assets&#x2F;...</a><p>A little blog on Cannoli itself here: <a href="https:&#x2F;&#x2F;margin.re&#x2F;2022&#x2F;05&#x2F;cannoli-the-fast-qemu-tracer&#x2F;" rel="nofollow">https:&#x2F;&#x2F;margin.re&#x2F;2022&#x2F;05&#x2F;cannoli-the-fast-qemu-tracer&#x2F;</a><p>Ultimately, mempipe is not really a big thing I&#x27;ve talked about, but it&#x27;s actually what makes cannoli so good and enables the design in the first place.</div><br/><div id="40724345" class="c"><input type="checkbox" id="c-40724345" checked=""/><div class="controls bullet"><span class="by">gamozolabs</span><span>|</span><a href="#40724088">parent</a><span>|</span><a href="#40723935">next</a><span>|</span><label class="collapse" for="c-40724345">[-]</label><label class="expand" for="c-40724345">[7 more]</label></div><br/><div class="children"><div class="content">Anyways, since you express disappointment in ~1000 cycle cost. That&#x27;s about right. The latency between cores is actually quite high and there&#x27;s not much you can do about it, especially on a system like x86 which has extremely strong cache coherency by default. One thing that is really important to understand is that the ownership of cache lines dramatically affects the cost of memory.<p>For IPC, this effectively requires one thread writing to memory (thus, making that cache line modified to that core, and evicted from all other cores). Then, when the polling thread checks in on the line, it will have to demote that cache line from modified, to shared by flushing it out to memory (usually L2 or L3, but also writing out to memory). This causes some memory traffic, and constantly means that the cores are fighting over the same cache line. Since x86 is strongly ordered and caches are coherent, this traffic is extremely expensive. Think of a write as &quot;tell all other cores that you modifed this memory, so they have to evict&#x2F;invalidate their cache lines&quot;. And a read as &quot;tell all cores to flush their cache lines if they&#x27;re modified, then wait for them to tell me they&#x27;re done, then I can read the memory&quot;. This effectively is a massively blocking operation. The simple act of reading the mailbox&#x2F;ticket&#x2F;whatever from another core to check if a message is ready will actually dramatically affect the speed the other core can write to it (as now that write is effectively full latency).<p>There are some tricks you can do to get extremely low latency between cores. One of them, is making sure you&#x27;re on cores that are physically near each other (eg. on the same processor socket). This is only really relevant on servers, but it&#x27;s a big thing. You can actually map out the physical processor layout, including on a single die, based on the latency between these cores. It&#x27;s quite subtle and requires low noise, but it&#x27;s really cool to map out the grid of cores on the actual silicon due to timing.<p>Another trick that you can do, is have both threads on the same core, thus, using hyperthreads. Hyperthreads share the same core and thus a lot of resources, and are able to actually skip some of the more expensive coherency traffic, as they share the same L1 cache (since L1 is per-core). The lowest latency you will be able to observe for IPC will be on the same core with hyperthreads, but that&#x27;s often not really useful for _processing_ the data, since performance will not be great on two busy cores. But in theory, you can signal a hyperthread, the hyperthread can then go and raise some other signal, while the original hyperthread still continues doing some relevant work. As long as one of them is blocking&#x2F;halted, the other won&#x27;t really be affected by two things on the same thread.<p>Finally, the most reasonable trick, is making sure your tickets&#x2F;buffers&#x2F;mailboxes&#x2F;whatever are _not_ sharing the same cache lines (unless they contain data which is passed at the same time). Once again, the CPU keeps things in sync at cache line levels. So having two pieces of data being hammered by two cores on the same cache line is asking for hundreds of cycles per trivial data access. This can be observed in an extreme case with many core systems, with multiple sockets, fighting over locks. I&#x27;ve done this on my 96C&#x2F;192T system and I&#x27;ve been able to get single `lock inc [mem]` instructions to take over 15,000 cycles to complete. Which is unreal for a single instruction. But that&#x27;s what happens when there&#x27;s 200-500 cycles of overhead every single time that cache line is &quot;stolen&quot; back from other cores. So, effectively, keep in your head which state cache lines will be in. If they&#x27;re going to be modified on one core, make sure they&#x27;re not going to be read on another core while still being written. These transitions are expensive, you&#x27;re only going to get your 3-4 cycle &quot;random L1 data hit performance&quot; if the cache line is being read, and it&#x27;s in the exclusive, modified, or shared state, and if it&#x27;s being written, it has to be exclusive or modified. Anything else and you&#x27;re probably paying hundreds of cycles for the access, and thus, also probably hurting the other side.<p>Ultimately, what you&#x27;re asking from the CPU is actually extremely complex. Think about how hard it would be for you to manage keeping a copy of a database in sync between hundreds of writers and reads (cores). The CPU is doing this automatically for you under the hood, on every single memory access. It is _not_ free. Thus, you really have to engineer around this problem, batch your operations, find a design that doesn&#x27;t require as intense of IPC, etc. On more weakly ordered systems you can use some more tricks in page tables to get a bit more control over how cache coherency should be applied for various chunks of memory to get more explicit control.</div><br/><div id="40725917" class="c"><input type="checkbox" id="c-40725917" checked=""/><div class="controls bullet"><span class="by">gamozolabs</span><span>|</span><a href="#40724088">root</a><span>|</span><a href="#40724345">parent</a><span>|</span><a href="#40724968">next</a><span>|</span><label class="collapse" for="c-40725917">[-]</label><label class="expand" for="c-40725917">[1 more]</label></div><br/><div class="children"><div class="content">Oh sliding in here late, but it’s also extremely important to pin your threads to specific cores. I kinda only think about computers in this mode so I didn’t bring it up, but the kernel will effectively randomly schedule your process to different physical cores. If you are doing intense IPC or otherwise relying on specific cache usage, getting assigned to a different core is a massive loss of existing state which takes time to recover from!</div><br/></div></div><div id="40724968" class="c"><input type="checkbox" id="c-40724968" checked=""/><div class="controls bullet"><span class="by">troad</span><span>|</span><a href="#40724088">root</a><span>|</span><a href="#40724345">parent</a><span>|</span><a href="#40725917">prev</a><span>|</span><a href="#40724926">next</a><span>|</span><label class="collapse" for="c-40724968">[-]</label><label class="expand" for="c-40724968">[3 more]</label></div><br/><div class="children"><div class="content">&gt; You can actually map out the physical processor layout, including on a single die, based on the latency between these cores. It&#x27;s quite subtle and requires low noise, but it&#x27;s really cool to map out the grid of cores on the actual silicon due to timing.<p>This is a very cool comment in general, but I&#x27;m intrigued by this bit in particular. I&#x27;d love to see an implementation, if anyone knows of any.</div><br/><div id="40726046" class="c"><input type="checkbox" id="c-40726046" checked=""/><div class="controls bullet"><span class="by">gamozolabs</span><span>|</span><a href="#40724088">root</a><span>|</span><a href="#40724968">parent</a><span>|</span><a href="#40725088">next</a><span>|</span><label class="collapse" for="c-40726046">[-]</label><label class="expand" for="c-40726046">[1 more]</label></div><br/><div class="children"><div class="content">There’s one big public work by utexas that covers this in a few places! <a href="https:&#x2F;&#x2F;sites.utexas.edu&#x2F;jdm4372&#x2F;2021&#x2F;05&#x2F;27&#x2F;locations-of-cores-and-l3-slices-on-xeon-scalable-processors&#x2F;" rel="nofollow">https:&#x2F;&#x2F;sites.utexas.edu&#x2F;jdm4372&#x2F;2021&#x2F;05&#x2F;27&#x2F;locations-of-cor...</a><p>See additional references for PDFs.<p>I’ve also played around with this in my research kernel, but never thoroughly enough to write up. Something I should revisit, I think it’d be a really fun thing to discuss and work on. Doing this level of timing requires doing BIOS configuration to make the CPU as deterministic as possible (turn off dynamic throttling, make sure you’re thermally keeping the CPU stable, etc).<p>I always highly encourage people to write advanced benchmarks. It’s a great way to learn how computers work!</div><br/></div></div><div id="40725088" class="c"><input type="checkbox" id="c-40725088" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#40724088">root</a><span>|</span><a href="#40724968">parent</a><span>|</span><a href="#40726046">prev</a><span>|</span><a href="#40724926">next</a><span>|</span><label class="collapse" for="c-40725088">[-]</label><label class="expand" for="c-40725088">[1 more]</label></div><br/><div class="children"><div class="content">For example, <a href="https:&#x2F;&#x2F;jprahman.substack.com&#x2F;p&#x2F;sapphire-rapids-core-to-core-latency#%C2%A7measurements-initial-observations" rel="nofollow">https:&#x2F;&#x2F;jprahman.substack.com&#x2F;p&#x2F;sapphire-rapids-core-to-core...</a></div><br/></div></div></div></div><div id="40724926" class="c"><input type="checkbox" id="c-40724926" checked=""/><div class="controls bullet"><span class="by">dwattttt</span><span>|</span><a href="#40724088">root</a><span>|</span><a href="#40724345">parent</a><span>|</span><a href="#40724968">prev</a><span>|</span><a href="#40723935">next</a><span>|</span><label class="collapse" for="c-40724926">[-]</label><label class="expand" for="c-40724926">[2 more]</label></div><br/><div class="children"><div class="content">You mention the most reasonable trick is to just avoid hammering&#x2F;read-write the same cache lines; I guess you didn&#x27;t hit this as a need because your QEMU instruction pipe was fast enough, but would you batch up your events along cache lines, fill up a line, and signal it&#x27;s free to the consumers instead?</div><br/><div id="40725910" class="c"><input type="checkbox" id="c-40725910" checked=""/><div class="controls bullet"><span class="by">gamozolabs</span><span>|</span><a href="#40724088">root</a><span>|</span><a href="#40724926">parent</a><span>|</span><a href="#40723935">next</a><span>|</span><label class="collapse" for="c-40725910">[-]</label><label class="expand" for="c-40725910">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. So I forget the original tuning I did for that project. But, I fill up a buffer which is on its own cache line (Chunk) and then signal that the chunk is ready for ownership on the other side, thus sending it. I’m not sure why the signaling atomics aren’t on their own cache lines, I imagine I tried both and this was faster? There’s also a chance I never tried it because I felt I didn’t need it? I’m not sure!</div><br/></div></div></div></div></div></div></div></div><div id="40723935" class="c"><input type="checkbox" id="c-40723935" checked=""/><div class="controls bullet"><span class="by">Subsentient</span><span>|</span><a href="#40724088">prev</a><span>|</span><a href="#40723725">next</a><span>|</span><label class="collapse" for="c-40723935">[-]</label><label class="expand" for="c-40723935">[7 more]</label></div><br/><div class="children"><div class="content">Write your own. Wrote one fairly recently in good-ole-sepples that uses UNIX domain sockets and pushes around half a gigabyte a second on weak ARM hardware. Domain sockets are great. Shared memory is even better. If you need a portable solution between processes, not threads, I recommend domain sockets and a simple binary protocol for high throughput.</div><br/><div id="40724103" class="c"><input type="checkbox" id="c-40724103" checked=""/><div class="controls bullet"><span class="by">infamouscow</span><span>|</span><a href="#40723935">parent</a><span>|</span><a href="#40723956">next</a><span>|</span><label class="collapse" for="c-40724103">[-]</label><label class="expand" for="c-40724103">[2 more]</label></div><br/><div class="children"><div class="content">I wholeheartedly agree. To add a bit more carrot for people on the fence, unlike TCP&#x2F;UDP sockets, you don&#x27;t have to be concerned about endianness because the bits never leave the machine. For a lot of use-cases, this is a big win that doesn&#x27;t get as much appreciation as I think it should.</div><br/><div id="40725120" class="c"><input type="checkbox" id="c-40725120" checked=""/><div class="controls bullet"><span class="by">01HNNWZ0MV43FF</span><span>|</span><a href="#40723935">root</a><span>|</span><a href="#40724103">parent</a><span>|</span><a href="#40723956">next</a><span>|</span><label class="collapse" for="c-40725120">[-]</label><label class="expand" for="c-40725120">[1 more]</label></div><br/><div class="children"><div class="content">I just use little endian everywhere. I figure unit tests will catch it if I screw up</div><br/></div></div></div></div><div id="40723956" class="c"><input type="checkbox" id="c-40723956" checked=""/><div class="controls bullet"><span class="by">parhamn</span><span>|</span><a href="#40723935">parent</a><span>|</span><a href="#40724103">prev</a><span>|</span><a href="#40723725">next</a><span>|</span><label class="collapse" for="c-40723956">[-]</label><label class="expand" for="c-40723956">[4 more]</label></div><br/><div class="children"><div class="content">A named pipe (fifo) would be an interest to test too. I believe theyre a good bit faster than a unix socket.</div><br/><div id="40724128" class="c"><input type="checkbox" id="c-40724128" checked=""/><div class="controls bullet"><span class="by">i80and</span><span>|</span><a href="#40723935">root</a><span>|</span><a href="#40723956">parent</a><span>|</span><a href="#40724151">next</a><span>|</span><label class="collapse" for="c-40724128">[-]</label><label class="expand" for="c-40724128">[1 more]</label></div><br/><div class="children"><div class="content">My recollection from Advanced Programming in the Unix Environment is that unix sockets are actually often faster than named pipes:<p>You got me curious; according to a quick search, that seems to hold up according to this more recent post (which doesn&#x27;t note the OS, annoyingly):<p><a href="https:&#x2F;&#x2F;www.yanxurui.cc&#x2F;posts&#x2F;server&#x2F;2023-11-28-benchmark-tcp-uds-namedpipe&#x2F;#results" rel="nofollow">https:&#x2F;&#x2F;www.yanxurui.cc&#x2F;posts&#x2F;server&#x2F;2023-11-28-benchmark-tc...</a><p>I&#x27;d be interested in more robust data here.</div><br/></div></div><div id="40724151" class="c"><input type="checkbox" id="c-40724151" checked=""/><div class="controls bullet"><span class="by">novafacing</span><span>|</span><a href="#40723935">root</a><span>|</span><a href="#40723956">parent</a><span>|</span><a href="#40724128">prev</a><span>|</span><a href="#40725166">next</a><span>|</span><label class="collapse" for="c-40724151">[-]</label><label class="expand" for="c-40724151">[1 more]</label></div><br/><div class="children"><div class="content">IIRC for most packets, UDS are actually faster (not by a massive amount) than named pipes: <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;1235958&#x2F;ipc-performance-named-pipe-vs-socket" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;1235958&#x2F;ipc-performance-...</a> and this advantage increases with packet size (from my own testing).</div><br/></div></div><div id="40725166" class="c"><input type="checkbox" id="c-40725166" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#40723935">root</a><span>|</span><a href="#40723956">parent</a><span>|</span><a href="#40724151">prev</a><span>|</span><a href="#40723725">next</a><span>|</span><label class="collapse" for="c-40725166">[-]</label><label class="expand" for="c-40725166">[1 more]</label></div><br/><div class="children"><div class="content">Have only seen the other way but shmem with a spinloop should be faster.</div><br/></div></div></div></div></div></div><div id="40723725" class="c"><input type="checkbox" id="c-40723725" checked=""/><div class="controls bullet"><span class="by">lukeh</span><span>|</span><a href="#40723935">prev</a><span>|</span><a href="#40723470">next</a><span>|</span><label class="collapse" for="c-40723725">[-]</label><label class="expand" for="c-40723725">[1 more]</label></div><br/><div class="children"><div class="content">Domain sockets aren&#x27;t really Linux-specific, even Windows partially supports them now. [1]<p>[1] <a href="https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;commandline&#x2F;af_unix-comes-to-windows&#x2F;" rel="nofollow">https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;commandline&#x2F;af_unix-comes-to-...</a></div><br/></div></div><div id="40723470" class="c"><input type="checkbox" id="c-40723470" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#40723725">prev</a><span>|</span><a href="#40723140">next</a><span>|</span><label class="collapse" for="c-40723470">[-]</label><label class="expand" for="c-40723470">[3 more]</label></div><br/><div class="children"><div class="content">IPC having a lot of &quot;overhead&quot; is nonsense (other than, like, managing a mutex or semaphore). I&#x27;ve used many header-only IPC libraries (this one[1] worked great on Windows&#x2F;MacOS). It&#x27;s way easier to get a hang of instead of jerry-rigging TCP or UDP to do IPC (yuck).<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;jarikomppa&#x2F;ipc">https:&#x2F;&#x2F;github.com&#x2F;jarikomppa&#x2F;ipc</a></div><br/><div id="40724047" class="c"><input type="checkbox" id="c-40724047" checked=""/><div class="controls bullet"><span class="by">DawsonBruce</span><span>|</span><a href="#40723470">parent</a><span>|</span><a href="#40723140">next</a><span>|</span><label class="collapse" for="c-40724047">[-]</label><label class="expand" for="c-40724047">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing this. I recently built something similar for a closed source project and am curious how I didn’t run across this library in my research.<p>Briefly: my implementation uses a templated RAII-style wrapper that persists objects in shared memory for as long as an instance of that object remains in scope. Once the last object goes out of scope, shared memory is freed. It works great so far. I’ll probably reference this library to compare implementations and see what I can learn. Thanks again.</div><br/><div id="40724090" class="c"><input type="checkbox" id="c-40724090" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#40723470">root</a><span>|</span><a href="#40724047">parent</a><span>|</span><a href="#40723140">next</a><span>|</span><label class="collapse" for="c-40724090">[-]</label><label class="expand" for="c-40724090">[1 more]</label></div><br/><div class="children"><div class="content">If you need something more powerful, Boost.Interprocess is also a thing[1]. I used it like 10 years ago and seemed to work well. Looking at the docs, it even has a RAII-ish thing called `boost::interprocess::remove_shared_memory_on_destroy`.<p>[1] <a href="https:&#x2F;&#x2F;www.boost.org&#x2F;doc&#x2F;libs&#x2F;1_85_0&#x2F;doc&#x2F;html&#x2F;interprocess.html" rel="nofollow">https:&#x2F;&#x2F;www.boost.org&#x2F;doc&#x2F;libs&#x2F;1_85_0&#x2F;doc&#x2F;html&#x2F;interprocess....</a></div><br/></div></div></div></div></div></div><div id="40723140" class="c"><input type="checkbox" id="c-40723140" checked=""/><div class="controls bullet"><span class="by">int08h</span><span>|</span><a href="#40723470">prev</a><span>|</span><a href="#40724444">next</a><span>|</span><label class="collapse" for="c-40723140">[-]</label><label class="expand" for="c-40723140">[2 more]</label></div><br/><div class="children"><div class="content">Sit in a Compare-and-Swap loop on `ping` and `pong` as uint32s. Go even faster in the shared memory case. Zoom zoom.</div><br/><div id="40723812" class="c"><input type="checkbox" id="c-40723812" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#40723140">parent</a><span>|</span><a href="#40724444">next</a><span>|</span><label class="collapse" for="c-40723812">[-]</label><label class="expand" for="c-40723812">[1 more]</label></div><br/><div class="children"><div class="content">At least try to be polite to the cache coherence system, please. Do a loop where you check (with a plain relaxed (when available) read) whether the compare and swap should work and, if not, do your platform’s pause operation (REP NOP on x86) and try again. Only do compare-and-swap if the optimistic read thinks it will work.</div><br/></div></div></div></div><div id="40724444" class="c"><input type="checkbox" id="c-40724444" checked=""/><div class="controls bullet"><span class="by">bfrog</span><span>|</span><a href="#40723140">prev</a><span>|</span><a href="#40723440">next</a><span>|</span><label class="collapse" for="c-40724444">[-]</label><label class="expand" for="c-40724444">[5 more]</label></div><br/><div class="children"><div class="content">iceoryx2 uses shared memory and is implemented in rust if anyone wants something off the shelf</div><br/><div id="40724746" class="c"><input type="checkbox" id="c-40724746" checked=""/><div class="controls bullet"><span class="by">Sytten</span><span>|</span><a href="#40724444">parent</a><span>|</span><a href="#40725170">next</a><span>|</span><label class="collapse" for="c-40724746">[-]</label><label class="expand" for="c-40724746">[3 more]</label></div><br/><div class="children"><div class="content">It looks nice but the lack of async is sadly a non starter for a lot of us.</div><br/><div id="40725134" class="c"><input type="checkbox" id="c-40725134" checked=""/><div class="controls bullet"><span class="by">mutatio</span><span>|</span><a href="#40724444">root</a><span>|</span><a href="#40724746">parent</a><span>|</span><a href="#40725170">next</a><span>|</span><label class="collapse" for="c-40725134">[-]</label><label class="expand" for="c-40725134">[2 more]</label></div><br/><div class="children"><div class="content">Is shared memory access naturally async like IO&#x2F;io_uring? If not, asking for async is misplaced and any implementation would be synonymous with wrapping sync calls in `async fn` wrappers.</div><br/><div id="40725506" class="c"><input type="checkbox" id="c-40725506" checked=""/><div class="controls bullet"><span class="by">dwattttt</span><span>|</span><a href="#40724444">root</a><span>|</span><a href="#40725134">parent</a><span>|</span><a href="#40725170">next</a><span>|</span><label class="collapse" for="c-40725506">[-]</label><label class="expand" for="c-40725506">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s as naturally async as anything else here actually. If you&#x27;re calling a function via IPC over shared memory, you&#x27;re sending a message, signalling it&#x27;s there to the other process, and waiting for a response message with the result.<p>The only thing needed to turn this async is to change the &quot;get result over IPC&quot; function from &quot;block until result is available&quot; to &quot;check if result is available, return TRY_AGAIN if it&#x27;s not&quot;</div><br/></div></div></div></div></div></div><div id="40725170" class="c"><input type="checkbox" id="c-40725170" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#40724444">parent</a><span>|</span><a href="#40724746">prev</a><span>|</span><a href="#40723440">next</a><span>|</span><label class="collapse" for="c-40725170">[-]</label><label class="expand" for="c-40725170">[1 more]</label></div><br/><div class="children"><div class="content">Thank you. Good tip.</div><br/></div></div></div></div><div id="40723440" class="c"><input type="checkbox" id="c-40723440" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#40724444">prev</a><span>|</span><label class="collapse" for="c-40723440">[-]</label><label class="expand" for="c-40723440">[1 more]</label></div><br/><div class="children"><div class="content">See also iceoryx2: <a href="https:&#x2F;&#x2F;github.com&#x2F;eclipse-iceoryx&#x2F;iceoryx2">https:&#x2F;&#x2F;github.com&#x2F;eclipse-iceoryx&#x2F;iceoryx2</a></div><br/></div></div></div></div></div></div></div></body></html>