<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1701075668270" as="style"/><link rel="stylesheet" href="styles.css?v=1701075668270"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/Frost-group/The-Oracle-of-Zotero">Oracle of Zotero: LLM QA of Your Research Library</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>SubiculumCode</span> | <span>21 comments</span></div><br/><div><div id="38425536" class="c"><input type="checkbox" id="c-38425536" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#38429047">next</a><span>|</span><label class="collapse" for="c-38425536">[-]</label><label class="expand" for="c-38425536">[3 more]</label></div><br/><div class="children"><div class="content">Nice project!<p>I&#x27;ve spent quite a lot of time in the medical&#x2F;scientific literature space. With regards to LLMs, specifically RAG, how the data is chunked is quite important. With that, I have a couple projects that might be beneficial additions.<p>paperetl (<a href="https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;paperetl">https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;paperetl</a>) - supports parsing arXiv, PubMed and integrates with GROBID to handle parsing metadata and text from arbitrary papers.<p>paperai (<a href="https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;paperai">https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;paperai</a>) - builds embeddings databases of medical&#x2F;scientific papers. Supports LLM prompting, semantic workflows and vector search. Built with txtai (<a href="https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;txtai">https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;txtai</a>).<p>While arbitrary chunking&#x2F;splitting can work, I&#x27;ve found that integrating parsing that has knowledge of medical&#x2F;scientific paper structure increases the overall accuracy and experience of downstream applications.</div><br/><div id="38425716" class="c"><input type="checkbox" id="c-38425716" checked=""/><div class="controls bullet"><span class="by">panabee</span><span>|</span><a href="#38425536">parent</a><span>|</span><a href="#38429047">next</a><span>|</span><label class="collapse" for="c-38425716">[-]</label><label class="expand" for="c-38425716">[2 more]</label></div><br/><div class="children"><div class="content">these are awesome projects. thanks for sharing.<p>it would accelerate research so much if LLM accuracy increased on biomedical papers.<p>very much agreed on the potential to extract signal from paper structures.<p>two questions if you don&#x27;t mind:<p>1. did you post a summary of your chunking analysis somewhere? i&#x27;m curious which method maximized accuracy, and which sentence-overlap methods were most effective.<p>2. do you think general tokenization methods limit LLMs on scientific&#x2F;biomedical papers?</div><br/><div id="38425874" class="c"><input type="checkbox" id="c-38425874" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#38425536">root</a><span>|</span><a href="#38425716">parent</a><span>|</span><a href="#38429047">next</a><span>|</span><label class="collapse" for="c-38425874">[-]</label><label class="expand" for="c-38425874">[1 more]</label></div><br/><div class="children"><div class="content">Appreciate it!<p>&gt; 1. did you post a summary of your chunking analysis somewhere? i&#x27;m curious which method maximized accuracy, and which sentence-overlap methods were most effective.<p>Good idea on this but nothing posted. In general, grouping by sections of a paper has worked best (i.e. methods, conclusions, results etc). GROBID is helpful with arbitrary papers.<p>&gt; 2. do you think general tokenization methods limit LLMs on scientific&#x2F;biomedical papers?<p>Possibly. For vectorization, specifically with medical, I do have this model (<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;NeuML&#x2F;pubmedbert-base-embeddings" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;NeuML&#x2F;pubmedbert-base-embeddings</a>) which is a fine-tuned sentence embeddings model using this base model (<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;BiomedNLP-BiomedBERT-base-u...</a>). The base model does have a custom vocabulary.<p>In terms of LLMs, I&#x27;ve found that this model (<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Open-Orca&#x2F;Mistral-7B-OpenOrca" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Open-Orca&#x2F;Mistral-7B-OpenOrca</a>) works well but haven&#x27;t experimented with domain specific LLMs.</div><br/></div></div></div></div></div></div><div id="38429047" class="c"><input type="checkbox" id="c-38429047" checked=""/><div class="controls bullet"><span class="by">Loic</span><span>|</span><a href="#38425536">prev</a><span>|</span><a href="#38426260">next</a><span>|</span><label class="collapse" for="c-38429047">[-]</label><label class="expand" for="c-38429047">[2 more]</label></div><br/><div class="children"><div class="content">Maybe a stupid question, but how are equations handled in the parsing of a paper? Are local runable LLM capable of proposing model equations like programming code? I have seen that GPT4 can, so just wondering if equations are &quot;treated&quot; like normal computer code. My Zotero papers are equations heavy.</div><br/><div id="38429366" class="c"><input type="checkbox" id="c-38429366" checked=""/><div class="controls bullet"><span class="by">Clueed</span><span>|</span><a href="#38429047">parent</a><span>|</span><a href="#38426260">next</a><span>|</span><label class="collapse" for="c-38429366">[-]</label><label class="expand" for="c-38429366">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve looked into the available options of parsing PDFs, including pypdf, which is what is being used here, a while ago and it&#x27;s not good. While I haven&#x27;t testing equations specifically, it think it&#x27;s fair so assume that the results will be subpar especially complex ones.<p>I guess, this could be an application of the agent model. I&#x27;ve seen multiple LLMs recently trained specifically on LateX parsing. One model would recognize from the parsed PDF garbage that there is probably an equation there and call a different want to parse it.</div><br/></div></div></div></div><div id="38426260" class="c"><input type="checkbox" id="c-38426260" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#38429047">prev</a><span>|</span><a href="#38427847">next</a><span>|</span><label class="collapse" for="c-38426260">[-]</label><label class="expand" for="c-38426260">[4 more]</label></div><br/><div class="children"><div class="content">The problem is that this is still just retrieval and mechanical. In RAG, you split a PDF into small chunks, but this is way different from how humans digest PDFs. If I hire an RA to go through my Zotero lib and make a mind-map of sorts, he&#x2F;she would combine papers, paragraphs, figures, etc. to come up with a &quot;concepts&quot; map, which is way richer than a retrieval system that merely finds the semantic similarity between my query and pieces of text.<p>RAG is good for semantic search, but really we need something that works at a knowledge&#x2F;understanding level as opposed to data&#x2F;information level.</div><br/><div id="38426626" class="c"><input type="checkbox" id="c-38426626" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#38426260">parent</a><span>|</span><a href="#38427302">next</a><span>|</span><label class="collapse" for="c-38426626">[-]</label><label class="expand" for="c-38426626">[2 more]</label></div><br/><div class="children"><div class="content">I think what you&#x27;re looking for is possible with LLM agents. For paperai (mentioned previously) at least, it&#x27;s possible to build workflows that connect multiple prompt steps together.<p>txtai (included with paperai) has the ability to build semantic graphs (<a href="https:&#x2F;&#x2F;neuml.hashnode.dev&#x2F;introducing-the-semantic-graph" rel="nofollow noreferrer">https:&#x2F;&#x2F;neuml.hashnode.dev&#x2F;introducing-the-semantic-graph</a>).<p>I agree that RAG is just one part of the equation. But the tools are available if one wanted to build their own complex multi-agent workflow.</div><br/><div id="38426755" class="c"><input type="checkbox" id="c-38426755" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38426260">root</a><span>|</span><a href="#38426626">parent</a><span>|</span><a href="#38427302">next</a><span>|</span><label class="collapse" for="c-38426755">[-]</label><label class="expand" for="c-38426755">[1 more]</label></div><br/><div class="children"><div class="content">In the example of the RA being automated by an LLM workflow by agents, I agree it’s very possible and it requires defining a set of specific agents, using prompts and allow function calling for tools, and then defining a full workflow between the agents. The workflow can likely be modeled by breaking down the individual steps the RA takes when doing their work.<p>The agents are likely very narrow and specific, they do one very very specific task. Then the workflow is a DAG chaining their work together.</div><br/></div></div></div></div><div id="38427302" class="c"><input type="checkbox" id="c-38427302" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#38426260">parent</a><span>|</span><a href="#38426626">prev</a><span>|</span><a href="#38427847">next</a><span>|</span><label class="collapse" for="c-38427302">[-]</label><label class="expand" for="c-38427302">[1 more]</label></div><br/><div class="children"><div class="content">There are lots of experiments around generating knowledge graph mutations and queries to build this kind of relational knowledge.<p>In neo4j, for example, relations tend to have natural language names.
(The cat BELONGS_TO the human)<p>So LLMs appear to be apt at making those queries</div><br/></div></div></div></div><div id="38427847" class="c"><input type="checkbox" id="c-38427847" checked=""/><div class="controls bullet"><span class="by">dbcooper</span><span>|</span><a href="#38426260">prev</a><span>|</span><a href="#38426800">next</a><span>|</span><label class="collapse" for="c-38427847">[-]</label><label class="expand" for="c-38427847">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing various projects. Any tools for materials science that can create summary tables of things like material, application, performance would be really valuable.</div><br/></div></div><div id="38426800" class="c"><input type="checkbox" id="c-38426800" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38427847">prev</a><span>|</span><a href="#38426060">next</a><span>|</span><label class="collapse" for="c-38426800">[-]</label><label class="expand" for="c-38426800">[6 more]</label></div><br/><div class="children"><div class="content">This is built on Langchain and I think it’s also possible to build this on top of Haystack now. I’m torn between the two and I’m wondering if this project provides a good example of why Langchain can be a better fit in certain situations, just not sure what those are exactly.</div><br/><div id="38426840" class="c"><input type="checkbox" id="c-38426840" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#38426800">parent</a><span>|</span><a href="#38426060">next</a><span>|</span><label class="collapse" for="c-38426840">[-]</label><label class="expand" for="c-38426840">[5 more]</label></div><br/><div class="children"><div class="content">There are a lot of great options. This paper gives a comprehensive overview on the state of prompting frameworks: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12785" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12785</a></div><br/><div id="38426954" class="c"><input type="checkbox" id="c-38426954" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#38426800">root</a><span>|</span><a href="#38426840">parent</a><span>|</span><a href="#38427800">next</a><span>|</span><label class="collapse" for="c-38426954">[-]</label><label class="expand" for="c-38426954">[3 more]</label></div><br/><div class="children"><div class="content">Oh no I&#x27;m just realizing that arxiv will be increasingly spammed with what should have been a blog post. I hope I&#x27;m wrong in assuming that in a few years the level of credibility that comes with a paper being on arxiv will have entirely worn off.<p>I know that in theory arxiv, being a pre-print server, shoulnd&#x27;t give any credibility but practically that is the case and it still is a good quality&#x2F;bs filter compared to e.g. Medium articles.</div><br/><div id="38429083" class="c"><input type="checkbox" id="c-38429083" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#38426800">root</a><span>|</span><a href="#38426954">parent</a><span>|</span><a href="#38427994">next</a><span>|</span><label class="collapse" for="c-38429083">[-]</label><label class="expand" for="c-38429083">[1 more]</label></div><br/><div class="children"><div class="content">In my field, ArXiv has about the same level of credibility as Wikipedia or random journal articles from the International Journal of Sciency Science, i.e. trust, but verify. Among non-peer-reviewed documents, they rank below things like DoE or NASA reports and tend to not be cited.<p>There are preprints of articles since then published (which have the same credibility as the peer-reviewed article), articles form mates (which are obviously great), and the rest, which might be interesting but not a solid source on its own.<p>It seems to be working as intended, to be fair. ArXiv has precious little ways of improving the accuracy of the preprints.</div><br/></div></div><div id="38427994" class="c"><input type="checkbox" id="c-38427994" checked=""/><div class="controls bullet"><span class="by">qumpis</span><span>|</span><a href="#38426800">root</a><span>|</span><a href="#38426954">parent</a><span>|</span><a href="#38429083">prev</a><span>|</span><a href="#38427800">next</a><span>|</span><label class="collapse" for="c-38427994">[-]</label><label class="expand" for="c-38427994">[1 more]</label></div><br/><div class="children"><div class="content">From the glance of it, the paper looks very polished. Combine this with the fact that arxiv is invite-only, your prediction might not come about</div><br/></div></div></div></div><div id="38427800" class="c"><input type="checkbox" id="c-38427800" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38426800">root</a><span>|</span><a href="#38426840">parent</a><span>|</span><a href="#38426954">prev</a><span>|</span><a href="#38426060">next</a><span>|</span><label class="collapse" for="c-38427800">[-]</label><label class="expand" for="c-38427800">[1 more]</label></div><br/><div class="children"><div class="content">The [1] git repo referenced in the paper is oddly very … basically empty? weird<p><a href="https:&#x2F;&#x2F;github.com&#x2F;lxx0628&#x2F;Prompting-Framework-Survey">https:&#x2F;&#x2F;github.com&#x2F;lxx0628&#x2F;Prompting-Framework-Survey</a></div><br/></div></div></div></div></div></div><div id="38426060" class="c"><input type="checkbox" id="c-38426060" checked=""/><div class="controls bullet"><span class="by">fl0id</span><span>|</span><a href="#38426800">prev</a><span>|</span><label class="collapse" for="c-38426060">[-]</label><label class="expand" for="c-38426060">[4 more]</label></div><br/><div class="children"><div class="content">mmh, I was kind of hoping for something more finished ^^</div><br/><div id="38426220" class="c"><input type="checkbox" id="c-38426220" checked=""/><div class="controls bullet"><span class="by">syntaxers</span><span>|</span><a href="#38426060">parent</a><span>|</span><a href="#38427289">next</a><span>|</span><label class="collapse" for="c-38426220">[-]</label><label class="expand" for="c-38426220">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a tool I use called Petal <a href="https:&#x2F;&#x2F;www.petal.org&#x2F;reference-manager" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.petal.org&#x2F;reference-manager</a>. The free tier allows up to 1GB of PDFs, which I believe are processed by GROBID and chunked for LLM QA.<p>The feature I find most useful is the table automation which I use for literature review, since it lets me run the same QA prompts on a collection of documents all at once.</div><br/></div></div><div id="38427289" class="c"><input type="checkbox" id="c-38427289" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#38426060">parent</a><span>|</span><a href="#38426220">prev</a><span>|</span><label class="collapse" for="c-38427289">[-]</label><label class="expand" for="c-38427289">[2 more]</label></div><br/><div class="children"><div class="content">I was too but I posted anyway. I&#x27;d like it built into a legitimate plugin inside the zotero app.</div><br/><div id="38429092" class="c"><input type="checkbox" id="c-38429092" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#38426060">root</a><span>|</span><a href="#38427289">parent</a><span>|</span><label class="collapse" for="c-38429092">[-]</label><label class="expand" for="c-38429092">[1 more]</label></div><br/><div class="children"><div class="content">That would be fantastic. At the moment the barrier to entry to use this kind of models is quite high. Something that could be used from the GUI would be great.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>