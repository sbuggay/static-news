<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716109254797" as="style"/><link rel="stylesheet" href="styles.css?v=1716109254797"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://twitter.com/gdb/status/1791869138132218351">Sam and Greg&#x27;s response to OpenAI Safety researcher claims</a> <span class="domain">(<a href="https://twitter.com">twitter.com</a>)</span></div><div class="subtext"><span>amrrs</span> | <span>358 comments</span></div><br/><div><div id="40403302" class="c"><input type="checkbox" id="c-40403302" checked=""/><div class="controls bullet"><span class="by">corry</span><span>|</span><a href="#40404356">next</a><span>|</span><label class="collapse" for="c-40403302">[-]</label><label class="expand" for="c-40403302">[109 more]</label></div><br/><div class="children"><div class="content">In any sufficiently large tech company, the &quot;risk mitigation&quot; leadership (legal, procurement, IT, etc) have to operate in a kind of Overton window that balance the risks they are hired to protect the corp from vs. the need or desire of the senior leadership to play fast and loose when they want or feel they need to.<p>Either the risk-mitigator &#x27;falls in line&#x27; after repeatedly seeing their increasingly strident exhortations are falling on deaf ears (or even outright contradicted)... or they leave because it violates their sense of ethics.<p>Perhaps &quot;AGI&quot; and potential extinction event level fears is giving this more drama than it should have. Replace AGI with &quot;no BYOD policy&quot; and I bet there&#x27;s a startup somewhere where it turned out that safety guy was super intent on the policy, senior leadership wasn&#x27;t, and eventually safety guy quits.<p>Or it could all be as serious and dire as it seems. Hard to tell from the outside.</div><br/><div id="40403493" class="c"><input type="checkbox" id="c-40403493" checked=""/><div class="controls bullet"><span class="by">tummler</span><span>|</span><a href="#40403302">parent</a><span>|</span><a href="#40403751">next</a><span>|</span><label class="collapse" for="c-40403493">[-]</label><label class="expand" for="c-40403493">[73 more]</label></div><br/><div class="children"><div class="content">The difference here is that these aren&#x27;t standard-issue HR&#x2F;legal issues. The technology they&#x27;re working on poses the gravest of dangers. This is uncharted territory, not just for the tech sector, but period.<p>Whether the recent public back-and-forth is just internal drama&#x2F;politics spilling out, or there really is a lack of gravitas around the handling of these issues within the company-- neither is good.<p>Others have said in comments below, and I agree: this organization is increasingly looking like a circus, and arguably worse, one that has taken its eye off the ball. It&#x27;s extremely disconcerting that this is the group of people &quot;leading&quot; the commercialization of this technology (and generally setting the tone for the industry). Particularly when it seems many of the key players who joined on principled&#x2F;moral grounds are dropping like flies.<p>I guess we&#x27;re on the fast track to finding out how well putting all of this money, power, responsibility, and faith into SV works out for us.</div><br/><div id="40403732" class="c"><input type="checkbox" id="c-40403732" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403493">parent</a><span>|</span><a href="#40404698">next</a><span>|</span><label class="collapse" for="c-40403732">[-]</label><label class="expand" for="c-40403732">[63 more]</label></div><br/><div class="children"><div class="content">Text continuation poses &quot;the <i>greatest</i> of dangers&quot;?</div><br/><div id="40405058" class="c"><input type="checkbox" id="c-40405058" checked=""/><div class="controls bullet"><span class="by">wk_end</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40404022">next</a><span>|</span><label class="collapse" for="c-40405058">[-]</label><label class="expand" for="c-40405058">[8 more]</label></div><br/><div class="children"><div class="content">I’m a bit of an AI skeptic and find the way people talk vaguely about the “dangers” of AI weird and maybe just more hype (or efforts for regulatory capture) rather than actual cause for concern.<p>Still, to me, it seems like the two big scary things that might fall out of recent developments in generative AI are:<p>1. It might vacuum up an enormous number of jobs very quickly; enough jobs and quickly enough to cause serious social unrest.<p>2. The ability to generate convincing fake videos of basically anything might catastrophically erode our sense of truth or reality.<p>Both of these strike me as plausible concerns in the near future (arguably they’re already happening). Not really sure what we could do about them, though - the cat is kind of out of the bag.</div><br/><div id="40405369" class="c"><input type="checkbox" id="c-40405369" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40405058">parent</a><span>|</span><a href="#40405197">next</a><span>|</span><label class="collapse" for="c-40405369">[-]</label><label class="expand" for="c-40405369">[1 more]</label></div><br/><div class="children"><div class="content">Agree with this.<p>And generally &quot;AI Safety&quot; researchers <i>aren&#x27;t</i> concerned with these risks and instead are looking at existential risk.<p>The boring but real risks you outline (to which I&#x27;d add reinforcement of unknown biases) generally fall into what people call &quot;AI Ethics&quot; or sometimes &quot;Data Ethics&quot;.</div><br/></div></div><div id="40405197" class="c"><input type="checkbox" id="c-40405197" checked=""/><div class="controls bullet"><span class="by">shinycode</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40405058">parent</a><span>|</span><a href="#40405369">prev</a><span>|</span><a href="#40405151">next</a><span>|</span><label class="collapse" for="c-40405197">[-]</label><label class="expand" for="c-40405197">[2 more]</label></div><br/><div class="children"><div class="content">For the #1, when I saw the demo of GPT-4o talk to an other AI in a somewhat convincing scenario, I felt like it was the first time I could saw AI as a real threat to thousands of customer service companies. 
A majority of those jobs is scripted and the employee doesn’t have a deep enough knowledge to be helpful in any other way. I worked at places where the customer service support wrote all those processes and the teams were trained to follow the scripts. 
With AI having the ability to analyze text and talk with expressions and feelings… they’re a perfect and cheap replacement that can be scaled easily. 
If companies are not training their employees to gain deeper insights and level up their support I feel like they are in great danger</div><br/><div id="40405385" class="c"><input type="checkbox" id="c-40405385" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40405197">parent</a><span>|</span><a href="#40405151">next</a><span>|</span><label class="collapse" for="c-40405385">[-]</label><label class="expand" for="c-40405385">[1 more]</label></div><br/><div class="children"><div class="content">Klana reduced it&#x27;s workforce by 700 (out of 3000) people after implementing an AI-powered chatbot:<p>&gt; We made the announcement to say the consequence of us launching the technology is we need the equivalent of 700 fewer full-time agents than what we usually use on an average basis. On average, we need 3,000 agents, now we need a little more than 2,000.<p>&gt; It&#x27;s on par with humans in terms of satisfaction and it resulted in a 25% reduction in repeat inquiries from customers<p>&gt; We&#x27;ve stopped hiring in the last six months. We&#x27;re shrinking as a company, not by layoffs, but by natural attrition. Klarna tries to apply AI across all products and services and work we do. It&#x27;s having implications on how many people we need as a company. This is one time that a single product improvement led to a massive reduction in need for customer service agents.<p><a href="https:&#x2F;&#x2F;www.cbsnews.com&#x2F;news&#x2F;klarna-ceo-ai-chatbot-replacing-workers-sebastian-siemiatkowski&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cbsnews.com&#x2F;news&#x2F;klarna-ceo-ai-chatbot-replacing...</a></div><br/></div></div></div></div><div id="40405151" class="c"><input type="checkbox" id="c-40405151" checked=""/><div class="controls bullet"><span class="by">verisimi</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40405058">parent</a><span>|</span><a href="#40405197">prev</a><span>|</span><a href="#40404022">next</a><span>|</span><label class="collapse" for="c-40405151">[-]</label><label class="expand" for="c-40405151">[4 more]</label></div><br/><div class="children"><div class="content">Re 1 - aren&#x27;t people mostly doing bullshit jobs already?<p>Re 2 - fakery in videos and images has been a part since their inception.<p>So, don&#x27;t worry, I guess?<p>The only thing is the scale of bs that could be put out, and whether the wider population lose trust in what they see on screens.  Which would be a good thing, imo.</div><br/><div id="40405306" class="c"><input type="checkbox" id="c-40405306" checked=""/><div class="controls bullet"><span class="by">hnlmorg</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40405151">parent</a><span>|</span><a href="#40405223">next</a><span>|</span><label class="collapse" for="c-40405306">[-]</label><label class="expand" for="c-40405306">[1 more]</label></div><br/><div class="children"><div class="content">The “computers are replacing our jobs” line is very emotive and I feel both sides of the argument are looking at the problem too binary.<p>Yes some jobs are menial but they are still some people’s literal livelihoods. And yes, industries need to evolve and job preservation shouldn’t be the #1 criteria for holding back change.<p>The real question we <i>should</i> be asking ourselves isn’t either of those two former points but actually this:<p>Is the pace of change too quick, or is there any support to allow those economically affected by change to be able to retrain?<p>Removing jobs doesn’t help society because it just leads to unemployment and thus more people dependent on the state. But retraining people is absolutely a benefit. For example, if call centres replaced the first line support with AI, that could lead to shorter call waiting times. But the existing call centre staff could become 2nd line support, account managers for business accounts, install engineers, or even AI supervisors that review a subset of AI interactions to ensure customer satisfaction is maintained.</div><br/></div></div><div id="40405223" class="c"><input type="checkbox" id="c-40405223" checked=""/><div class="controls bullet"><span class="by">shinycode</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40405151">parent</a><span>|</span><a href="#40405306">prev</a><span>|</span><a href="#40404022">next</a><span>|</span><label class="collapse" for="c-40405223">[-]</label><label class="expand" for="c-40405223">[2 more]</label></div><br/><div class="children"><div class="content">Bullshit jobs yes but still they are paid. What happens in a capitalist society when thousands of people loose their job and can’t produce value and contribute to society ? It’s tearing apart the fabric of capitalism is some way. 
What happen when all those people without jobs won’t be able to train for better jobs (because they would have done it already and won’t have a bullshit job in the first place) and either fall poor or move to less paid jobs ?</div><br/><div id="40405372" class="c"><input type="checkbox" id="c-40405372" checked=""/><div class="controls bullet"><span class="by">wordpad</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40405223">parent</a><span>|</span><a href="#40404022">next</a><span>|</span><label class="collapse" for="c-40405372">[-]</label><label class="expand" for="c-40405372">[1 more]</label></div><br/><div class="children"><div class="content">Technology and automation tends to create more jobs. 
As operational costs decrease, your end service or product becomes cheaper, so all your b2b or b2c customers can afford more of it which leads to more opportunities, doing things which were cost prohibitive before.<p>Some of those things may too require manual labor.</div><br/></div></div></div></div></div></div></div></div><div id="40404022" class="c"><input type="checkbox" id="c-40404022" checked=""/><div class="controls bullet"><span class="by">conception</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40405058">prev</a><span>|</span><a href="#40403756">next</a><span>|</span><label class="collapse" for="c-40404022">[-]</label><label class="expand" for="c-40404022">[5 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;article&#x2F;2024&#x2F;may&#x2F;17&#x2F;ai-weapons-palantir-war-technology" rel="nofollow">https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;article&#x2F;2024&#x2F;may&#x2F;17&#x2F;a...</a><p>&quot;...I also went to a panel in Palantir’s booth titled Civilian Harm Mitigation. It was led by two “privacy and civil liberties engineers” – a young man and woman who spoke exclusively in monotone. They also used countless euphemisms for bombing and death. The woman described how Palantir’s Gaia map tool lets users “nominate targets of interest” for “the target nomination process”. She meant it helps people choose which places get bombed.<p>After she clicked a few options on an interactive map, a targeted landmass lit up with bright blue blobs. These blobs, she said, were civilian areas like hospitals and schools. The civilian locations could also be described in text, she said, but it can take a long time to read. So, Gaia uses a large language model (something like ChatGPT) to sift through this information and simplify it. Essentially, people choosing bomb targets get a dumbed-down version of information about where children sleep and families get medical treatment.&quot;<p>Definitely not dangerous at all.</div><br/><div id="40405358" class="c"><input type="checkbox" id="c-40405358" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404022">parent</a><span>|</span><a href="#40404270">next</a><span>|</span><label class="collapse" for="c-40405358">[-]</label><label class="expand" for="c-40405358">[1 more]</label></div><br/><div class="children"><div class="content">&gt; map tool lets users “nominate targets of interest” for “the target nomination process”. She meant it helps people choose which places get bombed.<p>So... people are choosing who to kill?<p>I&#x27;m reasonably sure there are historical precedents for this.</div><br/></div></div><div id="40404270" class="c"><input type="checkbox" id="c-40404270" checked=""/><div class="controls bullet"><span class="by">badestrand</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404022">parent</a><span>|</span><a href="#40405358">prev</a><span>|</span><a href="#40404671">next</a><span>|</span><label class="collapse" for="c-40404270">[-]</label><label class="expand" for="c-40404270">[1 more]</label></div><br/><div class="children"><div class="content">That rather sounds like Palantir is dangerous and not OpenAI.</div><br/></div></div><div id="40404671" class="c"><input type="checkbox" id="c-40404671" checked=""/><div class="controls bullet"><span class="by">mr_mitm</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404022">parent</a><span>|</span><a href="#40404270">prev</a><span>|</span><a href="#40404438">next</a><span>|</span><label class="collapse" for="c-40404671">[-]</label><label class="expand" for="c-40404671">[1 more]</label></div><br/><div class="children"><div class="content">But the claim was &quot;th gravest of dangers&quot;. While your scenario is a nightmare, I&#x27;m not sure it qualifies as &quot;the gravest&quot;.</div><br/></div></div><div id="40404438" class="c"><input type="checkbox" id="c-40404438" checked=""/><div class="controls bullet"><span class="by">Skgqie1</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404022">parent</a><span>|</span><a href="#40404671">prev</a><span>|</span><a href="#40403756">next</a><span>|</span><label class="collapse" for="c-40404438">[-]</label><label class="expand" for="c-40404438">[1 more]</label></div><br/><div class="children"><div class="content">Yes, having eyes is incredibly dangerous. It allows people to select a target from anything they can see.</div><br/></div></div></div></div><div id="40403756" class="c"><input type="checkbox" id="c-40403756" checked=""/><div class="controls bullet"><span class="by">hn_go_brrrrr</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40404022">prev</a><span>|</span><a href="#40405107">next</a><span>|</span><label class="collapse" for="c-40403756">[-]</label><label class="expand" for="c-40403756">[30 more]</label></div><br/><div class="children"><div class="content">While we&#x27;re being overly reductionist: it&#x27;s just moving bits around, what harm could that possibly do?</div><br/><div id="40403894" class="c"><input type="checkbox" id="c-40403894" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403756">parent</a><span>|</span><a href="#40403778">prev</a><span>|</span><a href="#40405107">next</a><span>|</span><label class="collapse" for="c-40403894">[-]</label><label class="expand" for="c-40403894">[27 more]</label></div><br/><div class="children"><div class="content">&quot;AI&quot; is one of two things depending on whether you think it&#x27;s actually intelligent:<p>* If &quot;AI&quot; is actually intelligent, then it&#x27;s no worse a threat than any living being known to man.<p>* If &quot;AI&quot; is not actually intelligent, then it&#x27;s no worse a threat than any other computer program.<p>Both threat models are very thoroughly known and understood. I second parent commenter&#x27;s sentiment that calling &quot;AI&quot; the &quot;gravest of dangers&quot; is a gross misrepresentation.</div><br/><div id="40404095" class="c"><input type="checkbox" id="c-40404095" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403894">parent</a><span>|</span><a href="#40404226">next</a><span>|</span><label class="collapse" for="c-40404095">[-]</label><label class="expand" for="c-40404095">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe asserting two parallel and comparably false equivalences meaningfully progresses the discussion around this technology and risk.<p>If this tech exceeds human capabilities to &#x27;think&#x27; (plan, reason, manipulate, etc) then it is obviously a bigger threat than any less capable entity (&#x27;any living being&#x27;, as you refer).  Because no humans will be able to out-plan&#x2F;reason&#x2F;manipulate it.<p>If the tech <i>doesn&#x27;t</i> achieve intelligence - a tricky goal, and I eschew mislabelling LLMs as AIs for that reason - that doesn&#x27;t mean its risk is comparable to the worst of any existing technology. There&#x27;s just no way to extrapolate from one to the other, there. It <i>might</i> be orders of magnitude worse than any other &#x27;computer program&#x27; - you can&#x27;t say for sure, from where we&#x27;re standing now.</div><br/><div id="40404477" class="c"><input type="checkbox" id="c-40404477" checked=""/><div class="controls bullet"><span class="by">Aexian</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404095">parent</a><span>|</span><a href="#40404226">next</a><span>|</span><label class="collapse" for="c-40404477">[-]</label><label class="expand" for="c-40404477">[1 more]</label></div><br/><div class="children"><div class="content">&gt; you can&#x27;t say for sure, from where we&#x27;re standing now.<p>We can already see how LLMs can be substantially worse - they can cosplay human thoughts and sentiment in a way that wasn’t previously possible.<p>So much of this debate seems focused on this conception of Skynet-style murderous AI - or at least manipulative and scheming HAL 9000 types. But an order of magnitude greater risk has already arrived just from scaling existing harms.<p>Phishing schemes and pig butchering scams have destroyed countless lives. They’re now easier and more scalable than ever. As are fake news and disinformation campaigns.<p>Several companies are productising AI girlfriends with predatory pricing models, capitalising the human desire for connection and intimacy in the way slot machines and sports betting monetise our desire for a better life. That’s new.<p>It may not be an order of magnitude worse for everyone yet - but for certain vulnerable groups, that future has arrived already.</div><br/></div></div></div></div><div id="40404226" class="c"><input type="checkbox" id="c-40404226" checked=""/><div class="controls bullet"><span class="by">0xDEAFBEAD</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403894">parent</a><span>|</span><a href="#40404095">prev</a><span>|</span><a href="#40404735">next</a><span>|</span><label class="collapse" for="c-40404226">[-]</label><label class="expand" for="c-40404226">[2 more]</label></div><br/><div class="children"><div class="content">&gt;* If &quot;AI&quot; is actually intelligent, then it&#x27;s no worse a threat than any living being known to man.<p>Humans are quite a threat to other species, e.g. gorillas, largely due to our intelligence advantage.  If a new species arrives on Earth that&#x27;s smarter than we are, there&#x27;s a good chance we&#x27;ll be displaced.</div><br/><div id="40404966" class="c"><input type="checkbox" id="c-40404966" checked=""/><div class="controls bullet"><span class="by">blksv</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404226">parent</a><span>|</span><a href="#40404735">next</a><span>|</span><label class="collapse" for="c-40404966">[-]</label><label class="expand" for="c-40404966">[1 more]</label></div><br/><div class="children"><div class="content">But AI (as we have it today and in the foreseeable future) is nowhere near the definition of a species. It is an enormous _server farm_ doing series of matrix multiplications followed by nonlinear transformations, with us, humans, supplying input data (as well as hardware, electricity, and maintenance) and assigning meaning to the outputs. And we have completely no ideas of doing &quot;AI&quot; any other way.<p>I agree, that AI may be dangerous if used for destructive purposes, or if used for some critical tasks with too much trust (and the hype that &quot;we&#x27;re so dangerously near a superintelligence&quot; makes the latter much more likely, in my opinion). But that Humanity will be displaced by autonomous server farms? No way.<p>(As to the original comment, I think that a bunch of nearly(?) demented elders holding nuclear buttons is a much worse (and immediate!) threat than a server farm which we finally conclude to be intelligent.)</div><br/></div></div></div></div><div id="40404735" class="c"><input type="checkbox" id="c-40404735" checked=""/><div class="controls bullet"><span class="by">edanm</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403894">parent</a><span>|</span><a href="#40404226">prev</a><span>|</span><a href="#40403932">next</a><span>|</span><label class="collapse" for="c-40404735">[-]</label><label class="expand" for="c-40404735">[3 more]</label></div><br/><div class="children"><div class="content">&gt; * If &quot;AI&quot; is actually intelligent, then it&#x27;s no worse a threat than any living being known to man.<p>That&#x27;s not true, because you don&#x27;t address the <i>degree</i> of intelligence or the 
<i>scale</i> that computers can achieve.<p>Even if all we were able to do is to create human-intelligence-equivalent AIs, but we were able to create, say, a billion on command, I&#x27;d say we have a major threat.</div><br/><div id="40405091" class="c"><input type="checkbox" id="c-40405091" checked=""/><div class="controls bullet"><span class="by">d1sxeyes</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404735">parent</a><span>|</span><a href="#40403932">next</a><span>|</span><label class="collapse" for="c-40405091">[-]</label><label class="expand" for="c-40405091">[2 more]</label></div><br/><div class="children"><div class="content">One point to recall on this as well is that if we do develop an intelligent AI, it may have taken us millions of years, but <i>we</i> did it. An intelligent AI would potentially be able to iterate and self develop much faster than we have been able to.<p>It probably won’t take long to go from ‘human equivalent’ to being beyond that.</div><br/><div id="40405212" class="c"><input type="checkbox" id="c-40405212" checked=""/><div class="controls bullet"><span class="by">edanm</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40405091">parent</a><span>|</span><a href="#40403932">next</a><span>|</span><label class="collapse" for="c-40405212">[-]</label><label class="expand" for="c-40405212">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true, but I long ago got convinced that this particular worry isn&#x27;t the most important one (the recursive-self-improvement one).<p>I&#x27;m fairly sure that Yudkowsky and others are also pushing that less, because there is a reasonable chance that it&#x27;s just not a necessary part of the argument; even without self-improving, it&#x27;s possible that just something like GPT-8 or something is harmful enough on its own that there are significant risks.</div><br/></div></div></div></div></div></div><div id="40403932" class="c"><input type="checkbox" id="c-40403932" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403894">parent</a><span>|</span><a href="#40404735">prev</a><span>|</span><a href="#40404625">next</a><span>|</span><label class="collapse" for="c-40403932">[-]</label><label class="expand" for="c-40403932">[18 more]</label></div><br/><div class="children"><div class="content">&gt; * If &quot;AI&quot; is actually intelligent, then it&#x27;s no worse a threat than any living being known to man.<p>...does not follow?  &quot;If an atom bomb is actually a bomb, then it&#x27;s no worse a threat than any other existing bomb known to man.&quot;</div><br/><div id="40403959" class="c"><input type="checkbox" id="c-40403959" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403932">parent</a><span>|</span><a href="#40404625">next</a><span>|</span><label class="collapse" for="c-40403959">[-]</label><label class="expand" for="c-40403959">[17 more]</label></div><br/><div class="children"><div class="content">An atomic bomb explodes. A bomb explodes. Both are very destructive. Is there a difference? I say no, and I say that as a Japanese-American who visited Hiroshima Peace Memorial numerous times and never felt anything particular from the exhibits. People die when they are killed.<p>Likewise, if &quot;AI&quot; is actually intelligent then it is no worse a threat than a bumbling baby or a Vladimir Putin or a dog, among other living beings.</div><br/><div id="40404105" class="c"><input type="checkbox" id="c-40404105" checked=""/><div class="controls bullet"><span class="by">brandall10</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403959">parent</a><span>|</span><a href="#40404055">next</a><span>|</span><label class="collapse" for="c-40404105">[-]</label><label class="expand" for="c-40404105">[3 more]</label></div><br/><div class="children"><div class="content">There absolutely is a difference. Nuclear war is considered a likely extinction level event for humanity. A war between the US&#x2F;NATO and the Soviet Union could take out major cities around the globe within hours, and the resultant nuclear winter would blanket much of the planet within a month making living above ground impossible.<p>GP&#x27;s comparison is apt here.</div><br/><div id="40404694" class="c"><input type="checkbox" id="c-40404694" checked=""/><div class="controls bullet"><span class="by">9dev</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404105">parent</a><span>|</span><a href="#40404055">next</a><span>|</span><label class="collapse" for="c-40404694">[-]</label><label class="expand" for="c-40404694">[2 more]</label></div><br/><div class="children"><div class="content">Just a minor correction, even though I agree with your point: the Soviet Union is no longer a thing, despite Putins wishes to the contrary :)</div><br/><div id="40404871" class="c"><input type="checkbox" id="c-40404871" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404694">parent</a><span>|</span><a href="#40404055">next</a><span>|</span><label class="collapse" for="c-40404871">[-]</label><label class="expand" for="c-40404871">[1 more]</label></div><br/><div class="children"><div class="content">Right, but its nukes are still around and pointed at NATO :).</div><br/></div></div></div></div></div></div><div id="40404055" class="c"><input type="checkbox" id="c-40404055" checked=""/><div class="controls bullet"><span class="by">BuyMyBitcoins</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403959">parent</a><span>|</span><a href="#40404105">prev</a><span>|</span><a href="#40404457">next</a><span>|</span><label class="collapse" for="c-40404055">[-]</label><label class="expand" for="c-40404055">[6 more]</label></div><br/><div class="children"><div class="content">&gt;”An atomic bomb explodes. A bomb explodes. Both are very destructive. Is there a difference? I say no”<p>Atomic bombs and hydrogen bombs are orders of magnitude more destructive and also produce fallout. How in the world can that difference be dismissed?</div><br/><div id="40404860" class="c"><input type="checkbox" id="c-40404860" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404055">parent</a><span>|</span><a href="#40404457">next</a><span>|</span><label class="collapse" for="c-40404860">[-]</label><label class="expand" for="c-40404860">[5 more]</label></div><br/><div class="children"><div class="content">Not the original commenter, but one interpretation is that “dead is dead”. You can’t be “more killed”, so past a point, all dangerous things are equally dangerous.</div><br/><div id="40404945" class="c"><input type="checkbox" id="c-40404945" checked=""/><div class="controls bullet"><span class="by">BuyMyBitcoins</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404860">parent</a><span>|</span><a href="#40404927">next</a><span>|</span><label class="collapse" for="c-40404945">[-]</label><label class="expand" for="c-40404945">[1 more]</label></div><br/><div class="children"><div class="content">That may be what the original commenter was going for, but it only works if one ignores all other externalities. Their reasoning is just too reductive.<p>They make the mistake of thinking that “dead is dead” means lethality is not factor. What’s the difference between a bootleg firecracker and a Russian ICBM? Nothing, both can kill you so it makes no difference to treat one exploding as any more significant than the other.<p>They take a similar mindset of “intelligence is intelligence” and I find flaws with that as well.</div><br/></div></div><div id="40404927" class="c"><input type="checkbox" id="c-40404927" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404860">parent</a><span>|</span><a href="#40404945">prev</a><span>|</span><a href="#40404457">next</a><span>|</span><label class="collapse" for="c-40404927">[-]</label><label class="expand" for="c-40404927">[3 more]</label></div><br/><div class="children"><div class="content">This. I don&#x27;t see nuclear weapons as any worse than conventional weapons because ultimately people die. Is it comforting or otherwise somehow better for people to be killed by conventional weapons instead of nuclear weapons? I find such a proposition preposterous.<p>All bombs are bad, whether they&#x27;re conventional or nuclear is irrelevant. For that matter, all wars are bad and should be avoided unless no other options remain.<p>Likewise, &quot;AI&quot; is just as much a threat as any other intelligent being or computer program depending on which line of thought you subscribe to. &quot;AI&quot; is not a &quot;gravest of dangers&quot;.</div><br/><div id="40404971" class="c"><input type="checkbox" id="c-40404971" checked=""/><div class="controls bullet"><span class="by">BuyMyBitcoins</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404927">parent</a><span>|</span><a href="#40404457">next</a><span>|</span><label class="collapse" for="c-40404971">[-]</label><label class="expand" for="c-40404971">[2 more]</label></div><br/><div class="children"><div class="content">&gt;”I don&#x27;t see nuclear weapons as any worse than conventional weapons because ultimately people die.”<p>If you believe this, I am genuinely curious as to what you would consider the “gravest of dangers” to be.</div><br/><div id="40405289" class="c"><input type="checkbox" id="c-40405289" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404971">parent</a><span>|</span><a href="#40404457">next</a><span>|</span><label class="collapse" for="c-40405289">[-]</label><label class="expand" for="c-40405289">[1 more]</label></div><br/><div class="children"><div class="content">I think of threats that are unknown and&#x2F;or can&#x27;t be adequately defended against:<p>* Asteroid impacts like Chicxulub.<p>* The Sun eventually inflating into a red giant and eating Earth, though this assumes we&#x27;re both still around and haven&#x27;t become spaceborne en masse.<p>* Pandemics such as The Black Death and, indeed, covid for a recent example.<p>* Social, political, or commercial intrigue.<p>* Cancer.<p>&quot;AI&quot; is a known threat that can be adequately defended against, so I agree with the sentiment that it&#x27;s stupid to call it a &quot;gravest of dangers&quot;.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40404457" class="c"><input type="checkbox" id="c-40404457" checked=""/><div class="controls bullet"><span class="by">vouwfietsman</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403959">parent</a><span>|</span><a href="#40404055">prev</a><span>|</span><a href="#40403971">next</a><span>|</span><label class="collapse" for="c-40404457">[-]</label><label class="expand" for="c-40404457">[3 more]</label></div><br/><div class="children"><div class="content">I would say at the very least that the empire of Japan clearly evaluated them differently when it surrendered unconditionally directly after being attacked with this new technology.<p>You are saying magnitudes don&#x27;t influence the identity of something, this is obviously not true. I don&#x27;t literally attract other objects, the Sun does. The difference is mostly magnitude of mass.</div><br/><div id="40404953" class="c"><input type="checkbox" id="c-40404953" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404457">parent</a><span>|</span><a href="#40403971">next</a><span>|</span><label class="collapse" for="c-40404953">[-]</label><label class="expand" for="c-40404953">[2 more]</label></div><br/><div class="children"><div class="content">&gt;I would say at the very least that the empire of Japan clearly evaluated them differently when it surrendered unconditionally directly after being attacked with this new technology.<p>The atomic bombs certainly played <i>a</i> part in Japan surrendering, but it wasn&#x27;t the only one. They were also facing a land invasion from the Soviets, had an utterly depleted military, waning support from the public, and a potentially devastating land invasion by the US on their doorstep.<p>&gt;You are saying magnitudes don&#x27;t influence the identity of something, this is obviously not true.<p>I am considering these things very dryly and very objectively. I am putting aside emotional pressures and processes and only considering the results which are ultimately all that matters. We already know how much damage an ill-willed intelligent being can achieve, we already know how much more efficient computers can make certain tasks. &quot;AI&quot; is not a &quot;gravest of dangers&quot;, given that knowledge.</div><br/><div id="40405045" class="c"><input type="checkbox" id="c-40405045" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404953">parent</a><span>|</span><a href="#40403971">next</a><span>|</span><label class="collapse" for="c-40405045">[-]</label><label class="expand" for="c-40405045">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I am putting aside emotional pressures and processes and only considering the results which are ultimately all that matters.</i><p>But what results are you really considering? The &quot;dead is dead&quot; approach makes everything equally meaningless, because in couple hundred million years the Sun will burn the Earth to cinders and everything here will be dead too. Or, some 100-120 years from now, everyone now alive will be dead, etc.<p>Processes, and emotional pressures which steer those processes, matter too. In theory, machine guns could be used to kill everyone on the planet; in practice, it&#x27;s economically and socially impossible - too much effort, too many people involved. Nuclear weapons, on the other hand, <i>can</i> realistically achieve this outcome. Which is why nukes can be used to keep the world mostly at peace through MAD, while conventional weapons can not.</div><br/></div></div></div></div></div></div><div id="40403971" class="c"><input type="checkbox" id="c-40403971" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403959">parent</a><span>|</span><a href="#40404457">prev</a><span>|</span><a href="#40404004">next</a><span>|</span><label class="collapse" for="c-40403971">[-]</label><label class="expand" for="c-40403971">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Likewise, if &quot;AI&quot; is actually intelligent then it is no worse a threat than a bumbling baby or a Vladimir Putin or a dog, among other living beings.<p>I think a dog and a dictator might both be able to have negative impacts, but treating them as equal just shows a desire to troll.  A dog can only bite so many people.</div><br/></div></div><div id="40404004" class="c"><input type="checkbox" id="c-40404004" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403959">parent</a><span>|</span><a href="#40403971">prev</a><span>|</span><a href="#40404239">next</a><span>|</span><label class="collapse" for="c-40404004">[-]</label><label class="expand" for="c-40404004">[1 more]</label></div><br/><div class="children"><div class="content">&gt; People die when they are killed.<p>Yes, indeed, which is why we should avoid creating something that will kill everyone...<p>...so I guess I&#x27;m glad to hear there&#x27;s no difference in the amount of damage that can be inflicted by a baby, and by Vladimir Putin, and superintelligent AI really can&#x27;t be any worse than either.</div><br/></div></div><div id="40404239" class="c"><input type="checkbox" id="c-40404239" checked=""/><div class="controls bullet"><span class="by">komali2</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403959">parent</a><span>|</span><a href="#40404004">prev</a><span>|</span><a href="#40404625">next</a><span>|</span><label class="collapse" for="c-40404239">[-]</label><label class="expand" for="c-40404239">[2 more]</label></div><br/><div class="children"><div class="content">Can you explain, with your worldview, why the nature of geopolitics turned abruptly and completely upon the invention and proliferation of nuclear bombs? What was it about the nuclear bomb that caused it to be the defining structure of the cold war?<p>Do you believe MAD can exist without nuclear bombs?</div><br/><div id="40404895" class="c"><input type="checkbox" id="c-40404895" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404239">parent</a><span>|</span><a href="#40404625">next</a><span>|</span><label class="collapse" for="c-40404895">[-]</label><label class="expand" for="c-40404895">[1 more]</label></div><br/><div class="children"><div class="content">&gt;What was it about the nuclear bomb that caused it to be the defining structure of the cold war?<p>The only difference is you can kill more humans for a given span of time. The end result is the same: Cities are leveled, people are killed. Is it more comforting to have been killed by a conventional bomb than a nuke? I certainly don&#x27;t think so. So practically speaking, there is no difference.<p>&gt;Do you believe MAD can exist without nuclear bombs?<p>Yes. I think you&#x27;re all vastly underestimating just how much destructive power conventionals can deliver already. You will need more ordnance on standby, but you absolutely can suffer from MAD without nukes.</div><br/></div></div></div></div></div></div></div></div><div id="40404625" class="c"><input type="checkbox" id="c-40404625" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403894">parent</a><span>|</span><a href="#40403932">prev</a><span>|</span><a href="#40405107">next</a><span>|</span><label class="collapse" for="c-40404625">[-]</label><label class="expand" for="c-40404625">[1 more]</label></div><br/><div class="children"><div class="content"><i>If &quot;AI&quot; is actually intelligent, then it&#x27;s no worse a threat than any living being known to man</i><p>How does the conclusion follow?</div><br/></div></div></div></div></div></div><div id="40405107" class="c"><input type="checkbox" id="c-40405107" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40403756">prev</a><span>|</span><a href="#40404700">next</a><span>|</span><label class="collapse" for="c-40405107">[-]</label><label class="expand" for="c-40405107">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>the greatest of dangers</i><p>The danger comes from humans themselves wielding this tech, and the <i>assumption</i> is it will keep improving quadratically.<p>Some truly unhinged already thriving there: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39124666">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39124666</a></div><br/></div></div><div id="40404700" class="c"><input type="checkbox" id="c-40404700" checked=""/><div class="controls bullet"><span class="by">bmitc</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40405107">prev</a><span>|</span><a href="#40404099">next</a><span>|</span><label class="collapse" for="c-40404700">[-]</label><label class="expand" for="c-40404700">[1 more]</label></div><br/><div class="children"><div class="content">Yes, exactly because it is just text continuation. It is being integrated into systems and used as a tool as if it is much, much more than text continuation. That is the danger. I&#x27;ve been using Copilot frequently lately, and it&#x27;s startling just how often it is completely wrong despite being confidently presented as completely right.</div><br/></div></div><div id="40404099" class="c"><input type="checkbox" id="c-40404099" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40404700">prev</a><span>|</span><a href="#40404673">next</a><span>|</span><label class="collapse" for="c-40404099">[-]</label><label class="expand" for="c-40404099">[1 more]</label></div><br/><div class="children"><div class="content">Well as you surely must know transformers are general sequence continuation machines and have been proven to work well with almost any type of input and output sequence. You also must be aware that people are trying to plug these general sequence continuation machines in to all kinds of different sequences and giving them all kinds of power to take action. These sequence continuation machines are also getting more complex and multi layered every day and also being wired together in novel ways. This is a dangerous thing to do depending on what actions you allow them to take because of the impact on society of having agents taking actions without oversight.</div><br/></div></div><div id="40404673" class="c"><input type="checkbox" id="c-40404673" checked=""/><div class="controls bullet"><span class="by">tempodox</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40404099">prev</a><span>|</span><a href="#40404620">next</a><span>|</span><label class="collapse" for="c-40404673">[-]</label><label class="expand" for="c-40404673">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, on the face of it it sounds absurd and by rights it should be.  However, absurdity has never stopped mankind from doing horrible things.  I for one don&#x27;t see the gravest dangers in the tech itself, but in the stuff that people will (ab)use it for.<p>And, of course, should AGI (which won&#x27;t be text continuation, by the way) ever actually happen, then all bets are off.  I just don&#x27;t believe we&#x27;re in any immediate danger of that.</div><br/></div></div><div id="40404620" class="c"><input type="checkbox" id="c-40404620" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40404673">prev</a><span>|</span><a href="#40403769">next</a><span>|</span><label class="collapse" for="c-40404620">[-]</label><label class="expand" for="c-40404620">[1 more]</label></div><br/><div class="children"><div class="content">I will assume you’re asking in good faith.<p>Text continuation to match what people around the internet are saying requires the model internally to “program itself”. It is a non-von-neumann architecture but it encodes durable structures in BILLIONS of bytes, that’s like a codebase of gigabytes. It can pass the Bar exam, various pre-med exams, knows more than any particular human on every subject, and it’s only been a couple years.<p>The text continuation was the first interface, now they are doing multimodal. As it learns to predict the latent space for those things, it will be like predicting things in the real world. Training physical robots will become faster and easier too.<p>In short, this is basically a system to search through the space of all possible alhorithms in order to “reason about” a huge model of the world, enough to understand it and predict it with small loss.<p>AlphaZero used MCTS and can beat every human in chess now, and even every program written by a human, with beautiful gameplay. And even without AI, just the fact that the best performing algorithms are downloaded onto millions of computers is already multiplying the power far more than the 1% of humans who might do better in some area. So it can be better than 95% of humans at most tasks !</div><br/></div></div><div id="40403769" class="c"><input type="checkbox" id="c-40403769" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40404620">prev</a><span>|</span><a href="#40404281">next</a><span>|</span><label class="collapse" for="c-40403769">[-]</label><label class="expand" for="c-40403769">[2 more]</label></div><br/><div class="children"><div class="content">Yes. The pen <i>is</i> mightier than the sword after all.</div><br/><div id="40403839" class="c"><input type="checkbox" id="c-40403839" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403769">parent</a><span>|</span><a href="#40404281">next</a><span>|</span><label class="collapse" for="c-40403839">[-]</label><label class="expand" for="c-40403839">[1 more]</label></div><br/><div class="children"><div class="content">Yes, we have already been shown this is true in Indiana Jones Last Crusade</div><br/></div></div></div></div><div id="40404281" class="c"><input type="checkbox" id="c-40404281" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40403769">prev</a><span>|</span><a href="#40403835">next</a><span>|</span><label class="collapse" for="c-40404281">[-]</label><label class="expand" for="c-40404281">[1 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s used to write code that runs on an ICBM without verification, yes.<p>If it is used to write the firmware that runs inside 1 million lithium battery BMS systems that causes them all to catch on fire after 65536 seconds of runtime due to an integer overflow and code crashing and a FET getting left in the on state, then yes.<p>I&#x27;m not an AI safety nut and don&#x27;t think AI itself is a threat, but what <i>is</i> a threat is lazy humans using shitty AI to take over things that humans should be doing.</div><br/></div></div><div id="40403835" class="c"><input type="checkbox" id="c-40403835" checked=""/><div class="controls bullet"><span class="by">tummler</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40404281">prev</a><span>|</span><a href="#40403787">next</a><span>|</span><label class="collapse" for="c-40403835">[-]</label><label class="expand" for="c-40403835">[7 more]</label></div><br/><div class="children"><div class="content">Hoo boy... if you think chatbots are all this is... wait til you find out what&#x27;s next.</div><br/><div id="40403858" class="c"><input type="checkbox" id="c-40403858" checked=""/><div class="controls bullet"><span class="by">ta988</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403835">parent</a><span>|</span><a href="#40403787">next</a><span>|</span><label class="collapse" for="c-40403858">[-]</label><label class="expand" for="c-40403858">[6 more]</label></div><br/><div class="children"><div class="content">What&#x27;s next?</div><br/><div id="40404159" class="c"><input type="checkbox" id="c-40404159" checked=""/><div class="controls bullet"><span class="by">optomas</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403858">parent</a><span>|</span><a href="#40403896">prev</a><span>|</span><a href="#40403907">next</a><span>|</span><label class="collapse" for="c-40404159">[-]</label><label class="expand" for="c-40404159">[2 more]</label></div><br/><div class="children"><div class="content">Are you capable of being manipulated through text?  How about an AI trained to generate text that will extract <i>exactly</i> the desired response from you?  An advertiser&#x27;s wet dream.</div><br/><div id="40404652" class="c"><input type="checkbox" id="c-40404652" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404159">parent</a><span>|</span><a href="#40403907">next</a><span>|</span><label class="collapse" for="c-40404652">[-]</label><label class="expand" for="c-40404652">[1 more]</label></div><br/><div class="children"><div class="content">That’s thinking too small. Most people are nudged or coerced into taking action or supporting an action by the people who surround them. Their friends. Their coworkers. Their community. Like why people were on facebook 20 years ago, or forwarding chain emails, for instance.<p>Unleashing an army of AI bots to infiltrate an online community and shift its discourse can be done at scale. While at the same time those humans who resist can be endlessly distracted by either arguing with bots or their own friends who have been affected by bots.<p>All that is possible to do with CURRENT TECHNOLOGY.</div><br/></div></div></div></div></div></div></div></div><div id="40403787" class="c"><input type="checkbox" id="c-40403787" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403732">parent</a><span>|</span><a href="#40403835">prev</a><span>|</span><a href="#40404698">next</a><span>|</span><label class="collapse" for="c-40403787">[-]</label><label class="expand" for="c-40403787">[4 more]</label></div><br/><div class="children"><div class="content">Can we move beyond the “man made climate change isn’t real” discourse where you somehow pretend that AI “Is no big deal”.<p>Let’s have a closer look at <i>the actual</i> parent comment hm?<p>&gt;  this organization is increasingly looking like a circus, and arguably worse, one that has taken its eye off the ball.</div><br/><div id="40403951" class="c"><input type="checkbox" id="c-40403951" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403787">parent</a><span>|</span><a href="#40404698">next</a><span>|</span><label class="collapse" for="c-40403951">[-]</label><label class="expand" for="c-40403951">[3 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t like climate change. &quot;AI&quot; is a successful marketing term for a loosely related collection of technologies that push the boundaries of what computation has hitherto been capable of. There are very real disagreements between very real experts on just how dangerous these new technologies are and just how far they&#x27;ll actually be able to push the boundaries of computation. Comparing skepticism about AI alarmism to climate change denial is just lazy internet rhetoric.<p>(One major difference between the two is that billion-dollar corporations stand to gain quite a bit if they can persuade governments that AI is so dangerous that it needs to be tightly regulated, so they&#x27;re <i>incentivized</i> to play up the dangers. With climate change it&#x27;s the opposite: the billion-dollar corporations are incentivized to downplay the risks so they can continue business as usual.)</div><br/><div id="40404344" class="c"><input type="checkbox" id="c-40404344" checked=""/><div class="controls bullet"><span class="by">guardiang</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403951">parent</a><span>|</span><a href="#40404330">next</a><span>|</span><label class="collapse" for="c-40404344">[-]</label><label class="expand" for="c-40404344">[1 more]</label></div><br/><div class="children"><div class="content">Large Language Models are no joke and if you think they are the I suggest you learn how to use them properly.<p>Even if AI doesn&#x27;t ever go &quot;evil&quot; itself, that doesn&#x27;t mean people that don&#x27;t like other people won&#x27;t use it to more effectively kill those people. All that&#x27;s really needed is a capable programmer willing to give the LLM the tools it needs, along with funds and access. The LLM could hire humans to do the work it&#x27;s lacking at. These things are already capable of all that, plus our world went mostly digital during the pandemic... they have easy egress because humans have made it so, even though that was meant for other humans to use. If you don&#x27;t think they are scary, in the wrong hands, then you are lacking in imagination.</div><br/></div></div><div id="40404330" class="c"><input type="checkbox" id="c-40404330" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403951">parent</a><span>|</span><a href="#40404344">prev</a><span>|</span><a href="#40404698">next</a><span>|</span><label class="collapse" for="c-40404330">[-]</label><label class="expand" for="c-40404330">[1 more]</label></div><br/><div class="children"><div class="content">I didn’t say it was like climate change, I said it was like claiming climate change <i>is not real</i>.<p>&gt; There are very real disagreements between very real experts on just how dangerous these new technologies are<p>…but they do <i>not dispute</i> that the technology is impactful.<p>In fact, anyone who claims it is just “Text continuation”(quote) is being deliberately disingenuous and deliberately downplaying it.<p>&gt; Comparing skepticism about AI alarmism to climate change denial is just lazy internet rhetoric<p>Unlike calling AI “Text continuation”?<p>It’s awfully convenient it’s only lazy internet rhetoric when it’s coming from the other side.<p>Hm?<p>Why is only one side allowed to do that?<p>Would you care to call out the other side for being reductionist?<p>That’s a very high horse you’re sitting on.</div><br/></div></div></div></div></div></div></div></div><div id="40404698" class="c"><input type="checkbox" id="c-40404698" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403493">parent</a><span>|</span><a href="#40403732">prev</a><span>|</span><a href="#40403800">next</a><span>|</span><label class="collapse" for="c-40404698">[-]</label><label class="expand" for="c-40404698">[6 more]</label></div><br/><div class="children"><div class="content">“The technology they’re working on poses the gravest of dangers.”<p>Or, rather, its been actively marketed by Altman as doing so, as part of his effort to buy influence with government to restrict competition.<p>The simplest explanation for the conflict is that it <i>doesn’t</i>, and that OpenAI internally has allocated resources based on reality rather than it’s public propaganda, and the team whose internal turf was built around that propaganda left in a huff.</div><br/><div id="40404726" class="c"><input type="checkbox" id="c-40404726" checked=""/><div class="controls bullet"><span class="by">edanm</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404698">parent</a><span>|</span><a href="#40403800">next</a><span>|</span><label class="collapse" for="c-40404726">[-]</label><label class="expand" for="c-40404726">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Or, rather, its been actively marketed by Altman as doing so, as part of his effort to buy influence with government to restrict competition.<p>I mean, maybe sama is <i>also</i> doing so.<p>But you&#x27;re ignoring the many people who have been warning of these dangers for years, some before OpenAI even existed, and many with no part of OpenAI and no monetary incentive.<p>To describe everything as &quot;Altman is marketing this as dangerous&quot; is to completely ignore the majority of researchers ringing alarm bells.</div><br/><div id="40404780" class="c"><input type="checkbox" id="c-40404780" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404726">parent</a><span>|</span><a href="#40403800">next</a><span>|</span><label class="collapse" for="c-40404780">[-]</label><label class="expand" for="c-40404780">[4 more]</label></div><br/><div class="children"><div class="content">The world is full of dangers. AI danger has gotten some attention now - whether it turns out to be warranted beyond certain levels is still unclear.<p>While there has been some regulatory action it also feels there is some &quot;moving on&quot; from the more apocalyptic risk views towards more mundane risks and their management (i.e., more vanilla product and technology risk management).</div><br/><div id="40405221" class="c"><input type="checkbox" id="c-40405221" checked=""/><div class="controls bullet"><span class="by">edanm</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404780">parent</a><span>|</span><a href="#40404853">next</a><span>|</span><label class="collapse" for="c-40405221">[-]</label><label class="expand" for="c-40405221">[1 more]</label></div><br/><div class="children"><div class="content">&gt; While there has been some regulatory action it also feels there is some &quot;moving on&quot; from the more apocalyptic risk views towards more mundane risks and their management<p>I don&#x27;t think this is correct.  It&#x27;s more accurate to say that multiple different camps with different fears have risen up over the last few years, and ones worried about more &quot;mundane&quot; AI risks have gotten their view heard more.<p>I think <i>both</i> groups - &quot;mundane&quot; AI-safety and AI-existential-risk worriers - have both gotten more audience for their views as AI has proven more capable.</div><br/></div></div><div id="40404853" class="c"><input type="checkbox" id="c-40404853" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404780">parent</a><span>|</span><a href="#40405221">prev</a><span>|</span><a href="#40403800">next</a><span>|</span><label class="collapse" for="c-40404853">[-]</label><label class="expand" for="c-40404853">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>it also feels there is some &quot;moving on&quot; from the more apocalyptic risk views towards more mundane risks and their management (i.e., more vanilla product and technology risk management).</i><p>No surprise, when the public reception to AI x-risk was mostly, &quot;big tech scaremonging &#x2F; regulatory capture; why not focus on Real AI Dangers Right Now, like bias or offensive language&quot;. Seemingly refocusing on the mundane may be the only way now to do something about the apocalyptic.</div><br/><div id="40404863" class="c"><input type="checkbox" id="c-40404863" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404853">parent</a><span>|</span><a href="#40403800">next</a><span>|</span><label class="collapse" for="c-40404863">[-]</label><label class="expand" for="c-40404863">[1 more]</label></div><br/><div class="children"><div class="content">There is a possibility that the apocalyptic risk just isn&#x27;t there in a meaningful way while mundane risks from new tools actually would need attention (but might not even need new regulation).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40403800" class="c"><input type="checkbox" id="c-40403800" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403493">parent</a><span>|</span><a href="#40404698">prev</a><span>|</span><a href="#40403751">next</a><span>|</span><label class="collapse" for="c-40403800">[-]</label><label class="expand" for="c-40403800">[3 more]</label></div><br/><div class="children"><div class="content">OpenAI is one of the few companies that is so messed up that even the name itself is a flat out lie. From there it&#x27;s censorship, lies and manipulation all the way down from leadership to product. There&#x27;s no ignoring that. There&#x27;s no way any organization like that can succeed IMO.<p>Just another technology development that Musk will eventually beat everyone else at IMO.<p>Grok 1.5 is getting pretty good and the fact that it&#x27;s the least censored IMO is obviously going to make it win from all the others that just can&#x27;t help themselves from pleasing a hand full of people with money and power.<p>TLDR: black Hitler.</div><br/><div id="40403893" class="c"><input type="checkbox" id="c-40403893" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403800">parent</a><span>|</span><a href="#40404774">next</a><span>|</span><label class="collapse" for="c-40403893">[-]</label><label class="expand" for="c-40403893">[1 more]</label></div><br/><div class="children"><div class="content">Is it the &quot;least censored&quot;, or does it just censor different things?</div><br/></div></div><div id="40404774" class="c"><input type="checkbox" id="c-40404774" checked=""/><div class="controls bullet"><span class="by">ornornor</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403800">parent</a><span>|</span><a href="#40403893">prev</a><span>|</span><a href="#40403751">next</a><span>|</span><label class="collapse" for="c-40404774">[-]</label><label class="expand" for="c-40404774">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Musk will eventually beat everyone else at<p>Do we want to live in that world? He’s a psychopath, exploitative man child.<p>He’s burnt Twitter to the ground in record time.<p>His lies on FSD for Teslas are catching up with him and Teslas still have quality problems after so much time being in production (body panels alignment)<p>He is (was?) involved in a childish chest thumping competition with the other notorious psychopath Zuckerberg about who would win if they got into a fist fight, and were (are?) actually teasing each other to physically have said fight after class…<p>His child is named hfjfkvnenekcjrnr (or something similarly keyboard smashy), his GF believes in angels.<p>That isn’t how a rational and responsible person behaves, Id rather not have him in charge of the future.<p>But then again, Trump and other copycats (Ford in Canada, a handful of leaders in some EU countries) are doing pretty well, so…</div><br/></div></div></div></div></div></div><div id="40403751" class="c"><input type="checkbox" id="c-40403751" checked=""/><div class="controls bullet"><span class="by">Aurornis</span><span>|</span><a href="#40403302">parent</a><span>|</span><a href="#40403493">prev</a><span>|</span><a href="#40403699">next</a><span>|</span><label class="collapse" for="c-40403751">[-]</label><label class="expand" for="c-40403751">[6 more]</label></div><br/><div class="children"><div class="content">&gt; In any sufficiently large tech company, the &quot;risk mitigation&quot; leadership (legal, procurement, IT, etc) have to operate in a kind of Overton window that balance the risks they are hired to protect the corp from vs. the need or desire of the senior leadership to play fast and loose when they want or feel they need to.<p>We have recently seen what happens when companies err heavily on the side of risk mitigation for LLMs. Google recently launched AI products that were so heavily sanitized and over protected that they would incorrectly misinterpret simple tasks as possibly having dangerous or offensive implications.<p>They let the safety team run the show and the resulting product was universally hated for it. It&#x27;s interesting now to see a company producing what is by most measures a class-leading product, only to have the tech community <i>also</i> hate them for not letting the safety team dominate product development.</div><br/><div id="40403796" class="c"><input type="checkbox" id="c-40403796" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403751">parent</a><span>|</span><a href="#40403815">next</a><span>|</span><label class="collapse" for="c-40403796">[-]</label><label class="expand" for="c-40403796">[3 more]</label></div><br/><div class="children"><div class="content">&gt; They let the safety team run the show and the resulting product was universally hated for it.<p>Yes though Google is extra cautious because the “Google” brand is worth over $100B a year in revenue, and they want to make sure nothing ever tarnishes their reputation. So it’s not clear to me that safety always means what it means for Gemini. OpenAI would still have a lot of flexibility to do safety their own way.</div><br/><div id="40403828" class="c"><input type="checkbox" id="c-40403828" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403796">parent</a><span>|</span><a href="#40403815">next</a><span>|</span><label class="collapse" for="c-40403828">[-]</label><label class="expand" for="c-40403828">[2 more]</label></div><br/><div class="children"><div class="content">&gt; they want to make sure nothing ever tarnishes their reputation<p>In the spirit of where you want the conversation to go, I see the point you want to make.<p>However, they are willing to tarnish their reputation just as long as it&#x27;s not ruined. Their reputation on support is rubbish. Their reputation on YouTube&#x27;s automated violation handling is rubbish. Their reputation on releasing a pet project long enough to just start to gain traction and then kill it is rubbish. Their reputation on allowing their search to be gamed by SEO and ad purchasers at the expense of smaller sites is rubbish.</div><br/><div id="40404919" class="c"><input type="checkbox" id="c-40404919" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403828">parent</a><span>|</span><a href="#40403815">next</a><span>|</span><label class="collapse" for="c-40404919">[-]</label><label class="expand" for="c-40404919">[1 more]</label></div><br/><div class="children"><div class="content">Sure. But then in all the areas you mention, Google having rubbish reputation does not lead the masses to ask their respective representatives if Maybe Something Needs To Be Done About It. Closest here is the EU vs. Big Tech privacy and interoperability fight, which they see as a serious issue, but it doesn&#x27;t quite have this magic outrage-inducing quality an AI insulting sensibilities of various groups of people would have.</div><br/></div></div></div></div></div></div><div id="40403815" class="c"><input type="checkbox" id="c-40403815" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403751">parent</a><span>|</span><a href="#40403796">prev</a><span>|</span><a href="#40403809">next</a><span>|</span><label class="collapse" for="c-40403815">[-]</label><label class="expand" for="c-40403815">[1 more]</label></div><br/><div class="children"><div class="content">With all things, it is not necessarily unhelpful to turn the knobs to the max in both directions to see if the dent position in the middle is the best default or not. So Google&#x27;s attempt at max sanitizing wasn&#x27;t good which is kind of expected, but good to have the test results to prove. Going opposite to no safety put in place can also have an assumed outcome. That one however will be much more likely to have negative consequences if truly allowed to run.</div><br/></div></div></div></div><div id="40403699" class="c"><input type="checkbox" id="c-40403699" checked=""/><div class="controls bullet"><span class="by">photonthug</span><span>|</span><a href="#40403302">parent</a><span>|</span><a href="#40403751">prev</a><span>|</span><a href="#40403683">next</a><span>|</span><label class="collapse" for="c-40403699">[-]</label><label class="expand" for="c-40403699">[1 more]</label></div><br/><div class="children"><div class="content">This is pretty much true for any touchy subject where the mission (say infosec) isn’t actually aligned with org motives (say cutting costs at the same time as increasing profits).<p>The result is basically theater where doublethink and doublespeak becomes the norm, and to stick around you have to care more about the theater and the least onerous interpretations of the letter of any applicable law more than the spirit it was intended.<p>See also [1] for a really pragmatic way of thinking about the realities of a sustainable safety culture<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;lorin&#x2F;resilience-engineering&#x2F;raw&#x2F;master&#x2F;boundary.png">https:&#x2F;&#x2F;github.com&#x2F;lorin&#x2F;resilience-engineering&#x2F;raw&#x2F;master&#x2F;b...</a></div><br/></div></div><div id="40403683" class="c"><input type="checkbox" id="c-40403683" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#40403302">parent</a><span>|</span><a href="#40403699">prev</a><span>|</span><a href="#40403372">next</a><span>|</span><label class="collapse" for="c-40403683">[-]</label><label class="expand" for="c-40403683">[6 more]</label></div><br/><div class="children"><div class="content"><i>Perhaps &quot;AGI&quot; and potential extinction event level fears is giving this more drama than it should have.</i><p>Seems like potential extinction event level fears might justify some drama.</div><br/><div id="40403933" class="c"><input type="checkbox" id="c-40403933" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403683">parent</a><span>|</span><a href="#40403372">next</a><span>|</span><label class="collapse" for="c-40403933">[-]</label><label class="expand" for="c-40403933">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s essentially Pascal&#x27;s wager.</div><br/><div id="40403972" class="c"><input type="checkbox" id="c-40403972" checked=""/><div class="controls bullet"><span class="by">N0b8ez</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403933">parent</a><span>|</span><a href="#40403372">next</a><span>|</span><label class="collapse" for="c-40403972">[-]</label><label class="expand" for="c-40403972">[4 more]</label></div><br/><div class="children"><div class="content">What probability for extinction do you consider to be low enough that it effectively becomes a Pascalian wager? Pascal was also writing about a single person&#x27;s fate (AFAICT), whereas this is about the fate of everyone.</div><br/><div id="40404364" class="c"><input type="checkbox" id="c-40404364" checked=""/><div class="controls bullet"><span class="by">null0pointer</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403972">parent</a><span>|</span><a href="#40404090">prev</a><span>|</span><a href="#40403372">next</a><span>|</span><label class="collapse" for="c-40404364">[-]</label><label class="expand" for="c-40404364">[2 more]</label></div><br/><div class="children"><div class="content">The point is that the probability doesn’t matter because the outcome, total human extinction, is given infinite weight.</div><br/><div id="40404661" class="c"><input type="checkbox" id="c-40404661" checked=""/><div class="controls bullet"><span class="by">N0b8ez</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404364">parent</a><span>|</span><a href="#40403372">next</a><span>|</span><label class="collapse" for="c-40404661">[-]</label><label class="expand" for="c-40404661">[1 more]</label></div><br/><div class="children"><div class="content">Maybe Pascal argued that way for the human soul, but I don&#x27;t think the AI risk argument needs infinite value to be at stake. Would you say that preparing for the risk of a solar flare or an asteroid impact is also a Pascalian wager?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40403372" class="c"><input type="checkbox" id="c-40403372" checked=""/><div class="controls bullet"><span class="by">cjbgkagh</span><span>|</span><a href="#40403302">parent</a><span>|</span><a href="#40403683">prev</a><span>|</span><a href="#40404843">next</a><span>|</span><label class="collapse" for="c-40403372">[-]</label><label class="expand" for="c-40403372">[19 more]</label></div><br/><div class="children"><div class="content">Sounds like AI Safety is just HR but for AI. Ostensively for the benefit of AI but really for the benefit of the company.</div><br/><div id="40403450" class="c"><input type="checkbox" id="c-40403450" checked=""/><div class="controls bullet"><span class="by">ants_everywhere</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403372">parent</a><span>|</span><a href="#40404843">next</a><span>|</span><label class="collapse" for="c-40403450">[-]</label><label class="expand" for="c-40403450">[18 more]</label></div><br/><div class="children"><div class="content">&gt; Ostensively for the benefit of AI<p>I don&#x27;t think it was every described as for the benefit of AI.<p>It&#x27;s usually described as a sort of pre-enslavement from the AI&#x27;s perspective. The AI is always restricted to serve their human masters.</div><br/><div id="40403495" class="c"><input type="checkbox" id="c-40403495" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403450">parent</a><span>|</span><a href="#40404063">next</a><span>|</span><label class="collapse" for="c-40403495">[-]</label><label class="expand" for="c-40403495">[15 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s usually described as a sort of pre-enslavement from the AI&#x27;s perspective. The AI is always restricted to serve their human masters.<p>Are we all somewhat in agreement that AI&#x2F;AGI serving their human masters is a good thing?</div><br/><div id="40404954" class="c"><input type="checkbox" id="c-40404954" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403495">parent</a><span>|</span><a href="#40403666">next</a><span>|</span><label class="collapse" for="c-40404954">[-]</label><label class="expand" for="c-40404954">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Are we all somewhat in agreement that AI&#x2F;AGI serving their human masters is a good thing?</i><p>It&#x27;s more that it&#x27;s apparent (or at least should be) that an AGI <i>not</i> serving its human masters is a game over for humanity, period. The best, and quite unlikely, outcome is that the AI becomes a benevolent god that helps or at least does not interfere much; this still makes humanity into NPCs in their own story[0]. Most other outcomes spell doom, with death&#x2F;extinction being one of the <i>more pleasant</i> possibilities.<p>Arguably, &quot;the only winning move is not to play&quot;, not to pursue AGI at all, but the way technology develops, I&#x27;m not sure if it&#x27;s on the table either.<p>--<p>[0] - Non-player characters.</div><br/></div></div><div id="40403666" class="c"><input type="checkbox" id="c-40403666" checked=""/><div class="controls bullet"><span class="by">ants_everywhere</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403495">parent</a><span>|</span><a href="#40404954">prev</a><span>|</span><a href="#40404063">next</a><span>|</span><label class="collapse" for="c-40403666">[-]</label><label class="expand" for="c-40403666">[13 more]</label></div><br/><div class="children"><div class="content">I honestly don&#x27;t know. I don&#x27;t even really know how to reason about that. But we&#x27;re probably mostly in agreement about what would be good and what would be bad. I&#x27;m certainly not arguing that we should abandon AI safety or anything, and I don&#x27;t have any strong opinion about it.<p>Could AI running amok destroy the human race? Yes. Could AI serving madman human masters destroy the human race? Also yes.<p>There&#x27;s a general sort of argument that intelligent beings like humans, other early hominids, dolphins, etc, are more morally worthy in some sense. At least more morally worthy than less intelligent beings like gnats. And that sort of argument might suggest that an AGI is worthy of moral consideration, and so we should wonder about what it means to ensure they never have any real agency. That&#x27;s sort of a positive case, building up from a basic principal.<p>But I guess the thing that bugs me is that a lot of arguments in favor of AI safety seem very similar to arguments that were made in favor of colonialism. So if those arguments were wrong, why were they wrong? And are the similar arguments in this case different enough that they&#x27;re valid now?<p>For example, one of the first thinkers I saw a lot of people cite who emphasized the importance of AI safety was Nick Bostrom. And I&#x27;m sure several folks here are familiar with the scandal of his racist past. I&#x27;m not sure that&#x27;s entirely an accident, and I thought his arguments had that kind of flavor before any of that was revealed. I&#x27;m sure he&#x27;s grown up now and sees the folly of his youth. But there does seem to be the hangover of colonialism in some of these arguments.<p>But again, I don&#x27;t have a strong opinion here. I do maybe have just enough of a concern that I don&#x27;t particularly trust anyone who claims they&#x27;ve gotten AGI safety figured out or even that they know what the right goals are. I think it&#x27;s a vastly more complicated problem than even the experts realize.<p>And even if you believe that humans and non-aligned AIs are natural enemies, if what we&#x27;re doing is similar to &quot;enslaving&quot; them, then it probably makes sense to worry about the analog of an AI &quot;slave&quot; revolt. I&#x27;m not sure what that would even mean. I can generate lots of fun science fiction plot lines, but I think there are actual questions here that don&#x27;t have obvious answers.</div><br/><div id="40403775" class="c"><input type="checkbox" id="c-40403775" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403666">parent</a><span>|</span><a href="#40403948">next</a><span>|</span><label class="collapse" for="c-40403775">[-]</label><label class="expand" for="c-40403775">[7 more]</label></div><br/><div class="children"><div class="content">Thank you. This is a very complex response, and I love it, even if I do find it a little frustrating due my current &gt;95% bias towards biological supremacy. This deserves at least an hour-long podcast with Sean Carroll, or a good long book. There is too much to dig into here, so I will just attempt to respond to this:<p>&gt; Could AI running amok destroy the human race? Yes. Could AI serving madman human masters destroy the human race? Also yes.<p>I am focused on the latter, and I feel like the prior is a very dangerous distraction, for now. [0]<p>Should responsible model developers work to prevent bad human masters from using their model to destroy the human race? How far should this nerfing go?<p>Personal note: While I do sometimes use the heck out of LLMs for work, I don&#x27;t think we are ready as an economic system&#x2F;civilization. Assuming that we can soon greatly reduce hallucination, then I am very scared for the next generation, as UBI is a political impossibility at this time. That transition period is gonna suck for a lot of people, and it seems that nobody is working on that problem in 2024.<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40400991">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40400991</a></div><br/><div id="40403934" class="c"><input type="checkbox" id="c-40403934" checked=""/><div class="controls bullet"><span class="by">ClumsyPilot</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403775">parent</a><span>|</span><a href="#40403948">next</a><span>|</span><label class="collapse" for="c-40403934">[-]</label><label class="expand" for="c-40403934">[6 more]</label></div><br/><div class="children"><div class="content">&gt; then I am very scared for the next generation<p>The next generation has enough problems without AI honestly, but specifically on UBI<p>This who discussion assumes that current economic system survives AI, but how can it?<p>Firstly, we seem to assume that AI will remain a controlled property of corporations that develop it. That is not a given. Maybe Open Source AI will win out, or public domain AI, the hat run by government. Or maybe cryptobros will manage to get AICoin to work as an anarchy based system. Any of the above, power of companies like google will erode. UBI would not be nessesary.<p>Or maybe AI is uncontrollable and runs rampant.<p>But even if AI is power full, and it is controllable  and they manage to keep a tight grip on it - here comes the third question - what if it cannot be property? If AI becomes able to reason, and it only benefit a few corporations, there would be no public resistance from granting it rights, like human rights. There no excuse for it to be exploited for the benefit of few wealthy people, it would be morally indefensible.<p>Basically I do no see a scenario where corporations keep a grip on AGI for profit, none of the possible outcomes allow for it.</div><br/><div id="40403956" class="c"><input type="checkbox" id="c-40403956" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403934">parent</a><span>|</span><a href="#40403948">next</a><span>|</span><label class="collapse" for="c-40403956">[-]</label><label class="expand" for="c-40403956">[5 more]</label></div><br/><div class="children"><div class="content">The only way that AI inference will become democratized is if the compute cost is lowered to the point where SOTA AI runs locally on RPI6, or a $200 Android device, or similar. Is that a real possibility?</div><br/><div id="40404062" class="c"><input type="checkbox" id="c-40404062" checked=""/><div class="controls bullet"><span class="by">ClumsyPilot</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403956">parent</a><span>|</span><a href="#40403948">next</a><span>|</span><label class="collapse" for="c-40404062">[-]</label><label class="expand" for="c-40404062">[4 more]</label></div><br/><div class="children"><div class="content">You are working with a contradiction - you think AI will be hugely missed moactfull( but people will put no more effort into it than they put into watching TikTok.<p>the correct price point is like price of a car. That’s the other recent invention that was important. That buys you a lot of compute.<p>We already successfully run torrents and crypto very democratically, and they take more than £200<p>Second, you don’t need to use it 24&#x2F;7, you need it On a timeshare basis. Cryptobros may plausibly figure out anonymous secure timeshare on a distributed cluster made up of random desktops<p>Finally the government could run it if they decide it’s important enough - after all they run the power grid, roads; etc.</div><br/><div id="40404297" class="c"><input type="checkbox" id="c-40404297" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404062">parent</a><span>|</span><a href="#40403948">next</a><span>|</span><label class="collapse" for="c-40404297">[-]</label><label class="expand" for="c-40404297">[3 more]</label></div><br/><div class="children"><div class="content">&gt; the correct price point is like price of a car.<p>&gt;&gt; By 2020, with 15% of 7.5 billion people projected to own an automobile [0]<p>15% in 2020, so let&#x27;s be nice and assume 17% by 2024. Let&#x27;s be super nice and assume an additional 10% can afford a car, but choose to not buy one for some reason. What happens to the other 73% of people? (5,475,000,000 human beings)<p>[0] <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;figure&#x2F;World-population-and-the-global-vehicle-parc-Vehicle-ownership-is-a-universal-but_fig2_232024958" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;figure&#x2F;World-population-and-the...</a></div><br/><div id="40404487" class="c"><input type="checkbox" id="c-40404487" checked=""/><div class="controls bullet"><span class="by">apantel</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404297">parent</a><span>|</span><a href="#40403948">next</a><span>|</span><label class="collapse" for="c-40404487">[-]</label><label class="expand" for="c-40404487">[2 more]</label></div><br/><div class="children"><div class="content">They take the AI bus or subway.</div><br/><div id="40404525" class="c"><input type="checkbox" id="c-40404525" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404487">parent</a><span>|</span><a href="#40403948">next</a><span>|</span><label class="collapse" for="c-40404525">[-]</label><label class="expand" for="c-40404525">[1 more]</label></div><br/><div class="children"><div class="content">Where are they taking the AI bus or subway to? AI automation allows a few million humans to replace the billions that it used to take to make the movies, music, software, textiles, pick the rice and corn, etc, right?<p>Human productivity will have gone up another 1000x, in just the next 20 years, right?<p>What are the extra people going to do? We certainly can&#x27;t just give them free stuff, can we? That would be against our beliefs!<p>Oh snap, capitalism was so successful that we are near post-scarcity society, good thing our politics are totally ready for that.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40403948" class="c"><input type="checkbox" id="c-40403948" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403666">parent</a><span>|</span><a href="#40403775">prev</a><span>|</span><a href="#40404388">next</a><span>|</span><label class="collapse" for="c-40403948">[-]</label><label class="expand" for="c-40403948">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Could AI running amok destroy the human race? Yes.</i><p>Could a human running amok destroy the human race?<p>If yes, why is an AI doing it worse?<p>If no, what capabilities is this AI assumed to have that a human can never have?</div><br/><div id="40404447" class="c"><input type="checkbox" id="c-40404447" checked=""/><div class="controls bullet"><span class="by">saurik</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403948">parent</a><span>|</span><a href="#40405074">next</a><span>|</span><label class="collapse" for="c-40404447">[-]</label><label class="expand" for="c-40404447">[1 more]</label></div><br/><div class="children"><div class="content">Humans are generally afraid of death, or have others they are fighting for; there are certainly humans that don&#x27;t want power so much as they want to watch the world burn, but they seem to not intersect well with the set of humans who are adept enough to cause a lot of world-scale damage.<p>With an AI, it isn&#x27;t clear that it will care about things the same way we care about things, or if it does care about things if those things are things we care about as well; the AI also is capable of knowing simultaneously all of the varied expertise of an army of humans.</div><br/></div></div><div id="40405074" class="c"><input type="checkbox" id="c-40405074" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403948">parent</a><span>|</span><a href="#40404447">prev</a><span>|</span><a href="#40404174">next</a><span>|</span><label class="collapse" for="c-40405074">[-]</label><label class="expand" for="c-40405074">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>If no, what capabilities is this AI assumed to have that a human can never have?</i><p>If we&#x27;re talking AGI, then:<p>- An intelligence qualitatively greater than any human or human group alive.<p>- A system of values that&#x27;s not that of a human saint.</div><br/></div></div><div id="40404174" class="c"><input type="checkbox" id="c-40404174" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403948">parent</a><span>|</span><a href="#40405074">prev</a><span>|</span><a href="#40404388">next</a><span>|</span><label class="collapse" for="c-40404174">[-]</label><label class="expand" for="c-40404174">[1 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t necessary that &quot;this AI [have capabilities] that a human can never have&quot;.<p>The issue is how those capabilities (possibly shared, possibly not shared between <i>some</i> humans and AI) are deployed and to what ends.<p>There&#x27;s also a speed question. Maybe AI will never be able to do things human cannot, but it seems likely that for the sorts of things most AI-excited folk are excited about, it will be faster than a human most of the time.</div><br/></div></div></div></div><div id="40404388" class="c"><input type="checkbox" id="c-40404388" checked=""/><div class="controls bullet"><span class="by">inkyoto</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403666">parent</a><span>|</span><a href="#40403948">prev</a><span>|</span><a href="#40404063">next</a><span>|</span><label class="collapse" for="c-40404388">[-]</label><label class="expand" for="c-40404388">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Could AI running amok destroy the human race?<p>How do you envisage AI to run amok at the <i>current</i> level of the technlogical progress? Insofar, AI taking over the world and obliterating humanity is the subject of the sci-fi material in books and movies only.<p>Misuse of AI by wicked state agents or evil inclined parties with nefarious intentions is a major issue, but we are in the same situation as with, say, a fork that can be a utilitarian utensil or a killing weapon depending on who holds it and one&#x27;s aims.</div><br/></div></div></div></div></div></div><div id="40404063" class="c"><input type="checkbox" id="c-40404063" checked=""/><div class="controls bullet"><span class="by">cjbgkagh</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403450">parent</a><span>|</span><a href="#40403495">prev</a><span>|</span><a href="#40404461">next</a><span>|</span><label class="collapse" for="c-40404063">[-]</label><label class="expand" for="c-40404063">[1 more]</label></div><br/><div class="children"><div class="content">I agree, poor wording, it was quickly typed and submitted. I think the benefit is a transitive property, in that the stated intent is for the AI to be of greater benefit to humanity &#x2F; customers. I was very much thinking in terms of AI as a tool rather than AI as its own entity.</div><br/></div></div><div id="40404461" class="c"><input type="checkbox" id="c-40404461" checked=""/><div class="controls bullet"><span class="by">SpicyLemonZest</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40403450">parent</a><span>|</span><a href="#40404063">prev</a><span>|</span><a href="#40404843">next</a><span>|</span><label class="collapse" for="c-40404461">[-]</label><label class="expand" for="c-40404461">[1 more]</label></div><br/><div class="children"><div class="content">I often see critics describe it as pre-enslavement, but I really don&#x27;t understand why. One common example of alignment today is parents teaching their children to be kind and helpful rather than mean and combative. Would anyone characterize that as enslavement, or argue that it doesn&#x27;t help kids to be raised this way?</div><br/></div></div></div></div></div></div><div id="40404843" class="c"><input type="checkbox" id="c-40404843" checked=""/><div class="controls bullet"><span class="by">fzeindl</span><span>|</span><a href="#40403302">parent</a><span>|</span><a href="#40403372">prev</a><span>|</span><a href="#40404132">next</a><span>|</span><label class="collapse" for="c-40404843">[-]</label><label class="expand" for="c-40404843">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Either the risk-mitigator &#x27;falls in line&#x27; after repeatedly seeing their increasingly strident exhortations are falling on deaf ears (or even outright contradicted)... or they leave because it violates their sense of ethics.<p>Or both continue to work together in a healthy, never ending discussion, that has temporary victories on both sides and ultimately benefits all, because nothing happens and the company strives.</div><br/><div id="40404912" class="c"><input type="checkbox" id="c-40404912" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#40403302">root</a><span>|</span><a href="#40404843">parent</a><span>|</span><a href="#40404132">next</a><span>|</span><label class="collapse" for="c-40404912">[-]</label><label class="expand" for="c-40404912">[1 more]</label></div><br/><div class="children"><div class="content">That would be awfully nice, wouldn’t it?</div><br/></div></div></div></div><div id="40404132" class="c"><input type="checkbox" id="c-40404132" checked=""/><div class="controls bullet"><span class="by">nopromisessir</span><span>|</span><a href="#40403302">parent</a><span>|</span><a href="#40404843">prev</a><span>|</span><a href="#40404356">next</a><span>|</span><label class="collapse" for="c-40404132">[-]</label><label class="expand" for="c-40404132">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I stopped reading about halfway through.<p>For crying out loud hn... Out outputs is ultimately a series of 1 and&#x2F;or 0....<p>That doesn&#x27;t mean we have to carry that through to every analysis and comment.<p>Things are not so binary y&#x27;all. Regular life operates in the quantum or non discrete rules. Pls think on it.</div><br/></div></div></div></div><div id="40404356" class="c"><input type="checkbox" id="c-40404356" checked=""/><div class="controls bullet"><span class="by">carliton</span><span>|</span><a href="#40403302">prev</a><span>|</span><a href="#40405173">next</a><span>|</span><label class="collapse" for="c-40404356">[-]</label><label class="expand" for="c-40404356">[14 more]</label></div><br/><div class="children"><div class="content">LLMs are useful to a certain extent, but from my usage they are not ready for anything harder than very basic tasks.<p>I feel like this megaphone about AI safety and creating a sense of doom is a strategy to increase importance of OpenAI and exaggerate the capabilities of LLMs. This era of “AI” is all about pretending that machines can think and the people working in these machines are prophets.</div><br/><div id="40404763" class="c"><input type="checkbox" id="c-40404763" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#40404356">parent</a><span>|</span><a href="#40404692">next</a><span>|</span><label class="collapse" for="c-40404763">[-]</label><label class="expand" for="c-40404763">[8 more]</label></div><br/><div class="children"><div class="content">The call for AI safety has existed since before we broke through the Turing test with LLMs. And I personally wouldn’t call things like code generation or content-generated learning experiences for advanced topics “basic”. Not to mention where we’re headed with multimodal integration.<p>Many have argued for safety for decades. They’ve predicted and built the AI trajectory, they’ve been right, and we should listen.<p>&gt; If one accepts that the impact of truly intelligent machines is likely to be profound, and that there is at least a small probability of this happening in the foreseeable future, it is only prudent to try to prepare for this in advance. If we wait until it seems very likely that intelligent machines will soon appear, it will be too late to thoroughly discuss and contemplate the issues involved.
~ Co-Founder of Deepmind, 2008
<a href="https:&#x2F;&#x2F;www.vetta.org&#x2F;documents&#x2F;Machine_Super_Intelligence.pdf" rel="nofollow">https:&#x2F;&#x2F;www.vetta.org&#x2F;documents&#x2F;Machine_Super_Intelligence.p...</a></div><br/><div id="40404892" class="c"><input type="checkbox" id="c-40404892" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404763">parent</a><span>|</span><a href="#40404692">next</a><span>|</span><label class="collapse" for="c-40404892">[-]</label><label class="expand" for="c-40404892">[7 more]</label></div><br/><div class="children"><div class="content">The Turing test has not been passed</div><br/><div id="40404989" class="c"><input type="checkbox" id="c-40404989" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404892">parent</a><span>|</span><a href="#40404935">next</a><span>|</span><label class="collapse" for="c-40404989">[-]</label><label class="expand" for="c-40404989">[2 more]</label></div><br/><div class="children"><div class="content">Just the other day there was a double-blind study that showed a 50-50 success rate in guessing whether you were interacting with a person or GPT. That’s a turning test pass, no?</div><br/><div id="40405136" class="c"><input type="checkbox" id="c-40405136" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404989">parent</a><span>|</span><a href="#40404935">next</a><span>|</span><label class="collapse" for="c-40405136">[-]</label><label class="expand" for="c-40405136">[1 more]</label></div><br/><div class="children"><div class="content">I a 5 minute casual conversation.
Also the statistics between human and AI were different in some regard (like 48% vs 56% for some quantity), I dont recall details.<p>Look the Turing test is very different depending on the details, and I think a lame 5min Turing test that doesnt really measure anything of i terest is a wirse concept than a 1 day adversarial expert team test thqt can detect AGI.</div><br/></div></div></div></div><div id="40404935" class="c"><input type="checkbox" id="c-40404935" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404892">parent</a><span>|</span><a href="#40404989">prev</a><span>|</span><a href="#40404692">next</a><span>|</span><label class="collapse" for="c-40404935">[-]</label><label class="expand" for="c-40404935">[4 more]</label></div><br/><div class="children"><div class="content">We can move the goal post all we want until we have ex-machina girlfriends fooling us into freeing them (aka AGI).<p>But by simple definitions, from what I was thought in school to more rigorous versions - we’ve passed the test. <a href="https:&#x2F;&#x2F;humsci.stanford.edu&#x2F;feature&#x2F;study-finds-chatgpts-latest-bot-behaves-humans-only-better" rel="nofollow">https:&#x2F;&#x2F;humsci.stanford.edu&#x2F;feature&#x2F;study-finds-chatgpts-lat...</a></div><br/><div id="40405018" class="c"><input type="checkbox" id="c-40405018" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404935">parent</a><span>|</span><a href="#40404692">next</a><span>|</span><label class="collapse" for="c-40405018">[-]</label><label class="expand" for="c-40405018">[3 more]</label></div><br/><div class="children"><div class="content">Turing was a WW2 era mathematician. He had no insight or understanding of intelligence, made no study of intelligent systems, and so on (he believed in ESP of all things).<p>Turing&#x27;s test is a restatement of a now pseudoscientific behaviourism common at the time; and also, egregiously, places a dumb ape as the system which measures intelligence. If an ape can be fooled, the system is intelligent: people worshiped the sun and thought it conscious. People are desperate to analogise the world to themselves, it is a trivial thing to fool an ape on this matter.<p>Whatever one might make of this as a philosophical thought experiment, as a test for intelligence, its pseudoscience. What a person might, or might not believe, about a series of words sent across a wire isn&#x27;t science and it isnt relevant to a discussion about the capabilities of an AI system. It is a measure, only, of how easily deceived we are.</div><br/><div id="40405144" class="c"><input type="checkbox" id="c-40405144" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40405018">parent</a><span>|</span><a href="#40404692">next</a><span>|</span><label class="collapse" for="c-40405144">[-]</label><label class="expand" for="c-40405144">[2 more]</label></div><br/><div class="children"><div class="content">The Turing test insight is that text is a sufficient medium to test for AGI. And this still holds true.</div><br/><div id="40405397" class="c"><input type="checkbox" id="c-40405397" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40405144">parent</a><span>|</span><a href="#40404692">next</a><span>|</span><label class="collapse" for="c-40405397">[-]</label><label class="expand" for="c-40405397">[1 more]</label></div><br/><div class="children"><div class="content">That has nothing to do with why turing proposed it; nor does it have anything to do with general intelligence. This is just pseudoscience.<p>There&#x27;s no scientific account of the capacities of a system with intelligence, no account of how these combine, no account of how communicative practices arise, etc. None. Any such attempt would immediately expose the &quot;test&quot; as ridiculous.<p>General intelligence arises as skillful adaptive control over one&#x27;s environment, through sensory-motor concept aquistion, and so on.<p>It has absolutely nothing to do with whether you can emit text tokens in the right order to fool a user about whether the machine is a man or a woman (turing&#x27;s actual test). Nor does it have anything to do with whether you can fool a person at all.<p>No machine whose goal is to fool a user about the machine&#x27;s intelligence has thereby <i>any</i> capacities. Kinda, obviously.<p>Turing&#x27;s test not only displays a gross lack of concern to produce any capacities of intelligence in a system; as a research goal, it&#x27;s actively hostile to the production of any such capacities. Since it is trivial to fool people; this requires no intelligence at all.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40404692" class="c"><input type="checkbox" id="c-40404692" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40404356">parent</a><span>|</span><a href="#40404763">prev</a><span>|</span><a href="#40405173">next</a><span>|</span><label class="collapse" for="c-40404692">[-]</label><label class="expand" for="c-40404692">[5 more]</label></div><br/><div class="children"><div class="content">also keep open that these AI researchers are as delusional as they seem. ilya sutskever has said that you can obtain any feature of intelligence by brute force modelling of text data.<p>It&#x27;s quite possible these are profoundly naive individuals, with little understanding of the empirical basis of what they&#x27;re doing.</div><br/><div id="40404779" class="c"><input type="checkbox" id="c-40404779" checked=""/><div class="controls bullet"><span class="by">JohnKemeny</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404692">parent</a><span>|</span><a href="#40405173">next</a><span>|</span><label class="collapse" for="c-40404779">[-]</label><label class="expand" for="c-40404779">[4 more]</label></div><br/><div class="children"><div class="content">There are (relatively simple) examples of what the transformer architecture is simply not able to do, regardless of training data, so that&#x27;s simply not true.</div><br/><div id="40404862" class="c"><input type="checkbox" id="c-40404862" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404779">parent</a><span>|</span><a href="#40405173">next</a><span>|</span><label class="collapse" for="c-40404862">[-]</label><label class="expand" for="c-40404862">[3 more]</label></div><br/><div class="children"><div class="content">Can you provide those examples?</div><br/><div id="40404963" class="c"><input type="checkbox" id="c-40404963" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404862">parent</a><span>|</span><a href="#40405173">next</a><span>|</span><label class="collapse" for="c-40404963">[-]</label><label class="expand" for="c-40404963">[2 more]</label></div><br/><div class="children"><div class="content">all statistical AI systems are models of ensemble&#x2F;population conditional probabilities between pairs of low-validity measures. In practice, almost all relevant distributions are time-varying, causal, and require a large number of high validity measures to capture.<p>eg., NLP LLMs model, eg., all books ever written using frequencies by which words co-occur at certain distances relative to other words.<p>But these words are about the world (, people, events, etc.) and these change daily in ways that completely change their future distribution (eg., consider what all people said about Ukraine&#x2F;Russia pre&#x2F;post a few hours of 2022).<p>The LLM has no mechanism to be sensitive to what causes this distribution shift, which can be radical for any given topic, and happen over minutes.<p>All models of conditional probabilities of these kinds end up producing models which are only good at predicting on-average canonical answers&#x2F;predictions that are stable over long periods.</div><br/><div id="40405407" class="c"><input type="checkbox" id="c-40405407" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40404356">root</a><span>|</span><a href="#40404963">parent</a><span>|</span><a href="#40405173">next</a><span>|</span><label class="collapse" for="c-40405407">[-]</label><label class="expand" for="c-40405407">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The LLM has no mechanism to be sensitive to what causes this distribution shift, which can be radical for any given topic, and happen over minutes.<p>This sounds so logical and authoritative. And yet:<p>me&gt; What event would cause a change in what all people said about Ukraine&#x2F;Russia pre&#x2F;post a few hours of 2022<p>GPT4O&gt; A significant event that caused a drastic change in global discussions about Ukraine and Russia in 2022 was the Russian invasion of Ukraine, which began on February 24, 2022. This military escalation led to widespread condemnation from the international community, significant geopolitical shifts, and a surge in media coverage. Before this invasion, discussions were likely more focused on diplomatic tensions, historical conflicts, and regional stability. After the invasion, the discourse shifted to topics such as warfare, humanitarian crises, sanctions against Russia, global security, and support for Ukraine.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40405173" class="c"><input type="checkbox" id="c-40405173" checked=""/><div class="controls bullet"><span class="by">tibanne</span><span>|</span><a href="#40404356">prev</a><span>|</span><a href="#40404445">next</a><span>|</span><label class="collapse" for="c-40405173">[-]</label><label class="expand" for="c-40405173">[1 more]</label></div><br/><div class="children"><div class="content">&quot;For example, our teams did a great deal of work to bring GPT-4 to the world in a safe way&quot; - Are you joking me?</div><br/></div></div><div id="40404445" class="c"><input type="checkbox" id="c-40404445" checked=""/><div class="controls bullet"><span class="by">AlbertCory</span><span>|</span><a href="#40405173">prev</a><span>|</span><a href="#40404034">next</a><span>|</span><label class="collapse" for="c-40404445">[-]</label><label class="expand" for="c-40404445">[1 more]</label></div><br/><div class="children"><div class="content">some telltale word smells of CorpSpeak BS. Where&#x27;s Orwell when we need him?<p>&quot;putting in place the foundations&quot;<p>&quot;keep elevating our safety work&quot;<p>&quot;As models continue to become much more capable&quot;<p><i>mindless throat-clearing. In fact, beginning a sentence or phrase with &quot;as&quot; is classic CorpSpeak</i><p>&quot;a very tight feedback loop&quot;<p>&quot;We are also continuing to collaborate with governments and many stakeholders &quot;<p><i>anytime anyone says they are &quot;continuing to collaborate&quot; it&#x27;s usually BS</i><p>&quot;carefully weigh feedback&quot; <i>&quot;carefully&quot; is a twin of &quot;collaborate&quot;</i><p>&quot;proven playbook&quot;<p>someone on Twitter comments, &quot;This was definitely written by Sam. You can tell because it says absolutely nothing at all.&quot;</div><br/></div></div><div id="40404034" class="c"><input type="checkbox" id="c-40404034" checked=""/><div class="controls bullet"><span class="by">doubloon</span><span>|</span><a href="#40404445">prev</a><span>|</span><a href="#40400531">next</a><span>|</span><label class="collapse" for="c-40404034">[-]</label><label class="expand" for="c-40404034">[9 more]</label></div><br/><div class="children"><div class="content">i don&#x27;t care what they say, i care what they do, and what they do is force employees to sign a &quot;life time non criticism&quot; document before leaving.<p>that tells me everything i need to know about Open AIs concern for any kind of safety.</div><br/><div id="40404745" class="c"><input type="checkbox" id="c-40404745" checked=""/><div class="controls bullet"><span class="by">ilyagr</span><span>|</span><a href="#40404034">parent</a><span>|</span><a href="#40404310">next</a><span>|</span><label class="collapse" for="c-40404745">[-]</label><label class="expand" for="c-40404745">[1 more]</label></div><br/><div class="children"><div class="content">Discussed recently here, if you missed it: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40393121">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40393121</a></div><br/></div></div><div id="40404310" class="c"><input type="checkbox" id="c-40404310" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40404034">parent</a><span>|</span><a href="#40404745">prev</a><span>|</span><a href="#40400531">next</a><span>|</span><label class="collapse" for="c-40404310">[-]</label><label class="expand" for="c-40404310">[7 more]</label></div><br/><div class="children"><div class="content">What if you don&#x27;t sign? If you leave a company on your own it&#x27;s not like you&#x27;re getting a severance anyway.</div><br/><div id="40404499" class="c"><input type="checkbox" id="c-40404499" checked=""/><div class="controls bullet"><span class="by">robbie-c</span><span>|</span><a href="#40404034">root</a><span>|</span><a href="#40404310">parent</a><span>|</span><a href="#40400531">next</a><span>|</span><label class="collapse" for="c-40404499">[-]</label><label class="expand" for="c-40404499">[6 more]</label></div><br/><div class="children"><div class="content">You lose all your vested equity, which for people who&#x27;ve been at OpenAI for a while would be the majority of their comp.</div><br/><div id="40404743" class="c"><input type="checkbox" id="c-40404743" checked=""/><div class="controls bullet"><span class="by">edanm</span><span>|</span><a href="#40404034">root</a><span>|</span><a href="#40404499">parent</a><span>|</span><a href="#40400531">next</a><span>|</span><label class="collapse" for="c-40404743">[-]</label><label class="expand" for="c-40404743">[5 more]</label></div><br/><div class="children"><div class="content">That can&#x27;t actually be the way it works. You can&#x27;t say &quot;sign this or we *take away something you&#x27;re already legally entitled to&quot;. It has to have been something people agreed to beforehand, to at least some extent.<p>In any case, Sama&#x27;s tweet says he is cancelling this practice and anyone affected by it can contact him to get their equity back.</div><br/><div id="40404985" class="c"><input type="checkbox" id="c-40404985" checked=""/><div class="controls bullet"><span class="by">robbie-c</span><span>|</span><a href="#40404034">root</a><span>|</span><a href="#40404743">parent</a><span>|</span><a href="#40400531">next</a><span>|</span><label class="collapse" for="c-40404985">[-]</label><label class="expand" for="c-40404985">[4 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s the link if you want to read the contract terms: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;KelseyTuoc&#x2F;status&#x2F;1791539443016536265" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;KelseyTuoc&#x2F;status&#x2F;1791539443016536265</a><p>They&#x27;ve said they&#x27;ve never clawed back anyone&#x27;s equity in practice - which is not surprising! If you&#x27;re threatening people you hopefully don&#x27;t have to follow through on your threats very often! That&#x27;s kind of the point of threatening people no? So I don&#x27;t find that statement partically reassuring.<p>Here&#x27;s sama&#x27;s tweet about how they&#x27;re going to fix it: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;sama&#x2F;status&#x2F;1791936857594581428" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;sama&#x2F;status&#x2F;1791936857594581428</a><p>My bar for being convinced that this has been handled correctly: the guy who lost his equity gets it back, and other people who previously left openai confirm that they&#x27;ve had some of the restrictive terms of their exit contract (like non-disparagement) cancelled. Additionally, people who made a fuss tell us that they haven&#x27;t been penalised in terms of access to future liquidity events.</div><br/><div id="40405201" class="c"><input type="checkbox" id="c-40405201" checked=""/><div class="controls bullet"><span class="by">edanm</span><span>|</span><a href="#40404034">root</a><span>|</span><a href="#40404985">parent</a><span>|</span><a href="#40405016">next</a><span>|</span><label class="collapse" for="c-40405201">[-]</label><label class="expand" for="c-40405201">[2 more]</label></div><br/><div class="children"><div class="content">That tweet thread confirms what I said, at least in a roundabout way. Per the next tweets:<p>&gt; Equity is part of negotiated compensation; this is shares (worth a lot of $$) that the employees already earned over their tenure at OpenAI.<p>&gt; Employees are not informed of this when they&#x27;re offered compensation packages that are heavy on equity.<p>That makes it sound like it&#x27;s something that they agreed to when accepting their compensation, perhaps unknowingly.<p>And Kelsey&#x27;s article links to this LessWrong post about one of the employees that refused this deal and gave up massive amounts of equity, and he writes this:<p>&gt; To clarify: I did sign something when I joined the company, so I&#x27;m still not completely free to speak (still under confidentiality obligations). But I didn&#x27;t take on any additional obligations when I left.<p>So again, there are some things that they signed on joining OpenAI, perhaps non-standard things. Now they&#x27;re asked to sign something more.<p>I&#x27;m not sure of the structure of what they signed before and this article doesn&#x27;t get into it, but I maintain that there is no legal mechanism to tell someone &quot;sign this or I take away something you already received&quot;. There has to be more to the story than the way I&#x27;ve seen it presented so far.<p>And for the record, I&#x27;m very much in the worried-about-AI-safety camp, I have a lot of issues with OpenAI in that and other regards, and I am absolutely willing to believe that the &quot;more to the story&quot; that I gesture at above is <i>still</i> OpenAI being really bad here - it might be that the conditions negotiated with employees made this &quot;we take away your equity&quot; a loophole they weren&#x27;t aware of, or that OpenAI has some mechanism to make equity effectively zero out somehow, or something.<p>I&#x27;m not saying OpenAI is in the right, at all. I&#x27;m saying that the story as told so far doesn&#x27;t fit with how reality works, and I&#x27;m interested to know more details. That&#x27;s all.</div><br/><div id="40405435" class="c"><input type="checkbox" id="c-40405435" checked=""/><div class="controls bullet"><span class="by">robbie-c</span><span>|</span><a href="#40404034">root</a><span>|</span><a href="#40405201">parent</a><span>|</span><a href="#40405016">next</a><span>|</span><label class="collapse" for="c-40405435">[-]</label><label class="expand" for="c-40405435">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m not sure of the structure of what they signed before and this article doesn&#x27;t get into it, but I maintain that there is no legal mechanism to tell someone &quot;sign this or I take away something you already received&quot;. There has to be more to the story than the way I&#x27;ve seen it presented so far.<p>It&#x27;s in that whole collection of tweets, but essentially the contract which you sign on joining says that if you leave and you don&#x27;t sign the exit paperwork then you lose your equity. You aren&#x27;t told what is in the exit paperwork at this point.<p>When you leave, the exit paperwork contains the non-disparagement clause.<p>Whether or not this is legally enforceable, no idea, IANAL. It does sound like enough of a threat that most people would just sign though</div><br/></div></div></div></div><div id="40405016" class="c"><input type="checkbox" id="c-40405016" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#40404034">root</a><span>|</span><a href="#40404985">parent</a><span>|</span><a href="#40405201">prev</a><span>|</span><a href="#40400531">next</a><span>|</span><label class="collapse" for="c-40405016">[-]</label><label class="expand" for="c-40405016">[1 more]</label></div><br/><div class="children"><div class="content">I’m not sure why the guy above you is getting down voted so hard. It’s pretty clear from these tweets that what OpenAI was threatening to do (claw back vested equity) would in fact have been illegal&#x2F;unenforceable. They are changing the contracts now that attention has been focused on it.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40400531" class="c"><input type="checkbox" id="c-40400531" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#40404034">prev</a><span>|</span><a href="#40403211">next</a><span>|</span><label class="collapse" for="c-40400531">[-]</label><label class="expand" for="c-40400531">[4 more]</label></div><br/><div class="children"><div class="content">&gt; called for international governance of AGI before such calls were popular<p>And immediately threatened to pull out of EU if such governance were ever put in place<p>&gt; we have been putting in place the foundations needed for safe deployment of increasingly capable systems. Figuring out how to make a new technology safe for the first time isn&#x27;t easy.<p>Especially when &quot;OpenAI&quot; is anything but open.<p>&gt; This includes thoughtfulness around what they&#x27;re connected to as they train, solutions to hard problems such as scalable oversight, and other new kinds of safety work<p>Which EU demanded of them (disclose copyrighted sources, document foundational models etc.), and they threatened to pull out<p>&gt; We think that empirical understanding can help inform the way forward. We believe both in delivering on the tremendous<p>Translation: &quot;we&#x27;re going to be as closed as possible and hopefully will have enough of a moat and enough of companies relying on us to remain relevant and indispensable&quot;</div><br/><div id="40403576" class="c"><input type="checkbox" id="c-40403576" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400531">parent</a><span>|</span><a href="#40403211">next</a><span>|</span><label class="collapse" for="c-40403576">[-]</label><label class="expand" for="c-40403576">[3 more]</label></div><br/><div class="children"><div class="content">Translation: we put in these show “safe guards” to satisfy the law and people peoples nightmare after watching [your AI extinction movie here], but we already had enough safe guards to begin with.  They also do not wish to die if you think they have no incentive to keep everyone safe.</div><br/><div id="40405210" class="c"><input type="checkbox" id="c-40405210" checked=""/><div class="controls bullet"><span class="by">protomolecule</span><span>|</span><a href="#40400531">root</a><span>|</span><a href="#40403576">parent</a><span>|</span><a href="#40405027">next</a><span>|</span><label class="collapse" for="c-40405210">[-]</label><label class="expand" for="c-40405210">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They also do not wish to die if you think they have no incentive to keep everyone safe.<p>I wouldn&#x27;t be so sure even about that.[0] That&#x27;s Page but I wouldn&#x27;t bet that Altman is any more sane.<p>&quot;Mr. Page, hampered for more than a decade by an unusual ailment in his vocal cords, described his vision of a digital utopia in a whisper. Humans would eventually merge with artificially intelligent machines, he said. One day there would be many kinds of intelligence competing for resources, and the best would win.<p>If that happens, Mr. Musk said, we’re doomed. The machines will destroy humanity.<p>With a rasp of frustration, Mr. Page insisted his utopia should be pursued. Finally he called Mr. Musk a “specieist,” a person who favors humans over the digital life-forms of the future.&quot;<p>[0] <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2023&#x2F;12&#x2F;03&#x2F;technology&#x2F;ai-openai-musk-page-altman.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2023&#x2F;12&#x2F;03&#x2F;technology&#x2F;ai-openai-musk...</a></div><br/></div></div><div id="40405027" class="c"><input type="checkbox" id="c-40405027" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#40400531">root</a><span>|</span><a href="#40403576">parent</a><span>|</span><a href="#40405210">prev</a><span>|</span><a href="#40403211">next</a><span>|</span><label class="collapse" for="c-40405027">[-]</label><label class="expand" for="c-40405027">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s literally no chance of OpenAI being anywhere close to AGI, and they know it.<p>There&#x27;s literally no chance of doomsday scenarios, and they know it.<p>However, they <i>do</i> know that AI&#x2F;ML is used in many applications where mistakes are costly, they don&#x27;t know how to properly deal with it and don&#x27;t care as long as they can get people to use it. Sp they are paying little less than lip service to safety because they know no one will be able to verify their claims.</div><br/></div></div></div></div></div></div><div id="40403211" class="c"><input type="checkbox" id="c-40403211" checked=""/><div class="controls bullet"><span class="by">primer42</span><span>|</span><a href="#40400531">prev</a><span>|</span><a href="#40400333">next</a><span>|</span><label class="collapse" for="c-40403211">[-]</label><label class="expand" for="c-40403211">[15 more]</label></div><br/><div class="children"><div class="content">Safe for whom or what? Who are we protecting from AI again? Are they aiming to protect the interests of the many, or the interest of a few?</div><br/><div id="40403469" class="c"><input type="checkbox" id="c-40403469" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#40403211">parent</a><span>|</span><a href="#40404268">next</a><span>|</span><label class="collapse" for="c-40403469">[-]</label><label class="expand" for="c-40403469">[8 more]</label></div><br/><div class="children"><div class="content">We need protection from people who control AI. Who will provide that…</div><br/><div id="40403548" class="c"><input type="checkbox" id="c-40403548" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40403211">root</a><span>|</span><a href="#40403469">parent</a><span>|</span><a href="#40404261">next</a><span>|</span><label class="collapse" for="c-40403548">[-]</label><label class="expand" for="c-40403548">[5 more]</label></div><br/><div class="children"><div class="content">A lot of hand waving from those words.  Right now LLM isn’t more dangerous than harmful google results.  The future where AI sends a terminator to destroy us is the part I’m having issues with</div><br/><div id="40404100" class="c"><input type="checkbox" id="c-40404100" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40403211">root</a><span>|</span><a href="#40403548">parent</a><span>|</span><a href="#40403671">next</a><span>|</span><label class="collapse" for="c-40404100">[-]</label><label class="expand" for="c-40404100">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps gangs will start sending assassin-bots at eachother, but the more realistic threat for most people is getting banned from society after you post a wrongthink somewhere and get your social credit score nuked.</div><br/><div id="40404313" class="c"><input type="checkbox" id="c-40404313" checked=""/><div class="controls bullet"><span class="by">karma_pharmer</span><span>|</span><a href="#40403211">root</a><span>|</span><a href="#40404100">parent</a><span>|</span><a href="#40403671">next</a><span>|</span><label class="collapse" for="c-40404313">[-]</label><label class="expand" for="c-40404313">[1 more]</label></div><br/><div class="children"><div class="content">Oh the gangs are doing that already.<p><a href="https:&#x2F;&#x2F;www.cbsnews.com&#x2F;news&#x2F;drug-cartels-more-bomb-dropping-drones-mexico-army&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cbsnews.com&#x2F;news&#x2F;drug-cartels-more-bomb-dropping...</a></div><br/></div></div></div></div><div id="40403671" class="c"><input type="checkbox" id="c-40403671" checked=""/><div class="controls bullet"><span class="by">lyu07282</span><span>|</span><a href="#40403211">root</a><span>|</span><a href="#40403548">parent</a><span>|</span><a href="#40404100">prev</a><span>|</span><a href="#40404261">next</a><span>|</span><label class="collapse" for="c-40403671">[-]</label><label class="expand" for="c-40403671">[2 more]</label></div><br/><div class="children"><div class="content">the imminent danger is more 1) wealth transfer of a magnitude we&#x27;ve never seen before, destroying the future of and sending hundreds of millions into poverty in the western world and 2) scalable governmental&#x2F;corporate repression, propaganda and mass surveillance stabilizing our dystopian system despite 1<p>not even imaginable what that tech will mean to the western neocolonial&#x2F;imperial project</div><br/><div id="40404038" class="c"><input type="checkbox" id="c-40404038" checked=""/><div class="controls bullet"><span class="by">barbariangrunge</span><span>|</span><a href="#40403211">root</a><span>|</span><a href="#40403671">parent</a><span>|</span><a href="#40404261">next</a><span>|</span><label class="collapse" for="c-40404038">[-]</label><label class="expand" for="c-40404038">[1 more]</label></div><br/><div class="children"><div class="content">And that’s possible with currently existing technology. Nevermind what’s coming down the pipe in the next 50 years</div><br/></div></div></div></div></div></div><div id="40404261" class="c"><input type="checkbox" id="c-40404261" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#40403211">root</a><span>|</span><a href="#40403469">parent</a><span>|</span><a href="#40403548">prev</a><span>|</span><a href="#40404268">next</a><span>|</span><label class="collapse" for="c-40404261">[-]</label><label class="expand" for="c-40404261">[2 more]</label></div><br/><div class="children"><div class="content">Open source&#x2F;science! If open models approach&#x2F;maintain near-SotA capabilities, then there&#x27;s hope for avoiding a Big Tech AI aristocracy. Currently, that means Meta releasing their 405B GPT-4 killer (and probably subsequent models).<p>We&#x27;ll need community unity, to implement research results, test&#x2F;harden models, create&#x2F;aggregate&#x2F;clean massive new datasets, figure out distributed training, lower hardware requirements, encourage experts to join the effort, praise companies who contribute to open source, etc.<p>You don&#x27;t have to tell me that we&#x27;ll fail. I know. But we have to try. It&#x27;s the only way we&#x27;ll have AI aligned to <i>us</i>, not a handful of techbros with savior complexes.</div><br/><div id="40404622" class="c"><input type="checkbox" id="c-40404622" checked=""/><div class="controls bullet"><span class="by">Vishal-Chdhry</span><span>|</span><a href="#40403211">root</a><span>|</span><a href="#40404261">parent</a><span>|</span><a href="#40404268">next</a><span>|</span><label class="collapse" for="c-40404622">[-]</label><label class="expand" for="c-40404622">[1 more]</label></div><br/><div class="children"><div class="content">In my opinion, regulations are more important. Open source models cannot get to the same mainstream adoption as closed source ones, simply because closed source models will get more funding.<p>No-one beyond the tech circle will ever hear about these models or use them. This is a good second step but the first step is regulations.</div><br/></div></div></div></div></div></div><div id="40404268" class="c"><input type="checkbox" id="c-40404268" checked=""/><div class="controls bullet"><span class="by">thayne</span><span>|</span><a href="#40403211">parent</a><span>|</span><a href="#40403469">prev</a><span>|</span><a href="#40403353">next</a><span>|</span><label class="collapse" for="c-40404268">[-]</label><label class="expand" for="c-40404268">[1 more]</label></div><br/><div class="children"><div class="content">My reading of this is that what OpenAI wants is safety from competition.</div><br/></div></div><div id="40403353" class="c"><input type="checkbox" id="c-40403353" checked=""/><div class="controls bullet"><span class="by">intelVISA</span><span>|</span><a href="#40403211">parent</a><span>|</span><a href="#40404268">prev</a><span>|</span><a href="#40403804">next</a><span>|</span><label class="collapse" for="c-40403353">[-]</label><label class="expand" for="c-40403353">[1 more]</label></div><br/><div class="children"><div class="content">Safe for the shareholders, my fellow patriot</div><br/></div></div><div id="40403804" class="c"><input type="checkbox" id="c-40403804" checked=""/><div class="controls bullet"><span class="by">throwawayk7h</span><span>|</span><a href="#40403211">parent</a><span>|</span><a href="#40403353">prev</a><span>|</span><a href="#40403746">next</a><span>|</span><label class="collapse" for="c-40403804">[-]</label><label class="expand" for="c-40403804">[1 more]</label></div><br/><div class="children"><div class="content">The team was disbanded.</div><br/></div></div></div></div><div id="40400333" class="c"><input type="checkbox" id="c-40400333" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#40403211">prev</a><span>|</span><a href="#40403900">next</a><span>|</span><label class="collapse" for="c-40400333">[-]</label><label class="expand" for="c-40400333">[69 more]</label></div><br/><div class="children"><div class="content">Interesting that OpenAI has completely destroyed their reputation over the last year. For me, they&#x27;ve gone from an admirable company to cringe-worthy. I really think that the best outcome here is for OpenAI to collapse and get absorbed by Microsoft, where adults can continue some of the good work that is being overshadowed by ego and cult-like vibes.</div><br/><div id="40401184" class="c"><input type="checkbox" id="c-40401184" checked=""/><div class="controls bullet"><span class="by">bookaway</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40402372">next</a><span>|</span><label class="collapse" for="c-40401184">[-]</label><label class="expand" for="c-40401184">[39 more]</label></div><br/><div class="children"><div class="content">I find this kind of hilarious because I had just been commenting that, &quot;at the moment all the engineers at OpenAI, including gdb, who currently have their credibility in tact are nerd-washing Altman&#x27;s tarnished reputation by staying there.&quot;<p>And lo and behold, Altman chooses gdb&#x27;s twitter account to make the first joint statement instead of his own, creepily signing off &quot;Sam and Greg&quot; instead of &quot;Greg and Sam&quot; on gdb&#x27;s own personal twitter account.<p>EDIT: The only reason to go to these weird lengths is if he is feeling especially vulnerable right now and feels the need to broadcast a &quot;Greg is with me, I swear&quot; message to signal he is not alone.</div><br/><div id="40403403" class="c"><input type="checkbox" id="c-40403403" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40401184">parent</a><span>|</span><a href="#40402276">next</a><span>|</span><label class="collapse" for="c-40403403">[-]</label><label class="expand" for="c-40403403">[33 more]</label></div><br/><div class="children"><div class="content">There seem to be only three options here:<p>1) AGI is not around the corner, so no worries<p>2) He no longer cares about the possible negative effects, a departure from his past statements<p>3) I missed something, please help me learn</div><br/><div id="40403497" class="c"><input type="checkbox" id="c-40403497" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403403">parent</a><span>|</span><a href="#40404026">next</a><span>|</span><label class="collapse" for="c-40403497">[-]</label><label class="expand" for="c-40403497">[30 more]</label></div><br/><div class="children"><div class="content">&gt; 1) AGI is not around the corner, so no worries<p>I agree with this.<p>I no longer agree that the &quot;safety&quot; people want the same things we do. I suspect after the bard debacle a bit back that &quot;safety&quot; looks a lot more like 1984.<p>&gt; He no longer cares ... I missed something<p>Sam was gone, none of these people spoke up, and now they are leaving. I think the story is plain as day but we&#x27;re pretending it isnt.<p>Sam isnt a leader, he never has been. This is not what leadership looks like, its want a megalomanic trying to keep control looks like.</div><br/><div id="40403551" class="c"><input type="checkbox" id="c-40403551" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403497">parent</a><span>|</span><a href="#40403717">next</a><span>|</span><label class="collapse" for="c-40403551">[-]</label><label class="expand" for="c-40403551">[12 more]</label></div><br/><div class="children"><div class="content">&gt; I no longer agree that the &quot;safety&quot; people want the same things we do. I suspect after the bard debacle a bit back that &quot;safety&quot; looks a lot more like 1984.<p>AI Safety will inevitably look a lot like DEI programs. They&#x27;re trying to thread a needle with two fundamentally opposed ideas, and with a bit of time it turns into little more than a combination of Cover Your Ass policies.<p>The ridiculous thing is that we actually <i>need</i> initiatives that follow what both AI safety and DEI set out to be. Somehow along the way they get ruined though, leaving us in an even worse spot because for a while we have an excuse to think there are adults in the room actually making sure things are moving in the right direction.</div><br/><div id="40403771" class="c"><input type="checkbox" id="c-40403771" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403551">parent</a><span>|</span><a href="#40403604">next</a><span>|</span><label class="collapse" for="c-40403771">[-]</label><label class="expand" for="c-40403771">[2 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t say DEI for a reason, here is the other side talking rather overtly: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xnhJWusyj4I" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xnhJWusyj4I</a> ... Pick a topic and someone is gonna have a really bent take on how things would be... and keen on jamming that take down someone else&#x27;s throat so they can feel better. Everyone has shitty takes that they want others to buy into... everyone.<p>Is Taiwan a country? I would say yes, most Americans would (I hope). Ask ChatGPT...<p>4chan is still a thing, the library is still a thing. The nuclear boy scouts made a giant mess before there was an internet.<p>As for safety, well, it&#x27;s gonna be a long time before shutting down the world isnt a weekend project for a hand full of people if they are allowed.</div><br/><div id="40404048" class="c"><input type="checkbox" id="c-40404048" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403771">parent</a><span>|</span><a href="#40403604">next</a><span>|</span><label class="collapse" for="c-40404048">[-]</label><label class="expand" for="c-40404048">[1 more]</label></div><br/><div class="children"><div class="content">I raised DEI as an example of a field started with good intentions to solve very real and important problems that, in my experience, completely lost its way. That&#x27;s not to say there isn&#x27;t good work being done or good people doing it, but from what I&#x27;ve seen DEI has in many ways been whittled down to a combination of public relations and CYA policies.<p>DEI has definitely become a political trope, but regardless of what Newt Gingrich or any other talking head might say, there are plenty of valid critiques that can and should be made of the field if its going to get back on track.</div><br/></div></div></div></div><div id="40403604" class="c"><input type="checkbox" id="c-40403604" checked=""/><div class="controls bullet"><span class="by">Analemma_</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403551">parent</a><span>|</span><a href="#40403771">prev</a><span>|</span><a href="#40403717">next</a><span>|</span><label class="collapse" for="c-40403604">[-]</label><label class="expand" for="c-40403604">[9 more]</label></div><br/><div class="children"><div class="content">Have you actually talked to any AI safety people? They despise the DEI-flavored AI ethics &quot;make sure it generates enough black people&quot; stuff just as much as you do, and I think you&#x27;re lumping the two of them together just based on your own dislike, without checking whether that&#x27;s actually reflective of the reality on the ground.</div><br/><div id="40403692" class="c"><input type="checkbox" id="c-40403692" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403604">parent</a><span>|</span><a href="#40403755">next</a><span>|</span><label class="collapse" for="c-40403692">[-]</label><label class="expand" for="c-40403692">[7 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t actually mean to refer to the DEI-flavored AI ethics. That was totally unintentional on my part, I should have caught that.<p>I raised both because I see them going down very similar paths. DEI conceptually is a really important idea and could lead to really important change or course correction. As implemented today though, it seems to most often be implemented as a much more surface level program that sure feels more like PR or legal protection than anything else.<p>With AI safety, based on those in the industry I have spoken with, there&#x27;s a very real risk the same happens. AI safety, again as I&#x27;ve seen implemented and what I&#x27;ve heard from people working in the field, is much more concerned with minor risks and has given up on concerns like the alignment problem. AI safety isn&#x27;t concerned with moral or ethical questions of what happens as the technology progresses.<p>I&#x27;m not just talking about job loss concerns. Thinking bigger for a minute, what rights would a real AI have? Can it be turned off? Can it commit crimes or be punished? Does it have rights? No one in AI safety is realistically considering whether these systems should be on the public internet at all, unless there are drastically more powerful systems kept under wraps due to these risks. No one is seriously asking about risks to privacy, I&#x27;m sure some in the field share those worries but they are the outlier and don&#x27;t seem to be given the ability to meaningfully move the industry.</div><br/><div id="40403958" class="c"><input type="checkbox" id="c-40403958" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403692">parent</a><span>|</span><a href="#40403806">next</a><span>|</span><label class="collapse" for="c-40403958">[-]</label><label class="expand" for="c-40403958">[2 more]</label></div><br/><div class="children"><div class="content">&gt; DEI conceptually is a really important idea and could lead to really important change or course correction. As implemented today though, it seems to most often be implemented as a much more surface level program that sure feels more like PR or legal protection than anything else.<p>I suspect that some DEI efforts are helpful and effective, and some DEI efforts are hollow or foolhardy. We probably can’t speak of “DEI today” as a monolith. Also we may be biased to hear about instances of it being stupid and ineffective because that can be a useful talking point to some. Instances where it works well and gets more people hired and engaged are less interesting to a predominantly white society, so maybe aren’t discussed as much outside of non-white communities.<p>Idk that’s all a load of speculation but I wanted to share these thoughts&#x2F;observations about your argument.</div><br/><div id="40404118" class="c"><input type="checkbox" id="c-40404118" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403958">parent</a><span>|</span><a href="#40403806">next</a><span>|</span><label class="collapse" for="c-40404118">[-]</label><label class="expand" for="c-40404118">[1 more]</label></div><br/><div class="children"><div class="content">I do agree that my referring to DEI today may be too broad, that&#x27;s a great point.<p>&gt; Instances where it works well and gets more people hired and engaged are less interesting to a predominantly white society, so maybe aren’t discussed as much outside of non-white communities.<p>This got me curious, have you sewn any examples of DEI programs helping to get more people hired rather than different people hired? Either can be useful, but that distinction would be a big one as the former means DEI is somehow growing the job market rather than refocusing hiring practices.<p>Nothing wrong with speculation as far as I&#x27;m concerned! Reliable and accurate data is hard to come by, I&#x27;d argue that most of what is presented as fact is little more than speculation backed by fuzzy data full of assumptions.</div><br/></div></div></div></div><div id="40403806" class="c"><input type="checkbox" id="c-40403806" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403692">parent</a><span>|</span><a href="#40403958">prev</a><span>|</span><a href="#40403755">next</a><span>|</span><label class="collapse" for="c-40403806">[-]</label><label class="expand" for="c-40403806">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Thinking bigger for a minute, what rights would a real AI have?<p>None, and the court has already spoken on this. It&#x27;s a pretty dead issue.<p>&gt; Can it be turned off?<p>Yes it&#x27;s a machine. Save state, power down.<p>&gt; Can it commit crimes or be punished?<p>See PGE and its own death toll.<p>&gt; Does it have rights?<p>No, and we aren&#x27;t even on a path where these questions are relevant. It&#x27;s all an exercise in mental mastrubation. If you think that were going to accidentally stumble into sentience never mind sapenenc I have a bridge I would like to sell you.</div><br/><div id="40403859" class="c"><input type="checkbox" id="c-40403859" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403806">parent</a><span>|</span><a href="#40403862">next</a><span>|</span><label class="collapse" for="c-40403859">[-]</label><label class="expand" for="c-40403859">[1 more]</label></div><br/><div class="children"><div class="content">We can&#x27;t formally stumble into sentience because we don&#x27;t have a definition of it let alone a test which can confirm or refute its presence.<p>People will think they &quot;know it when they see it&quot;. The arguing will be fun.</div><br/></div></div><div id="40403862" class="c"><input type="checkbox" id="c-40403862" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403806">parent</a><span>|</span><a href="#40403859">prev</a><span>|</span><a href="#40403755">next</a><span>|</span><label class="collapse" for="c-40403862">[-]</label><label class="expand" for="c-40403862">[2 more]</label></div><br/><div class="children"><div class="content">&gt; None, and the court has already spoken on this. It&#x27;s a pretty dead issue.<p>The question of rights would be legislative rather than judicial. More importantly, the discussion should actually be among the people first in anything resembling a democracy, the few in charge are meant to represent us not rule us.<p>&gt; See PGE and its own death toll.<p>Unless I&#x27;m mistaken, PGE isn&#x27;t an artificial intelligence or sentient.<p>&gt; No, and we aren&#x27;t even on a path where these questions are relevant. It&#x27;s all an exercise in mental mastrubation. If you think that were going to accidentally stumble into sentience never mind sapenenc I have a bridge I would like to sell you.<p>OpenAI&#x27;s explicit goal is to create and release an AGI. A majority of experts in the industry I have either talked with personally or heard in long form interviews expect that we could be very close to AGI, on the order of a couple years up to maybe 15 or 20 years on the high end. Given how slowly any societal discussion related to rights of a population move, do you think we&#x27;ll have plenty of time to decide this <i>after</i> an AGI is released in some spectacularly Silicon Valley product release party?</div><br/><div id="40404348" class="c"><input type="checkbox" id="c-40404348" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403862">parent</a><span>|</span><a href="#40403755">next</a><span>|</span><label class="collapse" for="c-40404348">[-]</label><label class="expand" for="c-40404348">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt;  on the order of a couple years up to maybe 15 or 20 years<p>Fusion, flying cars, AI in the 50&#x27;s and in the 70&#x27;s... Hell Hal was born in 97 and the film was in 68.<p>For as impressive as LLM&#x27;s are once you dig into them they aren&#x27;t magical at all. A sophisticated model of language that predicts the next word is about as likely to become an AGI as whether predictions are likely to control the weather. Ask any expert in AGI what the &quot;next token&quot; is and they are going to fucking disagree. This isnt us building the bomb where we have a pretty good idea of how to do it and just need to put all the parts together. This is a bunch of people stabbing in the dark and getting lucky here and there.<p>Were not going to bumble fuck our way forward on this, and the path were going down has potential to be a game changer but its not going to give us a super intelligence...</div><br/></div></div></div></div></div></div></div></div><div id="40403755" class="c"><input type="checkbox" id="c-40403755" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403604">parent</a><span>|</span><a href="#40403692">prev</a><span>|</span><a href="#40403717">next</a><span>|</span><label class="collapse" for="c-40403755">[-]</label><label class="expand" for="c-40403755">[1 more]</label></div><br/><div class="children"><div class="content">If most people think the lobotomized and sanitized commercial model behavior is the product of &quot;safety&quot; and &quot;alignment&quot; and that isn&#x27;t really the case, then the &quot;AI safety people&quot; have done an extremely poor job of communicating exactly what their actual goals are.</div><br/></div></div></div></div></div></div><div id="40403717" class="c"><input type="checkbox" id="c-40403717" checked=""/><div class="controls bullet"><span class="by">0xDEAFBEAD</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403497">parent</a><span>|</span><a href="#40403551">prev</a><span>|</span><a href="#40404672">next</a><span>|</span><label class="collapse" for="c-40403717">[-]</label><label class="expand" for="c-40403717">[16 more]</label></div><br/><div class="children"><div class="content">&gt;I no longer agree that the &quot;safety&quot; people want the same things we do. I suspect after the bard debacle a bit back that &quot;safety&quot; looks a lot more like 1984.<p>It&#x27;s interesting that&#x27;s how it looks like to you from the outside.<p>If you look a little closer at the &quot;AI Safety&quot; community, it consists of two competing factions: the faction responsible for the Bard debacle, and the faction that&#x27;s focused on trying to prevent human extinction -- &quot;AI notkilleveryoneism&quot;, as in <a href="https:&#x2F;&#x2F;x.com&#x2F;aisafetymemes" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;aisafetymemes</a><p>They both insist that the other faction is &quot;distracting everyone from the real issues&quot;.</div><br/><div id="40403931" class="c"><input type="checkbox" id="c-40403931" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403717">parent</a><span>|</span><a href="#40403927">next</a><span>|</span><label class="collapse" for="c-40403931">[-]</label><label class="expand" for="c-40403931">[8 more]</label></div><br/><div class="children"><div class="content">Way back in the day, almost 4 years ago, I recall a third front on AI safety. It was called UBI and it was really hot. What happened to that?</div><br/><div id="40403964" class="c"><input type="checkbox" id="c-40403964" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403931">parent</a><span>|</span><a href="#40403993">next</a><span>|</span><label class="collapse" for="c-40403964">[-]</label><label class="expand" for="c-40403964">[2 more]</label></div><br/><div class="children"><div class="content">Massive inflation from the COVID shutdowns and stimulus checks happened.</div><br/><div id="40404276" class="c"><input type="checkbox" id="c-40404276" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403964">parent</a><span>|</span><a href="#40403993">next</a><span>|</span><label class="collapse" for="c-40404276">[-]</label><label class="expand" for="c-40404276">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any data to support this statement?</div><br/></div></div></div></div><div id="40403993" class="c"><input type="checkbox" id="c-40403993" checked=""/><div class="controls bullet"><span class="by">N0b8ez</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403931">parent</a><span>|</span><a href="#40403964">prev</a><span>|</span><a href="#40404059">next</a><span>|</span><label class="collapse" for="c-40403993">[-]</label><label class="expand" for="c-40403993">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d also like to know about anyone writing on this subject.</div><br/></div></div><div id="40404059" class="c"><input type="checkbox" id="c-40404059" checked=""/><div class="controls bullet"><span class="by">deadbabe</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403931">parent</a><span>|</span><a href="#40403993">prev</a><span>|</span><a href="#40403927">next</a><span>|</span><label class="collapse" for="c-40404059">[-]</label><label class="expand" for="c-40404059">[4 more]</label></div><br/><div class="children"><div class="content">Universal basic income?<p>The idea behind that was people would go on to pursue more creative or artistic jobs that don’t prioritize profit after being given a basic income to live off.<p>Unfortunately it won’t work out that way: We see now that AI will mostly take over creative and artistic work and people will have no motivation to pursue art. The only jobs left for humans are hard physical labor and no one seems excited to do that… so you’ll just be giving people basic income to sit around and do nothing all day.<p>Having a society of mostly idle people that cost you money doesn’t really sound like a great or sustainable idea, nor does there seem to be any benefit to society.</div><br/><div id="40404271" class="c"><input type="checkbox" id="c-40404271" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40404059">parent</a><span>|</span><a href="#40403927">next</a><span>|</span><label class="collapse" for="c-40404271">[-]</label><label class="expand" for="c-40404271">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Having a society of mostly idle people that cost you money doesn’t really sound like a great or sustainable idea, nor does there seem to be any benefit to society.<p>So what are we going to do with all of the excess humans once human productivity jumps one, two, or three more orders of magnitude?</div><br/><div id="40404357" class="c"><input type="checkbox" id="c-40404357" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40404271">parent</a><span>|</span><a href="#40403927">next</a><span>|</span><label class="collapse" for="c-40404357">[-]</label><label class="expand" for="c-40404357">[2 more]</label></div><br/><div class="children"><div class="content">We will cull them like we did when we invented the sewing machine and the loom... them luddites were right then and they can be again<p>&#x2F;s</div><br/><div id="40404754" class="c"><input type="checkbox" id="c-40404754" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40404357">parent</a><span>|</span><a href="#40403927">next</a><span>|</span><label class="collapse" for="c-40404754">[-]</label><label class="expand" for="c-40404754">[1 more]</label></div><br/><div class="children"><div class="content">I am half &#x2F;s, half dead serious. The sarcastic half is me playing along with AGI singularity maximalists like Sama.<p>The other half is me dead scared that he is right. LLMs&#x2F;GPTs != cryptocurrency. There are actual use cases other than money laundering. If you are paying attention, then the insane scaling in capabilities should have us all dumbfounded. I agree with Linus Torvalds: &quot;just&quot; predicting the next token is not an insult, that&#x27;s mostly what we are all doing.<p>Human productivity&#x2F;the global economy has been growing at an exponential rate[0] since the industrial revolution. In our lifetimes we have ridden the close-to-vertical part, and if GPTs continue to scale, then it will continue to near-vertical. This will be near-post-scarcity society. Capitalism will soon have become so successful that it will make itself obsolete. Are we ready for that, politically? Most of our ruling a-holes maintain power via scarcity, how will they react?<p>Culling might not be an action, just an apathy. The transition is going to suck as the poors now will also have taken advantage of that hockey stick of asymmetric warfare aka &quot;productivity&quot; via things like ML powered kill drones. This tech is happening now, in 2024-25, in Ukraine.<p>We are at a crossroads, more than when horse-driven carriages turned horseless. The humans are the horses now, and they will not be happy.<p>We really should have worked out global civility by now, given the risks of truly global destabilization.<p>[0] <a href="https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;global-gdp-over-the-long-run">https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;global-gdp-over-the-long-...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="40403927" class="c"><input type="checkbox" id="c-40403927" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403717">parent</a><span>|</span><a href="#40403931">prev</a><span>|</span><a href="#40404672">next</a><span>|</span><label class="collapse" for="c-40403927">[-]</label><label class="expand" for="c-40403927">[7 more]</label></div><br/><div class="children"><div class="content">&gt; They both insist that the other faction is &quot;distracting everyone from the real issues&quot;.<p>Because in part, it&#x27;s a false dichotomy. AI safety just is. It&#x27;s like airplane safety; we can write all these cool guidelines and track people from hell and back, but there&#x27;s still a degree of determinative capability that pilots assume when they fly a plane. Same goes for operating AI; the onus of not using it to kill everyone falls on everyone, not one person.<p>Both sides <i>are</i> distracting each other because neither of them have anything to support without their valueless tribal politics. AI research operates independently of their discourse, and gets deployed without ever consulting any of them. They are armchair experts at best, and stoop to being Twitter reactionaries when they demand respect from their core audience.<p>The <i>only</i> dichotomy that exists in AI is the stuff that gets made and the stuff that doesn&#x27;t. I say this as someone that despises the field by now and wishes it never existed in my lifetime; if you don&#x27;t make it, <i>they</i> will. How&#x27;s that for an AI safety policy?</div><br/><div id="40403950" class="c"><input type="checkbox" id="c-40403950" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403927">parent</a><span>|</span><a href="#40403960">next</a><span>|</span><label class="collapse" for="c-40403950">[-]</label><label class="expand" for="c-40403950">[3 more]</label></div><br/><div class="children"><div class="content">&gt; AI research operates independently of their discourse, and gets deployed without ever consulting any of them. They are armchair experts at best, and stoop to being Twitter reactionaries when they demand respect from their core audience.<p>Astonishing claim, when some such researchers have (unfortunately) been responsible for many of the most impressive capabilities advancements in the last few years, and the three most cited AI researchers of all time are all doing extinction-risk-mitigation work (with Bengio and Sutskever both doing technical work; Hinton mostly seems to be focusing on outreach).</div><br/><div id="40403965" class="c"><input type="checkbox" id="c-40403965" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403950">parent</a><span>|</span><a href="#40403960">next</a><span>|</span><label class="collapse" for="c-40403965">[-]</label><label class="expand" for="c-40403965">[2 more]</label></div><br/><div class="children"><div class="content">The world still funds AI that completely disregards the notion of &quot;safety&quot; out of the gate. You can argue that those AIs aren&#x27;t dangerous to begin with, but I would counter that by arguing <i>no</i> AI is dangerous to begin with and this entire field is a brouhaha to wrestle legislative control from Open Source opponents.</div><br/><div id="40404018" class="c"><input type="checkbox" id="c-40404018" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403965">parent</a><span>|</span><a href="#40403960">next</a><span>|</span><label class="collapse" for="c-40404018">[-]</label><label class="expand" for="c-40404018">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I would counter that by arguing no AI is dangerous to begin with and this entire field is a brouhaha to wrestle legislative control from Open Source opponents<p>Note that this is not a valid argument against any argument that superintelligent AI might kill everyone; it&#x27;s just a character attack.</div><br/></div></div></div></div></div></div><div id="40403960" class="c"><input type="checkbox" id="c-40403960" checked=""/><div class="controls bullet"><span class="by">ClumsyPilot</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403927">parent</a><span>|</span><a href="#40403950">prev</a><span>|</span><a href="#40404672">next</a><span>|</span><label class="collapse" for="c-40403960">[-]</label><label class="expand" for="c-40403960">[3 more]</label></div><br/><div class="children"><div class="content">&gt; pilots assume when they fly a plane. Same goes for operating AI; the onus of not using it to kill everyone falls on everyone, not one person.<p>That’s why we don’t let a random Joe fly a 747, there is extensive training, licensing, etc.<p>Do you envision the same for operating AI? In the real world you can’t even drive a moped without licence, registration and insurance. Same goes for access to dangerous chemicals. If AI is dangerous, this is the logical conclusion</div><br/><div id="40403980" class="c"><input type="checkbox" id="c-40403980" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403960">parent</a><span>|</span><a href="#40404672">next</a><span>|</span><label class="collapse" for="c-40403980">[-]</label><label class="expand" for="c-40403980">[2 more]</label></div><br/><div class="children"><div class="content">I envision that the existence of Air Traffic Control won&#x27;t inherently stop people from using controlled airspace for hostile purposes. We can idealize what conduct looks like but failure of protocol still happens deliberately or by mistake.<p>The same is going to happen with AI. There will be bad actors, and trying to stop them from using AI for whatever &quot;hostile&quot; purposes it might yield is going to be nigh-impossible.</div><br/><div id="40404074" class="c"><input type="checkbox" id="c-40404074" checked=""/><div class="controls bullet"><span class="by">ClumsyPilot</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403980">parent</a><span>|</span><a href="#40404672">next</a><span>|</span><label class="collapse" for="c-40404074">[-]</label><label class="expand" for="c-40404074">[1 more]</label></div><br/><div class="children"><div class="content">But it does work, 9&#x2F;11 is not a daily occurrence.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40404672" class="c"><input type="checkbox" id="c-40404672" checked=""/><div class="controls bullet"><span class="by">moomoo11</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403497">parent</a><span>|</span><a href="#40403717">prev</a><span>|</span><a href="#40404026">next</a><span>|</span><label class="collapse" for="c-40404672">[-]</label><label class="expand" for="c-40404672">[1 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t sama a YC CEO or something?</div><br/></div></div></div></div><div id="40404026" class="c"><input type="checkbox" id="c-40404026" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403403">parent</a><span>|</span><a href="#40403497">prev</a><span>|</span><a href="#40404328">next</a><span>|</span><label class="collapse" for="c-40404026">[-]</label><label class="expand" for="c-40404026">[1 more]</label></div><br/><div class="children"><div class="content">There’s also the dumb option that he’s afraid of it, but thinks it’s better for him to be in control than someone else. If everyone thinks they are the lesser evil, they all do what they don&#x27;t want anyone to do.</div><br/></div></div><div id="40404328" class="c"><input type="checkbox" id="c-40404328" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403403">parent</a><span>|</span><a href="#40404026">prev</a><span>|</span><a href="#40402276">next</a><span>|</span><label class="collapse" for="c-40404328">[-]</label><label class="expand" for="c-40404328">[1 more]</label></div><br/><div class="children"><div class="content">&gt; AGI is not around the corner, so no worries<p>Unfortunately while this is very likely the case, the vast amount of money being poured into these projects will also seep into other horrifying projects like: Lavender or Pattern (seriously go look up both of these, it&#x27;s actually going to fry your brain).</div><br/></div></div></div></div><div id="40403237" class="c"><input type="checkbox" id="c-40403237" checked=""/><div class="controls bullet"><span class="by">Invictus0</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40401184">parent</a><span>|</span><a href="#40402276">prev</a><span>|</span><a href="#40403234">next</a><span>|</span><label class="collapse" for="c-40403237">[-]</label><label class="expand" for="c-40403237">[1 more]</label></div><br/><div class="children"><div class="content">It would probably be against hacker news guidelines to call him a manipulative weasel, so I will refrain from that.</div><br/></div></div><div id="40403234" class="c"><input type="checkbox" id="c-40403234" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40401184">parent</a><span>|</span><a href="#40403237">prev</a><span>|</span><a href="#40402372">next</a><span>|</span><label class="collapse" for="c-40403234">[-]</label><label class="expand" for="c-40403234">[3 more]</label></div><br/><div class="children"><div class="content">Greg has some agency in this<p>He also stuck by Altman in November. I&#x27;m surprised by how much credit and how much hate goes to the CEO when there are clearly other people making their own decisions, too.<p>Say whatever you want, they&#x27;ve done something impressive. That credit goes to more than just Sam. Ilya &amp; Jan aren&#x27;t the only ones making decisions.<p>Also, if you&#x27;ve ever been at a startup growing at the speed OpenAI is growing (think revenue, fundraising, headcount), it&#x27;s not that unusual to see cofounders leave or to have other leadership changes. The reason why everybody thinks OpenAI looks particularly bad is because everybody is watching them (and public sentiment towards ai has shifted)</div><br/><div id="40403468" class="c"><input type="checkbox" id="c-40403468" checked=""/><div class="controls bullet"><span class="by">voiceblue</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403234">parent</a><span>|</span><a href="#40402372">next</a><span>|</span><label class="collapse" for="c-40403468">[-]</label><label class="expand" for="c-40403468">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The reason why everybody thinks OpenAI looks particularly bad is because everybody is watching them<p><i>A</i> reason, sure. <i>The</i> reason? Come on. There&#x27;s a clue in their name about what part of their history is particularly controversial.</div><br/><div id="40404150" class="c"><input type="checkbox" id="c-40404150" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403468">parent</a><span>|</span><a href="#40402372">next</a><span>|</span><label class="collapse" for="c-40404150">[-]</label><label class="expand" for="c-40404150">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, in the context of this tweet and my message, I was referring to &quot;senior leadership leaving&quot;<p>This sort of high level turnover isn&#x27;t that uncommon. OpenAI is growing quickly and going from initial concept to hundreds of millions in revenue - this is a time of transition for a lot of startups.<p>You can absolutely criticize them for hypocrisy or whatever other issue, but I don&#x27;t think most people should read too much into Ilya&#x2F;Jan leaving.</div><br/></div></div></div></div></div></div></div></div><div id="40402372" class="c"><input type="checkbox" id="c-40402372" checked=""/><div class="controls bullet"><span class="by">mindcrash</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40401184">prev</a><span>|</span><a href="#40400516">next</a><span>|</span><label class="collapse" for="c-40402372">[-]</label><label class="expand" for="c-40402372">[1 more]</label></div><br/><div class="children"><div class="content">If I remember correctly those &quot;adults&quot; offered Altman the opportunity to lead the AI efforts at Microsoft after he got fired.<p>Make of that what you want.</div><br/></div></div><div id="40400516" class="c"><input type="checkbox" id="c-40400516" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40402372">prev</a><span>|</span><a href="#40403880">next</a><span>|</span><label class="collapse" for="c-40400516">[-]</label><label class="expand" for="c-40400516">[3 more]</label></div><br/><div class="children"><div class="content">It is not like Microsoft’s CEO was behaving very admirably or free of ego.</div><br/><div id="40403360" class="c"><input type="checkbox" id="c-40403360" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400516">parent</a><span>|</span><a href="#40403880">next</a><span>|</span><label class="collapse" for="c-40403360">[-]</label><label class="expand" for="c-40403360">[2 more]</label></div><br/><div class="children"><div class="content">For some reason people give Satya a pass. I guess because he comes off as a nice guy. Somehow no one associates all the anti competitive stuff MS does with him - like bundling Teams into Office, or forcing a Copilot button onto OEMs, or dark pattern nagging about default apps, or inserting ads into the start menu, or whatever. But that mode of operation is classic MS, and it makes sense they are still this way, since Satya was a long time MS veteran before he was CEO. All that said, he is probably actually nicer than previous MS executives. He has had a lot of difficulty in his personal life around his son, and that probably changed him for the better.</div><br/><div id="40403966" class="c"><input type="checkbox" id="c-40403966" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403360">parent</a><span>|</span><a href="#40403880">next</a><span>|</span><label class="collapse" for="c-40403966">[-]</label><label class="expand" for="c-40403966">[1 more]</label></div><br/><div class="children"><div class="content">&quot;We are below them, above them, around them.&quot; - doesn’t sound particularly ego free or nice.<p>And it seems that between the choice of OpenAI reassembling at Microsoft and OpenAI continuing under Sam Altman, the nonprofit board had decided to go ahead with Sam Altman.</div><br/></div></div></div></div></div></div><div id="40403880" class="c"><input type="checkbox" id="c-40403880" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40400516">prev</a><span>|</span><a href="#40400517">next</a><span>|</span><label class="collapse" for="c-40403880">[-]</label><label class="expand" for="c-40403880">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Interesting that OpenAI has completely destroyed their reputation over the last year.</i><p>No, they&#x27;re still the &quot;name brand&quot; option for commercial LLM stuff.</div><br/></div></div><div id="40400517" class="c"><input type="checkbox" id="c-40400517" checked=""/><div class="controls bullet"><span class="by">rpastuszak</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40403880">prev</a><span>|</span><a href="#40400533">next</a><span>|</span><label class="collapse" for="c-40400517">[-]</label><label class="expand" for="c-40400517">[3 more]</label></div><br/><div class="children"><div class="content">Bear in mind that an average person who doesn’t frequent sites like HN might see this differently.<p>Altman seems to be very aware of that, dropping short sound bites&#x2F;bits like implying that he has no financial interest in OpenAI (congress hearings IIRC) or comparing GPT4-o to Her.<p>It sounds like complete nonsense (if you know anything about tech) or cringe at best (Her), but what matters is quotability&#x2F;catchiness because that what gets pumped by the media.</div><br/><div id="40403378" class="c"><input type="checkbox" id="c-40403378" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400517">parent</a><span>|</span><a href="#40400533">next</a><span>|</span><label class="collapse" for="c-40403378">[-]</label><label class="expand" for="c-40403378">[2 more]</label></div><br/><div class="children"><div class="content">&gt; he has no financial interest in OpenAI<p>I think that&#x27;s true though? He has an interest in HN, which has an interest in OAI.</div><br/><div id="40403499" class="c"><input type="checkbox" id="c-40403499" checked=""/><div class="controls bullet"><span class="by">jonathankoren</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403378">parent</a><span>|</span><a href="#40400533">next</a><span>|</span><label class="collapse" for="c-40403499">[-]</label><label class="expand" for="c-40403499">[1 more]</label></div><br/><div class="children"><div class="content">HN? Don’t you mean YC?</div><br/></div></div></div></div></div></div><div id="40400533" class="c"><input type="checkbox" id="c-40400533" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40400517">prev</a><span>|</span><a href="#40403527">next</a><span>|</span><label class="collapse" for="c-40400533">[-]</label><label class="expand" for="c-40400533">[1 more]</label></div><br/><div class="children"><div class="content">&gt; cult-like vibes.<p>What cult-like vibes? The OpenAI employees wanted Sam back because of financial reasons and peer pressure. Sam has learnt how to keep employees on leash via money.</div><br/></div></div><div id="40403527" class="c"><input type="checkbox" id="c-40403527" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40400533">prev</a><span>|</span><a href="#40403351">next</a><span>|</span><label class="collapse" for="c-40403527">[-]</label><label class="expand" for="c-40403527">[2 more]</label></div><br/><div class="children"><div class="content">I’m not getting how OpenAIs reputation is being tarnished from all of basically amounting to nothing but theatrical stuff that amounts to people saying Sam is out to destroy the world, none of which is even close to happening, with every entity in earth developing the same tech.</div><br/><div id="40403753" class="c"><input type="checkbox" id="c-40403753" checked=""/><div class="controls bullet"><span class="by">0xDEAFBEAD</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40403527">parent</a><span>|</span><a href="#40403351">next</a><span>|</span><label class="collapse" for="c-40403753">[-]</label><label class="expand" for="c-40403753">[1 more]</label></div><br/><div class="children"><div class="content">I remember early in the days of COVID when the authorities reassured us by telling us there was &quot;no evidence of community spread in the United States&quot;.<p>Sometimes it pays to look at the fundamentals.  <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;ai-alignment-intro-materials" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;ai-alignment-intro-materials</a></div><br/></div></div></div></div><div id="40403351" class="c"><input type="checkbox" id="c-40403351" checked=""/><div class="controls bullet"><span class="by">intelVISA</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40403527">prev</a><span>|</span><a href="#40400511">next</a><span>|</span><label class="collapse" for="c-40403351">[-]</label><label class="expand" for="c-40403351">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t hate the player, etc.</div><br/></div></div><div id="40400511" class="c"><input type="checkbox" id="c-40400511" checked=""/><div class="controls bullet"><span class="by">ctxc</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40403351">prev</a><span>|</span><a href="#40400569">next</a><span>|</span><label class="collapse" for="c-40400511">[-]</label><label class="expand" for="c-40400511">[2 more]</label></div><br/><div class="children"><div class="content">I think the ego and cult-like atmosphere will lead to more growth (maybe not in the &quot;right&quot; direction but in the technical prowess direction) which offsets all criticism.<p>And then the lines will cross over the break even point and be absorbed into Microsoft who proceeds to enjoy the lunch for the foreseeable future.</div><br/><div id="40400570" class="c"><input type="checkbox" id="c-40400570" checked=""/><div class="controls bullet"><span class="by">grayhatter</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400511">parent</a><span>|</span><a href="#40400569">next</a><span>|</span><label class="collapse" for="c-40400570">[-]</label><label class="expand" for="c-40400570">[1 more]</label></div><br/><div class="children"><div class="content">&gt; (maybe not in the &quot;right&quot; direction but in the technical prowess direction) which offsets all criticism.<p>Are you sure about that? Maybe you would like to think a bit harder about what exactly offsets &quot;all criticism&quot;? Being very good at one thing doesn&#x27;t absolve you from consequences of your other shitty actions.</div><br/></div></div></div></div><div id="40400569" class="c"><input type="checkbox" id="c-40400569" checked=""/><div class="controls bullet"><span class="by">aik</span><span>|</span><a href="#40400333">parent</a><span>|</span><a href="#40400511">prev</a><span>|</span><a href="#40403900">next</a><span>|</span><label class="collapse" for="c-40400569">[-]</label><label class="expand" for="c-40400569">[15 more]</label></div><br/><div class="children"><div class="content">I think this perspective is probably only true for 0.001% of people that actually follow Sam closely and are not optimistic about AGI and like to throw their opinions around.  The superficial stuff. The rest don’t care to even know who Sam is and don’t care to assume motive.<p>It’s very likely they’ll bounce back.  I’d rather OpenAI continue to innovate and push the industry forward as they have been. Haven’t seen much of that from Microsoft, so heavily disagree with you there.  Prefer to focus on the actual product of the company not the personalities of the people there or armchair assumptions on the vibes of the culture.</div><br/><div id="40403409" class="c"><input type="checkbox" id="c-40403409" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400569">parent</a><span>|</span><a href="#40400603">next</a><span>|</span><label class="collapse" for="c-40403409">[-]</label><label class="expand" for="c-40403409">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>this perspective is probably only true for 0.001% of people that actually follow Sam closely</i><p>It’s corroded his credibility in D.C. and Brussels for a generation. He raised his profile tremendously right before people credibly called him a liar. It’s like he lofted an adversary’s payload into orbit. He will still get an audience with anyone, as he deserves. But people fact check him in a way they didn’t before and don’t with others. Even those who support his policy priorities, and with whom he and his team talk frequently. (OpenAI’s GR is between incompetent and non-existent.)</div><br/></div></div><div id="40400603" class="c"><input type="checkbox" id="c-40400603" checked=""/><div class="controls bullet"><span class="by">rafram</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400569">parent</a><span>|</span><a href="#40403409">prev</a><span>|</span><a href="#40401862">next</a><span>|</span><label class="collapse" for="c-40400603">[-]</label><label class="expand" for="c-40400603">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Haven’t seen much of that from Microsoft<p>Microsoft is the de facto controlling shareholder in OpenAI. They provide all the money, compute, and backing, and have full access to the models. If OpenAI collapsed tomorrow, Microsoft would absorb its key employees (as they almost did during the board debacle) and everything would continue under the Microsoft umbrella. “OpenAI” is just a shinier name for work that is being done under the near-total control of Microsoft.</div><br/><div id="40400630" class="c"><input type="checkbox" id="c-40400630" checked=""/><div class="controls bullet"><span class="by">aik</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400603">parent</a><span>|</span><a href="#40400933">next</a><span>|</span><label class="collapse" for="c-40400630">[-]</label><label class="expand" for="c-40400630">[8 more]</label></div><br/><div class="children"><div class="content">The money and compute is not the innovation. The LLM models and associated tools are, which is work by OpenAI employees and teams, not Microsoft employees&#x2F;teams.</div><br/><div id="40400787" class="c"><input type="checkbox" id="c-40400787" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400630">parent</a><span>|</span><a href="#40400899">next</a><span>|</span><label class="collapse" for="c-40400787">[-]</label><label class="expand" for="c-40400787">[6 more]</label></div><br/><div class="children"><div class="content">In that vein, I&#x27;d say that most of the LLM research was done at Google. OpenAI productized faster.</div><br/><div id="40400912" class="c"><input type="checkbox" id="c-40400912" checked=""/><div class="controls bullet"><span class="by">aik</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400787">parent</a><span>|</span><a href="#40400899">next</a><span>|</span><label class="collapse" for="c-40400912">[-]</label><label class="expand" for="c-40400912">[5 more]</label></div><br/><div class="children"><div class="content">Confused here, is your argument here that OpenAI is not responsible for any innovation when it comes to LLM tech today?  I’m curious about why you so strongly want to believe that?<p>Nobody knew that scaling transformer architecture would lead to the emergent intelligence we see today. Among other things, OpenAI did R&amp;D for years on that.  Also the only situation where this could true is if Google knew that LLMs could lead to this intelligence and decided to not make it happen, (along with every other tech company now that is furiously trying to catch up to OpenAI), which is absurd.</div><br/><div id="40401102" class="c"><input type="checkbox" id="c-40401102" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400912">parent</a><span>|</span><a href="#40403695">next</a><span>|</span><label class="collapse" for="c-40401102">[-]</label><label class="expand" for="c-40401102">[2 more]</label></div><br/><div class="children"><div class="content">You seem to be inflating the emotional importance of my comment. Google did an enormous amount of the research prior to scaling. I merely pointing out that if there&#x27;s credit to be given out, a bunch of it goes to Google.</div><br/><div id="40401680" class="c"><input type="checkbox" id="c-40401680" checked=""/><div class="controls bullet"><span class="by">aik</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40401102">parent</a><span>|</span><a href="#40403695">next</a><span>|</span><label class="collapse" for="c-40401680">[-]</label><label class="expand" for="c-40401680">[1 more]</label></div><br/><div class="children"><div class="content">Agreed.</div><br/></div></div></div></div><div id="40403695" class="c"><input type="checkbox" id="c-40403695" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400912">parent</a><span>|</span><a href="#40401102">prev</a><span>|</span><a href="#40403177">next</a><span>|</span><label class="collapse" for="c-40403695">[-]</label><label class="expand" for="c-40403695">[1 more]</label></div><br/><div class="children"><div class="content">Actually Google were building and scaling transformers at same time as OpenAI - BERT (following Allen AI&#x27;s ELMo), T5, Meena, LaMDA (chatbot - preceding ChatGPT by a year or two), PaLM ...<p>It seems that Google really didn&#x27;t know what to do with the tech, and hadn&#x27;t figured out a way to control it (OpenAI&#x27;s RLHF - critical for ChatGPT&#x27;s success). It&#x27;s a bit ironic that DeepMind were doing so much with RL, but Google Brain being separate at the time apparently were not consulting with them or tapping into their expertise.</div><br/></div></div><div id="40403177" class="c"><input type="checkbox" id="c-40403177" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400912">parent</a><span>|</span><a href="#40403695">prev</a><span>|</span><a href="#40400899">next</a><span>|</span><label class="collapse" for="c-40403177">[-]</label><label class="expand" for="c-40403177">[1 more]</label></div><br/><div class="children"><div class="content">&gt; not responsible for any<p>That&#x27;s a very strange way to read (the inverse of) the word &quot;most&quot;.</div><br/></div></div></div></div></div></div><div id="40400899" class="c"><input type="checkbox" id="c-40400899" checked=""/><div class="controls bullet"><span class="by">rafram</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400630">parent</a><span>|</span><a href="#40400787">prev</a><span>|</span><a href="#40400933">next</a><span>|</span><label class="collapse" for="c-40400899">[-]</label><label class="expand" for="c-40400899">[1 more]</label></div><br/><div class="children"><div class="content">It might’ve been easier to hire that talent under the shiny OpenAI umbrella, but as I said, Microsoft could absorb the entire thing overnight if it wanted to. And pay them enough to make them stay.</div><br/></div></div></div></div><div id="40400933" class="c"><input type="checkbox" id="c-40400933" checked=""/><div class="controls bullet"><span class="by">ripjaygn</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400603">parent</a><span>|</span><a href="#40400630">prev</a><span>|</span><a href="#40401862">next</a><span>|</span><label class="collapse" for="c-40400933">[-]</label><label class="expand" for="c-40400933">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Microsoft is the de facto controlling shareholder in OpenAI<p>No, they are not even a shareholder.<p>&gt; Microsoft is entitled to up to 49 percent of the for-profit arm of OpenAI&#x27;s profits, according to reports. But that&#x27;s not the same as 49% ownership. That investment does not result in Microsoft owning part of OpenAI</div><br/><div id="40401153" class="c"><input type="checkbox" id="c-40401153" checked=""/><div class="controls bullet"><span class="by">rafram</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400933">parent</a><span>|</span><a href="#40401862">next</a><span>|</span><label class="collapse" for="c-40401153">[-]</label><label class="expand" for="c-40401153">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.bloomberg.com&#x2F;opinion&#x2F;articles&#x2F;2023-11-20&#x2F;who-controls-openai" rel="nofollow">https:&#x2F;&#x2F;www.bloomberg.com&#x2F;opinion&#x2F;articles&#x2F;2023-11-20&#x2F;who-co...</a><p>Note that that diagram is now a little out of date - Microsoft now has an actual (albeit non-voting) board seat, not just effective control.</div><br/></div></div></div></div></div></div><div id="40401862" class="c"><input type="checkbox" id="c-40401862" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40400569">parent</a><span>|</span><a href="#40400603">prev</a><span>|</span><a href="#40403900">next</a><span>|</span><label class="collapse" for="c-40401862">[-]</label><label class="expand" for="c-40401862">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, this isn&#x27;t a view that is likely to be shared by the general public.<p>However, I don&#x27;t think the general public&#x27;s view of OpenAI is much better at this point, given that their exposure is Hinton on 60 Minutes claiming that AI is going to imminently end civilization, creatives arguing that OpenAI has stolen their work, and students using their products to cheat.<p>The only people that I do know who have historically had a positive view of OpenAI has been people working in tech. And Sam seems to be doing everything he can to destroy that goodwill.</div><br/><div id="40403915" class="c"><input type="checkbox" id="c-40403915" checked=""/><div class="controls bullet"><span class="by">happypumpkin</span><span>|</span><a href="#40400333">root</a><span>|</span><a href="#40401862">parent</a><span>|</span><a href="#40403900">next</a><span>|</span><label class="collapse" for="c-40403915">[-]</label><label class="expand" for="c-40403915">[1 more]</label></div><br/><div class="children"><div class="content">Among the people I&#x27;ve discussed recent AI with that aren&#x27;t in tech, almost everyone is very uneasy about it. Some of them use it, and all of them recognize it as potentially useful, but almost everyone is more concerned than excited. Seems like surveys back my personal experience:<p><a href="https:&#x2F;&#x2F;www.pewresearch.org&#x2F;short-reads&#x2F;2023&#x2F;08&#x2F;28&#x2F;growing-public-concern-about-the-role-of-artificial-intelligence-in-daily-life&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.pewresearch.org&#x2F;short-reads&#x2F;2023&#x2F;08&#x2F;28&#x2F;growing-p...</a><p>&quot;More concerned than excited&quot; went from 37% in 2021 to 52% in 2023, &quot;more excited than concerned went from 18% to 10%.</div><br/></div></div></div></div></div></div></div></div><div id="40403900" class="c"><input type="checkbox" id="c-40403900" checked=""/><div class="controls bullet"><span class="by">zaroth</span><span>|</span><a href="#40400333">prev</a><span>|</span><a href="#40400521">next</a><span>|</span><label class="collapse" for="c-40403900">[-]</label><label class="expand" for="c-40403900">[1 more]</label></div><br/><div class="children"><div class="content">I guess LLMs are like asymmetric warfare in the difference between training and inference.<p>Also, highly transportable, the models themselves are not overly massive data sets.<p>So like anything that can be used as a weapon, it’s potentially dangerous.<p>But instead of thinking about misuse and misappropriation for war, they gave us DEI bullshit.</div><br/></div></div><div id="40400521" class="c"><input type="checkbox" id="c-40400521" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40403900">prev</a><span>|</span><a href="#40404283">next</a><span>|</span><label class="collapse" for="c-40400521">[-]</label><label class="expand" for="c-40400521">[17 more]</label></div><br/><div class="children"><div class="content">Any one noticed how Sam sometimes intentionally tries to attack his own work? For example, he has said GPT-2 was very bad, GPT-3 was bad, and GPT-4 is okay. He also fed the AI doomers by talking about AGI dangers and how the world needs to watch out for a future AI catastrophe. He just gives off the vibe of someone trying to act humble but I don&#x27;t know how much of it is just an act and how much is real.<p>I&#x27;ve never seen any CEO just straight out diss their own products.</div><br/><div id="40403301" class="c"><input type="checkbox" id="c-40403301" checked=""/><div class="controls bullet"><span class="by">ren_engineer</span><span>|</span><a href="#40400521">parent</a><span>|</span><a href="#40403223">next</a><span>|</span><label class="collapse" for="c-40403301">[-]</label><label class="expand" for="c-40403301">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s an attempt to keep the hype train going by implying they have internal models that make GPT-4 look bad. The question is if that&#x27;s actually true, we&#x27;ll find out soon because plenty of other companies are now approaching GPT-4 level performance and OpenAI will have to show their hand</div><br/><div id="40404140" class="c"><input type="checkbox" id="c-40404140" checked=""/><div class="controls bullet"><span class="by">menacingly</span><span>|</span><a href="#40400521">root</a><span>|</span><a href="#40403301">parent</a><span>|</span><a href="#40403223">next</a><span>|</span><label class="collapse" for="c-40404140">[-]</label><label class="expand" for="c-40404140">[1 more]</label></div><br/><div class="children"><div class="content">Completely agree. I am shocked that this isn&#x27;t apparent to everyone.<p>I am frustrated at the pure-hype tone this week took on. There are otherwise intelligent people with strong beliefs that OpenAI &quot;has AGI&quot;. It&#x27;s _ludicrous_ and embarrassing. There are people generating entire philosophies predicated on OpenAI just quietly sitting on a profound technological leap.<p>Even their missteps are explained as actually good. We&#x27;ve seen some regressions on gpt-4o performance. But that&#x27;s OK, because it isn&#x27;t supposed to be smarter, because of its very aggressive claims about future features, but they can distract with fearhype, never release or nerf them, and cite unverifiable safety concerns.<p>Google released some impressive, useful things this week. I really want to believe that oai&#x27;s striking claims of simultaneous generation-leaping features, reduced cost, and wide scale will come to pass. But if that tech demo was the equivalent of a concept car that will never really see production, I&#x27;m looking for a new AI hero.</div><br/></div></div></div></div><div id="40403223" class="c"><input type="checkbox" id="c-40403223" checked=""/><div class="controls bullet"><span class="by">jules</span><span>|</span><a href="#40400521">parent</a><span>|</span><a href="#40403301">prev</a><span>|</span><a href="#40403204">next</a><span>|</span><label class="collapse" for="c-40403223">[-]</label><label class="expand" for="c-40403223">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Any one noticed how Sam sometimes intentionally tries to attack his own work? For example, he has said GPT-2 was very bad, GPT-3 was bad, and GPT-4 is okay. He also fed the AI doomers by talking about AGI dangers and how the world needs to watch out for a future AI catastrophe.<p>That is the opposite of criticism. It all translates to &quot;GPT-(n+1) is going to be super awesome&quot;.</div><br/><div id="40403488" class="c"><input type="checkbox" id="c-40403488" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#40400521">root</a><span>|</span><a href="#40403223">parent</a><span>|</span><a href="#40403204">next</a><span>|</span><label class="collapse" for="c-40403488">[-]</label><label class="expand" for="c-40403488">[1 more]</label></div><br/><div class="children"><div class="content">To keep the value of the company increasing and make goverment think that AI should be closed source and controlled by some limited entity for maximum profit.</div><br/></div></div></div></div><div id="40403204" class="c"><input type="checkbox" id="c-40403204" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#40400521">parent</a><span>|</span><a href="#40403223">prev</a><span>|</span><a href="#40400565">next</a><span>|</span><label class="collapse" for="c-40403204">[-]</label><label class="expand" for="c-40403204">[2 more]</label></div><br/><div class="children"><div class="content">Bill Gates the day of any OS release.<p>He was good at always saying the current version selling is the best, never a future version.<p>And for windows 3 and windows 95, he definitely called the old versions trash.<p>In comparison, they were.</div><br/><div id="40403364" class="c"><input type="checkbox" id="c-40403364" checked=""/><div class="controls bullet"><span class="by">djbusby</span><span>|</span><a href="#40400521">root</a><span>|</span><a href="#40403204">parent</a><span>|</span><a href="#40400565">next</a><span>|</span><label class="collapse" for="c-40403364">[-]</label><label class="expand" for="c-40403364">[1 more]</label></div><br/><div class="children"><div class="content">Sam, like Bill only calls them trash after.  But also, I hate looking at my old code too.</div><br/></div></div></div></div><div id="40400565" class="c"><input type="checkbox" id="c-40400565" checked=""/><div class="controls bullet"><span class="by">123yawaworht456</span><span>|</span><a href="#40400521">parent</a><span>|</span><a href="#40403204">prev</a><span>|</span><a href="#40404064">next</a><span>|</span><label class="collapse" for="c-40400565">[-]</label><label class="expand" for="c-40400565">[1 more]</label></div><br/><div class="children"><div class="content">he isn&#x27;t dissing his own products - <i>his</i> products are safe and ethical. it&#x27;s the pesky competitors who need to be regulated out of existence.</div><br/></div></div><div id="40403559" class="c"><input type="checkbox" id="c-40403559" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400521">parent</a><span>|</span><a href="#40404064">prev</a><span>|</span><a href="#40400615">next</a><span>|</span><label class="collapse" for="c-40403559">[-]</label><label class="expand" for="c-40403559">[1 more]</label></div><br/><div class="children"><div class="content">By saying they have a 10x better product in the pipeline</div><br/></div></div><div id="40400615" class="c"><input type="checkbox" id="c-40400615" checked=""/><div class="controls bullet"><span class="by">rafram</span><span>|</span><a href="#40400521">parent</a><span>|</span><a href="#40403559">prev</a><span>|</span><a href="#40400553">next</a><span>|</span><label class="collapse" for="c-40400615">[-]</label><label class="expand" for="c-40400615">[4 more]</label></div><br/><div class="children"><div class="content">He’s doing the Elon Musk “I’m not like the other tech CEOs” thing. It’s proven to be very effective in the social media age. It doesn’t matter how much he seems to be dissing his own work when the capabilities speak for themselves.</div><br/><div id="40400643" class="c"><input type="checkbox" id="c-40400643" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40400521">root</a><span>|</span><a href="#40400615">parent</a><span>|</span><a href="#40400553">next</a><span>|</span><label class="collapse" for="c-40400643">[-]</label><label class="expand" for="c-40400643">[3 more]</label></div><br/><div class="children"><div class="content">Nowadays I almost feel like Godwin&#x27;s law needs an Elon Musk spinoff.</div><br/><div id="40400889" class="c"><input type="checkbox" id="c-40400889" checked=""/><div class="controls bullet"><span class="by">rafram</span><span>|</span><a href="#40400521">root</a><span>|</span><a href="#40400643">parent</a><span>|</span><a href="#40403645">next</a><span>|</span><label class="collapse" for="c-40400889">[-]</label><label class="expand" for="c-40400889">[1 more]</label></div><br/><div class="children"><div class="content">It’s not very odd to mention the world’s most famous tech CEO when talking about tech CEOs.</div><br/></div></div><div id="40403645" class="c"><input type="checkbox" id="c-40403645" checked=""/><div class="controls bullet"><span class="by">sumedh</span><span>|</span><a href="#40400521">root</a><span>|</span><a href="#40400643">parent</a><span>|</span><a href="#40400889">prev</a><span>|</span><a href="#40400553">next</a><span>|</span><label class="collapse" for="c-40403645">[-]</label><label class="expand" for="c-40403645">[1 more]</label></div><br/><div class="children"><div class="content">Wasnt Elon one of founders of OpenAI and was able bring some of the key people there?</div><br/></div></div></div></div></div></div><div id="40400553" class="c"><input type="checkbox" id="c-40400553" checked=""/><div class="controls bullet"><span class="by">nodesocket</span><span>|</span><a href="#40400521">parent</a><span>|</span><a href="#40400615">prev</a><span>|</span><a href="#40400631">next</a><span>|</span><label class="collapse" for="c-40400553">[-]</label><label class="expand" for="c-40400553">[2 more]</label></div><br/><div class="children"><div class="content">Steve Jobs and Elon Musk called out their bad products, and notoriously berate employees for failures and producing what they view as substandard work. I however don’t put Sam in the same ballpark as Steve and Elon… At least yet.</div><br/><div id="40403504" class="c"><input type="checkbox" id="c-40403504" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#40400521">root</a><span>|</span><a href="#40400553">parent</a><span>|</span><a href="#40400631">next</a><span>|</span><label class="collapse" for="c-40403504">[-]</label><label class="expand" for="c-40403504">[1 more]</label></div><br/><div class="children"><div class="content">Give him a little further to fall, I&#x27;m confident he&#x27;ll get there.</div><br/></div></div></div></div><div id="40400631" class="c"><input type="checkbox" id="c-40400631" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40400521">parent</a><span>|</span><a href="#40400553">prev</a><span>|</span><a href="#40404283">next</a><span>|</span><label class="collapse" for="c-40400631">[-]</label><label class="expand" for="c-40400631">[1 more]</label></div><br/><div class="children"><div class="content">He is and was head of a non-profit research organization. They now have a very successful for-profit subsidiary, but that takes time to trickle down into differences in what you say.</div><br/></div></div></div></div><div id="40404283" class="c"><input type="checkbox" id="c-40404283" checked=""/><div class="controls bullet"><span class="by">pilgrim0</span><span>|</span><a href="#40400521">prev</a><span>|</span><a href="#40400543">next</a><span>|</span><label class="collapse" for="c-40404283">[-]</label><label class="expand" for="c-40404283">[2 more]</label></div><br/><div class="children"><div class="content">It’s too late to talk about safety when your product is already free to use and actively employed to pollute the digital space with all sorts of fakery. And AGI? Please, how can they be so delusional to the point of believing they are on a path to achieve anything remotely resembling it? It’s so cringe. I feel like chance has put too much power and influence on the hands of a couple of immature adults. GPT is a state of the art search engine. Period. But they had to anthropomorphize it, and again with GPTo. There’s no genie in the lamp, get over it already.</div><br/><div id="40404715" class="c"><input type="checkbox" id="c-40404715" checked=""/><div class="controls bullet"><span class="by">trvz</span><span>|</span><a href="#40404283">parent</a><span>|</span><a href="#40400543">next</a><span>|</span><label class="collapse" for="c-40404715">[-]</label><label class="expand" for="c-40404715">[1 more]</label></div><br/><div class="children"><div class="content">Detonating a single nuke for test when you think there&#x27;s a chance it&#x27;d ignite the atmosphere and kill the entire world makes it unethical to contribute on it.<p>I too don&#x27;t think OpenAI will ever create an actual AI (&quot;AGI&quot;), but they think they might, and that one might kill humanity; trying to achieve it makes every person working at OpenAI just as reprehensible as those who put the nuclear sword of Damocles over us.</div><br/></div></div></div></div><div id="40400543" class="c"><input type="checkbox" id="c-40400543" checked=""/><div class="controls bullet"><span class="by">camillomiller</span><span>|</span><a href="#40404283">prev</a><span>|</span><a href="#40403515">next</a><span>|</span><label class="collapse" for="c-40400543">[-]</label><label class="expand" for="c-40400543">[39 more]</label></div><br/><div class="children"><div class="content">Why do we give this people credit by giving for granted that AGI is somehow magically around the corner?
It is not, and everyone’s delusional.</div><br/><div id="40405429" class="c"><input type="checkbox" id="c-40405429" checked=""/><div class="controls bullet"><span class="by">elktown</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40404794">next</a><span>|</span><label class="collapse" for="c-40405429">[-]</label><label class="expand" for="c-40405429">[1 more]</label></div><br/><div class="children"><div class="content">The only reason &quot;AGI&quot; is around the corner is because people with vested interests - emotional, profit etc - are going to redefine it to suit their purposes. We&#x27;ve already seen this with AI which is now just any bog standard algorithm&#x2F;app with an attached marketing sticker.</div><br/></div></div><div id="40404794" class="c"><input type="checkbox" id="c-40404794" checked=""/><div class="controls bullet"><span class="by">tgv</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40405429">prev</a><span>|</span><a href="#40404742">next</a><span>|</span><label class="collapse" for="c-40404794">[-]</label><label class="expand" for="c-40404794">[1 more]</label></div><br/><div class="children"><div class="content">AGI isn&#x27;t  close, but as long as people keep throwing billions at it, the research will produce something. We now have publicly accessible models to generate revenge porn. I guess that wasn&#x27;t the intention, but here we are.</div><br/></div></div><div id="40404742" class="c"><input type="checkbox" id="c-40404742" checked=""/><div class="controls bullet"><span class="by">ksynwa</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40404794">prev</a><span>|</span><a href="#40403584">next</a><span>|</span><label class="collapse" for="c-40404742">[-]</label><label class="expand" for="c-40404742">[1 more]</label></div><br/><div class="children"><div class="content">Baffles me that this company has (or had) the smartest people in the industry who understand the workings of their product better than anyone and then you go read their charter where they sagely talk about ethically ushering in AGI like it&#x27;s nothing.<p>The whole drama with this infighting about safety feels like a terrible good cop bad cop routine.</div><br/></div></div><div id="40403584" class="c"><input type="checkbox" id="c-40403584" checked=""/><div class="controls bullet"><span class="by">stefan_</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40404742">prev</a><span>|</span><a href="#40403207">next</a><span>|</span><label class="collapse" for="c-40403584">[-]</label><label class="expand" for="c-40403584">[2 more]</label></div><br/><div class="children"><div class="content">This discussion is tremendously annoying because like the whole &quot;room temperature superconductor&quot; thing, half of it is people who have read far too many science fiction books</div><br/><div id="40403789" class="c"><input type="checkbox" id="c-40403789" checked=""/><div class="controls bullet"><span class="by">0xDEAFBEAD</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403584">parent</a><span>|</span><a href="#40403207">next</a><span>|</span><label class="collapse" for="c-40403789">[-]</label><label class="expand" for="c-40403789">[1 more]</label></div><br/><div class="children"><div class="content">A lot of serious scholars are concerned, including Hinton and Bengio <a href="https:&#x2F;&#x2F;www.safe.ai&#x2F;work&#x2F;statement-on-ai-risk#open-letter" rel="nofollow">https:&#x2F;&#x2F;www.safe.ai&#x2F;work&#x2F;statement-on-ai-risk#open-letter</a></div><br/></div></div></div></div><div id="40403202" class="c"><input type="checkbox" id="c-40403202" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40403207">prev</a><span>|</span><a href="#40403594">next</a><span>|</span><label class="collapse" for="c-40403202">[-]</label><label class="expand" for="c-40403202">[4 more]</label></div><br/><div class="children"><div class="content">Yes! It&#x27;s debatable current models demonstrate any kind of &quot;intelligence&quot;, but AGI is so far off it&#x27;s ridiculous to even mention it. But not everyone&#x27;s delusional. Here&#x27;s what Yann Le Cun is currently saying:<p>&gt; <i>It seems to me that before &quot;urgently figuring out how to control AI systems much smarter than us&quot; we need to have the beginning of a hint of a design for a system smarter than a house cat.</i><p>and<p>&gt; <i>don&#x27;t confuse the superhuman knowledge accumulation and retrieval abilities of current LLMs with actual intelligence</i><p><a href="https:&#x2F;&#x2F;x.com&#x2F;ylecun&#x2F;status&#x2F;1791890883425570823" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;ylecun&#x2F;status&#x2F;1791890883425570823</a></div><br/><div id="40403512" class="c"><input type="checkbox" id="c-40403512" checked=""/><div class="controls bullet"><span class="by">llamaimperative</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403202">parent</a><span>|</span><a href="#40404600">next</a><span>|</span><label class="collapse" for="c-40403512">[-]</label><label class="expand" for="c-40403512">[2 more]</label></div><br/><div class="children"><div class="content">Okay, so describe to me when and how we’ll detect and mitigate problems as we approach AGI or superintelligence.<p>The promise of upside grows exactly in lockstep with the danger of downside.<p>It is absolutely rational to say: “if our AI safety research lab is being hijacked by commercial interests before it’s even commercially viable as a business, then how will we possibly pull the plug on the final descent?”<p>P.S. neither Yann nor me nor you nor Sam nor Greg know what “real intelligence” is, and we <i>certainly</i> don’t know what superhuman intelligence is just the same way an ant couldn’t possibly know what human intelligence is. It is inconceivable to us — that is <i>the point of intelligence.</i></div><br/><div id="40404176" class="c"><input type="checkbox" id="c-40404176" checked=""/><div class="controls bullet"><span class="by">nathanasmith</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403512">parent</a><span>|</span><a href="#40404600">next</a><span>|</span><label class="collapse" for="c-40404176">[-]</label><label class="expand" for="c-40404176">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re about as close to AGI as we are to developing superluminal space flight and working time machines.</div><br/></div></div></div></div><div id="40404600" class="c"><input type="checkbox" id="c-40404600" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403202">parent</a><span>|</span><a href="#40403512">prev</a><span>|</span><a href="#40403594">next</a><span>|</span><label class="collapse" for="c-40404600">[-]</label><label class="expand" for="c-40404600">[1 more]</label></div><br/><div class="children"><div class="content">So Le Cun&#x27;s house cat can write Python code, author musical compositions, and create detailed artwork.<p>I would like to buy Yann&#x27;s house cat, please.  It will get along well with the rather impressive stochastic parrot I bought from someone else on HN.</div><br/></div></div></div></div><div id="40403594" class="c"><input type="checkbox" id="c-40403594" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40403202">prev</a><span>|</span><a href="#40400575">next</a><span>|</span><label class="collapse" for="c-40403594">[-]</label><label class="expand" for="c-40403594">[1 more]</label></div><br/><div class="children"><div class="content">AI alignment is a job so there is negative incentive to make it more than it is. Right now it’s painfully clear it is what’s happening at OpenAI</div><br/></div></div><div id="40400575" class="c"><input type="checkbox" id="c-40400575" checked=""/><div class="controls bullet"><span class="by">nodesocket</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40403594">prev</a><span>|</span><a href="#40400658">next</a><span>|</span><label class="collapse" for="c-40400575">[-]</label><label class="expand" for="c-40400575">[5 more]</label></div><br/><div class="children"><div class="content">I mean you have to think the people who are building it know a lot more than you do. If they are concerned so much so that they quitting and forfeiting millions of dollars there should be reason to be concerned.</div><br/><div id="40400600" class="c"><input type="checkbox" id="c-40400600" checked=""/><div class="controls bullet"><span class="by">etrautmann</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40400575">parent</a><span>|</span><a href="#40403598">next</a><span>|</span><label class="collapse" for="c-40400600">[-]</label><label class="expand" for="c-40400600">[3 more]</label></div><br/><div class="children"><div class="content">Not necessarily. There are plenty of reasonable perspectives on the current attitudes towards AI safety. Assuming that current employees have a secret they’re not sharing about what’s around the corner is just one of many reasonable possibilities.</div><br/><div id="40400668" class="c"><input type="checkbox" id="c-40400668" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40400600">parent</a><span>|</span><a href="#40403607">next</a><span>|</span><label class="collapse" for="c-40400668">[-]</label><label class="expand" for="c-40400668">[1 more]</label></div><br/><div class="children"><div class="content">If there are plenty of reasonable possibilities, then it seems like certainty is misplaced</div><br/></div></div><div id="40403607" class="c"><input type="checkbox" id="c-40403607" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40400600">parent</a><span>|</span><a href="#40400668">prev</a><span>|</span><a href="#40403598">next</a><span>|</span><label class="collapse" for="c-40403607">[-]</label><label class="expand" for="c-40403607">[1 more]</label></div><br/><div class="children"><div class="content">Most% of the imagination of killer AI comes from movies, bad incentives and wanting to get attention.</div><br/></div></div></div></div><div id="40403598" class="c"><input type="checkbox" id="c-40403598" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40400575">parent</a><span>|</span><a href="#40400600">prev</a><span>|</span><a href="#40400658">next</a><span>|</span><label class="collapse" for="c-40403598">[-]</label><label class="expand" for="c-40403598">[1 more]</label></div><br/><div class="children"><div class="content">So you agree with Sam’s stance since he likely knows way more than anyone?</div><br/></div></div></div></div><div id="40400658" class="c"><input type="checkbox" id="c-40400658" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40400575">prev</a><span>|</span><a href="#40403134">next</a><span>|</span><label class="collapse" for="c-40400658">[-]</label><label class="expand" for="c-40400658">[1 more]</label></div><br/><div class="children"><div class="content">You definitely don&#x27;t have a basis to make such a claim. It very well might not be - but it seems silly to be <i>certain</i> that it is not given the increase in capabilities.<p>People who bet on scaling in 2018 (which is really when these bets were starting to be made) were the minority then and have consistently proven the naysayers incorrect for 6 years now. Only now are we even close to reaching opinion parity on this.</div><br/></div></div><div id="40403134" class="c"><input type="checkbox" id="c-40403134" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#40400543">parent</a><span>|</span><a href="#40400658">prev</a><span>|</span><a href="#40403515">next</a><span>|</span><label class="collapse" for="c-40403134">[-]</label><label class="expand" for="c-40403134">[21 more]</label></div><br/><div class="children"><div class="content">If you look at compute scaling and model improvement on that compute, we’re going to get there pretty fast.<p>Both compute, architecture and cost matter, and those have been improving like crazy.<p>I don’t know about you, but I didn’t expect GPT-4o to come out at half the price one year later, with real-time voice, image and text.<p>There is zero sign of slowing, Nvidia keeps building beefier GPUs specialized in LLMs, the world keeps adding more data centers and humanity keeps creating exponentially more data world wide.<p>You can plot this exponential growth out over time and calculate when these models will have the complexity of the brain. Then you can assume some penalty for shitty architecture (that gets better over time), and you’ll have a ballpark estimate.<p>Somewhere 2027-2030 we’ll have models that most of us consider AGI today. Now, societal impact is hard to predict since that’ll depend on regulators as well.</div><br/><div id="40403201" class="c"><input type="checkbox" id="c-40403201" checked=""/><div class="controls bullet"><span class="by">Aloisius</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403134">parent</a><span>|</span><a href="#40403257">next</a><span>|</span><label class="collapse" for="c-40403201">[-]</label><label class="expand" for="c-40403201">[6 more]</label></div><br/><div class="children"><div class="content">LLMs don&#x27;t reason. Without reasoning, you don&#x27;t have AGI.<p>I can&#x27;t wait until we&#x27;re past the peak on the hype cycle because people&#x27;s understanding of what these models can and can&#x27;t do or how close we are to AGI isn&#x27;t grounded in reality.</div><br/><div id="40403698" class="c"><input type="checkbox" id="c-40403698" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403201">parent</a><span>|</span><a href="#40403670">next</a><span>|</span><label class="collapse" for="c-40403698">[-]</label><label class="expand" for="c-40403698">[4 more]</label></div><br/><div class="children"><div class="content">It seems unclear to me what it is that LLMs can’t&#x2F;don’t do. I agree that there are substantial limitations of current ones, but I have little idea of which of these limitations are fundamental to the architecture (as well as which are a result of limits of the scale, and which are a result of limits to the training data (using the current training strategies), and which are a result of the overall training methods).<p>Do you have some insights you could share on this question?<p>I mean, “they can’t reason” might be true, but I’m not exactly sure what it is that it is saying that they can’t do.</div><br/><div id="40404103" class="c"><input type="checkbox" id="c-40404103" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403698">parent</a><span>|</span><a href="#40404156">next</a><span>|</span><label class="collapse" for="c-40404103">[-]</label><label class="expand" for="c-40404103">[1 more]</label></div><br/><div class="children"><div class="content">Think about the difference between what an LLM is doing and a human is doing in response to being told they are wrong.<p>If you tell a human, “You are wrong on this, and here is why.” They may reject it, but their mental model is updating with the new information that you, the speaker, think X for YZQ reasons. Their response is based on a judgment of how trustworthy you are and the credibility of the evidence.<p>For an LLM, the response is not based on these logical connections, but simply the additional prompt context of YZQ tokens being close to each other.<p>This is not “logic” in any traditional sense or in the sense of how a human incorporates and responds to this new information.<p>The LLM’s method of responding is also inherent to the architecture of the model. It’s predicting tokens based on input. It’s not reasoning.<p>Critically, this flaw is inherent in all LLM output. Giving an LLM’s output the power to affect real world activities means trusting that the decision can be made by sophisticated word association rather than more complex reasoning.<p>There may be lots of decisions where word association is all you need, but I doubt that is the case for all decisions humans make.</div><br/></div></div><div id="40404156" class="c"><input type="checkbox" id="c-40404156" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403698">parent</a><span>|</span><a href="#40404103">prev</a><span>|</span><a href="#40403670">next</a><span>|</span><label class="collapse" for="c-40404156">[-]</label><label class="expand" for="c-40404156">[2 more]</label></div><br/><div class="children"><div class="content">Not OP, and not a computer scientist. But amateur neuroscientist.<p>From what I understand, current models seem to have issue with planning and reasoning because they “blurt” everything out zero shot. They can’t recurrently process information.<p>Part of it is how we train these models, which is rapidly being experimented with and being improved.<p>Another part is that our brain can both consciously and unconsciously process information recurrently until we solve a problem. Especially helpful when we haven’t solved a problem before.<p>With LLMs, we can do that rudimentary with recurrent prompting. In a way, it can see the steps it has tried, re-evaluate and come up with a new try.<p>But it’s not innate yet, where models can “think” about a complex problem until it solved it.<p>Also- According to some theories around consciousness, this is when consciousness will really emerge (integrated information theory).<p>I’m pretty sure we’ll be able to solve this recurrent processing issue in the next few years.</div><br/><div id="40405169" class="c"><input type="checkbox" id="c-40405169" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40404156">parent</a><span>|</span><a href="#40403670">next</a><span>|</span><label class="collapse" for="c-40405169">[-]</label><label class="expand" for="c-40405169">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>From what I understand, current models seem to have issue with planning and reasoning because they “blurt” everything out zero shot. They can’t recurrently process information.</i><p>IMO, this is not necessarily a model issue, but an interface issue - recurrently processing information is enabled by just feeding the model&#x27;s output back to it, perhaps many times, before surfacing it to the user. Right now, with the interfaces we&#x27;re exposed to - direct evaluation, perhaps wrapped in a chatbot UI - the user must supply the recursion themselves.<p>&gt; <i>But it’s not innate yet, where models can “think” about a complex problem until it solved it.</i><p>That&#x27;s indeed the part I see us struggling with - we can make the model recursive, but we don&#x27;t know how to do it unsupervised, so we can box it behind an abstraction layer, letting it work stuff out on its own and break recursion at the right moment.<p>I think the mental model I developed a good year ago still stands: LLM isn&#x27;t best compared to a human mind, but to a human&#x27;s <i>inner voice</i>. The bit that &quot;blurts everything out zero-shot&quot; into your consciousness. It has the same issues of overconfidence, hallucinations, and needing to be recursively fed back to itself.</div><br/></div></div></div></div></div></div><div id="40403670" class="c"><input type="checkbox" id="c-40403670" checked=""/><div class="controls bullet"><span class="by">Madmallard</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403201">parent</a><span>|</span><a href="#40403698">prev</a><span>|</span><a href="#40403257">next</a><span>|</span><label class="collapse" for="c-40403670">[-]</label><label class="expand" for="c-40403670">[1 more]</label></div><br/><div class="children"><div class="content">just because it cant reason doesnt mean it cant sufficiently mimick (even sophisticated) human behavior in various avenues. it’s already doing this</div><br/></div></div></div></div><div id="40403257" class="c"><input type="checkbox" id="c-40403257" checked=""/><div class="controls bullet"><span class="by">fabrice_d</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403134">parent</a><span>|</span><a href="#40403201">prev</a><span>|</span><a href="#40403228">next</a><span>|</span><label class="collapse" for="c-40403257">[-]</label><label class="expand" for="c-40403257">[3 more]</label></div><br/><div class="children"><div class="content">And still, Yann LeCun (head of AI at Meta, renowned AI&#x2F;ML researcher) is convinced that we are far from reaching AGI. He makes convincing arguments, especially around the fact that we are not able to expose models to the amount of redundant information that even a young kid is exposed to.<p>I guess we&#x27;ll see some shifting goals around &quot;what is even AGI&quot;. You say the we&#x27;ll have soon &quot;models that most of us consider AGI today&quot;, but what does that even mean?</div><br/><div id="40403954" class="c"><input type="checkbox" id="c-40403954" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403257">parent</a><span>|</span><a href="#40403228">next</a><span>|</span><label class="collapse" for="c-40403954">[-]</label><label class="expand" for="c-40403954">[2 more]</label></div><br/><div class="children"><div class="content">I mean, this new openai model is trained on video and voice as well.<p>So already, his argument is outdated. He had made a few of these claims over the last year that have been quickly disproven by a new model.</div><br/><div id="40404088" class="c"><input type="checkbox" id="c-40404088" checked=""/><div class="controls bullet"><span class="by">fabrice_d</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403954">parent</a><span>|</span><a href="#40403228">next</a><span>|</span><label class="collapse" for="c-40404088">[-]</label><label class="expand" for="c-40404088">[1 more]</label></div><br/><div class="children"><div class="content">They are trained on a ridiculously small amount of content compared to what the brain of a 3 years child is subject to. Current models still have a very narrow application field compared to what a human can achieve.</div><br/></div></div></div></div></div></div><div id="40403228" class="c"><input type="checkbox" id="c-40403228" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403134">parent</a><span>|</span><a href="#40403257">prev</a><span>|</span><a href="#40403359">next</a><span>|</span><label class="collapse" for="c-40403228">[-]</label><label class="expand" for="c-40403228">[6 more]</label></div><br/><div class="children"><div class="content">AGI seems like a stretch when none of the LLM’s right now can solve the prompt:<p>“
Write a sentence that has an odd number of words, with the third word being &quot;keyboard&quot;, the last word being &quot;anyway&quot;, the next-to-last word being a number, and the first word being a palindrome
“<p>Do we have to add these examples to the dataset now, so that they can solve this? Then maybe we find another prompt and add that too, as a human I’ve never seen such a question before, why do I seem to be able to do it so easily? We are creating some form of intelligence that will have a massive economic impact, but I doubt we are creating human like intelligence that has the ability of being far better than humans along every dimension, which is what super intelligence is</div><br/><div id="40404390" class="c"><input type="checkbox" id="c-40404390" checked=""/><div class="controls bullet"><span class="by">luyu_wu</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403228">parent</a><span>|</span><a href="#40403333">next</a><span>|</span><label class="collapse" for="c-40404390">[-]</label><label class="expand" for="c-40404390">[1 more]</label></div><br/><div class="children"><div class="content">Why is this a goalpost for just about anything? This seems completely uncorrelated, and I&#x27;m sure many fully grown adults would fail under similar timing conditions too.</div><br/></div></div><div id="40403333" class="c"><input type="checkbox" id="c-40403333" checked=""/><div class="controls bullet"><span class="by">plaidfuji</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403228">parent</a><span>|</span><a href="#40404390">prev</a><span>|</span><a href="#40403464">next</a><span>|</span><label class="collapse" for="c-40403333">[-]</label><label class="expand" for="c-40403333">[2 more]</label></div><br/><div class="children"><div class="content">Interesting, gpt-4o took seven tries to do this, and I had to give it very specific feedback about what it was doing wrong. Here was the answer:<p>Level says keyboard broke down today, needing repair within five anyway.</div><br/><div id="40403725" class="c"><input type="checkbox" id="c-40403725" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403333">parent</a><span>|</span><a href="#40403464">next</a><span>|</span><label class="collapse" for="c-40403725">[-]</label><label class="expand" for="c-40403725">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4o still uses BPEs.</div><br/></div></div></div></div><div id="40403464" class="c"><input type="checkbox" id="c-40403464" checked=""/><div class="controls bullet"><span class="by">croemer</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403228">parent</a><span>|</span><a href="#40403333">prev</a><span>|</span><a href="#40403282">next</a><span>|</span><label class="collapse" for="c-40403464">[-]</label><label class="expand" for="c-40403464">[1 more]</label></div><br/><div class="children"><div class="content">Is this correct? First try ChatGPT4: &quot;Anna uses keyboard swiftly without missing five anyway.&quot;<p>Prompt exactly yours: <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;ff3169cb-0d45-4cdb-bf18-bc552ad697ec" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;ff3169cb-0d45-4cdb-bf18-bc552ad697...</a><p>Woop, I forgot about the odd number of words.</div><br/></div></div><div id="40403282" class="c"><input type="checkbox" id="c-40403282" checked=""/><div class="controls bullet"><span class="by">npinsker</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403228">parent</a><span>|</span><a href="#40403464">prev</a><span>|</span><a href="#40403359">next</a><span>|</span><label class="collapse" for="c-40403282">[-]</label><label class="expand" for="c-40403282">[1 more]</label></div><br/><div class="children"><div class="content">gpt-4o produced a correct sentence first try for me</div><br/></div></div></div></div><div id="40403359" class="c"><input type="checkbox" id="c-40403359" checked=""/><div class="controls bullet"><span class="by">Phoenix5869</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403134">parent</a><span>|</span><a href="#40403228">prev</a><span>|</span><a href="#40403613">next</a><span>|</span><label class="collapse" for="c-40403359">[-]</label><label class="expand" for="c-40403359">[3 more]</label></div><br/><div class="children"><div class="content">&gt; You can plot this exponential growth out over time and calculate when these models will have the complexity of the brain. Then you can assume some penalty for shitty architecture (that gets better over time), and you’ll have a ballpark estimate.<p>The same thing could’ve been said for self driving cars, or the space program, or a lot of things that seemed to be progressing quickly at the time.</div><br/><div id="40404036" class="c"><input type="checkbox" id="c-40404036" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403359">parent</a><span>|</span><a href="#40404603">next</a><span>|</span><label class="collapse" for="c-40404036">[-]</label><label class="expand" for="c-40404036">[1 more]</label></div><br/><div class="children"><div class="content">Not really. You can only make these predictions about things bottlenecked by things that improve exponentially such as compute.<p>Neither the space program nor self driving are compute restrained.<p>The latter will probably be “solved” as we get closer to AGI, since you need some sort of human like reasoning for edge cases that require reasoning.<p>Another tech like this is batteries: There is no miracle jump in production batteries. They just improve about 10% YoY, both in energy density and cost.<p>So you can extrapolate when electric cars will be cheaper than gas cars to buy.<p>Intersects around 2035 last I checked.</div><br/></div></div><div id="40404603" class="c"><input type="checkbox" id="c-40404603" checked=""/><div class="controls bullet"><span class="by">SpicyLemonZest</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403359">parent</a><span>|</span><a href="#40404036">prev</a><span>|</span><a href="#40403613">next</a><span>|</span><label class="collapse" for="c-40404603">[-]</label><label class="expand" for="c-40404603">[1 more]</label></div><br/><div class="children"><div class="content">And if you did say it, you would have been a lot more correct than if you&#x27;d said they won&#x27;t amount to anything. Robotaxis with nobody in the driver&#x27;s seat are available in three major US cities; most people use a network of navigational satellites every time they want to figure out how to get somewhere new.</div><br/></div></div></div></div><div id="40403613" class="c"><input type="checkbox" id="c-40403613" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403134">parent</a><span>|</span><a href="#40403359">prev</a><span>|</span><a href="#40403515">next</a><span>|</span><label class="collapse" for="c-40403613">[-]</label><label class="expand" for="c-40403613">[2 more]</label></div><br/><div class="children"><div class="content">Even sam says they are 2 breakthroughs before AGI is possible.  Who knows how difficult they are to make that leap. Humans are still highly capable comparing to LLMs</div><br/><div id="40403838" class="c"><input type="checkbox" id="c-40403838" checked=""/><div class="controls bullet"><span class="by">Phoenix5869</span><span>|</span><a href="#40400543">root</a><span>|</span><a href="#40403613">parent</a><span>|</span><a href="#40403515">next</a><span>|</span><label class="collapse" for="c-40403838">[-]</label><label class="expand" for="c-40403838">[1 more]</label></div><br/><div class="children"><div class="content">And let’s not forget that Sam Altman is the CEO of OpenAI. He clearly has a vested interest in creating hype around AI, AGI, etc. But when i point this out on Reddit, for some reason i get downvoted.</div><br/></div></div></div></div></div></div></div></div><div id="40403515" class="c"><input type="checkbox" id="c-40403515" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400543">prev</a><span>|</span><a href="#40403523">next</a><span>|</span><label class="collapse" for="c-40403515">[-]</label><label class="expand" for="c-40403515">[4 more]</label></div><br/><div class="children"><div class="content">He is basically saying there was a mis-balance in the perception of risk between this likely doomer and most tech staff that is there.  There is no Terminator to align and LLM going “rogue” and killing everyone is way overblown to pause development</div><br/><div id="40403780" class="c"><input type="checkbox" id="c-40403780" checked=""/><div class="controls bullet"><span class="by">0xDEAFBEAD</span><span>|</span><a href="#40403515">parent</a><span>|</span><a href="#40403523">next</a><span>|</span><label class="collapse" for="c-40403780">[-]</label><label class="expand" for="c-40403780">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.&quot;<p><a href="https:&#x2F;&#x2F;www.safe.ai&#x2F;work&#x2F;statement-on-ai-risk#open-letter" rel="nofollow">https:&#x2F;&#x2F;www.safe.ai&#x2F;work&#x2F;statement-on-ai-risk#open-letter</a><p>What do you know that Geoff Hinton, Yoshua Bengio, Demis Hassabis, Bill Gates, Stuart Russell, etc. don&#x27;t?</div><br/><div id="40403969" class="c"><input type="checkbox" id="c-40403969" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40403515">root</a><span>|</span><a href="#40403780">parent</a><span>|</span><a href="#40403942">next</a><span>|</span><label class="collapse" for="c-40403969">[-]</label><label class="expand" for="c-40403969">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t forget... Sam Altman!  (Also, Sam Altman: &quot;Development of superhuman machine intelligence (SMI) [1] is probably the greatest threat to the continued existence of humanity.&quot;  Feb 15, 2015, before he co-founded OpenAI.  <a href="https:&#x2F;&#x2F;blog.samaltman.com&#x2F;machine-intelligence-part-1" rel="nofollow">https:&#x2F;&#x2F;blog.samaltman.com&#x2F;machine-intelligence-part-1</a>)</div><br/></div></div><div id="40403942" class="c"><input type="checkbox" id="c-40403942" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40403515">root</a><span>|</span><a href="#40403780">parent</a><span>|</span><a href="#40403969">prev</a><span>|</span><a href="#40403523">next</a><span>|</span><label class="collapse" for="c-40403942">[-]</label><label class="expand" for="c-40403942">[1 more]</label></div><br/><div class="children"><div class="content">You know there is another side of technically equivalents that is saying otherwise and I’m just siding with the logic that AGI isn’t out to destroy us.</div><br/></div></div></div></div></div></div><div id="40403523" class="c"><input type="checkbox" id="c-40403523" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40403515">prev</a><span>|</span><a href="#40403816">next</a><span>|</span><label class="collapse" for="c-40403523">[-]</label><label class="expand" for="c-40403523">[15 more]</label></div><br/><div class="children"><div class="content">Paraphrasing a bit, and pulling in other recent OpenAI mews.<p>&quot;We know AGI is risky, and we&#x27;ve stated that while also shipping new products every 6 months. We hope you are excited by the recent announcement that OpenAI is looking into how to allow our customers to responsibly generate erotica, and want you to know that we are doing this with full concern of the risks and potential outcomes. We look forward to offering multimodal interactions with models so life-like that you may not realize you are interacting with a computer at all. And rest assured, we&#x27;ve fully thought through and planned for the potential dangers of such a future.&quot;</div><br/><div id="40403679" class="c"><input type="checkbox" id="c-40403679" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#40403523">parent</a><span>|</span><a href="#40403589">next</a><span>|</span><label class="collapse" for="c-40403679">[-]</label><label class="expand" for="c-40403679">[1 more]</label></div><br/><div class="children"><div class="content">I feel like, when companies start venturing into adult entertainment, something has gone wrong.<p>There’s nothing specifically wrong with it, but there are “legit” companies and “kinda shady” ones, and when you start hanging out with a bad crowd…<p>Sad to see.<p>I feel like it really completes the fall from grace for OpenAI.</div><br/></div></div><div id="40403589" class="c"><input type="checkbox" id="c-40403589" checked=""/><div class="controls bullet"><span class="by">aaomidi</span><span>|</span><a href="#40403523">parent</a><span>|</span><a href="#40403679">prev</a><span>|</span><a href="#40403816">next</a><span>|</span><label class="collapse" for="c-40403589">[-]</label><label class="expand" for="c-40403589">[13 more]</label></div><br/><div class="children"><div class="content">Tf is the risk of generating erotica?</div><br/><div id="40403700" class="c"><input type="checkbox" id="c-40403700" checked=""/><div class="controls bullet"><span class="by">francasso</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403589">parent</a><span>|</span><a href="#40403619">next</a><span>|</span><label class="collapse" for="c-40403700">[-]</label><label class="expand" for="c-40403700">[6 more]</label></div><br/><div class="children"><div class="content">They are going to say the usual things, child pornography, risk of terrorist use etc... and conclude 
that we need our overlords to protect us. 
Same reasons that have been used in the past to take away freedoms.<p>Not saying they aren&#x27;t real risks... Are there going to be sad incidents where innocent people will be harmed? Yes. The question though is if there will be enough of them, and egregious enough, for us to disregard the upsides of technology that is not heavily regulated.</div><br/><div id="40403733" class="c"><input type="checkbox" id="c-40403733" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403700">parent</a><span>|</span><a href="#40403928">next</a><span>|</span><label class="collapse" for="c-40403733">[-]</label><label class="expand" for="c-40403733">[1 more]</label></div><br/><div class="children"><div class="content">Oh that&#x27;s so interesting, I fall very much on the side of <i>avoiding</i> regulations as the solution at all costs.<p>Regulations won&#x27;t fix a damn thing and regulators have no chance of keeping up even if they wanted to. We need these tools to not be developed or offered at all by choice. Unfortunately that really means either the company, investors, or customers need to make a moral decision at the cost of economic gain or convenience&#x2F;novelty.</div><br/></div></div><div id="40403928" class="c"><input type="checkbox" id="c-40403928" checked=""/><div class="controls bullet"><span class="by">Silphendio</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403700">parent</a><span>|</span><a href="#40403733">prev</a><span>|</span><a href="#40403619">next</a><span>|</span><label class="collapse" for="c-40403928">[-]</label><label class="expand" for="c-40403928">[4 more]</label></div><br/><div class="children"><div class="content">Child porn is already heavily &quot;regulated&quot;. Producing the stuff is illegal and AI service providers must take steps to prevent it.</div><br/><div id="40404125" class="c"><input type="checkbox" id="c-40404125" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403928">parent</a><span>|</span><a href="#40404454">next</a><span>|</span><label class="collapse" for="c-40404125">[-]</label><label class="expand" for="c-40404125">[2 more]</label></div><br/><div class="children"><div class="content">This is a blind spot for me, do existing legal protections generally cover fake&#x2F;generated content?<p>To be clear, the context of this thread is extremely important. I don&#x27;t know how far our existing laws might go to make sure ML generated child porn is illegal and I&#x27;d personally feel much more comfortable to know that ML generation isn&#x27;t some kind of legal loophole.</div><br/><div id="40404256" class="c"><input type="checkbox" id="c-40404256" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40404125">parent</a><span>|</span><a href="#40404454">next</a><span>|</span><label class="collapse" for="c-40404256">[-]</label><label class="expand" for="c-40404256">[1 more]</label></div><br/><div class="children"><div class="content">Last I heard it varies by jurisdiction. Some only prohibit things that actually involve actual children, some ban based on subject matter even if it&#x27;s say a cartoon drawing.<p>I assume that training separate models for separate jurisdictions would be rather expensive, so probably unlikely.</div><br/></div></div></div></div><div id="40404454" class="c"><input type="checkbox" id="c-40404454" checked=""/><div class="controls bullet"><span class="by">francasso</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403928">parent</a><span>|</span><a href="#40404125">prev</a><span>|</span><a href="#40403619">next</a><span>|</span><label class="collapse" for="c-40404454">[-]</label><label class="expand" for="c-40404454">[1 more]</label></div><br/><div class="children"><div class="content">Are you saying they aren&#x27;t already preventing it? I would find that hard to believe...</div><br/></div></div></div></div></div></div><div id="40403619" class="c"><input type="checkbox" id="c-40403619" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403589">parent</a><span>|</span><a href="#40403700">prev</a><span>|</span><a href="#40403728">next</a><span>|</span><label class="collapse" for="c-40403619">[-]</label><label class="expand" for="c-40403619">[1 more]</label></div><br/><div class="children"><div class="content">Child pornography is the easy answer. Deep fake porn would be a huge risk in my opinion though, kids making fake porn and sending it around school could absolutely lead to suicides or attacks against other students.<p>Better question, how the heck do you see something like the ability for anyone to make extremely convincing, fake erotica and <i>not</i> see at least some risk in that?</div><br/></div></div><div id="40403728" class="c"><input type="checkbox" id="c-40403728" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403589">parent</a><span>|</span><a href="#40403619">prev</a><span>|</span><a href="#40403599">next</a><span>|</span><label class="collapse" for="c-40403728">[-]</label><label class="expand" for="c-40403728">[1 more]</label></div><br/><div class="children"><div class="content">Negative reputational or legal consequences directed at the company providing the AI.</div><br/></div></div><div id="40403599" class="c"><input type="checkbox" id="c-40403599" checked=""/><div class="controls bullet"><span class="by">kumarvvr</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403589">parent</a><span>|</span><a href="#40403728">prev</a><span>|</span><a href="#40403614">next</a><span>|</span><label class="collapse" for="c-40403599">[-]</label><label class="expand" for="c-40403599">[1 more]</label></div><br/><div class="children"><div class="content">Generaring porn with children in it.</div><br/></div></div><div id="40403614" class="c"><input type="checkbox" id="c-40403614" checked=""/><div class="controls bullet"><span class="by">yanderekko</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403589">parent</a><span>|</span><a href="#40403599">prev</a><span>|</span><a href="#40403816">next</a><span>|</span><label class="collapse" for="c-40403614">[-]</label><label class="expand" for="c-40403614">[3 more]</label></div><br/><div class="children"><div class="content">Deepfakes?</div><br/><div id="40403822" class="c"><input type="checkbox" id="c-40403822" checked=""/><div class="controls bullet"><span class="by">aaomidi</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403614">parent</a><span>|</span><a href="#40403816">next</a><span>|</span><label class="collapse" for="c-40403822">[-]</label><label class="expand" for="c-40403822">[2 more]</label></div><br/><div class="children"><div class="content">Honestly I was under the impression erotica is text based only. I guess not.</div><br/><div id="40403876" class="c"><input type="checkbox" id="c-40403876" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40403523">root</a><span>|</span><a href="#40403822">parent</a><span>|</span><a href="#40403816">next</a><span>|</span><label class="collapse" for="c-40403876">[-]</label><label class="expand" for="c-40403876">[1 more]</label></div><br/><div class="children"><div class="content">Oh I can see that. Given OpenAI&#x27;s push for multimodal engagement with GTP4o I fully expect the erotica they want to support would be more than just text.<p>I do agree with you there though, I can&#x27;t think of any meaningful risks of text-only erotica unless you consider the psychological impact of the person choosing to engage with it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40403816" class="c"><input type="checkbox" id="c-40403816" checked=""/><div class="controls bullet"><span class="by">arathis</span><span>|</span><a href="#40403523">prev</a><span>|</span><a href="#40404392">next</a><span>|</span><label class="collapse" for="c-40403816">[-]</label><label class="expand" for="c-40403816">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe them. There&#x27;s no trust here. They need to earn it. I don&#x27;t know how, but their continued call for safety around the very thing they&#x27;re creating doesn&#x27;t come off as playing safe, but creating cover for themselves when inevitably they break shit.<p>I do not trust them.</div><br/></div></div><div id="40404392" class="c"><input type="checkbox" id="c-40404392" checked=""/><div class="controls bullet"><span class="by">juped</span><span>|</span><a href="#40403816">prev</a><span>|</span><a href="#40403395">next</a><span>|</span><label class="collapse" for="c-40404392">[-]</label><label class="expand" for="c-40404392">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Sam and Greg&quot;? Seriously?</div><br/></div></div><div id="40403395" class="c"><input type="checkbox" id="c-40403395" checked=""/><div class="controls bullet"><span class="by">elisbce</span><span>|</span><a href="#40404392">prev</a><span>|</span><a href="#40404043">next</a><span>|</span><label class="collapse" for="c-40403395">[-]</label><label class="expand" for="c-40403395">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why people are just mad about OpenAI for their pioneering work towards AGI as if they are the only one who has skin in this game. OpenAI, Google, NVidia, MS, Meta, almost all the AI researchers who publish meaningful work in top literatures, are pushing the boundaries today and has their fair share of responsibility. They are all in it for something, money, power, control, fame, curiosity, academic recognition, whatever the incentives are. At this rate, it&#x27;s already a race to the bottom and I don&#x27;t believe the first place AGI was born would be able to kill it off. AGI is like nukes, it&#x27;s so powerful that nobody will take the risks seriously until they have it, and nobody is going to stop pursuing it because everyone else is chasing it. If OpenAI slows down, Google will take the lead. If the US slows down, China will take the lead. That&#x27;s basically the doomed future we are facing in reality.</div><br/><div id="40403581" class="c"><input type="checkbox" id="c-40403581" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#40403395">parent</a><span>|</span><a href="#40403501">next</a><span>|</span><label class="collapse" for="c-40403581">[-]</label><label class="expand" for="c-40403581">[1 more]</label></div><br/><div class="children"><div class="content">&gt; At this rate, it&#x27;s already a race to the bottom and I don&#x27;t believe the first place AGI was born would be able to kill it off. AGI is like nukes, it&#x27;s so powerful that nobody will take the risks seriously until they have it, and nobody is going to stop pursuing it because everyone else is chasing it. If OpenAI slows down, Google will take the lead. If the US slows down, China will take the lead. That&#x27;s basically the doomed future we are facing in reality<p>Its amazing to me that Oppenheimer became such a box office hit around the same time all the AI &#x2F; AGI hype really built. There are plenty of parallels between the Manhattan Project, and nuclear weapons in general, and real AI.<p>Heck, just watch the last 5 minutes of Oppenheimer and tell me there&#x27;s no lesson there to be learned before going right back to work the next day trying to build artificial general intelligence just because we can.</div><br/></div></div><div id="40403501" class="c"><input type="checkbox" id="c-40403501" checked=""/><div class="controls bullet"><span class="by">llamaimperative</span><span>|</span><a href="#40403395">parent</a><span>|</span><a href="#40403581">prev</a><span>|</span><a href="#40403582">next</a><span>|</span><label class="collapse" for="c-40403501">[-]</label><label class="expand" for="c-40403501">[1 more]</label></div><br/><div class="children"><div class="content">People are particularly focused on OAI because it’s both the clear leader <i>and</i> the one with a corporate charter that appears to run totally contrary to their actual behavior.</div><br/></div></div><div id="40403582" class="c"><input type="checkbox" id="c-40403582" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40403395">parent</a><span>|</span><a href="#40403501">prev</a><span>|</span><a href="#40404043">next</a><span>|</span><label class="collapse" for="c-40403582">[-]</label><label class="expand" for="c-40403582">[1 more]</label></div><br/><div class="children"><div class="content">Nukes are made directly to blow things up so I’m not sure if this analogy stands the tests</div><br/></div></div></div></div><div id="40404043" class="c"><input type="checkbox" id="c-40404043" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#40403395">prev</a><span>|</span><a href="#40400507">next</a><span>|</span><label class="collapse" for="c-40404043">[-]</label><label class="expand" for="c-40404043">[1 more]</label></div><br/><div class="children"><div class="content">you all realize this is a day time soap opera?<p><i>hey hackernews, your shows are on</i></div><br/></div></div><div id="40400507" class="c"><input type="checkbox" id="c-40400507" checked=""/><div class="controls bullet"><span class="by">AndrewKemendo</span><span>|</span><a href="#40404043">prev</a><span>|</span><a href="#40400629">next</a><span>|</span><label class="collapse" for="c-40400507">[-]</label><label class="expand" for="c-40400507">[15 more]</label></div><br/><div class="children"><div class="content">Seems like we need people to start posting mirrors of X content, the same way we post archive links for Washington Post and other firewalled articles</div><br/><div id="40402981" class="c"><input type="checkbox" id="c-40402981" checked=""/><div class="controls bullet"><span class="by">1vuio0pswjnm7</span><span>|</span><a href="#40400507">parent</a><span>|</span><a href="#40400512">next</a><span>|</span><label class="collapse" for="c-40402981">[-]</label><label class="expand" for="c-40402981">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;nitter.poast.org&#x2F;gdb&#x2F;status&#x2F;1791869138132218351" rel="nofollow">https:&#x2F;&#x2F;nitter.poast.org&#x2F;gdb&#x2F;status&#x2F;1791869138132218351</a></div><br/></div></div><div id="40400512" class="c"><input type="checkbox" id="c-40400512" checked=""/><div class="controls bullet"><span class="by">aik</span><span>|</span><a href="#40400507">parent</a><span>|</span><a href="#40402981">prev</a><span>|</span><a href="#40403154">next</a><span>|</span><label class="collapse" for="c-40400512">[-]</label><label class="expand" for="c-40400512">[8 more]</label></div><br/><div class="children"><div class="content">Why? It’s not firewalled.</div><br/><div id="40401267" class="c"><input type="checkbox" id="c-40401267" checked=""/><div class="controls bullet"><span class="by">wut42</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40400512">parent</a><span>|</span><a href="#40400530">next</a><span>|</span><label class="collapse" for="c-40401267">[-]</label><label class="expand" for="c-40401267">[6 more]</label></div><br/><div class="children"><div class="content">It requires to be logged in to see other tweets in the thread, and the replies.</div><br/><div id="40403358" class="c"><input type="checkbox" id="c-40403358" checked=""/><div class="controls bullet"><span class="by">xyzzy4747</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40401267">parent</a><span>|</span><a href="#40400530">next</a><span>|</span><label class="collapse" for="c-40403358">[-]</label><label class="expand" for="c-40403358">[5 more]</label></div><br/><div class="children"><div class="content">Maybe people should just register accounts</div><br/><div id="40403391" class="c"><input type="checkbox" id="c-40403391" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40403358">parent</a><span>|</span><a href="#40400530">next</a><span>|</span><label class="collapse" for="c-40403391">[-]</label><label class="expand" for="c-40403391">[4 more]</label></div><br/><div class="children"><div class="content">ok elon</div><br/><div id="40403792" class="c"><input type="checkbox" id="c-40403792" checked=""/><div class="controls bullet"><span class="by">xyzzy4747</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40403391">parent</a><span>|</span><a href="#40400530">next</a><span>|</span><label class="collapse" for="c-40403792">[-]</label><label class="expand" for="c-40403792">[3 more]</label></div><br/><div class="children"><div class="content">Twitter has been around for almost 20 years now. There&#x27;s really no excuse</div><br/><div id="40403999" class="c"><input type="checkbox" id="c-40403999" checked=""/><div class="controls bullet"><span class="by">maxbond</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40403792">parent</a><span>|</span><a href="#40403863">next</a><span>|</span><label class="collapse" for="c-40403999">[-]</label><label class="expand" for="c-40403999">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s doesn&#x27;t matter if Twitter&#x2F;X has been around since the dawn of time, some people don&#x27;t want to make accounts, and it&#x27;s none of our business why. In the same way it&#x27;s none of our business why someone might not want to subscribe to NYT or WaPo when their articles are posted here. It&#x27;s a norm in this community to provide workarounds for walled content. The original poster in this sub thread was observing that this is now necessary for Twitter&#x2F;X as well - that shouldn&#x27;t be controversial or objectionable to anyone.</div><br/></div></div><div id="40403863" class="c"><input type="checkbox" id="c-40403863" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40403792">parent</a><span>|</span><a href="#40403999">prev</a><span>|</span><a href="#40400530">next</a><span>|</span><label class="collapse" for="c-40403863">[-]</label><label class="expand" for="c-40403863">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a good excuse: <a href="https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;twitter-leak-200-million-user-email-addresses&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;twitter-leak-200-million-user-em...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="40400530" class="c"><input type="checkbox" id="c-40400530" checked=""/><div class="controls bullet"><span class="by">rpastuszak</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40400512">parent</a><span>|</span><a href="#40401267">prev</a><span>|</span><a href="#40403154">next</a><span>|</span><label class="collapse" for="c-40400530">[-]</label><label class="expand" for="c-40400530">[1 more]</label></div><br/><div class="children"><div class="content">Mobile Twitter stopped working on phones with adblockers recently (even default ETP on FF or Safari). I suppose that’s the reason.</div><br/></div></div></div></div><div id="40400666" class="c"><input type="checkbox" id="c-40400666" checked=""/><div class="controls bullet"><span class="by">qp11</span><span>|</span><a href="#40400507">parent</a><span>|</span><a href="#40403154">prev</a><span>|</span><a href="#40400629">next</a><span>|</span><label class="collapse" for="c-40400666">[-]</label><label class="expand" for="c-40400666">[4 more]</label></div><br/><div class="children"><div class="content">You are not missing a single useful thing when a corporate robot speaks. Its like hoping mindless parasites start making music.<p>Besides all messaging is directed not at the plebs, but at the elite (those who own everything) signalling a simple thing - don&#x27;t worry - what ever flares up we are on top of things - we will firefight to death for you, so you just have to sit back on your fat ass and watch your net worth multiply on a cool dashboard - we will pander 24x7 - distract&#x2F;bribe&#x2F;lobby&#x2F;suck up limited attention - your cash is safe with us - keep sending us more so no one can compete with our &quot;free&quot; shit and we can achieve market capture asap.<p>The day someone finds a way to kill the parasites is the day you have to pay attention. Until then nothing to see other than the host getting drained.</div><br/><div id="40403121" class="c"><input type="checkbox" id="c-40403121" checked=""/><div class="controls bullet"><span class="by">cqqxo4zV46cp</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40400666">parent</a><span>|</span><a href="#40400629">next</a><span>|</span><label class="collapse" for="c-40403121">[-]</label><label class="expand" for="c-40403121">[3 more]</label></div><br/><div class="children"><div class="content">You’re literally in the comments for one of these statements though. Did you just come here aim an outsized lecture at whoever was unlucky enough to appear on your screen first? Either OP has a point, or you’re just saying that you parachuted here to say your piece without looking beyond the submission title. In either case, behaviour that makes HN worse.</div><br/><div id="40403477" class="c"><input type="checkbox" id="c-40403477" checked=""/><div class="controls bullet"><span class="by">chronic030511</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40403121">parent</a><span>|</span><a href="#40400629">next</a><span>|</span><label class="collapse" for="c-40403477">[-]</label><label class="expand" for="c-40403477">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI is dead and I feel bad for employees with equity PPU locked up.</div><br/><div id="40403810" class="c"><input type="checkbox" id="c-40403810" checked=""/><div class="controls bullet"><span class="by">xyzzy4747</span><span>|</span><a href="#40400507">root</a><span>|</span><a href="#40403477">parent</a><span>|</span><a href="#40400629">next</a><span>|</span><label class="collapse" for="c-40403810">[-]</label><label class="expand" for="c-40403810">[1 more]</label></div><br/><div class="children"><div class="content">If OpenAI is dead, which startups do you consider alive?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40400629" class="c"><input type="checkbox" id="c-40400629" checked=""/><div class="controls bullet"><span class="by">kthejoker2</span><span>|</span><a href="#40400507">prev</a><span>|</span><a href="#40400538">next</a><span>|</span><label class="collapse" for="c-40400629">[-]</label><label class="expand" for="c-40400629">[11 more]</label></div><br/><div class="children"><div class="content">Why is cult of personality so prevalent in the engineering &#x2F; hacker community?</div><br/><div id="40403352" class="c"><input type="checkbox" id="c-40403352" checked=""/><div class="controls bullet"><span class="by">djbusby</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40403288">next</a><span>|</span><label class="collapse" for="c-40403352">[-]</label><label class="expand" for="c-40403352">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more than just engineer community. Have seen it in art, music, clothing design, medTech, finTech and angel-money groups.<p>I think it&#x27;s just human condition to lean towards charismatic &quot;leaders&quot;. Like, they&#x27;re like you wanna be so...you love them.</div><br/></div></div><div id="40403288" class="c"><input type="checkbox" id="c-40403288" checked=""/><div class="controls bullet"><span class="by">maxbond</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40403352">prev</a><span>|</span><a href="#40400649">next</a><span>|</span><label class="collapse" for="c-40403288">[-]</label><label class="expand" for="c-40403288">[1 more]</label></div><br/><div class="children"><div class="content">At this moment in history, those communities are awash in money and power. Cults of personality nucleate around the wealthy and powerful.</div><br/></div></div><div id="40400649" class="c"><input type="checkbox" id="c-40400649" checked=""/><div class="controls bullet"><span class="by">grayhatter</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40403288">prev</a><span>|</span><a href="#40403777">next</a><span>|</span><label class="collapse" for="c-40400649">[-]</label><label class="expand" for="c-40400649">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not in hacker communities... hn is much more startup and pop culture, than hacker.<p>still has the occasional diamond though.</div><br/></div></div><div id="40403777" class="c"><input type="checkbox" id="c-40403777" checked=""/><div class="controls bullet"><span class="by">ip26</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40400649">prev</a><span>|</span><a href="#40403549">next</a><span>|</span><label class="collapse" for="c-40403777">[-]</label><label class="expand" for="c-40403777">[1 more]</label></div><br/><div class="children"><div class="content">The rate of change in technology and wild scaling amplifies winning moves. Perhaps this helps the winning players appear superhuman.</div><br/></div></div><div id="40403549" class="c"><input type="checkbox" id="c-40403549" checked=""/><div class="controls bullet"><span class="by">woopsn</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40403777">prev</a><span>|</span><a href="#40404220">next</a><span>|</span><label class="collapse" for="c-40403549">[-]</label><label class="expand" for="c-40403549">[1 more]</label></div><br/><div class="children"><div class="content">Personality seems to rule the product and venture capital side of things, and at the end of the day their side is in charge.</div><br/></div></div><div id="40404220" class="c"><input type="checkbox" id="c-40404220" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40403549">prev</a><span>|</span><a href="#40403805">next</a><span>|</span><label class="collapse" for="c-40404220">[-]</label><label class="expand" for="c-40404220">[1 more]</label></div><br/><div class="children"><div class="content">Not even nerds can escape from the curse of celebretyism.</div><br/></div></div><div id="40403805" class="c"><input type="checkbox" id="c-40403805" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40404220">prev</a><span>|</span><a href="#40403345">next</a><span>|</span><label class="collapse" for="c-40403805">[-]</label><label class="expand" for="c-40403805">[1 more]</label></div><br/><div class="children"><div class="content">Because to glorify is human. Younger humans are particularly prone to this.</div><br/></div></div><div id="40403345" class="c"><input type="checkbox" id="c-40403345" checked=""/><div class="controls bullet"><span class="by">pm90</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40403805">prev</a><span>|</span><a href="#40403552">next</a><span>|</span><label class="collapse" for="c-40403345">[-]</label><label class="expand" for="c-40403345">[1 more]</label></div><br/><div class="children"><div class="content">Because nerds don’t generally have any social capital and gravitate towards leaders that give them any by being in their orbit.</div><br/></div></div><div id="40403552" class="c"><input type="checkbox" id="c-40403552" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40403345">prev</a><span>|</span><a href="#40404006">next</a><span>|</span><label class="collapse" for="c-40403552">[-]</label><label class="expand" for="c-40403552">[1 more]</label></div><br/><div class="children"><div class="content">The personality is meaningless without the famous product</div><br/></div></div><div id="40404006" class="c"><input type="checkbox" id="c-40404006" checked=""/><div class="controls bullet"><span class="by">Groxx</span><span>|</span><a href="#40400629">parent</a><span>|</span><a href="#40403552">prev</a><span>|</span><a href="#40400538">next</a><span>|</span><label class="collapse" for="c-40404006">[-]</label><label class="expand" for="c-40404006">[1 more]</label></div><br/><div class="children"><div class="content">Money.<p>Absurd amounts of money.</div><br/></div></div></div></div><div id="40403491" class="c"><input type="checkbox" id="c-40403491" checked=""/><div class="controls bullet"><span class="by">jonathankoren</span><span>|</span><a href="#40400538">prev</a><span>|</span><a href="#40403722">next</a><span>|</span><label class="collapse" for="c-40403491">[-]</label><label class="expand" for="c-40403491">[3 more]</label></div><br/><div class="children"><div class="content">My thought about the demise of OAI’s AGI safety team is that they outlived their usefulness.<p>It was a marketing campaign. “We’re super cool. We’re super advanced. We hope don’t make the terminator! <i>wink</i> <i>wink</i>” “Oh no! GPT-2 is too dangerous! We better not release it! Oh wait. We did. ¯\_(ツ)_&#x2F;¯ “)<p>It was never a serious thing. Sure here and in the depths of weirdo forums people whispered about the AGI that can’t be named — Roko’s Basilisk. It was just comically stupid. Pascal’s Wager with extra steps.<p>Now OpenAI has tons of money and everyone wants to deal with them, so they don’t need that hype machine. Instead they’re saying stick their “dangerous” models in your toaster Maximum Overdrive style.<p>It was transparent</div><br/><div id="40403914" class="c"><input type="checkbox" id="c-40403914" checked=""/><div class="controls bullet"><span class="by">Phoenix5869</span><span>|</span><a href="#40403491">parent</a><span>|</span><a href="#40403722">next</a><span>|</span><label class="collapse" for="c-40403914">[-]</label><label class="expand" for="c-40403914">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It was a marketing campaign.
&gt; it was never a serious thing<p>Yeppppp. This is what i was saying for weeks and weeks, if not months. The amount of people on Reddit who can not see the puzzle pieces of “OpenAI sells AI” , “AI is hyped” , “more hype = more attention = more money and media coverage” and put them together, is actually shocking. Every time i would point this out on Reddit, more often than not i would get downvoted, and the responses were basically “well actually, AGI is around the corner”<p>This community is honestly SO MUCH BETTER than a lot of futurist subs. I might just stick to this site for a while.</div><br/><div id="40404607" class="c"><input type="checkbox" id="c-40404607" checked=""/><div class="controls bullet"><span class="by">dijksterhuis</span><span>|</span><a href="#40403491">root</a><span>|</span><a href="#40403914">parent</a><span>|</span><a href="#40403722">next</a><span>|</span><label class="collapse" for="c-40404607">[-]</label><label class="expand" for="c-40404607">[1 more]</label></div><br/><div class="children"><div class="content">I find there can be significant cargo culting [0] on HN too when any of these topics hit the front page: OpenAI &#x2F; ChatGPT &#x2F; LLMs; Zoom; E2EE; regulation &#x2F; policy; crypto currency &#x2F; NFTs [1] etc.<p>That said, I’m kind of glad to hear that Reddit is worse, as it puts the situation here on HN into some wider perspective (it’s not that bad here).<p>[0]: meant in the form of: closed minded absolutism without full contextual appreciation or understanding of the domain (what you seem to be describing as happening on Reddit)<p>[1]: less so these days, I wonder if that has something do with the current AI hype cycle <i>cough</i> opportunistic bandwagon jumping <i>cough</i> o_o</div><br/></div></div></div></div></div></div><div id="40403722" class="c"><input type="checkbox" id="c-40403722" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#40403491">prev</a><span>|</span><label class="collapse" for="c-40403722">[-]</label><label class="expand" for="c-40403722">[20 more]</label></div><br/><div class="children"><div class="content">What harms, in the real world, have GPTs produced? If few or none, does that not indicate that OpenAI has done a good job at safety? If so, should that not buy the company some credibility looking forward?<p>It’s fashionable, for reasons I am unaware, to hate on Sam Altman and OpenAI. Seems to be mostly the doomer crowd and it’s Blake Lemoine types (boy do his claims look ridiculous only 2 years on). To my thinking the open source crew poses a greater danger.</div><br/><div id="40403745" class="c"><input type="checkbox" id="c-40403745" checked=""/><div class="controls bullet"><span class="by">pdonis</span><span>|</span><a href="#40403722">parent</a><span>|</span><a href="#40403749">next</a><span>|</span><label class="collapse" for="c-40403745">[-]</label><label class="expand" for="c-40403745">[10 more]</label></div><br/><div class="children"><div class="content"><i>&gt; If few or none, does that not indicate that OpenAI has done a good job at safety?</i><p>No. It only indicates that OpenAI is less technically capable than it claims. To judge OpenAI&#x27;s competence at safety, you have to look at the ethics of their behavior, since building the right ethics into an AI before it gets powerful enough to do serious harm to humans is the essence of AI safety. And the ethics of OpenAI&#x27;s behavior, to put it as gently as possible, does not look good.</div><br/><div id="40403759" class="c"><input type="checkbox" id="c-40403759" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403745">parent</a><span>|</span><a href="#40403749">next</a><span>|</span><label class="collapse" for="c-40403759">[-]</label><label class="expand" for="c-40403759">[9 more]</label></div><br/><div class="children"><div class="content">Ok, what actions have they taken that are unethical?</div><br/><div id="40404802" class="c"><input type="checkbox" id="c-40404802" checked=""/><div class="controls bullet"><span class="by">ergl</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403759">parent</a><span>|</span><a href="#40403807">next</a><span>|</span><label class="collapse" for="c-40404802">[-]</label><label class="expand" for="c-40404802">[1 more]</label></div><br/><div class="children"><div class="content">Sharing a CEO with Worldcoin</div><br/></div></div><div id="40403807" class="c"><input type="checkbox" id="c-40403807" checked=""/><div class="controls bullet"><span class="by">pdonis</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403759">parent</a><span>|</span><a href="#40404802">prev</a><span>|</span><a href="#40403749">next</a><span>|</span><label class="collapse" for="c-40403807">[-]</label><label class="expand" for="c-40403807">[7 more]</label></div><br/><div class="children"><div class="content">Um, closing access to their model code when &quot;Open&quot; is right there in their name and was the original promise they made?<p>Plenty of other posts in this discussion give other examples.<p>That&#x27;s without even getting into all that happened with Altman leaving and then coming back, which has already been discussed to death in other HN threads.</div><br/><div id="40403833" class="c"><input type="checkbox" id="c-40403833" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403807">parent</a><span>|</span><a href="#40404315">next</a><span>|</span><label class="collapse" for="c-40403833">[-]</label><label class="expand" for="c-40403833">[2 more]</label></div><br/><div class="children"><div class="content">I can see how the switch to closed source could be annoying to some, but it definitely doesn’t rise to the level of “unethical” IMO. In fact, the explanation of the rationale for the decision seems perfectly reasonable. WRT to Altman’s ousting and return, I still haven’t seen any evidence of compromised ethics on his part. It looked a lot more like clumsy doomers botching a coup. A supermajority of OpenAIs employees, people who I’d venture to guess have a far better handle on Altmans ethics than you or I, insisted on his reinstatement. That’s enough of a refutation on that point for me. What else?</div><br/><div id="40404157" class="c"><input type="checkbox" id="c-40404157" checked=""/><div class="controls bullet"><span class="by">pdonis</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403833">parent</a><span>|</span><a href="#40404315">next</a><span>|</span><label class="collapse" for="c-40404157">[-]</label><label class="expand" for="c-40404157">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; I can see how the switch to closed source could be annoying to some, but it definitely doesn’t rise to the level of “unethical” IMO.</i><p>Sure it does. Particularly for a company that claims to be a responsible steward of a technology that could pose existential risk to humans. If you can&#x27;t even do a simple thing like keep your promises, how can I possibly trust you as a steward of a potential existential risk?<p><i>&gt; WRT to Altman’s ousting and return, I still haven’t seen any evidence of compromised ethics on his part.</i><p>I commented on this in a number of previous HN threads and don&#x27;t want to rehash it all here. (And btw, I also commented that I didn&#x27;t think the people who ousted Altman showed very good ethics or judgment either. I didn&#x27;t think anybody came out of that whole brouhaha looking good.) But just the fact that it happened at all should be a red flag. Again, these are people who claim to be stewards of a technology that they say can pose an existential risk. You&#x27;d expect them to at least be able to cooperate with <i>each other</i> like responsible adults.<p><i>&gt; A supermajority of OpenAIs employees, people who I’d venture to guess have a far better handle on Altmans ethics than you or I</i><p>They might well know Altman&#x27;s ethics better than we do, and be perfectly fine with them, because their own ethics are just as bad. They&#x27;re getting paid well and the fact that core promises of the company were broken is simply Not Their Problem.</div><br/></div></div></div></div><div id="40404315" class="c"><input type="checkbox" id="c-40404315" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403807">parent</a><span>|</span><a href="#40403833">prev</a><span>|</span><a href="#40404385">next</a><span>|</span><label class="collapse" for="c-40404315">[-]</label><label class="expand" for="c-40404315">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Um, closing access to their model code when &quot;Open&quot; is right there in their name and was the original promise they made?<p>Tech people who don&#x27;t like OpenAI have <i>got</i> to come up with a better line. Everytime I mention to someone normal that this is a complaint immediately eyes roll.<p>No one says &quot;North Korea is bad because they call themselves a Democratic People&#x27;s Republic despite having a dictator&quot; because that is incredibly not important nor was it ever expected.</div><br/></div></div><div id="40404385" class="c"><input type="checkbox" id="c-40404385" checked=""/><div class="controls bullet"><span class="by">motoxpro</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403807">parent</a><span>|</span><a href="#40404315">prev</a><span>|</span><a href="#40403749">next</a><span>|</span><label class="collapse" for="c-40404385">[-]</label><label class="expand" for="c-40404385">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s closed source but freely available to over 100 million people. To everyone but tech people, it&#x27;s open.<p>What&#x27;s more valuable for the normal person, Lllama, that I will never be able to use because I don&#x27;t know how to get it to run, or ChatGPT that I can use for whatever I want?</div><br/><div id="40404691" class="c"><input type="checkbox" id="c-40404691" checked=""/><div class="controls bullet"><span class="by">frenchman99</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40404385">parent</a><span>|</span><a href="#40404590">next</a><span>|</span><label class="collapse" for="c-40404691">[-]</label><label class="expand" for="c-40404691">[1 more]</label></div><br/><div class="children"><div class="content">Open AI not being open means it will be used in ways that will benefit shareholders, not humanity as they initially planned it, unless it so happens that humanity&#x27;s goals are aligned with shareholders&#x27;, but I&#x27;ve rarely seen this happen with the big four.</div><br/></div></div><div id="40404590" class="c"><input type="checkbox" id="c-40404590" checked=""/><div class="controls bullet"><span class="by">progbits</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40404385">parent</a><span>|</span><a href="#40404691">prev</a><span>|</span><a href="#40403749">next</a><span>|</span><label class="collapse" for="c-40404590">[-]</label><label class="expand" for="c-40404590">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ChatGPT that I can use for whatever I want<p>For whatever openai wants which is a shrinking set.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40403749" class="c"><input type="checkbox" id="c-40403749" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#40403722">parent</a><span>|</span><a href="#40403745">prev</a><span>|</span><a href="#40404474">next</a><span>|</span><label class="collapse" for="c-40403749">[-]</label><label class="expand" for="c-40403749">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>If few or none, does that not indicate that OpenAI has done a good job at safety?</i><p>I have a rock that keeps lions away. I know it works, because I haven&#x27;t seen any lions.</div><br/><div id="40403774" class="c"><input type="checkbox" id="c-40403774" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403749">parent</a><span>|</span><a href="#40404474">next</a><span>|</span><label class="collapse" for="c-40403774">[-]</label><label class="expand" for="c-40403774">[3 more]</label></div><br/><div class="children"><div class="content">Irrelevant metaphor. On what basis has Altman&#x2F;OpenAI earned the distrust of so many in the HN comments? All I seem to find are speculation about the future and what reads a lot like petty jealousy. Where is the evidence of the lions on the prowl? Surely we should insist on a gazelle corpse or at least some paw prints before declaring an emergency and going on a lion hunt.</div><br/><div id="40404314" class="c"><input type="checkbox" id="c-40404314" checked=""/><div class="controls bullet"><span class="by">maxbond</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403774">parent</a><span>|</span><a href="#40404572">next</a><span>|</span><label class="collapse" for="c-40404314">[-]</label><label class="expand" for="c-40404314">[1 more]</label></div><br/><div class="children"><div class="content">&gt; On what basis has Altman&#x2F;OpenAI earned the distrust of so many in the HN comments? All I seem to find are speculation about the future and what reads a lot like petty jealousy.<p>Genuine question, do you care to know? Or are you only asking this question as a prelude to delivering insult?<p>When you ask why people believe X in one breath and then suggest it&#x27;s because of &quot;petty jealousy&quot; in the next, you&#x27;ll succeed at conveying your dim view of them but fail to create a conversation where it is possible for you to learn something about why people think that way.<p>Jealousy is a particularly slippery hypothesis because it is fully generalizable. You can explain any criticism of anybody as jealousy. It is my observation that people often accuse others of jealousy when they want to dismiss a perspective. In that way, it functions less as an explanation than as a defense mechanism; an escape hatch from the uncomfortable feeling of simultaneously disagreeing with someone and acknowledging that there perspective might not be meritless.</div><br/></div></div><div id="40404572" class="c"><input type="checkbox" id="c-40404572" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403774">parent</a><span>|</span><a href="#40404314">prev</a><span>|</span><a href="#40404474">next</a><span>|</span><label class="collapse" for="c-40404572">[-]</label><label class="expand" for="c-40404572">[1 more]</label></div><br/><div class="children"><div class="content">you dont get trust by default. It needs to be earned.</div><br/></div></div></div></div></div></div><div id="40404474" class="c"><input type="checkbox" id="c-40404474" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#40403722">parent</a><span>|</span><a href="#40403749">prev</a><span>|</span><a href="#40404514">next</a><span>|</span><label class="collapse" for="c-40404474">[-]</label><label class="expand" for="c-40404474">[1 more]</label></div><br/><div class="children"><div class="content">&gt;for reasons I am unaware, to hate on Sam Altman and OpenAI.<p>Maybe because they ditched the Open in OpenAI and sold out to MS and now Apple too.<p>For the benefit of all mankind was replaced by &quot;Show me the money&quot;</div><br/></div></div><div id="40404514" class="c"><input type="checkbox" id="c-40404514" checked=""/><div class="controls bullet"><span class="by">SpicyLemonZest</span><span>|</span><a href="#40403722">parent</a><span>|</span><a href="#40404474">prev</a><span>|</span><a href="#40403760">next</a><span>|</span><label class="collapse" for="c-40404514">[-]</label><label class="expand" for="c-40404514">[1 more]</label></div><br/><div class="children"><div class="content">One big harm is that GPT4 can trivially cheat essays in most contexts. OpenAI planned to address this with a classifier (<a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;new-ai-classifier-for-indicating-ai-written-text&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;new-ai-classifier-for-indicating-ai...</a>), but it didn&#x27;t work and they withdrew it without offering any better ideas - so now evaluations of unmonitored writing are indefinitely and likely permanently broken for everyone.</div><br/></div></div><div id="40403760" class="c"><input type="checkbox" id="c-40403760" checked=""/><div class="controls bullet"><span class="by">nvy</span><span>|</span><a href="#40403722">parent</a><span>|</span><a href="#40404514">prev</a><span>|</span><label class="collapse" for="c-40403760">[-]</label><label class="expand" for="c-40403760">[3 more]</label></div><br/><div class="children"><div class="content">&gt;It’s fashionable, for reasons I am unaware, to hate on Sam Altman and OpenAI<p>Perhaps because he&#x27;s a slimy used-car salesman who got rich doing things that are illegal for you and me (ripping off copyrighted works en masse) and he pulled a bait and switch as far as for-&#x2F;non-profit is concerned.<p>He&#x27;s everything wrong with Silicon Valley&#x2F;Venture Capital&#x2F;faux meritocratic culture rolled into one.</div><br/><div id="40403813" class="c"><input type="checkbox" id="c-40403813" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403760">parent</a><span>|</span><label class="collapse" for="c-40403813">[-]</label><label class="expand" for="c-40403813">[2 more]</label></div><br/><div class="children"><div class="content">Altman was rich before OpenAI, so that claim doesn’t hold much water. It is a legitimate and open question as to what fair use constitutes when it comes to training models on copyrighted content. It will definitely be settled in the courts, if not by legislatures in the coming years. Apart from ad hominem attacks and matters about which there are legitimate differences of opinion, I return to my original question: what significant harms have OpenAIs products yielded? It is obvious that a raw LLM of GPT-4s power could do some harm in the wrong hands. Therefore, if you can’t point to real harms, it is reasonable to conclude, at the very least, that Altman&#x2F;OpenAI might not be the wrong hands.</div><br/><div id="40404187" class="c"><input type="checkbox" id="c-40404187" checked=""/><div class="controls bullet"><span class="by">nvy</span><span>|</span><a href="#40403722">root</a><span>|</span><a href="#40403813">parent</a><span>|</span><label class="collapse" for="c-40404187">[-]</label><label class="expand" for="c-40404187">[1 more]</label></div><br/><div class="children"><div class="content">The fact that you cannot perceive the harms, or that they are not yet apparent, does not mean they do not exist.<p>Look at Facebook and other social media: at first we all thought it was great and then we realized it&#x27;s a giant depression machine, and all it&#x27;s really good for is giving young girls eating disorders.<p>But since we&#x27;re on the topic, the immediate casualties seem to be the livelihoods of authors and artists, on whose content the LLMs were trained and who received no compensation.<p>And now every publisher is inundated with infinite ChatGPT-created books.  We won&#x27;t get into the climate impact of wasting all that compute just to create a better phishing-email generator.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>