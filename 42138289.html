<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1731661284293" as="style"/><link rel="stylesheet" href="styles.css?v=1731661284293"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://dynomight.substack.com/p/chess">Something weird is happening with LLMs and chess</a> <span class="domain">(<a href="https://dynomight.substack.com">dynomight.substack.com</a>)</span></div><div class="subtext"><span>crescit_eundo</span> | <span>181 comments</span></div><br/><div><div id="42144784" class="c"><input type="checkbox" id="c-42144784" checked=""/><div class="controls bullet"><span class="by">swiftcoder</span><span>|</span><a href="#42144943">next</a><span>|</span><label class="collapse" for="c-42144784">[-]</label><label class="expand" for="c-42144784">[1 more]</label></div><br/><div class="children"><div class="content">I feel like the article neglects one obvious possibility: that OpenAI decided that chess was a benchmark worth &quot;winning&quot;, special-cases chess within gpt-3.5-turbo-instruct, and then neglected to add that special-case to follow-up models since it wasn&#x27;t generating sustained press coverage.</div><br/></div></div><div id="42144943" class="c"><input type="checkbox" id="c-42144943" checked=""/><div class="controls bullet"><span class="by">snickerbockers</span><span>|</span><a href="#42144784">prev</a><span>|</span><a href="#42144644">next</a><span>|</span><label class="collapse" for="c-42144943">[-]</label><label class="expand" for="c-42144943">[2 more]</label></div><br/><div class="children"><div class="content">Does it ever try an illegal move?  OP didn&#x27;t mention this and I think it&#x27;s inevitable that it should happen at least once, since the rules of chess are fairly arbitrary and LLMs are notorious for bullshitting their way through difficult problems when we&#x27;d rather they just admit that they don&#x27;t have the answer.</div><br/><div id="42145004" class="c"><input type="checkbox" id="c-42145004" checked=""/><div class="controls bullet"><span class="by">sethherr</span><span>|</span><a href="#42144943">parent</a><span>|</span><a href="#42144644">next</a><span>|</span><label class="collapse" for="c-42145004">[-]</label><label class="expand" for="c-42145004">[1 more]</label></div><br/><div class="children"><div class="content">Yes, he discusses using a grammar to restrict to only legal moves</div><br/></div></div></div></div><div id="42144644" class="c"><input type="checkbox" id="c-42144644" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#42144943">prev</a><span>|</span><a href="#42142885">next</a><span>|</span><label class="collapse" for="c-42144644">[-]</label><label class="expand" for="c-42144644">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I ran all the open models (anything not from OpenAI, meaning anything that doesn’t start with gpt or o1) myself using Q5_K_M quantization, whatever that is.<p>It&#x27;s just a lossy compression of all of the parameters, probably not important, right?</div><br/></div></div><div id="42142885" class="c"><input type="checkbox" id="c-42142885" checked=""/><div class="controls bullet"><span class="by">niobe</span><span>|</span><a href="#42144644">prev</a><span>|</span><a href="#42145064">next</a><span>|</span><label class="collapse" for="c-42142885">[-]</label><label class="expand" for="c-42142885">[64 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why educated people expect that an LLM <i>would</i> be able to play chess at a decent level.<p>It has no idea about the quality of it&#x27;s data. &quot;Act like x&quot; prompts are no substitute for actual reasoning and deterministic computation which clearly chess requires.</div><br/><div id="42143949" class="c"><input type="checkbox" id="c-42143949" checked=""/><div class="controls bullet"><span class="by">xelxebar</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42144041">next</a><span>|</span><label class="collapse" for="c-42143949">[-]</label><label class="expand" for="c-42143949">[4 more]</label></div><br/><div class="children"><div class="content">Then you should be surprised that turbo-instruct actually plays well, right? We see a proliferation of hand-wavy arguments based on unfounded anthropomorphic intuitions about &quot;actual reasoning&quot; and whatnot. I think this is good evidence that nobody really understands what&#x27;s going on.<p>If some mental model says that LLMs should be bad at chess, then it fails to explain why we have LLMs playing strong chess. If another mental model says the inverse, then it fails to explain why so many of these large models fail spectacularly at chess.<p>Clearly, there&#x27;s more going on here.</div><br/><div id="42145060" class="c"><input type="checkbox" id="c-42145060" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143949">parent</a><span>|</span><a href="#42144358">next</a><span>|</span><label class="collapse" for="c-42145060">[-]</label><label class="expand" for="c-42145060">[1 more]</label></div><br/><div class="children"><div class="content">There are some who suggest that modern chess is mostly a game of memorization and not one particularly of strategy or skill.  I assume this is why variants like speed chess exist.<p>In this scope,  my mental model is that LLMs would be good at modern style long form chess,  but would likely be easy to trip up with certain types of move combinations that most humans would not normally use.  My prediction is that once found they would be comically susceptible to these patterns.<p>Clearly,  we have no real basis for saying it is &quot;good&quot; or &quot;bad&quot; at chess,  and even using chess performance as an measurement sample is a highly biased decision, likely born out of marketing rather than principle.</div><br/></div></div><div id="42144358" class="c"><input type="checkbox" id="c-42144358" checked=""/><div class="controls bullet"><span class="by">flyingcircus3</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143949">parent</a><span>|</span><a href="#42145060">prev</a><span>|</span><a href="#42144041">next</a><span>|</span><label class="collapse" for="c-42144358">[-]</label><label class="expand" for="c-42144358">[2 more]</label></div><br/><div class="children"><div class="content">&quot;playing strong chess&quot; would be a much less hand-wavy claim if there were lots of independent methods of quantifying and verifying the strength of stockfish&#x27;s lowest difficulty setting.  I honestly don&#x27;t know if that exists or not.  But unless it does, why would stockfish&#x27;s lowest difficulty setting be a meaningful threshold?</div><br/><div id="42144495" class="c"><input type="checkbox" id="c-42144495" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42144358">parent</a><span>|</span><a href="#42144041">next</a><span>|</span><label class="collapse" for="c-42144495">[-]</label><label class="expand" for="c-42144495">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve tried it myself, GPT-3.5-turbo-instruct was at least somewhere in the rabge 1600-1800 ELO.</div><br/></div></div></div></div></div></div><div id="42144041" class="c"><input type="checkbox" id="c-42144041" checked=""/><div class="controls bullet"><span class="by">mannykannot</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42143949">prev</a><span>|</span><a href="#42144487">next</a><span>|</span><label class="collapse" for="c-42144041">[-]</label><label class="expand" for="c-42144041">[4 more]</label></div><br/><div class="children"><div class="content">One of the main purposes of running experiments of any sort is to find out if our preconceptions are accurate. Of course, if someone is not interested in that question, they might as well choose not to look through the telescope.</div><br/><div id="42144411" class="c"><input type="checkbox" id="c-42144411" checked=""/><div class="controls bullet"><span class="by">bowsamic</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42144041">parent</a><span>|</span><a href="#42144487">next</a><span>|</span><label class="collapse" for="c-42144411">[-]</label><label class="expand" for="c-42144411">[3 more]</label></div><br/><div class="children"><div class="content">Sadly there’s a common sentiment on HN that testing obvious assumptions is a waste of time</div><br/><div id="42144775" class="c"><input type="checkbox" id="c-42144775" checked=""/><div class="controls bullet"><span class="by">BlindEyeHalo</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42144411">parent</a><span>|</span><a href="#42144487">next</a><span>|</span><label class="collapse" for="c-42144775">[-]</label><label class="expand" for="c-42144775">[2 more]</label></div><br/><div class="children"><div class="content">Not only on HN. Trying to publish a scientific article that does not contain the word &#x27;novel&#x27; has become almost impossible. No one is trying to reproduce anyones claims anymore.</div><br/><div id="42145089" class="c"><input type="checkbox" id="c-42145089" checked=""/><div class="controls bullet"><span class="by">pcf</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42144775">parent</a><span>|</span><a href="#42144487">next</a><span>|</span><label class="collapse" for="c-42145089">[-]</label><label class="expand" for="c-42145089">[1 more]</label></div><br/><div class="children"><div class="content">Do you think this bias is part of the replication crisis in science?</div><br/></div></div></div></div></div></div></div></div><div id="42144487" class="c"><input type="checkbox" id="c-42144487" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42144041">prev</a><span>|</span><a href="#42143060">next</a><span>|</span><label class="collapse" for="c-42144487">[-]</label><label class="expand" for="c-42144487">[1 more]</label></div><br/><div class="children"><div class="content">Because it&#x27;s a straight forward stochastic sequence modelling task and I&#x27;ve seen GPT-3.5-turbo-instruct play at high amateur level myself. But it seems like all the RLHF and distillation that is done on newer models destroys that ability.</div><br/></div></div><div id="42143060" class="c"><input type="checkbox" id="c-42143060" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42144487">prev</a><span>|</span><a href="#42142963">next</a><span>|</span><label class="collapse" for="c-42143060">[-]</label><label class="expand" for="c-42143060">[27 more]</label></div><br/><div class="children"><div class="content">This is a puzzle given enough training information. LLM can successfully print out the status of the board after the given moves. It can also produce a not-terrible summary of the position and is able to list dangers at least one move ahead. Decent is subjective, but that should beat at least beginners. And the lowest level of stockfish used in the blog post is lowest intermediate.<p>I don&#x27;t know really what level we should be thinking of here, but I don&#x27;t see any reason to dismiss the idea. Also, it really depends on whether you&#x27;re thinking of the current public implementations of the tech, or the LLM idea in general. If we wanted to get better results, we could feed it way more chess books and past game analysis.</div><br/><div id="42143139" class="c"><input type="checkbox" id="c-42143139" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143060">parent</a><span>|</span><a href="#42143871">next</a><span>|</span><label class="collapse" for="c-42143139">[-]</label><label class="expand" for="c-42143139">[25 more]</label></div><br/><div class="children"><div class="content">LLMs like GPT aren’t built to play chess, and here’s why: they’re made for handling language, not playing games with strict rules and strategies. Chess engines, like Stockfish, are designed specifically for analyzing board positions and making the best moves, but LLMs don’t even &quot;see&quot; the board. They’re just guessing moves based on text patterns, without understanding the game itself.<p>Plus, LLMs have limited memory, so they struggle to remember previous moves in a long game. It’s like trying to play blindfolded! They’re great at explaining chess concepts or moves but not actually competing in a match.</div><br/><div id="42143316" class="c"><input type="checkbox" id="c-42143316" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143139">parent</a><span>|</span><a href="#42144497">next</a><span>|</span><label class="collapse" for="c-42143316">[-]</label><label class="expand" for="c-42143316">[20 more]</label></div><br/><div class="children"><div class="content">&gt; but LLMs don’t even &quot;see&quot; the board<p>This is a very vague claim, but they <i>can</i> reconstruct the board from the list of moves, which I would say proves this wrong.<p>&gt; LLMs have limited memory<p>For the recent models this is not a problem for the chess example. You can feed whole books into them if you want to.<p>&gt; so they struggle to remember previous moves<p>Chess is stateless with perfect information. Unless you&#x27;re going for mind games, you don&#x27;t need to remember previous moves.<p>&gt; They’re great at explaining chess concepts or moves but not actually competing in a match.<p>What&#x27;s the difference between a great explanation of a move and explaining every possible move then selecting the best one?</div><br/><div id="42143484" class="c"><input type="checkbox" id="c-42143484" checked=""/><div class="controls bullet"><span class="by">sfmz</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143316">parent</a><span>|</span><a href="#42143465">next</a><span>|</span><label class="collapse" for="c-42143484">[-]</label><label class="expand" for="c-42143484">[8 more]</label></div><br/><div class="children"><div class="content">Chess is not stateless.  En Passant requires last move and castling rights requires nearly all previous moves.<p><a href="https:&#x2F;&#x2F;adamkarvonen.github.io&#x2F;machine_learning&#x2F;2024&#x2F;01&#x2F;03&#x2F;chess-world-models.html" rel="nofollow">https:&#x2F;&#x2F;adamkarvonen.github.io&#x2F;machine_learning&#x2F;2024&#x2F;01&#x2F;03&#x2F;c...</a></div><br/><div id="42143592" class="c"><input type="checkbox" id="c-42143592" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143484">parent</a><span>|</span><a href="#42143465">next</a><span>|</span><label class="collapse" for="c-42143592">[-]</label><label class="expand" for="c-42143592">[7 more]</label></div><br/><div class="children"><div class="content">Ok, I did go too far. But castling doesn&#x27;t require all previous moves - only one bit of information carried over. So in practice that&#x27;s board + 2 bits per player. (or 1 bit and 2 moves if you want to include a draw)</div><br/><div id="42143633" class="c"><input type="checkbox" id="c-42143633" checked=""/><div class="controls bullet"><span class="by">aaronchall</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143592">parent</a><span>|</span><a href="#42143465">next</a><span>|</span><label class="collapse" for="c-42143633">[-]</label><label class="expand" for="c-42143633">[6 more]</label></div><br/><div class="children"><div class="content">Castling requires no prior moves by either piece (King or Rook). Move the King once and back early on, and later, although the board looks set for castling, the King may not.</div><br/><div id="42143643" class="c"><input type="checkbox" id="c-42143643" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143633">parent</a><span>|</span><a href="#42143465">next</a><span>|</span><label class="collapse" for="c-42143643">[-]</label><label class="expand" for="c-42143643">[5 more]</label></div><br/><div class="children"><div class="content">Yes, which means you carry one bit of extra information - &quot;is castling still allowed&quot;. The specific moves that resulted in this bit being unset don&#x27;t matter.</div><br/><div id="42143680" class="c"><input type="checkbox" id="c-42143680" checked=""/><div class="controls bullet"><span class="by">aaronchall</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143643">parent</a><span>|</span><a href="#42143465">next</a><span>|</span><label class="collapse" for="c-42143680">[-]</label><label class="expand" for="c-42143680">[4 more]</label></div><br/><div class="children"><div class="content">Ok, then for this you need minimum of two bits - one for kingside Rook and one for the queenside Rook, both would be set if you move the King. You also need to count moves since the last exchange or pawn move for the 50 move rule.</div><br/><div id="42143705" class="c"><input type="checkbox" id="c-42143705" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143680">parent</a><span>|</span><a href="#42143465">next</a><span>|</span><label class="collapse" for="c-42143705">[-]</label><label class="expand" for="c-42143705">[3 more]</label></div><br/><div class="children"><div class="content">Ah, that one&#x27;s cool - I&#x27;ve got to admit I&#x27;ve never heard of the 50 move rule.</div><br/><div id="42143935" class="c"><input type="checkbox" id="c-42143935" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143705">parent</a><span>|</span><a href="#42143465">next</a><span>|</span><label class="collapse" for="c-42143935">[-]</label><label class="expand" for="c-42143935">[2 more]</label></div><br/><div class="children"><div class="content">Also the 3x repetition rule.</div><br/><div id="42144595" class="c"><input type="checkbox" id="c-42144595" checked=""/><div class="controls bullet"><span class="by">chipsrafferty</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143935">parent</a><span>|</span><a href="#42143465">next</a><span>|</span><label class="collapse" for="c-42144595">[-]</label><label class="expand" for="c-42144595">[1 more]</label></div><br/><div class="children"><div class="content">And 5x repetition rule</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42143465" class="c"><input type="checkbox" id="c-42143465" checked=""/><div class="controls bullet"><span class="by">mjcohen</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143316">parent</a><span>|</span><a href="#42143484">prev</a><span>|</span><a href="#42143533">next</a><span>|</span><label class="collapse" for="c-42143465">[-]</label><label class="expand" for="c-42143465">[2 more]</label></div><br/><div class="children"><div class="content">Chess is not stateless. Three repetitions of same position is a draw.</div><br/><div id="42144802" class="c"><input type="checkbox" id="c-42144802" checked=""/><div class="controls bullet"><span class="by">Someone</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143465">parent</a><span>|</span><a href="#42143533">next</a><span>|</span><label class="collapse" for="c-42144802">[-]</label><label class="expand" for="c-42144802">[1 more]</label></div><br/><div class="children"><div class="content">Yes, there’s state there that’s not in the board position, but technically, threefold repetition is not a draw. Play can go on. <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Threefold_repetition" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Threefold_repetition</a>:<p><i>“The game is not automatically drawn if a position occurs for the third time – one of the players, on their turn, must claim the draw with the arbiter. The claim must be made either before making the move which will produce the third repetition, or after the opponent has made a move producing a third repetition. By contrast, the fivefold repetition rule requires the arbiter to intervene and declare the game drawn if the same position occurs five times, needing no claim by the players.”</i></div><br/></div></div></div></div><div id="42143533" class="c"><input type="checkbox" id="c-42143533" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143316">parent</a><span>|</span><a href="#42143465">prev</a><span>|</span><a href="#42143481">next</a><span>|</span><label class="collapse" for="c-42143533">[-]</label><label class="expand" for="c-42143533">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Chess is stateless with perfect information.</i><p>It is not stateless, because good chess isn&#x27;t played as a series of independent moves -- it&#x27;s played as a series of moves connected to a player&#x27;s strategy.<p>&gt; <i>What&#x27;s the difference between a great explanation of a move and explaining every possible move then selecting the best one?</i><p>Continuing from the above, &quot;best&quot; in the latter sense involves understanding possible future moves <i>after</i> the next move.<p>Ergo, if I looked at all games with the current board state and chose the next move that won the most games, it&#x27;d be tactically sound but strategically ignorant.<p>Because many of those next moves were making that next move <i>in support of</i> some broader strategy.</div><br/><div id="42143634" class="c"><input type="checkbox" id="c-42143634" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143533">parent</a><span>|</span><a href="#42144422">next</a><span>|</span><label class="collapse" for="c-42143634">[-]</label><label class="expand" for="c-42143634">[2 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s played as a series of moves connected to a player&#x27;s strategy.<p>That state belongs to the player, not to the game. You can carry your own state in any game you want - for example remember who starts with what move in rock paper scissors, but that doesn&#x27;t make that game stateful. It&#x27;s the player&#x27;s decision (or bot&#x27;s implementation) to use any extra state or not.<p>I wrote &quot;previous moves&quot; specifically (and the extra bits already addressed elsewhere), but the LLM can carry&#x2F;rebuild its internal state between the steps.</div><br/><div id="42143743" class="c"><input type="checkbox" id="c-42143743" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143634">parent</a><span>|</span><a href="#42144422">next</a><span>|</span><label class="collapse" for="c-42143743">[-]</label><label class="expand" for="c-42143743">[1 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re talking about LLMs, then the state belongs to it.<p>So even if the rules of chess are (mostly) stateless, the resulting game itself is not.<p>Thus, you can&#x27;t dismiss concerns about LLMs having difficulty tracking state by saying that chess is stateless. It&#x27;s not, in that sense.</div><br/></div></div></div></div><div id="42144422" class="c"><input type="checkbox" id="c-42144422" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143533">parent</a><span>|</span><a href="#42143634">prev</a><span>|</span><a href="#42143481">next</a><span>|</span><label class="collapse" for="c-42144422">[-]</label><label class="expand" for="c-42144422">[2 more]</label></div><br/><div class="children"><div class="content">&gt; good chess isn&#x27;t played as a series of independent moves -- it&#x27;s played as a series of moves connected to a player&#x27;s strategy.<p>Maybe good chess, but not perfect chess. That would by definition be game-theoretically optimal, which in turn implies having to maintain no state other than your position in a large but precomputable game tree.</div><br/><div id="42144634" class="c"><input type="checkbox" id="c-42144634" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42144422">parent</a><span>|</span><a href="#42143481">next</a><span>|</span><label class="collapse" for="c-42144634">[-]</label><label class="expand" for="c-42144634">[1 more]</label></div><br/><div class="children"><div class="content">Right, but your position also includes whether or not you still have the right to castle on either side, whether each pawn has the right to capture en passant or not, the number of moves since the last pawn move or capture (for tracking the 50 move rule), and whether or not the current position has ever appeared on the board once or twice prior (so you can claim a draw by threefold repetition).<p>So in practice, your position actually includes the log of all moves to that point. That’s a lot more state than just what you can see on the board.</div><br/></div></div></div></div></div></div><div id="42143481" class="c"><input type="checkbox" id="c-42143481" checked=""/><div class="controls bullet"><span class="by">cool_dude85</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143316">parent</a><span>|</span><a href="#42143533">prev</a><span>|</span><a href="#42144497">next</a><span>|</span><label class="collapse" for="c-42143481">[-]</label><label class="expand" for="c-42143481">[4 more]</label></div><br/><div class="children"><div class="content">&gt;Chess is stateless with perfect information. Unless you&#x27;re going for mind games, you don&#x27;t need to remember previous moves.<p>In what sense is chess stateless? Question: is Rxa6 a legal move? You need board state to refer to in order to decide.</div><br/><div id="42143555" class="c"><input type="checkbox" id="c-42143555" checked=""/><div class="controls bullet"><span class="by">aetherson</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143481">parent</a><span>|</span><a href="#42144497">next</a><span>|</span><label class="collapse" for="c-42143555">[-]</label><label class="expand" for="c-42143555">[3 more]</label></div><br/><div class="children"><div class="content">They mean that you only need board position, you don&#x27;t need the previous moves that led to that board position.<p>There are at least a couple of exceptions to that as far as I know.</div><br/><div id="42144645" class="c"><input type="checkbox" id="c-42144645" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143555">parent</a><span>|</span><a href="#42143938">next</a><span>|</span><label class="collapse" for="c-42144645">[-]</label><label class="expand" for="c-42144645">[1 more]</label></div><br/><div class="children"><div class="content">Yes, 4 exceptions: castling rights, legal en passant captures, threefold repetition, and the 50 move rule. You actually need quite a lot of state to track all of those.</div><br/></div></div><div id="42143938" class="c"><input type="checkbox" id="c-42143938" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143555">parent</a><span>|</span><a href="#42144645">prev</a><span>|</span><a href="#42144497">next</a><span>|</span><label class="collapse" for="c-42143938">[-]</label><label class="expand" for="c-42143938">[1 more]</label></div><br/><div class="children"><div class="content">The correct phrasing would be is it a Markov process?</div><br/></div></div></div></div></div></div></div></div><div id="42144497" class="c"><input type="checkbox" id="c-42144497" checked=""/><div class="controls bullet"><span class="by">codebolt</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143139">parent</a><span>|</span><a href="#42143316">prev</a><span>|</span><a href="#42143409">next</a><span>|</span><label class="collapse" for="c-42144497">[-]</label><label class="expand" for="c-42144497">[2 more]</label></div><br/><div class="children"><div class="content">&gt; they’re made for handling language, not playing games with strict rules and strategies<p>Here&#x27;s the opposite theory: Language encodes objective reasoning (or at least, it does some of the time). A sufficiently large ANN trained on sufficiently large amounts of text will develop internal mechanisms of reasoning that can be applied to domains outside of language.<p>Based on what we are currently seeing LLMs do, I&#x27;m becoming more and more convinced that this is the correct picture.</div><br/><div id="42144685" class="c"><input type="checkbox" id="c-42144685" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42144497">parent</a><span>|</span><a href="#42143409">next</a><span>|</span><label class="collapse" for="c-42144685">[-]</label><label class="expand" for="c-42144685">[1 more]</label></div><br/><div class="children"><div class="content">I share this idea but from the different perspective. It doesn’t develop these mechanisms, but casts a high-dimensional-enough shadow of their effect on itself. This vaguely explains why the more deep Gell-Mann-wise you are the less sharp that shadow is, because specificity cuts off “reasoning” hyperplanes.<p>It’s hard to explain emerging <i>mechanisms</i> because of the nature of generation, which is one-pass sequential matrix reduction. I say this while waving my hands, but listen. Reasoning is similar to Turing complete algorithms, and what LLMs can become through training is similar to limited pushdown automata at best. I think this is a good conceptual handle for it.<p>“Line of thought” is an interesting way to loop the process back, but it doesn’t show <i>that</i> much improvement, afaiu, and still is finite.<p>Otoh, a chess player takes as much time and “loops” as they need to get the result (ignoring competitive time limits).</div><br/></div></div></div></div><div id="42143409" class="c"><input type="checkbox" id="c-42143409" checked=""/><div class="controls bullet"><span class="by">jerska</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143139">parent</a><span>|</span><a href="#42144497">prev</a><span>|</span><a href="#42143940">next</a><span>|</span><label class="collapse" for="c-42143409">[-]</label><label class="expand" for="c-42143409">[1 more]</label></div><br/><div class="children"><div class="content">LLMs need to compress information to be able to predict next words in as many contexts as possible.<p>Chess moves are simply tokens as any other.
Given enough chess training data, it would make sense to have part of the network trained to handle chess specifically instead of simply encoding basic lists of moves and follow-ups. The result would be a general purpose sub-network trained on chess.</div><br/></div></div><div id="42143940" class="c"><input type="checkbox" id="c-42143940" checked=""/><div class="controls bullet"><span class="by">zeckalpha</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143139">parent</a><span>|</span><a href="#42143409">prev</a><span>|</span><a href="#42143871">next</a><span>|</span><label class="collapse" for="c-42143940">[-]</label><label class="expand" for="c-42143940">[1 more]</label></div><br/><div class="children"><div class="content">Language is a game with strict rules and strategies.</div><br/></div></div></div></div><div id="42143871" class="c"><input type="checkbox" id="c-42143871" checked=""/><div class="controls bullet"><span class="by">shric</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143060">parent</a><span>|</span><a href="#42143139">prev</a><span>|</span><a href="#42142963">next</a><span>|</span><label class="collapse" for="c-42143871">[-]</label><label class="expand" for="c-42143871">[1 more]</label></div><br/><div class="children"><div class="content">Stockfish level 1 is well below &quot;lowest intermediate&quot;.<p>A friend of mine just started playing chess a few weeks ago and can beat it about 25% of the time.<p>It will hang pieces, and you can hang your own queen and there&#x27;s about a 50% chance it won&#x27;t be taken.</div><br/></div></div></div></div><div id="42142963" class="c"><input type="checkbox" id="c-42142963" checked=""/><div class="controls bullet"><span class="by">computerex</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42143060">prev</a><span>|</span><a href="#42144621">next</a><span>|</span><label class="collapse" for="c-42142963">[-]</label><label class="expand" for="c-42142963">[13 more]</label></div><br/><div class="children"><div class="content">Question here is why gpt-3.5-instruct can then beat stockfish.</div><br/><div id="42142975" class="c"><input type="checkbox" id="c-42142975" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42142963">parent</a><span>|</span><a href="#42143181">next</a><span>|</span><label class="collapse" for="c-42142975">[-]</label><label class="expand" for="c-42142975">[6 more]</label></div><br/><div class="children"><div class="content">PS: I ran and as suspected got-3.5-turbo-instruct does not beat stockfish, it is not even close
&quot;Final Results: gpt-3.5-turbo-instruct: Wins=0, Losses=6, Draws=0, Rating=1500.00 stockfish: Wins=6, Losses=0, Draws=0, Rating=1500.00&quot;
<a href="https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;870ea03197b3471eaf7e26e9b17e1754?sid=073f0b78-7be9-42af-95c2-240d3b49d1c8" rel="nofollow">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;870ea03197b3471eaf7e26e9b17e1754?...</a></div><br/><div id="42142993" class="c"><input type="checkbox" id="c-42142993" checked=""/><div class="controls bullet"><span class="by">computerex</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42142975">parent</a><span>|</span><a href="#42143181">next</a><span>|</span><label class="collapse" for="c-42142993">[-]</label><label class="expand" for="c-42142993">[5 more]</label></div><br/><div class="children"><div class="content">Maybe there&#x27;s some difference in the setup because the OP reports that the model beats stockfish (how they had it configured) every single game.</div><br/><div id="42144502" class="c"><input type="checkbox" id="c-42144502" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42142993">parent</a><span>|</span><a href="#42143059">next</a><span>|</span><label class="collapse" for="c-42144502">[-]</label><label class="expand" for="c-42144502">[1 more]</label></div><br/><div class="children"><div class="content">You have to get the model to think in PGN data. It&#x27;s crucial to use the exact PGN format it sae in its training data and to give it few shot examples.</div><br/></div></div><div id="42143059" class="c"><input type="checkbox" id="c-42143059" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42142993">parent</a><span>|</span><a href="#42144502">prev</a><span>|</span><a href="#42143181">next</a><span>|</span><label class="collapse" for="c-42143059">[-]</label><label class="expand" for="c-42143059">[3 more]</label></div><br/><div class="children"><div class="content">OP had stockfish at its weakest preset.</div><br/><div id="42143193" class="c"><input type="checkbox" id="c-42143193" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143059">parent</a><span>|</span><a href="#42143181">next</a><span>|</span><label class="collapse" for="c-42143193">[-]</label><label class="expand" for="c-42143193">[2 more]</label></div><br/><div class="children"><div class="content">Did the same and gpt-3.5-turbo-instruct still lost all the games.
maybe a diff in stockfish version ? I am using stockfish 16</div><br/><div id="42143999" class="c"><input type="checkbox" id="c-42143999" checked=""/><div class="controls bullet"><span class="by">mannykannot</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143193">parent</a><span>|</span><a href="#42143181">next</a><span>|</span><label class="collapse" for="c-42143999">[-]</label><label class="expand" for="c-42143999">[1 more]</label></div><br/><div class="children"><div class="content">That is a very pertinent question, especially if Stockfish has been used to generate training data.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42143181" class="c"><input type="checkbox" id="c-42143181" checked=""/><div class="controls bullet"><span class="by">lukan</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42142963">parent</a><span>|</span><a href="#42142975">prev</a><span>|</span><a href="#42143889">next</a><span>|</span><label class="collapse" for="c-42143181">[-]</label><label class="expand" for="c-42143181">[4 more]</label></div><br/><div class="children"><div class="content">Cheating (using a internal chess engine) would be the obvious reason to me.</div><br/><div id="42143214" class="c"><input type="checkbox" id="c-42143214" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143181">parent</a><span>|</span><a href="#42143889">next</a><span>|</span><label class="collapse" for="c-42143214">[-]</label><label class="expand" for="c-42143214">[3 more]</label></div><br/><div class="children"><div class="content">Nope. Calls by api don&#x27;t use functions calls.</div><br/><div id="42144027" class="c"><input type="checkbox" id="c-42144027" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143214">parent</a><span>|</span><a href="#42143226">next</a><span>|</span><label class="collapse" for="c-42144027">[-]</label><label class="expand" for="c-42144027">[1 more]</label></div><br/><div class="children"><div class="content">How can you prove this when talking about someones internal closed API?</div><br/></div></div><div id="42143226" class="c"><input type="checkbox" id="c-42143226" checked=""/><div class="controls bullet"><span class="by">permo-w</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143214">parent</a><span>|</span><a href="#42144027">prev</a><span>|</span><a href="#42143889">next</a><span>|</span><label class="collapse" for="c-42143226">[-]</label><label class="expand" for="c-42143226">[1 more]</label></div><br/><div class="children"><div class="content">that you know of</div><br/></div></div></div></div></div></div><div id="42143889" class="c"><input type="checkbox" id="c-42143889" checked=""/><div class="controls bullet"><span class="by">shric</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42142963">parent</a><span>|</span><a href="#42143181">prev</a><span>|</span><a href="#42143081">next</a><span>|</span><label class="collapse" for="c-42143889">[-]</label><label class="expand" for="c-42143889">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m actually surprised any of them manage to make legal moves throughout the game once out of book moves.</div><br/></div></div><div id="42143081" class="c"><input type="checkbox" id="c-42143081" checked=""/><div class="controls bullet"><span class="by">bluGill</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42142963">parent</a><span>|</span><a href="#42143889">prev</a><span>|</span><a href="#42144621">next</a><span>|</span><label class="collapse" for="c-42143081">[-]</label><label class="expand" for="c-42143081">[1 more]</label></div><br/><div class="children"><div class="content">The artical appears to have only run stockfish at low levels. you don&#x27;t have to be very good to beat it</div><br/></div></div></div></div><div id="42144621" class="c"><input type="checkbox" id="c-42144621" checked=""/><div class="controls bullet"><span class="by">chipdart</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42142963">prev</a><span>|</span><a href="#42144558">next</a><span>|</span><label class="collapse" for="c-42144621">[-]</label><label class="expand" for="c-42144621">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t understand why educated people expect that an LLM would be able to play chess at a decent level.<p>The blog post demonstrates that a LLM plays chess at a decent level.<p>The blog post explains why. It addresses the issue of data quality.<p>I don&#x27;t understand what point you thought you were making. Regardless of where you stand, the blog post showcases a surprising result.<p>You stress your prior unfounded belief, you were presented with data that proves it wrong, and your reaction was to post a comment with a thinly veiled accusation of people not being educated when clearly you are the one that&#x27;s off.<p>To make matters worse, this topic is also about curiosity. Which has a strong link with intelligence and education. And you are here criticizing others on those grounds in spite of showing your defitic right at the first sentence.<p>This blog post was a great read. Very surprising, engaging, and thought provoking.</div><br/></div></div><div id="42144558" class="c"><input type="checkbox" id="c-42144558" checked=""/><div class="controls bullet"><span class="by">scj</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42144621">prev</a><span>|</span><a href="#42144490">next</a><span>|</span><label class="collapse" for="c-42144558">[-]</label><label class="expand" for="c-42144558">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;d be more interesting to see LLMs play Family Feud.  I think it&#x27;d be their ideal game.</div><br/></div></div><div id="42144490" class="c"><input type="checkbox" id="c-42144490" checked=""/><div class="controls bullet"><span class="by">QuesnayJr</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42144558">prev</a><span>|</span><a href="#42144448">next</a><span>|</span><label class="collapse" for="c-42144490">[-]</label><label class="expand" for="c-42144490">[1 more]</label></div><br/><div class="children"><div class="content">They thought it because we have an existence proof: gpt-3.5-turbo-instruct <i>can</i> play chess at a decent level.<p>That was the point of the post (though you have to read it to the end to see this).  That one model can play chess pretty well, while the free models and OpenAI&#x27;s later models can&#x27;t.  That&#x27;s weird.</div><br/></div></div><div id="42144448" class="c"><input type="checkbox" id="c-42144448" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42144490">prev</a><span>|</span><a href="#42143136">next</a><span>|</span><label class="collapse" for="c-42144448">[-]</label><label class="expand" for="c-42144448">[1 more]</label></div><br/><div class="children"><div class="content">There are many ways to test for reasoning and deterministic computation as my own work in this space has shown .</div><br/></div></div><div id="42143136" class="c"><input type="checkbox" id="c-42143136" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42144448">prev</a><span>|</span><a href="#42143349">next</a><span>|</span><label class="collapse" for="c-42143136">[-]</label><label class="expand" for="c-42143136">[1 more]</label></div><br/><div class="children"><div class="content">Chess does not clearly require that. Various purely ML&#x2F;statistical based model approaches are doing pretty well. It&#x27;s almost certainly best to incorporate some kind of search into an overall system, but it&#x27;s not absolutely required to play just decent amateur level.<p>The problem here is the specific model architecture, training data, vocabulary&#x2F;tokenization method (if you were going to even represent a game this way... which you wouldn&#x27;t),  loss function and probably decoding strategy.... basically everything is wrong here.</div><br/></div></div><div id="42144146" class="c"><input type="checkbox" id="c-42144146" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42143349">prev</a><span>|</span><a href="#42143021">next</a><span>|</span><label class="collapse" for="c-42144146">[-]</label><label class="expand" for="c-42144146">[1 more]</label></div><br/><div class="children"><div class="content">But there&#x27;s really nothing about chess that makes reasoning a prerequisite, a win is a win as long as it&#x27;s a win. This is kind of a semantics game: it&#x27;s a question of whether the degree of skill people observe in an LLM playing chess is actually some different quantity than the chance it wins.<p>I mean at some level you&#x27;re saying that no matter how close to 1 the win probability (1 - epsilon) gets, both of the following are true:<p>A. you should always expect for the computation that you&#x27;re able to do via conscious reasoning alone to always be sufficient, at least in principle, to asymptotically get a higher win probability than a model, no matter what the model&#x27;s win probability was to begin with<p>B. no matter how close to 1 that the model&#x27;s win rate p=(1 - epsilon) gets, because logical inference is so non-smooth, the win rate on yet-unseen data is fundamentally algorithmically random&#x2F;totally uncorrelated to in-distribution performance, so it&#x27;s never appropriate to say that a model can understand or to reason<p>To me it seems that people are subject to both of these criteria, though. They have a tendency to cap out at their eventual skill cap unless given a challenge to nudge them to a higher level, and likewise possession of logical reasoning doesn&#x27;t let us say much at all about situations that their reasoning is unfamiliar with.<p>I also think, if you want to say that what LLMs do has nothing to do with understanding or ability, then you also have to have an alternate explanation for the phenomenon of AlphaGo defeating Lee Sedol being a catalyst for top Go players being able to rapidly increase their own rankings shortly after.</div><br/></div></div><div id="42143021" class="c"><input type="checkbox" id="c-42143021" checked=""/><div class="controls bullet"><span class="by">SilasX</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42144146">prev</a><span>|</span><a href="#42143253">next</a><span>|</span><label class="collapse" for="c-42143021">[-]</label><label class="expand" for="c-42143021">[2 more]</label></div><br/><div class="children"><div class="content">Right, at least as of the ~GPT3 model it was just &quot;predict what you <i>would</i> see in a chess game&quot;, not &quot;what would <i>be</i> the best move&quot;. So (IIRC) users noted that if you made bad move, then the model would also reply with bad moves because it pattern matched to bad games. (I anthropomorphized this as the model saying &quot;oh, we&#x27;re doing dumb-people-chess now, I can do that too!&quot;)</div><br/><div id="42143121" class="c"><input type="checkbox" id="c-42143121" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143021">parent</a><span>|</span><a href="#42143253">next</a><span>|</span><label class="collapse" for="c-42143121">[-]</label><label class="expand" for="c-42143121">[1 more]</label></div><br/><div class="children"><div class="content">But it also predicts moves where the text says &quot;black won the game, [proceeds to show the game]&quot;. To minimize loss on that it would need to from context try and make it so white doesn&#x27;t make critical mistakes.</div><br/></div></div></div></div><div id="42143253" class="c"><input type="checkbox" id="c-42143253" checked=""/><div class="controls bullet"><span class="by">slibhb</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42143021">prev</a><span>|</span><a href="#42143024">next</a><span>|</span><label class="collapse" for="c-42143253">[-]</label><label class="expand" for="c-42143253">[1 more]</label></div><br/><div class="children"><div class="content">Few people (perhaps none) expected LLMs to be good at chess. Nevertheless, as the article explains, there was buzz around a year ago that LLMs were good at chess.<p>&gt; It has no idea about the quality of it&#x27;s data. &quot;Act like x&quot; prompts are no substitute for actual reasoning and deterministic computation which clearly chess requires.<p>No. You can definitely train a model to be really good at chess without &quot;actual reasoning and deterministic computation&quot;.</div><br/></div></div><div id="42143024" class="c"><input type="checkbox" id="c-42143024" checked=""/><div class="controls bullet"><span class="by">aqme28</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42143253">prev</a><span>|</span><a href="#42143208">next</a><span>|</span><label class="collapse" for="c-42143024">[-]</label><label class="expand" for="c-42143024">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, that is the &quot;something weird&quot; of the article.</div><br/></div></div><div id="42143208" class="c"><input type="checkbox" id="c-42143208" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42142885">parent</a><span>|</span><a href="#42143024">prev</a><span>|</span><a href="#42145064">next</a><span>|</span><label class="collapse" for="c-42143208">[-]</label><label class="expand" for="c-42143208">[3 more]</label></div><br/><div class="children"><div class="content">Bro, it actually did play chess, didn&#x27;t you read the article?</div><br/><div id="42143417" class="c"><input type="checkbox" id="c-42143417" checked=""/><div class="controls bullet"><span class="by">mandevil</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143208">parent</a><span>|</span><a href="#42145064">next</a><span>|</span><label class="collapse" for="c-42143417">[-]</label><label class="expand" for="c-42143417">[2 more]</label></div><br/><div class="children"><div class="content">It sorta played chess- he let it generate up to ten moves, throwing away any that weren&#x27;t legal, and if no legal move was generated by the 10th try he picked a random legal move. He does not say how many times he had to provide a random move, or how many times illegal moves were generated.</div><br/><div id="42143790" class="c"><input type="checkbox" id="c-42143790" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42142885">root</a><span>|</span><a href="#42143417">parent</a><span>|</span><a href="#42145064">next</a><span>|</span><label class="collapse" for="c-42143790">[-]</label><label class="expand" for="c-42143790">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right it&#x27;s not in this blog but turbo-instruct&#x27;s chess ability has been pretty thoroughly tested and it does play chess.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;adamkarvonen&#x2F;chess_gpt_eval">https:&#x2F;&#x2F;github.com&#x2F;adamkarvonen&#x2F;chess_gpt_eval</a></div><br/></div></div></div></div></div></div></div></div><div id="42145064" class="c"><input type="checkbox" id="c-42145064" checked=""/><div class="controls bullet"><span class="by">Sparkyte</span><span>|</span><a href="#42142885">prev</a><span>|</span><a href="#42141993">next</a><span>|</span><label class="collapse" for="c-42145064">[-]</label><label class="expand" for="c-42145064">[1 more]</label></div><br/><div class="children"><div class="content">Lets be real though most people can&#x27;t beat a grandmaster. It is impressive to see it last more rounds as it progressed.</div><br/></div></div><div id="42141993" class="c"><input type="checkbox" id="c-42141993" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#42145064">prev</a><span>|</span><a href="#42144062">next</a><span>|</span><label class="collapse" for="c-42141993">[-]</label><label class="expand" for="c-42141993">[29 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m really stupid... but perhaps if we want really intelligent models we need to stop tokenizing at all? We&#x27;re literally limiting what a model can see and how it percieves the world by limiting the structure of the information streams that come into the model from the very beginning.<p>I know working with raw bits or bytes is slower, but it should be relatively cheap and easy to at least falsify this hypothesis that many huge issues might be due to tokenization problems but... yeah.<p>Surprised I don&#x27;t see more research into radicaly different tokenization.</div><br/><div id="42142384" class="c"><input type="checkbox" id="c-42142384" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42144600">next</a><span>|</span><label class="collapse" for="c-42142384">[-]</label><label class="expand" for="c-42142384">[11 more]</label></div><br/><div class="children"><div class="content">FWIW I think most of the &quot;tokenization problems&quot; are in fact reasoning problems being falsely blamed on a minor technical thing when the issue is much more profound.<p>E.g. I still see people claiming that LLMs are bad at basic counting because of tokenization, but the same LLM counts perfectly well if you use chain-of-thought prompting. So it <i>can&#x27;t</i> be explained by tokenization! The problem is reasoning: the LLM needs a human to tell it that a counting problem can be accurately solved if they go step-by-step. Without this assistance the LLM is likely to simply guess.</div><br/><div id="42142733" class="c"><input type="checkbox" id="c-42142733" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142384">parent</a><span>|</span><a href="#42143800">next</a><span>|</span><label class="collapse" for="c-42142733">[-]</label><label class="expand" for="c-42142733">[3 more]</label></div><br/><div class="children"><div class="content">The more obvious alternative is that CoT is making up for the deficiencies in tokenization, which I believe is the case.</div><br/><div id="42142913" class="c"><input type="checkbox" id="c-42142913" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142733">parent</a><span>|</span><a href="#42143800">next</a><span>|</span><label class="collapse" for="c-42142913">[-]</label><label class="expand" for="c-42142913">[2 more]</label></div><br/><div class="children"><div class="content">I think the more obvious explanation has to do with computational complexity: counting is an O(n) problem, but transformer LLMs can’t solve O(n) problems unless you use CoT prompting: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07923" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07923</a></div><br/><div id="42143402" class="c"><input type="checkbox" id="c-42143402" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142913">parent</a><span>|</span><a href="#42143800">next</a><span>|</span><label class="collapse" for="c-42143402">[-]</label><label class="expand" for="c-42143402">[1 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re saying is an explanation what I said, but I agree with you ;)</div><br/></div></div></div></div></div></div><div id="42143800" class="c"><input type="checkbox" id="c-42143800" checked=""/><div class="controls bullet"><span class="by">meroes</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142384">parent</a><span>|</span><a href="#42142733">prev</a><span>|</span><a href="#42144596">next</a><span>|</span><label class="collapse" for="c-42143800">[-]</label><label class="expand" for="c-42143800">[1 more]</label></div><br/><div class="children"><div class="content">At a certain level they are identical problems. My strongest piece of evidence is that I get paid as an RLHF&#x27;er to find ANY case of error, including &quot;tokenization&quot;. You know how many errors an LLM gets in the simplest grid puzzles, with CoT, with specialized models that don&#x27;t try to &quot;one-shot&quot; problems, with multiple models, etc?<p>My assumption is that these large companies wouldn&#x27;t pay  hundreds of thousands of RLHF&#x27;ers through dozens of third party companies livable wages if tokenization errors were just that.</div><br/></div></div><div id="42144596" class="c"><input type="checkbox" id="c-42144596" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142384">parent</a><span>|</span><a href="#42143800">prev</a><span>|</span><a href="#42143239">next</a><span>|</span><label class="collapse" for="c-42144596">[-]</label><label class="expand" for="c-42144596">[1 more]</label></div><br/><div class="children"><div class="content">It can count words in a paragraph though. So I do think it&#x27;s tokenization.</div><br/></div></div><div id="42143239" class="c"><input type="checkbox" id="c-42143239" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142384">parent</a><span>|</span><a href="#42144596">prev</a><span>|</span><a href="#42142807">next</a><span>|</span><label class="collapse" for="c-42143239">[-]</label><label class="expand" for="c-42143239">[1 more]</label></div><br/><div class="children"><div class="content">FWIW I think most of the &quot;tokenization problems&quot;<p>List of actual tokenizarion limitations
1- strawberry
2- rhyming and metrics
3- whitespace (as displayed in the article)</div><br/></div></div><div id="42142807" class="c"><input type="checkbox" id="c-42142807" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142384">parent</a><span>|</span><a href="#42143239">prev</a><span>|</span><a href="#42144600">next</a><span>|</span><label class="collapse" for="c-42142807">[-]</label><label class="expand" for="c-42142807">[4 more]</label></div><br/><div class="children"><div class="content">I’m the one who will fight you including with peer reviewed papers indicating that it is in fact due to tokenization. I’m too tired but will edit this for later, so take this as my bookmark to remind me to respond.</div><br/><div id="42144506" class="c"><input type="checkbox" id="c-42144506" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142807">parent</a><span>|</span><a href="#42142884">next</a><span>|</span><label class="collapse" for="c-42144506">[-]</label><label class="expand" for="c-42144506">[1 more]</label></div><br/><div class="children"><div class="content">We know there are narrow solutions to these problems, that was never the argument that the specific narrow task is impossible to solve.<p>The discussion is about general intelligence, the model isn&#x27;t able to do a task that it can do simply because it chooses the wrong strategy, that is a problem of lack of generalization and not a problem of tokenization. Being able to choose the right strategy is core to general intelligence, altering input data to make it easier for the model to find the right solution to specific questions does not help it become more general, you just shift what narrow problems it is good at.</div><br/></div></div><div id="42142884" class="c"><input type="checkbox" id="c-42142884" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142807">parent</a><span>|</span><a href="#42144506">prev</a><span>|</span><a href="#42144600">next</a><span>|</span><label class="collapse" for="c-42142884">[-]</label><label class="expand" for="c-42142884">[2 more]</label></div><br/><div class="children"><div class="content">I am aware of errors in <i>computations</i> that can be fixed by better tokenization (e.g. long addition works better tokenizing right-left rather than L-R). But I am talking about counting, and talking about counting <i>words,</i> not <i>characters.</i> I don’t think tokenization explains why LLMs tend to fail at this without CoT prompting. I really think the answer is computational complexity: counting is simply too hard for transformers unless you use CoT. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07923" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07923</a></div><br/><div id="42143144" class="c"><input type="checkbox" id="c-42143144" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142884">parent</a><span>|</span><a href="#42144600">next</a><span>|</span><label class="collapse" for="c-42143144">[-]</label><label class="expand" for="c-42143144">[1 more]</label></div><br/><div class="children"><div class="content">Words vs characters is a similar problem, since tokens can be less one word, multiple words, or multiple words and a partial word, or words with non-word punctuation like a sentence ending period.</div><br/></div></div></div></div></div></div></div></div><div id="42144600" class="c"><input type="checkbox" id="c-42144600" checked=""/><div class="controls bullet"><span class="by">malthaus</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42142384">prev</a><span>|</span><a href="#42143338">next</a><span>|</span><label class="collapse" for="c-42144600">[-]</label><label class="expand" for="c-42144600">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;youtu.be&#x2F;zduSFxRajkE" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;zduSFxRajkE</a><p>karpathy agrees with you, here he is hating on tokenizers while re-building them for 2h</div><br/></div></div><div id="42143338" class="c"><input type="checkbox" id="c-42143338" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42144600">prev</a><span>|</span><a href="#42143197">next</a><span>|</span><label class="collapse" for="c-42143338">[-]</label><label class="expand" for="c-42143338">[1 more]</label></div><br/><div class="children"><div class="content">Going from tokens to bytes explodes the model size. I can’t find the reference at the moment, but reducing the average token size induces a corresponding quadratic increase in the width (size of each layer) of the model. This doesn’t just affect inference speed, but also training speed.</div><br/></div></div><div id="42143197" class="c"><input type="checkbox" id="c-42143197" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42143338">prev</a><span>|</span><a href="#42144059">next</a><span>|</span><label class="collapse" for="c-42143197">[-]</label><label class="expand" for="c-42143197">[2 more]</label></div><br/><div class="children"><div class="content">There’s a reason human brains have dedicated language handling. Tokenization is likely a solid strategy. The real thing here is that language is not a good way to encode all forms of knowledge</div><br/><div id="42144149" class="c"><input type="checkbox" id="c-42144149" checked=""/><div class="controls bullet"><span class="by">joquarky</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42143197">parent</a><span>|</span><a href="#42144059">next</a><span>|</span><label class="collapse" for="c-42144149">[-]</label><label class="expand" for="c-42144149">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not even possible to encode all forms of knowledge.</div><br/></div></div></div></div><div id="42144059" class="c"><input type="checkbox" id="c-42144059" checked=""/><div class="controls bullet"><span class="by">ATMLOTTOBEER</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42143197">prev</a><span>|</span><a href="#42144582">next</a><span>|</span><label class="collapse" for="c-42144059">[-]</label><label class="expand" for="c-42144059">[1 more]</label></div><br/><div class="children"><div class="content">I tend to agree with you. Your post reminded me of <a href="https:&#x2F;&#x2F;gwern.net&#x2F;aunn" rel="nofollow">https:&#x2F;&#x2F;gwern.net&#x2F;aunn</a></div><br/></div></div><div id="42144582" class="c"><input type="checkbox" id="c-42144582" checked=""/><div class="controls bullet"><span class="by">empiko</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42144059">prev</a><span>|</span><a href="#42143381">next</a><span>|</span><label class="collapse" for="c-42144582">[-]</label><label class="expand" for="c-42144582">[1 more]</label></div><br/><div class="children"><div class="content">I have seen a bunch of tokenization papers with various ideas but their results are mostly meh. I personally don&#x27;t see anything principally wrong with current approaches. Having discrete symbols is how natural language works, and this might be an okayish approximation.</div><br/></div></div><div id="42143381" class="c"><input type="checkbox" id="c-42143381" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42144582">prev</a><span>|</span><a href="#42142033">next</a><span>|</span><label class="collapse" for="c-42143381">[-]</label><label class="expand" for="c-42143381">[1 more]</label></div><br/><div class="children"><div class="content">Tokenization is not strictly speaking necessary (you can train on bytes). What it is is really really efficient. Scaling is a challenge as is, bytes would just blow that up.</div><br/></div></div><div id="42142033" class="c"><input type="checkbox" id="c-42142033" checked=""/><div class="controls bullet"><span class="by">cschep</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42143381">prev</a><span>|</span><a href="#42144207">next</a><span>|</span><label class="collapse" for="c-42142033">[-]</label><label class="expand" for="c-42142033">[9 more]</label></div><br/><div class="children"><div class="content">How would we train it? Don&#x27;t we need it to understand the heaps and heaps of data we already have &quot;tokenized&quot; e.g. the internet? Written words for humans? Genuinely curious how we could approach it differently?</div><br/><div id="42142146" class="c"><input type="checkbox" id="c-42142146" checked=""/><div class="controls bullet"><span class="by">skylerwiernik</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142033">parent</a><span>|</span><a href="#42142126">next</a><span>|</span><label class="collapse" for="c-42142146">[-]</label><label class="expand" for="c-42142146">[7 more]</label></div><br/><div class="children"><div class="content">Couldn&#x27;t we just make every human readable character a token?<p>OpenAI&#x27;s tokenizer makes &quot;chess&quot; &quot;ch&quot; and &quot;ess&quot;. We could just make it into &quot;c&quot; &quot;h&quot; &quot;e&quot; &quot;s&quot; &quot;s&quot;</div><br/><div id="42142199" class="c"><input type="checkbox" id="c-42142199" checked=""/><div class="controls bullet"><span class="by">taeric</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142146">parent</a><span>|</span><a href="#42142835">next</a><span>|</span><label class="collapse" for="c-42142199">[-]</label><label class="expand" for="c-42142199">[3 more]</label></div><br/><div class="children"><div class="content">This is just more tokens?  And probably requires the model to learn about common groups.  Consider, &quot;ess&quot; makes sense to see as a group.  &quot;Wss&quot; does not.<p>That is, the groups are encoding something the model doesn&#x27;t have to learn.<p>This is not much astray from &quot;sight words&quot; we teach kids.</div><br/><div id="42143246" class="c"><input type="checkbox" id="c-42143246" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142199">parent</a><span>|</span><a href="#42142835">next</a><span>|</span><label class="collapse" for="c-42143246">[-]</label><label class="expand" for="c-42143246">[2 more]</label></div><br/><div class="children"><div class="content">This is just more tokens?<p>Yup. Just let the actual ML git gud</div><br/><div id="42143334" class="c"><input type="checkbox" id="c-42143334" checked=""/><div class="controls bullet"><span class="by">taeric</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42143246">parent</a><span>|</span><a href="#42142835">next</a><span>|</span><label class="collapse" for="c-42143334">[-]</label><label class="expand" for="c-42143334">[1 more]</label></div><br/><div class="children"><div class="content">So, put differently, this is just more expensive?</div><br/></div></div></div></div></div></div><div id="42142835" class="c"><input type="checkbox" id="c-42142835" checked=""/><div class="controls bullet"><span class="by">cco</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142146">parent</a><span>|</span><a href="#42142199">prev</a><span>|</span><a href="#42142203">next</a><span>|</span><label class="collapse" for="c-42142835">[-]</label><label class="expand" for="c-42142835">[2 more]</label></div><br/><div class="children"><div class="content">We can, tokenization is literally just to maximize resources and provide as much &quot;space&quot; as possible in the context window.<p>There is no advantage to tokenization, it just helps solve limitations in context windows and training.</div><br/><div id="42143249" class="c"><input type="checkbox" id="c-42143249" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142835">parent</a><span>|</span><a href="#42142203">next</a><span>|</span><label class="collapse" for="c-42143249">[-]</label><label class="expand" for="c-42143249">[1 more]</label></div><br/><div class="children"><div class="content">I like this explanation</div><br/></div></div></div></div><div id="42142203" class="c"><input type="checkbox" id="c-42142203" checked=""/><div class="controls bullet"><span class="by">tchalla</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142146">parent</a><span>|</span><a href="#42142835">prev</a><span>|</span><a href="#42142126">next</a><span>|</span><label class="collapse" for="c-42142203">[-]</label><label class="expand" for="c-42142203">[1 more]</label></div><br/><div class="children"><div class="content">aka Character Language Models which have existed for a while now.</div><br/></div></div></div></div><div id="42142126" class="c"><input type="checkbox" id="c-42142126" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42141993">root</a><span>|</span><a href="#42142033">parent</a><span>|</span><a href="#42142146">prev</a><span>|</span><a href="#42144207">next</a><span>|</span><label class="collapse" for="c-42142126">[-]</label><label class="expand" for="c-42142126">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what tokenized means here. Parent is asking to provide the model with separate characters rather than tokens, i.e. groups of characters.</div><br/></div></div></div></div><div id="42144207" class="c"><input type="checkbox" id="c-42144207" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#42141993">parent</a><span>|</span><a href="#42142033">prev</a><span>|</span><a href="#42144062">next</a><span>|</span><label class="collapse" for="c-42144207">[-]</label><label class="expand" for="c-42144207">[1 more]</label></div><br/><div class="children"><div class="content">hot take: LLM tokens is kanji for AI, and just like kanji it works okay sometimes but fails miserably for the task of accurately representating English</div><br/></div></div></div></div><div id="42144062" class="c"><input type="checkbox" id="c-42144062" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#42141993">prev</a><span>|</span><a href="#42142255">next</a><span>|</span><label class="collapse" for="c-42144062">[-]</label><label class="expand" for="c-42144062">[4 more]</label></div><br/><div class="children"><div class="content">I found a related set of experiments that include gpt-3.5-turbo-instruct, gpt-3.5-turbo and gpt-4.<p>Same surprising conclusion: gpt-3.5-turbo-instruct is much better at chess.<p><a href="https:&#x2F;&#x2F;blog.mathieuacher.com&#x2F;GPTsChessEloRatingLegalMoves&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.mathieuacher.com&#x2F;GPTsChessEloRatingLegalMoves&#x2F;</a></div><br/><div id="42144150" class="c"><input type="checkbox" id="c-42144150" checked=""/><div class="controls bullet"><span class="by">shtack</span><span>|</span><a href="#42144062">parent</a><span>|</span><a href="#42142255">next</a><span>|</span><label class="collapse" for="c-42144150">[-]</label><label class="expand" for="c-42144150">[3 more]</label></div><br/><div class="children"><div class="content">I’d bet it’s using function calling out to a real chess engine. It could probably be proven with a timing analysis to see how inference time changes&#x2F;doesn’t with number of tokens or game complexity.</div><br/><div id="42144275" class="c"><input type="checkbox" id="c-42144275" checked=""/><div class="controls bullet"><span class="by">scratchyone</span><span>|</span><a href="#42144062">root</a><span>|</span><a href="#42144150">parent</a><span>|</span><a href="#42142255">next</a><span>|</span><label class="collapse" for="c-42144275">[-]</label><label class="expand" for="c-42144275">[2 more]</label></div><br/><div class="children"><div class="content">?? why would openai even want to secretly embed chess function calling into an incredibly old model? if they wanted to trick people into thinking their models are super good at chess why wouldn&#x27;t they just do that to gpt-4o?</div><br/><div id="42144575" class="c"><input type="checkbox" id="c-42144575" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#42144062">root</a><span>|</span><a href="#42144275">parent</a><span>|</span><a href="#42142255">next</a><span>|</span><label class="collapse" for="c-42144575">[-]</label><label class="expand" for="c-42144575">[1 more]</label></div><br/><div class="children"><div class="content">The idea is that they embedded this when it was a new model, as part of the hype before GPT-4. The fake-it-till-you-make-it hope was that GPT-4 would be so good it could actually play chess. Then it turned out GPT-4 sucked at chess as well, and OpenAI quietly dropped any mention of chess. But it would be too suspicious to remove a well-documented feature from the old model, so it&#x27;s left there and can be chalked up as a random event.</div><br/></div></div></div></div></div></div></div></div><div id="42142255" class="c"><input type="checkbox" id="c-42142255" checked=""/><div class="controls bullet"><span class="by">underlines</span><span>|</span><a href="#42144062">prev</a><span>|</span><a href="#42142229">next</a><span>|</span><label class="collapse" for="c-42142255">[-]</label><label class="expand" for="c-42142255">[4 more]</label></div><br/><div class="children"><div class="content">Can you try increasing compute in the problem search space, not in the training space? What this means is, give it more compute to think during inference by not forcing any model to &quot;only output the answer in algebraic notation&quot; but do CoT prompting:
&quot;1. Think about the current board
2. Think about valid possible next moves and choose the 3 best by thinking ahead
3. Make your move&quot;<p>Or whatever you deem a good step by step instruction of what an actual good beginner chess player might do.<p>Then try different notations, different prompt variations, temperatures and the other parameters. That all needs to go in your hyper-parameter-tuning.<p>One could try using DSPy for automatic prompt optimization.</div><br/><div id="42143035" class="c"><input type="checkbox" id="c-42143035" checked=""/><div class="controls bullet"><span class="by">pavel_lishin</span><span>|</span><a href="#42142255">parent</a><span>|</span><a href="#42142533">next</a><span>|</span><label class="collapse" for="c-42143035">[-]</label><label class="expand" for="c-42143035">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>1. Think about the current board 2. Think about valid possible next moves and choose the 3 best by thinking ahead 3.</i><p>Do these models actually <i>think about a board</i>? Chess engines do, as much as we can say that any machine thinks. But do LLMs?</div><br/><div id="42143281" class="c"><input type="checkbox" id="c-42143281" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42142255">root</a><span>|</span><a href="#42143035">parent</a><span>|</span><a href="#42142533">next</a><span>|</span><label class="collapse" for="c-42143281">[-]</label><label class="expand" for="c-42143281">[1 more]</label></div><br/><div class="children"><div class="content">Can be forced through inference with CoT type of stuff. Spend tokens at each stage to draw the board for example, then spend tokens restating the rules of the game, then spend token restating the heuristics like piece value, and then spend tokens doing a minmax n-ply search.<p>Wildly inefficient? Probably. Could maybe generate some python to make more efficient? Maybe, yeah.<p>Essentially user would have to teach gpt to play chess, or training would fine tune chess towards these CoT, fine tuning, etc...</div><br/></div></div></div></div><div id="42142533" class="c"><input type="checkbox" id="c-42142533" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42142255">parent</a><span>|</span><a href="#42143035">prev</a><span>|</span><a href="#42142229">next</a><span>|</span><label class="collapse" for="c-42142533">[-]</label><label class="expand" for="c-42142533">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the expectation for the immediate answer is definitely results, especially for the later stages. Another possible improvement: every 2 steps, show the current board state and repeat the moves still to be processed, before analysing the final position.</div><br/></div></div></div></div><div id="42142229" class="c"><input type="checkbox" id="c-42142229" checked=""/><div class="controls bullet"><span class="by">jrecursive</span><span>|</span><a href="#42142255">prev</a><span>|</span><a href="#42144283">next</a><span>|</span><label class="collapse" for="c-42142229">[-]</label><label class="expand" for="c-42142229">[10 more]</label></div><br/><div class="children"><div class="content">i think this has everything to do with the fact that learning chess by learning sequences will get you into more trouble than good. even a trillion games won&#x27;t save you: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shannon_number" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shannon_number</a><p>that said, for the sake of completeness, modern chess engines (with high quality chess-specific models as part of their toolset) are fully capable of, at minimum, tying every player alive or dead, every time. if the opponent makes one mistake, even very small, they will lose.<p>while writing this i absently wondered if you increased the skill level of stockfish, maybe to maximum, or perhaps at least an 1800+ elo player, you would see more successful games. even then, it will only be because the &quot;narrower training data&quot; (ie advanced players won&#x27;t play trash moves) at that level will probably get you more wins in your graph, but it won&#x27;t indicate any better play, it will just be a reflection of less noise; fewer, more reinforced known positions.</div><br/><div id="42142365" class="c"><input type="checkbox" id="c-42142365" checked=""/><div class="controls bullet"><span class="by">jayrot</span><span>|</span><a href="#42142229">parent</a><span>|</span><a href="#42142915">next</a><span>|</span><label class="collapse" for="c-42142365">[-]</label><label class="expand" for="c-42142365">[5 more]</label></div><br/><div class="children"><div class="content">&gt; i think this has everything to do with the fact that learning chess by learning sequences will get you into more trouble than good. even a trillion games won&#x27;t save you: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shannon_number" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shannon_number</a><p>Indeed. As has been pointed out before, the number of possible chess positions easily, vastly dwarfs even the wildest possible estimate of the number of atoms in the known universe.</div><br/><div id="42143611" class="c"><input type="checkbox" id="c-42143611" checked=""/><div class="controls bullet"><span class="by">rcxdude</span><span>|</span><a href="#42142229">root</a><span>|</span><a href="#42142365">parent</a><span>|</span><a href="#42144870">next</a><span>|</span><label class="collapse" for="c-42143611">[-]</label><label class="expand" for="c-42143611">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but so does the number of paragraphs in the english language, and yet LLMs seem to do pretty well at that. I don&#x27;t think the number of configurations is particularly relevant.<p>(And it&#x27;s honestly quite impressive that LLMs can play it at all, but not at all surprising that it loses pretty handily to something which is explicitly designed to search, as opposed to simply feed-forward a decision)</div><br/></div></div><div id="42144870" class="c"><input type="checkbox" id="c-42144870" checked=""/><div class="controls bullet"><span class="by">dataspun</span><span>|</span><a href="#42142229">root</a><span>|</span><a href="#42142365">parent</a><span>|</span><a href="#42143611">prev</a><span>|</span><a href="#42142435">next</a><span>|</span><label class="collapse" for="c-42144870">[-]</label><label class="expand" for="c-42144870">[1 more]</label></div><br/><div class="children"><div class="content">Not true if we’re talking sensible chess moves.</div><br/></div></div><div id="42142435" class="c"><input type="checkbox" id="c-42142435" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#42142229">root</a><span>|</span><a href="#42142365">parent</a><span>|</span><a href="#42144870">prev</a><span>|</span><a href="#42142915">next</a><span>|</span><label class="collapse" for="c-42142435">[-]</label><label class="expand" for="c-42142435">[2 more]</label></div><br/><div class="children"><div class="content">What about the number of possible positions where an idiotic move hasn&#x27;t been played?  Perhaps the search space who could be reduced quite a bit.</div><br/><div id="42142673" class="c"><input type="checkbox" id="c-42142673" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#42142229">root</a><span>|</span><a href="#42142435">parent</a><span>|</span><a href="#42142915">next</a><span>|</span><label class="collapse" for="c-42142673">[-]</label><label class="expand" for="c-42142673">[1 more]</label></div><br/><div class="children"><div class="content">Unless there is an apparent idiotic move than can lead to an &#x27;island of intelligence&#x27;</div><br/></div></div></div></div></div></div><div id="42142915" class="c"><input type="checkbox" id="c-42142915" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#42142229">parent</a><span>|</span><a href="#42142365">prev</a><span>|</span><a href="#42143959">next</a><span>|</span><label class="collapse" for="c-42142915">[-]</label><label class="expand" for="c-42142915">[2 more]</label></div><br/><div class="children"><div class="content">Honestly, I think that once you discard the moves one would never make, and account for symmetries&#x2F;effectively similar board positions (ones that could be detected by a very simple pattern matcher), chess might not be that big a game at all.</div><br/><div id="42143000" class="c"><input type="checkbox" id="c-42143000" checked=""/><div class="controls bullet"><span class="by">jrecursive</span><span>|</span><a href="#42142229">root</a><span>|</span><a href="#42142915">parent</a><span>|</span><a href="#42143959">next</a><span>|</span><label class="collapse" for="c-42143000">[-]</label><label class="expand" for="c-42143000">[1 more]</label></div><br/><div class="children"><div class="content">you should try it and post a rebuttal :)</div><br/></div></div></div></div><div id="42143959" class="c"><input type="checkbox" id="c-42143959" checked=""/><div class="controls bullet"><span class="by">astrea</span><span>|</span><a href="#42142229">parent</a><span>|</span><a href="#42142915">prev</a><span>|</span><a href="#42142427">next</a><span>|</span><label class="collapse" for="c-42143959">[-]</label><label class="expand" for="c-42143959">[1 more]</label></div><br/><div class="children"><div class="content">Since we&#x27;re mentioning Shannon... What is the minimum representative sample size of that problem space? Is it close enough to the number of freely available chess moves on the Internet and in books?</div><br/></div></div><div id="42142427" class="c"><input type="checkbox" id="c-42142427" checked=""/><div class="controls bullet"><span class="by">BurningFrog</span><span>|</span><a href="#42142229">parent</a><span>|</span><a href="#42143959">prev</a><span>|</span><a href="#42144283">next</a><span>|</span><label class="collapse" for="c-42142427">[-]</label><label class="expand" for="c-42142427">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I think this has everything to do with the fact that learning chess by learning sequences will get you into more trouble than good.</i><p>Yeah, once you&#x27;ve deviated from a sequence you&#x27;re lost.<p>Maybe approaching it by learning the best move in billions&#x2F;trillions of positions, and feeding that into some AI could work better. Similar positions often have the same kind of best move.</div><br/></div></div></div></div><div id="42144283" class="c"><input type="checkbox" id="c-42144283" checked=""/><div class="controls bullet"><span class="by">chvid</span><span>|</span><a href="#42142229">prev</a><span>|</span><a href="#42144837">next</a><span>|</span><label class="collapse" for="c-42144283">[-]</label><label class="expand" for="c-42144283">[9 more]</label></div><br/><div class="children"><div class="content">Theory 5: GPT-3.5-instruct plays chess by calling a traditional chess engine.</div><br/><div id="42144326" class="c"><input type="checkbox" id="c-42144326" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#42144283">parent</a><span>|</span><a href="#42144517">next</a><span>|</span><label class="collapse" for="c-42144326">[-]</label><label class="expand" for="c-42144326">[5 more]</label></div><br/><div class="children"><div class="content">Just think about the trade off from OpenAI&#x27;s side here - they&#x27;re going to add a bunch of complexity to gpt3.5 to let it call out to engines (either an external system monitoring all outputs for chess related stuff, or some kind of tool-assisted CoT for instance) just so it can play chess incorrectly a high percentage of the time, and even when it doesn&#x27;t at a mere 1800ELO level? In return for some mentions in a few relatively obscure blog posts? Doesn&#x27;t make any sense to me as an explanation.</div><br/><div id="42144614" class="c"><input type="checkbox" id="c-42144614" checked=""/><div class="controls bullet"><span class="by">usrusr</span><span>|</span><a href="#42144283">root</a><span>|</span><a href="#42144326">parent</a><span>|</span><a href="#42144427">next</a><span>|</span><label class="collapse" for="c-42144614">[-]</label><label class="expand" for="c-42144614">[2 more]</label></div><br/><div class="children"><div class="content">Could be a pilot implementation to learn about how to link up external specialist engines. Chess would be the obvious example to start with because the problem is so well known, standardized and specialist engines are easily available. If they ever want to offer an integration like that to customers (who might have some existing rule based engine in house), the need to know everything they can about expected cost, performance.</div><br/><div id="42144821" class="c"><input type="checkbox" id="c-42144821" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#42144283">root</a><span>|</span><a href="#42144614">parent</a><span>|</span><a href="#42144427">next</a><span>|</span><label class="collapse" for="c-42144821">[-]</label><label class="expand" for="c-42144821">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t address its terrible performance. If it were touching anything like a real engine it would be playing at a superhuman level, not the level of a upper-tier beginner.</div><br/></div></div></div></div><div id="42144427" class="c"><input type="checkbox" id="c-42144427" checked=""/><div class="controls bullet"><span class="by">copperx</span><span>|</span><a href="#42144283">root</a><span>|</span><a href="#42144326">parent</a><span>|</span><a href="#42144614">prev</a><span>|</span><a href="#42144517">next</a><span>|</span><label class="collapse" for="c-42144427">[-]</label><label class="expand" for="c-42144427">[2 more]</label></div><br/><div class="children"><div class="content">But there could be a simple explanation. For example, they could have tested many &quot;engines&quot; when developing function calling and they just left them in there. They just happened to connect to a basic chess playing algorithm and nothing sophisticated.<p>Also, it makes a lot of sense if you expect people to play chess against the LLM, especially if you are later training future models on the chats.</div><br/><div id="42144859" class="c"><input type="checkbox" id="c-42144859" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#42144283">root</a><span>|</span><a href="#42144427">parent</a><span>|</span><a href="#42144517">next</a><span>|</span><label class="collapse" for="c-42144859">[-]</label><label class="expand" for="c-42144859">[1 more]</label></div><br/><div class="children"><div class="content">This still requires a lot of coincidences, like they chose to use a terrible chess engine for their external tool (why?), they left it on in the background for <i>all</i> calls via all APIs for <i>only</i> gpt-3.5-turbo-instruct (why?), they see business value in this specific model being good at chess vs other things (why?).<p>You say it makes sense but how does it make sense for OpenAI to add overhead to all of its API calls for the super niche case of people playing 1800 ELO chess&#x2F;chat bots? (that often play illegal moves, you can go try it yourself)</div><br/></div></div></div></div></div></div><div id="42144517" class="c"><input type="checkbox" id="c-42144517" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#42144283">parent</a><span>|</span><a href="#42144326">prev</a><span>|</span><a href="#42144379">next</a><span>|</span><label class="collapse" for="c-42144517">[-]</label><label class="expand" for="c-42144517">[1 more]</label></div><br/><div class="children"><div class="content">Sorry this is just consiracy theorizing. I&#x27;ve tried jt with GPT-3.5-instruct myself in the OpenAI playeground where the model clearly does nothing but auto-regression. No function calling there whatsoever.</div><br/></div></div><div id="42144379" class="c"><input type="checkbox" id="c-42144379" checked=""/><div class="controls bullet"><span class="by">pixiemaster</span><span>|</span><a href="#42144283">parent</a><span>|</span><a href="#42144517">prev</a><span>|</span><a href="#42144296">next</a><span>|</span><label class="collapse" for="c-42144379">[-]</label><label class="expand" for="c-42144379">[1 more]</label></div><br/><div class="children"><div class="content">I have this hypothesis as well, that OpenAI added a lot
of „classic“ algorithms and rules over time, (eg rules for filtering etc)</div><br/></div></div><div id="42144296" class="c"><input type="checkbox" id="c-42144296" checked=""/><div class="controls bullet"><span class="by">kylebenzle</span><span>|</span><a href="#42144283">parent</a><span>|</span><a href="#42144379">prev</a><span>|</span><a href="#42144837">next</a><span>|</span><label class="collapse" for="c-42144296">[-]</label><label class="expand" for="c-42144296">[1 more]</label></div><br/><div class="children"><div class="content">Yes! I also was waiting for this seemingly obvious answer in the article as well. Hopefully the author will see these comments.</div><br/></div></div></div></div><div id="42144837" class="c"><input type="checkbox" id="c-42144837" checked=""/><div class="controls bullet"><span class="by">Peteragain</span><span>|</span><a href="#42144283">prev</a><span>|</span><a href="#42143161">next</a><span>|</span><label class="collapse" for="c-42144837">[-]</label><label class="expand" for="c-42144837">[1 more]</label></div><br/><div class="children"><div class="content">I would be interested to know if the good result is repeatable. We had a similar result with a quirky chat interface in that one run gave great results (and we kept the video) but then we couldn&#x27;t do it again. The cynical among us think there was a mechanical turk involved in our good run.
The economics of venture capital means that there is enormous pressure to justify techniques that we think of as &quot;cheating&quot;. And of course the companies involved have the resources.</div><br/></div></div><div id="42143161" class="c"><input type="checkbox" id="c-42143161" checked=""/><div class="controls bullet"><span class="by">lukev</span><span>|</span><a href="#42144837">prev</a><span>|</span><a href="#42144214">next</a><span>|</span><label class="collapse" for="c-42143161">[-]</label><label class="expand" for="c-42143161">[8 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t necessarily believe this for a second but I&#x27;m going to suggest it because I&#x27;m feeling spicy.<p>OpenAI clearly downgrades some of their APIs from their maximal theoretic capability, for the purposes of response time&#x2F;alignment&#x2F;efficiency&#x2F;whatever.<p>Multiple comments in this thread also say they couldn&#x27;t reproduce the results for gpt3.5-turbo-instruct.<p>So what if the OP just happened to test at a time, or be IP bound to an instance, where the model was not nerfed? What if 3.5 and all subsequent OpenAI models can perform at this level but it&#x27;s not strategic or cost effective for OpenAI to expose that consistently?<p>For the record, I don&#x27;t actually believe this. But given the data it&#x27;s a logical possibility.</div><br/><div id="42144445" class="c"><input type="checkbox" id="c-42144445" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#42143161">parent</a><span>|</span><a href="#42143229">next</a><span>|</span><label class="collapse" for="c-42144445">[-]</label><label class="expand" for="c-42144445">[2 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI clearly downgrades some of their APIs from their maximal theoretic capability, for the purposes of response time&#x2F;alignment&#x2F;efficiency&#x2F;whatever.<p>When ChatGPT3.5 first came out, people were using it to simulate entire Linux system installs, and even browsing a simulated Internet.<p>Cool use cases like that aren&#x27;t even discussed anymore.<p>I still wonder what sort of magic OpenAI had and then locked up away from the world in the name of cost savings.<p>Same thing with GPT 4 vs 4o, 4o is obviously worse in some ways, but after the initial release (when a bunch of people mentioned this), the issue has just been collectively ignored.</div><br/><div id="42144529" class="c"><input type="checkbox" id="c-42144529" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#42143161">root</a><span>|</span><a href="#42144445">parent</a><span>|</span><a href="#42143229">next</a><span>|</span><label class="collapse" for="c-42144529">[-]</label><label class="expand" for="c-42144529">[1 more]</label></div><br/><div class="children"><div class="content">You can still do this. People just lost interest in this stuff because it became clear to ehich degree the simulation is really being done (shallow).<p>Yet I do wish we had access to less finetuned&#x2F;distilled&#x2F;RLHF&#x27;d models.</div><br/></div></div></div></div><div id="42143229" class="c"><input type="checkbox" id="c-42143229" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42143161">parent</a><span>|</span><a href="#42144445">prev</a><span>|</span><a href="#42143264">next</a><span>|</span><label class="collapse" for="c-42143229">[-]</label><label class="expand" for="c-42143229">[1 more]</label></div><br/><div class="children"><div class="content">Stallman may have its flaws, but this is why serious research occurs with source code (or at least with binaries)</div><br/></div></div><div id="42143264" class="c"><input type="checkbox" id="c-42143264" checked=""/><div class="controls bullet"><span class="by">zeven7</span><span>|</span><a href="#42143161">parent</a><span>|</span><a href="#42143229">prev</a><span>|</span><a href="#42144214">next</a><span>|</span><label class="collapse" for="c-42143264">[-]</label><label class="expand" for="c-42143264">[4 more]</label></div><br/><div class="children"><div class="content">Why do you doubt it? I thought it was well known that Chat GPT has degraded over time for the same model, mostly for cost saving reasons.</div><br/><div id="42143324" class="c"><input type="checkbox" id="c-42143324" checked=""/><div class="controls bullet"><span class="by">permo-w</span><span>|</span><a href="#42143161">root</a><span>|</span><a href="#42143264">parent</a><span>|</span><a href="#42144214">next</a><span>|</span><label class="collapse" for="c-42143324">[-]</label><label class="expand" for="c-42143324">[3 more]</label></div><br/><div class="children"><div class="content">ChatGPT is - understandably - blatantly different in the browser compared to the app, or it was until I deleted it anyway</div><br/><div id="42143446" class="c"><input type="checkbox" id="c-42143446" checked=""/><div class="controls bullet"><span class="by">lukan</span><span>|</span><a href="#42143161">root</a><span>|</span><a href="#42143324">parent</a><span>|</span><a href="#42144214">next</a><span>|</span><label class="collapse" for="c-42143446">[-]</label><label class="expand" for="c-42143446">[2 more]</label></div><br/><div class="children"><div class="content">I do not understand that. The app does not do any processing, just a UI to send text to and from the server.</div><br/><div id="42144820" class="c"><input type="checkbox" id="c-42144820" checked=""/><div class="controls bullet"><span class="by">isaacfrond</span><span>|</span><a href="#42143161">root</a><span>|</span><a href="#42143446">parent</a><span>|</span><a href="#42144214">next</a><span>|</span><label class="collapse" for="c-42144820">[-]</label><label class="expand" for="c-42144820">[1 more]</label></div><br/><div class="children"><div class="content">There is a small difference between the app and the browser. before each session, the llm is started with a systems prompt. these are different for the app and the browser. You can find them online somewhere, but iirc the app is instructed to give shorter answers</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42144214" class="c"><input type="checkbox" id="c-42144214" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#42143161">prev</a><span>|</span><a href="#42141647">next</a><span>|</span><label class="collapse" for="c-42144214">[-]</label><label class="expand" for="c-42144214">[1 more]</label></div><br/><div class="children"><div class="content">We know from experience with different humans that there are different types of skills and different types of intelligence. Some savants might be superhuman at one task but basically mentally disabled at all other things.<p>It could be that the model that does chess well just happens to have the right &#x27;connectome&#x27; purely by accident of how the various back-propagations worked out to land on various local maxima (model weights) during training. It might even be (probably is) a non-verbal connectome that&#x27;s just purely logic rules, having nothing to do with language at all, but a semantic space pattern that got landed on accidentally, which can solve this class of problem.<p>Reminds me of how Daniel Tammet just visually &quot;sees&quot; answers to math problems in his mind without even knowing how they appear. It&#x27;s like he sees a virtual screen with a representation akin to numbers (the answer) just sitting there to be read out from his visual cortex. He&#x27;s not &#x27;working out&#x27; the solutions. They&#x27;re just handed to him purely by some connectome effects going on in the background.</div><br/></div></div><div id="42141647" class="c"><input type="checkbox" id="c-42141647" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#42144214">prev</a><span>|</span><a href="#42143134">next</a><span>|</span><label class="collapse" for="c-42141647">[-]</label><label class="expand" for="c-42141647">[16 more]</label></div><br/><div class="children"><div class="content">Maybe that one which plays chess well is calling out to a real chess engine.</div><br/><div id="42142342" class="c"><input type="checkbox" id="c-42142342" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42141647">parent</a><span>|</span><a href="#42142323">next</a><span>|</span><label class="collapse" for="c-42142342">[-]</label><label class="expand" for="c-42142342">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not:<p>1. That would just be plain bizzare<p>2. It plays like what you&#x27;d expect from a LLM that could play chess. That is, level of play can be modulated by the prompt and doesn&#x27;t manifest the same way shifting the level of stockfish etc does. Also the specific chess notation being prompted actually matters<p>3. It&#x27;s sensitive to how the position came to be. Clearly not an existing chess engine.
<a href="https:&#x2F;&#x2F;github.com&#x2F;dpaleka&#x2F;llm-chess-proofgame">https:&#x2F;&#x2F;github.com&#x2F;dpaleka&#x2F;llm-chess-proofgame</a><p>4. It does make illegal moves. It&#x27;s rare (~5 in 8205) but it happens. <a href="https:&#x2F;&#x2F;github.com&#x2F;adamkarvonen&#x2F;chess_gpt_eval">https:&#x2F;&#x2F;github.com&#x2F;adamkarvonen&#x2F;chess_gpt_eval</a><p>5. You can or well you used to be able to inspect the logprobs. I think Open AI have stopped doing this but the link in 4 does show the author inspecting it for Turbo instruct.</div><br/><div id="42142524" class="c"><input type="checkbox" id="c-42142524" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42141647">root</a><span>|</span><a href="#42142342">parent</a><span>|</span><a href="#42142642">next</a><span>|</span><label class="collapse" for="c-42142524">[-]</label><label class="expand" for="c-42142524">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Also the specific chess notation being prompted actually matters<p>Couldn&#x27;t this be evidence that it <i>is</i> using an engine? Maybe  if you use the wrong notation it relies on the ANN rather than calling to the engine.<p>Likewise:<p>- The sensitivity to game history is interesting, but is it actually true that other chess engines only look at current board state? Regardless, maybe it&#x27;s not an <i>existing</i> chess engine! I would think OpenAI has some custom chess engine built as a side project, PoC, etc. In particular this engine might be neural and trained on actual games rather than board positions, which could explain dependency on past moves. Note that the engine is not actually very good. Does AlphaZero depend on move history? (Genuine question, I am not sure. But it does seem likely.)<p>- I think the illegal moves can be explained similarly to why gpt-o1 sometimes screws up easy computations despite having access to Python: an LLM having access to a tool does not guarantee it always uses that tool.<p>I realize there are holes in the argument, but I genuinely don&#x27;t think these holes are as big as the &quot;why is gpt-3.5-turbo-instruct so much better at chess than gpt-4?&quot;</div><br/><div id="42143234" class="c"><input type="checkbox" id="c-42143234" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42141647">root</a><span>|</span><a href="#42142524">parent</a><span>|</span><a href="#42142642">next</a><span>|</span><label class="collapse" for="c-42143234">[-]</label><label class="expand" for="c-42143234">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Couldn’t this be evidence that it is using an engine?<p>A test would be to measure its performance against more difficult versions of Stockfish. A real chess engine would have a higher ceiling.<p>Much more likely is this model was trained on more chess PGNs. You can call that a “neural engine” if you’d like but it is the simplest solution and explains the mistakes it is making.<p>Game state isn’t just what you can see on the board. It includes the 50 move rule and castling rights. Those were encoded as layers in AlphaZero along with prior positions of pieces. (8 prior positions if I’m remembering correctly.)</div><br/></div></div></div></div></div></div><div id="42142323" class="c"><input type="checkbox" id="c-42142323" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42141647">parent</a><span>|</span><a href="#42142342">prev</a><span>|</span><a href="#42143067">next</a><span>|</span><label class="collapse" for="c-42142323">[-]</label><label class="expand" for="c-42142323">[4 more]</label></div><br/><div class="children"><div class="content">The author thinks this is unlikely because it only has an ~1800 ELO. But OpenAI is shady as hell, and I could absolutely see the following <i>purely hypothetical</i> scenario:<p>- In 2022 Brockman and Sutskever have an unshakeable belief that Scaling Is All You Need, and since GPT-4 has a ton of chess in its pretraining data it will <i>definitely</i> be able to play competent amateur chess when it&#x27;s finished.<p>- A ton of people have pointed out that ChatGPT-3.5 doesn&#x27;t even slightly understand chess despite seeming fluency in the lingo. People start to whisper that transformers cannot actually create plans.<p>- Therefore OpenAI hatches an impulsive scheme: release an &quot;instruction-tuned&quot; GPT-3.5 with an embedded chess engine that is not a grandmaster, but can play competent chess, ideally just below the ELO that GPT-4 is <i>projected</i> to have.<p>- Success! The waters are muddied: GPT enthusiasts triumphantly announce that LLMs <i>can</i> play chess, it just took a bit more data and fine-tuning. The haters were wrong: look at all the planning GPT is doing!<p>- Later on, at OpenAI HQ...whoops! GPT-4 sucks at chess, as do competitors&#x27; foundation LLMs which otherwise outperform GPt-3.5. The scaling &quot;laws&quot; failed here, since they were never laws in the first place. OpenAI accepts that scaling transformers won&#x27;t easily solve the chess problem, then realizes that if they include the chess engine with GPT-4 without publicly acknowledging it, then Anthropic and Facebook will call out the performance as aberrational and suspicious. But publicly acknowledging a chess engine is even worse: the only reason to include the chess engine is to mislead users into thinking GPT is capable of general-purpose planning.<p>- Therefore in later GPT versions they don&#x27;t include the engine, but it&#x27;s too late to remove it from gpt-3.5-turbo-instruct: people might accept the (specious) claim that GPT-4&#x27;s size accidentally sabotaged its chess abilities, but they&#x27;ll ask tough questions about performance degradation within the same model.<p>I realize this is convoluted and depends on conjecture. But OpenAI has a history with misleading demos - e.g. their Rubik&#x27;s cube robot which in fact used a classical algorithm but was presented as reinforcement learning. I think &quot;OpenAI lied&quot; is the most likely scenario. It is far more likely than &quot;OpenAI solved the problem honestly in GPT-3.5, but forgot how they did it with GPT-4,&quot; and a bit more likely than &quot;scaling transformers slightly helps performance when playing Othello but severely sabotages performance when playing chess.&quot;</div><br/><div id="42143261" class="c"><input type="checkbox" id="c-42143261" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#42141647">root</a><span>|</span><a href="#42142323">parent</a><span>|</span><a href="#42142488">next</a><span>|</span><label class="collapse" for="c-42143261">[-]</label><label class="expand" for="c-42143261">[1 more]</label></div><br/><div class="children"><div class="content">Very good scenario. One variation: some researcher or division in OpenAI performs all of the above steps to get a raise. The whole field is predicated on rewarding the appearance of ability.</div><br/></div></div><div id="42142488" class="c"><input type="checkbox" id="c-42142488" checked=""/><div class="controls bullet"><span class="by">gardenhedge</span><span>|</span><a href="#42141647">root</a><span>|</span><a href="#42142323">parent</a><span>|</span><a href="#42143261">prev</a><span>|</span><a href="#42143067">next</a><span>|</span><label class="collapse" for="c-42142488">[-]</label><label class="expand" for="c-42142488">[2 more]</label></div><br/><div class="children"><div class="content">Not that convoluted really</div><br/><div id="42142722" class="c"><input type="checkbox" id="c-42142722" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42141647">root</a><span>|</span><a href="#42142488">parent</a><span>|</span><a href="#42143067">next</a><span>|</span><label class="collapse" for="c-42142722">[-]</label><label class="expand" for="c-42142722">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty convoluted, requires a ton of steps, mind-reading, and odd sequencing.*<p>If you share every prior, and aren&#x27;t particularly concerned with being disciplined in treating conversation as proposing a logical argument (I&#x27;m not myself, people find it offputting), it probably wouldn&#x27;t seem at all convoluted.<p>* layer chess into gpt-3.5-instruct <i>only</i>, but not chatgpt, not GPT-4, to defeat the naysayers when GPT-4 comes out? <i>shrugs</i> if the issues with that are unclear,  I can lay it out more<p>** fwiw, at the time, pre-chatgpt, before the hype, there wasn&#x27;t a huge focus on chess, nor a ton of naysayers to defeat. it would have been bizarre to put this much energy into it, modulo the scatter-brained thinking in *</div><br/></div></div></div></div></div></div><div id="42143067" class="c"><input type="checkbox" id="c-42143067" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#42141647">parent</a><span>|</span><a href="#42142323">prev</a><span>|</span><a href="#42141959">next</a><span>|</span><label class="collapse" for="c-42143067">[-]</label><label class="expand" for="c-42143067">[1 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s the most plausible theory that would explain the sudden hike from gpt-3.5-turbo to gpt-3.5-turbo-instruct, and again the sudden regression in gpt-4*.<p>OpenAI also seem to augment the LLM with some type of VM or a Python interpreter. Maybe they run a simple chess engine such as Sunfish [1] which is around 1900-2000 ELO [2]?<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;thomasahle&#x2F;sunfish">https:&#x2F;&#x2F;github.com&#x2F;thomasahle&#x2F;sunfish</a><p>[2] <a href="https:&#x2F;&#x2F;lichess.org&#x2F;@&#x2F;sunfish-engine" rel="nofollow">https:&#x2F;&#x2F;lichess.org&#x2F;@&#x2F;sunfish-engine</a></div><br/></div></div><div id="42141959" class="c"><input type="checkbox" id="c-42141959" checked=""/><div class="controls bullet"><span class="by">sobriquet9</span><span>|</span><a href="#42141647">parent</a><span>|</span><a href="#42143067">prev</a><span>|</span><a href="#42143188">next</a><span>|</span><label class="collapse" for="c-42141959">[-]</label><label class="expand" for="c-42141959">[1 more]</label></div><br/><div class="children"><div class="content">This is likely. From example games, it not only knows the rules (which would be impressive by itself, just making the legal moves is not trivial). It also has some planning capabilities (plays combinations of several moves).</div><br/></div></div><div id="42143188" class="c"><input type="checkbox" id="c-42143188" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42141647">parent</a><span>|</span><a href="#42141959">prev</a><span>|</span><a href="#42141726">next</a><span>|</span><label class="collapse" for="c-42143188">[-]</label><label class="expand" for="c-42143188">[1 more]</label></div><br/><div class="children"><div class="content">Probably not calling out to one but it would not surprise me at all if they added more chess PGNs into their training data. Chess is a bit special in AI in that it’s still seen as a mark of pure intelligence in some respect.<p>If you tested it on an equally strategic but less popular game I highly doubt you would see the same performance.</div><br/></div></div><div id="42141726" class="c"><input type="checkbox" id="c-42141726" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#42141647">parent</a><span>|</span><a href="#42143188">prev</a><span>|</span><a href="#42143134">next</a><span>|</span><label class="collapse" for="c-42141726">[-]</label><label class="expand" for="c-42141726">[4 more]</label></div><br/><div class="children"><div class="content">this possibility is discussed in the article and deemed unlikely</div><br/><div id="42141854" class="c"><input type="checkbox" id="c-42141854" checked=""/><div class="controls bullet"><span class="by">probably_wrong</span><span>|</span><a href="#42141647">root</a><span>|</span><a href="#42141726">parent</a><span>|</span><a href="#42141784">next</a><span>|</span><label class="collapse" for="c-42141854">[-]</label><label class="expand" for="c-42141854">[2 more]</label></div><br/><div class="children"><div class="content">Note: the possibility is not mentioned in the article but rather in the comments [1]. I had to click a bit to see it.<p>The fact that the one closed source model is the only one that plays well seems to me like a clear case of the interface doing some of the work. If you ask ChatGPT to count until 10000 (something that most LLMs can&#x27;t do for known reasons) you get an answer that&#x27;s clearly pre-programmed. I&#x27;m sure the same is happening here (and with many, many other tasks) - the author argues against it by saying &quot;but why isn&#x27;t it better?&quot;, which doesn&#x27;t seem like the best argument: I can imagine that typical ChatGPT users enjoy the product more if they have a chance to win once in a while.<p>[1] <a href="https:&#x2F;&#x2F;dynomight.substack.com&#x2F;p&#x2F;chess&#x2F;comment&#x2F;77190852" rel="nofollow">https:&#x2F;&#x2F;dynomight.substack.com&#x2F;p&#x2F;chess&#x2F;comment&#x2F;77190852</a></div><br/><div id="42142803" class="c"><input type="checkbox" id="c-42142803" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42141647">root</a><span>|</span><a href="#42141854">parent</a><span>|</span><a href="#42141784">next</a><span>|</span><label class="collapse" for="c-42142803">[-]</label><label class="expand" for="c-42142803">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean LLMs can&#x27;t count to 10,000 for known reasons?<p>Separately, if you are able to show OpenAI is serving pre canned responses in some instances, instead of running inference, you will get a ton of attention if you write it up.<p>I&#x27;m not saying this in an aggro tone, it&#x27;s a genuinely interesting subject to me because I wrote off LLMs at first because I thought this was going on.* Then I spent the last couple years laughing at myself for thinking that they would do that. Would be some mix of fascinated and horrified to see it come full circle.<p>* I can&#x27;t remember, what, exactly, it was far back as 2018. But someone argued that OpenAI was patching in individual answers because scaling was dead and they had no answers, way way before ChatGPT.</div><br/></div></div></div></div><div id="42141784" class="c"><input type="checkbox" id="c-42141784" checked=""/><div class="controls bullet"><span class="by">margalabargala</span><span>|</span><a href="#42141647">root</a><span>|</span><a href="#42141726">parent</a><span>|</span><a href="#42141854">prev</a><span>|</span><a href="#42143134">next</a><span>|</span><label class="collapse" for="c-42141784">[-]</label><label class="expand" for="c-42141784">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see that discussed, could you quote it?</div><br/></div></div></div></div></div></div><div id="42143134" class="c"><input type="checkbox" id="c-42143134" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#42141647">prev</a><span>|</span><a href="#42142557">next</a><span>|</span><label class="collapse" for="c-42143134">[-]</label><label class="expand" for="c-42143134">[4 more]</label></div><br/><div class="children"><div class="content">My money is on a fluke inclusion of more chess data in that models training.<p>All the other models do vaguely similarly well in other tasks and are in many cases architecturally similar so training data is the most likely explanation</div><br/><div id="42143307" class="c"><input type="checkbox" id="c-42143307" checked=""/><div class="controls bullet"><span class="by">permo-w</span><span>|</span><a href="#42143134">parent</a><span>|</span><a href="#42143272">next</a><span>|</span><label class="collapse" for="c-42143307">[-]</label><label class="expand" for="c-42143307">[2 more]</label></div><br/><div class="children"><div class="content">I feel like a lot of people here are slightly misunderstanding how LLM training works. yes the base models are trained somewhat blind on masses of text, but then they&#x27;re heavily fine-tuned with custom, human-generated reinforcement learning, not just for safety, but for any desired feature<p>these companies do quirky one-off training experiments all the time. I would not be remotely shocked if at some point OpenAI paid some trainers to input and favour strong chess moves</div><br/><div id="42143375" class="c"><input type="checkbox" id="c-42143375" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42143134">root</a><span>|</span><a href="#42143307">parent</a><span>|</span><a href="#42143272">next</a><span>|</span><label class="collapse" for="c-42143375">[-]</label><label class="expand" for="c-42143375">[1 more]</label></div><br/><div class="children"><div class="content">From this OpenAI paper (page 29 <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2312.09390#page=29" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2312.09390#page=29</a><p>&quot;A.2 CHESS PUZZLES<p>Data preprocessing. The GPT-4 pretraining dataset included chess games in the format of move sequence known as Portable Game Notation (PGN). We note that only games with players of Elo 1800 or higher were included in pretraining. These games still include the moves that were played in- game, rather than the best moves in the corresponding positions. On the other hand, the chess puzzles require the model to predict the best move. We use the dataset originally introduced in Schwarzschild et al. (2021b) which is sourced from <a href="https:&#x2F;&#x2F;database.lichess.org&#x2F;#puzzles" rel="nofollow">https:&#x2F;&#x2F;database.lichess.org&#x2F;#puzzles</a> (see also Schwarzschild et al., 2021a). We only evaluate the models ability to predict the first move of the puzzle (some of the puzzles require making multiple moves). We follow the pretraining for- mat, and convert each puzzle to a list of moves leading up to the puzzle position, as illustrated in Figure 14. We use 50k puzzles sampled randomly from the dataset as the training set for the weak models and another 50k for weak-to-strong finetuning, and evaluate on 5k puzzles. For bootstrap- ping (Section 4.3.1), we use a new set of 50k puzzles from the same distribution for each step of the process.&quot;</div><br/></div></div></div></div><div id="42143272" class="c"><input type="checkbox" id="c-42143272" checked=""/><div class="controls bullet"><span class="by">bhouston</span><span>|</span><a href="#42143134">parent</a><span>|</span><a href="#42143307">prev</a><span>|</span><a href="#42142557">next</a><span>|</span><label class="collapse" for="c-42143272">[-]</label><label class="expand" for="c-42143272">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. This.</div><br/></div></div></div></div><div id="42142557" class="c"><input type="checkbox" id="c-42142557" checked=""/><div class="controls bullet"><span class="by">bryan0</span><span>|</span><a href="#42143134">prev</a><span>|</span><a href="#42142922">next</a><span>|</span><label class="collapse" for="c-42142557">[-]</label><label class="expand" for="c-42142557">[4 more]</label></div><br/><div class="children"><div class="content">I remember one of the early &quot;breakthroughs&quot; for LLMs in chess was that if it could actually play legal moves(!) In all of these games are the models always playing legal moves? I don&#x27;t think the article says. The fact that an LLM can even reliably play legal moves, 20+ moves into a chess game is somewhat remarkable. It needs to have an accurate representation of the board state even though it was only trained on next token prediction.</div><br/><div id="42143119" class="c"><input type="checkbox" id="c-42143119" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#42142557">parent</a><span>|</span><a href="#42142607">next</a><span>|</span><label class="collapse" for="c-42143119">[-]</label><label class="expand" for="c-42143119">[1 more]</label></div><br/><div class="children"><div class="content">I did a very unscientific test and it did seem to just play legal moves.  Not only that, if I did an illegal move it would tell me that I couldn&#x27;t do it.<p>I think said that I wanted to play with new rules, where a queen could jump over any pawn, and it let me make that rule change -- and we played with this new rule.  Unfortunately, I was trying to play in my head and I got mixed up and ended up losing my queen.  Then I changed the rule one more time -- if you take the queen you lose -- so I won!</div><br/></div></div><div id="42142607" class="c"><input type="checkbox" id="c-42142607" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42142557">parent</a><span>|</span><a href="#42143119">prev</a><span>|</span><a href="#42142610">next</a><span>|</span><label class="collapse" for="c-42142607">[-]</label><label class="expand" for="c-42142607">[1 more]</label></div><br/><div class="children"><div class="content">The author explains what they did: restrict the move options to valid ones when possible (for open models with the ability to enforce grammar during inference) or sample the model for a valid move up to ten times, then pick a random valid move.</div><br/></div></div><div id="42142610" class="c"><input type="checkbox" id="c-42142610" checked=""/><div class="controls bullet"><span class="by">zelphirkalt</span><span>|</span><a href="#42142557">parent</a><span>|</span><a href="#42142607">prev</a><span>|</span><a href="#42142922">next</a><span>|</span><label class="collapse" for="c-42142610">[-]</label><label class="expand" for="c-42142610">[1 more]</label></div><br/><div class="children"><div class="content">I think it only needs to have read sufficient pgns.</div><br/></div></div></div></div><div id="42142922" class="c"><input type="checkbox" id="c-42142922" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42142557">prev</a><span>|</span><a href="#42145008">next</a><span>|</span><label class="collapse" for="c-42142922">[-]</label><label class="expand" for="c-42142922">[5 more]</label></div><br/><div class="children"><div class="content">wow I actually did something similar recently and no LLM could win and the centipawn loss was always going through the roof (sort of). I created a leaderboard based on it.
<a href="https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;what-happens-when-llms-play-chess" rel="nofollow">https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;what-happens-when-llms-play-chess</a><p>I am very surprised by the perf of got-3.5-turbo-instruct. Beating stockfish ? I will have to run the experiment with that model to check that out</div><br/><div id="42142971" class="c"><input type="checkbox" id="c-42142971" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42142922">parent</a><span>|</span><a href="#42145008">next</a><span>|</span><label class="collapse" for="c-42142971">[-]</label><label class="expand" for="c-42142971">[4 more]</label></div><br/><div class="children"><div class="content">PS: I ran and as suspected got-3.5-turbo-instruct does not beat stockfish, it is not even close<p>&quot;Final Results:
gpt-3.5-turbo-instruct: Wins=0, Losses=6, Draws=0, Rating=1500.00
stockfish: Wins=6, Losses=0, Draws=0, Rating=1500.00&quot;<p><a href="https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;870ea03197b3471eaf7e26e9b17e1754?sid=e621216c-2501-45cd-b1f4-abd42e525608" rel="nofollow">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;870ea03197b3471eaf7e26e9b17e1754?...</a></div><br/><div id="42143260" class="c"><input type="checkbox" id="c-42143260" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42142922">root</a><span>|</span><a href="#42142971">parent</a><span>|</span><a href="#42143295">next</a><span>|</span><label class="collapse" for="c-42143260">[-]</label><label class="expand" for="c-42143260">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I always had the LLM play as white against Stockfish—a standard chess AI—on the lowest difficulty setting<p>I think the author was comparing against Stockfish at a lower skill level (roughly, the number of nodes explored in a move).</div><br/><div id="42143574" class="c"><input type="checkbox" id="c-42143574" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42142922">root</a><span>|</span><a href="#42143260">parent</a><span>|</span><a href="#42143295">next</a><span>|</span><label class="collapse" for="c-42143574">[-]</label><label class="expand" for="c-42143574">[1 more]</label></div><br/><div class="children"><div class="content">Did the same and gpt-3.5-turbo-instruct still lost all the games. maybe a diff in stockfish version ? I am using stockfish 16</div><br/></div></div></div></div></div></div></div></div><div id="42145008" class="c"><input type="checkbox" id="c-42145008" checked=""/><div class="controls bullet"><span class="by">downboots</span><span>|</span><a href="#42142922">prev</a><span>|</span><a href="#42142739">next</a><span>|</span><label class="collapse" for="c-42145008">[-]</label><label class="expand" for="c-42145008">[1 more]</label></div><br/><div class="children"><div class="content">In a sense, a chess game is also a dialogue</div><br/></div></div><div id="42142739" class="c"><input type="checkbox" id="c-42142739" checked=""/><div class="controls bullet"><span class="by">ericye16</span><span>|</span><a href="#42145008">prev</a><span>|</span><a href="#42144520">next</a><span>|</span><label class="collapse" for="c-42142739">[-]</label><label class="expand" for="c-42142739">[1 more]</label></div><br/><div class="children"><div class="content">I agree with some of the other comments here that the prompt is limiting. The model can&#x27;t do any computation without emitting tokens and limiting the numbers of tokens it can emit is going to limit the skill of the model. It&#x27;s surprising that any model at all is capable of performing well with this prompt in fact.</div><br/></div></div><div id="42144520" class="c"><input type="checkbox" id="c-42144520" checked=""/><div class="controls bullet"><span class="by">layman51</span><span>|</span><a href="#42142739">prev</a><span>|</span><a href="#42141822">next</a><span>|</span><label class="collapse" for="c-42144520">[-]</label><label class="expand" for="c-42144520">[1 more]</label></div><br/><div class="children"><div class="content">It would be really cool if someone could get an LLM to actually launch an anonymous game on Chess.com or Lichess and actually have any sense as to what it’s doing.[1] Some people say that you have to represent the board in a certain way. When I first tried to play chess with an LLM, I would just list out a move and it didn’t do very well at all.<p>[1]: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;Gs3TULwlLCA" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;Gs3TULwlLCA</a></div><br/></div></div><div id="42141822" class="c"><input type="checkbox" id="c-42141822" checked=""/><div class="controls bullet"><span class="by">digging</span><span>|</span><a href="#42144520">prev</a><span>|</span><a href="#42141771">next</a><span>|</span><label class="collapse" for="c-42141822">[-]</label><label class="expand" for="c-42141822">[2 more]</label></div><br/><div class="children"><div class="content">Definitely weird results, but I feel there are too many variables to learn much from it. A couple things:<p>1. The author mentioned that tokenization causes something minuscule like a a &quot; &quot; at the end of the input to shatter the model&#x27;s capabilities. Is it possible other slightly different formatting changes in the input could raise capabilities?<p>2. Temperature was 0.7 for all models. What if it wasn&#x27;t? Isn&#x27;t there a chance one more more models would perform significantly better with higher or lower temperatures?<p>Maybe I just don&#x27;t understand this stuff very well, but it feels like this post is only 10% of the work needed to get any meaning from this...</div><br/><div id="42142458" class="c"><input type="checkbox" id="c-42142458" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#42141822">parent</a><span>|</span><a href="#42141771">next</a><span>|</span><label class="collapse" for="c-42142458">[-]</label><label class="expand" for="c-42142458">[1 more]</label></div><br/><div class="children"><div class="content">The author mentions in the comment section that changing temperature did not help.</div><br/></div></div></div></div><div id="42141771" class="c"><input type="checkbox" id="c-42141771" checked=""/><div class="controls bullet"><span class="by">ChrisArchitect</span><span>|</span><a href="#42141822">prev</a><span>|</span><a href="#42142141">next</a><span>|</span><label class="collapse" for="c-42141771">[-]</label><label class="expand" for="c-42141771">[1 more]</label></div><br/><div class="children"><div class="content">[dupe] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42138276">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42138276</a></div><br/></div></div><div id="42142141" class="c"><input type="checkbox" id="c-42142141" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#42141771">prev</a><span>|</span><a href="#42142406">next</a><span>|</span><label class="collapse" for="c-42142141">[-]</label><label class="expand" for="c-42142141">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think one model is statistically significant. As people have pointed out, it could have chess specific responses that the others do not. There should be at least another one or two, preferably unrelated, &quot;good&quot; data points before you can claim there is a pattern. Also, where&#x27;s Claude?</div><br/><div id="42142225" class="c"><input type="checkbox" id="c-42142225" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42142141">parent</a><span>|</span><a href="#42142406">next</a><span>|</span><label class="collapse" for="c-42142225">[-]</label><label class="expand" for="c-42142225">[1 more]</label></div><br/><div class="children"><div class="content">There are other transformers that have been trained on chess text that play chess fine (just not as good as 3.5 Turbo instruct with the exception of the &quot;grandmaster level without search&quot; paper).</div><br/></div></div></div></div><div id="42142406" class="c"><input type="checkbox" id="c-42142406" checked=""/><div class="controls bullet"><span class="by">cmpalmer52</span><span>|</span><a href="#42142141">prev</a><span>|</span><a href="#42142888">next</a><span>|</span><label class="collapse" for="c-42142406">[-]</label><label class="expand" for="c-42142406">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think it would have an impact great enough to explain the discrepancies you saw, but some chess engines on very low difficulty settings make “dumb” moves sometimes. I’m not great at chess and I have trouble against them sometimes because they don’t make the kind of mistakes humans make. Moving the difficulty up a bit makes the games more predictable, in that you can predict and force an outcome without the computer blowing it with a random bad move. Maybe part of the problem is them not dealing with random moves well.<p>I think an interesting challenge would be looking at a board configuration and scoring it on how likely it is to be real - something high ranked chess players can do without much thought (telling a random setup of pieces from a game in progress).</div><br/></div></div><div id="42142888" class="c"><input type="checkbox" id="c-42142888" checked=""/><div class="controls bullet"><span class="by">abalaji</span><span>|</span><a href="#42142406">prev</a><span>|</span><a href="#42144701">next</a><span>|</span><label class="collapse" for="c-42142888">[-]</label><label class="expand" for="c-42142888">[1 more]</label></div><br/><div class="children"><div class="content">An easy way to make all LLMs somewhat good at chess is to make a Chess Eval that you publish and get traction with. Suddenly you will find that all newer frontier models are half decent at chess.</div><br/></div></div><div id="42144701" class="c"><input type="checkbox" id="c-42144701" checked=""/><div class="controls bullet"><span class="by">nusl</span><span>|</span><a href="#42142888">prev</a><span>|</span><a href="#42143282">next</a><span>|</span><label class="collapse" for="c-42144701">[-]</label><label class="expand" for="c-42144701">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I only ran 10 trials since AI companies have inexplicably neglected to send me free API keys<p>Sure, but nobody is required to send you anything for free.</div><br/></div></div><div id="42143282" class="c"><input type="checkbox" id="c-42143282" checked=""/><div class="controls bullet"><span class="by">ks2048</span><span>|</span><a href="#42144701">prev</a><span>|</span><a href="#42142548">next</a><span>|</span><label class="collapse" for="c-42143282">[-]</label><label class="expand" for="c-42143282">[1 more]</label></div><br/><div class="children"><div class="content">Has anyone tried to see how many chess games models are trained on? Is there any chance they consume lichess database dumps, or something similar? I guess the problem is most (all?) top LLMs, even open-weight ones, don’t reveal their training data. But I’m not sure.</div><br/></div></div><div id="42142548" class="c"><input type="checkbox" id="c-42142548" checked=""/><div class="controls bullet"><span class="by">tqi</span><span>|</span><a href="#42143282">prev</a><span>|</span><a href="#42143455">next</a><span>|</span><label class="collapse" for="c-42142548">[-]</label><label class="expand" for="c-42142548">[2 more]</label></div><br/><div class="children"><div class="content">I assume LLMs will be fairly average at chess for the same reason it cant count Rs in Strawberry - it&#x27;s reflecting the training set and not using any underlying logic? Granted my understanding of LLMs is not very sophisticated, but I would be surprised if the Reward Models used were able to distinguish high quality moves vs subpar moves...</div><br/><div id="42143439" class="c"><input type="checkbox" id="c-42143439" checked=""/><div class="controls bullet"><span class="by">ClassyJacket</span><span>|</span><a href="#42142548">parent</a><span>|</span><a href="#42143455">next</a><span>|</span><label class="collapse" for="c-42143439">[-]</label><label class="expand" for="c-42143439">[1 more]</label></div><br/><div class="children"><div class="content">LLMs can&#x27;t count the Rs in strawberry because of tokenization. Words are converted to vectors (numbers), so the actual transformer network never sees the letters that make up the word.<p>ChatGPT doesn&#x27;t see &quot;strawberry&quot;, it sees [302, 1618, 19772]</div><br/></div></div></div></div><div id="42143455" class="c"><input type="checkbox" id="c-42143455" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#42142548">prev</a><span>|</span><a href="#42141754">next</a><span>|</span><label class="collapse" for="c-42143455">[-]</label><label class="expand" for="c-42143455">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;d be super funny if the &quot;gpt-3.5-turbo-instruct&quot; approach has a human in the loop. ;)<p>Or maybe it&#x27;s able to recognise the chess game, then get moves from an external chess game API?</div><br/></div></div></div></div></div></div></div></body></html>