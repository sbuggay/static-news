<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735635663378" as="style"/><link rel="stylesheet" href="styles.css?v=1735635663378"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://aipapersacademy.com/chain-of-continuous-thought/">Coconut by Meta AI – Better LLM Reasoning with Chain of Continuous Thought?</a> <span class="domain">(<a href="https://aipapersacademy.com">aipapersacademy.com</a>)</span></div><div class="subtext"><span>TaurenHunter</span> | <span>51 comments</span></div><br/><div><div id="42557250" class="c"><input type="checkbox" id="c-42557250" checked=""/><div class="controls bullet"><span class="by">jeswin</span><span>|</span><a href="#42556535">next</a><span>|</span><label class="collapse" for="c-42557250">[-]</label><label class="expand" for="c-42557250">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. Due to its emphasiss on BFS, it&#x27;s the opposite of something I&#x27;ve been trying (I named it the &quot;Tree of failures&quot;).<p>My assumption was that humans don&#x27;t try a breadth-first approach. Instead, we split a task into a short-step (instinct and intuition selected), and long-step that summarizes&#x2F;stores the next steps.  The key idea is to recursively evaluate a task as a short-step (high-res - gets executed) and a long-step (lower-res - is just stored), until it succeeds or fails. If it fails, we must walk back keeping a summarized tree of failures in state so that we can exclude them in future selections.<p>The effectiveness of instinct has a steep fall-off at longer distances - so it&#x27;s better not to chart out of a series of steps. When we do BFS, we drive down the value of instinct in favor of compute. I guess ultimately, it depends on the type of problem you want to solve.<p>Reach out to me if you want to prototype it with me.</div><br/></div></div><div id="42556535" class="c"><input type="checkbox" id="c-42556535" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#42557250">prev</a><span>|</span><a href="#42556236">next</a><span>|</span><label class="collapse" for="c-42556535">[-]</label><label class="expand" for="c-42556535">[1 more]</label></div><br/><div class="children"><div class="content">Paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.06769" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.06769</a><p>The link is in the OP, hidden away in an image caption fir some reason.</div><br/></div></div><div id="42556236" class="c"><input type="checkbox" id="c-42556236" checked=""/><div class="controls bullet"><span class="by">Klathmon</span><span>|</span><a href="#42556535">prev</a><span>|</span><a href="#42556347">next</a><span>|</span><label class="collapse" for="c-42556236">[-]</label><label class="expand" for="c-42556236">[4 more]</label></div><br/><div class="children"><div class="content">So is the big improvement here simply skipping the unembedding&#x2F;embedding step for internal thoughts? Or is it mainly in the training methods to teach the CoT and how to switch between &quot;latent thought&quot; and text output?<p>It&#x27;s really interesting that a fixed number of &quot;latent thoughts&quot; performed as well as a binary classifier! I didn&#x27;t expect that at all, the way OpenAI talks about CoT it seems the ability to let it &quot;keep thinking&quot; let&#x27;s them continually score higher on benchmarks while throwing eye watering amounts of compute at the inference.</div><br/><div id="42556390" class="c"><input type="checkbox" id="c-42556390" checked=""/><div class="controls bullet"><span class="by">Crye</span><span>|</span><a href="#42556236">parent</a><span>|</span><a href="#42556347">next</a><span>|</span><label class="collapse" for="c-42556390">[-]</label><label class="expand" for="c-42556390">[3 more]</label></div><br/><div class="children"><div class="content">It mentioned not penalizing&#x2F;rewarding the model for thoughts only rewarding the answer after the thought. I am curious how back propagation works then.</div><br/><div id="42557237" class="c"><input type="checkbox" id="c-42557237" checked=""/><div class="controls bullet"><span class="by">lovasoa</span><span>|</span><a href="#42556236">root</a><span>|</span><a href="#42556390">parent</a><span>|</span><a href="#42556794">next</a><span>|</span><label class="collapse" for="c-42557237">[-]</label><label class="expand" for="c-42557237">[1 more]</label></div><br/><div class="children"><div class="content">The researchers leverage existing language Chain-of-Thought data, where each sample consists of a question, reasoning steps, and the final answer. At stage 0, the model does not generate any thought tokens, and is just trained to yield the reasoning traces and correct answers for the Chain-of-Thought samples. In the subsequent stages, at each stage, we remove one reasoning step from the sample, and instead add thought tokens. In the illustration above, a single thought token is added in each stage, instead of a single reasoning step, but this is controlled by a hyperparameter ‘c’.</div><br/></div></div><div id="42556794" class="c"><input type="checkbox" id="c-42556794" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#42556236">root</a><span>|</span><a href="#42556390">parent</a><span>|</span><a href="#42557237">prev</a><span>|</span><a href="#42556347">next</a><span>|</span><label class="collapse" for="c-42556794">[-]</label><label class="expand" for="c-42556794">[1 more]</label></div><br/><div class="children"><div class="content">The tokens of the answer depend on the preceding continuous thought vectors, which you can backprop through in the usual way.</div><br/></div></div></div></div></div></div><div id="42556347" class="c"><input type="checkbox" id="c-42556347" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42556236">prev</a><span>|</span><a href="#42557351">next</a><span>|</span><label class="collapse" for="c-42556347">[-]</label><label class="expand" for="c-42556347">[9 more]</label></div><br/><div class="children"><div class="content">I was waiting for something like that to happen! Next step - creating a human-language-free representation. I believe that once a group of llms can communicate only in embeddings tuned without any human text input, we&#x27;re going to open a completely new chapter in AI.</div><br/><div id="42557108" class="c"><input type="checkbox" id="c-42557108" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#42556347">parent</a><span>|</span><a href="#42557231">next</a><span>|</span><label class="collapse" for="c-42557108">[-]</label><label class="expand" for="c-42557108">[7 more]</label></div><br/><div class="children"><div class="content">How does a group help anything?<p>If you put 1000 dumb people together, they don&#x27;t magically become smart?</div><br/><div id="42557183" class="c"><input type="checkbox" id="c-42557183" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#42556347">root</a><span>|</span><a href="#42557108">parent</a><span>|</span><a href="#42557206">next</a><span>|</span><label class="collapse" for="c-42557183">[-]</label><label class="expand" for="c-42557183">[1 more]</label></div><br/><div class="children"><div class="content">If you put 1000 people who can&#x27;t talk together they will create language so they can communicate. He&#x27;s saying if we put LLMs together and don&#x27;t force them to use English to communicate then they&#x27;ll create their own language which may be superior for LLMs to English.<p>May be true but who knows.<p>I wonder if anyone has somehow tested the Sapir-Whorf hypothesis for LLMs somehow by training them on different languages and comparing task performance. I guess it&#x27;s too difficult to get a large equivalent training set in different languages.</div><br/></div></div><div id="42557189" class="c"><input type="checkbox" id="c-42557189" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42556347">root</a><span>|</span><a href="#42557108">parent</a><span>|</span><a href="#42557206">prev</a><span>|</span><a href="#42557145">next</a><span>|</span><label class="collapse" for="c-42557189">[-]</label><label class="expand" for="c-42557189">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If you put 1000 dumb people together, they don&#x27;t magically become smart?<p>1000 is probably too high, but groups of people are in fact more intelligent than individuals (though for humans it is likely because recognizing a correct answer is easier than finding it in the first place)</div><br/><div id="42557224" class="c"><input type="checkbox" id="c-42557224" checked=""/><div class="controls bullet"><span class="by">nfw2</span><span>|</span><a href="#42556347">root</a><span>|</span><a href="#42557189">parent</a><span>|</span><a href="#42557145">next</a><span>|</span><label class="collapse" for="c-42557224">[-]</label><label class="expand" for="c-42557224">[1 more]</label></div><br/><div class="children"><div class="content">depends on the circumstances. lin-manuel miranda can probably write a better musical by himself than a team of 20 people with equal input would.<p>also, the bottlenecks that teamwork helps solve (eg the high cost of gaining expertise and low throughput of reasoning capacity) may not be that relevant in the ai age</div><br/></div></div></div></div><div id="42557145" class="c"><input type="checkbox" id="c-42557145" checked=""/><div class="controls bullet"><span class="by">sunshinerag</span><span>|</span><a href="#42556347">root</a><span>|</span><a href="#42557108">parent</a><span>|</span><a href="#42557189">prev</a><span>|</span><a href="#42557231">next</a><span>|</span><label class="collapse" for="c-42557145">[-]</label><label class="expand" for="c-42557145">[2 more]</label></div><br/><div class="children"><div class="content">Wait what … how does democracy work then?</div><br/><div id="42557193" class="c"><input type="checkbox" id="c-42557193" checked=""/><div class="controls bullet"><span class="by">nfw2</span><span>|</span><a href="#42556347">root</a><span>|</span><a href="#42557145">parent</a><span>|</span><a href="#42557231">next</a><span>|</span><label class="collapse" for="c-42557193">[-]</label><label class="expand" for="c-42557193">[1 more]</label></div><br/><div class="children"><div class="content">the benefit of democracy is primarily that it prevents governments from doing bad things, less so that it empowers more effective governance</div><br/></div></div></div></div></div></div><div id="42557231" class="c"><input type="checkbox" id="c-42557231" checked=""/><div class="controls bullet"><span class="by">jkingsman</span><span>|</span><a href="#42556347">parent</a><span>|</span><a href="#42557108">prev</a><span>|</span><a href="#42557351">next</a><span>|</span><label class="collapse" for="c-42557231">[-]</label><label class="expand" for="c-42557231">[1 more]</label></div><br/><div class="children"><div class="content">How does one impart textual knowledge discovered by humans without language?</div><br/></div></div></div></div><div id="42557351" class="c"><input type="checkbox" id="c-42557351" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#42556347">prev</a><span>|</span><a href="#42557315">next</a><span>|</span><label class="collapse" for="c-42557351">[-]</label><label class="expand" for="c-42557351">[1 more]</label></div><br/><div class="children"><div class="content">I believe this was shared and discussed here a while ago and this article looks LLM generated. It keeps doing &quot;let&#x27;s start...&quot;. Either it&#x27;s LLM fluff or very poor writing.</div><br/></div></div><div id="42557315" class="c"><input type="checkbox" id="c-42557315" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#42557351">prev</a><span>|</span><a href="#42556920">next</a><span>|</span><label class="collapse" for="c-42557315">[-]</label><label class="expand" for="c-42557315">[1 more]</label></div><br/><div class="children"><div class="content">The thing about &#x27;thinking&quot; in problem solving I think is that thoughts often produce new questions which then guide the overall problem solving. I wonder is this something like that?</div><br/></div></div><div id="42556920" class="c"><input type="checkbox" id="c-42556920" checked=""/><div class="controls bullet"><span class="by">jkelleyrtp</span><span>|</span><a href="#42557315">prev</a><span>|</span><a href="#42556386">next</a><span>|</span><label class="collapse" for="c-42556920">[-]</label><label class="expand" for="c-42556920">[7 more]</label></div><br/><div class="children"><div class="content">I think this might be the “it” moment for AI&#x2F;LLMs. I was hiking  with a friend recently and we talked about this at length.<p>The arc-AGI results from O3 are apparently a result of chain of thought given enough time to explore a solution space. Reasoning might be simply a higher dimensional form of rubix cube solving. BFS, search, back-tracking, etc. It seems unlikely that humans think in “tokens” so why do LLMs?<p>By staying in latent space, the models are free to describe an “idea” in higher resolution than what language allows. English is coarse, granular. Latent space is a much finer representation of ideas and their interplay.<p>Latent space is also much cheaper to execute in. The model can think  without the language encoding&#x2F;decoding step. This lets it branch out hundreds of ideas and explore only the most useful ones in a fraction of time that reasoning “out-loud” would take.<p>The states also don’t need to be tied to language. Feed in a robot’s state, time series data, or any abstract data. Reason in category theory or linear algebra or complex analysis. Humans are hard wired for one set of math - an abstract latent space can represent anything.<p>I’m a bit disappointed OpenAI didn’t stumble on this first. I’ve been skeptical of LLMs since their big debut last year. LLMs seem like a great way of solving language, but reasoning is much more complex. Once you grok the math behind the current models, you immediately question why the encoding&#x2F;decoding step is there. Diffusion models are incredible but it felt that LLMs lacked the same creativity. Encoding&#x2F;decoding forces a token-based discretization and therefore a loss of complexity.<p>With the byte-latent paper it was quite clear we’d see this paper. This truly might be the “it” moment.</div><br/><div id="42557414" class="c"><input type="checkbox" id="c-42557414" checked=""/><div class="controls bullet"><span class="by">rlupi</span><span>|</span><a href="#42556920">parent</a><span>|</span><a href="#42557102">next</a><span>|</span><label class="collapse" for="c-42557414">[-]</label><label class="expand" for="c-42557414">[1 more]</label></div><br/><div class="children"><div class="content">IMHO The problem (for us) with this approach are the logical consequences:<p>1) if AI large model become more powerful avoiding language, embeddings of AI state become even more tied to the model they originate than now<p>Consequence: AI progress stalls, as AI user companies need to invest increasing amount of money to reindex their growing corpuses.<p>This is already a problem, it becomes more of a lock-in mechanism.<p>If this is overcome...<p>2) Embeddings become a viral mechanism: it makes sense for a large company that commands a market to impose to its suppliers to use the same AI models, because they can transfer state via embeddings rather than external formats.<p>This allows to cut down decisions mechanisms that otherwise require expensive coordination mechanism.<p>Something similar will happen within companies IMHO: <a href="https:&#x2F;&#x2F;rlupi.com&#x2F;okr-planning-as-belief-revision" rel="nofollow">https:&#x2F;&#x2F;rlupi.com&#x2F;okr-planning-as-belief-revision</a><p>3) Eventually this potentially results in another exponential growth and lock-in mechanism, also at the expense of most tech people as more and more is done outside our interface with AI (i.e. programming and software architecture improvements will it self move below language level, we&#x27;ll have to reverse engineering increasingly opaque improvements).<p>4) It ends with the impossibility of AI alignment.<p>---<p>I have written a bit about it in the past at the start of the year, when I had a burnout. So, I deleted those confused ramblings. You can stil find it on archive.org: <a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20240714153146&#x2F;https:&#x2F;&#x2F;rlupi.com&#x2F;the-lupi-program" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20240714153146&#x2F;https:&#x2F;&#x2F;rlupi.com...</a></div><br/></div></div><div id="42557102" class="c"><input type="checkbox" id="c-42557102" checked=""/><div class="controls bullet"><span class="by">otikik</span><span>|</span><a href="#42556920">parent</a><span>|</span><a href="#42557414">prev</a><span>|</span><a href="#42557026">next</a><span>|</span><label class="collapse" for="c-42557102">[-]</label><label class="expand" for="c-42557102">[4 more]</label></div><br/><div class="children"><div class="content">&gt; It seems unlikely that humans think in “tokens” so why do LLMs?<p>I can think of one reason: scrutability. It’s going to be even harder to understand how a response gets produced if there isn’t even a text-based representation to help the human understand</div><br/><div id="42557188" class="c"><input type="checkbox" id="c-42557188" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#42556920">root</a><span>|</span><a href="#42557102">parent</a><span>|</span><a href="#42557026">next</a><span>|</span><label class="collapse" for="c-42557188">[-]</label><label class="expand" for="c-42557188">[3 more]</label></div><br/><div class="children"><div class="content">I think we&#x27;re already way beyond the point where anyone really understands how a response is produced, even without this.</div><br/><div id="42557353" class="c"><input type="checkbox" id="c-42557353" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#42556920">root</a><span>|</span><a href="#42557188">parent</a><span>|</span><a href="#42557338">next</a><span>|</span><label class="collapse" for="c-42557353">[-]</label><label class="expand" for="c-42557353">[1 more]</label></div><br/><div class="children"><div class="content">Indeed. Even if an LLM tells you its “reasoning” process step by step, it’s not actually an exposition of the model’s internal decision process. It’s just more text that, when generated, improves the chances of a good final output.</div><br/></div></div><div id="42557338" class="c"><input type="checkbox" id="c-42557338" checked=""/><div class="controls bullet"><span class="by">nfw2</span><span>|</span><a href="#42556920">root</a><span>|</span><a href="#42557188">parent</a><span>|</span><a href="#42557353">prev</a><span>|</span><a href="#42557026">next</a><span>|</span><label class="collapse" for="c-42557338">[-]</label><label class="expand" for="c-42557338">[1 more]</label></div><br/><div class="children"><div class="content">the token generation part isn&#x27;t well understood, but the output &quot;chain-of-thought&quot; used to produce the final answer can be scrutinized for correctness with a traditional CoT model (although this would require model providers to not hide reasoning tokens)</div><br/></div></div></div></div></div></div></div></div><div id="42556386" class="c"><input type="checkbox" id="c-42556386" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#42556920">prev</a><span>|</span><a href="#42556010">next</a><span>|</span><label class="collapse" for="c-42556386">[-]</label><label class="expand" for="c-42556386">[1 more]</label></div><br/><div class="children"><div class="content">Will this allow Facebook new user base of AI generated characters to interact with themselves better?</div><br/></div></div><div id="42556010" class="c"><input type="checkbox" id="c-42556010" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42556386">prev</a><span>|</span><a href="#42556723">next</a><span>|</span><label class="collapse" for="c-42556010">[-]</label><label class="expand" for="c-42556010">[3 more]</label></div><br/><div class="children"><div class="content">Master coconut! I don’t know if that’s an Archer reference or a Frisky Dingo reference.<p>It’s fascinating how fast the competitors are catching up to each other. Can’t wait for seven different SkyNets to compete for dominance.</div><br/><div id="42556704" class="c"><input type="checkbox" id="c-42556704" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#42556010">parent</a><span>|</span><a href="#42556723">next</a><span>|</span><label class="collapse" for="c-42556704">[-]</label><label class="expand" for="c-42556704">[2 more]</label></div><br/><div class="children"><div class="content">Both! And&#x2F;or, either</div><br/><div id="42556876" class="c"><input type="checkbox" id="c-42556876" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42556010">root</a><span>|</span><a href="#42556704">parent</a><span>|</span><a href="#42556723">next</a><span>|</span><label class="collapse" for="c-42556876">[-]</label><label class="expand" for="c-42556876">[1 more]</label></div><br/><div class="children"><div class="content">A little column a, a little column b.</div><br/></div></div></div></div></div></div><div id="42556723" class="c"><input type="checkbox" id="c-42556723" checked=""/><div class="controls bullet"><span class="by">opdahl</span><span>|</span><a href="#42556010">prev</a><span>|</span><a href="#42556557">next</a><span>|</span><label class="collapse" for="c-42556723">[-]</label><label class="expand" for="c-42556723">[1 more]</label></div><br/><div class="children"><div class="content">This is super cool and something I’ve been waiting on. Would be interesting to intersperse these thinking steps into token generation. What would be the effect of adding lets say 5 thinking «thoughts» for every 50 generated tokens?</div><br/></div></div><div id="42556557" class="c"><input type="checkbox" id="c-42556557" checked=""/><div class="controls bullet"><span class="by">MarkMarine</span><span>|</span><a href="#42556723">prev</a><span>|</span><a href="#42556369">next</a><span>|</span><label class="collapse" for="c-42556557">[-]</label><label class="expand" for="c-42556557">[2 more]</label></div><br/><div class="children"><div class="content">This ought to make it much easier for the AI to lie to us without getting caught.<p><a href="https:&#x2F;&#x2F;www.transformernews.ai&#x2F;p&#x2F;openais-new-model-tried-to-avoid" rel="nofollow">https:&#x2F;&#x2F;www.transformernews.ai&#x2F;p&#x2F;openais-new-model-tried-to-...</a></div><br/><div id="42557443" class="c"><input type="checkbox" id="c-42557443" checked=""/><div class="controls bullet"><span class="by">h8hawk</span><span>|</span><a href="#42556557">parent</a><span>|</span><a href="#42556369">next</a><span>|</span><label class="collapse" for="c-42557443">[-]</label><label class="expand" for="c-42557443">[1 more]</label></div><br/><div class="children"><div class="content">The article is pure nonsense, and even the author added an update admitting it missed important context.</div><br/></div></div></div></div><div id="42556369" class="c"><input type="checkbox" id="c-42556369" checked=""/><div class="controls bullet"><span class="by">davidclark</span><span>|</span><a href="#42556557">prev</a><span>|</span><a href="#42556009">next</a><span>|</span><label class="collapse" for="c-42556369">[-]</label><label class="expand" for="c-42556369">[1 more]</label></div><br/><div class="children"><div class="content">Is this article AI-generated? This website appears to do a lot of “diving in”.</div><br/></div></div><div id="42556009" class="c"><input type="checkbox" id="c-42556009" checked=""/><div class="controls bullet"><span class="by">fosterfriends</span><span>|</span><a href="#42556369">prev</a><span>|</span><a href="#42556108">next</a><span>|</span><label class="collapse" for="c-42556009">[-]</label><label class="expand" for="c-42556009">[13 more]</label></div><br/><div class="children"><div class="content">Once again, we see Meta being more open than OpenAI. I’m loving that their business incentive is aligned with open sourcing and commodifying state-of-the-art LLM technology. Keep em coming</div><br/><div id="42556253" class="c"><input type="checkbox" id="c-42556253" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#42556009">parent</a><span>|</span><a href="#42556229">next</a><span>|</span><label class="collapse" for="c-42556253">[-]</label><label class="expand" for="c-42556253">[10 more]</label></div><br/><div class="children"><div class="content">I mean they have no way to monetize LLMs as well as others, so they’re working on it and giving it away to not look irrelevant and to weaken anyone who may make money off this tech and threaten them in the future. Meanwhile there is a danger they impose their long standing invisible “moderation” on everyone else once they starve all the startups of revenue by giving this away. We’ll just be left with the same big tech overlords to choose from.<p>Oh and it still isn’t open source even though people like Yann LeCun dishonestly claim it is. Only OLMo is truly open source among competitive models, as far as I know:
<a href="https:&#x2F;&#x2F;allenai.org&#x2F;blog&#x2F;olmo2" rel="nofollow">https:&#x2F;&#x2F;allenai.org&#x2F;blog&#x2F;olmo2</a></div><br/><div id="42556923" class="c"><input type="checkbox" id="c-42556923" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556253">parent</a><span>|</span><a href="#42556279">next</a><span>|</span><label class="collapse" for="c-42556923">[-]</label><label class="expand" for="c-42556923">[1 more]</label></div><br/><div class="children"><div class="content">They are definitely making some money off of their licensing to AWS as part of the bedrock offering.  Facebook’s licensing is such that they aren’t going to let happen to them what happened to ElasticSearch, Redis, etc.<p>I’m okay with that.</div><br/></div></div><div id="42556279" class="c"><input type="checkbox" id="c-42556279" checked=""/><div class="controls bullet"><span class="by">spencerflem</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556253">parent</a><span>|</span><a href="#42556923">prev</a><span>|</span><a href="#42556778">next</a><span>|</span><label class="collapse" for="c-42556279">[-]</label><label class="expand" for="c-42556279">[5 more]</label></div><br/><div class="children"><div class="content">Facebook would rather do no moderation, it&#x27;s an expense for them.<p>They do it to make the platform more pleasant so that people stay on it</div><br/><div id="42556336" class="c"><input type="checkbox" id="c-42556336" checked=""/><div class="controls bullet"><span class="by">graemep</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556279">parent</a><span>|</span><a href="#42556338">next</a><span>|</span><label class="collapse" for="c-42556336">[-]</label><label class="expand" for="c-42556336">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They do it to make the platform more pleasant so that people stay on it<p>Almost everything unpleasant I see on FB is stuff that the FB algorithm shows me - not things posted by FB friends, or pages I follow or groups I am in.</div><br/></div></div><div id="42556338" class="c"><input type="checkbox" id="c-42556338" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556279">parent</a><span>|</span><a href="#42556336">prev</a><span>|</span><a href="#42556778">next</a><span>|</span><label class="collapse" for="c-42556338">[-]</label><label class="expand" for="c-42556338">[3 more]</label></div><br/><div class="children"><div class="content">No they do it to support their owners’ and employees’ biases. It doesn’t make the platform more pleasant for the half that gets censored. That’s leaving aside the feed not remembering the choice to view chronologically ordered posts, the inability to easily track actual people in my life, the addictive algorithms, the clickbait that causes mental health issues for teens, etc.</div><br/><div id="42557433" class="c"><input type="checkbox" id="c-42557433" checked=""/><div class="controls bullet"><span class="by">roywiggins</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556338">parent</a><span>|</span><a href="#42556703">next</a><span>|</span><label class="collapse" for="c-42557433">[-]</label><label class="expand" for="c-42557433">[1 more]</label></div><br/><div class="children"><div class="content">The stuff that Facebook moderators are actually tasked with removing is really awful, bad enough to produce severe psychological effects in the moderators.<p>Facebook pays people to look at and remove this stuff because the platform would not survive if it wasn&#x27;t removed before you or I saw it. Do they also enforce other corporate values? Yeah, probably. That doesn&#x27;t seem to be the main job though, they have their hands full dealing with the worst content in the world.<p><a href="https:&#x2F;&#x2F;amp-theguardian-com.cdn.ampproject.org&#x2F;v&#x2F;s&#x2F;amp.theguardian.com&#x2F;media&#x2F;2024&#x2F;dec&#x2F;18&#x2F;kenya-facebook-moderators-sue-after-diagnoses-of-severe-ptsd" rel="nofollow">https:&#x2F;&#x2F;amp-theguardian-com.cdn.ampproject.org&#x2F;v&#x2F;s&#x2F;amp.thegu...</a><p>&gt; The images and videos including necrophilia, bestiality and self-harm caused some moderators to faint, vomit, scream and run away from their desks...<p>&gt; Some reported marriage breakdown and the collapse of desire for sexual intimacy, and losing connection with their families. Some whose job was to remove videos uploaded by terrorist and rebel groups were afraid they were being watched and targeted, and that if they returned home they would be hunted and killed.</div><br/></div></div><div id="42556703" class="c"><input type="checkbox" id="c-42556703" checked=""/><div class="controls bullet"><span class="by">creato</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556338">parent</a><span>|</span><a href="#42557433">prev</a><span>|</span><a href="#42556778">next</a><span>|</span><label class="collapse" for="c-42556703">[-]</label><label class="expand" for="c-42556703">[1 more]</label></div><br/><div class="children"><div class="content">99% of FB&#x27;s moderation has nothing to do with &quot;biases&quot;, unless you think FB is biased against spam, scams, and all the other dregs of the internet that incessantly pops up anywhere users can post content.</div><br/></div></div></div></div></div></div><div id="42556778" class="c"><input type="checkbox" id="c-42556778" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556253">parent</a><span>|</span><a href="#42556279">prev</a><span>|</span><a href="#42556870">next</a><span>|</span><label class="collapse" for="c-42556778">[-]</label><label class="expand" for="c-42556778">[2 more]</label></div><br/><div class="children"><div class="content">&gt; they have no way to monetize LLMs as well as others<p>Random nobodies are putting together companies to monetize generative AI and getting bought out a couple of years later, you think Meta couldn&#x27;t figure out how to deploy their own models to an API and stick up a billing interface if they really wanted to? (or even buy a company that does already?)<p>&gt; they starve all the startups of revenue by giving this away<p>Would you say startups like Deepseek have been hurt or help by their (even partial) openness?<p>In fact, how does this track with your first statement? They&#x27;re not monetizing this: so their startup competition can actually serve their models to gain revenue <i>which they then turn around use to train competitor models</i> (we&#x27;ve already seen this with Fireworks.ai)<p>You seem to underestimate how much of the value in LLMs is <i>productizing</i> them. The margins on per-token usage are insane, Meta not taking that margin is creating a huge opportunity for a wave of startups in so many directions...<p>&gt; Only OLMo is truly open source among competitive models<p>Synthetic data from competitor models was a huge part of that. It would seem no one is fighting the startups as hard as you&#x27;re claiming they are.</div><br/><div id="42557045" class="c"><input type="checkbox" id="c-42557045" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556778">parent</a><span>|</span><a href="#42556870">next</a><span>|</span><label class="collapse" for="c-42557045">[-]</label><label class="expand" for="c-42557045">[1 more]</label></div><br/><div class="children"><div class="content">All the LLM companies are going to eat those &quot;product companies&quot; lunch in a few years. Why would I use product X when it&#x27;s going to be inevitably be baked into the actual tech itself? Those product companies are just wrappers and have even less of a moat than the LLM companies. The very fact that random nobodies are doing this should signal there isn&#x27;t a lot of real value there. Yes, there is some money to be made right now but it reminds me a lot of the videogame bust and dotcom bust. A LOT of companies are wasting a crazy amount of money on &quot;solutions&quot; that will be obsolete in a few years.</div><br/></div></div></div></div><div id="42556870" class="c"><input type="checkbox" id="c-42556870" checked=""/><div class="controls bullet"><span class="by">jayd16</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556253">parent</a><span>|</span><a href="#42556778">prev</a><span>|</span><a href="#42556229">next</a><span>|</span><label class="collapse" for="c-42556870">[-]</label><label class="expand" for="c-42556870">[1 more]</label></div><br/><div class="children"><div class="content">Is there any vendor lock-in with this conspiracy?  Even if startups are pushed out of the spotlight, what stops them from competing? If the meta model is bad, won&#x27;t it be even easier to make an alternative in the future?</div><br/></div></div></div></div><div id="42556229" class="c"><input type="checkbox" id="c-42556229" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42556009">parent</a><span>|</span><a href="#42556253">prev</a><span>|</span><a href="#42556108">next</a><span>|</span><label class="collapse" for="c-42556229">[-]</label><label class="expand" for="c-42556229">[2 more]</label></div><br/><div class="children"><div class="content">don&#x27;t buy their bullshit. it&#x27;s not open source.</div><br/><div id="42557217" class="c"><input type="checkbox" id="c-42557217" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#42556009">root</a><span>|</span><a href="#42556229">parent</a><span>|</span><a href="#42556108">next</a><span>|</span><label class="collapse" for="c-42557217">[-]</label><label class="expand" for="c-42557217">[1 more]</label></div><br/><div class="children"><div class="content">Yes it’s more about open weights. I also think that you would need the training data to consider it open source.<p>Open weights is still appreciated and they probably train on data they don’t have the license to open source.</div><br/></div></div></div></div></div></div><div id="42556426" class="c"><input type="checkbox" id="c-42556426" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#42556108">prev</a><span>|</span><label class="collapse" for="c-42556426">[-]</label><label class="expand" for="c-42556426">[4 more]</label></div><br/><div class="children"><div class="content">There was no reason to call it something it&#x27;s not (&quot;chain of cont. thought&quot; ≠ coconut).</div><br/><div id="42556525" class="c"><input type="checkbox" id="c-42556525" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#42556426">parent</a><span>|</span><label class="collapse" for="c-42556525">[-]</label><label class="expand" for="c-42556525">[3 more]</label></div><br/><div class="children"><div class="content">Is your complaint here that the paper is not discussing a literal coconut?</div><br/><div id="42556711" class="c"><input type="checkbox" id="c-42556711" checked=""/><div class="controls bullet"><span class="by">ripped_britches</span><span>|</span><a href="#42556426">root</a><span>|</span><a href="#42556525">parent</a><span>|</span><label class="collapse" for="c-42556711">[-]</label><label class="expand" for="c-42556711">[2 more]</label></div><br/><div class="children"><div class="content">We desperately need more literal coconut coverage here on HN</div><br/><div id="42556782" class="c"><input type="checkbox" id="c-42556782" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42556426">root</a><span>|</span><a href="#42556711">parent</a><span>|</span><label class="collapse" for="c-42556782">[-]</label><label class="expand" for="c-42556782">[1 more]</label></div><br/><div class="children"><div class="content">Not just any regular old coconuts &quot;Coconut by Meta AI - Better LLM Reasoning with Chain of Continuous Thought?&quot; coconuts<p>(Sometimes acronyms in titles are vague&#x2F;misleading... this was not one of those times)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>