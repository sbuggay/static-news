<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735635663378" as="style"/><link rel="stylesheet" href="styles.css?v=1735635663378"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2412.18052">Beyond Gradient Averaging in Parallel Optimization</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>shinryudbz</span> | <span>23 comments</span></div><br/><div><div id="42557448" class="c"><input type="checkbox" id="c-42557448" checked=""/><div class="controls bullet"><span class="by">amarcheschi</span><span>|</span><a href="#42555306">next</a><span>|</span><label class="collapse" for="c-42557448">[-]</label><label class="expand" for="c-42557448">[1 more]</label></div><br/><div class="children"><div class="content">Not entirely related: I&#x27;ve just ended my internship at the Italian research council, where I worked on implementing federated learning algorithms on a simulator to make benchmarking easy. I couldn&#x27;t finish my job because an algorithm I was trying to port developed by IBM had hardcoded values that prevented the algorithm from working with different models than the ones already coded by them in their code. There was also a comment like &quot;this is hardcoded for now, will be changed later&quot; yeah last commit was 4 years ago<p><a href="https:&#x2F;&#x2F;github.com&#x2F;IBM&#x2F;FedMA">https:&#x2F;&#x2F;github.com&#x2F;IBM&#x2F;FedMA</a></div><br/></div></div><div id="42555306" class="c"><input type="checkbox" id="c-42555306" checked=""/><div class="controls bullet"><span class="by">deddy</span><span>|</span><a href="#42557448">prev</a><span>|</span><a href="#42555287">next</a><span>|</span><label class="collapse" for="c-42555306">[-]</label><label class="expand" for="c-42555306">[12 more]</label></div><br/><div class="children"><div class="content">Co-author here. Super excited to see this work posted on HN!<p>Happy to answer questions.</div><br/><div id="42556244" class="c"><input type="checkbox" id="c-42556244" checked=""/><div class="controls bullet"><span class="by">Majromax</span><span>|</span><a href="#42555306">parent</a><span>|</span><a href="#42556810">next</a><span>|</span><label class="collapse" for="c-42556244">[-]</label><label class="expand" for="c-42556244">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Happy to answer questions.<p>Do you expect instability between successive macrobatch gradients?  That is, why are you comparing microgradients within a single batch, adding a whole bunch of serialization headaches, rather than comparing with the macrogradient of the previous step?<p>Given your test setup of noisy labels, isn&#x27;t the sequential accumulation of microgradients dangerous?  Suppose we take this to the limit of a minibatch size of 1, and the first microgradient happens to come from an example with an incorrect label.  If I understand this correctly, gradient filtering would seek to add all the gradients that are consistent with the bad example, rejecting the gradients that belong to good examples.  The text only contemplates the perversity of rejecting all examples from a microbatch, not accidentally accepting only corrupted gradients.<p>The filtered gradients are used via SGD with momentum (although equation (6) looks like momentum-free SGD).  Have you seen &#x2F; do you expect different results when supplying the filtered gradients to Adam&#x2F;AdamW or other, more sophisticated optimizers?<p>Your thresholding is binary, either accepting or rejecting an entire microgradient.  Have you tested soft thresholding?  Is there an information-theoretic way to explain this effect?<p>In figure 7, why does GAF with a large threshold result in a lower validation accuracy than the baseline?  In GAF-terms, the baseline accepts every microgradient, so I&#x27;d expect GAF to converge to the baseline result as the threshold increases.  What does the figure-7-but-0%-error curve look like?</div><br/><div id="42556978" class="c"><input type="checkbox" id="c-42556978" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#42555306">root</a><span>|</span><a href="#42556244">parent</a><span>|</span><a href="#42556810">next</a><span>|</span><label class="collapse" for="c-42556978">[-]</label><label class="expand" for="c-42556978">[1 more]</label></div><br/><div class="children"><div class="content">Strong &quot;reviewer #2&quot; vibes in this comment...</div><br/></div></div></div></div><div id="42556810" class="c"><input type="checkbox" id="c-42556810" checked=""/><div class="controls bullet"><span class="by">szvsw</span><span>|</span><a href="#42555306">parent</a><span>|</span><a href="#42556244">prev</a><span>|</span><a href="#42556619">next</a><span>|</span><label class="collapse" for="c-42556810">[-]</label><label class="expand" for="c-42556810">[1 more]</label></div><br/><div class="children"><div class="content">Just skimmed the abstract and it immediately has me wondering if there are ways that this research intersects with explainability research - specifically, the notion that certain inputs only activate certain portions of a network. I wonder if at some point that kind of information could be leveraged to provide better sorting of datasets into batches. Obviously this conflicts a little bit with the notion that you want your batches to be just completely random permutations.<p>In some sense, if two inputs activate the same parts of the network at high intensity, then they are more likely to result in conflicting gradients if they are in separate microbatches from the same macrobatch.<p>I’m curious if you think long term, there could be some utility in trying to periodically extract information about how samples activate the network and use that (or some other representation) to better sort the micro&#x2F;macro batches to maximize your parallelism by making conflicts less likely.<p>Obviously there would be some sort of time penalty dealing with calculating whatever that representation is and sorting so that overhead might far outweigh any gains, but I could see it as at least plausible that there <i>might</i> be some contexts&#x2F;scales where making that periodic investment could pay off.</div><br/></div></div><div id="42556619" class="c"><input type="checkbox" id="c-42556619" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42555306">parent</a><span>|</span><a href="#42556810">prev</a><span>|</span><a href="#42556576">next</a><span>|</span><label class="collapse" for="c-42556619">[-]</label><label class="expand" for="c-42556619">[1 more]</label></div><br/><div class="children"><div class="content">This is probably a dumb idea, but I&#x27;ll air it anyway.<p>I just stumbled upon the model soup[1] paper where, as I understand it, they average weights of fine-tuned models and get an model that performs better. They have a more involved algorithm but even the uniform soup (simple weight average) seems to perform well.<p>In your paper you mention that especially in late stages the gradients of microbatches are often not aligned, hence the agreement filtering.<p>What you&#x27;re doing, from my brief glossing over, is effectively to do a k-means clustering with outlier rejection pass with k = 1. You then use the cluster mean to update the model.<p>What I&#x27;m curious of, assuming the above is correct, is what would happen if you combined the approaches.<p>That is, do a k-means clustering of the microbatch gradients with k &gt; 1, still rejecting outliers but perhaps with lower threshold, generate k updated models using the k cluster means, and then average the k models afterwards.<p>I&#x27;ve used something similar to k-means clustering with outlier rejection for noise filtering and it was quite effective, so curious how it would work out here.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.05482" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.05482</a></div><br/></div></div><div id="42555381" class="c"><input type="checkbox" id="c-42555381" checked=""/><div class="controls bullet"><span class="by">timmg</span><span>|</span><a href="#42555306">parent</a><span>|</span><a href="#42556576">prev</a><span>|</span><a href="#42556230">next</a><span>|</span><label class="collapse" for="c-42555381">[-]</label><label class="expand" for="c-42555381">[2 more]</label></div><br/><div class="children"><div class="content">From a non-expert here (maybe a dumb question): does “batching” affect quality in a similar way as averaging parallel batches?  (Like is there a difference between having a batch that is 10x the size versus averaging 10 batches that were calculated in parallel?)</div><br/><div id="42556245" class="c"><input type="checkbox" id="c-42556245" checked=""/><div class="controls bullet"><span class="by">deddy</span><span>|</span><a href="#42555306">root</a><span>|</span><a href="#42555381">parent</a><span>|</span><a href="#42556230">next</a><span>|</span><label class="collapse" for="c-42556245">[-]</label><label class="expand" for="c-42556245">[1 more]</label></div><br/><div class="children"><div class="content">Great question! There are a few different aspects to this.<p>With gradient agreement filtering having a greater number of batches (generally) increases the likelihood of finding another microbatch that agrees with the gradient simply by virtue of having more gradient &quot;samples&quot; to compare. So having more batches increases the chance of success there. The algorithm as laid out in the paper is a simple approach to combining groups of batches where larger numbers batches doesn&#x27;t necessarily improve you chances of success if the batch you&#x27;re comparing against is an outlier itself. There are almost certainly better ways of combining greater numbers of batches to get a successful update. This is one of the exciting areas of future work.<p>Increasing the batch size generally can be though of as &quot;averaging&quot; out the noise in your samples to find a consistent update. This has an interesting affect though where that as you increase the batch size, when using gradient agreement filtering you want to lower the filter threshold as your batch size increases to become &quot;stricter&quot; in terms of the level of agreement to look for to accept a gradient. This is Figure 9 of the paper. This is also consistent with what other researchers have found that simply increasing batch size isn&#x27;t always better. There is a trade here with diminishing returns of increasing batch size as well (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1812.06162" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1812.06162</a>). One interesting finding from the work was that smaller batch sizes actually improved training accuracy for CIFAR-100N, very roughly speaking this can be explained by having more &quot;signal&quot; in each batch with smaller batch sizes, at the cost of potentially throwing out batches&#x2F;gradients if they disagree.</div><br/></div></div></div></div><div id="42556230" class="c"><input type="checkbox" id="c-42556230" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#42555306">parent</a><span>|</span><a href="#42555381">prev</a><span>|</span><a href="#42555629">next</a><span>|</span><label class="collapse" for="c-42556230">[-]</label><label class="expand" for="c-42556230">[2 more]</label></div><br/><div class="children"><div class="content">Couple of questions. First, would you happen to have a code demo?<p>Second, and this is more of a hypothetical question for my own understanding rather than a practical one - in a single GPU scenario, could you take compute the loss per-sample without averaging (i.e. &quot;reduce=None&quot; in pytorch), and improve (on a sample efficiency basis) single GPU training with your algorithm? Sorry if this was covered in the paper already.</div><br/><div id="42556330" class="c"><input type="checkbox" id="c-42556330" checked=""/><div class="controls bullet"><span class="by">Maxious</span><span>|</span><a href="#42555306">root</a><span>|</span><a href="#42556230">parent</a><span>|</span><a href="#42555629">next</a><span>|</span><label class="collapse" for="c-42556330">[-]</label><label class="expand" for="c-42556330">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;Fchaubard&#x2F;gradient_agreement_filtering">https:&#x2F;&#x2F;github.com&#x2F;Fchaubard&#x2F;gradient_agreement_filtering</a> linked in the paper</div><br/></div></div></div></div><div id="42555629" class="c"><input type="checkbox" id="c-42555629" checked=""/><div class="controls bullet"><span class="by">olaulaja</span><span>|</span><a href="#42555306">parent</a><span>|</span><a href="#42556230">prev</a><span>|</span><a href="#42555388">next</a><span>|</span><label class="collapse" for="c-42555629">[-]</label><label class="expand" for="c-42555629">[1 more]</label></div><br/><div class="children"><div class="content">Did you explore microbatch sizes below 100? Curious about how far this can be pushed and what happens when approaching the limit microbatch size of 1.</div><br/></div></div><div id="42555388" class="c"><input type="checkbox" id="c-42555388" checked=""/><div class="controls bullet"><span class="by">fchaubard</span><span>|</span><a href="#42555306">parent</a><span>|</span><a href="#42555629">prev</a><span>|</span><a href="#42555287">next</a><span>|</span><label class="collapse" for="c-42555388">[-]</label><label class="expand" for="c-42555388">[1 more]</label></div><br/><div class="children"><div class="content">Here too!</div><br/></div></div></div></div><div id="42555287" class="c"><input type="checkbox" id="c-42555287" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42555306">prev</a><span>|</span><a href="#42555285">next</a><span>|</span><label class="collapse" for="c-42555287">[-]</label><label class="expand" for="c-42555287">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We show this technique consistently outperforms validation accuracy, in some cases by up to 18.2\% compared to traditional training approaches while reducing the computation required nearly an order of magnitude because we can now rely on smaller microbatch sizes without destabilizing training.<p>The accuracy improvement is great, but I&#x27;m really looking forward to the reduction in computation required!</div><br/><div id="42555390" class="c"><input type="checkbox" id="c-42555390" checked=""/><div class="controls bullet"><span class="by">fchaubard</span><span>|</span><a href="#42555287">parent</a><span>|</span><a href="#42555285">next</a><span>|</span><label class="collapse" for="c-42555390">[-]</label><label class="expand" for="c-42555390">[1 more]</label></div><br/><div class="children"><div class="content">Yes it will allow stable training at much smaller batch sizes. Test it out and let us know if it works for your use case!</div><br/></div></div></div></div><div id="42555285" class="c"><input type="checkbox" id="c-42555285" checked=""/><div class="controls bullet"><span class="by">timmg</span><span>|</span><a href="#42555287">prev</a><span>|</span><a href="#42554989">next</a><span>|</span><label class="collapse" for="c-42555285">[-]</label><label class="expand" for="c-42555285">[5 more]</label></div><br/><div class="children"><div class="content">As someone who only has a passing understanding of parallel training, I always found it wonderful that averaging gradients works at all. It seems non-intuitive to me.<p>Pretty cool, imho, that there are finding better ways to train in parallel.</div><br/><div id="42555421" class="c"><input type="checkbox" id="c-42555421" checked=""/><div class="controls bullet"><span class="by">d3m0t3p</span><span>|</span><a href="#42555285">parent</a><span>|</span><a href="#42554989">next</a><span>|</span><label class="collapse" for="c-42555421">[-]</label><label class="expand" for="c-42555421">[4 more]</label></div><br/><div class="children"><div class="content">Well, it&#x27;s just like stochastic gradient descent, if you think about it. The normal gradient descent is computed using the whole training set. The stochastic gradient is trained on a batch (a subset of the training set), and in the distributed case, we compute two batches at once by doing the gradient on each in parallel. 
The intuition works IMO, but indeed, having the first batch update and then the second, is not equal to having the mean update.<p>This is indeed super cool !</div><br/><div id="42555666" class="c"><input type="checkbox" id="c-42555666" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42555285">root</a><span>|</span><a href="#42555421">parent</a><span>|</span><a href="#42554989">next</a><span>|</span><label class="collapse" for="c-42555666">[-]</label><label class="expand" for="c-42555666">[3 more]</label></div><br/><div class="children"><div class="content">Does anyone actually use the &#x27;normal gradient descent&#x27; with the whole training set?  I only ever see it as a sort of straw man to make explanation easier.</div><br/><div id="42556002" class="c"><input type="checkbox" id="c-42556002" checked=""/><div class="controls bullet"><span class="by">jey</span><span>|</span><a href="#42555285">root</a><span>|</span><a href="#42555666">parent</a><span>|</span><a href="#42556003">next</a><span>|</span><label class="collapse" for="c-42556002">[-]</label><label class="expand" for="c-42556002">[1 more]</label></div><br/><div class="children"><div class="content">Generally yes, vanilla gradient descent gets plenty of use. But for LLMs: no, it’s not really used, and stochastic gradient descent provides a form of regularization, so it probably works better in addition to being more practical.</div><br/></div></div><div id="42556003" class="c"><input type="checkbox" id="c-42556003" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#42555285">root</a><span>|</span><a href="#42555666">parent</a><span>|</span><a href="#42556002">prev</a><span>|</span><a href="#42554989">next</a><span>|</span><label class="collapse" for="c-42556003">[-]</label><label class="expand" for="c-42556003">[1 more]</label></div><br/><div class="children"><div class="content">Full batch with L-BFGS, when possible, is wildly underappreciated.</div><br/></div></div></div></div></div></div></div></div><div id="42554989" class="c"><input type="checkbox" id="c-42554989" checked=""/><div class="controls bullet"><span class="by">cgdl</span><span>|</span><a href="#42555285">prev</a><span>|</span><a href="#42554552">next</a><span>|</span><label class="collapse" for="c-42554989">[-]</label><label class="expand" for="c-42554989">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting. A related paper from a couple of years ago proposed a similar idea to understand generalization in deep learning:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.10036" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.10036</a></div><br/></div></div><div id="42554552" class="c"><input type="checkbox" id="c-42554552" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#42554989">prev</a><span>|</span><label class="collapse" for="c-42554552">[-]</label><label class="expand" for="c-42554552">[1 more]</label></div><br/><div class="children"><div class="content">Is this the paper that the cohost of &#x27;Last Week in AI&#x27; said was the paper of the quarter to read back to front?</div><br/></div></div></div></div></div></div></div></body></html>