<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684054842543" as="style"/><link rel="stylesheet" href="styles.css?v=1684054842543"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.nature.com/articles/s41587-022-01618-2">Large language models generate functional protein sequences across families</a> <span class="domain">(<a href="https://www.nature.com">www.nature.com</a>)</span></div><div class="subtext"><span>samwillis</span> | <span>27 comments</span></div><br/><div><div id="35934679" class="c"><input type="checkbox" id="c-35934679" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#35932219">next</a><span>|</span><label class="collapse" for="c-35934679">[-]</label><label class="expand" for="c-35934679">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a LOT of interesting stuff happening with large language models in biology:<p>Transformer trained on human genomes, learns to identify different genomic elements like enhancers without knowing what enhancers are <a href="https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2023.01.11.523679v2" rel="nofollow">https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2023.01.11.523679v2</a><p>Similarly, a transformer trained on only plant genomes &#x27;knows&#x27; how the strength of genomic variants&#x27; impact <a href="https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2022.08.22.504706v2" rel="nofollow">https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2022.08.22.504706v2</a><p>Lots of experiments happening around GPT-4 too;<p>Using regular GPT-4 to curate cell type annotations <a href="https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2023.04.16.537094v1" rel="nofollow">https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2023.04.16.537094v1</a><p>Testing GPT-4 across a variety of &#x27;standard&#x27; tasks like linking gene IDs to protein IDs
<a href="https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2023.03.11.532238v1" rel="nofollow">https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2023.03.11.532238v1</a></div><br/><div id="35935694" class="c"><input type="checkbox" id="c-35935694" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#35934679">parent</a><span>|</span><a href="#35932219">next</a><span>|</span><label class="collapse" for="c-35935694">[-]</label><label class="expand" for="c-35935694">[2 more]</label></div><br/><div class="children"><div class="content">It would be super interesting if we figure out that everything in life including language are  fractals of molecular biology</div><br/><div id="35936062" class="c"><input type="checkbox" id="c-35936062" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#35934679">root</a><span>|</span><a href="#35935694">parent</a><span>|</span><a href="#35932219">next</a><span>|</span><label class="collapse" for="c-35936062">[-]</label><label class="expand" for="c-35936062">[1 more]</label></div><br/><div class="children"><div class="content">This is like saying it would be interesting if we figure out that everything in the universe including physics, biology and consciousness is fractals of OEIS</div><br/></div></div></div></div></div></div><div id="35932219" class="c"><input type="checkbox" id="c-35932219" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#35934679">prev</a><span>|</span><a href="#35932529">next</a><span>|</span><label class="collapse" for="c-35932219">[-]</label><label class="expand" for="c-35932219">[9 more]</label></div><br/><div class="children"><div class="content">Some notable technical information: It is a 1.2bln param model, trained on raw sequences only, can generate full length functional proteins of about 150-200 residues (approx lysozyme size). The generated proteins are very different to native ones (30-40% similarity).<p>The interesting thing about this model is that it also exhibit emergent capabilities. It was trained only on raw sequences but somehow managed to capture information about functionality and solubility of the folded proteins, and then implemented that in the generated sequences.<p>Amino acid sequences are just a bunch of jumbled words if you compared them to English. It usually has to go through folding to form proper &quot;sentences&quot; with meanings. I guess you can compare this to &quot;grammar&quot;. This is probably the model managed to learn protein grammar purely by brute force. Now if only we can get a model in the range of 100bln parameters...</div><br/><div id="35934621" class="c"><input type="checkbox" id="c-35934621" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#35932219">parent</a><span>|</span><a href="#35934184">next</a><span>|</span><label class="collapse" for="c-35934621">[-]</label><label class="expand" for="c-35934621">[3 more]</label></div><br/><div class="children"><div class="content">&gt; purely by brute force<p>the new underlying model of the world. First it was: Who needs to understand an algorithm when you can just simulate everything? Now it&#x27;s: Who needs to simulate everything when you can just let a large enough black box approximate a solution?</div><br/><div id="35935684" class="c"><input type="checkbox" id="c-35935684" checked=""/><div class="controls bullet"><span class="by">teekert</span><span>|</span><a href="#35932219">root</a><span>|</span><a href="#35934621">parent</a><span>|</span><a href="#35934184">next</a><span>|</span><label class="collapse" for="c-35935684">[-]</label><label class="expand" for="c-35935684">[2 more]</label></div><br/><div class="children"><div class="content">It’s only a black box because our brains aren’t good enough.</div><br/><div id="35936167" class="c"><input type="checkbox" id="c-35936167" checked=""/><div class="controls bullet"><span class="by">detrites</span><span>|</span><a href="#35932219">root</a><span>|</span><a href="#35935684">parent</a><span>|</span><a href="#35934184">next</a><span>|</span><label class="collapse" for="c-35936167">[-]</label><label class="expand" for="c-35936167">[1 more]</label></div><br/><div class="children"><div class="content">But if our brains came up with the black box, can that really be true?</div><br/></div></div></div></div></div></div><div id="35934184" class="c"><input type="checkbox" id="c-35934184" checked=""/><div class="controls bullet"><span class="by">_hl_</span><span>|</span><a href="#35932219">parent</a><span>|</span><a href="#35934621">prev</a><span>|</span><a href="#35932529">next</a><span>|</span><label class="collapse" for="c-35934184">[-]</label><label class="expand" for="c-35934184">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Now if only we can get a model in the range of 100bln parameters<p>Do we have enough data to train such a large model in a meaningful way?</div><br/><div id="35934817" class="c"><input type="checkbox" id="c-35934817" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#35932219">root</a><span>|</span><a href="#35934184">parent</a><span>|</span><a href="#35932529">next</a><span>|</span><label class="collapse" for="c-35934817">[-]</label><label class="expand" for="c-35934817">[4 more]</label></div><br/><div class="children"><div class="content">we&#x27;ve gotten pretty good at predicting proteins from genome assemblies, but we don&#x27;t have the manpower to manually curate that data.<p>Swissprot, the project that manually curates and scores protein sequences, has about half a million &#x27;supported&#x27; protein models in its last release: <a href="https:&#x2F;&#x2F;web.expasy.org&#x2F;docs&#x2F;relnotes&#x2F;relstat.html" rel="nofollow">https:&#x2F;&#x2F;web.expasy.org&#x2F;docs&#x2F;relnotes&#x2F;relstat.html</a><p>TrEMBL, the database that has curated + uncurated, has 249,308,459 proteins in its last release: <a href="https:&#x2F;&#x2F;www.ebi.ac.uk&#x2F;uniprot&#x2F;TrEMBLstats" rel="nofollow">https:&#x2F;&#x2F;www.ebi.ac.uk&#x2F;uniprot&#x2F;TrEMBLstats</a><p>Generating uncurated protein sequences has become fairly easy - I can generate a draft genome assembly in about a day, predicting proteins using MAKER or BRAKEr or other HMM-based pipelines takes another day or so. Most eukaryotes have about 20,000 to 30,000 genes that translate into proteins.
I&#x27;m involved in a project that assembled 300 draft genomes in the last 6 months-  now we have between 6 to 9 million proteins (most of them not &#x27;novel&#x27; in function - novel genes&#x2F;proteins are rare, and a HMM trained on known sequences cannot easily find novel genes).</div><br/><div id="35934968" class="c"><input type="checkbox" id="c-35934968" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#35932219">root</a><span>|</span><a href="#35934817">parent</a><span>|</span><a href="#35932529">next</a><span>|</span><label class="collapse" for="c-35934968">[-]</label><label class="expand" for="c-35934968">[3 more]</label></div><br/><div class="children"><div class="content">Addendum:<p>To train an LLM you want &#x27;diverse&#x27; training data, right? Not just Reddit posts but also technical manuals.<p>In protein space I&#x27;m confident we&#x27;ve sequenced the majority of what is out there. For the majority we don&#x27;t know what they do, but we know that they are there (being expressed in the host). 
In every genome sequencing project we can find a similar looking gene for &gt;95% of genes, with much of the remaining 5% being noise or contaminants.<p>To train LLMs we can&#x27;t really generate more &#x27;new&#x27; data in this space; this is it! Even &#x27;my&#x27; 6 to 9 million projects won&#x27;t add much new information, at most the LLMs will know the &#x27;space&#x27; of possible changes for those existing proteins.</div><br/><div id="35936373" class="c"><input type="checkbox" id="c-35936373" checked=""/><div class="controls bullet"><span class="by">inciampati</span><span>|</span><a href="#35932219">root</a><span>|</span><a href="#35934968">parent</a><span>|</span><a href="#35935042">next</a><span>|</span><label class="collapse" for="c-35936373">[-]</label><label class="expand" for="c-35936373">[1 more]</label></div><br/><div class="children"><div class="content">We have only scratched the surface of what&#x27;s out there.<p>Probably it&#x27;s unlikely that we will find totally novel proteins. But it&#x27;s a mistake to forget about prokaryotes and phages. Everywhere we look there is something new. The oceans are vast and full of life. And we have barely begun to dig into the life deeper in the crust.<p>Seeing how all of this varies will only make models stronger. It&#x27;s an enormous collection relative to human language.</div><br/></div></div><div id="35935042" class="c"><input type="checkbox" id="c-35935042" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#35932219">root</a><span>|</span><a href="#35934968">parent</a><span>|</span><a href="#35936373">prev</a><span>|</span><a href="#35932529">next</a><span>|</span><label class="collapse" for="c-35935042">[-]</label><label class="expand" for="c-35935042">[1 more]</label></div><br/><div class="children"><div class="content">I think English is also just the rehash of the same 2000-3000 words. The ability to meaningfully arrange those words in new ways and in longer sentences are what make the current LLMs so powerful. In that way, these protein models can still learn more from the new data but they will definitely need some sort of reinforcement learning and grounding. That is where wetlab scientists will have to do. Verifying the generated protein is functional and stable, i.e. &quot;making sense&quot;, is where we will need more data.<p>Only a miniscule amount of protein has verified structure and function. While an HMM can predict their functions pretty well based on homology, it can often be wrong. Having that kind of noise in the training data can degrade the performance of these protein models. Perhaps we need better curated dataset before we can scale these up.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35932529" class="c"><input type="checkbox" id="c-35932529" checked=""/><div class="controls bullet"><span class="by">lysozyme</span><span>|</span><a href="#35932219">prev</a><span>|</span><a href="#35935929">next</a><span>|</span><label class="collapse" for="c-35932529">[-]</label><label class="expand" for="c-35932529">[2 more]</label></div><br/><div class="children"><div class="content">When evaluating this work, it’s important to remember that the functional labels and protein family assignments on each of the 280 million input sequences were originally assigned by an HMM model using human curated sequence groups as part of the pfam project, so the model is predicting a prediction (or perhaps conditioned on a prediction would be more accurate).<p>Furthermore, the authors must engage a lot of human curation to ensure the sequences they generate are active. First, they pick an easy target. Second, they employ by-hand classical bioinformatics techniques on their predicted sequences after they are generated. For example, they manually align them and select those which contain specific important amino acids at specific positions which are present in 100% of functional proteins of that class, and are required for function. This is all done by a human bioinformatics expert (or automated) before they test the generated sequences. This is the protein equivalent of cherry-picking great examples of, for example, ChatGPT responses and presenting them as if the model only made predictions like that.<p>One other comment, in protein science, a sequence with 40% identity to another sequence is not “very different” if it is homologous. Since this model is essentially generating homologs from a particular class, it’s no surprise at a pairwise amino acid level, the generated sequences have this degree of similarity. Take proteins in any functional family and compare them. They will have the same overall 3-D structure—called their “fold”—yet have pairwise sequence identities much lower than 30–40%. This “degeneracy”, the notion that there are many diverse sequences that all fold into the same shape, is both a fundamental empirical observation in protein science as well as a grounded physical theory.<p>Not to be negative. I really enjoyed reading this paper and I think the work is important. Some related work by Meta AI is the ESM series of models [1] trained on the same data (the UniProt dataset [2]).<p>One thing I wonder is about the vocabulary size of this model. The number of tokens is 26 for the 20 amino acids and some extras, whereas for a LLM like Meta’s LLaMa the vocab size is 32,000. I wonder how that changes training and inference, and how we can adopt the transformer architecture for this scenario.<p>1. <a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;esm">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;esm</a><p>2. <a href="https:&#x2F;&#x2F;www.uniprot.org&#x2F;help&#x2F;downloads" rel="nofollow">https:&#x2F;&#x2F;www.uniprot.org&#x2F;help&#x2F;downloads</a></div><br/><div id="35932905" class="c"><input type="checkbox" id="c-35932905" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#35932529">parent</a><span>|</span><a href="#35935929">next</a><span>|</span><label class="collapse" for="c-35932905">[-]</label><label class="expand" for="c-35932905">[1 more]</label></div><br/><div class="children"><div class="content">I consider all the manual curation effectively a form of RLHF that can be imposed automatically later on. We saw how much this can improve a raw LLM by looking at the output of ChatGPT. Otherwise, the criticism of LLMs being just glorified autocomplete machines isn&#x27;t that far from reality. In other words, it is just an expected requirement for LLMs to be effective.<p>You are probably right that lysozyme is an easy target and may have large sequence variety between homologs so saying &quot;very different&quot; for 30-40% is not correct. But that is only in the context of biology and protein structure and function. This is an LLM trained on primary sequences only. It doesn&#x27;t know anything about the folds or domains or functional sites (unless I am wrong and those are part of the metadata fed to it during training). Yet it did learn enough to generalize to the point that even with only 30-40% identity, it still produces soluble proteins with the same function. I am sure you know that at 40% differences, one protein can be in an entirely different superfamily from another. So it is still an impressively low identity score.<p>Also, I think it is more appropriate to compare the amino acids to things like the alphabets than vocabs. Domains would probably be an equivalent to LLaMa vocab.</div><br/></div></div></div></div><div id="35935929" class="c"><input type="checkbox" id="c-35935929" checked=""/><div class="controls bullet"><span class="by">intalentive</span><span>|</span><a href="#35932529">prev</a><span>|</span><a href="#35931952">next</a><span>|</span><label class="collapse" for="c-35935929">[-]</label><label class="expand" for="c-35935929">[1 more]</label></div><br/><div class="children"><div class="content">That “large language models” are unfortunately named becomes clearer when they’re applied to non-language domains. They’re auto-associative models that learn mappings between discrete elements in a sequence or set.</div><br/></div></div><div id="35931952" class="c"><input type="checkbox" id="c-35931952" checked=""/><div class="controls bullet"><span class="by">hecanjog</span><span>|</span><a href="#35935929">prev</a><span>|</span><a href="#35932872">next</a><span>|</span><label class="collapse" for="c-35931952">[-]</label><label class="expand" for="c-35931952">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised that Salesforce has a research division, and they&#x27;re working on something like this.</div><br/><div id="35932718" class="c"><input type="checkbox" id="c-35932718" checked=""/><div class="controls bullet"><span class="by">ipnon</span><span>|</span><a href="#35931952">parent</a><span>|</span><a href="#35932341">next</a><span>|</span><label class="collapse" for="c-35932718">[-]</label><label class="expand" for="c-35932718">[1 more]</label></div><br/><div class="children"><div class="content">The game theory of useless corporate research departments is that by spinning plates for you they’re not building moats for your competitors. There is quite a lot of money to be saved by essentially nerd sniping with large wads of cash.</div><br/></div></div><div id="35932341" class="c"><input type="checkbox" id="c-35932341" checked=""/><div class="controls bullet"><span class="by">sb8244</span><span>|</span><a href="#35931952">parent</a><span>|</span><a href="#35932718">prev</a><span>|</span><a href="#35932872">next</a><span>|</span><label class="collapse" for="c-35932341">[-]</label><label class="expand" for="c-35932341">[1 more]</label></div><br/><div class="children"><div class="content">I know that Salesforce is committed to the 1% initiative, so maybe this falls into that. 1% can do a lot at their revenue.</div><br/></div></div></div></div><div id="35932872" class="c"><input type="checkbox" id="c-35932872" checked=""/><div class="controls bullet"><span class="by">ajuc</span><span>|</span><a href="#35931952">prev</a><span>|</span><a href="#35929393">next</a><span>|</span><label class="collapse" for="c-35932872">[-]</label><label class="expand" for="c-35932872">[3 more]</label></div><br/><div class="children"><div class="content">AI generating a virus&#x2F;prion&#x2F;whatever that we synthesize without understanding what it does is the easiest way to the bad singularity people were warning us about.</div><br/><div id="35934199" class="c"><input type="checkbox" id="c-35934199" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#35932872">parent</a><span>|</span><a href="#35929393">next</a><span>|</span><label class="collapse" for="c-35934199">[-]</label><label class="expand" for="c-35934199">[2 more]</label></div><br/><div class="children"><div class="content">The bad singularity is for those who can&#x27;t afford the coming life subscription plan.</div><br/><div id="35935798" class="c"><input type="checkbox" id="c-35935798" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#35932872">root</a><span>|</span><a href="#35934199">parent</a><span>|</span><a href="#35929393">next</a><span>|</span><label class="collapse" for="c-35935798">[-]</label><label class="expand" for="c-35935798">[1 more]</label></div><br/><div class="children"><div class="content">Context:<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;IFe9wiDfb0E" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;IFe9wiDfb0E</a></div><br/></div></div></div></div></div></div><div id="35929393" class="c"><input type="checkbox" id="c-35929393" checked=""/><div class="controls bullet"><span class="by">samwillis</span><span>|</span><a href="#35932872">prev</a><span>|</span><a href="#35932186">next</a><span>|</span><label class="collapse" for="c-35929393">[-]</label><label class="expand" for="c-35929393">[1 more]</label></div><br/><div class="children"><div class="content">PDF: <a href="http:&#x2F;&#x2F;cdn.fraserlab.com&#x2F;publications&#x2F;2023_madani.pdf" rel="nofollow">http:&#x2F;&#x2F;cdn.fraserlab.com&#x2F;publications&#x2F;2023_madani.pdf</a><p>Code: <a href="https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;progen">https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;progen</a></div><br/></div></div><div id="35932186" class="c"><input type="checkbox" id="c-35932186" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#35929393">prev</a><span>|</span><a href="#35934187">next</a><span>|</span><label class="collapse" for="c-35932186">[-]</label><label class="expand" for="c-35932186">[3 more]</label></div><br/><div class="children"><div class="content">2022, btw</div><br/><div id="35932399" class="c"><input type="checkbox" id="c-35932399" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#35932186">parent</a><span>|</span><a href="#35934187">next</a><span>|</span><label class="collapse" for="c-35932399">[-]</label><label class="expand" for="c-35932399">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Published: 26 January 2023<p>Well, technically it is out in 2023 but sure, you can argue it was probably in the prepub state since 2022...</div><br/><div id="35934062" class="c"><input type="checkbox" id="c-35934062" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#35932186">root</a><span>|</span><a href="#35932399">parent</a><span>|</span><a href="#35934187">next</a><span>|</span><label class="collapse" for="c-35934062">[-]</label><label class="expand" for="c-35934062">[1 more]</label></div><br/><div class="children"><div class="content">I was supposed to be reply to another comment. The GitHub is from 2022:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;progen">https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;progen</a></div><br/></div></div></div></div></div></div><div id="35934187" class="c"><input type="checkbox" id="c-35934187" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#35932186">prev</a><span>|</span><label class="collapse" for="c-35934187">[-]</label><label class="expand" for="c-35934187">[1 more]</label></div><br/><div class="children"><div class="content">Ooh, can&#x27;t wait for generative cancer. Thanks AI.</div><br/></div></div></div></div></div></div></div></body></html>