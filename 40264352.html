<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714986073264" as="style"/><link rel="stylesheet" href="styles.css?v=1714986073264"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ai.stanford.edu/~kzliu/blog/unlearning">Machine Unlearning in 2024</a> <span class="domain">(<a href="https://ai.stanford.edu">ai.stanford.edu</a>)</span></div><div class="subtext"><span>ignoramous</span> | <span>82 comments</span></div><br/><div><div id="40264969" class="c"><input type="checkbox" id="c-40264969" checked=""/><div class="controls bullet"><span class="by">motohagiography</span><span>|</span><a href="#40265846">next</a><span>|</span><label class="collapse" for="c-40264969">[-]</label><label class="expand" for="c-40264969">[2 more]</label></div><br/><div class="children"><div class="content">seems like there is a basic problem where if you specify something to be unlearned, it could still be re-learned by inference and prompting. the solution may not be in filtering the proscribed facts or data itself, but in the weights and incentives that form a final layer of reasoning. Look at &quot;safe&quot; models now like google&#x27;s last launch, where the results were often unsatisfying, as clearly we don&#x27;t want truthful models yet, but we want ones that enable our ability to develop them further, which for now means not selecting out by antagonizing other social stakeholders.<p>maybe we can encode and weight some principle of the models having been created by something external, with some loosely defined examples they can refer to as a way to evaluate what they return, then ones that don&#x27;t yield those results cease to be used, where the ones that find a way to align will get reused to train others. there will absolutely be bad ones, but in aggregate they should produce something more desirable, and if they really go off the rails, just send a meteor. the argument in how models can &quot;unlearn&quot; will be between those who favour incentives and those who favour rules- likely, incentives for ones I create, but rules for everyone elses&#x27;.</div><br/><div id="40269797" class="c"><input type="checkbox" id="c-40269797" checked=""/><div class="controls bullet"><span class="by">musicale</span><span>|</span><a href="#40264969">parent</a><span>|</span><a href="#40265846">next</a><span>|</span><label class="collapse" for="c-40269797">[-]</label><label class="expand" for="c-40269797">[1 more]</label></div><br/><div class="children"><div class="content">It is unsurprising that a system trained on human-generated content might end up encoding implicit bias, toxicity, and negative goals. And the more powerful and general-purpose a system is, the more suitable it is for a wide range of powerfully negative purposes.<p>Neither specializing the model nor filtering its output seems to have worked reliably in practice.</div><br/></div></div></div></div><div id="40265846" class="c"><input type="checkbox" id="c-40265846" checked=""/><div class="controls bullet"><span class="by">avi_vallarapu</span><span>|</span><a href="#40264969">prev</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40265846">[-]</label><label class="expand" for="c-40265846">[19 more]</label></div><br/><div class="children"><div class="content">We need to consider the practicality of unlearning methods in real-world applications and the legal acceptance of the same.<p>Given current technology and what advancements are needed to make Unlearning more possible, probably there should be a time-to-unlearn kind of an acceptable agreement that allows organizations to retrain or tune the response that does not involve any response from the to-be-unlearned copyright content.<p>Ultimately, legal acceptance for unlearning may be all about deleting the data set that is part of any kind of violations from the training data set. It may be very challenging to otherwise prove legally through the proposed unlearning techniques, that the model does not produce any type of response involving the private data.<p>The actual data set contains the private data violating privacy or copyright, and the model is trained on it, period. This means, it must involve retraining by deleting the documents&#x2F;data to be unlearned.</div><br/><div id="40265937" class="c"><input type="checkbox" id="c-40265937" checked=""/><div class="controls bullet"><span class="by">isodev</span><span>|</span><a href="#40265846">parent</a><span>|</span><a href="#40266004">next</a><span>|</span><label class="collapse" for="c-40265937">[-]</label><label class="expand" for="c-40265937">[9 more]</label></div><br/><div class="children"><div class="content">&gt;  a time-to-unlearn kind of an acceptable agreement<p>Why put the burden to end users? I think the technology should allow for unlearning and even &quot;never learn about me in any future models and derivative models&quot;.</div><br/><div id="40266222" class="c"><input type="checkbox" id="c-40266222" checked=""/><div class="controls bullet"><span class="by">avi_vallarapu</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40265937">parent</a><span>|</span><a href="#40266984">next</a><span>|</span><label class="collapse" for="c-40266222">[-]</label><label class="expand" for="c-40266222">[5 more]</label></div><br/><div class="children"><div class="content">No technology can guarantee 100% unlearning, and the only 100% guarantee is when the data is deleted before the model is retrained.
Legally, even 99.99% accuracy may not be acceptable, but, only 100%.</div><br/><div id="40271163" class="c"><input type="checkbox" id="c-40271163" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40266222">parent</a><span>|</span><a href="#40270448">next</a><span>|</span><label class="collapse" for="c-40271163">[-]</label><label class="expand" for="c-40271163">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the only 100% guarantee is when the data is deleted before the model is retrained<p>That’s not even a guarantee.  A model can hallucinate information about anyone, and by sheer luck some of those hallucinations will be correct.  And as a consequence of forging (see section 2.2.1) you’d never be able to prove whether the data was in the training set or not.</div><br/></div></div><div id="40270448" class="c"><input type="checkbox" id="c-40270448" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40266222">parent</a><span>|</span><a href="#40271163">prev</a><span>|</span><a href="#40266984">next</a><span>|</span><label class="collapse" for="c-40270448">[-]</label><label class="expand" for="c-40270448">[3 more]</label></div><br/><div class="children"><div class="content">Or rather some legal fiction that you can pretend is 100%.  You can never achieve real 100% in practice after all.  Eg the random initialisation of weights might already encode all the &#x27;bad&#x27; stuff you don&#x27;t want.  Extremely unlikely, but not strictly 0% unlikely.<p>The law cuts off at some point, and declares it 100%.</div><br/><div id="40272077" class="c"><input type="checkbox" id="c-40272077" checked=""/><div class="controls bullet"><span class="by">isodev</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40270448">parent</a><span>|</span><a href="#40266984">next</a><span>|</span><label class="collapse" for="c-40272077">[-]</label><label class="expand" for="c-40272077">[2 more]</label></div><br/><div class="children"><div class="content">All this is technically correct, but it also means this technology is absolutely not ready to be used for anything remotely involving humans or end user data.</div><br/><div id="40272217" class="c"><input type="checkbox" id="c-40272217" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40272077">parent</a><span>|</span><a href="#40266984">next</a><span>|</span><label class="collapse" for="c-40272217">[-]</label><label class="expand" for="c-40272217">[1 more]</label></div><br/><div class="children"><div class="content">Why?  We use random data in lots of applications, and there&#x27;s always the theoretical probability that it could &#x27;spell something naughty&#x27;.</div><br/></div></div></div></div></div></div></div></div><div id="40266984" class="c"><input type="checkbox" id="c-40266984" checked=""/><div class="controls bullet"><span class="by">Vampiero</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40265937">parent</a><span>|</span><a href="#40266222">prev</a><span>|</span><a href="#40266004">next</a><span>|</span><label class="collapse" for="c-40266984">[-]</label><label class="expand" for="c-40266984">[3 more]</label></div><br/><div class="children"><div class="content">The technology is on par with a Markov chain that&#x27;s grown a little too much. It has no notion of &quot;you&quot;, not in the conventional sense at least. Putting the infrastructure in place to allow people (and things) to be blacklisted from training is all you can really do, and even then it&#x27;s a massive effort. The current models are not trained in such a way that you can do this without starting over from scratch.</div><br/><div id="40268560" class="c"><input type="checkbox" id="c-40268560" checked=""/><div class="controls bullet"><span class="by">Retric</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40266984">parent</a><span>|</span><a href="#40269022">next</a><span>|</span><label class="collapse" for="c-40268560">[-]</label><label class="expand" for="c-40268560">[1 more]</label></div><br/><div class="children"><div class="content">That’s hardly accurate.  Deep learning among other things is another type of lossy compression algorithm.<p>It doesn’t have a 1:1 mapping of each bit of information it’s been trained with, but you can very much extract a subset of that data.  Which is why it’s easy to get DallE to recreate the Mona Lisa, variations on that image show up repeatedly in its training courpus.</div><br/></div></div><div id="40269022" class="c"><input type="checkbox" id="c-40269022" checked=""/><div class="controls bullet"><span class="by">xg15</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40266984">parent</a><span>|</span><a href="#40268560">prev</a><span>|</span><a href="#40266004">next</a><span>|</span><label class="collapse" for="c-40269022">[-]</label><label class="expand" for="c-40269022">[1 more]</label></div><br/><div class="children"><div class="content">Well then, maybe we shouldn&#x27;t use the technology.</div><br/></div></div></div></div></div></div><div id="40266004" class="c"><input type="checkbox" id="c-40266004" checked=""/><div class="controls bullet"><span class="by">beeboobaa3</span><span>|</span><a href="#40265846">parent</a><span>|</span><a href="#40265937">prev</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40266004">[-]</label><label class="expand" for="c-40266004">[9 more]</label></div><br/><div class="children"><div class="content">How to deal with &quot;unlearning&quot; is the problem of the org running the illegal models. If I have submitted a gdpr deletion request you better honor it. If it turns out you stole copyrighted content you should get punished for that. No one cares how much it might cost you to retrain your models. You put yourself in that situation to begin with.</div><br/><div id="40266203" class="c"><input type="checkbox" id="c-40266203" checked=""/><div class="controls bullet"><span class="by">avi_vallarapu</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40266004">parent</a><span>|</span><a href="#40266329">next</a><span>|</span><label class="collapse" for="c-40266203">[-]</label><label class="expand" for="c-40266203">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, I think is where it leads to eventually. And that is what I my original comment meant as well. &quot;Delete it&quot; rather than using some more techniques to &quot;unlearn it&quot;, unless you claim the unlearning is 100% accurate.</div><br/></div></div><div id="40266329" class="c"><input type="checkbox" id="c-40266329" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40266004">parent</a><span>|</span><a href="#40266203">prev</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40266329">[-]</label><label class="expand" for="c-40266329">[7 more]</label></div><br/><div class="children"><div class="content">&gt; No one cares how much it might cost you to retrain your models.<p>Playing tough? But it&#x27;s misguided. &quot;No one cares how much it might cost you to fix the damn internet&quot;<p>If you wanted to retro-fix facts, even if that could be achieved on a trained model, it would still get back by way of RAG or web search. But we don&#x27;t ask pure LLMs for facts and news unless we are stupid.<p>If someone wanted to pirate a content it would be easier to use Google search or torrents than generative AI. It would be faster, cheaper and higher quality. AIs move slow, are expensive, rate limited and lossy. AI providers have in-built checks to prevent copyright infringement.<p>If someone wanted to build something dangerous, it would be easier to hire a specialist than to <i>chatGPT their way into it</i>. All LLMs know is also on Google Search. Achieve security by cleaning the internet first.<p>The answer to all AI data issues - PII, Copyright, Dangerous Information - is coming back to the issue of Google search offering links to it, and websites hosting this information online. You can&#x27;t fix AI without fixing the internet.</div><br/><div id="40266424" class="c"><input type="checkbox" id="c-40266424" checked=""/><div class="controls bullet"><span class="by">beeboobaa3</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40266329">parent</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40266424">[-]</label><label class="expand" for="c-40266424">[6 more]</label></div><br/><div class="children"><div class="content">What do you mean playing tough? These are existing laws that should be enforced. The amount of people&#x27;s lives ruined by the American government because they were deemed copyright infringers is insane. The us has made it clear that copyright infringement is unacceptable.<p>We now have a new class of criminals infringing on copyright on a grand scale via their models and they seem desperate to avoid persecution hence all this bullshit.</div><br/><div id="40267376" class="c"><input type="checkbox" id="c-40267376" checked=""/><div class="controls bullet"><span class="by">cscurmudgeon</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40266424">parent</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40267376">[-]</label><label class="expand" for="c-40267376">[5 more]</label></div><br/><div class="children"><div class="content">1. You are assuming just training a model on copyrighted material is a violation. It is not. It may be under certain conditions but not by default.<p>2. Why should we aim for harsh punitive punishments just because it was done so in the past?</div><br/><div id="40267601" class="c"><input type="checkbox" id="c-40267601" checked=""/><div class="controls bullet"><span class="by">beeboobaa3</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40267376">parent</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40267601">[-]</label><label class="expand" for="c-40267601">[4 more]</label></div><br/><div class="children"><div class="content">&gt;  1. You are assuming just training a model on copyrighted material is a violation. It is not. It may be under certain conditions but not by default.<p>Using copyrighted content for commercial purposes should be a violation if it&#x27;s not already considered to be one. No different from playing copyrighted songs in your restaurant without paying a licensing fee.<p>&gt; 2. Why should we aim for harsh punitive punishments just because it was done so in the past?<p>I&#x27;d be fine with abolishing, or overhauling, the copyright system. This rules with harsh penalties for consumers&#x2F;small companies but not for bigtech double standard is bullshit, though.</div><br/><div id="40271303" class="c"><input type="checkbox" id="c-40271303" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40267601">parent</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40271303">[-]</label><label class="expand" for="c-40271303">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Using copyrighted content for commercial purposes should be a violation<p>so reading a book and using the book contents to help you in your job would be a violation too based on your logic</div><br/><div id="40271486" class="c"><input type="checkbox" id="c-40271486" checked=""/><div class="controls bullet"><span class="by">beeboobaa3</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40271303">parent</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40271486">[-]</label><label class="expand" for="c-40271486">[2 more]</label></div><br/><div class="children"><div class="content">A business cannot read a book, and your machine learning model is not given human rights.</div><br/><div id="40272138" class="c"><input type="checkbox" id="c-40272138" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40265846">root</a><span>|</span><a href="#40271486">parent</a><span>|</span><a href="#40267469">next</a><span>|</span><label class="collapse" for="c-40272138">[-]</label><label class="expand" for="c-40272138">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A business cannot read a book<p>Assume the human read the book <i>as part of their job</i>.  Is that using copyrighted material for commercial purposes?<p>If that doesn&#x27;t count then I&#x27;m not sure why you brought up &quot;commercial purposes&quot; at all.<p>&gt; This rules with harsh penalties for consumers&#x2F;small companies but not for bigtech double standard is bullshit, though.<p>Consumers and small companies get away with small copyright violations all the time.  And still bigger than having your image be one of millions in a training set.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40267469" class="c"><input type="checkbox" id="c-40267469" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40265846">prev</a><span>|</span><a href="#40268906">next</a><span>|</span><label class="collapse" for="c-40267469">[-]</label><label class="expand" for="c-40267469">[1 more]</label></div><br/><div class="children"><div class="content">How about a radial approach? How about not ingesting all content but only that which is explicitly marked as available for model-building purposes?</div><br/></div></div><div id="40268906" class="c"><input type="checkbox" id="c-40268906" checked=""/><div class="controls bullet"><span class="by">xg15</span><span>|</span><a href="#40267469">prev</a><span>|</span><a href="#40271700">next</a><span>|</span><label class="collapse" for="c-40268906">[-]</label><label class="expand" for="c-40268906">[1 more]</label></div><br/><div class="children"><div class="content">What I don&#x27;t get about the DP approach is how this would be reconciled with the &quot;exact&quot; question-answering functionality of LLMs.<p>DP makes perfect sense if all I care about is low-resolution statistical metrics or distributions of something and not the exact values - the entire purpose of DP is to prevent reconstructing the exact values.<p>However, the expectation for LLMs is usually to ask a question (or request a task) and get an exact value as a response: If you ask &quot;What&#x27;s the phone number of John Smith?&quot; the model will either tell you it doesn&#x27;t know or it will answer you with an actual phone number (real or hallucinated). It will not tell you &quot;the number is with 83% probability somewhere in New Jersey&quot;.<p>So if the model is trained with DP, then either the data is scrambled enough that the it won&#x27;t be able to return <i>any</i> kind of reliably correct data, effectively making it useless - or it&#x27;s <i>not</i> scrambled enough, so that the model can successfully reconstuct data despite the scrambling process, effectively making the DP step useless.<p>Or in other words, the OP defines &quot;DP unlearning&quot; as:<p>&gt; <i>The intuition is that if an adversary cannot (reliably) tell apart the models, then it is as if this data point has never been learned—thus no need to unlearn.</i><p>However, if my original model truthfully returns John Smith&#x27;s phone number on request and the &quot;unlearned&quot; model must not be distinguishable by an outside observer from the original model, then the &quot;unlearned&quot; model will <i>also</i> return the phone number. While I could say that &quot;technically&quot; the model has never seen the phone number in the training data due to my DP scrambling, this doesn&#x27;t solve the practical problem why the unlearning was requested in the first place, namely that John Smith doesn&#x27;t want the model to return his phone number. He could probably care less about the specific details of the training process.<p>So then, how would DP help here?</div><br/></div></div><div id="40271700" class="c"><input type="checkbox" id="c-40271700" checked=""/><div class="controls bullet"><span class="by">joshhansen</span><span>|</span><a href="#40268906">prev</a><span>|</span><a href="#40269830">next</a><span>|</span><label class="collapse" for="c-40271700">[-]</label><label class="expand" for="c-40271700">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Eternal Sunshine of the Spotless Mind&quot;<p>The erasure of knowledge is a troubling occupation</div><br/></div></div><div id="40269830" class="c"><input type="checkbox" id="c-40269830" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#40271700">prev</a><span>|</span><a href="#40264795">next</a><span>|</span><label class="collapse" for="c-40269830">[-]</label><label class="expand" for="c-40269830">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know — the post, reading the comments here, I am a little worried for the &quot;sanity&quot; of our AI that have been trained, untrained, retrained like a pawn in some kind of Cold War spy novel.</div><br/><div id="40272526" class="c"><input type="checkbox" id="c-40272526" checked=""/><div class="controls bullet"><span class="by">kombookcha</span><span>|</span><a href="#40269830">parent</a><span>|</span><a href="#40264795">next</a><span>|</span><label class="collapse" for="c-40272526">[-]</label><label class="expand" for="c-40272526">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s fine, the LLM AIs we have now are just fancy versions of autocorrect. They, and other LMs, guess at statistically probable words&#x2F;datapoints, and because they don&#x27;t understand context, you might need to put your thumb on the scales to make the output actually be useful. They&#x27;re at best very janky tools as soon as you&#x27;re working with things that require context that isn&#x27;t easily contained in some kind of confined area of work.<p>Currently we are seeing the phenomenon &#x27;habsburg AI&#x27; where AI&#x27;s consume their own outputs as training data, which rapidly deteriorates their ability to actually be useful for much of anything.<p>The thing is that there literally isn&#x27;t enough human-made data to keep feeding them (they already ate the entire internet), so if you both want to continue ramping their intake of data and you also don&#x27;t want them to get rapidly weird and completely useless, you pretty much have to get in there with elbow grease. Removing or deprioritizing data that&#x27;s tripping up the model is one of the few ways you can do human-assisted refinement of these things.<p>The sooner we all face the music that these things aren&#x27;t magical truth machines, have a long way to go and there is no guaranteed rate of growth, the sooner this hype cycle can end.</div><br/></div></div></div></div><div id="40264795" class="c"><input type="checkbox" id="c-40264795" checked=""/><div class="controls bullet"><span class="by">dataflow</span><span>|</span><a href="#40269830">prev</a><span>|</span><a href="#40265882">next</a><span>|</span><label class="collapse" for="c-40264795">[-]</label><label class="expand" for="c-40264795">[9 more]</label></div><br/><div class="children"><div class="content">&gt; However, RTBF wasn’t really proposed with machine learning in mind. In 2014, policymakers wouldn’t have predicted that deep learning will be a giant hodgepodge of data &amp; compute<p>Eh? Weren&#x27;t deep learning and big data already things in 2014? Pretty sure everyone understood ML models would have a tough time and they still wanted RTBF.</div><br/><div id="40272265" class="c"><input type="checkbox" id="c-40272265" checked=""/><div class="controls bullet"><span class="by">hooby</span><span>|</span><a href="#40264795">parent</a><span>|</span><a href="#40265773">next</a><span>|</span><label class="collapse" for="c-40272265">[-]</label><label class="expand" for="c-40272265">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure that the policymakers did NOT understand ML models in 2014 - and still do NOT understand it today.<p>I also don&#x27;t think that they care. They don&#x27;t care that ML is a hodgepodge of data &amp; compute, and they don&#x27;t care how hard it is to remove data from a model.<p>They didn&#x27;t care about the ease or difficulty of removing data from more traditional types of knowledge storage either - like search indexes, database backups and whatnot.<p>RTBF was not proposed with <i>any</i> specific technology in mind. What they had in mind, was to try and give individuals a tool, to keep their private information private. Like, if you have a private, unlisted phone number, and that number somehow ends up on the call-list of some pollster firm, you can force that firm to delete your number so that they can&#x27;t call you anymore.<p>The idea is, that if your private phone number (or similar data) ends up being shared or sold without your consent - you can try to undo the damage.<p>In practice it might still be easier to get a new number, than to have your leaked one erased... but not all private data is exchangeable like that.</div><br/></div></div><div id="40265773" class="c"><input type="checkbox" id="c-40265773" checked=""/><div class="controls bullet"><span class="by">indigovole</span><span>|</span><a href="#40264795">parent</a><span>|</span><a href="#40272265">prev</a><span>|</span><a href="#40265225">next</a><span>|</span><label class="collapse" for="c-40265773">[-]</label><label class="expand" for="c-40265773">[1 more]</label></div><br/><div class="children"><div class="content">GDPR and RTBF were formulated around the fears of data collection by the Stasi and other organizations. They were not formulated around easing the burdens of future entrepreneurs, but about mitigating the damage they might cause. Europeans were concerned about real harms that living people had experienced, not about enabling AGI or targeted advertising or digital personal assistants.<p>We have posts here at least weekly from people cut off from their services, and their work along with them, because of bad inference, bad data, and inability to update metadata based purely on BigGo routine automation and indifference to individual harm. Imagine the scale that such damage will take when this automation and indifference to individual harm are structured around repositories from which data cannot be deleted, cannot be corrected.</div><br/></div></div><div id="40265225" class="c"><input type="checkbox" id="c-40265225" checked=""/><div class="controls bullet"><span class="by">spennant</span><span>|</span><a href="#40264795">parent</a><span>|</span><a href="#40265773">prev</a><span>|</span><a href="#40265704">next</a><span>|</span><label class="collapse" for="c-40265225">[-]</label><label class="expand" for="c-40265225">[3 more]</label></div><br/><div class="children"><div class="content">Agreed. The media and advertising industry was most definitely leveraging cookie-level data for building attribution and targeting models. As soon as the EU established that this data was “personal data”, as it could, theoretically, be tied back to individual citizens, there were questions about the models. Namely “Would they have to be rebuilt after every RTBF request?” Needless to say, no one in the industry really wanted to address the question, as the wrong answer would essentially shut down a very profitable practice.</div><br/><div id="40265467" class="c"><input type="checkbox" id="c-40265467" checked=""/><div class="controls bullet"><span class="by">Aerroon</span><span>|</span><a href="#40264795">root</a><span>|</span><a href="#40265225">parent</a><span>|</span><a href="#40265704">next</a><span>|</span><label class="collapse" for="c-40265467">[-]</label><label class="expand" for="c-40265467">[2 more]</label></div><br/><div class="children"><div class="content">More likely: the wrong answer would&#x27;ve shut out a profitable market rather than the practice. The EU is not the world. Anthropic seems to not mind blocking the EU for example.</div><br/><div id="40266792" class="c"><input type="checkbox" id="c-40266792" checked=""/><div class="controls bullet"><span class="by">spennant</span><span>|</span><a href="#40264795">root</a><span>|</span><a href="#40265467">parent</a><span>|</span><a href="#40265704">next</a><span>|</span><label class="collapse" for="c-40266792">[-]</label><label class="expand" for="c-40266792">[1 more]</label></div><br/><div class="children"><div class="content">Sure. But two things:<p>1) At the time, the European data laws implied that it protected its citizens no matter where they are. Nobody wanted to be the first to test that in court.<p>2) The organizations and agencies performing this type of data modeling were often doing so on behalf of large multinational organizations with absurd advertising spends, so they were dealing with Other People’s Data. The responsibility of scrubbing it clean of EU citizen data was unclear.<p>What this meant was that an EU tourist who traveled to the US, and got served a targeted ad, could make a RTBF request to the advertiser (think Coca-Cola, Nestle or Unilever)<p>The whole thing was a mess.</div><br/></div></div></div></div></div></div><div id="40265704" class="c"><input type="checkbox" id="c-40265704" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#40264795">parent</a><span>|</span><a href="#40265225">prev</a><span>|</span><a href="#40265172">next</a><span>|</span><label class="collapse" for="c-40265704">[-]</label><label class="expand" for="c-40265704">[1 more]</label></div><br/><div class="children"><div class="content">RTBF was introduced to solve a specific issue, no?<p>Politicians and their lobbyist friends could no longer remove materials linking them to their misdeeds as the first Google Search link associated with their names. Hence RTBF.<p>Now, there’s similar issue with AI. Models are progressing towards being factual, useful and reliable.</div><br/></div></div><div id="40265172" class="c"><input type="checkbox" id="c-40265172" checked=""/><div class="controls bullet"><span class="by">isodev</span><span>|</span><a href="#40264795">parent</a><span>|</span><a href="#40265704">prev</a><span>|</span><a href="#40264850">next</a><span>|</span><label class="collapse" for="c-40265172">[-]</label><label class="expand" for="c-40265172">[1 more]</label></div><br/><div class="children"><div class="content">Of course, it’s not a regulation issue. The technology was introduced to users before it was ready. The very nature of training without opt-in consent or mechanism of being forgotten are all issues that should have been addressed before trying to make a keyboard with a special copilot button.</div><br/></div></div><div id="40264850" class="c"><input type="checkbox" id="c-40264850" checked=""/><div class="controls bullet"><span class="by">peteradio</span><span>|</span><a href="#40264795">parent</a><span>|</span><a href="#40265172">prev</a><span>|</span><a href="#40265882">next</a><span>|</span><label class="collapse" for="c-40264850">[-]</label><label class="expand" for="c-40264850">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know if people anticipated contemporary parroting behavior over huge datasets.  Modern well funded models can recall an obscure persons home address buried deep into the training set.  I guess the techniques described might be presented to the European audience in an attempt to maintain access to their data&#x2F;and or market for sales.  I hope they fail.</div><br/></div></div></div></div><div id="40265882" class="c"><input type="checkbox" id="c-40265882" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#40264795">prev</a><span>|</span><a href="#40264859">next</a><span>|</span><label class="collapse" for="c-40265882">[-]</label><label class="expand" for="c-40265882">[2 more]</label></div><br/><div class="children"><div class="content">I think &quot;unlearning&quot; is not the actual goal; we don&#x27;t want the model to stick its proverbial head in the sand.  Being unaware of racism is different from not producing racist content (and, in fact, one could argue that it is necessary to know about racism if one wishes to inhibit producing racist content; I remember in elementary school certain kids thought it would be funny to teach one of the special-ed kids to parrot offensive sentences).</div><br/><div id="40268306" class="c"><input type="checkbox" id="c-40268306" checked=""/><div class="controls bullet"><span class="by">krono</span><span>|</span><a href="#40265882">parent</a><span>|</span><a href="#40264859">next</a><span>|</span><label class="collapse" for="c-40268306">[-]</label><label class="expand" for="c-40268306">[1 more]</label></div><br/><div class="children"><div class="content">Say you tell me you want a red sphere. Taken at face value, you show a prejudice for red sphere&#x27;s and discriminate against all other coloured shapes.<p>We&#x27;ve all had to dance that dance with ChatGPT by now, where you ask for something perfectly ordinary, but receive a response telling you off for even daring to think like that, until eventually you manage to formulate the prompt in a way that it likes with just the right context and winner vocabulary + grammar, and finally the damned thing gives you the info you want without so much as any gaslighting or snarky insults hiding in the answer!<p>It doesn&#x27;t understand racism, it simply evaluates certain combinations of things according to how it was set up to do.</div><br/></div></div></div></div><div id="40264859" class="c"><input type="checkbox" id="c-40264859" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#40265882">prev</a><span>|</span><a href="#40265516">next</a><span>|</span><label class="collapse" for="c-40264859">[-]</label><label class="expand" for="c-40264859">[5 more]</label></div><br/><div class="children"><div class="content">“to edit away undesired things like private data, stale knowledge, copyrighted materials, toxic&#x2F;unsafe content, dangerous capabilities, and misinformation, without retraining models from scratch”<p>To say nothing of unlearning those safeguards and&#x2F;or “safeguards”.</div><br/><div id="40265251" class="c"><input type="checkbox" id="c-40265251" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40264859">parent</a><span>|</span><a href="#40265516">next</a><span>|</span><label class="collapse" for="c-40265251">[-]</label><label class="expand" for="c-40265251">[4 more]</label></div><br/><div class="children"><div class="content">It sounds like you&#x27;re mistakenly grouping together three very different methods of changing an AI&#x27;s behaviour.<p>You have some model, M™, which can do Stuff. Some of the Stuff is, by your personal standards Bad (I don&#x27;t care what your standard is, roll with this).<p>You have three solutions:<p>1) Bolt on a post-processor which takes the output of M™, and if the output is detectably Bad, you censor it.<p>Failure mode: this is trivial to remove, just delete the post-processor.<p>Analogy: put secret documents into a folder called &quot;secret do not read&quot;.<p>2) Retrain the weights within M™ to have a similar effect as 1.<p>Failure mode: this is still fairly easy to remove, but will require re-training to get there. Why? Because the weights containing this information are not completely zeroed-out by this process.<p>Analogy: how and why &quot;un-deletion&quot; is possible on file systems.<p>3) <i>Find and eliminate</i> the weights within M™ that lead to the Bad output.<p>Analogy: &quot;secure deletion&quot; involves overwriting files with random data before unlinking them, possibly several times if it&#x27;s a spinning disk.<p>--<p>People are still doing research on 3 to make sure that it actually happens, what with it being of very high importance for a lot of different reasons including legal obligation.</div><br/><div id="40265329" class="c"><input type="checkbox" id="c-40265329" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#40264859">root</a><span>|</span><a href="#40265251">parent</a><span>|</span><a href="#40267241">next</a><span>|</span><label class="collapse" for="c-40265329">[-]</label><label class="expand" for="c-40265329">[2 more]</label></div><br/><div class="children"><div class="content">Until we have a very different method of actually controlling LLM behavior, 1 is the only feasible one.<p>Your framing only makes sense when &quot;Bad&quot; is something so bad  that we can&#x27;t bear its existence, as opposed to just &quot;commercially bad&quot; where it shouldn&#x27;t behave that way with an end user. In the latter, your choice 1 - imposing external guardrails - is fine. I&#x27;m not aware of anything LLMs can do that fits in the former category.</div><br/><div id="40266798" class="c"><input type="checkbox" id="c-40266798" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40264859">root</a><span>|</span><a href="#40265329">parent</a><span>|</span><a href="#40267241">next</a><span>|</span><label class="collapse" for="c-40266798">[-]</label><label class="expand" for="c-40266798">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Until we have a very different method of actually controlling LLM behavior, 1 is the only feasible one.<p>Most of the stuff I&#x27;ve seen, is 2. I&#x27;ve only seen a few places use 1 — you can tell the difference, because when a LLM pops out a message <i>and then</i> deletes it, that&#x27;s a type 1 behaviour, whereas if the first thing it outputs directly is a sequence of tokens saying (any variant of) &quot;nope, not gonna do that&quot; that&#x27;s type 2 behaviour.<p>This appears to be what&#x27;s described in this thread: <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;bing&#x2F;comments&#x2F;11fryce&#x2F;why_do_bings_responses_sometimes_delete_themselves&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;bing&#x2F;comments&#x2F;11fryce&#x2F;why_do_bings_...</a><p>The research into going from  type 2 to type 3 is the entirety of the article.<p>&gt; Your framing only makes sense when &quot;Bad&quot; is something so bad that we can&#x27;t bear its existence, as opposed to just &quot;commercially bad&quot; where it shouldn&#x27;t behave that way with an end user. In the latter, your choice 1 - imposing external guardrails - is fine.<p>I disagree, I think my framing applies to all cases. Right now, LLMs are like old PCs with no user accounts and a single shared memory space, which is fine and dandy when you&#x27;re not facing malicious input, but we live in a world with malicious input.<p>You <i>might</i> be able to use a type 1 solution, but it&#x27;s <i>going</i> to be fragile, and more pertinently, slow, as you only know to reject content once it has finished and may therefore end up in an unbounded loop of an LLM generating content that a censor rejects.<p>A type 2 solution is still fragile, but it <i>just doesn&#x27;t</i> make the &quot;bad&quot; content in the first place — and, to be clear, &quot;bad&quot; in this context can be <i>anything</i> undesired, including &quot;uses vocabulary too advanced for a 5 year old who just started school&quot; if that&#x27;s what you care about using some specific LLM for.</div><br/></div></div></div></div><div id="40267241" class="c"><input type="checkbox" id="c-40267241" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#40264859">root</a><span>|</span><a href="#40265251">parent</a><span>|</span><a href="#40265329">prev</a><span>|</span><a href="#40265516">next</a><span>|</span><label class="collapse" for="c-40267241">[-]</label><label class="expand" for="c-40267241">[1 more]</label></div><br/><div class="children"><div class="content">I think you mistakenly replied to my comment instead of one that made some sort of grouping?<p>Alternatively, you&#x27;re assuming that because there is some possible technique that can&#x27;t be reversed, it&#x27;s no longer useful to remove the effects of techniques that _can_ be reversed?</div><br/></div></div></div></div></div></div><div id="40265516" class="c"><input type="checkbox" id="c-40265516" checked=""/><div class="controls bullet"><span class="by">gotoeleven</span><span>|</span><a href="#40264859">prev</a><span>|</span><a href="#40264792">next</a><span>|</span><label class="collapse" for="c-40265516">[-]</label><label class="expand" for="c-40265516">[1 more]</label></div><br/><div class="children"><div class="content">My new startup includes a pitchfork wielding mob in the ML training loop.</div><br/></div></div><div id="40264933" class="c"><input type="checkbox" id="c-40264933" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#40264792">prev</a><span>|</span><a href="#40267239">next</a><span>|</span><label class="collapse" for="c-40264933">[-]</label><label class="expand" for="c-40264933">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve wondered before if it was possible to unlearn facts, but retain the general &quot;reasoning&quot; capability that came from being trained on the facts, then dimensionality reduce the model.</div><br/><div id="40265636" class="c"><input type="checkbox" id="c-40265636" checked=""/><div class="controls bullet"><span class="by">Brian_K_White</span><span>|</span><a href="#40264933">parent</a><span>|</span><a href="#40271236">next</a><span>|</span><label class="collapse" for="c-40265636">[-]</label><label class="expand" for="c-40265636">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know about in AI, but it seems like that is what humans do.<p>We remember <i>some</i> facts but I know at least I have had a lot of facts pass through me and only leave their effects.<p>I once had some facts, did some reasoning, arrived at a conclusion, and only retained the conclusion and enough of the reasoning to identify other contexts where the same reasoning should apply. I no longer have the facts, I simply trust my earlier selfs process of reasoning, and even that isn&#x27;t actually trust or faith because I also still reason about new things today and observe the process.<p>But I also evolve. I don&#x27;t <i>only</i> trust a former reasoning unchanging forever. It&#x27;s just that when I do revisit something and basically &quot;reproduce the other scientists work&quot; even if I arrive a different conclusion today, I&#x27;m generally still ok with the earlier me&#x27;s reasoning and conclusion. It stands up as reasonable, and the new conclusion is usually just tuned a little, not wildly opposite. Or some things do change radically but I always knew they might, like in the process of self discovery you try a lot of opposite things.<p>Getting a little away from the point but the point is I think the way we ourselves develop answer-generating-rules is very much by retaining only the results (the developed rules) and not all the facts and steps of the work, at least much of the time. Certainly we remember some justifying &#x2F; exemplifying facts to explain some things we do.</div><br/></div></div><div id="40271236" class="c"><input type="checkbox" id="c-40271236" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40264933">parent</a><span>|</span><a href="#40265636">prev</a><span>|</span><a href="#40264991">next</a><span>|</span><label class="collapse" for="c-40271236">[-]</label><label class="expand" for="c-40271236">[1 more]</label></div><br/><div class="children"><div class="content">How much reasoning capability LLM’s have is up for debate.<p>With a true AGI you could just tell it to keep people’s personal information confidential and expect that it would understand that instruction.</div><br/></div></div><div id="40264991" class="c"><input type="checkbox" id="c-40264991" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#40264933">parent</a><span>|</span><a href="#40271236">prev</a><span>|</span><a href="#40265243">next</a><span>|</span><label class="collapse" for="c-40264991">[-]</label><label class="expand" for="c-40264991">[1 more]</label></div><br/><div class="children"><div class="content">If you think of knowledge as a (knowledge) graph, it seems there would be some nodes with low centrality that you could drop without much effect, and other key ones that would have a bigger impact if lost.</div><br/></div></div><div id="40265243" class="c"><input type="checkbox" id="c-40265243" checked=""/><div class="controls bullet"><span class="by">huygens6363</span><span>|</span><a href="#40264933">parent</a><span>|</span><a href="#40264991">prev</a><span>|</span><a href="#40267239">next</a><span>|</span><label class="collapse" for="c-40265243">[-]</label><label class="expand" for="c-40265243">[1 more]</label></div><br/><div class="children"><div class="content">Yes, me too. If it could somehow remember the “structure” instead of the instantiation. More “relationships between types of token relationships” instead of “relationships between tokens”.</div><br/></div></div></div></div><div id="40267239" class="c"><input type="checkbox" id="c-40267239" checked=""/><div class="controls bullet"><span class="by">greenavocado</span><span>|</span><a href="#40264933">prev</a><span>|</span><a href="#40264799">next</a><span>|</span><label class="collapse" for="c-40267239">[-]</label><label class="expand" for="c-40267239">[5 more]</label></div><br/><div class="children"><div class="content">Please use the correct terminology: censorship</div><br/><div id="40272180" class="c"><input type="checkbox" id="c-40272180" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40267239">parent</a><span>|</span><a href="#40267858">next</a><span>|</span><label class="collapse" for="c-40272180">[-]</label><label class="expand" for="c-40272180">[1 more]</label></div><br/><div class="children"><div class="content">Is it censorship to not include every piece of text you can possibly find into your training dataset?<p>What&#x27;s the difference between making that choice versus removing it from the model later?</div><br/></div></div><div id="40267858" class="c"><input type="checkbox" id="c-40267858" checked=""/><div class="controls bullet"><span class="by">qbit42</span><span>|</span><a href="#40267239">parent</a><span>|</span><a href="#40272180">prev</a><span>|</span><a href="#40267270">next</a><span>|</span><label class="collapse" for="c-40267858">[-]</label><label class="expand" for="c-40267858">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s a fair characterization. If a user requests a company to stop using their data, ML unlearning allows the company to do so without retraining their models from scratch.</div><br/></div></div><div id="40267270" class="c"><input type="checkbox" id="c-40267270" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#40267239">parent</a><span>|</span><a href="#40267858">prev</a><span>|</span><a href="#40267602">next</a><span>|</span><label class="collapse" for="c-40267270">[-]</label><label class="expand" for="c-40267270">[1 more]</label></div><br/><div class="children"><div class="content">If company X wants their model to say&#x2F;not say Y based on ideology, they aren&#x27;t stopping anyone saying anything. They are stopping their own model saying something. The fact that I don&#x27;t go around screaming nasty things about some group doesn&#x27;t make me against free speech.<p>It&#x27;s censorship to try to stop people producing models as they see fit.</div><br/></div></div><div id="40267602" class="c"><input type="checkbox" id="c-40267602" checked=""/><div class="controls bullet"><span class="by">62951413</span><span>|</span><a href="#40267239">parent</a><span>|</span><a href="#40267270">prev</a><span>|</span><a href="#40264799">next</a><span>|</span><label class="collapse" for="c-40267602">[-]</label><label class="expand" for="c-40267602">[1 more]</label></div><br/><div class="children"><div class="content">The prolefeed explains that deep duckspeaking is doubleplusgood. Nothing to see here, citizen.</div><br/></div></div></div></div><div id="40264799" class="c"><input type="checkbox" id="c-40264799" checked=""/><div class="controls bullet"><span class="by">negative_person</span><span>|</span><a href="#40267239">prev</a><span>|</span><label class="collapse" for="c-40264799">[-]</label><label class="expand" for="c-40264799">[27 more]</label></div><br/><div class="children"><div class="content">Why should we try to unlearn &quot;bad&quot; behaviours from AI?<p>There is no AGI without violence, its part of being free thinking and self survival.<p>But also by knowing that launching a first strike by a drunk president was a bad idea we averted a war because of a few people, AI needs to understand consequences.<p>It seems futile to try and hide &quot;bad&quot; from AI.</div><br/><div id="40264870" class="c"><input type="checkbox" id="c-40264870" checked=""/><div class="controls bullet"><span class="by">williamtrask</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40264852">next</a><span>|</span><label class="collapse" for="c-40264870">[-]</label><label class="expand" for="c-40264870">[9 more]</label></div><br/><div class="children"><div class="content">Because we can get AI related technologies to do things living creatures can’t, like provably forget things. And when it benefits us, we should.<p>Personal opinion, but I think AGI is a good heuristic to build against but in the end we’ll pivot away. Sort of like how birds were a good heuristic for human flight, but modern planes don’t flap their wings and greatly exceed bird capabilities in many ways.<p>Attribution for every prediction and deletion seem like prime examples of things which would break the analogy of AI&#x2F;AGI with something more economically and politically compelling&#x2F;competitive.</div><br/><div id="40264963" class="c"><input type="checkbox" id="c-40264963" checked=""/><div class="controls bullet"><span class="by">negative_person</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40264870">parent</a><span>|</span><a href="#40264852">next</a><span>|</span><label class="collapse" for="c-40264963">[-]</label><label class="expand" for="c-40264963">[8 more]</label></div><br/><div class="children"><div class="content">Can you point to any behaviour in human beings you&#x27;d unlearn if theyd also forget the consequences?<p>We spend billions trying to predict human behaviour and yet we are surprised everyday, &quot;AGI&quot; will be no simpler. We just have to hope the dataset was aligned so the consequences are understood, and find a way to contain models that don&#x27;t.</div><br/><div id="40265748" class="c"><input type="checkbox" id="c-40265748" checked=""/><div class="controls bullet"><span class="by">williamtrask</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40264963">parent</a><span>|</span><a href="#40265194">next</a><span>|</span><label class="collapse" for="c-40265748">[-]</label><label class="expand" for="c-40265748">[1 more]</label></div><br/><div class="children"><div class="content">You seem to be focusing a lot on remembering or forgetting consequences. Yes, ensuring models know enough about the world to only cause the consequences they desire is a good way for models to not create random harm. This is probably a good thing.<p>However, there are many other reasons why you might want a neural network to provably forget something. The main reason has to do with structuring an AGI&#x27;s power. Even though the simple-story of AGI is something like &quot;make it super powerful, general, and value aligned and humanity will prosper&quot;. However, the reality is more nuanced. Sometimes you want a model to be selectively not powerful as a part of managing value mis-alignment in practice.<p>To pick a trivial example, you might want a model to enter your password in some app one time, but not remember the password long term. You might want it to <i>use</i> and then provably <i>forget</i> your password so that it can&#x27;t use your password in the future without your consent.<p>This isn&#x27;t something that&#x27;s reliably doable with humans. If you give them your 
password, they have it — you can&#x27;t get it back. This is the point at which we&#x27;ll have the option to pursue the imitation of living creatures blindly, or choose to turn away from a blind adherence to the AI&#x2F;AGI story. Just like we reached the point at which we decided whether flying planes should have flapping wings dogmatically — or whether we should pursue the more economically and politically competitive thing. Planes don&#x27;t flap their wings, and AI&#x2F;AGI will be able to provably forget things. And that&#x27;s actually the better path.<p>A recent work co-authors and I published related to this: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2012.08347" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2012.08347</a></div><br/></div></div><div id="40265194" class="c"><input type="checkbox" id="c-40265194" checked=""/><div class="controls bullet"><span class="by">aeonik</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40264963">parent</a><span>|</span><a href="#40265748">prev</a><span>|</span><a href="#40265462">next</a><span>|</span><label class="collapse" for="c-40265194">[-]</label><label class="expand" for="c-40265194">[3 more]</label></div><br/><div class="children"><div class="content">The feeling of extreme euphoria and its connection to highly addictive drugs like Heroin might be a use case. Though I&#x27;m not sure how well something like that would work in practice.</div><br/><div id="40265396" class="c"><input type="checkbox" id="c-40265396" checked=""/><div class="controls bullet"><span class="by">everforward</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40265194">parent</a><span>|</span><a href="#40265462">next</a><span>|</span><label class="collapse" for="c-40265396">[-]</label><label class="expand" for="c-40265396">[2 more]</label></div><br/><div class="children"><div class="content">Is that possible to do without also forgetting why it’s dangerous? That seems like it would fuel a pattern of addiction where the person gets addicted, forgets why, then gets addicted again because we wiped their knowledge of the consequences the first time around.<p>Then again, I suppose if the addiction was in response to a particular stimulus (death of a family member, getting fired, etc) and that stimulus doesn’t happen again, maybe it would make a difference?<p>It does have a tinge of “those who don’t recall the past are doomed to repeat it”.</div><br/><div id="40266591" class="c"><input type="checkbox" id="c-40266591" checked=""/><div class="controls bullet"><span class="by">aeonik</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40265396">parent</a><span>|</span><a href="#40265462">next</a><span>|</span><label class="collapse" for="c-40266591">[-]</label><label class="expand" for="c-40266591">[1 more]</label></div><br/><div class="children"><div class="content">After a certain point I think someone can learn enough information to derive almost everything from first principles. But I think it might work temporarily.<p>There&#x27;s a movie about this idea called &quot;Eternal Sunshine of a Spotless Mind&quot;.<p>I find it hard I believe that you can surgically censor one chunk of information, and cut off the rest of the information. Especially if it&#x27;s general physical principles.<p>I also don&#x27;t have a nice topological map of how all the world&#x27;s information is connected to the moment, so I can&#x27;t back up by opinions.<p>Though I&#x27;m still rooting for the RDF&#x2F;OWL and Semantic Web folks, they might figure it out.</div><br/></div></div></div></div></div></div><div id="40265462" class="c"><input type="checkbox" id="c-40265462" checked=""/><div class="controls bullet"><span class="by">Brian_K_White</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40264963">parent</a><span>|</span><a href="#40265194">prev</a><span>|</span><a href="#40266932">next</a><span>|</span><label class="collapse" for="c-40265462">[-]</label><label class="expand" for="c-40265462">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like the only answer for AI is the same as the only answer for humans.<p>Wisdom. Arriving at actions and reactions based on better understanding of the interconnectedness and interdependency of everything and everyone. (knowing more not less, and not selective or bowdlerized)<p>And most humans don&#x27;t even have it. Most humans are not interested and don&#x27;t believe and certainly don&#x27;t act as though &quot;What&#x27;s good for you is what&#x27;s good for me, what harms you harms me.&quot; Every day a tech podcaster or youtuber says this or that privacy loss or security risk &quot;doesn&#x27;t affect you or me&quot;, <i>they all affect you and me</i>, when a government or company gives themselves and then abuses power over a single person anywhere, that is a hit to you and me even though we aren&#x27;t that person, because that person is somebody, and you and I are somebody.<p>Most humans ridicule anyone that talks like that and don&#x27;t let them near any levers of power at any scale. They might be ok with it in inconsequential conversational contexts like a dinner party or this or this forum, but not in any decision-making context. Anyone talking like that is an idiot and disconnected from reality, they might drive the bus off the bridge because the peace fairies told them to.<p>If an AI were better than most humans and had wisdom, and gave answers that conflicted with selfishness, most humans would just decide they don&#x27;t like the answers and instructions coming from the AI and just destroy it, or at least ignore it, pretty much as they do today with humans who say things they don&#x27;t like.<p>Perhaps one difference is an AI could actually be both wise and well-intentioned rather than a charlatan harnessing the power of a mass of gullables, and it could live longer than a human and it&#x27;s results could become proven-out over time. Some humans do get recognized eventually, but by then it doesn&#x27;t do the rest of us any good because they can no longer be a leader as they&#x27;re too old or dead. Then again maybe that&#x27;s required actually. Maybe the AI can&#x27;t prove itself because you can never say of the AI, &quot;What does he get out of it by now? He lived his entire life saying the same thing, if he was just trying to scam everyone for money or power or something, what good would it even do him now? He must have been sincere the whole time.&quot;<p>But probably even the actual good AI won&#x27;t do much good, again for the same reason as with actually good humans, it&#x27;s just not what most people want. Whatever individuals say about what their values are, by the numbers only the selfish organisations win. Even when a selfish organization goes too far and destroys itself, everyone else still keeps doing the same thing.</div><br/></div></div><div id="40266932" class="c"><input type="checkbox" id="c-40266932" checked=""/><div class="controls bullet"><span class="by">AvAn12</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40264963">parent</a><span>|</span><a href="#40265462">prev</a><span>|</span><a href="#40266448">next</a><span>|</span><label class="collapse" for="c-40266932">[-]</label><label class="expand" for="c-40266932">[1 more]</label></div><br/><div class="children"><div class="content">A few things to exclude from training might include:
- articles with mistakes such as incorrect product names, facts, dates, references
- fraudulent and non-repeatable research findings - see John Ioannidis among others
- outdated and incorrect scientific concepts like phlogiston and LaMarckian evolution
- junk content such as 4-chan comments section content
- flat earther &quot;science&quot; and other such nonsense
- debatable stuff like: do we want material that attributes human behavior to astrological signs or not? And when should a response make reference to such?
- prank stuff like script kiddies prompting 2+2=5 until an AI system &quot;remembers&quot; this
- intentional poisoning of a training set with disinformation
- suicidal and homicidal suggestions and ideation
- etc.<p>Even if we go with the notion that AGI is coming, there is no reason its training should include the worst in us.</div><br/></div></div><div id="40266448" class="c"><input type="checkbox" id="c-40266448" checked=""/><div class="controls bullet"><span class="by">beeboobaa3</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40264963">parent</a><span>|</span><a href="#40266932">prev</a><span>|</span><a href="#40264852">next</a><span>|</span><label class="collapse" for="c-40266448">[-]</label><label class="expand" for="c-40266448">[1 more]</label></div><br/><div class="children"><div class="content">Seeing dad have sex with mom.</div><br/></div></div></div></div></div></div><div id="40264852" class="c"><input type="checkbox" id="c-40264852" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40264870">prev</a><span>|</span><a href="#40265923">next</a><span>|</span><label class="collapse" for="c-40264852">[-]</label><label class="expand" for="c-40264852">[1 more]</label></div><br/><div class="children"><div class="content">This is presumably about a chatbot though, not AGI, so it&#x27;s basically a way of limiting what they say. (Not a way that I expect to succeed)</div><br/></div></div><div id="40265923" class="c"><input type="checkbox" id="c-40265923" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40264852">prev</a><span>|</span><a href="#40265578">next</a><span>|</span><label class="collapse" for="c-40265923">[-]</label><label class="expand" for="c-40265923">[1 more]</label></div><br/><div class="children"><div class="content">They are just trying to find a way to plausibly declare successful removal of copyrighted and&#x2F;or illegal material without discarding weights.<p>GPT-4 class models reportedly costs $10-100m to train, and that&#x27;s too much to throw away for Harry Potter or Russian child porn scrapes that could later reproduce verbatim despite representing &lt;0.1ppb or whatever minuscule part of dataset.</div><br/></div></div><div id="40265578" class="c"><input type="checkbox" id="c-40265578" checked=""/><div class="controls bullet"><span class="by">affgrff2</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40265923">prev</a><span>|</span><a href="#40267142">next</a><span>|</span><label class="collapse" for="c-40265578">[-]</label><label class="expand" for="c-40265578">[3 more]</label></div><br/><div class="children"><div class="content">Maybe it all boils down to copyright. Having a method that believably removes the capacity to generate copyrighted results might give you some advantage with respect to some legislation.</div><br/><div id="40265603" class="c"><input type="checkbox" id="c-40265603" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40265578">parent</a><span>|</span><a href="#40265594">next</a><span>|</span><label class="collapse" for="c-40265603">[-]</label><label class="expand" for="c-40265603">[1 more]</label></div><br/><div class="children"><div class="content">Also if you build some sort of search engine using an LLM governments will expect you to be able to remove websites or knowledge of certain websites for legal reasons (DMCA, right to be forgotten, etc).</div><br/></div></div></div></div><div id="40267142" class="c"><input type="checkbox" id="c-40267142" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40265578">prev</a><span>|</span><a href="#40265336">next</a><span>|</span><label class="collapse" for="c-40267142">[-]</label><label class="expand" for="c-40267142">[1 more]</label></div><br/><div class="children"><div class="content"><i>There is no AGI without violence, its part of being free thinking and self survival.</i><p>Self survival idea is a part of natural selection, AGI doesn&#x27;t have to have it. Maybe the problem is we are the only template to build AGI from, but that&#x27;s not inherent to &quot;I&quot; in any way. Otoh, lack of self preservation can make animals even more ferocious. Also there&#x27;s a reason they often leave a retreat path in warzones.<p>Long story short it&#x27;s not that straightforward, so I sort of agree cause it&#x27;s an uncharted defaults-lacking territory we&#x27;ll have to explore. &quot;Unlearn bad&quot; is as naive as not telling your kids about sex and drugs.</div><br/></div></div><div id="40265336" class="c"><input type="checkbox" id="c-40265336" checked=""/><div class="controls bullet"><span class="by">sk11001</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40267142">prev</a><span>|</span><a href="#40267325">next</a><span>|</span><label class="collapse" for="c-40265336">[-]</label><label class="expand" for="c-40265336">[1 more]</label></div><br/><div class="children"><div class="content">The point is to build things that are useful, not to attempt to replicate science fiction literature.</div><br/></div></div><div id="40267325" class="c"><input type="checkbox" id="c-40267325" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40265336">prev</a><span>|</span><a href="#40270366">next</a><span>|</span><label class="collapse" for="c-40267325">[-]</label><label class="expand" for="c-40267325">[1 more]</label></div><br/><div class="children"><div class="content">AI has no concept of children, family, or nation. It doesn&#x27;t have parental love or offspring protection instinct. Faced with danger to its children it cannot choose between fighting or sacrificing itself in order to protect others.  What it is good at is capturing value through destruction of value generated by existing business models; it does it by perpetrating mass theft of other people&#x27;s IP.</div><br/></div></div><div id="40270366" class="c"><input type="checkbox" id="c-40270366" checked=""/><div class="controls bullet"><span class="by">Jaygles</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40267325">prev</a><span>|</span><a href="#40265515">next</a><span>|</span><label class="collapse" for="c-40270366">[-]</label><label class="expand" for="c-40270366">[1 more]</label></div><br/><div class="children"><div class="content">Because corporations won&#x27;t buy the fancy chat bot if there&#x27;s a chance it will occasionally use slurs in it&#x27;s interactions with their customers.</div><br/></div></div><div id="40265515" class="c"><input type="checkbox" id="c-40265515" checked=""/><div class="controls bullet"><span class="by">542458</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40270366">prev</a><span>|</span><a href="#40265212">next</a><span>|</span><label class="collapse" for="c-40265515">[-]</label><label class="expand" for="c-40265515">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There is no AGI without violence, its part of being free thinking and self survival.<p>I disagree. Are committed pacifists not in possession of general intelligence?</div><br/></div></div><div id="40265212" class="c"><input type="checkbox" id="c-40265212" checked=""/><div class="controls bullet"><span class="by">doubloon</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40265515">prev</a><span>|</span><a href="#40265347">next</a><span>|</span><label class="collapse" for="c-40265212">[-]</label><label class="expand" for="c-40265212">[4 more]</label></div><br/><div class="children"><div class="content">AGI would not beGI unless it could change its mind after realizing its wrong about something</div><br/><div id="40265299" class="c"><input type="checkbox" id="c-40265299" checked=""/><div class="controls bullet"><span class="by">542458</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40265212">parent</a><span>|</span><a href="#40265347">next</a><span>|</span><label class="collapse" for="c-40265299">[-]</label><label class="expand" for="c-40265299">[3 more]</label></div><br/><div class="children"><div class="content">I disagree. People with anterograde amnesia still possess general intelligence.</div><br/><div id="40265459" class="c"><input type="checkbox" id="c-40265459" checked=""/><div class="controls bullet"><span class="by">saintfire</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40265299">parent</a><span>|</span><a href="#40265347">next</a><span>|</span><label class="collapse" for="c-40265459">[-]</label><label class="expand" for="c-40265459">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know I ton about amnesia, but I would think the facilities for changing their mind are still there.<p>E.g. ordering food, they might immediately change their mind after choosing something and correct their order.<p>I recognize they cannot form new memories but from what I understand they still would have a working memory, otherwise you&#x27;d be virtually unable to think and speak.</div><br/><div id="40270357" class="c"><input type="checkbox" id="c-40270357" checked=""/><div class="controls bullet"><span class="by">542458</span><span>|</span><a href="#40264799">root</a><span>|</span><a href="#40265459">parent</a><span>|</span><a href="#40265347">next</a><span>|</span><label class="collapse" for="c-40270357">[-]</label><label class="expand" for="c-40270357">[1 more]</label></div><br/><div class="children"><div class="content">LLMs will change their minds today. Most major ones can change their minds on subsequent generations within the same context (“I’m sorry, my previous answer was incorrect,..”), and the biggest ones can change their mind mid-answer (mostly observed with GPT4).</div><br/></div></div></div></div></div></div></div></div><div id="40265347" class="c"><input type="checkbox" id="c-40265347" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40265212">prev</a><span>|</span><a href="#40264869">next</a><span>|</span><label class="collapse" for="c-40265347">[-]</label><label class="expand" for="c-40265347">[1 more]</label></div><br/><div class="children"><div class="content">Thanks but no violent AGIs thanks</div><br/></div></div><div id="40264869" class="c"><input type="checkbox" id="c-40264869" checked=""/><div class="controls bullet"><span class="by">Cheer2171</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40265347">prev</a><span>|</span><a href="#40265573">next</a><span>|</span><label class="collapse" for="c-40264869">[-]</label><label class="expand" for="c-40264869">[1 more]</label></div><br/><div class="children"><div class="content">So you have a problem with supervised learning like spam classifiers?</div><br/></div></div><div id="40265573" class="c"><input type="checkbox" id="c-40265573" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40264799">parent</a><span>|</span><a href="#40264869">prev</a><span>|</span><label class="collapse" for="c-40265573">[-]</label><label class="expand" for="c-40265573">[1 more]</label></div><br/><div class="children"><div class="content">You seem to be ignoring the potential to use this to improve the performance of LLMs. If you can unlearn wrong answers you can ask the model using any scoring mechanism to check for correctness instead of scoring for token for token similarity to the prescribed answer.</div><br/></div></div></div></div></div></div></div></div></div></body></html>