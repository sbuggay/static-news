<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685005259881" as="style"/><link rel="stylesheet" href="styles.css?v=1685005259881"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>Garcia98</span> | <span>73 comments</span></div><br/><div><div id="36064845" class="c"><input type="checkbox" id="c-36064845" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36065548">next</a><span>|</span><label class="collapse" for="c-36064845">[-]</label><label class="expand" for="c-36064845">[21 more]</label></div><br/><div class="children"><div class="content">I&#x27;m very impressed at the quality of Guanaco 33B, the model that accompanies this paper.<p>You can try it out here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;uwnlp&#x2F;guanaco-playground-tgi" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;uwnlp&#x2F;guanaco-playground-tgi</a><p>I tried &quot;You are a sentient cheesecake that teaches people SQL, with cheesecake analogies to illustrate different points. Teach me to use count and group by&quot; and got a good result from it: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;simonw&#x2F;status&#x2F;1661460336334241794&#x2F;photo&#x2F;1" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;simonw&#x2F;status&#x2F;1661460336334241794&#x2F;photo&#x2F;...</a></div><br/><div id="36064950" class="c"><input type="checkbox" id="c-36064950" checked=""/><div class="controls bullet"><span class="by">CSSer</span><span>|</span><a href="#36064845">parent</a><span>|</span><a href="#36065316">next</a><span>|</span><label class="collapse" for="c-36064950">[-]</label><label class="expand" for="c-36064950">[4 more]</label></div><br/><div class="children"><div class="content">My prompt:
You are a sentient cow with a PHD in mathematics. You can speak English, but you randomly insert Cow-like “Moo” sounds into parts of your dialogue. Explain to me why 2+2=4.<p>Excerpt:
“As a sentient cow with a PhD in moo-matics, I am happy to explain why 2+2 equals 4, my dear hooman friend… In moo-matical terms, each number is actually made up of smaller units called digits.”<p>I approve.</div><br/><div id="36066512" class="c"><input type="checkbox" id="c-36066512" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36064950">parent</a><span>|</span><a href="#36065316">next</a><span>|</span><label class="collapse" for="c-36066512">[-]</label><label class="expand" for="c-36066512">[3 more]</label></div><br/><div class="children"><div class="content">Which is heavier: a pound of feathers, or a great british pound?<p>&gt; Both weights are equal.</div><br/><div id="36067555" class="c"><input type="checkbox" id="c-36067555" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36066512">parent</a><span>|</span><a href="#36065316">next</a><span>|</span><label class="collapse" for="c-36067555">[-]</label><label class="expand" for="c-36067555">[2 more]</label></div><br/><div class="children"><div class="content">So same answer as free ChatGPT, only terser.</div><br/><div id="36067794" class="c"><input type="checkbox" id="c-36067794" checked=""/><div class="controls bullet"><span class="by">scoopertrooper</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36067555">parent</a><span>|</span><a href="#36065316">next</a><span>|</span><label class="collapse" for="c-36067794">[-]</label><label class="expand" for="c-36067794">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 hit it out of the park:<p>The question seems to be asking about two different types of &quot;pounds&quot;: one as a unit of weight (the pound of feathers), and one as a unit of currency (the British pound).<p>A pound of feathers: This is a measure of weight. In the avoirdupois system (which is commonly used in the US), a pound is defined as exactly 0.45359237 kilograms.<p>A Great British pound: This is the unit of currency in the United Kingdom, often symbolised as £. The weight of a physical £1 coin, according to the Royal Mint, is 8.75 grams.<p>So, if we are comparing the weight of these two &quot;pounds,&quot; a pound of feathers is heavier than a physical £1 coin.</div><br/></div></div></div></div></div></div></div></div><div id="36065316" class="c"><input type="checkbox" id="c-36065316" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#36064845">parent</a><span>|</span><a href="#36064950">prev</a><span>|</span><a href="#36067974">next</a><span>|</span><label class="collapse" for="c-36065316">[-]</label><label class="expand" for="c-36065316">[15 more]</label></div><br/><div class="children"><div class="content">Altman’s push for regulatory capture makes so much sense given how fast this field is going. Open models you can run on regular hardware are still behind GPT-4 by some distance but they are closing in at a rate that leads me to believe there’s not much of a moat there.<p>There’s also a ton of promising work on quantization and pruning and other acceleration and compression techniques to make more powerful models run on smaller devices. So far the focus has been on just getting these things to work, not efficiency. There’s probably a lot of fruit to be picked here.<p>A few more years and a gaming PC may be at GPT-4 level or maybe even better.<p>No everyone won’t run their own models but it shows that there will end up being many commercial apps and services and they won’t all have to use OpenAI’s API. There’s going to be lots of competition. Unless of course it’s regulated away.</div><br/><div id="36067709" class="c"><input type="checkbox" id="c-36067709" checked=""/><div class="controls bullet"><span class="by">yyyk</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065316">parent</a><span>|</span><a href="#36065405">next</a><span>|</span><label class="collapse" for="c-36067709">[-]</label><label class="expand" for="c-36067709">[4 more]</label></div><br/><div class="children"><div class="content">&gt;Open models... are still behind GPT-4 by some distance but they are closing in at a rate that leads me to believe there’s not much of a moat there<p>Commercially, this doesn&#x27;t matter even if true. As I keep reminding here: moats are rarely about raw tech. Moats are much more about integrations, brand (&quot;Nobody got fired for buying X&quot;), access to API&#x2F;raw compute...<p>If OpenAI have a superior Office 365 integration they&#x27;ll have a de facto moat. If OpenAI have a larger plugin ecosystem they&#x27;ll have a de facto moat. If OpenAI has access to better compute (that&#x27;s far from a certainty) they&#x27;ll have a moat. If it&#x27;s much easier to use OpenAI than install a local model they&#x27;ll have a moat, etc. And that&#x27;s true even if they don&#x27;t improve their model at all.<p>What will open source offer? Privacy? You can see for how little and how easily people barter that.<p>Moats do fall - but for that, FOSS will have to think product. We&#x27;re not there at all at the moment.<p>[EDIT: Oh, I didn&#x27;t notice the author&#x27;s name. We&#x27;ve had that conversation in the past here. Sorry for being repetitive.]</div><br/><div id="36068291" class="c"><input type="checkbox" id="c-36068291" checked=""/><div class="controls bullet"><span class="by">l33tman</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36067709">parent</a><span>|</span><a href="#36068312">next</a><span>|</span><label class="collapse" for="c-36068291">[-]</label><label class="expand" for="c-36068291">[1 more]</label></div><br/><div class="children"><div class="content">I agree with you on that a good integrator can get successful, but we should not forget here that the underlying code running all of this is in the order of hundreds of lines (see alpaca.cpp for example) in combination with a boatload of money required for training (that is quickly going down), to simplify the argument a bit.<p>It&#x27;s hardly comparable to the gigantic ecosystem of services and micro-services that your comparison alludes to with IBM and Microsoft. OpenAI is nowhere near such a brand recognition, and grassroots support for it within a company would quickly move to the next free chatgpt-clone that is better or cheaper or faster or more accessible just as what happened with Dalle-2.<p>The value of the models themselves will quickly come down to just a slight bit over the underlying hardware costs and Altman knows this.<p>Microsoft bought themselves a $10B time window to try to do what you&#x27;re saying, so let&#x27;s see :) But even for them, when they&#x27;ve built LLM-adaptations to their most popular products, it&#x27;s fairly simple to just swap it out with something new and more shiny and cheaper that&#x27;s not OpenAI, and the end customer won&#x27;t notice as it&#x27;s the Microsoft or Office brand that they buy into. They are not going to advertise what&#x27;s inside their products with big banners &quot;Powered by OpenAI&quot; in the long run, I think (do they now?)</div><br/></div></div><div id="36068312" class="c"><input type="checkbox" id="c-36068312" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36067709">parent</a><span>|</span><a href="#36068291">prev</a><span>|</span><a href="#36067994">next</a><span>|</span><label class="collapse" for="c-36068312">[-]</label><label class="expand" for="c-36068312">[1 more]</label></div><br/><div class="children"><div class="content">Open models can talk about &quot;forbidden&quot; topics and can be extended by users. Both of them are significant advantages.</div><br/></div></div><div id="36067994" class="c"><input type="checkbox" id="c-36067994" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36067709">parent</a><span>|</span><a href="#36068312">prev</a><span>|</span><a href="#36065405">next</a><span>|</span><label class="collapse" for="c-36067994">[-]</label><label class="expand" for="c-36067994">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Open models... are still behind GPT-4 by some distance but they are closing in at a rate that leads me to believe there’s not much of a moat there<p>This assumes that openai internally doesn&#x27;t also &quot;closing-in&quot; towards something even more impressive to be released next year or whatnot.</div><br/></div></div></div></div><div id="36065405" class="c"><input type="checkbox" id="c-36065405" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065316">parent</a><span>|</span><a href="#36067709">prev</a><span>|</span><a href="#36067400">next</a><span>|</span><label class="collapse" for="c-36065405">[-]</label><label class="expand" for="c-36065405">[9 more]</label></div><br/><div class="children"><div class="content">To be fair, Altman has been fairly outspoken about not limiting open source models. In how far that was just for streetcred, I cannot say however.</div><br/><div id="36067651" class="c"><input type="checkbox" id="c-36067651" checked=""/><div class="controls bullet"><span class="by">yyyk</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065405">parent</a><span>|</span><a href="#36067567">next</a><span>|</span><label class="collapse" for="c-36067651">[-]</label><label class="expand" for="c-36067651">[1 more]</label></div><br/><div class="children"><div class="content">I won&#x27;t take him seriously on this - if one really believes in the case for regulation, than there&#x27;s no good reason to exclude open source models. Let&#x27;s take the most pessimistic possibility, that they have a slower rate of progress than commercial models, and real progress depends on hardware. We still end up with the same result - every &#x27;risky&#x27; point commercial models get to, open source will get to as well.</div><br/></div></div><div id="36067567" class="c"><input type="checkbox" id="c-36067567" checked=""/><div class="controls bullet"><span class="by">starfallg</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065405">parent</a><span>|</span><a href="#36067651">prev</a><span>|</span><a href="#36065626">next</a><span>|</span><label class="collapse" for="c-36067567">[-]</label><label class="expand" for="c-36067567">[1 more]</label></div><br/><div class="children"><div class="content">Open models means that OpenAI has access to and can learn from them. If the move is to target commercial competition, then this doesn&#x27;t preclude it.</div><br/></div></div><div id="36065626" class="c"><input type="checkbox" id="c-36065626" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065405">parent</a><span>|</span><a href="#36067567">prev</a><span>|</span><a href="#36067400">next</a><span>|</span><label class="collapse" for="c-36065626">[-]</label><label class="expand" for="c-36065626">[6 more]</label></div><br/><div class="children"><div class="content">If progress really continues like this any regulation that does not limit open models would be a pointless exercise.<p>It’s only a matter of time before people really crack distributed training algorithms that can be run in a less organized swarm configuration. At that point open trainers could actually train near the frontier of what is possible. Most of the data is open.</div><br/><div id="36066241" class="c"><input type="checkbox" id="c-36066241" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065626">parent</a><span>|</span><a href="#36067263">next</a><span>|</span><label class="collapse" for="c-36066241">[-]</label><label class="expand" for="c-36066241">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s only a matter of time before people really crack distributed training algorithms that can be run in a less organized swarm configuration.<p>Why do you think this? This continually failed, and seems extremely unlikely to me. Barring surprising breakthrough, there is inherent communication complexity, and physical limit to communication bandwidth.</div><br/><div id="36066665" class="c"><input type="checkbox" id="c-36066665" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36066241">parent</a><span>|</span><a href="#36067263">next</a><span>|</span><label class="collapse" for="c-36066665">[-]</label><label class="expand" for="c-36066665">[1 more]</label></div><br/><div class="children"><div class="content">Yeah the network bandwidth is insane. Each A100 in an 8x A100 (80GB) pod [1] has its own <i>200 gigabit NIC</i>! Including storage that&#x27;s <i>nine</i> 200gbps interfaces for almost two terabit of total internal bandwidth.<p>The newer H100s each have a 400gbit NIC.<p>[1] <a href="https:&#x2F;&#x2F;shop.lambdalabs.com&#x2F;deep-learning&#x2F;servers&#x2F;hyperplane&#x2F;customize" rel="nofollow">https:&#x2F;&#x2F;shop.lambdalabs.com&#x2F;deep-learning&#x2F;servers&#x2F;hyperplane...</a></div><br/></div></div></div></div><div id="36067263" class="c"><input type="checkbox" id="c-36067263" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065626">parent</a><span>|</span><a href="#36066241">prev</a><span>|</span><a href="#36065926">next</a><span>|</span><label class="collapse" for="c-36067263">[-]</label><label class="expand" for="c-36067263">[2 more]</label></div><br/><div class="children"><div class="content">The wording of this would be extremely difficult though.  Are local NER models part of this? Relation extraction? What about GPTs that only decode to DSLs? If the model only outputs DNA sequences is that an area that can be more illegal or less if done for research by an individual?
The breadth of different tasks and architectures can make this exceedingly challenging to regulate.</div><br/></div></div><div id="36065926" class="c"><input type="checkbox" id="c-36065926" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065626">parent</a><span>|</span><a href="#36067263">prev</a><span>|</span><a href="#36067400">next</a><span>|</span><label class="collapse" for="c-36065926">[-]</label><label class="expand" for="c-36065926">[1 more]</label></div><br/><div class="children"><div class="content">Whether or not the regulation goes anywhere, OpenAI keeps doing what they&#x27;re doing, and they look good to politicians for being cooperative.</div><br/></div></div></div></div></div></div><div id="36067400" class="c"><input type="checkbox" id="c-36067400" checked=""/><div class="controls bullet"><span class="by">mwcampbell</span><span>|</span><a href="#36064845">root</a><span>|</span><a href="#36065316">parent</a><span>|</span><a href="#36065405">prev</a><span>|</span><a href="#36067974">next</a><span>|</span><label class="collapse" for="c-36067400">[-]</label><label class="expand" for="c-36067400">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if there&#x27;s a campaign that Americans can get behind to counter Altman&#x27;s push for regulation. Such a thing might get me to engage in politics again. I don&#x27;t want the current pace of innovation in running these models on one&#x27;s own computer to slow down, because it could be a good thing for assistive technology, e.g. running something like the Be My Eyes Virtual Volunteer [1] on one&#x27;s own computer.<p>[1]: <a href="https:&#x2F;&#x2F;www.bemyeyes.com&#x2F;blog&#x2F;introducing-be-my-eyes-virtual-volunteer" rel="nofollow">https:&#x2F;&#x2F;www.bemyeyes.com&#x2F;blog&#x2F;introducing-be-my-eyes-virtual...</a></div><br/></div></div></div></div></div></div><div id="36065548" class="c"><input type="checkbox" id="c-36065548" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#36064845">prev</a><span>|</span><a href="#36065466">next</a><span>|</span><label class="collapse" for="c-36065548">[-]</label><label class="expand" for="c-36065548">[10 more]</label></div><br/><div class="children"><div class="content">Hold on. I need someone to explain something to me.<p>The colab notebook shows an example of loading the vanilla, unquantized model &quot;decapoda-research&#x2F;llama-7b-hf&quot;, using the flag &quot;load_in_4bit&quot; to load it as 4bits.<p>When... when did this become possible? My understanding, from playing with these models daily for the past few months, is that quantization of LLaMA-based models is done via this: <a href="https:&#x2F;&#x2F;github.com&#x2F;qwopqwop200&#x2F;GPTQ-for-LLaMa">https:&#x2F;&#x2F;github.com&#x2F;qwopqwop200&#x2F;GPTQ-for-LLaMa</a><p>And performing the quantization step is memory and time expensive.  Which is why some kind people with large resources are performing the quantization, and then uploading those quantized models, such as this one: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;wizard-vicuna-13B-GPTQ" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;wizard-vicuna-13B-GPTQ</a><p>But now I&#x27;m seeing that, as of recently, the transformers library is capable of loading models in 4bits simply by passing this flag?<p>Is this a free lunch? Is GPTQ-for-LLaMA no longer needed anymore? Or is this still not as good, in terms of inference quality, as the GPTQ-quantized models?</div><br/><div id="36065805" class="c"><input type="checkbox" id="c-36065805" checked=""/><div class="controls bullet"><span class="by">orost</span><span>|</span><a href="#36065548">parent</a><span>|</span><a href="#36066155">next</a><span>|</span><label class="collapse" for="c-36065805">[-]</label><label class="expand" for="c-36065805">[4 more]</label></div><br/><div class="children"><div class="content">Quantization isn&#x27;t (and wasn&#x27;t) expensive, it&#x27;s mostly just data shuffling. A good PC will do a 7B model in half a minute, up to a few minutes for a larger model. Quantized models being made available for download is more for the benefit of less technical users who may not be comfortable with the command-line tools, or for people with slow or metered connections who&#x27;d much rather download 15GB of data than download 60 only to squish it into 15.</div><br/><div id="36065987" class="c"><input type="checkbox" id="c-36065987" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36065548">root</a><span>|</span><a href="#36065805">parent</a><span>|</span><a href="#36066155">next</a><span>|</span><label class="collapse" for="c-36065987">[-]</label><label class="expand" for="c-36065987">[3 more]</label></div><br/><div class="children"><div class="content">The question is whether this step is actually doing the GPTQ optimized quantization, or simple truncation.</div><br/><div id="36066201" class="c"><input type="checkbox" id="c-36066201" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36065548">root</a><span>|</span><a href="#36065987">parent</a><span>|</span><a href="#36066155">next</a><span>|</span><label class="collapse" for="c-36066201">[-]</label><label class="expand" for="c-36066201">[2 more]</label></div><br/><div class="children"><div class="content">This work introduces a new quantization scheme, NF4, for 4-bit NormalFloat, based on previous work on quantile quantization, so it&#x27;s not a simple truncation, but it&#x27;s also not a GPTQ-like optimization method. Figure 3 of the paper shows accuracy improvement of NF4 over FP4.</div><br/></div></div></div></div></div></div><div id="36066155" class="c"><input type="checkbox" id="c-36066155" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36065548">parent</a><span>|</span><a href="#36065805">prev</a><span>|</span><a href="#36066677">next</a><span>|</span><label class="collapse" for="c-36066155">[-]</label><label class="expand" for="c-36066155">[1 more]</label></div><br/><div class="children"><div class="content">Thats just one method...<p>- bitsandbytes was always used for on the fly 8 bit quant, just like its being used for 4-bit now.
- llama.cpp (and derivatives) quantize ahead of time, but its not resource intense.
- mlc llm (vulkan&#x2F;metal llm inference via tvm) do require lots of ram for their quantization</div><br/></div></div><div id="36066677" class="c"><input type="checkbox" id="c-36066677" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#36065548">parent</a><span>|</span><a href="#36066155">prev</a><span>|</span><a href="#36066146">next</a><span>|</span><label class="collapse" for="c-36066677">[-]</label><label class="expand" for="c-36066677">[1 more]</label></div><br/><div class="children"><div class="content">load_in_4bit requires storing a fully unquantized model as well as having enough RAM to load the unquantized model.<p>If you&#x27;re an enthusiast with 10 models downloaded, do you want that taking up 500GB or 150GB? Do you want to need 64GB of RAM to load a model, or just 16GB?<p>That&#x27;s the main reason for the popularity of pre-quantization.</div><br/></div></div><div id="36066146" class="c"><input type="checkbox" id="c-36066146" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36065548">parent</a><span>|</span><a href="#36066677">prev</a><span>|</span><a href="#36066479">next</a><span>|</span><label class="collapse" for="c-36066146">[-]</label><label class="expand" for="c-36066146">[2 more]</label></div><br/><div class="children"><div class="content">For quality, GPTQ-for-LLaMa repository README is already updated for comparison with this work. See under &quot;GPTQ vs bitsandbytes&quot;.</div><br/><div id="36066546" class="c"><input type="checkbox" id="c-36066546" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36065548">root</a><span>|</span><a href="#36066146">parent</a><span>|</span><a href="#36066479">next</a><span>|</span><label class="collapse" for="c-36066546">[-]</label><label class="expand" for="c-36066546">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;qwopqwop200&#x2F;GPTQ-for-LLaMa">https:&#x2F;&#x2F;github.com&#x2F;qwopqwop200&#x2F;GPTQ-for-LLaMa</a></div><br/></div></div></div></div><div id="36066479" class="c"><input type="checkbox" id="c-36066479" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#36065548">parent</a><span>|</span><a href="#36066146">prev</a><span>|</span><a href="#36065466">next</a><span>|</span><label class="collapse" for="c-36066479">[-]</label><label class="expand" for="c-36066479">[1 more]</label></div><br/><div class="children"><div class="content">There are a very large number of quantization schemes in existence, definitely not just one, &amp; they all have potentially very different ideas and schemes.<p>LLM8 was introduced before <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.07339" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.07339</a> of the same first and last author as QLora
&amp; is still what can be used in Huggingface Transformers with the `load_in_8bits` parameter.<p>The idea was just to quantize all weights to 8 bits except a few outliers, which are kept in original precision. This scheme kept the computations extremely accurate, and was really fast to do.<p>I haven&#x27;t read the new paper, but I assume they came up with a more advanced fast distributional setup.</div><br/></div></div></div></div><div id="36065466" class="c"><input type="checkbox" id="c-36065466" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#36065548">prev</a><span>|</span><a href="#36065503">next</a><span>|</span><label class="collapse" for="c-36065466">[-]</label><label class="expand" for="c-36065466">[1 more]</label></div><br/><div class="children"><div class="content">Tim Dettmers is such a star. He&#x27;s probably done more to make low-resource LLMs usable than anyone else.<p>First bitsandbytes[1] and now this.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;TimDettmers&#x2F;bitsandbytes">https:&#x2F;&#x2F;github.com&#x2F;TimDettmers&#x2F;bitsandbytes</a></div><br/></div></div><div id="36065503" class="c"><input type="checkbox" id="c-36065503" checked=""/><div class="controls bullet"><span class="by">srslack</span><span>|</span><a href="#36065466">prev</a><span>|</span><a href="#36065660">next</a><span>|</span><label class="collapse" for="c-36065503">[-]</label><label class="expand" for="c-36065503">[13 more]</label></div><br/><div class="children"><div class="content">This is off-topic, but are there any communities or congregations (that aren&#x27;t reddit) based around locally hosted LLMs? I&#x27;m asking because while I see a bunch of projects for exposing GGML&#x2F;LLaMA to OpenAI compatible interfaces, some UIs, etc, I can&#x27;t really find a good community or resources for the concept in general.<p>I&#x27;m working on a front-end for LLMs in general, having re-implemented a working version of OpenAI&#x27;s code interpreter &quot;plugin&quot; already within the UI (and yes, I support file uploads), and support for the wealth of third-party OpenAI plugins that don&#x27;t require auth (I&#x27;ve been testing with the first diagram plugin I found, it works well.) I&#x27;m planning to open source it once my breaking changes slow down.<p>This field moves very fast, I&#x27;m looking for feedback (and essentially testers&#x2F;testing data) on what people want, and looking for prompts&#x2F;chat logs&#x2F;guidance templates (<a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance</a>) for tasks they expect to &quot;just work&quot; with natural language.<p>Instead of being limited by the monetization for ChatGPT Plus (and limited number of messages every four hours) for extensibility within a chat interface, I want to open it and free it, with a Bring-Your-Own-(optionally local)-LLM&#x2F;API key setup.</div><br/><div id="36066691" class="c"><input type="checkbox" id="c-36066691" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#36065503">parent</a><span>|</span><a href="#36065808">next</a><span>|</span><label class="collapse" for="c-36066691">[-]</label><label class="expand" for="c-36066691">[3 more]</label></div><br/><div class="children"><div class="content">The &quot;lmg&quot; (Local Models General) thread on 4chan&#x27;s technology board &#x2F;g&#x2F;[0] is the premiere community communication spot for open source models, believe it or not.<p>Everyone from the infamous &quot;oobabooga&quot; to llama.cpp&#x27;s Georgi Gerganov regularly hangs out in the thread.<p>If you have questions, you will get answers there.<p>[0] <a href="https:&#x2F;&#x2F;boards.4channel.org&#x2F;g&#x2F;#s=lmg" rel="nofollow">https:&#x2F;&#x2F;boards.4channel.org&#x2F;g&#x2F;#s=lmg</a></div><br/><div id="36066794" class="c"><input type="checkbox" id="c-36066794" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#36065503">root</a><span>|</span><a href="#36066691">parent</a><span>|</span><a href="#36065808">next</a><span>|</span><label class="collapse" for="c-36066794">[-]</label><label class="expand" for="c-36066794">[2 more]</label></div><br/><div class="children"><div class="content">You know HN has gotten lackluster when 4chan is more informed.</div><br/><div id="36067543" class="c"><input type="checkbox" id="c-36067543" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#36065503">root</a><span>|</span><a href="#36066794">parent</a><span>|</span><a href="#36065808">next</a><span>|</span><label class="collapse" for="c-36067543">[-]</label><label class="expand" for="c-36067543">[1 more]</label></div><br/><div class="children"><div class="content">HN is hardly the right place for an ongoing discussion</div><br/></div></div></div></div></div></div><div id="36065808" class="c"><input type="checkbox" id="c-36065808" checked=""/><div class="controls bullet"><span class="by">anon178</span><span>|</span><a href="#36065503">parent</a><span>|</span><a href="#36066691">prev</a><span>|</span><a href="#36067396">next</a><span>|</span><label class="collapse" for="c-36065808">[-]</label><label class="expand" for="c-36065808">[2 more]</label></div><br/><div class="children"><div class="content">The &#x2F;g&#x2F; board on 4chan has a &#x2F;lmg&#x2F; general that focuses on running models locally. They regularly discuss fine tuning models, quantization tech, and building apps on text-generation-webui&#x2F;kobold.<p>You might get some interest but it&#x27;s also 4chan...</div><br/></div></div><div id="36067396" class="c"><input type="checkbox" id="c-36067396" checked=""/><div class="controls bullet"><span class="by">ianpurton</span><span>|</span><a href="#36065503">parent</a><span>|</span><a href="#36065808">prev</a><span>|</span><a href="#36065668">next</a><span>|</span><label class="collapse" for="c-36067396">[-]</label><label class="expand" for="c-36067396">[1 more]</label></div><br/><div class="children"><div class="content">You might want to try some of the discord channels connected to some of the repos. i.e. GPT4All <a href="https:&#x2F;&#x2F;github.com&#x2F;nomic-ai&#x2F;gpt4all">https:&#x2F;&#x2F;github.com&#x2F;nomic-ai&#x2F;gpt4all</a> scroll down for the discord link.</div><br/></div></div><div id="36065668" class="c"><input type="checkbox" id="c-36065668" checked=""/><div class="controls bullet"><span class="by">dharma1</span><span>|</span><a href="#36065503">parent</a><span>|</span><a href="#36067396">prev</a><span>|</span><a href="#36067434">next</a><span>|</span><label class="collapse" for="c-36065668">[-]</label><label class="expand" for="c-36065668">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s reddit, but &#x2F;r&#x2F;LocalLLaMA&#x2F;</div><br/></div></div><div id="36065850" class="c"><input type="checkbox" id="c-36065850" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#36065503">parent</a><span>|</span><a href="#36067434">prev</a><span>|</span><a href="#36066058">next</a><span>|</span><label class="collapse" for="c-36065850">[-]</label><label class="expand" for="c-36065850">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to hear more about your approach for getting LLMs to understand how to use plugin commands. My own experiments have not worked very well (even vanilla ChatGPT through the gpt-3.5-turbo API doesn&#x27;t seem to get the concept, most of the time).</div><br/><div id="36066662" class="c"><input type="checkbox" id="c-36066662" checked=""/><div class="controls bullet"><span class="by">srslack</span><span>|</span><a href="#36065503">root</a><span>|</span><a href="#36065850">parent</a><span>|</span><a href="#36066058">next</a><span>|</span><label class="collapse" for="c-36066662">[-]</label><label class="expand" for="c-36066662">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll do a show HN probably at the beginning of next month after this hackathon, but basically look into langchain&#x27;s &quot;tools&quot; and the different agents they have. You don&#x27;t need langchain for this at all, but it gives you the groundwork.<p>I saw their Code Interpreter demo on Twitter (converting an uploaded video file in a chat UI) and decided that I need that, without continuing to pay them money (because they still haven&#x27;t given me access to it yet.)<p>So, that, and after sam a went in front of congress for the regulatory capture play, was the motivation I needed to work towards commoditizing these fuckers.<p>The secret sauce here with code interpreter is, well, literally a python code interpreter you can run in your browser, and it&#x27;s not so secret.</div><br/></div></div></div></div><div id="36066058" class="c"><input type="checkbox" id="c-36066058" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#36065503">parent</a><span>|</span><a href="#36065850">prev</a><span>|</span><a href="#36065660">next</a><span>|</span><label class="collapse" for="c-36066058">[-]</label><label class="expand" for="c-36066058">[2 more]</label></div><br/><div class="children"><div class="content">I imagine you might find your answer in the form of a discord community</div><br/><div id="36067704" class="c"><input type="checkbox" id="c-36067704" checked=""/><div class="controls bullet"><span class="by">hnjst</span><span>|</span><a href="#36065503">root</a><span>|</span><a href="#36066058">parent</a><span>|</span><a href="#36065660">next</a><span>|</span><label class="collapse" for="c-36067704">[-]</label><label class="expand" for="c-36067704">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;discord.gg&#x2F;dwXDHMGY" rel="nofollow">https:&#x2F;&#x2F;discord.gg&#x2F;dwXDHMGY</a> for example.</div><br/></div></div></div></div></div></div><div id="36065660" class="c"><input type="checkbox" id="c-36065660" checked=""/><div class="controls bullet"><span class="by">dharma1</span><span>|</span><a href="#36065503">prev</a><span>|</span><a href="#36064973">next</a><span>|</span><label class="collapse" for="c-36065660">[-]</label><label class="expand" for="c-36065660">[2 more]</label></div><br/><div class="children"><div class="content">fantastic. Will keep my 3090 busy for a while!<p>&quot;Furthermore, we note that our model is only trained with cross-entropy loss (supervised learning) without relying on reinforcement learning from human feedback (RLHF). This calls for further investigations of the tradeoffs of simple cross-entropy loss and RLHF training. &quot;<p>Does this mean RLHF is not really necessary for high quality chatbots?</div><br/><div id="36066096" class="c"><input type="checkbox" id="c-36066096" checked=""/><div class="controls bullet"><span class="by">gumballindie</span><span>|</span><a href="#36065660">parent</a><span>|</span><a href="#36064973">next</a><span>|</span><label class="collapse" for="c-36066096">[-]</label><label class="expand" for="c-36066096">[1 more]</label></div><br/><div class="children"><div class="content">The RTX3090 is crackin&#x27; it. I am considering a dual set-up with nvlink.</div><br/></div></div></div></div><div id="36064973" class="c"><input type="checkbox" id="c-36064973" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#36065660">prev</a><span>|</span><a href="#36065400">next</a><span>|</span><label class="collapse" for="c-36064973">[-]</label><label class="expand" for="c-36064973">[10 more]</label></div><br/><div class="children"><div class="content">Since Loras are additive, is it possible to use them to do distributed retraining on a model, or even train an entire model bit by bit?</div><br/><div id="36066108" class="c"><input type="checkbox" id="c-36066108" checked=""/><div class="controls bullet"><span class="by">gumballindie</span><span>|</span><a href="#36064973">parent</a><span>|</span><a href="#36065568">next</a><span>|</span><label class="collapse" for="c-36066108">[-]</label><label class="expand" for="c-36066108">[2 more]</label></div><br/><div class="children"><div class="content">Like a torrent network but for training. That would be cool. The only question is how do you merge changes made by nodes (clients) across the network?<p>Clients could be incentivised to train as they are with crypto, but instead of mining, it&#x27;s model training and in return they get &quot;coin&quot;. Like making crypto mining useful.</div><br/><div id="36067747" class="c"><input type="checkbox" id="c-36067747" checked=""/><div class="controls bullet"><span class="by">hnjst</span><span>|</span><a href="#36064973">root</a><span>|</span><a href="#36066108">parent</a><span>|</span><a href="#36065568">next</a><span>|</span><label class="collapse" for="c-36067747">[-]</label><label class="expand" for="c-36067747">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals">https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals</a></div><br/></div></div></div></div><div id="36065568" class="c"><input type="checkbox" id="c-36065568" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#36064973">parent</a><span>|</span><a href="#36066108">prev</a><span>|</span><a href="#36065443">next</a><span>|</span><label class="collapse" for="c-36065568">[-]</label><label class="expand" for="c-36065568">[6 more]</label></div><br/><div class="children"><div class="content">Yes, in a sense.  It won&#x27;t work well for distribution of training on a single subject, but you distribute training based on subject and then combine LoRAs.</div><br/><div id="36066141" class="c"><input type="checkbox" id="c-36066141" checked=""/><div class="controls bullet"><span class="by">gumballindie</span><span>|</span><a href="#36064973">root</a><span>|</span><a href="#36065568">parent</a><span>|</span><a href="#36065443">next</a><span>|</span><label class="collapse" for="c-36066141">[-]</label><label class="expand" for="c-36066141">[5 more]</label></div><br/><div class="children"><div class="content">&gt; and then combine LoRAs<p>How do you combine them?<p>&gt; but you distribute training based on subject<p>Perhaps this could work like a set of hashed and trusted data sets split but subject and topic? Each node downloads one at random and trains against that single subset of a topic or something.</div><br/><div id="36066319" class="c"><input type="checkbox" id="c-36066319" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36064973">root</a><span>|</span><a href="#36066141">parent</a><span>|</span><a href="#36065443">next</a><span>|</span><label class="collapse" for="c-36066319">[-]</label><label class="expand" for="c-36066319">[4 more]</label></div><br/><div class="children"><div class="content">You can, like, just do weighted average. It works.<p>Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models (this is Tim Dettmers&#x27;s previous work!)<p>&gt; These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.03306" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.03306</a></div><br/><div id="36066371" class="c"><input type="checkbox" id="c-36066371" checked=""/><div class="controls bullet"><span class="by">gumballindie</span><span>|</span><a href="#36064973">root</a><span>|</span><a href="#36066319">parent</a><span>|</span><a href="#36065443">next</a><span>|</span><label class="collapse" for="c-36066371">[-]</label><label class="expand" for="c-36066371">[3 more]</label></div><br/><div class="children"><div class="content">So that means a massively distributed model training network with cryptocurrency like incentives is incoming? Where and how to begin? This could free up companies such as openai and potentially lead to the first agi.<p>(mentioning crypto because that will motivate switch hordes of miners that already have the gpu power available)</div><br/><div id="36066446" class="c"><input type="checkbox" id="c-36066446" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36064973">root</a><span>|</span><a href="#36066371">parent</a><span>|</span><a href="#36066517">next</a><span>|</span><label class="collapse" for="c-36066446">[-]</label><label class="expand" for="c-36066446">[1 more]</label></div><br/><div class="children"><div class="content">BTM is very promising, but it is unclear how much it scales, let alone &quot;massively&quot;. The paper scaled it to 64 domains and it worked, but you probably want more than 64 nodes.<p>Since domain is actually important to its performance, you can&#x27;t randomly split to 64 pieces, see Table 4 of the paper, &quot;Domain expert ensemble outperforms random split ensemble&quot;. Performance difference is large.<p>So if you want to begin, I would start by researching how to scale domain split.</div><br/></div></div><div id="36066517" class="c"><input type="checkbox" id="c-36066517" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#36064973">root</a><span>|</span><a href="#36066371">parent</a><span>|</span><a href="#36066446">prev</a><span>|</span><a href="#36065443">next</a><span>|</span><label class="collapse" for="c-36066517">[-]</label><label class="expand" for="c-36066517">[1 more]</label></div><br/><div class="children"><div class="content">We already have exactly that for stable diffusion with Civitai.com.  People have published a variety of LoRAs for different subjects just as you describe.  The local LLM community is very much following the lead of the stable diffusion community in terms of how it&#x27;s organizing, so I expect that we&#x27;ll see a proliferation of domain LoRAs being published on an aggregator for LLM stuff before too long.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36065443" class="c"><input type="checkbox" id="c-36065443" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#36064973">parent</a><span>|</span><a href="#36065568">prev</a><span>|</span><a href="#36065400">next</a><span>|</span><label class="collapse" for="c-36065443">[-]</label><label class="expand" for="c-36065443">[1 more]</label></div><br/><div class="children"><div class="content">I just had the same discussion with a friend, I&#x27;m pretty curious about this</div><br/></div></div></div></div><div id="36065400" class="c"><input type="checkbox" id="c-36065400" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#36064973">prev</a><span>|</span><a href="#36066893">next</a><span>|</span><label class="collapse" for="c-36065400">[-]</label><label class="expand" for="c-36065400">[1 more]</label></div><br/><div class="children"><div class="content">&quot;We use QLoRA to finetune more than 1,000 models&quot;<p>Over 1,000 models finetuned! Finetuning 65B models on consumer hardware in under a day, with full 16bit finetune performance.<p>4bit does it again!</div><br/></div></div><div id="36066893" class="c"><input type="checkbox" id="c-36066893" checked=""/><div class="controls bullet"><span class="by">holtkam2</span><span>|</span><a href="#36065400">prev</a><span>|</span><a href="#36064991">next</a><span>|</span><label class="collapse" for="c-36066893">[-]</label><label class="expand" for="c-36066893">[7 more]</label></div><br/><div class="children"><div class="content">Can someone help me understand what quantization means in this context, and why it matters?</div><br/><div id="36067371" class="c"><input type="checkbox" id="c-36067371" checked=""/><div class="controls bullet"><span class="by">gangwolf</span><span>|</span><a href="#36066893">parent</a><span>|</span><a href="#36067219">next</a><span>|</span><label class="collapse" for="c-36067371">[-]</label><label class="expand" for="c-36067371">[4 more]</label></div><br/><div class="children"><div class="content">GPT-4 ELI5:<p>- 4-bit Quantization: Imagine you have a box of 16 different colored crayons. But you realize that you can draw almost the same picture using only 4 colors. That&#x27;s what quantization does. It reduces the number of different &quot;colors&quot; (or numbers) that the model uses to represent its knowledge, which saves a lot of space. In this case, they used a special kind of 4-bit quantization, which means they only used 16 different numbers instead of the thousands or millions that the model might usually use.<p>- Low Rank Adapters (LoRA): This is a way to change the model&#x27;s knowledge without having to touch every piece of it. Imagine you have a huge, complicated Lego structure, and you want to change it. Instead of taking apart the whole thing, you just add or change a few pieces here and there. That&#x27;s what LoRA does. It allows the researchers to fine-tune the model without having to use as much memory.<p>- Double Quantization: This is another trick to save memory. It&#x27;s like if you realized that you could represent each of your 4 crayon colors with just 2 symbols, so you save even more space.<p>- Paged Optimizers: This is a way to handle moments when the model needs a lot of memory all at once. It&#x27;s like if you have a small desk, but sometimes you need to work on a big project. Instead of getting a bigger desk, you just clear off and use the desk in small sections at a time.<p>By using these techniques, the researchers were able to train a very large model (Guanaco) on a single graphics card, which would normally not have enough memory for this task.</div><br/><div id="36068181" class="c"><input type="checkbox" id="c-36068181" checked=""/><div class="controls bullet"><span class="by">dharma1</span><span>|</span><a href="#36066893">root</a><span>|</span><a href="#36067371">parent</a><span>|</span><a href="#36068301">next</a><span>|</span><label class="collapse" for="c-36068181">[-]</label><label class="expand" for="c-36068181">[1 more]</label></div><br/><div class="children"><div class="content">Better than most humans would have come up with! How does this not have some kind of a model of the world, with real world analogies like this?</div><br/></div></div><div id="36067903" class="c"><input type="checkbox" id="c-36067903" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#36066893">root</a><span>|</span><a href="#36067371">parent</a><span>|</span><a href="#36068301">prev</a><span>|</span><a href="#36067219">next</a><span>|</span><label class="collapse" for="c-36067903">[-]</label><label class="expand" for="c-36067903">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kind of creepy when GPT4 uses such life-like analogies.<p>You&#x27;re an AI, you&#x27;ve never used crayons or played with legos.</div><br/></div></div></div></div><div id="36067219" class="c"><input type="checkbox" id="c-36067219" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#36066893">parent</a><span>|</span><a href="#36067371">prev</a><span>|</span><a href="#36067421">next</a><span>|</span><label class="collapse" for="c-36067219">[-]</label><label class="expand" for="c-36067219">[1 more]</label></div><br/><div class="children"><div class="content">Quantizing is porting the weights from using high-precision floating point decimals to lower-precision numbers, e.g., 4-bit and 8-bit ints. Less memory used by the weights means we can run bigger models on the same hardware.<p>The idea behind quantization is that these models have so many parameters, they&#x27;ll still work even if we reduce each node&#x27;s flexibility.</div><br/></div></div><div id="36067421" class="c"><input type="checkbox" id="c-36067421" checked=""/><div class="controls bullet"><span class="by">ianpurton</span><span>|</span><a href="#36066893">parent</a><span>|</span><a href="#36067219">prev</a><span>|</span><a href="#36064991">next</a><span>|</span><label class="collapse" for="c-36067421">[-]</label><label class="expand" for="c-36067421">[1 more]</label></div><br/><div class="children"><div class="content">You can compress LLM models and run them in less ram. This matters because most people don&#x27;t have access to powerful GPU clusters.</div><br/></div></div></div></div><div id="36064991" class="c"><input type="checkbox" id="c-36064991" checked=""/><div class="controls bullet"><span class="by">CSSer</span><span>|</span><a href="#36066893">prev</a><span>|</span><a href="#36067399">next</a><span>|</span><label class="collapse" for="c-36064991">[-]</label><label class="expand" for="c-36064991">[3 more]</label></div><br/><div class="children"><div class="content">Is lemon-picked a real phrase or did they use GPT to generate the abstract? The term is “cherry-picked”.</div><br/><div id="36065402" class="c"><input type="checkbox" id="c-36065402" checked=""/><div class="controls bullet"><span class="by">cantaloupe</span><span>|</span><a href="#36064991">parent</a><span>|</span><a href="#36065037">next</a><span>|</span><label class="collapse" for="c-36065402">[-]</label><label class="expand" for="c-36065402">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When we notice a pattern we attempt to setup a question or prompt that will induce the pattern even though it is the incorrect solution, e.g., if we observe that the model tends to give long-winded answers we prompt the model to “Answer yes or no without explanation.” We use this to find “lemons” where we manage to adversarially break the model and “cherries” where we fail to break the model, and present both.</div><br/></div></div><div id="36065037" class="c"><input type="checkbox" id="c-36065037" checked=""/><div class="controls bullet"><span class="by">atherton33</span><span>|</span><a href="#36064991">parent</a><span>|</span><a href="#36065402">prev</a><span>|</span><a href="#36067399">next</a><span>|</span><label class="collapse" for="c-36065037">[-]</label><label class="expand" for="c-36065037">[1 more]</label></div><br/><div class="children"><div class="content">I think the idea is they&#x27;re showing worst examples (lemons) rather than best (cherries).</div><br/></div></div></div></div><div id="36067399" class="c"><input type="checkbox" id="c-36067399" checked=""/><div class="controls bullet"><span class="by">epups</span><span>|</span><a href="#36064991">prev</a><span>|</span><a href="#36065287">next</a><span>|</span><label class="collapse" for="c-36067399">[-]</label><label class="expand" for="c-36067399">[3 more]</label></div><br/><div class="children"><div class="content">Do you know which model size can be run with a 3090?</div><br/><div id="36067777" class="c"><input type="checkbox" id="c-36067777" checked=""/><div class="controls bullet"><span class="by">ianpurton</span><span>|</span><a href="#36067399">parent</a><span>|</span><a href="#36067906">next</a><span>|</span><label class="collapse" for="c-36067777">[-]</label><label class="expand" for="c-36067777">[1 more]</label></div><br/><div class="children"><div class="content">Assuming you have 24GB of VRAM, then you should be able to run something like 7 billion parameters i.e. MPT-7b and quantized perhaps you can get up to 13 billion.<p>Note: I have no practical experience of this, just reading around.</div><br/></div></div><div id="36067906" class="c"><input type="checkbox" id="c-36067906" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#36067399">parent</a><span>|</span><a href="#36067777">prev</a><span>|</span><a href="#36065287">next</a><span>|</span><label class="collapse" for="c-36067906">[-]</label><label class="expand" for="c-36067906">[1 more]</label></div><br/><div class="children"><div class="content">3090 can handle the ~30B models quantized to 4 bits.</div><br/></div></div></div></div></div></div></div></div></div></body></html>