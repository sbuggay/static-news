<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726563668770" as="style"/><link rel="stylesheet" href="styles.css?v=1726563668770"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/satmihir/fair">Fair: A Go library for serving resources fairly</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>ngaut</span> | <span>39 comments</span></div><br/><div><div id="41555196" class="c"><input type="checkbox" id="c-41555196" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41555196">[-]</label><label class="expand" for="c-41555196">[14 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Since the state is stored in a multi-level Bloom Filter style data structure, the memory needed is constant and does not scale with the number of clients.</i><p>Constant memory, but those hashes will take up CPU cycles. If you&#x27;re running a workload that completes sub 20 milliseconds, these cycles spent hashing may not be worth it over, say, a <i>constant-time</i> admission control like token bucket.</div><br/><div id="41555809" class="c"><input type="checkbox" id="c-41555809" checked=""/><div class="controls bullet"><span class="by">mnadkvlb</span><span>|</span><a href="#41555196">parent</a><span>|</span><a href="#41556094">next</a><span>|</span><label class="collapse" for="c-41555809">[-]</label><label class="expand" for="c-41555809">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely my thought as well. During my thesis i found token buckets to be better and more fair compared to other algos[1]. This was in libvirt. my finding was that it scaled well up to around 1k buckets if memory serves right. after that the results were weird to say the least. (of course the servers i tested on were quite old with not too much memory). Would be nice to run my thesis tests again to find what happened in the last decade.<p>I tried writing another algorithm for network splitting but didn&#x27;t get any better results.  
[1]: <a href="https:&#x2F;&#x2F;www.csg.uzh.ch&#x2F;publications&#x2F;details.php?id=1007" rel="nofollow">https:&#x2F;&#x2F;www.csg.uzh.ch&#x2F;publications&#x2F;details.php?id=1007</a></div><br/></div></div><div id="41556094" class="c"><input type="checkbox" id="c-41556094" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#41555196">parent</a><span>|</span><a href="#41555809">prev</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41556094">[-]</label><label class="expand" for="c-41556094">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not exactly sure what you propose, a token bucket per client? In a hashmap client=&gt;bucket?</div><br/><div id="41556967" class="c"><input type="checkbox" id="c-41556967" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41556094">parent</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41556967">[-]</label><label class="expand" for="c-41556967">[11 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the typical design for a token-bucket rate limiter. A token-bucket limiter uses more memory but is a simpler design. I believe this bloom-filter based implementation is designed to be more memory efficient at the cost being less CPU efficient.<p>As usual, tradeoffs are everywhere.</div><br/><div id="41558187" class="c"><input type="checkbox" id="c-41558187" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41556967">parent</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41558187">[-]</label><label class="expand" for="c-41558187">[10 more]</label></div><br/><div class="children"><div class="content">Bloom filters really shine when they avoid a network round trip or pulling data from disk. Those are so far away you can easily afford to spend 5% of the cost of the request to avoid 99% of the requests.</div><br/><div id="41558401" class="c"><input type="checkbox" id="c-41558401" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41558187">parent</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41558401">[-]</label><label class="expand" for="c-41558401">[9 more]</label></div><br/><div class="children"><div class="content">Agreed! In a typical token bucket implementation, bucket state must be shared across multiple servers, which means you need a shared state repository such as Redis or a database (SQL or noSQL). This can add milliseconds to each request being handled.</div><br/><div id="41559963" class="c"><input type="checkbox" id="c-41559963" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41558401">parent</a><span>|</span><a href="#41558520">next</a><span>|</span><label class="collapse" for="c-41559963">[-]</label><label class="expand" for="c-41559963">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This can add milliseconds<p>My previous (very long) project was in such a state when I got there that it was only in the last year I was there that measuring things in microseconds was something I could get on other people&#x27;s radars.  I wish I had started sooner because I found a <i>lot</i> of microseconds in about a four month period. That was the most intensive period of user-visible latency reduction I saw the entire time it was there and second through fourth place took years of manpower to accomplish.  Past me and future me are both still mad about that.</div><br/></div></div><div id="41558520" class="c"><input type="checkbox" id="c-41558520" checked=""/><div class="controls bullet"><span class="by">foobazgt</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41558401">parent</a><span>|</span><a href="#41559963">prev</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41558520">[-]</label><label class="expand" for="c-41558520">[7 more]</label></div><br/><div class="children"><div class="content">Rate limiters seem like the poster child for something you&#x27;d want to host in an in-memory KVS. Typical access times look more like ~100us, not milliseconds. Even if you have a complicated algorithm, you can still limit it to a single round trip by using a Redis Function or some moral equivalent.</div><br/><div id="41561513" class="c"><input type="checkbox" id="c-41561513" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41558520">parent</a><span>|</span><a href="#41558587">next</a><span>|</span><label class="collapse" for="c-41561513">[-]</label><label class="expand" for="c-41561513">[1 more]</label></div><br/><div class="children"><div class="content">You can also rate limit on the server side.  Service A talks to Service B.  Service B is 3 replicas.  Each replica keeps track of how many times Service A talked to it.  If that goes over the limit &#x2F; 3, deny the request.  Now there is no IPC required.<p>This isn&#x27;t as accurate, but it&#x27;s often adequate.  I have never liked making a network request to see if I can make a network request; too slow.  (I do use this architecture to rate-limit my own website, mostly because I wanted to play with writing a service to do that.  But I&#x27;d hesitate to make someone else use that.)</div><br/></div></div><div id="41558587" class="c"><input type="checkbox" id="c-41558587" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41558520">parent</a><span>|</span><a href="#41561513">prev</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41558587">[-]</label><label class="expand" for="c-41558587">[5 more]</label></div><br/><div class="children"><div class="content">That would work if you can assign requests from a given tenant to a single instance, but there are many situations in which that&#x27;s either impossible or unwise. What if a single server doesn&#x27;t have enough capacity to handle all the traffic for a tenant? How do you preserve the state if that instance fails?</div><br/><div id="41560041" class="c"><input type="checkbox" id="c-41560041" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41558587">parent</a><span>|</span><a href="#41558782">next</a><span>|</span><label class="collapse" for="c-41560041">[-]</label><label class="expand" for="c-41560041">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t recall all the details but we did end up with lua to talk to redis or memcached to do some traffic shaping at one point. One for bouncing to error pages before we switched to CDN (long story), and another one for doing something too clever by half about TTFB. It&#x27;s still really cheap especially on a box that is just doing load balancing and nothing else.<p>If you wanted to throw another layer of load balancer in, there are consistent hashing-adjacent strategies in nginx+ that would allow you to go from 2 ingress routers to 3 shards with rate limiters to your services, using one KV store per box. But I highly suspect that the latency profile there will look remarkably similar to ingress routers doing rate limiting talking to a KV store cluster.</div><br/></div></div><div id="41558782" class="c"><input type="checkbox" id="c-41558782" checked=""/><div class="controls bullet"><span class="by">foobazgt</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41558587">parent</a><span>|</span><a href="#41560041">prev</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41558782">[-]</label><label class="expand" for="c-41558782">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry, I don&#x27;t think I understand your question. Are you talking about the KVS? You shard the servers for extra capacity. Several KVS&#x27;s have built-in clustering if you want to go that route. They&#x27;re usually incredibly stable, but if one goes down for whatever reason (say the physical machine fails), you just spin another one up to take its place.<p>In terms of preserving state, the answer for rate limiting is that it is almost always far, far less dangerous to fail open than it is to deny requests during a failure. If you really, really, wanted to preserve state (something I&#x27;d suggest avoiding for a rate limiter), several KVS&#x27;s have optional persistence you can turn on, for example, Redis&#x27; AOF.<p>The end services themselves should be designed with some sort of pushback mechanism, so they shouldn&#x27;t be in any danger of overloading, regardless of what&#x27;s going on with the rate limiter.</div><br/><div id="41559004" class="c"><input type="checkbox" id="c-41559004" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41558782">parent</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41559004">[-]</label><label class="expand" for="c-41559004">[2 more]</label></div><br/><div class="children"><div class="content">I think I misunderstood what you were saying. By &quot;in-memory KVS&quot; with &quot;access times in the microseconds&quot; I thought you were implying a KVS hosted on the server that handles the requests. Otherwise, even if the KVS can respond in 100us to a local query, network latency is going to add much more than that.</div><br/><div id="41559432" class="c"><input type="checkbox" id="c-41559432" checked=""/><div class="controls bullet"><span class="by">foobazgt</span><span>|</span><a href="#41555196">root</a><span>|</span><a href="#41559004">parent</a><span>|</span><a href="#41555146">next</a><span>|</span><label class="collapse" for="c-41559432">[-]</label><label class="expand" for="c-41559432">[1 more]</label></div><br/><div class="children"><div class="content">Ah ok, that makes sense. Over loopback, you can roundtrip Redis at least as fast as double-digit micros. Intra-DC, your network latency should be somewhere in the triple-digit micros. I&#x27;d say if you&#x27;re not seeing that, something is probably wrong.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41555146" class="c"><input type="checkbox" id="c-41555146" checked=""/><div class="controls bullet"><span class="by">tazu</span><span>|</span><a href="#41555196">prev</a><span>|</span><a href="#41544647">next</a><span>|</span><label class="collapse" for="c-41555146">[-]</label><label class="expand" for="c-41555146">[9 more]</label></div><br/><div class="children"><div class="content">Does anyone have some real-world use cases for something like this? The algorithm is cool but I&#x27;m struggling to see where this is applicable.</div><br/><div id="41555319" class="c"><input type="checkbox" id="c-41555319" checked=""/><div class="controls bullet"><span class="by">codaphiliac</span><span>|</span><a href="#41555146">parent</a><span>|</span><a href="#41555838">next</a><span>|</span><label class="collapse" for="c-41555319">[-]</label><label class="expand" for="c-41555319">[6 more]</label></div><br/><div class="children"><div class="content">Thinking this could be useful in a multi tenants service where you need to fairly allocate job processing capacity across tenants to a number of background workers (like data export api requests, encoding requests etc.)</div><br/><div id="41555354" class="c"><input type="checkbox" id="c-41555354" checked=""/><div class="controls bullet"><span class="by">jawns</span><span>|</span><a href="#41555146">root</a><span>|</span><a href="#41555319">parent</a><span>|</span><a href="#41555838">next</a><span>|</span><label class="collapse" for="c-41555354">[-]</label><label class="expand" for="c-41555354">[5 more]</label></div><br/><div class="children"><div class="content">That was my first thought as well. However, in a lot of real world cases, what matters is not the frequency of requests, but the duration of the jobs. For instance, one client might request a job that takes minutes or hours to complete, while another may only have requests that take a couple of seconds to complete. I don&#x27;t think this library handles such cases.</div><br/><div id="41558241" class="c"><input type="checkbox" id="c-41558241" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41555146">root</a><span>|</span><a href="#41555354">parent</a><span>|</span><a href="#41557163">next</a><span>|</span><label class="collapse" for="c-41558241">[-]</label><label class="expand" for="c-41558241">[1 more]</label></div><br/><div class="children"><div class="content">Lots of heuristics continue to work pretty well as long as the least and greatest are within an order of magnitude of each other. Itâs one of the reasons why we break stories down to 1-10 business days. Anything bigger and the statistical characteristics begin to break down.<p>That said, itâs quite easy for a big job to exceed 50x the cost of the smallest job.</div><br/></div></div><div id="41557163" class="c"><input type="checkbox" id="c-41557163" checked=""/><div class="controls bullet"><span class="by">dtjohnnymonkey</span><span>|</span><a href="#41555146">root</a><span>|</span><a href="#41555354">parent</a><span>|</span><a href="#41558241">prev</a><span>|</span><a href="#41555732">next</a><span>|</span><label class="collapse" for="c-41557163">[-]</label><label class="expand" for="c-41557163">[2 more]</label></div><br/><div class="children"><div class="content">To mitigate this case you could limit capacity in terms of concurrency instead of request rate. Basically it would be like a fairly-acquired semaphore.</div><br/><div id="41558277" class="c"><input type="checkbox" id="c-41558277" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41555146">root</a><span>|</span><a href="#41557163">parent</a><span>|</span><a href="#41555732">next</a><span>|</span><label class="collapse" for="c-41558277">[-]</label><label class="expand" for="c-41558277">[1 more]</label></div><br/><div class="children"><div class="content">I believe nginx+ has a feature that does max-conns by IP address. Itâs a similar solution to what you describe. Of course that falls down wrt fairness when fanout causes the cost of a request to not be proportional to the response time.</div><br/></div></div></div></div><div id="41555732" class="c"><input type="checkbox" id="c-41555732" checked=""/><div class="controls bullet"><span class="by">codaphiliac</span><span>|</span><a href="#41555146">root</a><span>|</span><a href="#41555354">parent</a><span>|</span><a href="#41557163">prev</a><span>|</span><a href="#41555838">next</a><span>|</span><label class="collapse" for="c-41555732">[-]</label><label class="expand" for="c-41555732">[1 more]</label></div><br/><div class="children"><div class="content">defining a unit of processing like duration or quantity and then feeding the algorithm with the equivalent of units consumed (pre or post processing a request) might help.</div><br/></div></div></div></div></div></div><div id="41555838" class="c"><input type="checkbox" id="c-41555838" checked=""/><div class="controls bullet"><span class="by">mnadkvlb</span><span>|</span><a href="#41555146">parent</a><span>|</span><a href="#41555319">prev</a><span>|</span><a href="#41558445">next</a><span>|</span><label class="collapse" for="c-41555838">[-]</label><label class="expand" for="c-41555838">[1 more]</label></div><br/><div class="children"><div class="content">I responded above, but it could be used maybe for network libraries for eg. libvirt. I did my thesis on this topic a couple years ago.<p>I am very intrigued to find out how this would fit in, if at all.</div><br/></div></div><div id="41558445" class="c"><input type="checkbox" id="c-41558445" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#41555146">parent</a><span>|</span><a href="#41555838">prev</a><span>|</span><a href="#41544647">next</a><span>|</span><label class="collapse" for="c-41558445">[-]</label><label class="expand" for="c-41558445">[1 more]</label></div><br/><div class="children"><div class="content">Rate limiters are used to protect servers from overload and to prevent attackers--or even legitimate but unintentionally greedy tenants--from starving other tenants of resources. They are a key component of a resilient distributed system.<p>See, e.g., <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;wellarchitected&#x2F;latest&#x2F;framework&#x2F;rel_mitigate_interaction_failure_throttle_requests.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;wellarchitected&#x2F;latest&#x2F;framework...</a><p>This project, however, looks like a concurrency limiter, not a rate limiter. I&#x27;m also not sure how it works across a load-balanced cluster.</div><br/></div></div></div></div><div id="41544647" class="c"><input type="checkbox" id="c-41544647" checked=""/><div class="controls bullet"><span class="by">nstateful</span><span>|</span><a href="#41555146">prev</a><span>|</span><a href="#41560616">next</a><span>|</span><label class="collapse" for="c-41544647">[-]</label><label class="expand" for="c-41544647">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting and looking forward to trying this. I am a big fan of SFB for this type of stuff but haven&#x27;t seen anything in distributed space that&#x27;s beyond a science project. Would be great if I can use it on my 10k+ user web site.</div><br/></div></div><div id="41560616" class="c"><input type="checkbox" id="c-41560616" checked=""/><div class="controls bullet"><span class="by">roboben</span><span>|</span><a href="#41544647">prev</a><span>|</span><a href="#41555159">next</a><span>|</span><label class="collapse" for="c-41560616">[-]</label><label class="expand" for="c-41560616">[1 more]</label></div><br/><div class="children"><div class="content">Shouldnât this be built into a queue somehow? Iâd love to see a queuing solution like SQS but has a built in fairness, where you can fully utilize a capacity but as soon as, letâs say customers compete on resources, some fairness kicks in.
Is there anything like that?</div><br/></div></div><div id="41555159" class="c"><input type="checkbox" id="c-41555159" checked=""/><div class="controls bullet"><span class="by">salomonk_mur</span><span>|</span><a href="#41560616">prev</a><span>|</span><a href="#41556065">next</a><span>|</span><label class="collapse" for="c-41555159">[-]</label><label class="expand" for="c-41555159">[10 more]</label></div><br/><div class="children"><div class="content">Why would you learn and use this over typical load-balancing solutions like K8S? Honest question.</div><br/><div id="41558492" class="c"><input type="checkbox" id="c-41558492" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#41555159">parent</a><span>|</span><a href="#41556124">next</a><span>|</span><label class="collapse" for="c-41558492">[-]</label><label class="expand" for="c-41558492">[1 more]</label></div><br/><div class="children"><div class="content">They are complementary solutions, not substitutes. Load balancers distribute traffic among servers and are a key component to enable horizontal scalability. Throttling is a prophylactic feature that prevents attackers or greedy consumers from overutilizing capacity and depriving it from other legitimate users. This library relates to the latter.<p>Unfortunately the title of the GitHub repo (&quot;A Go library for serving resources fairly&quot;) is misleading. This is not a server; it&#x27;s a library that a server can utilize to determine whether a request has exceeded fairness bounds and should be rejected with an HTTP 429 (too many requests) response.</div><br/></div></div><div id="41556124" class="c"><input type="checkbox" id="c-41556124" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#41555159">parent</a><span>|</span><a href="#41558492">prev</a><span>|</span><a href="#41555377">next</a><span>|</span><label class="collapse" for="c-41556124">[-]</label><label class="expand" for="c-41556124">[1 more]</label></div><br/><div class="children"><div class="content">Those are completely unrelated. K8s does not provide client rate-control and FAIR does not do load-balancing. It appears you misunderstood what this is.</div><br/></div></div><div id="41555377" class="c"><input type="checkbox" id="c-41555377" checked=""/><div class="controls bullet"><span class="by">joshuanapoli</span><span>|</span><a href="#41555159">parent</a><span>|</span><a href="#41556124">prev</a><span>|</span><a href="#41556065">next</a><span>|</span><label class="collapse" for="c-41555377">[-]</label><label class="expand" for="c-41555377">[7 more]</label></div><br/><div class="children"><div class="content">In a multi-tenant system, you might have one customer who drops a big job that creates a huge number of tasks. We&#x27;d like to process this as fast as possible, but not block the tasks of small jobs from other customers, which should normally be completed very quickly.</div><br/><div id="41557781" class="c"><input type="checkbox" id="c-41557781" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#41555159">root</a><span>|</span><a href="#41555377">parent</a><span>|</span><a href="#41556009">next</a><span>|</span><label class="collapse" for="c-41557781">[-]</label><label class="expand" for="c-41557781">[1 more]</label></div><br/><div class="children"><div class="content">We had to make something similar. We have both huge tenants (200k users in one tenant) and small tenants with 10 users. Sometimes there are spikes when a large tenant generates thousands of jobs. In a naive implementation, 1 tenant would be able to completely block job processing for all other tenants. We have to make sure the next job is picked from a different tenant each time, so that all tenants were served fairly. However, a large tenant may end up waiting for its jobs to complete for too long. In that case, we move such a tenant to a different infrastructure (sometimes, fully dedicated).</div><br/></div></div><div id="41556009" class="c"><input type="checkbox" id="c-41556009" checked=""/><div class="controls bullet"><span class="by">dpatterbee</span><span>|</span><a href="#41555159">root</a><span>|</span><a href="#41555377">parent</a><span>|</span><a href="#41557781">prev</a><span>|</span><a href="#41556065">next</a><span>|</span><label class="collapse" for="c-41556009">[-]</label><label class="expand" for="c-41556009">[5 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the exactly the problem that preemptive multitasking is built for? For example any program built on the BEAM[1] wouldn&#x27;t have this problem presumably. Do most languages not have a standard solution for this?<p>[1]: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;BEAM_(Erlang_virtual_machine)" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;BEAM_(Erlang_virtual_machine...</a></div><br/><div id="41556415" class="c"><input type="checkbox" id="c-41556415" checked=""/><div class="controls bullet"><span class="by">jerf</span><span>|</span><a href="#41555159">root</a><span>|</span><a href="#41556009">parent</a><span>|</span><a href="#41556352">next</a><span>|</span><label class="collapse" for="c-41556415">[-]</label><label class="expand" for="c-41556415">[3 more]</label></div><br/><div class="children"><div class="content">Pre-emptive multitasking is a tool that a solution might use, but it is not a solution on its own. If you have three users spawning one thread&#x2F;process&#x2F;task and one user spawning a million (literally, not figuratively), that one user can easily completely starve the three little users. Some of their million tasks may also be starved. But they&#x27;ll get more done overall.<p>This whole problem gets way more complicated than our intuition generally can work with. The pathological distribution of the size of various workloads and the pathological distribution of the variety of resources that tasks can consume is not modeled well by our human brains, who really want to work with tasks that are essentially uniform. But they never are. A lot of systems end up punting, either to the OS which has to deal with this anyhow, or to letting programs do their own cooperative internal scheduling, which is what this library implements. In general &quot;but what if I &#x27;just&#x27;&quot; solutions to this problem have undesirable pathological edge cases that seem like they &quot;ought&quot; to work, especially at the full generality of an operation system. See also the surprisingly difficult task of OOM-killing the &quot;correct&quot; process; the &quot;well obviously you &#x27;just&#x27;&quot; algorithms don&#x27;t work in the real world, for a very similar reason.<p>As computers have gotten larger, the pathological distributions have gotten worse. To be honest, if you&#x27;re thinking of using &quot;fair&quot; it is likely you&#x27;re better off working on the ability to scale resources instead. There&#x27;s a niche for this sort of library, but it is constantly shrinking relative to the totality of computing tasks we want to perform (even though it is growing in absolute terms).</div><br/><div id="41562659" class="c"><input type="checkbox" id="c-41562659" checked=""/><div class="controls bullet"><span class="by">dpatterbee</span><span>|</span><a href="#41555159">root</a><span>|</span><a href="#41556415">parent</a><span>|</span><a href="#41560290">next</a><span>|</span><label class="collapse" for="c-41562659">[-]</label><label class="expand" for="c-41562659">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the explanation, I hadn&#x27;t clocked the difference between fair allocation for tasks and fair allocation for users.</div><br/></div></div><div id="41560290" class="c"><input type="checkbox" id="c-41560290" checked=""/><div class="controls bullet"><span class="by">udkl</span><span>|</span><a href="#41555159">root</a><span>|</span><a href="#41556415">parent</a><span>|</span><a href="#41562659">prev</a><span>|</span><a href="#41556352">next</a><span>|</span><label class="collapse" for="c-41560290">[-]</label><label class="expand" for="c-41560290">[1 more]</label></div><br/><div class="children"><div class="content">Appreciate this very insightful comment</div><br/></div></div></div></div><div id="41556352" class="c"><input type="checkbox" id="c-41556352" checked=""/><div class="controls bullet"><span class="by">maxmcd</span><span>|</span><a href="#41555159">root</a><span>|</span><a href="#41556009">parent</a><span>|</span><a href="#41556415">prev</a><span>|</span><a href="#41556065">next</a><span>|</span><label class="collapse" for="c-41556352">[-]</label><label class="expand" for="c-41556352">[1 more]</label></div><br/><div class="children"><div class="content">In preemptive multitasking you are trying to get all the work done as quickly as possible, but with FAIR and similar systems you want to make sure that a single client&#x2F;user&#x2F;resource cannot steal an inordinate amount of capacity.<p>I do not think languages&#x2F;runtimes typically implement that kind of prioritization&#x2F;limiting mechanism.</div><br/></div></div></div></div></div></div></div></div><div id="41555103" class="c"><input type="checkbox" id="c-41555103" checked=""/><div class="controls bullet"><span class="by">AnnaMere</span><span>|</span><a href="#41561602">prev</a><span>|</span><label class="collapse" for="c-41555103">[-]</label><label class="expand" for="c-41555103">[1 more]</label></div><br/><div class="children"><div class="content">Extremely interesting and valuable</div><br/></div></div></div></div></div></div></div></body></html>