<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1727341275470" as="style"/><link rel="stylesheet" href="styles.css?v=1727341275470"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/?_fb_noscript=1">Llama 3.2: Revolutionizing edge AI and vision with open, customizable models</a> <span class="domain">(<a href="https://ai.meta.com">ai.meta.com</a>)</span></div><div class="subtext"><span>nmwnmw</span> | <span>143 comments</span></div><br/><div><div id="41652377" class="c"><input type="checkbox" id="c-41652377" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41650261">next</a><span>|</span><label class="collapse" for="c-41652377">[-]</label><label class="expand" for="c-41652377">[29 more]</label></div><br/><div class="children"><div class="content">I&#x27;m absolutely amazed at how capable the new 1B model is, considering it&#x27;s just a 1.3GB download (for the Ollama GGUF version).<p>I tried running a full codebase through it (since it can handle 128,000 tokens) and asking it to summarize the code - it did a surprisingly decent job, incomplete but still unbelievable for a model that tiny: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;64c5f5b111fe473999144932bef4218b" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;64c5f5b111fe473999144932bef42...</a><p>More of my notes here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;25&#x2F;llama-32&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;25&#x2F;llama-32&#x2F;</a><p>I&#x27;ve been trying out the larger image models to using the versions hosted on <a href="https:&#x2F;&#x2F;lmarena.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmarena.ai&#x2F;</a> - navigate to &quot;Direct Chat&quot; and you can select them from the dropdown and upload images to run prompts.</div><br/><div id="41653209" class="c"><input type="checkbox" id="c-41653209" checked=""/><div class="controls bullet"><span class="by">jackbravo</span><span>|</span><a href="#41652377">parent</a><span>|</span><a href="#41654106">next</a><span>|</span><label class="collapse" for="c-41653209">[-]</label><label class="expand" for="c-41653209">[10 more]</label></div><br/><div class="children"><div class="content">I saw that you mention <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm&#x2F;</a>. Hadn&#x27;t seen this before. What is its purpose? And why not use ollama instead?</div><br/><div id="41653228" class="c"><input type="checkbox" id="c-41653228" checked=""/><div class="controls bullet"><span class="by">dannyobrien</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653209">parent</a><span>|</span><a href="#41653676">next</a><span>|</span><label class="collapse" for="c-41653228">[-]</label><label class="expand" for="c-41653228">[8 more]</label></div><br/><div class="children"><div class="content">llm is Simon&#x27;s command line front-end to a lot of the llm apis, local and cloud-based. Along with aider-chat, it&#x27;s my main interface to any LLM work -- it works well with a chat model, one-off queries, and piping text or output into a llm chain. For people who live on the command line, or are just put-off by web interfaces, it&#x27;s a godsend.<p>About the only thing I need to look further abroad for is when I&#x27;m working multi-modally -- I know Simon and the community are mainly noodling over the best command line UX for that: <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm&#x2F;issues&#x2F;331">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm&#x2F;issues&#x2F;331</a></div><br/><div id="41654423" class="c"><input type="checkbox" id="c-41654423" checked=""/><div class="controls bullet"><span class="by">SOLAR_FIELDS</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653228">parent</a><span>|</span><a href="#41653354">next</a><span>|</span><label class="collapse" for="c-41654423">[-]</label><label class="expand" for="c-41654423">[2 more]</label></div><br/><div class="children"><div class="content">I use a fair amount of aider - what does Simon&#x27;s solution offer that aider doesn&#x27;t? I am usually using a mix of aider and the ChatGPT window. I use ChatGPT for one off queries that aren&#x27;t super context heavy for my codebase, since pricing can still add up for the API and a lot of the times the questions that I ask don&#x27;t really need deep context about what I&#x27;m doing in the terminal. But when I&#x27;m in flow state and I need deep integration with the files I&#x27;m changing I switch over to aider with Sonnet - my subjective experience is that Anthropic&#x27;s models are significantly better for that use case. Curious if Simon&#x27;s solution is more geared toward the first use case or the second.</div><br/><div id="41654818" class="c"><input type="checkbox" id="c-41654818" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41654423">parent</a><span>|</span><a href="#41653354">next</a><span>|</span><label class="collapse" for="c-41654818">[-]</label><label class="expand" for="c-41654818">[1 more]</label></div><br/><div class="children"><div class="content">The llm command is a general-purpose tool for writing shell scripts that use an llm somehow. For example, generating some llm output and sending it though a Unix pipeline. You can also use it interactively if you like working on the command line.<p>It’s not specifically about chatting or helping you write code, though you could use it for that if you like.</div><br/></div></div></div></div><div id="41653354" class="c"><input type="checkbox" id="c-41653354" checked=""/><div class="controls bullet"><span class="by">n8henrie</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653228">parent</a><span>|</span><a href="#41654423">prev</a><span>|</span><a href="#41653676">next</a><span>|</span><label class="collapse" for="c-41653354">[-]</label><label class="expand" for="c-41653354">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only used ollama over cli. As per the parent poster -- do you know if there are advantages over ollama for CLI use? Have you used both?</div><br/><div id="41654055" class="c"><input type="checkbox" id="c-41654055" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653354">parent</a><span>|</span><a href="#41653569">next</a><span>|</span><label class="collapse" for="c-41654055">[-]</label><label class="expand" for="c-41654055">[2 more]</label></div><br/><div class="children"><div class="content">Ollama can’t talk to OpenAI &#x2F; Anthropic &#x2F; etc. LLM gives you a single interface that can talk to both hosted and local models.<p>It also logs everything you do to a SQLite database, which is great for further analysis.<p>I use LLM and Ollama together quite a bit, because Ollama are really good at getting new models working and their server keeps those models in memory between requests.</div><br/><div id="41654085" class="c"><input type="checkbox" id="c-41654085" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41654055">parent</a><span>|</span><a href="#41653569">next</a><span>|</span><label class="collapse" for="c-41654085">[-]</label><label class="expand" for="c-41654085">[1 more]</label></div><br/><div class="children"><div class="content">You can run llamafile as a server, too, right? Still need to download gguf files if you don&#x27;t use one of their premade binaries, but if you haven&#x27;t set up llm to hit the running llamafile server I&#x27;m sure that&#x27;s easy to do</div><br/></div></div></div></div><div id="41653569" class="c"><input type="checkbox" id="c-41653569" checked=""/><div class="controls bullet"><span class="by">dannyobrien</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653354">parent</a><span>|</span><a href="#41654055">prev</a><span>|</span><a href="#41653842">next</a><span>|</span><label class="collapse" for="c-41653569">[-]</label><label class="expand" for="c-41653569">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t used Ollama, but from what I&#x27;ve seen, it seems to operate at a different level of abstraction compared to `llm`. I use `llm` to access both remote and local models through its plugin ecosystem[1]. One of the plugins allows you to use Ollama-served local models. This means you can use the same CLI interface with Ollama[2], as well as with OpenAI, Gemini, Anthropic, llamafile, llamacpp, mlc, and others. I select different models for different purposes. Recently, I&#x27;ve switched my default from OpenAI to Anthropic quite seamlessly.<p>[1] - <a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;plugins&#x2F;directory.html#plugin-directory" rel="nofollow">https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;plugins&#x2F;directory.html#pl...</a>
[2] - <a href="https:&#x2F;&#x2F;github.com&#x2F;taketwo&#x2F;llm-ollama">https:&#x2F;&#x2F;github.com&#x2F;taketwo&#x2F;llm-ollama</a></div><br/></div></div><div id="41653842" class="c"><input type="checkbox" id="c-41653842" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653354">parent</a><span>|</span><a href="#41653569">prev</a><span>|</span><a href="#41653676">next</a><span>|</span><label class="collapse" for="c-41653842">[-]</label><label class="expand" for="c-41653842">[1 more]</label></div><br/><div class="children"><div class="content">The llm CLI is much more unixy, letting you pipe data in and out easily. It can use hosted and local models, including ollama.</div><br/></div></div></div></div></div></div><div id="41653676" class="c"><input type="checkbox" id="c-41653676" checked=""/><div class="controls bullet"><span class="by">jerieljan</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653209">parent</a><span>|</span><a href="#41653228">prev</a><span>|</span><a href="#41654106">next</a><span>|</span><label class="collapse" for="c-41653676">[-]</label><label class="expand" for="c-41653676">[1 more]</label></div><br/><div class="children"><div class="content">It looks like a multi-purpose utility in the terminal for bridging together the terminal, your scripts or programs to both local and remote LLM providers.<p>And it looks very handy! I&#x27;ll use this myself because I do want to invoke OpenAI and other cloud providers just like I do in ollama and piping things around and this accomplishes that, and more.<p><a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;</a><p>I guess you can also accomplish similar results if you&#x27;re just looking for `&#x2F;chat&#x2F;completions` and such if you configured something like LiteLLM and connecting that to ollama and any other service.</div><br/></div></div></div></div><div id="41654106" class="c"><input type="checkbox" id="c-41654106" checked=""/><div class="controls bullet"><span class="by">lowyek</span><span>|</span><a href="#41652377">parent</a><span>|</span><a href="#41653209">prev</a><span>|</span><a href="#41652426">next</a><span>|</span><label class="collapse" for="c-41654106">[-]</label><label class="expand" for="c-41654106">[5 more]</label></div><br/><div class="children"><div class="content">Hi simon, is there a way to run the vision model easily on my mac locally?</div><br/><div id="41654377" class="c"><input type="checkbox" id="c-41654377" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41654106">parent</a><span>|</span><a href="#41652426">next</a><span>|</span><label class="collapse" for="c-41654377">[-]</label><label class="expand" for="c-41654377">[4 more]</label></div><br/><div class="children"><div class="content">Not that I’ve seen so far, but Ollama are pending a solution for that “soon”.</div><br/><div id="41654536" class="c"><input type="checkbox" id="c-41654536" checked=""/><div class="controls bullet"><span class="by">v3ss0n</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41654377">parent</a><span>|</span><a href="#41652426">next</a><span>|</span><label class="collapse" for="c-41654536">[-]</label><label class="expand" for="c-41654536">[3 more]</label></div><br/><div class="children"><div class="content">I doubt ollama team can do much about it. Ollama are just wrapper on top of heavy lifter</div><br/><div id="41654953" class="c"><input type="checkbox" id="c-41654953" checked=""/><div class="controls bullet"><span class="by">Patrick_Devine</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41654536">parent</a><span>|</span><a href="#41654925">next</a><span>|</span><label class="collapse" for="c-41654953">[-]</label><label class="expand" for="c-41654953">[1 more]</label></div><br/><div class="children"><div class="content">The draft PRs are already up in the repo.</div><br/></div></div></div></div></div></div></div></div><div id="41652426" class="c"><input type="checkbox" id="c-41652426" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41652377">parent</a><span>|</span><a href="#41654106">prev</a><span>|</span><a href="#41653531">next</a><span>|</span><label class="collapse" for="c-41652426">[-]</label><label class="expand" for="c-41652426">[7 more]</label></div><br/><div class="children"><div class="content">Llama 3.2 vision models don&#x27;t seem that great if they have to compare them to Claude 3 Haiku or GPT4o-mini. For an open alternative I would use Qwen-2-72B model, it&#x27;s smaller than the 90B and seems to perform quite better. Also Qwen2-VL-7B as an alternative to Llama-3.2-11B, smaller, better in visual benchmarks and also Apache 2.0.<p>Molmo models: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;collections&#x2F;allenai&#x2F;molmo-66f379e6fe3b8ef090a8ca19" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;collections&#x2F;allenai&#x2F;molmo-66f379e6fe3...</a>, also seem to perform better than Llama-3.2 models while being smaller and Apache 2.0.</div><br/><div id="41653260" class="c"><input type="checkbox" id="c-41653260" checked=""/><div class="controls bullet"><span class="by">dannyobrien</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41652426">parent</a><span>|</span><a href="#41653423">next</a><span>|</span><label class="collapse" for="c-41653260">[-]</label><label class="expand" for="c-41653260">[2 more]</label></div><br/><div class="children"><div class="content">What interface do you use for a locally-run Qwen2-VL-7B? Inspired by Simon Willison&#x27;s research[1], I have tried it out on Hugging Face[2]. Its handwriting recognition seems fantastic, but I haven&#x27;t figured out how to run it locally yet.<p>[1] <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;4&#x2F;qwen2-vl&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;4&#x2F;qwen2-vl&#x2F;</a>
[2] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;GanymedeNil&#x2F;Qwen2-VL-7B" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;GanymedeNil&#x2F;Qwen2-VL-7B</a></div><br/><div id="41654064" class="c"><input type="checkbox" id="c-41654064" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653260">parent</a><span>|</span><a href="#41653423">next</a><span>|</span><label class="collapse" for="c-41654064">[-]</label><label class="expand" for="c-41654064">[1 more]</label></div><br/><div class="children"><div class="content">MiniCPM-V 2.6 is based on Qwen 2 and is also great at handwriting. It works locally with KoboldCPP. Here are the results I got with a test I just did.<p>Image:<p>* <a href="https:&#x2F;&#x2F;imgur.com&#x2F;wg0kdQK" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;wg0kdQK</a><p>Output:<p>* <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;RKvYQasi" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;RKvYQasi</a><p>OCR script used:<p>* <a href="https:&#x2F;&#x2F;github.com&#x2F;jabberjabberjabber&#x2F;LLMOCR&#x2F;blob&#x2F;main&#x2F;llmocr.py">https:&#x2F;&#x2F;github.com&#x2F;jabberjabberjabber&#x2F;LLMOCR&#x2F;blob&#x2F;main&#x2F;llmoc...</a><p>Model weights: MiniCPM-V-2_6-Q6_K_L.gguf, mmproj-MiniCPM-V-2_6-f16.gguf<p>Inference:<p>* <a href="https:&#x2F;&#x2F;github.com&#x2F;LostRuins&#x2F;koboldcpp&#x2F;releases&#x2F;tag&#x2F;v1.75.2">https:&#x2F;&#x2F;github.com&#x2F;LostRuins&#x2F;koboldcpp&#x2F;releases&#x2F;tag&#x2F;v1.75.2</a></div><br/></div></div></div></div><div id="41653423" class="c"><input type="checkbox" id="c-41653423" checked=""/><div class="controls bullet"><span class="by">f38zf5vdt</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41652426">parent</a><span>|</span><a href="#41653260">prev</a><span>|</span><a href="#41653531">next</a><span>|</span><label class="collapse" for="c-41653423">[-]</label><label class="expand" for="c-41653423">[4 more]</label></div><br/><div class="children"><div class="content">1. Ignore the benchmarks. I&#x27;ve been A&#x2F;Bing 11B today with Molmo 72B [1], which itself has an ELO neck-and-neck with GPT4o, and it&#x27;s even. Because everyone in open source tends to train on validation benchmarks, you really can not trust them.<p>2. The method of tokenization&#x2F;adapter is novel and uses many fewer tokens than all comparable CLIP&#x2F;SigLIP-adapter models, making it _much_ faster. Attention is O(n^2) on memory&#x2F;compute per sequence length.<p>[1] <a href="https:&#x2F;&#x2F;molmo.allenai.org&#x2F;blog" rel="nofollow">https:&#x2F;&#x2F;molmo.allenai.org&#x2F;blog</a></div><br/><div id="41654494" class="c"><input type="checkbox" id="c-41654494" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653423">parent</a><span>|</span><a href="#41653690">next</a><span>|</span><label class="collapse" for="c-41654494">[-]</label><label class="expand" for="c-41654494">[1 more]</label></div><br/><div class="children"><div class="content">It’s not just open source that trains on the validation set. The big labs have already forgotten more about gaming MMLU down to the decimal than the open source community ever knew. Every once in a while they get sloppy and Claude does a faux pas with a BIGBENCH canary string or some other embarrassing little admission of dishonesty like that.<p>A big lab gets exactly the score on any public eval that they want to. They have their own holdouts for actual ML work, and they’re some of the most closely guarded IP artifacts, far more valuable than a snapshot of weights.</div><br/></div></div><div id="41653690" class="c"><input type="checkbox" id="c-41653690" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653423">parent</a><span>|</span><a href="#41654494">prev</a><span>|</span><a href="#41653531">next</a><span>|</span><label class="collapse" for="c-41653690">[-]</label><label class="expand" for="c-41653690">[2 more]</label></div><br/><div class="children"><div class="content">How about its performance compare to Qwen-2-72B tho?</div><br/><div id="41654102" class="c"><input type="checkbox" id="c-41654102" checked=""/><div class="controls bullet"><span class="by">f38zf5vdt</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653690">parent</a><span>|</span><a href="#41653531">next</a><span>|</span><label class="collapse" for="c-41654102">[-]</label><label class="expand" for="c-41654102">[1 more]</label></div><br/><div class="children"><div class="content">Refer to the blog post I linked. Molmo is ahead of Qwen2 72b.</div><br/></div></div></div></div></div></div></div></div><div id="41653531" class="c"><input type="checkbox" id="c-41653531" checked=""/><div class="controls bullet"><span class="by">forgingahead</span><span>|</span><a href="#41652377">parent</a><span>|</span><a href="#41652426">prev</a><span>|</span><a href="#41652621">next</a><span>|</span><label class="collapse" for="c-41653531">[-]</label><label class="expand" for="c-41653531">[2 more]</label></div><br/><div class="children"><div class="content">What are people using to check token length of code bases? I&#x27;d like to point certain app folders to a local LLM, but no idea how that stuff is calculated? Seems like some strategic prompting (eg: this is a rails app, here is the folder structure with file names, and btw here are the actual files to parse) would be more efficient than just giving it the full app folder? No point giving it stuff from &#x2F;lib and &#x2F;vendor for the most part I reckon.</div><br/><div id="41654063" class="c"><input type="checkbox" id="c-41654063" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653531">parent</a><span>|</span><a href="#41652621">next</a><span>|</span><label class="collapse" for="c-41654063">[-]</label><label class="expand" for="c-41654063">[1 more]</label></div><br/><div class="children"><div class="content">I use my <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;ttok">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;ttok</a> command for that - you can pipe stuff into it for a token count.<p>Unfortunately it only uses the OpenAI tokenizers at the moment (via tiktoken), so counts for other models may be inaccurate. I find they tend to be close enough though.</div><br/></div></div></div></div><div id="41652621" class="c"><input type="checkbox" id="c-41652621" checked=""/><div class="controls bullet"><span class="by">foxhop</span><span>|</span><a href="#41652377">parent</a><span>|</span><a href="#41653531">prev</a><span>|</span><a href="#41650261">next</a><span>|</span><label class="collapse" for="c-41652621">[-]</label><label class="expand" for="c-41652621">[4 more]</label></div><br/><div class="children"><div class="content">The llama 3.0, 3.1, &amp; 3.2 all use the TikToken tokenizer which is the open source openai tokenizer.</div><br/><div id="41652975" class="c"><input type="checkbox" id="c-41652975" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41652621">parent</a><span>|</span><a href="#41650261">next</a><span>|</span><label class="collapse" for="c-41652975">[-]</label><label class="expand" for="c-41652975">[3 more]</label></div><br/><div class="children"><div class="content">GP is talking about context windows, not the number of token used by the tokenizer.</div><br/><div id="41653142" class="c"><input type="checkbox" id="c-41653142" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41652975">parent</a><span>|</span><a href="#41650261">next</a><span>|</span><label class="collapse" for="c-41653142">[-]</label><label class="expand" for="c-41653142">[2 more]</label></div><br/><div class="children"><div class="content">Somewhat confusingly, it appears the tokenizer vocabulary as well as the context length are both 128k tokens!</div><br/><div id="41654941" class="c"><input type="checkbox" id="c-41654941" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41652377">root</a><span>|</span><a href="#41653142">parent</a><span>|</span><a href="#41650261">next</a><span>|</span><label class="collapse" for="c-41654941">[-]</label><label class="expand" for="c-41654941">[1 more]</label></div><br/><div class="children"><div class="content">Yup, that&#x27;s why I wanted to clarify things.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41650261" class="c"><input type="checkbox" id="c-41650261" checked=""/><div class="controls bullet"><span class="by">opdahl</span><span>|</span><a href="#41652377">prev</a><span>|</span><a href="#41655607">next</a><span>|</span><label class="collapse" for="c-41650261">[-]</label><label class="expand" for="c-41650261">[23 more]</label></div><br/><div class="children"><div class="content">I&#x27;m blown away with just how open the Llama team at Meta is. It is nice to see that they are not only giving access to the models, but they at the same time are open about how they built them. I don&#x27;t know how the future is going to go in the terms of models, but I sure am grateful that Meta has taken this position, and are pushing more openness.</div><br/><div id="41655873" class="c"><input type="checkbox" id="c-41655873" checked=""/><div class="controls bullet"><span class="by">cedws</span><span>|</span><a href="#41650261">parent</a><span>|</span><a href="#41653536">next</a><span>|</span><label class="collapse" for="c-41655873">[-]</label><label class="expand" for="c-41655873">[1 more]</label></div><br/><div class="children"><div class="content">Zuckerberg probably realises the value of currying favour with engineers. Also, I think he has a personal vendetta to compete with Musk in this space.</div><br/></div></div><div id="41653536" class="c"><input type="checkbox" id="c-41653536" checked=""/><div class="controls bullet"><span class="by">thefourthchime</span><span>|</span><a href="#41650261">parent</a><span>|</span><a href="#41655873">prev</a><span>|</span><a href="#41655857">next</a><span>|</span><label class="collapse" for="c-41653536">[-]</label><label class="expand" for="c-41653536">[3 more]</label></div><br/><div class="children"><div class="content">They have a hose of ad money and have nothing to lose doing this.<p>You can’t say that for the other guys.</div><br/><div id="41653621" class="c"><input type="checkbox" id="c-41653621" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41653536">parent</a><span>|</span><a href="#41655857">next</a><span>|</span><label class="collapse" for="c-41653621">[-]</label><label class="expand" for="c-41653621">[2 more]</label></div><br/><div class="children"><div class="content">I can absolutely say that about Google and Apple.</div><br/><div id="41654949" class="c"><input type="checkbox" id="c-41654949" checked=""/><div class="controls bullet"><span class="by">doubtfuluser</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41653621">parent</a><span>|</span><a href="#41655857">next</a><span>|</span><label class="collapse" for="c-41654949">[-]</label><label class="expand" for="c-41654949">[1 more]</label></div><br/><div class="children"><div class="content">For Apple - maybe, but they also recently open sourced some of their models.
For Google: they host and want to make money on the models by you using them on their platform.<p>Meta has no interest in that but directly benefits from advancements on top of Llama.</div><br/></div></div></div></div></div></div><div id="41655857" class="c"><input type="checkbox" id="c-41655857" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#41650261">parent</a><span>|</span><a href="#41653536">prev</a><span>|</span><a href="#41654664">next</a><span>|</span><label class="collapse" for="c-41655857">[-]</label><label class="expand" for="c-41655857">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re out to fuck over the competition by killing their moat. Classic commoditize your complement.</div><br/></div></div><div id="41654664" class="c"><input type="checkbox" id="c-41654664" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41650261">parent</a><span>|</span><a href="#41655857">prev</a><span>|</span><a href="#41654443">next</a><span>|</span><label class="collapse" for="c-41654664">[-]</label><label class="expand" for="c-41654664">[1 more]</label></div><br/><div class="children"><div class="content">Training data is crucial for performance and they do not (cannot) share that.</div><br/></div></div><div id="41654443" class="c"><input type="checkbox" id="c-41654443" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#41650261">parent</a><span>|</span><a href="#41654664">prev</a><span>|</span><a href="#41652062">next</a><span>|</span><label class="collapse" for="c-41654443">[-]</label><label class="expand" for="c-41654443">[2 more]</label></div><br/><div class="children"><div class="content">They are literally training on all the free personal data you provided, so they owe you this much</div><br/><div id="41655608" class="c"><input type="checkbox" id="c-41655608" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41654443">parent</a><span>|</span><a href="#41652062">next</a><span>|</span><label class="collapse" for="c-41655608">[-]</label><label class="expand" for="c-41655608">[1 more]</label></div><br/><div class="children"><div class="content">Given what I see in Facebook comments I&#x27;m surprised the AI doesn&#x27;t just respond with &quot;Amen. Happy Birthday&quot; to every query.<p>They&#x27;re clearly majorly scrubbing things somehow</div><br/></div></div></div></div><div id="41652062" class="c"><input type="checkbox" id="c-41652062" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#41650261">parent</a><span>|</span><a href="#41654443">prev</a><span>|</span><a href="#41655607">next</a><span>|</span><label class="collapse" for="c-41652062">[-]</label><label class="expand" for="c-41652062">[14 more]</label></div><br/><div class="children"><div class="content">Do they tell you what training data they use for alignment? As in, what biases they intentionally put in the system they’re widely deploying?</div><br/><div id="41652452" class="c"><input type="checkbox" id="c-41652452" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41652062">parent</a><span>|</span><a href="#41655607">next</a><span>|</span><label class="collapse" for="c-41652452">[-]</label><label class="expand" for="c-41652452">[13 more]</label></div><br/><div class="children"><div class="content">Do you have some concrete example of biases in their models? Or are you just fishing for something to complain about?</div><br/><div id="41653153" class="c"><input type="checkbox" id="c-41653153" checked=""/><div class="controls bullet"><span class="by">ericjmorey</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41652452">parent</a><span>|</span><a href="#41655691">next</a><span>|</span><label class="collapse" for="c-41653153">[-]</label><label class="expand" for="c-41653153">[6 more]</label></div><br/><div class="children"><div class="content">Even without intentionally biasing the model, without knowing the biases that exist in the training data, they&#x27;re just biased black boxes that come with the overhead of figuring out how it&#x27;s biased.<p>All data is biased, there&#x27;s no avoiding that fact.</div><br/><div id="41654593" class="c"><input type="checkbox" id="c-41654593" checked=""/><div class="controls bullet"><span class="by">slt2021</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41653153">parent</a><span>|</span><a href="#41655691">next</a><span>|</span><label class="collapse" for="c-41654593">[-]</label><label class="expand" for="c-41654593">[5 more]</label></div><br/><div class="children"><div class="content">bias is some normative lens that some people came up with, but it is purely subjective and is a social construct, that has roots in the area of social justice and has nothing to do with the LLM.<p>the proof is that all critics of AI&#x2F;LLM have never ever produced a single &quot;unbiased&quot; model. If unbiased model does not exist (at least I never seen an AI&#x2F;LLM sceptics community produce one), then the concept of bias is useless.<p>Just a fluffy word that does not mean anything</div><br/><div id="41655037" class="c"><input type="checkbox" id="c-41655037" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41654593">parent</a><span>|</span><a href="#41655691">next</a><span>|</span><label class="collapse" for="c-41655037">[-]</label><label class="expand" for="c-41655037">[4 more]</label></div><br/><div class="children"><div class="content">If you forget about the social justice stuff for a minute, there are many other types of bias relevant for an LLM.<p>One example is US-centric bias. If I ask the LLM a question where the answer is one thing in the US and another thing in Germany, you can&#x27;t really de-bias the model. But ideally you can have it request more details in order to give a good answer.</div><br/><div id="41655430" class="c"><input type="checkbox" id="c-41655430" checked=""/><div class="controls bullet"><span class="by">Al-Khwarizmi</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41655037">parent</a><span>|</span><a href="#41655472">next</a><span>|</span><label class="collapse" for="c-41655430">[-]</label><label class="expand" for="c-41655430">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but that bias has been present in everything related to computers for decades.<p>As someone from outside the US, it is quite common to face annoyances like address fields expecting addresses in US format, systems misbehaving and sometimes failing silently if you have two surnames, or accented characters in your personal data, etc. Years go by, tech gets better, but these issues don&#x27;t go away, they just reappear in different places.<p>It&#x27;s funny how some people seem to have discovered this kind of bias and started getting angry with LLMs, which are actually quite OK in this respect.<p>Not saying that it isn&#x27;t an issue that should be addressed, just that some people are using it as an excuse to get indignant at AI and it doesn&#x27;t make much sense. Just like the people who get indignant at AI because ChatGPT collects your input and uses it for training - what do they think social networks have been doing with their input in the last 20 years?</div><br/><div id="41655520" class="c"><input type="checkbox" id="c-41655520" checked=""/><div class="controls bullet"><span class="by">slt2021</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41655430">parent</a><span>|</span><a href="#41655472">next</a><span>|</span><label class="collapse" for="c-41655520">[-]</label><label class="expand" for="c-41655520">[1 more]</label></div><br/><div class="children"><div class="content">agree with you.<p>all arguments about supposed bias fall flat when you start asking question about ROI of the &quot;debiasing work&quot;.<p>When you calculate $$$ required to de-bias a model, for example to make LLM recognize Syrian phone numbers: in compute and labor, and compare it to the market opportunity than the ROI is simply not there.<p>There is a good reason why LLMs are English-specific - because it is the largest market with biggest number of highest paying users for such LLM.<p>If there is no market demand in &quot;de-biased&quot; model that covers the cost of development, then trying to spend $$$ on de-biasing is pure waste of resources</div><br/></div></div></div></div><div id="41655472" class="c"><input type="checkbox" id="c-41655472" checked=""/><div class="controls bullet"><span class="by">slt2021</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41655037">parent</a><span>|</span><a href="#41655430">prev</a><span>|</span><a href="#41655691">next</a><span>|</span><label class="collapse" for="c-41655472">[-]</label><label class="expand" for="c-41655472">[1 more]</label></div><br/><div class="children"><div class="content">What you call bias, I call simply a representation of a training corpus. There is no broad agreement on how to quantify a bias of the model, other than try one-shot prompts like your &quot;who is the most hated Austrian painter?&quot;.<p>If there was no Germany-specific data in the training corpus - it is not fair to expect LLM to know anything about Germany.<p>You can check a foundation model from Chinese LLM researchers, and you will most likely see Sino-centric bias just because of the training corpus + synthetic data generation was focused on their native&#x2F;working language, and their goal was to create foundation model for their language.<p>I challenge any LLM sceptics - instead of just lazily poking holes in models - create a supposedly better model that reduces bias and lets evaluate your model with specific metrics</div><br/></div></div></div></div></div></div></div></div><div id="41655691" class="c"><input type="checkbox" id="c-41655691" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41652452">parent</a><span>|</span><a href="#41653153">prev</a><span>|</span><a href="#41653805">next</a><span>|</span><label class="collapse" for="c-41655691">[-]</label><label class="expand" for="c-41655691">[1 more]</label></div><br/><div class="children"><div class="content">The concrete example is that Meta opted everyone on their platform by default into providing content for their models without any consent.<p>The source and the quality of training data is important without looking for specific examples of a bias.</div><br/></div></div><div id="41653805" class="c"><input type="checkbox" id="c-41653805" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41652452">parent</a><span>|</span><a href="#41655691">prev</a><span>|</span><a href="#41655607">next</a><span>|</span><label class="collapse" for="c-41653805">[-]</label><label class="expand" for="c-41653805">[5 more]</label></div><br/><div class="children"><div class="content">Google’s and OpenAI often answered far-left, Progressive, and atheist. Google’s was censoring white people at one point. Facebook seems to espouse similar values. They’ve funded work to increase those values. Many mention topics relevant to these things in their papers in the bias or alignment sections.<p>These political systems don’t represent the majority of the world. They might not even represent half the U.S.. People relying on these A.I.’s might want to know if the A.I.’s are being intentionally trained to promote their creators’ views and&#x2F;or suppress dissenters’ views. Also, people from multiple sides of the political spectrum should review such data to make sure it’s balanced.</div><br/><div id="41655860" class="c"><input type="checkbox" id="c-41655860" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41653805">parent</a><span>|</span><a href="#41654000">next</a><span>|</span><label class="collapse" for="c-41655860">[-]</label><label class="expand" for="c-41655860">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re objectively correct but judging from your downvotes there seems to be some denial here about that! The atheism alone means it&#x27;s different from a big chunk of the world&#x27;s population, possibly the majority. Supposedly around 80% of the world&#x27;s population identify with a religion though I guess you can debate how many people are truly devout.<p>The good news is that the big AI labs seem to be slowly getting a grip on the misalignment of their safety teams. If you look at the extensive docs Meta provide for this model they do talk about safety training, and it&#x27;s finally of the reasonable and non-ideological kind. They&#x27;re trying to stop it from hacking computers, telling people how to build advanced weaponry and so on. There are valid use cases for all of those things, and you could argue there&#x27;s no point when the knowledge came from books+internet to begin with, but everyone can agree that there are at least genuine safety-related issues with those topics.<p>The possible exception here is Google. They seem to be the worst affected of all the big labs.</div><br/></div></div><div id="41654000" class="c"><input type="checkbox" id="c-41654000" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41653805">parent</a><span>|</span><a href="#41655860">prev</a><span>|</span><a href="#41655607">next</a><span>|</span><label class="collapse" for="c-41654000">[-]</label><label class="expand" for="c-41654000">[3 more]</label></div><br/><div class="children"><div class="content">this provocative parent-post may or may not be accurate, but what is missing IMHO is any characterization of the question asked or other context of use.. lacking that basic part to the inquiry, this statement alone is clearly amateurish, zealous and as said, provocative. Fighting in words is too easy! like falling off a log, as they say.. in politics it is almost unavoidable. Please, not start fires.<p>All that said yes, there are legitimate questions and there is social context. This forum is worth better questions.</div><br/><div id="41654452" class="c"><input type="checkbox" id="c-41654452" checked=""/><div class="controls bullet"><span class="by">sunaookami</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41654000">parent</a><span>|</span><a href="#41654044">next</a><span>|</span><label class="collapse" for="c-41654452">[-]</label><label class="expand" for="c-41654452">[1 more]</label></div><br/><div class="children"><div class="content">&gt;This forum is worth better questions<p>That&#x27;s not for you to decide if some question is &quot;worth&quot;. At least for OpenAI and Anthropic it is a fact that these models are pre-censored by the US government: <a href="https:&#x2F;&#x2F;www.cnbc.com&#x2F;2024&#x2F;08&#x2F;29&#x2F;openai-and-anthropic-agree-to-let-us-ai-safety-institute-test-models.html" rel="nofollow">https:&#x2F;&#x2F;www.cnbc.com&#x2F;2024&#x2F;08&#x2F;29&#x2F;openai-and-anthropic-agree-t...</a></div><br/></div></div><div id="41654044" class="c"><input type="checkbox" id="c-41654044" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#41650261">root</a><span>|</span><a href="#41654000">parent</a><span>|</span><a href="#41654452">prev</a><span>|</span><a href="#41655607">next</a><span>|</span><label class="collapse" for="c-41654044">[-]</label><label class="expand" for="c-41654044">[1 more]</label></div><br/><div class="children"><div class="content">I don’t have time to reproduce them. Fortunately, it’s easy for them to show how open and fair they are by publishing all training data. They could also publish the unaligned version or allow 3rd-party alignment.<p>Instead, they’re keeping it secret. That’s to conceal wrongdoing. Copyright infringement more than politics but still.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41655607" class="c"><input type="checkbox" id="c-41655607" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#41650261">prev</a><span>|</span><a href="#41652320">next</a><span>|</span><label class="collapse" for="c-41655607">[-]</label><label class="expand" for="c-41655607">[2 more]</label></div><br/><div class="children"><div class="content">Tried the 1B model with the &quot;think step by step&quot; prompt.<p>It gets &quot;which is larger: 9.11 or 9.9?&quot; right if it manages to mention that decimals need to be compared first in its step-by-step thinking. If it skips mentioning decimals, then it says 9.11 is larger.<p>It gets the strawberry question wrong even after enumerating all the letters correctly, probably because it can&#x27;t properly count.</div><br/><div id="41655797" class="c"><input type="checkbox" id="c-41655797" checked=""/><div class="controls bullet"><span class="by">khafra</span><span>|</span><a href="#41655607">parent</a><span>|</span><a href="#41652320">next</a><span>|</span><label class="collapse" for="c-41655797">[-]</label><label class="expand" for="c-41655797">[1 more]</label></div><br/><div class="children"><div class="content">Of course, in many contexts, it is correct to put 9.11 after 9.9--software versioning does it that way, for example.</div><br/></div></div></div></div><div id="41652320" class="c"><input type="checkbox" id="c-41652320" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#41655607">prev</a><span>|</span><a href="#41653581">next</a><span>|</span><label class="collapse" for="c-41652320">[-]</label><label class="expand" for="c-41652320">[4 more]</label></div><br/><div class="children"><div class="content">&quot;The Llama jumped over the ______!&quot; (Fence? River? Wall? Synagogue?)<p>With 1-hot encoding, the answer is &quot;wall&quot;, with 100% probability. Oh, you gave plausibility to &quot;fence&quot; too? WRONG! ENJOY MORE PENALTY, SCRUB!<p>I believe this unforgiving dynamic is why model distillation works well. The original teacher model had to learn via the &quot;hot or cold&quot; game on <i>text</i> answers. But when the child instead imitates the teacher&#x27;s predictions, it learns <i>semantically rich</i> answers. That strikes me as vastly more compute-efficient. So to me, it makes sense why these Llama 3.2 edge models punch so far above their weight(s). But it still blows my mind thinking how far models have advanced from a year or two ago. Kudos to Meta for these releases.</div><br/><div id="41653235" class="c"><input type="checkbox" id="c-41653235" checked=""/><div class="controls bullet"><span class="by">adtac</span><span>|</span><a href="#41652320">parent</a><span>|</span><a href="#41655611">next</a><span>|</span><label class="collapse" for="c-41653235">[-]</label><label class="expand" for="c-41653235">[1 more]</label></div><br/><div class="children"><div class="content">&gt;WRONG! ENJOY MORE PENALTY, SCRUB!<p>Is that true tho? During training, the model predicts {&quot;wall&quot;: 0.65, &quot;fence&quot;: 0.25, &quot;river&quot;: 0.03}. Then backprop modifies the weights such that it produces {&quot;wall&quot;: 0.67, &quot;fence&quot;: 0.24, &quot;river&quot;: 0.02} next time.<p>But it does that with a much richer feedback than WRONG! because we&#x27;re also telling the model how much more likely &quot;fence&quot; is than &quot;wall&quot; in an indirect way. It&#x27;s likely most of the neurons that supported &quot;wall&quot; also supported &quot;fence&quot;, so the average neuron that supported &quot;river&quot; gets penalised much more than a neuron that supported &quot;fence&quot;.<p>I agree that distillation is more efficient for exactly the same reason, but I think even models as old as GPT-3 use this trick to work as well as they do.</div><br/></div></div><div id="41655611" class="c"><input type="checkbox" id="c-41655611" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41652320">parent</a><span>|</span><a href="#41653235">prev</a><span>|</span><a href="#41653425">next</a><span>|</span><label class="collapse" for="c-41655611">[-]</label><label class="expand" for="c-41655611">[1 more]</label></div><br/><div class="children"><div class="content">yeah i mean that is exactly why distillation works. if you just were one hotting it would be the same as training on same dataset</div><br/></div></div><div id="41653425" class="c"><input type="checkbox" id="c-41653425" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41652320">parent</a><span>|</span><a href="#41655611">prev</a><span>|</span><a href="#41653581">next</a><span>|</span><label class="collapse" for="c-41653425">[-]</label><label class="expand" for="c-41653425">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t, they&#x27;re playing &quot;hide the #s&quot; a bit. Llama 3.2 3B is definitively worse than Phi-3 from May, both on any given metric and in an hour of playing with the 2, trying to justify moving to Llama 3.2 at 3B, given I&#x27;m adding Llama 3.2 at 1B.</div><br/></div></div></div></div><div id="41653581" class="c"><input type="checkbox" id="c-41653581" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#41652320">prev</a><span>|</span><a href="#41654614">next</a><span>|</span><label class="collapse" for="c-41653581">[-]</label><label class="expand" for="c-41653581">[7 more]</label></div><br/><div class="children"><div class="content">If anyone else is looking for the bigger models on ollama and wondering where they are, the Ollama blog post answered that for me.  The are &quot;coming soon&quot; so they just aren&#x27;t ready quite yet[1].  I was a little worried when I couldn&#x27;t find them but sounds like we just need to be patient.<p>[1]: <a href="https:&#x2F;&#x2F;ollama.com&#x2F;blog&#x2F;llama3.2">https:&#x2F;&#x2F;ollama.com&#x2F;blog&#x2F;llama3.2</a></div><br/><div id="41654922" class="c"><input type="checkbox" id="c-41654922" checked=""/><div class="controls bullet"><span class="by">Patrick_Devine</span><span>|</span><a href="#41653581">parent</a><span>|</span><a href="#41653654">next</a><span>|</span><label class="collapse" for="c-41654922">[-]</label><label class="expand" for="c-41654922">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re working on it. There are already draft PRs up in the GH repo. We&#x27;re still working out some kinks though.</div><br/></div></div><div id="41653654" class="c"><input type="checkbox" id="c-41653654" checked=""/><div class="controls bullet"><span class="by">xena</span><span>|</span><a href="#41653581">parent</a><span>|</span><a href="#41654922">prev</a><span>|</span><a href="#41654051">next</a><span>|</span><label class="collapse" for="c-41653654">[-]</label><label class="expand" for="c-41653654">[1 more]</label></div><br/><div class="children"><div class="content">As a rule of thumb with AI stuff: it either works instantly, or wait a day or two.</div><br/></div></div><div id="41654051" class="c"><input type="checkbox" id="c-41654051" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41653581">parent</a><span>|</span><a href="#41653654">prev</a><span>|</span><a href="#41654614">next</a><span>|</span><label class="collapse" for="c-41654051">[-]</label><label class="expand" for="c-41654051">[4 more]</label></div><br/><div class="children"><div class="content">ollama is &quot;just&quot; llama.cpp underneath, I recommend switching to LM Studio or Jan, they don&#x27;t have this issue of proprietary wrapper that obfuscates, you can just use any ol GGUF</div><br/><div id="41655149" class="c"><input type="checkbox" id="c-41655149" checked=""/><div class="controls bullet"><span class="by">calgoo</span><span>|</span><a href="#41653581">root</a><span>|</span><a href="#41654051">parent</a><span>|</span><a href="#41654248">next</a><span>|</span><label class="collapse" for="c-41655149">[-]</label><label class="expand" for="c-41655149">[2 more]</label></div><br/><div class="children"><div class="content">I use gguf in ollama on a daily basis, so not sure what the issue is? Just wrap it in a modelfile and done!</div><br/><div id="41656061" class="c"><input type="checkbox" id="c-41656061" checked=""/><div class="controls bullet"><span class="by">vorticalbox</span><span>|</span><a href="#41653581">root</a><span>|</span><a href="#41655149">parent</a><span>|</span><a href="#41654248">next</a><span>|</span><label class="collapse" for="c-41656061">[-]</label><label class="expand" for="c-41656061">[1 more]</label></div><br/><div class="children"><div class="content">I think because the larger models support images.</div><br/></div></div></div></div><div id="41654248" class="c"><input type="checkbox" id="c-41654248" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#41653581">root</a><span>|</span><a href="#41654051">parent</a><span>|</span><a href="#41655149">prev</a><span>|</span><a href="#41654614">next</a><span>|</span><label class="collapse" for="c-41654248">[-]</label><label class="expand" for="c-41654248">[1 more]</label></div><br/><div class="children"><div class="content">What proprietary wrapper? Isn&#x27;t Ollama entirely open source?</div><br/></div></div></div></div></div></div><div id="41654614" class="c"><input type="checkbox" id="c-41654614" checked=""/><div class="controls bullet"><span class="by">alanzhuly</span><span>|</span><a href="#41653581">prev</a><span>|</span><a href="#41652098">next</a><span>|</span><label class="collapse" for="c-41654614">[-]</label><label class="expand" for="c-41654614">[2 more]</label></div><br/><div class="children"><div class="content">Llama3.2 3B feels a lot better than other models with same size (e.g. Gemma2, Phi3.5-mini models).<p>For anyone looking for a simple way to test Llama3.2 3B locally with UI, Install nexa-sdk(<a href="https:&#x2F;&#x2F;github.com&#x2F;NexaAI&#x2F;nexa-sdk">https:&#x2F;&#x2F;github.com&#x2F;NexaAI&#x2F;nexa-sdk</a>) and type in terminal:<p>nexa run llama3.2 --streamlit<p>Disclaimer: I am from Nexa AI and nexa-sdk is an open-sourced. We&#x27;d love your feedback.</div><br/><div id="41655185" class="c"><input type="checkbox" id="c-41655185" checked=""/><div class="controls bullet"><span class="by">alfredgg</span><span>|</span><a href="#41654614">parent</a><span>|</span><a href="#41652098">next</a><span>|</span><label class="collapse" for="c-41655185">[-]</label><label class="expand" for="c-41655185">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a great tool. Thanks!<p>I had to test it with Llama3.1 and was really easy. At a first glance Llama3.2 didn&#x27;t seem available. The command you provided did not work, raising &quot;An error occurred while pulling the model: not enough values to unpack (expected 2, got 1)&quot;.</div><br/></div></div></div></div><div id="41652098" class="c"><input type="checkbox" id="c-41652098" checked=""/><div class="controls bullet"><span class="by">dhbradshaw</span><span>|</span><a href="#41654614">prev</a><span>|</span><a href="#41652475">next</a><span>|</span><label class="collapse" for="c-41652098">[-]</label><label class="expand" for="c-41652098">[15 more]</label></div><br/><div class="children"><div class="content">Tried out 3B on ollama, asking questions in optics, bio, and rust.<p>It&#x27;s super fast with a lot of knowledge, a large context and great understanding. Really impressive model.</div><br/><div id="41652340" class="c"><input type="checkbox" id="c-41652340" checked=""/><div class="controls bullet"><span class="by">tomComb</span><span>|</span><a href="#41652098">parent</a><span>|</span><a href="#41652786">next</a><span>|</span><label class="collapse" for="c-41652340">[-]</label><label class="expand" for="c-41652340">[10 more]</label></div><br/><div class="children"><div class="content">I question whether a 3B model can have “a lot of knowledge”.</div><br/><div id="41654144" class="c"><input type="checkbox" id="c-41654144" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41652340">parent</a><span>|</span><a href="#41653448">next</a><span>|</span><label class="collapse" for="c-41654144">[-]</label><label class="expand" for="c-41654144">[1 more]</label></div><br/><div class="children"><div class="content">As a point of comparison, the Llama 3.2 3B model is 6.5GB. The entirety of English wikipedia text is 19GB (as compressed with an algorithm from 1996, newer compression formats might do better).<p>Its not a perfect comparison and Llama does a lot more than English, but I would say 6.5GB of data can certainly contain a lot of knowledge.</div><br/></div></div><div id="41653448" class="c"><input type="checkbox" id="c-41653448" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41652340">parent</a><span>|</span><a href="#41654144">prev</a><span>|</span><a href="#41652557">next</a><span>|</span><label class="collapse" for="c-41653448">[-]</label><label class="expand" for="c-41653448">[4 more]</label></div><br/><div class="children"><div class="content">From quizzing it a bit it has good knowledge but limited reasoning. For example it will tell you all about the life and death of Ho Chi Minh (and as far as I can verify factual and with more detail than what&#x27;s in English Wikipedia), but when quizzed whether 2kg of feathers are heavier than 1kg of lead it will get it wrong.<p>Though I wouldn&#x27;t treat it as a domain expert on anything. For example when I asked about the safety advantages of Rust over Python it oversold Rust a bit and claimed Python had issues it doesn&#x27;t actually have</div><br/><div id="41654796" class="c"><input type="checkbox" id="c-41654796" checked=""/><div class="controls bullet"><span class="by">apitman</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41653448">parent</a><span>|</span><a href="#41654101">next</a><span>|</span><label class="collapse" for="c-41654796">[-]</label><label class="expand" for="c-41654796">[1 more]</label></div><br/><div class="children"><div class="content">&gt; it oversold Rust a bit and claimed Python had issues it doesn&#x27;t actually have<p>So exactly like a human</div><br/></div></div><div id="41654101" class="c"><input type="checkbox" id="c-41654101" checked=""/><div class="controls bullet"><span class="by">ravetcofx</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41653448">parent</a><span>|</span><a href="#41654796">prev</a><span>|</span><a href="#41652557">next</a><span>|</span><label class="collapse" for="c-41654101">[-]</label><label class="expand" for="c-41654101">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if spelling out the weight would work better. two kilogram for wider token input.</div><br/><div id="41654272" class="c"><input type="checkbox" id="c-41654272" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41654101">parent</a><span>|</span><a href="#41652557">next</a><span>|</span><label class="collapse" for="c-41654272">[-]</label><label class="expand" for="c-41654272">[1 more]</label></div><br/><div class="children"><div class="content">It still confidently said that the feathers were lighter than the lead. It did correct itself when I asked it to check again though.</div><br/></div></div></div></div></div></div><div id="41652557" class="c"><input type="checkbox" id="c-41652557" checked=""/><div class="controls bullet"><span class="by">foxhop</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41652340">parent</a><span>|</span><a href="#41653448">prev</a><span>|</span><a href="#41652786">next</a><span>|</span><label class="collapse" for="c-41652557">[-]</label><label class="expand" for="c-41652557">[4 more]</label></div><br/><div class="children"><div class="content">My guess is it uses the same vocabulary size as llama 3.1 which is 128,000 different tokens (words) to support many languages. Parameter count is less of an indicator of fitness than previously thought.</div><br/><div id="41652747" class="c"><input type="checkbox" id="c-41652747" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41652557">parent</a><span>|</span><a href="#41652786">next</a><span>|</span><label class="collapse" for="c-41652747">[-]</label><label class="expand" for="c-41652747">[3 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t address the thing they&#x27;re skeptical about, which is how much <i>knowledge</i> can be encoded in 3B parameters.<p>3B models are great for text manipulation, but I&#x27;ve found them to be pretty bad at having a broad understanding of pragmatics or any given subject. The larger models encode a lot more than just language in those 70B+ parameters.</div><br/><div id="41653079" class="c"><input type="checkbox" id="c-41653079" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41652747">parent</a><span>|</span><a href="#41652786">next</a><span>|</span><label class="collapse" for="c-41653079">[-]</label><label class="expand" for="c-41653079">[2 more]</label></div><br/><div class="children"><div class="content">Ok, but what we are probably debating is knowledge versus wisdom. Like, if I know 1+1 = 2, and I know the numbers 1 through 10, my knowledge is just 11, but my wisdom is infinite in the scope of integer addition. I can find any number, given enough time.<p>I&#x27;m pretty sure the AI guys are well aware of which types of models they want to produce. Models that can intake knowledge and intelligently manipulate it would mean general intelligence.<p>Models that can intake knowledge and only produce subsets of it&#x27;s training data have a use but wouldn&#x27;t be general intelligence.</div><br/><div id="41653195" class="c"><input type="checkbox" id="c-41653195" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41653079">parent</a><span>|</span><a href="#41652786">next</a><span>|</span><label class="collapse" for="c-41653195">[-]</label><label class="expand" for="c-41653195">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this is right.<p>Usually the problem is much simpler with small models: they have less factual information, period.<p>So they&#x27;ll do great at manipulating text, like extraction and summarization... but they&#x27;ll get factual questions wrong.<p>And to add to the concern above, the more coherent the smaller models are, the more likely they very competently tell you wrong information. Without the usual telltale degraded output of a smaller model it might be harder to pick out the inaccuracies.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41652786" class="c"><input type="checkbox" id="c-41652786" checked=""/><div class="controls bullet"><span class="by">create-username</span><span>|</span><a href="#41652098">parent</a><span>|</span><a href="#41652340">prev</a><span>|</span><a href="#41652475">next</a><span>|</span><label class="collapse" for="c-41652786">[-]</label><label class="expand" for="c-41652786">[4 more]</label></div><br/><div class="children"><div class="content">Can it speak foreign languages like German, Spanish, Ancient Greek?</div><br/><div id="41653335" class="c"><input type="checkbox" id="c-41653335" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41652786">parent</a><span>|</span><a href="#41652475">next</a><span>|</span><label class="collapse" for="c-41653335">[-]</label><label class="expand" for="c-41653335">[3 more]</label></div><br/><div class="children"><div class="content">Yes. It can converse perfectly normal in German. However when quizzed about German idioms it hallucinates them (in fluent German). Though that&#x27;s the kind of stuff even larger models often have trouble with. For example if you ask GPT 4 about jokes in German it will give you jokes that depend on word play that only works when translated to English. In normal conversation Llama seems to speak fluent German<p>For Ancient Greek I just asked it (in German) to translate its previous answer to Ancient Greek, and the answer looks like Greek and according to google translate is a serviceable translation. However Llama did add a cheeky &quot;Πηγή: Google Translate&quot; at the end (Πηγή means source). I know little about the differences between ancient and modern Greek, but it did struggle to translate modern terms like &quot;climate change&quot; or &quot;Hawaii&quot; and added them as annotations in brackets. So I&#x27;ll assume it at least tried to use Ancient Greek.<p>However it doesn&#x27;t like switching language mid-conversation. If you start a conversation in German and after a couple messages switch to English it will understand you but answer in German. Most models switch to answering in English in that situation</div><br/><div id="41655282" class="c"><input type="checkbox" id="c-41655282" checked=""/><div class="controls bullet"><span class="by">create-username</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41653335">parent</a><span>|</span><a href="#41652475">next</a><span>|</span><label class="collapse" for="c-41655282">[-]</label><label class="expand" for="c-41655282">[2 more]</label></div><br/><div class="children"><div class="content">Thank you very much for taking your time.<p>Your findings are Amazing! I have used ChatGPT to proofread compositions in German and French lately, but it would have never occurred to me that I should have tested ability to understand idioms, which are the cherry on the cake. I’ll have it a go<p>As for Ancient Greek or Latin, ChatGPT has provided consistent translations and great explanations but its compositions had errors that prevented me from using it in the classroom.<p>All in all, chatGPT is a great multilingual and polyglot dictionary and I’d be glad if I could even use it offline for more autonomy</div><br/><div id="41655445" class="c"><input type="checkbox" id="c-41655445" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#41652098">root</a><span>|</span><a href="#41655282">parent</a><span>|</span><a href="#41652475">next</a><span>|</span><label class="collapse" for="c-41655445">[-]</label><label class="expand" for="c-41655445">[1 more]</label></div><br/><div class="children"><div class="content">I have tried to use Llama3-7b and 70b, for Ancient Greek and it is very bad. I will test Llama 3.2, but GPT is great at that. You might want to generate 2 or 3 GPT translations of Ancient Greek and select the best sentences from each one. Alongside with some human corrections, and it is almost unbeatable by any human alone.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41652475" class="c"><input type="checkbox" id="c-41652475" checked=""/><div class="controls bullet"><span class="by">JohnHammersley</span><span>|</span><a href="#41652098">prev</a><span>|</span><a href="#41655405">next</a><span>|</span><label class="collapse" for="c-41652475">[-]</label><label class="expand" for="c-41652475">[1 more]</label></div><br/><div class="children"><div class="content">Ollama post: <a href="https:&#x2F;&#x2F;ollama.com&#x2F;blog&#x2F;llama3.2">https:&#x2F;&#x2F;ollama.com&#x2F;blog&#x2F;llama3.2</a></div><br/></div></div><div id="41655405" class="c"><input type="checkbox" id="c-41655405" checked=""/><div class="controls bullet"><span class="by">josephernest</span><span>|</span><a href="#41652475">prev</a><span>|</span><a href="#41652123">next</a><span>|</span><label class="collapse" for="c-41655405">[-]</label><label class="expand" for="c-41655405">[1 more]</label></div><br/><div class="children"><div class="content">Can it run with llama-cpp-python?
If so, where can we find and download the gguf files? Are they distributed directly by meta, or are they converted to gguf format by third parties?</div><br/></div></div><div id="41652123" class="c"><input type="checkbox" id="c-41652123" checked=""/><div class="controls bullet"><span class="by">kingkongjaffa</span><span>|</span><a href="#41655405">prev</a><span>|</span><a href="#41655059">next</a><span>|</span><label class="collapse" for="c-41652123">[-]</label><label class="expand" for="c-41652123">[6 more]</label></div><br/><div class="children"><div class="content">llama3.2:3b-instruct-q8_0 is performing better than 3.1 8b-q4 on my macbookpro M1. It&#x27;s faster and the results are better. It answered a few riddles and thought experiments better despite being 3b vs 8b.<p>I just removed my install of 3.1-8b.<p>my ollama list is currently:<p>$ ollama list<p>NAME                              ID              SIZE MODIFIED<p>llama3.2:3b-instruct-q8_0         e410b836fe61    3.4 GB 2 hours ago<p>gemma2:9b-instruct-q4_1           5bfc4cf059e2    6.0 GB 3 days ago<p>phi3.5:3.8b-mini-instruct-q8_0    8b50e8e1e216    4.1 GB 3 days ago<p>mxbai-embed-large:latest          468836162de7    669 MB 3 months ago</div><br/><div id="41654926" class="c"><input type="checkbox" id="c-41654926" checked=""/><div class="controls bullet"><span class="by">PhilippGille</span><span>|</span><a href="#41652123">parent</a><span>|</span><a href="#41654937">next</a><span>|</span><label class="collapse" for="c-41654926">[-]</label><label class="expand" for="c-41654926">[2 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t the _0 quantizations considered deprecated and _K_S or _K_M preferable?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;5425">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;5425</a></div><br/><div id="41654991" class="c"><input type="checkbox" id="c-41654991" checked=""/><div class="controls bullet"><span class="by">Patrick_Devine</span><span>|</span><a href="#41652123">root</a><span>|</span><a href="#41654926">parent</a><span>|</span><a href="#41654937">next</a><span>|</span><label class="collapse" for="c-41654991">[-]</label><label class="expand" for="c-41654991">[1 more]</label></div><br/><div class="children"><div class="content">For _K_S definitely not. We quantized 3b with q4_K_M since we were getting good results out of it. Officially Meta has only talked about quantization for 405b and hasn&#x27;t given any actual guidance for what the &quot;best&quot; quantization should be for the smaller models. With The 1b model we didn&#x27;t see good results with any of the 4b quantizations and went with q8_0 as the default.</div><br/></div></div></div></div><div id="41654937" class="c"><input type="checkbox" id="c-41654937" checked=""/><div class="controls bullet"><span class="by">aryehof</span><span>|</span><a href="#41652123">parent</a><span>|</span><a href="#41654926">prev</a><span>|</span><a href="#41652676">next</a><span>|</span><label class="collapse" for="c-41654937">[-]</label><label class="expand" for="c-41654937">[1 more]</label></div><br/><div class="children"><div class="content">On what basis do you use these different models?</div><br/></div></div><div id="41652676" class="c"><input type="checkbox" id="c-41652676" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#41652123">parent</a><span>|</span><a href="#41654937">prev</a><span>|</span><a href="#41655059">next</a><span>|</span><label class="collapse" for="c-41652676">[-]</label><label class="expand" for="c-41652676">[2 more]</label></div><br/><div class="children"><div class="content">For a second I read that as “<i>it</i> just removed my install of 3.1-8b” :D</div><br/><div id="41654844" class="c"><input type="checkbox" id="c-41654844" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41652123">root</a><span>|</span><a href="#41652676">parent</a><span>|</span><a href="#41655059">next</a><span>|</span><label class="collapse" for="c-41654844">[-]</label><label class="expand" for="c-41654844">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;KillianLucas&#x2F;open-interpreter&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;KillianLucas&#x2F;open-interpreter&#x2F;</a></div><br/></div></div></div></div></div></div><div id="41655059" class="c"><input type="checkbox" id="c-41655059" checked=""/><div class="controls bullet"><span class="by">Ey7NFZ3P0nzAe</span><span>|</span><a href="#41652123">prev</a><span>|</span><a href="#41655645">next</a><span>|</span><label class="collapse" for="c-41655059">[-]</label><label class="expand" for="c-41655059">[1 more]</label></div><br/><div class="children"><div class="content">Interesting that its scores are somewhat helow Pixtral 12B <a href="https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;pixtral-12b&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;pixtral-12b&#x2F;</a></div><br/></div></div><div id="41655645" class="c"><input type="checkbox" id="c-41655645" checked=""/><div class="controls bullet"><span class="by">sgt</span><span>|</span><a href="#41655059">prev</a><span>|</span><a href="#41653004">next</a><span>|</span><label class="collapse" for="c-41655645">[-]</label><label class="expand" for="c-41655645">[1 more]</label></div><br/><div class="children"><div class="content">Anyone on HN running models on their own local machines, like smaller Llama models or such? Or something else?</div><br/></div></div><div id="41653004" class="c"><input type="checkbox" id="c-41653004" checked=""/><div class="controls bullet"><span class="by">arnaudsm</span><span>|</span><a href="#41655645">prev</a><span>|</span><a href="#41652310">next</a><span>|</span><label class="collapse" for="c-41653004">[-]</label><label class="expand" for="c-41653004">[1 more]</label></div><br/><div class="children"><div class="content">Is there an up-to-date leaderboard with multiple LLM benchmarks?<p>Livebench and Lmsys are weeks behind and sometimes refuse to add some major models. And press releases like this cherry pick their benchmarks and ignore better models like qwen2.5.<p>If it doesn&#x27;t exist I&#x27;m willing to create it</div><br/></div></div><div id="41652310" class="c"><input type="checkbox" id="c-41652310" checked=""/><div class="controls bullet"><span class="by">kombine</span><span>|</span><a href="#41653004">prev</a><span>|</span><a href="#41652083">next</a><span>|</span><label class="collapse" for="c-41652310">[-]</label><label class="expand" for="c-41652310">[2 more]</label></div><br/><div class="children"><div class="content">Are these models suitable for Code assistance - as an alternative to Cursor or Copilot?</div><br/><div id="41654651" class="c"><input type="checkbox" id="c-41654651" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#41652310">parent</a><span>|</span><a href="#41652083">next</a><span>|</span><label class="collapse" for="c-41654651">[-]</label><label class="expand" for="c-41654651">[1 more]</label></div><br/><div class="children"><div class="content">I use Continue on VScode, works well with Ollama and llama3.1 (but obviously not as good as Claude).</div><br/></div></div></div></div><div id="41652083" class="c"><input type="checkbox" id="c-41652083" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41652310">prev</a><span>|</span><a href="#41652022">next</a><span>|</span><label class="collapse" for="c-41652083">[-]</label><label class="expand" for="c-41652083">[2 more]</label></div><br/><div class="children"><div class="content">Llama 3.2 includes a 1B parameter model. This should be 8x higher throughput for data pipelines. In our experience, smaller models are just fine for simple tasks like reading paragraphs from PDF documents.</div><br/></div></div><div id="41652022" class="c"><input type="checkbox" id="c-41652022" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#41652083">prev</a><span>|</span><a href="#41655253">next</a><span>|</span><label class="collapse" for="c-41652022">[-]</label><label class="expand" for="c-41652022">[11 more]</label></div><br/><div class="children"><div class="content">This is great!  Does anyone know if the llama models are trained to do function calling like openAI models are?  And&#x2F;or are there any function calling training datasets?</div><br/><div id="41652076" class="c"><input type="checkbox" id="c-41652076" checked=""/><div class="controls bullet"><span class="by">TmpstsTrrctta</span><span>|</span><a href="#41652022">parent</a><span>|</span><a href="#41652070">next</a><span>|</span><label class="collapse" for="c-41652076">[-]</label><label class="expand" for="c-41652076">[1 more]</label></div><br/><div class="children"><div class="content">They mention tool calling in the link for the smaller models, and compare to 8B levels of function calling in benchmarks here:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41651126">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41651126</a></div><br/></div></div><div id="41652070" class="c"><input type="checkbox" id="c-41652070" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41652022">parent</a><span>|</span><a href="#41652076">prev</a><span>|</span><a href="#41652088">next</a><span>|</span><label class="collapse" for="c-41652070">[-]</label><label class="expand" for="c-41652070">[6 more]</label></div><br/><div class="children"><div class="content">Yes (rationale: 3.1 was, would be strange to rollback.)<p>In general, you&#x27;ll do a ton of damage by constraining token generation to valid JSON - I&#x27;ve seen models as small as 800M handle JSON with that. It&#x27;s ~impossible to train constraining into it with remotely the same reliability -- you have to erase a ton of conversational training that makes it say ex. &quot;Sure! Here&#x27;s the JSON you requested:&quot;</div><br/><div id="41655228" class="c"><input type="checkbox" id="c-41655228" checked=""/><div class="controls bullet"><span class="by">noahbp</span><span>|</span><a href="#41652022">root</a><span>|</span><a href="#41652070">parent</a><span>|</span><a href="#41652200">next</a><span>|</span><label class="collapse" for="c-41655228">[-]</label><label class="expand" for="c-41655228">[1 more]</label></div><br/><div class="children"><div class="content">What kind of damage is done by constraining token generation to valid JSON?</div><br/></div></div><div id="41652200" class="c"><input type="checkbox" id="c-41652200" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#41652022">root</a><span>|</span><a href="#41652070">parent</a><span>|</span><a href="#41655228">prev</a><span>|</span><a href="#41652088">next</a><span>|</span><label class="collapse" for="c-41652200">[-]</label><label class="expand" for="c-41652200">[4 more]</label></div><br/><div class="children"><div class="content">What about OpenAI Structured Outputs? This seems to do exactly this.</div><br/><div id="41652284" class="c"><input type="checkbox" id="c-41652284" checked=""/><div class="controls bullet"><span class="by">zackangelo</span><span>|</span><a href="#41652022">root</a><span>|</span><a href="#41652200">parent</a><span>|</span><a href="#41652278">next</a><span>|</span><label class="collapse" for="c-41652284">[-]</label><label class="expand" for="c-41652284">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m building this type of functionality on top of Llama models if you&#x27;re interested: <a href="https:&#x2F;&#x2F;docs.mixlayer.com&#x2F;examples&#x2F;json-output" rel="nofollow">https:&#x2F;&#x2F;docs.mixlayer.com&#x2F;examples&#x2F;json-output</a></div><br/><div id="41652825" class="c"><input type="checkbox" id="c-41652825" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41652022">root</a><span>|</span><a href="#41652284">parent</a><span>|</span><a href="#41652278">next</a><span>|</span><label class="collapse" for="c-41652825">[-]</label><label class="expand" for="c-41652825">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m writing a Flutter AI client app, integrates with llama.cpp. I used a PoC of llama.cpp running in WASM, I&#x27;m desperate to signal the app is agnostic to AI provider,  but it was horrifically slow, ended up backing out to WebMLC.<p>What are you doing underneath, here? If thats secret sauce, I&#x27;m curious what you&#x27;re seeing in tokens&#x2F;sec on ex. a phone vs. MacBook M-series.<p>Or are you deploying on servers?</div><br/></div></div></div></div><div id="41652278" class="c"><input type="checkbox" id="c-41652278" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41652022">root</a><span>|</span><a href="#41652200">parent</a><span>|</span><a href="#41652284">prev</a><span>|</span><a href="#41652088">next</a><span>|</span><label class="collapse" for="c-41652278">[-]</label><label class="expand" for="c-41652278">[1 more]</label></div><br/><div class="children"><div class="content">Correct, I think so too, seemed that update must be doing exactly this. tl;dr: in the context of Llama fn calling reliability, you don&#x27;t need to reach for training, in fact, you&#x27;ll do it and still have the same problem.</div><br/></div></div></div></div></div></div><div id="41652088" class="c"><input type="checkbox" id="c-41652088" checked=""/><div class="controls bullet"><span class="by">ushakov</span><span>|</span><a href="#41652022">parent</a><span>|</span><a href="#41652070">prev</a><span>|</span><a href="#41655253">next</a><span>|</span><label class="collapse" for="c-41652088">[-]</label><label class="expand" for="c-41652088">[3 more]</label></div><br/><div class="children"><div class="content">yes, but only the text-only models!<p><a href="https:&#x2F;&#x2F;www.llama.com&#x2F;docs&#x2F;model-cards-and-prompt-formats&#x2F;llama3_2#-tool-calling-(1b&#x2F;3b)-" rel="nofollow">https:&#x2F;&#x2F;www.llama.com&#x2F;docs&#x2F;model-cards-and-prompt-formats&#x2F;ll...</a></div><br/><div id="41652263" class="c"><input type="checkbox" id="c-41652263" checked=""/><div class="controls bullet"><span class="by">zackangelo</span><span>|</span><a href="#41652022">root</a><span>|</span><a href="#41652088">parent</a><span>|</span><a href="#41652286">next</a><span>|</span><label class="collapse" for="c-41652263">[-]</label><label class="expand" for="c-41652263">[1 more]</label></div><br/><div class="children"><div class="content">This is incorrect:<p>&gt; With text-only inputs, the Llama 3.2 Vision Models can do tool-calling exactly like their Llama 3.1 Text Model counterparts. You can use either the system or user prompts to provide the function definitions.<p>&gt; Currently the vision models don’t support tool-calling with text+image inputs.<p>They support it, but not when an image is submitted in the prompt. I&#x27;d be curious to see what the model does. Meta typically sets conservative expectations around this type of behavior (e.g., they say that the 3.1 8b model won&#x27;t do multiple tool calls, but in my experience it does so just fine).</div><br/></div></div><div id="41652286" class="c"><input type="checkbox" id="c-41652286" checked=""/><div class="controls bullet"><span class="by">winddude</span><span>|</span><a href="#41652022">root</a><span>|</span><a href="#41652088">parent</a><span>|</span><a href="#41652263">prev</a><span>|</span><a href="#41655253">next</a><span>|</span><label class="collapse" for="c-41652286">[-]</label><label class="expand" for="c-41652286">[1 more]</label></div><br/><div class="children"><div class="content">the vision models can also do tool calling according to the docs, but with text-only inputs, maybe that&#x27;s what you meant ~ &lt;<a href="https:&#x2F;&#x2F;www.llama.com&#x2F;docs&#x2F;model-cards-and-prompt-formats&#x2F;llama3_2&#x2F;#-tool-calling-" rel="nofollow">https:&#x2F;&#x2F;www.llama.com&#x2F;docs&#x2F;model-cards-and-prompt-formats&#x2F;ll...</a>&gt;</div><br/></div></div></div></div></div></div><div id="41653826" class="c"><input type="checkbox" id="c-41653826" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#41655253">prev</a><span>|</span><a href="#41652112">next</a><span>|</span><label class="collapse" for="c-41653826">[-]</label><label class="expand" for="c-41653826">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m currently fighting with a fastapi python app deployed to render. It&#x27;s interesting because I&#x27;m struggling to see how I encode the image and send it using curl. Their example sends directly from the browser and uses a data uri.<p>But, this is relevant because I&#x27;m curious how this new model allows image inputs. Do you paste a base64 image into the prompt?<p>It feels like these models can start not only providing the text generation backend, but start to replace the infrastructure for the API as well.<p>Can you input images without something in front of it like openwebui?</div><br/></div></div><div id="41652112" class="c"><input type="checkbox" id="c-41652112" checked=""/><div class="controls bullet"><span class="by">getcrunk</span><span>|</span><a href="#41653826">prev</a><span>|</span><a href="#41652739">next</a><span>|</span><label class="collapse" for="c-41652112">[-]</label><label class="expand" for="c-41652112">[7 more]</label></div><br/><div class="children"><div class="content">Still no 14&#x2F;30b parameter models since llama 2. Seriously killing real usability for power users&#x2F;diy.<p>The 7&#x2F;8B models are great for poc and moving to edge for minor use cases … but there’s a big and empty gap till 70b that most people can’t run.<p>The tin foil hat in me is saying this is the compromise the powers that be have agreed too. Basically being “open” but practically gimped for average joe techie. Basically arms control</div><br/><div id="41652822" class="c"><input type="checkbox" id="c-41652822" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#41652112">parent</a><span>|</span><a href="#41652171">next</a><span>|</span><label class="collapse" for="c-41652822">[-]</label><label class="expand" for="c-41652822">[1 more]</label></div><br/><div class="children"><div class="content">The Llama 3.2 11B multimodal model is a bit less than 14B but smaller models can do more these days, and Meta are not the only ones making models.
The 70B model has been pruned down by NVIDIA if I recall correctly.
The 405B model also will be shrunk down and can presumably be used to strengthen smaller models. I&#x27;m not convinced by your shiny hat.</div><br/></div></div><div id="41652171" class="c"><input type="checkbox" id="c-41652171" checked=""/><div class="controls bullet"><span class="by">swader999</span><span>|</span><a href="#41652112">parent</a><span>|</span><a href="#41652822">prev</a><span>|</span><a href="#41652593">next</a><span>|</span><label class="collapse" for="c-41652171">[-]</label><label class="expand" for="c-41652171">[2 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need an F-15 to play at least, a decent sniper rifle will do. You can still practise even with a pellet gun. I&#x27;m running 70b models on my M2 max with 96 ram. Even larger models sort of work, although I haven&#x27;t really put much time into anything above 70b.</div><br/><div id="41652871" class="c"><input type="checkbox" id="c-41652871" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#41652112">root</a><span>|</span><a href="#41652171">parent</a><span>|</span><a href="#41652593">next</a><span>|</span><label class="collapse" for="c-41652871">[-]</label><label class="expand" for="c-41652871">[1 more]</label></div><br/><div class="children"><div class="content">With a 128Gb Mac, you can even run 405b at 1-bit quantization - it&#x27;s large enough that even with the considerable quality drop that entails, it still appears to be smarter than 70b.</div><br/></div></div></div></div><div id="41652593" class="c"><input type="checkbox" id="c-41652593" checked=""/><div class="controls bullet"><span class="by">foxhop</span><span>|</span><a href="#41652112">parent</a><span>|</span><a href="#41652171">prev</a><span>|</span><a href="#41652739">next</a><span>|</span><label class="collapse" for="c-41652593">[-]</label><label class="expand" for="c-41652593">[3 more]</label></div><br/><div class="children"><div class="content">4090 has 24G<p>So we really need ~40B or G model (two cards) or like a ~20B with some room for context window.<p>5090 has ??G - still unreleased</div><br/><div id="41655505" class="c"><input type="checkbox" id="c-41655505" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#41652112">root</a><span>|</span><a href="#41652593">parent</a><span>|</span><a href="#41652767">next</a><span>|</span><label class="collapse" for="c-41655505">[-]</label><label class="expand" for="c-41655505">[1 more]</label></div><br/><div class="children"><div class="content">Qwen2.5 has a 32B release, and quantised at q5_k_m it *just about&quot; completely fills a 4090.<p>It&#x27;s a good model, too.</div><br/></div></div></div></div></div></div><div id="41652739" class="c"><input type="checkbox" id="c-41652739" checked=""/><div class="controls bullet"><span class="by">thimabi</span><span>|</span><a href="#41652112">prev</a><span>|</span><a href="#41652685">next</a><span>|</span><label class="collapse" for="c-41652739">[-]</label><label class="expand" for="c-41652739">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone know how these models fare in terms of multilingual real-world usage? I’ve used previous iterations of llama models and they all seemed to be lacking in that regard.</div><br/></div></div><div id="41652685" class="c"><input type="checkbox" id="c-41652685" checked=""/><div class="controls bullet"><span class="by">gunalx</span><span>|</span><a href="#41652739">prev</a><span>|</span><a href="#41655366">next</a><span>|</span><label class="collapse" for="c-41652685">[-]</label><label class="expand" for="c-41652685">[1 more]</label></div><br/><div class="children"><div class="content">3b was pretty good at multimodal (Norwegian) still a lot of gibberish at times, and way more sensitive than 8b but more usable than Gemma 2 2b at multi modal, fine at my python list sorter with args standard question. But 90b vision just refuses all my actually useful tasks like helping recreate the images in html or do anything useful with the image data other than describing it. Have not gotten as stuck with 70b or openai before. Insane amount of refusals all the time.</div><br/></div></div><div id="41655366" class="c"><input type="checkbox" id="c-41655366" checked=""/><div class="controls bullet"><span class="by">stogot</span><span>|</span><a href="#41652685">prev</a><span>|</span><a href="#41654212">next</a><span>|</span><label class="collapse" for="c-41655366">[-]</label><label class="expand" for="c-41655366">[1 more]</label></div><br/><div class="children"><div class="content">Surprised no mention of audio?</div><br/></div></div><div id="41654212" class="c"><input type="checkbox" id="c-41654212" checked=""/><div class="controls bullet"><span class="by">bombi</span><span>|</span><a href="#41655366">prev</a><span>|</span><a href="#41653751">next</a><span>|</span><label class="collapse" for="c-41654212">[-]</label><label class="expand" for="c-41654212">[2 more]</label></div><br/><div class="children"><div class="content">Is Termux enough to run the 1B model on Android?</div><br/><div id="41654409" class="c"><input type="checkbox" id="c-41654409" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#41654212">parent</a><span>|</span><a href="#41653751">next</a><span>|</span><label class="collapse" for="c-41654409">[-]</label><label class="expand" for="c-41654409">[1 more]</label></div><br/><div class="children"><div class="content">depends on your phone, but try a couple of these variants with ollama <a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;llama3.2&#x2F;tags">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;llama3.2&#x2F;tags</a><p>e.g. `ollama run llama3.2:1b-instruct-q4_0`</div><br/></div></div></div></div><div id="41653751" class="c"><input type="checkbox" id="c-41653751" checked=""/><div class="controls bullet"><span class="by">notpublic</span><span>|</span><a href="#41654212">prev</a><span>|</span><a href="#41652219">next</a><span>|</span><label class="collapse" for="c-41653751">[-]</label><label class="expand" for="c-41653751">[4 more]</label></div><br/><div class="children"><div class="content">Llama-3.2-11B-Vision-Instruct does an excellent job extracting&#x2F;answering questions from screenshots. It is even able to answer questions  based on information buried inside a flowchart. How is this even possible??</div><br/><div id="41655259" class="c"><input type="checkbox" id="c-41655259" checked=""/><div class="controls bullet"><span class="by">Ey7NFZ3P0nzAe</span><span>|</span><a href="#41653751">parent</a><span>|</span><a href="#41654730">next</a><span>|</span><label class="collapse" for="c-41655259">[-]</label><label class="expand" for="c-41655259">[1 more]</label></div><br/><div class="children"><div class="content">Because they trained the text model. Then froze the weights. Then trained a vision model on text image pairs of progressively higher quality. Then trained an adapter to align their latent spaces. So it became smart on text then gain a new input sense magically without changing its weights</div><br/></div></div><div id="41654730" class="c"><input type="checkbox" id="c-41654730" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#41653751">parent</a><span>|</span><a href="#41655259">prev</a><span>|</span><a href="#41654668">next</a><span>|</span><label class="collapse" for="c-41654730">[-]</label><label class="expand" for="c-41654730">[1 more]</label></div><br/><div class="children"><div class="content">Oh, this is promising. It&#x27;s not surprising to me: image models have been very oriented towards photography and scene understanding rather than understanding symbolic information in images (like text or diagrams), but I always thought that it should be possible to make the model better at the latter, for instance by training it more on historical handwritten documents.</div><br/></div></div><div id="41654668" class="c"><input type="checkbox" id="c-41654668" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#41653751">parent</a><span>|</span><a href="#41654730">prev</a><span>|</span><a href="#41652219">next</a><span>|</span><label class="collapse" for="c-41654668">[-]</label><label class="expand" for="c-41654668">[1 more]</label></div><br/><div class="children"><div class="content">magic</div><br/></div></div></div></div><div id="41652219" class="c"><input type="checkbox" id="c-41652219" checked=""/><div class="controls bullet"><span class="by">sk11001</span><span>|</span><a href="#41653751">prev</a><span>|</span><a href="#41653123">next</a><span>|</span><label class="collapse" for="c-41652219">[-]</label><label class="expand" for="c-41652219">[2 more]</label></div><br/><div class="children"><div class="content">Can one of thse models be run on a single machine? What specs do you need?</div><br/><div id="41652249" class="c"><input type="checkbox" id="c-41652249" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#41652219">parent</a><span>|</span><a href="#41653123">next</a><span>|</span><label class="collapse" for="c-41652249">[-]</label><label class="expand" for="c-41652249">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely! They have a billion-parameter model that will run on my first computer if we quantize it to 1.5 bits. But realistically yes, if you can fit in total ram you can run it slowly, if you can fit it in gpu ram you can probably run it fast enough to chat.</div><br/></div></div></div></div><div id="41653123" class="c"><input type="checkbox" id="c-41653123" checked=""/><div class="controls bullet"><span class="by">404mm</span><span>|</span><a href="#41652219">prev</a><span>|</span><a href="#41652081">next</a><span>|</span><label class="collapse" for="c-41653123">[-]</label><label class="expand" for="c-41653123">[6 more]</label></div><br/><div class="children"><div class="content">Can anyone recommend a webUI client for ollama?</div><br/><div id="41655272" class="c"><input type="checkbox" id="c-41655272" checked=""/><div class="controls bullet"><span class="by">Ey7NFZ3P0nzAe</span><span>|</span><a href="#41653123">parent</a><span>|</span><a href="#41655153">next</a><span>|</span><label class="collapse" for="c-41655272">[-]</label><label class="expand" for="c-41655272">[1 more]</label></div><br/><div class="children"><div class="content">Open webui has promising aspects, the same authors are pushing for &quot;pipelines&quot; which are a standard for how inputs and outputs are modified on the fly for different purposes.</div><br/></div></div><div id="41655153" class="c"><input type="checkbox" id="c-41655153" checked=""/><div class="controls bullet"><span class="by">fungi</span><span>|</span><a href="#41653123">parent</a><span>|</span><a href="#41655272">prev</a><span>|</span><a href="#41653201">next</a><span>|</span><label class="collapse" for="c-41655153">[-]</label><label class="expand" for="c-41655153">[1 more]</label></div><br/><div class="children"><div class="content">ive been using <a href="https:&#x2F;&#x2F;github.com&#x2F;valiantlynx&#x2F;ollama-docker">https:&#x2F;&#x2F;github.com&#x2F;valiantlynx&#x2F;ollama-docker</a> which comes with <a href="https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui">https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui</a></div><br/></div></div><div id="41653201" class="c"><input type="checkbox" id="c-41653201" checked=""/><div class="controls bullet"><span class="by">papascrubs</span><span>|</span><a href="#41653123">parent</a><span>|</span><a href="#41655153">prev</a><span>|</span><a href="#41653138">next</a><span>|</span><label class="collapse" for="c-41653201">[-]</label><label class="expand" for="c-41653201">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;get.big-agi.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;get.big-agi.com&#x2F;</a></div><br/></div></div><div id="41653138" class="c"><input type="checkbox" id="c-41653138" checked=""/><div class="controls bullet"><span class="by">iKlsR</span><span>|</span><a href="#41653123">parent</a><span>|</span><a href="#41653201">prev</a><span>|</span><a href="#41652081">next</a><span>|</span><label class="collapse" for="c-41653138">[-]</label><label class="expand" for="c-41653138">[2 more]</label></div><br/><div class="children"><div class="content">openwebui</div><br/><div id="41653202" class="c"><input type="checkbox" id="c-41653202" checked=""/><div class="controls bullet"><span class="by">404mm</span><span>|</span><a href="#41653123">root</a><span>|</span><a href="#41653138">parent</a><span>|</span><a href="#41652081">next</a><span>|</span><label class="collapse" for="c-41653202">[-]</label><label class="expand" for="c-41653202">[1 more]</label></div><br/><div class="children"><div class="content">Nice one. Thank you .. it looks like ChatGPT (not that there’s anything wrong with that)</div><br/></div></div></div></div></div></div><div id="41652081" class="c"><input type="checkbox" id="c-41652081" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41653123">prev</a><span>|</span><a href="#41652983">next</a><span>|</span><label class="collapse" for="c-41652081">[-]</label><label class="expand" for="c-41652081">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve just tested the 1B and 3B at Q8, some interesting bits:<p>- The 1B is extremely coherent (feels something like maybe Mistral 7B at 4 bits), and with flash attention and 4 bit KV cache it only uses about 4.2 GB of VRAM for 128k context<p>- A Pi 5 runs the 1B at 8.4 tok&#x2F;s, haven&#x27;t tested the 3B yet but it might need a lower quant to fit it and with 9T training tokens it&#x27;ll probably degrade pretty badly<p>- The 3B is a certified Gemma-2-2B killer<p>Given that llama.cpp doesn&#x27;t support any multimodality (they removed the old implementation), it might be a while before the 11B and 90B become runnable. Doesn&#x27;t seem like they outperform Qwen-2-VL at vision benchmarks though.</div><br/><div id="41652242" class="c"><input type="checkbox" id="c-41652242" checked=""/><div class="controls bullet"><span class="by">Patrick_Devine</span><span>|</span><a href="#41652081">parent</a><span>|</span><a href="#41652983">next</a><span>|</span><label class="collapse" for="c-41652242">[-]</label><label class="expand" for="c-41652242">[2 more]</label></div><br/><div class="children"><div class="content">Hoping to get this out soon w&#x2F; Ollama. Just working out a couple of last kinks. The 11b model is legit good though, particularly for tasks like OCR. It can actually read my cursive handwriting.</div><br/><div id="41655709" class="c"><input type="checkbox" id="c-41655709" checked=""/><div class="controls bullet"><span class="by">jsarv</span><span>|</span><a href="#41652081">root</a><span>|</span><a href="#41652242">parent</a><span>|</span><a href="#41652983">next</a><span>|</span><label class="collapse" for="c-41655709">[-]</label><label class="expand" for="c-41655709">[1 more]</label></div><br/><div class="children"><div class="content">Naah, Qwen2-VL-7b still is much much better than 11b model for handwritten OCR from what i have tested. The 11b model hallucinates in case of handwritten OCR.</div><br/></div></div></div></div></div></div><div id="41652983" class="c"><input type="checkbox" id="c-41652983" checked=""/><div class="controls bullet"><span class="by">oulipo</span><span>|</span><a href="#41652081">prev</a><span>|</span><a href="#41652345">next</a><span>|</span><label class="collapse" for="c-41652983">[-]</label><label class="expand" for="c-41652983">[2 more]</label></div><br/><div class="children"><div class="content">Can the 3B run on a M1 macbook? It seems that it hogs all the memory. The 1B runs fine</div><br/><div id="41655084" class="c"><input type="checkbox" id="c-41655084" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#41652983">parent</a><span>|</span><a href="#41652345">next</a><span>|</span><label class="collapse" for="c-41655084">[-]</label><label class="expand" for="c-41655084">[1 more]</label></div><br/><div class="children"><div class="content">It all depends on the amount of RAM. Since you&#x27;re not mentioning it, you probably have only the default 8GB? 
Still, the 3B should run even on an 8GB machine.</div><br/></div></div></div></div><div id="41652345" class="c"><input type="checkbox" id="c-41652345" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#41652983">prev</a><span>|</span><a href="#41654845">next</a><span>|</span><label class="collapse" for="c-41652345">[-]</label><label class="expand" for="c-41652345">[2 more]</label></div><br/><div class="children"><div class="content">What mobile devices can the smaller models run on? iPhone, Android?</div><br/><div id="41653793" class="c"><input type="checkbox" id="c-41653793" checked=""/><div class="controls bullet"><span class="by">jillion</span><span>|</span><a href="#41652345">parent</a><span>|</span><a href="#41654845">next</a><span>|</span><label class="collapse" for="c-41653793">[-]</label><label class="expand" for="c-41653793">[1 more]</label></div><br/><div class="children"><div class="content">apparently so, but im trying to find a working example &#x2F; some details on what specific iOS &#x2F; android devices are capable of running this</div><br/></div></div></div></div><div id="41654845" class="c"><input type="checkbox" id="c-41654845" checked=""/><div class="controls bullet"><span class="by">desireco42</span><span>|</span><a href="#41652345">prev</a><span>|</span><label class="collapse" for="c-41654845">[-]</label><label class="expand" for="c-41654845">[1 more]</label></div><br/><div class="children"><div class="content">I have to say that running this model locally I was pleasantly suprised how well it ran, it doesn&#x27;t use as much resources and produce decent output, comparable to ChatGPT, it is not quite as OpenAI but for a lot of tasks, since it doesn&#x27;t burden the computer, it can be used with local model.<p>Next I want to try to use Aider with it and see how this would work.</div><br/></div></div></div></div></div></div></div></body></html>