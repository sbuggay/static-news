<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700902853467" as="style"/><link rel="stylesheet" href="styles.css?v=1700902853467"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://dl.acm.org/doi/abs/10.1145/3445814.3446724">PacketMill: Toward per-Core 100-Gbps networking (2021)</a> <span class="domain">(<a href="https://dl.acm.org">dl.acm.org</a>)</span></div><div class="subtext"><span>teleforce</span> | <span>23 comments</span></div><br/><div><div id="38410485" class="c"><input type="checkbox" id="c-38410485" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#38410222">next</a><span>|</span><label class="collapse" for="c-38410485">[-]</label><label class="expand" for="c-38410485">[15 more]</label></div><br/><div class="children"><div class="content">Only somewhat related, I&#x27;ve been wondering if you could use zero copy networking with io_uring to send the same bytes over and over. Say you have some data that needs to be send to the different clients (imagine maybe it&#x27;s a video game sending updates to players), from what I can tell you should be able to create a &quot;fixed&quot; buffer with this contents and re-use it between calls in the ring. Not only would this be zero copy, but you also wouldn&#x27;t ever need to touch the data at all to send it after the first time. (normally, zero copy is more like 1 copy, since you need to copy it in to the kernel allocated buffer even with zero copy).<p>My understanding is that there&#x27;s some contention from the locking involved though which can make this not scale well, but maybe that could be avoided (e.g., maybe by trying to have per core buffers used by the kernel, although I&#x27;m not sure you&#x27;d have any control over the kernel threads).</div><br/><div id="38411984" class="c"><input type="checkbox" id="c-38411984" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#38410485">parent</a><span>|</span><a href="#38411363">next</a><span>|</span><label class="collapse" for="c-38411984">[-]</label><label class="expand" for="c-38411984">[3 more]</label></div><br/><div class="children"><div class="content">Multicast (and broadcast) was supposed to do things like this, but I have rarely seen it used for any sort of &quot;visible&quot; client application.<p>(I mean visible like a videoconferencing application as opposed to something like invisible multicast dns)</div><br/><div id="38412169" class="c"><input type="checkbox" id="c-38412169" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#38410485">root</a><span>|</span><a href="#38411984">parent</a><span>|</span><a href="#38411989">next</a><span>|</span><label class="collapse" for="c-38412169">[-]</label><label class="expand" for="c-38412169">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Multicast was supposed to do things like this, but I have rarely seen it used for any sort of &quot;visible&quot; client application.<p>Well multicast is not possible on internet so I guess that&#x27;s why you don&#x27;t see it often.<p>In private network centric protocols though (e.g. finance), multicast is ubiquitous.</div><br/></div></div><div id="38411989" class="c"><input type="checkbox" id="c-38411989" checked=""/><div class="controls bullet"><span class="by">bpye</span><span>|</span><a href="#38410485">root</a><span>|</span><a href="#38411984">parent</a><span>|</span><a href="#38412169">prev</a><span>|</span><a href="#38411363">next</a><span>|</span><label class="collapse" for="c-38411989">[-]</label><label class="expand" for="c-38411989">[1 more]</label></div><br/><div class="children"><div class="content">Multicast does get used for IPTV doesn’t it?</div><br/></div></div></div></div><div id="38411363" class="c"><input type="checkbox" id="c-38411363" checked=""/><div class="controls bullet"><span class="by">10000truths</span><span>|</span><a href="#38410485">parent</a><span>|</span><a href="#38411984">prev</a><span>|</span><a href="#38411396">next</a><span>|</span><label class="collapse" for="c-38411363">[-]</label><label class="expand" for="c-38411363">[2 more]</label></div><br/><div class="children"><div class="content">I imagine that wouldn&#x27;t be very useful, since most production traffic is encrypted (e.g. TLS) and key negotiations necessitate a per-client cipher stream.</div><br/><div id="38411410" class="c"><input type="checkbox" id="c-38411410" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#38410485">root</a><span>|</span><a href="#38411363">parent</a><span>|</span><a href="#38411396">next</a><span>|</span><label class="collapse" for="c-38411410">[-]</label><label class="expand" for="c-38411410">[1 more]</label></div><br/><div class="children"><div class="content">Advanced NICs can do bulk TLS encryption... probably other encryption if you work with the manufacturer. See the work Netflix CDN has done, at high bitrates, memory bandwidth is a major bottleneck for them, eliminating the memory reads and writes to do TLS in software (user level or kernel level) allowed them to hit much higher speeds.</div><br/></div></div></div></div><div id="38411396" class="c"><input type="checkbox" id="c-38411396" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#38410485">parent</a><span>|</span><a href="#38411363">prev</a><span>|</span><a href="#38411090">next</a><span>|</span><label class="collapse" for="c-38411396">[-]</label><label class="expand" for="c-38411396">[1 more]</label></div><br/><div class="children"><div class="content">&gt; although I&#x27;m not sure you&#x27;d have any control over the kernel threads<p>You don&#x27;t have control from the application, but if you&#x27;re really trying to get the most performance from your networking heavy application, you want to set up your server where the NIC rx and tx queues are cpu pinned, and the application thread that processes any given socket is pinned to the same cpu that the kernel is using for that socket. Then there&#x27;s no cross cpu traffic for that socket (if all else goes well).<p>Hopefully all the memory used is NUMA local for the CPU and the NIC as well. That gets trickier with multiple socket systems, although some NICs do have provisions to connect to two PCIe slots so they can be NUMA local to two sockets.</div><br/></div></div><div id="38411090" class="c"><input type="checkbox" id="c-38411090" checked=""/><div class="controls bullet"><span class="by">chatmasta</span><span>|</span><a href="#38410485">parent</a><span>|</span><a href="#38411396">prev</a><span>|</span><a href="#38410676">next</a><span>|</span><label class="collapse" for="c-38411090">[-]</label><label class="expand" for="c-38411090">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. But wouldn&#x27;t this mean some clients would need to wait a long time for their download to start? (I&#x27;m imagining a multi-gigabyte game update - but maybe you meant a continuous stream of in-game positional updates of players.) They basically idle until the next &quot;train&quot; shows up at their station, and then they hop on, right? Also, if they have a network hiccup, they can&#x27;t resume their download until one of the trains reaches the point where their download failed.<p>(That is, if you get my weird train analogy, and if I&#x27;m understanding your idea correctly.)</div><br/></div></div><div id="38410676" class="c"><input type="checkbox" id="c-38410676" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#38410485">parent</a><span>|</span><a href="#38411090">prev</a><span>|</span><a href="#38411008">next</a><span>|</span><label class="collapse" for="c-38410676">[-]</label><label class="expand" for="c-38410676">[2 more]</label></div><br/><div class="children"><div class="content">This sounds pretty straightforward using sendfile(); you shouldn&#x27;t even need io_uring.</div><br/><div id="38411017" class="c"><input type="checkbox" id="c-38411017" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#38410485">root</a><span>|</span><a href="#38410676">parent</a><span>|</span><a href="#38411008">next</a><span>|</span><label class="collapse" for="c-38411017">[-]</label><label class="expand" for="c-38411017">[1 more]</label></div><br/><div class="children"><div class="content">I guess you would have some kind of in memory fd that you would write to and then use with sendfile calls? I&#x27;m not sure if you&#x27;d run into similar contention on the data structures involved there. I guess it would depend on specifically what mechanism you&#x27;re using to store the data (something like tmpfs?), but probably doesn&#x27;t have to involve contention? (as there wouldn&#x27;t have to be any refcounting if you control the lifetime of the data).<p>I was thinking of a situation where you want to also limit the system call overhead per send. (think maybe there&#x27;s some number of 64k-ish chunks that you want to send to different clients, so each connection would involve multiple write per frame etc.,)</div><br/></div></div></div></div><div id="38411008" class="c"><input type="checkbox" id="c-38411008" checked=""/><div class="controls bullet"><span class="by">nelsondev</span><span>|</span><a href="#38410485">parent</a><span>|</span><a href="#38410676">prev</a><span>|</span><a href="#38410517">next</a><span>|</span><label class="collapse" for="c-38411008">[-]</label><label class="expand" for="c-38411008">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of a Kafka topic, where different Consumers each have their own commit that indicates how far along the stream they’ve caught up to.</div><br/></div></div><div id="38410517" class="c"><input type="checkbox" id="c-38410517" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#38410485">parent</a><span>|</span><a href="#38411008">prev</a><span>|</span><a href="#38410222">next</a><span>|</span><label class="collapse" for="c-38410517">[-]</label><label class="expand" for="c-38410517">[4 more]</label></div><br/><div class="children"><div class="content">There is nothing that forces at least a single copy. You could do some processing and write directly into a buffer registered to the kernel.</div><br/><div id="38410629" class="c"><input type="checkbox" id="c-38410629" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#38410485">root</a><span>|</span><a href="#38410517">parent</a><span>|</span><a href="#38410222">next</a><span>|</span><label class="collapse" for="c-38410629">[-]</label><label class="expand" for="c-38410629">[3 more]</label></div><br/><div class="children"><div class="content">I mean if we want to be pedantic, the buffer has to be copied from main memory into whatever the HW uses for staging to generate the PHY signal. But that’s typically an DMA operation not an CPU memcpy. Even with io_uring though, afaik there’s inevitably a memcpy to create the sk_buf to hand off to the network driver no? I’m more fuzzy on how that stuff works but I don’t think the sk_buf is implemented as a list of iovecs.<p>You could go the DPDK route but that has downsides of your application needing exclusive access to the network interface. AFAIK, any network stack that supports multiple simultaneous applications typically involves at least 1 copy but I could be off.</div><br/><div id="38412234" class="c"><input type="checkbox" id="c-38412234" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#38410485">root</a><span>|</span><a href="#38410629">parent</a><span>|</span><a href="#38411106">next</a><span>|</span><label class="collapse" for="c-38412234">[-]</label><label class="expand" for="c-38412234">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You could go the DPDK route but that has downsides of your application needing exclusive access to the network interface.<p>Never did it, but I guess you could reroute traffic you don&#x27;t care about to the Linux network stack for regular routing.<p><a href="https:&#x2F;&#x2F;doc.dpdk.org&#x2F;guides-16.07&#x2F;howto&#x2F;flow_bifurcation.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;doc.dpdk.org&#x2F;guides-16.07&#x2F;howto&#x2F;flow_bifurcation.htm...</a><p>Not sure that&#x27;s a realistic scenario though. You rarely expose a DPDK app directly on a regular &#x2F; open subnet. You often have a dedicated app, with a dedicated NIC, on a dedicated subnet, for a dedicated traffic.</div><br/></div></div><div id="38411106" class="c"><input type="checkbox" id="c-38411106" checked=""/><div class="controls bullet"><span class="by">camel_gopher</span><span>|</span><a href="#38410485">root</a><span>|</span><a href="#38410629">parent</a><span>|</span><a href="#38412234">prev</a><span>|</span><a href="#38410222">next</a><span>|</span><label class="collapse" for="c-38411106">[-]</label><label class="expand" for="c-38411106">[1 more]</label></div><br/><div class="children"><div class="content">“Even with io_uring though, afaik there’s inevitably a memcpy to create the sk_buf to hand off to the network driver no? I’m more fuzzy on how that stuff works but I don’t think the sk_buf is implemented as a list of iovecs.”<p>Having worked with sk_bufs a bit I hope a copy isn’t needed for a network send. All the tooling I worked with around those was zero copy.</div><br/></div></div></div></div></div></div></div></div><div id="38410222" class="c"><input type="checkbox" id="c-38410222" checked=""/><div class="controls bullet"><span class="by">throw0101b</span><span>|</span><a href="#38410485">prev</a><span>|</span><a href="#38411482">next</a><span>|</span><label class="collapse" for="c-38410222">[-]</label><label class="expand" for="c-38410222">[4 more]</label></div><br/><div class="children"><div class="content">Project home page:<p>* <a href="https:&#x2F;&#x2F;packetmill.io" rel="nofollow noreferrer">https:&#x2F;&#x2F;packetmill.io</a></div><br/><div id="38410415" class="c"><input type="checkbox" id="c-38410415" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#38410222">parent</a><span>|</span><a href="#38411482">next</a><span>|</span><label class="collapse" for="c-38410415">[-]</label><label class="expand" for="c-38410415">[3 more]</label></div><br/><div class="children"><div class="content">Too bad they aren&#x27;t sharing source code.  Ah well, one can dream of 100gbps per core.</div><br/><div id="38411658" class="c"><input type="checkbox" id="c-38411658" checked=""/><div class="controls bullet"><span class="by">shaklee3</span><span>|</span><a href="#38410222">root</a><span>|</span><a href="#38410415">parent</a><span>|</span><a href="#38410685">next</a><span>|</span><label class="collapse" for="c-38411658">[-]</label><label class="expand" for="c-38411658">[1 more]</label></div><br/><div class="children"><div class="content">You can do 100G per core with DPDK with bigger frame sizes</div><br/></div></div><div id="38410685" class="c"><input type="checkbox" id="c-38410685" checked=""/><div class="controls bullet"><span class="by">posnet</span><span>|</span><a href="#38410222">root</a><span>|</span><a href="#38410415">parent</a><span>|</span><a href="#38411658">prev</a><span>|</span><a href="#38411482">next</a><span>|</span><label class="collapse" for="c-38410685">[-]</label><label class="expand" for="c-38410685">[1 more]</label></div><br/><div class="children"><div class="content">What are you talking about, it&#x27;s the second link on the page called try now.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;aliireza&#x2F;packetmill">https:&#x2F;&#x2F;github.com&#x2F;aliireza&#x2F;packetmill</a></div><br/></div></div></div></div></div></div><div id="38411482" class="c"><input type="checkbox" id="c-38411482" checked=""/><div class="controls bullet"><span class="by">gonzo</span><span>|</span><a href="#38410222">prev</a><span>|</span><a href="#38409994">next</a><span>|</span><label class="collapse" for="c-38411482">[-]</label><label class="expand" for="c-38411482">[1 more]</label></div><br/><div class="children"><div class="content">One wonders why they saddled VPP with DPDK, rather than a VPP native driver that will do 35Mpps or more, but allowed themselves to modify a DPDK driver to show their results.</div><br/></div></div><div id="38409994" class="c"><input type="checkbox" id="c-38409994" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#38411482">prev</a><span>|</span><a href="#38412017">next</a><span>|</span><label class="collapse" for="c-38409994">[-]</label><label class="expand" for="c-38409994">[1 more]</label></div><br/><div class="children"><div class="content">I feel like a lot of these could be contributed back to DPDK instead of built as a layer on top of it.<p>Very good article though. I&#x27;m an addict user of DPDK but more for latency rather than throughput, and it&#x27;s interesting to see the challenges involved there.</div><br/></div></div></div></div></div></div></div></body></html>