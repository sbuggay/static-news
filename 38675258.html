<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702890066737" as="style"/><link rel="stylesheet" href="styles.css?v=1702890066737"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/">AMD&#x27;s CDNA 3 Compute Architecture</a> <span class="domain">(<a href="https://chipsandcheese.com">chipsandcheese.com</a>)</span></div><div class="subtext"><span>ksec</span> | <span>83 comments</span></div><br/><div><div id="38675529" class="c"><input type="checkbox" id="c-38675529" checked=""/><div class="controls bullet"><span class="by">jdewerd</span><span>|</span><a href="#38676628">next</a><span>|</span><label class="collapse" for="c-38675529">[-]</label><label class="expand" for="c-38675529">[62 more]</label></div><br/><div class="children"><div class="content">&gt; AMD diverged their GPU architecture development into separate CDNA and RDNA lines specialized for compute and graphics respectively.<p>Ooooh, is that why the consumer cards don&#x27;t do compute? Yikes, I was hoping that was just a bit of misguided segmentation but this sounds like a high level architecture problem, like an interstate without an on ramp. Oof.</div><br/><div id="38676199" class="c"><input type="checkbox" id="c-38676199" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38675641">next</a><span>|</span><label class="collapse" for="c-38676199">[-]</label><label class="expand" for="c-38676199">[34 more]</label></div><br/><div class="children"><div class="content">&gt; the consumer cards don&#x27;t do compute?<p>Typically, software developers only support a single GPGPU API, and that API is nVidia CUDA.<p>Technically, consumer AMD cards are awesome at compute. For example, UE5 renders triangle meshes with compute instead of graphics <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TMorJX3Nj6U" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TMorJX3Nj6U</a> Moreover, AMD cards often outperform nVidia equivalents because nVidia prioritized ray tracing and DLSS over compute power and memory bandwidth.<p>The issue is, no tech company is interested in adding D3D or Vulkan backend to AI libraries like PyTorch. nVidia is not doing that because they are happy with the status quo. Intel and AMD are not doing that because both hope to replace CUDA with their proprietary equivalents, instead of an open GPU API.</div><br/><div id="38676689" class="c"><input type="checkbox" id="c-38676689" checked=""/><div class="controls bullet"><span class="by">GeekyBear</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676199">parent</a><span>|</span><a href="#38676789">next</a><span>|</span><label class="collapse" for="c-38676689">[-]</label><label class="expand" for="c-38676689">[12 more]</label></div><br/><div class="children"><div class="content">&gt; The issue is, no tech company is interested in adding D3D or Vulkan backend to AI libraries like PyTorch.<p>AMD is interested in making PyTorch take advantage of their chips, and has done so.<p>&gt; we are delighted that the PyTorch 2.0 stable release includes support for AMD Instinct™ and Radeon™ GPUs that are supported by the ROCm™ software platform.<p>When NVIDIA is charging 1000% profit margins for a GPU aimed at use as an AI accelerator, you can expect competitors to be willing to do what it takes to move into that market.<p><a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;nvidia-makes-1000-profit-on-h100-gpus-report" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;nvidia-makes-1000-profit-o...</a></div><br/><div id="38679144" class="c"><input type="checkbox" id="c-38679144" checked=""/><div class="controls bullet"><span class="by">tasty_freeze</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676689">parent</a><span>|</span><a href="#38677362">next</a><span>|</span><label class="collapse" for="c-38679144">[-]</label><label class="expand" for="c-38679144">[4 more]</label></div><br/><div class="children"><div class="content">I know you are just quoting the tomhsardware article, but sheesh, &quot;1000% profit margin&quot; is nonsensical.  If the cost of goods is X and they sell it for 10X, then it is a 90% profit margin, not a 1000% profit margin.</div><br/><div id="38679636" class="c"><input type="checkbox" id="c-38679636" checked=""/><div class="controls bullet"><span class="by">RussianCow</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38679144">parent</a><span>|</span><a href="#38677362">next</a><span>|</span><label class="collapse" for="c-38679636">[-]</label><label class="expand" for="c-38679636">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, the word they were looking for is &quot;markup&quot;. People make this mistake all the time.</div><br/><div id="38679784" class="c"><input type="checkbox" id="c-38679784" checked=""/><div class="controls bullet"><span class="by">KingOfCoders</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38679636">parent</a><span>|</span><a href="#38677362">next</a><span>|</span><label class="collapse" for="c-38679784">[-]</label><label class="expand" for="c-38679784">[2 more]</label></div><br/><div class="children"><div class="content">People can&#x27;t do percentage.<p><i>Once when I was in an RPG shop I visited frequently, some youths came in and said &quot;We want to order 5 figures.&quot; After a short pause they added &quot;Can we get them cheaper, because we&#x27;re buying 5?&quot; The shop owner said &quot;Fine, I&#x27;ll give you one for free.&quot; They said &quot;No, the other owner promised us 10 percent&quot;. After a small pause the shop owner smiled and said &quot;Ok, I&#x27;ll stick with the promise, you get 10 percent.&quot;</i></div><br/><div id="38680175" class="c"><input type="checkbox" id="c-38680175" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38679784">parent</a><span>|</span><a href="#38677362">next</a><span>|</span><label class="collapse" for="c-38680175">[-]</label><label class="expand" for="c-38680175">[1 more]</label></div><br/><div class="children"><div class="content">Percentages suck. They feel almost designed to confuse. Just about anything is better, really. Doesn&#x27;t help that people using percentages either use them intentionally to confuse their victims - think shop owners, advertisers, banks, scammers, especially in finance and insurance adjacent areas - or just don&#x27;t realize how easy it is for everyone, <i>including them themselves</i>, to lost track of the base&#x2F;referent.<p>I&#x27;ve come to see the use of percentages as a warning sign.</div><br/></div></div></div></div></div></div></div></div><div id="38677362" class="c"><input type="checkbox" id="c-38677362" checked=""/><div class="controls bullet"><span class="by">MrBuddyCasino</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676689">parent</a><span>|</span><a href="#38679144">prev</a><span>|</span><a href="#38676789">next</a><span>|</span><label class="collapse" for="c-38677362">[-]</label><label class="expand" for="c-38677362">[7 more]</label></div><br/><div class="children"><div class="content">You would think so, and yet AMD dropped the ball so hard on the software side of things, the resulting hole in the ground might as well be bottomless.</div><br/><div id="38677485" class="c"><input type="checkbox" id="c-38677485" checked=""/><div class="controls bullet"><span class="by">jorvi</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677362">parent</a><span>|</span><a href="#38676789">next</a><span>|</span><label class="collapse" for="c-38677485">[-]</label><label class="expand" for="c-38677485">[6 more]</label></div><br/><div class="children"><div class="content">Is that really just on AMD?<p>All the companies that are now screaming hellfire because of Nvidia&#x27;s market maker position are also the companies that gave Nvidia a warchest filled with billions of dollars. How is AMD supposed to compete when the whole market is funding their rival?</div><br/><div id="38678057" class="c"><input type="checkbox" id="c-38678057" checked=""/><div class="controls bullet"><span class="by">HideousKojima</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677485">parent</a><span>|</span><a href="#38678440">next</a><span>|</span><label class="collapse" for="c-38678057">[-]</label><label class="expand" for="c-38678057">[4 more]</label></div><br/><div class="children"><div class="content">The whole market funded Nvidia because AMD failed to provide a viable alternative for 10+ years</div><br/><div id="38679800" class="c"><input type="checkbox" id="c-38679800" checked=""/><div class="controls bullet"><span class="by">doikor</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38678057">parent</a><span>|</span><a href="#38680012">next</a><span>|</span><label class="collapse" for="c-38679800">[-]</label><label class="expand" for="c-38679800">[1 more]</label></div><br/><div class="children"><div class="content">AMD was almost bankrupt 10 years ago. Basically AMD had to bet the whole company on their next CPU or GPU and their choice was the CPUs and put all their efforts into that and it payed off (Zen architecture).<p>It has only been the last 4 or 5 years that AMD has had any real money to put into their GPU&#x2F;AI accelerator sector and that seems to be developing quite well now (though they seem to be mostly interested in super computer&#x2F;massive data center deployments for now)</div><br/></div></div><div id="38680012" class="c"><input type="checkbox" id="c-38680012" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38678057">parent</a><span>|</span><a href="#38679800">prev</a><span>|</span><a href="#38678184">next</a><span>|</span><label class="collapse" for="c-38680012">[-]</label><label class="expand" for="c-38680012">[1 more]</label></div><br/><div class="children"><div class="content">GPU languages shouldn&#x27;t be proprietary. At one point this was a shared understanding and lots of companies were behind initiatives like OpenCL. Meanwhile progress in higher level non C++ GPU languages has stayed in niches without big industry backing, and we&#x27;re stuck with CUDA which is bad in both language design sense and market-wise.</div><br/></div></div></div></div><div id="38678440" class="c"><input type="checkbox" id="c-38678440" checked=""/><div class="controls bullet"><span class="by">7speter</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677485">parent</a><span>|</span><a href="#38678057">prev</a><span>|</span><a href="#38676789">next</a><span>|</span><label class="collapse" for="c-38678440">[-]</label><label class="expand" for="c-38678440">[1 more]</label></div><br/><div class="children"><div class="content">Have a solution (Rocm) that works on all of their modern cards (Rdna 1, 2 and 3 and CDNA)?</div><br/></div></div></div></div></div></div></div></div><div id="38676789" class="c"><input type="checkbox" id="c-38676789" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676199">parent</a><span>|</span><a href="#38676689">prev</a><span>|</span><a href="#38678382">next</a><span>|</span><label class="collapse" for="c-38676789">[-]</label><label class="expand" for="c-38676789">[5 more]</label></div><br/><div class="children"><div class="content">Vulkan Compute backends for numerical compute (as typified by both OpenCL and SYCL) are challenging, you can look at clspv <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;clspv">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;clspv</a> project for the nitty gritty details.  The lowest-effort path so far is most likely via some combination of Rocm&#x2F;HIP (for hardware that AMD bothers to support themselves) and the Mesa project&#x27;s RustiCL backend (for everything else).</div><br/><div id="38677130" class="c"><input type="checkbox" id="c-38677130" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676789">parent</a><span>|</span><a href="#38678382">next</a><span>|</span><label class="collapse" for="c-38677130">[-]</label><label class="expand" for="c-38677130">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Vulkan Compute backends for numerical compute (as typified by both OpenCL and SYCL) are challenging<p>Microsoft has an offline dxc.exe compiler which compiles HLSL to Spir-V. Also, DXVK has a JIT compiler which recompiles DXBC byte codes to Spir-V. Both technologies are old, stable and reliable, for example the DXVK’s JIT compiler is a critical software component of the SteamDeck console.<p>&gt; The lowest-effort path so far is most likely<p>I agree that’s most likely to happen, but the outcome is horrible from consumer PoV.<p>Mesa is Linux-only, Rust is too hard to use for vast majority of developers (myself included), AMD will never support older cards with ROCm, and we now have the third discrete GPU vendor, Intel.</div><br/><div id="38677154" class="c"><input type="checkbox" id="c-38677154" checked=""/><div class="controls bullet"><span class="by">my123</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677130">parent</a><span>|</span><a href="#38678382">next</a><span>|</span><label class="collapse" for="c-38677154">[-]</label><label class="expand" for="c-38677154">[3 more]</label></div><br/><div class="children"><div class="content">SPIR-V for OpenCL and for Vulkan are substantially different, with the translation between the two being quite non-trivial.<p>(note that rusticl + zink does deal with it _partially_ to some extent nowadays)<p>+ Vulkan memory management doesn&#x27;t expose unified address space primitives</div><br/><div id="38677217" class="c"><input type="checkbox" id="c-38677217" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677154">parent</a><span>|</span><a href="#38678382">next</a><span>|</span><label class="collapse" for="c-38677217">[-]</label><label class="expand" for="c-38677217">[2 more]</label></div><br/><div class="children"><div class="content">Why would you want OpenCL? Pretty sure D3D11 compute shaders gonna be adequate for a Torch backend, and they even work on Linux with Wine: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Whisper&#x2F;issues&#x2F;42">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Whisper&#x2F;issues&#x2F;42</a> Native Vulkan compute shaders would be even better.<p>Why would you want unified address space? At least in my experience, it’s often too slow to be useful. DMA transfers (CopyResource in D3D11, copy command queue in D3D12, transfer queue in VK) are implemented by dedicated hardware inside GPUs, and are way more efficient.</div><br/></div></div></div></div></div></div></div></div><div id="38678382" class="c"><input type="checkbox" id="c-38678382" checked=""/><div class="controls bullet"><span class="by">spookie</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676199">parent</a><span>|</span><a href="#38676789">prev</a><span>|</span><a href="#38677402">next</a><span>|</span><label class="collapse" for="c-38678382">[-]</label><label class="expand" for="c-38678382">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They aim to replace CUDA with their proprietary equivalents<p>Neither oneAPI, nor ROCm are proprietary?</div><br/></div></div><div id="38677402" class="c"><input type="checkbox" id="c-38677402" checked=""/><div class="controls bullet"><span class="by">jorvi</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676199">parent</a><span>|</span><a href="#38678382">prev</a><span>|</span><a href="#38676410">next</a><span>|</span><label class="collapse" for="c-38677402">[-]</label><label class="expand" for="c-38677402">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Intel and AMD are not doing that because both hope to replace CUDA with their proprietary equivalents, instead of an open GPU API<p>Like AMD did with FreeSync (now VRR) and FSR3 (whatever open standard it will morph into)?<p>I&#x27;ll never understand the hate AMD gets from the open source community. Mind-boggling.</div><br/><div id="38678446" class="c"><input type="checkbox" id="c-38678446" checked=""/><div class="controls bullet"><span class="by">7speter</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677402">parent</a><span>|</span><a href="#38676410">next</a><span>|</span><label class="collapse" for="c-38678446">[-]</label><label class="expand" for="c-38678446">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t Intel’s solution oneAPI, which is supposed to be open and have a sort of translation layer for CUDA?</div><br/></div></div></div></div><div id="38676410" class="c"><input type="checkbox" id="c-38676410" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676199">parent</a><span>|</span><a href="#38677402">prev</a><span>|</span><a href="#38679783">next</a><span>|</span><label class="collapse" for="c-38676410">[-]</label><label class="expand" for="c-38676410">[6 more]</label></div><br/><div class="children"><div class="content">&gt;Vulkan backend to AI libraries like PyTorch<p>How many man hours is a project like this?</div><br/><div id="38676622" class="c"><input type="checkbox" id="c-38676622" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676410">parent</a><span>|</span><a href="#38676778">next</a><span>|</span><label class="collapse" for="c-38676622">[-]</label><label class="expand" for="c-38676622">[4 more]</label></div><br/><div class="children"><div class="content">Couple times in the past I wanted to port open source ML models from CUDA&#x2F;Python to a better technology stack. I have ported Whisper <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Whisper&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Whisper&#x2F;</a> and Mistral <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;</a> to D3D11. I don’t remember how much time I spent, but given both were unpaid part-time hobby projects, probably under 160 hours &#x2F; each.<p>These software projects were great to validate the technology choices, but note I only did bare minimum to implement specific ML models. Implementing a complete PyTorch backend gonna involve dramatically more work. I can’t even estimate how much more because I’m not an expert in Python or these Python-based ML libraries.</div><br/><div id="38677327" class="c"><input type="checkbox" id="c-38677327" checked=""/><div class="controls bullet"><span class="by">versteegen</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676622">parent</a><span>|</span><a href="#38678402">next</a><span>|</span><label class="collapse" for="c-38677327">[-]</label><label class="expand" for="c-38677327">[2 more]</label></div><br/><div class="children"><div class="content">Wow, a very nice reimplementation.<p>To go on a tangent, I note your custom &#x27;BCML1&#x27; 5bit per weight compression codec and your optimised hand-coded AVX2 to encode it... was that really needed? Are the weights encoded on every startup? Why not do it once and save to disk?</div><br/><div id="38677567" class="c"><input type="checkbox" id="c-38677567" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677327">parent</a><span>|</span><a href="#38678402">next</a><span>|</span><label class="collapse" for="c-38677567">[-]</label><label class="expand" for="c-38677567">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Are the weights encoded on every startup?<p>Not really, that code only runs while importing the PyTorch format. See readme for the frontend app: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;tree&#x2F;master&#x2F;Mistral&#x2F;MistralChat">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;tree&#x2F;master&#x2F;Mistral&#x2F;Mistral...</a> When loading the model from *.cgml, the model file already contains compressed tensors. That’s how that file is only 4.55 GB, versus 13.4 GB in the original model.<p>&gt; was that really needed?<p>For desktops with many CPU cores, a simpler scalar version would probably work equally well. Still, low-end computers don’t always have many cores to use by these background encoding tasks. Also, CPU usage on laptops translates to battery drain.</div><br/></div></div></div></div><div id="38678402" class="c"><input type="checkbox" id="c-38678402" checked=""/><div class="controls bullet"><span class="by">spookie</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676622">parent</a><span>|</span><a href="#38677327">prev</a><span>|</span><a href="#38676778">next</a><span>|</span><label class="collapse" for="c-38678402">[-]</label><label class="expand" for="c-38678402">[1 more]</label></div><br/><div class="children"><div class="content">This is great!</div><br/></div></div></div></div></div></div><div id="38679783" class="c"><input type="checkbox" id="c-38679783" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676199">parent</a><span>|</span><a href="#38676410">prev</a><span>|</span><a href="#38676792">next</a><span>|</span><label class="collapse" for="c-38679783">[-]</label><label class="expand" for="c-38679783">[1 more]</label></div><br/><div class="children"><div class="content">DirectML is part of DirectX 12.</div><br/></div></div><div id="38676792" class="c"><input type="checkbox" id="c-38676792" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676199">parent</a><span>|</span><a href="#38679783">prev</a><span>|</span><a href="#38675641">next</a><span>|</span><label class="collapse" for="c-38676792">[-]</label><label class="expand" for="c-38676792">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Typically, software developers only support a single GPGPU API, and that API is nVidia CUDA.<p>There is very little NVIDIA and CUDA cards off of  X86_64 and maybe OpenPower architectures. So I disagree. Also, OpenCL, despite being kind of a &quot;betrayed standard&quot;, enjoys quite a lot of popularity even on x86_64 (sometimes even with NVIDIA hardware) - even if it is not as popular there.<p>&gt; AMD cards often outperform nVidia equivalents<p>Can you link to benchmarks or other analysis supporting this claim? This has not been my impression in recent years, though I don&#x27;t routinely look at high-end AMD hardware.<p>&gt; because nVidia prioritized ray tracing and DLSS over compute power and memory bandwidth.<p>Has it really?</div><br/><div id="38676989" class="c"><input type="checkbox" id="c-38676989" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676792">parent</a><span>|</span><a href="#38679794">next</a><span>|</span><label class="collapse" for="c-38676989">[-]</label><label class="expand" for="c-38676989">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Can you link to benchmarks or other analysis supporting this claim?<p>Current generation nVidia: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GeForce_40_series#Desktop" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GeForce_40_series#Desktop</a><p>Current generation AMD: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Template:AMD_Radeon_RX_7000" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Template:AMD_Radeon_RX_7000</a><p>The key performance characteristics are processing power TFlops, and memory bandwidth GB&#x2F;s.<p>nVidia 4080 which costs $1200: 43 TFlops FP16, 43 TFlops FP32, 0.672 TFlops FP64, 717 GB&#x2F;s memory.<p>AMD 7900 XTX which costs $1000: 93 TFlops FP16, 47 TFlops FP32, 1.5 TFlops FP64, 960 GB&#x2F;s memory.<p>Note that for applications which bottleneck on FP16 compute (many ML workloads) or FP64 compute (many traditional HPC workloads: numerical solvers, fluid dynamics, etc), the 7900 XTX even outperforms the 4090 which costs $1600.</div><br/><div id="38677545" class="c"><input type="checkbox" id="c-38677545" checked=""/><div class="controls bullet"><span class="by">ColonelPhantom</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676989">parent</a><span>|</span><a href="#38678017">next</a><span>|</span><label class="collapse" for="c-38677545">[-]</label><label class="expand" for="c-38677545">[1 more]</label></div><br/><div class="children"><div class="content">In ML workloads, usually the FP16 operations are matrix operations. On RDNA3, these execute at the same rate as normal shader&#x2F;vector operations, but on Nvidia RTX cards there are Tensor cores which accelerate them. The Ada whitepaper lists 48.7 shader TFlops (not 43 because boost vs base clock), and 195 TFlops for FP16 Tensor with FP16 Accumulate. That&#x27;s 4 times faster than regular, and almost double what the XTX lists!<p>Ampere and newer also have native sparsity support which means that you can skip over the zeroes &#x27;for free&#x27;, which Nvidia uses to market double the TFlops, which is kind of misleading imo. But the 195 TFlops are even before sparsity is included!<p>I&#x27;m not sure if the 93 TFlops (120 with boost clocks) on AMD are with FP16 or FP32 accumulation, as with FP32 accumulation the 4080 slows down significantly and gets much closer with 97.5 TFlops.<p>Intel Xe-HPG (used in the Arc A cards) also offers very aggressive matrix acceleration via XMX, with 137.6 FP16 TFlops at base clock, vs. 17.2 FP32 TFlops.</div><br/></div></div><div id="38678017" class="c"><input type="checkbox" id="c-38678017" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676989">parent</a><span>|</span><a href="#38677545">prev</a><span>|</span><a href="#38679794">next</a><span>|</span><label class="collapse" for="c-38678017">[-]</label><label class="expand" for="c-38678017">[2 more]</label></div><br/><div class="children"><div class="content">That is interesting, even though I would need to look at non-consumer-GFX cards. What about actual benchmarks, though? Passmark, for example:<p><a href="https:&#x2F;&#x2F;www.videocardbenchmark.net&#x2F;directCompute.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.videocardbenchmark.net&#x2F;directCompute.html</a><p>has:<p>GeForce RTX 4090    28,916<p>RTX 6000 Ada        23,296<p>GeForce RTX 4080    22,016<p>Radeon RX 7900 XTX  18,767<p>and Geekbench 6:<p><a href="https:&#x2F;&#x2F;browser.geekbench.com&#x2F;opencl-benchmarks" rel="nofollow noreferrer">https:&#x2F;&#x2F;browser.geekbench.com&#x2F;opencl-benchmarks</a><p>has:<p>GeForce RTX 4090    321928<p>L40                 292357 &lt;- L40S is out and may be better<p>RTX 6000 Ada        274348<p>H100                267514<p>NVIDIA GeForce RTX  239863<p>Radeon RX 7900 XTX  199412</div><br/><div id="38678550" class="c"><input type="checkbox" id="c-38678550" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38678017">parent</a><span>|</span><a href="#38679794">next</a><span>|</span><label class="collapse" for="c-38678550">[-]</label><label class="expand" for="c-38678550">[1 more]</label></div><br/><div class="children"><div class="content">Neither of those benchmarks is relevant to ML which is what GP is talking about</div><br/></div></div></div></div></div></div><div id="38679794" class="c"><input type="checkbox" id="c-38679794" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676792">parent</a><span>|</span><a href="#38676989">prev</a><span>|</span><a href="#38675641">next</a><span>|</span><label class="collapse" for="c-38679794">[-]</label><label class="expand" for="c-38679794">[1 more]</label></div><br/><div class="children"><div class="content">OpenCL is stuck in C99, that is why most people ignored it and adopted CUDA in first place.<p>The attempts to bring C++ to it never were properly done, and that is why OpenCL 3.0 is basically OpenCL 1.0 rebranded.</div><br/></div></div></div></div></div></div><div id="38675641" class="c"><input type="checkbox" id="c-38675641" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38676199">prev</a><span>|</span><a href="#38676067">next</a><span>|</span><label class="collapse" for="c-38675641">[-]</label><label class="expand" for="c-38675641">[6 more]</label></div><br/><div class="children"><div class="content">They added support for high end radeon cards 2 months ago. Rocm is &quot;eventually&quot; coming to RDNA in general but it&#x27;s a slow process which is more or less in line with how AMD has approached rocm from the start (targetting a very small subset of compute and slowly expanding it with each major version).<p><a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;amd-enables-rocm-and-pytorch-on-radeon-rx-7900-xtx" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;amd-enables-rocm-and-pytor...</a></div><br/><div id="38675656" class="c"><input type="checkbox" id="c-38675656" checked=""/><div class="controls bullet"><span class="by">mnau</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675641">parent</a><span>|</span><a href="#38676938">next</a><span>|</span><label class="collapse" for="c-38675656">[-]</label><label class="expand" for="c-38675656">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a good step forward. This separation is a massive barrier to entry. Making CUDA work reliably on every product was a masterstroke by NVIDIA.</div><br/><div id="38677743" class="c"><input type="checkbox" id="c-38677743" checked=""/><div class="controls bullet"><span class="by">deskamess</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675656">parent</a><span>|</span><a href="#38676938">next</a><span>|</span><label class="collapse" for="c-38677743">[-]</label><label class="expand" for="c-38677743">[1 more]</label></div><br/><div class="children"><div class="content">I am getting the impression their (AMD) understanding of software&#x2F;Devx is not as good as nvidia. There seems to be some sort of inertia in getting the software side moving - as if, now that they are behind, they are not willing to make any mistakes, so they hold off on doing anything decisive - instead opting for continuing the status quo mode but perhaps adding a few more resources.</div><br/></div></div></div></div><div id="38676938" class="c"><input type="checkbox" id="c-38676938" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675641">parent</a><span>|</span><a href="#38675656">prev</a><span>|</span><a href="#38676067">next</a><span>|</span><label class="collapse" for="c-38676938">[-]</label><label class="expand" for="c-38676938">[3 more]</label></div><br/><div class="children"><div class="content">The issue is that RDNA and CDNA are different so when an enthusiast makes a fast RDNA code it doesn&#x27;t mean it would work well on CDNA and vice-versa. Not sure why AMD had to go this route, only high-end pros will write software for CDNA and they won&#x27;t get any mindshare.</div><br/><div id="38679087" class="c"><input type="checkbox" id="c-38679087" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676938">parent</a><span>|</span><a href="#38677238">next</a><span>|</span><label class="collapse" for="c-38679087">[-]</label><label class="expand" for="c-38679087">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not accurate at all.<p>Someone writing for the rocm platform will be writing HIP and then HIPCC will compile that down into the actual runtime targeting the given architecture. HIP is pretty platform agnostic so very little device specific optimization tends to go into idiomatic HIP.</div><br/></div></div><div id="38677238" class="c"><input type="checkbox" id="c-38677238" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676938">parent</a><span>|</span><a href="#38679087">prev</a><span>|</span><a href="#38676067">next</a><span>|</span><label class="collapse" for="c-38677238">[-]</label><label class="expand" for="c-38677238">[1 more]</label></div><br/><div class="children"><div class="content">What do you have in mind for code that works noticeably better on one than the other?</div><br/></div></div></div></div></div></div><div id="38676067" class="c"><input type="checkbox" id="c-38676067" checked=""/><div class="controls bullet"><span class="by">vegabook</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38675641">prev</a><span>|</span><a href="#38675898">next</a><span>|</span><label class="collapse" for="c-38676067">[-]</label><label class="expand" for="c-38676067">[8 more]</label></div><br/><div class="children"><div class="content">AMD has always been a terrible steward of ATI.<p>They&#x27;re fundamentally a hardware company (as per Lisa Su&#x27;s CV) which didn&#x27;t cotton on to the fact that Cuda was the killer. I remember how @Bridgman kept playing a rearguard action on Phoronix, his job being to keep devs on board. Losing battle.<p>I kinda understand it because [80&#x2F;90]s era hardware people intrinsically think hardware is the top dog in the stack, and that&#x27;s where all AMD management come from including Su.<p>Kodura understood that Nvidia was killing AMD because the consumer cards could run CUDA. So he pushed through, against Lisa Su, the Radeon VII, which until very recently, and for many years, was the only consumer card that ROCm supported. He was basically fired shortly thereafter, and the RVII, which was a fantastic card, was shutdown sharpish. Then they brought in Wang who crystallised the consumer&#x2F;pro segmentation.<p>Now they&#x27;re furiously trying to backpedal, and it&#x27;s too late. There are multiple attempting competitors, but basically the only one worth talking about is AAPL and Metal.<p>AMD lost the window.</div><br/><div id="38676116" class="c"><input type="checkbox" id="c-38676116" checked=""/><div class="controls bullet"><span class="by">simfree</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676067">parent</a><span>|</span><a href="#38677603">next</a><span>|</span><label class="collapse" for="c-38676116">[-]</label><label class="expand" for="c-38676116">[4 more]</label></div><br/><div class="children"><div class="content">ATI on its own did not have a solid future, they could have easily ended up like their other local Canadian counterparts Nortel or Blackberry.</div><br/><div id="38677095" class="c"><input type="checkbox" id="c-38677095" checked=""/><div class="controls bullet"><span class="by">hylaride</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676116">parent</a><span>|</span><a href="#38676137">next</a><span>|</span><label class="collapse" for="c-38677095">[-]</label><label class="expand" for="c-38677095">[2 more]</label></div><br/><div class="children"><div class="content">I live in Toronto and knew people who worked for ATI back in the 1990s-2000s.  The management structure was <i>fucked</i>.  In those days it was generally regarded that ATI could and did produce better hardware than NVIDIA, but ATI’s drivers were horrible and only evened out near the end of the hardware’s lifetime.  But the end of the lifetime was far too late and all the benchmarks would have long been done on the buggy drivers, driving people to NVIDIA.<p>I kid you not, but the software side of ATI’s reviews and incentives were literally QA got rewarded for finding bugs and then the software engineers were then rewarded for fixing them.  You can imagine what happened next.  The bitch of it is that it was overall just a symptom of the problem that the company as a whole didn’t prioritize software.  NVIDIA knew that the drivers and software efficiency of using the hardware was just as if not more important than the hardware (the stories in the 1990s of how John Carmack used various hacks with hardware is a good example of that).</div><br/><div id="38677657" class="c"><input type="checkbox" id="c-38677657" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677095">parent</a><span>|</span><a href="#38676137">next</a><span>|</span><label class="collapse" for="c-38677657">[-]</label><label class="expand" for="c-38677657">[1 more]</label></div><br/><div class="children"><div class="content">I generally felt that AMD(&#x2F;ATI) were a hardware company first, while Nvidia were a software company first.<p>Though now Nvidia are outselling them in dGPUs by nearly 10x, so it&#x27;s pretty easy for them to do &quot;both&quot;.</div><br/></div></div></div></div><div id="38676137" class="c"><input type="checkbox" id="c-38676137" checked=""/><div class="controls bullet"><span class="by">vegabook</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676116">parent</a><span>|</span><a href="#38677095">prev</a><span>|</span><a href="#38677603">next</a><span>|</span><label class="collapse" for="c-38676137">[-]</label><label class="expand" for="c-38676137">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Dead or Canadian&quot; yes?</div><br/></div></div></div></div><div id="38677603" class="c"><input type="checkbox" id="c-38677603" checked=""/><div class="controls bullet"><span class="by">ColonelPhantom</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38676067">parent</a><span>|</span><a href="#38676116">prev</a><span>|</span><a href="#38675898">next</a><span>|</span><label class="collapse" for="c-38677603">[-]</label><label class="expand" for="c-38677603">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;d really not call the Radeon VII a &quot;fantastic card&quot; by any stretch of the imagination, outside of specialized use cases like FP64 compute. Its only selling point is that and the enormous 1 TB&#x2F;s memory bandwidth with the for the time large 16 GB, which a friend summarized as &quot;I can push 1 TB&#x2F;s of garbage to nowhere!&quot;.<p>Sure, if those things are relevant to you, it was a killer card, but most consumers judge GPUs first and foremost by their gaming performance, in which the VII was a total dud. The 5700 XT was nearly as fast in those with far less hardware because getting proper utilization on GCN&#x2F;CDNA is a pain compared to RDNA.<p>RDNA2 completed the gaming oriented flip with Infinity Cache (which is also useful outside of games, but still has limited application in GPGPU) which allowed them to offer competitive gaming performance to Nvidia with far less hardware, a big difference from the Vega era.</div><br/><div id="38678169" class="c"><input type="checkbox" id="c-38678169" checked=""/><div class="controls bullet"><span class="by">segfaultbuserr</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38677603">parent</a><span>|</span><a href="#38675898">next</a><span>|</span><label class="collapse" for="c-38678169">[-]</label><label class="expand" for="c-38678169">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I&#x27;d really not call the Radeon VII a &quot;fantastic card&quot; by any stretch of the imagination</i><p>An argument about whether the Radeon VII was a &quot;fantastic card&quot; misses the point, it could have been another GPU. OP&#x27;s main point was that AMD had a years-long policy of not officially supporting any of their consumer-grade GPUs for computing applications, as a result AMD had lost critical opportunities during this time period - even a small market share is better than none. OP mentioned Radeon VII because it was the only exception, and it only happened because there was strong support from some groups within AMD - which didn&#x27;t last long. After RDNA was released, computing support on consumer GPUs was put on pause again until several months ago. There&#x27;s a big difference between &quot;not optimally designed for computing&quot; and &quot;no official support of computing at all&quot;.<p>The outcome is that it created a great barrier of entry for developers and power users with desktop GPUs. You have to figure out everything by yourself, which is not supposed to be particularly difficult as it&#x27;s the norm in the FOSS world. The problem is that, in a community-driven project, you would have received help or at least a hint from your peers or project maintainers who&#x27;s familiar with the system. Not so for AMD. if something doesn&#x27;t work, no AMD developers will provide any help or even hints - and by not providing support, the &quot;community&quot; is never really established. The ROCm GitHub Issues page was (is?) basically a post-apocalypse world abandoned by AMD, full of user self-help threads with no input from developers. Almost nothing works and nobody known why. Even if the problem reported is a general one, it would still be ignored or closed if you&#x27;re not running an officially-supported system and GPU.<p>One time, I needed to understand a hardware feature of an AMD GPU, I eventually had to read ROCm and Linux kernel source code before finding an answer. For most users, it&#x27;s completely unacceptable.<p>&gt; <i>Sure, if those things are relevant to you, it was a killer card</i><p>Disclosure: I use the Radeon VII for bandwidth-heavy simulations so I have a pro-Radeon VII bias. But I don&#x27;t think my bias affected my judgement here.</div><br/><div id="38679335" class="c"><input type="checkbox" id="c-38679335" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38678169">parent</a><span>|</span><a href="#38675898">next</a><span>|</span><label class="collapse" for="c-38679335">[-]</label><label class="expand" for="c-38679335">[1 more]</label></div><br/><div class="children"><div class="content">Also it&#x27;s basically an Mi50 in a trench coat.
You could buy a 32 GiB version labeled Mi60, initially with a single mini DP port making it IMO classify as &quot;graphics card&quot; not &quot;compute accelerator&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="38675898" class="c"><input type="checkbox" id="c-38675898" checked=""/><div class="controls bullet"><span class="by">cogman10</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38676067">prev</a><span>|</span><a href="#38676087">next</a><span>|</span><label class="collapse" for="c-38675898">[-]</label><label class="expand" for="c-38675898">[4 more]</label></div><br/><div class="children"><div class="content">Looks like the split was around 2016?  Which could make sense given the state of crypto at the time.  One problem that hit nvidia more than AMD is consumer cards getting vacuumed up by crypto farms.  AMD making a conscious split effectively isolated their compute cards from their gamer cards.<p>That being said, I can&#x27;t imagine this is something that&#x27;s been good for adoption of AMD cards for compute tasks.  The wonderful thing about CUDA is you don&#x27;t need a special accelerator card to develop cuda code.</div><br/><div id="38676353" class="c"><input type="checkbox" id="c-38676353" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675898">parent</a><span>|</span><a href="#38675935">next</a><span>|</span><label class="collapse" for="c-38676353">[-]</label><label class="expand" for="c-38676353">[1 more]</label></div><br/><div class="children"><div class="content">The split had more to do with GCN being very compute heavy and AMD&#x27;s software completely failing to take advantage of that. GCN got transitioned into CDNA, and got heavily reworked for graphics as RDNA. This is why while RDNA is nowhere near as good at compute oriented features, it tends to outperform NVIDIA in pure rasterization.<p>In contrast, current NVIDIA GPUs are very compute heavy architectures (to the point that they often artificially limit certain aspects of their consumer cards other than fp64 to not interfere with the workstation cards), and they take great advantage of it all with their various features combining software and hardware capabilities.</div><br/></div></div><div id="38675935" class="c"><input type="checkbox" id="c-38675935" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675898">parent</a><span>|</span><a href="#38676353">prev</a><span>|</span><a href="#38675960">next</a><span>|</span><label class="collapse" for="c-38675935">[-]</label><label class="expand" for="c-38675935">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t really isolate crypto mining. If a gaming card is profitable to mine with, miners will buy it and use the gaming drivers.</div><br/></div></div><div id="38675960" class="c"><input type="checkbox" id="c-38675960" checked=""/><div class="controls bullet"><span class="by">Zardoz84</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675898">parent</a><span>|</span><a href="#38675935">prev</a><span>|</span><a href="#38676087">next</a><span>|</span><label class="collapse" for="c-38675960">[-]</label><label class="expand" for="c-38675960">[1 more]</label></div><br/><div class="children"><div class="content">if i remember correctly, AMD GPU cards was being sold very well to crypto miners. The split wasn&#x27;t because that. They did, because gamming and computing having different hardware optimization requisites, and that allowed to get better and more competitive GPUs and computing cards on the market.</div><br/></div></div></div></div><div id="38676087" class="c"><input type="checkbox" id="c-38676087" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38675898">prev</a><span>|</span><a href="#38675705">next</a><span>|</span><label class="collapse" for="c-38676087">[-]</label><label class="expand" for="c-38676087">[1 more]</label></div><br/><div class="children"><div class="content">Well the APU does, there are compute intrinsics and even a small guide by AMD on how to use them:<p><a href="https:&#x2F;&#x2F;gpuopen.com&#x2F;learn&#x2F;wmma_on_rdna3&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;gpuopen.com&#x2F;learn&#x2F;wmma_on_rdna3&#x2F;</a></div><br/></div></div><div id="38675705" class="c"><input type="checkbox" id="c-38675705" checked=""/><div class="controls bullet"><span class="by">DarkmSparks</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38676087">prev</a><span>|</span><a href="#38678428">next</a><span>|</span><label class="collapse" for="c-38675705">[-]</label><label class="expand" for="c-38675705">[3 more]</label></div><br/><div class="children"><div class="content">I was quite surprised to find at least one of the the top500 is based on AMD gpus, it seems to me not bringing that to the consumer market was a conscious choice made some time ago.</div><br/><div id="38679735" class="c"><input type="checkbox" id="c-38679735" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675705">parent</a><span>|</span><a href="#38678086">next</a><span>|</span><label class="collapse" for="c-38679735">[-]</label><label class="expand" for="c-38679735">[1 more]</label></div><br/><div class="children"><div class="content">There are eleven CDNA 2 systems in the top500. <a href="https:&#x2F;&#x2F;www.top500.org&#x2F;lists&#x2F;top500&#x2F;list&#x2F;2023&#x2F;11&#x2F;?page=1" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.top500.org&#x2F;lists&#x2F;top500&#x2F;list&#x2F;2023&#x2F;11&#x2F;?page=1</a></div><br/></div></div><div id="38678086" class="c"><input type="checkbox" id="c-38678086" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675705">parent</a><span>|</span><a href="#38679735">prev</a><span>|</span><a href="#38678428">next</a><span>|</span><label class="collapse" for="c-38678086">[-]</label><label class="expand" for="c-38678086">[1 more]</label></div><br/><div class="children"><div class="content">Some HPC applications are specially made for the HPC, so minor API is acceptable for some HPCs. That&#x27;s not the case for consumer GPGPU use case.</div><br/></div></div></div></div><div id="38678428" class="c"><input type="checkbox" id="c-38678428" checked=""/><div class="controls bullet"><span class="by">7speter</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38675705">prev</a><span>|</span><a href="#38675947">next</a><span>|</span><label class="collapse" for="c-38678428">[-]</label><label class="expand" for="c-38678428">[1 more]</label></div><br/><div class="children"><div class="content">The big kicker is that cdna is still based on the much maligned</div><br/></div></div><div id="38675947" class="c"><input type="checkbox" id="c-38675947" checked=""/><div class="controls bullet"><span class="by">ColonelPhantom</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38678428">prev</a><span>|</span><a href="#38676049">next</a><span>|</span><label class="collapse" for="c-38675947">[-]</label><label class="expand" for="c-38675947">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Ooooh, is that why the consumer cards don&#x27;t do compute?<p>No, not really. Sure, the consumer cards are based on a less suitable architecture, so they won&#x27;t be as good, but it&#x27;s not like it&#x27;s not a general purpose GPU architecture anymore.<p>The real problem is that ROCm has made the very weird technical decision of shipping GPU machine code, rather than bytecode. Nvidia has their own bytecode NVPTX, and Intel just uses SPIR-V for oneAPI.<p>This means that ROCm support for a GPU architecture requires shipping binaries of all the libraries (such as rocBLAS, rocDNN, rocsparse, rocFFT) for all architectures.<p>As the cherry on top, on paper one GPU family consists of different architectures! 
The RX 6700 XT identifies as a different &#x27;architecture&#x27; (gfx1031) than the RX 6800 XT (gfx1030). These are &#x27;close enough&#x27; that you can tell ROCm to use gfx1030 binaries on gfx1031 and it&#x27;ll usually just work, but still.<p>I feel like this is a large part of the reason AMD is so bad about supporting older GPUs in ROCm. It&#x27;d cost a bit of effort, sure, but I feel like the enormous packages you&#x27;d get are a bigger issue. (This is another factor that kills hobbyist ROCm, because anyone with a Pascal+ GPU can do CUDA, but ROCm support is usually limited to newer and higher end cards.)</div><br/><div id="38676801" class="c"><input type="checkbox" id="c-38676801" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675947">parent</a><span>|</span><a href="#38677288">next</a><span>|</span><label class="collapse" for="c-38676801">[-]</label><label class="expand" for="c-38676801">[1 more]</label></div><br/><div class="children"><div class="content">&gt; As the cherry on top, on paper one GPU family consists of different architectures! The RX 6700 XT identifies as a different &#x27;architecture&#x27; (gfx1031) than the RX 6800 XT (gfx1030). These are &#x27;close enough&#x27; that you can tell ROCm to use gfx1030 binaries on gfx1031 and it&#x27;ll usually just work, but still.<p>As far as I can tell, the gfx1030 and gfx1031 ISAs are identical in all but name. LLVM treats them exactly the same and all tests for the ROCm math libraries will pass when you run gfx1030 code on gfx1031 GPUs using the override.<p>The Debian package for HIP has a patch so that if the user has a gfx1031 GPU and there are no gfx1031 code objects available, it will fall back to using gfx1030 code objects instead. When I proposed that patch upstream, it was rejected in favour of adding a new ISA that explicitly supports all the GPUs in a family. That solution is more complex and is taking longer to implement, but will more thoroughly solve that problem (as the solution Debian is using only works well when there&#x27;s one ISA in the family that is a subset of all the others).</div><br/></div></div><div id="38677288" class="c"><input type="checkbox" id="c-38677288" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#38675529">root</a><span>|</span><a href="#38675947">parent</a><span>|</span><a href="#38676801">prev</a><span>|</span><a href="#38676049">next</a><span>|</span><label class="collapse" for="c-38677288">[-]</label><label class="expand" for="c-38677288">[1 more]</label></div><br/><div class="children"><div class="content">There was a ship-bytecode idea branded HSAIL, associated with opencl finalisers in some way. I&#x27;m not sure what killed that - would speculate that the compiler backend was annoyingly slow to run as a JIT. It&#x27;s somewhat conceptually popular under the spirv branding now.<p>Some of the ROCm libraries are partially written in assembly. Hopefully they all have C reference implementations that can be compiled for the other architectures.<p>My guess is that the AMD strategy of only shipping libraries for some cards + open source means ROCm-ish as built by Linux distros, who are likely to build the libs for all the architectures and not just the special few, can be expected to drive people away from the official releases.</div><br/></div></div></div></div><div id="38676049" class="c"><input type="checkbox" id="c-38676049" checked=""/><div class="controls bullet"><span class="by">beebeepka</span><span>|</span><a href="#38675529">parent</a><span>|</span><a href="#38675947">prev</a><span>|</span><a href="#38676628">next</a><span>|</span><label class="collapse" for="c-38676049">[-]</label><label class="expand" for="c-38676049">[1 more]</label></div><br/><div class="children"><div class="content">I thought trolling wasn&#x27;t welcome on HN</div><br/></div></div></div></div><div id="38676628" class="c"><input type="checkbox" id="c-38676628" checked=""/><div class="controls bullet"><span class="by">kaycebasques</span><span>|</span><a href="#38675529">prev</a><span>|</span><a href="#38677157">next</a><span>|</span><label class="collapse" for="c-38676628">[-]</label><label class="expand" for="c-38676628">[9 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t familiar with VLIW. Sounds cool!<p>&gt; Very long instruction word (VLIW) refers to instruction set architectures designed to exploit instruction level parallelism (ILP). Whereas conventional central processing units (CPU, processor) mostly allow programs to specify instructions to execute in sequence only, a VLIW processor allows programs to explicitly specify instructions to execute in parallel. This design is intended to allow higher performance without the complexity inherent in some other designs.<p>&gt; The traditional means to improve performance in processors include dividing instructions into substeps so the instructions can be executed partly at the same time (termed pipelining), dispatching individual instructions to be executed independently, in different parts of the processor (superscalar architectures), and even executing instructions in an order different from the program (out-of-order execution).[1] These methods all complicate hardware (larger circuits, higher cost and energy use) because the processor must make all of the decisions internally for these methods to work.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Very_long_instruction_word" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Very_long_instruction_word</a></div><br/><div id="38676884" class="c"><input type="checkbox" id="c-38676884" checked=""/><div class="controls bullet"><span class="by">mpweiher</span><span>|</span><a href="#38676628">parent</a><span>|</span><a href="#38676986">next</a><span>|</span><label class="collapse" for="c-38676884">[-]</label><label class="expand" for="c-38676884">[6 more]</label></div><br/><div class="children"><div class="content">The most prominent example of VLIW processors was the Itanic, er, Itanium.<p>It, er, didn&#x27;t work out well.<p>Hence Itanic.<p>Their premise was that the compiler could figure out dependencies sufficiently statically so that they could put multiple sequential and some divergent execution paths in the same instruction.  It turned out the compilers couldn&#x27;t actually do this, so processors figure out dependencies and parallelizable instructions dynamically from a sequential instruction stream.<p>Which is a lot of work, a lot of chip resources and a lot of energy.  And only works up to a point, after which you hit diminishing returns.  Which is where we appear to be these days.</div><br/><div id="38677075" class="c"><input type="checkbox" id="c-38677075" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#38676628">root</a><span>|</span><a href="#38676884">parent</a><span>|</span><a href="#38677514">next</a><span>|</span><label class="collapse" for="c-38677075">[-]</label><label class="expand" for="c-38677075">[3 more]</label></div><br/><div class="children"><div class="content">VLIW is also still heavily used in DSPs and similar. And arguably the dual issue instructions in both AMD&#x27;s and Nvidia&#x27;s latest GPUs is a step back towards that.<p>It feels like a lot of these things are cyclical - the core ideas aren&#x27;t new and phase in and out of favor as other parts of architecture design changes around and increases or decreases benefits accordingly.</div><br/><div id="38677379" class="c"><input type="checkbox" id="c-38677379" checked=""/><div class="controls bullet"><span class="by">hajile</span><span>|</span><a href="#38676628">root</a><span>|</span><a href="#38677075">parent</a><span>|</span><a href="#38677156">next</a><span>|</span><label class="collapse" for="c-38677379">[-]</label><label class="expand" for="c-38677379">[1 more]</label></div><br/><div class="children"><div class="content">This is less cyclical and more common sense changes.<p>AMD used to have 5-wide VLIW. Theoretical performance was massive, but NOTHING could take advantage of 5-wide parallelism consistently. When they switched to VLIW-4, essentially zero performance per CU was lost. GCN went the complete opposite direction and got rid of all the VLIW in exchange for tons of flexibility.<p>It turns out that (as we&#x27;ve known for a very long time) in-order parallelism has diminishing returns. If you look at in-order CPUs for example, two generic execution ports can be used something like 80% of the time. Moving to 3 in-order drops the usage of the third to something like 15-30% and the fourth is single digits of usage.<p>Adding a second port should guarantee lots of use, but only if the second unit is kept flexible. RDNA3 can only use the second port for a handful of operations which means that it can&#x27;t be used anywhere near that 80% metric for most applications. Future versions of RDNA and CDNA with more flexible second ports should start to be able to leverage those extra compute units (if they decide it&#x27;s worth the transistors).</div><br/></div></div><div id="38677156" class="c"><input type="checkbox" id="c-38677156" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#38676628">root</a><span>|</span><a href="#38677075">parent</a><span>|</span><a href="#38677379">prev</a><span>|</span><a href="#38677514">next</a><span>|</span><label class="collapse" for="c-38677156">[-]</label><label class="expand" for="c-38677156">[1 more]</label></div><br/><div class="children"><div class="content">It is not so much that it is cyclical than VLIW being good for numerical&#x2F;DSP stuff with more predictable memory access and terrible for general computation. Itanium was ok at floating point code, but terrible at typical pointer chasing loads.</div><br/></div></div></div></div><div id="38677514" class="c"><input type="checkbox" id="c-38677514" checked=""/><div class="controls bullet"><span class="by">Symmetry</span><span>|</span><a href="#38676628">root</a><span>|</span><a href="#38676884">parent</a><span>|</span><a href="#38677075">prev</a><span>|</span><a href="#38678299">next</a><span>|</span><label class="collapse" for="c-38677514">[-]</label><label class="expand" for="c-38677514">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d argue that the Hexagon DSPs shipped with all Qualcomm&#x27;s phone SoC which a large number of us have in our pockets right now are probably more prominent example.<p>Itanium&#x27;s problem wasn&#x27;t so much divergent control flow as not being able to handle unpredictable latencies on load instructions without halting all execution entirely.  There were some features that were, in theory, supposed to deal with this but using them was where the magic compiler problem came in.<p>For DSPs you can usually predict memory latency so this isn&#x27;t an issue and VLIW works great there even with some amount of branching.</div><br/></div></div><div id="38678299" class="c"><input type="checkbox" id="c-38678299" checked=""/><div class="controls bullet"><span class="by">ip26</span><span>|</span><a href="#38676628">root</a><span>|</span><a href="#38676884">parent</a><span>|</span><a href="#38677514">prev</a><span>|</span><a href="#38676986">next</a><span>|</span><label class="collapse" for="c-38678299">[-]</label><label class="expand" for="c-38678299">[1 more]</label></div><br/><div class="children"><div class="content">It worked fine for graphics chips for a long time. Graphics workloads have embarrassing levels of static ILP.</div><br/></div></div></div></div><div id="38676986" class="c"><input type="checkbox" id="c-38676986" checked=""/><div class="controls bullet"><span class="by">yarg</span><span>|</span><a href="#38676628">parent</a><span>|</span><a href="#38676884">prev</a><span>|</span><a href="#38677157">next</a><span>|</span><label class="collapse" for="c-38676986">[-]</label><label class="expand" for="c-38676986">[2 more]</label></div><br/><div class="children"><div class="content">Read up on SIMD in general.<p>(The means of processing as opposed to the language used to dispatch commands.)<p>(And worth baring in mind is the fact that terms such as VLIW4 and VLIW5 refer to specific implementations.)<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Single_instruction,_multiple_data" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Single_instruction,_multiple_d...</a></div><br/><div id="38677145" class="c"><input type="checkbox" id="c-38677145" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#38676628">root</a><span>|</span><a href="#38676986">parent</a><span>|</span><a href="#38677157">next</a><span>|</span><label class="collapse" for="c-38677145">[-]</label><label class="expand" for="c-38677145">[1 more]</label></div><br/><div class="children"><div class="content">simd != vliw though.</div><br/></div></div></div></div></div></div><div id="38677157" class="c"><input type="checkbox" id="c-38677157" checked=""/><div class="controls bullet"><span class="by">feanaro</span><span>|</span><a href="#38676628">prev</a><span>|</span><a href="#38677349">next</a><span>|</span><label class="collapse" for="c-38677157">[-]</label><label class="expand" for="c-38677157">[5 more]</label></div><br/><div class="children"><div class="content">A bit off topic, but when did &quot;compute&quot; become a noun? It is extremely grating to my ears.</div><br/><div id="38677433" class="c"><input type="checkbox" id="c-38677433" checked=""/><div class="controls bullet"><span class="by">EarthLaunch</span><span>|</span><a href="#38677157">parent</a><span>|</span><a href="#38677731">next</a><span>|</span><label class="collapse" for="c-38677433">[-]</label><label class="expand" for="c-38677433">[1 more]</label></div><br/><div class="children"><div class="content">I first recall it from the rise of AWS at least.  &quot;Amazon Elastic Compute Cloud (EC2)&quot; launched 2006 [0].  Google Trends [1].<p>0: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Amazon_Elastic_Compute_Cloud" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Amazon_Elastic_Compute_Cloud</a><p>1: <a href="https:&#x2F;&#x2F;trends.google.com&#x2F;trends&#x2F;explore?date=all&amp;q=Compute&amp;hl=en" rel="nofollow noreferrer">https:&#x2F;&#x2F;trends.google.com&#x2F;trends&#x2F;explore?date=all&amp;q=Compute&amp;...</a></div><br/></div></div><div id="38677731" class="c"><input type="checkbox" id="c-38677731" checked=""/><div class="controls bullet"><span class="by">overstay8930</span><span>|</span><a href="#38677157">parent</a><span>|</span><a href="#38677433">prev</a><span>|</span><a href="#38678320">next</a><span>|</span><label class="collapse" for="c-38677731">[-]</label><label class="expand" for="c-38677731">[1 more]</label></div><br/><div class="children"><div class="content">It was a thing in Deep Space Nine (1999) so either the lingo was really on point or it&#x27;s just something that has fallen in and out of favor.</div><br/></div></div><div id="38678320" class="c"><input type="checkbox" id="c-38678320" checked=""/><div class="controls bullet"><span class="by">ip26</span><span>|</span><a href="#38677157">parent</a><span>|</span><a href="#38677731">prev</a><span>|</span><a href="#38677247">next</a><span>|</span><label class="collapse" for="c-38678320">[-]</label><label class="expand" for="c-38678320">[1 more]</label></div><br/><div class="children"><div class="content">I don’t know the reason, but I suspect you can reason it out.</div><br/></div></div><div id="38677247" class="c"><input type="checkbox" id="c-38677247" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#38677157">parent</a><span>|</span><a href="#38678320">prev</a><span>|</span><a href="#38677349">next</a><span>|</span><label class="collapse" for="c-38677247">[-]</label><label class="expand" for="c-38677247">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s pretty common these days due to AI with GPU-alike chips.</div><br/></div></div></div></div><div id="38677349" class="c"><input type="checkbox" id="c-38677349" checked=""/><div class="controls bullet"><span class="by">Geisterde</span><span>|</span><a href="#38677157">prev</a><span>|</span><a href="#38676937">next</a><span>|</span><label class="collapse" for="c-38677349">[-]</label><label class="expand" for="c-38677349">[1 more]</label></div><br/><div class="children"><div class="content">5 points for the Luddite (me) that said amd would leverage their knowledge of chiplets and bus fabrics to make a comeback in AI. I wont pretend to be able to read this article, or read in general, just wanted to plant my flag.</div><br/></div></div><div id="38676937" class="c"><input type="checkbox" id="c-38676937" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#38677349">prev</a><span>|</span><label class="collapse" for="c-38676937">[-]</label><label class="expand" for="c-38676937">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Compute has been outpacing memory for decades. Like CPUs, GPUs have countered this with increasingly sophisticated caching strategies.<p>I&#x27;d say it&#x27;s rather the contrary. Unlike CPUs, GPUs don&#x27;t attempt to directly counter this. By accepting higher latencies, they have let themselves parallelize much more widely (or wildly) relative to CPUs - and the high number of parallel pseudo-threads provides a &quot;latency hiding&quot; effect.<p>This effect is illustrated for example, in this presentation on optimizing GPU code:<p><a href="https:&#x2F;&#x2F;www.olcf.ornl.gov&#x2F;wp-content&#x2F;uploads&#x2F;2019&#x2F;12&#x2F;03-CUDA-Fundamental-Optimization-Part-1.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.olcf.ornl.gov&#x2F;wp-content&#x2F;uploads&#x2F;2019&#x2F;12&#x2F;03-CUDA...</a><p>(lame) animation on slide 11 and onwards.</div><br/><div id="38677663" class="c"><input type="checkbox" id="c-38677663" checked=""/><div class="controls bullet"><span class="by">ColonelPhantom</span><span>|</span><a href="#38676937">parent</a><span>|</span><label class="collapse" for="c-38677663">[-]</label><label class="expand" for="c-38677663">[4 more]</label></div><br/><div class="children"><div class="content">GPUs do deal with memory in other ways than parallelism! That&#x27;s why GPUs tend to offer huge register files (up to 256 architectural registers per thread on RDNA1!) and local memories (up to 64KB LDS per workgroup on RDNA1). This means lots of work can be done purely in registers and LDS, and trips to global memory are way rarer compared to CPUs where global memory contain everything except 16 or so architectural registers.<p>Even then, global memory is an issue. And not just because of latency, but also bandwidth! That&#x27;s why RDNA2 and Ada both added a ton of last-level cache, not to better hide latency (although it&#x27;s a welcome addition), but mostly as a &#x27;bandwidth amplifier&#x27;.</div><br/><div id="38680110" class="c"><input type="checkbox" id="c-38680110" checked=""/><div class="controls bullet"><span class="by">m_mueller</span><span>|</span><a href="#38676937">root</a><span>|</span><a href="#38677663">parent</a><span>|</span><a href="#38679913">next</a><span>|</span><label class="collapse" for="c-38680110">[-]</label><label class="expand" for="c-38680110">[1 more]</label></div><br/><div class="children"><div class="content">I think register files, shared memory and even texture memory patterns are basically the GPU&#x27;s way of having fully programmable caches, as opposed to cache being a grey box like on CPUs. Overall I think for HPC code it works out better. Looking at vectorized and cache-optimized CPU code gives me the shivers...<p>Mostly due to the above I think Intel&#x27;s years long push to try to have people just run their CPU-optimized OpenMP codes on their Xeon Phis was the downfall of that architecture - it just can&#x27;t win against a data-parallel-first framework like CUDA, mainly because there the data parallel compute thread can be programmed just as such, rather than trying to get the compiler to behave exactly as you want (and in the end giving up and dropping down to assembly-like SSE instructions).</div><br/></div></div><div id="38679913" class="c"><input type="checkbox" id="c-38679913" checked=""/><div class="controls bullet"><span class="by">soundarana</span><span>|</span><a href="#38676937">root</a><span>|</span><a href="#38677663">parent</a><span>|</span><a href="#38680110">prev</a><span>|</span><a href="#38677877">next</a><span>|</span><label class="collapse" for="c-38679913">[-]</label><label class="expand" for="c-38679913">[1 more]</label></div><br/><div class="children"><div class="content">CPUs have been growing their register files too - AVX512 has 32 x 512 bits registers.</div><br/></div></div><div id="38677877" class="c"><input type="checkbox" id="c-38677877" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#38676937">root</a><span>|</span><a href="#38677663">parent</a><span>|</span><a href="#38679913">prev</a><span>|</span><label class="collapse" for="c-38677877">[-]</label><label class="expand" for="c-38677877">[1 more]</label></div><br/><div class="children"><div class="content">Two valid points, +1.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>