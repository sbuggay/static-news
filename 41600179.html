<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726909253206" as="style"/><link rel="stylesheet" href="styles.css?v=1726909253206"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2409.12917">Training Language Models to Self-Correct via Reinforcement Learning</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>weirdcat</span> | <span>89 comments</span></div><br/><div><div id="41600883" class="c"><input type="checkbox" id="c-41600883" checked=""/><div class="controls bullet"><span class="by">elcomet</span><span>|</span><a href="#41605554">next</a><span>|</span><label class="collapse" for="c-41600883">[-]</label><label class="expand" for="c-41600883">[10 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a similar approach to OpenAI&#x27;s o1 model ( it&#x27;s not cited, but there&#x27;s no available paper for o1).<p>I don&#x27;t see any mention of weight release unfortunately.</div><br/><div id="41601581" class="c"><input type="checkbox" id="c-41601581" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#41600883">parent</a><span>|</span><a href="#41601324">next</a><span>|</span><label class="collapse" for="c-41601581">[-]</label><label class="expand" for="c-41601581">[7 more]</label></div><br/><div class="children"><div class="content">I think this submission paper is talking about reinforcement learning as part of&#x2F;after the main training, then the model does inference as normal.<p>They might have done that for O1, but the bigger change is the &quot;runtime train of thought&quot; that once the model received the prompt and before giving a definitive answer, it &quot;thinks&quot; with words and readjusts at runtime.<p>At least that&#x27;s my understanding from these two approaches, and if that&#x27;s true, then it&#x27;s not similar.<p>AFAIK, OpenAI been doing reinforcement learning since the first version of ChatGPT for all future models, that&#x27;s why you can leave feedback in the UI in the first place.</div><br/><div id="41601738" class="c"><input type="checkbox" id="c-41601738" checked=""/><div class="controls bullet"><span class="by">numeri</span><span>|</span><a href="#41600883">root</a><span>|</span><a href="#41601581">parent</a><span>|</span><a href="#41602698">next</a><span>|</span><label class="collapse" for="c-41601738">[-]</label><label class="expand" for="c-41601738">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI stated [1] that one of the breakthroughs needed for o1&#x27;s train of thought to work was reinforcement learning to teach it to recover from faulty reasoning.<p>&gt; Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses. It learns to recognize and correct its mistakes. It learns to break down tricky steps into simpler ones. It learns to try a different approach when the current one isn’t working.<p>That&#x27;s incredibly similar to this paper, which is discusses the difficulty in finding a training method that guides the model to learn a self-correcting technique (in which subsequent attempts learn from and improve on previous attempts), instead of just &quot;collapsing&quot; into a mode of trying to get the answer right with the very first try.<p>[1]: <a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;learning-to-reason-with-llms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;learning-to-reason-with-llms&#x2F;</a></div><br/></div></div><div id="41602698" class="c"><input type="checkbox" id="c-41602698" checked=""/><div class="controls bullet"><span class="by">josh-sematic</span><span>|</span><a href="#41600883">root</a><span>|</span><a href="#41601581">parent</a><span>|</span><a href="#41601738">prev</a><span>|</span><a href="#41601705">next</a><span>|</span><label class="collapse" for="c-41602698">[-]</label><label class="expand" for="c-41602698">[1 more]</label></div><br/><div class="children"><div class="content">They are indeed similar and OpenAI did indeed use RL at training time in a way that has not been done before, as does this approach. Yes both also involve some additional inference-time generation, but the problem is that (at least as of now) you can&#x27;t get standard LLMs to actually do well with extra inference-time generation unless you have a training process that uses RL to teach them to do so effectively. I&#x27;m working on a blog post to explain more about this aimed at HN-level audiences. Stay tuned!</div><br/></div></div><div id="41601705" class="c"><input type="checkbox" id="c-41601705" checked=""/><div class="controls bullet"><span class="by">nsagent</span><span>|</span><a href="#41600883">root</a><span>|</span><a href="#41601581">parent</a><span>|</span><a href="#41602698">prev</a><span>|</span><a href="#41602667">next</a><span>|</span><label class="collapse" for="c-41601705">[-]</label><label class="expand" for="c-41601705">[1 more]</label></div><br/><div class="children"><div class="content">Both models generate an answer after multiple turns, where each turn has access to the outputs from a previous turn. Both refer to the chain of outputs as a trace.<p>Since OpenAI did not specify what exactly is in their reasoning trace, it&#x27;s not clear what if any difference there is between the approaches. They could be vastly different, or they could be slight variations of each other. Without details from OpenAI, it&#x27;s not currently possible to tell.</div><br/></div></div><div id="41602667" class="c"><input type="checkbox" id="c-41602667" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41600883">root</a><span>|</span><a href="#41601581">parent</a><span>|</span><a href="#41601705">prev</a><span>|</span><a href="#41601324">next</a><span>|</span><label class="collapse" for="c-41602667">[-]</label><label class="expand" for="c-41602667">[3 more]</label></div><br/><div class="children"><div class="content">you are describing the same thing?<p>sorry as a practitioner i’m having trouble understanding what point&#x2F;distinction you are trying to make</div><br/><div id="41607270" class="c"><input type="checkbox" id="c-41607270" checked=""/><div class="controls bullet"><span class="by">myownpetard</span><span>|</span><a href="#41600883">root</a><span>|</span><a href="#41602667">parent</a><span>|</span><a href="#41601324">next</a><span>|</span><label class="collapse" for="c-41607270">[-]</label><label class="expand" for="c-41607270">[2 more]</label></div><br/><div class="children"><div class="content">These are two very different things.<p>One is talking about an improvement made by making control flow changes during inference (no weights updates).<p>The other is talking about using reinforcement learning to do weight updates during training to promote a particular type response.<p>OpenAI had previously used reinforcement learning with human feedback (RLHF), which essentially relies on manual human scoring as its reward function, which is inherently slow and limited.<p>o1 and this paper talk about using techniques to create a useful reward function to use in RL that doesn&#x27;t rely on human feedback.</div><br/><div id="41607522" class="c"><input type="checkbox" id="c-41607522" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41600883">root</a><span>|</span><a href="#41607270">parent</a><span>|</span><a href="#41601324">next</a><span>|</span><label class="collapse" for="c-41607522">[-]</label><label class="expand" for="c-41607522">[1 more]</label></div><br/><div class="children"><div class="content">No?<p>&gt; I think this submission paper is talking about reinforcement learning as part of&#x2F;after the main training<p>Reinforcement learning to promote a particular type of self-correction response<p>&gt; They might have done that for O1, but the bigger change is the &quot;runtime train of thought&quot; that once the model received the prompt and before giving a definitive answer,<p>Also reinforcement learning to promote certain reasoning trace<p>&gt; o1 and this paper talk about using techniques to create a useful reward function to use in RL that doesn&#x27;t rely on human feedback.<p>Exactly -&gt; the same thing</div><br/></div></div></div></div></div></div></div></div><div id="41601324" class="c"><input type="checkbox" id="c-41601324" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41600883">parent</a><span>|</span><a href="#41601581">prev</a><span>|</span><a href="#41605554">next</a><span>|</span><label class="collapse" for="c-41601324">[-]</label><label class="expand" for="c-41601324">[2 more]</label></div><br/><div class="children"><div class="content">how is it similar?</div><br/><div id="41601667" class="c"><input type="checkbox" id="c-41601667" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41600883">root</a><span>|</span><a href="#41601324">parent</a><span>|</span><a href="#41605554">next</a><span>|</span><label class="collapse" for="c-41601667">[-]</label><label class="expand" for="c-41601667">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1821277264996352246" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1821277264996352246</a></div><br/></div></div></div></div></div></div><div id="41605554" class="c"><input type="checkbox" id="c-41605554" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#41600883">prev</a><span>|</span><a href="#41601470">next</a><span>|</span><label class="collapse" for="c-41605554">[-]</label><label class="expand" for="c-41605554">[2 more]</label></div><br/><div class="children"><div class="content">I found the paper a tad difficult to understand because it spends a lot of time circling around the main thesis instead of directly describing.  So, to the best of my understanding:<p>We want to improve LLM&#x27;s abilities to give correct answers to hard problems. One theory is that we can do that by training a &quot;Self Correcting&quot; behavior into the models where they can take as input a wrong answer and improve it to a better&#x2F;correct answer.<p>This has been explored previously, trying to train this behavior using various Reinforcement techniques where the reward is based on how good the &quot;corrected&quot; answer is.  So far it hasn&#x27;t worked well, and the trained behavior doesn&#x27;t generalize well.<p>The thesis of the paper is that this is because when the model is presented with a training example of `Answer 1, Reasoning, Corrected Answer`, and a signal of &quot;Make Corrected Answer Better&quot; it actually has _two_ perfectly viable ways to do that.  One is to improve `Reasoning, Corrected Answer`, which would yield a higher reward and is what we want.  The other, just as valid solution, is to simply improve `Answer 1` and have `Corrected Answer` = `Answer 1`.<p>The latter is what existing research has shown happens, and why so far attempts to train the desired behavior has failed.  The models just try to improve their answers, not their correcting behaviors.  This paper&#x27;s solution is to change the training regimen slightly to encourage the model to use the former approach.  And thus, hopefully, get the model to actually train the desired behavior of correcting previous answers.<p>This is done by doing two stages of training.  In the first stage, the model is forced (by KL divergence loss) to keep its first answers the same, while being rewarded for improving the second answer.  This helps keep the model&#x27;s distribution of initial answers the same, avoiding the issue later where the model doesn&#x27;t see as many &quot;wrong&quot; answers because wrong answers were trained out of the model.  But it helps initialize the &quot;self correcting&quot; behavior into the model.<p>In the second stage the model is free to change the first answer, but they tweak the reward function to give higher rewards for &quot;flips&quot; (where answer 1 was bad, but answer 2 was good).  So in this second stage it can use both strategies, improving its first answer or improving its self correcting, but it gets more rewards for the latter behavior.  This seems to be a kind of refinement on the model, to improve things overall, while still keeping the self correcting behavior intact.<p>Anyway, blah blah blah, metrics showing the technique working better and generalizing better.<p>Seems reasonable to me.  I&#x27;d be a bit worried about, in Stage 2, the model learning to write _worse_ answers for Answer 1 so it can maximize the reward for flipping answers.  So you&#x27;d need some kind of balancing to ensure Answer 1 doesn&#x27;t get worse.  Not sure if that&#x27;s in their reward function or not, or if its even a valid concern in practice.</div><br/><div id="41607893" class="c"><input type="checkbox" id="c-41607893" checked=""/><div class="controls bullet"><span class="by">jasfi</span><span>|</span><a href="#41605554">parent</a><span>|</span><a href="#41601470">next</a><span>|</span><label class="collapse" for="c-41607893">[-]</label><label class="expand" for="c-41607893">[1 more]</label></div><br/><div class="children"><div class="content">Circling around the idea in a response describes what I see in a lot of LLM output quite well. I haven&#x27;t tried o1 myself, but it does seem to fix that problem.</div><br/></div></div></div></div><div id="41601470" class="c"><input type="checkbox" id="c-41601470" checked=""/><div class="controls bullet"><span class="by">plaguuuuuu</span><span>|</span><a href="#41605554">prev</a><span>|</span><a href="#41601651">next</a><span>|</span><label class="collapse" for="c-41601470">[-]</label><label class="expand" for="c-41601470">[13 more]</label></div><br/><div class="children"><div class="content">LLMs have no direct recollection of the qualia of their own training. This is at least a major way that I self-correct myself: if I&#x27;m about to talk about something I know, I&#x27;ll try and figure out how&#x2F;why I know that thing and in so doing, try to gauge whether I actually know that thing, if I&#x27;m hallucinating, or if I actually heard it from a less than reliable source etc.<p>I don&#x27;t think LLMs can self-correct without remembering their own training in some way.</div><br/><div id="41601525" class="c"><input type="checkbox" id="c-41601525" checked=""/><div class="controls bullet"><span class="by">QuadmasterXLII</span><span>|</span><a href="#41601470">parent</a><span>|</span><a href="#41604120">next</a><span>|</span><label class="collapse" for="c-41601525">[-]</label><label class="expand" for="c-41601525">[4 more]</label></div><br/><div class="children"><div class="content">So you’re saying the solution is to prefix each training batch with a description of a sensory experience (You read the following in a paris cafe in 1997. While you read, you have an excellent baguette and some boiled eggs, and over-roasted coffee. The woman one table over is wearing a beautiful blue hat) and then post-train the final model into recalling the setting where it read any piece of text, or failing to recall any experience when presented with text it didn’t read?<p>(If someone tries this and it works, I’m quitting my phd and going back to camp counseling)</div><br/><div id="41601731" class="c"><input type="checkbox" id="c-41601731" checked=""/><div class="controls bullet"><span class="by">wpietri</span><span>|</span><a href="#41601470">root</a><span>|</span><a href="#41601525">parent</a><span>|</span><a href="#41604073">next</a><span>|</span><label class="collapse" for="c-41601731">[-]</label><label class="expand" for="c-41601731">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s what they&#x27;re saying at all. They&#x27;re talking not about qualia in the human sense, but specifically about &quot;the qualia of their own training&quot;. That is, the corpus that LLMs &quot;learn&quot; from and the &quot;experiences&quot; of those texts that are generalized during the training process. Both the raw data and the memory of &quot;learning&quot; is discarded.<p>So if one were to improve an LLM along those lines, I believe it would be something like: 1) LLM is asked a question. 2) LLM comes up with an initial response. 3) LLM retrieves the related &quot;learning&quot; history behind that answer and related portions of the corpus. 4) LLM compares the initial answer with the richer set of information, looking for conflicts between the initial answer and the broader set, or &quot;learning&quot; choices that may be false.  6) LLM generates a better answer and gives it. 7) LLM incorporates this new &quot;learning&quot;.<p>And that strikes me as a pretty reasonable long-term approach, if not one that fits within the constraints of the current gold rush.</div><br/><div id="41604093" class="c"><input type="checkbox" id="c-41604093" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#41601470">root</a><span>|</span><a href="#41601731">parent</a><span>|</span><a href="#41604073">next</a><span>|</span><label class="collapse" for="c-41604093">[-]</label><label class="expand" for="c-41604093">[1 more]</label></div><br/><div class="children"><div class="content">So...reinforcement learning?</div><br/></div></div></div></div></div></div><div id="41604120" class="c"><input type="checkbox" id="c-41604120" checked=""/><div class="controls bullet"><span class="by">triclops200</span><span>|</span><a href="#41601470">parent</a><span>|</span><a href="#41601525">prev</a><span>|</span><a href="#41603761">next</a><span>|</span><label class="collapse" for="c-41604120">[-]</label><label class="expand" for="c-41604120">[1 more]</label></div><br/><div class="children"><div class="content">Strong disagree:
<a href="https:&#x2F;&#x2F;mypapers.nyc3.cdn.digitaloceanspaces.com&#x2F;the_phenomenology_of_machine.pdf" rel="nofollow">https:&#x2F;&#x2F;mypapers.nyc3.cdn.digitaloceanspaces.com&#x2F;the_phenome...</a><p>See also: <a href="https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S1571064523001094" rel="nofollow">https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S157106452...</a>
o1&#x27;s training regime is described by the &quot;strange particle&quot; model in this formulation</div><br/></div></div><div id="41603761" class="c"><input type="checkbox" id="c-41603761" checked=""/><div class="controls bullet"><span class="by">numeri</span><span>|</span><a href="#41601470">parent</a><span>|</span><a href="#41604120">prev</a><span>|</span><a href="#41601637">next</a><span>|</span><label class="collapse" for="c-41603761">[-]</label><label class="expand" for="c-41603761">[1 more]</label></div><br/><div class="children"><div class="content">Sort of like this? It does help: Source-Aware Training Enables Knowledge Attribution in Language Models (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.01019" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.01019</a>)<p>From the abstract:<p>&gt; ... To give LLMs such ability, we explore source-aware training -- a recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning stage to teach the LLM to cite a supporting pretraining source when prompted.</div><br/></div></div><div id="41601637" class="c"><input type="checkbox" id="c-41601637" checked=""/><div class="controls bullet"><span class="by">williamcotton</span><span>|</span><a href="#41601470">parent</a><span>|</span><a href="#41603761">prev</a><span>|</span><a href="#41603662">next</a><span>|</span><label class="collapse" for="c-41601637">[-]</label><label class="expand" for="c-41601637">[5 more]</label></div><br/><div class="children"><div class="content">Unless you’re under the influence of something or having a severe mental health crisis you are not hallucinating, you’re confabulating.</div><br/><div id="41602198" class="c"><input type="checkbox" id="c-41602198" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#41601470">root</a><span>|</span><a href="#41601637">parent</a><span>|</span><a href="#41603662">next</a><span>|</span><label class="collapse" for="c-41602198">[-]</label><label class="expand" for="c-41602198">[4 more]</label></div><br/><div class="children"><div class="content">According to which philologist? In short: they are both weak terms, &#x27;hallucination&#x27; and &#x27;confabulation&#x27;, and we are using them in this context very loosely (and it should be in the open).<p>About the terms themselves, &quot;confabulate&quot; means &quot;exchanging stories&quot;, while &quot;hallucinate&quot; is less clear but probably means &quot;to err&quot;. In psychiatry, &quot;hallucinate&quot; was apparently introduced by Esquirol and &quot;confabulate&quot; by Wernicke and Bonhoeffer; neither concept seems to be akin to the substance of the phenomenon of &quot;<i>stochastic parrots bullshitting an unchecked narrative through formal plausibility</i>&quot;.<p>See: &quot;Hallucinations and related concepts - their conceptual background&quot; - <a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC4515540&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC4515540&#x2F;</a><p>and: &quot;The Confabulating Mind: How the Brain Creates Reality&quot; - <a href="https:&#x2F;&#x2F;psychiatryonline.org&#x2F;doi&#x2F;full&#x2F;10.1176&#x2F;appi.ajp.2008.08010037" rel="nofollow">https:&#x2F;&#x2F;psychiatryonline.org&#x2F;doi&#x2F;full&#x2F;10.1176&#x2F;appi.ajp.2008....</a></div><br/><div id="41604115" class="c"><input type="checkbox" id="c-41604115" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#41601470">root</a><span>|</span><a href="#41602198">parent</a><span>|</span><a href="#41603662">next</a><span>|</span><label class="collapse" for="c-41604115">[-]</label><label class="expand" for="c-41604115">[3 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s invent a new term, then!<p>I propose:<p>- digimagination<p>- stochafubalation<p>- statillucination<p>- matmulshitting<p>Let&#x27;s ask chatgpt free (coz I&#x27;m cheap) what it not-thinks about these, and ask for more statistically generated bullshit:<p>-- 8&lt; --<p>These are some creative and fun suggestions! Here&#x27;s my take:<p><pre><code>    Digimagination: This one has a sleek, almost sci-fi ring to it. It captures the digital, machine-driven nature of language model &quot;hallucination,&quot; but presents it in a positive, imaginative light. This could highlight the creative aspect of AI responses, even when they aren&#x27;t grounded in fact.

    Stochafubalation: Combining &quot;stochastic&quot; (random probability) with &quot;confabulation&quot; is clever. It emphasizes the statistical nature of language models making up facts, though it could be a bit of a mouthful.

    Statillucination: I like how this merges &quot;statistical&quot; and &quot;hallucination.&quot; It feels technical and would fit right into the AI research community&#x27;s vocabulary.

    Matmulshitting: This one is tongue-in-cheek and hilarious! It references matrix multiplication (which drives many neural network operations) with a humorous, blunt twist. Probably not for formal use, but great for informal contexts!
</code></pre>
Here are a few more neologisms I can propose:<p><pre><code>    Probafabrication: A combination of &quot;probability&quot; and &quot;fabrication,&quot; highlighting the statistical underpinnings of how LLMs generate falsehoods.

    Algoillusion: A fusion of &quot;algorithm&quot; and &quot;illusion,&quot; suggesting the misleading or illusionary nature of generated information.

    Synthifalse: A contraction of &quot;synthetic&quot; and &quot;false,&quot; pointing to AI-generated responses that are not grounded in reality.

    Pseudoformulation: Emphasizing that the response is a pseudo (false) formulation rather than a real answer.

    Modelucination: A blend of &quot;model&quot; and &quot;hallucination,&quot; capturing the fact that the machine learning model is generating false or misleading data.</code></pre></div><br/><div id="41604828" class="c"><input type="checkbox" id="c-41604828" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#41601470">root</a><span>|</span><a href="#41604115">parent</a><span>|</span><a href="#41603662">next</a><span>|</span><label class="collapse" for="c-41604828">[-]</label><label class="expand" for="c-41604828">[2 more]</label></div><br/><div class="children"><div class="content">My turn. What about &quot;pretending&quot;?<p>My judgement of yours: I love &#x27;stochfabulating&#x27; and &#x27;matmulshitting&#x27;, but I condemn &#x27;digimagining&#x27;. Digimagining is doable in proper terms (no pun intended).</div><br/><div id="41604970" class="c"><input type="checkbox" id="c-41604970" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#41601470">root</a><span>|</span><a href="#41604828">parent</a><span>|</span><a href="#41603662">next</a><span>|</span><label class="collapse" for="c-41604970">[-]</label><label class="expand" for="c-41604970">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Pretending&quot; is too human to my taste: it assumes the thing doing the pretend thing knows about the real thing. It&#x27;s something kids do during play. I&#x27;m too afraid of the consequences to admit LLMs are anywhere near this situation <i>wink</i></div><br/></div></div></div></div></div></div></div></div></div></div><div id="41603662" class="c"><input type="checkbox" id="c-41603662" checked=""/><div class="controls bullet"><span class="by">groby_b</span><span>|</span><a href="#41601470">parent</a><span>|</span><a href="#41601637">prev</a><span>|</span><a href="#41601651">next</a><span>|</span><label class="collapse" for="c-41603662">[-]</label><label class="expand" for="c-41603662">[1 more]</label></div><br/><div class="children"><div class="content">I think your overweighting the value of that in day-to-day use. As folks accumulate knowledge, a common pattern (especially for things not embedded in a framework - trivia-like data) is a &quot;I have no idea why I&#x27;d know this, but the answer is X&quot;.<p>But even if it&#x27;s embedded in a framework, say CS, the qualia fade in the background as time passes. E.g. like everybody in CS, I&#x27;m pretty much able to quote O() performance characteristics of a sizeable number of algorithms off the bat. If you ask me where I learned it, for that specific algorithm - that&#x27;s long receded into the past.<p>When humans self-correct, the normal process isn&#x27;t &quot;gauging whether you know the thing&quot; or the even more impressive feat of calling up if you heard it from a &quot;less than reliable source&quot;. There&#x27;s a fuzzy sense of &quot;I don&#x27;t fully understand it&quot;, and self-correction means re-verifying the info from a trusted source.<p>So, no, I don&#x27;t think the qualia matter for recall as much as you think.</div><br/></div></div></div></div><div id="41601651" class="c"><input type="checkbox" id="c-41601651" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#41601470">prev</a><span>|</span><a href="#41600937">next</a><span>|</span><label class="collapse" for="c-41601651">[-]</label><label class="expand" for="c-41601651">[1 more]</label></div><br/><div class="children"><div class="content">Is this effectively some sort of knowledge distillation?</div><br/></div></div><div id="41600937" class="c"><input type="checkbox" id="c-41600937" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#41601651">prev</a><span>|</span><a href="#41602412">next</a><span>|</span><label class="collapse" for="c-41600937">[-]</label><label class="expand" for="c-41600937">[42 more]</label></div><br/><div class="children"><div class="content">Spoiler: You&#x27;re never going to get rid of hallucinations in the autoregressive, next token prediction paradigm (aka LeCun&#x27;s Law).<p>The issue here is people trying to use language models as deterministic problem solvers, rather than for what they actually excel at (semi-creative text generation).</div><br/><div id="41602700" class="c"><input type="checkbox" id="c-41602700" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41600937">parent</a><span>|</span><a href="#41602309">next</a><span>|</span><label class="collapse" for="c-41602700">[-]</label><label class="expand" for="c-41602700">[3 more]</label></div><br/><div class="children"><div class="content">LeCuns argument is seriously flawed. It is not at all a rigorous one and you should not make such sweeping statements based on nothing.</div><br/><div id="41603415" class="c"><input type="checkbox" id="c-41603415" checked=""/><div class="controls bullet"><span class="by">barbarr</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41602700">parent</a><span>|</span><a href="#41602309">next</a><span>|</span><label class="collapse" for="c-41603415">[-]</label><label class="expand" for="c-41603415">[2 more]</label></div><br/><div class="children"><div class="content">At this point I just invert everything LeCun says about AI. Chances are he&#x27;ll flip flop on his own statement a few months later anyways.</div><br/><div id="41604838" class="c"><input type="checkbox" id="c-41604838" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41603415">parent</a><span>|</span><a href="#41602309">next</a><span>|</span><label class="collapse" for="c-41604838">[-]</label><label class="expand" for="c-41604838">[1 more]</label></div><br/><div class="children"><div class="content">LeCun has been pretty steady for years now.</div><br/></div></div></div></div></div></div><div id="41602309" class="c"><input type="checkbox" id="c-41602309" checked=""/><div class="controls bullet"><span class="by">shawnz</span><span>|</span><a href="#41600937">parent</a><span>|</span><a href="#41602700">prev</a><span>|</span><a href="#41601085">next</a><span>|</span><label class="collapse" for="c-41602309">[-]</label><label class="expand" for="c-41602309">[2 more]</label></div><br/><div class="children"><div class="content">Does anyone here know, has anyone tried something like feeding the perplexity of previous tokens back into the model, so that it has a way of knowing when it&#x27;s going off the rails? Maybe it could be trained to start responding less confidently in those cases, reducing its desire to hallucinate.</div><br/><div id="41604564" class="c"><input type="checkbox" id="c-41604564" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41602309">parent</a><span>|</span><a href="#41601085">next</a><span>|</span><label class="collapse" for="c-41604564">[-]</label><label class="expand" for="c-41604564">[1 more]</label></div><br/><div class="children"><div class="content">Models already know when they are going off the rails.
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41504226">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41504226</a>.
That&#x27;s not the problem. The problem is that they don&#x27;t care to tell you.</div><br/></div></div></div></div><div id="41601085" class="c"><input type="checkbox" id="c-41601085" checked=""/><div class="controls bullet"><span class="by">plewd</span><span>|</span><a href="#41600937">parent</a><span>|</span><a href="#41602309">prev</a><span>|</span><a href="#41602074">next</a><span>|</span><label class="collapse" for="c-41601085">[-]</label><label class="expand" for="c-41601085">[34 more]</label></div><br/><div class="children"><div class="content">Is LeCun&#x27;s Law even a thing? Searching up for it doesn&#x27;t yield many results, except for a HN comment where it has a different definition. I guess it could be from some obscure paper, but with how poorly it&#x27;s documented it seems weird to bring it up in this context.</div><br/><div id="41601206" class="c"><input type="checkbox" id="c-41601206" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601085">parent</a><span>|</span><a href="#41601119">next</a><span>|</span><label class="collapse" for="c-41601206">[-]</label><label class="expand" for="c-41601206">[30 more]</label></div><br/><div class="children"><div class="content">I think the OP may be referring to this slide that Yann LeCun has presented on several occasions:<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;MiqLoAZFRSE?si=tIQ_ya2tiMCymiAh&amp;t=901" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;MiqLoAZFRSE?si=tIQ_ya2tiMCymiAh&amp;t=901</a><p>To quote from the slide:<p><pre><code>  * Probability e that any produced token takes us outside the set of correct answers
  * Probability that answer of length n is correct
  * P(correct) = (1-e)^n
  * This diverges exponentially
  * It&#x27;s not fixable (without a major redesign)</code></pre></div><br/><div id="41601686" class="c"><input type="checkbox" id="c-41601686" checked=""/><div class="controls bullet"><span class="by">sharemywin</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601206">parent</a><span>|</span><a href="#41601521">next</a><span>|</span><label class="collapse" for="c-41601686">[-]</label><label class="expand" for="c-41601686">[12 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t this apply to all prediction machines that make errors.<p>Humans make bad predictions all the time but we still seem to manage to do some cool stuff here and there.<p>part of an agents architecture will be for it to minimize e and then ground the prediction loop against a reality check.<p>making LLMs bigger gets you a lower e with scale of data and compute but you will still need it to check against reality. test time compute also will play a roll as it can run through multiple scenarios and &quot;search&quot; for an answer.</div><br/><div id="41603319" class="c"><input type="checkbox" id="c-41603319" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601686">parent</a><span>|</span><a href="#41602237">next</a><span>|</span><label class="collapse" for="c-41603319">[-]</label><label class="expand" for="c-41603319">[8 more]</label></div><br/><div class="children"><div class="content">The difference between LLMs and other kinds of predictive models, or humans, is that those kinds of systems do not produce their output one token at a time, but all in one go, so their error basically stays constant. LeCun&#x27;s argument is that LLM error increases with every cycle of appending a token to the last cycle&#x27;s output. That&#x27;s very specific to LLMs (or, well, to LLM-based chatbots to be more precise).<p>&gt;&gt; part of an agents architecture will be for it to minimize e and then ground the prediction loop against a reality check.<p>The problem is that web-scale LLMs can only realistically be trained to maximise the probability of the next token in a sequence, but not the factuality, correctness, truthfullness, etc of the entire sequence. That&#x27;s because web-scale data is not annotated with such properties. So they can&#x27;t do a &quot;reality check&quot; because they don&#x27;t know what &quot;reality&quot; is, only what text looks like.<p>The paper above uses an &quot;oracle&quot; instead, meaning they have a labelled dataset of correct answers. They can only train their RL approach because they have this source of truth. This kind of approach just doesn&#x27;t scale as well as predicting the next token. It&#x27;s really a supervised learning approach hiding behind RL.</div><br/><div id="41603716" class="c"><input type="checkbox" id="c-41603716" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41603319">parent</a><span>|</span><a href="#41602237">next</a><span>|</span><label class="collapse" for="c-41603716">[-]</label><label class="expand" for="c-41603716">[7 more]</label></div><br/><div class="children"><div class="content">&quot;The difference between LLMs and other kinds of predictive models, or humans, is that those kinds of systems do not produce their output one token at a time, but all in one go, so their error basically stays constant.&quot; -- This is a big, unproven assumption. Any non-autoregressive model can be trivially converted to an autoregressive model by: (i) generating a full output sequence, (ii) removing all tokens except the first one, (iii) generating a full-1 output sequence conditioned on the first token. This wraps the non-autoregressive model in an &quot;MPC loop&quot;, thereby converting it to an autoregressive model where per-token error is no greater than that of the wrapped non-AR model. The explicit MPC planning behavior might reduce error per token compared to current naive applications of AR transformers, but the MPC-wrappped model is still an AR model, so the problem is not AR per se.<p>LeCun&#x27;s argument has some decent points, eg, allocating compute per token based solely on location within the sequence (due to increasing cost of attention ops for later locations) is indeed silly. However, the points about AR being unavoidably flawed due to exponential divergence from the true manifold are wrong and lazy. They&#x27;re not wrong because AR models don&#x27;t diverge, they&#x27;re wrong because this sort of divergence is also present in other models.</div><br/><div id="41604246" class="c"><input type="checkbox" id="c-41604246" checked=""/><div class="controls bullet"><span class="by">pptr</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41603716">parent</a><span>|</span><a href="#41604263">next</a><span>|</span><label class="collapse" for="c-41604246">[-]</label><label class="expand" for="c-41604246">[3 more]</label></div><br/><div class="children"><div class="content">The loop itself is claimed to be the problem. It doesn&#x27;t matter whether you use an AR or non-AR model. They both have a certain error probability that gets amplified in each iteration.</div><br/><div id="41604378" class="c"><input type="checkbox" id="c-41604378" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41604246">parent</a><span>|</span><a href="#41606495">next</a><span>|</span><label class="collapse" for="c-41604378">[-]</label><label class="expand" for="c-41604378">[1 more]</label></div><br/><div class="children"><div class="content">The per token error of the non-AR model wrapped with MPC is no higher than the per token error of the non-AR model without MPC. Likelihood of the entire sequence being off the true data manifold is just one minus the product of the per token errors, whether or not you&#x27;re running with the MPC loop. Ie, wrapping the non-AR model in an MPC loop and thereby converting it to an AR model (with a built-in planning mechanism) doesn&#x27;t increase its probability of going off track.<p>Per token error compounding over sequence length happens whether or not the model&#x27;s autoregressive. The way in which per token errors correlate across a sequence might be more favorable wrt probability of producing bad sequences if you incorporate some explicit planning mechanism -- like the non-AR model wrapped in an MPC loop, but that&#x27;s a more subtle argument than LeCun makes.</div><br/></div></div><div id="41606495" class="c"><input type="checkbox" id="c-41606495" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41604246">parent</a><span>|</span><a href="#41604378">prev</a><span>|</span><a href="#41604263">next</a><span>|</span><label class="collapse" for="c-41606495">[-]</label><label class="expand" for="c-41606495">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Also &quot;other kinds of predictive models&quot; in my comment refers to models other than generative language models, e.g. image classifiers or regression models etc. Those don&#x27;t generate tokens, they output labels and the error of the labeling is constant (well, within error bounds). This was in response to OP&#x27;s comment about &quot;all prediction machines that make errors.&quot;</div><br/></div></div></div></div><div id="41604263" class="c"><input type="checkbox" id="c-41604263" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41603716">parent</a><span>|</span><a href="#41604246">prev</a><span>|</span><a href="#41602237">next</a><span>|</span><label class="collapse" for="c-41604263">[-]</label><label class="expand" for="c-41604263">[3 more]</label></div><br/><div class="children"><div class="content">Could the argument be rescued by some additional assumptions?<p>I agree with, and have previously also stated, the point you make there about “any non-auto-regressive model can be converted into an equivalent auto-regressive model by […]”, but, if one imposes additional restrictions on e.g. computation time, or something like that, I think that construction no longer works.<p>Well, of course there are <i>some</i> additional assumptions which would rescue the argument, so I guess my real question is whether there’s some combination of extra assumptions which both rescue the argument, and actually result in it being interesting.<p>If one makes the assumptions that there is a positive common lower bound on the probability of each token being incorrect assuming each previous token is correct, and that if any token is incorrect, then the whole generated text is incorrect, then of course the argument goes through, though the assumption doesn’t necessarily seem very likely.<p>Then, if we apply the construction, you mentioned to a text generation process with a low enough probability of error, then by the contrapositive, there cannot be an especially high common lower bound on the probability of error per token.<p>[“edit” prior to posting: I notice that at this point I started using symbols as if I was going to start doing actual algebraic manipulations, but did not actually do any algebraic manipulations which would justify the use of said symbols. I think what I wrote below would be clearer if I had just used words. Unfortunately I don’t want to take the time to rewrite it. I apologize for introducing formalisms without having a good reason to do so.]<p>If we have the assumption that there is a procedure with error rate &lt; epsilon(x) for generating an entire text response of length l(x), and which can be computed within time t(x), the construction gives an autoregressive method  which has error rate less than epsilon(x) for the entire text, and doesn’t have an error rate higher than epsilon’(x) for all of the tokens, and runs in time t’(x) per token (err… I guess it should actually vary between the tokens in the generated string… depends on details I guess), where epsilon’(x) and t’(x) can be computed based on epsilon(x) and t(x) and based on how the construction works,<p>and epsilon’(x) will be much smaller than epsilon(x), while  t’(x) l(x) &gt;&gt; t(x) (at least, assuming l(x) is somewhat large).<p>So, that particular construction does not preclude the possibility that there is no algorithm that works auto-regressively and which both has an error rate(for overall generated text) as low as [the error rate for some non-auto-regressive model that runs quickly enough], <i>and which runs quickly enough</i> .<p>If there are cryptographically secure families of hashing functions (in the sense of, asymptotically in the size of the hash length, while the hash can be computed in polynomial time, finding preimages or collisions cannot be done in polynomial time) it seems that there should probably be functions from strings to strings which can be computed in time bounded above by some polynomial, but which can’t be computed autoregressively in time bounded above by a polynomial of the same degree.<p>(So like, maybe it can be computed in time 5n^4 when not autoregressive, but needs at least 2n^5 time to do auto regressively)<p>(I’m imagining something like, “compute a string of the form ‘hash(y), y’ where y is the result of some computation done on the input which takes a polynomial amount of time to compute from the input. So, the easiest way to compute this would be to compute y and the compute hash(y). So, to do this auto-regressively, it would need to compute y again for each token in the hash.)<p>Of course, a single factor of n might not be that compelling, and appealing to strong hashing functions is probably trying to kill a fly with a sledgehammer(probably there are arguments that work as well without assuming this), but it’s what came to mind.<p>Perhaps one could do something  like this to show that for some problems, any auto-regressive solution that has certain runtime bounds, will have some positive lower bound on the error rate per token?</div><br/><div id="41604681" class="c"><input type="checkbox" id="c-41604681" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41604263">parent</a><span>|</span><a href="#41602237">next</a><span>|</span><label class="collapse" for="c-41604681">[-]</label><label class="expand" for="c-41604681">[2 more]</label></div><br/><div class="children"><div class="content">I think it would be hard to make a solid argument that AR or non-AR is strictly better wrt full sequence error rates, whether or not we place constraints on compute, memory, etc. I&#x27;d guess that there&#x27;s some intrinsic form of complexity inherent to any particular distribution of sequences which requires spending at least some amount of compute to achieve sequence generation error less than some epsilon. I&#x27;d also guess that AR and non-AR models could both achieve this bound in principle, though maybe it&#x27;s practically harder with one or the other. It would be interesting to formally characterize this sort of complexity, but that&#x27;s above my analytical pay grade.<p>The hash function example is interesting. I think the model could compute y prior to outputting any tokens and then output the `hash(y), y&#x27; sequence deterministically. In architectures like transformers, all the compute in earlier steps can be reused in later steps via attention, so it wouldn&#x27;t be necessary to recompute y at each step as long as the model commits to a given y up front before starting to generate hash(y).</div><br/><div id="41605771" class="c"><input type="checkbox" id="c-41605771" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41604681">parent</a><span>|</span><a href="#41602237">next</a><span>|</span><label class="collapse" for="c-41605771">[-]</label><label class="expand" for="c-41605771">[1 more]</label></div><br/><div class="children"><div class="content">Ah, yeah, I guess that probably is true of transformers in practice. I was thinking about something which strictly takes in a sequence of tokens and outputs a (possibly 1-hot) probability distribution over all possible next tokens. Such a thing running autoregressively would have to recompute y each time. But, if intermediate computations are cached, as with transformers in practice, then this isn’t necessary.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41602237" class="c"><input type="checkbox" id="c-41602237" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601686">parent</a><span>|</span><a href="#41603319">prev</a><span>|</span><a href="#41604367">next</a><span>|</span><label class="collapse" for="c-41602237">[-]</label><label class="expand" for="c-41602237">[2 more]</label></div><br/><div class="children"><div class="content">No.  Many prediction machines can give you a confidence value on the full outcome.  By the nature of tokenization and the casual inference (you build a token one at a time, and they&#x27;re not really semantically connected except in the kv cache lookups, which are generally hidden to the user), the confidence values are thrown out in practice and even a weak confidence value would be hard to retrieve.<p>I don&#x27;t think it&#x27;s impossible to obtain content with confidence assessments with the transformer architecture but maybe not in the way it&#x27;s done now (like maybe another mayer on top).</div><br/></div></div><div id="41604367" class="c"><input type="checkbox" id="c-41604367" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601686">parent</a><span>|</span><a href="#41602237">prev</a><span>|</span><a href="#41601521">next</a><span>|</span><label class="collapse" for="c-41604367">[-]</label><label class="expand" for="c-41604367">[1 more]</label></div><br/><div class="children"><div class="content">Humans self-correct (they can push the delete button)</div><br/></div></div></div></div><div id="41601521" class="c"><input type="checkbox" id="c-41601521" checked=""/><div class="controls bullet"><span class="by">roboboffin</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601206">parent</a><span>|</span><a href="#41601686">prev</a><span>|</span><a href="#41601360">next</a><span>|</span><label class="collapse" for="c-41601521">[-]</label><label class="expand" for="c-41601521">[7 more]</label></div><br/><div class="children"><div class="content">Is this similar to the effect that I have seen when you have two different LLMs talking to each other, they tend to descend into nonsense ? A single error in one of the LLM&#x27;s output and that then pushes the other LLM out of distribution.<p>I kind of oscillatory effect when the train of tokens move further and further out of the distribution of correct tokens.</div><br/><div id="41602557" class="c"><input type="checkbox" id="c-41602557" checked=""/><div class="controls bullet"><span class="by">vjerancrnjak</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601521">parent</a><span>|</span><a href="#41601595">next</a><span>|</span><label class="collapse" for="c-41602557">[-]</label><label class="expand" for="c-41602557">[1 more]</label></div><br/><div class="children"><div class="content">This is equivalent to the problem of maximum entropy Markov models and their application to sequence output.<p>After some point you’re conditioning your next decision on tokens that are severely out of the learned path and you don’t even see it’s that bad.<p>Usually this was fixed with cost sensitive learning or increased sampling of weird distributions during learning and then making the model learn to correct the mistake.<p>Another approach was to have an inference algorithm that maximize the output probability, but these algorithms are expensive (viterbi and other dynamic programming methods).<p>Feature modeling in NNs somewhat allowed us to ignore these issues and get good performance but they will show up again.</div><br/></div></div><div id="41601595" class="c"><input type="checkbox" id="c-41601595" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601521">parent</a><span>|</span><a href="#41602557">prev</a><span>|</span><a href="#41601698">next</a><span>|</span><label class="collapse" for="c-41601595">[-]</label><label class="expand" for="c-41601595">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Is this similar to the effect that I have seen when you have two different LLMs talking to each other, they tend to descend into nonsense ?<p>Is that really true? I&#x27;d expect that with high temperature values, but otherwise I don&#x27;t see why this would happen, and I&#x27;ve experimented with pitting same models against each other and also different models against different models, but haven&#x27;t come across that particular problem.</div><br/><div id="41603462" class="c"><input type="checkbox" id="c-41603462" checked=""/><div class="controls bullet"><span class="by">roboboffin</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601595">parent</a><span>|</span><a href="#41602434">next</a><span>|</span><label class="collapse" for="c-41603462">[-]</label><label class="expand" for="c-41603462">[1 more]</label></div><br/><div class="children"><div class="content">I think this is similar to this point: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41601738">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41601738</a><p>That the chain-of-thought diverges from accepted truth as an incorrect token pushes it into a line of thinking that is not true. The use of RL is there to train the LLM to implement strategies to bring it back from this. In effect, two LLMs would be the same and would slow diverge into nonsense. Maybe it is something that is not so much of a problem anymore.<p>Yann LeCun talks about how the correct way to fix this is to use an internal consistent model of the truth; then the chain-of-thought exists as a loop within that consistent model meaning it cannot diverge. The language is a decoded output of this internal model resolution. He speaks about this here: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=N09C6oUQX5M" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=N09C6oUQX5M</a><p>Anyway, that&#x27;s my understanding. I&#x27;m no expert.</div><br/></div></div><div id="41602434" class="c"><input type="checkbox" id="c-41602434" checked=""/><div class="controls bullet"><span class="by">reportgunner</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601595">parent</a><span>|</span><a href="#41603462">prev</a><span>|</span><a href="#41601698">next</a><span>|</span><label class="collapse" for="c-41602434">[-]</label><label class="expand" for="c-41602434">[2 more]</label></div><br/><div class="children"><div class="content">Can you show examples ? In any AI related discussions there are only some claims by people and never examples of the AI working well.</div><br/><div id="41602713" class="c"><input type="checkbox" id="c-41602713" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41602434">parent</a><span>|</span><a href="#41601698">next</a><span>|</span><label class="collapse" for="c-41602713">[-]</label><label class="expand" for="c-41602713">[1 more]</label></div><br/><div class="children"><div class="content">you’re saying you have never seen an example of AI working well?</div><br/></div></div></div></div></div></div><div id="41601698" class="c"><input type="checkbox" id="c-41601698" checked=""/><div class="controls bullet"><span class="by">sharemywin</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601521">parent</a><span>|</span><a href="#41601595">prev</a><span>|</span><a href="#41601360">next</a><span>|</span><label class="collapse" for="c-41601698">[-]</label><label class="expand" for="c-41601698">[1 more]</label></div><br/><div class="children"><div class="content">this is like the human game of telephone.</div><br/></div></div></div></div><div id="41601360" class="c"><input type="checkbox" id="c-41601360" checked=""/><div class="controls bullet"><span class="by">atq2119</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601206">parent</a><span>|</span><a href="#41601521">prev</a><span>|</span><a href="#41601676">next</a><span>|</span><label class="collapse" for="c-41601360">[-]</label><label class="expand" for="c-41601360">[2 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t that argument make the fundamentally incorrect assumption that the space of produced output sequence has pockets where all output sequence with a certain prefix are incorrect?<p>Design your output space in such way that every prefix has a correct completion and this simplistic argument no longer applies. Humans do this in practice by saying &quot;hold on, I was wrong, here&#x27;s what&#x27;s right&quot;.<p>Of course, there&#x27;s still a question of whether you can get the probability mass of correct outputs large enough.</div><br/><div id="41602018" class="c"><input type="checkbox" id="c-41602018" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601360">parent</a><span>|</span><a href="#41601676">next</a><span>|</span><label class="collapse" for="c-41602018">[-]</label><label class="expand" for="c-41602018">[1 more]</label></div><br/><div class="children"><div class="content">How do you do this in something where the only memory is the last few things it said or heard?</div><br/></div></div></div></div><div id="41601676" class="c"><input type="checkbox" id="c-41601676" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601206">parent</a><span>|</span><a href="#41601360">prev</a><span>|</span><a href="#41604379">next</a><span>|</span><label class="collapse" for="c-41601676">[-]</label><label class="expand" for="c-41601676">[2 more]</label></div><br/><div class="children"><div class="content">Doesn’t this assume that the probability of a correct answer is iid? It can’t be that simple.</div><br/><div id="41601985" class="c"><input type="checkbox" id="c-41601985" checked=""/><div class="controls bullet"><span class="by">vbarrielle</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601676">parent</a><span>|</span><a href="#41604379">next</a><span>|</span><label class="collapse" for="c-41601985">[-]</label><label class="expand" for="c-41601985">[1 more]</label></div><br/><div class="children"><div class="content">Yes the main flaw of this reasoning is supposing that e does not depend on previous output. I think this was a good approximation to characterize vanilla LLMs, but the kind of RL in this paper is done with the explicit goal of making e depending on prior output (and specifically to lower it given a long enough chain of thought).</div><br/></div></div></div></div><div id="41604379" class="c"><input type="checkbox" id="c-41604379" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601206">parent</a><span>|</span><a href="#41601676">prev</a><span>|</span><a href="#41602152">next</a><span>|</span><label class="collapse" for="c-41604379">[-]</label><label class="expand" for="c-41604379">[1 more]</label></div><br/><div class="children"><div class="content">Simplistic, since it assumes probabilities are uncorrelated, when they clearly aren&#x27;t. Also, there are many ways of writing the correct solution to a problem (you do not need to replicated an exact sequence of tokens).</div><br/></div></div><div id="41602152" class="c"><input type="checkbox" id="c-41602152" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601206">parent</a><span>|</span><a href="#41604379">prev</a><span>|</span><a href="#41601723">next</a><span>|</span><label class="collapse" for="c-41602152">[-]</label><label class="expand" for="c-41602152">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s quite fitting that the topic of this thread is self-correction. Self-correction is a trivial existence proof that refutes what LeCun is saying, because all the LLM has to say is &quot;I made a mistake, let me start again&quot;.</div><br/></div></div><div id="41601723" class="c"><input type="checkbox" id="c-41601723" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601206">parent</a><span>|</span><a href="#41602152">prev</a><span>|</span><a href="#41601119">next</a><span>|</span><label class="collapse" for="c-41601723">[-]</label><label class="expand" for="c-41601723">[4 more]</label></div><br/><div class="children"><div class="content">&gt; * P(correct) = (1-e)^n
  * This diverges exponentially<p>I don&#x27;t get it, <i>1-e</i> is between 0 and 1, so <i>(1-e)^n</i> converge to zero. Also, a probability cannot <i>diverge</i> since it&#x27;s bounded by 1!<p>I think the argument is that <i>1 - e^n</i> converges to 1, which is what the law is about.</div><br/><div id="41601956" class="c"><input type="checkbox" id="c-41601956" checked=""/><div class="controls bullet"><span class="by">vbarrielle</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601723">parent</a><span>|</span><a href="#41601119">next</a><span>|</span><label class="collapse" for="c-41601956">[-]</label><label class="expand" for="c-41601956">[3 more]</label></div><br/><div class="children"><div class="content">P(correct) converges to zero, so you get almost certainly incorrect, at an exponential rate. The original choice of terms is not the most rigorous, but the reasoning is sound (under the assumption that e is a constant).</div><br/><div id="41602728" class="c"><input type="checkbox" id="c-41602728" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601956">parent</a><span>|</span><a href="#41607971">next</a><span>|</span><label class="collapse" for="c-41602728">[-]</label><label class="expand" for="c-41602728">[1 more]</label></div><br/><div class="children"><div class="content">P(correct) doesn&#x27;t go down with token count if you have self-correction. It can actually go up with token count.</div><br/></div></div><div id="41607971" class="c"><input type="checkbox" id="c-41607971" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601956">parent</a><span>|</span><a href="#41602728">prev</a><span>|</span><a href="#41601119">next</a><span>|</span><label class="collapse" for="c-41607971">[-]</label><label class="expand" for="c-41607971">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes I didn&#x27;t pay attention that it was the probability of being <i>correct</i> I misread  it as the probability of being <i>incorrect</i> since the claim was that it diverged.</div><br/></div></div></div></div></div></div></div></div><div id="41601119" class="c"><input type="checkbox" id="c-41601119" checked=""/><div class="controls bullet"><span class="by">vjerancrnjak</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601085">parent</a><span>|</span><a href="#41601206">prev</a><span>|</span><a href="#41602704">next</a><span>|</span><label class="collapse" for="c-41601119">[-]</label><label class="expand" for="c-41601119">[1 more]</label></div><br/><div class="children"><div class="content">“Label bias” or “observation bias” a phenomenon where going outside of the learned path lives little room for error correction. Lecun talks about the lack of joint learning in LLMs.</div><br/></div></div><div id="41602704" class="c"><input type="checkbox" id="c-41602704" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601085">parent</a><span>|</span><a href="#41601119">prev</a><span>|</span><a href="#41601134">next</a><span>|</span><label class="collapse" for="c-41602704">[-]</label><label class="expand" for="c-41602704">[1 more]</label></div><br/><div class="children"><div class="content">It’s a thing in that he said it but it’s not an actual law and it has several obvious logical flaws. It applies just as equally to human utterances.</div><br/></div></div><div id="41601134" class="c"><input type="checkbox" id="c-41601134" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#41600937">root</a><span>|</span><a href="#41601085">parent</a><span>|</span><a href="#41602704">prev</a><span>|</span><a href="#41602074">next</a><span>|</span><label class="collapse" for="c-41601134">[-]</label><label class="expand" for="c-41601134">[1 more]</label></div><br/><div class="children"><div class="content">A reference could be this:<p><a href="https:&#x2F;&#x2F;futurist.com&#x2F;2023&#x2F;02&#x2F;13&#x2F;metas-yann-lecun-thoughts-large-language-models-llms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;futurist.com&#x2F;2023&#x2F;02&#x2F;13&#x2F;metas-yann-lecun-thoughts-la...</a><p>(Speaking of &quot;law&quot; is rhetoric, but an idea is pretty clear.)</div><br/></div></div></div></div><div id="41602074" class="c"><input type="checkbox" id="c-41602074" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#41600937">parent</a><span>|</span><a href="#41601085">prev</a><span>|</span><a href="#41601199">next</a><span>|</span><label class="collapse" for="c-41602074">[-]</label><label class="expand" for="c-41602074">[1 more]</label></div><br/><div class="children"><div class="content">&quot;never&quot; is not itself a problem, people do the same<p>you only need to solve fusion correctly once</div><br/></div></div><div id="41601199" class="c"><input type="checkbox" id="c-41601199" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41600937">parent</a><span>|</span><a href="#41602074">prev</a><span>|</span><a href="#41602412">next</a><span>|</span><label class="collapse" for="c-41601199">[-]</label><label class="expand" for="c-41601199">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re talking about label bias then you don&#x27;t need to solve label bias to &#x27;solve&#x27; hallucinations when the model has already learnt internally when it&#x27;s bullshitting or going off the rails.</div><br/></div></div></div></div><div id="41602412" class="c"><input type="checkbox" id="c-41602412" checked=""/><div class="controls bullet"><span class="by">sensanaty</span><span>|</span><a href="#41600937">prev</a><span>|</span><a href="#41603191">next</a><span>|</span><label class="collapse" for="c-41602412">[-]</label><label class="expand" for="c-41602412">[18 more]</label></div><br/><div class="children"><div class="content">I hate that the AI pundits have succeeded in popularizing the notion of &quot;hallucination&quot;, anthropomorphizing these balls of statistics into something that seems like it&#x27;s actually in some sort of deep thought process akin to a person&#x27;s mind.<p>No, it&#x27;s not &quot;hallucinating&quot;. It&#x27;s not lying, or making things up, or anything like that either. It&#x27;s spitting out data according to what triggers the underlying weights. If this were a regular JSON API endpoint, you wouldn&#x27;t say the API is hallucinating, you&#x27;d say &quot;This API is shit&quot; because it&#x27;s <i>broken</i>.</div><br/><div id="41602541" class="c"><input type="checkbox" id="c-41602541" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41604326">next</a><span>|</span><label class="collapse" for="c-41602541">[-]</label><label class="expand" for="c-41602541">[7 more]</label></div><br/><div class="children"><div class="content">Do we really need to have this discussion in <i>every</i> thread about LLMs?</div><br/><div id="41602577" class="c"><input type="checkbox" id="c-41602577" checked=""/><div class="controls bullet"><span class="by">sensanaty</span><span>|</span><a href="#41602412">root</a><span>|</span><a href="#41602541">parent</a><span>|</span><a href="#41604326">next</a><span>|</span><label class="collapse" for="c-41602577">[-]</label><label class="expand" for="c-41602577">[6 more]</label></div><br/><div class="children"><div class="content">As long as AI-bros are pushing for making AI models seem like more than they are to pad their wallets, there&#x27;ll be someone like me pointing out that, no, it&#x27;s not &quot;hallucinating&quot;, it&#x27;s spitting bad data.</div><br/><div id="41602788" class="c"><input type="checkbox" id="c-41602788" checked=""/><div class="controls bullet"><span class="by">bithive123</span><span>|</span><a href="#41602412">root</a><span>|</span><a href="#41602577">parent</a><span>|</span><a href="#41602690">next</a><span>|</span><label class="collapse" for="c-41602788">[-]</label><label class="expand" for="c-41602788">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re being pedantic. Your statement that &quot;it&#x27;s spitting bad data&quot; is incorrect too, as it implies agency.  Actually, nothing is happening but electrons flowing.  The notion of an &quot;it&quot; that &quot;spits&quot; &quot;data&quot; which is &quot;bad&quot; is your own conceptual overlay.</div><br/><div id="41603296" class="c"><input type="checkbox" id="c-41603296" checked=""/><div class="controls bullet"><span class="by">bumby</span><span>|</span><a href="#41602412">root</a><span>|</span><a href="#41602788">parent</a><span>|</span><a href="#41602690">next</a><span>|</span><label class="collapse" for="c-41603296">[-]</label><label class="expand" for="c-41603296">[1 more]</label></div><br/><div class="children"><div class="content">Tbf, if you assume humans have agency, there’s plenty of people who would claim you’re making the same mistake because the reductionist view is that people are just either deterministic chemical soup (or maybe with a bit of randomness baked in).</div><br/></div></div></div></div><div id="41602690" class="c"><input type="checkbox" id="c-41602690" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41602412">root</a><span>|</span><a href="#41602577">parent</a><span>|</span><a href="#41602788">prev</a><span>|</span><a href="#41604326">next</a><span>|</span><label class="collapse" for="c-41602690">[-]</label><label class="expand" for="c-41602690">[3 more]</label></div><br/><div class="children"><div class="content">I know lots of people working on AI. they are among the least bro-y group of people I have ever met.<p>There is simply nothing similar to actual bro-y finance culture among AI research engineers. It is entirely a figment of the media and backreaction that we currently have to portray everyone we don’t like as a “bro” - truth be damned.</div><br/><div id="41602979" class="c"><input type="checkbox" id="c-41602979" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#41602412">root</a><span>|</span><a href="#41602690">parent</a><span>|</span><a href="#41604326">next</a><span>|</span><label class="collapse" for="c-41602979">[-]</label><label class="expand" for="c-41602979">[2 more]</label></div><br/><div class="children"><div class="content">no - the cliques are different but linked at the hip. Add international finance, too.. India, China and others.</div><br/><div id="41603053" class="c"><input type="checkbox" id="c-41603053" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41602412">root</a><span>|</span><a href="#41602979">parent</a><span>|</span><a href="#41604326">next</a><span>|</span><label class="collapse" for="c-41603053">[-]</label><label class="expand" for="c-41603053">[1 more]</label></div><br/><div class="children"><div class="content">whatever your information diet is, i recommend you change it</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41604326" class="c"><input type="checkbox" id="c-41604326" checked=""/><div class="controls bullet"><span class="by">qudat</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41602541">prev</a><span>|</span><a href="#41603864">next</a><span>|</span><label class="collapse" for="c-41604326">[-]</label><label class="expand" for="c-41604326">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I hate that the AI pundits have succeeded in popularizing the notion of &quot;hallucination&quot;, anthropomorphizing these balls of statistics into something that seems like it&#x27;s actually in some sort of deep thought process akin to a person&#x27;s mind.<p>I&#x27;d argue the opposite: people think a person&#x27;s mind is in &quot;deep thought&quot; when it&#x27;s actually just a ball of statistics.</div><br/><div id="41604808" class="c"><input type="checkbox" id="c-41604808" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#41602412">root</a><span>|</span><a href="#41604326">parent</a><span>|</span><a href="#41603864">next</a><span>|</span><label class="collapse" for="c-41604808">[-]</label><label class="expand" for="c-41604808">[2 more]</label></div><br/><div class="children"><div class="content">Do you think that an LLM would spit out Latin and English if you trained it with homo sapiens mumbling?<p>Yet, humans managed to do that (albeit over many generations)<p>Ergo, humans are not just balls of statistics</div><br/><div id="41605391" class="c"><input type="checkbox" id="c-41605391" checked=""/><div class="controls bullet"><span class="by">fleshmonad</span><span>|</span><a href="#41602412">root</a><span>|</span><a href="#41604808">parent</a><span>|</span><a href="#41603864">next</a><span>|</span><label class="collapse" for="c-41605391">[-]</label><label class="expand" for="c-41605391">[1 more]</label></div><br/><div class="children"><div class="content">Not intended to be snarky, but what would you consider them? Is it akin to a function in the mathematical sense, that takes (sensory) input and creates output based on that? If so, how does this function work, if not by statistics? I am genuinely interested in your point of view. Also: Don&#x27;t you think humans can be somewhat compared to a &quot;pretrained model&quot;, as in human genetics gives the brain a head start, so that it can start speaking latin from what you deam &quot;homo sapiens mumbling?</div><br/></div></div></div></div></div></div><div id="41603864" class="c"><input type="checkbox" id="c-41603864" checked=""/><div class="controls bullet"><span class="by">numeri</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41604326">prev</a><span>|</span><a href="#41604993">next</a><span>|</span><label class="collapse" for="c-41603864">[-]</label><label class="expand" for="c-41603864">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve got bad news for you – that term was used in deep learning research well before LLMs came on the scene. It has nothing to do with pundits trying to popularize anything or trying to justify LLMs&#x27; shortcomings, it was just a label researchers gave to a phenomenon they were trying to study.<p>A couple papers that use it in this way prior to LLMs:<p>- 2021: The Curious Case of Hallucinations in Neural Machine Translation (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2104.06683" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2104.06683</a>)<p>- 2019: Identifying Fluently Inadequate Output in Neural and Statistical Machine Translation (<a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;W19-6623&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;W19-6623&#x2F;</a>)</div><br/></div></div><div id="41604993" class="c"><input type="checkbox" id="c-41604993" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41603864">prev</a><span>|</span><a href="#41602680">next</a><span>|</span><label class="collapse" for="c-41604993">[-]</label><label class="expand" for="c-41604993">[1 more]</label></div><br/><div class="children"><div class="content">The right word is &quot;confabulation&quot;. Which is when we fill in missing information but may not be aware that we are doing it.<p>We all confabulate to some degree, as any neural system must, since no training data is stored perfectly.<p>Human &quot;hallucinations&quot; in contrast, are a particular kind of breakdown in our sensory feedback loops. Which is not a process LLMs even have.<p>Hallucinations occur when our internal sensory feedback loops overpower actual sensory input, resulting in a stream of false sensory experience&#x2F;signals being generated and processed. The false running experience might still incorporate some actual sensory information or not.<p>When we dream, we are hallucinating - our sensory experience loop running free of our actual senses - to a productive purpose.<p>The reason our senses have feedback is so that we can use our interpretation of sensory input as cues to make interpreting the next moments input easier. But its important that our running interpretation can reset when new input significantly diverges from our expectations so it can quickly reorient.<p>(Not only is it important to revert to a raw input interpretation to ensure our running interpretation keeps up the actual context changes and corrects misinterpretations, but such resets signal that something novel or unexpected has happened, so likely trigger learning.)<p>So &quot;hallucinations&quot; was an unfortunate and misleading choice of terminology.</div><br/></div></div><div id="41602680" class="c"><input type="checkbox" id="c-41602680" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41604993">prev</a><span>|</span><a href="#41603197">next</a><span>|</span><label class="collapse" for="c-41602680">[-]</label><label class="expand" for="c-41602680">[1 more]</label></div><br/><div class="children"><div class="content">can we make a siloed version of HN for your political faction? it’s tiresome reading these in every thread</div><br/></div></div><div id="41603197" class="c"><input type="checkbox" id="c-41603197" checked=""/><div class="controls bullet"><span class="by">hmmmhmmmhmmm</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41602680">prev</a><span>|</span><a href="#41602537">next</a><span>|</span><label class="collapse" for="c-41603197">[-]</label><label class="expand" for="c-41603197">[1 more]</label></div><br/><div class="children"><div class="content">Maybe an evolutionary &#x2F; structuralist lens is helpful here: terms that rapidly diffuse through discourse are those that people like most, and most people like to anthropomorphize, so &quot;hallucination&quot; has come to take on a new meaning, and we all (to different degrees) know what it is referring to.</div><br/></div></div><div id="41602537" class="c"><input type="checkbox" id="c-41602537" checked=""/><div class="controls bullet"><span class="by">frakt0x90</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41603197">prev</a><span>|</span><a href="#41602610">next</a><span>|</span><label class="collapse" for="c-41602537">[-]</label><label class="expand" for="c-41602537">[1 more]</label></div><br/><div class="children"><div class="content">Yeah it&#x27;s simply model error. All models from Linear Regression to LLMs have error. I guess because this type of error is in the form of deceptively reasonable human language, it gets a different moniker. It&#x27;s also notably harder to quantify so it might warrant a different name.</div><br/></div></div><div id="41602610" class="c"><input type="checkbox" id="c-41602610" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41602537">prev</a><span>|</span><a href="#41603163">next</a><span>|</span><label class="collapse" for="c-41602610">[-]</label><label class="expand" for="c-41602610">[1 more]</label></div><br/><div class="children"><div class="content">do you really want to have a discussion about &#x27;thought&#x27; and &#x27;mind&#x27;? i don&#x27;t</div><br/></div></div><div id="41603163" class="c"><input type="checkbox" id="c-41603163" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#41602412">parent</a><span>|</span><a href="#41602610">prev</a><span>|</span><a href="#41603191">next</a><span>|</span><label class="collapse" for="c-41603163">[-]</label><label class="expand" for="c-41603163">[1 more]</label></div><br/><div class="children"><div class="content">Give it a rest. Everything is statistics.<p>Sees space shuttle &quot;pff, it&#x27;s just a pile of engineering.&quot;</div><br/></div></div></div></div><div id="41603191" class="c"><input type="checkbox" id="c-41603191" checked=""/><div class="controls bullet"><span class="by">textlapse</span><span>|</span><a href="#41602412">prev</a><span>|</span><label class="collapse" for="c-41603191">[-]</label><label class="expand" for="c-41603191">[2 more]</label></div><br/><div class="children"><div class="content">Using an intelligent algorithm to guide a dumb non-intelligent next word predictor is still a non-intelligent algorithm at the end of the day.<p>Sure it’s sorting through garbage more elegantly but it’s still garbage at the end of the day.<p>I was hoping the RL-like approach replaced the transformers-like approach or something but that’s a pipe dream.</div><br/><div id="41603801" class="c"><input type="checkbox" id="c-41603801" checked=""/><div class="controls bullet"><span class="by">devoutsalsa</span><span>|</span><a href="#41603191">parent</a><span>|</span><label class="collapse" for="c-41603801">[-]</label><label class="expand" for="c-41603801">[1 more]</label></div><br/><div class="children"><div class="content">PolishedTurd.ai</div><br/></div></div></div></div></div></div></div></div></div></body></html>