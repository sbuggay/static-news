<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711962074806" as="style"/><link rel="stylesheet" href="styles.css?v=1711962074806"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2403.17297">InternLM2</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>milliondreams</span> | <span>16 comments</span></div><br/><div><div id="39890792" class="c"><input type="checkbox" id="c-39890792" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#39890595">next</a><span>|</span><label class="collapse" for="c-39890792">[-]</label><label class="expand" for="c-39890792">[1 more]</label></div><br/><div class="children"><div class="content">We really need better long context benchmarks than needle-in-a-haystack. There is LV-Eval (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.05136" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.05136</a>) with multi-hop QA that&#x27;s better but still pretty basic.</div><br/></div></div><div id="39890595" class="c"><input type="checkbox" id="c-39890595" checked=""/><div class="controls bullet"><span class="by">esha_manideep</span><span>|</span><a href="#39890792">prev</a><span>|</span><a href="#39889405">next</a><span>|</span><label class="collapse" for="c-39890595">[-]</label><label class="expand" for="c-39890595">[3 more]</label></div><br/><div class="children"><div class="content">Pretty amazing to see training data being discussed more openly</div><br/><div id="39891347" class="c"><input type="checkbox" id="c-39891347" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#39890595">parent</a><span>|</span><a href="#39889405">next</a><span>|</span><label class="collapse" for="c-39891347">[-]</label><label class="expand" for="c-39891347">[2 more]</label></div><br/><div class="children"><div class="content">Indeed. I think part of the reason when they are not discussed openly may be that much of the data used is copyrighted, which introduces some legal ambiguities.</div><br/><div id="39891935" class="c"><input type="checkbox" id="c-39891935" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#39890595">root</a><span>|</span><a href="#39891347">parent</a><span>|</span><a href="#39889405">next</a><span>|</span><label class="collapse" for="c-39891935">[-]</label><label class="expand" for="c-39891935">[1 more]</label></div><br/><div class="children"><div class="content">IANAL but hiding something doesn&#x27;t make someone legally immune. Any company could sue LLM companies and they can&#x27;t hide it during the case. e.g. there is already a similar case on OpenAI.</div><br/></div></div></div></div></div></div><div id="39889405" class="c"><input type="checkbox" id="c-39889405" checked=""/><div class="controls bullet"><span class="by">milliondreams</span><span>|</span><a href="#39890595">prev</a><span>|</span><a href="#39890635">next</a><span>|</span><label class="collapse" for="c-39889405">[-]</label><label class="expand" for="c-39889405">[2 more]</label></div><br/><div class="children"><div class="content">TLDR;
1. InternLM2 is an open-source Large Language Model that has shown improvements over previous models, particularly in long-context modeling.
2. The model uses a unique approach, combining traditional training with Supervised Fine-Tuning and Conditional Online Reinforcement Learning from Human Feedback.
3. It offers a variety of model sizes and training stages to the community, demonstrating significant advancements in AI research and application.</div><br/><div id="39890314" class="c"><input type="checkbox" id="c-39890314" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#39889405">parent</a><span>|</span><a href="#39890635">next</a><span>|</span><label class="collapse" for="c-39890314">[-]</label><label class="expand" for="c-39890314">[1 more]</label></div><br/><div class="children"><div class="content">Excited to see how it will perform on the lmsys leaderboard</div><br/></div></div></div></div><div id="39890370" class="c"><input type="checkbox" id="c-39890370" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#39890635">prev</a><span>|</span><a href="#39890970">next</a><span>|</span><label class="collapse" for="c-39890370">[-]</label><label class="expand" for="c-39890370">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone know how the free commercial license works? Do they usually grant it? <a href="https:&#x2F;&#x2F;wj.qq.com&#x2F;s2&#x2F;12727483&#x2F;5dba&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wj.qq.com&#x2F;s2&#x2F;12727483&#x2F;5dba&#x2F;</a> looks like a form there.<p>Apache 2 code, free commercial license with application form for weights.</div><br/></div></div><div id="39890970" class="c"><input type="checkbox" id="c-39890970" checked=""/><div class="controls bullet"><span class="by">barsonme</span><span>|</span><a href="#39890370">prev</a><span>|</span><a href="#39890691">next</a><span>|</span><label class="collapse" for="c-39890970">[-]</label><label class="expand" for="c-39890970">[4 more]</label></div><br/><div class="children"><div class="content">Is it normal for papers to have that many authors?</div><br/><div id="39891403" class="c"><input type="checkbox" id="c-39891403" checked=""/><div class="controls bullet"><span class="by">barkingcat</span><span>|</span><a href="#39890970">parent</a><span>|</span><a href="#39891958">next</a><span>|</span><label class="collapse" for="c-39891403">[-]</label><label class="expand" for="c-39891403">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not abnormal in many fields. A lot of biology or physics papers have more than that number.<p>In academic labs, so you&#x27;re a postgrad working the overnight shift watching some petri dish to make sure the bacteria doesn&#x27;t die, etc. It&#x27;s super boring grunt work but you do it so you get on the paper&#x27;s author list.</div><br/></div></div><div id="39891958" class="c"><input type="checkbox" id="c-39891958" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#39890970">parent</a><span>|</span><a href="#39891403">prev</a><span>|</span><a href="#39891021">next</a><span>|</span><label class="collapse" for="c-39891958">[-]</label><label class="expand" for="c-39891958">[1 more]</label></div><br/><div class="children"><div class="content">Particle physics papers usually have more pages for the names than for the work.</div><br/></div></div><div id="39891021" class="c"><input type="checkbox" id="c-39891021" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39890970">parent</a><span>|</span><a href="#39891958">prev</a><span>|</span><a href="#39890691">next</a><span>|</span><label class="collapse" for="c-39891021">[-]</label><label class="expand" for="c-39891021">[1 more]</label></div><br/><div class="children"><div class="content">Only in fields like foundation models and high-energy physics, where immense resources are required. Look at GPT-4&#x27;s credits: <a href="https:&#x2F;&#x2F;openai.com&#x2F;contributions&#x2F;gpt-4" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;contributions&#x2F;gpt-4</a></div><br/></div></div></div></div><div id="39890691" class="c"><input type="checkbox" id="c-39890691" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39890970">prev</a><span>|</span><a href="#39890690">next</a><span>|</span><label class="collapse" for="c-39890691">[-]</label><label class="expand" for="c-39890691">[1 more]</label></div><br/><div class="children"><div class="content">The repo is here: <a href="https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;InternLM">https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;InternLM</a></div><br/></div></div><div id="39890690" class="c"><input type="checkbox" id="c-39890690" checked=""/><div class="controls bullet"><span class="by">dannyw</span><span>|</span><a href="#39890691">prev</a><span>|</span><label class="collapse" for="c-39890690">[-]</label><label class="expand" for="c-39890690">[2 more]</label></div><br/><div class="children"><div class="content">How good is the base (non-instruction-tuned) model? Everyone is trying to make chat bots, but for my use cases, I find base models more suitable.</div><br/><div id="39891173" class="c"><input type="checkbox" id="c-39891173" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890690">parent</a><span>|</span><label class="collapse" for="c-39891173">[-]</label><label class="expand" for="c-39891173">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. What are some of those use cases?</div><br/></div></div></div></div></div></div></div></div></div></body></html>