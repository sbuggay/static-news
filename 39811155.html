<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711357261539" as="style"/><link rel="stylesheet" href="styles.css?v=1711357261539"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.quantamagazine.org/how-quickly-do-large-language-models-learn-unexpected-skills-20240213/">“Emergent” abilities in LLMs actually develop gradually and predictably – study</a> <span class="domain">(<a href="https://www.quantamagazine.org">www.quantamagazine.org</a>)</span></div><div class="subtext"><span>Anon84</span> | <span>64 comments</span></div><br/><div><div id="39813995" class="c"><input type="checkbox" id="c-39813995" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39812212">next</a><span>|</span><label class="collapse" for="c-39813995">[-]</label><label class="expand" for="c-39813995">[1 more]</label></div><br/><div class="children"><div class="content">There are several issues with the study:<p>1. Replacing pass&#x2F;fail accuracy with smoother alternatives (e.g token edit distance) could be a terrible proxy for skill, depending on the task.<p>2. Even by the authors&#x27; metrics, they _still_ find a few potentially emergent abilities.<p>3. Hindsight is 20-20. Yes, we can revisit the data and fiddle until we find transforms that erase emergence from aptitude plots. The fact is, folk used commonplace test accuracy measurements, and the results were unpredictable and surprising. That&#x27;s the true notable phenomenon.<p>I think there&#x27;s value in the paper. Just...don&#x27;t take its conclusions too far.</div><br/></div></div><div id="39812212" class="c"><input type="checkbox" id="c-39812212" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39813995">prev</a><span>|</span><a href="#39812300">next</a><span>|</span><label class="collapse" for="c-39812212">[-]</label><label class="expand" for="c-39812212">[1 more]</label></div><br/><div class="children"><div class="content">The paper: <i>Are Emergent Abilities of Large Language Models a Mirage?</i> <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.15004" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.15004</a></div><br/></div></div><div id="39812300" class="c"><input type="checkbox" id="c-39812300" checked=""/><div class="controls bullet"><span class="by">tambourine_man</span><span>|</span><a href="#39812212">prev</a><span>|</span><a href="#39812678">next</a><span>|</span><label class="collapse" for="c-39812300">[-]</label><label class="expand" for="c-39812300">[9 more]</label></div><br/><div class="children"><div class="content">&gt; They find that with a different measuring stick, emergence vanishes.<p>Isn’t that the case for most&#x2F;all emergent behavior? If you change the scale and watch individual water molecules, you’d see them snapping into a crystal structure one by one instead of a sudden emergent block of ice.</div><br/><div id="39812337" class="c"><input type="checkbox" id="c-39812337" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#39812300">parent</a><span>|</span><a href="#39813347">next</a><span>|</span><label class="collapse" for="c-39812337">[-]</label><label class="expand" for="c-39812337">[1 more]</label></div><br/><div class="children"><div class="content">Not quite. The problem is that the definition is especially poorly defined in ML. I elaborate more here[0]. You are describing emergence, but not what was claimed when LLMs were said to have emergent abilities (the distinction is explained in the article fwiw)<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39812315">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39812315</a></div><br/></div></div><div id="39813347" class="c"><input type="checkbox" id="c-39813347" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#39812300">parent</a><span>|</span><a href="#39812337">prev</a><span>|</span><a href="#39813439">next</a><span>|</span><label class="collapse" for="c-39813347">[-]</label><label class="expand" for="c-39813347">[1 more]</label></div><br/><div class="children"><div class="content">But knowing the molecule structures at 50C and 75C tells you very little about the freezing point.<p>Different example: If you measure the number of cases of a given virus, it will either spread across the entire globe (R0 &gt; 1, eg. COVID-19) or fail to spread widely (R0 &lt; 1, eg. Ebola). Even though it&#x27;s not completely binary, it&#x27;s emergent behaviour because it <i>looks</i> binary. But if you were to measure R0 directly, you&#x27;d see a gradual increase, and could predict future variants&#x2F;vaccine efficiencies&#x2F;etc much more easily.<p>&quot;Emergent&quot; refers to eg. sigmoids, while &quot;gradual&quot; refers to eg. linear or logarithmic functions.</div><br/></div></div><div id="39813439" class="c"><input type="checkbox" id="c-39813439" checked=""/><div class="controls bullet"><span class="by">throwaway290</span><span>|</span><a href="#39812300">parent</a><span>|</span><a href="#39813347">prev</a><span>|</span><a href="#39812678">next</a><span>|</span><label class="collapse" for="c-39813439">[-]</label><label class="expand" for="c-39813439">[6 more]</label></div><br/><div class="children"><div class="content">Nobody mistakes ice for something sudden emergent though, it&#x27;s obvious to naked eye that it gradually appears</div><br/><div id="39813630" class="c"><input type="checkbox" id="c-39813630" checked=""/><div class="controls bullet"><span class="by">maxcoder4</span><span>|</span><a href="#39812300">root</a><span>|</span><a href="#39813439">parent</a><span>|</span><a href="#39812678">next</a><span>|</span><label class="collapse" for="c-39813630">[-]</label><label class="expand" for="c-39813630">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what exactly &quot;emergent&quot; means in this context, but doesn&#x27;t ice appear pretty rapidly? It appears between -1 and 1 degrees, and when cooling down from 100 to 75 to 50 to 25 water you would learn nothing about the freezing point.</div><br/><div id="39813658" class="c"><input type="checkbox" id="c-39813658" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39812300">root</a><span>|</span><a href="#39813630">parent</a><span>|</span><a href="#39813650">next</a><span>|</span><label class="collapse" for="c-39813658">[-]</label><label class="expand" for="c-39813658">[2 more]</label></div><br/><div class="children"><div class="content">Not to mention, if water is pure and still enough, you can cool it below freezing point, and it still looks like a liquid, until it is disturbed and then it suddenly doesn&#x27;t. Same with heating up above boiling point.</div><br/><div id="39813952" class="c"><input type="checkbox" id="c-39813952" checked=""/><div class="controls bullet"><span class="by">lukan</span><span>|</span><a href="#39812300">root</a><span>|</span><a href="#39813658">parent</a><span>|</span><a href="#39813650">next</a><span>|</span><label class="collapse" for="c-39813952">[-]</label><label class="expand" for="c-39813952">[1 more]</label></div><br/><div class="children"><div class="content">Also with heating?
So all the bubbles rising up way before boiling and the visible evaporation does not happen with distilled water? Hard to believe, I have to say. You would have to do perfect even heating of all the water, to stop the movement from different temperate areas. Microwaving it?</div><br/></div></div></div></div><div id="39813650" class="c"><input type="checkbox" id="c-39813650" checked=""/><div class="controls bullet"><span class="by">throwaway290</span><span>|</span><a href="#39812300">root</a><span>|</span><a href="#39813630">parent</a><span>|</span><a href="#39813658">prev</a><span>|</span><a href="#39812678">next</a><span>|</span><label class="collapse" for="c-39813650">[-]</label><label class="expand" for="c-39813650">[2 more]</label></div><br/><div class="children"><div class="content">Every volume of water IRL will disagree about the rapidity though, go below 0 and first it&#x27;s water, then it has some ice, then more ice, and only after a while eventually it&#x27;s all ice (and even then maybe it has some water underneath, who knows). I can&#x27;t see how it is not common sense. But with LLMs the common sense was until now &quot;they can&#x27;t then they suddenly can&quot;</div><br/><div id="39813936" class="c"><input type="checkbox" id="c-39813936" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#39812300">root</a><span>|</span><a href="#39813650">parent</a><span>|</span><a href="#39812678">next</a><span>|</span><label class="collapse" for="c-39813936">[-]</label><label class="expand" for="c-39813936">[1 more]</label></div><br/><div class="children"><div class="content">The water usually isn&#x27;t below 0 in those cases.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39812678" class="c"><input type="checkbox" id="c-39812678" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#39812300">prev</a><span>|</span><a href="#39812448">next</a><span>|</span><label class="collapse" for="c-39812678">[-]</label><label class="expand" for="c-39812678">[6 more]</label></div><br/><div class="children"><div class="content">The partial credit approach is fine, but if we want to train a model to get the right answer, then it does matter.<p>What I&#x27;ve noticed is that my loss curves on small models for arithmetic will reach a steady state where they&#x27;re still getting the answer wrong even if some digits are right. Yes you can keep training but the number of training epochs necessary seems to be inversely correlated exponentially with the model size<p>So a model with x parameters is going to take n^2 time as long as a model with 2x parameters<p>At a certain parameter count this basically means it is nigh impossible to get the right answer via training using gradient descent<p>The more parameters, the easier it is to drive to convergence, which is a real, important metric<p>At some point the estimated time for the ability to emerge spontaneously becomes greater than a human lifetime of even the lifetime of all humanity. So in the sense that increasing model size makes it tractable... I think it&#x27;s fair to say that the ability emerges suddenly enough.</div><br/><div id="39813048" class="c"><input type="checkbox" id="c-39813048" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#39812678">parent</a><span>|</span><a href="#39812948">next</a><span>|</span><label class="collapse" for="c-39813048">[-]</label><label class="expand" for="c-39813048">[2 more]</label></div><br/><div class="children"><div class="content">I think the point is that if you want a model that gives the right answer, you should still use partial credit to figure out how far away you are from that goal instead of binary correct-or-not accuracy.<p>If you use a metric where improvement happens suddenly and unpredictably, you can&#x27;t even estimate how much longer you need to train, because the ability might emerge spontaneously. But if the partial credit metric improves smoothly and predictably, you have a chance of extrapolating your training progress to see when you might hit your accuracy target, better than if you extrapolated accuracy directly.<p>And if you find that the estimated time is too long and you decide to train a bigger model, you can try to extrapolate across model sizes to estimate how big of a model you might need.</div><br/><div id="39813094" class="c"><input type="checkbox" id="c-39813094" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#39812678">root</a><span>|</span><a href="#39813048">parent</a><span>|</span><a href="#39812948">next</a><span>|</span><label class="collapse" for="c-39813094">[-]</label><label class="expand" for="c-39813094">[1 more]</label></div><br/><div class="children"><div class="content">To those still missing the point, it&#x27;s easy to come up with a metric that&#x27;s guaranteed to be a huge jump.<p>Eg. imagine a big corpus of arithmetic operations (say 2500 additions, 2500 subtractions, 2500 multiplications and 2500 divisions): you can test when a model passes computing all of those correctly, and it&#x27;s probably obvious that you&#x27;ll have a long list of &quot;didn&#x27;t pass&quot; until you suddenly get it passing.</div><br/></div></div></div></div><div id="39812948" class="c"><input type="checkbox" id="c-39812948" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#39812678">parent</a><span>|</span><a href="#39813048">prev</a><span>|</span><a href="#39813137">next</a><span>|</span><label class="collapse" for="c-39812948">[-]</label><label class="expand" for="c-39812948">[2 more]</label></div><br/><div class="children"><div class="content">It seems like model learning is too optimized for continuity, eg. continuous variables can be subdivided infinitely, where logic and algorithms are firm rather than fuzzy like this. Learning agents arguably need to be able to generalize from fuzzy to firm concepts to properly learn logic and algorithms. Whether that can happen automatically by scaling or whether that requires a fundamental shift is unclear.</div><br/><div id="39813618" class="c"><input type="checkbox" id="c-39813618" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39812678">root</a><span>|</span><a href="#39812948">parent</a><span>|</span><a href="#39813137">next</a><span>|</span><label class="collapse" for="c-39813618">[-]</label><label class="expand" for="c-39813618">[1 more]</label></div><br/><div class="children"><div class="content">Except if we take humans as example, we do <i>not</i> &quot;generalize from fuzzy to firm&quot;. We don&#x27;t handle firm, we approximate it with fuzzy. Which is why formal logic makes for a fine family of puzzle games, but fares extremely badly at describing reality and reasoning about reality. Better suited mathematical frameworks are probabilistic in nature, and so happen to be fundamentally continuous.</div><br/></div></div></div></div><div id="39813137" class="c"><input type="checkbox" id="c-39813137" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#39812678">parent</a><span>|</span><a href="#39812948">prev</a><span>|</span><a href="#39812448">next</a><span>|</span><label class="collapse" for="c-39813137">[-]</label><label class="expand" for="c-39813137">[1 more]</label></div><br/><div class="children"><div class="content">Did you see this submission? <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39575264">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39575264</a>
They sound linked.</div><br/></div></div></div></div><div id="39812448" class="c"><input type="checkbox" id="c-39812448" checked=""/><div class="controls bullet"><span class="by">sigsergv</span><span>|</span><a href="#39812678">prev</a><span>|</span><a href="#39811441">next</a><span>|</span><label class="collapse" for="c-39812448">[-]</label><label class="expand" for="c-39812448">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s really interesting question how can we measure emergent abilities like arithmetic operations. We cannot test every operation on every possible combinations of numbers. Instead we must make sure somehow that LLM performing arithmetic operations using corresponding rules and axioms.</div><br/><div id="39813167" class="c"><input type="checkbox" id="c-39813167" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#39812448">parent</a><span>|</span><a href="#39812632">next</a><span>|</span><label class="collapse" for="c-39813167">[-]</label><label class="expand" for="c-39813167">[2 more]</label></div><br/><div class="children"><div class="content">Why would it do that? Rules and axioms scale (slowly) with the number of layers. The model can heuristically approximate more easily and more incrementally.</div><br/><div id="39813597" class="c"><input type="checkbox" id="c-39813597" checked=""/><div class="controls bullet"><span class="by">lewhoo</span><span>|</span><a href="#39812448">root</a><span>|</span><a href="#39813167">parent</a><span>|</span><a href="#39812632">next</a><span>|</span><label class="collapse" for="c-39813597">[-]</label><label class="expand" for="c-39813597">[1 more]</label></div><br/><div class="children"><div class="content">But in this case why would you prefer approximation over answer ?</div><br/></div></div></div></div><div id="39812632" class="c"><input type="checkbox" id="c-39812632" checked=""/><div class="controls bullet"><span class="by">eropple</span><span>|</span><a href="#39812448">parent</a><span>|</span><a href="#39813167">prev</a><span>|</span><a href="#39811441">next</a><span>|</span><label class="collapse" for="c-39812632">[-]</label><label class="expand" for="c-39812632">[5 more]</label></div><br/><div class="children"><div class="content"><i>&gt; Instead we must make sure somehow that LLM performing arithmetic operations using corresponding rules and axioms.</i><p>It isn&#x27;t. It&#x27;s stringing together likely tokens that approximate (often very effectively!) what its data corpus has done in the past. And, relatedly, the best way I&#x27;ve found GPT4 to solve a word problem is to tell it to write some Python code to spit out an answer; the actual computation part is an easier thing to figure out when it&#x27;s just running code.</div><br/><div id="39813404" class="c"><input type="checkbox" id="c-39813404" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#39812448">root</a><span>|</span><a href="#39812632">parent</a><span>|</span><a href="#39811441">next</a><span>|</span><label class="collapse" for="c-39813404">[-]</label><label class="expand" for="c-39813404">[4 more]</label></div><br/><div class="children"><div class="content">A very simple example “list the Presidents in the order they were born”.<p>It gets the order wrong unless you tell it to “use Python”<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;4a673ea0-67d3-4256-b57d-dc1cf873d605" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;4a673ea0-67d3-4256-b57d-dc1cf8...</a></div><br/><div id="39813940" class="c"><input type="checkbox" id="c-39813940" checked=""/><div class="controls bullet"><span class="by">chmod775</span><span>|</span><a href="#39812448">root</a><span>|</span><a href="#39813404">parent</a><span>|</span><a href="#39813571">next</a><span>|</span><label class="collapse" for="c-39813940">[-]</label><label class="expand" for="c-39813940">[1 more]</label></div><br/><div class="children"><div class="content">My favorite example is telling it to reverse some longer text character-by-character. Trivial for any human to perform perfectly, but all models I&#x27;ve tested struggle with it and make mistakes all over. It&#x27;s really hard for them because they lack hidden state to perform algorithms - or what you would call thought in a human. Instead (essentially) for each step they have to re-consider the entire thing and their past output, figure out what they already did, and what they have to do next.<p>On the other hand they&#x27;ll spit out python code that&#x27;ll get you the reversed text just fine.<p>It&#x27;s also one of their greatest shortcomings when it comes to coding: They lack the ability to do any meaningful symbolical execution.<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;9faaae97-e20f-454e-b245-3e4c19b0e7ed" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;9faaae97-e20f-454e-b245-3e4c19...</a></div><br/></div></div><div id="39813571" class="c"><input type="checkbox" id="c-39813571" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#39812448">root</a><span>|</span><a href="#39813404">parent</a><span>|</span><a href="#39813940">prev</a><span>|</span><a href="#39811441">next</a><span>|</span><label class="collapse" for="c-39813571">[-]</label><label class="expand" for="c-39813571">[2 more]</label></div><br/><div class="children"><div class="content">Does python involve calling &quot;get_us_presidents()&quot;?</div><br/><div id="39813655" class="c"><input type="checkbox" id="c-39813655" checked=""/><div class="controls bullet"><span class="by">maxcoder4</span><span>|</span><a href="#39812448">root</a><span>|</span><a href="#39813571">parent</a><span>|</span><a href="#39811441">next</a><span>|</span><label class="collapse" for="c-39813655">[-]</label><label class="expand" for="c-39813655">[1 more]</label></div><br/><div class="children"><div class="content">You can check the code it generated in the long OP provided (this button is not very visible so I understand if you missed it).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39811441" class="c"><input type="checkbox" id="c-39811441" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39812448">prev</a><span>|</span><a href="#39812784">next</a><span>|</span><label class="collapse" for="c-39811441">[-]</label><label class="expand" for="c-39811441">[3 more]</label></div><br/><div class="children"><div class="content">This is a good paper. Though emergence doesn’t necessarily require a sudden jump in metrics, or unpredictability. New abilities can emerge gradually.</div><br/><div id="39813065" class="c"><input type="checkbox" id="c-39813065" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#39811441">parent</a><span>|</span><a href="#39812784">next</a><span>|</span><label class="collapse" for="c-39813065">[-]</label><label class="expand" for="c-39813065">[2 more]</label></div><br/><div class="children"><div class="content">When we talk about &quot;emergence&quot; in machine learning, we talk about those metrics with a sudden jump, as explained in the paper that introduced the term: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.07682" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.07682</a></div><br/><div id="39813586" class="c"><input type="checkbox" id="c-39813586" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#39811441">root</a><span>|</span><a href="#39813065">parent</a><span>|</span><a href="#39812784">next</a><span>|</span><label class="collapse" for="c-39813586">[-]</label><label class="expand" for="c-39813586">[1 more]</label></div><br/><div class="children"><div class="content">Just because somebody overloaded a word doesn&#x27;t mean we can&#x27;t correct people who use it wrong.<p>The correct phrase should be &quot;sudden&#x2F;unexpected emergence&quot;. Otherwise the phrase &quot;gradual emergence&quot; would be an oxymoron.</div><br/></div></div></div></div></div></div><div id="39812784" class="c"><input type="checkbox" id="c-39812784" checked=""/><div class="controls bullet"><span class="by">ekez</span><span>|</span><a href="#39811441">prev</a><span>|</span><a href="#39812559">next</a><span>|</span><label class="collapse" for="c-39812784">[-]</label><label class="expand" for="c-39812784">[2 more]</label></div><br/><div class="children"><div class="content">The metric the authors use confuses me.<p>Edit distance seems like a strange way to test if the model understands arithmetic ([1], Figure 3). I think `1+3=3` would be equally as correct as `1+1=9`?<p>Why not consider how far off the model is `abs(actual-expected)`? I wonder if there is an inflection point with that metric.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.07682" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.07682</a></div><br/><div id="39813007" class="c"><input type="checkbox" id="c-39813007" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#39812784">parent</a><span>|</span><a href="#39812559">next</a><span>|</span><label class="collapse" for="c-39813007">[-]</label><label class="expand" for="c-39813007">[1 more]</label></div><br/><div class="children"><div class="content">It depends on how you do arithmetic. If you&#x27;re a human and you do column addition, 12345+35791=<i>5</i>8136 is just as big of a mistake as 481<i>4</i>6 (the actual result is 48136). It&#x27;s just one mistaken column in both. Binary half-adders work the same way.<p>We don&#x27;t really know how LLMs do arithmetic. Maybe token edit distance would be interesting, but either way it doesn&#x27;t really change the claim of the paper.<p>Unrelated: The link is incorrect, the one you&#x27;re referring to is here: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2304.15004.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2304.15004.pdf</a></div><br/></div></div></div></div><div id="39812559" class="c"><input type="checkbox" id="c-39812559" checked=""/><div class="controls bullet"><span class="by">inopinatus</span><span>|</span><a href="#39812784">prev</a><span>|</span><a href="#39811894">next</a><span>|</span><label class="collapse" for="c-39812559">[-]</label><label class="expand" for="c-39812559">[2 more]</label></div><br/><div class="children"><div class="content">Even a stopped clock gives the right time, twice a day.<p>LLMs are plausibility engines. The fundamental hypothesis on trial here is that increasing plausibility corresponds to increasing correctness. This is easily rejected for the human-sourced content that trains them, implying an upper bound on any dependent phenomenon. It follows that simply scaling LLMs will not produce  AGI.</div><br/><div id="39813703" class="c"><input type="checkbox" id="c-39813703" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#39812559">parent</a><span>|</span><a href="#39811894">next</a><span>|</span><label class="collapse" for="c-39813703">[-]</label><label class="expand" for="c-39813703">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re assuming that humans reliably pursue correctness rather than plausibility.<p>The whole scientific system specifically being designed to suppress plausible-sounding-but-incorrect claims says otherwise.</div><br/></div></div></div></div><div id="39811894" class="c"><input type="checkbox" id="c-39811894" checked=""/><div class="controls bullet"><span class="by">dataking</span><span>|</span><a href="#39812559">prev</a><span>|</span><a href="#39812477">next</a><span>|</span><label class="collapse" for="c-39811894">[-]</label><label class="expand" for="c-39811894">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.is&#x2F;Z0EOB" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;Z0EOB</a></div><br/></div></div><div id="39812477" class="c"><input type="checkbox" id="c-39812477" checked=""/><div class="controls bullet"><span class="by">ambicapter</span><span>|</span><a href="#39811894">prev</a><span>|</span><a href="#39812471">next</a><span>|</span><label class="collapse" for="c-39812477">[-]</label><label class="expand" for="c-39812477">[2 more]</label></div><br/><div class="children"><div class="content">This is very insightful. Another one of those &quot;obvious in retrospect&quot; ideas. We tend to think of addition as only correct or not but has anyone ever claimed that LLMs have a &quot;discrete&quot; output (might be using the wrong terminology)? It would then make sense that you need to measure performance in a continuous, not discrete way. Otherwise you&#x27;ll end up with a sort of &quot;aliasing&quot;-type error.</div><br/><div id="39813322" class="c"><input type="checkbox" id="c-39813322" checked=""/><div class="controls bullet"><span class="by">woopsn</span><span>|</span><a href="#39812477">parent</a><span>|</span><a href="#39812471">next</a><span>|</span><label class="collapse" for="c-39813322">[-]</label><label class="expand" for="c-39813322">[1 more]</label></div><br/><div class="children"><div class="content">Not quite in retrospect -- the earlier paper on emergence addresses this<p>&gt; It is also important to consider the evaluation metrics used to measure emergent abilities. For instance, using exact string match as the evaluation metric for long-sequence targets may disguise compounding incremental improvements as emergence. Similar logic may apply for multi-step or arithmetic
reasoning problems, where models are only scored on whether they get the final answer to a multi-step problem correct, without any credit given to partially correct solutions.<p>- Page 7, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2206.07682.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2206.07682.pdf</a><p>They say that cross entropy loss in these cases goes down with model size incrementally, well before &quot;emergent&quot; capabilities appear. If so the model is improving (in a sense) even though these capabilities aren&#x27;t observable below some critical size.</div><br/></div></div></div></div><div id="39812471" class="c"><input type="checkbox" id="c-39812471" checked=""/><div class="controls bullet"><span class="by">knallfrosch</span><span>|</span><a href="#39812477">prev</a><span>|</span><a href="#39813391">next</a><span>|</span><label class="collapse" for="c-39812471">[-]</label><label class="expand" for="c-39812471">[2 more]</label></div><br/><div class="children"><div class="content">Too bad the article only mentions addition – which shows gradual improvement.<p>Would have loved to see the claim of emergence supported by an example as well.
And more importantly, if the measure is boolean, completely wrong or completely right, then, well, I do expect to see a &quot;sudden&quot; jump in ability.</div><br/><div id="39813081" class="c"><input type="checkbox" id="c-39813081" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#39812471">parent</a><span>|</span><a href="#39813391">next</a><span>|</span><label class="collapse" for="c-39813081">[-]</label><label class="expand" for="c-39813081">[1 more]</label></div><br/><div class="children"><div class="content">The original emergence paper has multiple &quot;emergent&quot; metrics. [1]<p>Though, the paper this article is about [2] mentions that among the &quot;emergent&quot; tasks they tested, 92% were measuring accuracy on either exact-string-match or multiple choice, so probably just bad metrics.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2206.07682.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2206.07682.pdf</a><p>[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2304.15004.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2304.15004.pdf</a></div><br/></div></div></div></div><div id="39813391" class="c"><input type="checkbox" id="c-39813391" checked=""/><div class="controls bullet"><span class="by">Lockal</span><span>|</span><a href="#39812471">prev</a><span>|</span><a href="#39812311">next</a><span>|</span><label class="collapse" for="c-39813391">[-]</label><label class="expand" for="c-39813391">[2 more]</label></div><br/><div class="children"><div class="content">Study: scientists prefer metrics which create an illusion that every time-consuming process is gradual and predictable</div><br/><div id="39813719" class="c"><input type="checkbox" id="c-39813719" checked=""/><div class="controls bullet"><span class="by">maxcoder4</span><span>|</span><a href="#39813391">parent</a><span>|</span><a href="#39812311">next</a><span>|</span><label class="collapse" for="c-39813719">[-]</label><label class="expand" for="c-39813719">[1 more]</label></div><br/><div class="children"><div class="content">If a process can be measured by a linearly growing metric I&#x27;d say it&#x27;s predictible.<p>And of course scientists try to understand complex processes and make them more predictible. That distinguishes science from magic. You said that like it&#x27;s a bad thing</div><br/></div></div></div></div><div id="39812311" class="c"><input type="checkbox" id="c-39812311" checked=""/><div class="controls bullet"><span class="by">wpietri</span><span>|</span><a href="#39813391">prev</a><span>|</span><a href="#39813198">next</a><span>|</span><label class="collapse" for="c-39812311">[-]</label><label class="expand" for="c-39812311">[3 more]</label></div><br/><div class="children"><div class="content">I think the graph is especially interesting. If the progress of LLMs is less radical when measured correctly, that could be another factor driving a bubble and its later popping.</div><br/><div id="39812502" class="c"><input type="checkbox" id="c-39812502" checked=""/><div class="controls bullet"><span class="by">abound</span><span>|</span><a href="#39812311">parent</a><span>|</span><a href="#39813198">next</a><span>|</span><label class="collapse" for="c-39812502">[-]</label><label class="expand" for="c-39812502">[2 more]</label></div><br/><div class="children"><div class="content">The updated &quot;partial credit&quot; metric proposed by the researchers suggests that LLMs will continue to improve as parameter count increases (assuming you have the data to make use of XX trillion parameters). It&#x27;s just showing that it isn&#x27;t as &quot;step-wise&quot; as other rankings may have indicated.</div><br/><div id="39812696" class="c"><input type="checkbox" id="c-39812696" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#39812311">root</a><span>|</span><a href="#39812502">parent</a><span>|</span><a href="#39813198">next</a><span>|</span><label class="collapse" for="c-39812696">[-]</label><label class="expand" for="c-39812696">[1 more]</label></div><br/><div class="children"><div class="content">It still is stepwise for the kinds of use cases being envisioned.<p>For example, it&#x27;s useful to know your 1 trillion parameter self driving car model is actually 90 percent of the way there, but it&#x27;s also useful to know that the threshold you need to meet to implement the technology will likely be met at some point when your parameter count increases.</div><br/></div></div></div></div></div></div><div id="39813198" class="c"><input type="checkbox" id="c-39813198" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#39812311">prev</a><span>|</span><a href="#39812315">next</a><span>|</span><label class="collapse" for="c-39813198">[-]</label><label class="expand" for="c-39813198">[1 more]</label></div><br/><div class="children"><div class="content">Transformers have in-context meta learning. If you give them examples they can perform new tasks.<p>When you run AutoGPT, the outcome is not always predictable.</div><br/></div></div><div id="39812315" class="c"><input type="checkbox" id="c-39812315" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#39813198">prev</a><span>|</span><a href="#39812539">next</a><span>|</span><label class="collapse" for="c-39812315">[-]</label><label class="expand" for="c-39812315">[1 more]</label></div><br/><div class="children"><div class="content">Emergence is a weird topic. If we go with the physicist&#x27;s common definition [0,1] we&#x27;d need to differentiate into weak and strong.<p>As for weak emergence[2] that is de facto.<p>As for strong emergence[3] well... do we have an example of the phenomena anywhere?<p>To be clear, ML people have a different definition which I do not think is entirely useful. Let&#x27;s look at the wording used in [4]<p><pre><code>  Scaling up language models has been shown to predictably improve performance and sample</code></pre>
efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable
phenomenon that we refer to as emergent abilities of large language models. We consider an
ability to be emergent if it is not present in smaller models but is present in larger models.
Thus, emergent abilities cannot be predicted simply by extrapolating the performance of
smaller models. The existence of such emergence raises the question of whether additional
scaling could potentially further expand the range of capabilities of language models<p><pre><code>  Emergence is when quantitative changes in a system result in qualitative changes in behavior.
  
  An ability is emergent if it is not present in smaller models but is present in larger models.
</code></pre>
I&#x27;m not sure anything here is actually meaningful. We have very little knowledge about the interpretation of ML models, so it seems to be jumping the gun to say that phenomena cannot be predicted. The aspects are highly coupled with things like training techniques, optimization objectives (or loss functions), optimization methods, not just architectural designs.<p>We&#x27;ve also after more research found smaller capable of performing things that large models can do. So the goal post moves and makes the condition of emergent abilities. [RTFA]<p>So how do we define emergent abilities? Everything static except for the number of parameters? Does this make sense? Changing the number of parameters is an abstracted way of changing the optimization technique or training method. Clearly these are also different by nature of different batch sizes and&#x2F;or a LR scheduler. So it is hard to be consistent.<p>Not to mention that we are not great at predicting much of what networks can do in general. There&#x27;s plenty of people working on the subject (though this is a small proportion of total ML researchers, even if we exclude those that are highly application focused). So are we gonna call something emergent if it is just something we don&#x27;t know about? And who gets to be the one to decide? I&#x27;ve seen abilities called emergent that are clearly a result of training methods like KL Divergence but surprising to individuals who do not have a rigorous statistics and&#x2F;or metric theory background.<p>It just all seems to be jumping the gun. It&#x27;s a new field and a new science. I think it is okay if we&#x27;re willing to admit that our understanding just isn&#x27;t great yet. There&#x27;s no need to embellish or over attribute. These models are without a doubt powerful and useful, but at the same time I feel we are happy to greatly exaggerate all aspects about them. And for the life of me, I can&#x27;t figure out why critiques are interpreted as dismissive. You can call an LLM a stochastic parrot and still think it is a great achievement and useful tool. Moreso, criticism is essential. Criticism gives us direction in research. Hype gives us motivation. But the two have to be in balance. Motivation without direction is an undirected Monte Carlo search (can still work, but we can do better than a drunken man). Overly criticizing (turning into dismissing, such as calling LLMs useless) is just pulling wool over one&#x27;s eyes. It is the equivalent of lights being turned off and deciding to sit down and give up. When the lights get turned off you should find how to turn them back on. And we have more clues than a pure random process. After all, aren&#x27;t we trying to make these better? I know I am. It is why I got interested in researching these things in the first place. I just wish we could have moderate hype rather than this ridiculously excessive one (which I think operates in a feedback loop with excessive criticism as it drives polarization).<p>[0] Sabine Hossenfelder (I know HN loves her): <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bJE6-VTdbjw" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bJE6-VTdbjw</a><p>[1] Sean Carrol: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=0_PdLja-eGQ" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=0_PdLja-eGQ</a><p>Yes, Sabine and Sean are both controversial characters but they are well known.<p>[2] Weak emergence is where some larger phenomena forms out of smaller phenomena and results in something that the smaller thing can&#x27;t do. The result requires the interactions of parts of a system. Temperature is a common example because it is generally easier to discuss the aggregated value of all the particles&#x27; &quot;jigglyness&quot; rather than each individual particle. The resultant property can be derived from the individual components. Conway&#x27;s game of life is another common example. But in game theory we&#x27;d call a coalition an emergent property since utility is higher in the collective than the sum of each individual. Clearly this is true for ANNs since they are composed of neurons and a single neuron cannot perform these tasks.<p>[3] Strong convergence is about behavior that CANNOT be derived from the individual constituents.<p>[4] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.07682" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.07682</a></div><br/></div></div><div id="39812539" class="c"><input type="checkbox" id="c-39812539" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39812315">prev</a><span>|</span><a href="#39812896">next</a><span>|</span><label class="collapse" for="c-39812539">[-]</label><label class="expand" for="c-39812539">[1 more]</label></div><br/><div class="children"><div class="content">Okay, neat. Using a binary metric means that you observe sudden transitions from success to failure. Using a more granular metric means you observe smoother improvement. Logical and meaningful.<p>It does make sense to evaluate things like 3-digit-sum by “how close textually?” And “how close numerically?” and the phase change is an artifact of the actual question being “is fully correct?”</div><br/></div></div><div id="39812896" class="c"><input type="checkbox" id="c-39812896" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#39812539">prev</a><span>|</span><a href="#39811938">next</a><span>|</span><label class="collapse" for="c-39812896">[-]</label><label class="expand" for="c-39812896">[1 more]</label></div><br/><div class="children"><div class="content">I think it only feels like it is emergent just like how some people thinks LLMs are conscious.<p>It’s emergent because you don’t really understand what it was capable of?  Or is it you have a confirmation bias because you don’t say its an emergent flaw when LLMs get some stuff so crazy wrong</div><br/></div></div><div id="39811938" class="c"><input type="checkbox" id="c-39811938" checked=""/><div class="controls bullet"><span class="by">mellosouls</span><span>|</span><a href="#39812896">prev</a><span>|</span><a href="#39811969">next</a><span>|</span><label class="collapse" for="c-39811938">[-]</label><label class="expand" for="c-39811938">[5 more]</label></div><br/><div class="children"><div class="content">OP Should link to the original article which is listed in the first line:<p><a href="https:&#x2F;&#x2F;www.quantamagazine.org&#x2F;how-quickly-do-large-language-models-learn-unexpected-skills-20240213&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.quantamagazine.org&#x2F;how-quickly-do-large-language...</a></div><br/><div id="39812030" class="c"><input type="checkbox" id="c-39812030" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#39811938">parent</a><span>|</span><a href="#39811969">next</a><span>|</span><label class="collapse" for="c-39812030">[-]</label><label class="expand" for="c-39812030">[4 more]</label></div><br/><div class="children"><div class="content">How does this work? Why is the whole story just copied over?</div><br/><div id="39812135" class="c"><input type="checkbox" id="c-39812135" checked=""/><div class="controls bullet"><span class="by">animaomnium</span><span>|</span><a href="#39811938">root</a><span>|</span><a href="#39812030">parent</a><span>|</span><a href="#39812152">next</a><span>|</span><label class="collapse" for="c-39812135">[-]</label><label class="expand" for="c-39812135">[2 more]</label></div><br/><div class="children"><div class="content">Wired has a deal with Quanta to republish some of their stories. There are other stories from Quanta in Wired; I no longer read Wired, but I believe that this is how I first learned of Quanta.</div><br/><div id="39812144" class="c"><input type="checkbox" id="c-39812144" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#39811938">root</a><span>|</span><a href="#39812135">parent</a><span>|</span><a href="#39812152">next</a><span>|</span><label class="collapse" for="c-39812144">[-]</label><label class="expand" for="c-39812144">[1 more]</label></div><br/><div class="children"><div class="content">Ahh, thanks, I didn&#x27;t realize this was a thing.</div><br/></div></div></div></div><div id="39812152" class="c"><input type="checkbox" id="c-39812152" checked=""/><div class="controls bullet"><span class="by">mellosouls</span><span>|</span><a href="#39811938">root</a><span>|</span><a href="#39812030">parent</a><span>|</span><a href="#39812135">prev</a><span>|</span><a href="#39811969">next</a><span>|</span><label class="collapse" for="c-39812152">[-]</label><label class="expand" for="c-39812152">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know but would assume it&#x27;s marketing-related, the story seen as meeting the demographic requirements of both publications, with an established mutually-beneficial relationship satisfied in the transaction, eg. maybe cheaper content for Wired, bumped readership figures for Quanta.</div><br/></div></div></div></div></div></div><div id="39811969" class="c"><input type="checkbox" id="c-39811969" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39811938">prev</a><span>|</span><a href="#39812734">next</a><span>|</span><label class="collapse" for="c-39811969">[-]</label><label class="expand" for="c-39811969">[10 more]</label></div><br/><div class="children"><div class="content">That is a terrible headline.  It implies that the _abilities_ are a mirage, but it&#x27;s actually the &quot;emergent&quot; part that might be a mirage -- which is to say it&#x27;s not an unpredictable &quot;phase transition&quot; but gradual and predictable improvement in ability as the models scale.</div><br/><div id="39812032" class="c"><input type="checkbox" id="c-39812032" checked=""/><div class="controls bullet"><span class="by">unclenoriega</span><span>|</span><a href="#39811969">parent</a><span>|</span><a href="#39812803">next</a><span>|</span><label class="collapse" for="c-39812032">[-]</label><label class="expand" for="c-39812032">[3 more]</label></div><br/><div class="children"><div class="content">FWIW, I read the headline as saying the abilities of LLMs are not emergent.</div><br/><div id="39812684" class="c"><input type="checkbox" id="c-39812684" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#39811969">root</a><span>|</span><a href="#39812032">parent</a><span>|</span><a href="#39812803">next</a><span>|</span><label class="collapse" for="c-39812684">[-]</label><label class="expand" for="c-39812684">[2 more]</label></div><br/><div class="children"><div class="content">Emergent and breakthrough are not the same quality. Something can be emergent and develop gradually</div><br/><div id="39812879" class="c"><input type="checkbox" id="c-39812879" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#39811969">root</a><span>|</span><a href="#39812684">parent</a><span>|</span><a href="#39812803">next</a><span>|</span><label class="collapse" for="c-39812879">[-]</label><label class="expand" for="c-39812879">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Something can be emergent and develop gradually<p>Emergent can be defined as the sudden appearance of novel behavior.</div><br/></div></div></div></div></div></div><div id="39812803" class="c"><input type="checkbox" id="c-39812803" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#39811969">parent</a><span>|</span><a href="#39812032">prev</a><span>|</span><a href="#39812051">next</a><span>|</span><label class="collapse" for="c-39812803">[-]</label><label class="expand" for="c-39812803">[5 more]</label></div><br/><div class="children"><div class="content">Not only that, but what&#x27;s the point in evaluating a language model on its ability to do arithmetic?  It&#x27;s like criticizing a talking dog because the C++ code it wrote was full of buffer-overflow bugs.<p>In reality, if you ask a good LLM to solve a math-related problem, it will write and run a Python program to return the answer.  Sometimes this even works.  Sometimes it returns garbage.  And sometimes it realizes that its own answer isn&#x27;t realistic and tries a different approach.  Sometimes people claim this isn&#x27;t a valid manifestation of intelligence.<p>Completely pointless study, unworthy of Wired <i>or</i> HN.</div><br/><div id="39813148" class="c"><input type="checkbox" id="c-39813148" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#39811969">root</a><span>|</span><a href="#39812803">parent</a><span>|</span><a href="#39812899">next</a><span>|</span><label class="collapse" for="c-39813148">[-]</label><label class="expand" for="c-39813148">[1 more]</label></div><br/><div class="children"><div class="content">This is a study of &quot;emergent&quot; properties of LLMs — whether something unexpected shows up (i.e. imagine that talking dog suddenly becoming great at pointer arithmetic and never using-after-free).<p>It was noticed that LLMs can do some arithmetic, but we are yet uncertain how much and how it happens exactly.</div><br/></div></div><div id="39812899" class="c"><input type="checkbox" id="c-39812899" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#39811969">root</a><span>|</span><a href="#39812803">parent</a><span>|</span><a href="#39813148">prev</a><span>|</span><a href="#39812051">next</a><span>|</span><label class="collapse" for="c-39812899">[-]</label><label class="expand" for="c-39812899">[3 more]</label></div><br/><div class="children"><div class="content">Arithmetic is treated as a proxy for general reasoning abilities.</div><br/><div id="39813112" class="c"><input type="checkbox" id="c-39813112" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#39811969">root</a><span>|</span><a href="#39812899">parent</a><span>|</span><a href="#39812051">next</a><span>|</span><label class="collapse" for="c-39813112">[-]</label><label class="expand" for="c-39813112">[2 more]</label></div><br/><div class="children"><div class="content"><i>Arithmetic is treated as a proxy for general reasoning abilities.</i><p>Which is stupid, because it&#x27;s not.  A pocket calculator can perform arithmetic, as can a Python program, or for that matter a Japanese cormorant, who counts the fish it helps you catch.  None of those are considered capable of &quot;reasoning&quot; on their own.<p>Meanwhile, GPT4 will cheerfully write a program to carry out the required arithmetic operations (and then some.)  A study that doesn&#x27;t acknowledge that is worthless at best.</div><br/><div id="39813832" class="c"><input type="checkbox" id="c-39813832" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#39811969">root</a><span>|</span><a href="#39813112">parent</a><span>|</span><a href="#39812051">next</a><span>|</span><label class="collapse" for="c-39813832">[-]</label><label class="expand" for="c-39813832">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Meanwhile, GPT4 will cheerfully write a program to carry out the required arithmetic operations (and then some.) A study that doesn&#x27;t acknowledge that is worthless at best.<p>We don&#x27;t want to see if the LLM can be used as a tool to do arithmetics, but whether it can learn complex data relationships like arithmetics. Arithmetics is a stepping stone, not a goal so that the model can solve it by invoking a calculator isn&#x27;t relevant. The problems we want it to solve doesn&#x27;t have tools like calculators so it doesn&#x27;t help getting us there.</div><br/></div></div></div></div></div></div></div></div><div id="39812051" class="c"><input type="checkbox" id="c-39812051" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39811969">parent</a><span>|</span><a href="#39812803">prev</a><span>|</span><a href="#39812734">next</a><span>|</span><label class="collapse" for="c-39812051">[-]</label><label class="expand" for="c-39812051">[1 more]</label></div><br/><div class="children"><div class="content">And “emergent” in this context usually means “behaviour resulting from the interaction of a large number of individually simple units”, not “suddenly appearing.”</div><br/></div></div></div></div><div id="39812734" class="c"><input type="checkbox" id="c-39812734" checked=""/><div class="controls bullet"><span class="by">skeledrew</span><span>|</span><a href="#39811969">prev</a><span>|</span><label class="collapse" for="c-39812734">[-]</label><label class="expand" for="c-39812734">[2 more]</label></div><br/><div class="children"><div class="content">There seems to be some sort of flaw or misalignment in the hypothesis. By my reading, it seems this trio is saying LLMs do arithmetic by predicting the digits, and at intermediate stages there are actually partial predictions, and so there isn&#x27;t a huge jump, which would then be classifiable as emergent. But I&#x27;m thinking that in order for an LLM to generally predict arithmetic results in the way they are designed, there would be an infinite memory requirement as it&#x27;d have to &quot;store&quot; an infinite set of completions.<p>So the key may be to find sets of operands that GPT4&#x2F;Claude Opus are unable to solve, even with hints similar to what a teacher may give a student learning the topic. Until such a set is found, I&#x27;d say this capability continues to meet the &quot;emergent&quot; definition, as the only other explanation - that I can think of - is that the models &quot;discovered&quot; how to do arithmetic in a general way.</div><br/><div id="39812830" class="c"><input type="checkbox" id="c-39812830" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#39812734">parent</a><span>|</span><label class="collapse" for="c-39812830">[-]</label><label class="expand" for="c-39812830">[1 more]</label></div><br/><div class="children"><div class="content">&gt;that I can think of - is that the models &quot;discovered&quot; how to do arithmetic in a general way.<p>I mean..yeah. Maybe i&#x27;ve simply misunderstood but It seems like you&#x27;re presenting this as an outlandish idea but something like this is what Neural Networks regularly end up doing especially in the limit (as loss trends down to zero).<p><a href="https:&#x2F;&#x2F;cprimozic.net&#x2F;blog&#x2F;reverse-engineering-a-small-neural-network&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cprimozic.net&#x2F;blog&#x2F;reverse-engineering-a-small-neura...</a><p><a href="https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;posts&#x2F;N6WM6hs7RQMKDhYjB&#x2F;a-mechanistic-interpretability-analysis-of-grokking" rel="nofollow">https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;posts&#x2F;N6WM6hs7RQMKDhYjB&#x2F;a-mec...</a><p>Nobody is going to find any such set because time and time again, neural networks show us complete memorization is actually more difficult than just learning the distribution (even if imperfectly).</div><br/></div></div></div></div></div></div></div></div></div></body></html>