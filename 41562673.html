<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726563668770" as="style"/><link rel="stylesheet" href="styles.css?v=1726563668770"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.12875">Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>krackers</span> | <span>71 comments</span></div><br/><div><div id="41563168" class="c"><input type="checkbox" id="c-41563168" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#41563500">next</a><span>|</span><label class="collapse" for="c-41563168">[-]</label><label class="expand" for="c-41563168">[57 more]</label></div><br/><div class="children"><div class="content">In the words of an author:<p>&quot;What is the performance limit when scaling LLM inference? Sky&#x27;s the limit.<p>We have mathematically proven that transformers can solve any problem, provided they are allowed to generate as many intermediate reasoning tokens as needed. Remarkably, constant depth is sufficient.<p><a href="http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.12875" rel="nofollow">http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.12875</a> (ICLR 2024)&quot;<p><a href="https:&#x2F;&#x2F;x.com&#x2F;denny_zhou&#x2F;status&#x2F;1835761801453306089" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;denny_zhou&#x2F;status&#x2F;1835761801453306089</a></div><br/><div id="41565307" class="c"><input type="checkbox" id="c-41565307" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41563308">next</a><span>|</span><label class="collapse" for="c-41565307">[-]</label><label class="expand" for="c-41565307">[1 more]</label></div><br/><div class="children"><div class="content">One question, if anyone knows the details: does this prove that there exists a single LLM that can approximate any function to arbitrary precision given enough CoT, or does it prove that for every function, there exists a Transformer that fits those criteria?<p>That is, does this prove that a single LLM can solve any problem, or that for any problem, we can find an LLM that solves it?</div><br/></div></div><div id="41563308" class="c"><input type="checkbox" id="c-41563308" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41565307">prev</a><span>|</span><a href="#41563327">next</a><span>|</span><label class="collapse" for="c-41563308">[-]</label><label class="expand" for="c-41563308">[19 more]</label></div><br/><div class="children"><div class="content">Is this the infinite monkey Shakespeare trope?</div><br/><div id="41564277" class="c"><input type="checkbox" id="c-41564277" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563308">parent</a><span>|</span><a href="#41564715">next</a><span>|</span><label class="collapse" for="c-41564277">[-]</label><label class="expand" for="c-41564277">[2 more]</label></div><br/><div class="children"><div class="content">More like the universal approximation theorem extended to computation rather than network complexity: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Universal_approximation_theorem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Universal_approximation_theore...</a></div><br/><div id="41564464" class="c"><input type="checkbox" id="c-41564464" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564277">parent</a><span>|</span><a href="#41564715">next</a><span>|</span><label class="collapse" for="c-41564464">[-]</label><label class="expand" for="c-41564464">[1 more]</label></div><br/><div class="children"><div class="content">The universal approximation theorem is good to know because says there&#x27;s no theoretical upper bound to a function-approximating NN&#x27;s accuracy. In practice it says nothing about what can be realistically achieved, though.</div><br/></div></div></div></div><div id="41564715" class="c"><input type="checkbox" id="c-41564715" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563308">parent</a><span>|</span><a href="#41564277">prev</a><span>|</span><a href="#41563641">next</a><span>|</span><label class="collapse" for="c-41564715">[-]</label><label class="expand" for="c-41564715">[1 more]</label></div><br/><div class="children"><div class="content">A key difference is that the way LMMs (Large Multimodal Models) generate output is far from random. These models can imitate&#x2F;blend existing information or imitate&#x2F;probably blend known reasoning methods in the training data. The latter is a key distinguishing feature of the new OpenAI o1 models.<p>Thus, the signal-to-noise ratio of their output is generally way better than infinite monkeys.<p>Arguably, humans rely on similar modes of &quot;thinking&quot; most of the time as well.</div><br/></div></div><div id="41563641" class="c"><input type="checkbox" id="c-41563641" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563308">parent</a><span>|</span><a href="#41564715">prev</a><span>|</span><a href="#41563327">next</a><span>|</span><label class="collapse" for="c-41563641">[-]</label><label class="expand" for="c-41563641">[15 more]</label></div><br/><div class="children"><div class="content">Yeah.  Monkeys.  Monkeys that write useful C and Python code that needs a bit less revision every time there&#x27;s a model update.<p>Can we just give the &quot;stochastic parrot&quot; and &quot;monkeys with typewriters&quot; schtick a rest?  It made for novel commentary three or four years ago, but at this point, these posts themselves read like the work of parrots.  They are no longer interesting, insightful, or (for that matter) true.</div><br/><div id="41563831" class="c"><input type="checkbox" id="c-41563831" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563641">parent</a><span>|</span><a href="#41564014">next</a><span>|</span><label class="collapse" for="c-41563831">[-]</label><label class="expand" for="c-41563831">[6 more]</label></div><br/><div class="children"><div class="content">If you think about it, humans necessarily use abstractions, from the edge detectors in retina to concepts like democracy. But do we really understand? All abstractions leak, and nobody knows the whole stack. For all the poorly grasped abstractions we are using, we are also just parroting. How many times are we doing things because &quot;that is how they are done&quot; never wondering why?<p>Take ML itself, people are saying it&#x27;s little more than alchemy (stir the pile). Are we just parroting approaches that have worked in practice without real understanding? Is it possible to have centralized understanding, even in principle, or is all understanding distributed among us? My conclusion is that we have a patchwork of partial understanding, stitched together functionally by abstractions. When I go to the doctor, I don&#x27;t study medicine first, I trust the doctor. Trust takes the place of genuine understanding.<p>So humans, like AI, use distributed and functional understanding, we don&#x27;t have genuine understanding as meant by philosophers like Searle in the Chinese Room. No single neuron in the brain understands anything, but together they do. Similarly, no single human understands genuinely, but society together manages to function. There is no homunculus, no centralized understander anywhere. We humans are also stochastic parrots of abstractions we don&#x27;t really grok to the full extent.</div><br/><div id="41564291" class="c"><input type="checkbox" id="c-41564291" checked=""/><div class="controls bullet"><span class="by">kaechle</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563831">parent</a><span>|</span><a href="#41564306">next</a><span>|</span><label class="collapse" for="c-41564291">[-]</label><label class="expand" for="c-41564291">[1 more]</label></div><br/><div class="children"><div class="content">Great points. We&#x27;re pattern-matching shortcut machines, without a doubt. In most contexts, not even good ones.<p>&gt; When I go to the doctor, I don&#x27;t study medicine first, I trust the doctor. Trust takes the place of genuine understanding.<p>The ultimate abstraction! Trust is highly irrational by definition. But we do it all day every day, lest we be classified as psychologically unfit for society. Which is to say, mental health is predicated on a not-insignificant amount of rationalizations and self-deceptions. Hallucinations, even.</div><br/></div></div><div id="41564306" class="c"><input type="checkbox" id="c-41564306" checked=""/><div class="controls bullet"><span class="by">throwaway290</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563831">parent</a><span>|</span><a href="#41564291">prev</a><span>|</span><a href="#41564014">next</a><span>|</span><label class="collapse" for="c-41564306">[-]</label><label class="expand" for="c-41564306">[4 more]</label></div><br/><div class="children"><div class="content">&gt; My conclusion<p>Are you saying you understood something? Was it genuine? Do you think LLM feels the same thing?</div><br/><div id="41564976" class="c"><input type="checkbox" id="c-41564976" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564306">parent</a><span>|</span><a href="#41565026">next</a><span>|</span><label class="collapse" for="c-41564976">[-]</label><label class="expand" for="c-41564976">[1 more]</label></div><br/><div class="children"><div class="content">Haha, &quot;I doubt therefore I am^W don&#x27;t understand&quot;</div><br/></div></div><div id="41565026" class="c"><input type="checkbox" id="c-41565026" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564306">parent</a><span>|</span><a href="#41564976">prev</a><span>|</span><a href="#41564014">next</a><span>|</span><label class="collapse" for="c-41565026">[-]</label><label class="expand" for="c-41565026">[2 more]</label></div><br/><div class="children"><div class="content">do llms feel?</div><br/><div id="41565384" class="c"><input type="checkbox" id="c-41565384" checked=""/><div class="controls bullet"><span class="by">throwaway290</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41565026">parent</a><span>|</span><a href="#41564014">next</a><span>|</span><label class="collapse" for="c-41565384">[-]</label><label class="expand" for="c-41565384">[1 more]</label></div><br/><div class="children"><div class="content">seems like would be the implication if yes</div><br/></div></div></div></div></div></div></div></div><div id="41564014" class="c"><input type="checkbox" id="c-41564014" checked=""/><div class="controls bullet"><span class="by">kaechle</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563641">parent</a><span>|</span><a href="#41563831">prev</a><span>|</span><a href="#41564386">next</a><span>|</span><label class="collapse" for="c-41564014">[-]</label><label class="expand" for="c-41564014">[1 more]</label></div><br/><div class="children"><div class="content">Every time I read &quot;stochastic parrot,&quot; my always-deterministic human brain surfaces this quote:<p>&gt; “Most people are other people. Their thoughts are someone else&#x27;s opinions, their lives a mimicry, their passions a quotation.”<p>- Oscar Wilde, a great ape with a pen</div><br/></div></div><div id="41564386" class="c"><input type="checkbox" id="c-41564386" checked=""/><div class="controls bullet"><span class="by">ffsm8</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563641">parent</a><span>|</span><a href="#41564014">prev</a><span>|</span><a href="#41564255">next</a><span>|</span><label class="collapse" for="c-41564386">[-]</label><label class="expand" for="c-41564386">[4 more]</label></div><br/><div class="children"><div class="content">&gt; novel commentary three or four years ago,<p>Chatgpt was released November 2022. That&#x27;s one year and 10 months ago. Their marketing started in the summer of the same year, still far of from 3-4 years.</div><br/><div id="41565644" class="c"><input type="checkbox" id="c-41565644" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564386">parent</a><span>|</span><a href="#41564596">next</a><span>|</span><label class="collapse" for="c-41565644">[-]</label><label class="expand" for="c-41565644">[1 more]</label></div><br/><div class="children"><div class="content">GPT-3 paper announcement got 200 comments on HN back in 2020.<p>It doesn&#x27;t matter when marketing started, people were already discussing it in 2019-2020.<p>Stochastic parrot: The term was coined by Emily M. Bender[2][3] in the 2021 artificial intelligence research paper &quot;On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? &quot; by Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell.[4]</div><br/></div></div><div id="41564596" class="c"><input type="checkbox" id="c-41564596" checked=""/><div class="controls bullet"><span class="by">Banou</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564386">parent</a><span>|</span><a href="#41565644">prev</a><span>|</span><a href="#41565613">next</a><span>|</span><label class="collapse" for="c-41564596">[-]</label><label class="expand" for="c-41564596">[1 more]</label></div><br/><div class="children"><div class="content">But chatgpt wasnt the first, openai had coding playground with gpt2, and you could already code even before that, around 2020 already, so I&#x27;d say it has been 3-4years</div><br/></div></div></div></div><div id="41564255" class="c"><input type="checkbox" id="c-41564255" checked=""/><div class="controls bullet"><span class="by">93po</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563641">parent</a><span>|</span><a href="#41564386">prev</a><span>|</span><a href="#41563327">next</a><span>|</span><label class="collapse" for="c-41564255">[-]</label><label class="expand" for="c-41564255">[3 more]</label></div><br/><div class="children"><div class="content">AI news article comments bingo card:<p>* Tired ClosedAI joke<p>* Claiming it&#x27;s predictive text engine that isn&#x27;t useful for anything<p>* Safety regulations are either good or bad, depending on who&#x27;s proposing them<p>* Fear mongering about climate impact<p>* Bringing up Elon for no reason<p>* AI will never be able to [some pretty achievable task]<p>* Tired arguments from pro-IP &#x2F; copyright sympathizers</div><br/><div id="41564868" class="c"><input type="checkbox" id="c-41564868" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564255">parent</a><span>|</span><a href="#41564737">next</a><span>|</span><label class="collapse" for="c-41564868">[-]</label><label class="expand" for="c-41564868">[1 more]</label></div><br/><div class="children"><div class="content">Interestingly there should be one which is missing which is well appropriate unless everyone is super smart math professor level genius:<p>These papers become increasingly difficult to properly comprehend.<p>…and thus perhaps the plethora of arguably nonsensical follow ups.</div><br/></div></div><div id="41564737" class="c"><input type="checkbox" id="c-41564737" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564255">parent</a><span>|</span><a href="#41564868">prev</a><span>|</span><a href="#41563327">next</a><span>|</span><label class="collapse" for="c-41564737">[-]</label><label class="expand" for="c-41564737">[1 more]</label></div><br/><div class="children"><div class="content">&gt;* Claiming it&#x27;s predictive text engine that isn&#x27;t useful for anything<p>This one is very common on HN and it&#x27;s baffling. Even if it&#x27;s predictive text, who the hell cares if it achieves its goals? If an LLM is actually a bunch of dolphins typing on a keyboard made for dolphins, I could care less if it does what I need it to do. For people who continue to repeat this on HN, why? I just want to know out of my curiosity.<p>&gt;* AI will never be able to [some pretty achievable task]<p>Also very common on HN.<p>You forgot the &quot;AI will never be able to do what a human can do in the exact way a human does it so AI will never achieve x&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="41563327" class="c"><input type="checkbox" id="c-41563327" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41563308">prev</a><span>|</span><a href="#41563328">next</a><span>|</span><label class="collapse" for="c-41563327">[-]</label><label class="expand" for="c-41563327">[3 more]</label></div><br/><div class="children"><div class="content">Theoretical results exist that try to quantify the number of CoT tokens needed to reach different levels of computational expressibility:
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.07923" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.07923</a><p>TL;DR: Getting to Turing completeness can require polynomial CoT tokens, wrt the input problem size.
For a field that constantly harps on parallelism and compute efficiency, this requirement seems prohibitive.<p>We really need to get away from constant depth architectures.</div><br/><div id="41563840" class="c"><input type="checkbox" id="c-41563840" checked=""/><div class="controls bullet"><span class="by">benkuykendall</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563327">parent</a><span>|</span><a href="#41563328">next</a><span>|</span><label class="collapse" for="c-41563840">[-]</label><label class="expand" for="c-41563840">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Getting to Turing completeness can require polynomial CoT tokens, wrt the input problem size.<p>So, as stated, this is impossible since it violates the Time Hierarchy Theorem.<p>The actual result of the paper is that any poly-time computable function can be computed with poly-many tokens. Which is... not a particularly impressive bound? Any non-trivial fixed neural network can, for instance, compute the NAND of two inputs. And any polynomial computable function can be computed with a polynomial number of NAND gates.</div><br/><div id="41563892" class="c"><input type="checkbox" id="c-41563892" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563840">parent</a><span>|</span><a href="#41563328">next</a><span>|</span><label class="collapse" for="c-41563892">[-]</label><label class="expand" for="c-41563892">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The actual result of the paper is that any poly-time computable function can be computed with poly-many tokens.<p>You&#x27;re right.<p>Re: NAND of two inputs. Isn&#x27;t this doable even by a single layer (no hidden layers) neural network?<p>Re: Polynomial computable function. I&#x27;m assuming this makes no assumption of constant-depth.<p>Because my entire point was that the result of this paper is not actually impressive AND covered by a previous paper. Hopefully that&#x27;s clearer.</div><br/></div></div></div></div></div></div><div id="41563263" class="c"><input type="checkbox" id="c-41563263" checked=""/><div class="controls bullet"><span class="by">candiddevmike</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41564363">prev</a><span>|</span><a href="#41564691">next</a><span>|</span><label class="collapse" for="c-41563263">[-]</label><label class="expand" for="c-41563263">[23 more]</label></div><br/><div class="children"><div class="content">&gt; We have mathematically proven that transformers can solve any problem, provided they are allowed to generate as many intermediate reasoning tokens as needed.<p>That seems like a bit of a leap here to make this seem more impressive than it is (IMO).  You can say the same thing about humans, provided they are allowed to think across as many years&#x2F;generations as needed.<p>Wake me up when a LLM figures out stable fusion or room temperature superconductors.</div><br/><div id="41563330" class="c"><input type="checkbox" id="c-41563330" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563263">parent</a><span>|</span><a href="#41563283">next</a><span>|</span><label class="collapse" for="c-41563330">[-]</label><label class="expand" for="c-41563330">[4 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re misrepresenting the study. It builds upon previous work that examines the computation power of the transformer architecture from a circuit-complexity perspective. Previous work showed that the class of problems that a &quot;naive&quot; Transformer architecture could compute was within TC0 [1, 2] and as a consequence it was fundamentally impossible for transformers to solve certain classes of mathematical problems. This study actually provides a more realistic bound of AC0 (by analyzing the finite-precision case) which rules out even more problems, including such &#x27;simple&#x27; ones as modular parity.<p>We also had previous work that hinted that part of the reason why chain-of-thought works from a theoretical perspective is that it literally allows the model to perform types of computations it could not under the more limited setting (in the same way jumping from FSMs to pushdown automata allows you to solve new types of problems) [3].<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35609652">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35609652</a>
[2] <a href="https:&#x2F;&#x2F;blog.computationalcomplexity.org&#x2F;2023&#x2F;02&#x2F;why-cant-little-chatty-do-math.html" rel="nofollow">https:&#x2F;&#x2F;blog.computationalcomplexity.org&#x2F;2023&#x2F;02&#x2F;why-cant-li...</a>
[3] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.15408" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.15408</a></div><br/><div id="41563361" class="c"><input type="checkbox" id="c-41563361" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563330">parent</a><span>|</span><a href="#41563283">next</a><span>|</span><label class="collapse" for="c-41563361">[-]</label><label class="expand" for="c-41563361">[3 more]</label></div><br/><div class="children"><div class="content">Generally, literature on the computational power of the SAME neural architecture can differ on their conclusions based on their premises. Assuming finite precision will give a more restrictive result, and assuming arbitrary precision can give you Turing completeness.<p>From a quick skim this seems like it&#x27;s making finite precision assumptions? Which doesn&#x27;t actually tighten previous bounds, it just makes different starting assumptions.<p>Am author of [1].</div><br/><div id="41563371" class="c"><input type="checkbox" id="c-41563371" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563361">parent</a><span>|</span><a href="#41563283">next</a><span>|</span><label class="collapse" for="c-41563371">[-]</label><label class="expand" for="c-41563371">[2 more]</label></div><br/><div class="children"><div class="content">Ah my bad, great catch! I&#x27;ve updated my comment accordingly.</div><br/><div id="41563452" class="c"><input type="checkbox" id="c-41563452" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563371">parent</a><span>|</span><a href="#41563283">next</a><span>|</span><label class="collapse" for="c-41563452">[-]</label><label class="expand" for="c-41563452">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t really be blamed though, the language in the paper does seem to state what you originally said. Might be a matter of taste but I don&#x27;t think it&#x27;s quite accurate.<p>The prior work they referenced actually did account for finite precision cases and why they didn&#x27;t think it was useful to prove the result with those premises.<p>In this work they simply argued from their own perspective why finite precision made more sense.<p>The whole sub-field is kinda messy and I get quoted differing results all the time.<p>Edit: Also, your original point stands, obviously. Sorry for nitpicking on your post, but I also just thought people should know more about the nuances of this stuff.</div><br/></div></div></div></div></div></div></div></div><div id="41563283" class="c"><input type="checkbox" id="c-41563283" checked=""/><div class="controls bullet"><span class="by">Horffupolde</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563263">parent</a><span>|</span><a href="#41563330">prev</a><span>|</span><a href="#41564012">next</a><span>|</span><label class="collapse" for="c-41563283">[-]</label><label class="expand" for="c-41563283">[1 more]</label></div><br/><div class="children"><div class="content">It is actually impressive.<p>One could argue that writing enabled chain of thought across generations.</div><br/></div></div><div id="41564012" class="c"><input type="checkbox" id="c-41564012" checked=""/><div class="controls bullet"><span class="by">Veedrac</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563263">parent</a><span>|</span><a href="#41563283">prev</a><span>|</span><a href="#41563756">next</a><span>|</span><label class="collapse" for="c-41564012">[-]</label><label class="expand" for="c-41564012">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Wake me up when a LLM figures out stable fusion or room temperature superconductors.<p>Man, the goalposts these days.</div><br/><div id="41564698" class="c"><input type="checkbox" id="c-41564698" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564012">parent</a><span>|</span><a href="#41564217">next</a><span>|</span><label class="collapse" for="c-41564698">[-]</label><label class="expand" for="c-41564698">[1 more]</label></div><br/><div class="children"><div class="content">&quot;I love [goalposts]. I love the whooshing noise they make as they go by.&quot; --Douglas Adams, slightly adjusted</div><br/></div></div><div id="41564217" class="c"><input type="checkbox" id="c-41564217" checked=""/><div class="controls bullet"><span class="by">WalterSear</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564012">parent</a><span>|</span><a href="#41564698">prev</a><span>|</span><a href="#41563756">next</a><span>|</span><label class="collapse" for="c-41564217">[-]</label><label class="expand" for="c-41564217">[1 more]</label></div><br/><div class="children"><div class="content">Shh!! It&#x27;s working! It&#x27;s working!</div><br/></div></div></div></div><div id="41563756" class="c"><input type="checkbox" id="c-41563756" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563263">parent</a><span>|</span><a href="#41564012">prev</a><span>|</span><a href="#41564246">next</a><span>|</span><label class="collapse" for="c-41563756">[-]</label><label class="expand" for="c-41563756">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s a TCS result.<p>seems like many commenting don&#x27;t know about computability</div><br/></div></div><div id="41564246" class="c"><input type="checkbox" id="c-41564246" checked=""/><div class="controls bullet"><span class="by">WalterSear</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563263">parent</a><span>|</span><a href="#41563756">prev</a><span>|</span><a href="#41563437">next</a><span>|</span><label class="collapse" for="c-41564246">[-]</label><label class="expand" for="c-41564246">[3 more]</label></div><br/><div class="children"><div class="content">&gt; You can say the same thing about humans<p>1. Holy shit.<p>2. You can&#x27;t apply Moore&#x27;s law to humans.</div><br/><div id="41564675" class="c"><input type="checkbox" id="c-41564675" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564246">parent</a><span>|</span><a href="#41565536">next</a><span>|</span><label class="collapse" for="c-41564675">[-]</label><label class="expand" for="c-41564675">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t to chips any more either.<p>Density has continued to increase,  but so have prices.
The &#x27;law&#x27; was tied to the price to density ratio, and it&#x27;s been almost a decade now since it died.</div><br/></div></div><div id="41565536" class="c"><input type="checkbox" id="c-41565536" checked=""/><div class="controls bullet"><span class="by">gryn</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564246">parent</a><span>|</span><a href="#41564675">prev</a><span>|</span><a href="#41563437">next</a><span>|</span><label class="collapse" for="c-41565536">[-]</label><label class="expand" for="c-41565536">[1 more]</label></div><br/><div class="children"><div class="content">&gt; 2. You can&#x27;t apply Moore&#x27;s law to humans.<p>not with that attitude. &#x2F;s<p>if you take reproduction into account and ignore all the related externalities you can definitely double your count of transistors (humans) every two years.</div><br/></div></div></div></div><div id="41563437" class="c"><input type="checkbox" id="c-41563437" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563263">parent</a><span>|</span><a href="#41564246">prev</a><span>|</span><a href="#41564691">next</a><span>|</span><label class="collapse" for="c-41563437">[-]</label><label class="expand" for="c-41563437">[10 more]</label></div><br/><div class="children"><div class="content">&gt; You can say the same thing about humans, provided they are allowed to think across as many years&#x2F;generations as needed.<p>Isn’t this a good thing since compute can be scaled so that the LLM can do generations of human thinking in a much shorter amount of time?<p>Say humans can solve quantum gravity in 100 years of thinking by 10,000 really smart people. If one AGI is equal to 1 really smart person. Scale enough compute for 1 million AGI and we can solve quantum gravity in a year.<p>The major assumption here is that transformers can indeed solve every problem humans can.</div><br/><div id="41563929" class="c"><input type="checkbox" id="c-41563929" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563437">parent</a><span>|</span><a href="#41563497">next</a><span>|</span><label class="collapse" for="c-41563929">[-]</label><label class="expand" for="c-41563929">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Scale enough compute for 1 million AGI and we can solve quantum gravity in a year.<p>That is wrong, it misses the point. We learn from the environment, we don&#x27;t secrete quantum gravity from our pure brains. It&#x27;s a RL setting of exploration and exploitation, a search process in the space of ideas based on validation in reality. A LLM alone is like a human locked away in a cell, with no access to test ideas.<p>If you take child Einstein and put him on a remote island, and come back 30 years later, do you think he would impress you with is deep insights? It&#x27;s not the brain alone that made Einstein so smart. It&#x27;s also his environment that had a major contribution.</div><br/><div id="41564055" class="c"><input type="checkbox" id="c-41564055" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563929">parent</a><span>|</span><a href="#41565089">next</a><span>|</span><label class="collapse" for="c-41564055">[-]</label><label class="expand" for="c-41564055">[2 more]</label></div><br/><div class="children"><div class="content">Assumption is that the AGI can solve any problem humans can - including learning from the environment if that is what is needed.<p>But I think you&#x27;re missing the point of my post. I don&#x27;t want to devolve this topic into yet another argument centered around &quot;but AI can&#x27;t be AGI or can&#x27;t do what humans can do because so and so&quot;.</div><br/><div id="41564954" class="c"><input type="checkbox" id="c-41564954" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41564055">parent</a><span>|</span><a href="#41565089">next</a><span>|</span><label class="collapse" for="c-41564954">[-]</label><label class="expand" for="c-41564954">[1 more]</label></div><br/><div class="children"><div class="content">I often see this misconception that compute alone will lead us to surpass human level. No doubt it is inspired by the &quot;scaling laws&quot; we heard so much about. People forget that imitation is not sufficient to surpass human level.</div><br/></div></div></div></div><div id="41565089" class="c"><input type="checkbox" id="c-41565089" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563929">parent</a><span>|</span><a href="#41564055">prev</a><span>|</span><a href="#41563497">next</a><span>|</span><label class="collapse" for="c-41565089">[-]</label><label class="expand" for="c-41565089">[1 more]</label></div><br/><div class="children"><div class="content">if you told child Einstein that light travels at a constant speed in all inertial frames and taught him algebra, then yes, he would come up with special relativity.<p>in general, an AGI might want to perform experiments to guide its exploration, but it&#x27;s possible that the hypotheses that it would want to check have already been probed&#x2F;constrained sufficiently. which is to say, a theoretical physicist might still stumble upon the right theory without further experiments.</div><br/></div></div></div></div><div id="41563497" class="c"><input type="checkbox" id="c-41563497" checked=""/><div class="controls bullet"><span class="by">wizzwizz4</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563437">parent</a><span>|</span><a href="#41563929">prev</a><span>|</span><a href="#41564691">next</a><span>|</span><label class="collapse" for="c-41563497">[-]</label><label class="expand" for="c-41563497">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Isn’t this a good thing since compute and be scaled so that the LLM can do generations of human thinking in a much shorter amount of time?</i><p>But it can&#x27;t. There isn&#x27;t enough planet.<p>&gt; <i>The major assumption here is that transformers can indeed solve every problem humans can.</i><p>No, the major assumptions are (a) that ChatGPT can, and (b) that we can reduce the resource requirements by many orders of magnitude. The former assumption is highly-dubious, and the latter is plainly false.<p>Transformers are capable of representing any algorithm, if they&#x27;re allowed to be large enough and run large enough. That doesn&#x27;t give them any special algorithm-finding ability, and finding the correct algorithms is the hard part of the problem!</div><br/><div id="41563507" class="c"><input type="checkbox" id="c-41563507" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563497">parent</a><span>|</span><a href="#41564691">next</a><span>|</span><label class="collapse" for="c-41563507">[-]</label><label class="expand" for="c-41563507">[4 more]</label></div><br/><div class="children"><div class="content">&gt; But it can&#x27;t. There isn&#x27;t enough planet.<p>How much resource are you assuming an AGI would consume?</div><br/><div id="41563586" class="c"><input type="checkbox" id="c-41563586" checked=""/><div class="controls bullet"><span class="by">wizzwizz4</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563507">parent</a><span>|</span><a href="#41564691">next</a><span>|</span><label class="collapse" for="c-41563586">[-]</label><label class="expand" for="c-41563586">[3 more]</label></div><br/><div class="children"><div class="content">Are we talking about &quot;an AGI&quot;, or are we talking about overfitting large transformer models with human-written corpora and scaling up the result?<p>&quot;An AGI&quot;? I have no idea what that algorithm might look like. I do know that we can cover the majority of cases with not too much effort, so it all depends on the characteristics of that long tail.<p>ChatGPT-like transformer models? We <i>know</i> what that looks like, despite the AI companies creatively misrepresenting the resource use (ref: <a href="https:&#x2F;&#x2F;www.bnnbloomberg.ca&#x2F;business&#x2F;technology&#x2F;2024&#x2F;08&#x2F;21&#x2F;how-tech-companies-are-obscuring-ais-real-carbon-footprint&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.bnnbloomberg.ca&#x2F;business&#x2F;technology&#x2F;2024&#x2F;08&#x2F;21&#x2F;h...</a>). Look at <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.06405" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.06405</a>:<p>&gt; Combining Wu’s method with the classic synthetic methods of deductive databases and angle, ratio,
and distance chasing solves 21 out of 30 methods by just using a CPU-only laptop with a time limit of
5 minutes per problem.<p>AlphaGeometry had an <i>entire supercomputer cluster</i>, and <i>dozens of hours</i>. GOFAI approaches have a <i>laptop</i> and <i>five minutes</i>. Scale that inconceivable inefficiency up to AGI, and the total power output of the sun may not be enough.</div><br/><div id="41563943" class="c"><input type="checkbox" id="c-41563943" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563586">parent</a><span>|</span><a href="#41564691">next</a><span>|</span><label class="collapse" for="c-41563943">[-]</label><label class="expand" for="c-41563943">[2 more]</label></div><br/><div class="children"><div class="content">When computers first became useful, you needed computers the size of rooms to compute. In 2024, my earphones have more compute.</div><br/><div id="41564916" class="c"><input type="checkbox" id="c-41564916" checked=""/><div class="controls bullet"><span class="by">krige</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563943">parent</a><span>|</span><a href="#41564691">next</a><span>|</span><label class="collapse" for="c-41564916">[-]</label><label class="expand" for="c-41564916">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always a hindsight declaration though. Currently we can only say that Intel has reused the same architecture several times already and cranking up the voltage until it breaks because they seem to be yet to find the next design leap, while AMD has been toying around with 3D placement but their latest design is woefully unimpressive. We do not know when the next compute leap will happen until it happens.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41564691" class="c"><input type="checkbox" id="c-41564691" checked=""/><div class="controls bullet"><span class="by">ljsprague</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41563263">prev</a><span>|</span><a href="#41564607">next</a><span>|</span><label class="collapse" for="c-41564691">[-]</label><label class="expand" for="c-41564691">[1 more]</label></div><br/><div class="children"><div class="content">Skynet&#x27;s the limit.</div><br/></div></div><div id="41564607" class="c"><input type="checkbox" id="c-41564607" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41564691">prev</a><span>|</span><a href="#41563373">next</a><span>|</span><label class="collapse" for="c-41564607">[-]</label><label class="expand" for="c-41564607">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Remarkably, constant depth is sufficient.<p>I think article also says log(n) embedding size (width?) is required, where n is size of input.</div><br/></div></div><div id="41563373" class="c"><input type="checkbox" id="c-41563373" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41564607">prev</a><span>|</span><a href="#41563842">next</a><span>|</span><label class="collapse" for="c-41563373">[-]</label><label class="expand" for="c-41563373">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We have mathematically proven that transformers can solve any problem<p>We should require that you&#x27;ve passed an algorithms and a thermodynamics class before you can post.</div><br/><div id="41563401" class="c"><input type="checkbox" id="c-41563401" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563373">parent</a><span>|</span><a href="#41563842">next</a><span>|</span><label class="collapse" for="c-41563401">[-]</label><label class="expand" for="c-41563401">[1 more]</label></div><br/><div class="children"><div class="content">To be clear I think the tweet is a bit exaggerated (and the word ‘performance’ there doesn’t take into account efficiency, for example) but I don’t have the time to read the full paper (just skimmed the abstract and conclusion). I quoted the tweet by an author for people to discuss since it’s still a fairly remarkable result.</div><br/></div></div></div></div><div id="41563842" class="c"><input type="checkbox" id="c-41563842" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41563373">prev</a><span>|</span><a href="#41563429">next</a><span>|</span><label class="collapse" for="c-41563842">[-]</label><label class="expand" for="c-41563842">[1 more]</label></div><br/><div class="children"><div class="content">Sort of like quantum superposition state? So here is an idea, using quantum to produce all possible inferences and use some not yet invented algorithms to collapse to the final result</div><br/></div></div><div id="41563429" class="c"><input type="checkbox" id="c-41563429" checked=""/><div class="controls bullet"><span class="by">tooltower</span><span>|</span><a href="#41563168">parent</a><span>|</span><a href="#41563842">prev</a><span>|</span><a href="#41563500">next</a><span>|</span><label class="collapse" for="c-41563429">[-]</label><label class="expand" for="c-41563429">[3 more]</label></div><br/><div class="children"><div class="content">Constant depth circuits can solve everything? I feel like I missed some important part of circuit complexity. Or this is BS.</div><br/><div id="41563474" class="c"><input type="checkbox" id="c-41563474" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563429">parent</a><span>|</span><a href="#41563764">next</a><span>|</span><label class="collapse" for="c-41563474">[-]</label><label class="expand" for="c-41563474">[1 more]</label></div><br/><div class="children"><div class="content">Using CoT implicitly increases the depth of the circuit. But yes, poorly worded.</div><br/></div></div><div id="41563764" class="c"><input type="checkbox" id="c-41563764" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41563168">root</a><span>|</span><a href="#41563429">parent</a><span>|</span><a href="#41563474">prev</a><span>|</span><a href="#41563500">next</a><span>|</span><label class="collapse" for="c-41563764">[-]</label><label class="expand" for="c-41563764">[1 more]</label></div><br/><div class="children"><div class="content">CoT means you&#x27;re adding loops</div><br/></div></div></div></div></div></div><div id="41563500" class="c"><input type="checkbox" id="c-41563500" checked=""/><div class="controls bullet"><span class="by">lsy</span><span>|</span><a href="#41563168">prev</a><span>|</span><a href="#41565634">next</a><span>|</span><label class="collapse" for="c-41563500">[-]</label><label class="expand" for="c-41563500">[3 more]</label></div><br/><div class="children"><div class="content">Note that for the purposes of this paper a “problem” just means a formally decidable problem or a formal language, and the proof is that by creatively arranging transformers you can make individual transformer runs behave like individual Boolean circuits. However, this is a long way from any practical application of transformers: for one thing, most problems we care about are not stated as formal languages, and we already have an exceptionally more efficient way to implement Boolean circuits.</div><br/><div id="41563527" class="c"><input type="checkbox" id="c-41563527" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#41563500">parent</a><span>|</span><a href="#41565634">next</a><span>|</span><label class="collapse" for="c-41563527">[-]</label><label class="expand" for="c-41563527">[2 more]</label></div><br/><div class="children"><div class="content">If a &quot;problem we care about&quot; is not stated as a formal language, does it mean it does not exist in the hierarchy of formal languages? Or is it just as yet unclassified?</div><br/><div id="41565270" class="c"><input type="checkbox" id="c-41565270" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#41563500">root</a><span>|</span><a href="#41563527">parent</a><span>|</span><a href="#41565634">next</a><span>|</span><label class="collapse" for="c-41565270">[-]</label><label class="expand" for="c-41565270">[1 more]</label></div><br/><div class="children"><div class="content">It means that there are two problems: one, to formalize the problem as stated while capturing all relevant details, and two, solving the resulting formal problem. Until you solve problem one, you can&#x27;t use formal methods to say anything about the problem (it&#x27;s not even clear a priori that a problem is even solvable).<p>Unfortunately, the task of a formalizing an informal problem is itself an informal problem that we don&#x27;t know how to formalize, so we can&#x27;t say much about it. So overall, we can&#x27;t say much about how hard the general problem &quot;given a problem statement from a human, solve that problem&quot; is, whether any particular system (including a human!) can solve it and how long that might take with what resources.</div><br/></div></div></div></div></div></div><div id="41565634" class="c"><input type="checkbox" id="c-41565634" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#41563500">prev</a><span>|</span><a href="#41565380">next</a><span>|</span><label class="collapse" for="c-41565634">[-]</label><label class="expand" for="c-41565634">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m waiting for peoples of AI to discover syllogism and inference in its original PROLOG sense, which this CoT abomination basically tries to achieve. Interestingly, if all logical content is translated to rules, and then only rules are fed into the LLM training set, what would the result be, and can the probabilistic magic be made into actually following reason without all the dice.</div><br/></div></div><div id="41565380" class="c"><input type="checkbox" id="c-41565380" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#41565634">prev</a><span>|</span><a href="#41563559">next</a><span>|</span><label class="collapse" for="c-41565380">[-]</label><label class="expand" for="c-41565380">[1 more]</label></div><br/><div class="children"><div class="content">A reply in this twitter thread links to a detailed blog post titled &quot;Universal computation by attention: Running cellular automata and other programs on Claude 3 Opus.&quot; <a href="https:&#x2F;&#x2F;x.com&#x2F;ctjlewis&#x2F;status&#x2F;1786948443472339247" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;ctjlewis&#x2F;status&#x2F;1786948443472339247</a></div><br/></div></div><div id="41563559" class="c"><input type="checkbox" id="c-41563559" checked=""/><div class="controls bullet"><span class="by">glial</span><span>|</span><a href="#41565380">prev</a><span>|</span><a href="#41563231">next</a><span>|</span><label class="collapse" for="c-41563559">[-]</label><label class="expand" for="c-41563559">[7 more]</label></div><br/><div class="children"><div class="content">Apologies if this is a dumb question, but aren&#x27;t all computations inherently serial? In that a Turing machine performs operations serially?</div><br/><div id="41563880" class="c"><input type="checkbox" id="c-41563880" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#41563559">parent</a><span>|</span><a href="#41565376">next</a><span>|</span><label class="collapse" for="c-41563880">[-]</label><label class="expand" for="c-41563880">[3 more]</label></div><br/><div class="children"><div class="content"><i>Aren&#x27;t all computations inherently serial?</i><p>No. &quot;inherently serial&quot; refers to problems that are specified serially and can&#x27;t be spend up by parallel processing. The sum of a set of N numbers is an example of a problem that is not inherently serial. You can use parallel reduction to perform the computation in O(log(N)) time on an idealized parallel computer but it takes O(N) time on an idealized serial computer.<p>And, it turns, exactly which problems are really are inherently serial is somewhat challenging problem.</div><br/><div id="41563965" class="c"><input type="checkbox" id="c-41563965" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41563559">root</a><span>|</span><a href="#41563880">parent</a><span>|</span><a href="#41565376">next</a><span>|</span><label class="collapse" for="c-41563965">[-]</label><label class="expand" for="c-41563965">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The sum of a set of N numbers is an example of a problem that is not inherently serial.<p>But addition with floats (not reals) is non associative.</div><br/><div id="41564522" class="c"><input type="checkbox" id="c-41564522" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41563559">root</a><span>|</span><a href="#41563965">parent</a><span>|</span><a href="#41565376">next</a><span>|</span><label class="collapse" for="c-41564522">[-]</label><label class="expand" for="c-41564522">[1 more]</label></div><br/><div class="children"><div class="content">They didn&#x27;t say floats, and the sum of a set of floats is not uniquely defined as a float for the rain you stated, at least not without specifying a rounding mode. Most people use &quot;round to whatever my naïve code happens to do&quot; which has many correct answers. To add up a set of floats with only the usual 0.5ULP imprecision, yes, isn&#x27;t trivial.</div><br/></div></div></div></div></div></div><div id="41565376" class="c"><input type="checkbox" id="c-41565376" checked=""/><div class="controls bullet"><span class="by">tromp</span><span>|</span><a href="#41563559">parent</a><span>|</span><a href="#41563880">prev</a><span>|</span><a href="#41563677">next</a><span>|</span><label class="collapse" for="c-41565376">[-]</label><label class="expand" for="c-41565376">[1 more]</label></div><br/><div class="children"><div class="content">Turing Machines are just one of many computational models.
Others offer more parallelism. Two examples:<p>In lambda calculus, disjoint redexes can be reduced in parallel.<p>And in interaction nets, all active pairs can be reduced in parallel [1].<p>[]1 <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Interaction_nets" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Interaction_nets</a></div><br/></div></div><div id="41563677" class="c"><input type="checkbox" id="c-41563677" checked=""/><div class="controls bullet"><span class="by">ants_everywhere</span><span>|</span><a href="#41563559">parent</a><span>|</span><a href="#41565376">prev</a><span>|</span><a href="#41563660">next</a><span>|</span><label class="collapse" for="c-41563677">[-]</label><label class="expand" for="c-41563677">[1 more]</label></div><br/><div class="children"><div class="content">You can model parallel computation by an arbitrary finite product of Turing machines. And then, yes, you can simulate that product on a single Turing machine. I think that&#x27;s the sort of thing you have in mind?<p>But I&#x27;m not aware of what &quot;inherently serial&quot; means. The right idea likely involves talking about complexity classes. E.g. how efficiently does a single Turing machine simulate a product of Turing machines? An inherently serial computation would then be something like a problem where the simulation is significantly slower than running the machines in parallel.</div><br/></div></div><div id="41563660" class="c"><input type="checkbox" id="c-41563660" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#41563559">parent</a><span>|</span><a href="#41563677">prev</a><span>|</span><a href="#41563231">next</a><span>|</span><label class="collapse" for="c-41563660">[-]</label><label class="expand" for="c-41563660">[1 more]</label></div><br/><div class="children"><div class="content">Yeah it&#x27;s talking about a new feature for LLMs where the output of an LLM is fed back in as input and done again and again and again and this produces way more accurate output.</div><br/></div></div></div></div><div id="41563231" class="c"><input type="checkbox" id="c-41563231" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#41563559">prev</a><span>|</span><label class="collapse" for="c-41563231">[-]</label><label class="expand" for="c-41563231">[1 more]</label></div><br/><div class="children"><div class="content">Forget UBI, we&#x27;re going to need Universal Basic Compute.</div><br/></div></div></div></div></div></div></div></body></html>