<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737018071587" as="style"/><link rel="stylesheet" href="styles.css?v=1737018071587"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2501.00663">Titans: Learning to Memorize at Test Time</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>bicepjai</span> | <span>10 comments</span></div><br/><div><div id="42722887" class="c"><input type="checkbox" id="c-42722887" checked=""/><div class="controls bullet"><span class="by">Ratelman</span><span>|</span><a href="#42722722">next</a><span>|</span><label class="collapse" for="c-42722887">[-]</label><label class="expand" for="c-42722887">[1 more]</label></div><br/><div class="children"><div class="content">So Minimax just &quot;open-sourced&quot; (I add it in &quot;&quot; because they have a custom license for its use and I&#x27;ve not read through that) but they have context length of 4-million tokens and it scored 100% on the needle in a haystack problem. It uses lightning attention - so still attention, just a variation? So this is potentially not as groundbreaking as the publishers of the paper hoped or am I missing something fundamental here? Can this scale better? Does it train more efficiently? The test-time inference is amazing - is that what sets this apart and not necessarily the long context capability? Will it hallucinate a lot less because it stores long-term memory more efficiently and thus won&#x27;t make up facts but rather use what it has remembered in context?</div><br/></div></div><div id="42722722" class="c"><input type="checkbox" id="c-42722722" checked=""/><div class="controls bullet"><span class="by">suninsight</span><span>|</span><a href="#42722887">prev</a><span>|</span><a href="#42722645">next</a><span>|</span><label class="collapse" for="c-42722722">[-]</label><label class="expand" for="c-42722722">[2 more]</label></div><br/><div class="children"><div class="content">Key questions:<p>1. The key data point seems to be Figure 6a. Where it compares performance on BABILong and claims Titans performance is at ~62%, as compared to GPT-4o-mini at ~42% for 100k sequence length.<p>However, GPT-4o and Claude are missing in this comparison - maybe because they perform better ?<p>2. There is no example provided of the Neural Memory Module in action. This is the first question I would ask of this paper.</div><br/><div id="42722932" class="c"><input type="checkbox" id="c-42722932" checked=""/><div class="controls bullet"><span class="by">tigershark</span><span>|</span><a href="#42722722">parent</a><span>|</span><a href="#42722645">next</a><span>|</span><label class="collapse" for="c-42722932">[-]</label><label class="expand" for="c-42722932">[1 more]</label></div><br/><div class="children"><div class="content">The biggest model that they have used has only 760M parameters, and it outperforms models 1 order of magnitude larger.</div><br/></div></div></div></div><div id="42722645" class="c"><input type="checkbox" id="c-42722645" checked=""/><div class="controls bullet"><span class="by">amai</span><span>|</span><a href="#42722722">prev</a><span>|</span><a href="#42722291">next</a><span>|</span><label class="collapse" for="c-42722645">[-]</label><label class="expand" for="c-42722645">[2 more]</label></div><br/><div class="children"><div class="content">I wonder why the authors felt they need to use drop caps in this paper. It is a distraction and seems to value style over content.</div><br/><div id="42722824" class="c"><input type="checkbox" id="c-42722824" checked=""/><div class="controls bullet"><span class="by">331c8c71</span><span>|</span><a href="#42722645">parent</a><span>|</span><a href="#42722291">next</a><span>|</span><label class="collapse" for="c-42722824">[-]</label><label class="expand" for="c-42722824">[1 more]</label></div><br/><div class="children"><div class="content">They were not so secretely hoping their paper&#x27;s gonna go directly in history:) One could check the other papers by the authors to verify.</div><br/></div></div></div></div><div id="42722291" class="c"><input type="checkbox" id="c-42722291" checked=""/><div class="controls bullet"><span class="by">groceryheist</span><span>|</span><a href="#42722645">prev</a><span>|</span><label class="collapse" for="c-42722291">[-]</label><label class="expand" for="c-42722291">[4 more]</label></div><br/><div class="children"><div class="content">Is it just me, or does this seem like big news?</div><br/><div id="42722476" class="c"><input type="checkbox" id="c-42722476" checked=""/><div class="controls bullet"><span class="by">331c8c71</span><span>|</span><a href="#42722291">parent</a><span>|</span><a href="#42722716">prev</a><span>|</span><a href="#42722305">next</a><span>|</span><label class="collapse" for="c-42722476">[-]</label><label class="expand" for="c-42722476">[1 more]</label></div><br/><div class="children"><div class="content">Same here (seen it yesterday) but I haven&#x27;t parsed the technicals so far tbh.</div><br/></div></div><div id="42722305" class="c"><input type="checkbox" id="c-42722305" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#42722291">parent</a><span>|</span><a href="#42722476">prev</a><span>|</span><label class="collapse" for="c-42722305">[-]</label><label class="expand" for="c-42722305">[1 more]</label></div><br/><div class="children"><div class="content">A lot of ML papers that sound revolutionary end up being duds</div><br/></div></div></div></div></div></div></div></div></div></body></html>