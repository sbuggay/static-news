<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704704462120" as="style"/><link rel="stylesheet" href="styles.css?v=1704704462120"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://apps.apple.com/us/app/offline-chat-private-ai/id6474077941">I made an app that runs Mistral 7B 0.2 LLM locally on iPhone Pros</a> <span class="domain">(<a href="https://apps.apple.com">apps.apple.com</a>)</span></div><div class="subtext"><span>winstonschen</span> | <span>124 comments</span></div><br/><div><div id="38907422" class="c"><input type="checkbox" id="c-38907422" checked=""/><div class="controls bullet"><span class="by">alekseiprokopev</span><span>|</span><a href="#38907062">next</a><span>|</span><label class="collapse" for="c-38907422">[-]</label><label class="expand" for="c-38907422">[7 more]</label></div><br/><div class="children"><div class="content">Here is how to do that on Android: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;#android">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;#android</a></div><br/><div id="38907804" class="c"><input type="checkbox" id="c-38907804" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#38907422">parent</a><span>|</span><a href="#38907062">next</a><span>|</span><label class="collapse" for="c-38907804">[-]</label><label class="expand" for="c-38907804">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think running raw llama.cpp under termux in a shell on your phone, after downloading and compiling it from scratch,, is really comparable to &#x27;I made an app&#x27;.</div><br/><div id="38908103" class="c"><input type="checkbox" id="c-38908103" checked=""/><div class="controls bullet"><span class="by">halJordan</span><span>|</span><a href="#38907422">root</a><span>|</span><a href="#38907804">parent</a><span>|</span><a href="#38908452">prev</a><span>|</span><a href="#38907062">next</a><span>|</span><label class="collapse" for="c-38908103">[-]</label><label class="expand" for="c-38908103">[3 more]</label></div><br/><div class="children"><div class="content">Shh we&#x27;re in our &quot;Android is always better&quot; era</div><br/><div id="38908431" class="c"><input type="checkbox" id="c-38908431" checked=""/><div class="controls bullet"><span class="by">enticeing</span><span>|</span><a href="#38907422">root</a><span>|</span><a href="#38908103">parent</a><span>|</span><a href="#38909482">next</a><span>|</span><label class="collapse" for="c-38908431">[-]</label><label class="expand" for="c-38908431">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t read GP comment as saying &quot;Android is always better&quot;, unless there&#x27;s something I missed?<p>More options is a good thing, nobody said a manual process was better than an app.</div><br/></div></div><div id="38909482" class="c"><input type="checkbox" id="c-38909482" checked=""/><div class="controls bullet"><span class="by">_andrei_</span><span>|</span><a href="#38907422">root</a><span>|</span><a href="#38908103">parent</a><span>|</span><a href="#38908431">prev</a><span>|</span><a href="#38907062">next</a><span>|</span><label class="collapse" for="c-38909482">[-]</label><label class="expand" for="c-38909482">[1 more]</label></div><br/><div class="children"><div class="content">What we&#x27;re seeing here might be classic case of the iOS Freedom Choking Syndrome: when a device&#x27;s lack of freedom spreads to its owner and chokes cerebral circulation.</div><br/></div></div></div></div></div></div></div></div><div id="38907062" class="c"><input type="checkbox" id="c-38907062" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38907422">prev</a><span>|</span><a href="#38907384">next</a><span>|</span><label class="collapse" for="c-38907062">[-]</label><label class="expand" for="c-38907062">[16 more]</label></div><br/><div class="children"><div class="content">Does it save all conversations and let me revisit them later?<p>I use MLC Chat to run Mistral 7B on my iPhone at the moment, but the lack of conversation history is a real nuisance: <a href="https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;mlc-chat&#x2F;id6448482937" rel="nofollow">https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;mlc-chat&#x2F;id6448482937</a></div><br/><div id="38907363" class="c"><input type="checkbox" id="c-38907363" checked=""/><div class="controls bullet"><span class="by">brittlewis12</span><span>|</span><a href="#38907062">parent</a><span>|</span><a href="#38908454">next</a><span>|</span><label class="collapse" for="c-38907363">[-]</label><label class="expand" for="c-38907363">[8 more]</label></div><br/><div class="children"><div class="content">you can absolutely access and continue all your past chats in cnvrs!<p>would love to hear what you think:
<a href="https:&#x2F;&#x2F;testflight.apple.com&#x2F;join&#x2F;ERFxInZg" rel="nofollow">https:&#x2F;&#x2F;testflight.apple.com&#x2F;join&#x2F;ERFxInZg</a></div><br/><div id="38908113" class="c"><input type="checkbox" id="c-38908113" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38907363">parent</a><span>|</span><a href="#38908552">next</a><span>|</span><label class="collapse" for="c-38908113">[-]</label><label class="expand" for="c-38908113">[5 more]</label></div><br/><div class="children"><div class="content">EDIT: Attempting to converse with any Q4_K_M 7B parameter model on a 15 Pro Max... the phone just melts down. It feels like it is producing about one token per minute. MLC-Chat can handle 7B parameter models just fine even on a 14 Pro Max, which has less RAM, so I think there is an issue here.<p>EDIT 2: Even using StableLM, I am experiencing a total crash of the app fairly consistently if I chat in one conversation, then start a new conversation and try to chat in that. On a related note, since chat history is saved... I don&#x27;t think it&#x27;s necessary to have a confirmation prompt if the user clicks the &quot;new chat&quot; shortcut in the top right of a chat.<p>-----<p>That does seem much nicer than MLC Chat. I really like the selection of models and saving of conversations.<p>It looks like you’re still using the old version of TinyLlama. The 1.0 release is out now: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;TinyLlama-1.1B-Chat-v1.0-GGUF" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;TinyLlama-1.1B-Chat-v1.0-GGU...</a><p>Microsoft recently re-licensed Phi-2 to be MIT instead of non-commercial, so I would love to see that in the list of models. Similarly, there is a Dolphin-Phi fine tune.<p>The topic of discussion here is Mistral-7B v0.2, which is also missing from the model list, unfortunately. There are a few Mistral fine tunes in the list, but obviously not the same thing.<p>I also wish I could enable performance metrics to see how many tokens&#x2F;sec the model was running at after each message, and to see how much RAM is being used.<p>On the whole, this app seems really nice!</div><br/><div id="38908393" class="c"><input type="checkbox" id="c-38908393" checked=""/><div class="controls bullet"><span class="by">brittlewis12</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38908113">parent</a><span>|</span><a href="#38908552">next</a><span>|</span><label class="collapse" for="c-38908393">[-]</label><label class="expand" for="c-38908393">[4 more]</label></div><br/><div class="children"><div class="content">Wow, thanks so much for taking the time to test it out and share such great feedback!<p>Thrilled about all those developments! More model options as well as link-based GGUF downloads on the way.<p>On the 7b models: I’m very sorry for the poor experience. I wouldn’t recommend 7b over Q2_K at the moment, unless you’re on a 16GB iPad (or an Apple Silicon Mac!). This needs to be much clearer, as you observed the consequences can be severe. The larger models, and even 3b Q6_K can be crash prone due to memory pressure. Will work on improve handling of low level out-of-memory errors very soon.<p>Will also investigate the StableLM crashes, I’m sorry about that! Hopefully Testflight recorded a trace. Just speculating, it may be a similar issue to the larger models, due to the higher-fidelity quant (Q6_K) combined with the context length eventually running out of RAM. Could you give the Q4_K_M a shot? I heard something similar from a friend yesterday, I’m curious if you have a better time with that — perhaps that’s a more sensible default.<p>Re: the overly-protective new chat alert, I agree, thanks for the suggestion. I’ll incorporate that into the next build. Can I credit you? Let me know how you’d like for me to refer to you, and I’d be happy to.<p>Finally, please feel free to email me any further feedback, and thanks again for your time and consideration!<p>britt [at] bl3 [dot] dev</div><br/><div id="38908421" class="c"><input type="checkbox" id="c-38908421" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38908393">parent</a><span>|</span><a href="#38908607">next</a><span>|</span><label class="collapse" for="c-38908421">[-]</label><label class="expand" for="c-38908421">[1 more]</label></div><br/><div class="children"><div class="content">I just checked and MLC Chat is running the 3-bit quantized version of Mistral-7B. It works fine on the 14 Pro Max (6GB RAM) without crashing, and is able to stay resident in memory on the 15 Pro Max (8GB RAM) when switching with another not-too-heavy app. 2-bit quantization just feels like a step too far, but I’ll give it a try.<p>Regarding credit, I definitely don’t need any. Just happy to see someone working on a better LLM app!</div><br/></div></div><div id="38908607" class="c"><input type="checkbox" id="c-38908607" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38908393">parent</a><span>|</span><a href="#38908421">prev</a><span>|</span><a href="#38908552">next</a><span>|</span><label class="collapse" for="c-38908607">[-]</label><label class="expand" for="c-38908607">[2 more]</label></div><br/><div class="children"><div class="content">4-bit StableLM and 2-bit 7B models do seem to be working more consistently.</div><br/><div id="38908797" class="c"><input type="checkbox" id="c-38908797" checked=""/><div class="controls bullet"><span class="by">brittlewis12</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38908607">parent</a><span>|</span><a href="#38908552">next</a><span>|</span><label class="collapse" for="c-38908797">[-]</label><label class="expand" for="c-38908797">[1 more]</label></div><br/><div class="children"><div class="content">That’s great to hear. I’m sorry again about that poor experience, and please do reach out if you have any other feedback!<p>Britt</div><br/></div></div></div></div></div></div></div></div><div id="38908552" class="c"><input type="checkbox" id="c-38908552" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38907363">parent</a><span>|</span><a href="#38908113">prev</a><span>|</span><a href="#38908454">next</a><span>|</span><label class="collapse" for="c-38908552">[-]</label><label class="expand" for="c-38908552">[2 more]</label></div><br/><div class="children"><div class="content">My free &#x2F; mostly open source app also stores conversation history, synced via iCloud<p><a href="https:&#x2F;&#x2F;ChatOnMac.com" rel="nofollow">https:&#x2F;&#x2F;ChatOnMac.com</a><p>edit: I can&#x27;t reply to you below: Do you have the right app, there&#x27;s no TestFlight just App Store link - if it&#x27;s ChatOnMac then it should have a dropdown at the top of the chat room to select a model. If it&#x27;s empty or otherwise bugged out please let me know what you see in the top menu. It filters the available model presets based on how much RAM you have available, so let me know what specific device you have and I can look into it. Thank you.<p>The model presets are also configurable by forking the bot and loading your own via GitHub (bots run inside sandboxed hidden webviews inside the app). But this is not ergonomically friendly just yet.</div><br/><div id="38908623" class="c"><input type="checkbox" id="c-38908623" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38908552">parent</a><span>|</span><a href="#38908454">next</a><span>|</span><label class="collapse" for="c-38908623">[-]</label><label class="expand" for="c-38908623">[1 more]</label></div><br/><div class="children"><div class="content">Hey I tried the TestFlight.  What are the steps after a fresh download for hooking it up to model?<p>I saw you can spec an OpenAI key but presume it would take llama or something else.</div><br/></div></div></div></div></div></div><div id="38908454" class="c"><input type="checkbox" id="c-38908454" checked=""/><div class="controls bullet"><span class="by">vages</span><span>|</span><a href="#38907062">parent</a><span>|</span><a href="#38907363">prev</a><span>|</span><a href="#38908714">next</a><span>|</span><label class="collapse" for="c-38908454">[-]</label><label class="expand" for="c-38908454">[2 more]</label></div><br/><div class="children"><div class="content">No. Source: bought the app.</div><br/><div id="38908762" class="c"><input type="checkbox" id="c-38908762" checked=""/><div class="controls bullet"><span class="by">nipponese</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38908454">parent</a><span>|</span><a href="#38908714">next</a><span>|</span><label class="collapse" for="c-38908762">[-]</label><label class="expand" for="c-38908762">[1 more]</label></div><br/><div class="children"><div class="content">Also, new chat blows away previous chat.</div><br/></div></div></div></div><div id="38908714" class="c"><input type="checkbox" id="c-38908714" checked=""/><div class="controls bullet"><span class="by">Applemoi</span><span>|</span><a href="#38907062">parent</a><span>|</span><a href="#38908454">prev</a><span>|</span><a href="#38907531">next</a><span>|</span><label class="collapse" for="c-38908714">[-]</label><label class="expand" for="c-38908714">[1 more]</label></div><br/><div class="children"><div class="content">You have conversation history and you can use your own API Key on this app that supports GPT-4 Turbo, Claude 2.1, Gemini Pro and DALL-E 3. I’m adding mistral soon! 
<a href="https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;pal-ai-chat-client&#x2F;id6447545085" rel="nofollow">https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;pal-ai-chat-client&#x2F;id644754508...</a></div><br/></div></div><div id="38907531" class="c"><input type="checkbox" id="c-38907531" checked=""/><div class="controls bullet"><span class="by">jrvarela56</span><span>|</span><a href="#38907062">parent</a><span>|</span><a href="#38908714">prev</a><span>|</span><a href="#38907649">next</a><span>|</span><label class="collapse" for="c-38907531">[-]</label><label class="expand" for="c-38907531">[3 more]</label></div><br/><div class="children"><div class="content">In your experience, how could these local LLMs become snappier than using streamed API calls? How far are they if not? How soon do you guess they’ll get there?<p>I understand the motivation includes factors other than performance, I’m just curious about performance as it applies to UX.</div><br/><div id="38908767" class="c"><input type="checkbox" id="c-38908767" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38907531">parent</a><span>|</span><a href="#38908074">next</a><span>|</span><label class="collapse" for="c-38908767">[-]</label><label class="expand" for="c-38908767">[1 more]</label></div><br/><div class="children"><div class="content">Honestly I think being able to run any kind of LLM on a phone is a miracle. I&#x27;m astonished at how good (and how fast) Mistral 7B runs under MLC Chat on iOS, considering the constraints of the device.<p>I don&#x27;t use it as more than a cool demo though, because the large hosted LLMs (I tend to mostly use GPT-4) are massively more powerful.<p>But... I&#x27;m still intrigued at the idea of a local, slow LLM on my phone enhanced with function calling capabilities, and maybe usable for RAG against private data.<p>The rate of improvement in these smaller models over the past 6 months has been incredible. We may well find useful applications for them even despite their weaknesses compared to GPT-4 etc.</div><br/></div></div><div id="38908074" class="c"><input type="checkbox" id="c-38908074" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38907062">root</a><span>|</span><a href="#38907531">parent</a><span>|</span><a href="#38908767">prev</a><span>|</span><a href="#38907649">next</a><span>|</span><label class="collapse" for="c-38908074">[-]</label><label class="expand" for="c-38908074">[1 more]</label></div><br/><div class="children"><div class="content">What does snappier even mean in this context? The latency from connecting to a server over most network connections isn’t really noticeable when talking about text generation. If the server with a beefy datacenter-class GPU were running the same Mistral you can run on your phone, it would be spitting out hundreds of tokens per second. Most responses would appear on your screen before you blink.<p>There is no expectation that phones will ever be comparable in performance for LLMs.<p>Mistral runs at a decent clip on phones, but we’re talking like 11 tokens per second, not hundreds of tokens per second.<p>Server-based models tend to be only slightly faster than Mistral on my phone because they’re usually running <i>much</i> larger, much more accurate&#x2F;useful models. Models which currently can’t fit onto phones.<p>Running models locally is not motivated by performance, except if you’re in places without reliable internet.</div><br/></div></div></div></div><div id="38907649" class="c"><input type="checkbox" id="c-38907649" checked=""/><div class="controls bullet"><span class="by">winstonschen</span><span>|</span><a href="#38907062">parent</a><span>|</span><a href="#38907531">prev</a><span>|</span><a href="#38907384">next</a><span>|</span><label class="collapse" for="c-38907649">[-]</label><label class="expand" for="c-38907649">[1 more]</label></div><br/><div class="children"><div class="content">Saving convos will be added in the next update.</div><br/></div></div></div></div><div id="38907384" class="c"><input type="checkbox" id="c-38907384" checked=""/><div class="controls bullet"><span class="by">geuis</span><span>|</span><a href="#38907062">prev</a><span>|</span><a href="#38908240">next</a><span>|</span><label class="collapse" for="c-38907384">[-]</label><label class="expand" for="c-38907384">[20 more]</label></div><br/><div class="children"><div class="content">I have a 2020 16in MacBook Pro. I think it&#x27;s the last generation of Intel chips. I&#x27;ve been struggling to get some of the LLM models like Mixtral to run on it.<p>I hate the idea of needing to buy another $3k laptop less than 4 years after spending that much on my current machine. But if I want to get serious about developing non-chatgpt services, do I need a new M2 or M3 chip to get this stuff running locally?</div><br/><div id="38907518" class="c"><input type="checkbox" id="c-38907518" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#38907384">parent</a><span>|</span><a href="#38907506">next</a><span>|</span><label class="collapse" for="c-38907518">[-]</label><label class="expand" for="c-38907518">[4 more]</label></div><br/><div class="children"><div class="content">We should be happy that compute is once again improving and machines are getting outdated rapidly. Which is better - a world where your laptop is competitive for 5+ years but everything stays the same? Or one where entire new realms of advancement open up every 18 months?<p>It’s a no contest option 2 for me.<p>Just use llama.cpp with any of the available UIs. It will be usable with 4 but quantization on CPU. You can use any of the “Q4_M” “GGUF” models that TheBloke puts out on Huggingface.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a><p>UI projects in description.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke</a><p>A closed source option is LMStudio.<p><a href="https:&#x2F;&#x2F;lmstudio.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmstudio.ai&#x2F;</a></div><br/><div id="38907871" class="c"><input type="checkbox" id="c-38907871" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907518">parent</a><span>|</span><a href="#38907671">next</a><span>|</span><label class="collapse" for="c-38907871">[-]</label><label class="expand" for="c-38907871">[1 more]</label></div><br/><div class="children"><div class="content">“New realms of advancement” could open up because of faster computation algorithms. Those hypothetical scenarios don’t have to be mutually exclusive.</div><br/></div></div><div id="38907671" class="c"><input type="checkbox" id="c-38907671" checked=""/><div class="controls bullet"><span class="by">seanvelasco</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907518">parent</a><span>|</span><a href="#38907871">prev</a><span>|</span><a href="#38907506">next</a><span>|</span><label class="collapse" for="c-38907671">[-]</label><label class="expand" for="c-38907671">[2 more]</label></div><br/><div class="children"><div class="content">i love this perspective! makes me really happy of the advancements going around, and not feel sad about my macbook m1 getting old</div><br/></div></div></div></div><div id="38907506" class="c"><input type="checkbox" id="c-38907506" checked=""/><div class="controls bullet"><span class="by">jey</span><span>|</span><a href="#38907384">parent</a><span>|</span><a href="#38907518">prev</a><span>|</span><a href="#38909279">next</a><span>|</span><label class="collapse" for="c-38907506">[-]</label><label class="expand" for="c-38907506">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d suggest using a cloud VM with a GPU attached. For normal stuff like LLM inference, I just rent an instance with a small (cheap) GPU. But when I need to do something more exotic like train an image model from scratch, I can temporarily spin up a cluster that has high-end expensive A100s. This way I don&#x27;t have to invest in expensive hardware like an M3 that can still only do a small part of the full range.</div><br/></div></div><div id="38909279" class="c"><input type="checkbox" id="c-38909279" checked=""/><div class="controls bullet"><span class="by">iepathos</span><span>|</span><a href="#38907384">parent</a><span>|</span><a href="#38907506">prev</a><span>|</span><a href="#38907704">next</a><span>|</span><label class="collapse" for="c-38909279">[-]</label><label class="expand" for="c-38909279">[1 more]</label></div><br/><div class="children"><div class="content">ollamma <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;">https:&#x2F;&#x2F;ollama.ai&#x2F;</a> is popular choice for running local llm models and should work fine on intel.  It&#x27;s just wrapping docker so shouldn&#x27;t require m2&#x2F;m3.</div><br/></div></div><div id="38907704" class="c"><input type="checkbox" id="c-38907704" checked=""/><div class="controls bullet"><span class="by">elzbardico</span><span>|</span><a href="#38907384">parent</a><span>|</span><a href="#38909279">prev</a><span>|</span><a href="#38908185">next</a><span>|</span><label class="collapse" for="c-38907704">[-]</label><label class="expand" for="c-38907704">[1 more]</label></div><br/><div class="children"><div class="content">You can do a lot with either a VM instance with a GPU or within google collab. If you are just starting and doing this stuff mostly a few hours a week, I&#x27;d recommend going that way for a while.</div><br/></div></div><div id="38908185" class="c"><input type="checkbox" id="c-38908185" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#38907384">parent</a><span>|</span><a href="#38907704">prev</a><span>|</span><a href="#38907596">next</a><span>|</span><label class="collapse" for="c-38908185">[-]</label><label class="expand" for="c-38908185">[1 more]</label></div><br/><div class="children"><div class="content">An external thunderbolt gpu should work with an Intel MacBook Pro</div><br/></div></div><div id="38907596" class="c"><input type="checkbox" id="c-38907596" checked=""/><div class="controls bullet"><span class="by">K0balt</span><span>|</span><a href="#38907384">parent</a><span>|</span><a href="#38908185">prev</a><span>|</span><a href="#38907435">next</a><span>|</span><label class="collapse" for="c-38907596">[-]</label><label class="expand" for="c-38907596">[1 more]</label></div><br/><div class="children"><div class="content">If you want to run local, I’d get an m2 with 64gb of ram. That will enable you to run 30b models and mixtral 7bx8 . You need around 50gb to run those at 5&#x2F;6 bit quant.<p>I’m getting about 20 tokens&#x2F;second on my 64gb  m2 mbp with mixtral 5-k-m gguf in llamacpp using text generation webui., 35? Layers being sent to metal for acceleration.<p>I’m really pleased with the performance compared to my dual 3090 desktop rig, the mbp is actually faster.</div><br/></div></div><div id="38907435" class="c"><input type="checkbox" id="c-38907435" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#38907384">parent</a><span>|</span><a href="#38907596">prev</a><span>|</span><a href="#38907408">next</a><span>|</span><label class="collapse" for="c-38907435">[-]</label><label class="expand" for="c-38907435">[3 more]</label></div><br/><div class="children"><div class="content">On your CPU, you should be able to leverage the same AVX acceleration used on Linux and Windows machines. It&#x27;s not going to make any GPU owners envious, but it might be enough to keep you satisfied with your current hardware.</div><br/><div id="38907913" class="c"><input type="checkbox" id="c-38907913" checked=""/><div class="controls bullet"><span class="by">ace2358</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907435">parent</a><span>|</span><a href="#38907408">next</a><span>|</span><label class="collapse" for="c-38907913">[-]</label><label class="expand" for="c-38907913">[2 more]</label></div><br/><div class="children"><div class="content">AVX code on laptop cooling sounds like it could be even slower! I don’t miss the heat from an intel laptop!</div><br/><div id="38907931" class="c"><input type="checkbox" id="c-38907931" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907913">parent</a><span>|</span><a href="#38907408">next</a><span>|</span><label class="collapse" for="c-38907931">[-]</label><label class="expand" for="c-38907931">[1 more]</label></div><br/><div class="children"><div class="content">It runs faster and cooler than the software-accelerated alternative. Probably cooler than my 3070 too, my laptop sat ~50c when using AVX to generate Stable Diffusion Turbo images.</div><br/></div></div></div></div></div></div><div id="38907408" class="c"><input type="checkbox" id="c-38907408" checked=""/><div class="controls bullet"><span class="by">muricula</span><span>|</span><a href="#38907384">parent</a><span>|</span><a href="#38907435">prev</a><span>|</span><a href="#38908240">next</a><span>|</span><label class="collapse" for="c-38907408">[-]</label><label class="expand" for="c-38907408">[7 more]</label></div><br/><div class="children"><div class="content">Does your mac support an external GPU? A mid to high end nvidia card may or may not outperform the M3 GPU at a lower or similar price. You can also stick it in a PC or resell it separately.</div><br/><div id="38907421" class="c"><input type="checkbox" id="c-38907421" checked=""/><div class="controls bullet"><span class="by">xfitm3</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907408">parent</a><span>|</span><a href="#38907525">next</a><span>|</span><label class="collapse" for="c-38907421">[-]</label><label class="expand" for="c-38907421">[1 more]</label></div><br/><div class="children"><div class="content">Do you recommend any specific external GPU? I had one from Black Magic, it was not that great performance wise.</div><br/></div></div><div id="38907525" class="c"><input type="checkbox" id="c-38907525" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907408">parent</a><span>|</span><a href="#38907421">prev</a><span>|</span><a href="#38907613">next</a><span>|</span><label class="collapse" for="c-38907525">[-]</label><label class="expand" for="c-38907525">[2 more]</label></div><br/><div class="children"><div class="content">No Nvidia drivers for MacOS.</div><br/><div id="38907593" class="c"><input type="checkbox" id="c-38907593" checked=""/><div class="controls bullet"><span class="by">lights0123</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907525">parent</a><span>|</span><a href="#38907613">next</a><span>|</span><label class="collapse" for="c-38907593">[-]</label><label class="expand" for="c-38907593">[1 more]</label></div><br/><div class="children"><div class="content">Could dual boot Windows or Linux</div><br/></div></div></div></div><div id="38907613" class="c"><input type="checkbox" id="c-38907613" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907408">parent</a><span>|</span><a href="#38907525">prev</a><span>|</span><a href="#38907617">next</a><span>|</span><label class="collapse" for="c-38907613">[-]</label><label class="expand" for="c-38907613">[2 more]</label></div><br/><div class="children"><div class="content">eGPU isn’t supported on Apple silicon</div><br/><div id="38907664" class="c"><input type="checkbox" id="c-38907664" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907613">parent</a><span>|</span><a href="#38907617">next</a><span>|</span><label class="collapse" for="c-38907664">[-]</label><label class="expand" for="c-38907664">[1 more]</label></div><br/><div class="children"><div class="content">As GP said, the early 2020 MBP had an Intel CPU.</div><br/></div></div></div></div><div id="38907617" class="c"><input type="checkbox" id="c-38907617" checked=""/><div class="controls bullet"><span class="by">K0balt</span><span>|</span><a href="#38907384">root</a><span>|</span><a href="#38907408">parent</a><span>|</span><a href="#38907613">prev</a><span>|</span><a href="#38908240">next</a><span>|</span><label class="collapse" for="c-38907617">[-]</label><label class="expand" for="c-38907617">[1 more]</label></div><br/><div class="children"><div class="content">My 64gb m2 mbp is faster running inference than my dual 3090 desktop rig, and at 64g of unified memory it can hold slightly bigger models than the 48gb of vram of the desktop. The performance of the m2&#x2F;m3 with a big unified memory is very impressive.  Not much difference between m2&#x2F;m3 though, if all other things are the same.</div><br/></div></div></div></div></div></div><div id="38908240" class="c"><input type="checkbox" id="c-38908240" checked=""/><div class="controls bullet"><span class="by">dazzaji</span><span>|</span><a href="#38907384">prev</a><span>|</span><a href="#38907344">next</a><span>|</span><label class="collapse" for="c-38908240">[-]</label><label class="expand" for="c-38908240">[1 more]</label></div><br/><div class="children"><div class="content">I’m intrigued and currently downloading this app. Love the idea of having offline direct access to this model. One small-ish thing though: Looks like the URL for the privacy policy (<a href="http:&#x2F;&#x2F;opusnoma.com&#x2F;privacy" rel="nofollow">http:&#x2F;&#x2F;opusnoma.com&#x2F;privacy</a>) linked from the App Store page goes nowhere. Actually, opusnoma.com is likewise offline.</div><br/></div></div><div id="38907344" class="c"><input type="checkbox" id="c-38907344" checked=""/><div class="controls bullet"><span class="by">TacticalCoder</span><span>|</span><a href="#38908240">prev</a><span>|</span><a href="#38907712">next</a><span>|</span><label class="collapse" for="c-38907344">[-]</label><label class="expand" for="c-38907344">[9 more]</label></div><br/><div class="children"><div class="content">Are these LLMs you can run locally giving answers deterministically just as with, say, StableDiffusion? In StableDiffusion if you reuse the exact same version of SD &#x2F; model and same query and seed, you always get the same result (at least I think so).</div><br/><div id="38907580" class="c"><input type="checkbox" id="c-38907580" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#38907344">parent</a><span>|</span><a href="#38907433">next</a><span>|</span><label class="collapse" for="c-38907580">[-]</label><label class="expand" for="c-38907580">[4 more]</label></div><br/><div class="children"><div class="content">Even with Stable Diffusion, determinism is “best effort”- there are flags you can set in Torch to make it <i>more</i> deterministic at a performance cost, but it’s explicitly disclaimed:<p><a href="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;notes&#x2F;randomness.html" rel="nofollow">https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;notes&#x2F;randomness.html</a></div><br/><div id="38907682" class="c"><input type="checkbox" id="c-38907682" checked=""/><div class="controls bullet"><span class="by">Zetobal</span><span>|</span><a href="#38907344">root</a><span>|</span><a href="#38907580">parent</a><span>|</span><a href="#38907433">next</a><span>|</span><label class="collapse" for="c-38907682">[-]</label><label class="expand" for="c-38907682">[3 more]</label></div><br/><div class="children"><div class="content">The base models of stablediffusion were always deterministic if you use a deterministic noise scheduler...</div><br/><div id="38907863" class="c"><input type="checkbox" id="c-38907863" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#38907344">root</a><span>|</span><a href="#38907682">parent</a><span>|</span><a href="#38907433">next</a><span>|</span><label class="collapse" for="c-38907863">[-]</label><label class="expand" for="c-38907863">[2 more]</label></div><br/><div class="children"><div class="content">I think they’re referring to CUDA (and possibly other similar runtimes) being able to schedule floating point ops non-deterministically, combined with floating point arithmetic being potentially non-associative. I’m not personally sure how big an issue that would be for the output though.</div><br/><div id="38908095" class="c"><input type="checkbox" id="c-38908095" checked=""/><div class="controls bullet"><span class="by">Our_Benefactors</span><span>|</span><a href="#38907344">root</a><span>|</span><a href="#38907863">parent</a><span>|</span><a href="#38907433">next</a><span>|</span><label class="collapse" for="c-38908095">[-]</label><label class="expand" for="c-38908095">[1 more]</label></div><br/><div class="children"><div class="content">Differences in output are generally varying levels of difficulty of “spot the difference” and rarely changes the overall image composition by much. I always use nondeterministic algos and it doesn’t have any affect on my ability to refine prompts effectively.</div><br/></div></div></div></div></div></div></div></div><div id="38907433" class="c"><input type="checkbox" id="c-38907433" checked=""/><div class="controls bullet"><span class="by">tionis</span><span>|</span><a href="#38907344">parent</a><span>|</span><a href="#38907580">prev</a><span>|</span><a href="#38907712">next</a><span>|</span><label class="collapse" for="c-38907433">[-]</label><label class="expand" for="c-38907433">[4 more]</label></div><br/><div class="children"><div class="content">Yes, you can set the temperature to 0, then they should be deterministic.</div><br/><div id="38907512" class="c"><input type="checkbox" id="c-38907512" checked=""/><div class="controls bullet"><span class="by">dilawar</span><span>|</span><a href="#38907344">root</a><span>|</span><a href="#38907433">parent</a><span>|</span><a href="#38907712">next</a><span>|</span><label class="collapse" for="c-38907512">[-]</label><label class="expand" for="c-38907512">[3 more]</label></div><br/><div class="children"><div class="content">Someone mentions temperature in the context of algorithms, can&#x27;t stop thinking, cool, simulated annealing. Haven&#x27;t seen temperature used in any other family of algo before this.</div><br/><div id="38908121" class="c"><input type="checkbox" id="c-38908121" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#38907344">root</a><span>|</span><a href="#38907512">parent</a><span>|</span><a href="#38908038">next</a><span>|</span><label class="collapse" for="c-38908121">[-]</label><label class="expand" for="c-38908121">[1 more]</label></div><br/><div class="children"><div class="content">If you squint, it’s the same thing.  Simulated annealing generally attempts to sample from the Boltzmann distribution.  (Presumably because actual annealing is a thermodynamic thing, and you can often think of annealing in a way that the system is a sample from the Boltzmann distribution.)<p>And softmax is exactly the function that maps energies into the corresponding normalized probabilities under the Boltzmann distribution. And transformers are generally treated as modeling the probabilities of strings, and those probabilities are expressed as energies under the Boltzmann distribution (i.e., logits are on a log scale), and asking your favorite model a question works by sampling from the Boltzmann distribution based on the energies (log probabilities) the model predicts, and you can sample that distribution at any temperature you like.</div><br/></div></div><div id="38908038" class="c"><input type="checkbox" id="c-38908038" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#38907344">root</a><span>|</span><a href="#38907512">parent</a><span>|</span><a href="#38908121">prev</a><span>|</span><a href="#38907712">next</a><span>|</span><label class="collapse" for="c-38908038">[-]</label><label class="expand" for="c-38908038">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m interested, how does LLM temperature relate to simulated annealing?</div><br/></div></div></div></div></div></div></div></div><div id="38907712" class="c"><input type="checkbox" id="c-38907712" checked=""/><div class="controls bullet"><span class="by">johngalt2600</span><span>|</span><a href="#38907344">prev</a><span>|</span><a href="#38907493">next</a><span>|</span><label class="collapse" for="c-38907712">[-]</label><label class="expand" for="c-38907712">[2 more]</label></div><br/><div class="children"><div class="content">Where to leave feedback? I am trying the Mistral dolphin model but getting GGML ASSERT errors referencing Users&#x2F;tito lol (not me). Using iPhone 14 Pro Max.</div><br/><div id="38907750" class="c"><input type="checkbox" id="c-38907750" checked=""/><div class="controls bullet"><span class="by">winstonschen</span><span>|</span><a href="#38907712">parent</a><span>|</span><a href="#38907493">next</a><span>|</span><label class="collapse" for="c-38907750">[-]</label><label class="expand" for="c-38907750">[1 more]</label></div><br/><div class="children"><div class="content">Which app are you trying? The app posted here is Offline Chat, which doesn’t have a choice of model and works fine on iPhone 14 Pro.</div><br/></div></div></div></div><div id="38907493" class="c"><input type="checkbox" id="c-38907493" checked=""/><div class="controls bullet"><span class="by">perryizgr8</span><span>|</span><a href="#38907712">prev</a><span>|</span><a href="#38907197">next</a><span>|</span><label class="collapse" for="c-38907493">[-]</label><label class="expand" for="c-38907493">[5 more]</label></div><br/><div class="children"><div class="content">Are these apps using the neural compute parts of Apple&#x27;s chips? Or ar they just using the regular CPU&#x2F;GPU cores?</div><br/><div id="38907919" class="c"><input type="checkbox" id="c-38907919" checked=""/><div class="controls bullet"><span class="by">brittlewis12</span><span>|</span><a href="#38907493">parent</a><span>|</span><a href="#38907197">next</a><span>|</span><label class="collapse" for="c-38907919">[-]</label><label class="expand" for="c-38907919">[4 more]</label></div><br/><div class="children"><div class="content">TL;DR: No, nearly all these apps will use GPU (via Metal), or CPU, <i>not</i> Neural Engine (ANE).<p>Why? I suggest a few main reasons:
1) No Neural Engine API
2) CoreML has challenges modeling LLMs efficiently right now.
3) Not Enough Benefit (For the Cost... Yet!)<p>This is my best understanding based on my own work and research for a local LLM iOS app. Read on for more in-depth justifications of each point!<p>---<p>1) No Neural Engine API<p>- There is no developer API to use the Neural Engine programmatically, so CoreML is the only way to be able to use it.<p>2) CoreML has challenges modeling LLMs efficiently right now.<p>- Its most-optimized use cases seem tailored for image models, as it works best with fixed input lengths[1][2], which are fairly limiting for general language modeling (are all prompts, sentences and paragraphs, the same number of tokens? do you want to pad all your inputs?).<p>- CoreML features limited support for the leading approaches for compressing LLMs (quantization, whether weights-only or activation-aware). Falcon-7b-instruct (fp32) in CoreML is 27.7GB [3], Llama-2-chat (fp16) is 13.5GB [4] — neither will fit in memory on any currently shipping iPhone. They&#x27;d only barely fit on the newest, highest-end iPad Pros.<p>- HuggingFace‘s swift-transformers[5] is a CoreML-focused library under active development to eventually help developers with many of these problems, in addition to an `exporters` cli tool[6] that wraps Apple&#x27;s `coremltools` for converting PyTorch or other models to CoreML.<p>3) Not Enough Benefit (For the Cost... Yet!)<p>- ANE &amp; GPU (Metal) have access to the same unified memory. They are both subject to the same restrictions on background execution (you simply can&#x27;t use them in the background, or your app is killed[7]).<p>- So the main benefit from unlocking the ANE would be multitasking: running an ML task in parallel with non-ML tasks that might also require the GPU: e.g. SwiftUI Metal Shaders, background audio processing (shoutout Overcast!), screen recording&#x2F;sharing, etc. <i>Absolutely</i> worthwhile to achieve, but for the significant work required and the lack of ecosystem currently around CoreML for LLMs specifically, the benefits become less clear.<p>- Apple&#x27;s hot new ML library, MLX, only uses Metal for GPU[8], just like Llama.cpp. More nuanced differences arise on closer inspection related to MLX&#x27;s focus on unified memory optimizations. So perhaps we can squeeze out some performance from unified memory in Llama.cpp, but CoreML will be the only way to unlock ANE, which is lower priority according to lead maintainer Georgi Gerganov as of late this past summer[9], likely for many of the reasons enumerated above.<p>I&#x27;ve learned most of this while working on my own private LLM inference app, cnvrs[10] — would love to hear your feedback or thoughts!<p>Britt<p>---<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;exporters&#x2F;pull&#x2F;37">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;exporters&#x2F;pull&#x2F;37</a><p>[2] <a href="https:&#x2F;&#x2F;apple.github.io&#x2F;coremltools&#x2F;docs-guides&#x2F;source&#x2F;flexible-inputs.html" rel="nofollow">https:&#x2F;&#x2F;apple.github.io&#x2F;coremltools&#x2F;docs-guides&#x2F;source&#x2F;flexi...</a><p>[3] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;tiiuae&#x2F;falcon-7b-instruct&#x2F;tree&#x2F;main&#x2F;coreml&#x2F;text-generation&#x2F;falcon-7b-64-float32.mlpackage&#x2F;Data&#x2F;com.apple.CoreML&#x2F;weights" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;tiiuae&#x2F;falcon-7b-instruct&#x2F;tree&#x2F;main&#x2F;c...</a><p>[4] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;coreml-projects&#x2F;Llama-2-7b-chat-coreml&#x2F;tree&#x2F;main&#x2F;llama-2-7b-chat.mlpackage&#x2F;Data&#x2F;com.apple.CoreML&#x2F;weights" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;coreml-projects&#x2F;Llama-2-7b-chat-corem...</a><p>[5] <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;swift-transformers">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;swift-transformers</a><p>[6] <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;exporters">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;exporters</a><p>[7] <a href="https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;metal&#x2F;gpu_devices_and_work_submission&#x2F;preparing_your_metal_app_to_run_in_the_background" rel="nofollow">https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;metal&#x2F;gpu_devices_...</a><p>[8] <a href="https:&#x2F;&#x2F;github.com&#x2F;ml-explore&#x2F;mlx&#x2F;issues&#x2F;18">https:&#x2F;&#x2F;github.com&#x2F;ml-explore&#x2F;mlx&#x2F;issues&#x2F;18</a><p>[9] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1714#issuecomment-1693191133">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1714#issuecomm...</a><p>[10] <a href="https:&#x2F;&#x2F;testflight.apple.com&#x2F;join&#x2F;ERFxInZg" rel="nofollow">https:&#x2F;&#x2F;testflight.apple.com&#x2F;join&#x2F;ERFxInZg</a></div><br/><div id="38908463" class="c"><input type="checkbox" id="c-38908463" checked=""/><div class="controls bullet"><span class="by">joeconway</span><span>|</span><a href="#38907493">root</a><span>|</span><a href="#38907919">parent</a><span>|</span><a href="#38909118">next</a><span>|</span><label class="collapse" for="c-38908463">[-]</label><label class="expand" for="c-38908463">[2 more]</label></div><br/><div class="children"><div class="content">This is really interesting, thank you.<p>What would be the downside to padding all inputs to have consistent input token size?</div><br/><div id="38908777" class="c"><input type="checkbox" id="c-38908777" checked=""/><div class="controls bullet"><span class="by">brittlewis12</span><span>|</span><a href="#38907493">root</a><span>|</span><a href="#38908463">parent</a><span>|</span><a href="#38909118">next</a><span>|</span><label class="collapse" for="c-38908777">[-]</label><label class="expand" for="c-38908777">[1 more]</label></div><br/><div class="children"><div class="content">Conceptually, to the best of my understanding, nothing too serious; perhaps the inefficiency of processing a larger input than necessary?<p>Practically, a few things:<p>If you want to have your cake &amp; eat it too, they recommend Enumerated Shapes[1] in their coremltools docs, where CoreML precompiles up to 128 (!) variants of input shapes, but again this is fairly limiting (1 tok, 2 tok, 3 tok... up to 128 token prompts.. maybe you enforce a minimum, say 80 tokens to account for a system prompt, so up to 200 tokens, but... still pretty short). But this is only compatible with CPU inference, so that reduces its appeal.<p>It seems like its current state was designed for text embedding models, where you normalize input length by chunking (often 128 or 256 tokens) and operate on the chunks — and indeed, that’s the only text-based CoreML model that Apple ships today, a Bert embedding model tuned for Q&amp;A[2], not an LLM.<p>You could used a fixed input length that’s fairly large; I haven’t experimented with it once I grasped the memory requirements, but from what I gather from HuggingFace’s announcement blog post[3], it seems that is what they do with swift-transformers &amp; their CoreML conversions, handling the details for you[4][5]. I haven’t carefully investigated the implementation, but I’m curious to learn more!<p>You can be sure that no one is more aware of all this than Apple — they published &quot;Deploying Transformers on the Apple Neural Engine&quot; in June 2022[6]. I look forward to seeing what they cook up for developers at WWDC this year!<p>---<p>[1] &quot;Use `EnumeratedShapes` for best performance. During compilation the model can be optimized on the device for the finite set of input shapes. You can provide up to 128 different shapes.&quot; <a href="https:&#x2F;&#x2F;apple.github.io&#x2F;coremltools&#x2F;docs-guides&#x2F;source&#x2F;flexible-inputs.html#select-from-predetermined-shapes" rel="nofollow">https:&#x2F;&#x2F;apple.github.io&#x2F;coremltools&#x2F;docs-guides&#x2F;source&#x2F;flexi...</a><p>[2] BertSQUAD.mlmodel (fp16) <a href="https:&#x2F;&#x2F;developer.apple.com&#x2F;machine-learning&#x2F;models&#x2F;#text" rel="nofollow">https:&#x2F;&#x2F;developer.apple.com&#x2F;machine-learning&#x2F;models&#x2F;#text</a><p>[3] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;swift-coreml-llm#optimization" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;swift-coreml-llm#optimization</a><p>[4] `use_fixed_shapes` &quot;Retrieve the max sequence length from the model configuration, or use a hardcoded value (currently 128). This can be subclassed to support custom lengths.&quot; <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;exporters&#x2F;pull&#x2F;37&#x2F;files#diff-9efb11cf718ed446ee8267cf92b9821a82990cbd1065fff1622e9c347464e3d3R191">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;exporters&#x2F;pull&#x2F;37&#x2F;files#diff-...</a><p>[5] `use_flexible_shapes` &quot;When True, inputs are allowed to use sequence lengths of `1` up to `maxSequenceLength`. Unfortunately, this currently prevents the model from running on GPU or the Neural Engine. We default to `False`, but this can be overridden in custom configurations.&quot; <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;exporters&#x2F;pull&#x2F;37&#x2F;files#diff-9efb11cf718ed446ee8267cf92b9821a82990cbd1065fff1622e9c347464e3d3R203-R205">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;exporters&#x2F;pull&#x2F;37&#x2F;files#diff-...</a><p>[6] <a href="https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;neural-engine-transformers" rel="nofollow">https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;neural-engine-tra...</a></div><br/></div></div></div></div><div id="38909118" class="c"><input type="checkbox" id="c-38909118" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38907493">root</a><span>|</span><a href="#38907919">parent</a><span>|</span><a href="#38908463">prev</a><span>|</span><a href="#38907197">next</a><span>|</span><label class="collapse" for="c-38909118">[-]</label><label class="expand" for="c-38909118">[1 more]</label></div><br/><div class="children"><div class="content">great high effort answer, thanks so much!<p>to prod you to sell yourself a bit more - what is the goal&#x2F;selling point of cnvrs?</div><br/></div></div></div></div></div></div><div id="38907197" class="c"><input type="checkbox" id="c-38907197" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#38907493">prev</a><span>|</span><a href="#38907780">next</a><span>|</span><label class="collapse" for="c-38907197">[-]</label><label class="expand" for="c-38907197">[3 more]</label></div><br/><div class="children"><div class="content">Some other local LLM iOS apps:<p>- MLC Chat: <a href="https:&#x2F;&#x2F;llm.mlc.ai" rel="nofollow">https:&#x2F;&#x2F;llm.mlc.ai</a><p>- LLM Farm: <a href="https:&#x2F;&#x2F;llmfarm.site" rel="nofollow">https:&#x2F;&#x2F;llmfarm.site</a><p>- Enchanted (not local, just a frontend): <a href="https:&#x2F;&#x2F;github.com&#x2F;AugustDev&#x2F;enchanted">https:&#x2F;&#x2F;github.com&#x2F;AugustDev&#x2F;enchanted</a><p>But I don&#x27;t think any of these support Mistral 0.2 which is a pretty big deal.</div><br/><div id="38907356" class="c"><input type="checkbox" id="c-38907356" checked=""/><div class="controls bullet"><span class="by">stoorafa</span><span>|</span><a href="#38907197">parent</a><span>|</span><a href="#38909110">next</a><span>|</span><label class="collapse" for="c-38907356">[-]</label><label class="expand" for="c-38907356">[1 more]</label></div><br/><div class="children"><div class="content">MLC Chat supports Mistral 0.2 (7B, q3f16_1, it seems)</div><br/></div></div><div id="38909110" class="c"><input type="checkbox" id="c-38909110" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38907197">parent</a><span>|</span><a href="#38907356">prev</a><span>|</span><a href="#38907780">next</a><span>|</span><label class="collapse" for="c-38909110">[-]</label><label class="expand" for="c-38909110">[1 more]</label></div><br/><div class="children"><div class="content">where can i find info about Mistral 0.2? why is it harder to support than Mistral 0.1?</div><br/></div></div></div></div><div id="38907780" class="c"><input type="checkbox" id="c-38907780" checked=""/><div class="controls bullet"><span class="by">etaioinshrdlu</span><span>|</span><a href="#38907197">prev</a><span>|</span><a href="#38907389">next</a><span>|</span><label class="collapse" for="c-38907780">[-]</label><label class="expand" for="c-38907780">[9 more]</label></div><br/><div class="children"><div class="content">Does Apple enforce strict safety and content rules on these types of apps?</div><br/><div id="38908260" class="c"><input type="checkbox" id="c-38908260" checked=""/><div class="controls bullet"><span class="by">winstonschen</span><span>|</span><a href="#38907780">parent</a><span>|</span><a href="#38907965">next</a><span>|</span><label class="collapse" for="c-38908260">[-]</label><label class="expand" for="c-38908260">[1 more]</label></div><br/><div class="children"><div class="content">This app got through review pretty easily, especially since I flagged potentially offensive content which makes it age 12+. In comparison to social media these apps are positively angelic.</div><br/></div></div><div id="38907965" class="c"><input type="checkbox" id="c-38907965" checked=""/><div class="controls bullet"><span class="by">Dig1t</span><span>|</span><a href="#38907780">parent</a><span>|</span><a href="#38908260">prev</a><span>|</span><a href="#38908270">next</a><span>|</span><label class="collapse" for="c-38907965">[-]</label><label class="expand" for="c-38907965">[6 more]</label></div><br/><div class="children"><div class="content">What does that mean exactly? Like your phone won’t print text that says something offensive?</div><br/><div id="38908166" class="c"><input type="checkbox" id="c-38908166" checked=""/><div class="controls bullet"><span class="by">etaioinshrdlu</span><span>|</span><a href="#38907780">root</a><span>|</span><a href="#38907965">parent</a><span>|</span><a href="#38908049">next</a><span>|</span><label class="collapse" for="c-38908166">[-]</label><label class="expand" for="c-38908166">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m concerned Apple won&#x27;t approve your app for the App store if your chat app says something offensive.</div><br/></div></div></div></div><div id="38908270" class="c"><input type="checkbox" id="c-38908270" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#38907780">parent</a><span>|</span><a href="#38907965">prev</a><span>|</span><a href="#38907389">next</a><span>|</span><label class="collapse" for="c-38908270">[-]</label><label class="expand" for="c-38908270">[1 more]</label></div><br/><div class="children"><div class="content">No, just age ratings</div><br/></div></div></div></div><div id="38907389" class="c"><input type="checkbox" id="c-38907389" checked=""/><div class="controls bullet"><span class="by">scosman</span><span>|</span><a href="#38907780">prev</a><span>|</span><a href="#38907382">next</a><span>|</span><label class="collapse" for="c-38907389">[-]</label><label class="expand" for="c-38907389">[2 more]</label></div><br/><div class="children"><div class="content">How did you make it? llama.cpp?</div><br/><div id="38907449" class="c"><input type="checkbox" id="c-38907449" checked=""/><div class="controls bullet"><span class="by">castles</span><span>|</span><a href="#38907389">parent</a><span>|</span><a href="#38907382">next</a><span>|</span><label class="collapse" for="c-38907449">[-]</label><label class="expand" for="c-38907449">[1 more]</label></div><br/><div class="children"><div class="content">almost certainly - edit: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;llama.swiftui">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;...</a></div><br/></div></div></div></div><div id="38907382" class="c"><input type="checkbox" id="c-38907382" checked=""/><div class="controls bullet"><span class="by">lulznews</span><span>|</span><a href="#38907389">prev</a><span>|</span><a href="#38907305">next</a><span>|</span><label class="collapse" for="c-38907382">[-]</label><label class="expand" for="c-38907382">[2 more]</label></div><br/><div class="children"><div class="content">How much disk space does it need?</div><br/><div id="38907400" class="c"><input type="checkbox" id="c-38907400" checked=""/><div class="controls bullet"><span class="by">UnlockedSecrets</span><span>|</span><a href="#38907382">parent</a><span>|</span><a href="#38907305">next</a><span>|</span><label class="collapse" for="c-38907400">[-]</label><label class="expand" for="c-38907400">[1 more]</label></div><br/><div class="children"><div class="content">Size
    3.3 GB<p>source: The page that is linked</div><br/></div></div></div></div><div id="38907305" class="c"><input type="checkbox" id="c-38907305" checked=""/><div class="controls bullet"><span class="by">Firmwarrior</span><span>|</span><a href="#38907382">prev</a><span>|</span><a href="#38907080">next</a><span>|</span><label class="collapse" for="c-38907305">[-]</label><label class="expand" for="c-38907305">[11 more]</label></div><br/><div class="children"><div class="content">Is it weird if I carry a phone with this and a solar charger around at all times, in case I suddenly get hurled back in time?</div><br/><div id="38907359" class="c"><input type="checkbox" id="c-38907359" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#38907305">parent</a><span>|</span><a href="#38907527">next</a><span>|</span><label class="collapse" for="c-38907359">[-]</label><label class="expand" for="c-38907359">[2 more]</label></div><br/><div class="children"><div class="content">A machine that tells you that the Golden State Warriors won the 2012 Stanley Cup by bowling a perfect 300 would be invaluable in 1602</div><br/><div id="38907726" class="c"><input type="checkbox" id="c-38907726" checked=""/><div class="controls bullet"><span class="by">elzbardico</span><span>|</span><a href="#38907305">root</a><span>|</span><a href="#38907359">parent</a><span>|</span><a href="#38907527">next</a><span>|</span><label class="collapse" for="c-38907726">[-]</label><label class="expand" for="c-38907726">[1 more]</label></div><br/><div class="children"><div class="content">In that case no need for a LLM, just a wikipedia dump with a full-text index is enough.</div><br/></div></div></div></div><div id="38907527" class="c"><input type="checkbox" id="c-38907527" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#38907305">parent</a><span>|</span><a href="#38907359">prev</a><span>|</span><a href="#38907519">next</a><span>|</span><label class="collapse" for="c-38907527">[-]</label><label class="expand" for="c-38907527">[1 more]</label></div><br/><div class="children"><div class="content">Wikipedia is going to hallucinate significantly fewer facts than an LLM.</div><br/></div></div><div id="38907519" class="c"><input type="checkbox" id="c-38907519" checked=""/><div class="controls bullet"><span class="by">TN1ck</span><span>|</span><a href="#38907305">parent</a><span>|</span><a href="#38907527">prev</a><span>|</span><a href="#38907565">next</a><span>|</span><label class="collapse" for="c-38907519">[-]</label><label class="expand" for="c-38907519">[4 more]</label></div><br/><div class="children"><div class="content">Also put the latest dump of Wikipedia on your phone (Kiwix is great for this). The complete English one with images is about 100 GB.</div><br/><div id="38907892" class="c"><input type="checkbox" id="c-38907892" checked=""/><div class="controls bullet"><span class="by">peterburkimsher</span><span>|</span><a href="#38907305">root</a><span>|</span><a href="#38907519">parent</a><span>|</span><a href="#38907565">next</a><span>|</span><label class="collapse" for="c-38907892">[-]</label><label class="expand" for="c-38907892">[3 more]</label></div><br/><div class="children"><div class="content">And for older phones, Wiki2Touch is a jailbreak app that does the same without images in about 14 GB.<p>I&#x27;m writing a Wiki2Touch archive viewer for more modern iOS, because my iPhone SE doesn&#x27;t have 100 GB free for Kiwix.</div><br/><div id="38908190" class="c"><input type="checkbox" id="c-38908190" checked=""/><div class="controls bullet"><span class="by">TN1ck</span><span>|</span><a href="#38907305">root</a><span>|</span><a href="#38907892">parent</a><span>|</span><a href="#38907565">next</a><span>|</span><label class="collapse" for="c-38908190">[-]</label><label class="expand" for="c-38908190">[2 more]</label></div><br/><div class="children"><div class="content">Kiwix supports any .zim file, there are plenty of different wikipedia versions available, check <a href="https:&#x2F;&#x2F;download.kiwix.org&#x2F;zim&#x2F;wikipedia&#x2F;?C=S;O=D" rel="nofollow">https:&#x2F;&#x2F;download.kiwix.org&#x2F;zim&#x2F;wikipedia&#x2F;?C=S;O=D</a></div><br/><div id="38908245" class="c"><input type="checkbox" id="c-38908245" checked=""/><div class="controls bullet"><span class="by">peterburkimsher</span><span>|</span><a href="#38907305">root</a><span>|</span><a href="#38908190">parent</a><span>|</span><a href="#38907565">next</a><span>|</span><label class="collapse" for="c-38908245">[-]</label><label class="expand" for="c-38908245">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, but wikipedia_en_all_nopic_2023-12.zim is still 56 GB, whereas the BZ2-compressed Wiki2Touch archives are only about 14 GB for the latest (and only 8 GB for an archive from 2012 which I&#x27;m using).</div><br/></div></div></div></div></div></div></div></div><div id="38907565" class="c"><input type="checkbox" id="c-38907565" checked=""/><div class="controls bullet"><span class="by">thom</span><span>|</span><a href="#38907305">parent</a><span>|</span><a href="#38907519">prev</a><span>|</span><a href="#38907370">next</a><span>|</span><label class="collapse" for="c-38907565">[-]</label><label class="expand" for="c-38907565">[1 more]</label></div><br/><div class="children"><div class="content">I have offline Wikipedia and maps for similar exigencies.</div><br/></div></div><div id="38907370" class="c"><input type="checkbox" id="c-38907370" checked=""/><div class="controls bullet"><span class="by">LewisVerstappen</span><span>|</span><a href="#38907305">parent</a><span>|</span><a href="#38907565">prev</a><span>|</span><a href="#38907381">next</a><span>|</span><label class="collapse" for="c-38907370">[-]</label><label class="expand" for="c-38907370">[1 more]</label></div><br/><div class="children"><div class="content">I think it would be far better to just store a bunch of epubs on your phone in case you get hurled back.<p>Textbooks on physics, chem, etc.</div><br/></div></div></div></div><div id="38907080" class="c"><input type="checkbox" id="c-38907080" checked=""/><div class="controls bullet"><span class="by">hananova</span><span>|</span><a href="#38907305">prev</a><span>|</span><a href="#38907279">next</a><span>|</span><label class="collapse" for="c-38907080">[-]</label><label class="expand" for="c-38907080">[20 more]</label></div><br/><div class="children"><div class="content">Why do none of these apps allow you to set the system prompt? I find these LLM apps kind of useless without being able to refine the way in which the model will respond to later questions.</div><br/><div id="38907346" class="c"><input type="checkbox" id="c-38907346" checked=""/><div class="controls bullet"><span class="by">brittlewis12</span><span>|</span><a href="#38907080">parent</a><span>|</span><a href="#38907103">next</a><span>|</span><label class="collapse" for="c-38907346">[-]</label><label class="expand" for="c-38907346">[6 more]</label></div><br/><div class="children"><div class="content">would love for you to give cnvrs a shot!<p>- save characters (system prompt + temperature, and a name &amp; cosmetic color)
- download &amp; experiment with models from 1b, 3b, &amp; 7b, and quant options q2k, q4km, q6k
- save, search, continue, &amp; export past chats<p>along with smaller touches:
- custom theme colors
- haptics<p>and more coming soon!<p><a href="https:&#x2F;&#x2F;testflight.apple.com&#x2F;join&#x2F;ERFxInZg" rel="nofollow">https:&#x2F;&#x2F;testflight.apple.com&#x2F;join&#x2F;ERFxInZg</a></div><br/><div id="38908252" class="c"><input type="checkbox" id="c-38908252" checked=""/><div class="controls bullet"><span class="by">sockaddr</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907346">parent</a><span>|</span><a href="#38907103">next</a><span>|</span><label class="collapse" for="c-38908252">[-]</label><label class="expand" for="c-38908252">[5 more]</label></div><br/><div class="children"><div class="content">Do not download this.<p>I downloaded this on my 14 Pro and it completely locked up the system to the point where even the power button wouldn’t work. I couldn’t use my phone for about 10 minutes.</div><br/><div id="38908435" class="c"><input type="checkbox" id="c-38908435" checked=""/><div class="controls bullet"><span class="by">scottbartell</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38908252">parent</a><span>|</span><a href="#38908488">next</a><span>|</span><label class="collapse" for="c-38908435">[-]</label><label class="expand" for="c-38908435">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used it for a couple weeks on my 15 Pro and I haven&#x27;t experienced anything like that. (IMO it&#x27;s well worth the download)<p>The developer is also pretty responsive and actively looking for feedback (which is why it&#x27;s currently free on TestFlight)</div><br/></div></div><div id="38908488" class="c"><input type="checkbox" id="c-38908488" checked=""/><div class="controls bullet"><span class="by">brittlewis12</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38908252">parent</a><span>|</span><a href="#38908435">prev</a><span>|</span><a href="#38907103">next</a><span>|</span><label class="collapse" for="c-38908488">[-]</label><label class="expand" for="c-38908488">[3 more]</label></div><br/><div class="children"><div class="content">I’m very sorry about your experience. That’s definitely not what I was aiming for, and I can imagine that was a nasty surprise. Any hang like that is unacceptable, full stop.<p>My understanding is Metal is currently causing hangs on devices when there is barely enough RAM to fit the model and prompt, but not quite enough to run. Will work on falling back to CPU to avoid this kind of experience much more aggressively than today.<p>Thank you for taking the time to both try it out and to share your experience; I will use it to ensure it’s better in the future.</div><br/><div id="38908857" class="c"><input type="checkbox" id="c-38908857" checked=""/><div class="controls bullet"><span class="by">sockaddr</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38908488">parent</a><span>|</span><a href="#38907103">next</a><span>|</span><label class="collapse" for="c-38908857">[-]</label><label class="expand" for="c-38908857">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the response. Unfortunately on my device the behavior makes it impossible to report a bug using a screenshot as requested in the app. I can give you more device info if you want to narrow down the cause.</div><br/><div id="38908896" class="c"><input type="checkbox" id="c-38908896" checked=""/><div class="controls bullet"><span class="by">brittlewis12</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38908857">parent</a><span>|</span><a href="#38907103">next</a><span>|</span><label class="collapse" for="c-38908896">[-]</label><label class="expand" for="c-38908896">[1 more]</label></div><br/><div class="children"><div class="content">Yes of course, I would very much appreciate that, if you’d be so generous — thank you! You can email britt [at] bl3 [dot] dev</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38907103" class="c"><input type="checkbox" id="c-38907103" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#38907080">parent</a><span>|</span><a href="#38907346">prev</a><span>|</span><a href="#38907210">next</a><span>|</span><label class="collapse" for="c-38907103">[-]</label><label class="expand" for="c-38907103">[5 more]</label></div><br/><div class="children"><div class="content">I made a free &#x2F; mostly open source one for iOS that lets you edit the system prompt<p><a href="https:&#x2F;&#x2F;chatonmac.com" rel="nofollow">https:&#x2F;&#x2F;chatonmac.com</a></div><br/><div id="38907890" class="c"><input type="checkbox" id="c-38907890" checked=""/><div class="controls bullet"><span class="by">ricktdotorg</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907103">parent</a><span>|</span><a href="#38907505">next</a><span>|</span><label class="collapse" for="c-38907890">[-]</label><label class="expand" for="c-38907890">[2 more]</label></div><br/><div class="children"><div class="content">trying this out!<p>BTW and FYI i need to reduce the font size on my iOS device to be smaller than i like in order to use your add&#x2F;replace API key key pages. if the font is &quot;larger than normal&quot; i can&#x27;t see&#x2F;focus on the box to enter or paste in the API key. just increase your iOS system font size to trigger this. thanks in advance for fixing, will try out the app!</div><br/><div id="38908154" class="c"><input type="checkbox" id="c-38908154" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907890">parent</a><span>|</span><a href="#38907505">next</a><span>|</span><label class="collapse" for="c-38908154">[-]</label><label class="expand" for="c-38908154">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the detailed report - will fix asap, along with releasing the macOS v1.0. I&#x27;ve just soft launched this so far but have more to come so please let me know anything else.</div><br/></div></div></div></div><div id="38907505" class="c"><input type="checkbox" id="c-38907505" checked=""/><div class="controls bullet"><span class="by">ionwake</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907103">parent</a><span>|</span><a href="#38907890">prev</a><span>|</span><a href="#38907210">next</a><span>|</span><label class="collapse" for="c-38907505">[-]</label><label class="expand" for="c-38907505">[2 more]</label></div><br/><div class="children"><div class="content">Amazing! Does it submit any data online ?</div><br/><div id="38907771" class="c"><input type="checkbox" id="c-38907771" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907505">parent</a><span>|</span><a href="#38907210">next</a><span>|</span><label class="collapse" for="c-38907771">[-]</label><label class="expand" for="c-38907771">[1 more]</label></div><br/><div class="children"><div class="content">No.<p>I definitely do not want any liability of user-generated content or PII or similar. I have no analytics, besides the standard Apple opt-in crash&#x2F;reporting (not using any 3rd-party service and not sending anything to my own servers).<p>It downloads configuration from GitHub and HuggingFace directly. It also has OpenAI integration, directly to their servers via BYOK.</div><br/></div></div></div></div></div></div><div id="38907210" class="c"><input type="checkbox" id="c-38907210" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38907080">parent</a><span>|</span><a href="#38907103">prev</a><span>|</span><a href="#38907387">next</a><span>|</span><label class="collapse" for="c-38907210">[-]</label><label class="expand" for="c-38907210">[7 more]</label></div><br/><div class="children"><div class="content">Mistral instruct doesn&#x27;t have system prompt AFAIK. Also llama chat system prompt is very useless in my testing.</div><br/><div id="38907361" class="c"><input type="checkbox" id="c-38907361" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907210">parent</a><span>|</span><a href="#38907245">next</a><span>|</span><label class="collapse" for="c-38907361">[-]</label><label class="expand" for="c-38907361">[4 more]</label></div><br/><div class="children"><div class="content">Mistral Instruct <i>does</i> use a system prompt.<p>You can see the raw format here: <a href="https:&#x2F;&#x2F;www.promptingguide.ai&#x2F;models&#x2F;mistral-7b#chat-template-for-mistral-7b-instruct" rel="nofollow">https:&#x2F;&#x2F;www.promptingguide.ai&#x2F;models&#x2F;mistral-7b#chat-templat...</a> and you can see how LllamaIndex uses it here (as an example): <a href="https:&#x2F;&#x2F;github.com&#x2F;run-llama&#x2F;llama_index&#x2F;blob&#x2F;1d861a9440cdc9e1b2335320a7a2bf26667b90b2&#x2F;llama_index&#x2F;llms&#x2F;llama_utils.py#L27">https:&#x2F;&#x2F;github.com&#x2F;run-llama&#x2F;llama_index&#x2F;blob&#x2F;1d861a9440cdc9...</a></div><br/><div id="38907647" class="c"><input type="checkbox" id="c-38907647" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907361">parent</a><span>|</span><a href="#38907245">next</a><span>|</span><label class="collapse" for="c-38907647">[-]</label><label class="expand" for="c-38907647">[3 more]</label></div><br/><div class="children"><div class="content">So the system prompt is just part of the first prompt in a conversation? How is that different from not having a system prompt?</div><br/><div id="38908434" class="c"><input type="checkbox" id="c-38908434" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907647">parent</a><span>|</span><a href="#38907245">next</a><span>|</span><label class="collapse" for="c-38908434">[-]</label><label class="expand" for="c-38908434">[2 more]</label></div><br/><div class="children"><div class="content">No, system prompts are surrounded by [INST] [&#x2F;INST]</div><br/><div id="38908685" class="c"><input type="checkbox" id="c-38908685" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38908434">parent</a><span>|</span><a href="#38907245">next</a><span>|</span><label class="collapse" for="c-38908685">[-]</label><label class="expand" for="c-38908685">[1 more]</label></div><br/><div class="children"><div class="content">No, <i>every</i> user input is surrounded by those tags. Scroll down from that link you posted and read the next two example prompts.</div><br/></div></div></div></div></div></div></div></div><div id="38907245" class="c"><input type="checkbox" id="c-38907245" checked=""/><div class="controls bullet"><span class="by">rrr_oh_man</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907210">parent</a><span>|</span><a href="#38907361">prev</a><span>|</span><a href="#38907332">next</a><span>|</span><label class="collapse" for="c-38907245">[-]</label><label class="expand" for="c-38907245">[1 more]</label></div><br/><div class="children"><div class="content">It seems to work well on GPT4All (macOS) with system prompts?<p>Can you link to any doc why it shouldn&#x27;t work?</div><br/></div></div><div id="38907332" class="c"><input type="checkbox" id="c-38907332" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38907080">root</a><span>|</span><a href="#38907210">parent</a><span>|</span><a href="#38907245">prev</a><span>|</span><a href="#38907387">next</a><span>|</span><label class="collapse" for="c-38907332">[-]</label><label class="expand" for="c-38907332">[1 more]</label></div><br/><div class="children"><div class="content">It does; and if it&#x27;s LLaMa 2 7B Chat stock from Facebook, that was a little rushed imho, doesn&#x27;t seem as baked in.<p>(GPTs matters but it&#x27;s _very_ bizarre who it thinks it&#x27;s coming from)</div><br/></div></div></div></div></div></div><div id="38907975" class="c"><input type="checkbox" id="c-38907975" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#38907105">prev</a><span>|</span><label class="collapse" for="c-38907975">[-]</label><label class="expand" for="c-38907975">[5 more]</label></div><br/><div class="children"><div class="content">edit: my bad, I misread the price and it&#x27;s really hard to see the price after you bought it to double check.<p>$10 for something that (I think) doesn&#x27;t work on most phones but isn&#x27;t gated to ones it works on feels hostile.<p>Probably there&#x27;s no way to gate, in that case I&#x27;d suggest not charging for it. Or I guess adding a daily usage limit that&#x27;s lifted with an IAP.<p>I&#x27;ll admit I was off-put by the price to begin with, which probably amplifies what a slap in the face it feels like to pay and get something that doesn&#x27;t work at all.</div><br/><div id="38908019" class="c"><input type="checkbox" id="c-38908019" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#38907975">parent</a><span>|</span><label class="collapse" for="c-38908019">[-]</label><label class="expand" for="c-38908019">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s $1.99 and the description says:<p><i>The app requires a Pro iPhone with a minimum of 6GB of RAM. Only the following devices meet the requirement:<p>- iPhone 15 Pro, iPhone 14 Pro, iPhone 13 Pro, iPhone 12 Pro.<p>- iPads: Please check. RAM varies based on model and year.</i></div><br/><div id="38908097" class="c"><input type="checkbox" id="c-38908097" checked=""/><div class="controls bullet"><span class="by">halJordan</span><span>|</span><a href="#38907975">root</a><span>|</span><a href="#38908019">parent</a><span>|</span><label class="collapse" for="c-38908097">[-]</label><label class="expand" for="c-38908097">[3 more]</label></div><br/><div class="children"><div class="content">So if they know it wont work, and do not put that info into the store&#x27;s compatibility matrix then it&#x27;s still a bait&#x2F;switch to me. Compare to the Resident Evil page which does set the store limits on what devices can dl it.</div><br/><div id="38908311" class="c"><input type="checkbox" id="c-38908311" checked=""/><div class="controls bullet"><span class="by">winstonschen</span><span>|</span><a href="#38907975">root</a><span>|</span><a href="#38908097">parent</a><span>|</span><a href="#38908282">next</a><span>|</span><label class="collapse" for="c-38908311">[-]</label><label class="expand" for="c-38908311">[1 more]</label></div><br/><div class="children"><div class="content">There is no way to specify iPhone models or memory capacity when submitting an app to App Store. Believe me - I spent several days trying.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>