<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699866063396" as="style"/><link rel="stylesheet" href="styles.css?v=1699866063396"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.secondstate.io/articles/fast-llm-inference/">Run LLMs on my own Mac fast and efficient Only 2 MBs</a> <span class="domain">(<a href="https://www.secondstate.io">www.secondstate.io</a>)</span></div><div class="subtext"><span>3Sophons</span> | <span>70 comments</span></div><br/><div><div id="38247206" class="c"><input type="checkbox" id="c-38247206" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#38246959">next</a><span>|</span><label class="collapse" for="c-38247206">[-]</label><label class="expand" for="c-38247206">[7 more]</label></div><br/><div class="children"><div class="content">Confused about the title rewrite from “Fast and Portable Llama2 Inference on the Heterogeneous Edge” which more clearly communicates what this article is about - a wasm version of llama.cpp.<p>I feel like editorializing to highlight the fact that it’s 2MB and runs on a mac are missing some of the core aspects of the project and write up.</div><br/><div id="38248051" class="c"><input type="checkbox" id="c-38248051" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#38247206">parent</a><span>|</span><a href="#38248025">next</a><span>|</span><label class="collapse" for="c-38248051">[-]</label><label class="expand" for="c-38248051">[2 more]</label></div><br/><div class="children"><div class="content">Now I’m confused, because neither of the titles _clearly_ communicate that it’s a wasm version of llama.cpp in my opinion.<p>It would probably be helpful to use the words “wasm” and “llama” to achieve that</div><br/><div id="38248187" class="c"><input type="checkbox" id="c-38248187" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#38247206">root</a><span>|</span><a href="#38248051">parent</a><span>|</span><a href="#38248025">next</a><span>|</span><label class="collapse" for="c-38248187">[-]</label><label class="expand" for="c-38248187">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Run LlaMA 2 on WASM in 2 MB RAM&quot;<p>This has the added advantage of being completely gibberish to someone outside tech.<p>Edit: wait, it&#x27;s not RAM, the binary is just 2 MB. That&#x27;s disappointing.</div><br/></div></div></div></div><div id="38248025" class="c"><input type="checkbox" id="c-38248025" checked=""/><div class="controls bullet"><span class="by">doubloon</span><span>|</span><a href="#38247206">parent</a><span>|</span><a href="#38248051">prev</a><span>|</span><a href="#38247349">next</a><span>|</span><label class="collapse" for="c-38248025">[-]</label><label class="expand" for="c-38248025">[2 more]</label></div><br/><div class="children"><div class="content">well it requires nvidia so maybe its not actually portable.</div><br/><div id="38248174" class="c"><input type="checkbox" id="c-38248174" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#38247206">root</a><span>|</span><a href="#38248025">parent</a><span>|</span><a href="#38247349">next</a><span>|</span><label class="collapse" for="c-38248174">[-]</label><label class="expand" for="c-38248174">[1 more]</label></div><br/><div class="children"><div class="content">It also works with Metal hence why they mention it runs on Mac.</div><br/></div></div></div></div><div id="38247349" class="c"><input type="checkbox" id="c-38247349" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38247206">parent</a><span>|</span><a href="#38248025">prev</a><span>|</span><a href="#38246959">next</a><span>|</span><label class="collapse" for="c-38247349">[-]</label><label class="expand" for="c-38247349">[2 more]</label></div><br/><div class="children"><div class="content">ok.. should be run LLMs on my own devices with a 2MB portable app then?</div><br/></div></div></div></div><div id="38246959" class="c"><input type="checkbox" id="c-38246959" checked=""/><div class="controls bullet"><span class="by">ed</span><span>|</span><a href="#38247206">prev</a><span>|</span><a href="#38247278">next</a><span>|</span><label class="collapse" for="c-38246959">[-]</label><label class="expand" for="c-38246959">[2 more]</label></div><br/><div class="children"><div class="content">Whoa! Great work. To other folks checking it out, it still requires downloading the weights, which are pretty large. But they essentially made a fully portable, no-dependency llama.cpp, in 2mb.<p>If you&#x27;re an app developer this might be the easiest way to package an inference engine in a distributable file (the weights are already portable and can be downloaded on-demand — the inference engine is really the part you want to lock down).</div><br/><div id="38247587" class="c"><input type="checkbox" id="c-38247587" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#38246959">parent</a><span>|</span><a href="#38247278">next</a><span>|</span><label class="collapse" for="c-38247587">[-]</label><label class="expand" for="c-38247587">[1 more]</label></div><br/><div class="children"><div class="content">It might be more helpful if the title says 2MB of wasm. But as you say, the weights dwarf that.</div><br/></div></div></div></div><div id="38247278" class="c"><input type="checkbox" id="c-38247278" checked=""/><div class="controls bullet"><span class="by">FL33TW00D</span><span>|</span><a href="#38246959">prev</a><span>|</span><a href="#38248061">next</a><span>|</span><label class="collapse" for="c-38247278">[-]</label><label class="expand" for="c-38247278">[7 more]</label></div><br/><div class="children"><div class="content">This just wrapping llama.cpp right? 
I’m sorry but I’m pretty tired of projects wrapping x.cpp.<p>I’ve been developing a Rust + WebGPU ML framework for the past 6 months. I’ve learned quickly how impressive the work by GG is.<p>It’s early stages but you can check it out here:
<a href="https:&#x2F;&#x2F;www.ratchet.sh&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.ratchet.sh&#x2F;</a>
<a href="https:&#x2F;&#x2F;github.com&#x2F;FL33TW00D&#x2F;whisper-turbo">https:&#x2F;&#x2F;github.com&#x2F;FL33TW00D&#x2F;whisper-turbo</a></div><br/><div id="38248195" class="c"><input type="checkbox" id="c-38248195" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#38247278">parent</a><span>|</span><a href="#38248052">next</a><span>|</span><label class="collapse" for="c-38248195">[-]</label><label class="expand" for="c-38248195">[1 more]</label></div><br/><div class="children"><div class="content">Can you elaborate on what you find impressive? I know nothing about this stuff so I can&#x27;t appreciate it.</div><br/></div></div><div id="38248052" class="c"><input type="checkbox" id="c-38248052" checked=""/><div class="controls bullet"><span class="by">tansan</span><span>|</span><a href="#38247278">parent</a><span>|</span><a href="#38248195">prev</a><span>|</span><a href="#38247768">next</a><span>|</span><label class="collapse" for="c-38248052">[-]</label><label class="expand" for="c-38248052">[2 more]</label></div><br/><div class="children"><div class="content">Who&#x27;s GG?</div><br/><div id="38248076" class="c"><input type="checkbox" id="c-38248076" checked=""/><div class="controls bullet"><span class="by">europeanNyan</span><span>|</span><a href="#38247278">root</a><span>|</span><a href="#38248052">parent</a><span>|</span><a href="#38247768">next</a><span>|</span><label class="collapse" for="c-38248076">[-]</label><label class="expand" for="c-38248076">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov">https:&#x2F;&#x2F;github.com&#x2F;ggerganov</a></div><br/></div></div></div></div><div id="38247768" class="c"><input type="checkbox" id="c-38247768" checked=""/><div class="controls bullet"><span class="by">hluska</span><span>|</span><a href="#38247278">parent</a><span>|</span><a href="#38248052">prev</a><span>|</span><a href="#38248061">next</a><span>|</span><label class="collapse" for="c-38247768">[-]</label><label class="expand" for="c-38247768">[3 more]</label></div><br/><div class="children"><div class="content">You ripped on someone else’s work and promoted your own in the same comment?? You need to seriously reflect upon your ethics.</div><br/><div id="38247807" class="c"><input type="checkbox" id="c-38247807" checked=""/><div class="controls bullet"><span class="by">FL33TW00D</span><span>|</span><a href="#38247278">root</a><span>|</span><a href="#38247768">parent</a><span>|</span><a href="#38248061">next</a><span>|</span><label class="collapse" for="c-38247807">[-]</label><label class="expand" for="c-38247807">[2 more]</label></div><br/><div class="children"><div class="content">I appreciate the work that went into slimming this binary down, but it&#x27;s a ~negligble amount of work compared to llama.cpp itself.<p>HN is inundated with posts doing xyz on top of the x.cpp community. Whilst I appreciate it is exciting - I wish more people would explore the low-level themselves! We can be much more creative in this new playground.</div><br/><div id="38248046" class="c"><input type="checkbox" id="c-38248046" checked=""/><div class="controls bullet"><span class="by">spiderfarmer</span><span>|</span><a href="#38247278">root</a><span>|</span><a href="#38247807">parent</a><span>|</span><a href="#38248061">next</a><span>|</span><label class="collapse" for="c-38248046">[-]</label><label class="expand" for="c-38248046">[1 more]</label></div><br/><div class="children"><div class="content">Why not both.</div><br/></div></div></div></div></div></div></div></div><div id="38248061" class="c"><input type="checkbox" id="c-38248061" checked=""/><div class="controls bullet"><span class="by">nigma</span><span>|</span><a href="#38247278">prev</a><span>|</span><a href="#38247705">next</a><span>|</span><label class="collapse" for="c-38248061">[-]</label><label class="expand" for="c-38248061">[1 more]</label></div><br/><div class="children"><div class="content">I hate this kind of clickbait marketing suggesting the project is delivering 1&#x2F;100 of the size or 100x-35000x the speed of other solutions because it uses a different language for a wrapper around core library and completely neglecting tooling and community expertise built around other solutions.<p>First of all the project is based on llama.cpp[1], which does the heavy work of loading and running multi-GB model files on GPU&#x2F;CPU and the inference speed is not limited by the wrapper choice (there are other wrappers in Go, Python, Node, Rust, etc. or one can use llama.cpp directly). The size of the binary is also not that important when common quantized model files are often in the range of 5GB-40GB and require a beefy GPU or a MB with 16-64GB of RAM.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a></div><br/></div></div><div id="38247705" class="c"><input type="checkbox" id="c-38247705" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#38248061">prev</a><span>|</span><a href="#38246977">next</a><span>|</span><label class="collapse" for="c-38247705">[-]</label><label class="expand" for="c-38247705">[2 more]</label></div><br/><div class="children"><div class="content">The way things are going, we&#x27;ll see more efficient and faster methods to run transformer arch on edge, but I&#x27;m afraid we&#x27;re approaching the limit because you can&#x27;t just rust your way out of the VRAM requirements, which is the main bottleneck in loading large-enough models. One might say &quot;small models are getting better, look at Mistral vs. llama 2&quot;, but small models are also approaching their capacity (there&#x27;s only so much you can put in 7b parameters).<p>I don&#x27;t know man, this approach to AI doesn&#x27;t &quot;feel&quot; like it&#x27;ll lead to AGI—it&#x27;s too inefficient.</div><br/><div id="38248311" class="c"><input type="checkbox" id="c-38248311" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#38247705">parent</a><span>|</span><a href="#38246977">next</a><span>|</span><label class="collapse" for="c-38248311">[-]</label><label class="expand" for="c-38248311">[1 more]</label></div><br/><div class="children"><div class="content">I think we have plenty of headroom with MoE systems, dynamically loading LoRAs and such, even with the small models.</div><br/></div></div></div></div><div id="38246977" class="c"><input type="checkbox" id="c-38246977" checked=""/><div class="controls bullet"><span class="by">reidjs</span><span>|</span><a href="#38247705">prev</a><span>|</span><a href="#38247702">next</a><span>|</span><label class="collapse" for="c-38246977">[-]</label><label class="expand" for="c-38246977">[4 more]</label></div><br/><div class="children"><div class="content">Can I run this offline on my iPhone? That would be like having basic internet search regardless of reception. Could come in handy when camping</div><br/><div id="38247061" class="c"><input type="checkbox" id="c-38247061" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38246977">parent</a><span>|</span><a href="#38247120">next</a><span>|</span><label class="collapse" for="c-38247061">[-]</label><label class="expand" for="c-38247061">[1 more]</label></div><br/><div class="children"><div class="content">You can run it on a variety of Linux, Mac and Windows based devices, including the Raspberry Pi and most laptops &#x2F; servers you might have. But you still need a few GBs of memory in order to fit the model itself.</div><br/></div></div><div id="38247120" class="c"><input type="checkbox" id="c-38247120" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#38246977">parent</a><span>|</span><a href="#38247061">prev</a><span>|</span><a href="#38247702">next</a><span>|</span><label class="collapse" for="c-38247120">[-]</label><label class="expand" for="c-38247120">[2 more]</label></div><br/><div class="children"><div class="content">I got this project[0] running on a Pixel. Looks like it works on some iPhones&#x2F;iPads as well.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm">https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm</a></div><br/><div id="38247579" class="c"><input type="checkbox" id="c-38247579" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38246977">root</a><span>|</span><a href="#38247120">parent</a><span>|</span><a href="#38247702">next</a><span>|</span><label class="collapse" for="c-38247579">[-]</label><label class="expand" for="c-38247579">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I&#x27;ve been using their iPhone app for a while - it works great, though it does make the phone run pretty hot while it&#x27;s outputting tokens!<p><a href="https:&#x2F;&#x2F;llm.mlc.ai&#x2F;#ios" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm.mlc.ai&#x2F;#ios</a></div><br/></div></div></div></div></div></div><div id="38247702" class="c"><input type="checkbox" id="c-38247702" checked=""/><div class="controls bullet"><span class="by">danielEM</span><span>|</span><a href="#38246977">prev</a><span>|</span><a href="#38247976">next</a><span>|</span><label class="collapse" for="c-38247702">[-]</label><label class="expand" for="c-38247702">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m getting lost in all that.<p>Using llama cpp and mlc-llm. Both on my 2 years old mobile Ryzen APU with 64GB of RAM. First does not use GPU at all, tried plenty of options, nothing did work, but llama 34B works - painfully slow, but does work. Second is working on top of Vulkan and I didn&#x27;t take any precise measurements but it&#x27;s limit looks like is 32GB RAM (so no llama 34B), but it offloads CPU, unfortunately seem like performance is similar to CPU (that is my perception, didn&#x27;t take any measurements here too).<p>So ... will I get any benefits from switching to rust&#x2F;webassembly version???</div><br/></div></div><div id="38247323" class="c"><input type="checkbox" id="c-38247323" checked=""/><div class="controls bullet"><span class="by">est</span><span>|</span><a href="#38247976">prev</a><span>|</span><a href="#38247362">next</a><span>|</span><label class="collapse" for="c-38247323">[-]</label><label class="expand" for="c-38247323">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The core Rust source code is very simple. It is only 40 lines of code. The Rust program manages the user input, tracks the conversation history, transforms the text into the llama2’s chat template, and runs the inference operations using the WASI NN API.<p>TL;DR a 2MB executable that reads stdin and calls WASI-NN</div><br/><div id="38247532" class="c"><input type="checkbox" id="c-38247532" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#38247323">parent</a><span>|</span><a href="#38247362">next</a><span>|</span><label class="collapse" for="c-38247532">[-]</label><label class="expand" for="c-38247532">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Rust is the language of AGI.&quot;<p>Oh Rust Evangelism Strike Force, never change</div><br/></div></div></div></div><div id="38247362" class="c"><input type="checkbox" id="c-38247362" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#38247323">prev</a><span>|</span><a href="#38246855">next</a><span>|</span><label class="collapse" for="c-38247362">[-]</label><label class="expand" for="c-38247362">[1 more]</label></div><br/><div class="children"><div class="content">It looks like this is Rust for the application wrapped around a WASM port of llama.cpp that in turn uses an implementation of WASI-NN for the actual NN compute. It would be interesting to see how this compares to the TFLite, the new stuff in the PyTorch ecosystem, etc.</div><br/></div></div><div id="38246855" class="c"><input type="checkbox" id="c-38246855" checked=""/><div class="controls bullet"><span class="by">hnarayanan</span><span>|</span><a href="#38247362">prev</a><span>|</span><a href="#38247846">next</a><span>|</span><label class="collapse" for="c-38246855">[-]</label><label class="expand" for="c-38246855">[7 more]</label></div><br/><div class="children"><div class="content">If a large part of the size is essentially the trained weights of a model, how can one reduce the size by orders of magnitude (without losing any accuracy)?</div><br/><div id="38246897" class="c"><input type="checkbox" id="c-38246897" checked=""/><div class="controls bullet"><span class="by">rgbrgb</span><span>|</span><a href="#38246855">parent</a><span>|</span><a href="#38246922">next</a><span>|</span><label class="collapse" for="c-38246897">[-]</label><label class="expand" for="c-38246897">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you can reduce size without losing accuracy (though I think quantized GGUFs are great). But the 2 MB size here is a reference to the program size not including a model. It looks like it&#x27;s a way to run llama.cpp with wasm + a rust server that runs llama.cpp.<p>I like the tiny llama.cpp&#x2F;examples&#x2F;server and embed it in FreeChat, but always happy for more tooling options.<p>Edit: Just checked, the arm64&#x2F;x86 executable I embed is currently 4.2 MB. FreeChat is 12.1 MB but the default model is ~3 GB so I&#x27;m not really losing sleep over 2 MB.<p>[0]: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;server">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;...</a></div><br/></div></div><div id="38246922" class="c"><input type="checkbox" id="c-38246922" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38246855">parent</a><span>|</span><a href="#38246897">prev</a><span>|</span><a href="#38246920">next</a><span>|</span><label class="collapse" for="c-38246922">[-]</label><label class="expand" for="c-38246922">[4 more]</label></div><br/><div class="children"><div class="content">Hello you might be talking about reducing the size of the model itself (i.e., the trained weights) by orders of magnitude without losing accuracy, that&#x27;s indeed a different challenge. But the article discusses reducing the inference app size by 100x</div><br/><div id="38246950" class="c"><input type="checkbox" id="c-38246950" checked=""/><div class="controls bullet"><span class="by">hnarayanan</span><span>|</span><a href="#38246855">root</a><span>|</span><a href="#38246922">parent</a><span>|</span><a href="#38246920">next</a><span>|</span><label class="collapse" for="c-38246950">[-]</label><label class="expand" for="c-38246950">[3 more]</label></div><br/><div class="children"><div class="content">Oh. Did not think that was even a goal.</div><br/><div id="38246985" class="c"><input type="checkbox" id="c-38246985" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38246855">root</a><span>|</span><a href="#38246950">parent</a><span>|</span><a href="#38246920">next</a><span>|</span><label class="collapse" for="c-38246985">[-]</label><label class="expand" for="c-38246985">[2 more]</label></div><br/><div class="children"><div class="content">I guess making it portable is still quite important?</div><br/><div id="38248107" class="c"><input type="checkbox" id="c-38248107" checked=""/><div class="controls bullet"><span class="by">hnarayanan</span><span>|</span><a href="#38246855">root</a><span>|</span><a href="#38246985">parent</a><span>|</span><a href="#38246920">next</a><span>|</span><label class="collapse" for="c-38248107">[-]</label><label class="expand" for="c-38248107">[1 more]</label></div><br/><div class="children"><div class="content">I am not trying to troll. I genuinely don’t see why a few MB on some binary matter when the models are multiple GB large. This is why I fundamentally misunderstood the article, my brain was looking for the other number going down as that’s genuinely a barrier for edge devices.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38247846" class="c"><input type="checkbox" id="c-38247846" checked=""/><div class="controls bullet"><span class="by">syrusakbary</span><span>|</span><a href="#38246855">prev</a><span>|</span><a href="#38247963">next</a><span>|</span><label class="collapse" for="c-38247846">[-]</label><label class="expand" for="c-38247846">[1 more]</label></div><br/><div class="children"><div class="content">Congrats on the work... it&#x27;s an impressive demo!<p>It may be worth researching to add support of it into the Wasmer WebAssembly runtime [1]. (Note: I work at Wasmer!)<p>[1] <a href="https:&#x2F;&#x2F;wasmer.io&#x2F;">https:&#x2F;&#x2F;wasmer.io&#x2F;</a></div><br/></div></div><div id="38247963" class="c"><input type="checkbox" id="c-38247963" checked=""/><div class="controls bullet"><span class="by">anon23432343</span><span>|</span><a href="#38247846">prev</a><span>|</span><a href="#38247687">next</a><span>|</span><label class="collapse" for="c-38247963">[-]</label><label class="expand" for="c-38247963">[2 more]</label></div><br/><div class="children"><div class="content">So you need to mb2 for sending an api call to the edge?<p>Okaayyyy...</div><br/><div id="38248120" class="c"><input type="checkbox" id="c-38248120" checked=""/><div class="controls bullet"><span class="by">kamray23</span><span>|</span><a href="#38247963">parent</a><span>|</span><a href="#38247687">next</a><span>|</span><label class="collapse" for="c-38248120">[-]</label><label class="expand" for="c-38248120">[1 more]</label></div><br/><div class="children"><div class="content">This is offline.</div><br/></div></div></div></div><div id="38247687" class="c"><input type="checkbox" id="c-38247687" checked=""/><div class="controls bullet"><span class="by">gvand</span><span>|</span><a href="#38247963">prev</a><span>|</span><a href="#38247274">next</a><span>|</span><label class="collapse" for="c-38247687">[-]</label><label class="expand" for="c-38247687">[1 more]</label></div><br/><div class="children"><div class="content">The binary size is not really important in this case, llama.cpp should not be that far from this, what&#x27;s matter as we all know is how much gpu memory we need.</div><br/></div></div><div id="38247274" class="c"><input type="checkbox" id="c-38247274" checked=""/><div class="controls bullet"><span class="by">dkga</span><span>|</span><a href="#38247687">prev</a><span>|</span><a href="#38248161">next</a><span>|</span><label class="collapse" for="c-38247274">[-]</label><label class="expand" for="c-38247274">[2 more]</label></div><br/><div class="children"><div class="content">Very cool, but unless I missed it could someone please explain why not just compile a Rust application? Is the Wasm part needed for the GPU acceleration (whatever the user GPU is?)</div><br/><div id="38247428" class="c"><input type="checkbox" id="c-38247428" checked=""/><div class="controls bullet"><span class="by">bouke</span><span>|</span><a href="#38247274">parent</a><span>|</span><a href="#38248161">next</a><span>|</span><label class="collapse" for="c-38247428">[-]</label><label class="expand" for="c-38247428">[1 more]</label></div><br/><div class="children"><div class="content">I suppose wasm provides the portability between platforms. Compile once, run everywhere.</div><br/></div></div></div></div><div id="38248161" class="c"><input type="checkbox" id="c-38248161" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#38247274">prev</a><span>|</span><a href="#38247297">next</a><span>|</span><label class="collapse" for="c-38248161">[-]</label><label class="expand" for="c-38248161">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you can call anything wasm efficient.</div><br/></div></div><div id="38247297" class="c"><input type="checkbox" id="c-38247297" checked=""/><div class="controls bullet"><span class="by">rjzzleep</span><span>|</span><a href="#38248161">prev</a><span>|</span><a href="#38247172">next</a><span>|</span><label class="collapse" for="c-38247297">[-]</label><label class="expand" for="c-38247297">[2 more]</label></div><br/><div class="children"><div class="content">Is there any detailed info on how a 4090 + ryzen 7840 compares to any of the new Apple offerings with 64GB or more unified RAM?</div><br/><div id="38247691" class="c"><input type="checkbox" id="c-38247691" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#38247297">parent</a><span>|</span><a href="#38247172">next</a><span>|</span><label class="collapse" for="c-38247691">[-]</label><label class="expand" for="c-38247691">[1 more]</label></div><br/><div class="children"><div class="content">No. You just have to try it. Anecdotally, I can fit a larger Llama on my M1 Max with 64 GiB than my 3090 with 24 GiB.</div><br/></div></div></div></div><div id="38247172" class="c"><input type="checkbox" id="c-38247172" checked=""/><div class="controls bullet"><span class="by">diimdeep</span><span>|</span><a href="#38247297">prev</a><span>|</span><a href="#38247868">next</a><span>|</span><label class="collapse" for="c-38247172">[-]</label><label class="expand" for="c-38247172">[9 more]</label></div><br/><div class="children"><div class="content">I do not see the point to use this instead of directly using llama.cpp</div><br/><div id="38247209" class="c"><input type="checkbox" id="c-38247209" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#38247172">parent</a><span>|</span><a href="#38247216">next</a><span>|</span><label class="collapse" for="c-38247209">[-]</label><label class="expand" for="c-38247209">[2 more]</label></div><br/><div class="children"><div class="content">Hint: the Rewrite-it-in-Rust economy&#x27;s currency isn&#x27;t actually running things.</div><br/><div id="38247379" class="c"><input type="checkbox" id="c-38247379" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#38247172">root</a><span>|</span><a href="#38247209">parent</a><span>|</span><a href="#38247216">next</a><span>|</span><label class="collapse" for="c-38247379">[-]</label><label class="expand" for="c-38247379">[1 more]</label></div><br/><div class="children"><div class="content">The crypto of programming languages?</div><br/></div></div></div></div><div id="38247216" class="c"><input type="checkbox" id="c-38247216" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38247172">parent</a><span>|</span><a href="#38247209">prev</a><span>|</span><a href="#38247868">next</a><span>|</span><label class="collapse" for="c-38247216">[-]</label><label class="expand" for="c-38247216">[6 more]</label></div><br/><div class="children"><div class="content">llama.cpp typically needs to be compiled separately for each operating system and architecture (Windows, macOS, Linux, etc.), which is less portable.<p>Also, the article mentions the use of hardware acceleration on devices with heterogeneous hardware accelerators. This implies that the Wasm-compiled program can efficiently utilize different hardware resources (like GPUs and specialized AI chips) across various devices. A direct C++ implementation might require specific optimizations or versions for each type of hardware to achieve similar performance.</div><br/><div id="38247506" class="c"><input type="checkbox" id="c-38247506" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38247172">root</a><span>|</span><a href="#38247216">parent</a><span>|</span><a href="#38247862">next</a><span>|</span><label class="collapse" for="c-38247506">[-]</label><label class="expand" for="c-38247506">[1 more]</label></div><br/><div class="children"><div class="content">Where have I seen this WORA before, including for C and C++?<p>WASM does not provide access to hardware acceleration on devices with heterogeneous hardware accelerators, even its SIMD bytecodes are a subset of what most CPUs are capable of.</div><br/></div></div><div id="38247862" class="c"><input type="checkbox" id="c-38247862" checked=""/><div class="controls bullet"><span class="by">tomalbrc</span><span>|</span><a href="#38247172">root</a><span>|</span><a href="#38247216">parent</a><span>|</span><a href="#38247506">prev</a><span>|</span><a href="#38247285">next</a><span>|</span><label class="collapse" for="c-38247862">[-]</label><label class="expand" for="c-38247862">[1 more]</label></div><br/><div class="children"><div class="content">Just use Comsopolitan at this point.</div><br/></div></div><div id="38247285" class="c"><input type="checkbox" id="c-38247285" checked=""/><div class="controls bullet"><span class="by">diimdeep</span><span>|</span><a href="#38247172">root</a><span>|</span><a href="#38247216">parent</a><span>|</span><a href="#38247862">prev</a><span>|</span><a href="#38247868">next</a><span>|</span><label class="collapse" for="c-38247285">[-]</label><label class="expand" for="c-38247285">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Wasm-compiled program can efficiently utilize different hardware resources (like GPUs and specialized AI chips) across various devices<p>I do not buy it, but maybe I am ignorant of progress being made there.<p>&gt; A direct C++ implementation might require specific optimizations or versions for each type of hardware to achieve similar performance.<p>Because I do not buy previous one, I do not buy that similar performance can be painlessly(without extra developer time) achieved there, and that wasm runtime capable to achieve it.</div><br/><div id="38247531" class="c"><input type="checkbox" id="c-38247531" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#38247172">root</a><span>|</span><a href="#38247285">parent</a><span>|</span><a href="#38247868">next</a><span>|</span><label class="collapse" for="c-38247531">[-]</label><label class="expand" for="c-38247531">[2 more]</label></div><br/><div class="children"><div class="content">So the magic (or sleight of hand, if you prefer) seems to be in<p>&gt; You just need to install the WasmEdge with the GGML plugin.<p>And it turns out that all these plugins are native &amp; specific to the acceleration environment as well. But this has to happen after it lands in its environment so your &quot;portable&quot; application is now only portable in the sense that once it starts running it will bootstrap itself by downloading and installing native platform-specific code from the internet. Whether that is a reasonable thing for an &quot;edge&quot; application to do I am not sure.</div><br/><div id="38248132" class="c"><input type="checkbox" id="c-38248132" checked=""/><div class="controls bullet"><span class="by">kamray23</span><span>|</span><a href="#38247172">root</a><span>|</span><a href="#38247531">parent</a><span>|</span><a href="#38247868">next</a><span>|</span><label class="collapse" for="c-38248132">[-]</label><label class="expand" for="c-38248132">[1 more]</label></div><br/><div class="children"><div class="content">Basically, WASM is now what the JVM was in 2000. It&#x27;s portable because it is.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38247868" class="c"><input type="checkbox" id="c-38247868" checked=""/><div class="controls bullet"><span class="by">tomalbrc</span><span>|</span><a href="#38247172">prev</a><span>|</span><a href="#38246941">next</a><span>|</span><label class="collapse" for="c-38247868">[-]</label><label class="expand" for="c-38247868">[1 more]</label></div><br/><div class="children"><div class="content">&gt; No wonder Elon Musk said that Rust is the language of AGI.<p>What.</div><br/></div></div><div id="38246941" class="c"><input type="checkbox" id="c-38246941" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38247868">prev</a><span>|</span><a href="#38247620">next</a><span>|</span><label class="collapse" for="c-38246941">[-]</label><label class="expand" for="c-38246941">[13 more]</label></div><br/><div class="children"><div class="content">Wow, this is a “holy shit” moment for Rust in AI applications if this works as described. Also, so long Mojo!<p>EDIT:<p>Looks like I’m wrong, but I appreciate getting schooled by all the HNers with low-level expertise. Lots to go and learn about now.</div><br/><div id="38247099" class="c"><input type="checkbox" id="c-38247099" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#38246941">parent</a><span>|</span><a href="#38246987">next</a><span>|</span><label class="collapse" for="c-38247099">[-]</label><label class="expand" for="c-38247099">[1 more]</label></div><br/><div class="children"><div class="content">No it&#x27;s not. This does nothing to minimize the size of the models which inference on being run on. It&#x27;s cool for edge applications, kind of. And Rust is already a go to tool for edge.</div><br/></div></div><div id="38246987" class="c"><input type="checkbox" id="c-38246987" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#38246941">parent</a><span>|</span><a href="#38247099">prev</a><span>|</span><a href="#38247309">next</a><span>|</span><label class="collapse" for="c-38246987">[-]</label><label class="expand" for="c-38246987">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s &quot;just&quot; a port of GGML (written in C++) to wasm with some additional Rust code.</div><br/><div id="38246998" class="c"><input type="checkbox" id="c-38246998" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38246941">root</a><span>|</span><a href="#38246987">parent</a><span>|</span><a href="#38247309">next</a><span>|</span><label class="collapse" for="c-38246998">[-]</label><label class="expand" for="c-38246998">[6 more]</label></div><br/><div class="children"><div class="content">Right, but if the port achieves performance gains over GGML, which is already highly performant, that’s a. Wild b. a signal to move further GGML development into Rust, no?</div><br/><div id="38247106" class="c"><input type="checkbox" id="c-38247106" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#38246941">root</a><span>|</span><a href="#38246998">parent</a><span>|</span><a href="#38247041">next</a><span>|</span><label class="collapse" for="c-38247106">[-]</label><label class="expand" for="c-38247106">[1 more]</label></div><br/><div class="children"><div class="content">ML has extremely predictable and heavily optimized routines.  Languages that can target hardware ISA all tend to have comparable perf and there’s no reason to think Rust would offer much.</div><br/></div></div><div id="38247041" class="c"><input type="checkbox" id="c-38247041" checked=""/><div class="controls bullet"><span class="by">Nevin1901</span><span>|</span><a href="#38246941">root</a><span>|</span><a href="#38246998">parent</a><span>|</span><a href="#38247106">prev</a><span>|</span><a href="#38247315">next</a><span>|</span><label class="collapse" for="c-38247041">[-]</label><label class="expand" for="c-38247041">[1 more]</label></div><br/><div class="children"><div class="content">How would wasm&#x2F;rust be more performant over c++? I’m not sure the wasm version can take advantage of avx&#x2F;metal.<p>Edit: the wasm installer does take advantage by installing plugins.<p>Unless you’re talking about performance on devices where those two weren’t a thing anyways.</div><br/></div></div><div id="38247315" class="c"><input type="checkbox" id="c-38247315" checked=""/><div class="controls bullet"><span class="by">cozzyd</span><span>|</span><a href="#38246941">root</a><span>|</span><a href="#38246998">parent</a><span>|</span><a href="#38247041">prev</a><span>|</span><a href="#38247883">next</a><span>|</span><label class="collapse" for="c-38247315">[-]</label><label class="expand" for="c-38247315">[2 more]</label></div><br/><div class="children"><div class="content">As far as I understand, only the &quot;driver&quot; code is in rust. Everything else is just C++ compiled to WASM. Maybe it&#x27;s slightly better to have the driver code be in rust than python or scheme or whatever, but I imagine C++ would be basically equivalent (and.... you wouldn&#x27;t have to go through the trouble of compiling to WASM which likely loses significant performance).</div><br/><div id="38248151" class="c"><input type="checkbox" id="c-38248151" checked=""/><div class="controls bullet"><span class="by">kamray23</span><span>|</span><a href="#38246941">root</a><span>|</span><a href="#38247315">parent</a><span>|</span><a href="#38247883">next</a><span>|</span><label class="collapse" for="c-38248151">[-]</label><label class="expand" for="c-38248151">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what I find weird here. The bit of the code written in rust is almost comically tiny, and the rest is just C++ you compiled to WASM which someone else already wrote. I think comparing this to a Python wrapper for the same code would produce very minimal difference in performance, because the majority goes into performance and formatting the prompt string really isn&#x27;t that complex of a task. I just don&#x27;t see what advantage Rust produces here other than the fact that it&#x27;s a language you can compile to WASM so that you have one binary.</div><br/></div></div></div></div><div id="38247883" class="c"><input type="checkbox" id="c-38247883" checked=""/><div class="controls bullet"><span class="by">tomalbrc</span><span>|</span><a href="#38246941">root</a><span>|</span><a href="#38246998">parent</a><span>|</span><a href="#38247315">prev</a><span>|</span><a href="#38247309">next</a><span>|</span><label class="collapse" for="c-38247883">[-]</label><label class="expand" for="c-38247883">[1 more]</label></div><br/><div class="children"><div class="content">There is no mention of it running faster than the original llama2.cpp, if anything it is slower</div><br/></div></div></div></div></div></div><div id="38247309" class="c"><input type="checkbox" id="c-38247309" checked=""/><div class="controls bullet"><span class="by">est</span><span>|</span><a href="#38246941">parent</a><span>|</span><a href="#38246987">prev</a><span>|</span><a href="#38247003">next</a><span>|</span><label class="collapse" for="c-38247309">[-]</label><label class="expand" for="c-38247309">[3 more]</label></div><br/><div class="children"><div class="content">&gt; this is a “holy shit” moment for Rust in AI applications<p>Yeah because I realized the 2MB is just a wrapper that reads stdin and offloads everything to wasi-nn API.<p>&gt; The core Rust source code is very simple. It is only 40 lines of code. The Rust program manages the user input, tracks the conversation history, transforms the text into the llama2’s chat template, and runs the inference operations using the WASI NN API.<p>You can do the same using Python with fewer lines of code and maybe smaller executable size.</div><br/><div id="38247394" class="c"><input type="checkbox" id="c-38247394" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#38246941">root</a><span>|</span><a href="#38247309">parent</a><span>|</span><a href="#38247003">next</a><span>|</span><label class="collapse" for="c-38247394">[-]</label><label class="expand" for="c-38247394">[2 more]</label></div><br/><div class="children"><div class="content">Pretty damning if 40 lines of rust to read stdin generates a 2 MB binary!</div><br/><div id="38247539" class="c"><input type="checkbox" id="c-38247539" checked=""/><div class="controls bullet"><span class="by">lakpan</span><span>|</span><a href="#38246941">root</a><span>|</span><a href="#38247394">parent</a><span>|</span><a href="#38247003">next</a><span>|</span><label class="collapse" for="c-38247539">[-]</label><label class="expand" for="c-38247539">[1 more]</label></div><br/><div class="children"><div class="content">Presumably that also accounts for the WASM itself</div><br/></div></div></div></div></div></div><div id="38247003" class="c"><input type="checkbox" id="c-38247003" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#38246941">parent</a><span>|</span><a href="#38247309">prev</a><span>|</span><a href="#38247620">next</a><span>|</span><label class="collapse" for="c-38247003">[-]</label><label class="expand" for="c-38247003">[1 more]</label></div><br/><div class="children"><div class="content">yeah excited to see how this will evolve. BTW, maybe give it a try on your Mac and see how it performs.</div><br/></div></div></div></div></div></div></div></div></div></body></html>