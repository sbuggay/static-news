<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736586060714" as="style"/><link rel="stylesheet" href="styles.css?v=1736586060714"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://unsloth.ai/blog/phi4">Phi-4 Bug Fixes</a> <span class="domain">(<a href="https://unsloth.ai">unsloth.ai</a>)</span></div><div class="subtext"><span>danielhanchen</span> | <span>45 comments</span></div><br/><div><div id="42660336" class="c"><input type="checkbox" id="c-42660336" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662771">next</a><span>|</span><label class="collapse" for="c-42660336">[-]</label><label class="expand" for="c-42660336">[15 more]</label></div><br/><div class="children"><div class="content">Hey HN family! I found a few bugs for Phi-4 - Microsoft&#x27;s latest MIT licensed LLM to be on par with GPT-4o mini<p>1. End of sentence should be &lt;|im_end|&gt; not &lt;|endoftext|&gt;<p>2. Chat template should not auto add an assistant prompt<p>3. Padding token should not be EOS but &lt;|dummy_87|&gt;<p>I also converted Phi-4 to Llama-arch. I uploaded GGUFs, 4bit quants, dynamic quants and all fixes to <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth</a><p>I also made a Colab notebook to finetune Phi-4 on a free GPU: <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;github&#x2F;unslothai&#x2F;notebooks&#x2F;blob&#x2F;main&#x2F;nb&#x2F;Phi_4-Conversational.ipynb" rel="nofollow">https:&#x2F;&#x2F;colab.research.google.com&#x2F;github&#x2F;unslothai&#x2F;notebooks...</a></div><br/><div id="42662126" class="c"><input type="checkbox" id="c-42662126" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#42660336">parent</a><span>|</span><a href="#42661387">next</a><span>|</span><label class="collapse" for="c-42662126">[-]</label><label class="expand" for="c-42662126">[4 more]</label></div><br/><div class="children"><div class="content">&gt; We converted Phi-4 to Llama’s architecture for better accuracy and easier use.<p>What does this mean? When I think about &quot;model architecture&quot;, I think about the number of weights in each layer, the organization of the layers, etc. And AFAIK, it&#x27;s untenable to &quot;port&quot; a model from one to the other without effectively retraining it. So what does it actually mean to &quot;convert to Llama&#x27;s architecture&quot;?</div><br/><div id="42662377" class="c"><input type="checkbox" id="c-42662377" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42662126">parent</a><span>|</span><a href="#42662190">next</a><span>|</span><label class="collapse" for="c-42662377">[-]</label><label class="expand" for="c-42662377">[1 more]</label></div><br/><div class="children"><div class="content">Oh Phi-4&#x27;s architecture is inspired from Llama itself, except they merged the attention matrices into 1 large matrix for better FLOP utilization, and the gate&#x2F;up matrices in the MLP.<p>Phi-3 use to use sliding window attention, but they got rid of that in Phi-4.<p>So, you can &quot;Mistral-fy&quot; Phi-3 and convert it to Mistral arch (by unmerging the merges), and now you can &quot;Llama-fy&quot; Phi-4 to Llama arch.<p>The reason why accuracy increases in finetuning is because during LoRA finetuning, you learn only 1 A matrix for merged QKV, whilst unmerging it creates 3 A matrices - this allows the model to have more freedom to learn new features.</div><br/></div></div><div id="42662190" class="c"><input type="checkbox" id="c-42662190" checked=""/><div class="controls bullet"><span class="by">Sn0wCoder</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42662126">parent</a><span>|</span><a href="#42662377">prev</a><span>|</span><a href="#42661387">next</a><span>|</span><label class="collapse" for="c-42662190">[-]</label><label class="expand" for="c-42662190">[2 more]</label></div><br/><div class="children"><div class="content">Would guess GGUF so you can run on llama.cpp, LM Studio, etc..., but OP can hopefully clarity further for you.</div><br/><div id="42662379" class="c"><input type="checkbox" id="c-42662379" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42662190">parent</a><span>|</span><a href="#42661387">next</a><span>|</span><label class="collapse" for="c-42662379">[-]</label><label class="expand" for="c-42662379">[1 more]</label></div><br/><div class="children"><div class="content">Yep converting to Llama arch definitely makes accessibility much better - also many fast LLM serving libraries normally support Llama, so it makes it easier to port and use!</div><br/></div></div></div></div></div></div><div id="42661387" class="c"><input type="checkbox" id="c-42661387" checked=""/><div class="controls bullet"><span class="by">sunaookami</span><span>|</span><a href="#42660336">parent</a><span>|</span><a href="#42662126">prev</a><span>|</span><a href="#42661600">next</a><span>|</span><label class="collapse" for="c-42661387">[-]</label><label class="expand" for="c-42661387">[4 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t Phi-3 also bugged&#x2F;is still bugged? Seems like Microsoft just doesn&#x27;t care.<p>&gt;to be on par with GPT-4o mini<p>Phi is known to overfit benchmarks. It&#x27;s way, way worse then that.</div><br/><div id="42662144" class="c"><input type="checkbox" id="c-42662144" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42661387">parent</a><span>|</span><a href="#42661633">next</a><span>|</span><label class="collapse" for="c-42662144">[-]</label><label class="expand" for="c-42662144">[1 more]</label></div><br/><div class="children"><div class="content">Phi-3 should be fixed as well - but yes there were bugs as well! <a href="https:&#x2F;&#x2F;x.com&#x2F;danielhanchen&#x2F;status&#x2F;1782853167572832650" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;danielhanchen&#x2F;status&#x2F;1782853167572832650</a><p>Phi-3&#x27;s sliding window should be 2048 and not 2047, and they also had chat template issues - I uploaded correct versions to <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;Phi-3.5-mini-instruct" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;Phi-3.5-mini-instruct</a></div><br/></div></div><div id="42661633" class="c"><input type="checkbox" id="c-42661633" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42661387">parent</a><span>|</span><a href="#42662144">prev</a><span>|</span><a href="#42661600">next</a><span>|</span><label class="collapse" for="c-42661633">[-]</label><label class="expand" for="c-42661633">[2 more]</label></div><br/><div class="children"><div class="content">Anecdotally, I&#x27;ve been experimenting with Phi-4 the past hour or so (so, yeah, not very comprehensive) and it&#x27;s certainly a strong model. Definitely better than the previous Phi models.</div><br/><div id="42662159" class="c"><input type="checkbox" id="c-42662159" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42661633">parent</a><span>|</span><a href="#42661600">next</a><span>|</span><label class="collapse" for="c-42662159">[-]</label><label class="expand" for="c-42662159">[1 more]</label></div><br/><div class="children"><div class="content">Yep Phi-4 definitely is better than Phi-3.5!</div><br/></div></div></div></div></div></div><div id="42661600" class="c"><input type="checkbox" id="c-42661600" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42660336">parent</a><span>|</span><a href="#42661387">prev</a><span>|</span><a href="#42662389">next</a><span>|</span><label class="collapse" for="c-42661600">[-]</label><label class="expand" for="c-42661600">[2 more]</label></div><br/><div class="children"><div class="content">Huh! That may explain why I kept on getting visible &lt;|im_end|&gt; output when I tried running a Phi-4 GGUF file using llama.cpp.</div><br/><div id="42662106" class="c"><input type="checkbox" id="c-42662106" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42661600">parent</a><span>|</span><a href="#42662389">next</a><span>|</span><label class="collapse" for="c-42662106">[-]</label><label class="expand" for="c-42662106">[1 more]</label></div><br/><div class="children"><div class="content">Oh yes exactly! I trimmed it out now :)<p>The better chat template should be:<p>{% for message in messages %}{% if (message[&#x27;role&#x27;] == &#x27;system&#x27;) %}{{&#x27;&lt;|im_start|&gt;system&lt;|im_sep|&gt;&#x27; + message[&#x27;content&#x27;] + &#x27;&lt;|im_end|&gt;&#x27;}}{% elif (message[&#x27;role&#x27;] == &#x27;user&#x27;) %}{{&#x27;&lt;|im_start|&gt;user&lt;|im_sep|&gt;&#x27; + message[&#x27;content&#x27;] + &#x27;&lt;|im_end|&gt;&#x27;}}{% elif (message[&#x27;role&#x27;] == &#x27;assistant&#x27;) %}{{&#x27;&lt;|im_start|&gt;assistant&lt;|im_sep|&gt;&#x27; + message[&#x27;content&#x27;] + &#x27;&lt;|im_end|&gt;&#x27;}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ &#x27;&lt;|im_start|&gt;assistant&lt;|im_sep|&gt;&#x27; }}{% endif %}</div><br/></div></div></div></div><div id="42662389" class="c"><input type="checkbox" id="c-42662389" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#42660336">parent</a><span>|</span><a href="#42661600">prev</a><span>|</span><a href="#42664193">next</a><span>|</span><label class="collapse" for="c-42662389">[-]</label><label class="expand" for="c-42662389">[3 more]</label></div><br/><div class="children"><div class="content">Can you convert to ONNX so I can try in web browser?</div><br/><div id="42662400" class="c"><input type="checkbox" id="c-42662400" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42662389">parent</a><span>|</span><a href="#42662446">next</a><span>|</span><label class="collapse" for="c-42662400">[-]</label><label class="expand" for="c-42662400">[1 more]</label></div><br/><div class="children"><div class="content">Would like to update this:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;webml-community&#x2F;phi-3.5-webgpu" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;webml-community&#x2F;phi-3.5-webgpu</a></div><br/></div></div><div id="42662446" class="c"><input type="checkbox" id="c-42662446" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42660336">root</a><span>|</span><a href="#42662389">parent</a><span>|</span><a href="#42662400">prev</a><span>|</span><a href="#42664193">next</a><span>|</span><label class="collapse" for="c-42662446">[-]</label><label class="expand" for="c-42662446">[1 more]</label></div><br/><div class="children"><div class="content">Oh I can probs try doing this!</div><br/></div></div></div></div></div></div><div id="42662771" class="c"><input type="checkbox" id="c-42662771" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42660336">prev</a><span>|</span><a href="#42661679">next</a><span>|</span><label class="collapse" for="c-42662771">[-]</label><label class="expand" for="c-42662771">[1 more]</label></div><br/><div class="children"><div class="content">Update: The Phi-4 team is actively working on adding all our fixes into the original model! <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;phi-4&#x2F;discussions&#x2F;21" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;phi-4&#x2F;discussions&#x2F;21</a></div><br/></div></div><div id="42661679" class="c"><input type="checkbox" id="c-42661679" checked=""/><div class="controls bullet"><span class="by">t1amat</span><span>|</span><a href="#42662771">prev</a><span>|</span><a href="#42663035">next</a><span>|</span><label class="collapse" for="c-42661679">[-]</label><label class="expand" for="c-42661679">[2 more]</label></div><br/><div class="children"><div class="content">Daniel’s fixes to Phi-4 make it the best scoring Phi-4 on HF’s Open LLM Leaderboard.  Great job on that.<p>Unsloth is a masterpiece, keep up the great work!</div><br/><div id="42662162" class="c"><input type="checkbox" id="c-42662162" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42661679">parent</a><span>|</span><a href="#42663035">next</a><span>|</span><label class="collapse" for="c-42662162">[-]</label><label class="expand" for="c-42662162">[1 more]</label></div><br/><div class="children"><div class="content">Thanks a lot!</div><br/></div></div></div></div><div id="42663035" class="c"><input type="checkbox" id="c-42663035" checked=""/><div class="controls bullet"><span class="by">NooneAtAll3</span><span>|</span><a href="#42661679">prev</a><span>|</span><a href="#42662872">next</a><span>|</span><label class="collapse" for="c-42663035">[-]</label><label class="expand" for="c-42663035">[4 more]</label></div><br/><div class="children"><div class="content">Application Error<p>TypeError: m(...).findLast is not a function<p>at L (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;root-DexjOeLv.js:1:340">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;root-DexjOeLv.js:1:340</a>)<p>at ia (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:30549">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:30549</a>)<p>at Ac (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:98661">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:98661</a>)<p>at Am (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:94250">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:94250</a>)<p>at o0 (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:93401">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:93401</a>)<p>at ha (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:93212">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:93212</a>)<p>at Mm (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:90555">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:90555</a>)<p>at Om (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:89963">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:7:89963</a>)<p>at MessagePort.M (<a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:1:11235">https:&#x2F;&#x2F;unsloth.ai&#x2F;assets&#x2F;components-D38fXVcE.js:1:11235</a></div><br/><div id="42663103" class="c"><input type="checkbox" id="c-42663103" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42663035">parent</a><span>|</span><a href="#42662872">next</a><span>|</span><label class="collapse" for="c-42663103">[-]</label><label class="expand" for="c-42663103">[3 more]</label></div><br/><div class="children"><div class="content">Sorry are there some issues with our website?</div><br/><div id="42664121" class="c"><input type="checkbox" id="c-42664121" checked=""/><div class="controls bullet"><span class="by">NooneAtAll3</span><span>|</span><a href="#42663035">root</a><span>|</span><a href="#42663103">parent</a><span>|</span><a href="#42662872">next</a><span>|</span><label class="collapse" for="c-42664121">[-]</label><label class="expand" for="c-42664121">[2 more]</label></div><br/><div class="children"><div class="content">yep, it appears for a second - then displays only this :(</div><br/><div id="42664265" class="c"><input type="checkbox" id="c-42664265" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42663035">root</a><span>|</span><a href="#42664121">parent</a><span>|</span><a href="#42662872">next</a><span>|</span><label class="collapse" for="c-42664265">[-]</label><label class="expand" for="c-42664265">[1 more]</label></div><br/><div class="children"><div class="content">Oh no :( Do you know which device &#x2F; platform?</div><br/></div></div></div></div></div></div></div></div><div id="42662872" class="c"><input type="checkbox" id="c-42662872" checked=""/><div class="controls bullet"><span class="by">excerionsforte</span><span>|</span><a href="#42663035">prev</a><span>|</span><a href="#42661442">next</a><span>|</span><label class="collapse" for="c-42662872">[-]</label><label class="expand" for="c-42662872">[3 more]</label></div><br/><div class="children"><div class="content">Available on Ollama already: <a href="https:&#x2F;&#x2F;ollama.com&#x2F;vanilj&#x2F;phi-4-unsloth">https:&#x2F;&#x2F;ollama.com&#x2F;vanilj&#x2F;phi-4-unsloth</a></div><br/><div id="42663974" class="c"><input type="checkbox" id="c-42663974" checked=""/><div class="controls bullet"><span class="by">tandr</span><span>|</span><a href="#42662872">parent</a><span>|</span><a href="#42662887">next</a><span>|</span><label class="collapse" for="c-42663974">[-]</label><label class="expand" for="c-42663974">[1 more]</label></div><br/><div class="children"><div class="content">looking at &quot;original&quot; Phi4 on ollama, it looks like they have fixed parameters issue for im_start&#x2F;end</div><br/></div></div><div id="42662887" class="c"><input type="checkbox" id="c-42662887" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662872">parent</a><span>|</span><a href="#42663974">prev</a><span>|</span><a href="#42661442">next</a><span>|</span><label class="collapse" for="c-42662887">[-]</label><label class="expand" for="c-42662887">[1 more]</label></div><br/><div class="children"><div class="content">Oh fabulous! :)</div><br/></div></div></div></div><div id="42661442" class="c"><input type="checkbox" id="c-42661442" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#42662872">prev</a><span>|</span><a href="#42662227">next</a><span>|</span><label class="collapse" for="c-42661442">[-]</label><label class="expand" for="c-42661442">[2 more]</label></div><br/><div class="children"><div class="content">The benchmark results of the model before and after the &quot;fixes&quot; do not match numbers reported in the model card: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;phi-4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;phi-4</a><p>According to Microsoft MATH score should be 80.4, while both original and the &quot;fixed&quot; models as run by unsloth only score just over 12.3. So either Microsoft made a few huge mistakes, or unsloth was not able to run their model correctly.</div><br/><div id="42662208" class="c"><input type="checkbox" id="c-42662208" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42661442">parent</a><span>|</span><a href="#42662227">next</a><span>|</span><label class="collapse" for="c-42662208">[-]</label><label class="expand" for="c-42662208">[1 more]</label></div><br/><div class="children"><div class="content">Oh yes I found this to be a bit strange - I uploaded our versions and Microsoft&#x27;s own version to Hugging Face&#x27;s public LLM leaderboard - <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_leaderboard#&#x2F;?search=phi-4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_...</a><p>You can see Microsoft&#x27;s own original Phi-3 scores 12.31% - I&#x27;m unsure why. My fixes at least pushes it to 20%.<p>It&#x27;s possible because HF&#x27;s benchmark does &quot;Scoring: Exact match: Was the solution generated correct and in the expected format&quot; which might be the issue</div><br/></div></div></div></div><div id="42662227" class="c"><input type="checkbox" id="c-42662227" checked=""/><div class="controls bullet"><span class="by">adultSwim</span><span>|</span><a href="#42661442">prev</a><span>|</span><a href="#42662039">next</a><span>|</span><label class="collapse" for="c-42662227">[-]</label><label class="expand" for="c-42662227">[4 more]</label></div><br/><div class="children"><div class="content">Are there alternatives to unsloth?<p>I would love to use it but the open&#x2F;free version only handles one GPU, and it&#x27;s unclear how much the paid version would cost. I have some limited access to multiple older NVidia cards and would love to make better use of them while I&#x27;m still learning. My budget for learning&#x2F;projects is rather modest.<p>Hopefully they succeed. At work I could make a strong case for going with them as they allow keeping data local only, instead of relying on an API.</div><br/><div id="42662387" class="c"><input type="checkbox" id="c-42662387" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662227">parent</a><span>|</span><a href="#42662039">next</a><span>|</span><label class="collapse" for="c-42662387">[-]</label><label class="expand" for="c-42662387">[3 more]</label></div><br/><div class="children"><div class="content">Multi GPU support is definitely coming to Unsloth OSS! Our goal was to release it this month, but unsure on exact timelines - maybe next month!!</div><br/><div id="42663368" class="c"><input type="checkbox" id="c-42663368" checked=""/><div class="controls bullet"><span class="by">adultSwim</span><span>|</span><a href="#42662227">root</a><span>|</span><a href="#42662387">parent</a><span>|</span><a href="#42662039">next</a><span>|</span><label class="collapse" for="c-42663368">[-]</label><label class="expand" for="c-42663368">[2 more]</label></div><br/><div class="children"><div class="content">Thank you!</div><br/><div id="42664269" class="c"><input type="checkbox" id="c-42664269" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662227">root</a><span>|</span><a href="#42663368">parent</a><span>|</span><a href="#42662039">next</a><span>|</span><label class="collapse" for="c-42664269">[-]</label><label class="expand" for="c-42664269">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll ping you when it comes along!</div><br/></div></div></div></div></div></div></div></div><div id="42662039" class="c"><input type="checkbox" id="c-42662039" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#42662227">prev</a><span>|</span><a href="#42661963">next</a><span>|</span><label class="collapse" for="c-42662039">[-]</label><label class="expand" for="c-42662039">[7 more]</label></div><br/><div class="children"><div class="content">&quot;Yes it improves performance!&quot; <i>proceeds to show the most unconvincing stats ever</i><p>you can probably blow on your GPU and get a similar performance change</div><br/><div id="42662344" class="c"><input type="checkbox" id="c-42662344" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662039">parent</a><span>|</span><a href="#42662238">next</a><span>|</span><label class="collapse" for="c-42662344">[-]</label><label class="expand" for="c-42662344">[1 more]</label></div><br/><div class="children"><div class="content">I uploaded our fixed versions to <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_leaderboard#&#x2F;?search=phi-4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_...</a> which show the difference in scores.<p>I agree it&#x27;s not super convincing, so I provided anecdotal evidence as well - I&#x27;ll work with the Phi-4 team to upstream these fixes!<p>PS for further credibility, we also fixed 8 bugs in Gemma 1 - see <a href="https:&#x2F;&#x2F;x.com&#x2F;danielhanchen&#x2F;status&#x2F;1765446273661075609" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;danielhanchen&#x2F;status&#x2F;1765446273661075609</a> , multiple bugs in Llama, Mistral, Qwen and other models</div><br/></div></div><div id="42662238" class="c"><input type="checkbox" id="c-42662238" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42662039">parent</a><span>|</span><a href="#42662344">prev</a><span>|</span><a href="#42661963">next</a><span>|</span><label class="collapse" for="c-42662238">[-]</label><label class="expand" for="c-42662238">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry, I don&#x27;t understand what you mean. I checked the original article again too. As it stands, my understanding is you are claiming:<p>- blowing on a GPU (which I take to mean doing roughly nothing)<p>- gets roughly the same perf change<p>- as moving from fp16 to q4</div><br/><div id="42662416" class="c"><input type="checkbox" id="c-42662416" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662039">root</a><span>|</span><a href="#42662238">parent</a><span>|</span><a href="#42662784">next</a><span>|</span><label class="collapse" for="c-42662416">[-]</label><label class="expand" for="c-42662416">[3 more]</label></div><br/><div class="children"><div class="content">Are you referring to the finetuning part?<p>The multiple bug fixes are separate from the finetuning sections - Unsloth itself makes finetuning 2x faster and use 70% less memory - the bug fixes are totally detached from finetuning - ie you can take the fixed version we uploaded at <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;phi-4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;phi-4</a>, and use it in any framework or inference engine.<p>Apologies I&#x27;m confused on the comment sorry.<p>If you&#x27;re questioning the credibility of the bug fixes - we fixed 8 bugs in Gemma <a href="https:&#x2F;&#x2F;x.com&#x2F;danielhanchen&#x2F;status&#x2F;1765446273661075609" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;danielhanchen&#x2F;status&#x2F;1765446273661075609</a>, multiple bugs in Llama, Mistral, Qwen, a gradient accumulation bug <a href="https:&#x2F;&#x2F;x.com&#x2F;danielhanchen&#x2F;status&#x2F;1846235913443262891" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;danielhanchen&#x2F;status&#x2F;1846235913443262891</a> and much more</div><br/><div id="42663303" class="c"><input type="checkbox" id="c-42663303" checked=""/><div class="controls bullet"><span class="by">grumpopotamus</span><span>|</span><a href="#42662039">root</a><span>|</span><a href="#42662416">parent</a><span>|</span><a href="#42662784">next</a><span>|</span><label class="collapse" for="c-42663303">[-]</label><label class="expand" for="c-42663303">[2 more]</label></div><br/><div class="children"><div class="content">2x faster than what?</div><br/><div id="42663346" class="c"><input type="checkbox" id="c-42663346" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662039">root</a><span>|</span><a href="#42663303">parent</a><span>|</span><a href="#42662784">next</a><span>|</span><label class="collapse" for="c-42663346">[-]</label><label class="expand" for="c-42663346">[1 more]</label></div><br/><div class="children"><div class="content">Oh 2x faster and uses &gt;70% less memory than Hugging Face + Flash Attention 2! I did a CUDA &#x2F; GPU Mode talk about it here: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=hfb_AIhDYnA" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=hfb_AIhDYnA</a> Also to the PyTorch team here: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=MQwryfkydc0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=MQwryfkydc0</a> and the PyTorch Conference here: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=PdtKkc5jB4g" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=PdtKkc5jB4g</a></div><br/></div></div></div></div></div></div><div id="42662784" class="c"><input type="checkbox" id="c-42662784" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662039">root</a><span>|</span><a href="#42662238">parent</a><span>|</span><a href="#42662416">prev</a><span>|</span><a href="#42661963">next</a><span>|</span><label class="collapse" for="c-42662784">[-]</label><label class="expand" for="c-42662784">[1 more]</label></div><br/><div class="children"><div class="content">Update - the Phi-4 team is working on adding all our fixes to the original model! <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;phi-4&#x2F;discussions&#x2F;21" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;phi-4&#x2F;discussions&#x2F;21</a></div><br/></div></div></div></div></div></div><div id="42661963" class="c"><input type="checkbox" id="c-42661963" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42662039">prev</a><span>|</span><a href="#42662640">next</a><span>|</span><label class="collapse" for="c-42661963">[-]</label><label class="expand" for="c-42661963">[3 more]</label></div><br/><div class="children"><div class="content">Ah yes, drawing ASCII art, the de facto benchmark for evaluating LLM quality.</div><br/><div id="42662231" class="c"><input type="checkbox" id="c-42662231" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42661963">parent</a><span>|</span><a href="#42662640">next</a><span>|</span><label class="collapse" for="c-42662231">[-]</label><label class="expand" for="c-42662231">[2 more]</label></div><br/><div class="children"><div class="content">Anecdotal evidence was provided to show some Redditors tested it out - but I do agree it&#x27;s not correct to show that as an example - so I uploaded our fixed versions to Hugging Face&#x27;s public LLM leaderboard here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_leaderboard#&#x2F;?search=phi-4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_...</a> - this shows the fixes do in fact work!</div><br/></div></div></div></div><div id="42662640" class="c"><input type="checkbox" id="c-42662640" checked=""/><div class="controls bullet"><span class="by">wsintra2022</span><span>|</span><a href="#42661963">prev</a><span>|</span><label class="collapse" for="c-42662640">[-]</label><label class="expand" for="c-42662640">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Reddit comments show our fixes make Phi-4 inference much better<p>I’d like to try ‘Reddit comments show my fixes make app better’ in my next review</div><br/><div id="42662743" class="c"><input type="checkbox" id="c-42662743" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662640">parent</a><span>|</span><a href="#42662772">next</a><span>|</span><label class="collapse" for="c-42662743">[-]</label><label class="expand" for="c-42662743">[1 more]</label></div><br/><div class="children"><div class="content">Fixed versions are also independently scored by Hugging Face&#x27;s Open LLM Leaderboard: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_leaderboard#&#x2F;?search=phi-4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_...</a><p>The Reddit LocalLlama community is actually pretty cool - tonnes of research actually comes from the community - for example kaiokendev&#x27;s linear RoPE scaling, YaRN, NTK Aware RoPE Scaling, many LLM benchmarks - many researchers use LocalLlama to share research and discuss on new stuff.<p>I know a lot of AI researchers use the &quot;LocalLlama vibe check&quot; which essentially is an anecdotal approach to LLM evaluation - ie instead of relying on Chat LMsys or LLM benchmarks, 3rd party crowd sourced vibe checks sometimes do much better.</div><br/></div></div><div id="42662772" class="c"><input type="checkbox" id="c-42662772" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42662640">parent</a><span>|</span><a href="#42662743">prev</a><span>|</span><label class="collapse" for="c-42662772">[-]</label><label class="expand" for="c-42662772">[1 more]</label></div><br/><div class="children"><div class="content">As an update - the Phi-4 team is actively working on incorporating all fixes! See <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;phi-4&#x2F;discussions&#x2F;21" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;microsoft&#x2F;phi-4&#x2F;discussions&#x2F;21</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>