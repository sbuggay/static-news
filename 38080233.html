<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1698742865399" as="style"/><link rel="stylesheet" href="styles.css?v=1698742865399"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.tomshardware.com/news/dollar95-amd-cpu-becomes-16gb-gpu-to-run-ai-software">$95 AMD CPU Becomes 16GB GPU to Run AI Software</a>Â <span class="domain">(<a href="https://www.tomshardware.com">www.tomshardware.com</a>)</span></div><div class="subtext"><span>nateb2022</span> | <span>23 comments</span></div><br/><div><div id="38080592" class="c"><input type="checkbox" id="c-38080592" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38081188">next</a><span>|</span><label class="collapse" for="c-38080592">[-]</label><label class="expand" for="c-38080592">[5 more]</label></div><br/><div class="children"><div class="content">&gt; one minute and 50 seconds to generate a 512 x 512-pixel image with the default setting of 50 steps<p>2s&#x2F;iter? That is... Not great.<p>It is slower than CPU diffusion: <a href="https:&#x2F;&#x2F;github.com&#x2F;rupeshs&#x2F;fastsdcpu">https:&#x2F;&#x2F;github.com&#x2F;rupeshs&#x2F;fastsdcpu</a><p>Stable Diffusion in particular doesn&#x27;t need much VRAM anyway. I get that many people are stuck on lower end computers, but ~4GB is not a totally unreasonable requirement.</div><br/><div id="38081902" class="c"><input type="checkbox" id="c-38081902" checked=""/><div class="controls bullet"><span class="by">bongobingo1</span><span>|</span><a href="#38080592">parent</a><span>|</span><a href="#38080940">next</a><span>|</span><label class="collapse" for="c-38081902">[-]</label><label class="expand" for="c-38081902">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Stable Diffusion in particular doesn&#x27;t need much VRAM anyway. I get that many people are stuck on lower end computers, but ~4GB is not a totally unreasonable requirement<p>Wow, did it really get that low? I played with SD during the initial days, and I know there was some strides then to get it running on 8gb cards and maybe squeaking through on 6gb with a smaller image size (like, 256x256px).<p>Can you generate reasonably large images on 4gb now, with just a higher wait time? I was never sure why it wasn&#x27;t simple to just shuffle bits in and out of main RAM but I guess the model themselves are mostly big binary blobs that are hard to split up?</div><br/></div></div><div id="38080940" class="c"><input type="checkbox" id="c-38080940" checked=""/><div class="controls bullet"><span class="by">samsartor</span><span>|</span><a href="#38080592">parent</a><span>|</span><a href="#38081902">prev</a><span>|</span><a href="#38081056">next</a><span>|</span><label class="collapse" for="c-38080940">[-]</label><label class="expand" for="c-38080940">[2 more]</label></div><br/><div class="children"><div class="content">Looks more like 4 s&#x2F;iter which is actually quite good for CPU inference in Automatic111. It is an apples-to-oranges comparison to the LCM models used by fastsdcpu (since those can get away with fewer sampling steps) or to the quantized models favored by apple (since they use totally different underlying compute capabilities). Currently automatic111 and other popular GUIs are pretty focused on users with beefy GPUs that can sample images in seconds, so they haven&#x27;t yet implemented some of these really significant CPU-friendly speedups. But we&#x27;ll get there!</div><br/><div id="38081191" class="c"><input type="checkbox" id="c-38081191" checked=""/><div class="controls bullet"><span class="by">mosselman</span><span>|</span><a href="#38080592">root</a><span>|</span><a href="#38080940">parent</a><span>|</span><a href="#38081056">next</a><span>|</span><label class="collapse" for="c-38081191">[-]</label><label class="expand" for="c-38081191">[1 more]</label></div><br/><div class="children"><div class="content">Are those cpu speedups out there? Where can I find them?</div><br/></div></div></div></div><div id="38081056" class="c"><input type="checkbox" id="c-38081056" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#38080592">parent</a><span>|</span><a href="#38080940">prev</a><span>|</span><a href="#38081188">next</a><span>|</span><label class="collapse" for="c-38081056">[-]</label><label class="expand" for="c-38081056">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s probably exactly what you would expect from a GPU running off DDR4 instead off GDDR6X.</div><br/></div></div></div></div><div id="38081188" class="c"><input type="checkbox" id="c-38081188" checked=""/><div class="controls bullet"><span class="by">anaisbetts</span><span>|</span><a href="#38080592">prev</a><span>|</span><a href="#38080705">next</a><span>|</span><label class="collapse" for="c-38081188">[-]</label><label class="expand" for="c-38081188">[2 more]</label></div><br/><div class="children"><div class="content">This is similar to the Apple GPU trick where unified memory means it&#x27;s easier to load large models into GPU space, but as others have said, the AMD APU built into these chips is _really_ underpowered, even compared to other RDNA APUs like the one in the Steam Deck</div><br/><div id="38081772" class="c"><input type="checkbox" id="c-38081772" checked=""/><div class="controls bullet"><span class="by">MindSpunk</span><span>|</span><a href="#38081188">parent</a><span>|</span><a href="#38080705">next</a><span>|</span><label class="collapse" for="c-38081772">[-]</label><label class="expand" for="c-38081772">[1 more]</label></div><br/><div class="children"><div class="content">Minor nitpick but the 4600G and 5600G mentioned in the article are Vega, not RDNA. The iGPUs in those two processors are&#x2F;were quite powerful by iGPU standards at the time, especially considering they were competing with Intel HD graphics or very early Iris Xe iGPUs. Not a scratch on a good dedicated GPU though.<p>Steam Deck is RDNA2 though, and is significantly faster. I don&#x27;t think RDNA1 ever made it into an APU.</div><br/></div></div></div></div><div id="38080705" class="c"><input type="checkbox" id="c-38080705" checked=""/><div class="controls bullet"><span class="by">lithiumii</span><span>|</span><a href="#38081188">prev</a><span>|</span><a href="#38080772">next</a><span>|</span><label class="collapse" for="c-38080705">[-]</label><label class="expand" for="c-38080705">[1 more]</label></div><br/><div class="children"><div class="content">If you switch to the test-fp8 branch of AUTOMATIC1111 web UI, you can do 512 * 1024 on 4G GPU with SDXL.</div><br/></div></div><div id="38080772" class="c"><input type="checkbox" id="c-38080772" checked=""/><div class="controls bullet"><span class="by">pella</span><span>|</span><a href="#38080705">prev</a><span>|</span><a href="#38081257">next</a><span>|</span><label class="collapse" for="c-38080772">[-]</label><label class="expand" for="c-38080772">[1 more]</label></div><br/><div class="children"><div class="content">related:<p>(74 days ago )<p>&quot;AMD Ryzen APU turned into a 16GB VRAM GPU and it can run Stable Diffusion&quot;<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37162762">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37162762</a></div><br/></div></div><div id="38081257" class="c"><input type="checkbox" id="c-38081257" checked=""/><div class="controls bullet"><span class="by">ianpurton</span><span>|</span><a href="#38080772">prev</a><span>|</span><a href="#38081022">next</a><span>|</span><label class="collapse" for="c-38081257">[-]</label><label class="expand" for="c-38081257">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve just finished writing an article on hardware required for LLM&#x27;s in production <a href="https:&#x2F;&#x2F;bionic-gpt.com&#x2F;blog&#x2F;llm-hardware&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;bionic-gpt.com&#x2F;blog&#x2F;llm-hardware&#x2F;</a><p>Would be interesting to see how this performed running Llama 2 7b.</div><br/><div id="38081579" class="c"><input type="checkbox" id="c-38081579" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38081257">parent</a><span>|</span><a href="#38081022">next</a><span>|</span><label class="collapse" for="c-38081579">[-]</label><label class="expand" for="c-38081579">[1 more]</label></div><br/><div class="children"><div class="content">Last month I did testing on the latest gen AMD APU (7940HS @ 65W - 8C Zen4 CPU, Radeon 780M RDNA3 GPU) on how llama2-7b (4-bit quants) performed w&#x2F; a variety of inferencers (llama.cpp, exllama v2, mlc w&#x2F; CPU, CLBlast, ROCm, and Vulkan backends) and only llama.cpp&#x27;s ROCm implementation convincingly beating CPU inferencing.<p>Even then, interactive (batch=1) inferencing was still barely faster. I suspect memory bandwidth (I ran w&#x2F; 2 x DDR5-5600) is the main bottleneck. Just as a point of reference, the inferencing performance is roughly in the ballpark of (but slightly lower than) my 16GB M2 MBA.<p>Details: <a href="https:&#x2F;&#x2F;docs.google.com&#x2F;spreadsheets&#x2F;d&#x2F;1kT4or6b0Fedd-W_jMwYpb63e1ZR3aePczz3zlbJW-Y4&#x2F;edit#gid=1041125589" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.google.com&#x2F;spreadsheets&#x2F;d&#x2F;1kT4or6b0Fedd-W_jMwYp...</a></div><br/></div></div></div></div><div id="38081022" class="c"><input type="checkbox" id="c-38081022" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#38081257">prev</a><span>|</span><label class="collapse" for="c-38081022">[-]</label><label class="expand" for="c-38081022">[11 more]</label></div><br/><div class="children"><div class="content">At this point the M3 Max in a MacBook Pro with 96&#x2F;128GB V&#x2F;RAM needs to be benched against various Nvidia cards like 4090 &amp; the H100 for $&#x2F;performance numbers</div><br/><div id="38081500" class="c"><input type="checkbox" id="c-38081500" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38081022">parent</a><span>|</span><a href="#38081130">next</a><span>|</span><label class="collapse" for="c-38081500">[-]</label><label class="expand" for="c-38081500">[3 more]</label></div><br/><div class="children"><div class="content">The sad thing is <i>all</i> of this hardware is outrageously overpriced.<p>Its even sadder that AMD&#x2F;Intel have failed to take advantage of the price gouging. They could have released affordable 32GB-48GB cards with (afaik) no silicon changes.<p>The 7900 XTX is still somewhat reasonable where you can get it to work.</div><br/><div id="38081619" class="c"><input type="checkbox" id="c-38081619" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#38081022">root</a><span>|</span><a href="#38081500">parent</a><span>|</span><a href="#38081130">next</a><span>|</span><label class="collapse" for="c-38081619">[-]</label><label class="expand" for="c-38081619">[2 more]</label></div><br/><div class="children"><div class="content">True, Intel is trying its best at both hardware &amp; recently software contributions like Neural Compressor, so let&#x27;s see how far they take nvidia to task</div><br/><div id="38081638" class="c"><input type="checkbox" id="c-38081638" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38081022">root</a><span>|</span><a href="#38081619">parent</a><span>|</span><a href="#38081130">next</a><span>|</span><label class="collapse" for="c-38081638">[-]</label><label class="expand" for="c-38081638">[1 more]</label></div><br/><div class="children"><div class="content">Well, they never released a 32GB A770. This is particularly perplexing, as they have no professional market to lose like AMD does.<p>And the iGPUs are still pretty small.<p>We will see how Meteor Lake and Battlemage shake out, but I don&#x27;t like the delay rumors.</div><br/></div></div></div></div></div></div><div id="38081130" class="c"><input type="checkbox" id="c-38081130" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#38081022">parent</a><span>|</span><a href="#38081500">prev</a><span>|</span><a href="#38081280">next</a><span>|</span><label class="collapse" for="c-38081130">[-]</label><label class="expand" for="c-38081130">[3 more]</label></div><br/><div class="children"><div class="content">Why the H100? Comparing against similarly priced consumer gear (3x 4090) makes a lot more sense, but I think you already know that one of these systems has an order of magnitude better perf&#x2F;$ than the other</div><br/><div id="38081168" class="c"><input type="checkbox" id="c-38081168" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#38081022">root</a><span>|</span><a href="#38081130">parent</a><span>|</span><a href="#38081280">next</a><span>|</span><label class="collapse" for="c-38081168">[-]</label><label class="expand" for="c-38081168">[2 more]</label></div><br/><div class="children"><div class="content">Yeah even that as well</div><br/><div id="38081720" class="c"><input type="checkbox" id="c-38081720" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#38081022">root</a><span>|</span><a href="#38081168">parent</a><span>|</span><a href="#38081280">next</a><span>|</span><label class="collapse" for="c-38081720">[-]</label><label class="expand" for="c-38081720">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if you missed my point. 3x 4090 will be 10x faster while costing the same. There&#x27;s no point making the comparison.<p>(Assuming both machines are running the same model. I see a lotta crazy numbers thrown around based on comparing very different quantizations)</div><br/></div></div></div></div></div></div><div id="38081280" class="c"><input type="checkbox" id="c-38081280" checked=""/><div class="controls bullet"><span class="by">ianpurton</span><span>|</span><a href="#38081022">parent</a><span>|</span><a href="#38081130">prev</a><span>|</span><a href="#38081559">next</a><span>|</span><label class="collapse" for="c-38081280">[-]</label><label class="expand" for="c-38081280">[3 more]</label></div><br/><div class="children"><div class="content">Running LLama 2 70b a Mac Studio gets 13 tokens per second.<p>2 x 4090 is more like 38 tokens per second.<p>But it&#x27;s harder than that. As the 2 x 4090 is using <a href="https:&#x2F;&#x2F;llm.mlc.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm.mlc.ai&#x2F;</a> which is highly optimized so perhaps the Mac Studio inference could be better when tuned.<p>Then a lot of production inference engines use continous batching, which means they perform really well when processing multiple prompts at a time.</div><br/><div id="38081491" class="c"><input type="checkbox" id="c-38081491" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38081022">root</a><span>|</span><a href="#38081280">parent</a><span>|</span><a href="#38081574">next</a><span>|</span><label class="collapse" for="c-38081491">[-]</label><label class="expand" for="c-38081491">[1 more]</label></div><br/><div class="children"><div class="content">You can squeeze 70B onto a single 4090&#x2F;3090 with exllamav2.<p>As of the latest 8 bit cache commit, I can manage 2.6bpw with seemingly good quality.</div><br/></div></div><div id="38081574" class="c"><input type="checkbox" id="c-38081574" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#38081022">root</a><span>|</span><a href="#38081280">parent</a><span>|</span><a href="#38081491">prev</a><span>|</span><a href="#38081559">next</a><span>|</span><label class="collapse" for="c-38081574">[-]</label><label class="expand" for="c-38081574">[1 more]</label></div><br/><div class="children"><div class="content">What quantization are you talking about for these?</div><br/></div></div></div></div><div id="38081559" class="c"><input type="checkbox" id="c-38081559" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#38081022">parent</a><span>|</span><a href="#38081280">prev</a><span>|</span><label class="collapse" for="c-38081559">[-]</label><label class="expand" for="c-38081559">[1 more]</label></div><br/><div class="children"><div class="content">For inference or training?<p>For training, Apple has no CUDA, so you&#x27;re pretty much dead in the water, unless you&#x27;re specifically interested in e.g. local fine-tuning (for e.g. privacy or start-up strategy of &quot;use the user&#x27;s compute&quot;).<p>For inference, 4090 and H100 are not designed for throughput per $.  Nvidia GPUs like the A40 or T40 are tuned for cloud inference, but at the end of the day your product is either running inference server-side (so play your own CAPEX vs OPEX, maybe even try Jetson) or user-side and so the user chooses the hardware not you.<p>I mean maybe there are Wallstreet analysts or people at Qualcomm&#x2F;Broadcom who want an Apple vs Nvidia shootout to see who is getting the most of their silicon, but for the product space the comparison is basically irrelevant.  Which is likely what Apple and Nvidia want right now.</div><br/></div></div></div></div></div></div></div></div></div></body></html>